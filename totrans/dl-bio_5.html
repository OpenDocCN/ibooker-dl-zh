<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Detecting Skin Cancer in Medical Images"><div class="chapter" id="detecting-skin-cancer-in-medical-images">
 <h1><span class="label">Chapter 5. </span>Detecting Skin Cancer in Medical Images</h1>
 <p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-type="indexterm" id="ch05_cancer.html0"/>In previous chapters, we focused on small-scale biological phenomena, such as the molecular properties of proteins, DNA sequences, and drug molecules. In this chapter, we will zoom out to a larger biological scale, applying deep learning to analyze tissue-level and disease-related processes. Specifically, we will train a skin cancer detection model to classify images of skin into various cancerous or benign categories.
 </p>
 <p>This is an exciting application because deep learning models have made significant strides in skin analysis, with studies achieving dermatologist-level accuracy in distinguishing malignant from benign lesions since at least 2018.<sup><a data-type="noteref" id="id819-marker" href="ch05.html#id819">1</a></sup> While challenges remain in integrating these models into clinical workflows—such as regulatory approval, data standardization, and prediction explainability—their potential to assist medical professionals by enhancing early detection and reducing unnecessary biopsies is highly promising.
 </p>
 <p>We will be using skin cancer image data from the <a href="https://oreil.ly/h2DiY">International Skin Imaging Collaboration (ISIC)</a>, a project dedicated to advancing skin cancer imaging research and providing standardized datasets. Over the years, ISIC has released a range of challenges focused on skin lesion classification and pathology, with an increasing number of images available. To learn more, you can read this review paper of ISIC datasets and benchmarks.<sup><a data-type="noteref" id="id820-marker" href="ch05.html#id820">2</a></sup></p>
 <p>The dataset we will use is available as the <a href="https://oreil.ly/_2jqU">“Skin Cancer ISIC” challenge on Kaggle</a>, making it well prepared and relatively easy to get started with. However, to ensure that we cover important lessons on handling real-world data challenges, we have intentionally chosen a dataset that is relatively small and has significant class imbalance, allowing us to explore techniques for mitigating these issues.
 </p>
 <p>One advantage of working with image data is that humans are naturally skilled at interpreting visual information, enabling us to sanity-check both the dataset and model predictions. Throughout this chapter, we will examine many images to guide our modeling decisions. This will also highlight why skin cancer classification is a challenging problem—not only for humans, but for deep learning models as well.
 </p>
 <p>In terms of models, this chapter focuses on convolutional neural networks (CNNs)—specifically, ResNet CNNs, which have demonstrated strong performance across a wide range of image classification tasks. If you’d like to explore alternative approaches, consider checking out discussions and notebooks shared by other users on the <a href="https://oreil.ly/bUHNt">Kaggle discussion board</a>.
 </p>
 
 
<div data-type="tip"><h6>Tip</h6>
      <p>As always, to get the most out of this chapter, keep the companion Colab notebook from our repo open as you read. Experimenting with the code as you go will deepen your understanding and make the concepts stick.</p>
</div>
 <section data-type="sect1" data-pdf-bookmark="Biology Primer"><div class="sect1" id="biology-primer_8589580">
  <h1>Biology Primer</h1>
  <p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="biology primer" data-type="indexterm" id="ch05_cancer.html1"/>First, let’s introduce the biological phenomenon our models will address: skin cancer, its different types, and the challenges of classifying them.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Skin Cancer"><div class="sect2" id="skin-cancer">
   <h2>Skin Cancer</h2>
   <p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="biology primer" data-tertiary="skin cancer types" data-type="indexterm" id="ch05_cancer.html2"/>Skin cancer is the most common type of cancer worldwide, with an estimated 1.5 million new cases in 2022.<sup><a data-type="noteref" id="id821-marker" href="ch05.html#id821">3</a></sup> It encompasses a wide range of conditions caused by the abnormal growth of skin cells, often driven by a combination of genetic factors and environmental carcinogens such as ultraviolet (UV) radiation.
   </p>
   <p>In this chapter, we will examine both malignant (cancerous) and benign lesions. The term <em>lesion</em> broadly refers to any mark or abnormality on the skin, ranging from harmless growths to those requiring medical intervention. The following are some of the most common types:
   </p>
   <section data-type="sect3" data-pdf-bookmark="Malignant skin cancers"><div class="sect3" id="malignant_skin_cancers">
    <h3>Malignant skin cancers</h3>
    <dl>
      <dt>Basal cell carcinoma</dt>
      <dd>
       <p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="biology primer" data-tertiary="malignant skin cancers" data-type="indexterm" id="id822"/>The most common type of skin cancer, usually slow-growing and rarely spreading to other organs.
       </p>
      </dd>
      <dt>Squamous cell carcinoma</dt>
      <dd>
       <p>Another common type, typically localized but capable of becoming invasive if left untreated.
       </p>
      </dd>
      <dt>Melanoma</dt>
      <dd>
       <p>The deadliest form of skin cancer, known for its ability to metastasize (spread to other parts of the body) quickly.
       </p>
      </dd>
      <dt>Actinic keratosis</dt>
      <dd>
       <p>A precancerous lesion caused by sun damage. While not malignant, it can progress to squamous cell carcinoma if left untreated.
       </p>
      </dd>
    </dl>
    </div></section>

    <section data-type="sect3" data-pdf-bookmark="Benign skin lesions"><div class="sect3" id="benign_skin_lesions">
      <h3>Benign skin lesions</h3>
      <dl>
        <dt>Dermatofibroma</dt>
        <dd>
         <p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="biology primer" data-tertiary="benign skin lesions" data-type="indexterm" id="ch05_cancer.html3"/>A firm, benign growth often found on the legs.
         </p>
        </dd>
        <dt>Nevus (mole)</dt>
        <dd>
         <p>A common benign growth that varies in size, shape, and color.
         </p>
        </dd>
        <dt>Pigmented benign keratosis</dt>
        <dd>
         <p>A noncancerous pigmented lesion, often resembling seborrheic keratosis.
         </p>
        </dd>
        <dt>Seborrheic keratosis</dt>
        <dd>
         <p>A benign, wartlike growth that can appear brown, black, or tan. Sometimes called “age spots” or “wisdom warts,” these are often found in older adults.
         </p>
        </dd>
        <dt>Vascular lesions</dt>
        <dd>
         <p>Benign vascular growths such as hemangiomas and cherry angiomas, formed by abnormal blood vessel proliferation.
         </p>
        </dd>
       </dl>
      </div></section>

   <p>
    Many of these conditions can look quite similar. <a data-type="xref" href="#skin-lesion-types">Figure 5-1</a> shows example images from this chapter’s training dataset, illustrating the different lesion types.
   </p>
   <figure><div id="skin-lesion-types" class="figure">
    <img alt="" src="assets/dlfb_0501.png" width="600" height="461"/>
    <h6><span class="label">Figure 5-1. </span>Grid displaying various types of skin lesions, both benign and malignant, from the dataset used for classification in this chapter.
    </h6>
   </div></figure>
   <p>As you can probably imagine from looking at these example images, misclassifications are a key challenge in skin cancer detection. These errors arise because different lesion types can share similar visual characteristics, such as pigmentation, texture, or irregular borders. The most critical errors occur when melanoma is misclassified as a benign lesion (a false negative), potentially delaying life-saving treatment. Conversely, false positives, such as benign growths mistaken for melanoma, are less serious but can lead to unnecessary biopsies and patient anxiety.
   </p>
   <p>At the same time, there can be significant visual differences within a single class. For example, <a data-type="xref" href="#diverse-melanomas">Figure 5-2</a> shows how melanomas can vary widely in appearance, making classification even more challenging.
   </p>
   <figure><div id="diverse-melanomas" class="figure">
    <img alt="" src="assets/dlfb_0502.png" width="600" height="461"/>
    <h6><span class="label">Figure 5-2. </span>Melanomas can exhibit a wide range of visual characteristics, making consistent classification difficult. Some may appear dark brown or black with irregular borders, while others are lighter, reddish, or even patchy in color. Certain cases show a crusty texture, whereas others present as smooth, flat lesions.
    </h6>
   </div></figure>
   
   <p>This visual variability stems from underlying biological differences—including the type of cells involved, how deep or aggressively the lesion grows, and how much pigment is produced. For melanoma specifically, changes in melanin production and growth pattern can result in widely varying appearances, even within the same class<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html3" data-type="indexterm" id="id823"/>.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html2" data-type="indexterm" id="id824"/></p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Causes and Risk Factors"><div class="sect2" id="causes-and-risk-factors">
   <h2>Causes and Risk Factors</h2>
   <p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="biology primer" data-tertiary="causes and risk factors" data-type="indexterm" id="id825"/>Skin cancer develops when genetic mutations disrupt the normal regulation of skin cell growth and division. The most common trigger for these mutations is UV radiation—primarily from sunlight or artificial sources like tanning beds—which damages cellular DNA over time. If this damage isn’t properly repaired—for example, when the cell’s DNA repair systems make errors or become overwhelmed—it can lead to uncontrolled cell growth and, ultimately, tumor formation.</p>
    
    <p class="pagebreak-before">Several factors increase the risk of developing skin cancer:</p>
    
    <ul>
<li><p><em>Fair skin</em> contains less melanin, the pigment that provides some natural protection against UV damage.</p></li>
<li><p><em>Frequent sunburns</em>, especially in childhood, suggest repeated UV-induced damage, which can accumulate over a lifetime.</p></li>
<li><p><em>A high number of moles</em> (especially atypical or dysplastic moles) can reflect underlying instability in melanocyte behavior, increasing the chance that one could turn cancerous.</p></li>
<li><p><em>Family history</em> may point to inherited genetic susceptibilities, such as mutations in tumor suppressor genes.</p></li>
<li><p><em>Environmental carcinogens</em>, like arsenic or industrial chemicals, can also contribute to mutational burden.</p></li>
</ul>
<p>In addition to external factors, <em>specific genetic mutations</em>—such as in the <em>BRAF</em> gene—are commonly found in melanoma. These mutations may arise spontaneously or in response to environmental triggers like UV radiation, and they play a key role in driving tumor growth. Importantly, understanding these mutations has enabled the development of targeted therapies that are tailored to a patient’s individual tumor profile, marking a shift toward more personalized cancer treatment.</p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="How Skin Cancer Is Diagnosed"><div class="sect2" id="how-skin-cancer-is-diagnosed">
   <h2>How Skin Cancer Is Diagnosed</h2>
   <p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="biology primer" data-tertiary="diagnosing skin cancer" data-type="indexterm" id="id826"/>In clinical settings, dermatologists diagnose skin cancer through visual examination, dermoscopy, and biopsy. <em>Dermoscopy</em> is a noninvasive imaging technique that magnifies subsurface skin structures to help distinguish benign from malignant lesions. The images in commonly used skin cancer classification datasets primarily consist of dermoscopic images, captured using specialized <em>dermatoscopes</em> equipped with a light source and measurement markers, rather than standard cameras.</p>


<p>If a lesion appears suspicious, a <em>biopsy</em> is performed—a small tissue sample is taken and examined under a microscope to detect cellular abnormalities, such as irregular nuclei, atypical cell shapes, disorganized tissue architecture, or uncontrolled mitotic activity. These features help confirm whether the lesion is malignant and determine its type and stage.</p>
   <p>The ABCDE rule (Asymmetry, Border irregularity, Color variation, Diameter &gt;6 mm, Evolving changes) helps assess lesions for signs of melanoma. AI models can assist by analyzing dermoscopic and clinical images to flag high-risk lesions for further evaluation.
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Image-Based Skin Cancer Detection"><div class="sect2" id="image-based-skin-cancer-detection">
   <h2>Image-Based Skin Cancer Detection</h2>
   <p>
    <a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="biology primer" data-tertiary="image-based detection" data-type="indexterm" id="id827"/>Deep learning has revolutionized image-based skin cancer detection, with AI models now achieving diagnostic accuracy comparable to expert dermatologists. A 2024 meta-analysis<sup><a data-type="noteref" id="id828-marker" href="ch05.html#id828">4</a></sup>
    of 53 studies found that AI consistently outperformed general practitioners and less experienced dermatologists in distinguishing melanoma from benign lesions, while performing on par with specialists. This suggests that AI can serve as a valuable diagnostic aid, enhancing early detection and decision making.
   </p>
   <p>
    However, integrating AI into real-world clinical practice remains challenging. Most skin cancer models are trained on a handful of public datasets (e.g., ISIC, HAM10000), which lack diversity in skin types and imaging conditions, limiting generalizability. Additionally, AI performance in controlled, retrospective studies often does not translate to real-world settings, where factors like lighting, lesion presentation, and physician workflows introduce variability.
   </p>
   <p>
    Another key barrier is explainability. Clinicians need to understand
    <em>
     why
    </em>
    a model makes a specific prediction, not just receive an isolated probability score. For example, if an AI predicts a skin lesion to be melanoma, is it due to the asymmetry, irregular borders, or color variation? And how are these different factors combined and weighted by the model? Machine learning methods like saliency maps and attention mechanisms help visualize what the model is focusing on, but they remain imperfect. Without clear reasoning, AI recommendations are difficult to trust or integrate into medical decision making.
   </p>
   <div data-type="note" epub:type="note"><h6>Note</h6>
    <p>
     Regulatory approval and clinical validation are major hurdles for deploying AI in healthcare. Diagnostic models must meet rigorous safety, accuracy, and transparency standards before receiving approval from regulatory bodies like the FDA or CE. This typically involves extensive clinical trials, reproducibility testing, and post-deployment monitoring. Moreover, AI tools must be integrated into existing workflows without disrupting clinician judgment or introducing new biases—all while maintaining patient privacy and data security.</p>
   </div>
   <p>
    While challenges remain, AI is steadily moving toward real-world deployment, with some dermatology AI systems already <em>CE-marked</em>—a certification indicating compliance with EU safety and efficacy standards—allowing for clinical use in the European Union. Additionally, smartphone apps such as SkinVision and Miiskin offer AI-based skin lesion analysis. While these apps are usually not approved for clinical decision making, they can still provide risk assessments and encourage users to seek medical evaluation.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html1" data-type="indexterm" id="id829"/>
   </p>
  </div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Machine Learning Primer"><div class="sect1" id="machine-learning-primer_102580195">
  <h1>Machine Learning Primer</h1>
  <p><a contenteditable="false" data-primary="machine learning" data-secondary="for skin cancer detection in medical images" data-type="indexterm" id="ch05_cancer.html4"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="machine learning primer" data-type="indexterm" id="ch05_cancer.html5"/>In <a data-type="xref" href="ch03.html#learning-the-logic-of-dna">Chapter 3</a>, we applied a CNN to model 1D sequence data—specifically, DNA sequences. However, CNNs are more commonly used for 2D image processing, powering tasks such as:
  </p>
  <dl>
  <dt>Image classification</dt>
   <dd>
    <p>Assigning an image to a specific category, such as identifying whether it contains a dog or a cat
    </p>
   </dd>
   <dt>Object detection</dt>
   <dd>
    <p>Detecting and localizing objects within an image, such as drawing a bounding box around a cat
    </p>
   </dd>
   <dt>Segmentation</dt>
   <dd>
    <p>Partitioning an image into meaningful regions, such as labeling all pixels that belong to a cat
    </p>
   </dd>
  </dl>
  <p>
   This section provides a brief primer on how CNNs work for images. As additional learning material, we recommend the 3Blue1Brown introductory video titled <a href="https://oreil.ly/k8zoM">“But what is a convolution?”</a>, which offers a visual and intuitive explanation of CNNs. For a more in-depth exploration, the renowned <a href="https://oreil.ly/C_wPx">Stanford CS 231n: Convolutional Neural Networks for Visual Recognition course</a> provides a comprehensive introduction to the field.
  </p>
  <section data-type="sect2" data-pdf-bookmark="Convolutional Neural Networks"><div class="sect2" id="convolutional-neural-networks_122767671">
   <h2>Convolutional Neural Networks</h2>
   <p><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="skin cancer detection from medical images" data-type="indexterm" id="id830"/><a contenteditable="false" data-primary="machine learning" data-secondary="for skin cancer detection in medical images" data-tertiary="convolutional neural networks" data-type="indexterm" id="id831"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="machine learning primer" data-tertiary="convolutional neural networks" data-type="indexterm" id="id832"/>CNNs are a specialized type of neural network designed for grid-like data, such as images (or sequences, as seen in <a data-type="xref" href="ch03.html#learning-the-logic-of-dna">Chapter 3</a>). They automatically learn <em>hierarchical</em> patterns, meaning they extract features at different levels of abstraction:
   </p>
   <ul class="simple">
    <li>
     <p>Early layers in the neural network detect simple patterns like edges, textures, and color contrasts.
     </p>
    </li>
    <li>
     <p>Middle layers recognize shapes and structures by combining these basic features.
     </p>
    </li>
    <li>
     <p>Deeper layers build on these to identify complex objects or meaningful <span class="keep-together">categories</span>.
     </p>
    </li>
   </ul>
   <p>For example, in skin cancer detection, early CNN layers may detect edges and color variations in skin lesions, mid-level layers might recognize irregular borders or asymmetry, and the latest layers would combine these features into high-level learned patterns to distinguish between benign and malignant lesions.
   </p>
   <div data-type="warning" epub:type="warning"><h6>Warning</h6>
    <p>Although we describe CNNs as learning hierarchical representations, it’s important to avoid <em>anthropomorphizing</em> them. CNNs don’t “see” objects the way humans do. Instead, they learn statistical patterns in pixel values that maximize predictive accuracy. Techniques like <em>activation mapping</em> (highlighting which parts of an image influence a classification) and <em>probing</em> (examining what kinds of features different layers encode) help us understand and visualize correlations within the model. However, these methods provide post hoc insights for human interpretation—they don’t imply that the model itself has a structured or explainable reasoning process.</p>
   </div>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Understanding a Convolution"><div class="sect2" id="understanding-a-convolution">
   <h2>Understanding a Convolution</h2>
   <p>
    <a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="skin cancer detection from medical images" data-tertiary="understanding a convolution" data-type="indexterm" id="ch05_cancer.html6"/><a contenteditable="false" data-primary="machine learning" data-secondary="for skin cancer detection in medical images" data-tertiary="understanding a convolution" data-type="indexterm" id="ch05_cancer.html7"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="machine learning primer" data-tertiary="understanding a convolution" data-type="indexterm" id="ch05_cancer.html8"/>The <a contenteditable="false" data-primary="convolutional layers" data-type="indexterm" id="id833"/>core building block of a CNN is the <em>convolutional layer</em>, which applies <em>filters</em> (also called kernels) to extract features from the input image. A filter is a small grid of numbers that slides across the image, detecting local patterns such as edges, textures, or color transitions.</p>
    
<p>An image is represented as a grid of pixels—in grayscale images, each pixel holds a single intensity value (ranging from 0 for black to 255 for white), while color images typically have three channels (usually red, green, and blue or <em>RGB</em>), each with its own intensity map.</p>

<p>As a filter moves across the image, it performs a <em>dot product</em> operation at each position: the filter values are multiplied element-wise with the corresponding pixel values in the image patch underneath, and the results are summed. This produces a single output value per position, building a new representation called a <em>feature map</em>, that highlights where the pattern encoded by the filter appears in the image.</p>

<p>The specific values in the filter determine what it detects. For example, the following 3 × 3 filter emphasizes vertical <em>edges</em> by responding strongly to changes in pixel intensity along the horizontal (<em>x</em>) axis:
   </p>
   <div data-type="equation">
    <math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mo>[</mo>
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </mtd>
              <mtd>
                <mn>0</mn>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </mtd>
              <mtd>
                <mn>0</mn>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </mtd>
              <mtd>
                <mn>0</mn>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
          </mtable>
          <mo>]</mo>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
   </div>
   <p>When applied to an image by sliding across it and computing the dot product with pixel values, this filter enhances areas where pixel intensity changes vertically, such as object boundaries, making it useful for edge detection.</p>
    
    <p class="pagebreak-before">Similarly, a filter designed to detect horizontal edges responds to changes in pixel intensity along the vertical (<em>y</em>) axis. It typically looks like this:</p>
   <div data-type="equation">
    <math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mo>[</mo>
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </mtd>
              <mtd>
                <mrow>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </mtd>
              <mtd>
                <mrow>
                  <mo>-</mo>
                  <mn>1</mn>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mn>0</mn>
              </mtd>
              <mtd>
                <mn>0</mn>
              </mtd>
              <mtd>
                <mn>0</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mn>1</mn>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
          </mtable>
          <mo>]</mo>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
   </div>
   
   <p>This filter activates in regions where there is a strong transition from dark to light (or vice versa) from top to bottom, highlighting horizontal structures in the image.</p>
   <p>In a way, this concept is related to the idea of “filters” in social media, which often apply simple mathematical transformations (such as increasing contrast or sharpening details) to modify an image’s appearance.
   </p>
   <div data-type="note" epub:type="note"><h6>Note</h6>
    <p>
     <a contenteditable="false" data-primary="feature engineering" data-type="indexterm" id="id834"/>Before CNNs, in a task called <em>feature engineering</em>, researchers manually designed and optimized these filter matrices to detect edges, textures, and other features—a labor-intensive process. Now, neural networks learn these filters automatically, optimizing them for the task at hand.
    </p>
   </div>
   <p>
    A single convolutional layer doesn’t just apply one filter; it typically learns multiple filters (e.g., 64) in parallel. Each filter captures different features, producing multiple <em>feature maps</em>, which are stacked together as separate channels in the layer’s output.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html8" data-type="indexterm" id="id835"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html7" data-type="indexterm" id="id836"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html6" data-type="indexterm" id="id837"/>
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Understanding Dimensions"><div class="sect2" id="understanding-dimensions">
   <h2>Understanding Dimensions</h2>
   <p><a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="skin cancer detection from medical images" data-tertiary="understanding dimensions" data-type="indexterm" id="ch05_cancer.html9"/><a contenteditable="false" data-primary="machine learning" data-secondary="for skin cancer detection in medical images" data-tertiary="understanding dimensions" data-type="indexterm" id="ch05_cancer.html10"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="machine learning primer" data-tertiary="understanding dimensions" data-type="indexterm" id="ch05_cancer.html11"/>If a grayscale image of size 256 × 256
    (with a single channel) is passed through a convolutional layer with 64 filters, the output will have dimensions <code>(256, 256, 64)</code>—assuming padding is used to preserve spatial dimensions. The height and width remain unchanged, while the channel dimension expands, as each of the 64 filters extracts different feature representations from the image. This is true regardless of the filter size (e.g., 3 × 3, 5 × 5), as long as the stride and padding settings maintain spatial dimensions.
   </p>
   <div data-type="note" epub:type="note" class="pagebreak-after"><h6>Note</h6>
    <p>Remember that neural network filters are usually randomly initialized, meaning they start off extracting no meaningful patterns. Through backpropagation, the filters gradually learn to detect useful visual features such as edges, textures, or shapes.
    </p>
   </div>
   <p>After convolution, a nonlinearity (such as the ReLU activation function) is typically applied to the feature maps. This is crucial because convolution alone is just a linear operation, meaning that without a nonlinearity, stacking multiple layers would be equivalent to just one big matrix multiplication. Adding the activation function allows the network to learn more complex, nonlinear patterns.
   </p>
   <p>Now let’s consider a color image with dimensions <code>(256, 256, 3)</code>, where the three channels correspond to red, green, and blue (RGB). How does convolution work when an image has multiple input channels?
   </p>
   <p>Unlike grayscale images, where each filter operates on a single channel, a convolutional filter in a color image must process all three channels simultaneously. Instead of being a simple 3 × 3 matrix, each filter is actually a 3 × 3 × 3 tensor, meaning:
   </p>
   <ul class="simple">
    <li>
     <p>Each 3 × 3 slice of the filter is applied to the corresponding color channel (red, green, or blue) of the image.
     </p>
    </li>
    <li>
     <p>The results from all three channels are summed to produce a single output value per pixel. Typically, this is not a normal sum but actually a <em>weighted sum</em>
      where each channel’s contribution is multiplied by a separate learned weight.
     </p>
    </li>
    <li>
     <p>
      This process is repeated for all filters, creating multiple feature maps in the next layer.
     </p>
    </li>
   </ul>
   <p>For example, applying a convolutional layer with 64 filters to an input of shape <code>(256, 256, 3)</code>
    results in an output of <code>(256, 256, 64)</code>, where each filter has combined the three input channels in different ways to extract meaningful patterns. We would then apply an activation function such as a ReLU as before.
   </p>
   <div data-type="tip"><h6>Tip</h6>
    <p>
     You may have noticed we called a 3 × 3 filter a <em>matrix</em>, but a 3 × 3 × 3 filter a <em>tensor</em>. To briefly clarify the terminology:
    </p>
    <ul class="simple">
     <li><p>A <em>scalar</em> is a single number and can be seen as a 0D tensor (e.g., 5).
      </p>
     </li>
     <li><p>A <em>vector</em> is a 1D tensor (e.g., <code>[1, 2, 3]</code>).
      </p>
     </li>
     <li><p>A <em>matrix</em> is a 2D tensor (e.g., a 3 × 3 filter).
      </p>
     </li>
     <li><p>A <em>tensor</em> is a general term for arrays of any number of dimensions, including 3D+ structures like 3 × 3 × 3 filters.
      </p>
     </li>
    </ul>
    <p>In short, tensors are the fundamental data structure in deep learning, generalizing scalars, vectors, and matrices to arbitrary <span class="keep-together">dimensions</span>.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html11" data-type="indexterm" id="id838"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html10" data-type="indexterm" id="id839"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html9" data-type="indexterm" id="id840"/>
    </p>
   </div>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Pooling"><div class="sect2" id="pooling">
   <h2>Pooling</h2>
   <p>
    <a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="pooling" data-type="indexterm" id="id841"/><a contenteditable="false" data-primary="machine learning" data-secondary="for skin cancer detection in medical images" data-tertiary="pooling" data-type="indexterm" id="id842"/><a contenteditable="false" data-primary="pooling layers" data-type="indexterm" id="id843"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="machine learning primer" data-tertiary="pooling" data-type="indexterm" id="id844"/>Now that we’ve covered what convolutions do, let’s move on to <em>pooling layers</em>. Pooling reduces the spatial dimensions of feature maps, making computations more efficient and helping to prevent overfitting by keeping only the most prominent activations.</p>
   <p>The most common type is max pooling, which selects the highest value in a given region of the feature map. This ensures that strong activations are preserved while reducing spatial resolution (downsampling).
   </p>
   <p>Consider a 4 × 4 feature map (the image-like output of a convolution) before applying 2 × 2 max pooling (with stride 2, meaning the filter moves two pixels at a time):
   </p>
   <div data-type="equation">
    <math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mo>[</mo>
          <mtable>
            <mtr>
              <mtd>
                <mn>1</mn>
              </mtd>
              <mtd>
                <mn>3</mn>
              </mtd>
              <mtd>
                <mn>2</mn>
              </mtd>
              <mtd>
                <mn>1</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mn>4</mn>
              </mtd>
              <mtd>
                <mn>5</mn>
              </mtd>
              <mtd>
                <mn>7</mn>
              </mtd>
              <mtd>
                <mn>2</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mn>3</mn>
              </mtd>
              <mtd>
                <mn>2</mn>
              </mtd>
              <mtd>
                <mn>9</mn>
              </mtd>
              <mtd>
                <mn>6</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mn>1</mn>
              </mtd>
              <mtd>
                <mn>8</mn>
              </mtd>
              <mtd>
                <mn>6</mn>
              </mtd>
              <mtd>
                <mn>3</mn>
              </mtd>
            </mtr>
          </mtable>
          <mo>]</mo>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
   </div>
   <p>After applying 2 × 2 max pooling, the highest value in each 2 × 2 block is retained, reducing the matrix to a size of 2 × 2:
   </p>
   <div data-type="equation">
    <math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mo>[</mo>
          <mtable>
            <mtr>
              <mtd>
                <mn>5</mn>
              </mtd>
              <mtd>
                <mn>7</mn>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mn>8</mn>
              </mtd>
              <mtd>
                <mn>9</mn>
              </mtd>
            </mtr>
          </mtable>
          <mo>]</mo>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
   </div>
   <p>This greatly reduces the number of values while preserving the most important activations. Another common pooling type is <em>average pooling</em>, which takes the mean of each region instead of the max. This would result in this matrix:
   </p>
   <div data-type="equation" class="center">
    <math display="block">
  <mtable displaystyle="true">
    <mtr>
      <mtd columnalign="right">
        <mrow>
          <mo>[</mo>
          <mtable>
            <mtr>
              <mtd>
                <mrow>
                  <mn>3</mn>
                  <mo lspace="0%" rspace="0%">.</mo>
                  <mn>25</mn>
                </mrow>
              </mtd>
              <mtd>
                <mrow>
                  <mn>3</mn>
                  <mo lspace="0%" rspace="0%">.</mo>
                  <mn>0</mn>
                </mrow>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mrow>
                  <mn>3</mn>
                  <mo lspace="0%" rspace="0%">.</mo>
                  <mn>5</mn>
                </mrow>
              </mtd>
              <mtd>
                <mrow>
                  <mn>6</mn>
                  <mo lspace="0%" rspace="0%">.</mo>
                  <mn>0</mn>
                </mrow>
              </mtd>
            </mtr>
          </mtable>
          <mo>]</mo>
        </mrow>
      </mtd>
    </mtr>
  </mtable>
</math>
   </div>
   <p>To summarize, pooling:</p>
   
   <ul>
 <li><p>Improves computational efficiency by shrinking the representation</p></li>
 <li><p>Retains key information by preserving prominent activations (like maxima or averages)</p></li>
 <li><p>Adds a mild form of regularization by making the model less sensitive to small shifts in the input</p></li></ul>
 <p>However, pooling is not a substitute for more robust regularization methods like dropout or weight decay.</p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Other Components of a CNN"><div class="sect2" id="other-components-of-a-cnn">
   <h2>Other Components of a CNN</h2>
   <p>
    <a contenteditable="false" data-primary="convolutional neural networks (CNNs)" data-secondary="skin cancer detection from medical images" data-tertiary="various key components" data-type="indexterm" id="id845"/><a contenteditable="false" data-primary="machine learning" data-secondary="for skin cancer detection in medical images" data-tertiary="various key components of CNNs" data-type="indexterm" id="id846"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="machine learning primer" data-tertiary="various key components of CNNs" data-type="indexterm" id="id847"/>Beyond convolution and pooling, several other key components help CNNs function effectively:
   </p>
   <dl>
   <dt>Activation functions</dt>
    <dd>
     <p> As mentioned earlier, a nonlinearity (typically ReLU) is applied after each convolutional layer. This enhances the model’s expressivity, enabling it to learn complex, nonlinear relationships.</p>
    </dd>
    <dt>Batch normalization</dt>
    <dd>
     <p>This normalizes feature maps across a batch to keep activations in a stable range. It helps CNNs train more efficiently, reduces sensitivity to weight initialization, and enables the use of higher learning rates—especially useful for deep architectures (and many CNNs are quite deep).</p>
    </dd>
    <dt>Dropout</dt>
    <dd>
     <p>This is a regularization technique where random activations are set to zero during training to prevent overfitting. This forces the network to rely on multiple pathways for making predictions, improving generalization.
     </p>
    </dd>
    <dt>Fully connected layers</dt>
    <dd>
     <p>After feature extraction, the final layers of a CNN are typically fully connected. These layers combine the learned feature representations and produce the final output—such as class probabilities in an image classification task.</p>
    </dd>
   </dl>
   <p>With these components combined, we’ve covered the building blocks needed to train a fully functioning CNN. Next, we’ll explore a widely used architecture that brings them all together: ResNets.
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="ResNets"><div class="sect2" id="resnets">
   <h2>ResNets</h2>
   <p>
    <a contenteditable="false" data-primary="machine learning" data-secondary="for skin cancer detection in medical images" data-tertiary="ResNets" data-type="indexterm" id="ch05_cancer.html12"/><a contenteditable="false" data-primary="ResNets (residual networks)" data-type="indexterm" id="ch05_cancer.html13"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="machine learning primer" data-tertiary="ResNets" data-type="indexterm" id="ch05_cancer.html14"/>A ResNet (residual network) is a CNN architecture introduced in 2015<sup><a data-type="noteref" id="id848-marker" href="ch05.html#id848">5</a></sup>
    <a contenteditable="false" data-primary="vanishing gradients" data-type="indexterm" id="id849"/>that enables the training of very deep networks without suffering from the
    <em>vanishing gradient problem</em>, a common issue in which deeper networks struggle to propagate gradients effectively, slowing down learning. <a contenteditable="false" data-primary="residual connections" data-type="indexterm" id="id850"/><a contenteditable="false" data-primary="skip connections" data-type="indexterm" id="id851"/>ResNets address this by introducing <em>residual connections</em> (also called <em>skip connections</em>), which allow information to bypass some layers. This stabilizes training and improves performance, making ResNets a go-to architecture for computer vision tasks like image classification and object detection.
   </p>
   <p>The key idea behind a residual connection is simple: instead of learning a full transformation <code>f(x)</code>, the network learns <code>f(x) + x</code>, where <code>x</code>
    is the original input. This means that if <code>f(x)</code>
    is small or difficult to learn, the network can still default to simply passing through the input <code>x</code>
    unchanged (an identity function). This prevents layers from degrading the performance of deeper networks, effectively “skipping” operations that don’t contribute meaningful improvements.
   </p>
   <p>Here’s a simplified implementation of a residual block in pseudocode:
   </p>
   
    
     <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">ResidualBlock</code><code class="p">(</code><code class="n">x</code><code class="p">):</code>
  <code class="sd">"""Basic building block of a ResNet."""</code>
  <code class="n">identity</code> <code class="o">=</code> <code class="n">x</code>  <code class="c1"># Preserve the original input for the skip connection.</code>
  <code class="n">residual</code> <code class="o">=</code> <code class="n">Convolution</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>  <code class="c1"># Apply convolutional transformation.</code>
  <code class="k">return</code> <code class="n">identity</code> <code class="o">+</code> <code class="n">residual</code>  <code class="c1"># Add the identity (skip connection).</code>
</pre>
    
   
   <p>This simple skip connection allows gradients to flow more easily through the network, making training deep architectures much more feasible.
   </p>
   <p>We can rewrite this pseudocode in Flax using the Linen API:
   </p>
   
    
     <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">ResidualBlock</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""A minimal Flax CNN residual block."""</code>
  <code class="n">features</code><code class="p">:</code> <code class="nb">int</code>  <code class="c1"># Number of output channels.</code>
  <code class="n">kernel_size</code><code class="p">:</code> <code class="n">Tuple</code><code class="p">[</code><code class="nb">int</code><code class="p">,</code> <code class="o">...</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>  <code class="c1"># Convolution kernel size.</code>
  <code class="n">strides</code><code class="p">:</code> <code class="n">Tuple</code><code class="p">[</code><code class="nb">int</code><code class="p">,</code> <code class="o">...</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>  <code class="c1"># Convolution stride.</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
    <code class="n">identity</code> <code class="o">=</code> <code class="n">x</code>  <code class="c1"># Preserve the original input for the skip connection.</code>
    <code class="n">residual</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">features</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">kernel_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">strides</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"SAME"</code>
    <code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">identity</code> <code class="o">+</code> <code class="n">residual</code>  <code class="c1"># Add the identity (skip connection).</code>
</pre>
    
   
   <p>The following are some notes on the key arguments to <code>nn.Conv</code>:
   </p>
   <dl>
   <dt><code>features</code></dt>
    <dd>
     <p>Defines the number of learned filters. If <code>features=100</code>, an input <code>(224, 224, 3)</code> becomes <code>(224, 224, 100)</code>, extracting 100 different feature maps.
     </p>
    </dd>
    <dt><code>kernel_size</code></dt>
    <dd>
     <p>Defines the receptive field of the convolution. A small kernel like <code>(3, 3)</code> captures fine details, while a larger kernel like <code>(16, 16)</code> detects broader patterns.
     </p>
    </dd>
    <dt><code>strides</code></dt>
    <dd>
     <p>Controls the step size of the filter. <code>strides=(1, 1)</code> preserves resolution, while larger strides downsample the feature maps. Downsampling is more commonly done via pooling layers than by increasing the stride.</p>
    </dd>
    <dt><code>padding</code></dt>
    <dd>
     <p>Defines the padding for consistent shapes. <code>padding="SAME"</code> ensures that output dimensions match the input by adding necessary padding.
     </p>
    </dd>
   </dl>
   
   
<aside data-type="sidebar" epub:type="sidebar" class="less_space pagebreak-before"><div class="sidebar" id="id852">
  <h5>Kernel Size and Computational Cost</h5>
      <p><a contenteditable="false" data-primary="convolutional kernels" data-type="indexterm" id="id853"/><a contenteditable="false" data-primary="kernel size, computational cost and" data-type="indexterm" id="id854"/>While using large convolutional kernels might seem intuitive for capturing broader context, they significantly increase computational cost—especially in early layers, where feature maps are large and convolutions must be applied many times. Although the number of parameters doesn’t depend on input size, computation time scales with how often each kernel is applied.</p>
      
<p>A more efficient strategy is often to stack multiple small kernels—for example, three 3 × 3 convolutions instead of one 9 × 9 convolution. This increases the receptive field while introducing nonlinearities between layers, enabling the network to learn more expressive, hierarchical features with fewer parameters. Stacking small kernels grows the receptive field because each layer builds on the dependencies of the previous one, even if <code>padding='SAME'</code> keeps the output size unchanged.</p>
      
<p>Another technique is dilated convolution, which expands the receptive field without increasing parameters or downsampling the feature maps—useful when broader spatial context is important.
</p>
</div></aside>
   
   <p>With this in mind, let’s implement a more complete residual block with the following additions:
   </p>
   <ul class="simple">
    <li>
     <p>Two stacked convolutional layers to extract deeper features
     </p>
    </li>
    <li>
     <p>A ReLU activation function to introduce nonlinearity
     </p>
    </li>
    <li>
     <p>Batch normalization to stabilize training
     </p>
    </li>
    <li>
     <p>A check for channel mismatches, applying a 1 × 1 convolution when necessary to ensure compatibility for addition
     </p>
    </li>
   </ul>
   
    
     <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">ResidualBlock</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""A basic Flax CNN residual block with two convolutional layers."""</code>
  <code class="n">features</code><code class="p">:</code> <code class="nb">int</code>  <code class="c1"># Number of output channels.</code>
  <code class="n">kernel_size</code><code class="p">:</code> <code class="n">Tuple</code><code class="p">[</code><code class="nb">int</code><code class="p">,</code> <code class="o">...</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>  <code class="c1"># Convolution kernel size.</code>
  <code class="n">strides</code><code class="p">:</code> <code class="n">Tuple</code><code class="p">[</code><code class="nb">int</code><code class="p">,</code> <code class="o">...</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>  <code class="c1"># Convolution stride.</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
    <code class="n">identity</code> <code class="o">=</code> <code class="n">x</code>  <code class="c1"># Preserve the original input for the skip connection.</code>

    <code class="c1"># First convolution + batch normalization + ReLU activation.</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">features</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">kernel_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">strides</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"SAME"</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm</code><code class="p">()(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>

    <code class="c1"># Second convolution + batch normalization (no activation here).</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">features</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">kernel_size</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">strides</code><code class="p">,</code> <code class="n">padding</code><code class="o">=</code><code class="s2">"SAME"</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">BatchNorm</code><code class="p">()(</code><code class="n">x</code><code class="p">)</code>

    <code class="c1"># If the input and output dimensions do not match, apply a 1 x 1 convolution.</code>
    <code class="k">if</code> <code class="n">identity</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]</code> <code class="o">!=</code> <code class="n">x</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">]:</code>
      <code class="n">identity</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">features</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">),</code>
        <code class="n">strides</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">strides</code><code class="p">,</code>
        <code class="n">padding</code><code class="o">=</code><code class="s2">"SAME"</code>
      <code class="p">)(</code><code class="n">identity</code><code class="p">)</code>

    <code class="c1"># Add the skip connection.</code>
    <code class="n">x</code> <code class="o">+=</code> <code class="n">identity</code>

    <code class="c1"># Final ReLU activation.</code>
    <code class="k">return</code> <code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
</pre>
    
   
   <p>
    This implementation is now much closer to what you’ll encounter in real-world architectures.
   </p>
   
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id855">
  <h5>1 x 1 Convolutions</h5>
      <p><a contenteditable="false" data-primary="1 x 1 convolution" data-primary-sortas="one x one" data-type="indexterm" id="id856"/><a contenteditable="false" data-primary="pointwise (1 x 1) convolution" data-type="indexterm" id="id857"/>A 1 × 1 convolution (also called a pointwise convolution) operates on each spatial location independently across channels. Unlike standard convolutions that span spatial neighborhoods (e.g., 3 × 3 or 5 × 5), 1 × 1 convolutions capture cross-channel interactions while preserving the spatial resolution.</p>
      
<p>In residual blocks, a 1 × 1 convolution is often used to adjust the number of channels in the skip connection, ensuring that the input (<code>identity</code>) and output (<code>x</code>) have the same shape before addition. Without this adjustment, the addition would fail due to mismatched shapes, since tensor addition requires matching shapes across all dimensions.</p>
      
<p>Mathematically, a 1 × 1 convolution is actually equivalent to applying a fully connected (linear) transformation at each spatial location independently. However, it is more efficient than using dense layers in modern deep learning frameworks like JAX or TensorFlow, as it better utilizes GPU acceleration and tensor-level parallelism.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html14" data-type="indexterm" id="id858"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html13" data-type="indexterm" id="id859"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html12" data-type="indexterm" id="id860"/></p>
</div></aside>

   
   <p>With this foundation in CNNs and ResNets, we can now apply these techniques to our goal: building a deep learning model for skin cancer prediction.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html5" data-type="indexterm" id="id861"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html4" data-type="indexterm" id="id862"/>
   </p>
  </div></section>
 </div></section>
 <section data-type="sect1" class="pagebreak-before less_space" data-pdf-bookmark="Exploring the Data"><div class="sect1" id="exploring-the-data">
  <h1>Exploring the Data</h1>
  <p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="exploring the data" data-type="indexterm" id="ch05_cancer.html15"/>As always, before building a model, we first need to understand our dataset. In this chapter, we will use data from <a href="https://oreil.ly/g2EyR">ISIC</a>, an initiative that fosters collaboration between medical professionals and AI researchers.  ISIC regularly hosts machine learning challenges, encouraging researchers to develop and submit models for classifying skin lesion images.<sup><a data-type="noteref" id="id863-marker" href="ch05.html#id863">6</a></sup></p>
  <p>The ISIC archive is a freely accessible repository containing tens of thousands of skin lesion images, making it a valuable resource for developing and benchmarking AI-based diagnostic tools. You can explore the dataset through their online portal, as shown in <a data-type="xref" href="#isic-screenshot">Figure 5-3</a>.</p>
  <figure class="pagebreak-after"><div id="isic-screenshot" class="figure"><img alt="" src="assets/dlfb_0503.png" width="600" height="363"/><h6><span class="label">Figure 5-3. </span>A <a href="https://oreil.ly/F6BwK">screenshot</a> displaying a vast collection of 81,722 public skin lesion images.</h6></div></figure>
  
  <section data-type="sect2" class="less_space" data-pdf-bookmark="A First Glimpse"><div class="sect2" id="a-first-glimpse">
   <h2>A First Glimpse</h2>
   <p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="exploring the data" data-tertiary="initial steps" data-type="indexterm" id="ch05_cancer.html16"/>Rather than using the ISIC dataset directly, we are working with a version hosted on <a href="https://oreil.ly/cez_1">Kaggle</a>. This allows us to explore the initial steps of working with a new dataset, including essential sanity checks to ensure its integrity.
   </p>
   
   
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
  
    <p>A key advantage of using a Kaggle dataset is that we can compare our approach to existing work and establish reasonable performance expectations. However, caution is needed—while Kaggle notebooks can be a great source of inspiration, they are not peer reviewed and may contain serious errors.</p>
<p>For example, some models reported impressive performance on this dataset, but closer inspection revealed <em>data leakage</em>—where images from the training set also appeared in validation or test sets, leading to artificially inflated accuracy. In one extreme case, we even found a model that evaluated only on training images, making its results completely meaningless.</p>
</div>
  
   <p>Let’s ensure that we build a robust data pipeline by carefully inspecting the dataset before proceeding. We start by exploring the raw dataset directory to understand its structure. By listing all <em>.jpg</em>
    files, we can get an initial impression of the available labels, which correspond to different types of skin lesions. The <code>rglob</code>
    method is particularly useful here, as it scans directories recursively:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">re</code>
<code class="kn">from</code> <code class="nn">pathlib</code> <code class="kn">import</code> <code class="n">Path</code>

<code class="kn">from</code> <code class="nn">dlfb.utils.context</code> <code class="kn">import</code> <code class="n">assets</code>

<code class="n">image_file</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">Path</code><code class="p">(</code><code class="n">assets</code><code class="p">(</code><code class="s2">"cancer/datasets/raw"</code><code class="p">))</code><code class="o">.</code><code class="n">rglob</code><code class="p">(</code><code class="s2">"*.jpg"</code><code class="p">))</code>

<code class="nb">print</code><code class="p">(</code><code class="sa">rf</code><code class="s2">"One of the images: </code><code class="si">{</code><code class="n">re</code><code class="o">.</code><code class="n">sub</code><code class="p">(</code><code class="s1">'^.*?datasets/'</code><code class="p">,</code> <code class="s1">''</code><code class="p">,</code> <code class="nb">str</code><code class="p">(</code><code class="n">image_file</code><code class="p">))</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
</pre>
      
<p>Output:</p>
<pre data-type="programlisting">One of the images: raw/Test/melanoma/ISIC_0000031.jpg
</pre>
      
     
    
   
   <p>Examining the filepath, we can see that the dataset has already been split into <code>Train</code>
    and <code>Test</code> sets, with subdirectories for each skin lesion type.
   </p>
   <p>Next, we will count the number of images per class across both splits. To do this efficiently, we define a helper function, <code>load_metadata</code>,
    that:
   </p>
   <ul class="simple">
    <li>
     <p>Recursively collects all image filepaths
     </p>
    </li>
    <li>
     <p>Extracts the dataset split (<code>Train</code>/<code>Test</code>) and class label from each path
     </p>
    </li>
    <li>
     <p>Stores the results in a pandas <code>DataFrame</code> for easy inspection and visualization
     </p>
    </li>
   </ul>
   <p>We will also track <code>frame_id</code>s, which serve as a reference for quickly retrieving specific images during our later sanity checks.
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">pandas</code> <code class="k">as</code> <code class="nn">pd</code>


<code class="k">def</code> <code class="nf">load_metadata</code><code class="p">(</code><code class="n">data_dir</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">:</code>
  <code class="n">metadata</code> <code class="o">=</code> <code class="p">[]</code>
  <code class="k">for</code> <code class="n">path</code> <code class="ow">in</code> <code class="n">Path</code><code class="p">(</code><code class="n">data_dir</code><code class="p">)</code><code class="o">.</code><code class="n">rglob</code><code class="p">(</code><code class="s2">"*.jpg"</code><code class="p">):</code>
    <code class="n">split</code><code class="p">,</code> <code class="n">class_name</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">path</code><code class="o">.</code><code class="n">parts</code><code class="p">[</code><code class="o">-</code><code class="mi">3</code><code class="p">:]</code>
    <code class="n">metadata</code><code class="o">.</code><code class="n">append</code><code class="p">(</code>
      <code class="p">{</code>
        <code class="s2">"split_orig"</code><code class="p">:</code> <code class="n">split</code><code class="p">,</code>
        <code class="s2">"class_orig"</code><code class="p">:</code> <code class="n">class_name</code><code class="p">,</code>
        <code class="s2">"full_path"</code><code class="p">:</code> <code class="nb">str</code><code class="p">(</code><code class="n">path</code><code class="p">),</code>
      <code class="p">}</code>
    <code class="p">)</code>
  <code class="k">return</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">(</code><code class="n">metadata</code><code class="p">)</code><code class="o">.</code><code class="n">rename_axis</code><code class="p">(</code><code class="s2">"frame_id"</code><code class="p">)</code><code class="o">.</code><code class="n">reset_index</code><code class="p">()</code>


<code class="n">metadata</code> <code class="o">=</code> <code class="n">load_metadata</code><code class="p">(</code><code class="n">assets</code><code class="p">(</code><code class="s2">"cancer/datasets/raw"</code><code class="p">))</code>
<code class="nb">print</code><code class="p">(</code><code class="n">metadata</code><code class="p">)</code></pre>

    <p>Output:</p>
    <pre data-type="programlisting">      frame_id split_orig      class_orig            full_path
0            0       Test        melanoma  /content/drive/M...
1            1       Test        melanoma  /content/drive/M...
2            2       Test        melanoma  /content/drive/M...
...        ...        ...             ...                  ...
2354      2354      Train  dermatofibroma  /content/drive/M...
2355      2355      Train  dermatofibroma  /content/drive/M...
2356      2356      Train  dermatofibroma  /content/drive/M...

[2357 rows x 4 columns]
</pre>
     
     
      
     
    
   
   <p>Next, we count the number of images per class in both splits using the pandas <code>crosstab</code>
    function:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">counts</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">crosstab</code><code class="p">(</code>
  <code class="n">metadata</code><code class="p">[</code><code class="s2">"class_orig"</code><code class="p">],</code> <code class="n">metadata</code><code class="p">[</code><code class="s2">"split_orig"</code><code class="p">],</code> <code class="n">margins</code><code class="o">=</code><code class="kc">True</code>
<code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">counts</code><code class="p">)</code>
</pre>
     
<p>Output:</p>
<pre data-type="programlisting">
split_orig            Test  Train   All
class_orig                             
actinic keratosis       16    114   130
basal cell carcinoma    16    376   392
dermatofibroma          16     95   111
melanoma                16    438   454
nevus                   16    357   373
pigmented benign ...    16    462   478
seborrheic keratosis     3     77    80
squamous cell car...    16    181   197
vascular lesion          3    139   142
All                    118   2239  2357
</pre>         
   
   <p>This dataset is relatively small, containing a total of 2,357 images, split into 2,239 for training and 118 for testing. Additionally, the class distribution is highly imbalanced, with some classes having very few examples to train on. Such an imbalance poses challenges for model generalization and requires careful handling to prevent biased predictions. It also makes evaluation less reliable, as performance metrics based on very few examples can be noisy and unrepresentative.</p>
   <p> We visualize this distribution in <a data-type="xref" href="#class-vs-split-barplot">Figure 5-4</a>:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">fig</code> <code class="o">=</code> <code class="n">counts</code><code class="o">.</code><code class="n">drop</code><code class="p">([</code><code class="s2">"All"</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">drop</code><code class="p">([</code><code class="s2">"All"</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">barh</code><code class="p">(</code><code class="n">stacked</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_xlabel</code><code class="p">(</code><code class="s2">"Number of images"</code><code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_ylabel</code><code class="p">(</code><code class="s2">"Skin lesion type"</code><code class="p">)</code>
<code class="n">fig</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"Class distribution"</code><code class="p">);</code></pre>
      
     
    
    
     <figure><div id="class-vs-split-barplot" class="figure">
      <img alt="" src="assets/dlfb_0504.png" width="600" height="356"/>
      <h6><span class="label">Figure 5-4. </span>Bar plot of the distribution of classes across original training and test sets.
      </h6>
     </div></figure>
    
   
   <p>We can draw several key insights from the bar plot of class counts:
   </p>
   <dl>
   <dt>Class imbalance</dt>
    <dd>
     <p>Some categories, like pigmented benign keratosis, are overrepresented (454 total images), while others, like seborrheic keratosis, are severely underrepresented (80 total images). This imbalance could bias the model toward predicting the more frequent classes.
     </p>
    </dd>
    <dt class="pagebreak-before less_space">Small test set</dt>
    <dd>
     <p>Some categories have as few as three images in the test set, making it difficult to evaluate model performance across different lesion types. Such a small test set can easily lead to misleading performance metrics. We can also see that the ratio of train to test data is not even across lesion classes.</p>
    </dd>
    <dt>No validation set</dt>
    <dd>
     <p>The dataset only provides <code>Train</code> and <code>Test</code> splits, but no <code>Valid</code> subset. A validation set is crucial for tuning hyperparameters and assessing model improvements without touching the final test set.
     </p>
    </dd>
   </dl>
   <p>To effectively train a model on this dataset, we’ll need to address these points using techniques like data augmentation and resampling. Before diving into these, let’s visually inspect some images to better understand the dataset. This will help us verify that the images match the lesion types introduced earlier in this chapter.
   </p>
   
   
<div data-type="note" epub:type="note"><h6>Note</h6>
   <p>Another important consideration is that this dataset consists solely of images and labels, without additional metadata such as lesion location, patient demographics, or clinical notes. This lack of context makes the classification task more challenging, as real-world diagnosis often relies on more than just the visual appearance of a lesion. For example, a dark lesion on the scalp of a 70-year-old patient may increase the probability of melanoma, since both age and sun-exposed areas are known risk factors.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html16" data-type="indexterm" id="id864"/>
   </p>
</div>

  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Previewing the Images"><div class="sect2" id="previewing-the-images">
   <h2>Previewing the Images</h2>
   <p><a contenteditable="false" data-primary="Pillow" data-type="indexterm" id="ch05_cancer.html17"/><a contenteditable="false" data-primary="Python" data-secondary="image handling" data-type="indexterm" id="ch05_cancer.html18"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="exploring the data" data-tertiary="previewing the images" data-type="indexterm" id="ch05_cancer.html19"/>This is the first time in this book that we’re working with image data, so let’s take a moment to explore how to handle it in Python. A common library for loading and processing images is Pillow, the modern version of the original PIL (Python Imaging Library). It retains the <code>PIL</code>
    module name for compatibility and is widely used for image handling.
   </p>
   <p>Now let’s take a look at one of the images. We’ll start with a filepath we retrieved earlier, as shown in <a data-type="xref" href="#first-single-image">Figure 5-5</a>:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">PIL</code> <code class="kn">import</code> <code class="n">Image</code>

<code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">metadata</code><code class="p">[</code><code class="s2">"full_path"</code><code class="p">]</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
</pre>
      
     
    
    
     <figure><div id="first-single-image" class="figure">
      <img alt="" src="assets/dlfb_0505.png" width="600" height="450"/>
      <h6><span class="label">Figure 5-5. </span>An example skin lesion image loaded using Pillow in Python.
      </h6>
     </div></figure>
    
   
   <p>
    We’d like to inspect images from specific lesion classes. To make this process easier, we’ll create a function that:
   </p>
   <ul class="simple">
    <li>
     <p>Randomly selects an image from a specified class in the dataset
     </p>
    </li>
    <li>
     <p>Loads the image using <code>PIL</code> (Pillow)
     </p>
    </li>
    <li>
     <p>Displays it with Matplotlib, including the class name as a title for clarity
     </p>
    </li>
   </ul>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>


<code class="k">def</code> <code class="nf">show_random_image</code><code class="p">(</code><code class="n">metadata</code><code class="p">:</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">,</code> <code class="n">class_name</code><code class="p">:</code> <code class="nb">str</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">plt</code><code class="o">.</code><code class="n">Figure</code><code class="p">:</code>
  <code class="n">record</code> <code class="o">=</code> <code class="p">(</code>
    <code class="n">metadata</code><code class="p">[</code><code class="n">metadata</code><code class="p">[</code><code class="s2">"class_orig"</code><code class="p">]</code> <code class="o">==</code> <code class="n">class_name</code><code class="p">]</code>
    <code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
    <code class="o">.</code><code class="n">to_dict</code><code class="p">(</code><code class="n">orient</code><code class="o">=</code><code class="s2">"records"</code><code class="p">)[</code><code class="mi">0</code><code class="p">]</code>
  <code class="p">)</code>
  <code class="n">fig</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">figure</code><code class="p">(</code><code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">4</code><code class="p">,</code> <code class="mi">4</code><code class="p">))</code>
  <code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">record</code><code class="p">[</code><code class="s2">"full_path"</code><code class="p">]))</code>
  <code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="n">record</code><code class="p">[</code><code class="s2">"class_orig"</code><code class="p">]</code><code class="o">.</code><code class="n">capitalize</code><code class="p">())</code>
  <code class="k">return</code> <code class="n">fig</code></pre>
      
     
    
   
   <p class="pagebreak-before">Let’s now use this code to examine an example melanoma image, as shown in <a data-type="xref" href="#first-single-melanoma">Figure 5-6</a>:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">show_random_image</code><code class="p">(</code><code class="n">metadata</code><code class="p">,</code> <code class="s2">"melanoma"</code><code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="first-single-melanoma" class="figure">
      <img alt="" src="assets/dlfb_0506.png" width="600" height="452"/>
      <h6><span class="label">Figure 5-6. </span>Example image from the “melanoma” class.
      </h6>
     </div></figure>
    
   
   <p>Next, let’s write a <code>plot_random_image_grid</code>
    to help us visualize a sample image from each skin lesion class. This will help us quickly get a sense of the dataset’s diversity and variation in lesion appearance:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">plot_random_image_grid</code><code class="p">(</code>
  <code class="n">metadata</code><code class="p">:</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code><code class="p">,</code> <code class="n">ncols</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">3</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="n">plt</code><code class="o">.</code><code class="n">Figure</code><code class="p">:</code>
  <code class="sd">"""Display a random example image from each class in a grid."""</code>
  <code class="n">records</code> <code class="o">=</code> <code class="n">metadata</code><code class="o">.</code><code class="n">groupby</code><code class="p">(</code><code class="s2">"class_orig"</code><code class="p">)</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">to_dict</code><code class="p">(</code><code class="n">orient</code><code class="o">=</code><code class="s2">"records"</code><code class="p">)</code>
  <code class="n">nrows</code> <code class="o">=</code> <code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">records</code><code class="p">)</code> <code class="o">+</code> <code class="n">ncols</code> <code class="o">-</code> <code class="mi">1</code><code class="p">)</code> <code class="o">//</code> <code class="n">ncols</code>
  <code class="n">fig</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">nrows</code><code class="p">,</code> <code class="n">ncols</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mf">2.5</code> <code class="o">*</code> <code class="n">nrows</code><code class="p">))</code>
  <code class="n">axes</code> <code class="o">=</code> <code class="n">axes</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code>

  <code class="k">for</code> <code class="n">record</code><code class="p">,</code> <code class="n">ax</code> <code class="ow">in</code> <code class="nb">zip</code><code class="p">(</code><code class="n">records</code><code class="p">,</code> <code class="n">axes</code><code class="p">):</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">record</code><code class="p">[</code><code class="s2">"full_path"</code><code class="p">]))</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="n">record</code><code class="p">[</code><code class="s2">"class_orig"</code><code class="p">]</code><code class="o">.</code><code class="n">capitalize</code><code class="p">())</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>

  <code class="n">plt</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code>
  <code class="k">return</code> <code class="n">fig</code></pre>
      
     
    
   
   <p>Let’s run this function (see <a data-type="xref" href="#skin-image-grid">Figure 5-7</a>):
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">plot_random_image_grid</code><code class="p">(</code><code class="n">metadata</code><code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="skin-image-grid" class="figure">
      <img alt="" src="assets/dlfb_0507.png" width="600" height="462"/>
      <h6><span class="label">Figure 5-7. </span>Grid of skin images with their corresponding labels.
      </h6>
     </div></figure>
    
   <p>With a better visual understanding of our dataset, we’re now ready to build a flexible input pipeline that will support the rest of our experiments.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html19" data-type="indexterm" id="id865"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html18" data-type="indexterm" id="id866"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html17" data-type="indexterm" id="id867"/></p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Addressing Dataset Issues"><div class="sect2" id="building-a-datasetbuilder">
   <h2>Addressing Dataset Issues</h2>
   <p><a contenteditable="false" data-primary="DatasetBuilder class" data-type="indexterm" id="ch05_cancer.html20"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="exploring the data" data-tertiary="building a DatasetBuilder" data-type="indexterm" id="ch05_cancer.html21"/>Now that we’ve explored the dataset, let’s address some of its limitations. Since improving model performance will be an iterative process, it’s helpful to create a <code>DatasetBuilder</code> class that makes it easy to experiment with different dataset configurations. We’ll design it to be flexible enough to support both multiclass classification (all lesion types) and binary classification (e.g., melanoma versus non-melanoma), allowing us to explore a variety of setups as we develop our models.</p>
<p>First, to ensure proper evaluation, we define a 70/20/10 split for <code>train</code>, <code>valid</code>, and <code>test</code> sets:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="k">as</code> <code class="nn">np</code>

<code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">splits</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"train"</code><code class="p">:</code> <code class="mf">0.7</code><code class="p">,</code> <code class="s2">"valid"</code><code class="p">:</code> <code class="mf">0.20</code><code class="p">,</code> <code class="s2">"test"</code><code class="p">:</code> <code class="mf">0.10</code><code class="p">}</code>

<code class="n">metadata</code><code class="p">[</code><code class="s2">"split"</code><code class="p">]</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">choice</code><code class="p">(</code>
  <code class="nb">list</code><code class="p">(</code><code class="n">splits</code><code class="o">.</code><code class="n">keys</code><code class="p">()),</code> <code class="n">p</code><code class="o">=</code><code class="nb">list</code><code class="p">(</code><code class="n">splits</code><code class="o">.</code><code class="n">values</code><code class="p">()),</code> <code class="n">size</code><code class="o">=</code><code class="n">metadata</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
<code class="p">)</code>

<code class="n">counts</code> <code class="o">=</code> <code class="n">pd</code><code class="o">.</code><code class="n">crosstab</code><code class="p">(</code><code class="n">metadata</code><code class="p">[</code><code class="s2">"class_orig"</code><code class="p">],</code> <code class="n">metadata</code><code class="p">[</code><code class="s2">"split"</code><code class="p">],</code> <code class="n">margins</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">counts</code><code class="p">)</code></pre>
      
     <p>Output:</p>
<pre data-type="programlisting">
split                 test  train  valid   All
class_orig                                    
actinic keratosis       13     85     32   130
basal cell carcinoma    39    284     69   392
dermatofibroma          11     84     16   111
melanoma                55    307     92   454
nevus                   41    250     82   373
pigmented benign ...    39    358     81   478
seborrheic keratosis     9     50     21    80
squamous cell car...    14    138     45   197
vascular lesion         10     99     33   142
All                    231   1655    471  2357</pre>
      
     
    
   
   <p>We have visualized the class distribution across the new splits in <a data-type="xref" href="#fig5-8">Figure 5-8</a>:
   </p>
  
   
       <pre data-type="programlisting" data-code-language="python"><code class="n">counts</code><code class="o">.</code><code class="n">drop</code><code class="p">([</code><code class="s2">"All"</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code><code class="o">.</code><code class="n">drop</code><code class="p">([</code><code class="s2">"All"</code><code class="p">],</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code><code class="o">.</code><code class="n">plot</code><code class="o">.</code><code class="n">barh</code><code class="p">(</code><code class="n">stacked</code><code class="o">=</code><code class="kc">True</code><code class="p">);</code></pre>
     
      
  
     <figure><div id="fig5-8" class="figure"><img alt="" src="assets/dlfb_0508.png" width="600" height="321"/>
        <h6><span class="label">Figure 5-8. </span>Bar plot of the distribution of classes across new training, validation, and <span class="keep-together">test sets</span>.</h6>
</div></figure>
 

   <p>With this, we now have a better-structured dataset split—featuring a dedicated validation set, a larger test set, and consistent train/valid/test proportions across lesion classes. This sets the stage for building our <code>DatasetBuilder</code>, which will allow us to efficiently experiment with different training configurations.</p>


   <section data-type="sect3" data-pdf-bookmark="Balancing the batches"><div class="sect3" id="balancing-the-batches">
    <h3>Balancing the batches</h3>
    <p><a contenteditable="false" data-primary="DatasetBuilder class" data-secondary="balancing the batches" data-type="indexterm" id="ch05_cancer.html22"/>Some skin lesion classes are much more common than others in our dataset. For example, pigmented benign keratosis and nevus are heavily represented, while classes like seborrheic keratosis and dermatofibroma are much rarer. If we train directly on this imbalanced data, the model may focus on predicting the dominant classes correctly while ignoring the rare ones—a bias that can hurt generalization.</p>
    
    <p>To mitigate this, we implemented a <em>balanced sampler</em> during training. This ensures that each batch contains an equal number of examples from each class, giving the model more exposure to underrepresented categories. The validation and test sets, however, maintain their original class distributions to reflect real-world class frequencies during evaluation.</p>
    
    <p>Although the balanced sampler didn’t lead to measurable gains in our experiments for this chapter, it remains a useful tool when working with severely imbalanced datasets, which we encourage you to try in your future projects.
    </p>
 
   
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Augmenting the dataset"><div class="sect3" id="augmenting-the-dataset">
    <h3>Augmenting the dataset</h3>
    
    <p><a contenteditable="false" data-primary="data augmentation" data-type="indexterm" id="ch05_cancer.html23"/><a contenteditable="false" data-primary="DatasetBuilder class" data-secondary="data augmentation" data-type="indexterm" id="ch05_cancer.html24"/>Our dataset is relatively small, and in deep learning, few things improve model performance more reliably than having access to more high-quality data. Since collecting new labeled images is expensive and time-consuming, we turn to <em>data augmentation</em>—a technique that synthetically increases dataset size by applying label-preserving transformations to existing images.</p>
    <p>Augmentation encourages the model to generalize better by exposing it to natural variability in image appearance. For example, random rotations, flips, crops, color shifts, and brightness changes teach the model that the class of a lesion doesn’t change just because its lighting or orientation does. This helps reduce overfitting and improves robustness to real-world image variation.</p>
    <p>
However, it’s important to remember that augmentation does <em>not</em> introduce truly new information. Augmented examples are variations of the same underlying data—so any existing dataset biases (e.g., skin tone imbalance or limited anatomical diversity) will still be present. What augmentation does offer is a way to nudge the model toward learning more general, abstract features rather than memorizing surface-level details.</p>
    <p>
Rather than manually applying transformations, we use the <a href="https://oreil.ly/3FxiK">DeepMind PIX library</a>—a JAX-compatible image augmentation toolkit. It integrates cleanly into our pipeline and allows us to dynamically apply transformations on the fly during training, keeping memory usage low and training efficient.</p>
    <p class="pagebreak-before">In our setup, each image is randomly transformed with a fixed probability. As with other JAX operations, the random number generator (<code>rng</code>) is explicitly managed, ensuring that augmentations are reproducible and traceable:</p>
    
    
    
       
        <pre data-type="programlisting" data-code-language="python"><code class="nd">@jax</code><code class="o">.</code><code class="n">jit</code>
<code class="k">def</code> <code class="nf">rich_augmentor</code><code class="p">(</code><code class="n">image</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
  <code class="sd">"""Applies random flips, brightness, contrast, hue changes, and rotation."""</code>
  <code class="n">image</code> <code class="o">=</code> <code class="n">pix</code><code class="o">.</code><code class="n">random_flip_left_right</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="n">image</code><code class="p">)</code>
  <code class="n">image</code> <code class="o">=</code> <code class="n">pix</code><code class="o">.</code><code class="n">random_flip_up_down</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="n">image</code><code class="p">)</code>
  <code class="n">image</code> <code class="o">=</code> <code class="n">pix</code><code class="o">.</code><code class="n">random_brightness</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="n">image</code><code class="p">,</code> <code class="n">max_delta</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>
  <code class="n">image</code> <code class="o">=</code> <code class="n">pix</code><code class="o">.</code><code class="n">random_contrast</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="n">image</code><code class="p">,</code> <code class="n">lower</code><code class="o">=</code><code class="mf">0.9</code><code class="p">,</code> <code class="n">upper</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
  <code class="n">image</code> <code class="o">=</code> <code class="n">pix</code><code class="o">.</code><code class="n">random_hue</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="n">image</code><code class="p">,</code> <code class="n">max_delta</code><code class="o">=</code><code class="mf">0.05</code><code class="p">)</code>
  <code class="c1"># Angles are provided in radians, i.e. +/- 10 degrees.</code>
  <code class="n">image</code> <code class="o">=</code> <code class="n">pix</code><code class="o">.</code><code class="n">rotate</code><code class="p">(</code>
    <code class="n">image</code><code class="p">,</code>
    <code class="n">angle</code><code class="o">=</code><code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">uniform</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="n">shape</code><code class="o">=</code><code class="p">(),</code> <code class="n">minval</code><code class="o">=-</code><code class="mf">0.174533</code><code class="p">,</code> <code class="n">maxval</code><code class="o">=</code><code class="mf">0.174533</code><code class="p">),</code>
  <code class="p">)</code>
  <code class="k">return</code> <code class="n">image</code></pre>
       
      
     
    
    <p>To illustrate augmentation in practice, we select a melanoma image and apply our transformation pipeline multiple times. Each variation is slightly different, simulating realistic image variability. See the result in <a data-type="xref" href="#skin-image-grid-augmentation">Figure 5-9</a>:
    </p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">jax</code>
<code class="kn">import</code> <code class="nn">jax.numpy</code> <code class="k">as</code> <code class="nn">jnp</code>

<code class="n">rng</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code>
  <code class="n">metadata</code><code class="p">[</code><code class="n">metadata</code><code class="p">[</code><code class="s2">"class_orig"</code><code class="p">]</code> <code class="o">==</code> <code class="s2">"melanoma"</code><code class="p">]</code>
  <code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">1</code><code class="p">)[</code><code class="s2">"full_path"</code><code class="p">]</code>
  <code class="o">.</code><code class="n">values</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
<code class="p">)</code>

<code class="n">_</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">9</code><code class="p">,</code> <code class="mi">9</code><code class="p">))</code>
<code class="n">axes</code> <code class="o">=</code> <code class="n">axes</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code>
<code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
<code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"Original"</code><code class="p">)</code>
<code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">asarray</code><code class="p">(</code><code class="n">image</code><code class="p">)</code> <code class="o">/</code> <code class="mf">255.0</code>
<code class="k">for</code> <code class="n">ax</code> <code class="ow">in</code> <code class="n">axes</code><code class="p">[</code><code class="mi">1</code><code class="p">:]:</code>
  <code class="n">rng</code><code class="p">,</code> <code class="n">rng_augment</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="n">num</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
  <code class="n">ax</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">rich_augmentor</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="n">rng_augment</code><code class="p">))</code>
  <code class="n">ax</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"Augmented"</code><code class="p">)</code>
  <code class="n">ax</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code></pre>
       
      
     
     
      <figure><div id="skin-image-grid-augmentation" class="figure">
       <img alt="" src="assets/dlfb_0509.png" width="600" height="552"/>
       <h6><span class="label">Figure 5-9. </span>Original skin image (top left) followed by different augmented versions, demonstrating transformations like flipping, rotation, and color shifts.
       </h6>
      </div></figure>
     
    
    <p>This setup is a good starting point, but keep in mind that each augmentation method has tunable parameters—and the <a href="https://oreil.ly/74UjB">PIX documentation</a> lists many more options. Selecting and tuning augmentations is effectively a hyperparameter search.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html24" data-type="indexterm" id="id868"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html23" data-type="indexterm" id="id869"/> What works best depends heavily on the dataset and task. For a broader overview of effective augmentation strategies, see this survey.<sup><a data-type="noteref" id="id870-marker" href="ch05.html#id870">7</a></sup> </p>
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Preprocessing the images"><div class="sect3" id="preprocessing-the-images">
    <h3>Preprocessing the images</h3>
    <p><a contenteditable="false" data-primary="DatasetBuilder class" data-secondary="preprocessing the images" data-type="indexterm" id="ch05_cancer.html25"/>Before feeding images into a neural network, we need to standardize them. This involves ensuring that all inputs have the same shape and pixel value range. Without this step, differences in resolution, aspect ratio, or intensity could prevent the model from learning consistent patterns.</p>

<p>Many deep learning computer vision models—including ImageNet-trained ResNets we will work with in this chapter—expect fixed-size square input images. However, skin lesion photos vary widely in shape and size. Simply resizing these directly to a square can distort important clinical features. To avoid this, we first resize the image while preserving its aspect ratio, then center-crop a square from the result. This produces consistent input shapes without stretching or compressing lesions.</p>
    
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">resize_preserve_aspect</code><code class="p">(</code>
  <code class="n">image</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">short_side</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">256</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
  <code class="sd">"""Resize image with shorter side is `short_side`, keeping aspect ratio."""</code>
  <code class="n">h</code><code class="p">,</code> <code class="n">w</code><code class="p">,</code> <code class="n">c</code> <code class="o">=</code> <code class="n">image</code><code class="o">.</code><code class="n">shape</code>
  <code class="n">scale</code> <code class="o">=</code> <code class="n">short_side</code> <code class="o">/</code> <code class="n">jnp</code><code class="o">.</code><code class="n">minimum</code><code class="p">(</code><code class="n">h</code><code class="p">,</code> <code class="n">w</code><code class="p">)</code>
  <code class="n">new_h</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">h</code> <code class="o">*</code> <code class="n">scale</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">int32</code><code class="p">)</code>
  <code class="n">new_w</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">w</code> <code class="o">*</code> <code class="n">scale</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">int32</code><code class="p">)</code>
  <code class="n">resized</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">image</code><code class="o">.</code><code class="n">resize</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="p">(</code><code class="n">new_h</code><code class="p">,</code> <code class="n">new_w</code><code class="p">,</code> <code class="n">c</code><code class="p">),</code> <code class="n">method</code><code class="o">=</code><code class="s2">"bilinear"</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">resized</code>


<code class="k">def</code> <code class="nf">center_crop</code><code class="p">(</code><code class="n">image</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">size</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">224</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
  <code class="sd">"""Crop the center square of given size from an image."""</code>
  <code class="n">h</code><code class="p">,</code> <code class="n">w</code><code class="p">,</code> <code class="n">_</code> <code class="o">=</code> <code class="n">image</code><code class="o">.</code><code class="n">shape</code>
  <code class="n">top</code> <code class="o">=</code> <code class="p">(</code><code class="n">h</code> <code class="o">-</code> <code class="n">size</code><code class="p">)</code> <code class="o">//</code> <code class="mi">2</code>
  <code class="n">left</code> <code class="o">=</code> <code class="p">(</code><code class="n">w</code> <code class="o">-</code> <code class="n">size</code><code class="p">)</code> <code class="o">//</code> <code class="mi">2</code>
  <code class="k">return</code> <code class="n">image</code><code class="p">[</code><code class="n">top</code> <code class="p">:</code> <code class="n">top</code> <code class="o">+</code> <code class="n">size</code><code class="p">,</code> <code class="n">left</code> <code class="p">:</code> <code class="n">left</code> <code class="o">+</code> <code class="n">size</code><code class="p">]</code></pre>
       
      
     
    
    <p>We also need to normalize pixel values. Raw image intensities usually range from 0 to 255, but neural networks tend to train more effectively when inputs are scaled to a smaller, consistent range. A common method is <em>min-max scaling</em>, where each pixel is divided by 255 to bring values into the <code>[0, 1]</code> range. Another option is standardization, where each pixel is transformed to have zero mean and unit variance.</p>
    <div data-type="warning" epub:type="warning"><h6>Warning</h6>
     <p><a contenteditable="false" data-primary="standardization, image preprocessing versus normalization strategy" data-type="indexterm" id="id871"/>The term <em>standardization</em> is overloaded in this context and refers to two different concepts:
     </p>
     <ul>
      <li>
       <p><em>General image preprocessing</em>: Ensuring consistency in image size, format, and value ranges</p>
      </li>
      <li>
       <p><em>A specific normalization strategy</em>: Transforming pixel values to have a mean of 0 and a standard deviation of 1</p>
      </li>
    </ul>
     <p>Throughout this book, we’ll use <em>standardization</em> in the first sense—as a synonym for general input preprocessing.</p>
    </div>
    <p>For our model,we’ll use min-max scaling to normalize pixel values, dividing by 255 to bring intensities into the <code>[0, 1]</code> range. This approach is widely used in CNN training and works well for image data:</p>
    
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">rescale_image</code><code class="p">(</code><code class="n">image</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
  <code class="sd">"""Normalizes pixel values to the [0, 1] range by dividing by 255."""</code>
  <code class="k">return</code> <code class="n">image</code> <code class="o">/</code> <code class="mf">255.0</code></pre>
       
      
     
    
    <p>Now let’s apply all preprocessing steps—reading JPEG images, resizing while preserving aspect ratio, center-cropping, and normalizing—to ensure that our dataset is standardized before training. While we could choose any image size, we’ll use 224 × 224 pixels to match the expected input size of the original ResNet architecture, which we’ll explore later in the chapter. Since these transformations need to be applied only once, doing them in advance improves efficiency and avoids redundant computations during training.</p>
    
<p>In <a data-type="xref" href="#image-preprocessing-for-standarization">Figure 5-10</a>, you can see an example of an image before and after preprocessing:</p>
    
       
        <pre data-type="programlisting" data-code-language="python"><code class="n">original_image</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">metadata</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="mi">0</code><code class="p">][</code><code class="s2">"full_path"</code><code class="p">])</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">original_image</code><code class="p">)</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">resize_preserve_aspect</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">center_crop</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">rescale_image</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>

<code class="n">_</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">3</code><code class="p">))</code>
<code class="n">axes</code> <code class="o">=</code> <code class="n">axes</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code>
<code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">original_image</code><code class="p">)</code>
<code class="n">axes</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"Original"</code><code class="p">)</code>
<code class="n">axes</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
<code class="n">axes</code><code class="p">[</code><code class="mi">1</code><code class="p">]</code><code class="o">.</code><code class="n">set_title</code><code class="p">(</code><code class="s2">"Preprocessed"</code><code class="p">);</code></pre>
       
      
     
     
      <figure><div id="image-preprocessing-for-standarization" class="figure">
       <img alt="" src="assets/dlfb_0510.png" width="600" height="298"/>
       <h6><span class="label">Figure 5-10. </span>Images are standardized to ensure consistent dimensions and pixel value ranges. Note that although the preprocessed image has been rescaled from 0-255 to 0-1, <code>imshow</code> automatically adjusts the display scale, so the visual appearance <span class="keep-together">remains unchanged</span>.
       </h6>
      </div></figure>
     
    <p>With data preprocessing topics complete, we can now focus on efficiently storing and accessing the data—a crucial step for scaling training without overloading system memory.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html25" data-type="indexterm" id="id872"/></p>
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="Data storage with memory-mapped arrays"><div class="sect3" id="efficient-data-storage-with-memory-mapped-arrays">
    <h3>Data storage with memory-mapped arrays</h3>
    <p><a contenteditable="false" data-primary="DatasetBuilder class" data-secondary="data storage with memory-mapped NumPy arrays" data-type="indexterm" id="id873"/><a contenteditable="false" data-primary="memory-mapped NumPy arrays" data-type="indexterm" id="id874"/>We’ve already emphasized the importance of efficiency in model training: faster data processing means faster iterations and ultimately better models. A major inefficiency we want to avoid is repeatedly preprocessing the same data every time a batch is passed to the model. At the same time, storing all processed images in memory isn’t feasible. So, how can we balance efficiency and memory constraints?
    </p>
    <p>A practical solution is <em>memory-mapped NumPy arrays</em>. These allow us to store preprocessed images on disk while accessing them as if they were in memory, making retrieval fast and efficient. This way, we preprocess all images once before training and then load them dynamically during training without redundant computation.
    </p>
    <p>The implementation is straightforward: we first define the storage file, specify the data type (<code>float32</code>), and set the shape of the dataset: 224 × 224 pixels with three color channels. This creates an empty file on disk, preallocating the required storage space. We then preprocess and store each image in the memory-mapped array:
    </p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">tempfile</code> <code class="kn">import</code> <code class="n">TemporaryFile</code>

<code class="n">images_on_disk</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">memmap</code><code class="p">(</code>
  <code class="n">TemporaryFile</code><code class="p">(),</code> <code class="n">dtype</code><code class="o">=</code><code class="s2">"float32"</code><code class="p">,</code> <code class="n">mode</code><code class="o">=</code><code class="s2">"w+"</code><code class="p">,</code> <code class="n">shape</code><code class="o">=</code><code class="p">(</code><code class="mi">10</code><code class="p">,</code> <code class="mi">224</code><code class="p">,</code> <code class="mi">224</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
<code class="p">)</code>
<code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">full_path</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">metadata</code><code class="p">[:</code><code class="mi">10</code><code class="p">][</code><code class="s2">"full_path"</code><code class="p">]):</code>
  <code class="n">image</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">full_path</code><code class="p">)</code>
  <code class="n">image</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
  <code class="n">image</code> <code class="o">=</code> <code class="n">resize_preserve_aspect</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
  <code class="n">image</code> <code class="o">=</code> <code class="n">center_crop</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
  <code class="n">image</code> <code class="o">=</code> <code class="n">rescale_image</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
  <code class="n">images_on_disk</code><code class="p">[</code><code class="n">i</code><code class="p">,</code> <code class="p">:,</code> <code class="p">:,</code> <code class="p">:]</code> <code class="o">=</code> <code class="n">image</code>
<code class="n">images_on_disk</code><code class="o">.</code><code class="n">flush</code><code class="p">()</code></pre>
       
      
     
    
    <p>This approach gives us the best of both worlds: efficient preprocessing without excessive memory consumption.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html21" data-type="indexterm" id="id875"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html20" data-type="indexterm" id="id876"/>
    </p>
   </div></section>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Building a DatasetBuilder"><div class="sect2" id="putting-it-all-together">
   <h2>Building a DatasetBuilder</h2>
   <p>
    <a contenteditable="false" data-primary="DatasetBuilder class" data-secondary="organizing components into structured DatasetBuilder class" data-type="indexterm" id="ch05_cancer.html26"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="exploring the data" data-tertiary="organizing components into structured DatasetBuilder class" data-type="indexterm" id="ch05_cancer.html27"/>At this point, we have tackled the major challenges of working with this dataset: splitting it into training, validation, and test sets; handling class imbalances; preprocessing images; and augmenting the data. Now we will organize these components into a structured <code>DatasetBuilder</code>
    class that returns <code>Dataset</code>
    instances that have the data iterators that will actually fetch us batches of data that we can train or evaluate a model on.
   </p>
   <div data-type="tip"><h6>Tip</h6>
    <p>
    Designing a flexible dataset builder pays off when iterating on your model. It lets you quickly try different label mappings, sampling strategies, and preprocessing steps—all without rewriting core code. This modularity speeds up experimentation and helps you pinpoint which data choices actually improve performance.</p>
   </div>
   <p>Let’s take a closer look at the <code>DatasetBuilder</code>, which coordinates the creation of dataset splits and preprocessing:</p>
   
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">DatasetBuilder</code><code class="p">:</code>
  <code class="sd">"""Builds a dataset with metadata, loaded images, and class mappings."""</code>

  <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">data_dir</code><code class="p">:</code> <code class="nb">str</code><code class="p">,</code> <code class="n">out_dir</code><code class="p">:</code> <code class="nb">str</code> <code class="o">|</code> <code class="kc">None</code> <code class="o">=</code> <code class="kc">None</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="kc">None</code><code class="p">:</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">data_dir</code> <code class="o">=</code> <code class="n">data_dir</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">out_dir</code> <code class="o">=</code> <code class="n">out_dir</code> <code class="ow">or</code> <code class="n">data_dir</code>

  <code class="k">def</code> <code class="nf">build</code><code class="p">(</code>
    <code class="bp">self</code><code class="p">,</code>
    <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code>
    <code class="n">splits</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">float</code><code class="p">],</code>
    <code class="n">preprocessors</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="n">Callable</code><code class="p">]</code> <code class="o">=</code> <code class="p">[</code><code class="n">crop</code><code class="p">],</code>
    <code class="n">image_size</code><code class="p">:</code> <code class="nb">tuple</code><code class="p">[</code><code class="nb">int</code><code class="p">,</code> <code class="nb">int</code><code class="p">,</code> <code class="nb">int</code><code class="p">]</code> <code class="o">=</code> <code class="p">(</code><code class="mi">224</code><code class="p">,</code> <code class="mi">224</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code>
    <code class="n">class_map</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">Any</code><code class="p">]</code> <code class="o">|</code> <code class="kc">None</code> <code class="o">=</code> <code class="kc">None</code><code class="p">,</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">Dataset</code><code class="p">]:</code>
    <code class="sd">"""Builds the dataset splits from loaded metadata and loaded images."""</code>
    <code class="n">metadata</code> <code class="o">=</code> <code class="n">MetadataLoader</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">data_dir</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">out_dir</code><code class="p">)</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">class_map</code><code class="p">)</code>
    <code class="n">images</code> <code class="o">=</code> <code class="n">ImageLoader</code><code class="p">(</code><code class="n">metadata</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">out_dir</code><code class="p">)</code><code class="o">.</code><code class="n">load</code><code class="p">(</code><code class="n">preprocessors</code><code class="p">,</code> <code class="n">image_size</code><code class="p">)</code>

    <code class="c1"># Shuffle the dataset and assign each example to one of the dataset splits.</code>
    <code class="n">num_samples</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">metadata</code><code class="p">)</code>
    <code class="n">rng</code><code class="p">,</code> <code class="n">rng_perm</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
    <code class="n">shuffled_indices</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">permutation</code><code class="p">(</code><code class="n">rng_perm</code><code class="p">,</code> <code class="n">num_samples</code><code class="p">)</code>

    <code class="c1"># Create each dataset split using the shuffled indices and store the</code>
    <code class="c1"># corresponding metadata and image data in a Dataset object.</code>
    <code class="n">dataset_splits</code><code class="p">,</code> <code class="n">start</code> <code class="o">=</code> <code class="p">{},</code> <code class="mi">0</code>
    <code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">size</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">_get_split_sizes</code><code class="p">(</code><code class="n">splits</code><code class="p">,</code> <code class="n">num_samples</code><code class="p">):</code>
      <code class="n">indices</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">(</code><code class="n">shuffled_indices</code><code class="p">[</code><code class="n">start</code> <code class="p">:</code> <code class="p">(</code><code class="n">start</code> <code class="o">+</code> <code class="n">size</code><code class="p">)])</code>
      <code class="n">dataset_splits</code><code class="p">[</code><code class="n">name</code><code class="p">]</code> <code class="o">=</code> <code class="n">Dataset</code><code class="p">(</code>
        <code class="n">metadata</code><code class="o">=</code><code class="n">metadata</code><code class="o">.</code><code class="n">iloc</code><code class="p">[</code><code class="n">indices</code><code class="p">],</code>
        <code class="n">images</code><code class="o">=</code><code class="n">images</code><code class="p">,</code>
        <code class="n">num_classes</code><code class="o">=</code><code class="n">metadata</code><code class="p">[</code><code class="s2">"class"</code><code class="p">]</code><code class="o">.</code><code class="n">nunique</code><code class="p">(),</code>
      <code class="p">)</code>
      <code class="n">start</code> <code class="o">+=</code> <code class="n">size</code>
    <code class="k">return</code> <code class="n">dataset_splits</code>

  <code class="k">def</code> <code class="nf">_get_split_sizes</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">splits</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">float</code><code class="p">],</code> <code class="n">num_samples</code><code class="p">:</code> <code class="nb">int</code><code class="p">):</code>
    <code class="sd">"""Convert fractional split sizes to integer counts that sum to total."""</code>
    <code class="n">names</code> <code class="o">=</code> <code class="nb">list</code><code class="p">(</code><code class="n">splits</code><code class="p">)</code>
    <code class="n">sizes</code> <code class="o">=</code> <code class="p">[</code><code class="nb">int</code><code class="p">(</code><code class="n">num_samples</code> <code class="o">*</code> <code class="n">splits</code><code class="p">[</code><code class="n">name</code><code class="p">])</code> <code class="k">for</code> <code class="n">name</code> <code class="ow">in</code> <code class="n">names</code><code class="p">[:</code><code class="o">-</code><code class="mi">1</code><code class="p">]]</code>
    <code class="n">sizes</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">num_samples</code> <code class="o">-</code> <code class="nb">sum</code><code class="p">(</code><code class="n">sizes</code><code class="p">))</code>  <code class="c1"># last split gets the remainder</code>
    <code class="k">yield from</code> <code class="nb">zip</code><code class="p">(</code><code class="n">names</code><code class="p">,</code> <code class="n">sizes</code><code class="p">)</code>
</pre>
      
     
    
   
   <p>We’ve already implemented each of these components, but let’s highlight a few key details:
   </p>
   <dl>
    <dt>Metadata loading</dt><dd><p><code>MetadataLoader</code> ensures that all image filepaths and their associated class/split information are consistently saved to a CSV file. This avoids inconsistencies from relying on <code>rglob</code>, which does not guarantee a consistent file order. Because the image data is later stored in a memory-mapped array in the same order as the metadata, maintaining this consistency is crucial.</p></dd>
 <dt>Flexible class mapping</dt><dd><p>The <code>MetadataLoader</code> also supports binary or grouped classification setups via a customizable <code>class_map</code>. If no map is provided, it defaults to a standard multiclass configuration based on the directory structure.</p></dd>
<dt>Image preprocessing</dt><dd><p><code>ImageLoader</code> handles reading, processing, and storing images in memory-mapped arrays. To avoid recomputation, images are stored in a compact raw format once, and then multiple preprocessing strategies (e.g., resizing or cropping) can be applied and stored separately as memmaps.</p></dd>
 <dt>Dataset creation</dt><dd><p>The <code>build</code> method brings everything together: it loads metadata and images, shuffles the dataset, splits it into train/validation/test partitions, and wraps each into a <code>Dataset</code> object. Each dataset contains consistent metadata and preprocessed image views, ready for training.</p></dd>
 </dl>
   
   
<section data-type="sect3" data-pdf-bookmark="Loading the Metadata and Images"><div class="sect3" id="id125">
  <h3>Loading the Metadata and Images</h3>
    <p><a contenteditable="false" data-primary="DatasetBuilder class" data-secondary="organizing components into structured DatasetBuilder class" data-tertiary="MetadataLoader/ImageLoader/Dataset classes" data-type="indexterm" id="ch05_cancer.html28"/>You can find the code for <a contenteditable="false" data-primary="MetadataLoader class" data-type="indexterm" id="ch05_cancer.html29"/>the supporting classes for the <code>MetadataLoader</code> and <code>ImageLoader</code> in the <code>dlfb.cancer.dataset.builder</code> library. Here we will briefly describe how they work and fulfill their responsibility to create the <code>Dataset</code>.</p>

<p>The <code>MetadataLoader</code> constructs a metadata CSV file if one does not already exist. It extracts the original split and class information from the folder structure, applies any desired label mappings, and provides consistent row-level access to the dataset. <a contenteditable="false" data-primary="ImageLoader class" data-type="indexterm" id="ch05_cancer.html30"/>Image loading is a bit more involved. While <code>MetadataLoader</code> handles the filepaths and class labels, <code>ImageLoader</code> is responsible for reading, preprocessing, and storing the actual image data. Here are some notes on how <code>ImageLoader</code> works: </p>
  <dl>
  <dt>Image preprocessing and storage</dt><dd><p>To avoid redundant computations, <code>ImageLoader</code> first stores the raw images as a single memory-mapped array. This array is compact, storing flattened versions of each image along with their original shapes and offsets so they can later be reconstructed. Once this raw storage is complete, various preprocessing functions (such as cropping or resizing) can be applied and the results saved into additional memory-mapped files. This design allows us to support multiple preprocessing strategies without reloading the original JPEGs or repeating expensive operations.</p></dd>
  <dt>Preprocessing flexibility</dt><dd><p>Preprocessing functions are applied in a modular way. Any number of transforms can be passed into the <code>ImageLoader</code>, and each will produce its own memmap. These preprocessed views are stored under different keys (e.g., <code>"crop"</code>, <code>"resize"</code>) inside the <code>Images</code> object, which acts as a unified interface to access them.</p></dd>
  <dt>Dataset assembly</dt><dd><p>Finally, the <code>DatasetBuilder.build()</code> method pulls all of this together. It loads the metadata, processes the images, shuffles the dataset, splits it into training, validation, and test sets, and returns <code>Dataset</code> objects for each. These objects provide clean access to both image arrays and class labels, ready for model training<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html30" data-type="indexterm" id="id877"/>.</p></dd>
</dl>


<p><a contenteditable="false" data-primary="Dataset class" data-type="indexterm" id="id878"/>This is all stored in the <code>Dataset</code> class itself:</p>

<pre data-type="programlisting" data-code-language="python"><code class="nd">@dataclass</code>
<code class="k">class</code> <code class="nc">Dataset</code><code class="p">:</code>
  <code class="sd">"""Dataset class storing images and corresponding metadata."""</code>

  <code class="n">metadata</code><code class="p">:</code> <code class="n">pd</code><code class="o">.</code><code class="n">DataFrame</code>
  <code class="n">images</code><code class="p">:</code> <code class="n">Images</code>
  <code class="n">num_classes</code><code class="p">:</code> <code class="nb">int</code>

  <code class="k">def</code> <code class="nf">get_dummy_input</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jnp</code><code class="o">.</code><code class="n">ndarray</code><code class="p">:</code>
    <code class="sd">"""Returns dummy input with the correct shape for the model."""</code>
    <code class="k">return</code> <code class="n">jnp</code><code class="o">.</code><code class="n">empty</code><code class="p">((</code><code class="mi">1</code><code class="p">,)</code> <code class="o">+</code> <code class="bp">self</code><code class="o">.</code><code class="n">images</code><code class="o">.</code><code class="n">size</code><code class="p">)</code>

  <code class="k">def</code> <code class="nf">num_samples</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">int</code><code class="p">:</code>
    <code class="sd">"""Returns the number of samples in the dataset."""</code>
    <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">metadata</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>

  <code class="k">def</code> <code class="nf">get_images</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">preprocessor</code><code class="p">:</code> <code class="n">Callable</code><code class="p">,</code> <code class="n">indices</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Returns preprocessed images for the given indices."""</code>
    <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">images</code><code class="o">.</code><code class="n">loaded</code><code class="p">[</code><code class="n">preprocessor</code><code class="o">.</code><code class="vm">__name__</code><code class="p">][</code><code class="n">indices</code><code class="p">]</code>

  <code class="k">def</code> <code class="nf">get_labels</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">indices</code><code class="p">:</code> <code class="nb">list</code><code class="p">[</code><code class="nb">int</code><code class="p">])</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
    <code class="sd">"""Returns integer class labels for the given indices."""</code>
    <code class="k">return</code> <code class="n">jnp</code><code class="o">.</code><code class="n">int16</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">metadata</code><code class="o">.</code><code class="n">loc</code><code class="p">[</code><code class="n">indices</code><code class="p">][</code><code class="s2">"label"</code><code class="p">]</code><code class="o">.</code><code class="n">values</code><code class="p">)</code></pre>

    <p class="pagebreak-before">Where the <code>Images</code> are:</p>
    <pre data-type="programlisting" data-code-language="python"><code class="nd">@dataclass</code>
<code class="k">class</code> <code class="nc">Images</code><code class="p">:</code>
  <code class="sd">"""Stores image data and size information."""</code>

  <code class="n">loaded</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">np</code><code class="o">.</code><code class="n">memmap</code><code class="p">]</code>
  <code class="n">size</code><code class="p">:</code> <code class="nb">tuple</code><code class="p">[</code><code class="nb">int</code><code class="p">,</code> <code class="nb">int</code><code class="p">,</code> <code class="nb">int</code><code class="p">]</code></pre>
    

<p>The <code>Dataset</code> class is the primary interface for accessing data during training. It manages both the <code>metadata</code> (which tracks image paths and labels) and the memory-mapped <code>images</code>. Since we split the dataset into training, validation, and test sets, we can control their proportions using the <code>splits</code> argument.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html28" data-type="indexterm" id="id879"/></p>
</div></section>

   <section data-type="sect3" data-pdf-bookmark="Batching the data"><div class="sect3" id="id126">
  <h3>Batching the data</h3>
   <p><a contenteditable="false" data-primary="batching" data-type="indexterm" id="ch05_cancer.html31"/><a contenteditable="false" data-primary="DatasetBuilder class" data-secondary="organizing components into structured DatasetBuilder class" data-tertiary="batching the data" data-type="indexterm" id="ch05_cancer.html32"/>To streamline dataset handling, during training, we introduce the <code>BatchHandler</code> class. This class encapsulates the components required to generate batches from a <code>Dataset</code> object:</p>
    
    <ul>
<li><p>The <code>preprocessor</code> selects which preprocessed image view (e.g., cropped, resized) to use.</p></li>
 <li><p>The <code>sampler</code> determines how to draw samples for each batch (e.g., balanced versus random sampling).</p></li>
 <li><p>The optional <code>augmentor</code> applies real-time image transformations such as flipping, color jitter, or rotation to improve generalization.</p></li>
</ul>
   
    
       <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">BatchHandler</code><code class="p">:</code>
  <code class="sd">"""Helper to provide appropriately prepared batches form a dataset."""</code>

  <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code>
    <code class="bp">self</code><code class="p">,</code>
    <code class="n">preprocessor</code><code class="p">:</code> <code class="n">Callable</code> <code class="o">=</code> <code class="n">crop</code><code class="p">,</code>
    <code class="n">sampler</code><code class="p">:</code> <code class="n">Callable</code> <code class="o">=</code> <code class="n">epoch_sampler</code><code class="p">,</code>
    <code class="n">augmentor</code><code class="p">:</code> <code class="n">Callable</code> <code class="o">|</code> <code class="kc">None</code> <code class="o">=</code> <code class="kc">None</code><code class="p">,</code>
  <code class="p">):</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">preprocessor</code> <code class="o">=</code> <code class="n">preprocessor</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">sampler</code> <code class="o">=</code> <code class="n">sampler</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">augmentor</code> <code class="o">=</code> <code class="n">augmentor</code>

  <code class="k">def</code> <code class="nf">get_batches</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">dataset</code><code class="p">:</code> <code class="n">Dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code> <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">):</code>
    <code class="sd">"""Prepare dataset batches with the requested image manipulations."""</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">_validate_batch_size</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">)</code>
    <code class="n">rng</code><code class="p">,</code> <code class="n">rng_sampler</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="n">num</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>

    <code class="k">for</code> <code class="n">batch_indices</code> <code class="ow">in</code> <code class="bp">self</code><code class="o">.</code><code class="n">sampler</code><code class="p">(</code>
      <code class="n">dataset</code><code class="o">.</code><code class="n">metadata</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">,</code> <code class="n">rng_sampler</code>
    <code class="p">):</code>
      <code class="n">images</code> <code class="o">=</code> <code class="n">dataset</code><code class="o">.</code><code class="n">get_images</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">preprocessor</code><code class="p">,</code> <code class="n">batch_indices</code><code class="p">)</code>

      <code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">augmentor</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">:</code>
        <code class="n">rng</code><code class="p">,</code> <code class="n">rng_augment</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
        <code class="n">images</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">vmap</code><code class="p">(</code><code class="k">lambda</code> <code class="n">image</code><code class="p">,</code> <code class="n">r</code><code class="p">:</code> <code class="bp">self</code><code class="o">.</code><code class="n">augmentor</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="n">rng</code><code class="o">=</code><code class="n">r</code><code class="p">))(</code>
          <code class="n">images</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng_augment</code><code class="p">,</code> <code class="n">images</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">])</code>
        <code class="p">)</code>

      <code class="n">batch</code> <code class="o">=</code> <code class="p">{</code>
        <code class="s2">"frame_ids"</code><code class="p">:</code> <code class="n">batch_indices</code><code class="p">,</code>
        <code class="s2">"images"</code><code class="p">:</code> <code class="n">images</code><code class="p">,</code>
        <code class="s2">"labels"</code><code class="p">:</code> <code class="n">dataset</code><code class="o">.</code><code class="n">get_labels</code><code class="p">(</code><code class="n">batch_indices</code><code class="p">),</code>
      <code class="p">}</code>
      <code class="k">yield</code> <code class="n">batch</code>

  <code class="nd">@staticmethod</code>
  <code class="k">def</code> <code class="nf">_validate_batch_size</code><code class="p">(</code><code class="n">dataset</code><code class="p">:</code> <code class="n">Dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="p">:</code> <code class="nb">int</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="kc">None</code><code class="p">:</code>
    <code class="sd">"""Ensures that batch_size is within feasible bounds."""</code>
    <code class="k">if</code> <code class="n">batch_size</code> <code class="o">&gt;</code> <code class="n">dataset</code><code class="o">.</code><code class="n">num_samples</code><code class="p">():</code>
      <code class="k">raise</code> <code class="ne">ValueError</code><code class="p">(</code>
        <code class="sa">f</code><code class="s2">"Batch size (</code><code class="si">{</code><code class="n">batch_size</code><code class="si">}</code><code class="s2">) cannot be larger than dataset size "</code>
        <code class="s2">"({len(frame_ids)})."</code>
      <code class="p">)</code>
    <code class="k">if</code> <code class="n">batch_size</code> <code class="o">&gt;</code> <code class="n">dataset</code><code class="o">.</code><code class="n">num_classes</code><code class="p">:</code>
      <code class="k">raise</code> <code class="ne">ValueError</code><code class="p">(</code>
        <code class="sa">f</code><code class="s2">"batch_size (</code><code class="si">{</code><code class="n">batch_size</code><code class="si">}</code><code class="s2">) has to be larger than "</code>
        <code class="sa">f</code><code class="s2">"number of unique labels."</code>
      <code class="p">)</code>
</pre>
      
     
    
   
   <p>The <code>get_batches</code> method is the core of <code>BatchHandler</code>. It’s a generator function, meaning it yields one batch at a time, which is memory-efficient and well-suited for training loops. Here’s what happens inside:</p>
    
    <ol>
 <li><p>It validates the requested <code>batch_size</code>.</p></li>
  <li><p>It samples batch indices using the <code>sampler</code> function.</p></li>
  <li><p>It fetches preprocessed images and labels from the <code>Dataset</code>.</p></li>
  <li><p>If an <code>augmentor</code> is provided, it applies augmentation per image using JAX’s <code>vmap</code> for efficiency.</p></li>
  <li><p>It yields batches as dictionaries containing <code>frame_ids</code>, <code>images</code>, and <code>labels</code>.</p></li></ol>

<p>This design cleanly separates dataset structure from augmentation and sampling logic, making it easy to experiment with different batching strategies during training.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html32" data-type="indexterm" id="id880"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html31" data-type="indexterm" id="id881"/>
   </p>
   </div></section>
   </div></section>
   
<section data-type="sect2" data-pdf-bookmark="Readying the Dataset"><div class="sect2" id="id127">
  <h2>Readying the Dataset</h2>
   
   <p><a contenteditable="false" data-primary="DatasetBuilder class" data-secondary="organizing components into structured DatasetBuilder class" data-tertiary="building and inspecting the dataset" data-type="indexterm" id="ch05_cancer.html33"/>We’re now ready to construct our dataset and sanity-check the outputs of the data pipeline:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.cancer.train.handlers.augmentors</code> <code class="kn">import</code> <code class="n">rich_augmentor</code>
<code class="kn">from</code> <code class="nn">dlfb.cancer.train.handlers.samplers</code> <code class="kn">import</code> <code class="n">balanced_sampler</code>

<code class="n">rng</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">rng</code><code class="p">,</code> <code class="n">rng_dataset</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>

<code class="n">builder</code> <code class="o">=</code> <code class="n">DatasetBuilder</code><code class="p">(</code><code class="n">data_dir</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"cancer/datasets/raw"</code><code class="p">))</code>

<code class="n">dataset_splits</code> <code class="o">=</code> <code class="n">builder</code><code class="o">.</code><code class="n">build</code><code class="p">(</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">rng_dataset</code><code class="p">,</code>
  <code class="n">splits</code><code class="o">=</code><code class="p">{</code><code class="s2">"train"</code><code class="p">:</code> <code class="mf">0.7</code><code class="p">,</code> <code class="s2">"valid"</code><code class="p">:</code> <code class="mf">0.20</code><code class="p">,</code> <code class="s2">"test"</code><code class="p">:</code> <code class="mf">0.10</code><code class="p">},</code>
  <code class="n">image_size</code><code class="o">=</code><code class="p">(</code><code class="mi">224</code><code class="p">,</code> <code class="mi">224</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code>
<code class="p">)</code>

<code class="nb">print</code><code class="p">(</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">metadata</code><code class="p">)</code></pre>
      
<p>Output:</p>
<pre>
frame_id split_orig           class_orig            full_path     label  \
177        177      Train    actinic keratosis  /content/drive/M...      0   
408        408      Train  basal cell carci...  /content/drive/M...      1   
445        445      Train  basal cell carci...  /content/drive/M...      1   
...        ...        ...                  ...                  ...    ...   
1579      1579      Train  pigmented benign...  /content/drive/M...      5   
1966      1966      Train  seborrheic kerat...  /content/drive/M...      6   
305        305      Train  basal cell carci...  /content/drive/M...      1   

                    class  
177     actinic keratosis  
408   basal cell carci...  
445   basal cell carci...  
...                   ...  
1579  pigmented benign...  
1966  seborrheic kerat...  
305   basal cell carci...  

[1649 rows x 6 columns]</pre>
      
     
    
   
   <p>This call initializes the full data pipeline: it loads metadata; processes images; splits the dataset into training, validation, and test partitions; and returns them as <code>Dataset</code> objects (as keys in the <code>dataset_splits</code> dict).</p>
<p>We can now use the <code>BatchHandler</code> to generate a batch from the training set:</p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">rng</code><code class="p">,</code> <code class="n">rng_train</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>

<code class="n">train_batcher</code> <code class="o">=</code> <code class="n">BatchHandler</code><code class="p">(</code><code class="n">sampler</code><code class="o">=</code><code class="n">balanced_sampler</code><code class="p">,</code> <code class="n">augmentor</code><code class="o">=</code><code class="n">rich_augmentor</code><code class="p">)</code>
<code class="n">train_batches</code> <code class="o">=</code> <code class="n">train_batcher</code><code class="o">.</code><code class="n">get_batches</code><code class="p">(</code>
  <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">],</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">rng</code><code class="o">=</code><code class="n">rng_train</code>
<code class="p">)</code>
<code class="n">batch</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">train_batches</code><code class="p">)</code></pre>
      
     
    
   
   <p class="pagebreak-before">Each batch contains three keys—<code>images</code>, <code>labels</code>, and <code>frame_ids</code>:
   </p>
  
  <ul>
  <li><p><code>images</code>: A float32 tensor with normalized pixel values.</p></li>
  <li><p><code>labels</code>: Integer class labels corresponding to each image.</p></li>
  <li><p><code>frame_ids</code>: Indices into the original metadata, useful for inspection and <span class="keep-together">debugging</span>.</p></li>
  </ul>
   <p>Let’s confirm the shape of the <code>images</code> tensor:</p>
  
    
     
      <p>Output:</p>
       <pre data-type="programlisting" data-code-language="python"><code class="n">batch</code><code class="p">[</code><code class="s2">"images"</code><code class="p">]</code><code class="o">.</code><code class="n">shape</code>
</pre>
      
     
    
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="p">(</code><code class="mi">32</code><code class="p">,</code> <code class="mi">224</code><code class="p">,</code> <code class="mi">224</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
</pre>
      
     
    
   
   <p>Each batch consists of <code>32</code> images, and we can see that each image has dimensions <code>(224, 224, 3)</code>.
   </p>
   <p>Before moving forward, let’s visualize the first image in the batch to ensure our preprocessing pipeline has worked as intended. This simple sanity check helps catch any unexpected artifacts early on (see <a data-type="xref" href="#skin-lesion-sanity-check-image">Figure 5-11</a>):
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">batch</code><code class="p">[</code><code class="s2">"images"</code><code class="p">][</code><code class="mi">0</code><code class="p">]);</code>
</pre>
      
     
    
    
     <figure><div id="skin-lesion-sanity-check-image" class="figure">
      <img alt="" src="assets/dlfb_0511.png" width="600" height="584"/>
      <h6><span class="label">Figure 5-11. </span>A processed skin lesion image from our dataset, confirming that our pipeline is working correctly.</h6>
     </div></figure>
    
   
   <p class="pagebreak-before">Yup, it still looks like some sort of skin lesion<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html33" data-type="indexterm" id="id882"/>!<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html27" data-type="indexterm" id="id883"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html26" data-type="indexterm" id="id884"/> With a fully functional dataset pipeline prepared and validated, we’re ready for the next step: training our model.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html15" data-type="indexterm" id="id885"/></p>
  </div></section>
  </div></section>
 <section data-type="sect1" data-pdf-bookmark="Building Skin Cancer Classification Models"><div class="sect1" id="training-skin-cancer-classification-models">
  <h1>Building Skin Cancer Classification Models</h1>
  
 <p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="skin cancer classification models" data-type="indexterm" id="ch05_cancer.html34"/>With our dataset ready, we now turn to the central modeling exploration of this chapter. We’ll compare a range of architectures, gradually increasing in complexity. Here’s an overview of the models we’ll build and evaluate:</p>
 
 <dl>
 <dt><code>SimpleCNN</code></dt><dd><p>A lightweight two-layer convolutional neural network that serves as our baseline. It shares the same classification head (<code>SkinLesionClassifierHead</code>) as the other models but is otherwise kept minimal. We won’t tune it heavily—its purpose is to establish a performance baseline without any architectural sophistication.</p></dd>
 <dt><code>ResNetFromScratch</code></dt><dd><p>A full ResNet50 architecture trained from randomly initialized weights. This model assesses the impact of the architecture alone, without any benefits from pretraining on ImageNet. It helps isolate the contribution of the ResNet design itself.</p></dd>
 <dt><code>FinetunedResNet</code></dt><dd><p>A ResNet50 initialized with pretrained weights, with all layers fine-tuned on our skin lesion dataset. This approach leverages transfer learning, incorporating knowledge learned from large-scale datasets like ImageNet to improve convergence speed and generalization.</p></dd>
 <dt><code>FinetunedHeadResNet</code></dt><dd><p>Similar to the previous item, but with the pretrained ResNet50 backbone frozen. Only the final classification head is trained. This variant tests how far we can get by leveraging pretrained features without modifying the backbone.</p></dd>
 </dl>
 
<p class="pagebreak-after">This progression—from scratch to full fine-tuning—lets us directly compare training efficiency and classification performance across strategies. Training from scratch gives full control but demands more data and compute. Transfer learning, especially when freezing the backbone, is typically faster and more robust in low-data regimes like ours.</p>
 
 
  <div data-type="note" epub:type="note" class="less_space"><h6>Note</h6>
   <p>ResNet50 is a widely used model convolutional architecture with 50 layers. You can easily experiment with deeper variants such as <a href="https://oreil.ly/P8Cvr">ResNet101</a> or ResNet200. These offer greater representational power but come at the cost of increased compute and a higher risk of overfitting—especially when training data is limited. All variants are compatible with the same ImageNet preprocessor.</p>
  </div>
  
  <p>To implement these models, we’ll follow these steps:</p>
  
  <ol>
  <li><p>Load the ResNet50 model for Flax from transformers.</p></li>
  <li><p>Extract the model backbone.</p></li>
  <li><p>Attach a custom classification head (<code>SkinLesionClassifierHead</code>).</p></li>
  <li><p>Train and evaluate each model variant.</p></li>
  </ol>
  
<p>Let’s start by loading the pretrained ResNet50 model.</p>

  <section data-type="sect2" data-pdf-bookmark="Loading the Flax ResNet50 Model"><div class="sect2" id="loading-the-flax-resnet50-model">
   <h2>Loading the Flax ResNet50 Model</h2>
   <p><a contenteditable="false" data-primary="FlaxResNetModel" data-type="indexterm" id="ch05_cancer.html35"/><a contenteditable="false" data-primary="ResNet50 model" data-type="indexterm" id="ch05_cancer.html36"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="skin cancer classification models" data-tertiary="loading the Flax ResNet50 model" data-type="indexterm" id="ch05_cancer.html37"/>Flax offers a <a href="https://oreil.ly/Yaf_p">prebuilt ResNet implementation</a>, which provides a flexible and well-optimized architecture for feature extraction. However, prebuilt models aren’t always available—or pretrained—depending on your needs. To demonstrate how to work with pretrained models in Flax, we’ll load a ResNet50 model from Hugging Face instead of using the Flax example directly.</p>
     
   <p>The Hugging Face <a href="https://oreil.ly/xWVhz"><code>FlaxResNetModel</code></a> provides a convenient way to import and use pretrained ResNet models, making it easy to fine-tune or extract features from standard models.</p>
<p>To see how this works in practice, we’ll follow Hugging Face’s example code and apply a pretrained ResNet50 to a sample image. We’ve been looking at skin lesions for quite a while, so let’s take a break with something more cheerful: a photo of cats lounging on a couch (see <a data-type="xref" href="#cats-on-couch">Figure 5-12</a>):
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">requests</code>

<code class="c1"># Load the example image from the documentation.</code>
<code class="n">url</code> <code class="o">=</code> <code class="s2">"http://images.cocodataset.org/val2017/000000039769.jpg"</code>
<code class="n">image</code> <code class="o">=</code> <code class="n">Image</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="n">url</code><code class="p">,</code> <code class="n">stream</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code><code class="o">.</code><code class="n">raw</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">image</code><code class="p">);</code></pre>
      
     
    
    
     <figure><div id="cats-on-couch" class="figure">
      <img alt="" src="assets/dlfb_0512.png" width="600" height="448"/>
      <h6><span class="label">Figure 5-12. </span>Cats on a couch.
      </h6>
     </div></figure>
    
   
   <p>Loading a pretrained ResNet model from Hugging Face is straightforward. We’ll use a ResNet50 model trained on ImageNet and pair it with an image preprocessor to ensure that our input image is correctly formatted:
   </p>
   
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">AutoImageProcessor</code><code class="p">,</code> <code class="n">FlaxResNetForImageClassification</code>
<code class="n">resnet_model</code> <code class="o">=</code> <code class="n">FlaxResNetForImageClassification</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code>
  <code class="s2">"microsoft/resnet-50"</code>
<code class="p">)</code>
<code class="n">image_processor</code> <code class="o">=</code> <code class="n">AutoImageProcessor</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"microsoft/resnet-50"</code><code class="p">)</code></pre>
      
     
    
   
   <p>Now let’s preprocess the cat image to obtain the <code>pixel_values</code> input format expected by the model:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">inputs</code> <code class="o">=</code> <code class="n">image_processor</code><code class="p">(</code><code class="n">images</code><code class="o">=</code><code class="n">image</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"jax"</code><code class="p">)</code>
<code class="n">inputs</code><code class="o">.</code><code class="n">keys</code><code class="p">()</code>
</pre>
      
     
    
    
     
 <p>Output:</p>     
       <pre data-type="programlisting">dict_keys(['pixel_values'])
</pre>
      
     
    
   
   <p>Let’s inspect what the preprocessor does to input images in <a data-type="xref" href="#cats-on-couch-standardized">Figure 5-13</a>:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">jnp</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="n">inputs</code><code class="p">[</code><code class="s2">"pixel_values"</code><code class="p">],</code> <code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">1</code><code class="p">))[</code><code class="mi">0</code><code class="p">]);</code></pre>
      
     
    
    
     <figure><div id="cats-on-couch-standardized" class="figure">
      <img alt="" src="assets/dlfb_0513.png" width="600" height="584"/>
      <h6><span class="label">Figure 5-13. </span>Cats on a couch after image processor standardization.
      </h6>
     </div></figure>
    
   
   <p>The processed image looks noticeably different from the original; the colors have shifted, and the image has been cropped to 224 × 224 pixels. This transformation is essential when using pretrained weights: models expect inputs normalized to the mean and standard deviations of the dataset they were trained on. This standardization ensures consistent performance, even if your input data (like medical images) has different color distributions.</p>
    
    <p>Here are two important guidelines: </p>
    
 <ul>
 <li><p>When fine-tuning a pretrained model, normalize inputs using the original training dataset’s statistics (e.g., ImageNet mean and std).</p></li>
<li><p>When training from scratch, you can normalize using your own dataset’s range or statistics (mean and standard deviations).</p></li>
 </ul>
  
  
   <p>Now let’s make a prediction:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">outputs</code> <code class="o">=</code> <code class="n">resnet_model</code><code class="p">(</code><code class="o">**</code><code class="n">inputs</code><code class="p">)</code>
<code class="n">logits</code> <code class="o">=</code> <code class="n">outputs</code><code class="o">.</code><code class="n">logits</code>
<code class="n">predicted_class_idx</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">logits</code><code class="p">,</code> <code class="n">axis</code><code class="o">=-</code><code class="mi">1</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code>
  <code class="s2">"Predicted class:"</code><code class="p">,</code> <code class="n">resnet_model</code><code class="o">.</code><code class="n">config</code><code class="o">.</code><code class="n">id2label</code><code class="p">[</code><code class="n">predicted_class_idx</code><code class="o">.</code><code class="n">item</code><code class="p">()]</code>
<code class="p">)</code></pre>
      
     
    
    
     
  <p>Output:</p>    
       <pre data-type="programlisting">Predicted class: tiger cat
</pre>
      
     
    
   
   <p>Success! Well, sort of—the model classifies the image as the class “tiger cat.” While not a perfect match, it’s a reasonable guess given the image. This highlights both the strengths and limitations of pretrained models: they excel at recognizing familiar patterns but may struggle with patterns outside of original training data distribution.
   </p>
   
   <p>Let’s now explore how we can reuse the core architecture of this model while adapting it to a new task: classifying skin lesions.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html37" data-type="indexterm" id="id886"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html36" data-type="indexterm" id="id887"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html35" data-type="indexterm" id="id888"/></p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Extracting the ResNet Backbone"><div class="sect2" id="extracting-the-resnet-backbone">
   <h2>Extracting the ResNet Backbone</h2>
   <p><a contenteditable="false" data-primary="backbone (ResNet module)" data-type="indexterm" id="ch05_cancer.html38"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="skin cancer classification models" data-tertiary="extracting the ResNet backbone" data-type="indexterm" id="ch05_cancer.html39"/>As mentioned, the pretrained ResNet50 model we’re using has been trained on ImageNet to classify images into one of 1,000 classes such as “tiger cat,” “airplane,” and “fire truck.” You might wonder: is melanoma one of the classes? Unfortunately, it is not. So how can we still use this model?
   </p>
   <p>The key lies in the layered architecture of deep learning models. The pretrained ResNet50 consists of:
   </p>
   <ul class="simple">
    <li>
     <p>
      A <em>backbone</em> (ResNet module) that extracts general features from images
     </p>
    </li>
    <li>
     <p>
      A <em>classifier head</em> that maps these features to one of ImageNet’s 1,000 categories
     </p>
    </li>
   </ul>
   <p>Since we don’t need the ImageNet classification head, we’ll replace it with our own custom classifier for melanoma detection.
   </p>
   <div data-type="tip"><h6>Tip</h6>
    <p>While Hugging Face offers <code>FlaxResNetModel</code>—a ResNet variant without the classification head—for exactly this use case, such headless versions aren’t available for every model. Learning how to manually extract and repurpose parts of a pretrained model is an essential skill. It gives you full control over what components are used and prepares you for working with a wider range of architectures where clean abstractions may not exist. That’s why we demonstrate the manual approach here.</p>
   </div>
   <p>To separate the backbone from the classifier head:
   </p>
   
   <pre data-type="programlisting" data-code-language="python"><code class="n">module</code><code class="p">,</code> <code class="n">variables</code> <code class="o">=</code> <code class="n">resnet_model</code><code class="o">.</code><code class="n">module</code><code class="p">,</code> <code class="n">resnet_model</code><code class="o">.</code><code class="n">params</code>
<code class="n">backbone_module</code><code class="p">,</code> <code class="n">backbone_vars</code> <code class="o">=</code> <code class="n">module</code><code class="o">.</code><code class="n">bind</code><code class="p">(</code><code class="n">variables</code><code class="p">)</code><code class="o">.</code><code class="n">resnet</code><code class="o">.</code><code class="n">unbind</code><code class="p">()</code></pre>

<p>Here’s what each step does:</p>

<ul>
   <li><p>We extract the components of the <code>FlaxResNetForImageClassification</code> model provided by Hugging Face:</p>
   <ul>
 <li><p><code>module</code>: The full architecture definition</p></li>
 <li><p><code>variables</code>: The pretrained weights for all parts of the model</p></li></ul></li>
 <li><p>To isolate just the ResNet backbone:</p>
 <ul><li><p>We <code>bind</code> the weights to the module.</p></li>
 <li><p>We access the <code>resnet</code> submodule, which excludes the classification head.</p></li>
 <li><p>We then <code>unbind</code> it, separating the backbone into its own callable Flax module with its own parameters.</p></li></ul></li>
 </ul>
 
<p>This technique gives you direct access to intermediate model components—even when they aren’t explicitly exposed. If you’re unsure what submodules are available, you can inspect the bound module by listing its attributes or checking the Hugging Face docs.</p>
  
   
   <p>Now, we can use the <code>backbone_module</code> just like any other Flax model by calling its <code>apply</code> method. For input, we pass the <code>backbone_vars</code> and a preprocessed image.</p>
  
   <div data-type="warning" epub:type="warning"><h6>Warning</h6>
    <p>Unlike many Hugging Face transformer models, Flax models typically expect input images in NHWC format (batch, height, width, channels). If the image is in NCHW format (batch, channels, height, width), use <code>transpose</code> or <code>moveaxis</code> to first reorder the dimensions.</p>
   </div>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">outputs</code> <code class="o">=</code> <code class="n">backbone_module</code><code class="o">.</code><code class="n">apply</code><code class="p">(</code>
  <code class="n">backbone_vars</code><code class="p">,</code> <code class="n">jnp</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="n">inputs</code><code class="p">[</code><code class="s2">"pixel_values"</code><code class="p">],</code> <code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">1</code><code class="p">))</code>
<code class="p">)</code>
<code class="n">last_hidden_state</code> <code class="o">=</code> <code class="n">outputs</code><code class="o">.</code><code class="n">last_hidden_state</code>
<code class="n">last_hidden_state</code><code class="o">.</code><code class="n">shape</code></pre>
      
     
    
    
     
   <p>Output:</p>   
       <pre data-type="programlisting">(1, 2048, 7, 7)
</pre>
      
     
    
   
   <p>The output shape <code>(1, 2048, 7, 7)</code> represents:
   </p>
   <ul>
    <li><p><code>2048</code>: The number of feature channels output by the final ResNet block.</p>
    </li>
    <li>
     <p><code>7, 7</code>: The spatial dimensions after all convolutions and pooling layers have been applied. In other words, these operations have downsampled the image from 224 × 224 to 7 x 7 height and width.
     </p>
    </li>
  </ul>
   <p>At this stage, the feature maps are highly abstract, far removed from the original cat image. However, they still capture meaningful patterns learned by the model. These 2,048 feature maps would then be fed into the fully connected layers for the purpose of actually classifying the image. If we visualize them, we get something like <a data-type="xref" href="#abstract-cat-features">Figure 5-14</a>:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="k">def</code> <code class="nf">display_feature_maps</code><code class="p">(</code><code class="n">feature_map</code><code class="p">,</code> <code class="n">ncols</code><code class="o">=</code><code class="mi">8</code><code class="p">):</code>
  <code class="sd">"""Plot grid of the first 64 feature maps."""</code>
  <code class="n">num_features</code> <code class="o">=</code> <code class="n">feature_map</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
  <code class="n">nrows</code> <code class="o">=</code> <code class="p">(</code><code class="n">num_features</code> <code class="o">+</code> <code class="n">ncols</code> <code class="o">-</code> <code class="mi">1</code><code class="p">)</code> <code class="o">//</code> <code class="n">ncols</code>
  <code class="n">_</code><code class="p">,</code> <code class="n">axes</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="n">nrows</code><code class="p">,</code> <code class="n">ncols</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="n">ncols</code><code class="p">,</code> <code class="n">nrows</code><code class="p">))</code>
  <code class="n">axes</code> <code class="o">=</code> <code class="n">axes</code><code class="o">.</code><code class="n">flatten</code><code class="p">()</code>

  <code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="p">(</code><code class="n">ax</code><code class="p">,</code> <code class="n">feature</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="nb">zip</code><code class="p">(</code><code class="n">axes</code><code class="p">,</code> <code class="n">feature_map</code><code class="p">)):</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">imshow</code><code class="p">(</code><code class="n">feature</code><code class="p">,</code> <code class="n">cmap</code><code class="o">=</code><code class="s2">"viridis"</code><code class="p">)</code>
    <code class="n">ax</code><code class="o">.</code><code class="n">axis</code><code class="p">(</code><code class="s2">"off"</code><code class="p">)</code>

  <code class="n">plt</code><code class="o">.</code><code class="n">tight_layout</code><code class="p">()</code>
  <code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code>


<code class="n">display_feature_maps</code><code class="p">(</code><code class="n">last_hidden_state</code><code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">:</code><code class="mi">64</code><code class="p">,</code> <code class="o">...</code><code class="p">])</code></pre>
      
     
    
    
     <figure><div id="abstract-cat-features" class="figure">
      <img alt="" src="assets/dlfb_0514.png" width="600" height="600"/>
      <h6><span class="label">Figure 5-14. </span>Feature maps from the final convolutional layer of ResNet50, illustrating how the model “sees” the cat image at that stage. This visualization shows activations from only the first 64 of 2048 learned filters. Each square represents the response of a different convolutional filter, with bright regions indicating areas of high activation. Many filters remain largely inactive, while others detect specific patterns or textures.
      </h6>
     </div></figure>
    
   
   <p>Some feature maps remain largely inactive (like those in the first row), while others highlight specific aspects of the cat image.  At this late stage in the neural network, the representation of the cat image is highly abstract and no longer interpretable in a human-recognizable way. However, these learned patterns are still essential for the model’s classification process.
   </p>
   <div data-type="tip"><h6>Tip</h6>
    <p>In this example, we simply examined the activations of late-stage convolutional filters in response to a cat image to demonstrate how to extract feature maps from a pretrained model. While useful as a sanity check, this doesn’t reveal what each filter has actually learned.</p>
     
<p><a contenteditable="false" data-primary="feature visualization" data-type="indexterm" id="id889"/>To dig deeper, try <em>feature visualization</em>: a technique where you iteratively modify an input image to maximize the activation of a specific filter. This reveals the kinds of patterns—like textures, edges, or object parts—that the filter responds to.</p>

<p>For an excellent deep dive, check out the <a href="https://oreil.ly/XpdSL">classic Distill blog post from 2017</a> on visualizing convolutional filters.
    </p>
   </div>
   <p>We can also access the model’s final <code>pooler_output</code>, which represents the last hidden state after global average pooling. This operation collapses the spatial dimensions, leaving us with a single 2,048-dimensional feature vector—the final output of the model’s feature extraction process. This vector serves as the condensed representation of the image, as shown in <a data-type="xref" href="#last-hidden-state-of-resnet50">Figure 5-15</a>:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">_</code><code class="p">,</code> <code class="n">ax</code> <code class="o">=</code> <code class="n">plt</code><code class="o">.</code><code class="n">subplots</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="n">figsize</code><code class="o">=</code><code class="p">(</code><code class="mi">6</code><code class="p">,</code> <code class="mi">2</code><code class="p">))</code>
<code class="n">ax</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">squeeze</code><code class="p">(</code><code class="n">outputs</code><code class="o">.</code><code class="n">pooler_output</code><code class="p">),</code> <code class="n">c</code><code class="o">=</code><code class="s2">"grey"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Shape of outputs after pooling: </code><code class="si">{</code><code class="n">outputs</code><code class="o">.</code><code class="n">pooler_output</code><code class="o">.</code><code class="n">shape</code><code class="si">}</code><code class="s2">"</code><code class="p">);</code></pre>
      
     
    
    
     <figure><div id="last-hidden-state-of-resnet50" class="figure">
      <img alt="" src="assets/dlfb_0515.png" width="600" height="243"/>
      <h6><span class="label">Figure 5-15. </span>Visualization of the final pooled output of ResNet50 on the cat image input, also known as the last hidden state after global average pooling. This 2048-dimensional feature vector serves as the input to the classifier head.
      </h6>
     </div></figure>
    
   
   <p>All right, we now have a standalone ResNet module extracted from the full classifier. We’ve also inspected its final output—a 2,048-dimensional feature vector produced by global average pooling. This vector captures the distilled representation of the input image and will serve as the input to our custom classifier head.</p>
   <p><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html39" data-type="indexterm" id="id890"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html38" data-type="indexterm" id="id891"/>With the backbone in place, we’re now ready to construct the <code>SkinLesionClassifierHead</code>.
   </p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Building the SkinLesionClassifierHead"><div class="sect2" id="building-the-skinlesionclassifier">
   <h2>Building the SkinLesionClassifierHead</h2>
   <p>
    <a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="skin cancer classification models" data-tertiary="building the SkinLesionClassifierHead" data-type="indexterm" id="ch05_cancer.html40"/><a contenteditable="false" data-primary="SkinLesionClassifierHead" data-type="indexterm" id="ch05_cancer.html41"/>Now that we have extracted the pretrained ResNet50 backbone, we can add a new classification head tailored for skin lesion detection. This head will take the feature vector output from <a data-type="xref" href="#last-hidden-state-of-resnet50">Figure 5-15</a> and produce logits for our target classes.
   </p>
   <p>Although Hugging Face follows a naming convention like <code>FlaxResNetForSkinLesionClassification</code>, we will slightly simplify and call our head <code>SkinLesionClassifierHead</code>. This will perform the final classification by returning the logits for each skin class we want to predict. This is a small feedforward neural network consisting of sequential <code>Dense</code> layers with ReLU activations. Let’s take a look at its implementation:</p>

       <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">SkinLesionClassifierHead</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Skin lesion classification MLP head."""</code>
  <code class="n">num_classes</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">dropout_rate</code><code class="p">:</code> <code class="nb">float</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">is_training</code><code class="p">:</code> <code class="nb">bool</code><code class="p">):</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">256</code><code class="p">,</code> <code class="n">kernel_init</code><code class="o">=</code><code class="n">nn</code><code class="o">.</code><code class="n">initializers</code><code class="o">.</code><code class="n">xavier_uniform</code><code class="p">())(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">rate</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code><code class="p">)(</code><code class="n">x</code><code class="p">,</code> <code class="n">deterministic</code><code class="o">=</code><code class="ow">not</code> <code class="n">is_training</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="mi">128</code><code class="p">,</code> <code class="n">kernel_init</code><code class="o">=</code><code class="n">nn</code><code class="o">.</code><code class="n">initializers</code><code class="o">.</code><code class="n">xavier_uniform</code><code class="p">())(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">rate</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code><code class="p">)(</code><code class="n">x</code><code class="p">,</code> <code class="n">deterministic</code><code class="o">=</code><code class="ow">not</code> <code class="n">is_training</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code>
      <code class="bp">self</code><code class="o">.</code><code class="n">num_classes</code><code class="p">,</code> <code class="n">kernel_init</code><code class="o">=</code><code class="n">nn</code><code class="o">.</code><code class="n">initializers</code><code class="o">.</code><code class="n">xavier_uniform</code><code class="p">()</code>
    <code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">x</code></pre>
      
     
    
   
   <p>The final layer has as many neurons as the number of target classes, defined by the <code>num_classes</code>
    argument.
   </p>
   <div data-type="note" epub:type="note"><h6>Note</h6>
    <p>In this chapter, we focus on multiclass classification, but the same code generalizes to binary classification. While a single <code>nn.Dense</code> neuron with sigmoid activation is perhaps more typical for binary tasks, we use a two-neuron output instead—one per class (melanoma or nonmelanoma)—producing logits that sum to 1 after softmax. This approach is slightly less efficient but more flexible, as it naturally extends to multiclass problems.</p>
   </div>
   
   <p>You will also notice that we specify <code>nn.initializers.xavier_uniform()</code> for our weight initialization. While the most naïve approach is to initialize all weights to zero, this leads to a major problem: all neurons in a layer would receive the same gradients during backpropagation and update identically, resulting in redundant feature learning. This is known as a <em>failure to break symmetry</em>, and it can severely limit the network’s ability to learn.</p>
<p>Instead, we use <em>Xavier initialization</em> (also known as <em>Glorot initialization</em>), which assigns small, random values to each weight. This helps ensure that neurons start with diverse parameters and can learn different features. Xavier initialization is designed to maintain stable variance in both activations and gradients across layers, improving training dynamics.</p>
   
   
<div data-type="tip"><h6>Tip</h6>

      <p>Most modern deep learning libraries—including Flax—automatically apply good initializers for standard layers. We specify <code>xavier_uniform()</code> here for clarity, but in practice, you can often rely on the library’s defaults.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html41" data-type="indexterm" id="id892"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html40" data-type="indexterm" id="id893"/></p>
</div>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Building Our Models"><div class="sect2" id="id279">
  <h2>Building Our Models</h2>
    <p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="skin cancer classification models" data-tertiary="building the models" data-type="indexterm" id="ch05_cancer.html42"/>We’re now ready to begin the core modeling section of this chapter. We’ll compare several architectures, gradually increasing in complexity. For the more promising ones, we’ll also explore enhancements to improve performance. Let’s begin with our baseline.</p>
    
    
<section data-type="sect3" data-pdf-bookmark="SimpleCnn model as a baseline"><div class="sect3" id="id131">
  <h3>SimpleCnn model as a baseline</h3>
<p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="skin cancer classification models" data-tertiary="simple CNN model as a baseline" data-type="indexterm" id="ch05_cancer.html43"/>Our first model is a straightforward two-layer convolutional neural network trained from scratch. It doesn’t use any pretrained features, and we won’t spend time tuning it. The goal here is simple: to establish a reference point. How well can a minimal architecture perform on this task without any bells or whistles?</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">SimpleCnn</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Simple CNN model with small convolutional backbone and classifier head."""</code>

  <code class="n">num_classes</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">dropout_rate</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.0</code>

  <code class="k">def</code> <code class="nf">setup</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
    <code class="sd">"""Initializes the CNN backbone and classification head."""</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">backbone</code> <code class="o">=</code> <code class="n">CnnBackbone</code><code class="p">()</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">head</code> <code class="o">=</code> <code class="n">SkinLesionClassifierHead</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">num_classes</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code><code class="p">)</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">is_training</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">False</code><code class="p">):</code>
    <code class="sd">"""Applies the backbone and classifier head to the input."""</code>
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">backbone</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">is_training</code><code class="o">=</code><code class="n">is_training</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">x</code>

  <code class="k">def</code> <code class="nf">create_train_state</code><code class="p">(</code>
    <code class="bp">self</code><code class="p">,</code> <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">dummy_input</code><code class="p">,</code> <code class="n">tx</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="n">TrainStateWithBatchNorm</code><code class="p">:</code>
    <code class="sd">"""Creates the training state with initialized parameters."""</code>
    <code class="n">rng</code><code class="p">,</code> <code class="n">rng_init</code><code class="p">,</code> <code class="n">rng_dropout</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
    <code class="n">variables</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">init</code><code class="p">(</code><code class="n">rng_init</code><code class="p">,</code> <code class="n">dummy_input</code><code class="p">)</code>
    <code class="n">state</code> <code class="o">=</code> <code class="n">TrainStateWithBatchNorm</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
      <code class="n">apply_fn</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">apply</code><code class="p">,</code>
      <code class="n">tx</code><code class="o">=</code><code class="n">tx</code><code class="p">,</code>
      <code class="n">params</code><code class="o">=</code><code class="n">variables</code><code class="p">[</code><code class="s2">"params"</code><code class="p">],</code>
      <code class="n">batch_stats</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>
      <code class="n">key</code><code class="o">=</code><code class="n">rng_dropout</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">state</code></pre>

<p>As you can see, it uses our shared classification head, <code>SkinLesionClassifierHead</code>, and a custom lightweight backbone:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">CnnBackbone</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""Compact convolutional feature extractor for image data."""</code>

  <code class="nd">@nn</code><code class="o">.</code><code class="n">compact</code>
  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">):</code>
    <code class="sd">"""Applies two conv-pool blocks and a dense layer to the input."""</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code><code class="n">features</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">))(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">avg_pool</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">window_shape</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">),</code> <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">))</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Conv</code><code class="p">(</code><code class="n">features</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code> <code class="n">kernel_size</code><code class="o">=</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">))(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">avg_pool</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">window_shape</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">),</code> <code class="n">strides</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">2</code><code class="p">))</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">x</code><code class="o">.</code><code class="n">reshape</code><code class="p">((</code><code class="n">x</code><code class="o">.</code><code class="n">shape</code><code class="p">[</code><code class="mi">0</code><code class="p">],</code> <code class="o">-</code><code class="mi">1</code><code class="p">))</code>  <code class="c1"># flatten</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dense</code><code class="p">(</code><code class="n">features</code><code class="o">=</code><code class="mi">256</code><code class="p">)(</code><code class="n">x</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">x</code></pre>

<p>This backbone is intentionally simple—no residual connections, no batch normalization. It’s just a stack of convolutional layers followed by flattening. It won’t win any benchmarks, but it should give us a useful lower bound on performance for this task.</p>

<p>Even though this model doesn’t include batch normalization, we’ll still use a training state that supports it—<code>TrainStateWithBatchNorm</code>—to keep our training loop compatible across models:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">TrainStateWithBatchNorm</code><code class="p">(</code><code class="n">train_state</code><code class="o">.</code><code class="n">TrainState</code><code class="p">):</code>
  <code class="sd">"""Train state that tracks batch statistics and a PRNG key."""</code>

  <code class="n">batch_stats</code><code class="p">:</code> <code class="nb">dict</code> <code class="o">|</code> <code class="kc">None</code>
  <code class="n">key</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code></pre>

<p>Using a unified training state lets us reuse the same training loop for both this simple CNN and the more advanced ResNet models we’ll explore next.</p>

<p>Now let’s move on to training our ResNet variants to better understand the impact of architecture and transfer learning on skin lesion classification.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html43" data-type="indexterm" id="id894"/> <a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="skin cancer classification models" data-tertiary="defining three ResNet models" data-type="indexterm" id="ch05_cancer.html44"/>With the ResNet50 backbone we extracted earlier and our custom skin lesion classification head already in place, we now have all the components we need to assemble and train more advanced models.</p>

     <p>We’ll build three models, starting with the simplest: ResNet50 from scratch.</p>
</div></section>
  

<section data-type="sect3" data-pdf-bookmark="Building the ResNetFromScratch model"><div class="sect3" id="building-the-resnetfromscratch-model">
    <h3>Building the ResNetFromScratch model</h3>
   
     <p><a contenteditable="false" data-primary="ResNetFromScratch model" data-type="indexterm" id="ch05_cancer.html45"/>Our first ResNet variant is <code>ResNetFromScratch</code>, defined next. It uses the full ResNet50 architecture but does not load any pretrained weights—all parameters are initialized randomly and learned from scratch. While this model still benefits from the structure of a relatively deep, well-tested architecture, it doesn’t directly inherit any knowledge from ImageNet or other large-scale datasets.</p>
     
<p>This gives us a useful comparison point: how far can we get with a good architecture alone, without any pretrained weights?
Here’s the code for the model:</p>
 
        <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">ResNetFromScratch</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
  <code class="sd">"""ResNet model initialized from scratch with a custom classification head."""</code>

  <code class="n">num_classes</code><code class="p">:</code> <code class="nb">int</code>
  <code class="n">layers</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">50</code>
  <code class="n">dropout_rate</code><code class="p">:</code> <code class="nb">float</code> <code class="o">=</code> <code class="mf">0.0</code>

  <code class="k">def</code> <code class="nf">setup</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
    <code class="sd">"""Initializes the backbone and classification head."""</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">backbone</code> <code class="o">=</code> <code class="n">PRETRAINED_RESNETS</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">layers</code><code class="p">]</code><code class="o">.</code><code class="n">module</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">head</code> <code class="o">=</code> <code class="n">SkinLesionClassifierHead</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">num_classes</code><code class="p">,</code> <code class="bp">self</code><code class="o">.</code><code class="n">dropout_rate</code><code class="p">)</code>

  <code class="k">def</code> <code class="fm">__call__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">is_training</code><code class="p">:</code> <code class="nb">bool</code> <code class="o">=</code> <code class="kc">False</code><code class="p">):</code>
    <code class="sd">"""Runs a forward pass through the model."""</code>
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">backbone</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">deterministic</code><code class="o">=</code><code class="ow">not</code> <code class="n">is_training</code><code class="p">)</code><code class="o">.</code><code class="n">pooler_output</code>
    <code class="n">x</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">squeeze</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">))</code>
    <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">head</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">is_training</code><code class="o">=</code><code class="n">is_training</code><code class="p">)</code>
    <code class="k">return</code> <code class="n">x</code>

  <code class="k">def</code> <code class="nf">create_train_state</code><code class="p">(</code>
    <code class="bp">self</code><code class="p">,</code> <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">dummy_input</code><code class="p">,</code> <code class="n">tx</code>
  <code class="p">)</code> <code class="o">-&gt;</code> <code class="n">TrainStateWithBatchNorm</code><code class="p">:</code>
    <code class="sd">"""Initializes model parameters and optimizer state."""</code>
    <code class="n">rng</code><code class="p">,</code> <code class="n">rng_init</code><code class="p">,</code> <code class="n">rng_dropout</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
    <code class="n">variables</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">init</code><code class="p">(</code><code class="n">rng_init</code><code class="p">,</code> <code class="n">dummy_input</code><code class="p">,</code> <code class="n">is_training</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code>
    <code class="n">variables</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">transfer_parameters</code><code class="p">(</code><code class="n">variables</code><code class="p">)</code>
    <code class="n">tx</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">set_trainable_parameters</code><code class="p">(</code><code class="n">tx</code><code class="p">,</code> <code class="n">variables</code><code class="p">)</code>
    <code class="n">state</code> <code class="o">=</code> <code class="n">TrainStateWithBatchNorm</code><code class="o">.</code><code class="n">create</code><code class="p">(</code>
      <code class="n">apply_fn</code><code class="o">=</code><code class="bp">self</code><code class="o">.</code><code class="n">apply</code><code class="p">,</code>
      <code class="n">tx</code><code class="o">=</code><code class="n">tx</code><code class="p">,</code>
      <code class="n">params</code><code class="o">=</code><code class="n">variables</code><code class="p">[</code><code class="s2">"params"</code><code class="p">],</code>
      <code class="n">batch_stats</code><code class="o">=</code><code class="n">variables</code><code class="p">[</code><code class="s2">"batch_stats"</code><code class="p">],</code>
      <code class="n">key</code><code class="o">=</code><code class="n">rng_dropout</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="k">return</code> <code class="n">state</code>

  <code class="k">def</code> <code class="nf">transfer_parameters</code><code class="p">(</code><code class="n">_</code><code class="p">,</code> <code class="n">variables</code><code class="p">):</code>
    <code class="sd">"""Returns variables unchanged (no transfer learning)."""</code>
    <code class="k">return</code> <code class="n">variables</code>

  <code class="nd">@staticmethod</code>
  <code class="k">def</code> <code class="nf">set_trainable_parameters</code><code class="p">(</code><code class="n">tx</code><code class="p">,</code> <code class="n">_</code><code class="p">):</code>
    <code class="sd">"""Returns optimizer configuration with all parameters trainable."""</code>
    <code class="k">return</code> <code class="n">tx</code>
</pre>
       
      
     
    
    <p>In the <code>setup</code> method, we instantiate both the <code>backbone</code> and the <code>head</code>. These are then used sequentially in the <code>__call__</code> method to compute predictions. The backbone is accessed through the Hugging Face wrapper, so we reference it using <code>.module</code> to extract the underlying Flax-compatible model.</p>

    <div data-type="note" epub:type="note"><h6>Note</h6>
<p>You will have noticed the the model takes a <code>layer</code> parameter. It picks the ResNet model with the required number of layers from the Hugging Face library of preloaded models:</p>
  <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">transformers</code> <code class="kn">import</code> <code class="n">FlaxResNetModel</code>


<code class="c1"># Dictionary of pretrained ResNet models from Hugging Face.</code>
<code class="n">PRETRAINED_RESNETS</code> <code class="o">=</code> <code class="p">{</code>
  <code class="mi">18</code><code class="p">:</code> <code class="n">FlaxResNetModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"microsoft/resnet-18"</code><code class="p">),</code>
  <code class="mi">34</code><code class="p">:</code> <code class="n">FlaxResNetModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"microsoft/resnet-34"</code><code class="p">),</code>
  <code class="mi">50</code><code class="p">:</code> <code class="n">FlaxResNetModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"microsoft/resnet-50"</code><code class="p">),</code>
<code class="p">}</code>
</pre>
<p>We have settled for a value of 50 to get the ResNet50 backbone, but the layers parameter could be conveniently explored as a hyperparameter.</p>
</div>
    
    <p>To prepare the model for training, we define a training state via the <code>create_train_state</code> method. This initializes the parameters, optionally transfers pretrained weights (not used here), and sets which parameters should remain trainable. While <code>ResNetFromScratch</code> doesn’t apply any changes to the weights, this method is designed to be overridden by subclasses—as we’ll see next with <code>FinetunedResNet</code>.</p>
    
<p>Even though <code>ResNetFromScratch</code> does not use any pretrained weights, it defines a <code>transfer_parameters</code> method (which is a no-op here) and a <code>set_trainable_parameters</code> method (also a placeholder). These provide a consistent interface across models and make it easy to plug in transfer learning logic in downstream variants.</p>

<p>Since ResNet uses batch normalization, our training state must also track batch statistics—so we use the <code>TrainStateWithBatchNorm</code> class defined earlier.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html45" data-type="indexterm" id="id895"/></p>
       </div></section>
      
    <section data-type="sect3" data-pdf-bookmark="Fully fine-tuning a pretrained model"><div class="sect3" id="fully-fine-tuning-a-pretrained-model">
     <h3>Fully fine-tuning a pretrained model</h3>
     <p><a contenteditable="false" data-primary="FinetunedResNet50 model" data-type="indexterm" id="id896"/>The second model, <code>FinetunedResNet50</code>, is a subclass of <code>ResNet50FromScratch</code> that modifies just one key detail: it loads pretrained weights from the Hugging Face ResNet50 model.</p>
<p>This is done by overriding the <code>transfer_parameters</code> method. Unlike the base class (where this method is a no-op), here it copies over both the backbone’s weights and its batch normalization statistics. The classification head remains randomly initialized, since it’s specific to our skin lesion classification task:</p>
    
     
      
       
        
         <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">FinetunedResNet</code><code class="p">(</code><code class="n">ResNetFromScratch</code><code class="p">):</code>
  <code class="sd">"""ResNet model with pretrained weights and full fine-tuning."""</code>

  <code class="k">def</code> <code class="nf">transfer_parameters</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">variables</code><code class="p">):</code>
    <code class="sd">"""Replaces model parameters with pretrained ResNet weights."""</code>
    <code class="n">resnet_variables</code> <code class="o">=</code> <code class="n">PRETRAINED_RESNETS</code><code class="p">[</code><code class="bp">self</code><code class="o">.</code><code class="n">layers</code><code class="p">]</code><code class="o">.</code><code class="n">params</code>
    <code class="n">variables</code><code class="p">[</code><code class="s2">"params"</code><code class="p">][</code><code class="s2">"backbone"</code><code class="p">]</code> <code class="o">=</code> <code class="n">resnet_variables</code><code class="p">[</code><code class="s2">"params"</code><code class="p">]</code>
    <code class="n">variables</code><code class="p">[</code><code class="s2">"batch_stats"</code><code class="p">][</code><code class="s2">"backbone"</code><code class="p">]</code> <code class="o">=</code> <code class="n">resnet_variables</code><code class="p">[</code><code class="s2">"batch_stats"</code><code class="p">]</code>
    <code class="k">return</code> <code class="n">variables</code></pre>
        
   <p>Once the parameters are loaded, we will fine-tune <code>all</code> of them—both the pretrained ResNet backbone and the newly added classification head. This setup allows the model to fully adapt to the skin cancer classification task, and should in theory lead to stronger performance when the source (ImageNet) and target (skin) domains share similar visual features, such as textures, colors, and edges, which should be the case here.</p>    
      
     
    </div></section>
    <section data-type="sect3" data-pdf-bookmark="Freezing the backbone with FinetunedHeadResNet"><div class="sect3" id="only-fine-tuning-the-model-s-classification-head">
     <h3>Freezing the backbone with FinetunedHeadResNet</h3>
     <p><a contenteditable="false" data-primary="backbone (ResNet module)" data-type="indexterm" id="id897"/><a contenteditable="false" data-primary="FinetunedHeadResNet model" data-type="indexterm" id="id898"/>The third model, <code>FinetunedHeadResNet</code>, uses a different transfer learning strategy. Instead of fine-tuning all layers, it freezes the pretrained ResNet backbone and updates only the final classification head. This approach is common when working with small datasets. It allows us to leverage powerful pretrained features while reducing the number of trainable parameters—which can help avoid overfitting and reduce training time.</p>
<p>Flax and Optax make it easy to define separate optimizer behaviors for different parts of the model. Here, we apply our usual optimizer to the classification head and use a no-op optimizer (which performs no updates) for the frozen backbone, ensuring only the final layers are trained. This is implemented by overriding the <code>set_trainable_parameters</code> method.</p>
<p>This is implemented in the following model, which subclasses <code>FinetunedResNet</code> to inherit pre-trained weights but overrides the <code>set_trainable_parameters</code> method to freeze the backbone:</p>
    
      
      
      
       
        
         <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">FinetunedHeadResNet</code><code class="p">(</code><code class="n">FinetunedResNet</code><code class="p">):</code>
  <code class="sd">"""ResNet model with a frozen backbone and trainable classification head."""</code>

  <code class="nd">@staticmethod</code>
  <code class="k">def</code> <code class="nf">set_trainable_parameters</code><code class="p">(</code><code class="n">tx</code><code class="p">,</code> <code class="n">variables</code><code class="p">):</code>
    <code class="sd">"""Freezes backbone parameters and trains only the classification head."""</code>
    <code class="k">return</code> <code class="n">optax</code><code class="o">.</code><code class="n">multi_transform</code><code class="p">(</code>
      <code class="n">transforms</code><code class="o">=</code><code class="p">{</code><code class="s2">"trainable"</code><code class="p">:</code> <code class="n">tx</code><code class="p">,</code> <code class="s2">"frozen"</code><code class="p">:</code> <code class="n">optax</code><code class="o">.</code><code class="n">set_to_zero</code><code class="p">()},</code>
      <code class="n">param_labels</code><code class="o">=</code><code class="n">traverse_util</code><code class="o">.</code><code class="n">path_aware_map</code><code class="p">(</code>
        <code class="k">lambda</code> <code class="n">path</code><code class="p">,</code> <code class="n">_</code><code class="p">:</code> <code class="s2">"frozen"</code> <code class="k">if</code> <code class="s2">"backbone"</code> <code class="ow">in</code> <code class="n">path</code> <code class="k">else</code> <code class="s2">"trainable"</code><code class="p">,</code>
        <code class="n">variables</code><code class="p">[</code><code class="s2">"params"</code><code class="p">],</code>
      <code class="p">),</code>
    <code class="p">)</code></pre>
        
       
      
     
     <p>By subclassing <code>FinetunedResNet</code>, we inherit all the logic for loading pretrained weights and constructing the model—and only need to override the part that controls which parameters are trainable. This modular design maximizes code reuse while making it easy to explore different fine-tuning strategies with minimal duplication.</p>
  </div></section>
  <section data-type="sect3" data-pdf-bookmark="More customization options"><div class="sect3" id="id135">
   <h3>More customization options</h3>
   
<p>It’s worth highlighting that fine-tuning doesn’t have to be all or nothing—you’re not limited to freezing the whole backbone or simply training all parameters. With tools like <code>optax.multi_transform</code>, we can easily assign different optimizer behaviors to different parts of the model.</p>

<p>For example, a common strategy is to freeze the earliest convolutional layers (which tend to learn general-purpose features like edges and textures) while fine-tuning later layers that capture more task-specific patterns. Some layers can even be updated using a reduced learning rate, allowing for more cautious adaptation.</p>

<p>The following is an example model, <code>PartiallyFinetunedResNet</code>, which demonstrates this idea. It applies:</p>

<ul>
 <li><p>A standard learning rate to the classification head and late-stage backbone layers</p></li>
  <li><p>A reduced learning rate to intermediate layers (i.e., <code>stages/3</code>)</p></li>
  <li><p>A complete freezing of the early backbone layers:</p>
  
 <pre data-type="programlisting" data-code-language="python"><code class="k">class</code> <code class="nc">PartiallyFinetunedResNet</code><code class="p">(</code><code class="n">FinetunedResNet</code><code class="p">):</code>
  <code class="sd">"""ResNet model with selective fine-tuning of deeper layers."""</code>

  <code class="nd">@staticmethod</code>
  <code class="k">def</code> <code class="nf">set_trainable_parameters</code><code class="p">(</code><code class="n">tx</code><code class="p">,</code> <code class="n">variables</code><code class="p">):</code>
    <code class="sd">"""Freezes early layers, fine-tunes other layers at variable LR."""</code>

    <code class="k">def</code> <code class="nf">label_fn</code><code class="p">(</code><code class="n">path</code><code class="p">,</code> <code class="n">_</code><code class="p">):</code>
      <code class="n">joined</code> <code class="o">=</code> <code class="s2">"/"</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="n">path</code><code class="p">)</code>
      <code class="k">if</code> <code class="s2">"backbone"</code> <code class="ow">in</code> <code class="n">path</code><code class="p">:</code>
        <code class="k">if</code> <code class="s2">"stages/3"</code> <code class="ow">in</code> <code class="n">joined</code><code class="p">:</code>
          <code class="k">return</code> <code class="s2">"reduced_lr"</code>
        <code class="k">return</code> <code class="s2">"frozen"</code>
      <code class="k">return</code> <code class="s2">"trainable"</code>

    <code class="k">return</code> <code class="n">optax</code><code class="o">.</code><code class="n">multi_transform</code><code class="p">(</code>
      <code class="n">transforms</code><code class="o">=</code><code class="p">{</code>
        <code class="s2">"trainable"</code><code class="p">:</code> <code class="n">tx</code><code class="p">,</code>
        <code class="s2">"reduced_lr"</code><code class="p">:</code> <code class="n">optax</code><code class="o">.</code><code class="n">adam</code><code class="p">(</code><code class="n">learning_rate</code><code class="o">=</code><code class="mf">1e-5</code><code class="p">),</code>
        <code class="s2">"frozen"</code><code class="p">:</code> <code class="n">optax</code><code class="o">.</code><code class="n">set_to_zero</code><code class="p">(),</code>
      <code class="p">},</code>
      <code class="n">param_labels</code><code class="o">=</code><code class="n">traverse_util</code><code class="o">.</code><code class="n">path_aware_map</code><code class="p">(</code><code class="n">label_fn</code><code class="p">,</code> <code class="n">variables</code><code class="p">[</code><code class="s2">"params"</code><code class="p">]),</code>
    <code class="p">)</code></pre></li>
 
</ul> 

 <p>We won’t be training or evaluating this model in the rest of the chapter, but it’s useful to be aware of this level of flexibility. Once your model is structured cleanly, these kinds of fine-tuning schemes are straightforward to implement and can make a big difference when adapting to smaller or more specialized<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html44" data-type="indexterm" id="id899"/> datasets<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html42" data-type="indexterm" id="id900"/>.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html34" data-type="indexterm" id="id901"/></p>
  </div></section>
  </div></section>
  </div></section>
  
  <section data-type="sect1" data-pdf-bookmark="Training the Models"><div class="sect1" id="training-the-models">
   <h1>Training the Models</h1>
   <p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="training the models" data-type="indexterm" id="ch05_cancer.html46"/><a contenteditable="false" data-primary="training" data-secondary="detecting skin cancer in medical images" data-type="indexterm" id="ch05_cancer.html47"/>With our <code>DatasetBuilder</code> ready to generate data and all of our model variants defined, it’s time to train them. We’ll now implement a training loop that follows a familiar structure.
   </p>
   <section data-type="sect2" data-pdf-bookmark="The Training Loop"><div class="sect2" id="setting-up-the-training-loop">
    <h2>The Training Loop</h2>
    <p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="training the models" data-tertiary="training loop" data-type="indexterm" id="ch05_cancer.html48"/><a contenteditable="false" data-primary="training loop" data-secondary="for detecting skin cancer in medical images" data-type="indexterm" id="ch05_cancer.html49"/>The training loop follows the same approach as in previous chapters:
    </p>
    <ol>
     <li>
      <p>
       Initialize the model’s training state.
      </p>
     </li>
     <li>
      <p>
       Iterate over a defined number of steps.
      </p>
     </li>
     <li>
      <p>
       At each step, fetch a batch of training data and update the model.
      </p>
     </li>
     <li><p>Periodically evaluate on a validation set to track progress.</p> 
           </li>
          </ol>
      <p>In code:</p>
     
      <pre data-type="programlisting" data-code-language="python"><code class="nd">@restorable</code>
<code class="k">def</code> <code class="nf">train</code><code class="p">(</code>
  <code class="n">state</code><code class="p">:</code> <code class="n">TrainStateWithBatchNorm</code><code class="p">,</code>
  <code class="n">rng</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">Dataset</code><code class="p">],</code>
  <code class="n">num_steps</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>
  <code class="n">batch_size</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>
  <code class="n">preprocessor</code><code class="p">:</code> <code class="n">Callable</code> <code class="o">=</code> <code class="n">crop</code><code class="p">,</code>
  <code class="n">sampler</code><code class="p">:</code> <code class="n">Callable</code> <code class="o">=</code> <code class="n">repeating_sampler</code><code class="p">,</code>
  <code class="n">augmentor</code><code class="p">:</code> <code class="n">Callable</code> <code class="o">=</code> <code class="kc">None</code><code class="p">,</code>
  <code class="n">eval_every</code><code class="p">:</code> <code class="nb">int</code> <code class="o">=</code> <code class="mi">10</code><code class="p">,</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">tuple</code><code class="p">[</code><code class="n">TrainStateWithBatchNorm</code><code class="p">,</code> <code class="nb">dict</code><code class="p">]:</code>
  <code class="sd">"""Trains a model using the provided dataset splits and logs metrics."""</code>
  <code class="c1"># Set up with metrics logger and classes numbers.</code>
  <code class="n">num_classes</code> <code class="o">=</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">num_classes</code>
  <code class="n">metrics</code> <code class="o">=</code> <code class="n">MetricsLogger</code><code class="p">()</code>

  <code class="c1"># Get train batch iterator from which to take batches.</code>
  <code class="n">rng</code><code class="p">,</code> <code class="n">rng_train</code><code class="p">,</code> <code class="n">rng_eval</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
  <code class="n">train_batcher</code> <code class="o">=</code> <code class="n">BatchHandler</code><code class="p">(</code><code class="n">preprocessor</code><code class="p">,</code> <code class="n">sampler</code><code class="p">,</code> <code class="n">augmentor</code><code class="p">)</code>
  <code class="n">train_batches</code> <code class="o">=</code> <code class="n">train_batcher</code><code class="o">.</code><code class="n">get_batches</code><code class="p">(</code>
    <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">],</code> <code class="n">batch_size</code><code class="p">,</code> <code class="n">rng_train</code>
  <code class="p">)</code>

  <code class="n">steps</code> <code class="o">=</code> <code class="n">tqdm</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">num_steps</code><code class="p">))</code>  <code class="c1"># Steps with progress bar.</code>
  <code class="k">for</code> <code class="n">step</code> <code class="ow">in</code> <code class="n">steps</code><code class="p">:</code>
    <code class="n">steps</code><code class="o">.</code><code class="n">set_description</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Step </code><code class="si">{</code><code class="n">step</code> <code class="o">+</code> <code class="mi">1</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>

    <code class="n">rng</code><code class="p">,</code> <code class="n">rng_dropout</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
    <code class="n">train_batch</code> <code class="o">=</code> <code class="nb">next</code><code class="p">(</code><code class="n">train_batches</code><code class="p">)</code>
    <code class="n">state</code><code class="p">,</code> <code class="n">batch_metrics</code> <code class="o">=</code> <code class="n">train_step</code><code class="p">(</code>
      <code class="n">state</code><code class="p">,</code> <code class="n">train_batch</code><code class="p">,</code> <code class="n">rng_dropout</code><code class="p">,</code> <code class="n">num_classes</code>
    <code class="p">)</code>
    <code class="n">metrics</code><code class="o">.</code><code class="n">log_step</code><code class="p">(</code><code class="n">split</code><code class="o">=</code><code class="s2">"train"</code><code class="p">,</code> <code class="o">**</code><code class="n">batch_metrics</code><code class="p">)</code>

    <code class="k">if</code> <code class="n">step</code> <code class="o">%</code> <code class="n">eval_every</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
      <code class="k">for</code> <code class="n">batch</code> <code class="ow">in</code> <code class="n">BatchHandler</code><code class="p">(</code><code class="n">preprocessor</code><code class="p">)</code><code class="o">.</code><code class="n">get_batches</code><code class="p">(</code>
        <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">],</code> <code class="n">batch_size</code><code class="p">,</code> <code class="n">rng_eval</code>
      <code class="p">):</code>
        <code class="n">batch_metrics</code> <code class="o">=</code> <code class="n">eval_step</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">batch</code><code class="p">,</code> <code class="n">num_classes</code><code class="p">)</code>
        <code class="n">metrics</code><code class="o">.</code><code class="n">log_step</code><code class="p">(</code><code class="n">split</code><code class="o">=</code><code class="s2">"valid"</code><code class="p">,</code> <code class="o">**</code><code class="n">batch_metrics</code><code class="p">)</code>
      <code class="n">metrics</code><code class="o">.</code><code class="n">flush</code><code class="p">(</code><code class="n">step</code><code class="o">=</code><code class="n">step</code><code class="p">)</code>

    <code class="n">steps</code><code class="o">.</code><code class="n">set_postfix_str</code><code class="p">(</code><code class="n">metrics</code><code class="o">.</code><code class="n">latest</code><code class="p">([</code><code class="s2">"loss"</code><code class="p">]))</code>

  <code class="k">return</code> <code class="n">state</code><code class="p">,</code> <code class="n">metrics</code><code class="o">.</code><code class="n">export</code><code class="p">()</code>
</pre>

    
  <p>At a high level, this function:</p>
  
  <ol>
 <li><p>Sets up a metrics logger and prepares the data loaders</p></li>
 <li><p>Iterates through the given number of training steps</p></li>
 <li><p>Calls <code>train_step</code> to apply a gradient update on each training batch</p></li>
 <li><p>Periodically runs <code>eval_step</code> on validation batches</p></li>
 <li><p>Logs and flushes metrics so we can monitor progress</p></li>
  </ol>
 
<p>This design gives us a steady stream of training and validation feedback. The validation metrics are useful not just for model selection but also for deciding when to stop training or intervene (for example, if a model is doing horrendously poorly, we can just kill the hopeless run).</p>
<p>Next, let’s take a look at what happens inside a single training step.</p>

   <section data-type="sect3" data-pdf-bookmark="The training step"><div class="sect3" id="the-training-step">
    <h3>The training step</h3>
    <p><a contenteditable="false" data-primary="training loop" data-secondary="for detecting skin cancer in medical images" data-tertiary="training step" data-type="indexterm" id="ch05_cancer.html50"/>The core of our training process is the <code>train_step</code> function. This function wraps around a helper called <code>calculate_loss</code>, which handles forward propagation, loss computation, and gradient updates. This is a common design pattern in JAX code.</p>
    
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="nd">@partial</code><code class="p">(</code><code class="n">jax</code><code class="o">.</code><code class="n">jit</code><code class="p">,</code> <code class="n">static_argnums</code><code class="o">=</code><code class="p">(</code><code class="mi">3</code><code class="p">,))</code>
<code class="k">def</code> <code class="nf">train_step</code><code class="p">(</code>
  <code class="n">state</code><code class="p">:</code> <code class="n">TrainStateWithBatchNorm</code><code class="p">,</code>
  <code class="n">batch</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="nb">list</code><code class="p">[</code><code class="n">Any</code><code class="p">]],</code>
  <code class="n">rng_dropout</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code>
  <code class="n">num_classes</code><code class="p">:</code> <code class="nb">int</code><code class="p">,</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">tuple</code><code class="p">[</code><code class="n">TrainStateWithBatchNorm</code><code class="p">,</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">]]:</code>
  <code class="sd">"""Performs a single training step and returns updated state and metrics."""</code>

  <code class="k">def</code> <code class="nf">calculate_loss</code><code class="p">(</code><code class="n">params</code><code class="p">,</code> <code class="n">images</code><code class="p">,</code> <code class="n">labels</code><code class="p">):</code>
    <code class="n">variables</code><code class="p">,</code> <code class="n">kwargs</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">params</code><code class="p">},</code> <code class="p">{</code><code class="s2">"mutable"</code><code class="p">:</code> <code class="p">[]}</code>
    <code class="k">if</code> <code class="n">state</code><code class="o">.</code><code class="n">batch_stats</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">:</code>
      <code class="n">variables</code><code class="o">.</code><code class="n">update</code><code class="p">({</code><code class="s2">"batch_stats"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">batch_stats</code><code class="p">})</code>
      <code class="n">kwargs</code><code class="o">.</code><code class="n">update</code><code class="p">({</code><code class="s2">"mutable"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"batch_stats"</code><code class="p">]})</code>
    <code class="n">logits</code><code class="p">,</code> <code class="n">updates</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_fn</code><code class="p">(</code>
      <code class="n">variables</code><code class="p">,</code>
      <code class="n">x</code><code class="o">=</code><code class="n">images</code><code class="p">,</code>
      <code class="n">is_training</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
      <code class="n">rngs</code><code class="o">=</code><code class="p">{</code><code class="s2">"dropout"</code><code class="p">:</code> <code class="n">rng_dropout</code><code class="p">},</code>
      <code class="o">**</code><code class="n">kwargs</code><code class="p">,</code>
    <code class="p">)</code>
    <code class="n">loss</code> <code class="o">=</code> <code class="n">optax</code><code class="o">.</code><code class="n">softmax_cross_entropy_with_integer_labels</code><code class="p">(</code>
      <code class="n">logits</code><code class="p">,</code> <code class="n">labels</code>
    <code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>
    <code class="k">return</code> <code class="n">loss</code><code class="p">,</code> <code class="p">(</code><code class="n">logits</code><code class="p">,</code> <code class="n">updates</code><code class="p">)</code>

  <code class="n">grad_fn</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">value_and_grad</code><code class="p">(</code><code class="n">calculate_loss</code><code class="p">,</code> <code class="n">has_aux</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
  <code class="p">(</code><code class="n">loss</code><code class="p">,</code> <code class="p">(</code><code class="n">logits</code><code class="p">,</code> <code class="n">updates</code><code class="p">)),</code> <code class="n">grads</code> <code class="o">=</code> <code class="n">grad_fn</code><code class="p">(</code>
    <code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">,</code> <code class="n">batch</code><code class="p">[</code><code class="s2">"images"</code><code class="p">],</code> <code class="n">batch</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">]</code>
  <code class="p">)</code>
  <code class="n">state</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_gradients</code><code class="p">(</code><code class="n">grads</code><code class="o">=</code><code class="n">grads</code><code class="p">)</code>
  <code class="k">if</code> <code class="n">state</code><code class="o">.</code><code class="n">batch_stats</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">:</code>
    <code class="n">state</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">replace</code><code class="p">(</code><code class="n">batch_stats</code><code class="o">=</code><code class="n">updates</code><code class="p">[</code><code class="s2">"batch_stats"</code><code class="p">])</code>

  <code class="n">metrics</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"loss"</code><code class="p">:</code> <code class="n">loss</code><code class="p">,</code>
    <code class="o">**</code><code class="n">compute_metrics</code><code class="p">(</code><code class="n">batch</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">],</code> <code class="n">logits</code><code class="p">,</code> <code class="n">num_classes</code><code class="p">),</code>
  <code class="p">}</code>

  <code class="k">return</code> <code class="n">state</code><code class="p">,</code> <code class="n">metrics</code></pre>
       
      <p class="pagebreak-before">Inside <code>calculate_loss</code>:</p>
      
 <ul><li><p>Images and labels are extracted from the batch.</p></li>
 <li><p>The model performs a forward pass to compute logits (unnormalized prediction scores for each class).</p></li>
 <li><p>The predicted logits are compared to ground truth labels using <code>optax.softmax_cross_entropy_with_integer_labels</code>.</p></li>
 <li><p>The mean cross-entropy loss is computed—averaged over the examples in the batch.</p></li>
 <li><p>Gradients are then calculated with respect to this loss and applied to update the model’s parameters.</p></li></ul>
 
<p>One important detail: if the model uses batch normalization (as ResNet does), we also need to update the running statistics tracked by the batch norm layers. The function <code>calculate_loss</code> handles both cases—whether batch norm is used or not—by conditionally including and updating the <code>batch_stats</code> entry in the variable collection.</p>

<p>This structure makes the training step general and reusable across all models we’ve defined, regardless of their architectural complexity or normalization layers.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html50" data-type="indexterm" id="id902"/></p>

   </div></section>
   <section data-type="sect3" data-pdf-bookmark="The evaluation step"><div class="sect3" id="the-evaluation-step">
    <h3>The evaluation step</h3>
    <p><a contenteditable="false" data-primary="training loop" data-secondary="for detecting skin cancer in medical images" data-tertiary="evaluation step" data-type="indexterm" id="id903"/>The evaluation step mirrors the training step but is simpler, as it does <em>not</em> update model weights. Its sole purpose is to compute performance metrics on a validation (or test) batch.</p>

     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="nd">@partial</code><code class="p">(</code><code class="n">jax</code><code class="o">.</code><code class="n">jit</code><code class="p">,</code> <code class="n">static_argnums</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,))</code>
<code class="k">def</code> <code class="nf">eval_step</code><code class="p">(</code>
  <code class="n">state</code><code class="p">:</code> <code class="n">TrainStateWithBatchNorm</code><code class="p">,</code> <code class="n">batch</code><code class="p">:</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">Any</code><code class="p">],</code> <code class="n">num_classes</code><code class="p">:</code> <code class="nb">int</code>
<code class="p">):</code>
  <code class="sd">"""Evaluates model performance on a batch and computes metrics."""</code>
  <code class="n">variables</code><code class="p">,</code> <code class="n">kwargs</code> <code class="o">=</code> <code class="p">{</code><code class="s2">"params"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">params</code><code class="p">},</code> <code class="p">{}</code>
  <code class="k">if</code> <code class="n">state</code><code class="o">.</code><code class="n">batch_stats</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">:</code>
    <code class="n">variables</code><code class="o">.</code><code class="n">update</code><code class="p">({</code><code class="s2">"batch_stats"</code><code class="p">:</code> <code class="n">state</code><code class="o">.</code><code class="n">batch_stats</code><code class="p">})</code>
    <code class="n">kwargs</code><code class="o">.</code><code class="n">update</code><code class="p">({</code><code class="s2">"mutable"</code><code class="p">:</code> <code class="kc">False</code><code class="p">})</code>

  <code class="n">logits</code> <code class="o">=</code> <code class="n">state</code><code class="o">.</code><code class="n">apply_fn</code><code class="p">(</code>
    <code class="n">variables</code><code class="p">,</code> <code class="n">x</code><code class="o">=</code><code class="n">batch</code><code class="p">[</code><code class="s2">"images"</code><code class="p">],</code> <code class="n">is_training</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <code class="o">**</code><code class="n">kwargs</code>
  <code class="p">)</code>
  <code class="n">loss</code> <code class="o">=</code> <code class="n">optax</code><code class="o">.</code><code class="n">softmax_cross_entropy_with_integer_labels</code><code class="p">(</code>
    <code class="n">logits</code><code class="p">,</code> <code class="n">batch</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">]</code>
  <code class="p">)</code><code class="o">.</code><code class="n">mean</code><code class="p">()</code>

  <code class="n">metrics</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"loss"</code><code class="p">:</code> <code class="n">loss</code><code class="p">,</code>
    <code class="o">**</code><code class="n">compute_metrics</code><code class="p">(</code><code class="n">batch</code><code class="p">[</code><code class="s2">"labels"</code><code class="p">],</code> <code class="n">logits</code><code class="p">,</code> <code class="n">num_classes</code><code class="p">),</code>
  <code class="p">}</code>
  <code class="k">return</code> <code class="n">metrics</code>
</pre>
       
      <p>Here’s how it works:</p>
      <ul>
  <li><p>The model runs a forward pass with <code>is_training=False</code>, disabling training-time behavior like dropout and updating batch norm statistics.</p></li>
  <li><p>If batch normalization is used, the stored <code>batch_stats</code> are included in the variable collection—but they are <em>not</em> updated. Instead, we rely on the running averages (exponential moving averages of the mean and variance of activations) that were collected during training.</p></li>
  <li><p>Logits are compared to the ground truth labels using the same loss function as during training.</p></li>
  <li><p>In addition to computing the loss, the function returns performance metrics using <code>compute_metrics</code>, which includes weighted precision and recall (explained later).</p></li></ul>
  
<p>By keeping evaluation stateless and side-effect-free—that is, avoiding any updates to model parameters or internal statistics—we ensure that validation scores remain consistent, reliable, and comparable across runs.</p>
     
    
   </div></section>
   <section data-type="sect3" data-pdf-bookmark="The evaluation metrics"><div class="sect3" id="evaluation-metrics">
    <h3>The evaluation metrics</h3>
    <p><a contenteditable="false" data-primary="training loop" data-secondary="for detecting skin cancer in medical images" data-tertiary="evaluation metrics" data-type="indexterm" id="ch05_cancer.html51"/>During training, we typically monitor the loss, but for evaluation, we will also compute <em>precision</em> and <em>recall</em> to gain a more complete picture of the model’s performance on the skin lesion classification task.</p>
<p>Let’s first discuss how these metrics work in the binary case—for example, distinguishing melanoma from nonmelanoma—and then generalize to the multiclass setting we actually care about:</p>

    <ul class="simple">
     <li>
      <p>
       <a contenteditable="false" data-primary="precision, as evaluation metric" data-type="indexterm" id="id904"/><em>Precision</em> measures how many of the predicted positive cases (melanoma) are actually correct:
      </p>

    <div data-type="equation">
     <math display="block">
  <mrow>
    <mtext>Precision</mtext>
    <mo>=</mo>
    <mfrac><mrow><mtext>True</mtext><mspace width="4.pt"/><mtext>Positives</mtext></mrow> <mrow><mtext>True</mtext><mspace width="4.pt"/><mtext>Positives</mtext><mspace width="4.pt"/><mtext>+</mtext><mspace width="4.pt"/><mtext>False</mtext><mspace width="4.pt"/><mtext>Positives</mtext></mrow></mfrac>
  </mrow>
</math>
    </div>
 

    <p>High precision means few false positives; so when the model flags a melanoma, it’s likely to be correct. This is important in medical settings to avoid unnecessary follow-up procedures and patient anxiety.
    </p>
     </li>
     
     <li>
      <p><a contenteditable="false" data-primary="recall, as evaluation metric" data-type="indexterm" id="id905"/><em>Recall</em> (also called sensitivity) measures how many actual melanoma cases were correctly identified:
      </p>

    <div data-type="equation">
     <math display="block">
  <mrow>
    <mtext>Recall</mtext>
    <mo>=</mo>
    <mfrac><mrow><mtext>True</mtext><mspace width="4.pt"/><mtext>Positives</mtext></mrow> <mrow><mtext>True</mtext><mspace width="4.pt"/><mtext>Positives</mtext><mspace width="4.pt"/><mtext>+</mtext><mspace width="4.pt"/><mtext>False</mtext><mspace width="4.pt"/><mtext>Negatives</mtext></mrow></mfrac>
  </mrow>
</math>
    </div>
    </li>
     </ul>
         <p>High recall means few missed melanoma cases: critical for ensuring early detection. In many clinical settings, recall is prioritized, since missing a true case (i.e., making a false negative prediction) is more harmful than incorrectly flagging a benign case.</p>
     
     <p>In our case, we’re working with multiple classes, not just melanoma versus nonmelanoma. <a contenteditable="false" data-primary="weighted metrics" data-type="indexterm" id="id906"/>To summarize performance across all classes, we use <em>weighted precision</em> and <em>weighted recall</em>. These metrics average per-class values while accounting for class imbalance—classes with more examples contribute more to the final score.</p>

<p>We’ll be using weighted precision and recall as our main evaluation metrics, and we will plot them across training and validation splits as training proceeds. Here is our <code>compute_metrics</code> code:</p>
    
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="nd">@partial</code><code class="p">(</code><code class="n">jax</code><code class="o">.</code><code class="n">jit</code><code class="p">,</code> <code class="n">static_argnums</code><code class="o">=</code><code class="p">(</code><code class="mi">2</code><code class="p">,))</code>
<code class="k">def</code> <code class="nf">compute_metrics</code><code class="p">(</code>
  <code class="n">y_true</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">logits</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">,</code> <code class="n">n_labels</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code>
<code class="p">)</code> <code class="o">-&gt;</code> <code class="nb">dict</code><code class="p">[</code><code class="nb">str</code><code class="p">,</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">]:</code>
  <code class="sd">"""Computes weighted precision and recall metrics from logits and labels."""</code>
  <code class="n">y_scores</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">softmax</code><code class="p">(</code><code class="n">logits</code><code class="p">)</code>
  <code class="n">y_pred</code> <code class="o">=</code> <code class="n">jnp</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">y_scores</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
  <code class="n">metrics</code> <code class="o">=</code> <code class="p">{</code>
    <code class="s2">"recall_weighted"</code><code class="p">:</code> <code class="n">recall_score</code><code class="p">(</code>
      <code class="n">y_true</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">,</code> <code class="n">n_labels</code><code class="p">,</code> <code class="n">average</code><code class="o">=</code><code class="s2">"weighted"</code>
    <code class="p">),</code>
    <code class="s2">"precision_weighted"</code><code class="p">:</code> <code class="n">precision_score</code><code class="p">(</code>
      <code class="n">y_true</code><code class="p">,</code> <code class="n">y_pred</code><code class="p">,</code> <code class="n">n_labels</code><code class="p">,</code> <code class="n">average</code><code class="o">=</code><code class="s2">"weighted"</code>
    <code class="p">),</code>
  <code class="p">}</code>
  <code class="k">return</code> <code class="n">metrics</code>
</pre>
       
     <p>This function uses the predicted logits to compute softmax scores, takes the most likely class, and then calculates weighted precision and recall based on the true and predicted labels.</p>
     
     
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
    <p>Weighted metrics are concise and helpful but can mask problems—for instance, if the model performs poorly on a rare class, this might not show up clearly in the overall score. Always check <em>per-class metrics</em> as well. This can highlight weaknesses that might justify upweighting certain classes in the loss function or resampling your dataset to better balance class representation.</p>
    </div>
    
<p>You may also encounter F1 scores in classification tasks. The F1 score is the harmonic mean of precision and recall—it provides a single number that balances both. Here, we’ll stick to just plotting both precision and recall, as they are often easier metrics to interpret and let you diagnose whether your model struggles more with false positives or false negatives.</p>


<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id907">
  <h5>Why Not Use Accuracy to Track Performance?</h5>

<p><a contenteditable="false" data-primary="accuracy, as performance metric" data-type="indexterm" id="id908"/><a contenteditable="false" data-primary="performance metric, accuracy as" data-type="indexterm" id="id909"/>Accuracy measures the overall percentage of correctly classified examples. It’s simple and easy to understand, but it can break down in imbalanced datasets. For example, if 90% of the data is non-melanoma, a model that always predicts “non-melanoma” will achieve 90% accuracy—even though it fails to detect a single melanoma case.</p>

<p>That’s why we use <em>precision</em> and <em>recall</em>, which provide a more nuanced view of model performance. These metrics distinguish between different types of errors. As mentioned, missing a melanoma (false negative) is often far worse than incorrectly flagging a benign case (false positive). Precision and recall allow us to focus on these critical trade-offs. In practice, you might, for example, track melanoma-specific recall alongside other metrics as you train.</p>

<p>What about <em>weighted</em> precision and recall? These average per-class values according to class frequency—giving more weight to common classes. This is often better than plain accuracy, but still not perfect: in many biological or clinical applications, rare classes can be the most important. A model might perform poorly on a rare but critical class, and the weighted metric won’t reflect it clearly.</p>

<p>So, while weighted metrics are useful for summary plots, <em>per-class metrics</em> are essential. They help you spot blind spots and inform decisions about class weighting, sampling, or model design.</p>
</div></aside>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Faster evaluation metrics"><div class="sect3" id="id138">
  <h3>Faster evaluation metrics</h3>
    <p>You may have noticed that the <code>compute_metrics</code> function is jitted. Standard evaluation metrics—such as those from <code>sklearn</code> or <code>scipy</code>—are not typically JIT-compatible, which can become a bottleneck during training.</p>

<p>To address this, we’ve implemented JIT-friendly versions of precision and recall. These custom functions have been tested to match the outputs of their standard counterparts, but run significantly faster—especially during frequent batch-wise evaluation.</p>
<p>The metrics are computed on each batch and logged using the <code>MetricsLogger</code>, which aggregates results to provide full-dataset performance summaries.</p>

<div data-type="note" epub:type="note"><h6>Note</h6>
  <p>In large-scale production models, the overhead from non-jitted metrics might be negligible. But for the relatively small models and fast iteration cycles we use here, the speedup from JIT compatibility is noticeable<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html51" data-type="indexterm" id="id910"/>.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html49" data-type="indexterm" id="id911"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html48" data-type="indexterm" id="id912"/></p>
</div>

</div></section>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Creating the Multiclass Dataset"><div class="sect2" id="id139">
<h2>Creating the Multiclass Dataset</h2>
 
<p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="training the models" data-tertiary="creating the multiclass dataset" data-type="indexterm" id="ch05_cancer.html52"/><a contenteditable="false" data-primary="training" data-secondary="detecting skin cancer in medical images" data-tertiary="creating the multiclass dataset" data-type="indexterm" id="ch05_cancer.html53"/>Finally, we can use our <code>DatasetBuilder</code> to construct the dataset that will be used to train and evaluate all models in this chapter. Using the same dataset setup across experiments allows us to make clean comparisons and attribute performance differences to model choices rather than data variation.</p>
<p>We also initialize and split a random seed up front, so we can reuse consistent seeds across model initializations and training runs—this helps ensure reproducibility.</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.cancer.dataset.preprocessors</code> <code class="kn">import</code> <code class="n">skew</code>

<code class="n">rng</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">PRNGKey</code><code class="p">(</code><code class="n">seed</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">rng</code><code class="p">,</code> <code class="n">rng_dataset</code><code class="p">,</code> <code class="n">rng_init</code><code class="p">,</code> <code class="n">rng_train</code> <code class="o">=</code> <code class="n">jax</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">split</code><code class="p">(</code><code class="n">rng</code><code class="p">,</code> <code class="n">num</code><code class="o">=</code><code class="mi">4</code><code class="p">)</code>

<code class="n">dataset_splits</code> <code class="o">=</code> <code class="n">DatasetBuilder</code><code class="p">(</code>
  <code class="n">data_dir</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"cancer/datasets/raw"</code><code class="p">),</code>
<code class="p">)</code><code class="o">.</code><code class="n">build</code><code class="p">(</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">rng_dataset</code><code class="p">,</code>
  <code class="n">preprocessors</code><code class="o">=</code><code class="p">[</code><code class="n">skew</code><code class="p">,</code> <code class="n">crop</code><code class="p">,</code> <code class="n">resnet</code><code class="p">],</code>
  <code class="n">image_size</code><code class="o">=</code><code class="p">(</code><code class="mi">224</code><code class="p">,</code> <code class="mi">224</code><code class="p">,</code> <code class="mi">3</code><code class="p">),</code>
  <code class="n">splits</code><code class="o">=</code><code class="p">{</code><code class="s2">"train"</code><code class="p">:</code> <code class="mf">0.70</code><code class="p">,</code> <code class="s2">"valid"</code><code class="p">:</code> <code class="mf">0.20</code><code class="p">,</code> <code class="s2">"test"</code><code class="p">:</code> <code class="mf">0.10</code><code class="p">},</code>
<code class="p">)</code>

<code class="n">num_classes</code> <code class="o">=</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">num_classes</code></pre>

<p>There are three preprocessing options implemented:</p>

<dl>
  <dt><code>crop</code></dt><dd><p>Focuses on the central region of the image, where lesions are typically located. While simple and effective, it can discard potentially useful peripheral features such as hairs, texture, or pigment variation.</p></dd>
  <dt><code>skew</code></dt><dd><p>Retains as much of the lesion context as possible by resizing the entire image to a square without cropping—this means stretching or compressing the original aspect ratio. Since most of the input images are wider than they are tall, cropping to a square can cut out potentially useful surrounding tissue. Skewing avoids this by distorting the image to fit a square shape, preserving all pixels.</p> <p>While the image becomes slightly warped, it may still help the model capture broader contextual cues like skin texture or peripheral features.</p></dd>
  <dt><code>resnet</code></dt><dd><p>Applies standard preprocessing required for ResNet-based models, including resizing to the expected input shape and normalizing pixel values. Ensures compatibility with pretrained ResNet architectures.</p></dd>
</dl>

<p>These preprocessing steps are modular and can be swapped in and out easily. We can treat the choice between them as yet another tunable hyperparameter during experimentation.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html53" data-type="indexterm" id="id913"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html52" data-type="indexterm" id="id914"/></p>
</div></section>
 
   <section data-type="sect2" data-pdf-bookmark="Training the Baseline Model"><div class="sect2" id="training-the-baseline-model">
    <h2>Training the Baseline Model</h2>
    <p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="training the models" data-tertiary="training the baseline model" data-type="indexterm" id="ch05_cancer.html54"/><a contenteditable="false" data-primary="training" data-secondary="detecting skin cancer in medical images" data-tertiary="training the baseline model" data-type="indexterm" id="ch05_cancer.html55"/>Let’s first train the baseline <code>SimpleCnn</code> model. There’s nothing fancy here—but the point is to show that even a naive architecture can learn a surprising amount from the raw images. Here is the training setup:</p>
   
     
        <pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">optax</code>

<code class="kn">from</code> <code class="nn">dlfb.cancer.train.handlers.samplers</code> <code class="kn">import</code> <code class="n">repeating_sampler</code>
<code class="kn">from</code> <code class="nn">dlfb.cancer.utils</code> <code class="kn">import</code> <code class="n">decay_mask</code>

<code class="n">learning_rate</code> <code class="o">=</code> <code class="mf">0.001</code>
<code class="n">num_steps</code> <code class="o">=</code> <code class="mi">2000</code>

<code class="n">state</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">train</code><code class="p">(</code>
  <code class="n">state</code><code class="o">=</code><code class="n">SimpleCnn</code><code class="p">(</code><code class="n">num_classes</code><code class="o">=</code><code class="n">num_classes</code><code class="p">)</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
    <code class="n">dummy_input</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">get_dummy_input</code><code class="p">(),</code>
    <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adamw</code><code class="p">(</code><code class="n">learning_rate</code><code class="p">,</code> <code class="n">weight_decay</code><code class="o">=</code><code class="mf">0.0</code><code class="p">,</code> <code class="n">mask</code><code class="o">=</code><code class="n">decay_mask</code><code class="p">),</code>
  <code class="p">),</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">rng_train</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">,</code>
  <code class="n">num_steps</code><code class="o">=</code><code class="n">num_steps</code><code class="p">,</code>
  <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code>
  <code class="n">preprocessor</code><code class="o">=</code><code class="n">crop</code><code class="p">,</code>
  <code class="n">sampler</code><code class="o">=</code><code class="n">repeating_sampler</code><code class="p">,</code>
  <code class="n">augmentor</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>
  <code class="n">eval_every</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
  <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"cancer/models/baseline"</code><code class="p">),</code>
<code class="p">)</code>
</pre>
       
      
     
    
    <p>A few quick notes on this “no frills” setup:</p>
    
    <ul>
 <li><p>We use <code>optax.adamw</code> but set <code>weight_decay=0.0</code>, which makes it equivalent to plain <code>optax.adam</code>. This means no weight decay regularization is applied (we also do not need to worry about the <code>mask</code> parameter).</p></li>
 <li><p>The preprocessor is set to crop.</p></li>
 <li><p>We’re using the <code>repeating_sampler</code> to cycle through the data. This does not resample images to balance class frequencies.</p></li>
 <li><p>No data augmentation is applied (<code>augmentor=None</code>).</p></li>
 </ul>

 <div data-type="note" epub:type="note"><h6>Note</h6>
<p>Although we specify the <code>optax.adamw</code> optimizer, we initially leave weight decay unused. Why? Defining it uniformly across all of our models ensures consistency and makes hyperparameter tuning much simpler. Later on, we’ll activate this placeholder to help reduce model overfitting.</p>
</div>
 
<p>Let’s now evaluate the model’s performance by plotting its metrics over the training steps. The results are shown in
     <a data-type="xref" href="#resnet-from-scratch-evaluation-plot">Figure 5-16</a>.
    </p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.cancer.inspect</code> <code class="kn">import</code> <code class="n">plot_learning</code>

<code class="n">plot_learning</code><code class="p">(</code><code class="n">metrics</code><code class="p">);</code>
</pre>
       
      
     
     
      <figure><div id="resnet-from-scratch-evaluation-plot" class="figure">
       <img alt="" src="assets/dlfb_0516.png" width="600" height="289"/>
       <h6><span class="label">Figure 5-16. </span>Evaluation metrics for the <code>SimpleCnn</code> model. The left plot shows the evolution of loss for the <code>train</code> and <code>valid</code> dataset splits, while the right plot tracks (weighted) precision and recall for the <code>valid</code> dataset.</h6>
      </div></figure>
     
    
   <p>We see that this baseline model reaches around 0.4 weighted precision and recall on the validation set—not bad for such a simple architecture. This gives us a reference point to compare against more advanced models.</p>
<p>Training metrics are much higher than validation metrics, indicating substantial overfitting. We also observe that validation loss increases over time, which is consistent with overfitting.</p>
<p>Another useful way to assess model performance is through the confusion matrix, shown in <a data-type="xref" href="#resnet-from-scratch-confusion-matrix">Figure 5-17</a>. This gives a detailed view of how predictions align with ground truth labels—including which classes are commonly confused:</p>
   
     
      
       
        <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">dlfb.cancer.inspect</code> <code class="kn">import</code> <code class="n">plot_classified_images</code>
<code class="kn">from</code> <code class="nn">dlfb.cancer.train</code> <code class="kn">import</code> <code class="n">get_predictions</code>

<code class="n">predictions</code> <code class="o">=</code> <code class="n">get_predictions</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">],</code> <code class="n">resnet</code><code class="p">,</code> <code class="mi">32</code><code class="p">)</code>
<code class="n">plot_classified_images</code><code class="p">(</code>
  <code class="n">predictions</code><code class="p">,</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">],</code> <code class="n">crop</code><code class="p">,</code> <code class="n">max_images</code><code class="o">=</code><code class="mi">8</code>
<code class="p">);</code>
</pre>
       
      
     
     
      <figure><div id="resnet-from-scratch-confusion-matrix" class="figure">
       <img alt="" src="assets/dlfb_0517.png" width="600" height="599"/>
       <h6><span class="label">Figure 5-17. </span>Confusion matrix for the <code>SimpleCnn</code> model on the nine-class skin lesion classification task. The rows represent the true labels, and the columns represent the predicted labels. Diagonal cells show correct predictions; off-diagonal cells indicate misclassifications. Each cell also includes example validation images for qualitative inspection, with the count of predictions shown in the lower-right. Precision (P:) and recall (R:) scores are provided per class.</h6>
      </div></figure>
     
    
  <p>Some guidance on reading this plot—which we’ll use throughout the chapter:</p>
  <ul>
 <li><p>Diagonal is good: These represent correctly classified images. An ideal model would have predictions only on the diagonal.</p></li>
 <li><p>Off-diagonal is bad: These are misclassifications.</p></li>
 <li><p>The number in the lower-right of each square indicates how many examples fall into that cell (in the validation set). Many confusion matrices simply report the counts here, but we also include actual images so that you can start to build intuition over what the different lesion types look like.</p></li>
 <li><p>Precision (<code>P:</code>) is shown above each column (per predicted class), and recall (<code>R:</code>) is shown along each row (per true class).</p></li>
 <li><p>The weighted precision and recall reported in the previous training plots are not simple averages of these values—they are class-weighted, meaning more common classes contribute more to the overall metric.</p></li></ul>
 
With that in mind, interpreting the confusion matrix in <a data-type="xref" href="#resnet-from-scratch-confusion-matrix">Figure 5-17</a> specifically, we can see that:

<ul>
 <li><p>This model is not that great—there are many off-diagonal entries.</p></li>
 <li><p>The model generally performs better on more common classes and struggles with rarer ones—we’ll try resampling later to address this.</p></li>
 <li><p>It performs best on vascular lesions (bottom-right corner), which are visually distinct and easier to recognize, even to untrained eyes.</p></li>
 <li><p>The model also does relatively well on pigmented benign keratosis, nevus, and melanoma—though there’s significant room for improvement.</p></li>
 <li><p>A key issue is that melanomas are often misclassified as nevi or other benign categories. This is particularly concerning given the clinical importance of detecting melanoma. We’ll keep a close eye on whether more advanced models reduce these errors.</p></li>
 </ul>
 
<p>With this baseline in mind, let’s turn our attention to the <code>ResNetFromScratch</code> model<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html55" data-type="indexterm" id="id915"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html54" data-type="indexterm" id="id916"/>.</p>
     
 </div></section>
 <section data-type="sect2" data-pdf-bookmark="Training the ResNetFromScratch Model"><div class="sect2" id="building-a-better-model">
  <h2>Training the ResNetFromScratch Model</h2>
  <p><a contenteditable="false" data-primary="ResNetFromScratch model" data-type="indexterm" id="ch05_cancer.html56"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="training the models" data-tertiary="training the ResNetFromScratch model" data-type="indexterm" id="ch05_cancer.html57"/><a contenteditable="false" data-primary="training" data-secondary="detecting skin cancer in medical images" data-tertiary="training the ResNetFromScratch model" data-type="indexterm" id="ch05_cancer.html58"/>Recall that this model follows the ResNet architecture but starts from randomly initialized weights—meaning it will probably require the most training time to reach strong performance compared to models that makes use of pretrained weights.</p>
<p>Before training, we need to set up both the dataset and model instance. We are aware of the class imbalance and small dataset size, and we have already prepared techniques like <code>balanced_sampler</code> and <code>rich_augmentor</code> to mitigate these issues. However, to better understand their individual impact, we will first run training without any augmentation or class balancing.</p>
<p>For this model, we do not need to apply <code>resnet</code> preprocessing to the images, since we are training from scratch and don’t require compatibility with pretrained ResNet weights.</p>

<p>Now, let’s train our model:</p>

 
     
      <pre data-type="programlisting" data-code-language="python"><code class="n">state</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">train</code><code class="p">(</code>
  <code class="n">state</code><code class="o">=</code><code class="n">ResNetFromScratch</code><code class="p">(</code><code class="n">num_classes</code><code class="o">=</code><code class="n">num_classes</code><code class="p">)</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
    <code class="n">dummy_input</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">get_dummy_input</code><code class="p">(),</code>
    <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adamw</code><code class="p">(</code><code class="n">learning_rate</code><code class="p">,</code> <code class="n">weight_decay</code><code class="o">=</code><code class="mf">0.0</code><code class="p">,</code> <code class="n">mask</code><code class="o">=</code><code class="n">decay_mask</code><code class="p">),</code>
  <code class="p">),</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">rng_train</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">,</code>
  <code class="n">num_steps</code><code class="o">=</code><code class="n">num_steps</code><code class="p">,</code>
  <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code>
  <code class="n">preprocessor</code><code class="o">=</code><code class="n">crop</code><code class="p">,</code>  <code class="c1"># No preprocessing.</code>
  <code class="n">sampler</code><code class="o">=</code><code class="n">repeating_sampler</code><code class="p">,</code>  <code class="c1"># No balancing.</code>
  <code class="n">augmentor</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>  <code class="c1"># No augmentation.</code>
  <code class="n">eval_every</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
  <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"cancer/models/resnet_from_scratch"</code><code class="p">),</code>
<code class="p">)</code></pre>
     
    
   
  
  <p>As before, let’s evaluate the model’s performance by plotting its metrics over the training steps. The results are shown in <a data-type="xref" href="#fine-tuned-resnet-learning-schedule">Figure 5-18</a>:</p>
 
   
    
     
      <pre data-type="programlisting" data-code-language="python"><code class="n">plot_learning</code><code class="p">(</code><code class="n">metrics</code><code class="p">);</code></pre>
     
    
   
   
    <figure><div id="fine-tuned-resnet-learning-schedule" class="figure">
     <img alt="" src="assets/dlfb_0518.png" width="600" height="289"/>
     <h6><span class="label">Figure 5-18. </span>Model performance over training and validation steps for the <code>ResNetFromScratch</code> multiclass skin lesion classification model.</h6>
    </div></figure>
   
  
  <p>The results are promising and already show an improvement over the baseline—precision and recall metrics are significantly higher. This makes sense, as the ResNet architecture is generally strong for image tasks.</p>
<p>Importantly, both train and validation metrics increase steadily and remain closely aligned. Likewise, the train and validation losses both decrease over time, suggesting the model is not significantly overfitting. In fact, the training curves indicate that the model is still learning—so with more training steps, we might be able to achieve even better results.</p>
<p>To see which classes benefit most from this improved model, let’s look again at the confusion matrix in <a data-type="xref" href="#fig518NEW">Figure 5-19</a>:</p>
  
   
      <pre data-type="programlisting" data-code-language="python"><code class="n">predictions</code> <code class="o">=</code> <code class="n">get_predictions</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">],</code> <code class="n">resnet</code><code class="p">,</code> <code class="mi">32</code><code class="p">)</code>
<code class="n">plot_classified_images</code><code class="p">(</code>
  <code class="n">predictions</code><code class="p">,</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">],</code> <code class="n">crop</code><code class="p">,</code> <code class="n">max_images</code><code class="o">=</code><code class="mi">8</code>
<code class="p">);</code></pre>
     
    <figure><div id="fig518NEW" class="figure">
     <img alt="" src="assets/dlfb_0519.png" width="600" height="599"/>
     <h6><span class="label">Figure 5-19. </span>Confusion matrix for the <code>ResNetFromScratch</code> model on the nine-class skin lesion classification task.</h6>
    </div></figure>
   
   <p>Some observations:</p>
   
   <ul>
  <li><p>Overall improvement: We see better recall and precision across most categories compared to the baseline model. The model is generally more confident and more accurate. The diagonal is visibly stronger overall, indicating more correct predictions.</p></li>
  <li><p>Vascular lesions are now being classified almost perfectly (bottom-right corner). These lesions are visually distinctive, and the model is clearly picking up on that.</p></li>
  <li><p>Nevus and melanoma both show solid improvements in recall. However, there is still some confusion between the two, as seen in the off-diagonal cells between those classes.</p></li>
  <li><p>Pigmented benign keratosis shows a marked improvement in both precision and recall, with a notable jump from the baseline.</p></li>
  <li><p>Actinic keratosis and seborrheic keratosis are now never predicted by the <span class="keep-together">model—resulting</span> in <code>NaN</code> precision values for those classes. This is odd and is something to revisit with future models.</p></li>
  </ul>
  
<p>We now move on to the <code>FinetunedHeadResNet</code>, which incorporates pretrained ImageNet features. This should give us a much stronger model, especially given the small size of our training dataset.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html58" data-type="indexterm" id="id917"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html57" data-type="indexterm" id="id918"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html56" data-type="indexterm" id="id919"/></p>
  </div></section>
  <section data-type="sect2" data-pdf-bookmark="Training the FinetunedHeadResNet Model"><div class="sect2" id="evaluating-the-fine-tuned-model">
   <h2>Training the FinetunedHeadResNet Model</h2>
   
   
   <p><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="training the models" data-tertiary="evaluating the fine-tuned model" data-type="indexterm" id="ch05_cancer.html59"/><a contenteditable="false" data-primary="training" data-secondary="detecting skin cancer in medical images" data-tertiary="evaluating the fine-tuned model" data-type="indexterm" id="ch05_cancer.html60"/>This time around, we need to be careful about image preprocessing. Since we’re using pretrained weights from a ResNet model originally trained on ImageNet, our input images must be preprocessed in the same way as those used for ImageNet.</p>


<div data-type="warning" epub:type="warning"><h6>Warning</h6>
<p>If the images deviate too much—for example, if they’re scaled differently or normalized inconsistently—the pretrained features may no longer be as applicable. This can force the model to overcorrect during training, reducing the benefits of transfer learning.</p></div>

<p>To handle this, we apply the <code>resnet</code> preprocessing function, which ensures that input images are resized and normalized in a way that matches the expectations of the pretrained ResNet backbone:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">IMAGE_PROCESSOR</code> <code class="o">=</code> <code class="n">AutoImageProcessor</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"microsoft/resnet-50"</code><code class="p">)</code>


<code class="k">def</code> <code class="nf">resnet</code><code class="p">(</code><code class="n">image</code><code class="p">:</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">)</code> <code class="o">-&gt;</code> <code class="n">jax</code><code class="o">.</code><code class="n">Array</code><code class="p">:</code>
  <code class="sd">"""Preprocess from pretrained model with transpose for compatibility."""</code>
  <code class="n">image</code> <code class="o">=</code> <code class="n">IMAGE_PROCESSOR</code><code class="p">(</code><code class="n">image</code><code class="p">,</code> <code class="n">return_tensors</code><code class="o">=</code><code class="s2">"jax"</code><code class="p">,</code> <code class="n">do_rescale</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
  <code class="n">image</code> <code class="o">=</code> <code class="n">image</code><code class="p">[</code><code class="s2">"pixel_values"</code><code class="p">]</code>
  <code class="n">image</code> <code class="o">=</code> <code class="n">convert_nchw_to_nhwc</code><code class="p">(</code><code class="n">image</code><code class="p">)</code>
  <code class="k">return</code> <code class="n">image</code>
</pre>


<p>We are now ready to train the <code>FinetunedHeadResNet</code> model, which freezes the pretrained backbone and trains only the classification head. To isolate the effects of transfer learning, we keep the setup minimal—no regularization, augmentation, or class rebalancing is applied at this stage.</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">state</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">train</code><code class="p">(</code>
  <code class="n">state</code><code class="o">=</code><code class="n">FinetunedHeadResNet</code><code class="p">(</code><code class="n">num_classes</code><code class="o">=</code><code class="n">num_classes</code><code class="p">)</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
    <code class="n">dummy_input</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">get_dummy_input</code><code class="p">(),</code>
    <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adamw</code><code class="p">(</code><code class="n">learning_rate</code><code class="p">,</code> <code class="n">weight_decay</code><code class="o">=</code><code class="mf">0.0</code><code class="p">,</code> <code class="n">mask</code><code class="o">=</code><code class="n">decay_mask</code><code class="p">),</code>
  <code class="p">),</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">rng_train</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">,</code>
  <code class="n">num_steps</code><code class="o">=</code><code class="n">num_steps</code><code class="p">,</code>
  <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code>
  <code class="n">preprocessor</code><code class="o">=</code><code class="n">resnet</code><code class="p">,</code>
  <code class="n">sampler</code><code class="o">=</code><code class="n">repeating_sampler</code><code class="p">,</code>
  <code class="n">augmentor</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>
  <code class="n">eval_every</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
  <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"cancer/models/resnet_just_head"</code><code class="p">),</code>
<code class="p">)</code></pre>

<p>Let’s examine the model’s performance in <a data-type="xref" href="#fine-tuned-resnet-evaluation-plot">Figure 5-20</a>.</p>
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">plot_learning</code><code class="p">(</code><code class="n">metrics</code><code class="p">);</code></pre>
      
     
    
    
     <figure><div id="fine-tuned-resnet-evaluation-plot" class="figure">
      <img alt="" src="assets/dlfb_0520.png" width="600" height="289"/>
      <h6><span class="label">Figure 5-20. </span>Model performance over training and validation steps for the <code>FinetunedHeadResNet</code> multiclass skin lesion classification model.</h6>
     </div></figure>
    
   
   <p class="pagebreak-before">Some observations:</p>
   
   <ul>
 <li><p>Loss curves: The training loss steadily decreases, as expected, but the validation loss begins to rise again after around step 250. This is a classic sign of overfitting—the model is becoming more confident on the training set, but its generalization to new data begins to degrade.</p></li>
 <li><p>Train vs. validation gap: There is still a large gap between training and validation metrics, though it’s smaller than what we saw with the <code>SimpleCnn</code> baseline.</p></li>
 <li><p>Precision and recall: Interestingly, while the validation loss increases, the validation precision and recall metrics remain relatively stable. This suggests that although the model’s softmax confidence may be worsening, its actual classification decisions are not deteriorating significantly as training proceeds—at least not yet.</p></li>
 <li><p>Plateauing metrics: Validation performance levels off around step 250. This likely reflects the fact that we are only training the classification head—the majority of the model (the ResNet backbone) remains frozen. That limited flexibility may be capping performance, despite early gains.</p></li>
   </ul>
 
<p>It’s worth noting that validation performance here is not noticeably better than what we achieved by training ResNet from scratch (though notice how much faster <code>FinetunedHeadResNet</code> trained). This may suggest that training only the classification head doesn’t provide enough flexibility for the model to properly adapt to our dataset. The much higher training performance relative to validation performance also points to overfitting. While we could try to address this with regularization or augmentation, we’ll instead move on to what we expect will be a significantly stronger approach—fine-tuning the entire network.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html60" data-type="indexterm" id="id920"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html59" data-type="indexterm" id="id921"/></p>

 </div></section>
 <section data-type="sect2" data-pdf-bookmark="Training the FinetunedResNet Model"><div class="sect2" id="classifying-all-lesion-types">
  <h2>Training the FinetunedResNet Model</h2>
  <p><a contenteditable="false" data-primary="FinetunedResNet model" data-secondary="training" data-type="indexterm" id="ch05_cancer.html61"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="training the models" data-tertiary="training the FinetunedResNet model" data-type="indexterm" id="ch05_cancer.html62"/><a contenteditable="false" data-primary="training" data-secondary="detecting skin cancer in medical images" data-tertiary="training the FinetunedResNet model" data-type="indexterm" id="ch05_cancer.html63"/>Next, we’ll train a model that updates all the pretrained weights—not just the classification head. This approach, known as full fine-tuning, allows the model to gradually adapt its internal feature representations to better match our dataset:</p>

     
      <pre data-type="programlisting" data-code-language="python"><code class="n">state</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">train</code><code class="p">(</code>
  <code class="n">state</code><code class="o">=</code><code class="n">FinetunedResNet</code><code class="p">(</code><code class="n">num_classes</code><code class="o">=</code><code class="n">num_classes</code><code class="p">)</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
    <code class="n">dummy_input</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">get_dummy_input</code><code class="p">(),</code>
    <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adamw</code><code class="p">(</code><code class="n">learning_rate</code><code class="p">,</code> <code class="n">weight_decay</code><code class="o">=</code><code class="mf">0.0</code><code class="p">,</code> <code class="n">mask</code><code class="o">=</code><code class="n">decay_mask</code><code class="p">),</code>
  <code class="p">),</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">rng_train</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">,</code>
  <code class="n">num_steps</code><code class="o">=</code><code class="n">num_steps</code><code class="p">,</code>
  <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code>
  <code class="n">preprocessor</code><code class="o">=</code><code class="n">resnet</code><code class="p">,</code>
  <code class="n">sampler</code><code class="o">=</code><code class="n">repeating_sampler</code><code class="p">,</code>
  <code class="n">augmentor</code><code class="o">=</code><code class="kc">None</code><code class="p">,</code>
  <code class="n">eval_every</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
  <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"cancer/models/resnet50_basic"</code><code class="p">),</code>
<code class="p">)</code>
</pre>
     
  
   <p>Let’s examine the model’s performance in <a data-type="xref" href="#last-lesion-model-multiclass-performance-over-steps">Figure 5-21</a>:
   </p>
  
    
     
      
       <pre data-type="programlisting" data-code-language="python"><code class="n">plot_learning</code><code class="p">(</code><code class="n">metrics</code><code class="p">);</code>
</pre>
      
     
    
    
     <figure><div id="last-lesion-model-multiclass-performance-over-steps" class="figure">
      <img alt="" src="assets/dlfb_0521.png" width="600" height="289"/>
      <h6><span class="label">Figure 5-21. </span>Model performance over training and validation steps for the initial (non-optimized) <code>FinetunedResNet</code> multiclass skin lesion classification model.</h6>
     </div></figure>
    
   
<p>There are a few takeaways from these plots:</p>

<ul>
 <li><p>Comparison to previous models: This model clearly outperforms the baseline <code>SimpleCnn</code>, the <code>ResNetFromScratch</code>, and the <code>FinetunedHeadResNet</code> models in terms of validation metrics.</p></li>
 <li><p>Precision and recall: Validation metrics are consistently higher than in previous models—with weighted precision and recall reaching around 0.65–0.7 by the end of training. However, there is still a noticeable gap between training and validation performance, indicating that overfitting remains an issue we’ll need to address.</p></li>
 <li><p>Loss curves: The training loss steadily drops to near-zero, while the validation loss stays high and rises gradually. This is consistent with overfitting, although it’s worth remembering that rising softmax loss doesn’t always reflect a drop in classification quality (and indeed, here the validation metrics stay stable while validation loss rises).</p></li>
</ul>
 
<p class="pagebreak-before">In summary, this is our best-performing model so far—confirming the benefit of full fine-tuning when using pretrained backbones. Before examining the confusion matrix of the fully fine-tuned model, let’s first apply a few targeted optimizations to further improve headline metric performance.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html63" data-type="indexterm" id="id922"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html62" data-type="indexterm" id="id923"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html61" data-type="indexterm" id="id924"/></p>

</div></section>
<section data-type="sect2" data-pdf-bookmark="Optimizing the FinetunedResNet Model"><div class="sect2" id="id234">
  <h2>Optimizing the FinetunedResNet Model</h2>
  
  <p><a contenteditable="false" data-primary="FinetunedResNet model" data-secondary="optimizing" data-type="indexterm" id="ch05_cancer.html64"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="training the models" data-tertiary="optimizing the FinetunedResNet model" data-type="indexterm" id="ch05_cancer.html65"/><a contenteditable="false" data-primary="training" data-secondary="detecting skin cancer in medical images" data-tertiary="optimizing the FinetunedResNet model" data-type="indexterm" id="ch05_cancer.html66"/>We’ll apply four key modifications:</p>

  <ul>  
  <li><p>A warmup plus cosine decay learning rate schedule</p></li>
  <li><p>Data augmentation (as introduced in <a data-type="xref" href="#augmenting-the-dataset">“Augmenting the dataset”</a>)</p></li>
  <li><p>High-rate dropout to reduce overfitting (set to <code>0.7</code>)</p></li>
  <li><p>Weight decay using the <code>adamw</code> optimizer (set to <code>1e-4</code>)</p></li>
  </ul>
  
<p>We’ll walk through each of these in turn.</p>

<section data-type="sect3" data-pdf-bookmark="Learning rate schedule"><div class="sect3" id="id144">
  <h3>Learning rate schedule</h3>
  
<p><a contenteditable="false" data-primary="FinetunedResNet model" data-secondary="optimizing" data-tertiary="learning rate schedule" data-type="indexterm" id="id925"/>Fine-tuning a pretrained model requires care to preserve the valuable low-level features it learned from large datasets like ImageNet. <a contenteditable="false" data-primary="warmup learning rate schedule" data-type="indexterm" id="id926"/>A common strategy is to use a <em>warmup learning rate schedule</em>, which starts training with a small learning rate that gradually increases—allowing the model to adapt gently—before decaying smoothly over time. This helps:</p>

<ul>
  <li><p>Stabilize training in the early stages</p></li>
  <li><p>Prevent abrupt updates to pretrained weights</p></li>
  <li><p>Allow gradual adaptation to our new task</p></li>
  </ul>
  
<p>We’ll use a warmup and cosine decay schedule where the learning rate will:</p>
  
  <ul><li><p>Warm up for the first 20% of steps, gradually increasing the learning rate</p></li>
  <li><p>Peak at a learning rate of <code>0.001</code> (<code>1e-3</code>)</p></li>
  <li><p>Smooth decay to <code>0.00001</code> (<code>1e-5</code>) by the end of training</p></li>
  </ul>
  
<p>Let’s visualize how the learning rate evolves in <a data-type="xref" href="#fig521NEW">Figure 5-22</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">import</code> <code class="nn">optax</code>

<code class="n">learning_rate</code> <code class="o">=</code> <code class="mf">0.001</code>
<code class="n">num_steps</code> <code class="o">=</code> <code class="mi">2000</code>

<code class="n">warmup_cosine_decay_scheduler</code> <code class="o">=</code> <code class="n">optax</code><code class="o">.</code><code class="n">warmup_cosine_decay_schedule</code><code class="p">(</code>
  <code class="n">init_value</code><code class="o">=</code><code class="mf">0.0001</code><code class="p">,</code>
  <code class="n">peak_value</code><code class="o">=</code><code class="n">learning_rate</code><code class="p">,</code>
  <code class="n">end_value</code><code class="o">=</code><code class="mf">0.00001</code><code class="p">,</code>
  <code class="n">warmup_steps</code><code class="o">=</code><code class="nb">int</code><code class="p">(</code><code class="n">num_steps</code> <code class="o">*</code> <code class="mf">0.2</code><code class="p">),</code>
  <code class="n">decay_steps</code><code class="o">=</code><code class="n">num_steps</code><code class="p">,</code>
<code class="p">)</code>

<code class="n">lrs</code> <code class="o">=</code> <code class="p">[</code><code class="n">warmup_cosine_decay_scheduler</code><code class="p">(</code><code class="n">i</code><code class="p">)</code> <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">num_steps</code><code class="p">)]</code>

<code class="n">plt</code><code class="o">.</code><code class="n">scatter</code><code class="p">(</code><code class="nb">range</code><code class="p">(</code><code class="n">num_steps</code><code class="p">),</code> <code class="n">lrs</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">title</code><code class="p">(</code><code class="s2">"Learning Rate over Steps"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">ylabel</code><code class="p">(</code><code class="s2">"Learning Rate"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">xlabel</code><code class="p">(</code><code class="s2">"Step"</code><code class="p">);</code></pre>


  <figure><div id="fig521NEW" class="figure">
      <img alt="" src="assets/dlfb_0522.png" width="600" height="452"/>
      <h6><span class="label">Figure 5-22. </span>Learning rate evolution during training, following a warmup-cosine decay schedule. The learning rate starts small, gradually increases over the first 20% of training steps (warmup), then peaks and decays smoothly.</h6>
     </div></figure>
</div></section>

<section data-type="sect3" data-pdf-bookmark="Data augmentation"><div class="sect3" id="id145">
  <h3>Data augmentation</h3>

<p><a contenteditable="false" data-primary="data augmentation" data-type="indexterm" id="id927"/><a contenteditable="false" data-primary="FinetunedResNet model" data-secondary="optimizing" data-tertiary="data augmentation" data-type="indexterm" id="id928"/><a contenteditable="false" data-primary="overfitting" data-secondary="data augmentation to address" data-type="indexterm" id="id929"/>We previously observed overfitting even in our best model. One way to mitigate this is to apply data augmentation, which increases the effective diversity of the training set by introducing small, label-preserving transformations (e.g., flips, brightness jitter, crops). As discussed earlier in this chapter, augmentation helps the model generalize more robustly by preventing it from memorizing the training images.</p>

<p class="pagebreak-after">We’ll apply these augmentation strategies by using the <code>rich_augmentor</code> defined earlier in the chapter.</p>
</div></section>

<section data-type="sect3" class="less_space" data-pdf-bookmark="Regularization via dropout and adamw"><div class="sect3" id="id146">
  <h3>Regularization via dropout and adamw</h3>
  
<p><a contenteditable="false" data-primary="FinetunedResNet model" data-secondary="optimizing" data-tertiary="regularization via dropout and adamw" data-type="indexterm" id="id930"/>In addition to data augmentation, we’ll also apply explicit regularization using two methods:</p>

<dl>
  <dt>Dropout</dt><dd><p>Randomly disables neurons during training, encouraging the network to rely on distributed representations and reducing co-adaptation. We set a high dropout rate of 0.7 to strongly combat overfitting.</p></dd>
  <dt>Weight decay</dt><dd><p>Discourages overly large weights by penalizing them in the loss function. We apply this using <code>optax.adamw</code>, which is designed to work correctly with L2 regularization.</p></dd>
  </dl>
  
  
<div data-type="warning" epub:type="warning"><h6>Warning</h6>
    <p>You cannot safely use L2 regularization with the standard Adam optimizer—it won’t apply weight decay in the intended way. This is exactly why <code>adamw</code> was introduced, and it should be used when adding weight decay.</p></div>
    
<p>It’s also standard practice <em>not</em> to apply weight decay to:</p>

    <dl><dt>Bias parameters</dt><dd><p>These are typically small and few in number and don’t contribute substantially to model complexity. Regularizing them doesn’t tend to help generalization and can sometimes hurt performance.</p></dd>
  <dt>Batch normalization parameters</dt><dd><p>These include the learned scale (<code>gamma</code>) and shift (<code>beta</code>) terms, as well as running statistics. Applying weight decay to these can destabilize training, especially in fine-tuning scenarios, as they control the distribution of activations rather than model capacity.</p></dd> </dl>
  
<p>Instead, weight decay is usually applied only to the main weights of convolutional or linear layers, where it can help prevent overfitting by discouraging overly complex solutions. To enforce this, we’ll use a parameter mask when constructing the optimizer—applying decay only to the relevant parameters.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html66" data-type="indexterm" id="id931"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html65" data-type="indexterm" id="id932"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html64" data-type="indexterm" id="id933"/></p>
</div></section>
</div></section>

<section data-type="sect2" data-pdf-bookmark="Training the Optimized FinetunedResNet Model"><div class="sect2" id="id147">
  <h2>Training the Optimized FinetunedResNet Model</h2>

<p><a contenteditable="false" data-primary="FinetunedResNet model" data-secondary="training the optimized model" data-type="indexterm" id="ch05_cancer.html67"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="training the models" data-tertiary="training the optimized FinetunedResNet model" data-type="indexterm" id="ch05_cancer.html68"/><a contenteditable="false" data-primary="training" data-secondary="detecting skin cancer in medical images" data-tertiary="training the optimized FinetunedResNet model" data-type="indexterm" id="ch05_cancer.html69"/>Now that we’ve introduced a learning rate schedule, data augmentation, dropout, and weight decay—it’s time to put everything together. We can train the optimized <code>FinetunedResNet</code> model and see whether these changes help reduce overfitting and improve validation performance:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">state</code><code class="p">,</code> <code class="n">metrics</code> <code class="o">=</code> <code class="n">train</code><code class="p">(</code>
  <code class="n">state</code><code class="o">=</code><code class="n">FinetunedResNet</code><code class="p">(</code>
    <code class="n">num_classes</code><code class="o">=</code><code class="n">num_classes</code><code class="p">,</code> <code class="n">dropout_rate</code><code class="o">=</code><code class="mf">0.7</code>
  <code class="p">)</code><code class="o">.</code><code class="n">create_train_state</code><code class="p">(</code>
    <code class="n">rng</code><code class="o">=</code><code class="n">rng_init</code><code class="p">,</code>
    <code class="n">dummy_input</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"train"</code><code class="p">]</code><code class="o">.</code><code class="n">get_dummy_input</code><code class="p">(),</code>
    <code class="n">tx</code><code class="o">=</code><code class="n">optax</code><code class="o">.</code><code class="n">adamw</code><code class="p">(</code>
      <code class="n">warmup_cosine_decay_scheduler</code><code class="p">,</code> <code class="n">weight_decay</code><code class="o">=</code><code class="mf">1e-4</code><code class="p">,</code> <code class="n">mask</code><code class="o">=</code><code class="n">decay_mask</code>
    <code class="p">),</code>
  <code class="p">),</code>
  <code class="n">rng</code><code class="o">=</code><code class="n">rng_train</code><code class="p">,</code>
  <code class="n">dataset_splits</code><code class="o">=</code><code class="n">dataset_splits</code><code class="p">,</code>
  <code class="n">num_steps</code><code class="o">=</code><code class="n">num_steps</code><code class="p">,</code>
  <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code>
  <code class="n">preprocessor</code><code class="o">=</code><code class="n">resnet</code><code class="p">,</code>
  <code class="n">sampler</code><code class="o">=</code><code class="n">repeating_sampler</code><code class="p">,</code>
  <code class="n">augmentor</code><code class="o">=</code><code class="n">rich_augmentor</code><code class="p">,</code>
  <code class="n">eval_every</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code>
  <code class="n">store_path</code><code class="o">=</code><code class="n">assets</code><code class="p">(</code><code class="s2">"cancer/models/resnet50_optimized"</code><code class="p">),</code>
<code class="p">)</code>
</pre>

<p>Now, let’s analyze the model’s performance over the training steps, as shown in <a data-type="xref" href="#fig522NEW">Figure 5-23</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">plot_learning</code><code class="p">(</code><code class="n">metrics</code><code class="p">);</code></pre>

<figure class="pagebreak-after"><div id="fig522NEW" class="figure">
      <img alt="" src="assets/dlfb_0523.png" width="600" height="289"/>
      <h6><span class="label">Figure 5-23. </span>Model performance over training and validation steps for the optimized <code>FinetunedResNet</code> multiclass skin lesion classification model.</h6>
     </div></figure>
     

<p>This training run demonstrates the cumulative benefit of regularization and optimization strategies. Some observations:</p>

<ul>
  <li><p>Precision and recall: Both metrics improve steadily on both the training and validation sets. Most notably, the validation precision and recall reach nearly 0.75 by the end of training—our best performance so far. The gap between training and validation metrics is also now narrower and more stable.</p></li>
  <li><p>Loss curves: While the validation loss remains higher than training loss, it’s still lower than before and appears to not impact the validation metrics—a major improvement.</p></li>
</ul>
  
<p>In short, we’ve trained a model that generalizes much better and achieves strong, stable performance across all metrics. This serves as a great foundation for further experimentation.</p>

<p>To better understand this model’s behavior, let’s plot the confusion matrix as <a data-type="xref" href="#fig524">Figure 5-24</a>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">predictions</code> <code class="o">=</code> <code class="n">get_predictions</code><code class="p">(</code><code class="n">state</code><code class="p">,</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">],</code> <code class="n">resnet</code><code class="p">,</code> <code class="mi">32</code><code class="p">)</code>
<code class="n">plot_classified_images</code><code class="p">(</code>
  <code class="n">predictions</code><code class="p">,</code> <code class="n">dataset_splits</code><code class="p">[</code><code class="s2">"valid"</code><code class="p">],</code> <code class="n">crop</code><code class="p">,</code> <code class="n">max_images</code><code class="o">=</code><code class="mi">8</code>
<code class="p">);</code></pre>

<!-- ASHLEY: moved the following p + ul + p to precede figure#fig524,
as we were going to have a huge gap of whitespace otherwise -->

<p>Some observations about the model’s behavior:</p>

  <ul>
  <li><p>Most predictions lie along the diagonal: This indicates strong overall performance and class-specific accuracy. Nearly all classes show high recall, with vascular lesions and basal cell carcinoma especially well classified.</p></li>
  <li><p>Melanoma versus nevus: While there are still some melanoma cases being misclassified as nevi, the number is much lower than in earlier models and melanoma recall is the highest level yet.</p></li>
  <li><p>Rare classes: Performance on rare classes like actinic keratosis, dermatofibroma, and squamous cell carcinoma has improved. These categories are now being correctly identified more consistently, with fewer scattered misclassifications.</p></li>
<li><p>Emptier off-diagonal cells: Many potential misclassification combinations are completely empty. This indicates that the model is no longer making widespread or erratic mistakes—it’s more selective and confident.</p></li>
  <li><p>Visual inspection: The image thumbnails in each cell help confirm that correct predictions tend to look visually consistent, and many of the remaining errors involve subtle or understandable confusions.</p></li>
  </ul>

<p>Altogether, this confusion matrix reinforces the earlier metrics: the optimized model is significantly more accurate, more reliable, and ultimately would be more clinically useful than earlier versions.</p>

<figure><div id="fig524" class="figure">
      <img alt="" src="assets/dlfb_0524.png" width="600" height="599"/>
      <h6><span class="label">Figure 5-24. </span>Confusion matrix for the final optimized <code>FinetunedResNet</code> model on the nine-class classification task.</h6>
     </div></figure>
     
<div data-type="tip"><h6>Tip</h6>
 
      <p>Confusion matrices can reveal a lot—but they won’t turn us into dermatologists. Interpreting model errors often requires deep domain knowledge. Collaborating with subject matter experts, like dermatologists in this case, can provide critical insights into misclassifications, uncover hidden biases in the data, and suggest clinically meaningful improvements. For any real-world machine learning project, involving domain experts early and often is one of the most effective ways to build useful, trustworthy models.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html69" data-type="indexterm" id="id934"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html68" data-type="indexterm" id="id935"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html67" data-type="indexterm" id="id936"/></p>
</div>

  
  </div></section>
  
<section data-type="sect2" data-pdf-bookmark="Further Improving the Model"><div class="sect2" id="id148">
  <h2>Further Improving the Model</h2>
  
<p><a contenteditable="false" data-primary="FinetunedResNet model" data-secondary="further improving the model" data-type="indexterm" id="ch05_cancer.html70"/><a contenteditable="false" data-primary="skin cancer, detecting in medical images" data-secondary="training the models" data-tertiary="further improving the FinetunedResNet model" data-type="indexterm" id="ch05_cancer.html71"/><a contenteditable="false" data-primary="training" data-secondary="detecting skin cancer in medical images" data-tertiary="further improving the FinetunedResNet model" data-type="indexterm" id="ch05_cancer.html72"/>We’ll stop optimizing this model for now, but it’s far from perfect—for example, it completely fails to predict seborrheic keratosis. Here are several ideas for pushing it further, roughly ordered by expected bang for your buck:</p>

<dl>
  <dt>Longer training</dt><dd><p>Metrics were still improving slightly at the end of 2000 steps. Training for longer could yield further gains, especially since we are using a warmup schedule (which starts learning slowly to protect pretrained weights).</p></dd>
  <dt>Ensembling</dt><dd><p>Combine predictions from multiple trained models to reduce variance and improve robustness:</p>
  <ul>
  <li><p>The most common ensembling approach here would be to average the softmax probabilities across models. Alternatively, you could use majority voting on predicted labels (less smooth, but sometimes effective).</p></li>
  <li><p>You can ensemble models trained with different random seeds (e.g., 2, 4, or 8), or even ensemble structurally different models—such as the various ResNet models we’ve explored.</p></li></ul></dd>
  <dt>Exploring augmentation strategies</dt><dd><p>There’s still plenty of room to be more creative with data augmentation. Try playing around with our augmentation setup (e.g., the specific transformations we encoded) and check their effect on model learning.</p></dd>
  <dt>Class re-weighting</dt><dd><p>Increase the loss contribution from rare or clinically important classes like melanoma. This can be done by computing inverse class frequencies and passing per-example weights into the loss.</p></dd>
  <dt>Hard example mining</dt><dd><p>Focus on examples that are consistently misclassified (e.g., melanoma confused with nevus). You could maintain a pool of high-loss samples and upsample them in training batches, or alternatively, increase loss weights for these types of samples.</p></dd>
  <dt>Stronger regularization</dt><dd><p>Tune <code>dropout_rate</code> and <code>weight_decay</code>, or explore additional regularization methods such as:</p>
  <dl>
  <dt>Label smoothing</dt><dd><p>Instead of using hard one-hot labels, you assign most of the probability to the true class (e.g., 0.9) and distribute the rest evenly across other classes. This reduces model overconfidence and can improve generalization.</p></dd>
  <dt>Mixup or CutMix</dt><dd><p>Blend or patch together two images and their labels. These techniques regularize the model and encourage the model to learn more robust decision boundaries between classes.</p></dd></dl></dd>
  <dt>Multimodal data</dt><dd><p>Dig into the original dataset to see if you can incorporate metadata such as patient age or sex, anatomical site of the lesion, or dermoscopic features. These features can be embedded (e.g., as one-hot or learned embeddings) and concatenated with image features before classification. Metadata often provides key disambiguating context that pixels alone can’t capture.</p></dd>
</dl>

<div data-type="tip"><h6>Tip</h6>
      <p>It can be hard to know where to start when trying to improve a model—and changes can interact in unexpected ways. Progress is rarely linear, so think of it as an exploration process. Use intuition, literature, and diagnostics to guide you—and have fun getting to know your<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html72" data-type="indexterm" id="id937"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html71" data-type="indexterm" id="id938"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html70" data-type="indexterm" id="id939"/> model<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html47" data-type="indexterm" id="id940"/><a contenteditable="false" data-primary="" data-startref="ch05_cancer.html46" data-type="indexterm" id="id941"/>.</p>
</div>


 
</div></section>
 </div></section>
 <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="concluding-remarks">
  <h1>Summary</h1>
  <p>Classifying skin lesions is a genuinely difficult task—if it weren’t, early detection would be routine, and fewer cases would be missed. Still, the models we built in this chapter demonstrate how machine learning can meaningfully assist in this challenge, offering scalable tools to support (not replace) clinical expertise.</p>
  
<p>This project came with plenty of constraints, especially the limitations of the dataset. We had to adapt carefully—testing model variants, countering overfitting, and making the most of each part of the training setup. While more powerful models and larger datasets exist, we hope the mindset and practical tools introduced here help you tackle similar problems—particularly when working with limited or imperfect data.</p>

<p>The techniques we explored—class balancing, data augmentation, fine-tuning pretrained models, diagnosing overfitting, and incrementally improving architectures—are widely applicable. Whether you’re detecting lung disease in chest X-rays, identifying tumors in MRIs, or analyzing retinal images for diabetic retinopathy, the same principles apply: adapt to the data, learn from errors, and iterate thoughtfully.</p>

<p>Now let’s move on. We’ve seen more than enough skin lesion images for one chapter. But before we go, a final reminder: keep an eye on your own skin. If something looks unusual or changes over time, don’t hesitate to seek medical advice. Early detection truly does save lives.<a contenteditable="false" data-primary="" data-startref="ch05_cancer.html0" data-type="indexterm" id="id942"/></p>

 </div></section>
<div data-type="footnotes"><p data-type="footnote" id="id819"><sup><a href="ch05.html#id819-marker">1</a></sup> You can read about one such study <a href="https://oreil.ly/YmXDg">online</a>.</p><p data-type="footnote" id="id820"><sup><a href="ch05.html#id820-marker">2</a></sup> Cassidy, Bill, et al., <a href="https://doi.org/10.1016/j.media.2021.102305">“Analysis of the ISIC Image Datasets: Usage, Benchmarks and Recommendations”</a>, <em>Medical Image Analysis</em> 75 (January 2022).</p><p data-type="footnote" id="id821"><sup><a href="ch05.html#id821-marker">3</a></sup> 
     Statistics by the <a href="https://oreil.ly/79RmS">World Health Organization (WHO)</a>.</p><p data-type="footnote" id="id828"><sup><a href="ch05.html#id828-marker">4</a></sup> M. P. Salinas, et al., <a href="https://doi.org/10.1038/s41746-024-01103-x">“A systematic review and meta-analysis of artificial intelligence versus clinicians for skin cancer diagnosis”</a>. <em>NPJ Digital Medicine</em> 7, no. 1 (2024): 125.</p><p data-type="footnote" id="id848"><sup><a href="ch05.html#id848-marker">5</a></sup> He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015, December 10). <a href="https://oreil.ly/ZqFSo">Deep residual learning for image recognition</a>. arXiv.org.</p><p data-type="footnote" id="id863"><sup><a href="ch05.html#id863-marker">6</a></sup> Codella, N. C. F., et al. (2018). Skin lesion analysis toward melanoma detection: A challenge at the 2017 International symposium on biomedical imaging (ISBI), hosted by the international skin imaging collaboration (ISIC). 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI), 168–172. https://doi.org/10.1109/isbi.2018.8363547</p><p data-type="footnote" id="id870"><sup><a href="ch05.html#id870-marker">7</a></sup> Shorten, C., &amp; Khoshgoftaar, T. M. (2019). <a href="https://doi.org/10.1186/s40537-019-0197-0">A survey on Image Data Augmentation for Deep Learning</a>. <em>Journal of Big Data</em>, 6(1). </p></div></div></section></div></div></body></html>