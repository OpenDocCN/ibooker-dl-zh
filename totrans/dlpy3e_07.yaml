- en: A deep dive on Keras
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨 Keras
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter07_deep-dive-keras](https://deeplearningwithpython.io/chapters/chapter07_deep-dive-keras)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://deeplearningwithpython.io/chapters/chapter07_deep-dive-keras](https://deeplearningwithpython.io/chapters/chapter07_deep-dive-keras)
- en: You’re starting to have some amount of experience with Keras. You’re familiar
    with the `Sequential` model, `Dense` layers, and built-in APIs for training, evaluation,
    and inference — `compile()`, `fit()`, `evaluate()`, and `predict()`. You’ve even
    learned in chapter 3 how to inherit from the `Layer` class to create custom layers,
    and how to use the gradient APIs in TensorFlow, JAX and PyTorch to implement a
    step-by-step training loop.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经开始对 Keras 有了一些经验。你对 `Sequential` 模型、`Dense` 层以及用于训练、评估和推理的内置 API（`compile()`、`fit()`、`evaluate()`
    和 `predict()`）都很熟悉。你甚至在第 3 章学习了如何从 `Layer` 类继承以创建自定义层，以及如何使用 TensorFlow、JAX 和
    PyTorch 中的梯度 API 来实现逐步训练循环。
- en: 'In the coming chapters, we’ll dig into computer vision, timeseries forecasting,
    natural language processing, and generative deep learning. These complex applications
    will require much more than a `Sequential` architecture and the default `fit()`
    loop. So let’s first turn you into a Keras expert! In this chapter, you’ll get
    a complete overview of the key ways to work with Keras APIs: everything you’re
    going to need to handle the advanced deep learning use cases you’ll encounter
    next.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将深入研究计算机视觉、时间序列预测、自然语言处理和生成式深度学习。这些复杂的应用将需要比 `Sequential` 架构和默认的
    `fit()` 循环多得多的东西。所以，让我们首先把你培养成一个 Keras 专家！在本章中，你将获得使用 Keras API 的关键方法的全面概述：你将需要的一切来处理你接下来会遇到的高级深度学习用例。
- en: A spectrum of workflows
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作流程的谱系
- en: 'The design of the Keras API is guided by the principle of *progressive disclosure
    of complexity*: make it easy to get started, yet make it possible to handle high-complexity
    use cases, only requiring incremental learning at each step. Simple use cases
    should be easy and approachable, and arbitrarily advanced workflows should be
    *possible*: no matter how niche and complex the thing you want to do, there should
    be a clear path to it, a path that builds upon the various things you’ve learned
    from simpler workflows. This means that you can grow from beginner to expert and
    still use the same tools — only in different ways.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Keras API 的设计遵循了 *渐进式复杂性披露* 原则：易于上手，同时又能处理高复杂度用例，只需在每一步进行增量学习。简单的用例应该易于接近，任意高级的工作流程应该是
    *可行的*：无论你想要做的事情多么小众和复杂，都应该有一条清晰的路径，一条基于你从更简单的工作流程中学到的各种知识的路径。这意味着你可以从初学者成长为专家，同时仍然使用相同的工具——只是以不同的方式。
- en: As such, there’s not a single “true” way of using Keras. Rather, Keras offers
    a *spectrum of workflows*, from the very simple to the very flexible. There are
    different ways to build Keras models, and different ways to train them, answering
    different needs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，没有一种“真正”使用 Keras 的方法。相反，Keras 提供了一组 *工作流程的谱系*，从非常简单到非常灵活。有不同方式来构建 Keras 模型，以及不同的方式来训练它们，满足不同的需求。
- en: For instance, you have a range of ways to build models and an array of ways
    to train them, each representing a certain tradeoff between usability and flexibility.
    You could be using Keras like you would use scikit-learn — just calling `fit()`
    and letting the framework do its thing — or you could be using it like NumPy —
    taking full control of every little detail.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你有多种构建模型的方法和多种训练它们的方法，每种方法都代表了可用性和灵活性之间的一定权衡。你可以像使用 scikit-learn 一样使用 Keras——只是调用
    `fit()` 并让框架做它的事情——或者你可以像使用 NumPy 一样使用它——完全控制每一个细节。
- en: 'Because all these workflows are based on shared APIs, such as `Layer` and `Model`,
    components from any workflow can be used in any other workflow: they can all talk
    to each other. This means that everything you’re learning now as you’re getting
    started will still be relevant once you’ve become an expert. You can get started
    easily and then gradually dive into workflows where you’re writing more and more
    logic from scratch. You won’t have to switch to an entirely different framework
    as you go from student to researcher, or from data scientist to deep learning
    engineer.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有这些工作流程都基于共享的 API，如 `Layer` 和 `Model`，任何工作流程的组件都可以用于任何其他工作流程：它们都可以相互通信。这意味着你现在在学习的一切，一旦你成为专家，仍然会相关。你可以轻松上手，然后逐渐深入到需要从头编写更多逻辑的工作流程。在你从学生到研究人员，或从数据科学家到深度学习工程师的过程中，你不需要切换到完全不同的框架。
- en: 'This philosophy is not unlike that of Python itself! Some languages only offer
    one way to write programs — for instance, object-oriented programming or functional
    programming. Meanwhile, Python is a multiparadigm language: it offers a range
    of possible usage patterns, which all work nicely together. This makes Python
    suitable for a wide range of very different use cases: system administration,
    data science, machine learning engineering, web development, or just learning
    how to program. Likewise, you can think of Keras as the Python of deep learning:
    a user-friendly deep learning language that offers a variety of workflows for
    different user profiles.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这种理念与Python本身非常相似！一些语言只提供一种编写程序的方式——例如，面向对象编程或函数式编程。而Python是一种多范式语言：它提供了一系列可能的用法模式，它们可以很好地协同工作。这使得Python适用于广泛的非常不同的用例：系统管理、数据科学、机器学习工程、Web开发，或者仅仅是学习如何编程。同样，您可以将Keras视为深度学习的Python：一个用户友好的深度学习语言，为不同的用户配置文件提供各种工作流程。
- en: Different ways to build Keras models
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建Keras模型的不同方法
- en: 'There are three APIs for building models in Keras, as shown in figure 7.1:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中有三种构建模型的API，如图7.1所示：
- en: The *Sequential model* is the most approachable API — it’s basically a Python
    list. As such, it’s limited to simple stacks of layers.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Sequential模型*是最易于接近的API——它基本上是一个Python列表。因此，它仅限于简单的层堆叠。'
- en: The *Functional API*, which focuses on graph-like model architectures. It represents
    a nice mid-point between usability and flexibility, and as such, it’s the most
    commonly used model-building API.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*功能API*，专注于类似图的结构化模型架构。它代表了可用性和灵活性之间的一个很好的平衡点，因此，它是使用最广泛的模型构建API。'
- en: '*Model subclassing*, a low-level option where you write everything yourself
    from scratch. This is ideal if you want full control over every little thing.
    However, you won’t get access to many built-in Keras features, and you will be
    more at risk of making mistakes.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型子类化*，这是一个低级选项，您需要从头开始编写一切。如果您希望对每个细节都拥有完全的控制权，这是理想的。然而，您将无法访问许多内置的Keras功能，并且更容易犯错。'
- en: '![](../Images/ee1d6c9fc1dddc087edee3cc5b26d384.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ee1d6c9fc1dddc087edee3cc5b26d384.png)'
- en: '[Figure 7.1](#figure-7-1): Progressive disclosure of complexity for model building'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.1](#figure-7-1)：模型构建的复杂性逐步揭示'
- en: The Sequential model
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sequential模型
- en: The simplest way to build a Keras model is the `Sequential` model, which you
    already know about.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 构建Keras模型最简单的方式是`Sequential`模型，您已经了解过它。
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[Listing 7.1](#listing-7-1): The `Sequential` class'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.1](#listing-7-1)：`Sequential`类'
- en: Note that it’s possible to build the same model incrementally via the `add()`
    method, similar to the `append()` method of a Python list.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，可以通过`add()`方法逐步构建相同的模型，类似于Python列表的`append()`方法。
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[Listing 7.2](#listing-7-2): Incrementally building a `Sequential` model'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.2](#listing-7-2)：逐步构建`Sequential`模型'
- en: 'You’ve seen in chapter 3 that layers only get built (which is to say, create
    their weights) when they are called for the first time. That’s because the shape
    of the layers’ weights depends on the shape of their input: until the input shape
    is known, they can’t be created.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您在第三章中看到，层只有在第一次被调用时才会构建（也就是说，创建它们的权重）。这是因为层的权重形状取决于它们的输入形状：直到输入形状已知，它们才能被创建。
- en: As such, the previous `Sequential` model does not have any weights until you
    actually call it on some data, or call its `build()` method with an input shape.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，之前的`Sequential`模型在您实际在数据上调用它或使用输入形状调用其`build()`方法之前没有任何权重。
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[Listing 7.3](#listing-7-3): Models that aren’t yet built have no weights'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.3](#listing-7-3)：尚未构建的模型没有权重'
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Listing 7.4](#listing-7-4): Calling a model for the first time to build it'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.4](#listing-7-4)：首次调用模型以构建它'
- en: After the model is built, you can display its contents via the `summary()` method,
    which comes in handy for debugging.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 模型构建完成后，您可以通过`summary()`方法显示其内容，这对于调试非常有用。
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[Listing 7.5](#listing-7-5): The summary method'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.5](#listing-7-5)：`summary`方法'
- en: As you can see, your model happens to be named `sequential_1`. You can actually
    give names to everything in Keras — every model, every layer.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，您的模型恰好命名为`sequential_1`。实际上，您可以在Keras中为一切命名——每个模型，每个层。
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[Listing 7.6](#listing-7-6): Naming models and layers with the `name` argument'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.6](#listing-7-6)：使用`name`参数命名模型和层'
- en: 'When building a `Sequential` model incrementally, it’s useful to be able to
    print a summary of what the current model looks like after you add each layer.
    But you can’t print a summary until the model is built! There’s actually a way
    to have your `Sequential` model get built on the fly: just declare the shape of
    the model’s inputs in advance. You can do this via the `Input` class.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当逐步构建 `Sequential` 模型时，能够在添加每个层后打印当前模型的外观摘要非常有用。但是，您必须在模型构建后才能打印摘要！实际上有一种方法可以让您的
    `Sequential` 模型即时构建：只需提前声明模型输入的形状。您可以通过 `Input` 类来完成此操作。
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Listing 7.7](#listing-7-7): Specifying the input shape of your model in advance'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 7.7](#listing-7-7)：提前指定模型输入的形状'
- en: 'Now you can use `summary()` to follow how the output shape of your model changes
    as you add more layers:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以使用 `summary()` 来跟踪随着您添加更多层，模型输出形状的变化：
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is a pretty common debugging workflow when dealing with layers that transform
    their inputs in complex ways, such as the convolutional layers you’ll learn about
    in chapter 8.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在处理像第 8 章中将要学习的卷积层这样的层以复杂方式转换其输入时的一个相当常见的调试工作流程。
- en: The Functional API
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 功能 API
- en: 'The `Sequential` model is easy to use, but its applicability is extremely limited:
    it can only express models with a single input and a single output, applying one
    layer after the other in a sequential fashion. In practice, it’s pretty common
    to encounter models with multiple inputs (say, an image and its metadata), multiple
    outputs (different things you want to predict about the data), or a nonlinear
    topology.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sequential` 模型易于使用，但其适用性极其有限：它只能表达具有单个输入和单个输出的模型，以顺序方式逐层应用。在实践中，遇到具有多个输入（例如，图像及其元数据）、多个输出（您想要预测的数据的不同方面）或非线性拓扑的模型是很常见的。'
- en: In such cases, you’d build your model using the Functional API. This is what
    most Keras models you’ll encounter in the wild use. It’s fun and powerful — it
    feels like playing with LEGO bricks.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您将使用功能 API 构建模型。这是您在野外遇到的绝大多数 Keras 模型所使用的。它很有趣且功能强大——感觉就像在玩乐高积木。
- en: A simple example
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个简单的例子
- en: 'Let’s start with something simple: the two-layer stack we used in the previous
    section. Its Functional API version looks like the following listing.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单的东西开始：我们在上一节中使用过的两层堆叠。其功能 API 版本如下所示。
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Listing 7.8](#listing-7-8): A simple Functional model with two `Dense` layers'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 7.8](#listing-7-8)：一个包含两个 `Dense` 层的简单功能模型'
- en: 'Let’s go over this step by step. We started by declaring an `Input` (note that
    you can also give names to these input objects, like everything else):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步地过一遍。我们首先声明了一个 `Input`（请注意，您也可以像其他所有对象一样给这些输入对象命名）：
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This `inputs` object holds information about the shape and `dtype` of the data
    that the model will process:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 `inputs` 对象包含有关模型将处理的数据的形状和 `dtype` 的信息：
- en: '[PRE10]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We call such an object a *symbolic tensor*. It doesn’t contain any actual data,
    but it encodes the specifications of the actual tensors of data that the model
    will see when you use it. It *stands for* future tensors of data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称这样的对象为 *符号张量*。它不包含任何实际数据，但它编码了模型在您使用它时将看到的实际数据张量的规格。它 *代表*未来的数据张量。
- en: 'Next, we created a layer and called it on the input:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建了一个层并在输入上调用它：
- en: '[PRE11]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'All Keras layers can be called both on real tensors of data or on these symbolic
    tensors. In the latter case, they return a new symbolic tensor, with updated shape
    and dtype information:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 所有 Keras 层都可以在真实张量数据或这些符号张量上调用。在后一种情况下，它们返回一个新的符号张量，包含更新后的形状和 dtype 信息：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After obtaining the final outputs, we instantiated the model by specifying
    its inputs and outputs in the `Model` constructor:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得最终输出后，我们通过在 `Model` 构造函数中指定其输入和输出实例化了模型：
- en: '[PRE13]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here’s the summary of our model:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的模型摘要：
- en: '[PRE14]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Multi-input, multi-output models
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多输入、多输出模型
- en: Unlike this toy model, most deep learning models don’t look like lists — they
    look like graphs. They may, for instance, have multiple inputs or multiple outputs.
    It’s for this kind of model that the Functional API really shines.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 与这个玩具模型不同，大多数深度学习模型看起来不像列表——它们看起来像图。例如，它们可能有多个输入或多个输出。正是为了这种类型的模型，功能 API 才真正闪耀。
- en: 'Let’s say you’re building a system to rank customer support tickets by priority
    and route them to the appropriate department. Your model has three inputs:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在构建一个系统，根据优先级对客户支持工单进行排序并将它们路由到适当的部门。你的模型有三个输入：
- en: The title of the ticket (text input)
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工单的标题（文本输入）
- en: The text body of the ticket (text input)
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工单的文本正文（文本输入）
- en: Any tags added by the user (categorical input, assumed here to be multi-hot
    encoded)
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户添加的任何标签（分类输入，假设为多热编码）
- en: We can encode the text inputs as arrays of 1s and 0s of size `vocabulary_size`
    (see chapter 14 for detailed information about text encoding techniques).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将文本输入编码为大小为`vocabulary_size`的1s和0s的数组（有关文本编码技术的详细信息，请参阅第14章）。
- en: 'Your model also has two outputs:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你的模型也有两个输出：
- en: The priority score of the ticket, a scalar between 0 and 1 (sigmoid output)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 票据的优先级分数，介于0和1之间的标量（sigmoid输出）
- en: The department that should handle the ticket (a softmax over the set of departments)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该处理票据的部门（部门集合上的softmax）
- en: You can build this model in a few lines with the Functional API.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用功能API在几行代码内构建这个模型。
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Listing 7.9](#listing-7-9): A multi-input, multi-output Functional model'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.9](#listing-7-9)：一个多输入、多输出的功能模型'
- en: The Functional API is a simple, LEGO-like, yet very flexible way to define arbitrary
    graphs of layers like these.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 功能API是一种简单、类似乐高且非常灵活的方式来定义任意层图，如这些。
- en: Training a multi-input, multi-output model
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练多输入、多输出模型
- en: You can train your model in much the same way as you would train a `Sequential`
    model, by calling `fit()` with lists of input and output data. These lists of
    data should respect the same order as the inputs you passed to the `Model()` constructor.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过调用`fit()`并传递输入和输出数据的列表来以与训练`Sequential`模型相同的方式训练你的模型。这些数据列表应遵循与传递给`Model()`构造函数的输入相同的顺序。
- en: '[PRE16]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[Listing 7.10](#listing-7-10): Training a model by providing lists of input
    and target arrays'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.10](#listing-7-10)：通过提供输入和目标数组的列表来训练模型'
- en: If you don’t want to rely on input order (for instance, because you have many
    inputs or outputs), you can also use the names you gave to the `Input` objects
    and to the output layers, and pass data via dictionaries.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想依赖于输入顺序（例如，因为你有很多输入或输出），你也可以使用你给`Input`对象和输出层起的名字，并通过字典传递数据。
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Listing 7.11](#listing-7-11): Training a model by providing dicts of input
    and target arrays'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.11](#listing-7-11)：通过提供输入和目标数组的字典来训练模型'
- en: 'The power of the Functional API: Access to layer connectivity'
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 功能API的强大之处：访问层连接性
- en: 'A Functional model is an explicit graph data structure. This makes it possible
    to *inspect how layers are connected* and *reuse previous graph nodes* (which
    are layer outputs) as part of new models. It also nicely fits the “mental model”
    that most researchers use when thinking about a deep neural network: a graph of
    layers.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 功能模型是一个显式的图数据结构。这使得可以*检查层是如何连接的*，并且可以将之前的图节点（层输出）作为新模型的一部分进行重用。它也很好地符合大多数研究人员在思考深度神经网络时使用的“心智模型”：层图。
- en: 'This enables two important use cases: model visualization and feature extraction.
    Let’s take a look.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得两个重要的用例成为可能：模型可视化和特征提取。让我们看看。
- en: Plotting layer connectivity
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 绘制层连接性
- en: 'Let’s visualize the connectivity of the model we just defined (the *topology*
    of the model). You can plot a Functional model as a graph with the `plot_model()`
    utility, as shown in figure 7.2:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化我们刚刚定义的模型的连接性（模型的*拓扑结构*）。你可以使用`plot_model()`实用工具将功能模型作为图形绘制出来，如图7.2所示：
- en: '[PRE18]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/8c2cc58914d82bedb18b51124f7a37be.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8c2cc58914d82bedb18b51124f7a37be.png)'
- en: '[Figure 7.2](#figure-7-2): Plot generated by `plot_model()` on our ticket classifier
    model'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.2](#figure-7-2)：`plot_model()`在我们票据分类器模型上生成的绘图'
- en: 'You can add to this plot the input and output shapes of each layer in the model,
    as well as layer names (rather than just layer types), which can be helpful during
    debugging (figure 7.3):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以向此图添加模型中每一层的输入和输出形状，以及层名（而不仅仅是层类型），这在调试期间可能很有帮助（图7.3）：
- en: '[PRE19]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](../Images/aa99a9385808a3053cbc09450b3cb3f9.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/aa99a9385808a3053cbc09450b3cb3f9.png)'
- en: '[Figure 7.3](#figure-7-3): Model plot with shape information added'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.3](#figure-7-3)：添加了形状信息的模型绘图'
- en: 'The `None` in the tensor shapes represents the batch size: this model allows
    batches of any size.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 张量形状中的`None`表示批处理大小：此模型允许任何大小的批处理。
- en: Feature extraction with a Functional model
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用功能模型进行特征提取
- en: Access to layer connectivity also means that you can inspect and reuse individual
    nodes (layer calls) in the graph. The model property `model.layers` provides the
    list of layers that make up the model, and for each layer, you can query `layer.input`
    and `layer.output`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 访问层连接性还意味着你可以检查和重用图中的单个节点（层调用）。模型属性`model.layers`提供了构成模型的层列表，对于每一层，你可以查询`layer.input`和`layer.output`。
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[Listing 7.12](#listing-7-12): Retrieving the inputs or outputs of a layer
    in a Functional model'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表7.12](#listing-7-12)：在功能模型中检索层的输入或输出'
- en: 'This enables you to do *feature extraction*: creating models that reuse intermediate
    features from another model.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这使你能够进行*特征提取*：创建重用另一个模型中间特征的模型。
- en: Let’s say you want to add another output to the model we previously defined
    — you want to also predict an estimate of how long a given issue ticket will take
    to resolve, a kind of difficulty rating. You could do this via a classification
    layer over three categories — “quick,” “medium,” and “difficult.” You don’t need
    to recreate and retrain a model from scratch! You can just start from the intermediate
    features of your previous model, since you have access to them.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想向之前定义的模型添加另一个输出——你想要预测给定问题工单解决所需的时间估计，一种难度评级。你可以通过三个类别——“快速”、“中等”和“困难”——的分类层来实现这一点。你不需要从头开始重新创建和重新训练模型！你可以从之前模型的中间特征开始，因为你已经可以访问它们。
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[Listing 7.13](#listing-7-13): Creating a new model by reusing intermediate
    layer outputs'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表7.13](#listing-7-13)：通过重用中间层输出创建新模型'
- en: 'Let’s plot our new model, as shown in figure 7.4:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制我们的新模型，如图7.4所示：
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](../Images/3af1086d9c8bdfa1368caff0b43fd622.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3af1086d9c8bdfa1368caff0b43fd622.png)'
- en: '[Figure 7.4](#figure-7-4): Plot of our new model'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.4](#figure-7-4)：我们新模型的绘图'
- en: Subclassing the Model class
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 子类化`Model`类
- en: 'The last model-building pattern you should know about is the most advanced
    one: `Model` subclassing. You’ve already learned in chapter 3 how to subclass
    the `Layer` class to create custom layers. Subclassing `Model` is pretty similar:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该了解的最后一种模型构建模式是最先进的：`Model`子类化。你已经在第3章学习了如何子类化`Layer`类来创建自定义层。子类化`Model`非常相似：
- en: In the `__init__` method, define the layers the model will use.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`__init__`方法中，定义模型将使用的层。
- en: In the `call` method, define the forward pass of the model, reusing the layers
    previously created.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`call`方法中，定义模型的正向传递，重用之前创建的层。
- en: Instantiate your subclass and call it on data to create its weights.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实例化你的子类，并在数据上调用它以创建其权重。
- en: Rewriting our previous example as a subclassed model
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将我们的前一个示例重写为子类化模型
- en: 'Let’s take a look at a simple example: we will reimplement the customer support
    ticket management model using a `Model` subclass.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个简单的例子：我们将使用`Model`子类重新实现客户支持工单管理模型。
- en: '[PRE23]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[Listing 7.14](#listing-7-14): A simple subclassed model'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表7.14](#listing-7-14)：一个简单的子类化模型'
- en: 'Once you’ve defined the model, you can instantiate it. Note that it will only
    create its weights the first time you call it on some data — much like `Layer`
    subclasses:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你定义了模型，你就可以实例化它。注意，它只会在你第一次在某个数据上调用它时创建其权重——就像`Layer`子类一样：
- en: '[PRE24]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'So far, everything looks very similar to `Layer` subclassing, a workflow you’ve
    already encountered in chapter 3\. What, then, is the difference between a `Layer`
    subclass and a `Model` subclass? It’s simple: a *layer* is a building block you
    use to create models, and a *model* is the top-level object that you will actually
    train, export for inference, etc. In short, a `Model` has a `fit()`, `evaluate()`,
    and `predict()` method. Layers don’t. Other than that, the two classes are virtually
    identical (another difference is that you can *save* a model to a file on disk
    — which we will cover in a few sections).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切看起来都非常类似于`Layer`子类化，这是你在第3章中已经遇到的工作流程。那么，`Layer`子类和`Model`子类之间的区别是什么？很简单：*层*是你用来创建模型的基本构建块，而*模型*是你将实际训练、导出用于推理等的顶级对象。简而言之，`Model`有`fit()`、`evaluate()`和`predict()`方法。层没有。除此之外，这两个类在功能上几乎相同（另一个区别是你可以将模型*保存*到磁盘上的文件——我们将在接下来的几节中介绍）。
- en: 'You can compile and train a `Model` subclass just like a Sequential or Functional
    model:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以像对Sequential或Functional模型一样编译和训练`Model`子类：
- en: '[PRE25]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `Model` subclassing workflow is the most flexible way to build a model:
    it enables you to build models that cannot be expressed as directed acyclic graphs
    of layers — imagine, for instance, a model where the `call()` method uses layers
    inside a `for` loop, or even calls them recursively. Anything is possible — you’re
    in charge.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`Model`子类化工作流程是构建模型最灵活的方式：它允许你构建无法表示为层的有向无环图（DAG）的模型——例如，想象一个`call()`方法在`for`循环中使用层，或者甚至递归调用它们的模型。任何可能的事情都是可能的——你说了算。'
- en: 'Beware: What subclassed models don’t support'
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意：子类化模型不支持的功能
- en: 'This freedom comes at a cost: with subclassed models, you are responsible for
    more of the model logic, which means your potential error surface is much larger.
    As a result, you will have more debugging work to do. You are developing a new
    Python object, not just snapping together LEGO bricks.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自由是有代价的：使用子类化模型，你需要负责更多的模型逻辑，这意味着你的潜在错误面要大得多。因此，你将需要进行更多的调试工作。你正在开发一个新的Python对象，而不仅仅是拼接乐高积木。
- en: 'Functional and subclassed models are also substantially different in nature:
    a Functional model is an explicit data structure — a graph of layers, which you
    can view, inspect, and modify. Meanwhile, a subclassed model is a piece of bytecode
    — a Python class with a `call()` method that contains raw code. This is the source
    of the subclassing workflow’s flexibility — you can just code up whatever functionality
    you like — but it introduces new limitations.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 函数模型和子类化模型在本质上也有很大不同：函数模型是一个显式的数据结构——一个层的图，你可以查看、检查和修改。与此同时，子类化模型是一段字节码——一个包含`call()`方法的Python类，其中包含原始代码。这是子类化工作流程灵活性的来源——你可以编写任何你喜欢的功能——但它也引入了新的限制。
- en: For instance, because the way layers are connected to each other is hidden inside
    the body of the `call()` method, you cannot access that information. Calling `summary()`
    will not display layer connectivity, and you cannot plot the model topology via
    `plot_model()`. Likewise, if you have a subclassed model, you cannot access the
    nodes of the graph of layers to do feature extraction — because there is simply
    no graph. Once the model is instantiated, its forward pass becomes a complete
    black box.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，因为层之间的连接方式隐藏在`call()`方法的主体中，你无法访问这些信息。调用`summary()`不会显示层连接，你也不能通过`plot_model()`来绘制模型拓扑。同样，如果你有一个子类化模型，你无法访问层的图来执行特征提取——因为根本不存在这样的图。一旦模型被实例化，其前向传递就变成了一个完全的黑盒。
- en: Mixing and matching different components
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合和匹配不同的组件
- en: Crucially, choosing one of these patterns — the `Sequential` model, the Functional
    API, `Model` subclassing — does not lock you out of the others. All models in
    the Keras API can smoothly interoperate with each other, whether they’re Sequential
    models, Functional models, or subclassed models written from scratch. They’re
    all part of the same spectrum of workflows. For instance, you can use a subclassed
    layer or model in a Functional model.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的是，选择这些模式之一——`Sequential`模型、函数API、`Model`子类化——并不会将你排除在其他模式之外。Keras API中的所有模型都可以与其他模型无缝交互，无论是`Sequential`模型、函数模型，还是从头开始编写的子类模型。它们都是同一工作流程谱系的一部分。例如，你可以在函数模型中使用子类化的层或模型。
- en: '[PRE26]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[Listing 7.15](#listing-7-15): Creating a Functional model that includes a
    subclassed model'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 7.15](#listing-7-15)：创建一个包含子类模型的函数模型'
- en: Inversely, you can use a Functional model as part of a subclassed layer or model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，你还可以将函数模型用作子类化层或模型的一部分。
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[Listing 7.16](#listing-7-16): Creating a subclassed model that includes a
    Functional model'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 7.16](#listing-7-16)：创建一个包含函数模型的子类模型'
- en: 'Remember: Use the right tool for the job'
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 记住：使用合适的工具来完成工作
- en: You’ve learned about the spectrum of workflows for building Keras models, from
    the simplest workflow — the `Sequential` model — to the most advanced one, model
    subclassing. When should you use one over the other? Each one has its pros and
    cons — pick the one most suitable for the job at hand.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解了构建Keras模型的工作流程谱系，从最简单的`Sequential`模型到最复杂的模型子类化。何时应该使用一个而不是另一个？每个都有其优缺点——选择最适合当前任务的那个。
- en: In general, the Functional API provides you with a pretty good tradeoff between
    ease of use and flexibility. It also gives you direct access to layer connectivity,
    which is very powerful for use cases such as model plotting or feature extraction.
    If you *can* use the Functional API — that is, if your model can be expressed
    as a directed acyclic graph of layers — we recommend using it over model subclassing.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，函数API在易用性和灵活性之间提供了一个相当不错的权衡。它还提供了对层连接的直接访问，这对于模型绘图或特征提取等用例非常强大。如果你可以使用函数API——也就是说，如果你的模型可以表示为一个层的有向无环图——我们建议你使用它而不是模型子类化。
- en: 'Going forward, all examples in this book will use the Functional API — simply
    because all of the models we will work with are expressible as graphs of layers.
    We will, however, make frequent use of subclassed layers. In general, using Functional
    models that include subclassed layers provides the best of both worlds: high development
    flexibility while retaining the advantages of the Functional API.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在起，本书中的所有示例都将使用功能API——仅仅因为我们将要处理的模型都可以表示为层的图。然而，我们将会频繁地使用派生层。一般来说，使用包含派生层的功能模型可以提供两全其美的效果：高开发灵活性同时保留功能API的优势。
- en: Using built-in training and evaluation loops
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用内置的训练和评估循环
- en: The principle of progressive disclosure of complexity — access to a spectrum
    of workflows that go from dead easy to arbitrarily flexible, one step at a time
    — also applies to model training. Keras provides you with different workflows
    for training models — it can be as simple as calling `fit()` on your data or as
    advanced as writing a new training algorithm from scratch.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 渐进式披露复杂性的原则——逐步访问从非常简单到任意灵活的工作流程范围——也适用于模型训练。Keras为您提供了不同的模型训练工作流程——它可以简单到在您的数据上调用`fit()`，也可以复杂到从头编写新的训练算法。
- en: You are already familiar with the `compile()`, `fit()`, `evaluate()`, `predict()`
    workflow. As a reminder, it looks like the following listing.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经熟悉`compile()`、`fit()`、`evaluate()`、`predict()`工作流程。作为提醒，它看起来如下所示。
- en: '[PRE28]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[Listing 7.17](#listing-7-17): The standard workflow: `compile()`, `fit()`,
    `evaluate()`, `predict()`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 7.17](#listing-7-17)：标准工作流程：`compile()`、`fit()`、`evaluate()`、`predict()`'
- en: 'There are a couple of ways you can customize this simple workflow:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以自定义这个简单的流程：
- en: By providing your own custom metrics
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过提供您自己的自定义指标
- en: By passing *callbacks* to the `fit()` method to schedule actions to be taken
    at specific points during training
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将*回调*传递给`fit()`方法来安排在训练过程中的特定点执行的操作
- en: Let’s take a look at these.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些。
- en: Writing your own metrics
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写您自己的指标
- en: Metrics are key to measuring the performance of your model — in particular,
    to measure the difference between its performance on the training data and its
    performance on the test data. Commonly used metrics for classification and regression
    are already part of the built-in `keras.metrics` module — most of the time, that’s
    what you will use. But if you’re doing anything out of the ordinary, you will
    need to be able to write your own metrics. It’s simple!
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 指标是衡量模型性能的关键——特别是衡量其在训练数据和测试数据上的性能差异。常用的分类和回归指标已经包含在内置的`keras.metrics`模块中——大多数情况下，您将使用它。但如果您正在做任何非同寻常的事情，您将需要能够编写自己的指标。这很简单！
- en: A Keras metric is a subclass of the `keras.metrics.Metric` class. Similarly
    to layers, a metric has an internal state stored in Keras variables. Unlike layers,
    these variables aren’t updated via backpropagation, so you have to write the state
    update logic yourself — which happens in the `update_state()` method. For example,
    here’s a simple custom metric that measures the root mean squared error (RMSE).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Keras指标是`keras.metrics.Metric`类的子类。与层类似，指标有一个存储在Keras变量中的内部状态。与层不同，这些变量不是通过反向传播更新的，因此您必须自己编写状态更新逻辑——这发生在`update_state()`方法中。例如，这里有一个简单的自定义指标，用于衡量均方根误差（RMSE）。
- en: '[PRE29]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[Listing 7.18](#listing-7-18): Implementing a custom metric by subclassing
    the `Metric` class'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 7.18](#listing-7-18)：通过继承`Metric`类实现自定义指标'
- en: 'You use the `result()` method to return the current value of the metric:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`result()`方法来返回当前指标值：
- en: '[PRE30]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Meanwhile, you also need to expose a way to reset the metric state without
    having to reinstantiate it — this enables the same metric objects to be used across
    different epochs of training or across both training and evaluation. You do this
    in the `reset_state()` method:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，您还需要提供一个方法来重置指标状态，而无需重新实例化它——这允许相同的指标对象在不同的训练时期或训练和评估之间使用。您可以在`reset_state()`方法中这样做：
- en: '[PRE31]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Custom metrics can be used just like built-in ones. Let’s test-drive our own
    metric:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义指标可以像内置指标一样使用。让我们测试一下我们自己的指标：
- en: '[PRE32]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: You can now see the `fit()` progress bar display the RMSE of your model.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以看到`fit()`进度条显示您模型的RMSE。
- en: Using callbacks
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用回调
- en: 'Launching a training run on a large dataset for tens of epochs using `model.fit()`
    can be a bit like launching a paper airplane: past the initial impulse, you don’t
    have any control over its trajectory or its landing spot. If you want to avoid
    bad outcomes (and thus wasted paper airplanes), it’s smarter to use, not a paper
    plane, but a drone that can sense its environment, send data back to its operator,
    and automatically make steering decisions based on its current state. The Keras
    *callbacks* API will help you transform your call to `model.fit()` from a paper
    airplane into a smart, autonomous drone that can self-introspect and dynamically
    take action.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `model.fit()` 在大型数据集上运行数十个epoch的训练可能有点像发射纸飞机：在初始推动之后，您对其轨迹或着陆点没有任何控制。如果您想避免不良结果（以及因此浪费的纸飞机），更明智的做法是使用，不是纸飞机，而是一个能够感知其环境的无人机，将数据发送回其操作员，并根据其当前状态自动做出转向决策。Keras
    *回调* API将帮助您将 `model.fit()` 的调用从纸飞机转变为一个智能、自主的无人机，它可以自我反思并动态采取行动。
- en: 'A *callback* is an object (a class instance implementing specific methods)
    that is passed to the model in the call to `fit()` and that is called by the model
    at various points during training. It has access to all the available data about
    the state of the model and its performance, and it can take action: interrupt
    training, save a model, load a different weight set, or otherwise alter the state
    of the model.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '*回调* 是一个对象（一个实现特定方法的类实例），在调用 `fit()` 时传递给模型，并在训练的各个阶段由模型调用。它有权访问有关模型状态及其性能的所有可用数据，并且可以采取行动：中断训练、保存模型、加载不同的权重集，或者以其他方式改变模型的状态。'
- en: 'Here are some examples of ways you can use callbacks:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些您可以使用回调的例子：
- en: '*Model checkpointing* — Saving the current state of the model at different
    points during training.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型检查点* — 在训练的不同阶段保存模型的当前状态。'
- en: '*Early stopping* — Interrupting training when the validation loss is no longer
    improving (and of course, saving the best model obtained during training).'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*早期停止* — 当验证损失不再改善时（当然，保存训练期间获得的最佳模型）中断训练。'
- en: '*Dynamically adjusting the value of certain parameters during training* — Such
    as the learning rate of the optimizer.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在训练期间动态调整某些参数的值* — 例如优化器的学习率。'
- en: '*Logging training and validation metrics during training, or visualizing the
    representations learned by the model as they’re updated* — The `fit()` progress
    bar that you’re familiar with is in fact a callback!'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在训练期间记录训练和验证指标，或者可视化模型在更新过程中学习到的表示* — 您熟悉的 `fit()` 进度条实际上是一个回调！'
- en: 'The `keras.callbacks` module includes a number of built-in callbacks (this
    is not an exhaustive list):'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '`keras.callbacks` 模块包含了许多内置回调（这并不是一个详尽的列表）：'
- en: '[PRE33]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Let’s review two of them to give you an idea of how to use them: `EarlyStopping`
    and `ModelCheckpoint`.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾其中两个，以便您了解如何使用它们：`EarlyStopping` 和 `ModelCheckpoint`。
- en: The EarlyStopping and ModelCheckpoint callbacks
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: EarlyStopping 和 ModelCheckpoint 回调
- en: When you’re training a model, there are many things you can’t predict at the
    start. In particular, you can’t tell how many epochs will be needed to get to
    an optimal validation loss. Our examples so far have adopted the strategy of training
    for enough epochs that you begin overfitting, using the first run to figure out
    the optimal number of epochs, and then finally launching a new training run from
    scratch using this optimal number. Of course, this approach is wasteful. A much
    better way to handle this is to stop training when you measure that the validation
    loss is no longer improving. This can be achieved using the `EarlyStopping` callback.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当您训练一个模型时，在开始时有很多事情是无法预测的。特别是，您无法知道需要多少个epoch才能达到最佳的验证损失。我们之前的例子已经采用了训练足够多的epoch，以至于您开始过拟合的策略，使用第一次运行来确定最佳的epoch数量，然后最终从头开始使用这个最佳数量启动新的训练运行。当然，这种方法是浪费的。一个更好的处理方法是，当您测量到验证损失不再改善时停止训练。这可以通过使用
    `EarlyStopping` 回调来实现。
- en: 'The `EarlyStopping` callback interrupts training once a target metric being
    monitored has stopped improving for a fixed number of epochs. For instance, this
    callback allows you to interrupt training as soon as you start overfitting, thus
    avoiding having to retrain your model for a smaller number of epochs. This callback
    is typically used in combination with `ModelCheckpoint`, which lets you continually
    save the model during training (and, optionally, save only the current best model
    so far: the version of the model that achieved the best performance at the end
    of an epoch).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`EarlyStopping`回调在目标监控指标停止改进固定数量的epochs后中断训练。例如，这个回调允许你在开始过拟合时立即中断训练，从而避免需要重新训练模型以更少的epochs。这个回调通常与`ModelCheckpoint`一起使用，它允许你在训练过程中持续保存模型（并且可选地只保存迄今为止的最佳模型：在epoch结束时实现最佳性能的模型版本）。'
- en: '[PRE34]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[Listing 7.19](#listing-7-19): Using the `callbacks` argument in the `fit()`
    method'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.19](#listing-7-19)：在`fit()`方法中使用`callbacks`参数'
- en: Note that you can always save models manually after training as well — just
    call `model.save("my_checkpoint_path.keras")`. To reload the model you’ve saved,
    use
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你可以在训练后手动保存模型——只需调用`model.save("my_checkpoint_path.keras")`。要重新加载你保存的模型，使用
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Writing your own callbacks
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写你自己的回调
- en: 'If you need to take a specific action during training that isn’t covered by
    one of the built-in callbacks, you can write your own callback. Callbacks are
    implemented by subclassing the class `keras.callbacks.Callback`. You can then
    implement any number of the following transparently named methods, which are called
    at various points during training:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要在训练期间执行一个不是由内置回调覆盖的特定操作，你可以编写自己的回调。回调是通过继承`keras.callbacks.Callback`类实现的。然后你可以透明地实现以下方法，这些方法在训练的不同阶段被调用：
- en: '[PRE36]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'These methods are all called with a `logs` argument, which is a dictionary
    containing information about the previous batch, epoch, or training run: training
    and validation metrics, and so on. The `on_epoch_*` and `on_batch_*` methods also
    take the epoch or batch index as first argument (an integer).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法都是使用`logs`参数调用的，它是一个包含有关先前批次、epoch或训练运行信息的字典：训练和验证指标等。`on_epoch_*`和`on_batch_*`方法也接受epoch或批次索引作为第一个参数（一个整数）。
- en: Here’s a simple example callback that saves a list of per-batch loss values
    during training and plots these values at the end of each epoch.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的示例回调，它在训练过程中保存每个批次的损失值列表，并在每个epoch结束时绘制这些值。
- en: '[PRE37]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[Listing 7.20](#listing-7-20): Creating a custom callback by subclassing the
    `Callback` class'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.20](#listing-7-20)：通过继承`Callback`类创建自定义回调'
- en: 'Let’s test-drive it:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试驾一下：
- en: '[PRE38]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We get plots that look like figure 7.5.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如图7.5所示的图表。
- en: '![](../Images/65e8a00724c46a5848204de4a36d1301.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/65e8a00724c46a5848204de4a36d1301.png)'
- en: '[Figure 7.5](#figure-7-5): The output of our custom history-plotting callback'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.5](#figure-7-5)：我们自定义的历史绘图回调的输出'
- en: Monitoring and visualization with TensorBoard
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用TensorBoard进行监控和可视化
- en: 'To do good research or develop good models, you need rich, frequent feedback
    about what’s going on inside your models during your experiments. That’s the point
    of running experiments: to get information about how well a model performs — as
    much information as possible. Making progress is an iterative process, a loop:
    you start with an idea and express it as an experiment, attempting to validate
    or invalidate your idea. You run this experiment and process the information it
    generates, as shown in figure 7.6. This inspires your next idea. The more iterations
    of this loop you’re able to run, the more refined and powerful your ideas become.
    Keras helps you go from idea to experiment in the least possible time, and fast
    GPUs can help you get from experiment to result as quickly as possible. But what
    about processing the experiment results? That’s where TensorBoard comes in.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 要进行良好的研究或开发良好的模型，你需要在你实验过程中对模型内部发生情况的丰富、频繁的反馈。这就是进行实验的目的：获取有关模型表现如何的信息——尽可能多的信息。取得进步是一个迭代过程，一个循环：你从一个想法开始，将其表达为一个实验，试图验证或证伪你的想法。你运行这个实验并处理它生成的信息，如图7.6所示。这激发了你下一个想法。你能运行这个循环的迭代次数越多，你的想法就越精细、越强大。Keras帮助你以最短的时间从想法到实验，快速的GPU可以帮助你尽可能快地从实验到结果。但是，如何处理实验结果呢？这就是TensorBoard发挥作用的地方。
- en: '![](../Images/09e0e7e7f7a9f3f5ce986f95921f2f9a.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09e0e7e7f7a9f3f5ce986f95921f2f9a.png)'
- en: '[Figure 7.6](#figure-7-6): The loop of progress'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.6](#figure-7-6)：进步的循环'
- en: TensorBoard is a browser-based application that you can run locally. It’s the
    best way to monitor everything that goes on inside your model during training.
    With TensorBoard, you can
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: TensorBoard是一个基于浏览器的应用程序，您可以在本地运行。它是监控训练过程中模型内部发生的一切的最佳方式。使用TensorBoard，您可以
- en: Visually monitor metrics during training
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练期间可视化指标
- en: Visualize your model architecture
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化您的模型架构
- en: Visualize histograms of activations and gradients
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化激活和梯度的直方图
- en: Explore embeddings in 3D
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索3D中的嵌入
- en: If you’re monitoring more information than just the model’s final loss, you
    can develop a clearer vision of what the model does and doesn’t do, and you can
    make progress more quickly.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您监控的信息不仅仅是模型的最终损失，您可以对模型做什么以及不做什么有一个更清晰的了解，并且可以更快地取得进展。
- en: 'The easiest way to use TensorBoard with a Keras model and the `fit()` method
    is the `keras.callbacks.TensorBoard` callback. In the simplest case, just specify
    where you want the callback to write logs, and you’re good to go:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 使用TensorBoard与Keras模型和`fit()`方法的最简单方法是`keras.callbacks.TensorBoard`回调。在最简单的情况下，只需指定回调要写入日志的位置，然后就可以开始了：
- en: '[PRE39]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Once the model starts running, it will write logs at the target location. If
    you are running your Python script on a local machine, you can then launch the
    local TensorBoard server using the following command (note that the `tensorboard`
    executable should already be available if you have installed TensorFlow via `pip`;
    if not, you can install TensorBoard manually via `pip install tensorboard`):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型开始运行，它将在目标位置写入日志。如果您在本地机器上运行Python脚本，您可以使用以下命令启动本地TensorBoard服务器（注意，如果通过`pip`安装了TensorFlow，则`tensorboard`可执行文件应该已经可用；如果没有，您可以通过`pip
    install tensorboard`手动安装TensorBoard）：
- en: '[PRE40]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: You can then navigate to the URL that the command returns to access the TensorBoard
    interface.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以导航到命令返回的URL以访问TensorBoard界面。
- en: 'If you are running your script in a Colab notebook, you can run an embedded
    TensorBoard instance as part of your notebook, using the following commands:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Colab笔记本中运行脚本，您可以使用以下命令作为笔记本的一部分运行嵌入的TensorBoard实例：
- en: '[PRE41]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: In the TensorBoard interface, you will be able to monitor live graphs of your
    training and evaluation metrics, as shown in figure 7.7.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorBoard界面中，您将能够监控训练和评估指标的实时图表，如图7.7所示。
- en: '![](../Images/b3ca45ce23be41a15a383f54209b78fc.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/b3ca45ce23be41a15a383f54209b78fc.png)'
- en: '[Figure 7.7](#figure-7-7): TensorBoard can be used for easy monitoring of training
    and evaluation metrics.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.7](#figure-7-7)：TensorBoard可以用于轻松监控训练和评估指标。'
- en: Writing your own training and evaluation loops
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写自己的训练和评估循环
- en: The `fit()` workflow strikes a nice balance between ease of use and flexibility.
    It’s what you will use most of the time. However, it isn’t meant to support everything
    a deep learning researcher may want to do — even with custom metrics, custom losses,
    and custom callbacks.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit()`工作流程在易用性和灵活性之间取得了很好的平衡。您将大部分时间都会使用它。然而，它并不旨在支持深度学习研究人员可能想要做的所有事情——即使有自定义指标、自定义损失和自定义回调。'
- en: 'After all, the built-in `fit()` workflow is solely focused on *supervised learning*:
    a setup where there are known *targets* (also called *labels* or *annotations*)
    associated with your input data and where you compute your loss as a function
    of these targets and the model’s predictions. However, not every form of machine
    learning falls into this category. There are other setups where no explicit targets
    are present, such as *generative learning* (which we will introduce in chapter
    16), *self-supervised learning* (where targets are obtained from the inputs),
    or *reinforcement learning* (where learning is driven by occasional “rewards”
    — much like training a dog). And even if you’re doing regular supervised learning,
    as a researcher, you may want to add some novel bells and whistles that require
    low-level flexibility.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 毕竟，内置的`fit()`工作流程仅专注于**监督学习**：这是一种存在已知**目标**（也称为**标签**或**注释**）与您的输入数据相关联的设置，并且您将损失计算为目标函数和模型预测的函数。然而，并非所有形式的机器学习都属于这一类别。还有其他设置，其中不存在显式的目标，例如**生成学习**（我们将在第16章中介绍），**自监督学习**（目标从输入中获取）或**强化学习**（学习由偶尔的“奖励”驱动——就像训练狗一样）。即使您正在进行常规的监督学习，作为研究人员，您可能希望添加一些需要低级灵活性的新颖功能。
- en: 'Whenever you find yourself in a situation where the built-in `fit()` is not
    enough, you will need to write your own custom training logic. You’ve already
    seen simple examples of low-level training loops in chapters 2 and 3. As a reminder,
    the contents of a typical training loop look like this:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 无论何时你发现自己处于内置的`fit()`不足以应对的情况，你将需要编写自己的自定义训练逻辑。你已经在第2章和第3章中看到了低级训练循环的简单示例。作为提醒，典型训练循环的内容如下：
- en: Run the “forward pass” (compute the model’s output) to obtain a loss value for
    the current batch of data.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行“前向传递”（计算模型的输出）以获得当前批次数据的损失值。
- en: Retrieve the gradients of the loss with regard to the model’s weights.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检索关于模型权重的损失梯度。
- en: Update the model’s weights so as to lower the loss value on the current batch
    of data.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新模型的权重，以降低当前批次数据的损失值。
- en: These steps are repeated for as many batches as necessary. This is essentially
    what `fit()` does under the hood. In this section, you will learn to reimplement
    `fit()` from scratch, which will give you all the knowledge you need to write
    any training algorithm you may come up with.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤会重复进行，直到达到必要的批次数量。这本质上就是`fit()`在底层所做的事情。在本节中，你将学习从头开始重新实现`fit()`，这将为你提供编写任何可能想到的训练算法所需的所有知识。
- en: Let’s go over the details. Throughout the next few sections, you’ll work your
    way up to writing a fully featured custom training loop in TensorFlow, PyTorch,
    and JAX.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细了解一下。在接下来的几节中，你将逐步学习如何在TensorFlow、PyTorch和JAX中编写一个功能齐全的自定义训练循环。
- en: Training vs. inference
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练与推理
- en: In the low-level training loop examples you’ve seen so far, step 1 (the forward
    pass) was done via `predictions = model(inputs)`, and step 2 (retrieving the gradients
    computed by the gradient tape) was done via a backend-specific API, such as
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在你迄今为止看到的低级训练循环示例中，步骤1（前向传递）是通过`predictions = model(inputs)`完成的，而步骤2（通过后端特定的API检索由梯度记录计算出的梯度）是通过以下方式完成的：
- en: '`gradients = tape.gradient(loss, model.weights)` in TensorFlow'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow中的`gradients = tape.gradient(loss, model.weights)`
- en: '`loss.backward()` in PyTorch'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch中的`loss.backward()`
- en: '`jax.value_and_grad()` in JAX'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: JAX中的`jax.value_and_grad()`
- en: In the general case, there are actually two subtleties you need to take into
    account.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般情况中，实际上有两个细微之处你需要考虑。
- en: Some Keras layers, such as the `Dropout` layer, have different behaviors during
    *training* and during *inference* (when you use them to generate predictions).
    Such layers expose a `training` Boolean argument in their `call()` method. Calling
    `dropout(inputs, training=True)` will drop some activation entries, while calling
    `dropout(inputs, training=False)` does nothing. By extension, Functional models
    and Sequential models also expose this `training` argument in their `call()` methods.
    Remember to pass `training=True` when you call a Keras model during the forward
    pass! Our forward pass thus becomes `predictions = model(inputs, training=True)`.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 一些Keras层，如`Dropout`层，在*训练*期间和*推理*期间（当你使用它们生成预测时）有不同的行为。这些层在其`call()`方法中暴露一个`training`布尔参数。调用`dropout(inputs,
    training=True)`将丢弃一些激活条目，而调用`dropout(inputs, training=False)`则不执行任何操作。通过扩展，Functional模型和Sequential模型也在它们的`call()`方法中暴露了这个`training`参数。记住，在正向传递时调用Keras模型时，要传递`training=True`！因此，我们的正向传递变为`predictions
    = model(inputs, training=True)`。
- en: 'In addition, note that when you retrieve the gradients of the weights of your
    model, you should not use `model.weights`, but rather `model.trainable_weights`.
    Indeed, layers and models own two kinds of weights:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，当你检索模型权重的梯度时，不应使用`model.weights`，而应使用`model.trainable_weights`。确实，层和模型拥有两种类型的权重：
- en: '*Trainable weights*, meant to be updated via backpropagation to minimize the
    loss of the model, such as the kernel and bias of a `Dense` layer.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可训练权重*，意味着通过反向传播来最小化模型损失，例如`Dense`层的核和偏置。'
- en: '*Non-trainable weights*, which are meant to be updated during the forward pass
    by the layers that own them. For instance, if you wanted a custom layer to keep
    a counter of how many batches it has processed so far, that information would
    be stored in a non-trainable weight, and at each batch, your layer would increment
    the counter by one.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不可训练权重*，意味着在正向传递过程中由拥有它们的层进行更新。例如，如果你想让一个自定义层保持一个计数器，记录它已经处理了多少批次，那么这个信息将存储在一个不可训练的权重中，并且在每个批次中，你的层将计数器增加一。'
- en: Among Keras built-in layers, the only layer that features non-trainable weights
    is the `BatchNormalization` layer, which we will introduce in chapter 9. The `BatchNormalization`
    layer needs non-trainable weights to track information about the mean and standard
    deviation of the data that passes through it, so as to perform an online approximation
    of *feature normalization* (a concept you’ve learned about in chapters 4 and 6).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 内置层中，唯一具有非可训练权重的层是 `BatchNormalization` 层，我们将在第 9 章中介绍。`BatchNormalization`
    层需要非可训练的权重来跟踪通过它的数据的均值和标准差信息，以便执行 *特征归一化*（你在第 4 章和第 6 章中学到的概念）的在线近似。
- en: Writing custom training step functions
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写自定义训练步骤函数
- en: 'Taking into account these two details, a supervised learning training step
    ends up looking like this in pseudocode:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这两个细节，监督学习训练步骤在伪代码中看起来是这样的：
- en: '[PRE42]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This snippet is pseudocode rather than real code because it includes an imaginary
    function, `get_gradients_of()`. In reality, retrieving gradients is done in a
    way that is specific to your current backend — JAX, TensorFlow, or PyTorch.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码是伪代码而不是真实代码，因为它包含了一个想象中的函数，`get_gradients_of()`。实际上，检索梯度是以你当前后端（JAX、TensorFlow
    或 PyTorch）特有的方式完成的。
- en: Let’s use what you learned about each framework in chapter 3 to implement a
    real version of this `train_step()` function. We’ll start with TensorFlow and
    PyTorch because these two make the job relatively easy, so they’re a good place
    to start. We’ll end with JAX, which is quite a bit more complex.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用第 3 章中关于每个框架的知识来实现这个 `train_step()` 函数的真正版本。我们将从 TensorFlow 和 PyTorch 开始，因为这两个框架使这项工作相对容易，所以这是一个好的起点。我们将以
    JAX 结尾，它相当复杂。
- en: A TensorFlow training step function
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: TensorFlow 训练步骤函数
- en: 'TensorFlow lets you write code that looks pretty much like our pseudocode snippet.
    The only difference is that your forward pass should take place inside a `GradientTape`
    scope. You can then use the `tape` object to retrieve the gradients:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 允许你编写看起来几乎与我们的伪代码片段相同的代码。唯一的区别是，你的前向传播应该在 `GradientTape` 范围内进行。然后你可以使用
    `tape` 对象来检索梯度：
- en: '[PRE43]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Let’s run it for a single step:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一个单步操作：
- en: '[PRE44]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Easy enough! Let’s do PyTorch next.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 很简单！让我们接下来做 PyTorch。
- en: A PyTorch training step function
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: PyTorch 训练步骤函数
- en: 'When you use the PyTorch backend, all of your Keras layers and models inherit
    from the PyTorch `torch.nn.Module` class and expose the native `Module` API. As
    a result, your model, its trainable weights, and your loss tensor are all aware
    of each other and interact via three methods: `loss.backward()`, `weight.value.grad`,
    and `model.zero_grad()`.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用 PyTorch 后端时，所有的 Keras 层和模型都继承自 PyTorch 的 `torch.nn.Module` 类，并暴露了原生的 `Module`
    API。因此，你的模型、其可训练的权重以及你的损失张量都彼此了解，并通过三种方法进行交互：`loss.backward()`、`weight.value.grad`
    和 `model.zero_grad()`。
- en: 'As a reminder from chapter 3, the mental model you’ve got to keep in mind is
    this:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第 3 章的提醒，你必须记住的思考模式是这样的：
- en: With each forward pass, PyTorch builds up a one-time computation graph that
    keeps track of the computation that just happened.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每次前向传播时，PyTorch 都会构建一个一次性的计算图，该图跟踪刚刚发生的计算。
- en: Calling `.backward()` on any given scalar node of this graph (like your loss)
    will run the graph backward starting from that node, automatically populating
    a `tensor.grad` attribute on all tensors involved (if they satisfy `requires_grad=True`),
    containing the gradient of the output node with respect to that tensor. In particular,
    it will populate the `grad` attribute of your trainable parameters.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在此图的任何给定标量节点（如你的损失）上调用 `.backward()` 将从该节点开始运行图的反向传播，自动在所有涉及的张量上填充 `tensor.grad`
    属性（如果它们满足 `requires_grad=True`），包含输出节点相对于该张量的梯度。特别是，它将填充你的可训练参数的 `grad` 属性。
- en: 'To clear the contents of that `tensor.grad` attribute, you should call `tensor.grad
    = None` on all your tensors. Because it would be a bit cumbersome to do this on
    all model variables individually, you can just do it at the model level via `model.zero_grad()`
    — the `zero_grad()` call will propagate to all variables tracked by the model.
    Clearing gradients is critical because calls to `backward()` are additive: if
    you don’t clear the gradients at each step, the gradient values will accumulate
    and training won’t proceed.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要清除 `tensor.grad` 属性的内容，你应该在所有你的张量上调用 `tensor.grad = None`。因为逐个对模型变量执行此操作可能会有些繁琐，你可以在模型级别通过
    `model.zero_grad()` 来完成——`zero_grad()` 调用将传播到模型跟踪的所有变量。清除梯度是至关重要的，因为 `backward()`
    调用是累加的：如果你不在每一步清除梯度，梯度值将累积，训练将无法进行。
- en: 'Let’s chain these steps:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们链接这些步骤：
- en: '[PRE45]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let’s run it for a single step:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一个单步操作：
- en: '[PRE46]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: That wasn’t too difficult! Now, let’s move on to JAX.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不太难！现在，让我们继续学习JAX。
- en: A JAX training step function
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一个JAX训练步骤函数
- en: When it comes to low-level training code, JAX tends to be the most complex of
    the three backends because of its fully stateless nature. Statelessness makes
    JAX highly performant and scalable, making it amenable to compilation and automatic
    performance optimizations. However, writing stateless code requires you to jump
    through some hoops.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到低级训练代码时，由于其完全无状态的性质，JAX通常是三个后端中最复杂的。无状态使得JAX高度性能和可扩展，使其适合编译和自动性能优化。然而，编写无状态代码需要你跳过一些障碍。
- en: Since the gradient function is obtained via metaprogramming, you first need
    to define the function that returns your loss. Further, this function needs to
    be stateless, so it needs to take as arguments all the variables it’s going to
    be using, and it needs to return the value of any variable it has updated. Remember
    those non-trainable weights that can get modified during the forward pass? Those
    are the variables we need to return.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 由于梯度函数是通过元编程获得的，因此你首先需要定义一个返回你的损失函数。此外，这个函数需要是无状态的，因此它需要接受所有它将要使用的变量作为参数，并且它需要返回任何它已更新的变量的值。还记得那些在正向传播过程中可以修改的非训练权重吗？这些就是我们需要的变量。
- en: 'To make it easier to work with the stateless programming paradigm of JAX, Keras
    models make available a stateless forward pass method: the `stateless_call()`
    method. It behaves just like `__call__`, except that'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使它更容易与JAX的无状态编程范式一起工作，Keras模型提供了一个无状态的前向传递方法：`stateless_call()` 方法。它的行为就像
    `__call__`，除了
- en: It takes as input the model’s trainable weights and non-trainable weights, in
    addition to the `inputs` and `training` arguments.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它接受模型的训练权重和非训练权重作为输入，以及 `inputs` 和 `training` 参数。
- en: It returns the model’s updated non-trainable weights, in addition to the model’s
    outputs.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它除了返回模型的输出外，还返回模型更新的非训练权重。
- en: 'It works like this:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 它的工作原理是这样的：
- en: '[PRE47]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can use `stateless_call()` to implement our JAX loss function. Since the
    loss function also computes updates for all non-trainable variables, we name it
    `compute_loss_and_updates()`:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `stateless_call()` 来实现我们的JAX损失函数。由于损失函数还计算所有非训练变量的更新，我们将其命名为 `compute_loss_and_updates()`：
- en: '[PRE48]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Once we have this `compute_loss_and_updates()` function, we can pass it to
    `jax.value_and_grad` to obtain the gradient computation:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了这个 `compute_loss_and_updates()` 函数，我们就可以将其传递给 `jax.value_and_grad` 来获得梯度计算：
- en: '[PRE49]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Now, there’s just a small problem. Both `jax.grad()` and `jax.value_and_grad()`
    require `fn` to return a scalar value only. Our `compute_loss_and_updates()` function
    returns a scalar value as its first output, but it also returns the new value
    for the non-trainable weights. Remember what you learned in chapter 3? The solution
    is to pass a `has_aux` argument to `grad()` or `value_and_grad()`, like this:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，有一个小问题。`jax.grad()` 和 `jax.value_and_grad()` 都需要 `fn` 只返回一个标量值。我们的 `compute_loss_and_updates()`
    函数作为其第一个输出返回一个标量值，但它还返回非训练权重的新的值。还记得你在第3章中学到的吗？解决方案是将 `has_aux` 参数传递给 `grad()`
    或 `value_and_grad()`，如下所示：
- en: '[PRE50]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You would use it like this:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 你会这样使用它：
- en: '[PRE51]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Okay, that was a lot of JAXiness. But now we’ve got almost everything we need
    to assemble our JAX training step. We just need the last piece of the puzzle:
    `optimizer.apply()`.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这有很多JAX特性。但现在我们几乎已经拥有了组装JAX训练步骤所需的一切。我们只需要最后一部分：`optimizer.apply()`。
- en: 'When you wrote your first basic training step in TensorFlow at the beginning
    of chapter 2, you wrote an update step function that looked like this:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在第2章开始时编写你的第一个基本训练步骤时，你编写了一个看起来像这样的更新步骤函数：
- en: '[PRE52]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: This corresponds to what the optimizer `keras.optimizers.SGD` would do. However,
    every other optimizer in the Keras API is somewhat more complex than that and
    keeps track of auxiliary variables that help speed up training — in particular,
    most optimizers use some form of *momentum*, which you learned about in chapter
    2\. These extra variables get updated at each step of training, and in the JAX
    world, that means that you need to get your hands on a stateless function that
    takes these variables as arguments and returns their new value.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这对应于优化器 `keras.optimizers.SGD` 会执行的操作。然而，Keras API中的每个其他优化器都比这复杂一些，并跟踪有助于加速训练的辅助变量——特别是，大多数优化器使用某种形式的
    *动量*，这在第2章中你已经学到了。这些额外的变量在训练的每一步都会更新，在JAX的世界里，这意味着你需要得到一个无状态函数，这些变量作为参数，并返回它们的新值。
- en: 'To make this easy, Keras makes available the `stateless_apply()` method on
    all optimizers. It works like this:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这变得容易，Keras 在所有优化器上提供了 `stateless_apply()` 方法。它的工作方式如下：
- en: '[PRE53]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, we have enough to assemble an end-to-end training step:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经有了组装端到端训练步骤所需的一切：
- en: '[PRE54]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let’s run it for a single step:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一个单步操作：
- en: '[PRE55]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: It’s definitely a bit more work than TensorFlow and PyTorch, but the speed and
    scalability benefits of JAX more than make up for it.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实比 TensorFlow 和 PyTorch 要多做一些工作，但 JAX 的速度和可扩展性优势足以弥补这一点。
- en: 'Next, let’s take a look at another important element of a custom training loop:
    *metrics*.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看自定义训练循环的另一个重要元素：*指标*。
- en: Low-level usage of metrics
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指标的高级使用
- en: 'In a low-level training loop, you will probably want to use Keras metrics (whether
    custom ones or the built-in ones). You’ve already learned about the metrics API:
    simply call `update_state(y_true, y_pred)` for each batch of targets and predictions,
    and then use `result()` to query the current metric value:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在低级训练循环中，你可能想使用 Keras 指标（无论是自定义的还是内置的）。你已经了解了指标 API：只需为每个目标批次和预测批次调用 `update_state(y_true,
    y_pred)`，然后使用 `result()` 查询当前指标值：
- en: '[PRE56]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'You may also need to track the average of a scalar value, such as the model’s
    loss. You can do this via the `keras.metrics.Mean` metric:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还需要跟踪标量值的平均值，例如模型的损失。你可以通过 `keras.metrics.Mean` 指标来完成这项工作：
- en: '[PRE57]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Remember to use `metric.reset_state()` when you want to reset the current results
    (at the start of a training epoch or at the start of evaluation).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在你想重置当前结果时（在训练周期的开始或评估的开始）使用 `metric.reset_state()`。
- en: 'Now, if you’re using JAX, state-modifying methods like `update_state()` or
    `reset()` can’t be used inside a stateless function. Instead, you can use the
    stateless metrics API, which is similar to the `model.stateless_call()` and `optimizer.stateless_apply()`
    methods you’ve already learned about. Here’s how it works:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果你使用 JAX，则不能在无状态函数内部使用修改状态的方法，如 `update_state()` 或 `reset()`。相反，你可以使用无状态指标
    API，这与你已经了解的 `model.stateless_call()` 和 `optimizer.stateless_apply()` 方法类似。以下是它的工作方式：
- en: '[PRE58]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Using fit() with a custom training loop
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 fit() 和自定义训练循环
- en: In the previous sections, we were writing our own training logic entirely from
    scratch. Doing so provides you with the most flexibility, but you end up writing
    a lot of code, while simultaneously missing out on many convenient features of
    `fit()`, such as callbacks, performance optimizations, or built-in support for
    distributed training.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们完全从头开始编写自己的训练逻辑。这样做为你提供了最大的灵活性，但最终你会编写大量的代码，同时也会错过 `fit()` 的许多便利功能，例如回调、性能优化或内置的分布式训练支持。
- en: 'What if you need a custom training algorithm, but you still want to use the
    power of the built-in Keras training loop? There’s actually a middle ground between
    `fit()` and a training loop written from scratch: you can provide a custom training
    step function and let the framework do the rest.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要一个自定义训练算法，但仍然想使用内置 Keras 训练循环的强大功能，那么在 `fit()` 和从头开始编写的训练循环之间实际上有一个折衷方案：你可以提供一个自定义训练步骤函数，让框架完成其余的工作。
- en: You can do this by overriding the `train_step()` method of the `Model` class.
    This is the function that is called by `fit()` for every batch of data. You will
    then be able to call `fit()` as usual — and it will be running your own learning
    algorithm under the hood.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过重写 `Model` 类的 `train_step()` 方法来实现这一点。这是 `fit()` 为每个数据批次调用的函数。然后你将能够像往常一样调用
    `fit()`——并且它将在底层运行你自己的学习算法。
- en: 'Here’s how it works:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的工作方式：
- en: Create a new class that subclasses `keras.Model`.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个新的类，该类继承自 `keras.Model`。
- en: Override the `train_step()` method. Its contents are nearly identical to what
    we used in the previous section.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重写 `train_step()` 方法。其内容几乎与我们之前使用的内容相同。
- en: Return a dictionary mapping metric names (including the loss) to their current
    value.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回一个字典，将指标名称（包括损失）映射到它们的当前值。
- en: 'Note the following:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 注意以下事项：
- en: This pattern does not prevent you from building models with the Functional API.
    You can do this whether you’re building `Sequential` models, Functional API models,
    or subclassed models.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种模式不会阻止你使用功能 API 构建模型。无论你是构建 `Sequential` 模型、功能 API 模型还是子类化模型，你都可以这样做。
- en: You don’t need to use a `@tf.function` or `@jax.jit` decorator when you override
    `train_step()` — the framework does it for you.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你重写 `train_step()` 方法时，不需要使用 `@tf.function` 或 `@jax.jit` 装饰器——框架会为你完成这项工作。
- en: Customizing fit() with TensorFlow
  id: totrans-298
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 自定义 fit()
- en: 'Let’s start by coding a custom TensorFlow train step:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从编写自定义 TensorFlow 训练步骤开始：
- en: '[PRE59]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[Listing 7.21](#listing-7-21): Customizing `fit()`: TensorFlow version'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表7.21](#listing-7-21)：自定义`fit()`：TensorFlow版本'
- en: We can now instantiate our custom model, compile it (we only pass the optimizer,
    since the loss is already defined outside of the model), and train it using `fit()`
    as usual.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以实例化我们的自定义模型，像往常一样使用`fit()`进行编译（我们只传递优化器，因为损失已经在模型外部定义了），并使用`fit()`进行训练。
- en: 'Let’s put the model definition in its own reusable function:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将模型定义放在一个可重用的函数中：
- en: '[PRE60]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Let’s give it a whirl:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试：
- en: '[PRE61]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Customizing fit() with PyTorch
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用PyTorch自定义fit()
- en: 'Next, the PyTorch version:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是PyTorch版本：
- en: '[PRE62]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Let’s try it:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试：
- en: '[PRE63]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Customizing fit() with JAX
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用JAX自定义fit()
- en: 'Finally, let’s write the JAX version. First we need to define a `compute_loss_and_updates()`
    method, similar to the `compute_loss_and_updates()` function we used in our custom
    training step example:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们编写JAX版本。首先，我们需要定义一个`compute_loss_and_updates()`方法，类似于我们在自定义训练步骤示例中使用的`compute_loss_and_updates()`函数：
- en: '[PRE64]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Note we aren’t computing a moving average of the loss like we did for the other
    two backends. Instead we just return the per-batch loss value, which is less useful.
    We do this to simplify metric state management in the example: the code would
    get very verbose if we included it (you will learn about metric management in
    the next section):'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们不像在其他两个后端中那样计算损失的移动平均值。相反，我们只返回每个批次的损失值，这不太有用。我们这样做是为了简化示例中的指标状态管理：如果包括它，代码会变得非常冗长（你将在下一节中了解指标管理）：
- en: '[PRE65]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Let’s try it out:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试：
- en: '[PRE66]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Handling metrics in a custom train_step()
  id: totrans-319
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在自定义train_step()中处理指标
- en: Finally, what about the `loss` and `metrics` that you can pass to `compile()`?
    After you’ve called `compile()`, you get access to
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，关于您可以传递给`compile()`的`loss`和`metrics`是什么？在您调用`compile()`之后，您将获得访问权限
- en: '`self.compute_loss` — This combines the loss function you passed to `compile()`
    together with regularization losses that may be added by certain layers.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.compute_loss` — 这将您传递给`compile()`的损失函数与可能由某些层添加的正则化损失结合起来。'
- en: '`self.metrics` — The list of metrics you passed to `compile()`. Note that it
    also includes a metric that tracks the loss.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`self.metrics` — 您传递给`compile()`的指标列表。请注意，它还包括一个跟踪损失的指标。'
- en: train_step() metrics handling with TensorFlow
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用TensorFlow处理train_step()指标
- en: 'Here’s what it looks like with TensorFlow:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用TensorFlow时的样子：
- en: '[PRE67]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Let’s try it:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试：
- en: '[PRE68]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: train_step() metrics handling with PyTorch
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用PyTorch处理train_step()指标
- en: And here’s what it looks like with PyTorch — it’s exactly the same code change!
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用PyTorch时的样子——代码更改完全相同！
- en: '[PRE69]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Let’s see how it runs:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是如何运行的：
- en: '[PRE70]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: train_step() metrics handling with JAX
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用JAX处理train_step()指标
- en: 'Finally, here’s what it looks like with JAX. To start with, you can use `compute_loss()`
    in your `compute_loss_and_updates()` method to hit the loss passed to `compile()`:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这是使用JAX时的样子。首先，您可以在`compute_loss_and_updates()`方法中使用`compute_loss()`来获取传递给`compile()`的损失：
- en: '[PRE71]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Next up: metric management. As usual, it’s a tad more complicated due to JAX’s
    statelessness requirements:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来：指标管理。由于JAX的无状态要求，这通常要复杂一些：
- en: '[PRE72]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: That was a lot of information, but by now you know enough to use Keras to do
    almost anything!
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然信息量很大，但到目前为止，您已经知道足够多的知识来使用Keras做几乎所有的事情！
- en: Summary
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: Keras offers a spectrum of different workflows, based on the principle of *progressive
    disclosure of complexity*. They all smoothly interoperate.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras提供了一系列不同的工作流程，基于*渐进式复杂性披露*的原则。它们都可以无缝互操作。
- en: You can build models via the `Sequential` class, via the Functional API, or
    by subclassing the `Model` class. Most of the time, you’ll be using the Functional
    API.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过`Sequential`类、功能API或通过子类化`Model`类来构建模型。大多数情况下，您将使用功能API。
- en: The simplest way to train and evaluate a model is via the default `fit()` and
    `evaluate()` methods.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和评估模型的最简单方法是使用默认的`fit()`和`evaluate()`方法。
- en: Keras callbacks provide a simple way to monitor models during your call to `fit()`
    and automatically take action based on the state of the model.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keras回调提供了一种简单的方法来在调用`fit()`期间监控模型，并根据模型的状态自动采取行动。
- en: You can also fully control what `fit()` does by overriding the `train_step()`
    method, using APIs from your backend of choice — JAX, TensorFlow, or PyTorch.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您也可以通过重写`train_step()`方法，使用您选择的后端API（JAX、TensorFlow或PyTorch）来完全控制`fit()`函数的行为。
- en: Beyond `fit()`, you can also write your own training loops entirely from scratch,
    in a backend-native way. This is useful for researchers implementing brand-new
    training algorithms.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了`fit()`之外，您还可以从头开始完全编写自己的训练循环，以原生后端的方式。这对于实施全新训练算法的研究人员来说很有用。
