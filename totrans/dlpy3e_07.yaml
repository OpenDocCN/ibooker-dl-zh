- en: A deep dive on Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter07_deep-dive-keras](https://deeplearningwithpython.io/chapters/chapter07_deep-dive-keras)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: You’re starting to have some amount of experience with Keras. You’re familiar
    with the `Sequential` model, `Dense` layers, and built-in APIs for training, evaluation,
    and inference — `compile()`, `fit()`, `evaluate()`, and `predict()`. You’ve even
    learned in chapter 3 how to inherit from the `Layer` class to create custom layers,
    and how to use the gradient APIs in TensorFlow, JAX and PyTorch to implement a
    step-by-step training loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the coming chapters, we’ll dig into computer vision, timeseries forecasting,
    natural language processing, and generative deep learning. These complex applications
    will require much more than a `Sequential` architecture and the default `fit()`
    loop. So let’s first turn you into a Keras expert! In this chapter, you’ll get
    a complete overview of the key ways to work with Keras APIs: everything you’re
    going to need to handle the advanced deep learning use cases you’ll encounter
    next.'
  prefs: []
  type: TYPE_NORMAL
- en: A spectrum of workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The design of the Keras API is guided by the principle of *progressive disclosure
    of complexity*: make it easy to get started, yet make it possible to handle high-complexity
    use cases, only requiring incremental learning at each step. Simple use cases
    should be easy and approachable, and arbitrarily advanced workflows should be
    *possible*: no matter how niche and complex the thing you want to do, there should
    be a clear path to it, a path that builds upon the various things you’ve learned
    from simpler workflows. This means that you can grow from beginner to expert and
    still use the same tools — only in different ways.'
  prefs: []
  type: TYPE_NORMAL
- en: As such, there’s not a single “true” way of using Keras. Rather, Keras offers
    a *spectrum of workflows*, from the very simple to the very flexible. There are
    different ways to build Keras models, and different ways to train them, answering
    different needs.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, you have a range of ways to build models and an array of ways
    to train them, each representing a certain tradeoff between usability and flexibility.
    You could be using Keras like you would use scikit-learn — just calling `fit()`
    and letting the framework do its thing — or you could be using it like NumPy —
    taking full control of every little detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because all these workflows are based on shared APIs, such as `Layer` and `Model`,
    components from any workflow can be used in any other workflow: they can all talk
    to each other. This means that everything you’re learning now as you’re getting
    started will still be relevant once you’ve become an expert. You can get started
    easily and then gradually dive into workflows where you’re writing more and more
    logic from scratch. You won’t have to switch to an entirely different framework
    as you go from student to researcher, or from data scientist to deep learning
    engineer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This philosophy is not unlike that of Python itself! Some languages only offer
    one way to write programs — for instance, object-oriented programming or functional
    programming. Meanwhile, Python is a multiparadigm language: it offers a range
    of possible usage patterns, which all work nicely together. This makes Python
    suitable for a wide range of very different use cases: system administration,
    data science, machine learning engineering, web development, or just learning
    how to program. Likewise, you can think of Keras as the Python of deep learning:
    a user-friendly deep learning language that offers a variety of workflows for
    different user profiles.'
  prefs: []
  type: TYPE_NORMAL
- en: Different ways to build Keras models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three APIs for building models in Keras, as shown in figure 7.1:'
  prefs: []
  type: TYPE_NORMAL
- en: The *Sequential model* is the most approachable API — it’s basically a Python
    list. As such, it’s limited to simple stacks of layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The *Functional API*, which focuses on graph-like model architectures. It represents
    a nice mid-point between usability and flexibility, and as such, it’s the most
    commonly used model-building API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model subclassing*, a low-level option where you write everything yourself
    from scratch. This is ideal if you want full control over every little thing.
    However, you won’t get access to many built-in Keras features, and you will be
    more at risk of making mistakes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/ee1d6c9fc1dddc087edee3cc5b26d384.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 7.1](#figure-7-1): Progressive disclosure of complexity for model building'
  prefs: []
  type: TYPE_NORMAL
- en: The Sequential model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest way to build a Keras model is the `Sequential` model, which you
    already know about.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.1](#listing-7-1): The `Sequential` class'
  prefs: []
  type: TYPE_NORMAL
- en: Note that it’s possible to build the same model incrementally via the `add()`
    method, similar to the `append()` method of a Python list.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.2](#listing-7-2): Incrementally building a `Sequential` model'
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ve seen in chapter 3 that layers only get built (which is to say, create
    their weights) when they are called for the first time. That’s because the shape
    of the layers’ weights depends on the shape of their input: until the input shape
    is known, they can’t be created.'
  prefs: []
  type: TYPE_NORMAL
- en: As such, the previous `Sequential` model does not have any weights until you
    actually call it on some data, or call its `build()` method with an input shape.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.3](#listing-7-3): Models that aren’t yet built have no weights'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.4](#listing-7-4): Calling a model for the first time to build it'
  prefs: []
  type: TYPE_NORMAL
- en: After the model is built, you can display its contents via the `summary()` method,
    which comes in handy for debugging.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.5](#listing-7-5): The summary method'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, your model happens to be named `sequential_1`. You can actually
    give names to everything in Keras — every model, every layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.6](#listing-7-6): Naming models and layers with the `name` argument'
  prefs: []
  type: TYPE_NORMAL
- en: 'When building a `Sequential` model incrementally, it’s useful to be able to
    print a summary of what the current model looks like after you add each layer.
    But you can’t print a summary until the model is built! There’s actually a way
    to have your `Sequential` model get built on the fly: just declare the shape of
    the model’s inputs in advance. You can do this via the `Input` class.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.7](#listing-7-7): Specifying the input shape of your model in advance'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you can use `summary()` to follow how the output shape of your model changes
    as you add more layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This is a pretty common debugging workflow when dealing with layers that transform
    their inputs in complex ways, such as the convolutional layers you’ll learn about
    in chapter 8.
  prefs: []
  type: TYPE_NORMAL
- en: The Functional API
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `Sequential` model is easy to use, but its applicability is extremely limited:
    it can only express models with a single input and a single output, applying one
    layer after the other in a sequential fashion. In practice, it’s pretty common
    to encounter models with multiple inputs (say, an image and its metadata), multiple
    outputs (different things you want to predict about the data), or a nonlinear
    topology.'
  prefs: []
  type: TYPE_NORMAL
- en: In such cases, you’d build your model using the Functional API. This is what
    most Keras models you’ll encounter in the wild use. It’s fun and powerful — it
    feels like playing with LEGO bricks.
  prefs: []
  type: TYPE_NORMAL
- en: A simple example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s start with something simple: the two-layer stack we used in the previous
    section. Its Functional API version looks like the following listing.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.8](#listing-7-8): A simple Functional model with two `Dense` layers'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go over this step by step. We started by declaring an `Input` (note that
    you can also give names to these input objects, like everything else):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This `inputs` object holds information about the shape and `dtype` of the data
    that the model will process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We call such an object a *symbolic tensor*. It doesn’t contain any actual data,
    but it encodes the specifications of the actual tensors of data that the model
    will see when you use it. It *stands for* future tensors of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we created a layer and called it on the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'All Keras layers can be called both on real tensors of data or on these symbolic
    tensors. In the latter case, they return a new symbolic tensor, with updated shape
    and dtype information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After obtaining the final outputs, we instantiated the model by specifying
    its inputs and outputs in the `Model` constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s the summary of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Multi-input, multi-output models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unlike this toy model, most deep learning models don’t look like lists — they
    look like graphs. They may, for instance, have multiple inputs or multiple outputs.
    It’s for this kind of model that the Functional API really shines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say you’re building a system to rank customer support tickets by priority
    and route them to the appropriate department. Your model has three inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: The title of the ticket (text input)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The text body of the ticket (text input)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any tags added by the user (categorical input, assumed here to be multi-hot
    encoded)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can encode the text inputs as arrays of 1s and 0s of size `vocabulary_size`
    (see chapter 14 for detailed information about text encoding techniques).
  prefs: []
  type: TYPE_NORMAL
- en: 'Your model also has two outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: The priority score of the ticket, a scalar between 0 and 1 (sigmoid output)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The department that should handle the ticket (a softmax over the set of departments)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can build this model in a few lines with the Functional API.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.9](#listing-7-9): A multi-input, multi-output Functional model'
  prefs: []
  type: TYPE_NORMAL
- en: The Functional API is a simple, LEGO-like, yet very flexible way to define arbitrary
    graphs of layers like these.
  prefs: []
  type: TYPE_NORMAL
- en: Training a multi-input, multi-output model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can train your model in much the same way as you would train a `Sequential`
    model, by calling `fit()` with lists of input and output data. These lists of
    data should respect the same order as the inputs you passed to the `Model()` constructor.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.10](#listing-7-10): Training a model by providing lists of input
    and target arrays'
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t want to rely on input order (for instance, because you have many
    inputs or outputs), you can also use the names you gave to the `Input` objects
    and to the output layers, and pass data via dictionaries.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.11](#listing-7-11): Training a model by providing dicts of input
    and target arrays'
  prefs: []
  type: TYPE_NORMAL
- en: 'The power of the Functional API: Access to layer connectivity'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A Functional model is an explicit graph data structure. This makes it possible
    to *inspect how layers are connected* and *reuse previous graph nodes* (which
    are layer outputs) as part of new models. It also nicely fits the “mental model”
    that most researchers use when thinking about a deep neural network: a graph of
    layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This enables two important use cases: model visualization and feature extraction.
    Let’s take a look.'
  prefs: []
  type: TYPE_NORMAL
- en: Plotting layer connectivity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s visualize the connectivity of the model we just defined (the *topology*
    of the model). You can plot a Functional model as a graph with the `plot_model()`
    utility, as shown in figure 7.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/8c2cc58914d82bedb18b51124f7a37be.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 7.2](#figure-7-2): Plot generated by `plot_model()` on our ticket classifier
    model'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can add to this plot the input and output shapes of each layer in the model,
    as well as layer names (rather than just layer types), which can be helpful during
    debugging (figure 7.3):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/aa99a9385808a3053cbc09450b3cb3f9.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 7.3](#figure-7-3): Model plot with shape information added'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `None` in the tensor shapes represents the batch size: this model allows
    batches of any size.'
  prefs: []
  type: TYPE_NORMAL
- en: Feature extraction with a Functional model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Access to layer connectivity also means that you can inspect and reuse individual
    nodes (layer calls) in the graph. The model property `model.layers` provides the
    list of layers that make up the model, and for each layer, you can query `layer.input`
    and `layer.output`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.12](#listing-7-12): Retrieving the inputs or outputs of a layer
    in a Functional model'
  prefs: []
  type: TYPE_NORMAL
- en: 'This enables you to do *feature extraction*: creating models that reuse intermediate
    features from another model.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s say you want to add another output to the model we previously defined
    — you want to also predict an estimate of how long a given issue ticket will take
    to resolve, a kind of difficulty rating. You could do this via a classification
    layer over three categories — “quick,” “medium,” and “difficult.” You don’t need
    to recreate and retrain a model from scratch! You can just start from the intermediate
    features of your previous model, since you have access to them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.13](#listing-7-13): Creating a new model by reusing intermediate
    layer outputs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s plot our new model, as shown in figure 7.4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/3af1086d9c8bdfa1368caff0b43fd622.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 7.4](#figure-7-4): Plot of our new model'
  prefs: []
  type: TYPE_NORMAL
- en: Subclassing the Model class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The last model-building pattern you should know about is the most advanced
    one: `Model` subclassing. You’ve already learned in chapter 3 how to subclass
    the `Layer` class to create custom layers. Subclassing `Model` is pretty similar:'
  prefs: []
  type: TYPE_NORMAL
- en: In the `__init__` method, define the layers the model will use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the `call` method, define the forward pass of the model, reusing the layers
    previously created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiate your subclass and call it on data to create its weights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rewriting our previous example as a subclassed model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s take a look at a simple example: we will reimplement the customer support
    ticket management model using a `Model` subclass.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.14](#listing-7-14): A simple subclassed model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve defined the model, you can instantiate it. Note that it will only
    create its weights the first time you call it on some data — much like `Layer`
    subclasses:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, everything looks very similar to `Layer` subclassing, a workflow you’ve
    already encountered in chapter 3\. What, then, is the difference between a `Layer`
    subclass and a `Model` subclass? It’s simple: a *layer* is a building block you
    use to create models, and a *model* is the top-level object that you will actually
    train, export for inference, etc. In short, a `Model` has a `fit()`, `evaluate()`,
    and `predict()` method. Layers don’t. Other than that, the two classes are virtually
    identical (another difference is that you can *save* a model to a file on disk
    — which we will cover in a few sections).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can compile and train a `Model` subclass just like a Sequential or Functional
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Model` subclassing workflow is the most flexible way to build a model:
    it enables you to build models that cannot be expressed as directed acyclic graphs
    of layers — imagine, for instance, a model where the `call()` method uses layers
    inside a `for` loop, or even calls them recursively. Anything is possible — you’re
    in charge.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Beware: What subclassed models don’t support'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This freedom comes at a cost: with subclassed models, you are responsible for
    more of the model logic, which means your potential error surface is much larger.
    As a result, you will have more debugging work to do. You are developing a new
    Python object, not just snapping together LEGO bricks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Functional and subclassed models are also substantially different in nature:
    a Functional model is an explicit data structure — a graph of layers, which you
    can view, inspect, and modify. Meanwhile, a subclassed model is a piece of bytecode
    — a Python class with a `call()` method that contains raw code. This is the source
    of the subclassing workflow’s flexibility — you can just code up whatever functionality
    you like — but it introduces new limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, because the way layers are connected to each other is hidden inside
    the body of the `call()` method, you cannot access that information. Calling `summary()`
    will not display layer connectivity, and you cannot plot the model topology via
    `plot_model()`. Likewise, if you have a subclassed model, you cannot access the
    nodes of the graph of layers to do feature extraction — because there is simply
    no graph. Once the model is instantiated, its forward pass becomes a complete
    black box.
  prefs: []
  type: TYPE_NORMAL
- en: Mixing and matching different components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Crucially, choosing one of these patterns — the `Sequential` model, the Functional
    API, `Model` subclassing — does not lock you out of the others. All models in
    the Keras API can smoothly interoperate with each other, whether they’re Sequential
    models, Functional models, or subclassed models written from scratch. They’re
    all part of the same spectrum of workflows. For instance, you can use a subclassed
    layer or model in a Functional model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.15](#listing-7-15): Creating a Functional model that includes a
    subclassed model'
  prefs: []
  type: TYPE_NORMAL
- en: Inversely, you can use a Functional model as part of a subclassed layer or model.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.16](#listing-7-16): Creating a subclassed model that includes a
    Functional model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember: Use the right tool for the job'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You’ve learned about the spectrum of workflows for building Keras models, from
    the simplest workflow — the `Sequential` model — to the most advanced one, model
    subclassing. When should you use one over the other? Each one has its pros and
    cons — pick the one most suitable for the job at hand.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the Functional API provides you with a pretty good tradeoff between
    ease of use and flexibility. It also gives you direct access to layer connectivity,
    which is very powerful for use cases such as model plotting or feature extraction.
    If you *can* use the Functional API — that is, if your model can be expressed
    as a directed acyclic graph of layers — we recommend using it over model subclassing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Going forward, all examples in this book will use the Functional API — simply
    because all of the models we will work with are expressible as graphs of layers.
    We will, however, make frequent use of subclassed layers. In general, using Functional
    models that include subclassed layers provides the best of both worlds: high development
    flexibility while retaining the advantages of the Functional API.'
  prefs: []
  type: TYPE_NORMAL
- en: Using built-in training and evaluation loops
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The principle of progressive disclosure of complexity — access to a spectrum
    of workflows that go from dead easy to arbitrarily flexible, one step at a time
    — also applies to model training. Keras provides you with different workflows
    for training models — it can be as simple as calling `fit()` on your data or as
    advanced as writing a new training algorithm from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: You are already familiar with the `compile()`, `fit()`, `evaluate()`, `predict()`
    workflow. As a reminder, it looks like the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.17](#listing-7-17): The standard workflow: `compile()`, `fit()`,
    `evaluate()`, `predict()`'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of ways you can customize this simple workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: By providing your own custom metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By passing *callbacks* to the `fit()` method to schedule actions to be taken
    at specific points during training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s take a look at these.
  prefs: []
  type: TYPE_NORMAL
- en: Writing your own metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Metrics are key to measuring the performance of your model — in particular,
    to measure the difference between its performance on the training data and its
    performance on the test data. Commonly used metrics for classification and regression
    are already part of the built-in `keras.metrics` module — most of the time, that’s
    what you will use. But if you’re doing anything out of the ordinary, you will
    need to be able to write your own metrics. It’s simple!
  prefs: []
  type: TYPE_NORMAL
- en: A Keras metric is a subclass of the `keras.metrics.Metric` class. Similarly
    to layers, a metric has an internal state stored in Keras variables. Unlike layers,
    these variables aren’t updated via backpropagation, so you have to write the state
    update logic yourself — which happens in the `update_state()` method. For example,
    here’s a simple custom metric that measures the root mean squared error (RMSE).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.18](#listing-7-18): Implementing a custom metric by subclassing
    the `Metric` class'
  prefs: []
  type: TYPE_NORMAL
- en: 'You use the `result()` method to return the current value of the metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Meanwhile, you also need to expose a way to reset the metric state without
    having to reinstantiate it — this enables the same metric objects to be used across
    different epochs of training or across both training and evaluation. You do this
    in the `reset_state()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Custom metrics can be used just like built-in ones. Let’s test-drive our own
    metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: You can now see the `fit()` progress bar display the RMSE of your model.
  prefs: []
  type: TYPE_NORMAL
- en: Using callbacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Launching a training run on a large dataset for tens of epochs using `model.fit()`
    can be a bit like launching a paper airplane: past the initial impulse, you don’t
    have any control over its trajectory or its landing spot. If you want to avoid
    bad outcomes (and thus wasted paper airplanes), it’s smarter to use, not a paper
    plane, but a drone that can sense its environment, send data back to its operator,
    and automatically make steering decisions based on its current state. The Keras
    *callbacks* API will help you transform your call to `model.fit()` from a paper
    airplane into a smart, autonomous drone that can self-introspect and dynamically
    take action.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A *callback* is an object (a class instance implementing specific methods)
    that is passed to the model in the call to `fit()` and that is called by the model
    at various points during training. It has access to all the available data about
    the state of the model and its performance, and it can take action: interrupt
    training, save a model, load a different weight set, or otherwise alter the state
    of the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples of ways you can use callbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Model checkpointing* — Saving the current state of the model at different
    points during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Early stopping* — Interrupting training when the validation loss is no longer
    improving (and of course, saving the best model obtained during training).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dynamically adjusting the value of certain parameters during training* — Such
    as the learning rate of the optimizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Logging training and validation metrics during training, or visualizing the
    representations learned by the model as they’re updated* — The `fit()` progress
    bar that you’re familiar with is in fact a callback!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `keras.callbacks` module includes a number of built-in callbacks (this
    is not an exhaustive list):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s review two of them to give you an idea of how to use them: `EarlyStopping`
    and `ModelCheckpoint`.'
  prefs: []
  type: TYPE_NORMAL
- en: The EarlyStopping and ModelCheckpoint callbacks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When you’re training a model, there are many things you can’t predict at the
    start. In particular, you can’t tell how many epochs will be needed to get to
    an optimal validation loss. Our examples so far have adopted the strategy of training
    for enough epochs that you begin overfitting, using the first run to figure out
    the optimal number of epochs, and then finally launching a new training run from
    scratch using this optimal number. Of course, this approach is wasteful. A much
    better way to handle this is to stop training when you measure that the validation
    loss is no longer improving. This can be achieved using the `EarlyStopping` callback.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `EarlyStopping` callback interrupts training once a target metric being
    monitored has stopped improving for a fixed number of epochs. For instance, this
    callback allows you to interrupt training as soon as you start overfitting, thus
    avoiding having to retrain your model for a smaller number of epochs. This callback
    is typically used in combination with `ModelCheckpoint`, which lets you continually
    save the model during training (and, optionally, save only the current best model
    so far: the version of the model that achieved the best performance at the end
    of an epoch).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.19](#listing-7-19): Using the `callbacks` argument in the `fit()`
    method'
  prefs: []
  type: TYPE_NORMAL
- en: Note that you can always save models manually after training as well — just
    call `model.save("my_checkpoint_path.keras")`. To reload the model you’ve saved,
    use
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Writing your own callbacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you need to take a specific action during training that isn’t covered by
    one of the built-in callbacks, you can write your own callback. Callbacks are
    implemented by subclassing the class `keras.callbacks.Callback`. You can then
    implement any number of the following transparently named methods, which are called
    at various points during training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'These methods are all called with a `logs` argument, which is a dictionary
    containing information about the previous batch, epoch, or training run: training
    and validation metrics, and so on. The `on_epoch_*` and `on_batch_*` methods also
    take the epoch or batch index as first argument (an integer).'
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a simple example callback that saves a list of per-batch loss values
    during training and plots these values at the end of each epoch.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.20](#listing-7-20): Creating a custom callback by subclassing the
    `Callback` class'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s test-drive it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: We get plots that look like figure 7.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/65e8a00724c46a5848204de4a36d1301.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 7.5](#figure-7-5): The output of our custom history-plotting callback'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring and visualization with TensorBoard
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To do good research or develop good models, you need rich, frequent feedback
    about what’s going on inside your models during your experiments. That’s the point
    of running experiments: to get information about how well a model performs — as
    much information as possible. Making progress is an iterative process, a loop:
    you start with an idea and express it as an experiment, attempting to validate
    or invalidate your idea. You run this experiment and process the information it
    generates, as shown in figure 7.6. This inspires your next idea. The more iterations
    of this loop you’re able to run, the more refined and powerful your ideas become.
    Keras helps you go from idea to experiment in the least possible time, and fast
    GPUs can help you get from experiment to result as quickly as possible. But what
    about processing the experiment results? That’s where TensorBoard comes in.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/09e0e7e7f7a9f3f5ce986f95921f2f9a.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 7.6](#figure-7-6): The loop of progress'
  prefs: []
  type: TYPE_NORMAL
- en: TensorBoard is a browser-based application that you can run locally. It’s the
    best way to monitor everything that goes on inside your model during training.
    With TensorBoard, you can
  prefs: []
  type: TYPE_NORMAL
- en: Visually monitor metrics during training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize your model architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualize histograms of activations and gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore embeddings in 3D
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’re monitoring more information than just the model’s final loss, you
    can develop a clearer vision of what the model does and doesn’t do, and you can
    make progress more quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to use TensorBoard with a Keras model and the `fit()` method
    is the `keras.callbacks.TensorBoard` callback. In the simplest case, just specify
    where you want the callback to write logs, and you’re good to go:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model starts running, it will write logs at the target location. If
    you are running your Python script on a local machine, you can then launch the
    local TensorBoard server using the following command (note that the `tensorboard`
    executable should already be available if you have installed TensorFlow via `pip`;
    if not, you can install TensorBoard manually via `pip install tensorboard`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: You can then navigate to the URL that the command returns to access the TensorBoard
    interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are running your script in a Colab notebook, you can run an embedded
    TensorBoard instance as part of your notebook, using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: In the TensorBoard interface, you will be able to monitor live graphs of your
    training and evaluation metrics, as shown in figure 7.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/b3ca45ce23be41a15a383f54209b78fc.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 7.7](#figure-7-7): TensorBoard can be used for easy monitoring of training
    and evaluation metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: Writing your own training and evaluation loops
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `fit()` workflow strikes a nice balance between ease of use and flexibility.
    It’s what you will use most of the time. However, it isn’t meant to support everything
    a deep learning researcher may want to do — even with custom metrics, custom losses,
    and custom callbacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'After all, the built-in `fit()` workflow is solely focused on *supervised learning*:
    a setup where there are known *targets* (also called *labels* or *annotations*)
    associated with your input data and where you compute your loss as a function
    of these targets and the model’s predictions. However, not every form of machine
    learning falls into this category. There are other setups where no explicit targets
    are present, such as *generative learning* (which we will introduce in chapter
    16), *self-supervised learning* (where targets are obtained from the inputs),
    or *reinforcement learning* (where learning is driven by occasional “rewards”
    — much like training a dog). And even if you’re doing regular supervised learning,
    as a researcher, you may want to add some novel bells and whistles that require
    low-level flexibility.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whenever you find yourself in a situation where the built-in `fit()` is not
    enough, you will need to write your own custom training logic. You’ve already
    seen simple examples of low-level training loops in chapters 2 and 3. As a reminder,
    the contents of a typical training loop look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the “forward pass” (compute the model’s output) to obtain a loss value for
    the current batch of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve the gradients of the loss with regard to the model’s weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the model’s weights so as to lower the loss value on the current batch
    of data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These steps are repeated for as many batches as necessary. This is essentially
    what `fit()` does under the hood. In this section, you will learn to reimplement
    `fit()` from scratch, which will give you all the knowledge you need to write
    any training algorithm you may come up with.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go over the details. Throughout the next few sections, you’ll work your
    way up to writing a fully featured custom training loop in TensorFlow, PyTorch,
    and JAX.
  prefs: []
  type: TYPE_NORMAL
- en: Training vs. inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the low-level training loop examples you’ve seen so far, step 1 (the forward
    pass) was done via `predictions = model(inputs)`, and step 2 (retrieving the gradients
    computed by the gradient tape) was done via a backend-specific API, such as
  prefs: []
  type: TYPE_NORMAL
- en: '`gradients = tape.gradient(loss, model.weights)` in TensorFlow'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`loss.backward()` in PyTorch'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`jax.value_and_grad()` in JAX'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the general case, there are actually two subtleties you need to take into
    account.
  prefs: []
  type: TYPE_NORMAL
- en: Some Keras layers, such as the `Dropout` layer, have different behaviors during
    *training* and during *inference* (when you use them to generate predictions).
    Such layers expose a `training` Boolean argument in their `call()` method. Calling
    `dropout(inputs, training=True)` will drop some activation entries, while calling
    `dropout(inputs, training=False)` does nothing. By extension, Functional models
    and Sequential models also expose this `training` argument in their `call()` methods.
    Remember to pass `training=True` when you call a Keras model during the forward
    pass! Our forward pass thus becomes `predictions = model(inputs, training=True)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, note that when you retrieve the gradients of the weights of your
    model, you should not use `model.weights`, but rather `model.trainable_weights`.
    Indeed, layers and models own two kinds of weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Trainable weights*, meant to be updated via backpropagation to minimize the
    loss of the model, such as the kernel and bias of a `Dense` layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Non-trainable weights*, which are meant to be updated during the forward pass
    by the layers that own them. For instance, if you wanted a custom layer to keep
    a counter of how many batches it has processed so far, that information would
    be stored in a non-trainable weight, and at each batch, your layer would increment
    the counter by one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among Keras built-in layers, the only layer that features non-trainable weights
    is the `BatchNormalization` layer, which we will introduce in chapter 9. The `BatchNormalization`
    layer needs non-trainable weights to track information about the mean and standard
    deviation of the data that passes through it, so as to perform an online approximation
    of *feature normalization* (a concept you’ve learned about in chapters 4 and 6).
  prefs: []
  type: TYPE_NORMAL
- en: Writing custom training step functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Taking into account these two details, a supervised learning training step
    ends up looking like this in pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This snippet is pseudocode rather than real code because it includes an imaginary
    function, `get_gradients_of()`. In reality, retrieving gradients is done in a
    way that is specific to your current backend — JAX, TensorFlow, or PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use what you learned about each framework in chapter 3 to implement a
    real version of this `train_step()` function. We’ll start with TensorFlow and
    PyTorch because these two make the job relatively easy, so they’re a good place
    to start. We’ll end with JAX, which is quite a bit more complex.
  prefs: []
  type: TYPE_NORMAL
- en: A TensorFlow training step function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'TensorFlow lets you write code that looks pretty much like our pseudocode snippet.
    The only difference is that your forward pass should take place inside a `GradientTape`
    scope. You can then use the `tape` object to retrieve the gradients:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s run it for a single step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Easy enough! Let’s do PyTorch next.
  prefs: []
  type: TYPE_NORMAL
- en: A PyTorch training step function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'When you use the PyTorch backend, all of your Keras layers and models inherit
    from the PyTorch `torch.nn.Module` class and expose the native `Module` API. As
    a result, your model, its trainable weights, and your loss tensor are all aware
    of each other and interact via three methods: `loss.backward()`, `weight.value.grad`,
    and `model.zero_grad()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder from chapter 3, the mental model you’ve got to keep in mind is
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: With each forward pass, PyTorch builds up a one-time computation graph that
    keeps track of the computation that just happened.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calling `.backward()` on any given scalar node of this graph (like your loss)
    will run the graph backward starting from that node, automatically populating
    a `tensor.grad` attribute on all tensors involved (if they satisfy `requires_grad=True`),
    containing the gradient of the output node with respect to that tensor. In particular,
    it will populate the `grad` attribute of your trainable parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To clear the contents of that `tensor.grad` attribute, you should call `tensor.grad
    = None` on all your tensors. Because it would be a bit cumbersome to do this on
    all model variables individually, you can just do it at the model level via `model.zero_grad()`
    — the `zero_grad()` call will propagate to all variables tracked by the model.
    Clearing gradients is critical because calls to `backward()` are additive: if
    you don’t clear the gradients at each step, the gradient values will accumulate
    and training won’t proceed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s chain these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s run it for a single step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: That wasn’t too difficult! Now, let’s move on to JAX.
  prefs: []
  type: TYPE_NORMAL
- en: A JAX training step function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When it comes to low-level training code, JAX tends to be the most complex of
    the three backends because of its fully stateless nature. Statelessness makes
    JAX highly performant and scalable, making it amenable to compilation and automatic
    performance optimizations. However, writing stateless code requires you to jump
    through some hoops.
  prefs: []
  type: TYPE_NORMAL
- en: Since the gradient function is obtained via metaprogramming, you first need
    to define the function that returns your loss. Further, this function needs to
    be stateless, so it needs to take as arguments all the variables it’s going to
    be using, and it needs to return the value of any variable it has updated. Remember
    those non-trainable weights that can get modified during the forward pass? Those
    are the variables we need to return.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make it easier to work with the stateless programming paradigm of JAX, Keras
    models make available a stateless forward pass method: the `stateless_call()`
    method. It behaves just like `__call__`, except that'
  prefs: []
  type: TYPE_NORMAL
- en: It takes as input the model’s trainable weights and non-trainable weights, in
    addition to the `inputs` and `training` arguments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It returns the model’s updated non-trainable weights, in addition to the model’s
    outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It works like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use `stateless_call()` to implement our JAX loss function. Since the
    loss function also computes updates for all non-trainable variables, we name it
    `compute_loss_and_updates()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have this `compute_loss_and_updates()` function, we can pass it to
    `jax.value_and_grad` to obtain the gradient computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, there’s just a small problem. Both `jax.grad()` and `jax.value_and_grad()`
    require `fn` to return a scalar value only. Our `compute_loss_and_updates()` function
    returns a scalar value as its first output, but it also returns the new value
    for the non-trainable weights. Remember what you learned in chapter 3? The solution
    is to pass a `has_aux` argument to `grad()` or `value_and_grad()`, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'You would use it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Okay, that was a lot of JAXiness. But now we’ve got almost everything we need
    to assemble our JAX training step. We just need the last piece of the puzzle:
    `optimizer.apply()`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you wrote your first basic training step in TensorFlow at the beginning
    of chapter 2, you wrote an update step function that looked like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: This corresponds to what the optimizer `keras.optimizers.SGD` would do. However,
    every other optimizer in the Keras API is somewhat more complex than that and
    keeps track of auxiliary variables that help speed up training — in particular,
    most optimizers use some form of *momentum*, which you learned about in chapter
    2\. These extra variables get updated at each step of training, and in the JAX
    world, that means that you need to get your hands on a stateless function that
    takes these variables as arguments and returns their new value.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this easy, Keras makes available the `stateless_apply()` method on
    all optimizers. It works like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have enough to assemble an end-to-end training step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s run it for a single step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: It’s definitely a bit more work than TensorFlow and PyTorch, but the speed and
    scalability benefits of JAX more than make up for it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s take a look at another important element of a custom training loop:
    *metrics*.'
  prefs: []
  type: TYPE_NORMAL
- en: Low-level usage of metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In a low-level training loop, you will probably want to use Keras metrics (whether
    custom ones or the built-in ones). You’ve already learned about the metrics API:
    simply call `update_state(y_true, y_pred)` for each batch of targets and predictions,
    and then use `result()` to query the current metric value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'You may also need to track the average of a scalar value, such as the model’s
    loss. You can do this via the `keras.metrics.Mean` metric:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Remember to use `metric.reset_state()` when you want to reset the current results
    (at the start of a training epoch or at the start of evaluation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if you’re using JAX, state-modifying methods like `update_state()` or
    `reset()` can’t be used inside a stateless function. Instead, you can use the
    stateless metrics API, which is similar to the `model.stateless_call()` and `optimizer.stateless_apply()`
    methods you’ve already learned about. Here’s how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: Using fit() with a custom training loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous sections, we were writing our own training logic entirely from
    scratch. Doing so provides you with the most flexibility, but you end up writing
    a lot of code, while simultaneously missing out on many convenient features of
    `fit()`, such as callbacks, performance optimizations, or built-in support for
    distributed training.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if you need a custom training algorithm, but you still want to use the
    power of the built-in Keras training loop? There’s actually a middle ground between
    `fit()` and a training loop written from scratch: you can provide a custom training
    step function and let the framework do the rest.'
  prefs: []
  type: TYPE_NORMAL
- en: You can do this by overriding the `train_step()` method of the `Model` class.
    This is the function that is called by `fit()` for every batch of data. You will
    then be able to call `fit()` as usual — and it will be running your own learning
    algorithm under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new class that subclasses `keras.Model`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Override the `train_step()` method. Its contents are nearly identical to what
    we used in the previous section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return a dictionary mapping metric names (including the loss) to their current
    value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: This pattern does not prevent you from building models with the Functional API.
    You can do this whether you’re building `Sequential` models, Functional API models,
    or subclassed models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You don’t need to use a `@tf.function` or `@jax.jit` decorator when you override
    `train_step()` — the framework does it for you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customizing fit() with TensorFlow
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s start by coding a custom TensorFlow train step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[Listing 7.21](#listing-7-21): Customizing `fit()`: TensorFlow version'
  prefs: []
  type: TYPE_NORMAL
- en: We can now instantiate our custom model, compile it (we only pass the optimizer,
    since the loss is already defined outside of the model), and train it using `fit()`
    as usual.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s put the model definition in its own reusable function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s give it a whirl:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Customizing fit() with PyTorch
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Next, the PyTorch version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s try it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Customizing fit() with JAX
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Finally, let’s write the JAX version. First we need to define a `compute_loss_and_updates()`
    method, similar to the `compute_loss_and_updates()` function we used in our custom
    training step example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Note we aren’t computing a moving average of the loss like we did for the other
    two backends. Instead we just return the per-batch loss value, which is less useful.
    We do this to simplify metric state management in the example: the code would
    get very verbose if we included it (you will learn about metric management in
    the next section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s try it out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Handling metrics in a custom train_step()
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, what about the `loss` and `metrics` that you can pass to `compile()`?
    After you’ve called `compile()`, you get access to
  prefs: []
  type: TYPE_NORMAL
- en: '`self.compute_loss` — This combines the loss function you passed to `compile()`
    together with regularization losses that may be added by certain layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self.metrics` — The list of metrics you passed to `compile()`. Note that it
    also includes a metric that tracks the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: train_step() metrics handling with TensorFlow
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here’s what it looks like with TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s try it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: train_step() metrics handling with PyTorch
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: And here’s what it looks like with PyTorch — it’s exactly the same code change!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see how it runs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: train_step() metrics handling with JAX
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Finally, here’s what it looks like with JAX. To start with, you can use `compute_loss()`
    in your `compute_loss_and_updates()` method to hit the loss passed to `compile()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up: metric management. As usual, it’s a tad more complicated due to JAX’s
    statelessness requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: That was a lot of information, but by now you know enough to use Keras to do
    almost anything!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Keras offers a spectrum of different workflows, based on the principle of *progressive
    disclosure of complexity*. They all smoothly interoperate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can build models via the `Sequential` class, via the Functional API, or
    by subclassing the `Model` class. Most of the time, you’ll be using the Functional
    API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The simplest way to train and evaluate a model is via the default `fit()` and
    `evaluate()` methods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras callbacks provide a simple way to monitor models during your call to `fit()`
    and automatically take action based on the state of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also fully control what `fit()` does by overriding the `train_step()`
    method, using APIs from your backend of choice — JAX, TensorFlow, or PyTorch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beyond `fit()`, you can also write your own training loops entirely from scratch,
    in a backend-native way. This is useful for researchers implementing brand-new
    training algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
