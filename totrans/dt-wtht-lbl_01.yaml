- en: 1 Introduction to machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 机器学习简介
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: An introduction to data, types of datasets, quality, and sources
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据、数据集类型、质量和来源的介绍
- en: Machine learning and types of machine learning algorithms
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习和机器学习算法类型
- en: An overview of different types of algorithms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同类型算法的概述
- en: There are only patterns, patterns on top of patterns, patterns that affect other
    patterns. Patterns hidden by patterns. Patterns within patterns.—Chuck Palahniuk
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 只存在模式，模式之上的模式，影响其他模式的模式。被模式隐藏的模式。模式中的模式。——查克·帕拉纽克
- en: 'There is a saying going around: “Data is the new electricity.” Data is indeed
    transforming our world, much like electricity has; nobody can deny that. But like
    electricity, we must remember that data must be properly harnessed to utilize
    its value. We have to clean the data and analyze and visualize it, and only then
    can we develop insights from it. The fields of data science, machine learning
    (ML), and AI are helping us to better harness data and extract trends and patterns
    so we can make more insightful and balanced decisions in our activities and business.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种说法在流传：“数据是新的电力。”数据确实正在改变我们的世界，就像电力一样；没有人能否认这一点。但就像电力一样，我们必须记住，数据必须得到适当的利用才能发挥其价值。我们必须清理数据、分析和可视化它，然后才能从中发展出洞察力。数据科学、机器学习（ML）和人工智能领域正在帮助我们更好地利用数据，提取趋势和模式，以便我们能在我们的活动和业务中做出更深入和平衡的决策。
- en: In this book, we unravel the puzzles of data and see how we can find the patterns
    hidden within. We will be studying a branch of ML referred to as *unsupervised
    learning*. Unsupervised learning solutions are one of the most influential approaches
    and are changing the face of the industry. They are utilized in banking and finance,
    retail, insurance, manufacturing, aviation, medical sciences, telecom, and almost
    every other sector.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将解开数据的谜团，看看我们如何找到隐藏在其中的模式。我们将研究被称为*无监督学习*的机器学习的一个分支。无监督学习解决方案是最有影响力的方法之一，正在改变行业的面貌。它们被用于银行和金融、零售、保险、制造、航空、医学科学、电信以及几乎每个其他行业。
- en: Throughout the book, we discuss concepts of ML with a focus on unsupervised
    learning—the building blocks of algorithms, their nuts and bolts, background processes,
    and mathematical foundation. We will examine concepts, study best practices, analyze
    common errors and pitfalls, and use a case study–based approach that complements
    the learning. At the same time, we develop actual Python code for solving such
    problems. All the codes are accompanied by step-by-step explanations and comments.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们将侧重于无监督学习来讨论机器学习的概念——算法的构建块、它们的细节、后台过程和数学基础。我们将检查概念、研究最佳实践、分析常见错误和陷阱，并采用基于案例研究的方法来补充学习。同时，我们将开发实际的Python代码来解决这些问题。所有代码都附有逐步解释和注释。
- en: By the time you finish this book, you will have a very good understanding of
    unsupervised technique-based ML, various algorithms, the mathematics and statistical
    foundation on which the algorithm rests, business use cases, Python implementation,
    and best practices.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 当你完成这本书时，你将对基于无监督技术的机器学习、各种算法、算法所依赖的数学和统计基础、商业用例、Python实现和最佳实践有一个非常好的理解。
- en: 'This first chapter is designed to introduce the concepts of ML. We’ll begin
    by discussing the concepts fundamental to all data analysis and ML: data itself,
    how it is managed, and what constitutes good-quality data. We’ll then move on
    to discuss data analysis in the context of ML and deep learning, consider different
    types of ML algorithms, and wrap up by considering the technical toolkit recommended
    for getting hands-on with the content in this book. Welcome to the first chapter
    and all the very best!'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这第一章旨在介绍机器学习的概念。我们将从讨论所有数据分析与机器学习的基础概念开始：数据本身、数据的管理方式以及构成高质量数据的要素。然后，我们将转向在机器学习和深度学习的背景下讨论数据分析，考虑不同类型的机器学习算法，并以考虑为实际操作本书内容所推荐的工具集来结束。欢迎来到第一章，并祝您一切顺利！
- en: 1.1 Technical toolkit
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 技术工具集
- en: 'The following tools are used for different facets of the project:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下工具用于项目的不同方面：
- en: '*Data engineering*—Hadoop, Spark, Scala, Java, C++, SQL, Redshift, Azure, PySpark'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据工程*——Hadoop、Spark、Scala、Java、C++、SQL、Redshift、Azure、PySpark'
- en: '*Data analysis*—SQL, R, Python, Excel'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据分析*——SQL、R、Python、Excel'
- en: '*ML*—SQL, R, Python, Excel, Weka, Julia, MATLAB, SPSS, SAS'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ML*——SQL、R、Python、Excel、Weka、Julia、MATLAB、SPSS、SAS'
- en: '*Visualization*—Tableau, Power BI, Qlik, COGNOS'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可视化*——Tableau、Power BI、Qlik、COGNOS'
- en: '*Model deployment*—Docker, Flask, Amazon S3'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型部署*—Docker、Flask、Amazon S3'
- en: '*Cloud services*—Azure, AWS, GCP'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*云服务*—Azure、AWS、GCP'
- en: In this book, we are going to use Python. You are advised to install the latest
    version of Python on your system. At least version 3.5+ is advisable, though the
    latest version as of this writing is 3.13\. We will also use Jupyter Notebook,
    so installing Anaconda on your system is advisable.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将使用Python。建议你在系统上安装Python的最新版本。至少建议使用3.5+版本，尽管截至本书写作时的最新版本是3.13。我们还将使用Jupyter
    Notebook，因此建议你在系统上安装Anaconda。
- en: 'Note  All the codes and datasets will be checked in at the GitHub repository:
    [https://github.com/vverdhan/DataWithoutLabels](https://github.com/vverdhan/DataWithoutLabels).
    You are expected to replicate them and try to reproduce the results.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：所有代码和数据集都将存放在GitHub仓库中：[https://github.com/vverdhan/DataWithoutLabels](https://github.com/vverdhan/DataWithoutLabels)。你被期望复制它们并尝试重现结果。
- en: 1.2 Data, data types, data management, and quality
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 数据、数据类型、数据管理和质量
- en: 'We begin by introducing the protagonist of this book: *data*. Data can be thought
    of as facts and statistics that are collected for performing any kind of analysis
    or study. But data also has its own traits, attributes, quality measures, and
    management principles. It is stored, exported, loaded, transformed, and measured.
    In that sense, data is a tangible “thing” in its own regard, and it must be handled
    properly to correctly utilize it. To do that, we must properly understand data.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍本书的主角：*数据*。数据可以被看作是为了执行任何类型的分析或研究而收集的事实和统计数据。但数据也有其自身的特性、属性、质量指标和管理原则。它被存储、导出、加载、转换和度量。从这个意义上说，数据在其自身范围内是一个有形的“东西”，并且必须妥善处理才能正确利用它。为了做到这一点，我们必须正确理解数据。
- en: 'Let’s start with the fundamentals: the definition of data. Once we’ve defined
    data, we will proceed to discuss different types of data, their respective examples,
    and the attributes of data that make it useful and of good quality.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从基础开始：数据的定义。一旦我们定义了数据，我们将继续讨论不同类型的数据、它们的相应示例以及使数据有用和高质量的数据属性。
- en: 1.2.1 What is data?
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 什么是数据？
- en: Data is ubiquitous. You make a phone call using a mobile network; as you do,
    you are generating data. You book a flight ticket and hotel for an upcoming vacation;
    data is being created. Our day-to-day activity-generated data might include performing
    a bank transaction, surfing social media, or shopping websites online. That data
    is transformed from one form to another, stored, cleaned, managed, and analyzed.
    So what actually is it?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 数据无处不在。你使用移动网络打电话；在这个过程中，你正在生成数据。你预订即将到来的假期机票和酒店；数据正在被创建。我们日常活动生成的数据可能包括进行银行交易、浏览社交媒体或在网上购物。这些数据从一种形式转换成另一种形式，被存储、清理、管理和分析。那么，它实际上是什么呢？
- en: Formally put, data is a collection of facts, observations, measures, text, numbers,
    images, and videos. A dataset might be clean (i.e., organized to be free from
    errors, inconsistencies, and irrelevant information) or unclean, be ordered (e.g.,
    alphabetically) or unordered, or have mixed data types or all one type. As mentioned,
    data in itself is not useful until we clean it, arrange it, analyze it, and draw
    insights from it. We can visualize the transition from raw to more useful forms
    in figure 1.1.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 正式来说，数据是一系列事实、观察、度量、文本、数字、图像和视频的集合。数据集可能是干净的（即，组织得没有错误、不一致和不相关信息）或不干净的，可能是有序的（例如，按字母顺序）或无序的，或者包含混合的数据类型或全部是同一类型。正如提到的，数据本身在没有经过清理、整理、分析和从中提取见解之前是没有用的。我们可以在图1.1中可视化从原始数据到更有用形式的转变。
- en: '![figure](../Images/CH01_F01_Verdhan.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH01_F01_Verdhan.png)'
- en: Figure 1.1 How we can transform raw data to become information, knowledge, and,
    finally, insights that can be used in business to drive decisions and actions
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.1 我们如何将原始数据转换为信息、知识，最终成为可用于商业以驱动决策和行动的见解
- en: Raw data is converted to information when we can find distinctions in it. When
    we relate the terms and “connect the dots,” the same piece of information becomes
    knowledge. Insight is the stage where we can find the major centers and significant
    points. An insight should be actionable, succinct, and direct. For example, if
    a customer retention team of a telecom operator is told that customers who do
    not make a call for nine days have a 30% higher chance of churn than those who
    make calls, this will be a useful insight that they can work on and try to resolve.
    Similarly, if a line technician in a manufacturing plant is informed that using
    mold X results in 60% more defects than using mold Y, they will refrain from using
    the poorly performing mold in the future. An insight is quite useful for a business
    team because they can consider it and take corrective measures.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们能在数据中找到区别时，原始数据就转化为信息。当我们关联术语并“连接点”时，同一块信息就变成了知识。洞察力是我们可以找到主要中心和重要点的阶段。一个洞察力应该是可操作的、简洁的、直接的。例如，如果一个电信运营商的客户保留团队被告知，九天未打电话的客户比打电话的客户有30%更高的流失率，这将是一个他们可以着手解决的有用洞察力。同样，如果一个制造工厂的线路技术人员被告知，使用模具X比使用模具Y导致60%更多的缺陷，他们将来将避免使用表现不佳的模具。洞察力对业务团队非常有用，因为他们可以考虑到它并采取纠正措施。
- en: 1.2.2 Various types of data
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 各种类型的数据
- en: As we’ve discussed, data is generated by much of our day-to-day activity. We
    can broadly classify that data into different *types*, as shown in figure 1.2.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，数据是由我们日常生活中的许多活动产生的。我们可以将这些数据大致分为不同的*类型*，如图1.2所示。
- en: '![figure](../Images/CH01_F02_Verdhan.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH01_F02_Verdhan.png)'
- en: Figure 1.2 The divisions and subdivisions of data
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.2 数据的划分和细分
- en: 'Data can be divided into quantitative and qualitative categories, which are
    further subclassified:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可以分为定量和定性类别，这些类别进一步细分为子类别：
- en: '*Qualitative data* is the data type that cannot be measured or weighed—for
    example, taste, color, odor, fitness, name, etc. They can only be observed subjectively.
    Formally put, when we categorize something or make a classification for it, the
    data generated is qualitative in nature. Examples are colors in a rainbow, cities
    in a country, quality of a product, gender, etc. They are also called *categorical*
    variables. Qualitative data can be further subcategorized into binary, nominal,
    and ordinal datasets:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*定性数据*是那种无法测量或称重的数据类型——例如，味道、颜色、气味、健康、名称等。它们只能主观地观察。正式地说，当我们对某物进行分类或为其做出分类时，产生的数据就是定性的。例如，彩虹中的颜色、一个国家中的城市、产品的质量、性别等。它们也被称为*分类变量*。定性数据可以进一步细分为二元、名义和有序数据集：'
- en: '*Binary* data, as the name suggests, has only two classes that are mutually
    exclusive to each other. Examples are yes/no, dry/wet, hard/soft, good/bad, true/false,
    etc.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*二元数据*，正如其名所示，只有两个相互排斥的类别。例如，是/否、干/湿、硬/软、好/坏、真/假等。'
- en: '*Nominal* data can be described as the type of data that, though categorized,
    does not have any sequence or order. Examples are distinct languages that are
    spoken in a country, colors in a rainbow, types of services available to a customer,
    cities in a country, etc.'
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*名义数据*可以描述为一种类型的数据，尽管被分类，但没有任何顺序或顺序。例如，一个国家中使用的不同语言、彩虹中的颜色、提供给客户的各类服务、一个国家中的城市等。'
- en: '*Ordinal* data is similar to nominal data, except we can order it in a sequence.
    Examples are fast/medium/slow, positive/neutral/negative, etc.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*有序数据*与名义数据相似，但我们可以在一个序列中对其进行排序。例如，快/中/慢、积极/中性/消极等。'
- en: '*Quantitative* data is all the types of data points that can be measured, weighed,
    scaled, recorded, etc. Examples are height, revenue, number of customers, demand
    quantity, area, volume, etc. They are the most common form of data and allow mathematical
    and statistical operations. Quantitative data is further subcategorized as discrete
    and continuous:'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*定量数据*是所有可以测量、称重、缩放、记录等的数据点类型。例如，身高、收入、客户数量、需求量、面积、体积等。它们是最常见的数据形式，允许进行数学和统计操作。定量数据进一步细分为离散和连续：'
- en: '*Discrete* data is precise, to the point, and represented as integers. For
    example, the number of passengers in a plane or the population of a city cannot
    be in decimals.'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*离散数据*是精确的、直接的，并以整数表示。例如，飞机上的乘客数量或一个城市的总人口不能是小数。'
- en: '*Continuous* data points can take any value, usually in a range. For example,
    height can take decimal values or the price of a product need not be an integer.'
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*连续*数据点可以取任何值，通常在一个范围内。例如，身高可以取小数值，或者产品的价格不必是整数。'
- en: Any data point will generally will fall into one of these classes, based on
    its properties. There is one more logical grouping that can be done using source
    and usage, which makes a lot of sense while solving business problems. This grouping
    allows us to design solutions customized to the data type.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 任何数据点通常都会根据其属性落入这些类别之一。还可以通过来源和使用方式进一步进行一种逻辑分组，这在解决商业问题时非常有意义。这种分组使我们能够设计出针对数据类型的定制化解决方案。
- en: 'Depending on the source and usage, we can also think of data in two broad classes:
    structured and unstructured data. A dataset that can be represented in a row-column
    structure easily is a *structured* dataset. For example, transactions made by
    five customers in a retail store can be stored, as shown in table 1.1.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 根据来源和使用方式，我们还可以将数据分为两大类：结构化数据和非结构化数据。可以轻松表示为行列结构的数据集是*结构化*数据集。例如，一家零售店五名客户的交易可以像表1.1所示那样存储。
- en: Table 1.1 An example of a structured dataset with attributes like amount, date,
    city, items, etc.
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表1.1 一个具有金额、日期、城市、项目等属性的示例结构化数据集。
- en: '| Customer ID | Transaction date | Amount ($) | No. of items | Payment mode
    | City |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 客户ID | 交易日期 | 金额（$） | 项目数量 | 支付方式 | 城市 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 1001  | 01-June-2024  | 100  | 5  | Cash  | New Delhi  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 1001  | 01-June-2024  | 100  | 5  | 现金  | 新德里  |'
- en: '| 1002  | 02-June-2024  | 101  | 6  | Card  | New York  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 1002  | 02-June-2024  | 101  | 6  | 卡  | 纽约  |'
- en: '| 1003  | 03-June-2024  | 102  | 7  | Card  | London  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 1003  | 03-June-2024  | 102  | 7  | 卡  | 伦敦  |'
- en: '| 1004  | 04-June-2024  | 103  | 8  | Cash  | Dublin  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 1004  | 04-June-2024  | 103  | 8  | 现金  | 都柏林  |'
- en: '| 1005  | 05-June-2024  | 104  | 9  | Cash  | Tokyo  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 1005  | 05-June-2024  | 104  | 9  | 现金  | 东京  |'
- en: In table 1.1, for each unique customer ID, we have the transaction date, the
    amount spent in dollars, the number of items purchased, the mode of payment, and
    the city in which the transaction was made. Such a data type can be extended to
    employee details, call records, banking transactions, etc.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在表1.1中，对于每个唯一的客户ID，我们有交易日期、花费的美元金额、购买的项目数量、支付方式和交易发生的城市。这种数据类型可以扩展到员工详情、通话记录、银行交易等。
- en: Note  Most of the data used in analysis and model building is structured. Structured
    data is easier to store, analyze, and visualize in the form of graphs and charts.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在分析和模型构建中使用的多数数据是有结构的。结构化数据更容易存储、分析和以图表和图形的形式可视化。
- en: 'Many algorithms and techniques cater to structured data—in normal real-world
    language, we refer to structured data primarily. Unstructured data is not easily
    sorted into a row-column structure. It can be text, audio, image, or video. Figure
    1.3 shows examples of unstructured data and their respective sources, as well
    as the primary types of unstructured data: text, images, audio, and video along
    with their examples.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 许多算法和技术针对结构化数据——在正常现实世界的语言中，我们主要指的是结构化数据。非结构化数据不容易排序到行列结构中。它可以是有文本、音频、图像或视频。图1.3显示了非结构化数据及其相应的来源，以及非结构化数据的主要类型：文本、图像、音频和视频及其示例。
- en: '![figure](../Images/CH01_F03_Verdhan.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH01_F03_Verdhan.png)'
- en: Figure 1.3 Unstructured data, along with its various types and examples. This
    data is usually complex to analyze and generally requires deep learning-based
    algorithms.
  id: totrans-56
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.3 非结构化数据及其各种类型和示例。这类数据通常复杂，通常需要基于深度学习的算法。
- en: Computers and processors understand only binary numbers. So these unstructured
    data points still need to be represented as numbers so that we can perform mathematical
    and statistical calculations on them. For example, an image is made up of pixels.
    If it is a colored image, each pixel will have RGB (red, green, blue) values and
    each RGB can take a value (0–255). Hence, we will be able to represent an image
    as a matrix on which further mathematical calculations can be made. Text, audio,
    and video can be represented similarly.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机和处理器只理解二进制数。因此，这些非结构化数据点仍然需要以数字的形式表示，以便我们可以在其上进行数学和统计计算。例如，一张图片由像素组成。如果是一张彩色图片，每个像素将具有RGB（红、绿、蓝）值，每个RGB可以取值（0–255）。因此，我们可以将图片表示为一个矩阵，可以在其上进行进一步的数学计算。文本、音频和视频可以类似地表示。
- en: Note  In general, deep learning-based solutions like convolutional neural networks
    (CNN) and recurrent neural networks (RNN) are used for unstructured data. We are
    going to work on text and explore CNN and RNN at a later stage in the book.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：一般来说，卷积神经网络（CNN）和循环神经网络（RNN）等基于深度学习的解决方案用于非结构化数据。我们将在本书的后续阶段研究文本，并探讨CNN和RNN。
- en: 'Unstructured data can be understood through an example: consider a picture
    of a vacuum cleaner, as shown in figure 1.4\. A portion of the image can be represented
    as a matrix and will look like the matrix seen in the figure. This example is
    only for illustration purposes and doesn’t show actual values.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个例子可以理解非结构化数据：考虑一个吸尘器的图片，如图1.4所示。图像的一部分可以表示为一个矩阵，看起来就像图中的矩阵。这个例子只是为了说明目的，并不显示实际值。
- en: '![figure](../Images/CH01_F04_Verdhan.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH01_F04_Verdhan.png)'
- en: Figure 1.4 An example of how unstructured data can be represented as a matrix
    to analyze. The matrix on the right is only an illustration and not the actual
    numbers.
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.4 如何将非结构化数据表示为矩阵以进行分析的示例。右侧的矩阵仅是示意，并非实际数字。
- en: Similarly, we can have representations of text, audio, or video data. Due to
    the size and large number of dimensions typically present in such data, this kind
    of unstructured data is complex to process and model, and hence, in general, deep
    learning-based models serve that purpose.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以有文本、音频或视频数据的表示。由于这类数据通常具有较大的尺寸和大量的维度，这种非结构化数据处理和建模比较复杂，因此，通常基于深度学习的模型可以满足这一需求。
- en: In addition to the broad types of data we’ve discussed so far, we can have more
    categories like ratios or scales, which can be used to define the relationship
    of one variable with another. All these data points (whether structured or unstructured)
    are defined by the way they are generated in real life.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们之前讨论的广泛类型的数据外，我们还可以有更多类别，如比率或刻度，可以用来定义一个变量与另一个变量的关系。所有这些数据点（无论是结构化还是非结构化）都是根据它们在现实生活中的生成方式来定义的。
- en: 'All of these data points have to be captured, stored, and managed. There are
    quite a few tools available for managing data, which we will discuss in due course.
    But before that, let’s examine one of the most crucial but often less talked about
    subjects: *data quality*.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些数据点都需要被捕捉、存储和管理。有许多工具可用于管理数据，我们将在适当的时候讨论。但在那之前，让我们检查一个最关键但通常讨论较少的主题：*数据质量*。
- en: 1.2.3 Data quality
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.3 数据质量
- en: “Garbage in, garbage out”—this principle summarizes the importance of good-quality
    data. If the data is dirty or incorrect and lacks any business relationship between
    variables, we will not be able to solve the business problem at hand. But what
    is the meaning of “good quality”? Imagine you want to predict rainfall this year
    based on last year’s daily rainfall measurements. A good-quality dataset for this
    task would be as complete as possible (very few missing days of rainfall measurements).
    It would be relevant and valid (e.g., covering the same local area as where you
    are making your predictions), the measurements would be accurate, and the data
    would be readily available for you to access and use without permission problems.
    A bad dataset, in contrast, might have lots of “holes” in the data, might have
    been taken in an area distant from the site you wish to study (making it less
    relevant), or might be difficult to access. As you can no doubt gather, good-quality
    data facilitates good-quality outputs, while bad data quality actively hinders
    your work and will likely result in a poor outcome. The major components of data
    quality are shown in figure 1.5\. Let’s explore them one by one.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: “垃圾输入，垃圾输出”——这一原则总结了高质量数据的重要性。如果数据是脏的或错误的，并且变量之间缺乏任何业务关系，我们就无法解决手头的业务问题。但“高质量”的含义是什么？想象一下，你想根据去年的每日降雨量预测今年的降雨量。为此任务，一个高质量的数据集应该是尽可能完整的（非常少的降雨量测量缺失日）。它应该是相关和有效的（例如，覆盖你预测的同一地区），测量应该是准确的，数据应该容易访问和使用，而无需处理权限问题。相反，一个差劲的数据集可能有很多“漏洞”，可能在远离你希望研究地点的地区进行测量（使其相关性降低），或者可能难以访问。正如你可以无疑地推断出的，高质量数据有助于产生高质量的结果，而差的数据质量会积极阻碍你的工作，并可能导致结果不佳。数据质量的主要组成部分如图1.5所示。让我们逐一探讨它们。
- en: '![figure](../Images/CH01_F05_Verdhan.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH01_F05_Verdhan.png)'
- en: Figure 1.5 Data quality is of paramount importance; attributes of good-quality
    data are shown.
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.5 数据质量至关重要；展示了高质量数据的属性。
- en: The major attributes of good-quality data are
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 良好质量数据的重大属性包括
- en: '*Completeness*—We would expect our dataset to be proper and not missing any
    values. For example, if we are working on sales data for a year, good data will
    have all the values for all 12 months. Then it will be a complete data source.
    The completeness of a dataset ensures that we are not missing an important variable
    or data point.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*完整性*—我们希望我们的数据集是完整的，不缺少任何值。例如，如果我们正在处理一年的销售数据，良好的数据将包含所有12个月的所有值。然后它将是一个完整的数据源。数据集的完整性确保我们没有遗漏重要的变量或数据点。'
- en: '*Validity*—The validity of data is its conformance to the properties, characteristics,
    and variations that are present and being analyzed in our use case. Validity indicates
    if the observation and measurement we have captured are reliable and valid. For
    example, if the scope of the study is for 2015–2019, then using 2014 data will
    be invalid.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*有效性*—数据的有效性是指其符合我们在用例中分析存在的属性、特征和变化。有效性表明我们捕获的观察和测量是否可靠和有效。例如，如果研究的范围是2015-2019年，那么使用2014年的数据将是无效的。'
- en: '*Accuracy*—Accuracy is an attribute focusing on the correctness of data. If
    we have inaccurate data, we will generate inaccurate insights, and actions will
    be faulty. It is a good practice to start the project by generating key performance
    indicators (KPIs) and comparing them with the numbers reported by the business
    to check the authenticity of the data available to us.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*准确性*—准确性是一个关注数据正确性的属性。如果我们有不准确的数据，我们将生成不准确的见解，行动将出错。一个好的做法是在项目开始时生成关键绩效指标（KPIs），并将它们与业务报告的数字进行比较，以检查我们可用的数据的真实性。'
- en: '*Representativeness*—This is one of the most important attributes of the data
    and often the most undermined. Representation of data means that the data in use
    truly captures the business need and is not biased. If the dataset is biased or
    is not representative enough, the model generated will not be able to make predictions
    on the new and unseen data, and the entire effort will go down the drain.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*代表性*—这是数据最重要的属性之一，通常也是最受忽视的。数据的代表性意味着正在使用的数据真正捕捉到业务需求，并且没有偏见。如果数据集有偏见或代表性不足，生成的模型将无法对新数据和未见数据做出预测，整个努力将付诸东流。'
- en: '*Availability*—Nonavailability of data is a challenge we face often. Data might
    not be available for the business problem, and then we face a dilemma on whether
    to continue the use case. Sometimes we face operational challenges and do not
    have access to the database or permission problems, or data might not be available
    at all for a particular variable since it is not captured. In such cases, we have
    to work with the data available and use surrogate variables. For example, imagine
    we are working on a demand generation problem. We want to predict how many customers
    can be expected during the upcoming sales season for a particular store. But we
    do not record the number of customers visiting for a few months. We can then use
    revenue as a surrogate field and synthesize the missing data points.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可用性*—数据不可用是我们经常面临的一个挑战。对于商业问题，数据可能不可用，然后我们面临是否继续使用用例的困境。有时我们面临运营挑战，无法访问数据库或存在权限问题，或者由于未捕获，特定变量的数据可能根本不可用。在这种情况下，我们必须使用可用的数据并使用代理变量。例如，假设我们正在处理一个需求生成问题。我们希望预测在即将到来的销售季节中，特定商店可以预期有多少客户。但我们没有记录过去几个月访问的客户数量。然后我们可以使用收入作为代理字段并综合缺失的数据点。'
- en: '*Consistency*—Here we check whether the data points are consistent across systems
    and interfaces. It should not be the case that one system is reporting a different
    revenue figure while another system is showing a completely different value. When
    faced with such a problem, we generate the respective KPIs as per the data available
    and seek guidance from the business team.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*一致性*—在这里，我们检查数据点是否在系统和接口之间保持一致。不应该出现一个系统报告不同的收入数字，而另一个系统显示完全不同的值的情况。面对这样的问题时，我们将根据可用的数据生成相应的KPIs，并寻求业务团队的指导。'
- en: '*Timeliness*—Timeliness simply means that we have all the data that is required
    at this point. If the dataset is not available now but might become available
    in the future, then it might be prudent to wait.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*及时性*—及时性简单地说就是我们现在拥有所有所需的数据。如果数据集现在不可用，但将来可能会可用，那么等待可能是明智的。'
- en: '*Integrity*—The data tables and variables we have are interlinked and interrelated
    to each other. For example, an employee’s details can be spread over multiple
    tables that are linked to each other using the employee’s ID. Data integrity addresses
    this requirement and ensures that all such relations between the tables and respective
    entities are consistent.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*完整性*——我们所拥有的数据表和变量是相互关联和相互关联的。例如，员工的详细信息可能分布在多个表中，这些表通过员工的ID相互链接。数据完整性解决了这一需求，并确保所有此类表与相应实体之间的关系都是一致的。'
- en: The quality of data is of paramount importance. In pragmatic day-to-day business,
    often we do *not* get good-quality data. Due to multiple challenges, good, clean
    data that is accessible, consistent, representative, and complete is seldom found.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 数据质量至关重要。在务实的日常业务中，我们往往得不到高质量的数据。由于多种挑战，高质量、干净、可访问、一致、具有代表性且完整的数据很少找到。
- en: 'Degradation in quality can be due to challenges during data capturing and collection,
    exporting or loading, transformations done, etc. A few of the possibilities are
    as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 质量下降可能是由于数据捕获和收集、导出或加载、转换等过程中的挑战。以下是一些可能性：
- en: We can get integers as names, or special characters like “#$!&” in a few columns,
    or null values, blanks, or not a number (NaN) as some of the values.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在一些列中获取整数作为名称，或者像“#$!&”这样的特殊字符，或者某些值可能是空值、空白或非数字（NaN）。
- en: There may be duplicates in the records.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录中可能有重复项。
- en: Outliers may occur. This is a nuisance we deal with quite a lot. For example,
    let’s say that the average daily transactions are 1,000 for an online retailer.
    One fine day, due to a server problem, there were no transactions done. It is
    an outlier situation. Or, one fine day, the number of transactions was 1,000,000\.
    It is again an example of an outlier. Outliers can bias the algorithms we create.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会出现异常值。这是我们经常需要处理的一个麻烦。例如，假设一个在线零售商的平均每日交易量是1,000。有一天，由于服务器问题，没有进行任何交易。这是一个异常情况。或者，有一天，交易量达到了1,000,000。这又是一个异常的例子。异常值可能会偏颇我们创建的算法。
- en: There may be seasonal variations and movements concerning the time of the day
    and days of the week—all of them should be representative enough in the dataset.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能存在关于一天中的时间和一周中的日子的季节性变化和移动——所有这些都应该在数据集中有足够的代表性。
- en: Inconsistencies in the date format can lead to multiple challenges when we try
    to merge multiple data sources. For example, source 1 might be using DD/MM/YYYY
    while another might be using MM/DD/YYYY. This is taken care of during the data
    loading step itself.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日期格式的不一致性可能导致我们在尝试合并多个数据源时遇到多个挑战。例如，源1可能使用DD/MM/YYYY，而另一个可能使用MM/DD/YYYY。这已经在数据加载步骤中得到了处理。
- en: All these aberrations and quality problems should be addressed and cleaned thoroughly.
    We will be solving these data problems throughout the book and sharing the best
    practices to be followed.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些异常和质量问题都应该得到解决并彻底清理。我们将在整本书中解决这些数据问题，并分享应遵循的最佳实践。
- en: Note  The quality of your raw data and the rigor shown during the cleaning process
    directly affect the quality of your final analysis and the maturity of your solution.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您原始数据的质量以及清理过程中的严谨性直接影响到您最终分析的质量和解决方案的成熟度。
- en: We have now defined the major attributes of data. We next study the broad process
    and techniques used for data engineering and management.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经定义了数据的主要属性。接下来，我们将研究用于数据工程和管理的广泛流程和技术。
- en: 1.2.4 Data engineering and management
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.4 数据工程和管理
- en: A strong data engineering process and mature data management practice are prerequisites
    for a successful ML model solution. Whether you come from a data engineering or
    data science background, each goes hand in hand; a data engineer would be well
    served by understanding the basics of data science, and vice versa. Figure 1.6
    provides a high-level overview of what the engineering process and management
    practice might look like. The end-to-end journey of data is described—right from
    the process of data capturing, data pipeline, and data loading to the point it
    is ready for analysis.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 强大的数据工程流程和成熟的数据管理实践是成功机器学习模型解决方案的先决条件。无论您来自数据工程还是数据科学背景，两者都相辅相成；数据工程师通过了解数据科学的基础知识将受益匪浅，反之亦然。图1.6提供了一个高级概述，说明了工程流程和管理实践可能的样子。数据从捕获、数据管道到加载，直到准备分析的全过程都被描述了。
- en: '![figure](../Images/CH01_F06_Verdhan.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH01_F06_Verdhan.png)'
- en: Figure 1.6 Data engineering paves the way for data analysis. It involves data
    loading, transformation, enrichment, cleaning, preparation, etc., which leads
    to the creation of data ready for analysis.
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.6 数据工程为数据分析铺平了道路。它涉及数据加载、转换、丰富、清洗、准备等，从而产生了适合分析的数据。
- en: In the data engineering step, data is cleansed, conformed, reshaped, transformed,
    and ingested. Generally, we have a server where the final data is hosted and is
    ready for access. The most used process is the creation of an export, transform,
    load (ETL) process. Then we make the data ready for analysis. We create new variables,
    treat null values, enrich the data with methods, and then finally proceed to the
    analysis/model-building stage.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据工程步骤中，数据被清洗、规范、重塑、转换和摄取。通常，我们有一个服务器，最终数据托管在那里，并准备好供访问。最常用的过程是创建一个导出、转换、加载（ETL）过程。然后我们使数据准备好进行分析。我们创建新的变量，处理空值，用方法丰富数据，然后最终进入分析/模型构建阶段。
- en: Many times, we find that terms like data analysis, data science, machine learning,
    data mining, artificial intelligence, business intelligence, big data, etc., are
    used quite interchangeably in business. It is a good idea to clarify them, which
    is the topic of the next section. There are plenty of tools available for each
    respective function that we are discussing. We will also understand the role of
    software engineering in this entire journey.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 许多时候，我们发现像数据分析、数据科学、机器学习、数据挖掘、人工智能、商业智能、大数据等术语在商业中经常被互换使用。明确它们是一个好主意，这也是下一节的主题。针对我们讨论的每个相应功能，都有大量的工具可用。我们还将了解软件工程在整个过程中的作用。
- en: 1.3 Data analysis, ML, AI, and business intelligence
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 数据分析、机器学习、人工智能和商业智能
- en: ML and AI are relatively new fields, and as such, there is little standardization
    and differentiation in the scope of their work. This has resulted in unclear definitions
    and demarcation of these fields. We examine these fields—where they overlap, where
    they differ, and how one empowers the other. Each of the functions empowers and
    complements the other, as visualized in figure 1.7\.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）和人工智能（AI）是相对较新的领域，因此，它们的工作范围在标准化和区分上很少。这导致了这些领域的定义和界限不明确。我们考察了这些领域——它们的重叠部分、差异部分以及一个如何增强另一个。每个功能都增强并补充了另一个，如图1.7所示。
- en: '![figure](../Images/CH01_F07_Verdhan.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH01_F07_Verdhan.png)'
- en: Figure 1.7 How the various fields are interlinked with each other and how they
    are dependent on each other
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图1.7 各个领域如何相互联系以及它们如何相互依赖
- en: After the business problem has been defined and scoped properly, we start with
    the technical process. Data mining and data engineering start the whole process
    by providing the data required for analysis. It also exports, transforms, cleans,
    and loads the data so that it can be consumed by all of the respective functions.
    Business intelligence and visualizations use this data to generate reports and
    dashboards. Data analytics generates insights and trends using data. Data science
    stands on the pillars of data analysis, statistics, business intelligence, data
    visualization, ML, and data mining. ML creates statistical and mathematical models,
    and AI further pushes the capabilities.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在业务问题被正确定义和范围界定之后，我们开始技术流程。数据挖掘和数据工程通过提供分析所需的数据来启动整个过程。它还导出、转换、清洗和加载数据，以便所有相关功能都可以使用。商业智能和可视化使用这些数据生成报告和仪表板。数据分析使用数据生成洞察和趋势。数据科学建立在数据分析、统计学、商业智能、数据可视化、机器学习和数据挖掘的基础上。机器学习创建统计和数学模型，而人工智能进一步扩展了这些能力。
- en: ML uses traditional coding. The coding is performed in traditional languages
    (such as Python), and hence, all the logic and rules of computer science and software
    engineering are valid in ML too. ML helps us make sense of data that we are otherwise
    not able to comprehend. The most fascinating advantage of ML is its ability to
    work on very complex and high-dimensional data points like video, audio, image,
    text, or complex datasets generated by sensors. It allows us to think beyond the
    obvious. Now AI can achieve feats that were previously thought impossible. This
    level of pattern recognition and learning has resulted in technological breakthroughs
    such as self-driving cars, chatbots conversing like humans, speech-to-text conversion
    and translation to another language, automated grading of essays, photo captioning,
    etc. With the advent of generative AI, using large language models like ChatGPT,
    we can create images, videos, and text based on the prompt given by the user.
    And that is just the start!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习使用传统的编码。编码是在传统语言（如Python）中进行的，因此计算机科学和软件工程的逻辑和规则在机器学习中同样有效。机器学习帮助我们理解我们通常无法理解的数据。机器学习最迷人的优势是它能够处理非常复杂和高维度的数据点，如视频、音频、图像、文本或由传感器生成的复杂数据集。它使我们能够超越显而易见的事物。现在，人工智能可以完成以前被认为不可能的壮举。这种模式识别和学习水平导致了自动驾驶汽车、像人类一样交谈的聊天机器人、语音转文本转换和翻译到另一种语言、自动批改论文、照片标题等技术的突破。随着生成式人工智能的出现，使用像ChatGPT这样的大型语言模型，我们可以根据用户提供的提示创建图像、视频和文本。而这只是开始！
- en: 1.4 Nuts and bolts of ML
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 机器学习的要点
- en: 'Consider this: if a child has to be taught how to open a door, we show them
    the exact steps quite a few times. The child tries to open it but fails. They
    try again and fail again. But with each subsequent try, the child improvises their
    approach. And, after some time, the child can open the door. Another example is
    when we learn to drive: we make mistakes, we learn from them, and we improve.
    ML works similarly, wherein the statistical algorithm looks at the historical
    data and finds patterns and insights. The algorithm uncovers relationships and
    anomalies, trends and deviations, similarities and differences—and then shares
    actionable results with us.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这样一个例子：如果一个孩子需要学习如何开门，我们会多次展示给他确切的步骤。孩子尝试开门但失败了。他们再次尝试，但再次失败。但随着每一次后续尝试，孩子都会改进他们的方法。经过一段时间，孩子就能打开门了。另一个例子是我们学习开车：我们犯错，从错误中学习，并改进。机器学习的工作方式与此类似，其中统计算法分析历史数据，寻找模式和洞察力。算法揭示关系和异常、趋势和偏差、相似性和差异性——然后与我们分享可操作的结果。
- en: Formally put, ML can be called a branch or a study of computer algorithms that
    works on historical data to generate insights and helps in making data-driven
    decisions. The algorithms are based on statistical and mathematical foundations
    and hence have a sound logical explanation. ML algorithms require coding, which
    can be done in any of the languages and tools available such as Python, R, SPSS,
    SAS, MATLAB, Weka, Julia, Java, etc. It also requires a domain understanding of
    the business.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 正式来说，机器学习（ML）可以称为计算机算法的一个分支或研究领域，它通过分析历史数据来生成洞察力，并帮助进行数据驱动的决策。这些算法基于统计学和数学基础，因此具有坚实的逻辑解释。机器学习算法需要编码，这可以在任何可用的语言和工具中进行，例如Python、R、SPSS、SAS、MATLAB、Weka、Julia、Java等。它还需要对业务领域的理解。
- en: Whenever you are doing some online shopping for clothing and the website recommends
    accessories that go along with it or you are booking an airplane ticket and the
    travel operator shows you a customized deal as per your needs and plan, most of
    the time, ML is working in the background. It has learned your preferences and
    compared them with your historical trends. It is also looking for similarities
    you have with other customers who behave almost the same. Based on all that analysis,
    the algorithm is making an intelligent recommendation to you. Quite fascinating,
    right?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 无论何时你在网上购物买衣服，网站推荐与之搭配的配饰，或者你预订机票，旅行运营商根据你的需求和计划展示定制套餐，大多数情况下，机器学习在幕后工作。它已经学会了你的偏好，并将它们与你的历史趋势进行比较。它还在寻找与其他行为几乎相同的客户之间的相似性。基于所有这些分析，算法为你做出智能推荐。是不是很神奇？
- en: Why exactly is ML so good at finding patterns? We humans can analyze only two
    or maybe three dimensions simultaneously; for example, we can pick up a pattern
    between two or three interacting variables. But what if there are 50 different
    variables all interacting? We wouldn’t have a chance. An ML algorithm can work
    on 50, 60, or maybe 100s of dimensions simultaneously. It can work on any type
    of data, structured or unstructured, and it can help in the automation of tasks.
    Hence, it generates patterns and insights quite difficult for a human mind to
    visualize.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'ML, like any other project, requires a team of experts who work closely with
    each other and complement each other’s skill sets. As shown in figure 1.8, an
    ML project requires the following roles:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '*Business team*—Business stakeholders and subject matter experts define the
    business problem for the project. They own the solution, have a clear understanding
    of the ask, and have a clear measurable goal in sight. They course-correct the
    team in case of confusion and serve as experts who have a deep understanding of
    the business processes and operations. They are marketing managers, product owners,
    process engineers, quality experts, risk analysts, portfolio leads, etc. It is
    imperative that business stakeholders are closely knit into the team from day
    one. They help in course correction of the overall direction.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Operations team*—This team comprises the scrum master, project manager, business
    analysts, etc. The role of the team can be compared to a typical project management
    team, which tracks the progress, maintains the records, reports the day-to-day
    activities, and keeps the entire project on track. They create user stories and
    act as a bridge between the business team and the data team.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F08_Verdhan.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 Team required for a data science project and the respective interactions
    of them with each other—truly a team effort
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Data team*—The core team that creates the solution, does the coding, and generates
    the output in the form of a model, dashboard, report, and insights is the data
    team. It comprises three main pillars: the data engineering team, the UI/visualization
    team, and the data science team. Their functions are as follows:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data engineering team is responsible for building, maintaining, integrating,
    and ingesting all the data points. They do a periodic data refresh and act as
    a prime custodian of data. They use ETL, SQL, AWS, Kafka, PySpark, etc.
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The UI/visualization team builds dashboards, reports, interactive modules, and
    web applications. They use SQL, Tableau, Qlik, Power BI, and others.
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The data science team is responsible for all the data analysis and model-building
    tasks. They discover patterns and insights, test hypotheses, and generate the
    final output that is to be finally consumed by all. The final output can be an
    ML model that will be used to solve the business problem. In situations where
    an ML model is not possible, the team might generate actionable insights that
    can be useful for the business. This team requires SQL, Python, R, SAS, SPSS,
    etc., to complete their job.
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The DevOps team is generally a part of the data engineering team, or they can
    exist as a separate entity. They focus on the operationalization of the ML model.
    Remember: if your ML model is not being used, it is just a shiny piece of software
    sitting on a shelf. The UI/UX team will lead the development of the final product
    layer where the ML-based outputs will be surfaced to the end user. User experience
    is often ignored, and without an interactive and engaging user experience, ML
    will not be used to its full potential.'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The team sometimes has a testing team as well to assess the functionality, various
    use cases, and overall look and feel of the application.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Having discussed the typical team structure for a data science project, we will
    now examine the broad steps involved in a data science project.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: A data science project runs like any other project that has deadlines, stages,
    testing, phases, etc. The raw material is the data that passes through various
    phases to be cleaned, analyzed, and modeled.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.9 shows an illustration of a data science project’s stages. It starts
    with a business problem definition of the project. The business problem must be
    concise, clear, measurable, and achievable. Table 1.2 depicts an example of a
    bad (ill-defined) and a good business problem.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F09_Verdhan.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 A data science project is like any other project, having stages and
    deadlines, dependencies, and processes.
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Table 1.2 Examples of how to define a business problem to make it clear, concise,
    and measurable
  id: totrans-121
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Examples |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: '| Ill-defined business problems | Good business problems |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| Increase the production Decrease the cost'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '| Optimize the various cost heads (A, B, C, and D) and identify the most optimal
    combination to decrease the cost by 1.2% in the next six months  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: '| Increase the revenue by 80% in one month Automate the entire process'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '| From the various factors of defects in the process (X, Y, Z), identify the
    most significant factors to reduce the defect % by 1.8% in the next three months  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
- en: Then we move to the data discovery phase, during which we list all the data
    sources and host them. All the various datasets, like customer details, purchase
    histories, social media data, portfolios, etc., are identified and accessed. The
    data tables that are to be used are finalized in this step, and most of the time,
    we create a database for us to work, test, and learn.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: We then go ahead with data preprocessing. It involves cleaning data like the
    removal of null values, outliers, duplicates, junk values, etc. The previous step
    and this one can take 60% to 70% of the project time.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: We create a few reports and generate initial insights during the exploratory
    data analysis phase. These insights are discussed with the business stakeholders,
    and they guide course correction.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The data is now ready for modeling. Quite a few versions of the solution are
    tested. Then, depending on the requirements, we choose the best version. Generally,
    parameters like accuracy and statistical measures like precision and recall drive
    the selection of the model. We will be exploring the process of choosing the best
    model and terms like precision and recall in later chapters of the book. Once
    we choose the final model, we are ready to deploy the model in the production
    environment, where it will work on unseen data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: These are the broad steps in an ML project. Like any other project, there is
    a code repository, best practices, coding standards, common errors, pitfalls,
    etc., which we will discuss throughout the book.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Types of ML algorithms
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ML models affect decision-making and follow a statistical approach to solve
    a business problem. They work on historical data and find patterns and trends
    in it. The raw material is the historical data, which is analyzed and modeled
    to generate a predictive algorithm. The historical data available and the sort
    of problem that needs to be solved informs the ML approach that should be taken.
    ML algorithms can be split broadly into four classes: supervised learning, unsupervised
    learning, semisupervised learning, and reinforcement learning, as depicted in
    figure 1.10\. We will examine each of the four types in detail, with a focus on
    unsupervised learning—the topic of this book.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: You might have heard about generative AI (GenAI) in the news. GenAI-based solutions
    generally start with unsupervised and may include supervised or reinforcement
    learning to specialize the model for certain tasks. We will discuss GenAI further
    throughout the book.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F10_Verdhan.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 ML algorithms can be classified as supervised learning algorithms,
    unsupervised learning algorithms, semisupervised learning algorithms, and reinforcement
    learning algorithms.
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 1.5.1 Supervised learning
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Formally put, supervised models are statistical models that use both the input
    data and the desired output to predict the future. The output is the value that
    we wish to predict and is referred to as the *target variable,* and the data used
    to make that prediction is called *training data*. The target variable is sometimes
    referred to as the *label*. The various attributes or variables present in the
    data are called *independent variables*. Each of the historical data points or
    a *training example* contains these independent variables and the corresponding
    target variable. Supervised learning algorithms make a prediction for unseen future
    data. The accuracy of the solution depends on the training done and patterns learned
    from the labeled historical data. An example is described in the next section.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning problems are used in demand prediction, credit card fraud
    detection, customer churn prediction, premium estimation, etc. They are heavily
    used across domains like retail, telecom, banking and finance, aviation, insurance,
    and more and for functions like marketing, CRM, quality, supply chain, pricing,
    and so on.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning algorithms can be further broken into regression algorithms
    and classification algorithms. Let’s consider each of these in turn.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Regression algorithms
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Regression algorithms are supervised learning algorithms—that is, they require
    target variables that need to be predicted. These algorithms are used to predict
    the values of a *continuous* *variable*. Examples include revenue, amount of rainfall,
    number of transactions, production yield, and so on. In supervised classification
    problems, we predict a categorical variable like whether it will rain (yes/no),
    whether the credit card transaction is fraudulent or genuine, and so on. This
    is the main difference between classification and regression problems.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Let us understand the regression problem with an example. Say we assume that
    the weight of a person is only dependent on height and not on other parameters
    like gender, ethnicity, diet, etc. In such a case, we want to predict the weight
    of a person based on height. The dataset and the graph plotted for the same data
    will look like figure 1.11.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: A regression model will be able to find the inherent patterns in the data and
    fit a mathematical equation describing the relationship. It can then take height
    as an input and predict the weight. Here, height is the independent variable,
    and weight is the dependent variable or the target variable or the label we want
    to predict.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F11_Verdhan.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 Data and plot of relationship between height and weight that is
    used for regression problem
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are quite a few algorithms available for regression problems. Some of
    the major ones are as follows (although this list is certainly not exhaustive):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-nearest neighbor
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting algorithm
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use any of the algorithms to solve this problem. We will explore more
    by using linear regression to solve a problem.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'The linear regression algorithm models the relationship between dependent variables
    and target variables by assuming a linear relationship between them. The linear
    regression algorithm would result in a mathematical equation for the problem,
    shown in equation 1.1:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: (1.1)
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Weight = *β*[0] * height + *β*[1]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally put, linear regression is used to fit a mathematical equation depicting
    the relationship between dependent and independent variables, shown as equation
    1.2:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: (1.2)
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Y = *β*[0] + *β*[1] x[1] + *β*[2]x[2] + ….+ *ε*
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Here, *Y* is the target variable that we want to predict; *x*[1] is the first
    independent variable; *x*[2] is the second independent variable; *ε* is the error
    term in the equation; and *β*[0] is the intercept of the equation.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: A simple visualization for a linear regression problem is shown in figure 1.12\.
    Here we have the x and Y variables where x is the independent variable and Y is
    the target variable. The objective of the linear regression problem is to find
    the *line of best fit,* which can explain the randomness present in the data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F12_Verdhan.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 Raw data that needs to be modeled (left). Using regression, a line
    of best fit is identified (right).
  id: totrans-167
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Equation 1.2 is used to make predictions for the unseen data. There are variations
    in linear regression too, like simple linear regression, multiple linear regression,
    nonlinear regression, etc. Depending on the data at hand, we choose the correct
    algorithm. A complex dataset might require a nonlinear relationship between the
    various variables.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The next type of regression algorithm we shall explore is tree-based solutions.
    For tree-based algorithms like decision trees, random forests, etc., the algorithm
    will start from the top and then, like an `if`/`else` block, will split iteratively
    to create nodes and subnodes until we reach a terminal node (see figure 1.13).
    In the decision tree diagram, we start from the top with the root node, and then
    we perform splitting until we reach the endpoint, which is the terminal node.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are simple to comprehend and implement, and they are fast to
    train. Their usability lies in the fact that they are intuitive enough to understand
    without much technical background.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F13_Verdhan.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: Figure 1.13 A decision tree has a root node, and after splitting, we get a decision
    node and a terminal node, which is the final node and cannot be split further.
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are other famous regression algorithms like k-nearest neighbor, gradient
    boosting, and deep learning–based solutions. Different regression algorithms are
    best suited to specific contexts.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the effect of regression use cases, let’s consider a few business-relevant
    use cases that are implemented in the industry:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: An airport operations team is assessing staffing requirements and wants to estimate
    the amount of passenger traffic expected. The estimate will help the team prepare
    a plan regarding future resource requirements and will help in the optimization
    of the resources required. Regression algorithms can help in predicting the number
    of passengers.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A retailer wants to understand the expected demand for the upcoming sales season
    so it can plan the inventory. This will result in cost savings and avoid stock-outs.
    Regression algorithms can help in such planning.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A manufacturing plant wishes to improve the yield from the existing use of various
    molds and raw materials. The regression solutions can suggest the best combination
    of molds and predict the expected yield.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bank offers credit cards to its customers. Consider how the credit limit offered
    to new customers is calculated. Based on the attributes of customers like age,
    occupation, income, and previous transaction history, regression algorithms can
    help in suggesting credit limits at a customer level.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An insurance company wants to come up with a premium table for its customers
    using historical claims. The risk can be assessed based on the historical data
    around driver details, car information, etc. Regression can surely help with such
    problems.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression problems form the foundation of supervised learning problems and
    are quite heavily used in the industry. Along with classification algorithms,
    they serve as a go-to solution for most of the predictive problems used in real-world
    business.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Classification algorithms
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Classification algorithms are used to predict the values of a categorical variable,
    which is the dependent variable. This target variable can be binary (yes/no, good/bad,
    fraud/genuine, pass/fail, etc.) or multiclass (such as positive/negative/neutral
    or yes/no/don’t know). Classification algorithms will ascertain whether the target
    event will happen by generating a probability score for the target variable.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: After the model has been trained on historical data, a classification algorithm
    will generate a probability score for the unseen dataset, which can be used to
    make the final decision. Depending on the number of classes present in the target
    variable, our business decision will vary. Let’s have a look at a use case for
    classification problems.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this: a telecom operator is facing a problem with its decreasing subscriber
    base. The number of existing subscribers is shrinking, and the telecom operator
    would like to arrest this churn of subscribers. For this purpose, an ML model
    is envisioned.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the historical data or the training data available for model building
    might look like table 1.3\. These data points are only for illustration purposes
    and are not exhaustive. There can be many other significant variables available.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Table 1.3 Example of a structured dataset for a telecom operator showing multiple
    data attributes
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| ID | Revenue ($) | Duration of service (years) | Avg. cost | Monthly usage
    (days) | Churned (Y/N) |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '| 1001  | 100  | 1.1  | 0.10  | 10  | Y  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| 1002  | 200  | 4.1  | 0.09  | 25  | N  |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| 1003  | 300  | 5.2  | 0.05  | 28  | N  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| 1004  | 200  | 0.9  | 0.25  | 11  | Y  |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| 1005  | 100  | 0.5  | 0.45  | 12  | Y  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: In the example in table 1.3, the dataset comprises the past usage data of subscribers.
    The last column (Churned) depicts if that subscriber churned out of the system
    or not. For example, subscriber 1001 churned while 1002 did not. Hence, the business
    problem is to build an ML model based on this historical data and predict if a
    new unseen customer will churn or not.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Here, the churned status (yes/no) is the target variable. It is also referred
    to as the dependent variable. The other attributes like revenue, duration, average
    cost, monthly usage, etc., are independent variables that are used to create the
    ML model. The historical data is called the training data. Before the training
    of the model, the trained supervised learning model will generate prediction probabilities
    for a new customer.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'There are quite a few algorithms available for classification problems; the
    major ones are as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-nearest neighbor
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayes
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machine
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting algorithms
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the most popular classification algorithms is logistic regression. Logistic
    regression uses a logit function to model the classification problem. If we are
    solving for a binary classification problem, it will be binary logistic regression
    or multiple logistic regression. Similar to linear regression, logistic regression
    also fits an equation, albeit it uses a sigmoid function to generate the probability
    score for the event to happen.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'A sigmoid function is a mathematical function that has a characteristic S-shaped
    curve or a sigmoid curve. The mathematical equation of a sigmoid function is shown
    in equation 3.1:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: (1.3)
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*S*(*x*) = 1/(1 + *e*^–^(*x*))'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: which can be rewritten as equation 1.4
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: (1.4)
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*S*(*x*) = *e*^(*x*)/(*e*^(*x*) + 1)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression uses the sigmoid function. The equation used in the logistic
    regression problem is shown in equation 1.5:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: (1.5)
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: log (p/1 – p) = *β*[0] + *β*[1] x[1]
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: where *p* is the probability of the event happening; *β*[0] is the intercept
    term; *β*[1] is the coefficient for the independent variable *x*[1]; log(*p*/1
    – *p*) is called the logit; and (*p*/1 – *p*) is the odds. As depicted in figure
    1.14, if we try to fit a linear regression equation for the probability function,
    it will not do a good job. We want to obtain the probability scores (i.e., a value
    between 0 and 1). The linear regression will not only return values between 0
    and 1 but also probability scores that are greater than 1 or less than 0\. Hence,
    we have a sigmoid function at right in the figure, which generates probability
    scores for us between 0 and 1 only.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F14_Verdhan.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: Figure 1.14 A linear regression model will not be able to do justice (left);
    hence, we have logistic regression for classification. Linear regression can generate
    probability scores more than 1 or less than 0 too, which is mathematically incorrect,
    whereas the sigmoid function generates probability scores between 0 and 1 only
    (right).
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The logistic regression algorithm is one of the most widely used techniques
    for classification problems. It is easy to train and deploy and is often the benchmark
    algorithm whenever we start any supervised classification learning project.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based algorithms like decision trees and random forests can also be used
    for classification problems. The other algorithms are also used as per the requirements.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.2 Unsupervised algorithms
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine you are given some paper labels, as shown in figure 1.15\. The task
    is to arrange them by similarity. Now, there are multiple approaches to that problem.
    You can use color, shape, or size. Here, we do not have any label to guide this
    arrangement. This is what makes unsupervised algorithms different.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F15_Verdhan.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: Figure 1.15 Example of various shapes that can be grouped together using different
    parameters
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Formally put, unsupervised learning only takes the input data and then finds
    patterns in it without referencing the target variable. An unsupervised learning
    algorithm therefore reacts based on the presence or lack of patterns in the dataset.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning is hence used for pattern detection, exploring the insights
    in the dataset and understanding the structure of it, segmentation, and anomaly
    detection.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: We can understand unsupervised learning algorithms by looking at figure 1.16\.
    The figure on the left shows the raw data points represented in a vector space
    diagram. On the right is the clustering, which will be done using an unsupervised
    learning algorithm.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F16_Verdhan.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
- en: Figure 1.16 An unsupervised learning algorithm finds patterns in the data on
    the left and results in clusters on the right.
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Some use cases for unsupervised algorithms are as follows:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: A retail group wants to understand its customers better. The task is to improve
    the customer’s stickiness, revenue, number of visits, basket size, etc. Customer
    segmentation using unsupervised learning can be done here. Depending on the customer’s
    attributes like revenue, number of visits, last visit date, age since joining,
    demographic attributes, etc., the segmentation will result in clusters that can
    be targeted personally. The result will be improved customer experience, increased
    customer lifetime value, etc.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A network provider needs to create an anomaly detection system. The historical
    data will serve as the anomalies data. The unsupervised learning algorithm will
    be able to find patterns, and the outliers will be given out by the algorithm.
    The distinguished anomalies will be the ones that need to be addressed.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A medical product company wishes to find if there are any underlying patterns
    in the image data of its patients. If there are any patterns and factors, those
    patients can be treated better, and maybe they require a different approach. Unsupervised
    learning can help with the image data, which will help address the patients’ needs
    better.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A digital marketing company wants to understand the “unknowns” in the incoming
    customer data like social media interactions, page clicks, comments, stars, etc.
    This understanding will help improve customers’ recommendations and overall purchasing
    experience.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms offer flexibility and performance when it comes
    to finding patterns. They are usable for all kinds of data—the core topic of this
    book—including structured data, text, or images.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: The major unsupervised learning algorithms are
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means clustering
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBSCAN clustering
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectral clustering
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular value decomposition
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rules
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: t-distributed stochastic neighbor embedding
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We cover all these algorithms in detail in the coming chapters. We will examine
    the mathematical concepts, the hidden processes, Python implementation, and the
    best practices throughout the book. Let’s first understand the basic process by
    means of a case study.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: A retailer wants to develop a deeper understanding of its consumer base and
    then wants to offer personalized recommendations, promotions, discounts, offers,
    etc. The entire customer dataset should be segmented using attributes like persona,
    previous purchase, response, external data, and so on (see figure 1.17).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F17_Verdhan.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: Figure 1.17 Steps in an unsupervised learning algorithm from data sources to
    the final solution ready for deployment
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For the use case, the steps in an unsupervised learning project are as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: We start the project by defining the business problem. We wish to understand
    the customer base better. A customer segmentation approach can be a good solution.
    We want segments that are distinguishable using mathematical KPIs.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is the data discovery phase. All the various datasets, like customer details,
    purchase histories, social media data, portfolios, etc., are identified and accessed.
    The data tables to be used are finalized in this step. Then, all the data tables
    are generally loaded into a common database, which we will use to analyze, test,
    and learn.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we have access to the data. The next step is to clean it and make it usable.
    We treat all the null values, NaN, junk values, duplicates, etc.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the data is clean and ready to be used, we perform an exploratory data
    analysis of it. Usually, during exploratory analysis, we identify patterns, cyclicity,
    aberrations, max-min range, standard deviation, etc. The outputs of the exploratory
    data analysis stage will be insights and understandings. We will also generate
    a few graphs and charts, as shown in figure 1.18\.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We begin with the unsupervised approach now. We want to implement clustering
    methods, and hence we can try a few clustering methods like k-means, hierarchical
    clustering, etc. The clustering algorithms will result in homogeneous segments
    of customers based on their various attributes.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F18_Verdhan.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
- en: Figure 1.18 Examples of the graphs and charts from the exploratory data analysis
    of the data
  id: totrans-257
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the case study, we will be working on the past two to three years of data,
    which is the training data. Since we are using an unsupervised approach, there
    is no target variable here. The algorithm will merge the customer segments that
    behave alike using their transactional patterns, their demographic patterns, and
    their purchase preferences. It will look like the results in figure 1.19.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F19_Verdhan.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: Figure 1.19 Output of the clustering algorithm where we can segment customers
    using various attributes
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6\. We now check how the various algorithms have performed; in other words,
    we will compare the accuracy of each algorithm. The final clustering algorithm
    chosen will result in homogeneous segments of customers, which can be targeted
    and offered customized offers.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 7\. We discuss the results with the business stakeholders and make iterations
    based on the feedback.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 8\. We deploy the solution in the production environment and are ready to work
    on new unseen datasets.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These are the broad steps in an unsupervised problem. Algorithm creation and
    selection are tedious tasks. We will be studying these in detail later in the
    book.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: GenAI most often starts with an unsupervised stage. This stage enables the model
    to learn patterns, structures, and relationships without explicit labels. It is
    sometimes referred to as the pretraining stage. Once the model has been pretrained,
    we move to the supervised stage. Here, the pretrained model is tailored to a specific
    task or domain using a labeled dataset.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.3 Semisupervised algorithms
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Semisupervised learning is a middle path of the supervised and unsupervised
    approaches. The primary reason for a semisupervised approach is the lack of availability
    of a complete *labeled* dataset for training. Formally put, the semisupervised
    approach uses both supervised and unsupervised approaches: supervised to classify
    the data points and unsupervised to group them together.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: In semisupervised learning, we train initially on a smaller number of labeled
    data points available using a supervised algorithm. Then we use it to label or
    *pseudo-label* new data points. The two datasets (labeled and pseudo-labeled)
    are combined, and we use this dataset for further analysis.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Semisupervised algorithms are used in cases where the dataset is partially available,
    like images in the medical industry. If we are creating a cancer detection solution
    by analyzing the images of patients, we will likely not have enough sample sets
    of training images. Here, the semisupervised approach can be helpful.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.4 Reinforcement learning
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Imagine you are playing a game of chess with a computer, and it goes like this:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '*Round 1*—You win after 5 moves.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Round 2*—You win after 8 moves.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Round 3*—You win after 14 moves.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Round 4*—You win after 21 moves.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Round 5*—The computer wins!'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is happening here is the algorithm is training itself iteratively depending
    on each interaction and then correcting/improving itself.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Formally, reinforcement learning solutions are self-sustained solutions that
    train themselves using a sequence of trial and error. One sequence follows the
    other. The heart of reinforcement learning is reward signals. If the action is
    positive, the reward is positive, indicating to continue. If the action is negative,
    the reward will penalize the activity. Hence, the solution will always correct
    itself and move ahead, thereby improving itself iteratively.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Self-driving cars are the best examples of reinforcement learning algorithms.
    They detect when they should turn left or right, when to move, and when to stop.
    Modern video games also employ reinforcement learning algorithms. Reinforcement
    learning allows us to break the barriers of technology and imagine things that
    were earlier thought impossible.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have covered the different types of ML algorithms. Together, they
    are harnessing the true power of data and creating a long-lasting effect on our
    lives. But the heart of the solutions is the technology, which we have not discussed
    yet. We now move to the technology stack required to make these solutions tick.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 1.1
  id: totrans-281
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Use these questions to check your understanding:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Why is ML so powerful that it is being used very heavily now?
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the different types of ML algorithms, and how are they different from
    each other?
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the steps in an ML project?
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the role of data engineering, and why is it important?
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the various tools available for ML?
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1.6 Concluding thoughts
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A common question is: Which is better, R or Python? Both are fantastic languages.
    Both are heavily used. But after the introduction of TensorFlow, Keras’s libraries
    on AI, the balance has slightly tilted in favor of Python.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: You’ve now taken your first step in the journey toward learning unsupervised
    machine learning techniques. It is time to wrap up.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: ML and AI are indeed pathbreaking. They are changing the way we travel, order
    food, plan, buy, see a doctor, order prescriptions—they are making a dent everywhere.
    ML is indeed a powerful capability that is paving the path for the future and
    is proving much better than existing technology stacks when it comes to pattern
    identification, anomaly detection, customizations, and automation of tasks. Autonomous
    driving, cancer detection, fraud identification, facial recognition, image captioning,
    and chatbots are only a few examples where ML and AI are outperforming traditional
    technologies. And now is the best time to enter this field. This sector is attracting
    investments from almost all business functions. The field has created thousands
    of job opportunities across the spectrum.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'At the same time, the field lacks trained professionals: data analysts, data
    engineers, visualization experts, data scientists, and data practitioners. They
    are all rare breeds now. The field requires a regular supply of budding talents
    who will become the leaders of tomorrow and will make data-driven decisions. We
    have only scratched the surface of understanding the power of data—there are still
    miles to be covered.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following chapter, we will dive deeper into the unsupervised learning
    concepts of clustering. The mathematical and statistical foundations, a pragmatic
    case study, and Python implementation are discussed. The discussion includes the
    simpler clustering algorithms: k-means clustering, hierarchical clustering, and
    DBSCAN. In the later chapters of the book, we will study more complex clustering
    topics like Gaussian mixture modeling clustering, time series clustering, fuzzy
    clustering, etc.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data can be conceptualized as an interconnected set of facts and statistics
    necessary for analysis, characterized by unique traits and governed by specific
    management principles.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world activities such as mobile calls, online transactions, and social
    media interactions continually generate data, underscoring its omnipresence in
    modern life.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raw data requires cleaning, organization, and analysis to be converted effectively
    into information and insights that can drive business decisions and actions.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data can be broadly classified into structured datasets, which follow a clear
    row-column format, and unstructured datasets, like text and images, which require
    more advanced analysis techniques.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To analyze unstructured data, we typically transform it into numerical representations,
    often utilizing deep learning models such as CNNs and RNNs.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A clear, concise, achievable, and measurable business problem is a vital step
    to ensure the success of a data science project.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-quality data is essential for reliable analysis and is characterized by
    attributes such as completeness, validity, accuracy, representativeness, availability,
    consistency, timeliness, and integrity.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective data engineering and management are crucial for preparing data for
    analysis involving techniques like ETL processes and data cleaning.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The role of UI/UX is of paramount importance to ensure adoption and usage by
    the end consumers; otherwise, ML will just be a shiny piece sitting on a shelf.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interconnected fields like data analysis, ML, AI, and business intelligence
    each play a critical role in processing and deriving insights from data.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning is an ML approach that uses existing data to predict future
    outcomes, common in tasks like demand prediction and fraud detection.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning is divided into regression and classification tasks, each
    with numerous available algorithms to model quantitative or categorical outcomes,
    respectively.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms discover patterns and relationships in data
    independently of predefined target variables, useful in activities like segmentation
    and anomaly detection.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variants of unsupervised learning algorithms include clustering techniques and
    methods for reducing data dimensionality, offering flexibility and performance
    in pattern recognition.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semisupervised learning bridges supervised and unsupervised methods and is effective
    when dealing with datasets that are partially labeled.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning involves systems that learn by trial and error, rewarding
    desired outcomes, and are applied in dynamic decision-making tasks, such as autonomous
    vehicle navigation.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technological solutions are at the heart of modern data-driven strategies, and
    understanding the technological stack is essential to maximize the effect and
    benefits of data solutions.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
