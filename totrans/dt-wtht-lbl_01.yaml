- en: 1 Introduction to machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An introduction to data, types of datasets, quality, and sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning and types of machine learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An overview of different types of algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are only patterns, patterns on top of patterns, patterns that affect other
    patterns. Patterns hidden by patterns. Patterns within patterns.—Chuck Palahniuk
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'There is a saying going around: “Data is the new electricity.” Data is indeed
    transforming our world, much like electricity has; nobody can deny that. But like
    electricity, we must remember that data must be properly harnessed to utilize
    its value. We have to clean the data and analyze and visualize it, and only then
    can we develop insights from it. The fields of data science, machine learning
    (ML), and AI are helping us to better harness data and extract trends and patterns
    so we can make more insightful and balanced decisions in our activities and business.'
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we unravel the puzzles of data and see how we can find the patterns
    hidden within. We will be studying a branch of ML referred to as *unsupervised
    learning*. Unsupervised learning solutions are one of the most influential approaches
    and are changing the face of the industry. They are utilized in banking and finance,
    retail, insurance, manufacturing, aviation, medical sciences, telecom, and almost
    every other sector.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the book, we discuss concepts of ML with a focus on unsupervised
    learning—the building blocks of algorithms, their nuts and bolts, background processes,
    and mathematical foundation. We will examine concepts, study best practices, analyze
    common errors and pitfalls, and use a case study–based approach that complements
    the learning. At the same time, we develop actual Python code for solving such
    problems. All the codes are accompanied by step-by-step explanations and comments.
  prefs: []
  type: TYPE_NORMAL
- en: By the time you finish this book, you will have a very good understanding of
    unsupervised technique-based ML, various algorithms, the mathematics and statistical
    foundation on which the algorithm rests, business use cases, Python implementation,
    and best practices.
  prefs: []
  type: TYPE_NORMAL
- en: 'This first chapter is designed to introduce the concepts of ML. We’ll begin
    by discussing the concepts fundamental to all data analysis and ML: data itself,
    how it is managed, and what constitutes good-quality data. We’ll then move on
    to discuss data analysis in the context of ML and deep learning, consider different
    types of ML algorithms, and wrap up by considering the technical toolkit recommended
    for getting hands-on with the content in this book. Welcome to the first chapter
    and all the very best!'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Technical toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following tools are used for different facets of the project:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data engineering*—Hadoop, Spark, Scala, Java, C++, SQL, Redshift, Azure, PySpark'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Data analysis*—SQL, R, Python, Excel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ML*—SQL, R, Python, Excel, Weka, Julia, MATLAB, SPSS, SAS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Visualization*—Tableau, Power BI, Qlik, COGNOS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model deployment*—Docker, Flask, Amazon S3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cloud services*—Azure, AWS, GCP'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this book, we are going to use Python. You are advised to install the latest
    version of Python on your system. At least version 3.5+ is advisable, though the
    latest version as of this writing is 3.13\. We will also use Jupyter Notebook,
    so installing Anaconda on your system is advisable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note  All the codes and datasets will be checked in at the GitHub repository:
    [https://github.com/vverdhan/DataWithoutLabels](https://github.com/vverdhan/DataWithoutLabels).
    You are expected to replicate them and try to reproduce the results.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Data, data types, data management, and quality
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We begin by introducing the protagonist of this book: *data*. Data can be thought
    of as facts and statistics that are collected for performing any kind of analysis
    or study. But data also has its own traits, attributes, quality measures, and
    management principles. It is stored, exported, loaded, transformed, and measured.
    In that sense, data is a tangible “thing” in its own regard, and it must be handled
    properly to correctly utilize it. To do that, we must properly understand data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the fundamentals: the definition of data. Once we’ve defined
    data, we will proceed to discuss different types of data, their respective examples,
    and the attributes of data that make it useful and of good quality.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.1 What is data?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data is ubiquitous. You make a phone call using a mobile network; as you do,
    you are generating data. You book a flight ticket and hotel for an upcoming vacation;
    data is being created. Our day-to-day activity-generated data might include performing
    a bank transaction, surfing social media, or shopping websites online. That data
    is transformed from one form to another, stored, cleaned, managed, and analyzed.
    So what actually is it?
  prefs: []
  type: TYPE_NORMAL
- en: Formally put, data is a collection of facts, observations, measures, text, numbers,
    images, and videos. A dataset might be clean (i.e., organized to be free from
    errors, inconsistencies, and irrelevant information) or unclean, be ordered (e.g.,
    alphabetically) or unordered, or have mixed data types or all one type. As mentioned,
    data in itself is not useful until we clean it, arrange it, analyze it, and draw
    insights from it. We can visualize the transition from raw to more useful forms
    in figure 1.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F01_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 How we can transform raw data to become information, knowledge, and,
    finally, insights that can be used in business to drive decisions and actions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Raw data is converted to information when we can find distinctions in it. When
    we relate the terms and “connect the dots,” the same piece of information becomes
    knowledge. Insight is the stage where we can find the major centers and significant
    points. An insight should be actionable, succinct, and direct. For example, if
    a customer retention team of a telecom operator is told that customers who do
    not make a call for nine days have a 30% higher chance of churn than those who
    make calls, this will be a useful insight that they can work on and try to resolve.
    Similarly, if a line technician in a manufacturing plant is informed that using
    mold X results in 60% more defects than using mold Y, they will refrain from using
    the poorly performing mold in the future. An insight is quite useful for a business
    team because they can consider it and take corrective measures.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.2 Various types of data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we’ve discussed, data is generated by much of our day-to-day activity. We
    can broadly classify that data into different *types*, as shown in figure 1.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F02_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 The divisions and subdivisions of data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Data can be divided into quantitative and qualitative categories, which are
    further subclassified:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Qualitative data* is the data type that cannot be measured or weighed—for
    example, taste, color, odor, fitness, name, etc. They can only be observed subjectively.
    Formally put, when we categorize something or make a classification for it, the
    data generated is qualitative in nature. Examples are colors in a rainbow, cities
    in a country, quality of a product, gender, etc. They are also called *categorical*
    variables. Qualitative data can be further subcategorized into binary, nominal,
    and ordinal datasets:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Binary* data, as the name suggests, has only two classes that are mutually
    exclusive to each other. Examples are yes/no, dry/wet, hard/soft, good/bad, true/false,
    etc.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Nominal* data can be described as the type of data that, though categorized,
    does not have any sequence or order. Examples are distinct languages that are
    spoken in a country, colors in a rainbow, types of services available to a customer,
    cities in a country, etc.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Ordinal* data is similar to nominal data, except we can order it in a sequence.
    Examples are fast/medium/slow, positive/neutral/negative, etc.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Quantitative* data is all the types of data points that can be measured, weighed,
    scaled, recorded, etc. Examples are height, revenue, number of customers, demand
    quantity, area, volume, etc. They are the most common form of data and allow mathematical
    and statistical operations. Quantitative data is further subcategorized as discrete
    and continuous:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Discrete* data is precise, to the point, and represented as integers. For
    example, the number of passengers in a plane or the population of a city cannot
    be in decimals.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Continuous* data points can take any value, usually in a range. For example,
    height can take decimal values or the price of a product need not be an integer.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Any data point will generally will fall into one of these classes, based on
    its properties. There is one more logical grouping that can be done using source
    and usage, which makes a lot of sense while solving business problems. This grouping
    allows us to design solutions customized to the data type.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the source and usage, we can also think of data in two broad classes:
    structured and unstructured data. A dataset that can be represented in a row-column
    structure easily is a *structured* dataset. For example, transactions made by
    five customers in a retail store can be stored, as shown in table 1.1.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1.1 An example of a structured dataset with attributes like amount, date,
    city, items, etc.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Customer ID | Transaction date | Amount ($) | No. of items | Payment mode
    | City |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1001  | 01-June-2024  | 100  | 5  | Cash  | New Delhi  |'
  prefs: []
  type: TYPE_TB
- en: '| 1002  | 02-June-2024  | 101  | 6  | Card  | New York  |'
  prefs: []
  type: TYPE_TB
- en: '| 1003  | 03-June-2024  | 102  | 7  | Card  | London  |'
  prefs: []
  type: TYPE_TB
- en: '| 1004  | 04-June-2024  | 103  | 8  | Cash  | Dublin  |'
  prefs: []
  type: TYPE_TB
- en: '| 1005  | 05-June-2024  | 104  | 9  | Cash  | Tokyo  |'
  prefs: []
  type: TYPE_TB
- en: In table 1.1, for each unique customer ID, we have the transaction date, the
    amount spent in dollars, the number of items purchased, the mode of payment, and
    the city in which the transaction was made. Such a data type can be extended to
    employee details, call records, banking transactions, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Note  Most of the data used in analysis and model building is structured. Structured
    data is easier to store, analyze, and visualize in the form of graphs and charts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many algorithms and techniques cater to structured data—in normal real-world
    language, we refer to structured data primarily. Unstructured data is not easily
    sorted into a row-column structure. It can be text, audio, image, or video. Figure
    1.3 shows examples of unstructured data and their respective sources, as well
    as the primary types of unstructured data: text, images, audio, and video along
    with their examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F03_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 Unstructured data, along with its various types and examples. This
    data is usually complex to analyze and generally requires deep learning-based
    algorithms.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Computers and processors understand only binary numbers. So these unstructured
    data points still need to be represented as numbers so that we can perform mathematical
    and statistical calculations on them. For example, an image is made up of pixels.
    If it is a colored image, each pixel will have RGB (red, green, blue) values and
    each RGB can take a value (0–255). Hence, we will be able to represent an image
    as a matrix on which further mathematical calculations can be made. Text, audio,
    and video can be represented similarly.
  prefs: []
  type: TYPE_NORMAL
- en: Note  In general, deep learning-based solutions like convolutional neural networks
    (CNN) and recurrent neural networks (RNN) are used for unstructured data. We are
    going to work on text and explore CNN and RNN at a later stage in the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unstructured data can be understood through an example: consider a picture
    of a vacuum cleaner, as shown in figure 1.4\. A portion of the image can be represented
    as a matrix and will look like the matrix seen in the figure. This example is
    only for illustration purposes and doesn’t show actual values.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F04_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 An example of how unstructured data can be represented as a matrix
    to analyze. The matrix on the right is only an illustration and not the actual
    numbers.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Similarly, we can have representations of text, audio, or video data. Due to
    the size and large number of dimensions typically present in such data, this kind
    of unstructured data is complex to process and model, and hence, in general, deep
    learning-based models serve that purpose.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the broad types of data we’ve discussed so far, we can have more
    categories like ratios or scales, which can be used to define the relationship
    of one variable with another. All these data points (whether structured or unstructured)
    are defined by the way they are generated in real life.
  prefs: []
  type: TYPE_NORMAL
- en: 'All of these data points have to be captured, stored, and managed. There are
    quite a few tools available for managing data, which we will discuss in due course.
    But before that, let’s examine one of the most crucial but often less talked about
    subjects: *data quality*.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.3 Data quality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: “Garbage in, garbage out”—this principle summarizes the importance of good-quality
    data. If the data is dirty or incorrect and lacks any business relationship between
    variables, we will not be able to solve the business problem at hand. But what
    is the meaning of “good quality”? Imagine you want to predict rainfall this year
    based on last year’s daily rainfall measurements. A good-quality dataset for this
    task would be as complete as possible (very few missing days of rainfall measurements).
    It would be relevant and valid (e.g., covering the same local area as where you
    are making your predictions), the measurements would be accurate, and the data
    would be readily available for you to access and use without permission problems.
    A bad dataset, in contrast, might have lots of “holes” in the data, might have
    been taken in an area distant from the site you wish to study (making it less
    relevant), or might be difficult to access. As you can no doubt gather, good-quality
    data facilitates good-quality outputs, while bad data quality actively hinders
    your work and will likely result in a poor outcome. The major components of data
    quality are shown in figure 1.5\. Let’s explore them one by one.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F05_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 Data quality is of paramount importance; attributes of good-quality
    data are shown.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The major attributes of good-quality data are
  prefs: []
  type: TYPE_NORMAL
- en: '*Completeness*—We would expect our dataset to be proper and not missing any
    values. For example, if we are working on sales data for a year, good data will
    have all the values for all 12 months. Then it will be a complete data source.
    The completeness of a dataset ensures that we are not missing an important variable
    or data point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Validity*—The validity of data is its conformance to the properties, characteristics,
    and variations that are present and being analyzed in our use case. Validity indicates
    if the observation and measurement we have captured are reliable and valid. For
    example, if the scope of the study is for 2015–2019, then using 2014 data will
    be invalid.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Accuracy*—Accuracy is an attribute focusing on the correctness of data. If
    we have inaccurate data, we will generate inaccurate insights, and actions will
    be faulty. It is a good practice to start the project by generating key performance
    indicators (KPIs) and comparing them with the numbers reported by the business
    to check the authenticity of the data available to us.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Representativeness*—This is one of the most important attributes of the data
    and often the most undermined. Representation of data means that the data in use
    truly captures the business need and is not biased. If the dataset is biased or
    is not representative enough, the model generated will not be able to make predictions
    on the new and unseen data, and the entire effort will go down the drain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Availability*—Nonavailability of data is a challenge we face often. Data might
    not be available for the business problem, and then we face a dilemma on whether
    to continue the use case. Sometimes we face operational challenges and do not
    have access to the database or permission problems, or data might not be available
    at all for a particular variable since it is not captured. In such cases, we have
    to work with the data available and use surrogate variables. For example, imagine
    we are working on a demand generation problem. We want to predict how many customers
    can be expected during the upcoming sales season for a particular store. But we
    do not record the number of customers visiting for a few months. We can then use
    revenue as a surrogate field and synthesize the missing data points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Consistency*—Here we check whether the data points are consistent across systems
    and interfaces. It should not be the case that one system is reporting a different
    revenue figure while another system is showing a completely different value. When
    faced with such a problem, we generate the respective KPIs as per the data available
    and seek guidance from the business team.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Timeliness*—Timeliness simply means that we have all the data that is required
    at this point. If the dataset is not available now but might become available
    in the future, then it might be prudent to wait.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Integrity*—The data tables and variables we have are interlinked and interrelated
    to each other. For example, an employee’s details can be spread over multiple
    tables that are linked to each other using the employee’s ID. Data integrity addresses
    this requirement and ensures that all such relations between the tables and respective
    entities are consistent.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The quality of data is of paramount importance. In pragmatic day-to-day business,
    often we do *not* get good-quality data. Due to multiple challenges, good, clean
    data that is accessible, consistent, representative, and complete is seldom found.
  prefs: []
  type: TYPE_NORMAL
- en: 'Degradation in quality can be due to challenges during data capturing and collection,
    exporting or loading, transformations done, etc. A few of the possibilities are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We can get integers as names, or special characters like “#$!&” in a few columns,
    or null values, blanks, or not a number (NaN) as some of the values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There may be duplicates in the records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers may occur. This is a nuisance we deal with quite a lot. For example,
    let’s say that the average daily transactions are 1,000 for an online retailer.
    One fine day, due to a server problem, there were no transactions done. It is
    an outlier situation. Or, one fine day, the number of transactions was 1,000,000\.
    It is again an example of an outlier. Outliers can bias the algorithms we create.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There may be seasonal variations and movements concerning the time of the day
    and days of the week—all of them should be representative enough in the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inconsistencies in the date format can lead to multiple challenges when we try
    to merge multiple data sources. For example, source 1 might be using DD/MM/YYYY
    while another might be using MM/DD/YYYY. This is taken care of during the data
    loading step itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these aberrations and quality problems should be addressed and cleaned thoroughly.
    We will be solving these data problems throughout the book and sharing the best
    practices to be followed.
  prefs: []
  type: TYPE_NORMAL
- en: Note  The quality of your raw data and the rigor shown during the cleaning process
    directly affect the quality of your final analysis and the maturity of your solution.
  prefs: []
  type: TYPE_NORMAL
- en: We have now defined the major attributes of data. We next study the broad process
    and techniques used for data engineering and management.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2.4 Data engineering and management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A strong data engineering process and mature data management practice are prerequisites
    for a successful ML model solution. Whether you come from a data engineering or
    data science background, each goes hand in hand; a data engineer would be well
    served by understanding the basics of data science, and vice versa. Figure 1.6
    provides a high-level overview of what the engineering process and management
    practice might look like. The end-to-end journey of data is described—right from
    the process of data capturing, data pipeline, and data loading to the point it
    is ready for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F06_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.6 Data engineering paves the way for data analysis. It involves data
    loading, transformation, enrichment, cleaning, preparation, etc., which leads
    to the creation of data ready for analysis.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the data engineering step, data is cleansed, conformed, reshaped, transformed,
    and ingested. Generally, we have a server where the final data is hosted and is
    ready for access. The most used process is the creation of an export, transform,
    load (ETL) process. Then we make the data ready for analysis. We create new variables,
    treat null values, enrich the data with methods, and then finally proceed to the
    analysis/model-building stage.
  prefs: []
  type: TYPE_NORMAL
- en: Many times, we find that terms like data analysis, data science, machine learning,
    data mining, artificial intelligence, business intelligence, big data, etc., are
    used quite interchangeably in business. It is a good idea to clarify them, which
    is the topic of the next section. There are plenty of tools available for each
    respective function that we are discussing. We will also understand the role of
    software engineering in this entire journey.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Data analysis, ML, AI, and business intelligence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ML and AI are relatively new fields, and as such, there is little standardization
    and differentiation in the scope of their work. This has resulted in unclear definitions
    and demarcation of these fields. We examine these fields—where they overlap, where
    they differ, and how one empowers the other. Each of the functions empowers and
    complements the other, as visualized in figure 1.7\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F07_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.7 How the various fields are interlinked with each other and how they
    are dependent on each other
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: After the business problem has been defined and scoped properly, we start with
    the technical process. Data mining and data engineering start the whole process
    by providing the data required for analysis. It also exports, transforms, cleans,
    and loads the data so that it can be consumed by all of the respective functions.
    Business intelligence and visualizations use this data to generate reports and
    dashboards. Data analytics generates insights and trends using data. Data science
    stands on the pillars of data analysis, statistics, business intelligence, data
    visualization, ML, and data mining. ML creates statistical and mathematical models,
    and AI further pushes the capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: ML uses traditional coding. The coding is performed in traditional languages
    (such as Python), and hence, all the logic and rules of computer science and software
    engineering are valid in ML too. ML helps us make sense of data that we are otherwise
    not able to comprehend. The most fascinating advantage of ML is its ability to
    work on very complex and high-dimensional data points like video, audio, image,
    text, or complex datasets generated by sensors. It allows us to think beyond the
    obvious. Now AI can achieve feats that were previously thought impossible. This
    level of pattern recognition and learning has resulted in technological breakthroughs
    such as self-driving cars, chatbots conversing like humans, speech-to-text conversion
    and translation to another language, automated grading of essays, photo captioning,
    etc. With the advent of generative AI, using large language models like ChatGPT,
    we can create images, videos, and text based on the prompt given by the user.
    And that is just the start!
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Nuts and bolts of ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider this: if a child has to be taught how to open a door, we show them
    the exact steps quite a few times. The child tries to open it but fails. They
    try again and fail again. But with each subsequent try, the child improvises their
    approach. And, after some time, the child can open the door. Another example is
    when we learn to drive: we make mistakes, we learn from them, and we improve.
    ML works similarly, wherein the statistical algorithm looks at the historical
    data and finds patterns and insights. The algorithm uncovers relationships and
    anomalies, trends and deviations, similarities and differences—and then shares
    actionable results with us.'
  prefs: []
  type: TYPE_NORMAL
- en: Formally put, ML can be called a branch or a study of computer algorithms that
    works on historical data to generate insights and helps in making data-driven
    decisions. The algorithms are based on statistical and mathematical foundations
    and hence have a sound logical explanation. ML algorithms require coding, which
    can be done in any of the languages and tools available such as Python, R, SPSS,
    SAS, MATLAB, Weka, Julia, Java, etc. It also requires a domain understanding of
    the business.
  prefs: []
  type: TYPE_NORMAL
- en: Whenever you are doing some online shopping for clothing and the website recommends
    accessories that go along with it or you are booking an airplane ticket and the
    travel operator shows you a customized deal as per your needs and plan, most of
    the time, ML is working in the background. It has learned your preferences and
    compared them with your historical trends. It is also looking for similarities
    you have with other customers who behave almost the same. Based on all that analysis,
    the algorithm is making an intelligent recommendation to you. Quite fascinating,
    right?
  prefs: []
  type: TYPE_NORMAL
- en: Why exactly is ML so good at finding patterns? We humans can analyze only two
    or maybe three dimensions simultaneously; for example, we can pick up a pattern
    between two or three interacting variables. But what if there are 50 different
    variables all interacting? We wouldn’t have a chance. An ML algorithm can work
    on 50, 60, or maybe 100s of dimensions simultaneously. It can work on any type
    of data, structured or unstructured, and it can help in the automation of tasks.
    Hence, it generates patterns and insights quite difficult for a human mind to
    visualize.
  prefs: []
  type: TYPE_NORMAL
- en: 'ML, like any other project, requires a team of experts who work closely with
    each other and complement each other’s skill sets. As shown in figure 1.8, an
    ML project requires the following roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Business team*—Business stakeholders and subject matter experts define the
    business problem for the project. They own the solution, have a clear understanding
    of the ask, and have a clear measurable goal in sight. They course-correct the
    team in case of confusion and serve as experts who have a deep understanding of
    the business processes and operations. They are marketing managers, product owners,
    process engineers, quality experts, risk analysts, portfolio leads, etc. It is
    imperative that business stakeholders are closely knit into the team from day
    one. They help in course correction of the overall direction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Operations team*—This team comprises the scrum master, project manager, business
    analysts, etc. The role of the team can be compared to a typical project management
    team, which tracks the progress, maintains the records, reports the day-to-day
    activities, and keeps the entire project on track. They create user stories and
    act as a bridge between the business team and the data team.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F08_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.8 Team required for a data science project and the respective interactions
    of them with each other—truly a team effort
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Data team*—The core team that creates the solution, does the coding, and generates
    the output in the form of a model, dashboard, report, and insights is the data
    team. It comprises three main pillars: the data engineering team, the UI/visualization
    team, and the data science team. Their functions are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data engineering team is responsible for building, maintaining, integrating,
    and ingesting all the data points. They do a periodic data refresh and act as
    a prime custodian of data. They use ETL, SQL, AWS, Kafka, PySpark, etc.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The UI/visualization team builds dashboards, reports, interactive modules, and
    web applications. They use SQL, Tableau, Qlik, Power BI, and others.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The data science team is responsible for all the data analysis and model-building
    tasks. They discover patterns and insights, test hypotheses, and generate the
    final output that is to be finally consumed by all. The final output can be an
    ML model that will be used to solve the business problem. In situations where
    an ML model is not possible, the team might generate actionable insights that
    can be useful for the business. This team requires SQL, Python, R, SAS, SPSS,
    etc., to complete their job.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The DevOps team is generally a part of the data engineering team, or they can
    exist as a separate entity. They focus on the operationalization of the ML model.
    Remember: if your ML model is not being used, it is just a shiny piece of software
    sitting on a shelf. The UI/UX team will lead the development of the final product
    layer where the ML-based outputs will be surfaced to the end user. User experience
    is often ignored, and without an interactive and engaging user experience, ML
    will not be used to its full potential.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The team sometimes has a testing team as well to assess the functionality, various
    use cases, and overall look and feel of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Having discussed the typical team structure for a data science project, we will
    now examine the broad steps involved in a data science project.
  prefs: []
  type: TYPE_NORMAL
- en: A data science project runs like any other project that has deadlines, stages,
    testing, phases, etc. The raw material is the data that passes through various
    phases to be cleaned, analyzed, and modeled.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1.9 shows an illustration of a data science project’s stages. It starts
    with a business problem definition of the project. The business problem must be
    concise, clear, measurable, and achievable. Table 1.2 depicts an example of a
    bad (ill-defined) and a good business problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F09_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.9 A data science project is like any other project, having stages and
    deadlines, dependencies, and processes.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Table 1.2 Examples of how to define a business problem to make it clear, concise,
    and measurable
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Examples |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Ill-defined business problems | Good business problems |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Increase the production Decrease the cost'
  prefs: []
  type: TYPE_NORMAL
- en: '| Optimize the various cost heads (A, B, C, and D) and identify the most optimal
    combination to decrease the cost by 1.2% in the next six months  |'
  prefs: []
  type: TYPE_TB
- en: '| Increase the revenue by 80% in one month Automate the entire process'
  prefs: []
  type: TYPE_NORMAL
- en: '| From the various factors of defects in the process (X, Y, Z), identify the
    most significant factors to reduce the defect % by 1.8% in the next three months  |'
  prefs: []
  type: TYPE_TB
- en: Then we move to the data discovery phase, during which we list all the data
    sources and host them. All the various datasets, like customer details, purchase
    histories, social media data, portfolios, etc., are identified and accessed. The
    data tables that are to be used are finalized in this step, and most of the time,
    we create a database for us to work, test, and learn.
  prefs: []
  type: TYPE_NORMAL
- en: We then go ahead with data preprocessing. It involves cleaning data like the
    removal of null values, outliers, duplicates, junk values, etc. The previous step
    and this one can take 60% to 70% of the project time.
  prefs: []
  type: TYPE_NORMAL
- en: We create a few reports and generate initial insights during the exploratory
    data analysis phase. These insights are discussed with the business stakeholders,
    and they guide course correction.
  prefs: []
  type: TYPE_NORMAL
- en: The data is now ready for modeling. Quite a few versions of the solution are
    tested. Then, depending on the requirements, we choose the best version. Generally,
    parameters like accuracy and statistical measures like precision and recall drive
    the selection of the model. We will be exploring the process of choosing the best
    model and terms like precision and recall in later chapters of the book. Once
    we choose the final model, we are ready to deploy the model in the production
    environment, where it will work on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: These are the broad steps in an ML project. Like any other project, there is
    a code repository, best practices, coding standards, common errors, pitfalls,
    etc., which we will discuss throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Types of ML algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'ML models affect decision-making and follow a statistical approach to solve
    a business problem. They work on historical data and find patterns and trends
    in it. The raw material is the historical data, which is analyzed and modeled
    to generate a predictive algorithm. The historical data available and the sort
    of problem that needs to be solved informs the ML approach that should be taken.
    ML algorithms can be split broadly into four classes: supervised learning, unsupervised
    learning, semisupervised learning, and reinforcement learning, as depicted in
    figure 1.10\. We will examine each of the four types in detail, with a focus on
    unsupervised learning—the topic of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: You might have heard about generative AI (GenAI) in the news. GenAI-based solutions
    generally start with unsupervised and may include supervised or reinforcement
    learning to specialize the model for certain tasks. We will discuss GenAI further
    throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F10_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.10 ML algorithms can be classified as supervised learning algorithms,
    unsupervised learning algorithms, semisupervised learning algorithms, and reinforcement
    learning algorithms.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 1.5.1 Supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Formally put, supervised models are statistical models that use both the input
    data and the desired output to predict the future. The output is the value that
    we wish to predict and is referred to as the *target variable,* and the data used
    to make that prediction is called *training data*. The target variable is sometimes
    referred to as the *label*. The various attributes or variables present in the
    data are called *independent variables*. Each of the historical data points or
    a *training example* contains these independent variables and the corresponding
    target variable. Supervised learning algorithms make a prediction for unseen future
    data. The accuracy of the solution depends on the training done and patterns learned
    from the labeled historical data. An example is described in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning problems are used in demand prediction, credit card fraud
    detection, customer churn prediction, premium estimation, etc. They are heavily
    used across domains like retail, telecom, banking and finance, aviation, insurance,
    and more and for functions like marketing, CRM, quality, supply chain, pricing,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning algorithms can be further broken into regression algorithms
    and classification algorithms. Let’s consider each of these in turn.
  prefs: []
  type: TYPE_NORMAL
- en: Regression algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Regression algorithms are supervised learning algorithms—that is, they require
    target variables that need to be predicted. These algorithms are used to predict
    the values of a *continuous* *variable*. Examples include revenue, amount of rainfall,
    number of transactions, production yield, and so on. In supervised classification
    problems, we predict a categorical variable like whether it will rain (yes/no),
    whether the credit card transaction is fraudulent or genuine, and so on. This
    is the main difference between classification and regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: Let us understand the regression problem with an example. Say we assume that
    the weight of a person is only dependent on height and not on other parameters
    like gender, ethnicity, diet, etc. In such a case, we want to predict the weight
    of a person based on height. The dataset and the graph plotted for the same data
    will look like figure 1.11.
  prefs: []
  type: TYPE_NORMAL
- en: A regression model will be able to find the inherent patterns in the data and
    fit a mathematical equation describing the relationship. It can then take height
    as an input and predict the weight. Here, height is the independent variable,
    and weight is the dependent variable or the target variable or the label we want
    to predict.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F11_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.11 Data and plot of relationship between height and weight that is
    used for regression problem
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'There are quite a few algorithms available for regression problems. Some of
    the major ones are as follows (although this list is certainly not exhaustive):'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-nearest neighbor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use any of the algorithms to solve this problem. We will explore more
    by using linear regression to solve a problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The linear regression algorithm models the relationship between dependent variables
    and target variables by assuming a linear relationship between them. The linear
    regression algorithm would result in a mathematical equation for the problem,
    shown in equation 1.1:'
  prefs: []
  type: TYPE_NORMAL
- en: (1.1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Weight = *β*[0] * height + *β*[1]
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally put, linear regression is used to fit a mathematical equation depicting
    the relationship between dependent and independent variables, shown as equation
    1.2:'
  prefs: []
  type: TYPE_NORMAL
- en: (1.2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Y = *β*[0] + *β*[1] x[1] + *β*[2]x[2] + ….+ *ε*
  prefs: []
  type: TYPE_NORMAL
- en: Here, *Y* is the target variable that we want to predict; *x*[1] is the first
    independent variable; *x*[2] is the second independent variable; *ε* is the error
    term in the equation; and *β*[0] is the intercept of the equation.
  prefs: []
  type: TYPE_NORMAL
- en: A simple visualization for a linear regression problem is shown in figure 1.12\.
    Here we have the x and Y variables where x is the independent variable and Y is
    the target variable. The objective of the linear regression problem is to find
    the *line of best fit,* which can explain the randomness present in the data.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F12_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.12 Raw data that needs to be modeled (left). Using regression, a line
    of best fit is identified (right).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Equation 1.2 is used to make predictions for the unseen data. There are variations
    in linear regression too, like simple linear regression, multiple linear regression,
    nonlinear regression, etc. Depending on the data at hand, we choose the correct
    algorithm. A complex dataset might require a nonlinear relationship between the
    various variables.
  prefs: []
  type: TYPE_NORMAL
- en: The next type of regression algorithm we shall explore is tree-based solutions.
    For tree-based algorithms like decision trees, random forests, etc., the algorithm
    will start from the top and then, like an `if`/`else` block, will split iteratively
    to create nodes and subnodes until we reach a terminal node (see figure 1.13).
    In the decision tree diagram, we start from the top with the root node, and then
    we perform splitting until we reach the endpoint, which is the terminal node.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are simple to comprehend and implement, and they are fast to
    train. Their usability lies in the fact that they are intuitive enough to understand
    without much technical background.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F13_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.13 A decision tree has a root node, and after splitting, we get a decision
    node and a terminal node, which is the final node and cannot be split further.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: There are other famous regression algorithms like k-nearest neighbor, gradient
    boosting, and deep learning–based solutions. Different regression algorithms are
    best suited to specific contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the effect of regression use cases, let’s consider a few business-relevant
    use cases that are implemented in the industry:'
  prefs: []
  type: TYPE_NORMAL
- en: An airport operations team is assessing staffing requirements and wants to estimate
    the amount of passenger traffic expected. The estimate will help the team prepare
    a plan regarding future resource requirements and will help in the optimization
    of the resources required. Regression algorithms can help in predicting the number
    of passengers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A retailer wants to understand the expected demand for the upcoming sales season
    so it can plan the inventory. This will result in cost savings and avoid stock-outs.
    Regression algorithms can help in such planning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A manufacturing plant wishes to improve the yield from the existing use of various
    molds and raw materials. The regression solutions can suggest the best combination
    of molds and predict the expected yield.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bank offers credit cards to its customers. Consider how the credit limit offered
    to new customers is calculated. Based on the attributes of customers like age,
    occupation, income, and previous transaction history, regression algorithms can
    help in suggesting credit limits at a customer level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An insurance company wants to come up with a premium table for its customers
    using historical claims. The risk can be assessed based on the historical data
    around driver details, car information, etc. Regression can surely help with such
    problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression problems form the foundation of supervised learning problems and
    are quite heavily used in the industry. Along with classification algorithms,
    they serve as a go-to solution for most of the predictive problems used in real-world
    business.
  prefs: []
  type: TYPE_NORMAL
- en: Classification algorithms
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Classification algorithms are used to predict the values of a categorical variable,
    which is the dependent variable. This target variable can be binary (yes/no, good/bad,
    fraud/genuine, pass/fail, etc.) or multiclass (such as positive/negative/neutral
    or yes/no/don’t know). Classification algorithms will ascertain whether the target
    event will happen by generating a probability score for the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: After the model has been trained on historical data, a classification algorithm
    will generate a probability score for the unseen dataset, which can be used to
    make the final decision. Depending on the number of classes present in the target
    variable, our business decision will vary. Let’s have a look at a use case for
    classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this: a telecom operator is facing a problem with its decreasing subscriber
    base. The number of existing subscribers is shrinking, and the telecom operator
    would like to arrest this churn of subscribers. For this purpose, an ML model
    is envisioned.'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the historical data or the training data available for model building
    might look like table 1.3\. These data points are only for illustration purposes
    and are not exhaustive. There can be many other significant variables available.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1.3 Example of a structured dataset for a telecom operator showing multiple
    data attributes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| ID | Revenue ($) | Duration of service (years) | Avg. cost | Monthly usage
    (days) | Churned (Y/N) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1001  | 100  | 1.1  | 0.10  | 10  | Y  |'
  prefs: []
  type: TYPE_TB
- en: '| 1002  | 200  | 4.1  | 0.09  | 25  | N  |'
  prefs: []
  type: TYPE_TB
- en: '| 1003  | 300  | 5.2  | 0.05  | 28  | N  |'
  prefs: []
  type: TYPE_TB
- en: '| 1004  | 200  | 0.9  | 0.25  | 11  | Y  |'
  prefs: []
  type: TYPE_TB
- en: '| 1005  | 100  | 0.5  | 0.45  | 12  | Y  |'
  prefs: []
  type: TYPE_TB
- en: In the example in table 1.3, the dataset comprises the past usage data of subscribers.
    The last column (Churned) depicts if that subscriber churned out of the system
    or not. For example, subscriber 1001 churned while 1002 did not. Hence, the business
    problem is to build an ML model based on this historical data and predict if a
    new unseen customer will churn or not.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the churned status (yes/no) is the target variable. It is also referred
    to as the dependent variable. The other attributes like revenue, duration, average
    cost, monthly usage, etc., are independent variables that are used to create the
    ML model. The historical data is called the training data. Before the training
    of the model, the trained supervised learning model will generate prediction probabilities
    for a new customer.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are quite a few algorithms available for classification problems; the
    major ones are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-nearest neighbor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naïve Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support vector machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the most popular classification algorithms is logistic regression. Logistic
    regression uses a logit function to model the classification problem. If we are
    solving for a binary classification problem, it will be binary logistic regression
    or multiple logistic regression. Similar to linear regression, logistic regression
    also fits an equation, albeit it uses a sigmoid function to generate the probability
    score for the event to happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sigmoid function is a mathematical function that has a characteristic S-shaped
    curve or a sigmoid curve. The mathematical equation of a sigmoid function is shown
    in equation 3.1:'
  prefs: []
  type: TYPE_NORMAL
- en: (1.3)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*S*(*x*) = 1/(1 + *e*^–^(*x*))'
  prefs: []
  type: TYPE_NORMAL
- en: which can be rewritten as equation 1.4
  prefs: []
  type: TYPE_NORMAL
- en: (1.4)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*S*(*x*) = *e*^(*x*)/(*e*^(*x*) + 1)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Logistic regression uses the sigmoid function. The equation used in the logistic
    regression problem is shown in equation 1.5:'
  prefs: []
  type: TYPE_NORMAL
- en: (1.5)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: log (p/1 – p) = *β*[0] + *β*[1] x[1]
  prefs: []
  type: TYPE_NORMAL
- en: where *p* is the probability of the event happening; *β*[0] is the intercept
    term; *β*[1] is the coefficient for the independent variable *x*[1]; log(*p*/1
    – *p*) is called the logit; and (*p*/1 – *p*) is the odds. As depicted in figure
    1.14, if we try to fit a linear regression equation for the probability function,
    it will not do a good job. We want to obtain the probability scores (i.e., a value
    between 0 and 1). The linear regression will not only return values between 0
    and 1 but also probability scores that are greater than 1 or less than 0\. Hence,
    we have a sigmoid function at right in the figure, which generates probability
    scores for us between 0 and 1 only.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F14_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.14 A linear regression model will not be able to do justice (left);
    hence, we have logistic regression for classification. Linear regression can generate
    probability scores more than 1 or less than 0 too, which is mathematically incorrect,
    whereas the sigmoid function generates probability scores between 0 and 1 only
    (right).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The logistic regression algorithm is one of the most widely used techniques
    for classification problems. It is easy to train and deploy and is often the benchmark
    algorithm whenever we start any supervised classification learning project.
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based algorithms like decision trees and random forests can also be used
    for classification problems. The other algorithms are also used as per the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.2 Unsupervised algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imagine you are given some paper labels, as shown in figure 1.15\. The task
    is to arrange them by similarity. Now, there are multiple approaches to that problem.
    You can use color, shape, or size. Here, we do not have any label to guide this
    arrangement. This is what makes unsupervised algorithms different.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F15_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.15 Example of various shapes that can be grouped together using different
    parameters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Formally put, unsupervised learning only takes the input data and then finds
    patterns in it without referencing the target variable. An unsupervised learning
    algorithm therefore reacts based on the presence or lack of patterns in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning is hence used for pattern detection, exploring the insights
    in the dataset and understanding the structure of it, segmentation, and anomaly
    detection.
  prefs: []
  type: TYPE_NORMAL
- en: We can understand unsupervised learning algorithms by looking at figure 1.16\.
    The figure on the left shows the raw data points represented in a vector space
    diagram. On the right is the clustering, which will be done using an unsupervised
    learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F16_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.16 An unsupervised learning algorithm finds patterns in the data on
    the left and results in clusters on the right.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Some use cases for unsupervised algorithms are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A retail group wants to understand its customers better. The task is to improve
    the customer’s stickiness, revenue, number of visits, basket size, etc. Customer
    segmentation using unsupervised learning can be done here. Depending on the customer’s
    attributes like revenue, number of visits, last visit date, age since joining,
    demographic attributes, etc., the segmentation will result in clusters that can
    be targeted personally. The result will be improved customer experience, increased
    customer lifetime value, etc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A network provider needs to create an anomaly detection system. The historical
    data will serve as the anomalies data. The unsupervised learning algorithm will
    be able to find patterns, and the outliers will be given out by the algorithm.
    The distinguished anomalies will be the ones that need to be addressed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A medical product company wishes to find if there are any underlying patterns
    in the image data of its patients. If there are any patterns and factors, those
    patients can be treated better, and maybe they require a different approach. Unsupervised
    learning can help with the image data, which will help address the patients’ needs
    better.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A digital marketing company wants to understand the “unknowns” in the incoming
    customer data like social media interactions, page clicks, comments, stars, etc.
    This understanding will help improve customers’ recommendations and overall purchasing
    experience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms offer flexibility and performance when it comes
    to finding patterns. They are usable for all kinds of data—the core topic of this
    book—including structured data, text, or images.
  prefs: []
  type: TYPE_NORMAL
- en: The major unsupervised learning algorithms are
  prefs: []
  type: TYPE_NORMAL
- en: Clustering algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBSCAN clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectral clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singular value decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Association rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: t-distributed stochastic neighbor embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We cover all these algorithms in detail in the coming chapters. We will examine
    the mathematical concepts, the hidden processes, Python implementation, and the
    best practices throughout the book. Let’s first understand the basic process by
    means of a case study.
  prefs: []
  type: TYPE_NORMAL
- en: A retailer wants to develop a deeper understanding of its consumer base and
    then wants to offer personalized recommendations, promotions, discounts, offers,
    etc. The entire customer dataset should be segmented using attributes like persona,
    previous purchase, response, external data, and so on (see figure 1.17).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F17_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.17 Steps in an unsupervised learning algorithm from data sources to
    the final solution ready for deployment
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For the use case, the steps in an unsupervised learning project are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We start the project by defining the business problem. We wish to understand
    the customer base better. A customer segmentation approach can be a good solution.
    We want segments that are distinguishable using mathematical KPIs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is the data discovery phase. All the various datasets, like customer details,
    purchase histories, social media data, portfolios, etc., are identified and accessed.
    The data tables to be used are finalized in this step. Then, all the data tables
    are generally loaded into a common database, which we will use to analyze, test,
    and learn.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we have access to the data. The next step is to clean it and make it usable.
    We treat all the null values, NaN, junk values, duplicates, etc.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the data is clean and ready to be used, we perform an exploratory data
    analysis of it. Usually, during exploratory analysis, we identify patterns, cyclicity,
    aberrations, max-min range, standard deviation, etc. The outputs of the exploratory
    data analysis stage will be insights and understandings. We will also generate
    a few graphs and charts, as shown in figure 1.18\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We begin with the unsupervised approach now. We want to implement clustering
    methods, and hence we can try a few clustering methods like k-means, hierarchical
    clustering, etc. The clustering algorithms will result in homogeneous segments
    of customers based on their various attributes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F18_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.18 Examples of the graphs and charts from the exploratory data analysis
    of the data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the case study, we will be working on the past two to three years of data,
    which is the training data. Since we are using an unsupervised approach, there
    is no target variable here. The algorithm will merge the customer segments that
    behave alike using their transactional patterns, their demographic patterns, and
    their purchase preferences. It will look like the results in figure 1.19.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH01_F19_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.19 Output of the clustering algorithm where we can segment customers
    using various attributes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6\. We now check how the various algorithms have performed; in other words,
    we will compare the accuracy of each algorithm. The final clustering algorithm
    chosen will result in homogeneous segments of customers, which can be targeted
    and offered customized offers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 7\. We discuss the results with the business stakeholders and make iterations
    based on the feedback.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 8\. We deploy the solution in the production environment and are ready to work
    on new unseen datasets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These are the broad steps in an unsupervised problem. Algorithm creation and
    selection are tedious tasks. We will be studying these in detail later in the
    book.
  prefs: []
  type: TYPE_NORMAL
- en: GenAI most often starts with an unsupervised stage. This stage enables the model
    to learn patterns, structures, and relationships without explicit labels. It is
    sometimes referred to as the pretraining stage. Once the model has been pretrained,
    we move to the supervised stage. Here, the pretrained model is tailored to a specific
    task or domain using a labeled dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.3 Semisupervised algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Semisupervised learning is a middle path of the supervised and unsupervised
    approaches. The primary reason for a semisupervised approach is the lack of availability
    of a complete *labeled* dataset for training. Formally put, the semisupervised
    approach uses both supervised and unsupervised approaches: supervised to classify
    the data points and unsupervised to group them together.'
  prefs: []
  type: TYPE_NORMAL
- en: In semisupervised learning, we train initially on a smaller number of labeled
    data points available using a supervised algorithm. Then we use it to label or
    *pseudo-label* new data points. The two datasets (labeled and pseudo-labeled)
    are combined, and we use this dataset for further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Semisupervised algorithms are used in cases where the dataset is partially available,
    like images in the medical industry. If we are creating a cancer detection solution
    by analyzing the images of patients, we will likely not have enough sample sets
    of training images. Here, the semisupervised approach can be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: 1.5.4 Reinforcement learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Imagine you are playing a game of chess with a computer, and it goes like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Round 1*—You win after 5 moves.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Round 2*—You win after 8 moves.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Round 3*—You win after 14 moves.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Round 4*—You win after 21 moves.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Round 5*—The computer wins!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is happening here is the algorithm is training itself iteratively depending
    on each interaction and then correcting/improving itself.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, reinforcement learning solutions are self-sustained solutions that
    train themselves using a sequence of trial and error. One sequence follows the
    other. The heart of reinforcement learning is reward signals. If the action is
    positive, the reward is positive, indicating to continue. If the action is negative,
    the reward will penalize the activity. Hence, the solution will always correct
    itself and move ahead, thereby improving itself iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: Self-driving cars are the best examples of reinforcement learning algorithms.
    They detect when they should turn left or right, when to move, and when to stop.
    Modern video games also employ reinforcement learning algorithms. Reinforcement
    learning allows us to break the barriers of technology and imagine things that
    were earlier thought impossible.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have covered the different types of ML algorithms. Together, they
    are harnessing the true power of data and creating a long-lasting effect on our
    lives. But the heart of the solutions is the technology, which we have not discussed
    yet. We now move to the technology stack required to make these solutions tick.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 1.1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Use these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: Why is ML so powerful that it is being used very heavily now?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the different types of ML algorithms, and how are they different from
    each other?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the steps in an ML project?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the role of data engineering, and why is it important?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the various tools available for ML?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 1.6 Concluding thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A common question is: Which is better, R or Python? Both are fantastic languages.
    Both are heavily used. But after the introduction of TensorFlow, Keras’s libraries
    on AI, the balance has slightly tilted in favor of Python.'
  prefs: []
  type: TYPE_NORMAL
- en: You’ve now taken your first step in the journey toward learning unsupervised
    machine learning techniques. It is time to wrap up.
  prefs: []
  type: TYPE_NORMAL
- en: ML and AI are indeed pathbreaking. They are changing the way we travel, order
    food, plan, buy, see a doctor, order prescriptions—they are making a dent everywhere.
    ML is indeed a powerful capability that is paving the path for the future and
    is proving much better than existing technology stacks when it comes to pattern
    identification, anomaly detection, customizations, and automation of tasks. Autonomous
    driving, cancer detection, fraud identification, facial recognition, image captioning,
    and chatbots are only a few examples where ML and AI are outperforming traditional
    technologies. And now is the best time to enter this field. This sector is attracting
    investments from almost all business functions. The field has created thousands
    of job opportunities across the spectrum.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the same time, the field lacks trained professionals: data analysts, data
    engineers, visualization experts, data scientists, and data practitioners. They
    are all rare breeds now. The field requires a regular supply of budding talents
    who will become the leaders of tomorrow and will make data-driven decisions. We
    have only scratched the surface of understanding the power of data—there are still
    miles to be covered.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following chapter, we will dive deeper into the unsupervised learning
    concepts of clustering. The mathematical and statistical foundations, a pragmatic
    case study, and Python implementation are discussed. The discussion includes the
    simpler clustering algorithms: k-means clustering, hierarchical clustering, and
    DBSCAN. In the later chapters of the book, we will study more complex clustering
    topics like Gaussian mixture modeling clustering, time series clustering, fuzzy
    clustering, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data can be conceptualized as an interconnected set of facts and statistics
    necessary for analysis, characterized by unique traits and governed by specific
    management principles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-world activities such as mobile calls, online transactions, and social
    media interactions continually generate data, underscoring its omnipresence in
    modern life.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raw data requires cleaning, organization, and analysis to be converted effectively
    into information and insights that can drive business decisions and actions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data can be broadly classified into structured datasets, which follow a clear
    row-column format, and unstructured datasets, like text and images, which require
    more advanced analysis techniques.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To analyze unstructured data, we typically transform it into numerical representations,
    often utilizing deep learning models such as CNNs and RNNs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A clear, concise, achievable, and measurable business problem is a vital step
    to ensure the success of a data science project.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High-quality data is essential for reliable analysis and is characterized by
    attributes such as completeness, validity, accuracy, representativeness, availability,
    consistency, timeliness, and integrity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Effective data engineering and management are crucial for preparing data for
    analysis involving techniques like ETL processes and data cleaning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The role of UI/UX is of paramount importance to ensure adoption and usage by
    the end consumers; otherwise, ML will just be a shiny piece sitting on a shelf.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interconnected fields like data analysis, ML, AI, and business intelligence
    each play a critical role in processing and deriving insights from data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning is an ML approach that uses existing data to predict future
    outcomes, common in tasks like demand prediction and fraud detection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supervised learning is divided into regression and classification tasks, each
    with numerous available algorithms to model quantitative or categorical outcomes,
    respectively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning algorithms discover patterns and relationships in data
    independently of predefined target variables, useful in activities like segmentation
    and anomaly detection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variants of unsupervised learning algorithms include clustering techniques and
    methods for reducing data dimensionality, offering flexibility and performance
    in pattern recognition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semisupervised learning bridges supervised and unsupervised methods and is effective
    when dealing with datasets that are partially labeled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning involves systems that learn by trial and error, rewarding
    desired outcomes, and are applied in dynamic decision-making tasks, such as autonomous
    vehicle navigation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technological solutions are at the heart of modern data-driven strategies, and
    understanding the technological stack is essential to maximize the effect and
    benefits of data solutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
