<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">11 <a id="idTextAnchor000"/>Building a generative pretrained Transformer from scratch</h1>
<p class="co-summary-head"><a id="marker-238"/>This chapter covers<a id="idIndexMarker000"/></p>
<ul class="calibre5">
<li class="co-summary-bullet">Building a generative pretrained Transformer from scratch</li>
<li class="co-summary-bullet">Causal self-attention</li>
<li class="co-summary-bullet">Extracting and loading weights from a pretrained model</li>
<li class="co-summary-bullet">Generating coherent text with GPT-2, the predecessor of ChatGPT and GPT-4</li>
</ul>
<p class="body">Generative Pretrained Transformer 2 (GPT-2) is an advanced large language model (LLM) developed by OpenAI and announced in February 2019. It represents a significant milestone in the field of natural language processing (NLP) and has paved the way for the development of even more sophisticated models, including its successors, ChatGPT and GPT-4. <a id="idIndexMarker001"/><a id="idIndexMarker002"/></p>
<p class="body">GPT-2, an improvement over its predecessor, GPT-1, was designed to generate coherent and contextually relevant text based on a given prompt, demonstrating a remarkable ability to mimic human-like text generation across various styles and topics. Upon its announcement, OpenAI initially decided not to release to the public the most powerful version of GPT-2 (also the one you’ll build from scratch in this chapter, with 1.5 billion parameters). The main concern was potential misuse, such as generating misleading news articles, impersonating individuals online, or automating the production of abusive or fake content. This decision sparked a significant debate within the AI and tech communities about the ethics of AI development and the balance between innovation and safety.</p>
<p class="body">OpenAI later adopted a staggered release strategy, gradually making smaller versions of the model available while monitoring the effect and exploring safe deployment strategies. Eventually, in November 2019, OpenAI released the full model, along with several datasets and a tool to detect model-generated text, contributing to discussions on responsible AI usage. Because of this release, you’ll learn to extract the pretrained weights from GPT-2 and load them to the GPT-2 model that you create.</p>
<p class="body">GPT-2 is based on the Transformer architecture that we discussed in chapters 9 and 10. However, unlike the English-to-French translator you created before, GPT-2 is a decoder-only Transformer, meaning there is no encoder stack in the model. When translating an English phrase into French, the encoder captures the meaning of the English phrase and passes it to the decoder to generate the translation. However, in text generation tasks, the model does not need an encoder to understand a different language. Instead, it generates text based on the previous tokens in the sentence, using only a decoder-only architecture. Like other Transformer models, GPT-2 uses self-attention mechanisms to process input data in parallel, significantly improving the efficiency and effectiveness of training LLMs.</p>
<p class="body">GPT-2 is pretrained on a large corpus of text data, essentially predicting the next word in a sentence given the words that precede it. This training enables the model to learn a wide range of language patterns, grammar, and knowledge.</p>
<p class="body"><a id="marker-239"/>In this chapter, you’ll learn to build GPT-2XL, the largest version of GPT-2, from scratch. After that, you’ll learn how to extract the pretrained weights from Hugging Face (an AI community that hosts and collaborates on machine learning models, datasets, and applications) and load them to your own GPT-2 model. You’ll use your GPT-2 to generate text by feeding a prompt to the model. GPT-2 calculates the probabilities of possible next tokens and samples from these probabilities. It can produce coherent and contextually relevant paragraphs of text based on the input prompt it receives. Additionally, as you did in chapter 8, you can control the creativeness of the generated text by using <code class="fm-code-in-text">temperature</code> and <code class="fm-code-in-text">top-K</code> sampling. <a id="idIndexMarker003"/><a id="idIndexMarker004"/></p>
<p class="body">While GPT-2 marks a notable advance in NLP, it’s essential to moderate your expectations and recognize its inherent limitations. It’s crucial not to compare GPT-2 with ChatGPT or GPT-4 directly, as GPT-2XL has only 1.5 billion parameters compared to ChatGPT’s 175 billion and GPT-4’s estimated 1.76 trillion parameters. One of the main limitations of GPT-2 is its lack of genuine comprehension of the content it generates. The model predicts the next word in a sequence based on the probability distribution of words in its training data, which can produce syntactically correct and seemingly logical text. However, the model lacks a true understanding of the meaning behind the words, leading to potential inaccuracies, nonsensical statements, or superficial content.</p>
<p class="body">Another key factor is GPT-2’s limited contextual awareness. While it can maintain coherence over short spans of text, it struggles with longer passages, potentially resulting in a loss of coherence, contradictions, or irrelevant content. We should be cautious not to overestimate the model’s ability to generate long-form content that requires sustained attention to context and detail. Therefore, while GPT-2 represents a significant step forward in NLP, it’s important to approach its generated text with a healthy dose of skepticism and set realistic expectations.<a id="idIndexMarker005"/></p>
<h2 class="fm-head" id="heading_id_3">11.1 GPT-2 architecture and causal self-attention</h2>
<p class="body"><a id="marker-240"/>GPT-2 operates as a solely decoder-based Transformer (it generates text based on previous tokens in the sentence without the need for an encoder to understand a different language), mirroring the decoder component of the English-to-French translator discussed in chapters 9 and 10. Unlike its bilingual counterpart, GPT-2 lacks an encoder and thus does not incorporate encoder-derived inputs in its output generation process. The model relies entirely on preceding tokens within the sequence to produce its output.<a id="idIndexMarker006"/><a id="idIndexMarker007"/><a id="idIndexMarker008"/></p>
<p class="body">In this section, we’ll discuss the architecture of GPT-2. We will also dive into the causal self-attention mechanism, which is the core of the GPT-2 model.</p>
<h3 class="fm-head1" id="heading_id_4">11.1.1 The architecture of GPT-2</h3>
<p class="body">GPT-2 comes in four different sizes: small (S), medium (M), large (L), and extra-large (XL), each varying in capability. Our primary focus will be on the most powerful version, GPT-2XL. The smallest GPT-2 model has around 124 million parameters, while the extra-large version has about 1.5 billion parameters. It is the most powerful among the GPT-2 models, with the highest number of parameters. GPT-2XL can understand complex contexts, generating coherent and nuanced text.<a id="idIndexMarker009"/><a id="idIndexMarker010"/></p>
<p class="body">GPT-2 consists of many identical decoder blocks. The extra-large version has 48 decoder blocks, while the other three versions have 12, 24, and 36 decoder blocks, respectively. Each of these decoder blocks comprises two distinct sublayers. The first sublayer is a causal self-attention layer, which I’ll explain in detail soon. The second sublayer is a basic, position-wise, fully connected feed-forward network, as we have seen in the encoder and decoder blocks in the English-to-French translator. Each sublayer incorporates layer normalization and a residual connection to stabilize the training process.</p>
<p class="body">Figure 11.1 is a diagram of the architecture of GPT-2.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="593" src="../../OEBPS/Images/CH11_F01_Liu.png" width="613"/></p>
<p class="figurecaption">Figure 11.1 The architecture of the GPT-2 model. GPT-2 is a decoder-only Transformer, consisting of N identical decoder layers. Each decoder block contains two sublayers. The first sublayer is a causal self-attention layer. The second is a feed-forward network. Each sublayer uses layer normalization and a residual connection. The input is first passed through word embedding and positional encoding, and the sum is then passed through the decoder. The output from the decoder goes through layer normalization and a linear layer.</p>
</div>
<p class="body"><a id="marker-241"/>GPT-2 first passes indexes for a sequence of tokens through word embedding and positional encoding to obtain input embedding (I’ll explain soon how this process works). The input embedding is passed through N decoder blocks sequentially. After that, the output is passed through layer normalization and a linear layer. The number of outputs in GPT-2 is the number of unique tokens in the vocabulary (50,257 tokens for all GPT-2 versions). The model is designed to predict the next token based on all previous tokens in the sequence.</p>
<p class="body">To train GPT-2, OpenAI used a dataset called WebText, which was collected automatically from the internet. The dataset contained a wide variety of text, including websites like Reddit links that were highly upvoted, aiming to cover a broad spectrum of human languages and topics. This dataset is estimated to contain about 40GB of text. <a id="idIndexMarker011"/></p>
<p class="body">The training data was broken into sequences of a fixed length (1,024 tokens for all GPT-2 versions) and used as inputs. The sequences were shifted to the right by one token and used as outputs to the model during training. Since the model uses causal self-attention, in which future tokens in a sequence are masked (i.e., hidden) during the training process, this is effectively training the model to predict the next token based on all previous tokens in the sequence.</p>
<h3 class="fm-head1" id="heading_id_5">11.1.2 Word embedding and positional encoding in GPT-2</h3>
<p class="body">GPT-2 uses a subword tokenization method called the Byte Pair Encoder (BPE) to break text into individual tokens (whole words or punctuation marks in most cases but syllables for uncommon words). These tokens are then mapped into an index between 0 and 50,256 since the vocabulary size is 50,257. GPT-2 transforms text in the training data into vector representations that capture its meaning through word embedding, similar to what you’ve done in the previous two chapters. <a id="idIndexMarker012"/><a id="idIndexMarker013"/><a id="idIndexMarker014"/><a id="idIndexMarker015"/><a id="idIndexMarker016"/><a id="marker-242"/></p>
<p class="body">To give you a concrete example, the phrase “this is a prompt” is first converted into four tokens through BPE tokenization, <code class="fm-code-in-text">['this', ' is', ' a', ' prompt']</code>. Each token is then represented by a one-hot variable of size 50,257. The GPT-2 model passes them through a word embedding layer to compress them into condensed vectors with floating point values of a much smaller size, such as a length of 1,600 in GPT-2XL (the lengths are 768, 1,024, and 1,280, for the other three versions of GPT-2, respectively). With word embedding, the phrase “this is a prompt” is represented by a matrix with size <span class="times">4 <span class="cambria">×</span> 1,600</span> instead of the original <span class="times">4 <span class="cambria">×</span> 50,257</span>. Word embedding significantly reduces the number of the model’s parameters and makes training more efficient. The left side of figure 11.2 depicts how word embedding works.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="301" src="../../OEBPS/Images/CH11_F02_Liu.png" width="452"/></p>
<p class="figurecaption">Figure 11.2 GPT-2 first represents each token in a sequence with a 50,276-value one-hot vector. The token representation of the sequence goes through a word embedding layer to compress it into an embedding with a dimension of 1,600. GPT-2 also represents each position in a sequence with a 1,024-value one-hot vector. The positional representation of the sequence goes through a positional encoding layer to compress it into an embedding also with a dimension of 1,600. The word embedding and positional encoding are added together to form the input embedding.</p>
</div>
<p class="body">GPT-2, like other Transformers, processes input data in parallel, and this inherently doesn’t allow it to recognize the sequence order of the input. To address this, we need to add positional encodings to the input embeddings. GPT-2 adopts a unique approach to positional encoding, diverging from the methodology outlined in the seminal 2017 “Attention Is All You Need” paper. Instead, GPT-2’s technique for positional encoding parallels that of word embeddings. Given the model’s capacity to handle up to 1,024 tokens in an input sequence, each position within the sequence is initially denoted by a one-hot vector of the same size. For instance, in the sequence “this is a prompt,” the first token is represented by a one-hot vector where all elements are zero except for the first, which is set to one. The second token follows suit, represented by a vector where all but the second element are zero. Consequently, the positional representation for the phrase “this is a prompt” manifests as a <span class="times">4 <span class="cambria">×</span> 1,024 matrix</span>, as illustrated in the upper right section of figure 11.2.</p>
<p class="body">To generate positional encoding, the sequence’s positional representation undergoes processing through a linear neural network, which is dimensioned at <span class="times">1,024 <span class="cambria">×</span> 1,600</span>. The weights within this network are randomly initialized and subsequently refined through the training process. As a result, the positional encoding for each token in the sequence is a 1,600-value vector, matching the dimension of the word embedding vector. A sequence’s input embedding is the sum of its word embedding and positional encoding, as depicted at the bottom of figure 11.2. In the context of the phrase “this is a prompt,” both the word embedding and positional encoding are structured as <span class="times">4 <span class="cambria">×</span> 1,600 matrices</span>. Therefore, the input embedding for “this is a prompt,” which is the sum of these two matrices, maintains a dimensionality of <span class="times">4 <span class="cambria">×</span> 1,600</span>.</p>
<h3 class="fm-head1" id="heading_id_6">11.1.3 Causal self-attention in GPT-2</h3>
<p class="body">Causal self-attention is a crucial mechanism within the GPT-2 model (and broadly in the GPT series of models), enabling the model to generate text by conditioning on the sequence of previously generated tokens. It’s similar to the masked self-attention in the first sublayer of each decoder layer in the English-to-French translator we discussed in chapters 9 and 10, though the implementation differs slightly.<a id="marker-243"/><a id="idIndexMarker017"/><a id="idIndexMarker018"/></p>
<p class="fm-callout"><span class="fm-callout-head">Note</span> The concept of “causal” in this context refers to the model’s ability to ensure that predictions for a given token can only be influenced by the tokens that precede it in the sequence, respecting the causal (time-forward) direction of text generation. This is essential for generating coherent and contextually relevant text outputs.</p>
<p class="body">Self-attention is a mechanism that allows each token in the input sequence to attend to all other tokens in the same sequence. In the context of Transformer models like GPT-2, self-attention enables the model to weigh the importance of other tokens when processing a specific token, thereby capturing the context and relationships between words in a sentence.</p>
<p class="body">To ensure causality, GPT-2’s self-attention mechanism is modified so that any given token can only attend to itself and the tokens that have come before it in the sequence. This is achieved by masking future tokens (i.e., tokens that come after the current token in the sequence) in the attention calculation, ensuring that the model cannot “see” or be influenced by future tokens when predicting the next token in a sequence. For example, in the phrase “this is a prompt,” the mask hides the last three words in the first time step when the model uses the word “this” to predict the word “is.” To implement this, positions corresponding to future tokens are set to minus infinity when we compute the attention scores. After softmax activation, future tokens are allocated zero weights, effectively removing them from the attention calculation.</p>
<p class="body">Let’s use a concrete example to illustrate exactly how causal self-attention works in code. The input embedding for the phrase “this is a prompt” is a <span class="times">4 <span class="cambria">×</span> 1,600 matrix</span> after word embedding and positional encoding. We then pass this input embedding through N decoder layers in GPT-2. In each decoder layer, it first goes through the causal self-attention sublayer as follows. The input embedding is passed through three neural networks to create query <span class="times">Q</span>, key <span class="times">K</span>, and value <span class="times">V</span>, as shown in the following listing.<a id="marker-244"/></p>
<p class="fm-code-listing-caption">Listing 11.1 Creating <code class="fm-code-in-text">query</code>, <code class="fm-code-in-text">key</code>, and <code class="fm-code-in-text">value</code> vectors</p>
<pre class="programlisting">import torch
import torch.nn as nn
  
torch.manual_seed(42)
x=torch.randn((1,4,1600))                        <span class="fm-combinumeral">①</span>
c_attn=nn.Linear(1600,1600*3)                    <span class="fm-combinumeral">②</span>
B,T,C=x.size()
q,k,v=c_attn(x).split(1600,dim=2)                <span class="fm-combinumeral">③</span>
print(f"the shape of Q vector is {q.size()}")
print(f"the shape of K vector is {k.size()}")
print(f"the shape of V vector is {v.size()}")    <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates three neural networks</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates an input embedding x</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Passes the input embedding the three neural networks to create <span class="times">Q</span>, <span class="times">K</span>, and <span class="times">V</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Prints out the sizes of <span class="times">Q</span>, <span class="times">K</span>, and <span class="times">V</span></p>
<p class="body">We first create a matrix with size <span class="times">4 <span class="cambria">×</span> 1,600</span>, the same size as the input embedding for “this is a prompt”. We then pass the input embedding through three neural networks, each with a size of <span class="times">1,600 <span class="cambria">×</span> 1,600</span>, to obtain query <span class="times">Q</span>, key <span class="times">K</span>, and value <span class="times">V</span>. If you run the preceding code block, you’ll see the following output:</p>
<pre class="programlisting">the shape of Q vector is torch.Size([1, 4, 1600])
the shape of K vector is torch.Size([1, 4, 1600])
the shape of V vector is torch.Size([1, 4, 1600])</pre>
<p class="body">The shapes of <span class="times">Q</span>, <span class="times">K</span>, and <span class="times">V</span> are all 4 <span class="cambria">×</span> 1,600. Next, instead of using one head, we split them into 25 parallel heads. Each head pays attention to different parts or aspects of the input, enabling the model to capture a broader range of information and form a more detailed and contextual understanding of the input data. As a result, we have 25 sets of <span class="times">Q</span>, <span class="times">K</span>, and <span class="times">V</span>:</p>
<pre class="programlisting">hs=C//25
k = k.view(B, T, 25, hs).transpose(1, 2) 
q = q.view(B, T, 25, hs).transpose(1, 2) 
v = v.view(B, T, 25, hs).transpose(1, 2)         <span class="fm-combinumeral">①</span>
print(f"the shape of Q vector is {q.size()}")
print(f"the shape of K vector is {k.size()}")
print(f"the shape of V vector is {v.size()}")    <span class="fm-combinumeral">②</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Splits <span class="times">Q</span>, <span class="times">K</span>, and <span class="times">V</span> into 25 heads</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Prints out the size of the multihead <span class="times">Q</span>, <span class="times">K</span>, and <span class="times">V</span></p>
<p class="body">If you run the preceding code block, you’ll see the following output:</p>
<pre class="programlisting">the shape of Q vector is torch.Size([1, 25, 4, 64])
the shape of K vector is torch.Size([1, 25, 4, 64])
the shape of V vector is torch.Size([1, 25, 4, 64])</pre>
<p class="body">The shapes of <span class="times">Q</span>, <span class="times">K</span>, and <span class="times">V</span> are now <span class="times">25 <span class="cambria">×</span> 4 <span class="cambria">×</span> 64</span>: this means we have 25 heads; each head has a set of query, key, and value, all having a size of <span class="times">4 <span class="cambria">×</span> 64</span>.</p>
<p class="body">Next, we calculate the scaled attention scores in each head:</p>
<pre class="programlisting">import math
scaled_att = (q @ k.transpose(-2, -1)) *\
            (1.0 / math.sqrt(k.size(-1)))
print(scaled_att[0,0])</pre>
<p class="body"><a id="marker-245"/>The scaled attention scores are the dot product of <span class="times">Q</span> and <span class="times">K</span> in each head, scaled by the square root of the dimension of <span class="times">K</span>, which is <span class="times">1,600/25 = 64</span>. The scaled attention scores form a <span class="times">4 <span class="cambria">×</span> 4</span> matrix in each head, and we print out those in the first head:</p>
<pre class="programlisting">tensor([[ 0.2334,  0.1385, -0.1305,  0.2664],
        [ 0.2916,  0.1044,  0.0095,  0.0993],
        [ 0.8250,  0.2454,  0.0214,  0.8667],
        [-0.1557,  0.2034,  0.2172, -0.2740]], grad_fn=&lt;SelectBackward0&gt;)</pre>
<p class="body">The scaled attention scores in the first head are also shown in the bottom left table in figure 11.3.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 11.1</p>
<p class="fm-sidebar-text">The tensor <code class="fm-code-in-text1">scaled_att</code> contains the scaled attention scores in the 25 heads. We have printed out those in the first head previously. How do you print out the scaled attention scores in the second head?</p>
</div>
<p class="body">Next, we apply a mask to the scaled attention scores to hide future tokens in the sequence:</p>
<pre class="programlisting">mask=torch.tril(torch.ones(4,4))              <span class="fm-combinumeral">①</span>
print(mask)
masked_scaled_att=scaled_att.masked_fill(\
    mask == 0, float('-inf'))                 <span class="fm-combinumeral">②</span>
print(masked_scaled_att[0,0])</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a mask</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Applies the mask on the scaled attention scores by changing the values to –∞ for future tokens</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="415" src="../../OEBPS/Images/CH11_F03_Liu.png" width="655"/></p>
<p class="figurecaption">Figure 11.3 How to calculate masked attention weights in causal self-attention. A mask is applied to the scaled attention scores so that values corresponding to future tokens (those above the main diagonal in the<a id="idTextAnchor001"/> matrix) become –∞. We then apply the softmax function on the masked scaled attention scores and obtain the masked attention weights. The masking ensures that predictions for a given token can only be influenced by the tokens that precede it in the sequence, not by future tokens. This is essential for generating coherent and contextually relevant text outputs.<a id="marker-24"/></p>
</div>
<p class="body"><a id="marker-246"/>If you run the preceding code, you’ll see the following output:</p>
<pre class="programlisting">tensor([[1., 0., 0., 0.],
        [1., 1., 0., 0.],
        [1., 1., 1., 0.],
        [1., 1., 1., 1.]])
tensor([[ 0.2334,    -inf,    -inf,    -inf],
        [ 0.2916,  0.1044,    -inf,    -inf],
        [ 0.8250,  0.2454,  0.0214,    -inf],
        [-0.1557,  0.2034,  0.2172, -0.2740]], grad_fn=&lt;SelectBackward0&gt;)</pre>
<p class="body">The mask is a <span class="times">4 <span class="cambria">×</span> 4</span> matrix as shown at the top of figure 11.3. The lower half of the mask (values below the main diagonal) are 1s while the upper half of the mask (values above the main diagonal) are 0s. When this mask is applied to the scaled attention scores, the values in the upper half of the matrix become –∞ (the middle bottom of figure 11.3). This way, when we apply the softmax function on the scaled attention scores, the upper half of the attention weights matrix is filled with 0s (bottom right of figure 11.3):</p>
<pre class="programlisting">import torch.nn.functional as F
att = F.softmax(masked_scaled_att, dim=-1)
print(att[0,0])</pre>
<p class="body">We print out the attention weights in the first head with the following values:</p>
<pre class="programlisting">tensor([[1.0000, 0.0000, 0.0000, 0.0000],
        [0.5467, 0.4533, 0.0000, 0.0000],
        [0.4980, 0.2790, 0.2230, 0.0000],
        [0.2095, 0.3001, 0.3042, 0.1862]], grad_fn=&lt;SelectBackward0&gt;)</pre>
<p class="body">The first row means in the first time step, the token “this” attends only to itself and not to any future tokens. Similarly, if you look at the second row, the tokens “this is” attend to each other but not to future tokens “a prompt”.</p>
<p class="fm-callout"><span class="fm-callout-head">Note</span> The weights in this numerical example are not trained, so don’t take these values in attention weights literally. We use them as an example to illustrate how causal self-attention works.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 11.2</p>
<p class="fm-sidebar-text">We have printed out the attention weights in the first head. How do you print out the attention weights in the last (i.e., the 25<sup class="fm-superscript1">th</sup>) head?</p>
</div>
<p class="body">Finally, we calculate the attention vector in each head as the dot product of attention weights and the value vector. The attention vectors in the 25 heads are then joined together as one single attention vector:</p>
<pre class="programlisting">y=att@v
y = y.transpose(1, 2).contiguous().view(B, T, C)
print(y.shape)</pre>
<p class="body">The output is</p>
<pre class="programlisting">torch.Size([1, 4, 1600])</pre>
<p class="body">The final output after causal self-attention is a <span class="times">4 <span class="cambria">×</span> 1,600 matrix</span>, the same size as the input to the causal self-attention sublayer. The decoder layers are designed in such a way that the input and output have the same dimensions, and this allows us to stack many decoder layers together to increase the representation capacity of the model and to enable hierarchical feature extraction during training.<a id="idIndexMarker019"/><a id="idIndexMarker020"/><a id="idIndexMarker021"/><a id="marker-247"/><a id="idIndexMarker022"/></p>
<h2 class="fm-head" id="heading_id_7">11.2 Building GPT-2XL from scratch</h2>
<p class="body">Now that you understand the architecture of GPT-2 and how its core ingredient, causal self-attention, functions, let’s create the largest version of GPT-2 from scratch.<a id="idIndexMarker023"/><a id="idIndexMarker024"/></p>
<p class="body">In this section, you’ll first learn to use the subword tokenization method in GPT-2, the byte pair encoder (BPE) tokenizer, to break text into individual tokens. You’ll also learn the GELU activation function used in the feed-forward network in GPT-2. After that, you’ll code in the causal self-attention mechanism and combine it with a feed-forward network to form a decoder block. Finally, you’ll stack 48 decoder blocks to create the GPT-2XL model. The code in this chapter is adapted from the excellent GitHub repository by Andrej Kaparthy (<a class="url" href="https://github.com/karpathy/minGPT">https://github.com/karpathy/minGPT</a>). I encourage you to read through the repository if you want to dive deeper into how GPT-2 works. <a id="idIndexMarker025"/></p>
<h3 class="fm-head1" id="heading_id_8">11.2.1 BPE tokenization</h3>
<p class="body">GPT-2 uses a subword tokenization method called byte pair encoder (BPE), which is a data compression technique that has been adapted for use in tokenizing text in NLP tasks. It’s particularly well-known for its application in training LLMs, such as the GPT series and BERT (Bidirectional Encoder Representations from Transformers). The primary goal of BPE is to encode a piece of text into a sequence of tokens in a way that balances the vocabulary size and the length of the tokenized text.<a id="idIndexMarker026"/><a id="idIndexMarker027"/><a id="idIndexMarker028"/><a id="marker-248"/></p>
<p class="body">BPE operates by iteratively merging the most frequent pair of consecutive characters in a dataset into a single new token, subject to certain conditions. This process is repeated until a desired vocabulary size is reached or no more merges are beneficial. BPE allows for an efficient representation of text, balancing between character-level and word-level tokenization. It helps to reduce the vocabulary size without significantly increasing the sequence length, which is crucial for the performance of NLP models.</p>
<p class="body">We discussed the pros and cons of the three types of tokenization methods (character-level, word-level, and subword tokenizations) in chapter 8. Further, you implemented a word-level tokenizer from scratch in chapter 8 (and will do so again in chapter 12). Therefore, in this chapter, we’ll borrow the tokenization method from OpenAI directly. The detailed workings of BPE are beyond the scope of this book. All you need to know is that it first converts text into subword tokens and then the corresponding indexes.</p>
<p class="body">Download the file <code class="fm-code-in-text">bpe.py</code> from Andrej Karp<a id="idTextAnchor002"/>athy’s GitHub repository, <a class="url" href="https://mng.bz/861B">https://mng.bz/861B</a>, and place the file in the folder /utils/ on your computer. We’ll use the file as a local module in this chapter. As Andrej Karpathy explained in his GitHub repository, the module is based on OpenAI’s implementation at <a class="url" href="https://mng.bz/EOlj">https://mng.bz/EOlj</a> but was mildly modified to make it easier to understand.</p>
<p class="body">To see how the module <code class="fm-code-in-text">bpe.py</code> converts text into tokens and then indexes, let’s try an example:</p>
<pre class="programlisting">from utils.bpe import get_encoder
  
example="This is the original text."                       <span class="fm-combinumeral">①</span>
bpe_encoder=get_encoder()                                  <span class="fm-combinumeral">②</span>
response=bpe_encoder.encode_and_show_work(example)
print(response["tokens"])                                  <span class="fm-combinumeral">③</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> The text for an example sentence</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Instantiates the get_encoder() class from the bpe.py module</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Tokenizes the example text and print out the tokens</p>
<p class="body">The output is</p>
<pre class="programlisting">['This', ' is', ' the', ' original', ' text', '.']</pre>
<p class="body">The BPE tokenizer splits the example text “This is the original text.” into six tokens as shown in the preceding output. Note that the BPE tokenizer doesn’t convert uppercase letters to lowercase ones. This leads to more meaningful tokenization but also a much larger number of unique tokens. In fact, all versions of GPT-2 models have a vocabulary size of 50,276, several times larger than the vocabulary size in the previous chapters.</p>
<p class="body">We can also use the module <code class="fm-code-in-text">bpe.py</code> to map tokens to indexes:</p>
<pre class="programlisting">print(response['bpe_idx'])</pre>
<p class="body">The output is</p>
<pre class="programlisting">[1212, 318, 262, 2656, 2420, 13]</pre>
<p class="body">The preceding list contains the six indexes corresponding to the six tokens in the example text “This is the original text.”</p>
<p class="body">We can also restore the text based on the indexes:</p>
<pre class="programlisting">from utils.bpe import BPETokenizer 
  
tokenizer = BPETokenizer()                                  <span class="fm-combinumeral">①</span>
out=tokenizer.decode(torch.LongTensor(response['bpe_idx'])) <span class="fm-combinumeral">②</span>
print(out) </pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Instantiates the BPETokenizer() class from the bpe.py module</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Uses the tokenizer to restore text based on indexes</p>
<p class="body">The output from the preceding code block is</p>
<pre class="programlisting">This is the original text.</pre>
<p class="body">As you can see, the BPE tokenizer has restored the example text to its original form.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 11.3</p>
<p class="fm-sidebar-text">Use the BPE tokenizer to split the phrase “this is a prompt” into tokens. After that, map the tokens to indexes. Finally, restore the phrase based on the indexes. <a id="idIndexMarker029"/><a id="idIndexMarker030"/><a id="idIndexMarker031"/></p>
</div>
<h3 class="fm-head1" id="heading_id_9">11.2.2 The Gaussian error linear unit activation function</h3>
<p class="body"><a id="marker-249"/>The Gaussian error linear unit (GELU) activation function is used in the feed-forward sublayers of each decoder block in GPT-2. GELU provides a blend of linear and nonlinear activation properties that have been found to enhance model performance in deep learning tasks, particularly NLP. <a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="idIndexMarker034"/></p>
<p class="body">GELU offers a nonlinear, smooth curve that allows for more nuanced adjustments during training compared to other functions like the rectified linear unit (ReLU). This smoothness helps in optimizing the neural network more effectively, as it provides a more continuous gradient for backpropagation. To compare GELU with ReLU, our go-to activation function, let’s first define a GELU() class:<a id="idIndexMarker035"/></p>
<pre class="programlisting">class GELU(nn.Module):
    def forward(self, x):
        return 0.5*x*(1.0+torch.tanh(math.sqrt(2.0/math.pi)*\
                       (x + 0.044715 * torch.pow(x, 3.0))))</pre>
<p class="body">The ReLU function is not differentiable everywhere since it has a kink in it. The GELU activation function, in contrast, is differentiable everywhere and provides a better learning process. Next we draw a picture of the GELU activation function and compare it to ReLU.</p>
<p class="fm-code-listing-caption">Listing 11.2 Comparing two activation functions: GELU and ReLU</p>
<pre class="programlisting">import matplotlib.pyplot as plt
import numpy as np
  
genu=GELU()
def relu(x):                                           <span class="fm-combinumeral">①</span>
    y=torch.zeros(len(x))
    for i in range(len(x)):
        if x[i]&gt;0:
            y[i]=x[i]
    return y                 
xs = torch.linspace(-6,6,300)
ys=relu(xs)
gs=genu(xs)
fig, ax = plt.subplots(figsize=(6,4),dpi=300)
plt.xlim(-3,3)
plt.ylim(-0.5,3.5)
plt.plot(xs, ys, color='blue', label="ReLU")           <span class="fm-combinumeral">②</span>
plt.plot(xs, gs, "--", color='red', label="GELU")      <span class="fm-combinumeral">③</span>
plt.legend(fontsize=15)
plt.xlabel("values of x")
plt.ylabel("values of $ReLU(x)$ and $GELU(x)$")
plt.title("The ReLU and GELU Activation Functions")
plt.show()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines a function to represent ReLU</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Plots the ReLU activation function in solid lines</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Plots the GELU activation function in dashed lines</p>
<p class="body"><a id="marker-250"/>If you run the preceding code block, you’ll see a graph as shown in figure 11.4.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="404" src="../../OEBPS/Images/CH11_F04_Liu.png" width="570"/></p>
<p class="figurecaption">Figure 11.4 Comparing the GELU activation function with ReLU. The solid line is the ReLU activation function, while the dashed line is the GELU activation function. ReLU is not differentiable everywhere since there is a kink in it. GELU, in contrast, is differentiable everywhere. This smoothness in GELU helps to optimize the neural network more effectively, as it provides a more continuous gradient for backpropagation during the training process.</p>
</div>
<p class="body">Furthermore, the formulation of GELU allows it to model input data distributions more effectively. It combines the properties of linear and Gaussian distribution modeling, which can be particularly beneficial for the complex, varied data encountered in NLP tasks. This capability helps in capturing subtle patterns in language data, improving the model’s understanding and generation of text.</p>
<h3 class="fm-head1" id="heading_id_10">11.2.3 Causal self-attention</h3>
<p class="body">As we explained earlier, causal self-attention is the core element in GPT-2 models. Next, we’ll implement this mechanism from scratch in PyTorch.<a id="idIndexMarker036"/><a id="idIndexMarker037"/><a id="idIndexMarker038"/></p>
<p class="body"><a id="marker-251"/>We first specify the hyperparameters in the GPT-2XL model that we’ll build in this chapter. To that end, we define a <code class="fm-code-in-text">Config()</code> class with the values shown in the following listing.<a id="idIndexMarker039"/></p>
<p class="fm-code-listing-caption">Listing 11.3 Specifying hyperparameters in GPT-2XL</p>
<pre class="programlisting">class Config():                                       <span class="fm-combinumeral">①</span>
    def __init__(self):
        self.n_layer = 48
        self.n_head = 25
        self.n_embd = 1600
        self.vocab_size = 50257
        self.block_size = 1024 
        self.embd_pdrop = 0.1 
        self.resid_pdrop = 0.1 
        self.attn_pdrop = 0.1                         <span class="fm-combinumeral">②</span>
        
config=Config()                                       <span class="fm-combinumeral">③</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines a Config() class</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Places model hyperparameters as attributes in the class</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Instantiates the Config() class</p>
<p class="body">We define a <code class="fm-code-in-text">Config()</code> class and create several attributes in it to be used as the hyperparameters in the GPT-2XL model. The <code class="fm-code-in-text">n_layer</code> attribute means the GPT-2XL model we construct will have 48 decoder layers (we use the terms “decoder block” and “decoder layer” interchangeably). The <code class="fm-code-in-text">n_head</code> attribute means we’ll split <span class="times">Q</span>, <span class="times">K</span>, and <span class="times">V</span> into 25 parallel heads when calculating causal self-attention. The <code class="fm-code-in-text">n_embd</code> attribute means the embedding dimension is 1,600: each token will be represented by a 1,600-value vector. The <code class="fm-code-in-text">vocab_size</code> attribute means there are 50,257 unique tokens in the vocabulary. The <code class="fm-code-in-text">block_size</code> attribute means the input sequence to the GPT-2XL model contains at most 1,024 tokens. The dropout rates are all set to 0.1. <a id="idIndexMarker040"/><a id="idIndexMarker041"/><a id="idIndexMarker042"/><a id="idIndexMarker043"/><a id="idIndexMarker044"/><a id="idIndexMarker045"/></p>
<p class="body">In the last section, I explained in detail how causal self-attention works. Next, we define a <code class="fm-code-in-text">CausalSelfAttention()</code> class to implement it.<a id="idIndexMarker046"/></p>
<p class="fm-code-listing-caption">Listing 11.4 Implementing causal self-attention</p>
<pre class="programlisting">class CausalSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)
        self.c_proj = nn.Linear(config.n_embd, config.n_embd)
        self.attn_dropout = nn.Dropout(config.attn_pdrop)
        self.resid_dropout = nn.Dropout(config.resid_pdrop)
        self.register_buffer("bias", torch.tril(torch.ones(\
                   config.block_size, config.block_size))
             .view(1, 1, config.block_size, config.block_size)) <span class="fm-combinumeral">①</span>
        self.n_head = config.n_head
        self.n_embd = config.n_embd
    def forward(self, x):
        B, T, C = x.size() 
        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)     <span class="fm-combinumeral">②</span>
        hs = C // self.n_head
        k = k.view(B, T, self.n_head, hs).transpose(1, 2) 
        q = q.view(B, T, self.n_head, hs).transpose(1, 2) 
        v = v.view(B, T, self.n_head, hs).transpose(1, 2)       <span class="fm-combinumeral">③</span>
        att = (q @ k.transpose(-2, -1)) *\
            (1.0 / math.sqrt(k.size(-1)))
        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, \
                              float(‚-inf'))
        att = F.softmax(att, dim=-1)                            <span class="fm-combinumeral">④</span>
        att = self.attn_dropout(att)
        y = att @ v 
        y = y.transpose(1, 2).contiguous().view(B, T, C)        <span class="fm-combinumeral">⑤</span>
        y = self.resid_dropout(self.c_proj(y))
        return y</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a mask and registers it as a buffer since it doesn’t need to be updated</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Passes input embedding through three neural networks to obtain <span class="times">Q</span>, <span class="times">K</span>, and <span class="times">V</span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Splits <span class="times">Q</span>, <span class="times">K</span>, and <span class="times">V</span> into multiple heads</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Calculates masked attention weights in each head</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Concatenates attention vectors in all heads into one single attention vector</p>
<p class="body"><a id="marker-252"/>In PyTorch, <code class="fm-code-in-text">register_buffer</code> is a method used to register a tensor as a buffer. Variables in a buffer are not considered learnable parameters of the model; hence they are not updated during backpropagation. In the preceding code block, we have created a mask and registered it as a buffer. This has implications for how we extract and load model weights later: we’ll omit the masks when retrieving weights from GPT-2XL.<a id="idIndexMarker047"/></p>
<p class="body">As we explained in the first section, the input embedding is passed through three neural networks to obtain query <span class="times">Q</span>, key <span class="times">K</span>, and value <span class="times">V</span>. We then split them into 25 heads and calculate masked self-attention in each head. After that, we join the 25 attention vectors back into one single attention vector, which is the output of the previous <code class="fm-code-in-text">CausalSelfAttention()</code> class. <a id="idIndexMarker048"/><a id="idIndexMarker049"/><a id="idIndexMarker050"/><a id="idIndexMarker051"/></p>
<h3 class="fm-head1" id="heading_id_11">11.2.4 Constructing the GPT-2XL model</h3>
<p class="body">Next, we add a feed-forward network to the causal self-attention sublayer to form a decoder block, as follows.<a id="idIndexMarker052"/><a id="idIndexMarker053"/></p>
<p class="fm-code-listing-caption">Listing 11.5 Constructing a decoder block</p>
<pre class="programlisting">class Block(nn.Module):
    def __init__(self, config):                                 <span class="fm-combinumeral">①</span>
        super().__init__()
        self.ln_1 = nn.LayerNorm(config.n_embd)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = nn.LayerNorm(config.n_embd)
        self.mlp = nn.ModuleDict(dict(
            c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd),
            c_proj = nn.Linear(4 * config.n_embd, config.n_embd),
            act    = GELU(),
            dropout = nn.Dropout(config.resid_pdrop),
        ))
        m = self.mlp
        self.mlpf=lambda x:m.dropout(m.c_proj(m.act(m.c_fc(x)))) 
    def forward(self, x):
        x = x + self.attn(self.ln_1(x))                         <span class="fm-combinumeral">②</span>
        x = x + self.mlpf(self.ln_2(x))                         <span class="fm-combinumeral">③</span>
        return x</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Initiates the Block() class</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The first sublayer in the block is the causal self-attention sublayer, with layer normalization and residual connection.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> The second sublayer in the block is a feed-forward network, with GELU activation, layer normalization, and residual connection.</p>
<p class="body">Every decoder block is composed of two sublayers. The first sublayer is the causal self-attention mechanism, with the integration of layer normalization and residual connection. The second sublayer within the decoder block is the feed-forward network, which incorporates the GELU activation function, alongside layer normalization and residual connection.</p>
<p class="body">We stack 48 decoder layers to form the main body of the GPT-2XL model, as shown in the following listing.<a id="marker-253"/></p>
<p class="fm-code-listing-caption">Listing 11.6 Building the GPT-2XL model</p>
<pre class="programlisting">class GPT2XL(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.block_size = config.block_size
        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            wpe = nn.Embedding(config.block_size, config.n_embd),
            drop = nn.Dropout(config.embd_pdrop),
            h = nn.ModuleList([Block(config) 
                               for _ in range(config.n_layer)]),
            ln_f = nn.LayerNorm(config.n_embd),))
        self.lm_head = nn.Linear(config.n_embd,
                                 config.vocab_size, bias=False)
    def forward(self, idx, targets=None):
        b, t = idx.size()
        pos = torch.arange(0,t,dtype=torch.long).unsqueeze(0)
        tok_emb = self.transformer.wte(idx)    
        pos_emb = self.transformer.wpe(pos)    
        x = self.transformer.drop(tok_emb + pos_emb)            <span class="fm-combinumeral">①</span>
        for block in self.transformer.h:
            x = block(x)                                        <span class="fm-combinumeral">②</span>
        x = self.transformer.ln_f(x)                            <span class="fm-combinumeral">③</span>
        logits = self.lm_head(x)                                <span class="fm-combinumeral">④</span>
        loss = None
        if targets is not None:
            loss=F.cross_entropy(logits.view(-1,logits.size(-1)),
                           targets.view(-1), ignore_index=-1)
        return logits, loss</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Calculates input embedding as the sum of word embedding and positional encoding</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Passes the input embedding through 48 decoder blocks</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Applies layer normalization one more time</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Attaches a linear head to the output so the number of outputs equals the number of unique tokens</p>
<p class="body">We construct the model in the <code class="fm-code-in-text">GPT2XL()</code> class as we explained in the first section of this chapter. The input to the model consists of sequences of indexes corresponding to tokens in the vocabulary. We first pass the input through word embedding and positional encoding; we then add the two to form the input embedding. The input embedding goes through 48 decoder blocks. After that, we apply layer normalization to the output and then attach a linear head to it so that the number of outputs is 50,257, the size of the vocabulary. The outputs are the logits corresponding to the 50,257 tokens in the vocabulary. Later, we’ll apply the softmax activation on the logits to obtain the probability distribution over the unique tokens in the vocabulary when generating text. <a id="idIndexMarker054"/><a id="marker-254"/></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Since the model size is too large, we didn’t move the model to a GPU. This leads to a lower speed in text generation later in the chapter. However, if you have access to a CUDA-enabled GPU with large memory (say, above 32GB), you can move the model to a GPU for faster text generation.</p>
<p class="body">Next, we’ll create the GPT-2XL model by instantiating the <code class="fm-code-in-text">GPT2XL()</code> class we defined earlier:<a id="idIndexMarker055"/></p>
<pre class="programlisting">model=GPT2XL(config)
num=sum(p.numel() for p in model.transformer.parameters())
print("number of parameters: %.2fM" % (num/1e6,))</pre>
<p class="body">We also count the number of parameters in the main body of the model. The output is</p>
<pre class="programlisting">number of parameters: 1557.61M</pre>
<p class="body">The preceding output shows that GPT-2XL has more than 1.5 billion parameters. Note that the number doesn’t include the parameters in the linear head at the end of the model. Depending on what the downstream task is, we can attach different heads to the model. Since our focus is on text generation, we have attached a linear head to ensure the number of outputs is equal to the number of unique tokens in the vocabulary.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> In LLMs like GPT-2, ChatGPT, or BERT, an output head refers to the final layer of the model that is responsible for producing the actual output based on the processed input. This output can vary depending on the downstream task the model is performing. In text generation, the output head is often a linear layer that transforms the final hidden states into logits for each token in the vocabulary. These logits are then passed through a softmax function to generate a probability distribution over the vocabulary, which is used to predict the next token in a sequence. For classification tasks, the output head typically consists of a linear layer followed by a softmax function. The linear layer transforms the final hidden states of the model into logits for each class, and the softmax function converts these logits into probabilities for each class. The specific architecture of the output head can vary depending on the model and the task, but its primary function is to map the processed input to the desired output format (e.g., class probabilities, token probabilities, etc.).</p>
<p class="body"><a id="marker-255"/>Finally, you can print out the GPT-2XL model structure:</p>
<pre class="programlisting">print(model)</pre>
<p class="body">The output is</p>
<pre class="programlisting">GPT2XL(
  (transformer): ModuleDict(
    (wte): Embedding(50257, 1600)
    (wpe): Embedding(1024, 1600)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-47): 48 x Block(
        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
        (attn): CausalSelfAttention(
          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)
          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
        (mlp): ModuleDict(
          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)
          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)
          (act): GELU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)
)</pre>
<p class="body">It shows the detailed blocks and layers in the GPT-2XL model.</p>
<p class="body">And just like that, you have created the GPT-2XL model from scratch!<a id="idIndexMarker056"/><a id="idIndexMarker057"/><a id="idIndexMarker058"/><a id="idIndexMarker059"/></p>
<h2 class="fm-head" id="heading_id_12">11.3 Loading up pretrained weights and generating text</h2>
<p class="body">Even though you have just created the GPT-2XL model, it is not trained. Therefore, you cannot use it to generate any meaningful text.<a id="idIndexMarker060"/><a id="idIndexMarker061"/><a id="idIndexMarker062"/><a id="marker-256"/></p>
<p class="body">Given the sheer number of the model’s parameters, it’s impossible to train the model without supercomputing facilities, let alone the amount of data needed to train the model. Luckily, the pretrained weights of GPT-2 models, including the largest one, GPT-2XL, were released by OpenAI to the public on November 5, 2019 (see the statement on the OpenAI website, <a class="url" href="https://openai.com/research/gpt-2-1-5b-release">https://openai.com/research/gpt-2-1-5b-release</a>, as well as a report by an American technology news website, The Verge, <a class="url" href="https://mng.bz/NBm7">https://mng.bz/NBm7</a>). We, therefore, will load up the pretrained weights to generate text in this section.</p>
<h3 class="fm-head1" id="heading_id_13">11.3.1 Loading up pretrained parameters in GPT-2XL</h3>
<p class="body">We’ll use the <code class="fm-code-in-text">transformers</code> library developed by the Hugging Face team to extract pretrained weights in GPT-2XL. <a id="idIndexMarker063"/><a id="idIndexMarker064"/><a id="idIndexMarker065"/><a id="idIndexMarker066"/><a id="idIndexMarker067"/><a id="idIndexMarker068"/><a id="idIndexMarker069"/><a id="idIndexMarker070"/></p>
<p class="body">First, run the following line of code in a new cell in this Jupyter Notebook to install the <code class="fm-code-in-text">transformers</code> library on your computer:<a id="idIndexMarker071"/></p>
<pre class="programlisting">!pip install transformers</pre>
<p class="body">Next, we import the GPT2 model from the <code class="fm-code-in-text">transformers</code> library and extract the pretrained weights in GPT-2XL:</p>
<pre class="programlisting">from transformers import GPT2LMHeadModel
  
model_hf = GPT2LMHeadModel.from_pretrained('gpt2-xl')         <span class="fm-combinumeral">①</span>
sd_hf = model_hf.state_dict()                                 <span class="fm-combinumeral">②</span>
print(model_hf)                                               <span class="fm-combinumeral">③</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Loads the pretrained GPT-2XL model</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Extracts model weights</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Prints out the model structure of the original OpenAI GTP-2XL model</p>
<p class="body">The output from the preceding code block is</p>
<pre class="programlisting">GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 1600)
    (wpe): Embedding(1024, 1600)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-47): 48 x GPT2Block(
        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()                                   <span class="fm-combinumeral">①</span>
          (c_proj): Conv1D()                                   <span class="fm-combinumeral">①</span>
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()                                     <span class="fm-combinumeral">①</span>
          (c_proj): Conv1D()                                   <span class="fm-combinumeral">①</span>
          (act): NewGELUActivation()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)
)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> OpenAI used a Conv1d layer instead of a linear layer as we did</p>
<p class="body">If you compare this model structure with the one from the previous section, you’ll notice that they are the same except that the linear layers are replaced with Conv1d layers. As we explained in chapters 9 and 10, in feed-forward networks, we treat values in an input as independent elements rather than a sequence. Therefore, we often call it a 1D convolutional network. OpenAI checkpoints use a Conv1d module in places of the model where we use a linear layer. As a result, we need to transpose certain weight matrices when we extract model weights from Hugging Face and place them in our model.</p>
<p class="body"><a id="marker-257"/>To understand how this works, let’s look at the weights in the first layer of the feed-forward network in the first decoder block of the OpenAI GPT-2XL model. We can print out its shape as follows:</p>
<pre class="programlisting">print(model_hf.transformer.h[0].mlp.c_fc.weight.shape)</pre>
<p class="body">The output is</p>
<pre class="programlisting">torch.Size([1600, 6400])</pre>
<p class="body">The weight matrix in the Conv1d layer is a tensor with size (1,600, 6,400).</p>
<p class="body">Now, if we look at the same weight matrix in the model we just constructed, its shape is</p>
<pre class="programlisting">print(model.transformer.h[0].mlp.c_fc.weight.shape)</pre>
<p class="body">The output this time is</p>
<pre class="programlisting">torch.Size([6400, 1600])</pre>
<p class="body">The weight matrix in the linear layer in our model is a tensor with size (6,400, 1,600), which is a transposed matrix of the weight matrix in OpenAI GPT-2XL. Therefore, we need to transpose the weight matrix in all Conv1d layers in the OpenAI GPT-2XL model before we place them in our model.</p>
<p class="body">Next, we name the parameters in the original OpenAI GPT-2XL model as <code class="fm-code-in-text">keys</code>:<a id="idIndexMarker072"/></p>
<pre class="programlisting">keys = [k for k in sd_hf if not k.endswith('attn.masked_bias')] </pre>
<p class="body">Note that we have excluded parameters ending with <code class="fm-code-in-text">attn.masked_bias</code> in the preceding line of code. OpenAI GPT-2 uses them to implement future token masking. Since we have created our own masking in the <code class="fm-code-in-text">CausalSelfAttention()</code> class and registered it as a buffer in PyTorch, we don’t need to load parameters ending with <code class="fm-code-in-text">attn.masked_bias</code> from OpenAI. <a id="idIndexMarker073"/></p>
<p class="body">We name the parameters in the GPT-2XL model we created from scratch as <code class="fm-code-in-text">sd</code>:</p>
<pre class="programlisting">sd=model.state_dict()</pre>
<p class="body">Next, we’ll extract the pretrained weights in OpenAI GPT-2XL and place them in our own model:</p>
<pre class="programlisting">transposed = ['attn.c_attn.weight', 'attn.c_proj.weight',
              'mlp.c_fc.weight', 'mlp.c_proj.weight']          <span class="fm-combinumeral">①</span>
for k in keys:
    if any(k.endswith(w) for w in transposed):
        with torch.no_grad():
            sd[k].copy_(sd_hf[k].t())                          <span class="fm-combinumeral">②</span>
    else:
        with torch.no_grad():
            sd[k].copy_(sd_hf[k])                              <span class="fm-combinumeral">③</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Finds out layers that OpenAI uses a Conv1d module instead of a linear module</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> For those layers, we transpose the weight matrix before placing weights in our model.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Otherwise, simply copies the weights from OpenAI and places them in our model</p>
<p class="body">We extract the OpenAI pretrained weights from Hugging Face and place them in our own model. In the process, we make sure that we transpose the weight matrix whenever OpenAI checkpoints use a Conv1d module instead of a plain linear module.</p>
<p class="body">Now our model is equipped with pre-trained weights from OpenAI. We can use the model to generate coherent text. <a id="idIndexMarker074"/><a id="idIndexMarker075"/><a id="idIndexMarker076"/><a id="idIndexMarker077"/><a id="idIndexMarker078"/><a id="idIndexMarker079"/><a id="idIndexMarker080"/><a id="idIndexMarker081"/><a id="marker-258"/></p>
<h3 class="fm-head1" id="heading_id_14">11.3.2 Defining a generate() function to produce text</h3>
<p class="body">Armed with pretrained weights from the OpenAI GPT-2XL model, we’ll use the GPT2 model we created from scratch to generate text.<a id="idIndexMarker082"/><a id="idIndexMarker083"/><a id="idIndexMarker084"/><a id="idIndexMarker085"/><a id="idIndexMarker086"/></p>
<p class="body">When generating text, we’ll feed a sequence of indexes that correspond to tokens in a prompt to the model. The model predicts the index corresponding to the next token and attaches the prediction to the end of the sequence to form a new sequence. It then uses the new sequence to make predictions again. It keeps doing this until the model has generated a fixed number of new tokens or the conversation is over (signified by the special token <code class="fm-code-in-text">&lt;|endoftext|&gt;</code>).</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">The special token &lt;|endoftext|&gt; in GPTs</p>
<p class="fm-sidebar-text">GPT models undergo training using text from a diverse range of sources. A unique token, <code class="fm-code-in-text1">&lt;|endoftext|&gt;</code>, is employed during this phase to delineate text from different origins. In the text generation phase, it’s crucial to halt the conversation upon encountering this special token. Failing to do so may trigger the initiation of an unrelated new topic, resulting in subsequent generated text that bears no relevance to the ongoing discussion.</p>
</div>
<p class="body">To that end, we define a <code class="fm-code-in-text">sample()</code> function to add a certain number of new indexes to the current sequence. It takes a sequence of indexes as input to feed to the GPT-2XL model. It predicts one index at a time and adds the new index to the end of the running sequence. It stops until the specified number of time steps, <code class="fm-code-in-text">max_new_tokens</code>, is reached or when the predicted next token is <code class="fm-code-in-text">&lt;|endoftext|&gt;</code>, which signals the end of the conversation. If we don’t stop, the model will randomly start an unrelated topic. The <code class="fm-code-in-text">sample()</code> function is defined as shown in the following listing.<a id="idIndexMarker087"/></p>
<p class="fm-code-listing-caption">Listing 11.7 Iteratively predicting the next index</p>
<pre class="programlisting">model.eval()
def sample(idx, max_new_tokens, temperature=1.0, top_k=None):
    for _ in range(max_new_tokens):                            <span class="fm-combinumeral">①</span>
        if idx.size(1) &lt;= config.block_size:
            idx_cond = idx  
        else:
            idx_cond = idx[:, -config.block_size:]
        logits, _ = model(idx_cond)                            <span class="fm-combinumeral">②</span>
        logits = logits[:, -1, :] / temperature
        if top_k is not None:
            v, _ = torch.topk(logits, top_k)
            logits[logits &lt; v[:, [-1]]] = -float('Inf')        <span class="fm-combinumeral">③</span>
        probs = F.softmax(logits, dim=-1)
        idx_next = torch.multinomial(probs, num_samples=1)
        if idx_next.item()==tokenizer.encoder.encoder['&lt;|endoftext|&gt;']:
            break                                              <span class="fm-combinumeral">④</span>
        idx = torch.cat((idx, idx_next), dim=1)                <span class="fm-combinumeral">⑤</span>
    return idx</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Generates a fixed number of new indexes</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Predicts the next index using GPT-2XL</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> If using top-K sampling, sets the logits below the top K choices to –∞</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Stops predicting if the next token is &lt;|endoftext|&gt;</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Attaches the new prediction to the end of the sequence</p>
<p class="body">The <code class="fm-code-in-text">sample()</code> function uses GPT-2XL to add new indexes to a running sequence. It incorporates two arguments, <code class="fm-code-in-text">temperature</code> and <code class="fm-code-in-text">top_k</code>, to modulate the generated output’s novelty, operating in the same manner as described in chapter 8. The function returns a new sequence of indexes. <a id="idIndexMarker088"/><a id="marker-259"/></p>
<p class="body">Next, we define a <code class="fm-code-in-text">generate()</code> function to generate text based on a prompt. It first converts the text in the prompt to a sequence of indexes. It then feeds the sequence to the <code class="fm-code-in-text">sample()</code> function we just defined to generate a new sequence of indexes. Finally, the function <code class="fm-code-in-text">generate()</code> converts the new sequence of indexes back to text.<a id="idIndexMarker089"/></p>
<p class="fm-code-listing-caption">Listing 11.8 A function to generate text with GPT-2XL</p>
<pre class="programlisting">def generate(prompt, max_new_tokens, temperature=1.0,
             top_k=None):
    if prompt == '':
        x=torch.tensor([[tokenizer.encoder.encoder['&lt;|endoftext|&gt;']]],
                         dtype=torch.long)                     <span class="fm-combinumeral">①</span>
    else:
        x = tokenizer(prompt)                                  <span class="fm-combinumeral">②</span>
    y = sample(x, max_new_tokens, temperature, top_k)          <span class="fm-combinumeral">③</span>
    out = tokenizer.decode(y.squeeze())                        <span class="fm-combinumeral">④</span>
    print(out)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> If the prompt is empty, uses &lt;|endoftext|&gt; as the prompt</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Converts prompt into a sequence of indexes</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Uses the sample() function to generate new indexes</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Converts the new sequence of indexes back to text</p>
<p class="body">The <code class="fm-code-in-text">generate()</code> function bears resemblance to the version we introduced in chapter 8 but with a notable distinction: it employs GPT-2XL for prediction purposes, moving away from the LSTM model previously utilized. The function accepts a prompt as its initial input, transforming this prompt into a series of indexes that are then fed into the model to forecast the subsequent index. Upon producing a predetermined number of new indexes, the function reverts the entire index sequence back into textual form.<a id="idIndexMarker090"/><a id="idIndexMarker091"/><a id="idIndexMarker092"/><a id="idIndexMarker093"/><a id="idIndexMarker094"/><a id="idIndexMarker095"/></p>
<h3 class="fm-head1" id="heading_id_15">11.3.3 Text generation with GPT-2XL</h3>
<p class="body">Now that we have defined the <code class="fm-code-in-text">generate()</code> function, we can use it to generate text.<a id="idIndexMarker096"/><a id="idIndexMarker097"/><a id="idIndexMarker098"/><a id="idIndexMarker099"/><a id="idIndexMarker100"/><a id="marker-260"/></p>
<p class="body">In particular, the <code class="fm-code-in-text">generate()</code> function allows for unconditional text generation, which means the prompt is empty. The model will generate text randomly. This can be beneficial in creative writing: the generated text can be used as inspiration or a starting point for one’s own creative work. Let’s try that:</p>
<pre class="programlisting">prompt=""
torch.manual_seed(42)
generate(prompt, max_new_tokens=100, temperature=1.0,
             top_k=None)</pre>
<p class="body">The output is</p>
<pre class="programlisting">&lt;|endoftext|&gt;Feedback from Ham Radio Recalls
  
I discovered a tune sticking in my head -- I'd heard it mentioned on several occasions, but hadn't investigated further.
  
The tune sounded familiar to a tune I'd previously heard on the 550 micro. 
During that same time period I've heard other people's receipients drone on
the idea of the DSH-94013, notably Kim Weaver's instructions in her 
Interview on Radio Ham; and both Scott Mcystem and Steve Simmons' concepts.</pre>
<p class="body">As you can see, the preceding output is coherent and grammatically correct but may not be factually accurate. I did a quick Google search, and the text doesn’t seem to be copied from any online source.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 11.4</p>
<p class="fm-sidebar-text">Generate text unconditionally by setting the prompt as an empty string, temperature to 0.9, maximum number of new tokens to 100, and <code class="fm-code-in-text1">top_k</code> to 40. Set the random seed number to 42 in PyTorch. See what the output is.</p>
</div>
<p class="body">To evaluate whether GPT-2XL can produce coherent text based on preceding tokens, we will use the prompt “I went to the kitchen and” and generate 10 additional tokens after the prompt. We will repeat this process five times to determine if the generated text aligns with typical kitchen activities:</p>
<pre class="programlisting">prompt="I went to the kitchen and"
for i in range(5):
    torch.manual_seed(i)
    generate(prompt, max_new_tokens=10, temperature=1.0,
                 top_k=None)</pre>
<p class="body">The output is</p>
<pre class="programlisting">I went to the kitchen and said, you're not going to believe this.
I went to the kitchen and noticed a female producer open a drawer in which was
I went to the kitchen and asked who was going to be right there and A
I went to the kitchen and took a small vial of bourbon and a little
I went to the kitchen and found the bottle of wine, and poured it into</pre>
<p class="body"><a id="marker-261"/>These results indicate that the generated text includes activities such as conversing with someone, noticing something, and taking beverages, all of which are typical kitchen activities. This demonstrates that GPT-2XL can generate text relevant to the given context.</p>
<p class="body">Next, we use “Lexington is the second largest city in the state of Kentucky” as the prompt and ask the <code class="fm-code-in-text">generate()</code> function to add up to 100 new tokens:</p>
<pre class="programlisting">prompt="Lexington is the second largest city in the state of Kentucky"
torch.manual_seed(42)
generate(prompt, max_new_tokens=100, temperature=1.0,
             top_k=None)</pre>
<p class="body">The output is</p>
<pre class="programlisting">Lexington is the second largest city in the state of Kentucky. It caters to
those who want to make everything in tune with being with friends and 
enjoying a jaunt through the down to Earth lifestyle. To do so, they are 
blessed with several venues large and small to fill their every need while 
residing micro- cozy with nature within the landmarks of the city.
  
In a moment we look at ten up and coming suchache music acts from the 
Lexington area to draw upon your attention.
  
Lyrikhop
  
  
This Lexington-based group</pre>
<p class="body">Again, this text is coherent. Even though the generated content may not be factually accurate. The GPT-2XL model is, fundamentally, trained to predict the next token based on preceding tokens in the sentence. The preceding output shows that the model has achieved that goal: the generated text is grammatically correct and seemingly logical. It shows the ability to remember the text in the early parts of the sequence and generate subsequent words that are relevant to the context. For example, while the first sentence discusses the city of Lexington, about 90 tokens later, the model mentions the music acts from the Lexington area.</p>
<p class="body">Additionally, as noted in the introduction, GPT-2 has its limitations. It should not be held to the same standard as ChatGPT or GPT-4, given that its size is less than 1% of ChatGPT and less than 0.1% of GPT-4. GPT-3 has 175 billion parameters and produces more coherent text than GPT-2, but the pretrained weights are not released to the public.</p>
<p class="body">Next, we’ll explore how <code class="fm-code-in-text">temperature</code> and <code class="fm-code-in-text">top-K</code> sampling affect the generated text from GPT-2XL. We’ll set the <code class="fm-code-in-text">temperature</code> to 0.9 and <code class="fm-code-in-text">top_k</code> to 50 and keep other arguments the same. Let’s see what the generated text looks like:</p>
<pre class="programlisting">torch.manual_seed(42)
generate(prompt, max_new_tokens=100, temperature=0.9,
             top_k=50)  </pre>
<p class="body">The output is</p>
<pre class="programlisting">Lexington is the second largest city in the state of Kentucky. It is also 
the state capital. The population of Lexington was 1,731,947 in the 2011 
Census. The city is well-known for its many parks, including Arboretum, 
Zoo, Aquarium and the Kentucky Science Center, as well as its restaurants, 
such as the famous Kentucky Derby Festival.
  
In the United States, there are at least 28 counties in this state with a
population of more than 100,000, according to the 2010 census.</pre>
<p class="body">The generated text seems more coherent than before. However, the content is not factually accurate. It made up many facts about the city of Lexington, Kentucky, such as “The population of Lexington was 1,731,947 in the 2011 Census.”</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 11.5</p>
<p class="fm-sidebar-text">Generate text by setting the <code class="fm-code-in-text1">temperature</code> to 1.2 and <code class="fm-code-in-text1">top_k</code> to None and using “Lexington is the second largest city in the state of Kentucky” as the starting prompt. Set the random seed number to 42 in PyTorch and the maximum number of new tokens to 100.</p>
</div>
<p class="body">In this chapter, you have learned how to build GPT-2, the predecessor of ChatGPT and GPT-4, from scratch. After that, you extracted the pretrained weights from the GPT-2XL model released by OpenAI and loaded them into your model. You witnessed the coherent text generated by the model. <a id="idIndexMarker101"/><a id="idIndexMarker102"/><a id="idIndexMarker103"/><a id="idIndexMarker104"/><a id="idIndexMarker105"/><a id="idIndexMarker106"/><a id="idIndexMarker107"/><a id="marker-262"/><a id="idIndexMarker108"/></p>
<p class="body">Due to the large size of the GPT-2XL model (1.5 billion parameters), it’s impossible to train the model without supercomputing facilities. In the next chapter, you’ll create a smaller version of a GPT model, with a similar structure as GPT-2 but only about 5.12 million parameters. You’ll train the model with the text from Ernest Hemingway’s novels. The trained model will generate coherent text with a style matching that of Hemingway!</p>
<h2 class="fm-head" id="heading_id_16">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">GPT-2 is an advanced LLM developed by OpenAI and announced in February 2019. It represents a significant milestone in the field of NLP and has paved the way for the development of even more sophisticated models, including its successors, ChatGPT and GPT-4.</p>
</li>
<li class="fm-list-bullet">
<p class="list">GPT-2 is a decoder-only Transformer, meaning there is no encoder stack in the model. Like other Transformer models, GPT-2 uses self-attention mechanisms to process input data in parallel, significantly improving the efficiency and effectiveness of training LLMs.</p>
</li>
<li class="fm-list-bullet">
<p class="list">GPT-2 adopts a different approach to positional encoding than the one used in the seminal 2017 paper “Attention Is All You Need.” Instead, GPT-2’s technique for positional encoding parallels that of word embeddings.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The GELU activation function is used in the feed-forward sublayers of GPT-2. GELU provides a blend of linear and nonlinear activation properties that have been found to enhance model performance in deep learning tasks, particularly in NLPs and in training LLMs.</p>
</li>
<li class="fm-list-bullet">
<p class="list">We can build a GPT-2 model from scratch and load up the pretrained weights released by OpenAI. The GPT-2 model you created can generate coherent text just as the original OpenAI GPT-2 model does.<a id="marker-263"/></p>
</li>
</ul>
</div></body></html>