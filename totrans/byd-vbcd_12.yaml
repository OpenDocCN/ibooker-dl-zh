- en: Chapter 9\. The Ethical Implications of Vibe Coding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As AI-assisted development becomes increasingly commonplace, it’s critical
    to address the ethical and societal implications of this new paradigm. This chapter
    steps back from the technical details to examine vibe coding through an ethical
    lens: these new development methods can be effective, but they also need to be
    implemented responsibly and to benefit individuals and society at large.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I begin with questions of intellectual property (IP). Who owns the code that
    AI generates, and is it permissible to use AI outputs that may be derived from
    open source code without attribution? From there, I consider bias and fairness.
    Transparency is another focus: should developers disclose which parts of a codebase
    were AI-generated, and how can teams ensure accountability for code quality and
    bugs?'
  prefs: []
  type: TYPE_NORMAL
- en: I outline responsible development practices in AI usage, from establishing transparency
    and accountability to avoiding sensitive data in prompts to ensuring accessibility
    and inclusivity. The chapter finishes with a set of guidelines for using AI tools
    responsibly.
  prefs: []
  type: TYPE_NORMAL
- en: Legal Disclaimer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following section touches on complex legal topics, particularly concerning
    copyright and intellectual property law, from a primarily US perspective. Legal
    systems and interpretations are evolving worldwide, especially concerning artificial
    intelligence. This information is for educational purposes only and does not constitute
    legal advice. You should consult with a qualified intellectual property lawyer
    before making any decisions based on this information, especially if you have
    concerns about the ownership or licensing of code you or an AI tool generates.
  prefs: []
  type: TYPE_NORMAL
- en: Intellectual Property Considerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Who owns AI-generated code? And does using it respect the licenses and copyrights
    of the source material on which the AI was trained? AI models like GPT have been
    trained on huge swaths of code from the internet, including open source repositories
    with various licenses (MIT, GPL, Apache, etc.). If the AI generates a snippet
    that is [very similar](https://oreil.ly/I3HxT) (or identical) to something from
    a GPL-licensed project, using that snippet in a proprietary codebase could inadvertently
    violate the GPL, which generally requires [sharing derivative code](https://oreil.ly/8inJc).
  prefs: []
  type: TYPE_NORMAL
- en: According to open source norms and general copyright principles, small snippets
    of a few lines *might not* be copyrightable if they lack sufficient originality
    to be considered an independent creative work, or their use *could potentially*
    be considered de minimis (too trivial to warrant legal concern). However, anything
    substantial or expressing a unique creative choice is more likely to be protected
    by copyright. It’s crucial to understand that “open source” does not mean “public
    domain.” By default, creative work, including code, is under exclusive copyright
    by its author. Open source licenses explicitly grant permissions that would otherwise
    be restricted by copyright law.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to know more about open source norms, good places to start include
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The Open Source Initiative
  prefs: []
  type: TYPE_NORMAL
- en: The [OSI](https://oreil.ly/hmJVN) defines and promotes open source software,
    maintains the Open Source Definition, and approves licenses that meet its criteria.
  prefs: []
  type: TYPE_NORMAL
- en: The Free Software Foundation (FSF)
  prefs: []
  type: TYPE_NORMAL
- en: The [FSF](https://fsf.org) advocates for “free software” (which has a strong
    overlap with open source principles) and is the steward of licenses like the GNU
    General Public License (GPL).
  prefs: []
  type: TYPE_NORMAL
- en: Project-specific documentation
  prefs: []
  type: TYPE_NORMAL
- en: Individual open source projects typically include *LICENSE* files, *README*
    files, and *CONTRIBUTING* guidelines that detail the terms of use and contribution
    for that specific project.
  prefs: []
  type: TYPE_NORMAL
- en: Community and legal resources
  prefs: []
  type: TYPE_NORMAL
- en: Websites like GitHub offer extensive documentation and discussions on open source
    practices. Organizations like the Linux Foundation and legal information sites
    also provide valuable resources on open source compliance and legal aspects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The question of whether using small code snippets overlaps with the [fair use
    doctrine](https://oreil.ly/d0ZK8) (in the US; “fair dealing” in many other jurisdictions)
    is complex and highly fact-dependent. [Fair use](https://oreil.ly/EwrJ2) permits
    limited use of copyrighted material without permission for purposes such as criticism,
    comment, news reporting, teaching, scholarship, or research. US courts typically
    consider four factors to determine fair use:'
  prefs: []
  type: TYPE_NORMAL
- en: The [purpose](https://oreil.ly/1TE5B) and character of the use (commercial versus
    nonprofit, transformative versus duplicative)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The nature of the copyrighted work (highly creative versus factual)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The amount and substantiality of the portion used in relation to the copyrighted
    work as a whole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The effect of the use upon the potential market for or value of the copyrighted
    work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While some might argue that copying very small, functional code snippets for
    interoperability or to access uncopyrightable ideas could fall under fair use,
    especially if the use is transformative, this is not a clearly settled area of
    law for code, and there’s no universally agreed-upon number of lines that is definitively
    “fair use” or de minimis. The safest course is often to get permission or to understand
    the underlying idea and rewrite the code in your own way. The U.S. Supreme Court
    case *Google LLC v. Oracle America, Inc.* addressed fair use in the context of
    software APIs, finding Google’s reimplementation of Java API declaring code to
    be fair use, but this was a specific and complex ruling focused on API declarations,
    not all code. It’s generally understood that copyright protects the specific expression
    of an idea, not the idea, procedure, or method of operation itself.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the developer *using* the AI is considered the “author” in the sense
    that the AI is a tool, similar to a compiler or a word processor. Thus, if code
    is generated in a work context, the developer’s company would likely own the code
    produced by the developer using the tool, subject to the AI tool’s terms of service
    and underlying IP issues. However, the terms of service (ToS) of AI tools are
    critical. Most ToS grant the user rights to the output they generate. OpenAI’s
    ToS, for instance, states, “You own the outputs you create with GPT-4, including
    code.”
  prefs: []
  type: TYPE_NORMAL
- en: This “ownership,” however, needs careful consideration. It generally means that
    the AI provider isn’t claiming ownership of what *you* create *with their tool*.
    But this assumes you have the rights to the *inputs* you provide, and it doesn’t
    automatically mean the output is itself eligible for copyright protection or that
    it’s free from third-party intellectual property claims. If you input your own
    original code to the tool for modification or extension, the output is most likely
    yours (or your employer’s), again, subject to how the AI processes it and what
    it incorporates from its training data. But if you input someone else’s copyrighted
    code to fix or transform, the output *might* be considered a [derivative work
    of that third-party code](https://oreil.ly/mBPyq).
  prefs: []
  type: TYPE_NORMAL
- en: In the US and many other jurisdictions, whether AI-generated output that is
    substantially similar to training data, or output based on copyrighted input,
    constitutes a derivative work is a subject of ongoing legal debate and lacks full
    clarity. Don’t feed large chunks of copyrighted code that isn’t yours (or licensed
    appropriately) into an AI tool, because the output could be deemed a [derivative
    work](https://oreil.ly/O4ktq) and thus fall under the license of that original
    copyrighted code.
  prefs: []
  type: TYPE_NORMAL
- en: Given these uncertainties, to be safe, treat AI-generated code as if it’s under
    an ambiguous license, and only use it if you are comfortable that it doesn’t infringe
    on existing copyrights and that you can comply with any potential open source
    license obligations. Regarding the copyright status of the AI output itself, the
    [US Copyright Office has stated](https://oreil.ly/Y0PYG) that works generated
    solely by AI without sufficient human authorship are not copyrightable. If a human
    significantly modifies or arranges AI-generated material in a creative way, that
    human contribution might be [copyrightable](https://oreil.ly/NV3Gl) but not the
    AI-generated elements standing alone. Thus, it’s often wise to assume that purely
    AI-generated outputs might not be copyrightable by anyone or that copyright would
    extend only to the human’s creative contributions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is not a hypothetical worry. In fact, there’s ongoing legal debate. A
    [prominent class-action lawsuit, *Doe v. GitHub, Inc.*](https://githubcopilotlitigation.com),
    was filed against GitHub, Microsoft, and OpenAI, claiming that GitHub Copilot
    produces code that is too similar to licensed open source code without proper
    attribution or adherence to license terms.  While some claims in this case have
    been dismissed or are under appeal (as of mid-2025, the case involves ongoing
    proceedings, including an appeal to the Ninth Circuit regarding DMCA claims and
    remaining breach of contract claims), it highlights a genuine concern: AI can
    and sometimes does regurgitate or closely paraphrase copyrighted code from its
    training data.^([1](ch09.html#id1032))'
  prefs: []
  type: TYPE_NORMAL
- en: An older (but still relevant and later substantiated) [study by GitHub itself](https://oreil.ly/fFUUd)
    noted that, in some cases, Copilot’s output included suggestions that matched
    training data, including rare instances of longer verbatim snippets. While most
    AI tools are designed to avoid direct, extensive copying of identifiable code
    unless specifically prompted or dealing with very standard algorithms, the risk
    exists. Furthermore, it’s not just open source code that’s a concern; numerous
    lawsuits have been filed by authors, artists, and media companies alleging that
    their fully copyrighted, privately owned intellectual property was used without
    permission or compensation to train large language models and other generative
    AI systems. The challenge with proprietary code is that, unlike open source, it’s
    often not publicly visible, making it harder for an end user to confirm if an
    AI’s output is inadvertently similar to such private code.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, the ethical and prudent practice is to *act as if any code you
    accept from an AI tool is your responsibility*. Thoroughly review, test, and understand
    any AI-generated code before incorporating it into your projects, and ensure its
    use complies with all applicable licenses and copyright laws.
  prefs: []
  type: TYPE_NORMAL
- en: What to Do If You Get Suspicious Output
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If an AI output seems like a verbatim or near-verbatim copy of known code (especially
    if it includes distinctive comments or author names), treat it carefully. Consider
    running a similarity check using a plagiarism detector tool, or do a web search
    for unique strings to see if you find any matches that could indicate copying.
  prefs: []
  type: TYPE_NORMAL
- en: Another principle to follow is *When in doubt, leave it out*. Either avoid using
    the output or make sure it’s under a compatible license and give attribution if
    required. For example, if Copilot spits out a well-known algorithm implementation
    that you recognize from Stack Overflow or an open source project, cite the source
    or rewrite it in your own way, using the AI’s answer as a guide but not quoting
    it verbatim.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you suspect the output matches an existing library solution, consider including
    the library itself instead (with proper license). You can also prompt the AI:'
  prefs: []
  type: TYPE_NORMAL
- en: Please provide an original implementation rather than one copied from a library.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: It might then synthesize a more unique solution. (There’s no guarantee it won’t
    be influenced by its training code, but at least it will try to not copy outright).
  prefs: []
  type: TYPE_NORMAL
- en: 'The ethics here also touch on not using AI to willfully strip attribution.
    For example, it would be unethical to copy code from Stack Overflow via AI without
    attribution to circumvent a policy that you should credit the answer. That erodes
    trust in the open knowledge ecosystem. It’s better to incorporate the material
    with proper credit. Depending on the circumstances, that might mean the following:'
  prefs: []
  type: TYPE_NORMAL
- en: If an AI writes a code comment from some source that has an author’s name (like
    copying a snippet with “John Doe 2018” in a comment), you should keep that or
    move it to a proper attribution section with a full citation rather than deleting
    it. That respects the original author’s credit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If an AI provided a solution that you know comes from a known algorithm or code
    snippet, cite that source as you normally would if you had looked it up yourself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If an AI tool creates something arguably creative (like a unique approach or
    text for documentation), acknowledge its contribution. Though it doesn’t have
    rights, it’s about transparency (and maybe a nod to the tech).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some open source licenses (like MIT) are permissive enough that including copied
    code with attribution would satisfy the license. Others, like GPL or AGPL, would
    “infect” your whole codebase if you include that code, which is undesirable for
    closed projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short: if you suspect the AI has given you something that might cause IP
    issues, either avoid using it or transform it sufficiently to ensure you’re complying
    with any possible license.'
  prefs: []
  type: TYPE_NORMAL
- en: Gray Areas
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even as I write this, AI tools continue to raise new questions about IP, copyright,
    and ethics. For instance:'
  prefs: []
  type: TYPE_NORMAL
- en: If your vibe coding includes using AI to generate noncode assets like documentation
    text, config files, or images, similar IP questions arise. For instance, if you
    generate an icon image via an AI tool that was trained on copyrighted images,
    who owns that new image?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If an AI writes a significant part of a software product, should the original
    authors of the code on which the AI was trained get credit?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Could someone claim that your AI-generated code infringes on their copyright
    because it looks similar to theirs? If sections of nontrivial lengths are possibly
    identical, this is where similarity checking comes in.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s an emerging notion that AI companies might need to implement license-respecting
    filters or allow teams to opt out of their code being included in AI training
    data. It’s evolving, but developers on the ground should act conservatively to
    not violate rights.
  prefs: []
  type: TYPE_NORMAL
- en: It will take time for courts to settle all of the legal issues, but in the meantime,
    intellectual honesty and respect should guide us. If AI uses a known algorithm
    from a published paper, cite the paper in a comment. If it uses a common open
    source helper code, credit the project. It’s about respect for authorship. If
    you recognize where something came from, err on the side of giving credit. It’s
    a good practice that fosters transparency.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that under the hood, the AI’s knowledge comes from thousands of developers
    who shared their code publicly. Ethically, the software industry owes that community
    the respect of upholding open source licenses and norms. Give credit where it’s
    due and don’t abuse others’ work under the guise of “the AI wrote it, not me.”
  prefs: []
  type: TYPE_NORMAL
- en: Transparency and Attribution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Transparency* refers to being open about the use of AI in your development
    process and outputs, and *attribution* refers to giving proper credit when AI-derived
    code comes from identifiable sources.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transparency is important for the sake of accountability. For example, if AI-generated
    code introduces a bug or security flaw, being transparent that “this code was
    AI-suggested” might help you analyze the root cause—perhaps an ambiguous prompt
    should be rewritten. In code comments or a project’s README or documentation,
    you might mention generally that “this project was built with assistance from
    AI tools like ChatGPT.” Or get more specific: “Added a function to parse CSV (generated
    with ChatGPT’s help, then modified).” It’s a bit like acknowledging your use of
    frameworks or libraries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Transparency is also key to trust: stakeholders (your team, clients, end users,
    or industry regulators) might want to know how your software was developed and
    validated. If an AI was involved in code generation, some stakeholders might wrongly
    trust it too much or too little. Transparency allows a conversation about reliability:
    “Yes, we used AI, but we tested it thoroughly” or “This part was tricky—we had
    AI generate the initial code, but we’ve since verified it.”'
  prefs: []
  type: TYPE_NORMAL
- en: Attributions are also expected or required in many academic venues. Some open
    source projects restrict or even forbid AI contributions due to IP concerns, so
    check the contributor guidelines before using AI. Being transparent with maintainers
    if a patch was AI-generated helps them evaluate it, especially if licensing is
    a worry.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, some highly regulated industries require software vendors to disclose
    any AI use for auditing purposes. [The EU’s AI Act](https://oreil.ly/wDNKs) mandates
    transparency for automated decision making that affects individuals (such as credit-scoring
    algorithms). If vibe coding leads to such systems, it becomes a legal/ethical
    necessity to inform users that “recommendations are generated automatically and
    may reflect patterns in data.”
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if your product feeds user data or proprietary data like user-provided
    code examples into an AI model to fine-tune it and help program its analysis,
    you might need to say in the privacy policy that user data may be used with permission
    to improve AI models (as always, do consult a lawyer for legal matters). Transparency
    intersects with privacy here.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also just generally ethical to acknowledge the tools and sources you use.
    If 30% of your code was generated by Copilot, it’s fair to mention that in your
    documentation or internal communication—not to diminish your own role but to be
    honest about the process.
  prefs: []
  type: TYPE_NORMAL
- en: Some developers might fear admitting that AI helped, worried that it could undermine
    their perceived contribution or skill or be seen as “cheating.” As vibe coding
    becomes more normalized, this stigma should decrease; eventually, you might be
    seen as behind the times if you’re *not* using the AI available to you. We need
    to normalize AI as a tool—it’s no more “cheating” than using Stack Overflow or
    an IDE.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the flip side, providing too many disclaimers could cause undue worry. If
    you tell a client, “We used AI to code this product,” they might question its
    safety (even if that’s due to misconceptions). It’s important how you phrase it.
    Emphasize quality measures in the same breath: “We utilized advanced coding assistants
    to speed up development, and all AI-generated code was rigorously reviewed and
    tested to meet our quality standards.”'
  prefs: []
  type: TYPE_NORMAL
- en: In sum, transparency and attribution foster trust and community values. They
    ensure that credit flows to human creators and that we remain honest about how
    our software is built. It’s akin to an artist listing their tools or inspirations;
    it doesn’t diminish the art; it contextualizes it. If, like me, you want vibe
    coding to be accepted widely, being open about using AI and how you mitigate its
    risks is important.
  prefs: []
  type: TYPE_NORMAL
- en: Bias and Fairness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you know well by this point in the book, AI models’ output reflects the data
    they’re trained on. If that data contains biases or exclusionary patterns, the
    models can produce outputs that are biased or unfair.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might ask: “How can code be biased? It’s not like an LLM is making hiring
    decisions or something.” But bias can creep into your coding in subtle ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Code often reflects assumptions on its creators’ part. User-facing text or content
    the AI generates might reflect cultural biases or insensitive language present
    in its training data. For instance, [Microsoft’s Tay](https://oreil.ly/d8wxO),
    an early chatbot in 2016, infamously learned to parrot racist and misogynistic
    slurs from Twitter interactions within hours of launch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assumptions can also be geared toward specific cultural norms, like a middle-class
    North American lifestyle (such as assuming car ownership or universal access to
    certain technologies). A notable example of unexamined assumptions leading to
    exclusionary products was the initial [2014 release of Apple’s Health app](https://oreil.ly/67sZG),
    which lacked a period tracker—a significant oversight likely stemming from a lack
    of diversity and perspective on the design team. Even in example code, comments,
    or synthetic data, the model might always use *he/him* pronouns, reinforcing gender
    bias.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is well known that code repositories and the broader software development
    landscape predominantly reflect Western perspectives and English speakers. As
    a result, an AI trained on these repositories might overlook crucial internationalization
    aspects, such as proper support for Unicode and multibyte characters (essential
    for languages like Chinese, Japanese, Korean, Arabic, Hindi, and many others using
    non-Latin or syllabary scripts), or it might default to English-centric examples
    for things like type names. Developers must bring awareness and design and code
    for internationalization, even if the AI doesn’t spontaneously do so.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If writing algorithms, be wary of certain variables like race, gender, age,
    etc. The AI might not spontaneously include them unless asked, but if it hallucinates
    some criteria or if you’re using an AI like Code Assistant on a dataset, apply
    fairness constraints; the AI won’t inherently know the moral or legal context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beyond just coding, models can mirror *data bias* in their content domain:
    the historical biases present in their training data. For example, consider an
    AI tasked with writing code for a credit-scoring algorithm for loan approvals.
    In the United States, credit scoring systems have a documented history of reflecting
    and perpetuating racial biases. These biases stem from historical practices like
    redlining and other forms of systemic discrimination that have had lasting financial
    repercussions, particularly for Black communities and other marginalized groups.
    (See Richard Rothstein’s *The Color of Law* [Economic Policy Institute, 2017]
    for a comprehensive history of how government policies segregated America.)'
  prefs: []
  type: TYPE_NORMAL
- en: If the training data reflects these historical biases, the AI might incorporate
    discriminatory variables, such as using zip codes (which can be a proxy for racial
    demographics due to segregated housing patterns) or other seemingly neutral data
    points that correlate with protected characteristics. If not properly guided,
    the AI might produce code that leads banks to make unfair lending decisions, thus
    perpetuating historical inequalities and affecting real people’s lives. Similar
    issues arise in areas like predictive policing algorithms, where historical arrest
    data (itself potentially biased) can lead to AI systems that [disproportionately
    target certain communities](https://oreil.ly/H4rmr).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if you’re using specialized models (like an AI code assistant fine-tuned
    for, say, medical software), ensure the model isn’t locked into biases from that
    domain’s data. For example, historically, some medical guidelines were biased
    by research studies that predominantly used male subjects, leading to misdiagnoses
    or less effective treatments for other genders. If AI is recommending code or
    solutions for medical diagnostics, you need to double-check that it doesn’t inadvertently
    encode those biases.
  prefs: []
  type: TYPE_NORMAL
- en: There are tools emerging to detect bias in AI outputs, though these are more
    common in GPT models used to generate content, and AI providers themselves attempt
    to filter overtly biased or toxic outputs. Code-oriented AIs rarely produce hate
    speech spontaneously, but it’s good that they have content filters for it. Building
    in ethical constraints means, in many AI tools, that if a user tries to get the
    AI to create malware or discriminatory algorithms, it will refuse. Don’t try to
    break those filters to get unethical outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are lots of other ways to recognize and mitigate bias at different stages
    of the development process, though. These include:'
  prefs: []
  type: TYPE_NORMAL
- en: Testing with diverse examples
  prefs: []
  type: TYPE_NORMAL
- en: If your AI generates user-facing components or logic that deals with human-related
    data, test it with diverse inputs. For example, if an AI-generated form validation
    expects “First Name” and “Last Name,” does it allow single names, which are common
    in some cultures? If not, that’s a bias in assumption. If it generates sample
    usernames, are they all like “JohnDoe”? If so, consider incorporating more diversity
    in the examples.
  prefs: []
  type: TYPE_NORMAL
- en: Prompting for inclusivity
  prefs: []
  type: TYPE_NORMAL
- en: 'You can explicitly instruct the AI to be neutral or inclusive: “Generate examples
    using a variety of names from different cultures.” If it always refers to the
    user as “he,” you might prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid gendered language in this code comment; use neutral phrasing or they/them
    pronouns.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Also, be cautious about jokes or examples the AI might produce that could be
    culturally insensitive; you can prompt it to use a professional tone to avoid
    that. The AI will usually comply. It doesn’t have an agenda; it just outputs what
    seems normal to it, unless told otherwise. We shape that “normal.”
  prefs: []
  type: TYPE_NORMAL
- en: Hiring diverse teams
  prefs: []
  type: TYPE_NORMAL
- en: Having a diverse team review outputs can catch issues. For example, someone
    might say, “Hey, our AI always picks variable names like foo/bar, which is fine,
    but in documentation, all of its personas are male-typed.” Then you can correct
    that systematically. If all developers are from similar backgrounds, they might
    not catch a subtle bias. If possible, involve people from underrepresented groups—or
    at least consider their perspectives—when reviewing AI usage guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, bias and fairness are about using vibe-coding tools to produce code
    that is fair to users of all backgrounds and that doesn’t reflect—or, worse, perpetuate—historical
    discrimination. The way we use these tools in teams should also be fair to developers
    and other colleagues of varying levels and backgrounds. See [Chapter 4](ch04.html#ch04_beyond_the_70_maximizing_human_contribution_1752630043401362)
    for a discussion of the ethical implications of how AI tools are changing workplaces,
    especially for junior developers.
  prefs: []
  type: TYPE_NORMAL
- en: Golden Rules for Responsible AI Use
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bringing together a lot of what we’ve covered, it’s worth articulating a set
    of responsible practices for vibe coding:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Always keep a human in the loop.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Again: never let the AI work unsupervised. Responsible AI-assisted dev means
    you, the developer, are reviewing every line and making decisions, not deploying
    raw AI output without human validation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Take responsibility for your code.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If something goes wrong, it’s not the AI’s fault—it’s the development team’s
    responsibility. Keeping that mindset avoids complacency. Be prepared to justify
    your code, whether you wrote it from scratch or accepted AI code. If someone asks
    you, “Why does the code do this?” don’t say, “I don’t know; Copilot did that.”
    That’s why one of [Chapter 3](ch03.html#ch03_the_70_problem_ai_assisted_workflows_that_actual_1752630043200933)’s
    golden rules is “Never commit code you don’t fully understand.” That’s responsible
    engineering.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Protect users’ privacy and ask for their consent.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ethically, you owe it to users and your company to keep their secret data secret.
    When using AI tools, especially cloud-based ones, be careful not to expose sensitive
    data in your prompts or conversations. For instance, if you’re debugging an issue
    with a user database, don’t feed actual user records to ChatGPT. Use sanitized
    or synthetic data instead.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Many tools now allow users (or at least business users) to opt out of having
    their input data used for training. If you’re an enterprise user, use those settings
    or use on-prem solutions for sensitive code. If you do feed any user data to a
    model, or if any AI functionality directly touches users (like a chatbot in your
    app that uses an LLM), get users’ consent and allow them to opt out if appropriate.
    A warning like “This feature uses an AI service; your input will be sent to it
    for processing” is transparent and lets privacy-conscious users decide for themselves.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Comply with laws and regulations.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Keep an eye on legal requirements around AI, which are constantly evolving.
    For instance, data protection laws like the EU’s General Data Protection Regulation
    (GDPR) and AI Act consider some AI outputs as personal data if they include any
    personal data. Training a model on users’ data might require those users’ consent.
    Regulatory bodies may classify code generation as “general AI” and impose transparency
    or risk management obligations. Stay informed and work closely with your legal
    and compliance professionals to avoid breaking any regulations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: While this should go without saying, do not use AI to generate malware, exploit
    code without ethical justification, or automate unethical or illegal practices.^([2](ch09.html#id1062))
    While an AI could probably write a very effective phishing email or code injection
    attack, using it for that purpose violates ethics, the laws of most countries,
    and likely the AI’s terms of service. Focus on constructive use.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Foster a responsible AI culture in your organization.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If your team adopts vibe coding, encourage discussions about ethics and provide
    relevant ethics training. Consider having developers and code reviewers use a
    brief checklist like the one in [Figure 9-1](#ch09_figure_1_1752630044839745).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](assets/bevc_0901.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 9-1\. Responsible AI development checklist: essential validation steps
    including intellectual property review, bias assessment, and security audits before
    integrating AI-generated code into production systems.'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Everyone should feel responsible for ethical AI use; it’s a collective effort,
    not just the burden of the individual using the tool at any given moment. To formalize
    this, consider designating an “ethics champion” or a small ethics committee within
    your team or organization. This individual or group wouldn’t be the sole owner
    of ethics (as that responsibility remains shared), but they would take the lead
    on:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Staying abreast of the latest developments in AI ethics, emerging best practices,
    and new regulatory landscapes
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Facilitating discussions about ethical considerations in specific projects
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Championing the integration of ethical principles into the development lifecycle
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Helping to curate and disseminate relevant resources and training materials
    to the broader team
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Acting as a point of contact for team members who have ethical questions or
    concerns
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Since this field is moving incredibly fast, it’s crucial to work as a team to
    stay updated on new versions of AI tools and their capabilities, limitations,
    and evolving best practices for responsible use.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Since this field is moving fast, work as a team to stay updated on new versions
    of AI tools and best practices. One important concept to integrate into your workflows
    is the use of model cards. *Model cards* are essentially standardized documents
    that provide transparency about a machine learning model. Think of them as nutrition
    labels for AI models. They typically include details about:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What the model is, its version, and when it was developed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The specific use cases the model was designed and tested for
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Scenarios where the model should not be used, due to limitations or potential
    for harm
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How well the model performs on various benchmarks, including evaluations for
    fairness and bias across different demographic groups
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Information about the datasets used to train the model, including any known
    limitations or biases in the data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Potential risks and societal implications and any mitigation strategies employed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Whenever you are using a pretrained model or evaluating a model for use, look
    for its model card. If you are fine-tuning or developing models, creating your
    own model cards is a best practice.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Create guardrails and safety nets.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Practicing responsible design means that your AI-generated systems should have
    safety nets. For example, if AI suggests an out-of-bounds index fix that might
    mask an underlying issue, it’s better for the system to fail safely than to cause
    silent errors. If an AI-generated recommendation system might be wrong, providing
    ways for users to correct or override it shows respect for their human agency.
    Strive to build systems that degrade gracefully if AI components misbehave.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Document AI usage decisions within your team.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Keep an internal log of why you used certain AI suggestions (or didn’t): “We
    tried AI for module X, but it tended to produce too much duplicate code, so we
    wrote that part manually.” This can help you refine your processes, provide context
    to new team members about AI’s role in the codebase’s history, and augment your
    team’s collective memory. It can also be useful during audits.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Proactively work to avoid bias, discrimination, and unfairness.*'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Be vigilant for signs that your AI usage could lead to discrimination, and work
    to avoid such situations before they happen. For example, if your app is global,
    is your AI multilingual or does it favor those who speak English? Do all of your
    team members have equal access to AI tools and training?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As the AI landscape continues changing and growing, the software industry is
    likely to introduce AI standards or certifications. It’s early, but your company
    could even help shape those guidelines by engaging in standardization efforts,
    like IEEE or ISO working groups on AI software engineering. Ethically, it’s better
    for the dev community to help set the rules than to leave it solely to regulators
    or the courts.
  prefs: []
  type: TYPE_NORMAL
- en: Summary and Next Steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Responsible vibe coding means integrating AI into the software development
    lifecycle in a way that respects all stakeholders: original creators (by respecting
    their IP), colleagues (through transparency and fairness), users (through privacy,
    security, and fairness in outcomes), and society (by not letting misuse cause
    harm). It’s about leveraging AI’s strengths while diligently guarding against
    its weaknesses.'
  prefs: []
  type: TYPE_NORMAL
- en: I’ve often said that vibe coding is not an excuse for low-quality work. It’s
    not an excuse for ethical shortcuts either. As the humans in charge, developers
    must ensure that speed doesn’t compromise values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, [Chapter 10](ch10.html#ch10_autonomous_background_coding_agents_1752630045087844)
    looks at a new technology that’s changing the way we work with AI models: autonomous
    coding agents.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch09.html#id1032-marker)) Case information can often be found on [court
    dockets](https://oreil.ly/BdDiV), like those for the US District Court for the
    Northern District of California and the Ninth Circuit Court of Appeals, or through
    [legal news outlets and case trackers](https://oreil.ly/AZrc-).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch09.html#id1062-marker)) There are some ethically justified exceptions.
    Penetration testers and security researchers can ethically use AI to find vulnerabilities
    that should be fixed, as long as they work under responsible disclosure protocols.
  prefs: []
  type: TYPE_NORMAL
