# 人工智能的未来

> [深度学习 Python 教程](https://deeplearningwithpython.io/chapters/chapter19_future_of_ai)

要恰当地使用一个工具，你不仅应该了解它能做什么，还应该知道它不能做什么。我将概述一些深度学习的关键局限性。然后，我将提供一些关于人工智能未来演化和达到人类水平通用智能所需条件的推测性思考。如果你对基础研究感兴趣，这应该特别吸引你。

## 深度学习的局限性

你可以用深度学习做无数的事情。但深度学习并不能做*一切*。要很好地使用一个工具，你应该了解它的局限性，而不仅仅是它的优势。那么，深度学习在哪里不足？

### 深度学习模型难以适应新事物

深度学习模型的特性是大型、参数化的曲线，这些曲线拟合到大量数据集上。这是它们力量的来源——它们容易训练，并且在模型大小和数据集大小方面都能很好地扩展。但这也是一个显著弱点的来源。曲线拟合有固有的局限性。

首先，参数化曲线只能用于信息存储——它是一种*数据库*。回想一下我们在第十五章中关于 Transformer 作为“插值数据库”的讨论？其次，关键的是，这个数据库是*静态的*。模型的参数在特定的“训练时间”阶段确定。之后，这些参数被冻结，这个固定版本在“推理时间”用于对新数据进行预测。

使用静态数据库唯一能做的就是信息检索。这正是深度学习模型擅长的：识别或生成与训练过程中遇到的模式高度相似的图案。另一方面，它们在*适应性*方面天生不足。数据库是向后看的——它适合过去的数据，但无法处理不断变化的未来。在推理时间，你最好希望模型面对的情况是训练数据分布的一部分，否则模型将会崩溃。例如，在 ImageNet 上训练的模型会将豹纹沙发分类为真正的豹子——沙发不是其训练数据的一部分。

这也适用于最大的生成模型。近年来，大型语言模型（LLMs）的兴起及其在编程辅助和类似推理问题中的应用，为这一观点提供了广泛的实证证据。尽管经常有人声称 LLMs 可以通过*上下文学习*从几个例子中获取新技能，但大量证据表明，他们实际上正在执行的是在训练期间记忆的向量函数的检索和重新应用。通过学习在网页大小的文本数据集上进行下一标记预测，LLM 收集了数百万个可能有用的迷你文本处理程序，并且可以很容易地提示它们在新问题上的重复使用。但是，如果展示给它的是其训练数据中没有直接对应的内容，它就无能为力了。

看一下图 19.1 中的谜题。你找到解决方案了吗？很好。这并不难，对吧？但是今天，没有任何最先进的 LLM 或视觉语言模型能够做到这一点，因为这个问题并没有直接映射到他们在训练时间看到的任何东西——即使是在整个互联网上训练过之后。LLM 解决特定问题的能力与问题复杂性无关，而与*熟悉度*有关——他们会在任何足够新颖的问题上咬紧牙关，无论这个问题多么简单。

![](img/b23cd5277a88b93d0a3bf331695f5c36.png)

图 19.1：一个简单而新颖的谜题

这种失败模式甚至适用于 LLM 在训练数据中多次遇到的模式的微小变化。例如，在 ChatGPT 发布后的几个月里，如果你问它，“10 公斤的钢铁和 1 公斤的羽毛哪个更重？”，它会回答说它们重量相同。这是因为“10 公斤的钢铁和 1 公斤的羽毛哪个更重？”这个问题在互联网上多次出现——作为一个陷阱问题。当然，正确的答案是它们重量相同，所以 GPT 模型只是重复它记忆中的答案，而没有注意到查询中的实际数字，或者查询真正*意味着*什么。同样，LLM 在适应蒙提霍尔问题的变体（见图 19.2）方面也遇到困难，并且倾向于总是输出他们在训练期间多次看到的经典答案，而不管这在上下文中是否有意义。

![](img/bbb2a6ce26449fc1e118892bfb73a357.png)

图 19.2：蒙提霍尔问题的变体

值得注意的是，这些特定的提示后来通过特殊处理得到了修复。如今，有超过 25,000 人全职工作，通过审查失败案例和提出更好的答案来为 LLMs 提供训练数据。LLMs 的维护是一场持续的打地鼠游戏，一次修复一个失败的提示，而没有解决更普遍的根本问题。即使已经修复的提示，如果你对其做出小的修改，它们仍然会失败！

### 深度学习模型对措辞和其他干扰因素非常敏感

与之密切相关的问题是深度学习模型对输入呈现方式的极端敏感性。例如，图像模型会受到*对抗样本*的影响，这些样本被输入到深度学习网络中，旨在欺骗模型将其错误分类。你已经知道，在输入空间中可以进行梯度上升来生成最大化某些卷积神经网络（ConvNet）滤波器激活的输入——这是第十章中引入的滤波器可视化技术的基础。

同样，通过梯度上升，你可以稍微修改一个图像，以最大化给定类别的预测。通过拍摄一只熊猫的照片并添加长臂猿的梯度，我们可以让神经网络将熊猫分类为长臂猿（见图 19.3）。这既证明了这些模型的脆弱性，也证明了它们的输入到输出映射与我们的人类感知之间的深刻差异。

![](img/31f0c0132cc718d30238b6ec0e944f54.png)

图 19.3：一个对抗样本：图像中的细微变化可能会颠覆模型对图像的分类。

同样，大型语言模型（LLMs）对提示中的细微细节具有极高的敏感性。无害的提示修改，如更改文本段落中的地点和人物名称或代码块中的变量名称，可能会显著降低 LLMs 的性能。考虑一下著名的*爱丽丝梦游仙境*谜题^([[1]](#footnote-1))：

“爱丽丝有 N 个兄弟，她还有 M 个姐妹。爱丽丝的兄弟有多少个姐妹？”

答案当然是*M* + 1（爱丽丝的姐妹加上爱丽丝自己）。对于一个 LLM 来说，用在线实例中常见的值（如*N* = 3 和*M* = 2）提问通常会得到正确答案，但如果你调整*M*和*N*的值，你很快就会得到错误的答案。

这种对措辞的过度敏感催生了**提示工程**的概念。提示工程是制定 LLM 提示的艺术，以最大限度地提高任务性能。例如，将“请逐步思考”这样的指令添加到涉及推理的提示中，可以显著提高性能。术语**提示工程**是对潜在问题的非常乐观的表述：“你的模型比你想象的要好！你只需要正确使用它们！”更消极的表述可能是指出，对于任何看似有效的查询，都有一系列微小的变化可能会严重影响性能。如果你可以通过简单的改写来破坏 LLMs 的理解，那么 LLMs 对某事的理解程度有多大？

这种现象背后的原因是，LLM 是一个大的参数曲线——一个存储知识和程序的中介，你可以在这两个对象之间进行插值，以产生无限多的中间对象。你的提示是一种指向数据库特定位置的方式：如果你问，“如何在 Python 中排序一个列表？像海盗一样回答”，那是一种数据库查找，你首先检索一段知识（如何在 Python 中排序列表），然后检索并执行一个风格转换程序（“像海盗一样回答”）。

由于 LLM 索引的知识和程序是插值的，你可以在潜在空间中*移动*来探索附近的区域。一个略有不同的提示，比如“解释 Python 列表排序，但像海盗一样回答”，仍然会指向数据库中非常相似的位置，从而得到一个相当接近但并不完全相同的答案。你可以使用数千种不同的变体，每种变体都会得到一个相似但略有不同的答案。这就是为什么需要提示工程的原因。你的第一个、天真的提示没有先验理由是针对你的任务的优化。LLM 不会理解你的意思，然后以最佳方式执行它——它只是会在许多可能的着陆点中检索你的提示所指向的程序。

提示工程是通过试错法在潜在空间中搜索，以找到在目标任务上似乎表现最好的查找查询的过程。这与在谷歌搜索时尝试不同的关键词没有区别。如果大型语言模型（LLMs）实际上理解了你所问的内容，那么这个搜索过程就没有必要了，因为关于你的目标任务所传达的信息量不会因为你的提示使用“重写”而不是“改写”或者是否在提示前加上“逐步思考”而改变。永远不要假设 LLM 第一次就能“理解”你的意思——记住，你的提示只是无限程序海洋中的一个地址，所有这些程序都是作为学习完成大量标记序列的副产品而被记忆的。

### 深度学习模型在学习可泛化程序方面存在困难

深度学习模型的问题不仅仅是它们局限于盲目地重新应用在训练时记住的模式，或者它们对输入的呈现方式高度敏感。即使你只需要查询和应用一个已知的程序，并且你知道如何在潜在空间中精确地定位这个程序，你仍然面临一个主要问题：深度学习模型记住的程序通常泛化能力不好。它们对某些输入值有效，但对其他输入值无效。这对于编码任何类型的离散逻辑的程序尤其如此。

考虑一下加法的问题，即以字符序列表示的两个数字的相加——例如“4 3 5 7 + 8 9 3 6”。尝试训练一个 Transformer 在成千上万的此类数字对上：你将达到非常高的准确率。非常高，但不是 100%——你将经常看到错误的答案，因为 Transformer 无法成功编码精确的加法算法（你知道的，你在小学学过的那个）。相反，它通过在训练时看到的各个数据点之间进行插值来猜测输出。

这也适用于最先进的 LLMs——至少对于那些没有明确硬编码在 Python 中执行“4357 + 8936”等片段以提供正确答案的 LLMs。它们已经看到了足够多的数字加法示例，可以加法，但它们的准确率只有大约 70%——相当令人失望。此外，它们的准确率强烈依赖于*哪些*数字正在被加，更常见的数字会导致更高的准确率。

即使看到数百万个示例，深度学习模型也无法最终学习到精确的加法算法的原因是，它只是一个静态的简单、连续的几何变换链，将一个向量空间映射到另一个向量空间。这对于感知模式识别来说是一个很好的匹配，但对于编码任何类型的逐步离散逻辑（例如，如数位或进位这样的概念）来说，匹配得非常差。它所能做的就是将一个数据流形 X 映射到另一个流形 Y，假设存在一个可学习的连续变换从 X 到 Y。深度学习模型可以被解释为一种程序，但反过来，*大多数程序都不能表示为深度学习模型*。对于大多数任务来说，要么不存在合理大小的相应神经网络来解决该任务，要么即使存在，也可能不可学习：相应的几何变换可能过于复杂，或者可能没有适当的数据来学习它。

### 将机器学习模型拟人化的风险

我们对图像、声音和语言的理解根植于我们作为人类的感觉运动经验。机器学习模型无法访问这样的经验，因此无法以人类相关的方式理解它们的输入。通过向我们的模型输入大量训练示例，我们让它们学习一种几何变换，将数据映射到特定示例集上的人类概念，但这种映射只是我们心中原始模型——即从我们作为具身代理的经验中发展出来的模型——的简单草图。这就像镜子中的模糊图像（见图 19.4）。你创建的模型将采取任何可用的捷径来适应它们的训练数据。

![](img/b7716b74d6114efd798acee9fa22164e.png)

图 19.4：当前机器学习模型：就像镜子中的模糊图像

当代人工智能的一个真实风险是误解深度学习模型所做的事情，并高估它们的能力。人类的一个基本特征是我们的**心智理论**：我们倾向于将意图、信念和知识投射到周围的事物上。在石头上画一个笑脸突然让它在我们心中变得“快乐”。应用于深度学习，这意味着当我们训练能够使用语言的模型时，我们会认为模型“理解”它们生成的单词序列的内容，就像我们一样。然后，当我们发现任何与训练数据中存在的模式略有偏离时，模型就会产生完全荒谬的答案，我们会感到惊讶。

作为机器学习从业者，始终要意识到这一点，并永远不要陷入相信神经网络理解它们所执行的任务的陷阱——它们不理解，至少不是以对我们有意义的方式。它们被训练在一个与我们想要教给它们的任务截然不同、范围更窄的任务上：将训练输入映射到训练目标，一点一点地。向它们展示任何与它们的训练数据不符的东西，它们就会以荒谬的方式崩溃。

## 规模并不是一切

我们是否可以仅仅通过扩大模型规模来克服深度学习的局限性？**规模**是我们所需要的全部吗？这长期以来一直是该领域的普遍观点，特别是在 2023 年初，在 LLM 炒作的高峰期尤为突出。当时，GPT-4 刚刚发布，本质上只是 GPT-3 的扩大版：更多的参数，更多的训练数据。其显著提高的性能似乎表明，你可以继续前进——可能会有一个 GPT-5，它只是更多相同的东西，并且从它那里会自然地出现通用人工智能（AGI）。

这种观点的支持者会指出“缩放定律”作为证据。缩放定律是观察到的经验关系，即深度学习模型的大小（以及其训练数据集的大小）与其在特定任务上的性能之间的关系。它们表明，增加模型的大小可以可靠地以可预测的方式提高性能。但缩放定律的爱好者们忽略的关键问题是，他们用来衡量“性能”的基准实际上是记忆测试，这是我们喜欢给大学生出的测试。LLMs 通过记忆答案在这些测试中表现良好，而且自然地，将更多问题和答案塞入模型相应地提高了它们的性能。

事实上，扩大我们的模型并没有解决我在这些页面中列出的任何问题——无法适应新颖性、对措辞过度敏感以及无法推理出推理问题的通用程序——因为这些问题是曲线拟合，深度学习范式的固有属性。我从 2017 年开始指出这些问题，我们至今仍在努力解决这些问题——现在模型的大小已经增加了四到五个数量级，并且知识更加丰富。我们没有在这些问题上取得任何进展，因为*我们使用的模型仍然是相同的*。它们已经保持了七年多——它们仍然是通过对数据集进行梯度下降拟合的参数曲线，并且它们仍在使用 Transformer 架构。

通过堆叠更多层和使用更多训练数据来扩展当前的深度学习技术并不能解决深度学习的根本问题：

+   深度学习模型局限于使用它们在训练时记忆的内插程序。它们无法在推理时独立地合成全新的程序以适应大量新颖的情况。

+   即使在已知的情况下，这些内插程序也存在着泛化问题，这导致了对措辞的过度敏感和混淆特征。

+   深度学习模型在它们能表示的内容上有所局限，你可能会希望学习的多数程序不能被表示为数据流形连续几何变形。这在算法推理任务中尤其如此。

让我们更仔细地看看是什么将生物智能与深度学习方法区分开来。

### 自动机与智能体

深度学习模型从输入到输出的直接几何变形与人类思考和学习的差异是根本性的。这不仅仅是人类通过具身经验自学，而不是被提供明确的训练示例。与可微分的参数函数相比，人脑是完全不同的生物。

让我们稍微放大视角，问一问，智能的目的是什么？它最初为什么会出现？我们只能进行推测，但我们可以做出相当有根据的推测。我们可以从大脑开始看起——产生智能的器官。大脑是一种进化适应——一种在数亿年间通过自然选择引导的随机尝试和错误逐步发展起来的机制，它极大地扩展了生物适应其环境的能力。大脑最初在超过五十亿年前出现，作为一种存储和执行行为程序的方式。行为程序只是一系列指令，使生物对其环境产生反应：“如果发生这种情况，那么就做那件事。”它们将生物的感觉输入与其运动控制联系起来。最初，大脑的作用可能是硬编码行为程序（作为神经网络连接模式），这样生物就能对其感觉输入做出适当的反应。这就是昆虫大脑仍然工作的方式——苍蝇、蚂蚁、*C. elegans*（见图 19.5）等。因为这些程序的原初“源代码”是 DNA，它会被解码为神经网络连接模式，进化突然能够以在很大程度上不受限制的方式在行为空间中进行搜索——这是一个主要的进化转变。

![图片](img/bb8c04b7246b281dc75a4e71a9e4adf1.png)

图 19.5：秀丽线虫（*C. elegans*）的大脑网络：一个由自然进化“编程”的行为自动机。图由 Emma Towlson 创作（来自“网络控制原理预测秀丽隐杆线虫连接组中的神经元功能”，Yan 等人，《自然》，2017 年 10 月）。

进化是程序员，大脑是执行进化所提供代码的计算机。由于神经网络连接是一个非常通用的计算基础，所有大脑启用物种的感官运动空间突然开始经历巨大的扩张。眼睛、耳朵、颚、4 条腿、24 条腿——只要你有大脑，进化就会为你找到利用这些特性的行为程序。大脑可以处理你扔给它的任何模态，或模态的组合。

现在，请注意，这些早期的头脑本身并不一定具有智能。它们非常像自动机：它们只会执行生物 DNA 中硬编码的行为程序。它们只能被描述为具有与恒温器“智能”相同的智能。或者是一个列表排序程序。或者是一个训练有素的深度神经网络（人工的那种）。这是一个重要的区别，让我们仔细看看：自动机和真正的智能代理之间有什么区别？

### 局部泛化与极端泛化

人工智能领域长期以来一直受到将*智能*和*自动化*概念混淆的困扰。一个自动化系统（或自动机）是静态的，被设计在特定环境中完成特定任务——“如果这个，那么那个”——而智能代理可以即时适应新颖、意外的情况。当一个自动机遇到不符合其“编程”去做的事情时（无论是关于人类编写的程序、进化生成的程序，还是将模型拟合到训练数据集上的隐式编程过程），它将失败。

同时，像我们人类这样的智能代理将利用他们的流动智能找到前进的道路。你如何区分一个只是记住了过去三年考试问题但没有理解学科的学生和一个真正理解材料的学生？你给他们一个全新的问题。

人类的能力远不止像深度网络或昆虫那样将即时刺激映射到即时反应。我们能够即时构建关于我们当前情况、我们自己以及他人的复杂、抽象模型，并可以使用这些模型来预测不同的可能未来并执行长期规划。我们能够快速适应意外情况，并在少量练习后掌握新技能。

这种使用*抽象*和*推理*来处理我们没有准备好的经验的能力是人类认知的标志性特征。我称之为*极端泛化*：一种使用少量数据甚至没有新数据就能适应新颖、从未经历过的情境的能力。这种能力是人类和高级动物所展现的智能的关键。

这与类似自动机系统的行为形成了鲜明的对比。一个非常僵化的自动机根本不具备任何泛化能力；它无法处理任何它事先没有精确告知的事情。Python 字典，或者作为硬编码的 if-then-else 语句实现的简单问答程序就会属于这一类。深度网络做得稍微好一些：它们可以成功处理与它们熟悉的情况略有偏差的输入，这正是它们有用的地方。我们第八章中的狗与猫模型可以分类它之前没有见过的猫或狗图片，只要它们足够接近训练时的样本。然而，深度网络局限于我所说的*局部泛化*（见图 19.6）：当输入开始偏离网络在训练时看到的输入时，深度网络从输入到输出的映射很快就会变得没有意义。深度网络只能泛化到*已知未知*，到在模型开发期间预测到的变化因素，这些因素在训练数据中广泛出现，例如宠物图片的不同角度或光照条件。这是因为深度网络通过在流形上的插值进行泛化（记得第五章）：它们输入空间中的任何变化因素都需要被它们学习的流形所捕捉。这就是为什么基本的数据增强在提高深度网络泛化能力方面非常有帮助。与人类不同，这些模型在面对数据很少或没有数据的情况时，没有任何即兴发挥的能力。

![](img/d7476909b13b29de39c7b82a79798be3.png)

图 19.6：局部泛化与极端泛化

例如，考虑这样一个问题：学习适当的发射参数，使火箭能够成功着陆在月球上。如果你为这个任务使用深度网络，并使用监督学习或强化学习来训练它，你将不得不给它提供成千上万甚至数百万次的发射试验：你需要让它接触到输入空间的密集采样，以便它能够学习从输入空间到输出空间的可靠映射。相比之下，作为人类，我们可以利用我们的抽象能力来提出物理模型——火箭科学——并推导出一个精确的解决方案，这个方案将使火箭在一次性或几次试验中成功着陆在月球上。同样，如果你开发了一个控制人类身体的深度网络，并且你希望它学会在不会撞到汽车的情况下安全地导航城市，网络将不得不在各种情况下死亡成千上万次，直到它能够推断出汽车是危险的，并发展出适当的避免行为。被投入到一个新的城市中，网络将不得不重新学习它所知道的大部分内容。另一方面，人类能够在不死亡的情况下学会安全的行为——这再次得益于我们抽象建模新情况的能力。

### 智能的目的

这种高度适应性的智能代理和僵化的自动机之间的区别让我们回到了大脑进化的主题。为什么大脑——最初只是自然进化发展行为自动机的媒介——最终变得智能？像每一个重要的进化里程碑一样，这是由于自然选择的限制鼓励了这一变化的发生。

大脑负责行为生成。如果一个生物体必须面对的情况大多是静态的且提前已知，那么行为生成将是一个简单的问题：进化只需通过随机尝试和错误找出正确的行为，并将它们硬编码到生物体的 DNA 中。大脑进化的这个第一阶段——作为自动机的头脑——就已经是最佳的。然而，关键的是，随着生物体复杂性和与之相伴的环境复杂性的持续增加，动物必须应对的情况变得更加动态和不可预测。如果你仔细观察，你生活中的每一天都不像你以前经历过的任何一天，也不像你的进化祖先们经历过的任何一天。你需要能够不断面对未知和意外的情况。进化无法找到并将你从几个小时前醒来以来成功导航你一天的行为序列硬编码到 DNA 中。它必须每天即时生成。

大脑，作为一个良好的行为生成引擎，只是适应了这一需求。它优化了适应性和通用性本身，而不仅仅是优化对一组固定情况的适应性。这种转变很可能在进化历史的多个时期发生，导致了在非常遥远的进化分支上的高度智能动物——猿类、章鱼、乌鸦等等。智能是对复杂、动态生态系统提出的挑战的回答。

这就是智能的本质：它是在不确定、不断变化的未来中，有效地利用你拥有的信息来产生成功行为的能力。笛卡尔所说的“理解”是这种非凡能力的关键：挖掘你过去经验的能力，以开发模块化、可重用的抽象，这些抽象可以快速重新用于处理新情况并实现极端的泛化。

### 爬升泛化谱系

用一种粗略的讽刺画来说，你可以将生物智能的进化历史总结为一种缓慢的沿着*泛化光谱*的攀登。它始于只能执行局部泛化的类似自动机的头脑。随着时间的推移，进化开始产生能够进行越来越广泛泛化的生物体，这些生物体能够在越来越复杂和多变的环境中生存。最终，在过去的几百万年——在进化术语中是一瞬间——某些人属物种开始趋向于实现能够进行极端泛化的生物智能的实施，从而引发了人类世的开端，永远改变了地球上生命的历史。

过去 70 年间人工智能的进步与这一演变过程有着惊人的相似之处。早期的 AI 系统是纯粹的自动机，例如 20 世纪 60 年代的 ELIZA 聊天程序，或者 SHRDLU：^([[2]](#footnote-2))，这是一种 1970 年能够通过自然语言命令操纵简单物体的 AI。在 1990 年代和 2000 年代，我们见证了能够处理一定程度的不确定性和新颖性的局部泛化机器学习系统的兴起。在 2010 年代，深度学习通过使工程师能够使用更大的数据集和更具有表现力的模型，进一步扩展了这些系统的局部泛化能力。

今天，我们可能正处于下一个进化步骤的边缘。我们正在走向能够实现*广泛泛化*的系统，我将这定义为在单个广泛的任务领域内处理*未知未知*的能力（包括系统未接受过训练的情况以及其创造者无法预见的情况）。例如，一辆能够安全应对任何情况的自动驾驶汽车或能够通过“沃兹智力测试”的家庭机器人——进入一个随机的厨房并煮一杯咖啡：^([[3]](#footnote-3))。通过结合深度学习和精心手工制作的抽象模型，我们已经在朝着这些目标取得明显的进展。

然而，深度学习范式仍然局限于认知自动化：在“人工智能”中的“智能”标签是一个分类错误。更准确地说，我们应该称我们的领域为“人工认知”，其中“认知自动化”和“人工智能”是其中的两个几乎独立的子领域。在这个细分中，AI 将是一个绿洲，其中几乎一切都还有待发现。

现在我不打算贬低深度学习的成就。认知自动化非常有用，深度学习模型仅通过数据暴露就能自动化任务的能力，代表了一种特别强大的认知自动化形式，比显式编程更实用、更灵活。做好这一点对于几乎所有行业来说都是一场变革。但它离人类（或动物）的智能还差得很远。到目前为止，我们的模型只能进行局部泛化：它们通过从 X 到 Y 的数据点的密集采样中学习到的平滑几何变换，将空间 X 映射到空间 Y，而 X 或 Y 中的任何干扰都会使这种映射无效。它们只能泛化到与过去数据相似的新情况，而人类的认知能够进行极端的泛化，快速适应极端新颖的情况，并为长期未来的情况做出规划。

## 如何构建智能

到目前为止，你已经了解到，智能远不止深度学习所进行的这种潜在流形插值。那么，我们究竟需要从哪里开始构建真正的智能？目前我们还在逃避的核心要素是什么？

### 万花筒假说

智能是利用你的过去经验（以及先天的先验知识）来面对新颖、意外的未来情况的能力。现在，如果你必须面对的未来是*真正新颖的*——与之前所见的一切都没有共同点——无论你多么聪明，你都无法对它做出反应。

智能之所以有效，是因为没有任何事物是完全没有先例的。当我们遇到新事物时，我们能够通过将它们与过去的经验进行类比，并使用我们收集多年的抽象概念来阐述它们，从而理解它们。一个 17 世纪的人第一次看到喷气式飞机时，可能会描述它为一只大而响亮的金属鸟，它不会拍打翅膀。汽车？那是一种无马的马车。如果你试图向小学生讲解物理，你可以解释电就像水管中的水，或者时空就像被重物扭曲的橡皮膜。

除了这样明确、显式的类比之外，我们还在不断地做出更小、更隐晦的类比——每秒钟，每思考一次。类比是我们导航生活的方式。在一家新的超市购物？你会通过将其与你去过的类似商店联系起来找到自己的路。与新人交谈？他们会让你想起你之前遇到的一些人。甚至看似随机的模式，如云的形状，也会立刻在我们脑海中唤起生动的图像——一头大象，一艘船，一条鱼。

这些类比不仅存在于我们的脑海中：物理现实本身充满了同构。电磁学与重力相似。由于共同的起源，动物在结构上彼此相似。二氧化硅晶体与冰晶体相似。等等。

我把这个称为*万花筒假说*：我们对世界的体验似乎具有难以置信的复杂性和永无止境的新颖性，但这个复杂性海洋中的每一件事都与其他事物相似。你需要描述你所处的宇宙的独特*意义原子*的数量相对较小，而你周围的一切都是这些原子的重组：一些种子，无尽的变异，就像万花筒内部发生的事情一样，其中一些玻璃珠被一组镜子反射，产生丰富、看似无尽的图案（见图 19.7）。

![图片](img/70a8b1d2b9cca867dc91b790b56e9ae2.png)

图 19.7：万花筒仅从几颗彩色玻璃珠中产生丰富（但重复）的图案。

### 智力的本质：抽象获取和重组

智力是挖掘你的经验以识别这些可以看似在不同情况下重复使用的意义原子——万花筒的核心珠子。一旦提取出来，它们就被称为*抽象*。无论何时你遇到一个新情况，你都会通过即时重新组合你收藏中的抽象来理解它，以编织一个全新的“模型”，适应该情况。

这个过程包括两个关键部分：

+   *抽象获取*——有效地从一系列经验或数据中提取紧凑、可重用的抽象。这涉及到识别潜在的结构、原则或不变量。

+   *即时重组*——以新颖的方式高效地选择和重组这些抽象来模拟新的问题和情况，甚至那些与以往经验截然不同的情况。

对*效率*的强调至关重要。你的智能取决于你从有限的经验中获取良好抽象的效率，以及你如何有效地重组它们以导航不确定性和新颖性。如果你需要数万小时的练习来掌握一项技能，那么你并不聪明。如果你需要列出棋盘上所有可能的走法来找到最佳走法，那么你也不聪明。

这就是经典深度学习范式中的两个主要问题的来源：

+   这些模型完全缺乏即时重组。它们在训练时间通过梯度下降在获取抽象方面做得相当不错，但按照设计，它们在测试时间没有能力重组它们所知道的内容。它们的行为就像一个静态的抽象数据库，仅限于检索。它们遗漏了整个画面的一半——最重要的那一半。

+   他们效率极低。梯度下降需要大量的数据来提炼整洁的抽象——比人类多出许多数量级的更多数据。

那么，我们如何超越这些限制？

### 设置正确目标的重要性

生物智能是自然界提出的问题的答案。同样，如果我们想开发真正的 AI，首先，我们需要提出正确的问题。最终，AI 系统的能力反映了它们被设计和优化的目标。

在系统设计中，你经常会看到一种效应，即*捷径规则*：如果你专注于优化一个成功指标，你将实现你的目标，但要以牺牲系统内所有未涵盖在你成功指标中的东西为代价。你最终会采取通往目标的每一个可用的捷径。你的创造物是由你给予自己的激励所塑造的。

你在机器学习竞赛中经常看到这种情况。2009 年，Netflix 举办了一场挑战赛，承诺向实现电影推荐任务最高分数的团队提供 100 万美元的奖金。结果，他们从未使用获胜团队创建的系统，因为它过于复杂且计算密集。获胜者只优化了预测准确性——他们被激励去实现的目标——而牺牲了系统其他所有可取的特性：推理成本、可维护性、可解释性。在大多数 Kaggle 竞赛中，捷径规则也是成立的——Kaggle 获胜者产生的模型很少，如果有的话，可以在生产中使用。

在过去几十年中，*捷径规则*在 AI 领域无处不在。在 20 世纪 70 年代，心理学家和计算机科学先驱艾伦·纽厄尔担心他的领域没有在向正确的认知理论取得任何有意义的进展，因此为 AI 提出了一个新的宏伟目标：下棋。其理由是，在人类中，下棋似乎涉及到——也许甚至需要——诸如感知、推理和分析、记忆和从书籍中学习等能力。当然，如果我们能构建一个下棋的机器，它也必须具备这些属性。对吧？

二十多年后，梦想成真：1997 年，IBM 的 Deep Blue 击败了世界最佳棋手加里·卡斯帕罗夫。那时，研究人员不得不面对这样一个事实：创建一个棋类冠军 AI 并没有让他们对人类智能有太多了解。Deep Blue 的核心算法 A*并不是人类大脑的模型，也不能推广到除类似棋类游戏之外的任务。结果证明，构建一个只能下棋的 AI 比构建一个人工智能大脑要容易得多——这就是研究人员采取的捷径。

到目前为止，AI 领域的*驱动成功指标一直是解决特定任务*，从棋类到围棋，从 MNIST 分类到 ImageNet，从高中数学考试到律师资格考试。因此，该领域的历史是由一系列“成功”定义的，在这些“成功”中，*我们找到了解决这些任务的方法，而没有涉及任何智能*。

如果这听起来像是一个令人惊讶的陈述，请记住，类似人类的智能并不是由任何特定任务的技能所定义——相反，它是指适应新事物以高效获取新技能和掌握从未见过的任务的能力。通过固定任务，你使得提供对需要完成的事情的任意精确描述成为可能——无论是通过硬编码人类提供的知识，还是通过提供巨大的数据量。你使得工程师只需添加数据或添加硬编码的知识，就能“购买”更多技能给他们的 AI，而不需要增加 AI 的泛化能力（见图 19.8）。如果你有近乎无限的训练数据，即使是像最近邻搜索这样非常粗糙的算法也能以超人的技能玩电子游戏。同样，如果你有近乎无限的人类编写的 if-then-else 语句——也就是说，直到你对游戏规则进行微小改变，这种改变是人类可以立即适应的——这将要求无智能的系统重新训练或从头开始重建。

![图片](img/888660ebed8c3c6535cba344ae288fb5.png)

图 19.8：一个低泛化能力的系统在给定无限的任务特定信息的情况下，可以在固定任务上实现任意技能。

简而言之，通过固定任务，你消除了处理不确定性和新事物的需求，而智能的本质就是处理不确定性和新事物，因此你实际上消除了对智能的需求。而且，由于找到特定任务的低智能解决方案总是比解决智能的普遍问题更容易，所以这就是你 100%会采取的捷径。人类可以用他们的通用智能来学习任何新任务的技能，但反过来，从一系列特定任务的技能到通用智能没有路径。

### 新的目标：即时适应

要使 AI 真正智能并赋予它处理现实世界难以置信的多样性和不断变化性质的能力，首先，我们需要摆脱寻求实现*特定任务技能*的愿望，转而开始针对泛化能力本身。我们需要新的进展指标，这将帮助我们开发越来越智能的系统：指标将指引正确的方向，并给我们一个可操作的反馈信号。只要我们设定的目标是“创建一个解决任务 X 的模型”，捷径规则就会适用，我们最终会得到一个只做 X 的模型。

在我看来，智能可以精确地量化为一个*效率比率*：你关于世界的相关信息量（这可能是过去的经验或先天的先验知识）与你的*未来操作区域*之间的转换比率，即你将能够产生适当行为的新的情境集合（你可以将其视为你的*技能集*）。一个更智能的代理将能够使用更少的过去经验处理更广泛的未来任务和情境。为了测量这个比率，你只需要固定你系统可用的信息——它的经验和它的先验知识——并测量它在一系列已知与系统所接触到的内容足够不同的参考情境或任务上的表现。试图最大化这个比率应该会引导你走向智能。关键的是，为了避免作弊，你需要确保只在系统没有被编程或训练来处理的任务上对其进行测试——实际上，你需要的是系统*创造者无法预料的*任务。

在 2018 年和 2019 年，我开发了一个名为*抽象与推理语料库用于通用人工智能（ARC-AGI）*的基准数据集（^[[4]](#footnote-4)），旨在捕捉这种智能的定义。ARC-AGI 旨在使机器和人类都能接近，它看起来非常类似于人类智商测试，例如拉文渐进矩阵测试。在测试时，你会看到一系列“任务”。每个任务都通过三到四个“示例”进行解释，这些示例以输入网格和相应的输出网格的形式出现（见图 19.9）。然后你会得到一个全新的输入网格，你将有三次机会产生正确的输出网格，之后才能进行下一个任务。

![图片](img/9659a2add458e3a593bbcf376baaf32f.png)

图 19.9：一个 ARC-AGI 任务：任务性质通过几个输入-输出对示例进行展示。给定一个新的输入，你必须构建相应的输出。

与智商测试相比，ARC-AGI 有两个独特之处。首先，ARC 通过只测试你从未见过的任务来寻求衡量泛化能力。这意味着 ARC-AGI 是一个你无法练习的游戏，至少在理论上是这样：你将要接受测试的任务将具有其独特的逻辑，你必须即时理解。你不能只是记住过去任务中的特定策略。

此外，ARC-AGI 试图控制你在测试中带来的*先验知识*。你永远不会完全从头开始接近一个新问题——你带着先前的技能和信息接近它。ARC-AGI 假设所有测试者都应该从一组称为*核心知识先验*的知识集合开始，这代表了人类天生具有的知识系统。与智商测试不同，ARC-AGI 任务永远不会涉及获得的知识，例如英语句子等。

### ARC 奖

在 2024 年，为了加速向能够进行类似 ARC-AGI 所测量的那种流畅抽象和推理的人工智能系统迈进，我与 Mike Knoop 合作成立了非营利性的 ARC Prize 基金会。该基金会每年举办一次竞赛，奖金池丰厚（2024 年的版本超过 100 万美元）以激励研究人员开发能够解决 ARC-AGI 并因此展现真正流畅智能的人工智能。

ARC-AGI 基准对主流的深度学习扩展范式表现出惊人的抵抗力。在 LLM 时代，大多数其他基准很快就已经饱和。那是因为它们可以通过记忆来破解，而 ARC-AGI 被设计成对此具有抵抗力。从 2019 年 ARC-AGI 首次发布到 2025 年，基础 LLM 经历了大约 50,000 倍的扩展——从 GPT-2（2019）到 GPT-4.5（2025），但它们在 2019 版 ARC-AGI 上的表现仅从 0%上升到大约 10%。考虑到读者您很容易就能得到 95%以上的分数，这并不好。

如果你将你的系统扩展了 50,000 倍，但你仍然没有取得有意义的进展，那就像一个巨大的警告信号告诉你需要尝试新的想法。仅仅使模型更大或用更多数据训练它们并没有解锁 ARC-AGI 所需的流畅智能。ARC-AGI 明显表明，即时重组能力是解决推理所必需的。

### 测试时自适应时代

在 2024 年，一切发生了改变。那一年见证了主要叙事的转折——部分是由 ARC Prize 催化的。2023 年的主流“规模就是一切”的故事，曾是那个时代的基石教条，开始让位于“实际上，我们需要即时重组”。2024 年 12 月宣布的竞赛结果令人启迪：领先的解决方案并非仅仅通过扩展现有的深度学习架构产生。它们都使用了某种形式的测试时自适应（TTA）——要么是测试时搜索，要么是测试时训练。

TTA 指的是 AI 系统在测试过程中进行主动推理或学习的方法，使用特定问题信息——这是经典深度学习范式所缺失的关键组件。

有几种方法可以实现测试时自适应：

+   *测试时训练*——模型根据测试任务中给出的示例调整其部分参数，使用梯度下降。

+   *搜索方法*——系统在测试时搜索许多可能的推理步骤或潜在解决方案，以找到最佳方案。这可以是自然语言（思维链合成）或在一个符号、可验证的程序空间中（程序综合）。

这些 TTA 方法使 AI 系统更加灵活，并能比静态模型更好地处理新颖性。ARC Prize 2024 的每个顶级参赛作品都使用了它们。

在比赛结束不久，2024 年 12 月底，OpenAI 预览了其 o3 测试时推理模型，并使用 ARC-AGI 展示了其前所未有的能力。利用大量的测试时计算资源，该模型以每项任务约 200 美元的成本实现了 76%的得分，以每项任务超过 20,000 美元的成本实现了 88%的得分，超过了名义上的人类基准。我们第一次看到了一个显示出真正流畅智能迹象的人工智能模型。这一突破打开了类似技术的新一波兴趣和投资的闸门——测试时适应时代开始了。重要的是，ARC-AGI 是当时唯一提供明确信号的基准之一，表明正在进行一场重大的范式转变。

### ARC-AGI 2

这是否意味着 AGI 已经被解决了？o3 的智能是否与人类相当？

还不尽然。首先，虽然 o3 的表现是一个里程碑式的成就，但它付出了巨大的代价——每个 ARC-AGI 谜题的计算成本高达数万美元。智能不仅仅是关于能力；它本质上关乎效率。在巨大的计算能力下强行搜索解决方案空间是一种捷径，它使得各种任务成为可能，而不需要智能。原则上，你甚至可以通过简单地遍历每个可能的解决方案程序树并测试每一个，直到找到在演示对中能工作的一个来解决问题 ARC-AGI。尽管 o3 的结果令人印象深刻，但它们更像是用超级计算机破解代码，而不是展示灵活、类似人类的流畅推理。智能的整个目的就是以尽可能少的资源实现结果。

其次，我们发现 o3 仍然被许多人类认为非常简单（如图 19.10 所示）的任务难住了。这强烈表明 o3 还没有达到人类水平。这里的关键是，2019 版本的 ARC-AGI 旨在简单。它本质上是对流体智能的二进制测试：要么你没有流体智能，就像所有基础 LLM 一样，在这种情况下你的得分接近零，要么你确实表现出一些真正的流体智能，在这种情况下你将立即获得极高的分数，就像任何人类——或者 o3 一样。中间没有太多空间。很明显，基准需要随着它旨在衡量的 AI 能力的发展而发展。需要一个新的 ARC-AGI 版本，它不那么容易强行解决，并且能够更好地区分具有不同水平流体推理能力的系统，直至达到人类水平的流体智能。好消息是：我们自 2022 年以来一直在研究一个。

![图片](img/9077c542d7115d9d9680d5a0e76a3f51.png)

图 19.10：o3 在最高计算设置下（每项任务超过 20,000 美元）无法解决的问题示例

因此，在 2025 年 3 月，ARC Prize Foundation 推出了 ARC-AGI-2。它保留了与第一版完全相同的格式，但显著提高了任务内容。新的迭代旨在提高标准，包括需要更复杂推理链和本质上更难以穷举搜索方法的任务。目标是创建一个基准，其中计算效率成为成功的关键因素，推动系统走向更真实、更有效的策略，而不仅仅是探索数十亿种可能性。虽然大多数 ARC-AGI-1 任务几乎可以立即由人类解决，而不需要太多认知努力，但 ARC-AGI-2 中的所有任务都需要一定程度的深思熟虑（见图 19.11）——例如，在我们实验中人类测试者完成任务的平均时间是 5 分钟。

![图片](img/b57cb6381b4a41c9cf47ade9836d89bc.png)

图 19.11：典型的 ARC-AGI-1 任务（左）与典型的 ARC-AGI-2 任务（右）

在 ARC-AGI 2 上的初始 AI 测试结果令人沮丧：即使是 o3 在与这一新挑战的斗争中也遇到了重大困难，其分数在合理计算预算限制下暴跌至个位数以下。至于基础 LLM 呢？它们在 ARC-AGI-2 上的表现实际上回到了 0%——这是合适的，因为基础 LLM 没有流体智力。构建具有真正高效、类似人类流体智力的 AI 的挑战仍然远未解决。我们需要超越当前 TTA 技术的某种东西。

## 缺少的要素：搜索和符号

要完全解决 ARC-AGI，特别是版本 2，需要什么？希望这个挑战能让你思考。这正是 ARC-AGI 的全部目的：给你一个不同类型的目标，这将推动你走向新的方向——希望是一个富有成效的方向。现在，让我们快速看一下如果你想要回应这个召唤，你需要的关键要素。

我说过，智力由两个组成部分组成：**抽象获取**和**抽象重组**。它们紧密相连——你操作的是哪种类型的抽象决定了你如何以及如何有效地重组它们。深度学习模型只操作通过参数曲线存储的抽象，通过梯度下降拟合。是否可能有更好的方法？

### 抽象的两个极端

抽象获取始于**相互比较**。关键的是，有两种不同的比较方式，由此产生了两种不同的抽象类型和两种不同的思维方式，每种方式更适合解决不同类型的问题。这两个抽象的极端共同构成了我们所有思想的基础。

将事物相互关联的第一种方式是*相似性比较*，这产生了以*价值为中心的类比*。第二种方式是*精确的结构匹配*，这产生了以*程序为中心的类比*（或以结构为中心的类比）。在这两种情况下，你都是从某个事物的一个*实例*开始，并将相关的实例合并在一起，以产生一个*抽象*，这个抽象能够捕捉到潜在实例的共同元素。变化的是你如何判断两个实例之间的关系，以及你如何将实例合并到抽象中。让我们逐一仔细看看每种类型。

#### 价值中心类比

假设你在后院看到许多不同的甲虫，属于多个物种。你会注意到它们之间的相似性。有些会彼此更相似，有些则不那么相似：相似性的概念隐含着一个平滑的、连续的*距离函数*，它定义了一个潜在流形，你的实例就生活在这个流形上。一旦你看过足够的甲虫，你就可以开始将更相似的实例聚在一起，并将它们合并成一个*原型*集合，这个集合能够捕捉到每个集群共享的视觉特征（图 19.12）。这些原型是抽象的：它们看起来不像你见过的任何特定实例，尽管它们编码了它们共有的属性。当你遇到一个新的甲虫时，你不需要将它与你之前看到的每一个甲虫进行比较，就知道如何处理它。你只需将它与你手中的几个原型进行比较，找到最接近的原型——甲虫的*类别*——然后用它做出有用的预测：甲虫可能会咬你吗？它会吃你的苹果吗？

![图片](img/3d23d2d57a0d3e7fe97a45187ff3ca68.png)

图 19.12：价值中心类比通过连续的相似性概念将实例联系起来，以获得抽象原型。

这听起来熟悉吗？这几乎是对无监督机器学习（如 K-means 聚类算法）的描述。一般来说，所有现代机器学习，无论是监督还是无监督，都是通过学习潜在流形来工作的，这些流形描述了一个实例空间，并通过原型进行编码。（还记得第十章中可视化的 ConvNet 特征吗？它们是视觉原型。）以价值为中心的类比是那种能够使深度学习模型进行局部泛化的类比制作方式。

这也是许多你自己的认知能力所依赖的。作为一个人类，你一直在进行以价值为中心的类比。这是*模式识别*、*感知*和*直觉*背后的抽象类型。如果你能不思考就能完成任务，你很大程度上是在依赖以价值为中心的类比。如果你在看电影，并开始无意识地将不同的角色分类为“类型”，那是以价值为中心的抽象。

#### 程序中心类比

关键的是，认知不仅仅是价值中心类比所允许的那种直接、近似、直觉的分类。还有一种抽象生成机制，较慢、精确、深思熟虑：程序中心（或结构中心）类比。

在软件工程中，你经常编写看起来有很多共同点的不同函数或类。当你注意到这些冗余时，你开始思考，是否有一个更抽象的函数可以执行相同的工作，并且可以被重复使用两次？是否有一个抽象的基类，你的两个类都可以从中继承？你在这里使用的抽象定义对应于程序中心类比。你并不是试图通过它们看起来**多么相似**来比较你的类和函数，就像你通过隐含的距离函数比较两个人的脸一样。相反，你感兴趣的是它们是否有**部分**具有**完全相同的结构**。你正在寻找所谓的**子图同构**（见图 19.13）：程序可以用操作符的图来表示，你正在尝试找到在不同程序中完全共享的子图（程序子集）。

![图片](img/d1ce12ea5f821fdbae5cc53e4a36dcfe.png)

图 19.13：程序中心类比识别并隔离不同实例间的同构子结构。

这种在不同离散结构内部通过精确结构匹配进行类比制作的过程，并不局限于像计算机科学或数学这样的专业领域——你一直在不知不觉中使用它。它构成了**推理**、**规划**以及**严谨**（相对于直觉）的一般概念。每当你思考通过离散关系网络相互关联的对象（而不是连续相似性函数）时，你就是在使用以程序为中心的类比。

### 认知作为两种抽象的结合

表 19.1 并排比较了这两个抽象的极端。

| 价值中心抽象 | 程序中心抽象 |
| --- | --- |
| 通过距离关联事物 | 通过精确的结构匹配关联事物 |
| 连续的，基于几何的。 | 离散的，基于拓扑的 |
| 通过“平均”实例到“原型”产生抽象 | 通过隔离实例间的同构子结构产生抽象 |
| 基于感知和直觉 | 基于推理和规划 |
| 立即的，模糊的，近似的 | 慢速的，精确的，严谨的 |
| 产生可靠结果需要大量经验 | 经验高效：可以操作最少的两个实例 |

表 19.1：抽象的两个极端

我们所做的一切，我们思考的一切，都是这两种抽象类型的组合。你很难找到只涉及其中一种的任务。即使是看似“纯粹感知”的任务，比如在场景中识别物体，也涉及到大量关于你所观察到的物体之间关系的隐含推理。而即使是看似“纯粹推理”的任务，比如寻找数学定理的证明，也涉及到大量的直觉。当数学家把笔放在纸上时，他们已经对将要走的方向有一个模糊的愿景。他们为了到达目的地所采取的离散推理步骤是由高级直觉引导的。

这两个极端是互补的，正是它们的交织使得极端泛化成为可能。没有哪一个心智可以没有它们而完整。

### 为什么深度学习不是抽象生成完整答案

深度学习在编码以价值为中心的抽象方面非常出色，但它基本上没有能力生成以程序为中心的抽象。类似人类的智能是这两种类型的紧密交织，所以我们实际上缺少了我们需要的另一半——可以说是最重要的另一半。

现在，这里有一个警告。到目前为止，我把每种类型的抽象都描述为完全独立于另一种——甚至是相反的。然而，在实践中，它们更像是一个光谱：在一定程度上，你可以通过在连续流形中嵌入离散程序来进行推理——就像你可能通过任何一组离散点拟合多项式函数一样，只要你有多达足够的系数。反过来，你可以使用离散程序来模拟连续的距离函数——毕竟，当你用计算机进行线性代数时，你是在连续空间中工作，完全是通过在 1 和 0 上操作的离散程序来实现的。

然而，显然有一些问题更适合其中一种类型。比如，尝试训练一个深度学习模型来排序五个数字的列表。有了正确的架构，这并非不可能，但这是一个令人沮丧的练习。你需要大量的训练数据才能让它发生——即使如此，当面对新的数字时，模型仍然会偶尔犯错。而如果你想要开始排序十个数字的列表，你需要完全重新训练模型——在更多的数据上。与此同时，用 Python 编写一个排序算法只需要几行代码，一旦在几个额外的例子上验证通过，它就可以在任何大小的列表上工作。这相当强的泛化能力：从几个演示示例和测试示例到可以成功处理任何数字列表的程序。

反过来，感知问题与离散推理过程非常不匹配。尝试编写一个纯 Python 程序来分类 MNIST 数字，不使用任何机器学习技术：你将面临一场挑战。你会发现自己在费力地编写可以检测数字中闭合环数的函数、数字质心的坐标等等。在编写数千行代码之后，你可能会达到 90%的测试准确率。在这种情况下，拟合参数模型要简单得多；它可以更好地利用大量可用数据，并实现更稳健的结果。如果你有很多数据，并且面临一个适用于流形假设的问题，那么选择深度学习。

因此，我们不太可能看到一种将推理问题简化为流形插值或把感知问题简化为离散推理的方法的出现。在人工智能领域的前进方向是开发一个统一的框架，该框架包含**两种**抽象生成类型。

### 人工智能的另一种方法：程序综合

直到 2024 年，能够进行真正离散推理的人工智能系统都是由人类程序员硬编码的——例如，依赖于搜索算法、图操作和形式逻辑的软件。在测试时自适应（TTA）时代，这种情况终于开始改变。特别有前途的 TTA 分支之一是**程序综合**——一个今天仍然非常小众的领域，但我预计在接下来的几十年里它将迎来大发展。

程序综合是通过使用搜索算法（可能是遗传搜索，如**遗传编程**）来探索大量可能的程序空间（见图 19.14）来自动生成简单的程序。当找到一个符合所需规格的程序时，搜索停止，这些规格通常以一组输入输出对的形式提供。这非常类似于机器学习：给定作为输入输出对提供的训练数据，我们找到一个匹配输入到输出的程序，并且可以推广到新的输入。区别在于，我们不是在硬编码的程序（神经网络）中学习参数值，而是通过离散搜索过程生成源代码（见表 19.2）。

![](img/7d0d57c1aff590444ccd30601e61c047.png)

图 19.14：程序综合的示意图：给定一个程序规范和一组构建块，搜索过程将构建块组装成候选程序，然后对这些候选程序进行测试，以符合规范。搜索会继续进行，直到找到一个有效的程序。

| 机器学习 | 程序综合 |
| --- | --- |
| 模型：可微分的参数函数 | 模型：编程语言操作符的图 |
| 引擎：梯度下降 | 引擎：离散搜索（如遗传搜索） |
| 需要大量数据来产生可靠的结果 | 数据高效：可以用几个训练示例工作 |

表 19.2：机器学习与程序综合的比较

程序综合是我们将向我们的 AI 系统添加以程序为中心的抽象能力的方法。它是拼图缺失的一块。

### 混合深度学习和程序综合

当然，深度学习不会消失。程序综合不是它的替代品；它是它的补充。这是我们人工大脑迄今为止缺失的半球。我们将两者结合使用。这种结合将主要有两种方式：

+   开发集成深度学习模块和离散算法模块的系统

+   使用深度学习使程序搜索过程本身更高效

让我们回顾一下这些可能的途径。

#### 将深度学习模块和算法模块集成到混合系统中

今天，许多最强大的 AI 系统都是混合型的：它们既使用深度学习模型，也使用手工编写的符号操作程序。例如，在 DeepMind 的 AlphaGo 中，展示的大部分智能都是由人类程序员（如蒙特卡洛树搜索）设计和硬编码的。数据学习仅发生在专门的子模块（价值网络和政策网络）中。或者考虑 Waymo 自动驾驶汽车：它能够处理大量不同的场景，因为它维护着周围世界的模型——一个字面上的 3D 模型——其中充满了由人类工程师硬编码的假设。这个模型通过深度学习感知模块（由 Keras 提供动力）不断更新，这些模块将其与汽车周围的环境接口。

对于这两个系统——AlphaGo 和自动驾驶汽车——人类创建的离散程序和学习的连续模型相结合，才能解锁一种单独使用任何一种方法都无法达到的性能水平，例如端到端的深度网络或不含机器学习元素的软件。到目前为止，这种混合系统的离散算法元素都是人类工程师费力地硬编码的。但在未来，这样的系统可能完全是通过学习实现的，没有任何人类参与。

这会是什么样子呢？考虑一种著名的网络类型：循环神经网络（RNNs）。需要注意的是，RNNs 比前馈网络有稍微少的限制。这是因为 RNNs 不仅仅是几何变换：它们是在`for`循环中反复应用的几何变换。时间`for`循环本身是由人类开发者硬编码的：这是网络的内置假设。自然地，RNNs 在它们能表示的内容上仍然极为有限，主要是因为它们每一步执行的都是可微的几何变换，并且它们通过连续几何空间中的点（状态向量）从一步传递到下一步携带信息。现在想象一个以类似方式增强的神经网络，它使用编程原语，但不是单个硬编码的`for`循环和硬编码的连续空间内存，而是包含了一个大型的编程原语集合，模型可以自由地操作以扩展其处理功能，例如`if`分支，`while`语句，变量创建，用于长期记忆的磁盘存储，排序运算符，高级数据结构（如列表、图和哈希表）等等。这样一个网络可以表示的程序空间将比当前深度学习模型所能表示的更广泛，其中一些程序可以实现更优越的泛化能力。重要的是，这样的程序将不是端到端可微的，尽管特定的模块仍然可微，因此需要通过离散程序搜索和梯度下降的组合来生成。

我们将不再仅仅拥有硬编码的算法智能（手工软件）和学习的几何智能（深度学习）。相反，我们将拥有一个结合了提供推理和抽象能力的正式算法模块以及提供非正式直觉和模式识别能力的几何模块（图 19.15）。整个系统将几乎不需要人工参与进行学习。这应该会极大地扩展可以用机器学习解决的问题的范围——给定适当的训练数据，我们可以自动生成的程序空间。像 AlphaGo 这样的系统——甚至 RNNs——可以被视为这种混合算法-几何模型的史前祖先。

![图片](img/cb571dcdb0fc9974cabe50470bf1ab16.png)

图 19.15：一个既依赖几何原语（模式识别，直觉）又依赖算法原语（推理，搜索，记忆）的学习程序

#### 使用深度学习引导程序搜索

今天，程序综合面临一个主要障碍：它效率极低。为了夸张，典型的程序综合技术是通过在搜索空间中尝试每一个可能程序，直到找到一个与提供的规范相匹配的程序。随着程序规范复杂性的增加，或者用于编写程序的原始词汇的扩展，程序搜索过程会遇到所谓的*组合爆炸*：要考虑的可能程序集增长非常快，实际上，比仅仅指数级增长还要快。因此，今天，程序综合只能用来生成非常短小的程序。你不会很快为你的电脑生成一个新的操作系统。

为了前进，我们需要通过使其更接近人类编写软件的方式，使程序综合变得高效。当你打开你的编辑器来编写脚本时，你不会考虑每一个可能编写程序。你只会在心中考虑几种可能的方法：你可以利用你对问题的理解和以往的经验，大大减少要考虑的可能选项的空间。

深度学习可以帮助程序综合做到同样的事情：尽管我们希望生成的每个特定程序可能是一个基本上离散的对象，执行非插值数据操作，但到目前为止的证据表明，*所有有用程序的集合*可能看起来非常像一个连续流形。这意味着一个在数百万个成功的程序生成场景上训练过的深度学习模型可能会开始发展出关于搜索过程应该采取的*程序空间路径*的*直觉*——就像软件工程师可能立即对即将编写的脚本的总体架构以及他们应该用作通往目标途径的垫脚石的中间函数和类有直觉一样。

记住，人类的推理很大程度上是由以价值为中心的抽象所指导的——也就是说，通过模式识别和直觉。程序综合也应该如此。我预计，在接下来的 10 到 20 年里，通过学习启发式方法引导程序搜索的一般方法将越来越受到研究兴趣的关注。

### 模块化组件重组和终身学习

如果模型变得更加复杂，并且建立在更丰富的算法原语之上，那么这种增加的复杂性将需要任务之间更高的重用率，而不是每次遇到新任务或新数据集时从头开始训练新模型。许多数据集不包含足够的信息，使我们能够从头开始开发新的复杂模型，因此有必要使用先前遇到的数据集中的信息（就像你每次打开新书时不会从头开始学习英语一样——那是不可能的）。由于当前任务与先前遇到的任务之间有大量重叠，因此在每个新任务上从头开始训练模型也是低效的。

随着现代基础模型的发展，我们正逐渐接近一个世界，其中 AI 系统拥有大量的知识和技能，并将它们应用于任何他们遇到的情况。但是，LLMs 缺少一个关键成分：重组。LLMs 非常擅长检索和重新应用记忆中的函数，但它们还无法将这些函数即时重新组合成适应当前情况的新程序。实际上，正如 Dziri 等人最近的研究论文所调查的，它们完全无法执行函数组合。更重要的是，它们学习的函数类型既不够抽象也不够模块化，这使得它们一开始就不适合重组。记得我们指出 LLMs 在添加大整数时准确性较低吗？你可能不会想在如此脆弱的函数之上构建你的下一个代码库。

要解决*组合泛化*问题，我们需要重用像人类编程语言中找到的函数和类这样的强大*程序组件*。这些组件将专门为在新的环境中模块化重用而进化——与 LLMs 记忆的模式不同。我们的 AI 将在运行时重新组合它们，以合成适应当前任务的新的程序。关键的是，这样的可重用组件库将通过我们 AI 所有实例的累积经验来构建，并将永久对所有用户开放。我们的 AI 遇到的任何单个问题只需解决一次——使它们不断自我改进。

想象一下今天的软件开发过程：一旦工程师解决了特定问题（例如 Python 中的 HTTP 查询），他们就会将其打包成一个抽象的、可重用的库，任何人都可以在地球上访问。未来面临类似问题的工程师将能够搜索现有库，下载一个，并在自己的项目中使用它。以类似的方式，在未来，元学习系统将通过筛选全球高级可重用块库来组装新的程序。当系统发现自己为几个不同的任务开发类似的程序子例程时，它可以提出一个**抽象的**、可重用的子例程版本，并将其存储在全局库中（见图 19.16）。这些子例程可以是几何的（具有预训练表示的深度学习模块）或算法的（更接近当代软件工程师操作的库）。

![](img/5c408e5c057c76442835fd1ab119022e.png)

图 19.16：一个能够快速开发特定任务模型的元学习器，使用可重用的原语（既包括算法也包括几何），从而实现极端的泛化

### 长期愿景

简而言之，这是我对于人工智能的长期愿景：

+   模型将更像程序，并将具有超越我们目前工作的输入数据的连续几何变换的能力。这些程序将无庸置疑地更接近人类对其周围环境和自身的抽象心理模型，并且由于它们丰富的算法性质，它们将能够实现更强的泛化。

+   尤其是模型将融合提供形式推理、搜索和抽象能力的**算法模块**，以及提供非正式直觉和模式识别能力的**几何模块**。这将实现以价值为中心和以程序为中心的抽象的融合。AlphaGo 或自动驾驶汽车（需要大量手动软件工程和人工设计决策的系统）是这种符号和几何人工智能融合的早期例子。

+   这样的模型将通过自动**生长**而不是由人类工程师硬编码，使用存储在全局可重用子例程库中的模块化部分——这个库是通过在数千个先前任务和数据集上学习高性能模型而演化的。随着元学习系统识别出频繁的问题解决模式，它们将被转化为可重用的子例程——就像软件工程中的函数和类一样——并添加到全局库中。

+   搜索可能的子例程组合以增长新模型的流程将是一个离散搜索过程（程序综合），但它将受到由深度学习提供的**程序空间直觉**的强烈指导。

+   这个全球子程序库及其相关的模型增长系统将能够实现某种形式的人类似*极端泛化*：给定一个新的任务或情况，系统将能够使用非常少的数据组装一个新的适用于该任务的模型，这得益于丰富的程序化原语，这些原语具有很好的泛化能力，以及丰富的类似任务的经验。同样，如果人们有大量先前游戏的经验，他们可以快速学会玩复杂的全新电子游戏，因为从这种先前经验中得出的模型是抽象的、程序化的，而不是刺激和行动之间基本映射。

这个持续学习且模型不断增长的系统可以被解释为一种*通用人工智能*（AGI）。但不要期待随之而来的任何单一化机器人末日：那只是纯粹的幻想，源于对智能和技术的长期深刻误解。然而，这样的批评并不属于这本书的内容。

### 脚注

1.  参见玛丽安娜·涅祖里娜、卢西亚·奇波利娜-库恩、梅赫迪·查尔蒂和叶尼娅·吉特谢夫，《爱丽丝梦游仙境：简单任务显示最先进大型语言模型中完全推理崩溃》，arXiv，[`arxiv.org/abs/2406.02061`](https://arxiv.org/abs/2406.02061)。[[↩]](#footnote-link-1)

1.  特里·温格罗德，《作为计算机程序中理解自然语言数据表示的过程》，1971 年。[[↩]](#footnote-link-2)

1.  迈克尔·希克，《沃兹尼亚克：电脑能煮一杯咖啡吗？》快公司，2010 年 3 月。[[↩]](#footnote-link-3)

1.  弗朗索瓦·肖莱，《关于智能的度量》，2019 年。[[↩]](#footnote-link-4)

1.  Nouha Dziri 等人，《信仰与命运：Transformer 在组合性上的局限性》，第 37 届国际神经网络信息处理系统会议论文集（2023 年），[`arxiv.org/abs/2305.18654`](https://arxiv.org/abs/2305.18654)。[[↩]](#footnote-link-5)
