- en: Chapter 8\. Making Transformers Efficient in Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。使transformers在生产中更高效
- en: In the previous chapters, you’ve seen how transformers can be fine-tuned to
    produce great results on a wide range of tasks. However, in many situations accuracy
    (or whatever metric you’re optimizing for) is not enough; your state-of-the-art
    model is not very useful if it’s too slow or large to meet the business requirements
    of your application. An obvious alternative is to train a faster and more compact
    model, but the reduction in model capacity is often accompanied by a degradation
    in performance. So what can you do when you need a fast, compact, yet highly accurate
    model?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，您已经看到了transformers如何被微调以在各种任务上产生出色的结果。然而，在许多情况下，准确性（或者您正在优化的任何指标）是不够的；如果您的最先进模型太慢或太大，无法满足应用程序的业务需求，那么它就不是很有用。一个明显的替代方案是训练一个更快、更紧凑的模型，但模型容量的减少通常会伴随着性能的下降。那么当您需要一个快速、紧凑但高度准确的模型时，您该怎么办呢？
- en: 'In this chapter we will explore four complementary techniques that can be used
    to speed up the predictions and reduce the memory footprint of your transformer
    models: *knowledge distillation*, *quantization*, *pruning*, and *graph optimization*
    with the Open Neural Network Exchange (ONNX) format and ONNX Runtime (ORT). We’ll
    also see how some of these techniques can be combined to produce significant performance
    gains. For example, this was the approach taken by the Roblox engineering team
    in their article [“How We Scaled Bert to Serve 1+ Billion Daily Requests on CPUs”](https://oreil.ly/QdNIk),
    who as shown in [Figure 8-1](#roblox) found that combining knowledge distillation
    and quantization enabled them to improve the latency and throughput of their BERT
    classifier by over a factor of 30!'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨四种互补的技术，可以用来加速预测并减少您的transformer模型的内存占用：*知识蒸馏*、*量化*、*修剪*和使用Open Neural
    Network Exchange (ONNX)格式和ONNX Runtime (ORT)进行*图优化*。我们还将看到其中一些技术如何结合起来产生显著的性能提升。例如，这是Roblox工程团队在他们的文章[“我们如何在CPU上扩展BERT以处理10亿+日请求”](https://oreil.ly/QdNIk)中采取的方法，正如[图8-1](#roblox)所示，他们发现结合知识蒸馏和量化使他们的BERT分类器的延迟和吞吐量提高了30倍以上！
- en: '![Scaling BERT at Roblox](Images/nlpt_0801.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![在Roblox扩展BERT](Images/nlpt_0801.png)'
- en: Figure 8-1\. How Roblox scaled BERT with knowledge distillation, dynamic padding,
    and weight quantization (photo courtesy of Roblox employees Quoc N. Le and Kip
    Kaehler)
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-1\. Roblox如何通过知识蒸馏、动态填充和权重量化扩展BERT（照片由Roblox员工Quoc N. Le和Kip Kaehler提供）
- en: To illustrate the benefits and trade-offs associated with each technique, we’ll
    use intent detection as a case study; this is an important component of text-based
    assistants, where low latencies are critical for maintaining a conversation in
    real time. Along the way you’ll learn how to create custom trainers, perform efficient
    hyperparameter search, and gain a sense of what it takes to implement cutting-edge
    research with ![nlpt_pin01](Images/nlpt_pin01.png) Transformers. Let’s dive in!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明与每种技术相关的好处和权衡，我们将以意图检测为案例研究；这是基于文本的助手的重要组成部分，低延迟对于实时维持对话至关重要。在学习的过程中，您将学习如何创建自定义训练器，执行高效的超参数搜索，并了解实施最前沿研究所需的内容，使用![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers。让我们开始吧！
- en: Intent Detection as a Case Study
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以意图检测为案例研究
- en: 'Let’s suppose that we’re trying to build a text-based assistant for our company’s
    call center so that customers can request their account balance or make bookings
    without needing to speak with a human agent. In order to understand the goals
    of a customer, our assistant will need to be able to classify a wide variety of
    natural language text into a set of predefined actions or *intents*. For example,
    a customer might send a message like the following about an upcoming trip:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在尝试为公司的呼叫中心构建一个基于文本的助手，以便客户可以在不需要与人类代理交谈的情况下请求其账户余额或进行预订。为了理解客户的目标，我们的助手需要能够将各种自然语言文本分类为一组预定义的动作或*意图*。例如，客户可能会发送以下关于即将到来的旅行的消息：
- en: Hey, I’d like to rent a vehicle from Nov 1st to Nov 15th in Paris and I need
    a 15 passenger van
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 嘿，我想在11月1日到11月15日在巴黎租一辆车，我需要一辆15座位的面包车。
- en: and our intent classifier could automatically categorize this as a *Car Rental*
    intent, which then triggers an action and response. To be robust in a production
    environment, our classifier will also need to be able to handle *out-of-scope*
    queries, where a customer makes a query that doesn’t belong to any of the predefined
    intents and the system should yield a fallback response. For example, in the second
    case shown in [Figure 8-2](#oos), a customer asks a question about sports (which
    is out of scope), and the text assistant mistakenly classifies it as one of the
    known in-scope intents and returns the payday response. In the third case, the
    text assistant has been trained to detect out-of-scope queries (usually labeled
    as a separate class) and informs the customer about which topics it can answer
    questions about.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的意图分类器可以自动将此分类为*租车*意图，然后触发一个动作和响应。为了在生产环境中具有鲁棒性，我们的分类器还需要能够处理*超出范围*的查询，即客户提出不属于任何预定义意图的查询，系统应该产生一个回退响应。例如，在[图8-2](#oos)中显示的第二种情况中，客户询问有关体育的问题（超出范围），文本助手错误地将其分类为已知的范围内意图之一，并返回发薪日的响应。在第三种情况下，文本助手已经被训练来检测超出范围的查询（通常标记为一个单独的类），并告知客户它可以回答关于哪些主题的问题。
- en: '![Out of Scope Query](Images/nlpt_0802.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![超出范围的查询](Images/nlpt_0802.png)'
- en: Figure 8-2\. Three exchanges between a human (right) and a text-based assistant
    (left) for personal finance (courtesy of Stefan Larson et al.)
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2\. 人类（右）和基于文本的助手（左）之间的三次交流，涉及个人理财（由Stefan Larson等人提供）
- en: As a baseline, we’ve fine-tuned a BERT-base model that achieves around 94% accuracy
    on the CLINC150 dataset.^([1](ch08.xhtml#idm46238709482512)) This dataset includes
    22,500 in-scope queries across 150 intents and 10 domains like banking and travel,
    and also includes 1,200 out-of-scope queries that belong to an `oos` intent class.
    In practice we would also gather our own in-house dataset, but using public data
    is a great way to iterate quickly and generate preliminary results.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基准，我们微调了一个BERT-base模型，在CLINC150数据集上达到了约94%的准确性。这个数据集包括150个意图和10个领域（如银行和旅行）中的22,500个范围内查询，还包括属于`oos`意图类别的1,200个范围外查询。在实践中，我们还会收集自己的内部数据集，但使用公共数据是快速迭代和生成初步结果的好方法。
- en: 'To get started, let’s download our fine-tuned model from the Hugging Face Hub
    and wrap it in a pipeline for text classification:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从Hugging Face Hub下载我们微调的模型，并将其包装成文本分类的管道：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that we have a pipeline, we can pass a query to get the predicted intent
    and confidence score from the model:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个管道，我们可以传递一个查询以从模型获取预测的意图和置信度分数：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Great, the `car_rental` intent makes sense. Let’s now look at creating a benchmark
    that we can use to evaluate the performance of our baseline model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，`car_rental`意图是有意义的。现在让我们看看创建一个基准，我们可以用来评估我们基准模型的性能。
- en: Creating a Performance Benchmark
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建性能基准
- en: Like other machine learning models, deploying transformers in production environments
    involves a trade-off among several constraints, the most common being:^([2](ch08.xhtml#idm46238709422384))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他机器学习模型一样，在生产环境中部署transformers涉及在几个约束条件之间进行权衡，最常见的是：
- en: '*Model performance*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型性能*'
- en: How well does our model perform on a well-crafted test set that reflects production
    data? This is especially important when the cost of making errors is large (and
    best mitigated with a human in the loop), or when we need to run inference on
    millions of examples and small improvements to the model metrics can translate
    into large gains in aggregate.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在反映生产数据的精心设计的测试集上表现如何？当错误的成本很高时（最好通过人为干预来减轻），或者当我们需要对数百万个示例进行推断，并且模型指标的小幅改进可以转化为大幅增益时，这一点尤为重要。
- en: '*Latency*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*延迟*'
- en: How fast can our model deliver predictions? We usually care about latency in
    real-time environments that deal with a lot of traffic, like how Stack Overflow
    needed a classifier to quickly [detect unwelcome comments on the website](https://oreil.ly/cf7QX).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型能够多快地提供预测？我们通常关心实时环境中的延迟，这些环境处理大量流量，就像Stack Overflow需要一个分类器来快速[检测网站上不受欢迎的评论](https://oreil.ly/cf7QX)一样。
- en: '*Memory*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*内存*'
- en: How can we deploy billion-parameter models like GPT-2 or T5 that require gigabytes
    of disk storage and RAM? Memory plays an especially important role in mobile or
    edge devices, where a model has to generate predictions without access to a powerful
    cloud server.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何部署像GPT-2或T5这样需要占用几GB磁盘存储和内存的百亿参数模型？内存在移动设备或边缘设备中扮演着特别重要的角色，因为模型必须在没有强大的云服务器的情况下生成预测。
- en: 'Failing to address these constraints can have a negative impact on the user
    experience of your application. More commonly, it can lead to ballooning costs
    from running expensive cloud servers that may only need to handle a few requests.
    To explore how each of these constraints can be optimized with various compression
    techniques, let’s begin by creating a simple benchmark that measures each quantity
    for a given pipeline and test set. A skeleton of what we’ll need is given by the
    following class:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 未能解决这些约束条件可能会对应用程序的用户体验产生负面影响。更常见的是，可能会导致运行昂贵的云服务器的成本激增，而这些服务器可能只需要处理少量请求。为了探索如何使用各种压缩技术优化这些约束条件，让我们从创建一个简单的基准开始，该基准可以测量给定管道和测试集的每个数量：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We’ve defined an `optim_type` parameter to keep track of the different optimization
    techniques that we’ll cover in this chapter. We’ll use the `run_benchmark()` method
    to collect all the metrics in a dictionary, with keys given by `optim_type`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个`optim_type`参数，以跟踪我们在本章中将涵盖的不同优化技术。我们将使用`run_benchmark()`方法将所有指标收集到一个字典中，键由`optim_type`给出。
- en: 'Let’s now put some flesh on the bones of this class by computing the model
    accuracy on the test set. First we need some data to test on, so let’s download
    the CLINC150 dataset that was used to fine-tune our baseline model. We can get
    the dataset from the Hub with ![nlpt_pin01](Images/nlpt_pin01.png) Datasets as
    follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在通过在测试集上计算模型的准确性来为这个类添加一些具体内容。首先，我们需要一些数据进行测试，所以让我们下载用于微调基准模型的CLINC150数据集。我们可以通过以下方式从Hub获取数据集：![nlpt_pin01](Images/nlpt_pin01.png)。
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, the `plus` configuration refers to the subset that contains the out-of-scope
    training examples. Each example in the CLINC150 dataset consists of a query in
    the `text` column and its corresponding intent. We’ll use the test set to benchmark
    our models, so let’s take a look at one of the dataset’s examples:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`plus`配置是指包含超出范围的训练示例的子集。CLINC150数据集中的每个示例都包括`text`列中的查询及其对应的意图。我们将使用测试集来对我们的模型进行基准测试，所以让我们看一下数据集的一个示例：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The intents are provided as IDs, but we can easily get the mapping to strings
    (and vice versa) by accessing the `features` attribute of the dataset:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 意图以ID的形式提供，但我们可以通过访问数据集的`features`属性轻松获取到字符串的映射（反之亦然）：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that we have a basic understanding of the contents in the CLINC150 dataset,
    let’s implement the `compute_accuracy()` method of `PerformanceBenchmark`. Since
    the dataset is balanced across the intent classes, we’ll use accuracy as our metric.
    We can load this metric with ![nlpt_pin01](Images/nlpt_pin01.png) Datasets as
    follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对CLINC150数据集的内容有了基本的了解，让我们实现`PerformanceBenchmark`的`compute_accuracy()`方法。由于数据集在意图类别上是平衡的，我们将使用准确性作为我们的度量标准。我们可以通过以下方式使用![nlpt_pin01](Images/nlpt_pin01.png)数据集加载这个度量标准：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The accuracy metric expects the predictions and references (i.e., the ground
    truth labels) to be integers. We can use the pipeline to extract the predictions
    from the `text` field and then use the `str2int()` method of our `intents` object
    to map each prediction to its corresponding ID. The following code collects all
    the predictions and labels in lists before returning the accuracy on the dataset.
    Let’s also add it to our `Perform​an⁠ce​Benchmark` class:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度指标期望预测和参考（即，真实标签）是整数。我们可以使用管道从“text”字段中提取预测，然后使用我们的“intents”对象的“str2int（）”方法将每个预测映射到其相应的ID。以下代码在返回数据集的准确度之前收集所有的预测和标签。让我们也将其添加到我们的“PerformanceBenchmark”类中：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, let’s compute the size of our model by using the `torch.save()` function
    from PyTorch to serialize the model to disk. Under the hood, `torch.save()` uses
    Python’s `pickle` module and can be used to save anything from models to tensors
    to ordinary Python objects. In PyTorch, the recommended way to save a model is
    by using its `state_dict`, which is a Python dictionary that maps each layer in
    a model to its learnable parameters (i.e., weights and biases). Let’s see what
    is stored in the `state_dict` of our baseline model:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用PyTorch的“torch.save（）”函数来计算我们模型的大小，将模型序列化到磁盘上。在内部，“torch.save（）”使用Python的“pickle”模块，可以用来保存从模型到张量到普通Python对象的任何东西。在PyTorch中，保存模型的推荐方式是使用它的“state_dict”，这是一个Python字典，将模型中的每一层映射到它的可学习参数（即，权重和偏置）。让我们看看我们基准模型的“state_dict”中存储了什么：
- en: '[PRE11]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can clearly see that each key/value pair corresponds to a specific layer
    and tensor in BERT. So if we save our model with:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到每个键/值对对应于BERT中的特定层和张量。因此，如果我们用以下方式保存我们的模型：
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'we can then use the `Path.stat()` function from Python’s `pathlib` module to
    get information about the underlying files. In particular, `Path(​"model.​pt").​stat().​st_size`
    will give us the model size in bytes. Let’s put this all together in the `com⁠pute_​size()`
    function and add it to `PerformanceBenchmark`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Python的“pathlib”模块中的“Path.stat（）”函数来获取有关底层文件的信息。特别是，“Path（"model.​pt"）.​stat（）.​st_size”将给出模型的大小（以字节为单位）。让我们将所有这些放在“compute_​size（）”函数中，并将其添加到“PerformanceBenchmark”中：
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Finally let’s implement the `time_pipeline()` function so that we can time the
    average latency per query. For this application, latency refers to the time it
    takes to feed a text query to the pipeline and return the predicted intent from
    the model. Under the hood the pipeline also tokenizes the text, but this is around
    one thousand times faster than generating the predictions and thus adds a negligible
    contribution to the overall latency. A simple way to measure the execution time
    of a code snippet is to use the `perf_counter()` function from Python’s `time`
    module. This function has a better time resolution than the `time.time()` function
    and is well suited for getting precise results.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们实现“time_pipeline（）”函数，以便我们可以计算每个查询的平均延迟时间。对于这个应用程序，延迟时间指的是将文本查询输入到管道中并从模型返回预测意图所需的时间。在内部，管道还会对文本进行标记化，但这比生成预测快了大约一千倍，因此对整体延迟时间的贡献可以忽略不计。衡量代码片段的执行时间的一个简单方法是使用Python的“time”模块中的“perf_counter（）”函数。这个函数比“time.time（）”函数具有更好的时间分辨率，非常适合获取精确的结果。
- en: 'We can use `perf_counter()` to time our pipeline by passing our test query
    and calculating the time difference in milliseconds between the start and end:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用“perf_counter（）”通过传递我们的测试查询来计时我们的管道，并计算开始和结束之间的毫秒时间差：
- en: '[PRE15]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'These results exhibit quite some spread in the latencies and suggest that timing
    a single pass through the pipeline can give wildly different results each time
    we run the code. So instead, we’ll collect the latencies over many runs and then
    use the resulting distribution to calculate the mean and standard deviation, which
    will give us an idea about the spread in values. The following code does what
    we need and includes a phase to warm up the CPU before performing the actual timed
    run:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果展示了延迟时间的相当大的差异，并且表明通过管道的单次计时可能每次运行代码时都会得到完全不同的结果。因此，我们将收集多次运行的延迟时间，然后使用得到的分布来计算均值和标准差，这将让我们对数值的差异有一个概念。以下代码实现了我们需要的功能，并包括了在执行实际计时运行之前预热CPU的阶段：
- en: '[PRE17]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To keeps things simple, we’ll use the same `query` value to benchmark all our
    models. In general, the latency will depend on the query length, and a good practice
    is to benchmark your models with queries that they’re likely to encounter in production
    environments.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，我们将使用相同的“query”值来对我们所有的模型进行基准测试。一般来说，延迟时间将取决于查询长度，一个好的做法是使用模型可能在生产环境中遇到的查询来对模型进行基准测试。
- en: 'Now that our `PerformanceBenchmark` class is complete, let’s give it a spin!
    Let’s start by benchmarking our BERT baseline. For the baseline model, we just
    need to pass the pipeline and the dataset we wish to perform the benchmark on.
    We’ll collect the results in the `perf_metrics` dictionary to keep track of each
    model’s performance:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的“PerformanceBenchmark”类已经完成，让我们来试一试吧！让我们从对我们的BERT基准模型进行基准测试开始。对于基准模型，我们只需要传递管道和我们希望进行基准测试的数据集。我们将在“perf_metrics”字典中收集结果，以跟踪每个模型的性能：
- en: '[PRE18]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that we have a reference point, let’s look at our first compression technique:
    knowledge distillation.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个参考点，让我们来看看我们的第一个压缩技术：知识蒸馏。
- en: Note
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The average latency values will differ depending on what type of hardware you
    are running on. For example, you can usually get better performance by running
    inference on a GPU since it enables batch processing. For the purposes of this
    chapter, what’s important is the relative difference in latencies between models.
    Once we have determined the best-performing model, we can then explore different
    backends to reduce the absolute latency if needed.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 平均延迟值将取决于您所运行的硬件类型。例如，通常可以通过在GPU上运行推断来获得更好的性能，因为它可以实现批处理。对于本章的目的，重要的是模型之间延迟时间的相对差异。一旦确定了性能最佳的模型，我们可以探索不同的后端来减少绝对延迟时间（如果需要）。
- en: Making Models Smaller via Knowledge Distillation
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过知识蒸馏使模型变得更小
- en: Knowledge distillation is a general-purpose method for training a smaller *student*
    model to mimic the behavior of a slower, larger, but better-performing *teacher*.
    Originally introduced in 2006 in the context of ensemble models,^([3](ch08.xhtml#idm46238708497152))
    it was later popularized in a famous 2015 paper that generalized the method to
    deep neural networks and applied it to image classification and automatic speech
    recognition.^([4](ch08.xhtml#idm46238708494384))
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏是一种通用方法，用于训练一个较小的“学生”模型来模仿速度较慢、更大但性能更好的“教师”模型的行为。最初是在2006年在集成模型的背景下引入的，后来在一篇著名的2015年论文中将该方法推广到深度神经网络，并将其应用于图像分类和自动语音识别。
- en: Given the trend toward pretraining language models with ever-increasing parameter
    counts (the largest at the time of writing having over one trillion parameters),^([5](ch08.xhtml#idm46238708466160))
    knowledge distillation has also become a popular strategy to compress these huge
    models and make them more suitable for building practical applications.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于预训练语言模型参数数量不断增加的趋势（撰写时最大的模型参数超过一万亿），知识蒸馏也成为压缩这些庞大模型并使其更适合构建实际应用的流行策略。
- en: Knowledge Distillation for Fine-Tuning
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调的知识蒸馏
- en: So how is knowledge actually “distilled” or transferred from the teacher to
    the student during training? For supervised tasks like fine-tuning, the main idea
    is to augment the ground truth labels with a distribution of “soft probabilities”
    from the teacher which provide complementary information for the student to learn
    from. For example, if our BERT-base classifier assigns high probabilities to multiple
    intents, then this could be a sign that these intents lie close to each other
    in the feature space. By training the student to mimic these probabilities, the
    goal is to distill some of this “dark knowledge”^([6](ch08.xhtml#idm46238708457120))
    that the teacher has learned—that is, knowledge that is not available from the
    labels alone.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 那么在训练过程中，知识实际上是如何从教师传递给学生的呢？对于微调等监督任务，主要思想是用教师的“软概率”分布来增强地面真实标签，为学生提供补充信息。例如，如果我们的BERT-base分类器为多个意图分配高概率，那么这可能表明这些意图在特征空间中相互靠近。通过训练学生模仿这些概率，目标是蒸馏教师学到的一些“暗知识”——也就是，仅从标签中无法获得的知识。
- en: 'Mathematically, the way this works is as follows. Suppose we feed an input
    sequence *x* to the teacher to generate a vector of logits <math alttext="bold
    z left-parenthesis x right-parenthesis"><mrow><mi>𝐳</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></math> = [ <math alttext="z 1 left-parenthesis x right-parenthesis
    comma ellipsis comma z Subscript upper N Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>z</mi>
    <mn>1</mn></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>z</mi> <mi>N</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    ]. We can convert these logits into probabilities by applying a softmax function:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，这是如何工作的。假设我们将输入序列*x*提供给教师，以生成一个对数向量<math alttext="bold z left-parenthesis
    x right-parenthesis"><mrow><mi>𝐳</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    = [ <math alttext="z 1 left-parenthesis x right-parenthesis comma ellipsis comma
    z Subscript upper N Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>z</mi>
    <mn>1</mn></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>z</mi> <mi>N</mi></sub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    ]。我们可以通过应用softmax函数将这些对数转换为概率：
- en: <math alttext="StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis
    x right-parenthesis right-parenthesis Over sigma-summation Underscript j Endscripts
    exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    right-parenthesis EndFraction" display="block"><mrow><mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow>
    <mrow><msub><mo>∑</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis
    x right-parenthesis right-parenthesis Over sigma-summation Underscript j Endscripts
    exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    right-parenthesis EndFraction" display="block"><mrow><mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow>
    <mrow><msub><mo>∑</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: This isn’t quite what we want, though, because in many cases the teacher will
    assign a high probability to one class, with all other class probabilities close
    to zero. When that happens, the teacher doesn’t provide much additional information
    beyond the ground truth labels, so instead we “soften” the probabilities by scaling
    the logits with a temperature hyperparameter *T* before applying the softmax:^([7](ch08.xhtml#idm46238708428448))
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是我们想要的，因为在许多情况下，教师会为一个类分配高概率，而其他类的概率接近于零。当发生这种情况时，教师除了地面真实标签外并没有提供太多额外信息，因此我们会在应用softmax之前，通过一个温度超参数*T*来缩放对数，从而“软化”概率。
- en: <math alttext="p Subscript i Baseline left-parenthesis x right-parenthesis equals
    StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    slash upper T right-parenthesis Over sigma-summation Underscript j Endscripts
    exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    slash upper T right-parenthesis EndFraction" display="block"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mo
    form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi> <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow>
    <mrow><msub><mo>∑</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="p Subscript i Baseline left-parenthesis x right-parenthesis equals
    StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    slash upper T right-parenthesis Over sigma-summation Underscript j Endscripts
    exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    slash upper T right-parenthesis EndFraction" display="block"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mo
    form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi> <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow>
    <mrow><msub><mo>∑</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: As shown in [Figure 8-3](#soft-probs), higher values of *T* produce a softer
    probability distribution over the classes and reveal much more information about
    the decision boundary that the teacher has learned for each training example.
    When <math alttext="upper T equals 1"><mrow><mi>T</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    we recover the original softmax distribution.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图8-3](#soft-probs)所示，*T*的值越高，类别上的软化概率分布就越软，可以更多地揭示老师对每个训练示例学习的决策边界。当<math
    alttext="upper T equals 1"><mrow><mi>T</mi> <mo>=</mo> <mn>1</mn></mrow></math>时，我们恢复了原始的softmax分布。
- en: '![Soft Probabilities](Images/nlpt_0803.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![软概率](Images/nlpt_0803.png)'
- en: Figure 8-3\. Comparison of a hard label that is one-hot encoded (left), softmax
    probabilities (middle), and softened class probabilities (right)
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3。一个使用one-hot编码的硬标签（左）、softmax概率（中）和软化类别概率（右）的比较。
- en: 'Since the student also produces softened probabilities <math alttext="q Subscript
    i Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>i</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> of its own, we can
    use the [Kullback–Leibler (KL)](https://oreil.ly/8nKQG) divergence to measure
    the difference between the two probability distributions:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于学生还产生了自己的软化概率<math alttext="q Subscript i Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>，我们可以使用[Kullback-Leibler（KL）](https://oreil.ly/8nKQG)散度来衡量两个概率分布之间的差异：
- en: <math alttext="upper D Subscript upper K upper L Baseline left-parenthesis p
    comma q right-parenthesis equals sigma-summation Underscript i Endscripts p Subscript
    i Baseline left-parenthesis x right-parenthesis log StartFraction p Subscript
    i Baseline left-parenthesis x right-parenthesis Over q Subscript i Baseline left-parenthesis
    x right-parenthesis EndFraction" display="block"><mrow><msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo> <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munder><mo>∑</mo> <mi>i</mi></munder> <msub><mi>p</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mfrac><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>q</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper D Subscript upper K upper L Baseline left-parenthesis p
    comma q right-parenthesis equals sigma-summation Underscript i Endscripts p Subscript
    i Baseline left-parenthesis x right-parenthesis log StartFraction p Subscript
    i Baseline left-parenthesis x right-parenthesis Over q Subscript i Baseline left-parenthesis
    x right-parenthesis EndFraction" display="block"><mrow><msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo> <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munder><mo>∑</mo> <mi>i</mi></munder> <msub><mi>p</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mfrac><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>q</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: 'With the KL divergence we can calculate how much is lost when we approximate
    the probability distribution of the teacher with the student. This allows us to
    define a knowledge distillation loss:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过KL散度，我们可以计算当我们用学生来近似老师的概率分布时损失了多少。这使我们能够定义知识蒸馏损失：
- en: <math alttext="upper L Subscript upper K upper D Baseline equals upper T squared
    upper D Subscript upper K upper L" display="block"><mrow><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub>
    <mo>=</mo> <msup><mi>T</mi> <mn>2</mn></msup> <msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub></mrow></math>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L Subscript upper K upper D Baseline equals upper T squared
    upper D Subscript upper K upper L" display="block"><mrow><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub>
    <mo>=</mo> <msup><mi>T</mi> <mn>2</mn></msup> <msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub></mrow></math>
- en: 'where <math alttext="upper T squared"><msup><mi>T</mi> <mn>2</mn></msup></math>
    is a normalization factor to account for the fact that the magnitude of the gradients
    produced by soft labels scales as <math alttext="1 slash upper T squared"><mrow><mn>1</mn>
    <mo>/</mo> <msup><mi>T</mi> <mn>2</mn></msup></mrow></math> . For classification
    tasks, the student loss is then a weighted average of the distillation loss with
    the usual cross-entropy loss <math alttext="upper L Subscript upper C upper E"><msub><mi>L</mi>
    <mrow><mi>C</mi><mi>E</mi></mrow></msub></math> of the ground truth labels:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="upper T squared"><msup><mi>T</mi> <mn>2</mn></msup></math>是一个归一化因子，用于考虑软标签产生的梯度大小按<math
    alttext="1 slash upper T squared"><mrow><mn>1</mn> <mo>/</mo> <msup><mi>T</mi>
    <mn>2</mn></msup></mrow></math>缩放的事实。对于分类任务，学生的损失是蒸馏损失和地面真实标签的交叉熵损失<math alttext="upper
    L Subscript upper C upper E"><msub><mi>L</mi> <mrow><mi>C</mi><mi>E</mi></mrow></msub></math>的加权平均：
- en: <math alttext="upper L Subscript normal s normal t normal u normal d normal
    e normal n normal t Baseline equals alpha upper L Subscript upper C upper E Baseline
    plus left-parenthesis 1 minus alpha right-parenthesis upper L Subscript upper
    K upper D" display="block"><mrow><msub><mi>L</mi> <mi>student</mi></msub> <mo>=</mo>
    <mi>α</mi> <msub><mi>L</mi> <mrow><mi>C</mi><mi>E</mi></mrow></msub> <mo>+</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>α</mi> <mo>)</mo></mrow> <msub><mi>L</mi>
    <mrow><mi>K</mi><mi>D</mi></mrow></msub></mrow></math>
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L Subscript normal s normal t normal u normal d normal
    e normal n normal t Baseline equals alpha upper L Subscript upper C upper E Baseline
    plus left-parenthesis 1 minus alpha right-parenthesis upper L Subscript upper
    K upper D" display="block"><mrow><msub><mi>L</mi> <mi>student</mi></msub> <mo>=</mo>
    <mi>α</mi> <msub><mi>L</mi> <mrow><mi>C</mi><mi>E</mi></mrow></msub> <mo>+</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>α</mi> <mo>)</mo></mrow> <msub><mi>L</mi>
    <mrow><mi>K</mi><mi>D</mi></mrow></msub></mrow></math>
- en: where <math alttext="alpha"><mi>α</mi></math> is a hyperparameter that controls
    the relative strength of each loss. A diagram of the whole process is shown in
    [Figure 8-4](#kd); the temperature is set to 1 at inference time to recover the
    standard softmax probabilities.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="alpha"><mi>α</mi></math>是一个控制每个损失相对强度的超参数。整个过程的图表如[图8-4](#kd)所示；在推断时，温度被设置为1，以恢复标准的softmax概率。
- en: '![Knowledge distillation](Images/nlpt_0804.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![知识蒸馏](Images/nlpt_0804.png)'
- en: Figure 8-4\. The knowledge distillation process
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4。知识蒸馏过程
- en: Knowledge Distillation for Pretraining
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练的知识蒸馏
- en: 'Knowledge distillation can also be used during pretraining to create a general-purpose
    student that can be subsequently fine-tuned on downstream tasks. In this case,
    the teacher is a pretrained language model like BERT, which transfers its knowledge
    about masked language modeling to the student. For example, in the DistilBERT
    paper,^([8](ch08.xhtml#idm46238708343392)) the masked language modeling loss <math
    alttext="upper L Subscript m l m"><msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub></math>
    is augmented with a term from knowledge distillation and a cosine embedding loss
    <math alttext="upper L Subscript c o s Baseline equals 1 minus cosine left-parenthesis
    h Subscript s Baseline comma h Subscript t Baseline right-parenthesis"><mrow><msub><mi>L</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub> <mo>=</mo> <mn>1</mn> <mo>-</mo>
    <mo form="prefix">cos</mo> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>s</mi></msub>
    <mo>,</mo> <msub><mi>h</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
    to align the directions of the hidden state vectors between the teacher and student:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏也可以在预训练期间使用，以创建一个通用的学生模型，随后可以在下游任务上进行精细调整。在这种情况下，教师是一个预训练的语言模型，如BERT，它将其关于掩码语言建模的知识转移到学生身上。例如，在DistilBERT论文中，^([8](ch08.xhtml#idm46238708343392))掩码语言建模损失<math
    alttext="upper L Subscript m l m"><msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub></math>被知识蒸馏的一个项和余弦嵌入损失<math
    alttext="upper L Subscript c o s Baseline equals 1 minus cosine left-parenthesis
    h Subscript s Baseline comma h Subscript t Baseline right-parenthesis"><mrow><msub><mi>L</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub> <mo>=</mo> <mn>1</mn> <mo>-</mo>
    <mo form="prefix">cos</mo> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>s</mi></msub>
    <mo>,</mo> <msub><mi>h</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>来对齐教师和学生之间的隐藏状态向量的方向：
- en: <math alttext="upper L Subscript normal upper D normal i normal s normal t normal
    i normal l normal upper B normal upper E normal upper R normal upper T Baseline
    equals alpha upper L Subscript m l m Baseline plus beta upper L Subscript upper
    K upper D Baseline plus gamma upper L Subscript c o s" display="block"><mrow><msub><mi>L</mi>
    <mi>DistilBERT</mi></msub> <mo>=</mo> <mi>α</mi> <msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub>
    <mo>+</mo> <mi>β</mi> <msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub>
    <mo>+</mo> <mi>γ</mi> <msub><mi>L</mi> <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L Subscript normal upper D normal i normal s normal t normal
    i normal l normal upper B normal upper E normal upper R normal upper T Baseline
    equals alpha upper L Subscript m l m Baseline plus beta upper L Subscript upper
    K upper D Baseline plus gamma upper L Subscript c o s" display="block"><mrow><msub><mi>L</mi>
    <mi>DistilBERT</mi></msub> <mo>=</mo> <mi>α</mi> <msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub>
    <mo>+</mo> <mi>β</mi> <msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub>
    <mo>+</mo> <mi>γ</mi> <msub><mi>L</mi> <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>
- en: Since we already have a fine-tuned BERT-base model, let’s see how we can use
    knowledge distillation to fine-tune a smaller and faster model. To do that we’ll
    need a way to augment the cross-entropy loss with an <math alttext="upper L Subscript
    upper K upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math>
    term. Fortunately we can do this by creating our own trainer!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经有了一个经过精细调整的BERT-base模型，让我们看看如何使用知识蒸馏来对一个更小更快的模型进行精细调整。为了做到这一点，我们需要一种方法来将交叉熵损失与<math
    alttext="upper L Subscript upper K upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math>项相结合。幸运的是，我们可以通过创建自己的训练器来实现这一点！
- en: Creating a Knowledge Distillation Trainer
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建知识蒸馏训练器
- en: 'To implement knowledge distillation we need to add a few things to the `Trainer`
    base class:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现知识蒸馏，我们需要向`Trainer`基类添加一些内容：
- en: The new hyperparameters <math alttext="alpha"><mi>α</mi></math> and *T*, which
    control the relative weight of the distillation loss and how much the probability
    distribution of the labels should be smoothed
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新的超参数<math alttext="alpha"><mi>α</mi></math>和*T*，它们控制蒸馏损失的相对权重以及标签的概率分布应该被平滑的程度
- en: The fine-tuned teacher model, which in our case is BERT-base
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过精细调整的教师模型，我们的情况下是BERT-base
- en: A new loss function that combines the cross-entropy loss with the knowledge
    distillation loss
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合交叉熵损失和知识蒸馏损失的新损失函数
- en: 'Adding the new hyperparameters is quite simple, since we just need to subclass
    `TrainingArguments` and include them as new attributes:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 添加新的超参数非常简单，因为我们只需要对`TrainingArguments`进行子类化，并将它们包含为新的属性：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For the trainer itself, we need a new loss function. The way to implement this
    is by subclassing `Trainer` and overriding the `compute_loss()` method to include
    the knowledge distillation loss term <math alttext="upper L Subscript upper K
    upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math> :'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练器本身，我们需要一个新的损失函数。实现这一点的方法是通过对`Trainer`进行子类化，并覆盖`compute_loss()`方法，以包括知识蒸馏损失项<math
    alttext="upper L Subscript upper K upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math>：
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Let’s unpack this code a bit. When we instantiate `DistillationTrainer` we pass
    a `teacher_model` argument with a teacher that has already been fine-tuned on
    our task. Next, in the `compute_loss()` method we extract the logits from the
    student and teacher, scale them by the temperature, and then normalize them with
    a softmax before passing them to PyTorch’s `nn.KLDivLoss()` function for computing
    the KL divergence. One quirk with `nn.KLDivLoss()` is that it expects the inputs
    in the form of log probabilities and the labels as normal probabilities. That’s
    why we’ve used the `F.log_softmax()` function to normalize the student’s logits,
    while the teacher’s logits are converted to probabilities with a standard softmax.
    The `reduction=batchmean` argument in `nn.KLDivLoss()` specifies that we average
    the losses over the batch dimension.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解开一下这段代码。当我们实例化`DistillationTrainer`时，我们传递了一个已经在我们的任务上进行了微调的老师模型。接下来，在`compute_loss()`方法中，我们从学生和老师那里提取logits，通过温度对它们进行缩放，然后在传递给PyTorch的`nn.KLDivLoss()`函数之前，使用softmax对它们进行归一化以计算KL散度。`nn.KLDivLoss()`的一个怪癖是，它期望输入以对数概率的形式，标签以正常概率的形式。这就是为什么我们使用`F.log_softmax()`函数对学生的logits进行归一化，而老师的logits则使用标准softmax转换为概率。`nn.KLDivLoss()`中的`reduction=batchmean`参数指定我们在批维度上平均损失。
- en: Tip
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can also perform knowledge distillation with the Keras API of the ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers library. To do this, you’ll need to implement a custom `Distiller`
    class that overrides the `train_step()`, `test_step()`, and `compile()` methods
    of `tf.keras.Model()`. See the [Keras documentation](https://oreil.ly/6qp0F) for
    an example of how to do this.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用![nlpt_pin01](Images/nlpt_pin01.png) Transformers库的Keras API进行知识蒸馏。为此，您需要实现一个自定义的`Distiller`类，覆盖`tf.keras.Model()`的`train_step()`、`test_step()`和`compile()`方法。请参阅[Keras文档](https://oreil.ly/6qp0F)了解如何实现。
- en: Choosing a Good Student Initialization
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择一个好的学生初始化
- en: Now that we have our custom trainer, the first question you might have is which
    pretrained language model should we pick for the student? In general we should
    pick a smaller model for the student to reduce the latency and memory footprint.
    A good rule of thumb from the literature is that knowledge distillation works
    best when the teacher and student are of the same *model type*.^([9](ch08.xhtml#idm46238707933552))
    One possible reason for this is that different model types, say BERT and RoBERTa,
    can have different output embedding spaces, which hinders the ability of the student
    to mimic the teacher. In our case study the teacher is BERT, so DistilBERT is
    a natural candidate to initialize the student with since it has 40% fewer parameters
    and has been shown to achieve strong results on downstream tasks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了自定义的训练器，您可能会问的第一个问题是，我们应该为学生选择哪个预训练语言模型？一般来说，我们应该为学生选择一个较小的模型，以减少延迟和内存占用。从文献中得出的一个很好的经验法则是，当老师和学生是相同的*模型类型*时，知识蒸馏效果最好。^([9](ch08.xhtml#idm46238707933552))这样做的一个可能原因是，不同的模型类型，比如BERT和RoBERTa，可能具有不同的输出嵌入空间，这会妨碍学生模仿老师的能力。在我们的案例研究中，老师是BERT，因此DistilBERT是一个自然的候选，因为它的参数少了40%，并且已经在下游任务中取得了良好的结果。
- en: 'First we’ll need to tokenize and encode our queries, so let’s instantiate the
    tokenizer from DistilBERT and create a simple `tokenize_text()` function to take
    care of the preprocessing:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要对我们的查询进行标记化和编码，因此让我们实例化来自DistilBERT的标记器，并创建一个简单的`tokenize_text()`函数来处理预处理：
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here we’ve removed the `text` column since we no longer need it, and we’ve also
    renamed the `intent` column to `labels` so it can be automatically detected by
    the trainer.^([10](ch08.xhtml#idm46238707834608))
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经删除了`text`列，因为我们不再需要它，我们还将`intent`列重命名为`labels`，以便训练器可以自动检测到它。^([10](ch08.xhtml#idm46238707834608))
- en: 'Now that we’ve processed our texts, the next thing we need to do is define
    the hyperparameters and `compute_metrics()` function for our `DistillationTrainer`.
    We’ll also push all of our models to the Hugging Face Hub, so let’s start by logging
    in to our account:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经处理了我们的文本，接下来我们需要做的是为我们的`DistillationTrainer`定义超参数和`compute_metrics()`函数。我们还将把所有的模型推送到Hugging
    Face Hub，所以让我们首先登录到我们的账户：
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we’ll define the metrics to track during training. As we did in the performance
    benchmark, we’ll use accuracy as the main metric. This means we can reuse our
    `accuracy_score()` function in the `compute_metrics()` function that we’ll include
    in `DistillationTrainer`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义训练过程中要跟踪的指标。就像我们在性能基准测试中所做的那样，我们将使用准确性作为主要指标。这意味着我们可以在`compute_metrics()`函数中重用我们的`accuracy_score()`函数，这个函数将包含在`DistillationTrainer`中：
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this function, the predictions from the sequence modeling head come in the
    form of logits, so we use the `np.argmax()` function to find the most confident
    class prediction and compare that against the ground truth label.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，序列建模头部的预测以logits的形式出现，因此我们使用`np.argmax()`函数找到最有信心的类别预测，并将其与地面真相标签进行比较。
- en: 'Next we need to define the training arguments. To warm up, we’ll set <math
    alttext="alpha equals 1"><mrow><mi>α</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    to see how well DistilBERT performs without any signal from the teacher.^([11](ch08.xhtml#idm46238707732144))
    Then we will push our fine-tuned model to a new repository called `distilbert-base-uncased-finetuned-clinc`,
    so we just need to specify that in the `output_dir` argument of `DistillationTrainingArguments`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们需要定义训练参数。为了热身，我们将设置<math alttext="alpha equals 1"><mrow><mi>α</mi> <mo>=</mo>
    <mn>1</mn></mrow></math>，以查看DistilBERT在没有来自教师的任何信号的情况下的表现。^([11](ch08.xhtml#idm46238707732144))然后我们将我们的微调模型推送到一个名为`distilbert-base-uncased-finetuned-clinc`的新存储库，所以我们只需要在`DistillationTrainingArguments`的`output_dir`参数中指定它：
- en: '[PRE25]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We’ve also tweaked a few of the default hyperparameter values, like the number
    of epochs, the weight decay, and the learning rate. The next thing to do is initialize
    a student model. Since we will be doing multiple runs with the trainer, we’ll
    create a `student_init()` function to initialize the model with each new run.
    When we pass this function to the `DistillationTrainer`, this will ensure we initialize
    a new model each time we call the `train()` method.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还调整了一些默认超参数值，比如epochs的数量，权重衰减和学习率。接下来要做的是初始化一个学生模型。由于我们将使用训练器进行多次运行，我们将创建一个`student_init()`函数，以便在每次调用`train()`方法时初始化一个新模型。当我们将这个函数传递给`DistillationTrainer`时，这将确保我们每次调用`train()`方法时初始化一个新模型。
- en: 'One other thing we need to do is provide the student model with the mappings
    between each intent and label ID. These mappings can be obtained from our BERT-base
    model that we downloaded in the pipeline:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要做的另一件事是为学生模型提供每个意图和标签ID之间的映射。这些映射可以从我们在流水线中下载的BERT-base模型中获得：
- en: '[PRE26]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'With these mappings, we can now create a custom model configuration with the
    `AutoConfig` class hat we encountered in Chapters [3](ch03.xhtml#chapter_anatomy)
    and [4](ch04.xhtml#chapter_ner). Let’s use this to create a configuration for
    our student with the information about the label mappings:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些映射，我们现在可以使用`AutoConfig`类创建一个自定义模型配置，这是我们在第[3](ch03.xhtml#chapter_anatomy)章和第[4](ch04.xhtml#chapter_ner)章中遇到的。让我们使用这个为我们的学生创建一个包含标签映射信息的配置：
- en: '[PRE27]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here we’ve also specified the number of classes our model should expect. We
    can then provide this configuration to the `from_pretrained()` function of the
    `AutoModelForSequenceClassification` class as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们还指定了我们的模型应该期望的类的数量。然后我们可以将这个配置提供给`AutoModelForSequenceClassification`类的`from_pretrained()`函数，如下所示：
- en: '[PRE28]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We now have all the ingredients needed for our distillation trainer, so let’s
    load the teacher and fine-tune:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了我们的蒸馏训练器所需的所有要素，让我们加载教师并进行微调：
- en: '[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '| Epoch | Training Loss | Validation Loss | Accuracy |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: Epoch | Training Loss | Validation Loss | Accuracy
- en: '| --- | --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '--- | --- | --- | ---'
- en: '| 1 | 4.2923 | 3.289337 | 0.742258 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: 1 | 4.2923 | 3.289337 | 0.742258
- en: '| 2 | 2.6307 | 1.883680 | 0.828065 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: 2 | 2.6307 | 1.883680 | 0.828065
- en: '| 3 | 1.5483 | 1.158315 | 0.896774 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: 3 | 1.5483 | 1.158315 | 0.896774
- en: '| 4 | 1.0153 | 0.861815 | 0.909355 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: 4 | 1.0153 | 0.861815 | 0.909355
- en: '| 5 | 0.7958 | 0.777289 | 0.917419 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: 5 | 0.7958 | 0.777289 | 0.917419
- en: 'The 92% accuracy on the validation set looks quite good compared to the 94%
    that the BERT-base teacher achieves. Now that we’ve fine-tuned DistilBERT, let’s
    push the model to the Hub so we can reuse it later:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集上的92%准确率看起来相当不错，与BERT-base教师实现的94%相比。现在我们已经对DistilBERT进行了微调，让我们将模型推送到Hub，以便以后重用：
- en: '[PRE31]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'With our model now safely stored on the Hub, we can immediately use it in a
    pipeline for our performance benchmark:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的模型已经安全地存储在Hub上，我们可以立即在性能基准测试的流水线中使用它：
- en: '[PRE32]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can then pass this pipeline to our `PerformanceBenchmark` class to compute
    the metrics associated with this model:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以将这个流水线传递给我们的`PerformanceBenchmark`类，以计算与这个模型相关的指标：
- en: '[PRE33]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To compare these results against our baseline, let’s create a scatter plot
    of the accuracy against the latency, with the radius of each point corresponding
    to the size of the model on disk. The following function does what we need and
    marks the current optimization type as a dashed circle to aid the comparison to
    previous results:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这些结果与我们的基准进行比较，让我们创建一个散点图，显示准确性与延迟之间的关系，每个点的半径对应于磁盘上模型的大小。以下函数可以满足我们的需求，并将当前优化类型标记为虚线圆圈，以便与以前的结果进行比较：
- en: '[PRE35]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](Images/nlpt_08in01.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_08in01.png)'
- en: From the plot we can see that by using a smaller model we’ve managed to significantly
    decrease the average latency. And all this at the price of just over a 1% reduction
    in accuracy! Let’s see if we can close that last gap by including the distillation
    loss of the teacher and finding good values for <math alttext="alpha"><mi>α</mi></math>
    and *T*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中我们可以看到，通过使用一个更小的模型，我们成功地显著降低了平均延迟。而这一切只需牺牲了略微超过1%的准确性！让我们看看是否可以通过包括教师的蒸馏损失并找到<math
    alttext="alpha"><mi>α</mi></math>和*T*的良好值来缩小最后的差距。
- en: Finding Good Hyperparameters with Optuna
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Optuna找到良好的超参数
- en: 'To find good values for <math alttext="alpha"><mi>α</mi></math> and *T*, we
    could do a grid search over the 2D parameter space. But a much better alternative
    is to use *Optuna*,^([12](ch08.xhtml#idm46238706954496)) which is an optimization
    framework designed for just this type of task. Optuna formulates the search problem
    in terms of an objective function that is optimized through multiple *trials*.
    For example, suppose we wished to minimize Rosenbrock’s [“banana function”](https://oreil.ly/hPk8h):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到<math alttext="alpha"><mi>α</mi></math>和*T*的良好值，我们可以在2D参数空间上进行网格搜索。但一个更好的选择是使用*Optuna*，^([12](ch08.xhtml#idm46238706954496))这是一个专为这种任务设计的优化框架。Optuna通过多次*trials*优化目标函数来制定搜索问题。例如，假设我们希望最小化Rosenbrock的[“香蕉函数”](https://oreil.ly/hPk8h)：
- en: <math alttext="f left-parenthesis x comma y right-parenthesis equals left-parenthesis
    1 minus x right-parenthesis squared plus 100 left-parenthesis y minus x squared
    right-parenthesis squared" display="block"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>x</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>100</mn> <msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="f left-parenthesis x comma y right-parenthesis equals left-parenthesis
    1 minus x right-parenthesis squared plus 100 left-parenthesis y minus x squared
    right-parenthesis squared" display="block"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>x</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>100</mn> <msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: which is a famous test case for optimization frameworks. As shown in [Figure 8-5](#banana-function),
    the function gets its name from the curved contours and has a global minimum at
    <math alttext="left-parenthesis x comma y right-parenthesis equals left-parenthesis
    1 comma 1 right-parenthesis"><mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>)</mo> <mo>=</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math>
    . Finding the valley is an easy optimization problem, but converging to the global
    minimum is not.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个著名的优化框架的测试案例。如[图8-5](#banana-function)所示，该函数因其曲线轮廓而得名，并且在<math alttext="left-parenthesis
    x comma y right-parenthesis equals left-parenthesis 1 comma 1 right-parenthesis"><mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo> <mo>=</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo>
    <mn>1</mn> <mo>)</mo></mrow></math>处有一个全局最小值。找到这个谷是一个简单的优化问题，但收敛到全局最小值却不是。
- en: '![A banana plot](Images/nlpt_0805.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![香蕉图](Images/nlpt_0805.png)'
- en: Figure 8-5\. Plot of the Rosenbrock function of two variables
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5。两个变量的Rosenbrock函数的绘图
- en: 'In Optuna, we can find the minimum of <math alttext="f left-parenthesis x comma
    y right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>)</mo></mrow></math> by defining an `objective()` function that returns the
    value of <math alttext="f left-parenthesis x comma y right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></math> :'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在Optuna中，我们可以通过定义一个`objective()`函数来找到<math alttext="f left-parenthesis x comma
    y right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>)</mo></mrow></math>的最小值，该函数返回<math alttext="f left-parenthesis x comma y
    right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>)</mo></mrow></math>的值：
- en: '[PRE36]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `trial.suggest_float` object specifies the parameter ranges to sample uniformly
    from; Optuna also provides `suggest_int` and `suggest_categorical` for integer
    and categorical parameters, respectively. Optuna collects multiple trials as a
    *study*, so to create one we just pass the `objective()` function to `study.optimize()`
    as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`trial.suggest_float`对象指定要均匀采样的参数范围；Optuna还提供`suggest_int`和`suggest_categorical`用于整数和分类参数。Optuna将多个试验收集为一个*study*，因此我们只需将`objective()`函数传递给`study.optimize()`来创建一个如下：'
- en: '[PRE37]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Once the study is completed, we can then find the best parameters as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦研究完成，我们就可以按照以下方式找到最佳参数：
- en: '[PRE38]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We see that with one thousand trials, Optuna has managed to find values for
    *x* and *y* that are reasonably close to the global minimum. To use Optuna in
    ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, we use similar logic by first
    defining the hyperparameter space that we wish to optimize over. In addition to
    <math alttext="alpha"> <mi>α</mi> </math> and *T*, we’ll include the number of
    training epochs as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一千次试验，Optuna已经成功找到了* x *和* y *的值，这些值与全局最小值相当接近。要在![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers中使用Optuna，我们首先定义要优化的超参数空间。除了<math alttext="alpha"> <mi>α</mi> </math>和*T*之外，我们还将包括训练周期的数量如下：
- en: '[PRE40]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Running the hyperparameter search with the `Trainer` is then quite simple;
    we just need to specify the number of trials to run and a direction to optimize
    for. Because we want the best possible accuracy, we specify `direction="maximize"`
    in the `hyper​para⁠meter_​search()` method of the trainer and pass the hyperparameter
    search space as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Trainer`进行超参数搜索非常简单；我们只需要指定要运行的试验次数和要优化的方向。因为我们希望获得最佳准确度，所以在训练器的`hyper​para⁠meter_​search()`方法中指定`direction="maximize"`，并按如下方式传递超参数搜索空间：
- en: '[PRE41]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The `hyperparameter_search()` method returns a `BestRun` object, which contains
    the value of the objective that was maximized (by default, the sum of all metrics)
    and the hyperparameters it used for that run:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`hyperparameter_search()`方法返回一个`BestRun`对象，其中包含了被最大化的目标值（默认为所有指标的总和）和该运行所使用的超参数：'
- en: '[PRE42]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This value of <math alttext="alpha"><mi>α</mi></math> tells us that most of
    the training signal is coming from the knowledge distillation term. Let’s update
    our training arguments with these values and run the final training run:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个<math alttext="alpha"><mi>α</mi></math>的值告诉我们，大部分的训练信号来自知识蒸馏项。让我们使用这些值更新我们的训练参数，并运行最终的训练：
- en: '[PRE44]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '| Epoch | Training Loss | Validation Loss | Accuracy |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: Epoch | Training Loss | Validation Loss | Accuracy |
- en: '| --- | --- | --- | --- |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '--- | --- | --- | ---'
- en: '| 1 | 0.9031 | 0.574540 | 0.736452 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: 1 | 0.9031 | 0.574540 | 0.736452 |
- en: '| 2 | 0.4481 | 0.285621 | 0.874839 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: 2 | 0.4481 | 0.285621 | 0.874839
- en: '| 3 | 0.2528 | 0.179766 | 0.918710 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: 3 | 0.2528 | 0.179766 | 0.918710
- en: '| 4 | 0.1760 | 0.139828 | 0.929355 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: 4 | 0.1760 | 0.139828 | 0.929355
- en: '| 5 | 0.1416 | 0.121053 | 0.934839 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: 5 | 0.1416 | 0.121053 | 0.934839
- en: '| 6 | 0.1243 | 0.111640 | 0.934839 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: 6 | 0.1243 | 0.111640 | 0.934839
- en: '| 7 | 0.1133 | 0.106174 | 0.937742 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: 7 | 0.1133 | 0.106174 | 0.937742
- en: '| 8 | 0.1075 | 0.103526 | 0.938710 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: 8 | 0.1075 | 0.103526 | 0.938710
- en: '| 9 | 0.1039 | 0.101432 | 0.938065 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: 9 | 0.1039 | 0.101432 | 0.938065
- en: '| 10 | 0.1018 | 0.100493 | 0.939355 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: 10 | 0.1018 | 0.100493 | 0.939355
- en: 'Remarkably, we’ve been able to train the student to match the accuracy of the
    teacher, despite it having almost half the number of parameters! Let’s push the
    model to the Hub for future use:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管参数数量几乎减少了一半，我们已经成功训练出学生模型与教师模型的准确度相匹配！让我们将模型推送到Hub以供将来使用：
- en: '[PRE45]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Benchmarking Our Distilled Model
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准测试我们的精炼模型
- en: 'Now that we have an accurate student, let’s create a pipeline and redo our
    benchmark to see how we perform on the test set:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To put these results in context, let’s also visualize them with our `plot_metrics()`
    function:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](Images/nlpt_08in02.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: As expected, the model size and latency remain essentially unchanged compared
    to the DistilBERT benchmark, but the accuracy has improved and even surpassed
    the performance of the teacher! One way to interpret this surprising result is
    that the teacher has likely not been fine-tuned as systematically as the student.
    This is great, but we can actually compress our distilled model even further using
    a technique known as quantization. That’s the topic of the next section.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Making Models Faster with Quantization
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve now seen that with knowledge distillation we can reduce the computational
    and memory cost of running inference by transferring the information from a teacher
    into a smaller student. Quantization takes a different approach; instead of reducing
    the number of computations, it makes them much more efficient by representing
    the weights and activations with low-precision data types like 8-bit integer (INT8)
    instead of the usual 32-bit floating point (FP32). Reducing the number of bits
    means the resulting model requires less memory storage, and operations like matrix
    multiplication can be performed much faster with integer arithmetic. Remarkably,
    these performance gains can be realized with little to no loss in accuracy!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea behind quantization is that we can “discretize” the floating-point
    values *f* in each tensor by mapping their range [ <math alttext="f Subscript
    normal m normal a normal x Baseline comma f Subscript normal m normal i normal
    n Baseline"><mrow><msub><mi>f</mi> <mi>max</mi></msub> <mo>,</mo> <msub><mi>f</mi>
    <mi>min</mi></msub></mrow></math> ] into a smaller one [ <math alttext="q Subscript
    normal m normal a normal x Baseline comma q Subscript normal m normal i normal
    n Baseline"><mrow><msub><mi>q</mi> <mi>max</mi></msub> <mo>,</mo> <msub><mi>q</mi>
    <mi>min</mi></msub></mrow></math> ] of fixed-point numbers <math alttext="q"><mi>q</mi></math>
    , and linearly distributing all values in between. Mathematically, this mapping
    is described by the following equation:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="f equals left-parenthesis StartFraction f Subscript normal m
    normal a normal x Baseline minus f Subscript normal m normal i normal n Baseline
    Over q Subscript normal m normal a normal x Baseline minus q Subscript normal
    m normal i normal n Baseline EndFraction right-parenthesis left-parenthesis q
    minus upper Z right-parenthesis equals upper S left-parenthesis q minus upper
    Z right-parenthesis" display="block"><mrow><mi>f</mi> <mo>=</mo> <mfenced open="("
    close=")"><mfrac><mrow><msub><mi>f</mi> <mi>max</mi></msub> <mo>-</mo><msub><mi>f</mi>
    <mi>min</mi></msub></mrow> <mrow><msub><mi>q</mi> <mi>max</mi></msub> <mo>-</mo><msub><mi>q</mi>
    <mi>min</mi></msub></mrow></mfrac></mfenced> <mrow><mo>(</mo> <mi>q</mi> <mo>-</mo>
    <mi>Z</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>S</mi> <mrow><mo>(</mo> <mi>q</mi>
    <mo>-</mo> <mi>Z</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: where the scale factor <math alttext="upper S"><mi>S</mi></math> is a positive
    floating-point number and the constant <math alttext="upper Z"><mi>Z</mi></math>
    has the same type as <math alttext="q"><mi>q</mi></math> and is called the *zero
    point* because it corresponds to the quantized value of the floating-point value
    <math alttext="f equals 0"><mrow><mi>f</mi> <mo>=</mo> <mn>0</mn></mrow></math>
    . Note that the map needs to be *affine* so that we get back floating-point numbers
    when we dequantize the fixed-point ones.^([13](ch08.xhtml#idm46238706293280))
    An illustration of the conversion is shown in [Figure 8-6](#fp32toint8).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Mapping floating-point numbers to 8-bit integers](Images/nlpt_0806.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. Quantizing floating-point numbers as unsigned 8-bit integers (courtesy
    of Manas Sahni)
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, one of the main reasons why transformers (and deep neural networks more
    generally) are prime candidates for quantization is that the weights and activations
    tend to take values in relatively small ranges. This means we don’t have to squeeze
    the whole range of possible FP32 numbers into, say, the <math alttext="2 Superscript
    8 Baseline equals 256"><mrow><msup><mn>2</mn> <mn>8</mn></msup> <mo>=</mo> <mn>256</mn></mrow></math>
    numbers represented by INT8\. To see this, let’s pick out one of the attention
    weight matrices from our distilled model and plot the frequency distribution of
    the values:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![](Images/nlpt_08in03.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, the values of the weights are distributed in the small range
    [ <math alttext="negative 0.1 comma 0.1"><mrow><mo>-</mo> <mn>0</mn> <mo>.</mo>
    <mn>1</mn> <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>1</mn></mrow></math> ] around
    zero. Now, suppose we want to quantize this tensor as a signed 8-bit integer.
    In that case, the range of possible values for our integers is [ <math alttext="q
    Subscript normal m normal a normal x Baseline comma q Subscript normal m normal
    i normal n Baseline"><mrow><msub><mi>q</mi> <mi>max</mi></msub> <mo>,</mo> <msub><mi>q</mi>
    <mi>min</mi></msub></mrow></math> ] = [ <math alttext="negative 128 comma 127"><mrow><mo>-</mo>
    <mn>128</mn> <mo>,</mo> <mn>127</mn></mrow></math> ]. The zero point coincides
    with the zero of FP32 and the scale factor is calculated according to the previous
    equation:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'To obtain the quantized tensor, we just need to invert the mapping <math alttext="q
    equals f slash upper S plus upper Z"><mrow><mi>q</mi> <mo>=</mo> <mi>f</mi> <mo>/</mo>
    <mi>S</mi> <mo>+</mo> <mi>Z</mi></mrow></math> , clamp the values, round them
    to the nearest integer, and represent the result in the `torch.int8` data type
    using the `Tensor.char()` function:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Great, we’ve just quantized our first tensor! In PyTorch we can simplify the
    conversion by using the `quantize_per_tensor()` function together with a quantized
    data type, `torch.qint`, that is optimized for integer arithmetic operations:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The plot in [Figure 8-7](#weight-quantization) shows very clearly the discretization
    that’s induced by only mapping some of the weight values precisely and rounding
    the rest.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of quantization on a transformer''s weights](Images/nlpt_0807.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. Effect of quantization on a transformer’s weights
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To round out our little analysis, let’s compare how long it takes to compute
    the multiplication of two weight tensors with FP32 and INT8 values. For the FP32
    tensors, we can multiply them using PyTorch’s nifty `@` operator:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'For the quantized tensors we need the `QFunctional` wrapper class so that we
    can perform operations with the special `torch.qint8` data type:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This class supports various elementary operations, like addition, and in our
    case we can time the multiplication of our quantized tensors as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Compared to our FP32 computation, using the INT8 tensors is almost 100 times
    faster! Even larger gains can be obtained by using dedicated backends for running
    quantized operators efficiently. As of this book’s writing, PyTorch supports:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: x86 CPUs with AVX2 support or higher
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ARM CPUs (typically found in mobile/embedded devices)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since INT8 numbers have four times fewer bits than FP32 numbers, quantization
    also reduces the memory storage requirements by up to a factor of four. In our
    simple example we can verify this by comparing the underlying storage size of
    our weight tensor and its quantized cousin by using the `Tensor.storage()` function
    and the `getsizeof()` function from Python’s `sys` module:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: For a full-scale transformer, the actual compression rate depends on which layers
    are quantized (as we’ll see in the next section it is only the linear layers that
    typically get quantized).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'So what’s the catch with quantization? Changing the precision for all computations
    in our model introduces small disturbances at each point in the model’s computational
    graph, which can compound and affect the model’s performance. There are several
    ways to quantize a model, which all have pros and cons. For deep neural networks,
    there are typically three main approaches to quantization:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic quantization
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: When using dynamic quantization nothing is changed during training and the adaptations
    are only performed during inference. Like with all the quantization methods we
    will discuss, the weights of the model are converted to INT8 ahead of inference
    time. In addition to the weights, the model’s activations are also quantized.
    This approach is dynamic because the quantization happens on the fly. This means
    that all the matrix multiplications can be calculated with highly optimized INT8
    functions. Of all the quantization methods discussed here, dynamic quantization
    is the simplest one. However, with dynamic quantization the activations are written
    and read to memory in floating-point format. This conversion between integer and
    floating point can be a performance bottleneck.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Static quantization
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of computing the quantization of the activations on the fly, we can
    avoid the conversion to floating point by precomputing the quantization scheme.
    Static quantization achieves this by observing the activation patterns on a representative
    sample of the data ahead of inference time. The ideal quantization scheme is calculated
    and then saved. This enables us to skip the conversion between INT8 and FP32 values
    and speeds up the computations. However, it requires access to a good data sample
    and introduces an additional step in the pipeline, since we now need to train
    and determine the quantization scheme before we can perform inference. There is
    also one aspect that static quantization does not address: the discrepancy between
    the precision during training and inference, which leads to a performance drop
    in the model’s metrics (e.g., accuracy).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Quantization-aware training
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: The effect of quantization can be effectively simulated during training by “fake”
    quantization of the FP32 values. Instead of using INT8 values during training,
    the FP32 values are rounded to mimic the effect of quantization. This is done
    during both the forward and the backward pass and improves performance in terms
    of model metrics over static and dynamic quantization.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: The main bottleneck for running inference with transformers is the compute and
    memory bandwidth associated with the enormous numbers of weights in these models.
    For this reason, dynamic quantization is currently the best approach for transformer-based
    models in NLP. In smaller computer vision models the limiting factor is the memory
    bandwidth of the activations, which is why static quantization is generally used
    (or quantization-aware training in cases where the performance drops are too significant).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing dynamic quantization in PyTorch is quite simple and can be done
    with a single line of code:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Here we pass to `quantize_dynamic()` the full-precision model and specify the
    set of PyTorch layer classes in that model that we want to quantize. The `dtype`
    argument specifies the target precision and can be `fp16` or `qint8`. A good practice
    is to pick the lowest precision that you can tolerate with respect to your evaluation
    metrics. In this chapter we’ll use INT8, which as we’ll soon see has little impact
    on our model’s accuracy.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking Our Quantized Model
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With our model now quantized, let’s pass it through the benchmark and visualize
    the results:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '![](Images/nlpt_08in04.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: Nice, the quantized model is almost half the size of our distilled one and has
    even gained a slight accuracy boost! Let’s see if we can push our optimization
    to the limit with a powerful framework called the ONNX Runtime.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Inference with ONNX and the ONNX Runtime
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[ONNX](https://onnx.ai) is an open standard that defines a common set of operators
    and a common file format to represent deep learning models in a wide variety of
    frameworks, including PyTorch and TensorFlow.^([14](ch08.xhtml#idm46238705705264))
    When a model is exported to the ONNX format, these operators are used to construct
    a computational graph (often called an *intermediate representation*) that represents
    the flow of data through the neural network. An example of such a graph for BERT-base
    is shown in [Figure 8-8](#bert-onnx), where each node receives some input, applies
    an operation like `Add` or `Squeeze`, and then feeds the output to the next set
    of nodes.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[ONNX](https://onnx.ai)是一个开放标准，定义了一组通用的操作符和一种通用的文件格式，用于在各种框架中表示深度学习模型，包括PyTorch和TensorFlow。^([14](ch08.xhtml#idm46238705705264))当模型导出为ONNX格式时，这些操作符用于构建一个计算图（通常称为*中间表示*），表示数据通过神经网络的流动。例如，BERT-base的这样一个图示例显示在[图8-8](#bert-onnx)中，其中每个节点接收一些输入，应用操作如`Add`或`Squeeze`，然后将输出馈送到下一组节点。'
- en: '![Example ONNX graph](Images/nlpt_0808.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![ONNX图示例](Images/nlpt_0808.png)'
- en: Figure 8-8\. A section of the ONNX graph for BERT-base, visualized in Netron
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-8\. BERT-base的ONNX图的一个部分，在Netron中可视化
- en: By exposing a graph with standardized operators and data types, ONNX makes it
    easy to switch between frameworks. For example, a model trained in PyTorch can
    be exported to ONNX format and then imported in TensorFlow (and vice versa).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 通过公开具有标准化操作符和数据类型的图，ONNX使得在不同框架之间切换变得容易。例如，在PyTorch中训练的模型可以导出为ONNX格式，然后在TensorFlow中导入（反之亦然）。
- en: Where ONNX really shines is when it is coupled with a dedicated accelerator
    like [ONNX Runtime](https://onnxruntime.ai), or ORT for short.^([15](ch08.xhtml#idm46238705675632))
    ORT provides tools to optimize the ONNX graph through techniques like operator
    fusion and constant folding,^([16](ch08.xhtml#idm46238705672032)) and defines
    an interface to *execution providers* that allow you to run the model on different
    types of hardware. This is a powerful abstraction. [Figure 8-9](#onnx-ort) shows
    the high-level architecture of the ONNX and ORT ecosystem.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 当ONNX与专用加速器如[ONNX Runtime](https://onnxruntime.ai)或ORT配合使用时，它的优势就显现出来了。^([15](ch08.xhtml#idm46238705675632))ORT通过操作符融合和常量折叠等技术提供了优化ONNX图的工具，^([16](ch08.xhtml#idm46238705672032))并定义了一个接口，允许您在不同类型的硬件上运行模型。这是一个强大的抽象。[图8-9](#onnx-ort)显示了ONNX和ORT生态系统的高级架构。
- en: '![Architecture of the ONNX and ONNX Runtime ecosystem](Images/nlpt_0809.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![ONNX和ONNX Runtime生态系统的架构](Images/nlpt_0809.png)'
- en: Figure 8-9\. Architecture of the ONNX and ONNX Runtime ecosystem (courtesy of
    the ONNX Runtime team)
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-9\. ONNX和ONNX Runtime生态系统的架构（由ONNX Runtime团队提供）
- en: 'To see ORT in action, the first thing we need to do is convert our distilled
    model into the ONNX format. The ![nlpt_pin01](Images/nlpt_pin01.png) Transformers
    library has a built-in function called `con⁠vert_graph_to_onnx.convert()` that
    simplifies the process by taking the following steps:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 要看到ORT的运行情况，我们需要做的第一件事是将我们的精炼模型转换为ONNX格式。![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers库有一个内置函数叫做`con⁠vert_graph_to_onnx.convert()`，它简化了这个过程，采取以下步骤：
- en: Initialize the model as a `Pipeline`.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型初始化为`Pipeline`。
- en: Run placeholder inputs through the pipeline so that ONNX can record the computational
    graph.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过管道运行占位符输入，以便ONNX可以记录计算图。
- en: Define dynamic axes to handle dynamic sequence lengths.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义动态轴以处理动态序列长度。
- en: Save the graph with network parameters.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保存具有网络参数的图。
- en: 'To use this function, we first need to set some [OpenMP](https://openmp.org)
    environment variables for ONNX:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用这个函数，我们首先需要为ONNX设置一些[OpenMP](https://openmp.org)环境变量：
- en: '[PRE66]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: OpenMP is an API designed for developing highly parallelized applications. The
    `OMP_NUM_THREADS` environment variable sets the number of threads to use for parallel
    computations in the ONNX Runtime, while `OMP_WAIT_POLICY=ACTIVE` specifies that
    waiting threads should be active (i.e., using CPU processor cycles).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: OpenMP是一个为开发高度并行化应用程序而设计的API。`OMP_NUM_THREADS`环境变量设置并行计算中使用的线程数，在ONNX Runtime中，`OMP_WAIT_POLICY=ACTIVE`指定等待线程应处于活动状态（即使用CPU处理器周期）。
- en: 'Next, let’s convert our distilled model to the ONNX format. Here we need to
    specify the argument `pipeline_name="text-classification"` since `convert()` wraps
    the model in a ![nlpt_pin01](Images/nlpt_pin01.png) Transformers `pipeline()`
    function during the conversion. In addition to the `model_ckpt`, we also pass
    the tokenizer to initialize the pipeline:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将我们的精炼模型转换为ONNX格式。在这里，我们需要指定参数`pipeline_name="text-classification"`，因为`convert()`在转换过程中将模型包装在一个![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers `pipeline()`函数中。除了`model_ckpt`之外，我们还传递了tokenizer来初始化管道：
- en: '[PRE67]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: ONNX uses *operator sets* to group together immutable operator specifications,
    so `opset=12` corresponds to a specific version of the ONNX library.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX使用*操作符集*来将不可变的操作符规范分组在一起，因此`opset=12`对应于ONNX库的特定版本。
- en: 'Now that we have our model saved, we need to create an `InferenceSession` instance
    to feed inputs to the model:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经保存了我们的模型，我们需要创建一个`InferenceSession`实例来向模型输入数据：
- en: '[PRE68]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now when we call `onnx_model.run()`, we can get the class logits from the ONNX
    model. Let’s test this out with an example from the test set. Since the output
    from `convert()` tells us that ONNX expects just the `input_ids` and `attention_mask`
    as inputs, we need to drop the `label` column from our sample:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在当我们调用`onnx_model.run()`时，我们可以从ONNX模型中获取类别对数。让我们用测试集中的一个例子来测试一下。由于`convert()`的输出告诉我们ONNX只期望`input_ids`和`attention_mask`作为输入，我们需要从我们的样本中删除`label`列：
- en: '[PRE70]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Once we have the logits, we can easily get the predicted label by taking the
    argmax:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了对数，我们可以通过取argmax轻松获得预测的标签：
- en: '[PRE72]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'which indeed agrees with the ground truth label:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实与地面真实标签一致：
- en: '[PRE74]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The ONNX model is not compatible with the `text-classification` pipeline, so
    we’ll create our own class that mimics the core behavior:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ONNX模型与`text-classification`管道不兼容，因此我们将创建一个模仿核心行为的自定义类：
- en: '[PRE76]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'We can then test this on our simple query to see if we recover the `car_rental`
    intent:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以测试这个简单的查询，看看我们是否恢复了`car_rental`意图：
- en: '[PRE77]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Great, our pipeline works as expected. The next step is to create a performance
    benchmark for ONNX models. Here we can build on the work we did with the `Per⁠formanceBenchmark`
    class by simply overriding the `compute_size()` method and leaving the `compute_accuracy()`
    and `time_pipeline()` methods intact. The reason we need to override the `compute_size()`
    method is that we cannot rely on the `state_dict` and `torch.save()` to measure
    a model’s size, since `onnx_model` is technically an ONNX `InferenceSession` object
    that doesn’t have access to the attributes of PyTorch’s `nn.Module`. In any case,
    the resulting logic is simple and can be implemented as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，我们的流水线按预期工作。下一步是为ONNX模型创建性能基准测试。在这里，我们可以借鉴我们与`Per⁠formanceBenchmark`类一起完成的工作，只需重写`compute_size()`方法，保留`compute_accuracy()`和`time_pipeline()`方法。我们需要重写`compute_size()`方法的原因是，我们不能依赖`state_dict`和`torch.save()`来测量模型的大小，因为`onnx_model`在技术上是一个ONNX`InferenceSession`对象，无法访问PyTorch的`nn.Module`的属性。无论如何，结果逻辑很简单，可以实现如下：
- en: '[PRE79]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'With our new benchmark, let’s see how our distilled model performs when converted
    to ONNX format:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的新基准测试，让我们看看我们的蒸馏模型转换为ONNX格式后的性能：
- en: '[PRE80]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '![](Images/nlpt_08in05.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_08in05.png)'
- en: Remarkably, converting to the ONNX format and using the ONNX Runtime has given
    our distilled model (i.e. the “Distillation” circle in the plot) a boost in latency!
    Let’s see if we can squeeze out a bit more performance by adding quantization
    to the mix.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，转换为ONNX格式并使用ONNX Runtime为我们的蒸馏模型（即图中的“蒸馏”圈）提供了延迟增益！让我们看看是否可以通过添加量化来挤出更多性能。
- en: 'Similar to PyTorch, ORT offers three ways to quantize a model: dynamic, static,
    and quantization-aware training. As we did with PyTorch, we’ll apply dynamic quantization
    to our distilled model. In ORT, the quantization is applied through the `quan⁠tize_dynamic()`
    function, which requires a path to the ONNX model to quantize, a target path to
    save the quantized model to, and the data type to reduce the weights to:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 与PyTorch类似，ORT提供了三种模型量化的方式：动态量化、静态量化和量化感知训练。与PyTorch一样，我们将对我们的蒸馏模型应用动态量化。在ORT中，量化是通过`quan⁠tize_dynamic()`函数应用的，该函数需要一个ONNX模型的路径进行量化，一个目标路径来保存量化后的模型，以及要将权重减少到的数据类型：
- en: '[PRE83]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Now that the model is quantized, let’s run it through our benchmark:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经被量化，让我们通过我们的基准测试运行它：
- en: '[PRE84]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '![](Images/nlpt_08in06.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_08in06.png)'
- en: ORT quantization has reduced the model size and latency by around 30% compared
    to the model obtained from PyTorch quantization (the distillation + quantization
    blob). One reason for this is that PyTorch only optimizes the `nn.Linear` modules,
    while ONNX quantizes the embedding layer as well. From the plot we can also see
    that applying ORT quantization to our distilled model has provided an almost three-fold
    gain compared to our BERT baseline!
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 与PyTorch量化获得的模型相比，ORT量化已经将模型大小和延迟减少了约30%（蒸馏+量化blob）。其中一个原因是PyTorch只优化`nn.Linear`模块，而ONNX还量化了嵌入层。从图中我们还可以看到，将ORT量化应用于我们的蒸馏模型与我们的BERT基线相比，提供了近三倍的增益！
- en: This concludes our analysis of techniques to speed up transformers for inference.
    We have seen that methods such as quantization reduce the model size by reducing
    the precision of the representation. Another strategy to reduce the size is to
    remove some weights altogether. This technique is called *weight pruning*, and
    it’s the focus of the next section.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对加速变压器进行推断的技术的分析。我们已经看到，诸如量化之类的方法通过降低表示的精度来减小模型大小。另一种减小大小的策略是彻底删除一些权重。这种技术称为*权重修剪*，并且是下一节的重点。
- en: Making Models Sparser with Weight Pruning
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用权重修剪使模型更稀疏
- en: So far we’ve seen that knowledge distillation and weight quantization are quite
    effective at producing faster models for inference, but in some cases you might
    also have strong constraints on the memory footprint of your model. For example,
    if our product manager suddenly decides that our text assistant needs to be deployed
    on a mobile device, then we’ll need our intent classifier to take up as little
    storage space as possible. To round out our survey of compression methods, let’s
    take a look at how we can shrink the number of parameters in our model by identifying
    and removing the least important weights in the network.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到知识蒸馏和权重量化在产生更快的推断模型方面非常有效，但在某些情况下，您可能还对模型的内存占用有很强的约束。例如，如果我们的产品经理突然决定我们的文本助手需要部署在移动设备上，那么我们需要我们的意图分类器尽可能少地占用存储空间。为了完成我们对压缩方法的调查，让我们看看如何通过识别和删除网络中最不重要的权重来减少模型参数的数量。
- en: Sparsity in Deep Neural Networks
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度神经网络中的稀疏性
- en: As shown in [Figure 8-10](#network-pruning), the main idea behind pruning is
    to gradually remove weight connections (and potentially neurons) during training
    such that the model becomes progressively sparser. The resulting pruned model
    has a smaller number of nonzero parameters, which can then be stored in a compact
    sparse matrix format. Pruning can be also combined with quantization to obtain
    further compression.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图8-10](#network-pruning)所示，修剪的主要思想是在训练过程中逐渐移除权重连接（可能还有神经元），使模型逐渐变得更稀疏。结果修剪后的模型具有更少的非零参数，然后可以以紧凑的稀疏矩阵格式存储。修剪也可以与量化结合以获得进一步的压缩。
- en: '![Network Pruning](Images/nlpt_0810.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![网络修剪](Images/nlpt_0810.png)'
- en: Figure 8-10\. Weights and neurons before and after pruning (courtesy of Song
    Han)
  id: totrans-295
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-10。修剪前后的权重和神经元（由Song Han提供）
- en: Weight Pruning Methods
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 权重修剪方法
- en: 'Mathematically, the way most weight pruning methods work is to calculate a
    matrix <math alttext="bold upper S"><mi>𝐒</mi></math> of *importance scores* and
    then select the top <math alttext="k"><mi>k</mi></math> percent of weights by
    importance:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学上，大多数权重修剪方法的工作方式是计算一个*重要性分数*矩阵<math alttext="bold upper S"><mi>𝐒</mi></math>，然后按重要性选择前<math
    alttext="k"><mi>k</mi></math>百分比的权重：
- en: <math alttext="normal upper T normal o normal p Subscript k Baseline left-parenthesis
    bold upper S right-parenthesis Subscript i j Baseline equals StartLayout Enlarged
    left-brace 1st Row 1st Column 1 2nd Column Blank 3rd Column normal i normal f
    upper S Subscript i j Baseline normal i normal n normal t normal o normal p k
    percent-sign 2nd Row 1st Column 0 2nd Column Blank 3rd Column normal o normal
    t normal h normal e normal r normal w normal i normal s normal e EndLayout" display="block"><mrow><msub><mi>Top</mi>
    <mi>k</mi></msub> <msub><mrow><mo>(</mo><mi>𝐒</mi><mo>)</mo></mrow> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mn>1</mn></mrow></mtd>
    <mtd><mrow><mi>if</mi> <msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mi>in</mi> <mi>top</mi> <mi>k</mi> <mo>%</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd>
    <mtd><mi>otherwise</mi></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'In effect, <math alttext="k"><mi>k</mi></math> acts as a new hyperparameter
    to control the amount of sparsity in the model—that is, the proportion of weights
    that are zero-valued. Lower values of <math alttext="k"><mi>k</mi></math> correspond
    to sparser matrices. From these scores we can then define a *mask matrix* <math
    alttext="bold upper M"><mi>𝐌</mi></math> that masks the weights <math alttext="upper
    W Subscript i j"><msub><mi>W</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>
    during the forward pass with some input <math alttext="x Subscript i"><msub><mi>x</mi>
    <mi>i</mi></msub></math> and effectively creates a sparse network of activations
    <math alttext="a Subscript i"><msub><mi>a</mi> <mi>i</mi></msub></math> :'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="a Subscript i Baseline equals sigma-summation Underscript k Endscripts
    upper W Subscript i k Baseline upper M Subscript i k Baseline x Subscript k" display="block"><mrow><msub><mi>a</mi>
    <mi>i</mi></msub> <mo>=</mo> <munder><mo>∑</mo> <mi>k</mi></munder> <msub><mi>W</mi>
    <mrow><mi>i</mi><mi>k</mi></mrow></msub> <msub><mi>M</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub>
    <msub><mi>x</mi> <mi>k</mi></msub></mrow></math>
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed in the tongue-in-cheek “Optimal Brain Surgeon” paper,^([17](ch08.xhtml#idm46238704674352))
    at the heart of each pruning method are a set of questions that need to be considered:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Which weights should be eliminated?
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should the remaining weights be adjusted for best performance?
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can such network pruning be done in a computationally efficient way?
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The answers to these questions inform how the score matrix <math alttext="bold
    upper S"><mi>𝐒</mi></math> is computed, so let’s begin by looking at one of the
    earliest and most popular pruning methods: magnitude pruning.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Magnitude pruning
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name suggests, magnitude pruning calculates the scores according to the
    magnitude of the weights <math alttext="bold upper S equals left-parenthesis bar
    upper W Subscript i j Baseline bar right-parenthesis Subscript 1 less-than-or-equal-to
    j comma j less-than-or-equal-to n"><mrow><mi>𝐒</mi> <mo>=</mo> <msub><mfenced
    separators="" open="(" close=")"><mo>∣</mo> <msub><mi>W</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>∣</mo></mfenced> <mrow><mn>1</mn><mo>≤</mo><mi>j</mi><mo>,</mo><mi>j</mi><mo>≤</mo><mi>n</mi></mrow></msub></mrow></math>
    and then derives the masks from <math alttext="bold upper M equals normal upper
    T normal o normal p Subscript k Baseline left-parenthesis bold upper S right-parenthesis"><mrow><mi>𝐌</mi>
    <mo>=</mo> <msub><mi>Top</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi>𝐒</mi> <mo>)</mo></mrow></mrow></math>
    . In the literature it is common to apply magnitude pruning in an iterative fashion
    by first training the model to learn which connections are important and pruning
    the weights of least importance.^([18](ch08.xhtml#idm46238704650928)) The sparse
    model is then retrained and the process repeated until the desired sparsity is
    reached.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'One drawback with this approach is that it is computationally demanding: at
    every step of pruning we need to train the model to convergence. For this reason
    it is generally better to gradually increase the initial sparsity <math alttext="s
    Subscript i"><msub><mi>s</mi> <mi>i</mi></msub></math> (which is usually zero)
    to a final value <math alttext="s Subscript f"><msub><mi>s</mi> <mi>f</mi></msub></math>
    after some number of steps <math alttext="upper N"><mi>N</mi></math> :^([19](ch08.xhtml#idm46238704643440))'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的一个缺点是计算需求量大：在每一步修剪中，我们都需要将模型训练到收敛。因此，通常最好逐渐增加初始稀疏度<math alttext="s Subscript
    i"><msub><mi>s</mi> <mi>i</mi></msub></math>（通常为零）到一定步数<math alttext="upper N"><mi>N</mi></math>后的最终值<math
    alttext="s Subscript f"><msub><mi>s</mi> <mi>f</mi></msub></math>。^([19](ch08.xhtml#idm46238704643440))
- en: <math alttext="s Subscript t Baseline equals s Subscript f Baseline plus left-parenthesis
    s Subscript i Baseline minus s Subscript f Baseline right-parenthesis left-parenthesis
    1 minus StartFraction t minus t 0 Over upper N normal upper Delta t EndFraction
    right-parenthesis cubed normal f normal o normal r t element-of StartSet t 0 comma
    t 0 plus normal upper Delta t comma ellipsis comma t 0 plus upper N normal upper
    Delta t EndSet" display="block"><mrow><msub><mi>s</mi> <mi>t</mi></msub> <mo>=</mo>
    <msub><mi>s</mi> <mi>f</mi></msub> <mo>+</mo> <mrow><mo>(</mo> <msub><mi>s</mi>
    <mi>i</mi></msub> <mo>-</mo> <msub><mi>s</mi> <mi>f</mi></msub> <mo>)</mo></mrow>
    <msup><mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mfrac><mrow><mi>t</mi><mo>-</mo><msub><mi>t</mi>
    <mn>0</mn></msub></mrow> <mrow><mi>N</mi><mi>Δ</mi><mi>t</mi></mrow></mfrac></mfenced>
    <mn>3</mn></msup> <mi>for</mi> <mi>t</mi> <mo>∈</mo> <mrow><mo>{</mo> <msub><mi>t</mi>
    <mn>0</mn></msub> <mo>,</mo> <msub><mi>t</mi> <mn>0</mn></msub> <mo>+</mo> <mi>Δ</mi>
    <mi>t</mi> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>t</mi> <mn>0</mn></msub>
    <mo>+</mo> <mi>N</mi> <mi>Δ</mi> <mi>t</mi> <mo>}</mo></mrow></mrow></math>
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="s Subscript t Baseline equals s Subscript f Baseline plus left-parenthesis
    s Subscript i Baseline minus s Subscript f Baseline right-parenthesis left-parenthesis
    1 minus StartFraction t minus t 0 Over upper N normal upper Delta t EndFraction
    right-parenthesis cubed normal f normal o normal r t element-of StartSet t 0 comma
    t 0 plus normal upper Delta t comma ellipsis comma t 0 plus upper N normal upper
    Delta t EndSet" display="block"><mrow><msub><mi>s</mi> <mi>t</mi></msub> <mo>=</mo>
    <msub><mi>s</mi> <mi>f</mi></msub> <mo>+</mo> <mrow><mo>(</mo> <msub><mi>s</mi>
    <mi>i</mi></msub> <mo>-</mo> <msub><mi>s</mi> <mi>f</mi></msub> <mo>)</mo></mrow>
    <msup><mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mfrac><mrow><mi>t</mi><mo>-</mo><msub><mi>t</mi>
    <mn>0</mn></msub></mrow> <mrow><mi>N</mi><mi>Δ</mi><mi>t</mi></mrow></mfrac></mfenced>
    <mn>3</mn></msup> <mi>for</mi> <mi>t</mi> <mo>∈</mo> <mrow><mo>{</mo> <msub><mi>t</mi>
    <mn>0</mn></msub> <mo>,</mo> <msub><mi>t</mi> <mn>0</mn></msub> <mo>+</mo> <mi>Δ</mi>
    <mi>t</mi> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>t</mi> <mn>0</mn></msub>
    <mo>+</mo> <mi>N</mi> <mi>Δ</mi> <mi>t</mi> <mo>}</mo></mrow></mrow></math>
- en: Here the idea is to update the binary masks <math alttext="bold upper M"><mi>𝐌</mi></math>
    every <math alttext="normal upper Delta t"><mrow><mi>Δ</mi> <mi>t</mi></mrow></math>
    steps to allow masked weights to reactivate during training and recover from any
    potential loss in accuracy that is induced by the pruning process. As shown in
    [Figure 8-11](#sparsity-scheduler), the cubic factor implies that the rate of
    weight pruning is highest in the early phases (when the number of redundant weights
    is large) and gradually tapers off.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是每隔<math alttext="normal upper Delta t"><mrow><mi>Δ</mi> <mi>t</mi></mrow></math>步更新一次二进制掩码<math
    alttext="bold upper M"><mi>𝐌</mi></math>，以允许被屏蔽的权重在训练过程中重新激活，并从修剪过程中可能导致的任何精度损失中恢复过来。如[图8-11](#sparsity-scheduler)所示，立方因子意味着权重修剪的速率在早期阶段最高（当冗余权重数量较大时），并逐渐减小。
- en: '![Sparsity scheduler](Images/nlpt_0811.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![稀疏调度器](Images/nlpt_0811.png)'
- en: Figure 8-11\. The cubic sparsity scheduler used for pruning
  id: totrans-312
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-11。用于修剪的立方稀疏调度器。
- en: One problem with magnitude pruning is that it is really designed for pure supervised
    learning, where the importance of each weight is directly related to the task
    at hand. By contrast, in transfer learning the importance of the weights is primarily
    determined by the pretraining phase, so magnitude pruning can remove connections
    that are important for the fine-tuning task. Recently, an adaptive approach called
    movement pruning has been proposed by Hugging Face researchers—let’s take a look.^([20](ch08.xhtml#idm46238704604576))
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 幅度修剪的一个问题是，它实际上是为纯监督学习而设计的，其中每个权重的重要性与手头的任务直接相关。相比之下，在迁移学习中，权重的重要性主要由预训练阶段确定，因此幅度修剪可能会移除对微调任务重要的连接。最近，Hugging
    Face的研究人员提出了一种称为移动修剪的自适应方法——让我们来看一下。^([20](ch08.xhtml#idm46238704604576))
- en: Movement pruning
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 移动修剪
- en: The basic idea behind movement pruning is to *gradually* remove weights during
    fine-tuning such that the model becomes progressively *sparser*. The key novelty
    is that both the weights and the scores are learned during fine-tuning. So, instead
    of being derived directly from the weights (like with magnitude pruning), the
    scores in movement pruning are arbitrary and are learned through gradient descent
    like any other neural network parameter. This implies that in the backward pass,
    we also track the gradient of the loss <math alttext="upper L"><mi>L</mi></math>
    with respect to the scores <math alttext="upper S Subscript i j"><msub><mi>S</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub></math> .
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 移动修剪背后的基本思想是*逐渐*在微调过程中移除权重，使模型逐渐变得*更稀疏*。关键的新颖之处在于，在微调过程中，权重和分数都是可学习的。因此，与幅度修剪直接从权重派生（如幅度修剪）不同，移动修剪中的分数是任意的，并且通过梯度下降学习，就像任何其他神经网络参数一样。这意味着在反向传播中，我们还要跟踪损失<math
    alttext="upper L"><mi>L</mi></math>相对于分数<math alttext="upper S Subscript i j"><msub><mi>S</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>的梯度。
- en: Once the scores are learned, it is then straightforward to generate the binary
    mask using <math alttext="bold upper M equals normal upper T normal o normal p
    Subscript k Baseline left-parenthesis bold upper S right-parenthesis"><mrow><mi>𝐌</mi>
    <mo>=</mo> <msub><mi>Top</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi>𝐒</mi> <mo>)</mo></mrow></mrow></math>
    .^([21](ch08.xhtml#idm46238704590336))
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦学习了分数，就很容易使用<math alttext="bold upper M equals normal upper T normal o normal
    p Subscript k Baseline left-parenthesis bold upper S right-parenthesis"><mrow><mi>𝐌</mi>
    <mo>=</mo> <msub><mi>Top</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi>𝐒</mi> <mo>)</mo></mrow></mrow></math>生成二进制掩码。^([21](ch08.xhtml#idm46238704590336))
- en: The intuition behind movement pruning is that the weights that are “moving”
    the most from zero are the most important ones to keep. In other words, the positive
    weights increase during fine-tuning (and vice versa for the negative weights),
    which is equivalent to saying that the scores increase as the weights move away
    from zero. As shown in [Figure 8-12](#magnitude-vs-movement), this behavior differs
    from magnitude pruning, which selects as the most important weights those that
    are *furthest* from zero.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 运动剪枝背后的直觉是，“移动”离零最远的权重是最重要的。换句话说，正权重在精细调整期间增加（负权重相反），这相当于说分数随着权重远离零而增加。如[图8-12](#magnitude-vs-movement)所示，这种行为与幅值剪枝不同，后者选择离零最远的权重作为最重要的权重。
- en: '![Magnitude vs Movement Pruning](Images/nlpt_0812.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![幅值与运动剪枝](Images/nlpt_0812.png)'
- en: Figure 8-12\. Comparison of weights removed during magnitude pruning (left)
    and movement pruning (right)
  id: totrans-319
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-12。幅值剪枝（左）和运动剪枝（右）中移除的权重的比较
- en: These differences between the two pruning methods are also evident in the distribution
    of the remaining weights. As shown in [Figure 8-13](#pruning-dists), magnitude
    pruning produces two clusters of weights, while movement pruning produces a smoother
    distribution.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种剪枝方法之间的差异也在剩余权重的分布中显而易见。如[图8-13](#pruning-dists)所示，幅值剪枝产生两个权重簇，而运动剪枝产生更平滑的分布。
- en: As of this book’s writing, ![nlpt_pin01](Images/nlpt_pin01.png) Transformers
    does not support pruning methods out of the box. Fortunately, there is a nifty
    library called [*Neural Networks Block Movement Pruning*](https://oreil.ly/aHEvD)
    that implements many of these ideas, and we recommend checking it out if memory
    constraints are a concern.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本书撰写时，![nlpt_pin01](Images/nlpt_pin01.png) Transformers不支持开箱即用的剪枝方法。幸运的是，有一个名为[*神经网络块运动剪枝*](https://oreil.ly/aHEvD)的巧妙库实现了许多这些想法，如果内存限制是一个问题，我们建议查看它。
- en: '![Pruning Distributions](Images/nlpt_0813.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![剪枝分布](Images/nlpt_0813.png)'
- en: Figure 8-13\. Distribution of remaining weights for magnitude pruning (MaP)
    and movement pruning (MvP)
  id: totrans-323
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-13。剩余权重的分布，用于幅值剪枝（MaP）和运动剪枝（MvP）
- en: Conclusion
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: 'We’ve seen that optimizing transformers for deployment in production environments
    involves compression along two dimensions: latency and memory footprint. Starting
    from a fine-tuned model, we applied distillation, quantization, and optimizations
    through ORT to significantly reduce both of these. In particular, we found that
    quantization and conversion in ORT gave the largest gains with minimal effort.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到，优化变压器以部署到生产环境中涉及沿两个维度的压缩：延迟和内存占用。从经过精细调整的模型开始，我们应用了蒸馏、量化和ORT优化，显著减少了这两者。特别是，我们发现量化和ORT中的转换给出了最大的收益，而付出的努力最小。
- en: Although pruning is an effective strategy for reducing the storage size of transformer
    models, current hardware is not optimized for sparse matrix operations, which
    limits the usefulness of this technique. However, this is an active area of research,
    and by the time this book hits the shelves many of these limitations may have
    been resolved.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管剪枝是减少变压器模型存储大小的有效策略，但当前的硬件并未针对稀疏矩阵运算进行优化，这限制了这种技术的实用性。然而，这是一个活跃的研究领域，到本书上市时，许多这些限制可能已经得到解决。
- en: So where to from here? All of the techniques in this chapter can be adapted
    to other tasks, such as question answering, named entity recognition, or language
    modeling. If you find yourself struggling to meet the latency requirements or
    your model is eating up all your compute budget, we suggest giving one of them
    a try.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 那么接下来呢？本章中的所有技术都可以应用到其他任务中，比如问答、命名实体识别或语言建模。如果您发现自己难以满足延迟要求，或者您的模型占用了所有的计算预算，我们建议尝试其中之一。
- en: 'In the next chapter, we’ll switch gears away from performance optimization
    and explore every data scientist’s worst nightmare: dealing with few to no labels.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将摆脱性能优化，探讨每个数据科学家的噩梦：处理少量或没有标签的情况。
- en: ^([1](ch08.xhtml#idm46238709482512-marker)) S. Larson et al., [“An Evaluation
    Dataset for Intent Classification and Out-of-Scope Prediction”](https://arxiv.org/abs/1909.02027),
    (2019).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch08.xhtml#idm46238709482512-marker)) S. Larson等人，[“意图分类和超出范围预测的评估数据集”](https://arxiv.org/abs/1909.02027)，（2019年）。
- en: ^([2](ch08.xhtml#idm46238709422384-marker)) As described by Emmanuel Ameisen
    in *Building Machine Learning Powered Applications* (O’Reilly), business or product
    metrics are the *most* important ones to consider. After all, it doesn’t matter
    how accurate your model is if it doesn’t solve a problem your business cares about.
    In this chapter we’ll assume that you have already defined the metrics that matter
    for your application and focus on optimizing the model metrics.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch08.xhtml#idm46238709422384-marker)) 正如Emmanuel Ameisen在*构建机器学习驱动的应用*（O'Reilly）中所描述的，业务或产品指标是*最*重要的考虑因素。毕竟，如果您的模型不能解决业务关心的问题，那么它的准确性就无关紧要。在本章中，我们将假设您已经为应用程序定义了重要的指标，并专注于优化模型指标。
- en: '^([3](ch08.xhtml#idm46238708497152-marker)) C. Buciluă et al., “Model Compression,”
    *Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining* (August 2006): 535–541, [*https://doi.org/10.1145/1150402.1150464*](https://doi.org/10.1145/1150402.1150464).'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch08.xhtml#idm46238708497152-marker)) C. Buciluă等人，“模型压缩”，*第12届ACM SIGKDD国际知识发现和数据挖掘会议论文集*（2006年8月）：535-541，[*https://doi.org/10.1145/1150402.1150464*](https://doi.org/10.1145/1150402.1150464)。
- en: ^([4](ch08.xhtml#idm46238708494384-marker)) G. Hinton, O. Vinyals, and J. Dean,
    [“Distilling the Knowledge in a Neural Network”](https://arxiv.org/abs/1503.02531),
    (2015).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch08.xhtml#idm46238708494384-marker)) G. Hinton, O. Vinyals和J. Dean，[“蒸馏神经网络中的知识”](https://arxiv.org/abs/1503.02531)，（2015年）。
- en: '^([5](ch08.xhtml#idm46238708466160-marker)) W. Fedus, B. Zoph, and N. Shazeer,
    [“Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient
    Sparsity”](https://arxiv.org/abs/2101.03961), (2021).'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch08.xhtml#idm46238708457120-marker)) Geoff Hinton coined this term in
    a [talk](https://oreil.ly/OkHGp) to refer to the observation that softened probabilities
    reveal the hidden knowledge of the teacher.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch08.xhtml#idm46238708428448-marker)) We also encountered temperature
    in the context of text generation in [Chapter 5](ch05.xhtml#chapter_generation).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch08.xhtml#idm46238708343392-marker)) V. Sanh et al., [“DistilBERT, a
    Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter”](https://arxiv.org/abs/1910.01108),
    (2019).'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch08.xhtml#idm46238707933552-marker)) Y. Kim and H. Awadalla, [“FastFormers:
    Highly Efficient Transformer Models for Natural Language Understanding”](https://arxiv.org/abs/2010.13382),
    (2020).'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch08.xhtml#idm46238707834608-marker)) By default, the `Trainer` looks
    for a column called `labels` when fine-tuning on classification tasks. You can
    also override this behavior by specifying the `label_names` argument of `TrainingArguments`.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch08.xhtml#idm46238707732144-marker)) This approach of fine-tuning a
    general-purpose, distilled language model is sometimes referred to as “task-agnostic”
    distillation.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '^([12](ch08.xhtml#idm46238706954496-marker)) T. Akiba et al., [“Optuna: A Next-Generation
    Hyperparameter Optimization Framework”](https://arxiv.org/abs/1907.10902), (2019).'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch08.xhtml#idm46238706293280-marker)) An affine map is just a fancy name
    for the <math alttext="y equals upper A x plus b"><mrow><mi>y</mi> <mo>=</mo>
    <mi>A</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi></mrow></math> map that you’re familiar
    with in the linear layers of a neural network.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch08.xhtml#idm46238705705264-marker)) There is a separate standard called
    ONNX-ML that is designed for traditional machine learning models like random forests
    and frameworks like Scikit-learn.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch08.xhtml#idm46238705675632-marker)) Other popular accelerators include
    [NVIDIA’s TensorRT](https://oreil.ly/HnNZx) and [Apache TVM](https://oreil.ly/7KUyt).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch08.xhtml#idm46238705672032-marker)) A fused operation involves merging
    one operator (usually an activation function) into another so that they can be
    executed together. For example, suppose we want to apply an activation *f* to
    a matrix product *A* × *B*. Normally the result of the product needs to be written
    back to the GPU memory before the activation is computed. Operator fusion allows
    as to compute <math alttext="f left-parenthesis upper A times upper B right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>A</mi> <mo>×</mo> <mi>B</mi> <mo>)</mo></mrow></math> in a single
    step. Constant folding refers to the process of evaluating constant expressions
    at compile time instead of runtime.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '^([17](ch08.xhtml#idm46238704674352-marker)) B. Hassibi and D. Stork, “Second
    Order Derivatives for Network Pruning: Optimal Brain Surgeon,” *Proceedings of
    the 5th International Conference on Neural Information Processing Systems* (November
    1992): 164–171, [*https://papers.nips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html*](https://papers.nips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html).'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch08.xhtml#idm46238704650928-marker)) S. Han et al., [“Learning Both
    Weights and Connections for Efficient Neural Networks”](https://arxiv.org/abs/1506.02626),
    (2015).
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '^([19](ch08.xhtml#idm46238704643440-marker)) M. Zhu and S. Gupta, [“To Prune,
    or Not to Prune: Exploring the Efficacy of Pruning for Model Compression”](https://arxiv.org/abs/1710.01878),
    (2017).'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '^([20](ch08.xhtml#idm46238704604576-marker)) V. Sanh, T. Wolf, and A.M. Rush,
    [“Movement Pruning: Adaptive Sparsity by Fine-Tuning”](https://arxiv.org/abs/2005.07683),
    (2020).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '^([21](ch08.xhtml#idm46238704590336-marker)) There is also a “soft” version
    of movement pruning where instead of picking the top <math alttext="k"><mi>k</mi></math>
    % of weights, one uses a global threshold <math alttext="tau"><mi>τ</mi></math>
    to define the binary mask: <math alttext="bold upper M equals left-parenthesis
    bold upper S greater-than tau right-parenthesis"><mrow><mi>𝐌</mi> <mo>=</mo> <mo>(</mo>
    <mi>𝐒</mi> <mo>></mo> <mi>τ</mi> <mo>)</mo></mrow></math> .'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch08.xhtml#idm46238704590336-marker)) 还有一种“软”版本的移动修剪，其中不是选择权重的前<math
    alttext="k"><mi>k</mi></math> %，而是使用全局阈值<math alttext="tau"><mi>τ</mi></math>来定义二进制掩码：<math
    alttext="bold upper M equals left-parenthesis bold upper S greater-than tau right-parenthesis"><mrow><mi>𝐌</mi>
    <mo>=</mo> <mo>(</mo> <mi>𝐒</mi> <mo>></mo> <mi>τ</mi> <mo>)</mo></mrow></math>。
