- en: Chapter 8\. Making Transformers Efficient in Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬8ç« ã€‚ä½¿transformersåœ¨ç”Ÿäº§ä¸­æ›´é«˜æ•ˆ
- en: In the previous chapters, youâ€™ve seen how transformers can be fine-tuned to
    produce great results on a wide range of tasks. However, in many situations accuracy
    (or whatever metric youâ€™re optimizing for) is not enough; your state-of-the-art
    model is not very useful if itâ€™s too slow or large to meet the business requirements
    of your application. An obvious alternative is to train a faster and more compact
    model, but the reduction in model capacity is often accompanied by a degradation
    in performance. So what can you do when you need a fast, compact, yet highly accurate
    model?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¹‹å‰çš„ç« èŠ‚ä¸­ï¼Œæ‚¨å·²ç»çœ‹åˆ°äº†transformerså¦‚ä½•è¢«å¾®è°ƒä»¥åœ¨å„ç§ä»»åŠ¡ä¸Šäº§ç”Ÿå‡ºè‰²çš„ç»“æœã€‚ç„¶è€Œï¼Œåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œå‡†ç¡®æ€§ï¼ˆæˆ–è€…æ‚¨æ­£åœ¨ä¼˜åŒ–çš„ä»»ä½•æŒ‡æ ‡ï¼‰æ˜¯ä¸å¤Ÿçš„ï¼›å¦‚æœæ‚¨çš„æœ€å…ˆè¿›æ¨¡å‹å¤ªæ…¢æˆ–å¤ªå¤§ï¼Œæ— æ³•æ»¡è¶³åº”ç”¨ç¨‹åºçš„ä¸šåŠ¡éœ€æ±‚ï¼Œé‚£ä¹ˆå®ƒå°±ä¸æ˜¯å¾ˆæœ‰ç”¨ã€‚ä¸€ä¸ªæ˜æ˜¾çš„æ›¿ä»£æ–¹æ¡ˆæ˜¯è®­ç»ƒä¸€ä¸ªæ›´å¿«ã€æ›´ç´§å‡‘çš„æ¨¡å‹ï¼Œä½†æ¨¡å‹å®¹é‡çš„å‡å°‘é€šå¸¸ä¼šä¼´éšç€æ€§èƒ½çš„ä¸‹é™ã€‚é‚£ä¹ˆå½“æ‚¨éœ€è¦ä¸€ä¸ªå¿«é€Ÿã€ç´§å‡‘ä½†é«˜åº¦å‡†ç¡®çš„æ¨¡å‹æ—¶ï¼Œæ‚¨è¯¥æ€ä¹ˆåŠå‘¢ï¼Ÿ
- en: 'In this chapter we will explore four complementary techniques that can be used
    to speed up the predictions and reduce the memory footprint of your transformer
    models: *knowledge distillation*, *quantization*, *pruning*, and *graph optimization*
    with the Open Neural Network Exchange (ONNX) format and ONNX Runtime (ORT). Weâ€™ll
    also see how some of these techniques can be combined to produce significant performance
    gains. For example, this was the approach taken by the Roblox engineering team
    in their article [â€œHow We Scaled Bert to Serve 1+ Billion Daily Requests on CPUsâ€](https://oreil.ly/QdNIk),
    who as shown in [FigureÂ 8-1](#roblox) found that combining knowledge distillation
    and quantization enabled them to improve the latency and throughput of their BERT
    classifier by over a factor of 30!'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ¢è®¨å››ç§äº’è¡¥çš„æŠ€æœ¯ï¼Œå¯ä»¥ç”¨æ¥åŠ é€Ÿé¢„æµ‹å¹¶å‡å°‘æ‚¨çš„transformeræ¨¡å‹çš„å†…å­˜å ç”¨ï¼š*çŸ¥è¯†è’¸é¦*ã€*é‡åŒ–*ã€*ä¿®å‰ª*å’Œä½¿ç”¨Open Neural
    Network Exchange (ONNX)æ ¼å¼å’ŒONNX Runtime (ORT)è¿›è¡Œ*å›¾ä¼˜åŒ–*ã€‚æˆ‘ä»¬è¿˜å°†çœ‹åˆ°å…¶ä¸­ä¸€äº›æŠ€æœ¯å¦‚ä½•ç»“åˆèµ·æ¥äº§ç”Ÿæ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚ä¾‹å¦‚ï¼Œè¿™æ˜¯Robloxå·¥ç¨‹å›¢é˜Ÿåœ¨ä»–ä»¬çš„æ–‡ç« [â€œæˆ‘ä»¬å¦‚ä½•åœ¨CPUä¸Šæ‰©å±•BERTä»¥å¤„ç†10äº¿+æ—¥è¯·æ±‚â€](https://oreil.ly/QdNIk)ä¸­é‡‡å–çš„æ–¹æ³•ï¼Œæ­£å¦‚[å›¾8-1](#roblox)æ‰€ç¤ºï¼Œä»–ä»¬å‘ç°ç»“åˆçŸ¥è¯†è’¸é¦å’Œé‡åŒ–ä½¿ä»–ä»¬çš„BERTåˆ†ç±»å™¨çš„å»¶è¿Ÿå’Œååé‡æé«˜äº†30å€ä»¥ä¸Šï¼
- en: '![Scaling BERT at Roblox](Images/nlpt_0801.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![åœ¨Robloxæ‰©å±•BERT](Images/nlpt_0801.png)'
- en: Figure 8-1\. How Roblox scaled BERT with knowledge distillation, dynamic padding,
    and weight quantization (photo courtesy of Roblox employees Quoc N. Le and Kip
    Kaehler)
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾8-1\. Robloxå¦‚ä½•é€šè¿‡çŸ¥è¯†è’¸é¦ã€åŠ¨æ€å¡«å……å’Œæƒé‡é‡åŒ–æ‰©å±•BERTï¼ˆç…§ç‰‡ç”±Robloxå‘˜å·¥Quoc N. Leå’ŒKip Kaehleræä¾›ï¼‰
- en: To illustrate the benefits and trade-offs associated with each technique, weâ€™ll
    use intent detection as a case study; this is an important component of text-based
    assistants, where low latencies are critical for maintaining a conversation in
    real time. Along the way youâ€™ll learn how to create custom trainers, perform efficient
    hyperparameter search, and gain a sense of what it takes to implement cutting-edge
    research with ![nlpt_pin01](Images/nlpt_pin01.png) Transformers. Letâ€™s dive in!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è¯´æ˜ä¸æ¯ç§æŠ€æœ¯ç›¸å…³çš„å¥½å¤„å’Œæƒè¡¡ï¼Œæˆ‘ä»¬å°†ä»¥æ„å›¾æ£€æµ‹ä¸ºæ¡ˆä¾‹ç ”ç©¶ï¼›è¿™æ˜¯åŸºäºæ–‡æœ¬çš„åŠ©æ‰‹çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä½å»¶è¿Ÿå¯¹äºå®æ—¶ç»´æŒå¯¹è¯è‡³å…³é‡è¦ã€‚åœ¨å­¦ä¹ çš„è¿‡ç¨‹ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•åˆ›å»ºè‡ªå®šä¹‰è®­ç»ƒå™¨ï¼Œæ‰§è¡Œé«˜æ•ˆçš„è¶…å‚æ•°æœç´¢ï¼Œå¹¶äº†è§£å®æ–½æœ€å‰æ²¿ç ”ç©¶æ‰€éœ€çš„å†…å®¹ï¼Œä½¿ç”¨![nlpt_pin01](Images/nlpt_pin01.png)
    Transformersã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼
- en: Intent Detection as a Case Study
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»¥æ„å›¾æ£€æµ‹ä¸ºæ¡ˆä¾‹ç ”ç©¶
- en: 'Letâ€™s suppose that weâ€™re trying to build a text-based assistant for our companyâ€™s
    call center so that customers can request their account balance or make bookings
    without needing to speak with a human agent. In order to understand the goals
    of a customer, our assistant will need to be able to classify a wide variety of
    natural language text into a set of predefined actions or *intents*. For example,
    a customer might send a message like the following about an upcoming trip:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: å‡è®¾æˆ‘ä»¬æ­£åœ¨å°è¯•ä¸ºå…¬å¸çš„å‘¼å«ä¸­å¿ƒæ„å»ºä¸€ä¸ªåŸºäºæ–‡æœ¬çš„åŠ©æ‰‹ï¼Œä»¥ä¾¿å®¢æˆ·å¯ä»¥åœ¨ä¸éœ€è¦ä¸äººç±»ä»£ç†äº¤è°ˆçš„æƒ…å†µä¸‹è¯·æ±‚å…¶è´¦æˆ·ä½™é¢æˆ–è¿›è¡Œé¢„è®¢ã€‚ä¸ºäº†ç†è§£å®¢æˆ·çš„ç›®æ ‡ï¼Œæˆ‘ä»¬çš„åŠ©æ‰‹éœ€è¦èƒ½å¤Ÿå°†å„ç§è‡ªç„¶è¯­è¨€æ–‡æœ¬åˆ†ç±»ä¸ºä¸€ç»„é¢„å®šä¹‰çš„åŠ¨ä½œæˆ–*æ„å›¾*ã€‚ä¾‹å¦‚ï¼Œå®¢æˆ·å¯èƒ½ä¼šå‘é€ä»¥ä¸‹å…³äºå³å°†åˆ°æ¥çš„æ—…è¡Œçš„æ¶ˆæ¯ï¼š
- en: Hey, Iâ€™d like to rent a vehicle from Nov 1st to Nov 15th in Paris and I need
    a 15 passenger van
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å˜¿ï¼Œæˆ‘æƒ³åœ¨11æœˆ1æ—¥åˆ°11æœˆ15æ—¥åœ¨å·´é»ç§Ÿä¸€è¾†è½¦ï¼Œæˆ‘éœ€è¦ä¸€è¾†15åº§ä½çš„é¢åŒ…è½¦ã€‚
- en: and our intent classifier could automatically categorize this as a *Car Rental*
    intent, which then triggers an action and response. To be robust in a production
    environment, our classifier will also need to be able to handle *out-of-scope*
    queries, where a customer makes a query that doesnâ€™t belong to any of the predefined
    intents and the system should yield a fallback response. For example, in the second
    case shown in [FigureÂ 8-2](#oos), a customer asks a question about sports (which
    is out of scope), and the text assistant mistakenly classifies it as one of the
    known in-scope intents and returns the payday response. In the third case, the
    text assistant has been trained to detect out-of-scope queries (usually labeled
    as a separate class) and informs the customer about which topics it can answer
    questions about.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ„å›¾åˆ†ç±»å™¨å¯ä»¥è‡ªåŠ¨å°†æ­¤åˆ†ç±»ä¸º*ç§Ÿè½¦*æ„å›¾ï¼Œç„¶åè§¦å‘ä¸€ä¸ªåŠ¨ä½œå’Œå“åº”ã€‚ä¸ºäº†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å…·æœ‰é²æ£’æ€§ï¼Œæˆ‘ä»¬çš„åˆ†ç±»å™¨è¿˜éœ€è¦èƒ½å¤Ÿå¤„ç†*è¶…å‡ºèŒƒå›´*çš„æŸ¥è¯¢ï¼Œå³å®¢æˆ·æå‡ºä¸å±äºä»»ä½•é¢„å®šä¹‰æ„å›¾çš„æŸ¥è¯¢ï¼Œç³»ç»Ÿåº”è¯¥äº§ç”Ÿä¸€ä¸ªå›é€€å“åº”ã€‚ä¾‹å¦‚ï¼Œåœ¨[å›¾8-2](#oos)ä¸­æ˜¾ç¤ºçš„ç¬¬äºŒç§æƒ…å†µä¸­ï¼Œå®¢æˆ·è¯¢é—®æœ‰å…³ä½“è‚²çš„é—®é¢˜ï¼ˆè¶…å‡ºèŒƒå›´ï¼‰ï¼Œæ–‡æœ¬åŠ©æ‰‹é”™è¯¯åœ°å°†å…¶åˆ†ç±»ä¸ºå·²çŸ¥çš„èŒƒå›´å†…æ„å›¾ä¹‹ä¸€ï¼Œå¹¶è¿”å›å‘è–ªæ—¥çš„å“åº”ã€‚åœ¨ç¬¬ä¸‰ç§æƒ…å†µä¸‹ï¼Œæ–‡æœ¬åŠ©æ‰‹å·²ç»è¢«è®­ç»ƒæ¥æ£€æµ‹è¶…å‡ºèŒƒå›´çš„æŸ¥è¯¢ï¼ˆé€šå¸¸æ ‡è®°ä¸ºä¸€ä¸ªå•ç‹¬çš„ç±»ï¼‰ï¼Œå¹¶å‘ŠçŸ¥å®¢æˆ·å®ƒå¯ä»¥å›ç­”å…³äºå“ªäº›ä¸»é¢˜çš„é—®é¢˜ã€‚
- en: '![Out of Scope Query](Images/nlpt_0802.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![è¶…å‡ºèŒƒå›´çš„æŸ¥è¯¢](Images/nlpt_0802.png)'
- en: Figure 8-2\. Three exchanges between a human (right) and a text-based assistant
    (left) for personal finance (courtesy of Stefan Larson et al.)
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾8-2\. äººç±»ï¼ˆå³ï¼‰å’ŒåŸºäºæ–‡æœ¬çš„åŠ©æ‰‹ï¼ˆå·¦ï¼‰ä¹‹é—´çš„ä¸‰æ¬¡äº¤æµï¼Œæ¶‰åŠä¸ªäººç†è´¢ï¼ˆç”±Stefan Larsonç­‰äººæä¾›ï¼‰
- en: As a baseline, weâ€™ve fine-tuned a BERT-base model that achieves around 94% accuracy
    on the CLINC150 dataset.^([1](ch08.xhtml#idm46238709482512)) This dataset includes
    22,500 in-scope queries across 150 intents and 10 domains like banking and travel,
    and also includes 1,200 out-of-scope queries that belong to an `oos` intent class.
    In practice we would also gather our own in-house dataset, but using public data
    is a great way to iterate quickly and generate preliminary results.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä½œä¸ºåŸºå‡†ï¼Œæˆ‘ä»¬å¾®è°ƒäº†ä¸€ä¸ªBERT-baseæ¨¡å‹ï¼Œåœ¨CLINC150æ•°æ®é›†ä¸Šè¾¾åˆ°äº†çº¦94%çš„å‡†ç¡®æ€§ã€‚è¿™ä¸ªæ•°æ®é›†åŒ…æ‹¬150ä¸ªæ„å›¾å’Œ10ä¸ªé¢†åŸŸï¼ˆå¦‚é“¶è¡Œå’Œæ—…è¡Œï¼‰ä¸­çš„22,500ä¸ªèŒƒå›´å†…æŸ¥è¯¢ï¼Œè¿˜åŒ…æ‹¬å±äº`oos`æ„å›¾ç±»åˆ«çš„1,200ä¸ªèŒƒå›´å¤–æŸ¥è¯¢ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬è¿˜ä¼šæ”¶é›†è‡ªå·±çš„å†…éƒ¨æ•°æ®é›†ï¼Œä½†ä½¿ç”¨å…¬å…±æ•°æ®æ˜¯å¿«é€Ÿè¿­ä»£å’Œç”Ÿæˆåˆæ­¥ç»“æœçš„å¥½æ–¹æ³•ã€‚
- en: 'To get started, letâ€™s download our fine-tuned model from the Hugging Face Hub
    and wrap it in a pipeline for text classification:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»Hugging Face Hubä¸‹è½½æˆ‘ä»¬å¾®è°ƒçš„æ¨¡å‹ï¼Œå¹¶å°†å…¶åŒ…è£…æˆæ–‡æœ¬åˆ†ç±»çš„ç®¡é“ï¼š
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that we have a pipeline, we can pass a query to get the predicted intent
    and confidence score from the model:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªç®¡é“ï¼Œæˆ‘ä»¬å¯ä»¥ä¼ é€’ä¸€ä¸ªæŸ¥è¯¢ä»¥ä»æ¨¡å‹è·å–é¢„æµ‹çš„æ„å›¾å’Œç½®ä¿¡åº¦åˆ†æ•°ï¼š
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Great, the `car_rental` intent makes sense. Letâ€™s now look at creating a benchmark
    that we can use to evaluate the performance of our baseline model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œ`car_rental`æ„å›¾æ˜¯æœ‰æ„ä¹‰çš„ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹åˆ›å»ºä¸€ä¸ªåŸºå‡†ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ¥è¯„ä¼°æˆ‘ä»¬åŸºå‡†æ¨¡å‹çš„æ€§èƒ½ã€‚
- en: Creating a Performance Benchmark
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ›å»ºæ€§èƒ½åŸºå‡†
- en: Like other machine learning models, deploying transformers in production environments
    involves a trade-off among several constraints, the most common being:^([2](ch08.xhtml#idm46238709422384))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å…¶ä»–æœºå™¨å­¦ä¹ æ¨¡å‹ä¸€æ ·ï¼Œåœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²transformersæ¶‰åŠåœ¨å‡ ä¸ªçº¦æŸæ¡ä»¶ä¹‹é—´è¿›è¡Œæƒè¡¡ï¼Œæœ€å¸¸è§çš„æ˜¯ï¼š
- en: '*Model performance*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ¨¡å‹æ€§èƒ½*'
- en: How well does our model perform on a well-crafted test set that reflects production
    data? This is especially important when the cost of making errors is large (and
    best mitigated with a human in the loop), or when we need to run inference on
    millions of examples and small improvements to the model metrics can translate
    into large gains in aggregate.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ¨¡å‹åœ¨åæ˜ ç”Ÿäº§æ•°æ®çš„ç²¾å¿ƒè®¾è®¡çš„æµ‹è¯•é›†ä¸Šè¡¨ç°å¦‚ä½•ï¼Ÿå½“é”™è¯¯çš„æˆæœ¬å¾ˆé«˜æ—¶ï¼ˆæœ€å¥½é€šè¿‡äººä¸ºå¹²é¢„æ¥å‡è½»ï¼‰ï¼Œæˆ–è€…å½“æˆ‘ä»¬éœ€è¦å¯¹æ•°ç™¾ä¸‡ä¸ªç¤ºä¾‹è¿›è¡Œæ¨æ–­ï¼Œå¹¶ä¸”æ¨¡å‹æŒ‡æ ‡çš„å°å¹…æ”¹è¿›å¯ä»¥è½¬åŒ–ä¸ºå¤§å¹…å¢ç›Šæ—¶ï¼Œè¿™ä¸€ç‚¹å°¤ä¸ºé‡è¦ã€‚
- en: '*Latency*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*å»¶è¿Ÿ*'
- en: How fast can our model deliver predictions? We usually care about latency in
    real-time environments that deal with a lot of traffic, like how Stack Overflow
    needed a classifier to quickly [detect unwelcome comments on the website](https://oreil.ly/cf7QX).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„æ¨¡å‹èƒ½å¤Ÿå¤šå¿«åœ°æä¾›é¢„æµ‹ï¼Ÿæˆ‘ä»¬é€šå¸¸å…³å¿ƒå®æ—¶ç¯å¢ƒä¸­çš„å»¶è¿Ÿï¼Œè¿™äº›ç¯å¢ƒå¤„ç†å¤§é‡æµé‡ï¼Œå°±åƒStack Overflowéœ€è¦ä¸€ä¸ªåˆ†ç±»å™¨æ¥å¿«é€Ÿ[æ£€æµ‹ç½‘ç«™ä¸Šä¸å—æ¬¢è¿çš„è¯„è®º](https://oreil.ly/cf7QX)ä¸€æ ·ã€‚
- en: '*Memory*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*å†…å­˜*'
- en: How can we deploy billion-parameter models like GPT-2 or T5 that require gigabytes
    of disk storage and RAM? Memory plays an especially important role in mobile or
    edge devices, where a model has to generate predictions without access to a powerful
    cloud server.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•éƒ¨ç½²åƒGPT-2æˆ–T5è¿™æ ·éœ€è¦å ç”¨å‡ GBç£ç›˜å­˜å‚¨å’Œå†…å­˜çš„ç™¾äº¿å‚æ•°æ¨¡å‹ï¼Ÿå†…å­˜åœ¨ç§»åŠ¨è®¾å¤‡æˆ–è¾¹ç¼˜è®¾å¤‡ä¸­æ‰®æ¼”ç€ç‰¹åˆ«é‡è¦çš„è§’è‰²ï¼Œå› ä¸ºæ¨¡å‹å¿…é¡»åœ¨æ²¡æœ‰å¼ºå¤§çš„äº‘æœåŠ¡å™¨çš„æƒ…å†µä¸‹ç”Ÿæˆé¢„æµ‹ã€‚
- en: 'Failing to address these constraints can have a negative impact on the user
    experience of your application. More commonly, it can lead to ballooning costs
    from running expensive cloud servers that may only need to handle a few requests.
    To explore how each of these constraints can be optimized with various compression
    techniques, letâ€™s begin by creating a simple benchmark that measures each quantity
    for a given pipeline and test set. A skeleton of what weâ€™ll need is given by the
    following class:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: æœªèƒ½è§£å†³è¿™äº›çº¦æŸæ¡ä»¶å¯èƒ½ä¼šå¯¹åº”ç”¨ç¨‹åºçš„ç”¨æˆ·ä½“éªŒäº§ç”Ÿè´Ÿé¢å½±å“ã€‚æ›´å¸¸è§çš„æ˜¯ï¼Œå¯èƒ½ä¼šå¯¼è‡´è¿è¡Œæ˜‚è´µçš„äº‘æœåŠ¡å™¨çš„æˆæœ¬æ¿€å¢ï¼Œè€Œè¿™äº›æœåŠ¡å™¨å¯èƒ½åªéœ€è¦å¤„ç†å°‘é‡è¯·æ±‚ã€‚ä¸ºäº†æ¢ç´¢å¦‚ä½•ä½¿ç”¨å„ç§å‹ç¼©æŠ€æœ¯ä¼˜åŒ–è¿™äº›çº¦æŸæ¡ä»¶ï¼Œè®©æˆ‘ä»¬ä»åˆ›å»ºä¸€ä¸ªç®€å•çš„åŸºå‡†å¼€å§‹ï¼Œè¯¥åŸºå‡†å¯ä»¥æµ‹é‡ç»™å®šç®¡é“å’Œæµ‹è¯•é›†çš„æ¯ä¸ªæ•°é‡ï¼š
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Weâ€™ve defined an `optim_type` parameter to keep track of the different optimization
    techniques that weâ€™ll cover in this chapter. Weâ€™ll use the `run_benchmark()` method
    to collect all the metrics in a dictionary, with keys given by `optim_type`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ª`optim_type`å‚æ•°ï¼Œä»¥è·Ÿè¸ªæˆ‘ä»¬åœ¨æœ¬ç« ä¸­å°†æ¶µç›–çš„ä¸åŒä¼˜åŒ–æŠ€æœ¯ã€‚æˆ‘ä»¬å°†ä½¿ç”¨`run_benchmark()`æ–¹æ³•å°†æ‰€æœ‰æŒ‡æ ‡æ”¶é›†åˆ°ä¸€ä¸ªå­—å…¸ä¸­ï¼Œé”®ç”±`optim_type`ç»™å‡ºã€‚
- en: 'Letâ€™s now put some flesh on the bones of this class by computing the model
    accuracy on the test set. First we need some data to test on, so letâ€™s download
    the CLINC150 dataset that was used to fine-tune our baseline model. We can get
    the dataset from the Hub with ![nlpt_pin01](Images/nlpt_pin01.png) Datasets as
    follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç°åœ¨é€šè¿‡åœ¨æµ‹è¯•é›†ä¸Šè®¡ç®—æ¨¡å‹çš„å‡†ç¡®æ€§æ¥ä¸ºè¿™ä¸ªç±»æ·»åŠ ä¸€äº›å…·ä½“å†…å®¹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€äº›æ•°æ®è¿›è¡Œæµ‹è¯•ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬ä¸‹è½½ç”¨äºå¾®è°ƒåŸºå‡†æ¨¡å‹çš„CLINC150æ•°æ®é›†ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä»Hubè·å–æ•°æ®é›†ï¼š![nlpt_pin01](Images/nlpt_pin01.png)ã€‚
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, the `plus` configuration refers to the subset that contains the out-of-scope
    training examples. Each example in the CLINC150 dataset consists of a query in
    the `text` column and its corresponding intent. Weâ€™ll use the test set to benchmark
    our models, so letâ€™s take a look at one of the datasetâ€™s examples:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œ`plus`é…ç½®æ˜¯æŒ‡åŒ…å«è¶…å‡ºèŒƒå›´çš„è®­ç»ƒç¤ºä¾‹çš„å­é›†ã€‚CLINC150æ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹éƒ½åŒ…æ‹¬`text`åˆ—ä¸­çš„æŸ¥è¯¢åŠå…¶å¯¹åº”çš„æ„å›¾ã€‚æˆ‘ä»¬å°†ä½¿ç”¨æµ‹è¯•é›†æ¥å¯¹æˆ‘ä»¬çš„æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬çœ‹ä¸€ä¸‹æ•°æ®é›†çš„ä¸€ä¸ªç¤ºä¾‹ï¼š
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The intents are provided as IDs, but we can easily get the mapping to strings
    (and vice versa) by accessing the `features` attribute of the dataset:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æ„å›¾ä»¥IDçš„å½¢å¼æä¾›ï¼Œä½†æˆ‘ä»¬å¯ä»¥é€šè¿‡è®¿é—®æ•°æ®é›†çš„`features`å±æ€§è½»æ¾è·å–åˆ°å­—ç¬¦ä¸²çš„æ˜ å°„ï¼ˆåä¹‹äº¦ç„¶ï¼‰ï¼š
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that we have a basic understanding of the contents in the CLINC150 dataset,
    letâ€™s implement the `compute_accuracy()` method of `PerformanceBenchmark`. Since
    the dataset is balanced across the intent classes, weâ€™ll use accuracy as our metric.
    We can load this metric with ![nlpt_pin01](Images/nlpt_pin01.png) Datasets as
    follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯¹CLINC150æ•°æ®é›†çš„å†…å®¹æœ‰äº†åŸºæœ¬çš„äº†è§£ï¼Œè®©æˆ‘ä»¬å®ç°`PerformanceBenchmark`çš„`compute_accuracy()`æ–¹æ³•ã€‚ç”±äºæ•°æ®é›†åœ¨æ„å›¾ç±»åˆ«ä¸Šæ˜¯å¹³è¡¡çš„ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‡†ç¡®æ€§ä½œä¸ºæˆ‘ä»¬çš„åº¦é‡æ ‡å‡†ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä½¿ç”¨![nlpt_pin01](Images/nlpt_pin01.png)æ•°æ®é›†åŠ è½½è¿™ä¸ªåº¦é‡æ ‡å‡†ï¼š
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The accuracy metric expects the predictions and references (i.e., the ground
    truth labels) to be integers. We can use the pipeline to extract the predictions
    from the `text` field and then use the `str2int()` method of our `intents` object
    to map each prediction to its corresponding ID. The following code collects all
    the predictions and labels in lists before returning the accuracy on the dataset.
    Letâ€™s also add it to our `Performâ€‹anâ ceâ€‹Benchmark` class:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å‡†ç¡®åº¦æŒ‡æ ‡æœŸæœ›é¢„æµ‹å’Œå‚è€ƒï¼ˆå³ï¼ŒçœŸå®æ ‡ç­¾ï¼‰æ˜¯æ•´æ•°ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç®¡é“ä»â€œtextâ€å­—æ®µä¸­æå–é¢„æµ‹ï¼Œç„¶åä½¿ç”¨æˆ‘ä»¬çš„â€œintentsâ€å¯¹è±¡çš„â€œstr2intï¼ˆï¼‰â€æ–¹æ³•å°†æ¯ä¸ªé¢„æµ‹æ˜ å°„åˆ°å…¶ç›¸åº”çš„IDã€‚ä»¥ä¸‹ä»£ç åœ¨è¿”å›æ•°æ®é›†çš„å‡†ç¡®åº¦ä¹‹å‰æ”¶é›†æ‰€æœ‰çš„é¢„æµ‹å’Œæ ‡ç­¾ã€‚è®©æˆ‘ä»¬ä¹Ÿå°†å…¶æ·»åŠ åˆ°æˆ‘ä»¬çš„â€œPerformanceBenchmarkâ€ç±»ä¸­ï¼š
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, letâ€™s compute the size of our model by using the `torch.save()` function
    from PyTorch to serialize the model to disk. Under the hood, `torch.save()` uses
    Pythonâ€™s `pickle` module and can be used to save anything from models to tensors
    to ordinary Python objects. In PyTorch, the recommended way to save a model is
    by using its `state_dict`, which is a Python dictionary that maps each layer in
    a model to its learnable parameters (i.e., weights and biases). Letâ€™s see what
    is stored in the `state_dict` of our baseline model:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨PyTorchçš„â€œtorch.saveï¼ˆï¼‰â€å‡½æ•°æ¥è®¡ç®—æˆ‘ä»¬æ¨¡å‹çš„å¤§å°ï¼Œå°†æ¨¡å‹åºåˆ—åŒ–åˆ°ç£ç›˜ä¸Šã€‚åœ¨å†…éƒ¨ï¼Œâ€œtorch.saveï¼ˆï¼‰â€ä½¿ç”¨Pythonçš„â€œpickleâ€æ¨¡å—ï¼Œå¯ä»¥ç”¨æ¥ä¿å­˜ä»æ¨¡å‹åˆ°å¼ é‡åˆ°æ™®é€šPythonå¯¹è±¡çš„ä»»ä½•ä¸œè¥¿ã€‚åœ¨PyTorchä¸­ï¼Œä¿å­˜æ¨¡å‹çš„æ¨èæ–¹å¼æ˜¯ä½¿ç”¨å®ƒçš„â€œstate_dictâ€ï¼Œè¿™æ˜¯ä¸€ä¸ªPythonå­—å…¸ï¼Œå°†æ¨¡å‹ä¸­çš„æ¯ä¸€å±‚æ˜ å°„åˆ°å®ƒçš„å¯å­¦ä¹ å‚æ•°ï¼ˆå³ï¼Œæƒé‡å’Œåç½®ï¼‰ã€‚è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬åŸºå‡†æ¨¡å‹çš„â€œstate_dictâ€ä¸­å­˜å‚¨äº†ä»€ä¹ˆï¼š
- en: '[PRE11]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can clearly see that each key/value pair corresponds to a specific layer
    and tensor in BERT. So if we save our model with:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°æ¯ä¸ªé”®/å€¼å¯¹å¯¹åº”äºBERTä¸­çš„ç‰¹å®šå±‚å’Œå¼ é‡ã€‚å› æ­¤ï¼Œå¦‚æœæˆ‘ä»¬ç”¨ä»¥ä¸‹æ–¹å¼ä¿å­˜æˆ‘ä»¬çš„æ¨¡å‹ï¼š
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'we can then use the `Path.stat()` function from Pythonâ€™s `pathlib` module to
    get information about the underlying files. In particular, `Path(â€‹"model.â€‹pt").â€‹stat().â€‹st_size`
    will give us the model size in bytes. Letâ€™s put this all together in the `comâ pute_â€‹size()`
    function and add it to `PerformanceBenchmark`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Pythonçš„â€œpathlibâ€æ¨¡å—ä¸­çš„â€œPath.statï¼ˆï¼‰â€å‡½æ•°æ¥è·å–æœ‰å…³åº•å±‚æ–‡ä»¶çš„ä¿¡æ¯ã€‚ç‰¹åˆ«æ˜¯ï¼Œâ€œPathï¼ˆ"model.â€‹pt"ï¼‰.â€‹statï¼ˆï¼‰.â€‹st_sizeâ€å°†ç»™å‡ºæ¨¡å‹çš„å¤§å°ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ã€‚è®©æˆ‘ä»¬å°†æ‰€æœ‰è¿™äº›æ”¾åœ¨â€œcompute_â€‹sizeï¼ˆï¼‰â€å‡½æ•°ä¸­ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ°â€œPerformanceBenchmarkâ€ä¸­ï¼š
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Finally letâ€™s implement the `time_pipeline()` function so that we can time the
    average latency per query. For this application, latency refers to the time it
    takes to feed a text query to the pipeline and return the predicted intent from
    the model. Under the hood the pipeline also tokenizes the text, but this is around
    one thousand times faster than generating the predictions and thus adds a negligible
    contribution to the overall latency. A simple way to measure the execution time
    of a code snippet is to use the `perf_counter()` function from Pythonâ€™s `time`
    module. This function has a better time resolution than the `time.time()` function
    and is well suited for getting precise results.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œè®©æˆ‘ä»¬å®ç°â€œtime_pipelineï¼ˆï¼‰â€å‡½æ•°ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è®¡ç®—æ¯ä¸ªæŸ¥è¯¢çš„å¹³å‡å»¶è¿Ÿæ—¶é—´ã€‚å¯¹äºè¿™ä¸ªåº”ç”¨ç¨‹åºï¼Œå»¶è¿Ÿæ—¶é—´æŒ‡çš„æ˜¯å°†æ–‡æœ¬æŸ¥è¯¢è¾“å…¥åˆ°ç®¡é“ä¸­å¹¶ä»æ¨¡å‹è¿”å›é¢„æµ‹æ„å›¾æ‰€éœ€çš„æ—¶é—´ã€‚åœ¨å†…éƒ¨ï¼Œç®¡é“è¿˜ä¼šå¯¹æ–‡æœ¬è¿›è¡Œæ ‡è®°åŒ–ï¼Œä½†è¿™æ¯”ç”Ÿæˆé¢„æµ‹å¿«äº†å¤§çº¦ä¸€åƒå€ï¼Œå› æ­¤å¯¹æ•´ä½“å»¶è¿Ÿæ—¶é—´çš„è´¡çŒ®å¯ä»¥å¿½ç•¥ä¸è®¡ã€‚è¡¡é‡ä»£ç ç‰‡æ®µçš„æ‰§è¡Œæ—¶é—´çš„ä¸€ä¸ªç®€å•æ–¹æ³•æ˜¯ä½¿ç”¨Pythonçš„â€œtimeâ€æ¨¡å—ä¸­çš„â€œperf_counterï¼ˆï¼‰â€å‡½æ•°ã€‚è¿™ä¸ªå‡½æ•°æ¯”â€œtime.timeï¼ˆï¼‰â€å‡½æ•°å…·æœ‰æ›´å¥½çš„æ—¶é—´åˆ†è¾¨ç‡ï¼Œéå¸¸é€‚åˆè·å–ç²¾ç¡®çš„ç»“æœã€‚
- en: 'We can use `perf_counter()` to time our pipeline by passing our test query
    and calculating the time difference in milliseconds between the start and end:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨â€œperf_counterï¼ˆï¼‰â€é€šè¿‡ä¼ é€’æˆ‘ä»¬çš„æµ‹è¯•æŸ¥è¯¢æ¥è®¡æ—¶æˆ‘ä»¬çš„ç®¡é“ï¼Œå¹¶è®¡ç®—å¼€å§‹å’Œç»“æŸä¹‹é—´çš„æ¯«ç§’æ—¶é—´å·®ï¼š
- en: '[PRE15]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'These results exhibit quite some spread in the latencies and suggest that timing
    a single pass through the pipeline can give wildly different results each time
    we run the code. So instead, weâ€™ll collect the latencies over many runs and then
    use the resulting distribution to calculate the mean and standard deviation, which
    will give us an idea about the spread in values. The following code does what
    we need and includes a phase to warm up the CPU before performing the actual timed
    run:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›ç»“æœå±•ç¤ºäº†å»¶è¿Ÿæ—¶é—´çš„ç›¸å½“å¤§çš„å·®å¼‚ï¼Œå¹¶ä¸”è¡¨æ˜é€šè¿‡ç®¡é“çš„å•æ¬¡è®¡æ—¶å¯èƒ½æ¯æ¬¡è¿è¡Œä»£ç æ—¶éƒ½ä¼šå¾—åˆ°å®Œå…¨ä¸åŒçš„ç»“æœã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†æ”¶é›†å¤šæ¬¡è¿è¡Œçš„å»¶è¿Ÿæ—¶é—´ï¼Œç„¶åä½¿ç”¨å¾—åˆ°çš„åˆ†å¸ƒæ¥è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼Œè¿™å°†è®©æˆ‘ä»¬å¯¹æ•°å€¼çš„å·®å¼‚æœ‰ä¸€ä¸ªæ¦‚å¿µã€‚ä»¥ä¸‹ä»£ç å®ç°äº†æˆ‘ä»¬éœ€è¦çš„åŠŸèƒ½ï¼Œå¹¶åŒ…æ‹¬äº†åœ¨æ‰§è¡Œå®é™…è®¡æ—¶è¿è¡Œä¹‹å‰é¢„çƒ­CPUçš„é˜¶æ®µï¼š
- en: '[PRE17]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To keeps things simple, weâ€™ll use the same `query` value to benchmark all our
    models. In general, the latency will depend on the query length, and a good practice
    is to benchmark your models with queries that theyâ€™re likely to encounter in production
    environments.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç®€åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ç›¸åŒçš„â€œqueryâ€å€¼æ¥å¯¹æˆ‘ä»¬æ‰€æœ‰çš„æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå»¶è¿Ÿæ—¶é—´å°†å–å†³äºæŸ¥è¯¢é•¿åº¦ï¼Œä¸€ä¸ªå¥½çš„åšæ³•æ˜¯ä½¿ç”¨æ¨¡å‹å¯èƒ½åœ¨ç”Ÿäº§ç¯å¢ƒä¸­é‡åˆ°çš„æŸ¥è¯¢æ¥å¯¹æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ã€‚
- en: 'Now that our `PerformanceBenchmark` class is complete, letâ€™s give it a spin!
    Letâ€™s start by benchmarking our BERT baseline. For the baseline model, we just
    need to pass the pipeline and the dataset we wish to perform the benchmark on.
    Weâ€™ll collect the results in the `perf_metrics` dictionary to keep track of each
    modelâ€™s performance:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„â€œPerformanceBenchmarkâ€ç±»å·²ç»å®Œæˆï¼Œè®©æˆ‘ä»¬æ¥è¯•ä¸€è¯•å§ï¼è®©æˆ‘ä»¬ä»å¯¹æˆ‘ä»¬çš„BERTåŸºå‡†æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•å¼€å§‹ã€‚å¯¹äºåŸºå‡†æ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€è¦ä¼ é€’ç®¡é“å’Œæˆ‘ä»¬å¸Œæœ›è¿›è¡ŒåŸºå‡†æµ‹è¯•çš„æ•°æ®é›†ã€‚æˆ‘ä»¬å°†åœ¨â€œperf_metricsâ€å­—å…¸ä¸­æ”¶é›†ç»“æœï¼Œä»¥è·Ÿè¸ªæ¯ä¸ªæ¨¡å‹çš„æ€§èƒ½ï¼š
- en: '[PRE18]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that we have a reference point, letâ€™s look at our first compression technique:
    knowledge distillation.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ªå‚è€ƒç‚¹ï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªå‹ç¼©æŠ€æœ¯ï¼šçŸ¥è¯†è’¸é¦ã€‚
- en: Note
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: The average latency values will differ depending on what type of hardware you
    are running on. For example, you can usually get better performance by running
    inference on a GPU since it enables batch processing. For the purposes of this
    chapter, whatâ€™s important is the relative difference in latencies between models.
    Once we have determined the best-performing model, we can then explore different
    backends to reduce the absolute latency if needed.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¹³å‡å»¶è¿Ÿå€¼å°†å–å†³äºæ‚¨æ‰€è¿è¡Œçš„ç¡¬ä»¶ç±»å‹ã€‚ä¾‹å¦‚ï¼Œé€šå¸¸å¯ä»¥é€šè¿‡åœ¨GPUä¸Šè¿è¡Œæ¨æ–­æ¥è·å¾—æ›´å¥½çš„æ€§èƒ½ï¼Œå› ä¸ºå®ƒå¯ä»¥å®ç°æ‰¹å¤„ç†ã€‚å¯¹äºæœ¬ç« çš„ç›®çš„ï¼Œé‡è¦çš„æ˜¯æ¨¡å‹ä¹‹é—´å»¶è¿Ÿæ—¶é—´çš„ç›¸å¯¹å·®å¼‚ã€‚ä¸€æ—¦ç¡®å®šäº†æ€§èƒ½æœ€ä½³çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥æ¢ç´¢ä¸åŒçš„åç«¯æ¥å‡å°‘ç»å¯¹å»¶è¿Ÿæ—¶é—´ï¼ˆå¦‚æœéœ€è¦ï¼‰ã€‚
- en: Making Models Smaller via Knowledge Distillation
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: é€šè¿‡çŸ¥è¯†è’¸é¦ä½¿æ¨¡å‹å˜å¾—æ›´å°
- en: Knowledge distillation is a general-purpose method for training a smaller *student*
    model to mimic the behavior of a slower, larger, but better-performing *teacher*.
    Originally introduced in 2006 in the context of ensemble models,^([3](ch08.xhtml#idm46238708497152))
    it was later popularized in a famous 2015 paper that generalized the method to
    deep neural networks and applied it to image classification and automatic speech
    recognition.^([4](ch08.xhtml#idm46238708494384))
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ¥è¯†è’¸é¦æ˜¯ä¸€ç§é€šç”¨æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒä¸€ä¸ªè¾ƒå°çš„â€œå­¦ç”Ÿâ€æ¨¡å‹æ¥æ¨¡ä»¿é€Ÿåº¦è¾ƒæ…¢ã€æ›´å¤§ä½†æ€§èƒ½æ›´å¥½çš„â€œæ•™å¸ˆâ€æ¨¡å‹çš„è¡Œä¸ºã€‚æœ€åˆæ˜¯åœ¨2006å¹´åœ¨é›†æˆæ¨¡å‹çš„èƒŒæ™¯ä¸‹å¼•å…¥çš„ï¼Œåæ¥åœ¨ä¸€ç¯‡è‘—åçš„2015å¹´è®ºæ–‡ä¸­å°†è¯¥æ–¹æ³•æ¨å¹¿åˆ°æ·±åº¦ç¥ç»ç½‘ç»œï¼Œå¹¶å°†å…¶åº”ç”¨äºå›¾åƒåˆ†ç±»å’Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ã€‚
- en: Given the trend toward pretraining language models with ever-increasing parameter
    counts (the largest at the time of writing having over one trillion parameters),^([5](ch08.xhtml#idm46238708466160))
    knowledge distillation has also become a popular strategy to compress these huge
    models and make them more suitable for building practical applications.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: é‰´äºé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹å‚æ•°æ•°é‡ä¸æ–­å¢åŠ çš„è¶‹åŠ¿ï¼ˆæ’°å†™æ—¶æœ€å¤§çš„æ¨¡å‹å‚æ•°è¶…è¿‡ä¸€ä¸‡äº¿ï¼‰ï¼ŒçŸ¥è¯†è’¸é¦ä¹Ÿæˆä¸ºå‹ç¼©è¿™äº›åºå¤§æ¨¡å‹å¹¶ä½¿å…¶æ›´é€‚åˆæ„å»ºå®é™…åº”ç”¨çš„æµè¡Œç­–ç•¥ã€‚
- en: Knowledge Distillation for Fine-Tuning
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å¾®è°ƒçš„çŸ¥è¯†è’¸é¦
- en: So how is knowledge actually â€œdistilledâ€ or transferred from the teacher to
    the student during training? For supervised tasks like fine-tuning, the main idea
    is to augment the ground truth labels with a distribution of â€œsoft probabilitiesâ€
    from the teacher which provide complementary information for the student to learn
    from. For example, if our BERT-base classifier assigns high probabilities to multiple
    intents, then this could be a sign that these intents lie close to each other
    in the feature space. By training the student to mimic these probabilities, the
    goal is to distill some of this â€œdark knowledgeâ€^([6](ch08.xhtml#idm46238708457120))
    that the teacher has learnedâ€”that is, knowledge that is not available from the
    labels alone.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼ŒçŸ¥è¯†å®é™…ä¸Šæ˜¯å¦‚ä½•ä»æ•™å¸ˆä¼ é€’ç»™å­¦ç”Ÿçš„å‘¢ï¼Ÿå¯¹äºå¾®è°ƒç­‰ç›‘ç£ä»»åŠ¡ï¼Œä¸»è¦æ€æƒ³æ˜¯ç”¨æ•™å¸ˆçš„â€œè½¯æ¦‚ç‡â€åˆ†å¸ƒæ¥å¢å¼ºåœ°é¢çœŸå®æ ‡ç­¾ï¼Œä¸ºå­¦ç”Ÿæä¾›è¡¥å……ä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬çš„BERT-baseåˆ†ç±»å™¨ä¸ºå¤šä¸ªæ„å›¾åˆ†é…é«˜æ¦‚ç‡ï¼Œé‚£ä¹ˆè¿™å¯èƒ½è¡¨æ˜è¿™äº›æ„å›¾åœ¨ç‰¹å¾ç©ºé—´ä¸­ç›¸äº’é è¿‘ã€‚é€šè¿‡è®­ç»ƒå­¦ç”Ÿæ¨¡ä»¿è¿™äº›æ¦‚ç‡ï¼Œç›®æ ‡æ˜¯è’¸é¦æ•™å¸ˆå­¦åˆ°çš„ä¸€äº›â€œæš—çŸ¥è¯†â€â€”â€”ä¹Ÿå°±æ˜¯ï¼Œä»…ä»æ ‡ç­¾ä¸­æ— æ³•è·å¾—çš„çŸ¥è¯†ã€‚
- en: 'Mathematically, the way this works is as follows. Suppose we feed an input
    sequence *x* to the teacher to generate a vector of logits <math alttext="bold
    z left-parenthesis x right-parenthesis"><mrow><mi>ğ³</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></math> = [ <math alttext="z 1 left-parenthesis x right-parenthesis
    comma ellipsis comma z Subscript upper N Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>z</mi>
    <mn>1</mn></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>z</mi> <mi>N</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    ]. We can convert these logits into probabilities by applying a softmax function:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ•°å­¦ä¸Šè®²ï¼Œè¿™æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚å‡è®¾æˆ‘ä»¬å°†è¾“å…¥åºåˆ—*x*æä¾›ç»™æ•™å¸ˆï¼Œä»¥ç”Ÿæˆä¸€ä¸ªå¯¹æ•°å‘é‡<math alttext="bold z left-parenthesis
    x right-parenthesis"><mrow><mi>ğ³</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    = [ <math alttext="z 1 left-parenthesis x right-parenthesis comma ellipsis comma
    z Subscript upper N Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>z</mi>
    <mn>1</mn></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>z</mi> <mi>N</mi></sub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    ]ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åº”ç”¨softmaxå‡½æ•°å°†è¿™äº›å¯¹æ•°è½¬æ¢ä¸ºæ¦‚ç‡ï¼š
- en: <math alttext="StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis
    x right-parenthesis right-parenthesis Over sigma-summation Underscript j Endscripts
    exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    right-parenthesis EndFraction" display="block"><mrow><mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow>
    <mrow><msub><mo>âˆ‘</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis
    x right-parenthesis right-parenthesis Over sigma-summation Underscript j Endscripts
    exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    right-parenthesis EndFraction" display="block"><mrow><mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow>
    <mrow><msub><mo>âˆ‘</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: This isnâ€™t quite what we want, though, because in many cases the teacher will
    assign a high probability to one class, with all other class probabilities close
    to zero. When that happens, the teacher doesnâ€™t provide much additional information
    beyond the ground truth labels, so instead we â€œsoftenâ€ the probabilities by scaling
    the logits with a temperature hyperparameter *T* before applying the softmax:^([7](ch08.xhtml#idm46238708428448))
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œè¿™å¹¶ä¸æ˜¯æˆ‘ä»¬æƒ³è¦çš„ï¼Œå› ä¸ºåœ¨è®¸å¤šæƒ…å†µä¸‹ï¼Œæ•™å¸ˆä¼šä¸ºä¸€ä¸ªç±»åˆ†é…é«˜æ¦‚ç‡ï¼Œè€Œå…¶ä»–ç±»çš„æ¦‚ç‡æ¥è¿‘äºé›¶ã€‚å½“å‘ç”Ÿè¿™ç§æƒ…å†µæ—¶ï¼Œæ•™å¸ˆé™¤äº†åœ°é¢çœŸå®æ ‡ç­¾å¤–å¹¶æ²¡æœ‰æä¾›å¤ªå¤šé¢å¤–ä¿¡æ¯ï¼Œå› æ­¤æˆ‘ä»¬ä¼šåœ¨åº”ç”¨softmaxä¹‹å‰ï¼Œé€šè¿‡ä¸€ä¸ªæ¸©åº¦è¶…å‚æ•°*T*æ¥ç¼©æ”¾å¯¹æ•°ï¼Œä»è€Œâ€œè½¯åŒ–â€æ¦‚ç‡ã€‚
- en: <math alttext="p Subscript i Baseline left-parenthesis x right-parenthesis equals
    StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    slash upper T right-parenthesis Over sigma-summation Underscript j Endscripts
    exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    slash upper T right-parenthesis EndFraction" display="block"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mo
    form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi> <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow>
    <mrow><msub><mo>âˆ‘</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="p Subscript i Baseline left-parenthesis x right-parenthesis equals
    StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    slash upper T right-parenthesis Over sigma-summation Underscript j Endscripts
    exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    slash upper T right-parenthesis EndFraction" display="block"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mo
    form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi> <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow>
    <mrow><msub><mo>âˆ‘</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: As shown in [FigureÂ 8-3](#soft-probs), higher values of *T* produce a softer
    probability distribution over the classes and reveal much more information about
    the decision boundary that the teacher has learned for each training example.
    When <math alttext="upper T equals 1"><mrow><mi>T</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    we recover the original softmax distribution.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[å›¾8-3](#soft-probs)æ‰€ç¤ºï¼Œ*T*çš„å€¼è¶Šé«˜ï¼Œç±»åˆ«ä¸Šçš„è½¯åŒ–æ¦‚ç‡åˆ†å¸ƒå°±è¶Šè½¯ï¼Œå¯ä»¥æ›´å¤šåœ°æ­ç¤ºè€å¸ˆå¯¹æ¯ä¸ªè®­ç»ƒç¤ºä¾‹å­¦ä¹ çš„å†³ç­–è¾¹ç•Œã€‚å½“<math
    alttext="upper T equals 1"><mrow><mi>T</mi> <mo>=</mo> <mn>1</mn></mrow></math>æ—¶ï¼Œæˆ‘ä»¬æ¢å¤äº†åŸå§‹çš„softmaxåˆ†å¸ƒã€‚
- en: '![Soft Probabilities](Images/nlpt_0803.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![è½¯æ¦‚ç‡](Images/nlpt_0803.png)'
- en: Figure 8-3\. Comparison of a hard label that is one-hot encoded (left), softmax
    probabilities (middle), and softened class probabilities (right)
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾8-3ã€‚ä¸€ä¸ªä½¿ç”¨one-hotç¼–ç çš„ç¡¬æ ‡ç­¾ï¼ˆå·¦ï¼‰ã€softmaxæ¦‚ç‡ï¼ˆä¸­ï¼‰å’Œè½¯åŒ–ç±»åˆ«æ¦‚ç‡ï¼ˆå³ï¼‰çš„æ¯”è¾ƒã€‚
- en: 'Since the student also produces softened probabilities <math alttext="q Subscript
    i Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>i</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> of its own, we can
    use the [Kullbackâ€“Leibler (KL)](https://oreil.ly/8nKQG) divergence to measure
    the difference between the two probability distributions:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºå­¦ç”Ÿè¿˜äº§ç”Ÿäº†è‡ªå·±çš„è½¯åŒ–æ¦‚ç‡<math alttext="q Subscript i Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨[Kullback-Leiblerï¼ˆKLï¼‰](https://oreil.ly/8nKQG)æ•£åº¦æ¥è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼š
- en: <math alttext="upper D Subscript upper K upper L Baseline left-parenthesis p
    comma q right-parenthesis equals sigma-summation Underscript i Endscripts p Subscript
    i Baseline left-parenthesis x right-parenthesis log StartFraction p Subscript
    i Baseline left-parenthesis x right-parenthesis Over q Subscript i Baseline left-parenthesis
    x right-parenthesis EndFraction" display="block"><mrow><msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo> <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munder><mo>âˆ‘</mo> <mi>i</mi></munder> <msub><mi>p</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mfrac><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>q</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper D Subscript upper K upper L Baseline left-parenthesis p
    comma q right-parenthesis equals sigma-summation Underscript i Endscripts p Subscript
    i Baseline left-parenthesis x right-parenthesis log StartFraction p Subscript
    i Baseline left-parenthesis x right-parenthesis Over q Subscript i Baseline left-parenthesis
    x right-parenthesis EndFraction" display="block"><mrow><msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo> <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munder><mo>âˆ‘</mo> <mi>i</mi></munder> <msub><mi>p</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mfrac><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>q</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: 'With the KL divergence we can calculate how much is lost when we approximate
    the probability distribution of the teacher with the student. This allows us to
    define a knowledge distillation loss:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡KLæ•£åº¦ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—å½“æˆ‘ä»¬ç”¨å­¦ç”Ÿæ¥è¿‘ä¼¼è€å¸ˆçš„æ¦‚ç‡åˆ†å¸ƒæ—¶æŸå¤±äº†å¤šå°‘ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå®šä¹‰çŸ¥è¯†è’¸é¦æŸå¤±ï¼š
- en: <math alttext="upper L Subscript upper K upper D Baseline equals upper T squared
    upper D Subscript upper K upper L" display="block"><mrow><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub>
    <mo>=</mo> <msup><mi>T</mi> <mn>2</mn></msup> <msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub></mrow></math>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L Subscript upper K upper D Baseline equals upper T squared
    upper D Subscript upper K upper L" display="block"><mrow><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub>
    <mo>=</mo> <msup><mi>T</mi> <mn>2</mn></msup> <msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub></mrow></math>
- en: 'where <math alttext="upper T squared"><msup><mi>T</mi> <mn>2</mn></msup></math>
    is a normalization factor to account for the fact that the magnitude of the gradients
    produced by soft labels scales as <math alttext="1 slash upper T squared"><mrow><mn>1</mn>
    <mo>/</mo> <msup><mi>T</mi> <mn>2</mn></msup></mrow></math> . For classification
    tasks, the student loss is then a weighted average of the distillation loss with
    the usual cross-entropy loss <math alttext="upper L Subscript upper C upper E"><msub><mi>L</mi>
    <mrow><mi>C</mi><mi>E</mi></mrow></msub></math> of the ground truth labels:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­<math alttext="upper T squared"><msup><mi>T</mi> <mn>2</mn></msup></math>æ˜¯ä¸€ä¸ªå½’ä¸€åŒ–å› å­ï¼Œç”¨äºè€ƒè™‘è½¯æ ‡ç­¾äº§ç”Ÿçš„æ¢¯åº¦å¤§å°æŒ‰<math
    alttext="1 slash upper T squared"><mrow><mn>1</mn> <mo>/</mo> <msup><mi>T</mi>
    <mn>2</mn></msup></mrow></math>ç¼©æ”¾çš„äº‹å®ã€‚å¯¹äºåˆ†ç±»ä»»åŠ¡ï¼Œå­¦ç”Ÿçš„æŸå¤±æ˜¯è’¸é¦æŸå¤±å’Œåœ°é¢çœŸå®æ ‡ç­¾çš„äº¤å‰ç†µæŸå¤±<math alttext="upper
    L Subscript upper C upper E"><msub><mi>L</mi> <mrow><mi>C</mi><mi>E</mi></mrow></msub></math>çš„åŠ æƒå¹³å‡ï¼š
- en: <math alttext="upper L Subscript normal s normal t normal u normal d normal
    e normal n normal t Baseline equals alpha upper L Subscript upper C upper E Baseline
    plus left-parenthesis 1 minus alpha right-parenthesis upper L Subscript upper
    K upper D" display="block"><mrow><msub><mi>L</mi> <mi>student</mi></msub> <mo>=</mo>
    <mi>Î±</mi> <msub><mi>L</mi> <mrow><mi>C</mi><mi>E</mi></mrow></msub> <mo>+</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>Î±</mi> <mo>)</mo></mrow> <msub><mi>L</mi>
    <mrow><mi>K</mi><mi>D</mi></mrow></msub></mrow></math>
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L Subscript normal s normal t normal u normal d normal
    e normal n normal t Baseline equals alpha upper L Subscript upper C upper E Baseline
    plus left-parenthesis 1 minus alpha right-parenthesis upper L Subscript upper
    K upper D" display="block"><mrow><msub><mi>L</mi> <mi>student</mi></msub> <mo>=</mo>
    <mi>Î±</mi> <msub><mi>L</mi> <mrow><mi>C</mi><mi>E</mi></mrow></msub> <mo>+</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>Î±</mi> <mo>)</mo></mrow> <msub><mi>L</mi>
    <mrow><mi>K</mi><mi>D</mi></mrow></msub></mrow></math>
- en: where <math alttext="alpha"><mi>Î±</mi></math> is a hyperparameter that controls
    the relative strength of each loss. A diagram of the whole process is shown in
    [FigureÂ 8-4](#kd); the temperature is set to 1 at inference time to recover the
    standard softmax probabilities.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­<math alttext="alpha"><mi>Î±</mi></math>æ˜¯ä¸€ä¸ªæ§åˆ¶æ¯ä¸ªæŸå¤±ç›¸å¯¹å¼ºåº¦çš„è¶…å‚æ•°ã€‚æ•´ä¸ªè¿‡ç¨‹çš„å›¾è¡¨å¦‚[å›¾8-4](#kd)æ‰€ç¤ºï¼›åœ¨æ¨æ–­æ—¶ï¼Œæ¸©åº¦è¢«è®¾ç½®ä¸º1ï¼Œä»¥æ¢å¤æ ‡å‡†çš„softmaxæ¦‚ç‡ã€‚
- en: '![Knowledge distillation](Images/nlpt_0804.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![çŸ¥è¯†è’¸é¦](Images/nlpt_0804.png)'
- en: Figure 8-4\. The knowledge distillation process
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾8-4ã€‚çŸ¥è¯†è’¸é¦è¿‡ç¨‹
- en: Knowledge Distillation for Pretraining
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é¢„è®­ç»ƒçš„çŸ¥è¯†è’¸é¦
- en: 'Knowledge distillation can also be used during pretraining to create a general-purpose
    student that can be subsequently fine-tuned on downstream tasks. In this case,
    the teacher is a pretrained language model like BERT, which transfers its knowledge
    about masked language modeling to the student. For example, in the DistilBERT
    paper,^([8](ch08.xhtml#idm46238708343392)) the masked language modeling loss <math
    alttext="upper L Subscript m l m"><msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub></math>
    is augmented with a term from knowledge distillation and a cosine embedding loss
    <math alttext="upper L Subscript c o s Baseline equals 1 minus cosine left-parenthesis
    h Subscript s Baseline comma h Subscript t Baseline right-parenthesis"><mrow><msub><mi>L</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub> <mo>=</mo> <mn>1</mn> <mo>-</mo>
    <mo form="prefix">cos</mo> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>s</mi></msub>
    <mo>,</mo> <msub><mi>h</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
    to align the directions of the hidden state vectors between the teacher and student:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ¥è¯†è’¸é¦ä¹Ÿå¯ä»¥åœ¨é¢„è®­ç»ƒæœŸé—´ä½¿ç”¨ï¼Œä»¥åˆ›å»ºä¸€ä¸ªé€šç”¨çš„å­¦ç”Ÿæ¨¡å‹ï¼Œéšåå¯ä»¥åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ•™å¸ˆæ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå¦‚BERTï¼Œå®ƒå°†å…¶å…³äºæ©ç è¯­è¨€å»ºæ¨¡çš„çŸ¥è¯†è½¬ç§»åˆ°å­¦ç”Ÿèº«ä¸Šã€‚ä¾‹å¦‚ï¼Œåœ¨DistilBERTè®ºæ–‡ä¸­ï¼Œ^([8](ch08.xhtml#idm46238708343392))æ©ç è¯­è¨€å»ºæ¨¡æŸå¤±<math
    alttext="upper L Subscript m l m"><msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub></math>è¢«çŸ¥è¯†è’¸é¦çš„ä¸€ä¸ªé¡¹å’Œä½™å¼¦åµŒå…¥æŸå¤±<math
    alttext="upper L Subscript c o s Baseline equals 1 minus cosine left-parenthesis
    h Subscript s Baseline comma h Subscript t Baseline right-parenthesis"><mrow><msub><mi>L</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub> <mo>=</mo> <mn>1</mn> <mo>-</mo>
    <mo form="prefix">cos</mo> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>s</mi></msub>
    <mo>,</mo> <msub><mi>h</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>æ¥å¯¹é½æ•™å¸ˆå’Œå­¦ç”Ÿä¹‹é—´çš„éšè—çŠ¶æ€å‘é‡çš„æ–¹å‘ï¼š
- en: <math alttext="upper L Subscript normal upper D normal i normal s normal t normal
    i normal l normal upper B normal upper E normal upper R normal upper T Baseline
    equals alpha upper L Subscript m l m Baseline plus beta upper L Subscript upper
    K upper D Baseline plus gamma upper L Subscript c o s" display="block"><mrow><msub><mi>L</mi>
    <mi>DistilBERT</mi></msub> <mo>=</mo> <mi>Î±</mi> <msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub>
    <mo>+</mo> <mi>Î²</mi> <msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub>
    <mo>+</mo> <mi>Î³</mi> <msub><mi>L</mi> <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L Subscript normal upper D normal i normal s normal t normal
    i normal l normal upper B normal upper E normal upper R normal upper T Baseline
    equals alpha upper L Subscript m l m Baseline plus beta upper L Subscript upper
    K upper D Baseline plus gamma upper L Subscript c o s" display="block"><mrow><msub><mi>L</mi>
    <mi>DistilBERT</mi></msub> <mo>=</mo> <mi>Î±</mi> <msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub>
    <mo>+</mo> <mi>Î²</mi> <msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub>
    <mo>+</mo> <mi>Î³</mi> <msub><mi>L</mi> <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>
- en: Since we already have a fine-tuned BERT-base model, letâ€™s see how we can use
    knowledge distillation to fine-tune a smaller and faster model. To do that weâ€™ll
    need a way to augment the cross-entropy loss with an <math alttext="upper L Subscript
    upper K upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math>
    term. Fortunately we can do this by creating our own trainer!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬å·²ç»æœ‰äº†ä¸€ä¸ªç»è¿‡ç²¾ç»†è°ƒæ•´çš„BERT-baseæ¨¡å‹ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä½¿ç”¨çŸ¥è¯†è’¸é¦æ¥å¯¹ä¸€ä¸ªæ›´å°æ›´å¿«çš„æ¨¡å‹è¿›è¡Œç²¾ç»†è°ƒæ•´ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•æ¥å°†äº¤å‰ç†µæŸå¤±ä¸<math
    alttext="upper L Subscript upper K upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math>é¡¹ç›¸ç»“åˆã€‚å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡åˆ›å»ºè‡ªå·±çš„è®­ç»ƒå™¨æ¥å®ç°è¿™ä¸€ç‚¹ï¼
- en: Creating a Knowledge Distillation Trainer
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åˆ›å»ºçŸ¥è¯†è’¸é¦è®­ç»ƒå™¨
- en: 'To implement knowledge distillation we need to add a few things to the `Trainer`
    base class:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å®ç°çŸ¥è¯†è’¸é¦ï¼Œæˆ‘ä»¬éœ€è¦å‘`Trainer`åŸºç±»æ·»åŠ ä¸€äº›å†…å®¹ï¼š
- en: The new hyperparameters <math alttext="alpha"><mi>Î±</mi></math> and *T*, which
    control the relative weight of the distillation loss and how much the probability
    distribution of the labels should be smoothed
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ–°çš„è¶…å‚æ•°<math alttext="alpha"><mi>Î±</mi></math>å’Œ*T*ï¼Œå®ƒä»¬æ§åˆ¶è’¸é¦æŸå¤±çš„ç›¸å¯¹æƒé‡ä»¥åŠæ ‡ç­¾çš„æ¦‚ç‡åˆ†å¸ƒåº”è¯¥è¢«å¹³æ»‘çš„ç¨‹åº¦
- en: The fine-tuned teacher model, which in our case is BERT-base
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ•™å¸ˆæ¨¡å‹ï¼Œæˆ‘ä»¬çš„æƒ…å†µä¸‹æ˜¯BERT-base
- en: A new loss function that combines the cross-entropy loss with the knowledge
    distillation loss
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç»“åˆäº¤å‰ç†µæŸå¤±å’ŒçŸ¥è¯†è’¸é¦æŸå¤±çš„æ–°æŸå¤±å‡½æ•°
- en: 'Adding the new hyperparameters is quite simple, since we just need to subclass
    `TrainingArguments` and include them as new attributes:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æ·»åŠ æ–°çš„è¶…å‚æ•°éå¸¸ç®€å•ï¼Œå› ä¸ºæˆ‘ä»¬åªéœ€è¦å¯¹`TrainingArguments`è¿›è¡Œå­ç±»åŒ–ï¼Œå¹¶å°†å®ƒä»¬åŒ…å«ä¸ºæ–°çš„å±æ€§ï¼š
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For the trainer itself, we need a new loss function. The way to implement this
    is by subclassing `Trainer` and overriding the `compute_loss()` method to include
    the knowledge distillation loss term <math alttext="upper L Subscript upper K
    upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math> :'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè®­ç»ƒå™¨æœ¬èº«ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ–°çš„æŸå¤±å‡½æ•°ã€‚å®ç°è¿™ä¸€ç‚¹çš„æ–¹æ³•æ˜¯é€šè¿‡å¯¹`Trainer`è¿›è¡Œå­ç±»åŒ–ï¼Œå¹¶è¦†ç›–`compute_loss()`æ–¹æ³•ï¼Œä»¥åŒ…æ‹¬çŸ¥è¯†è’¸é¦æŸå¤±é¡¹<math
    alttext="upper L Subscript upper K upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math>ï¼š
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Letâ€™s unpack this code a bit. When we instantiate `DistillationTrainer` we pass
    a `teacher_model` argument with a teacher that has already been fine-tuned on
    our task. Next, in the `compute_loss()` method we extract the logits from the
    student and teacher, scale them by the temperature, and then normalize them with
    a softmax before passing them to PyTorchâ€™s `nn.KLDivLoss()` function for computing
    the KL divergence. One quirk with `nn.KLDivLoss()` is that it expects the inputs
    in the form of log probabilities and the labels as normal probabilities. Thatâ€™s
    why weâ€™ve used the `F.log_softmax()` function to normalize the studentâ€™s logits,
    while the teacherâ€™s logits are converted to probabilities with a standard softmax.
    The `reduction=batchmean` argument in `nn.KLDivLoss()` specifies that we average
    the losses over the batch dimension.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è§£å¼€ä¸€ä¸‹è¿™æ®µä»£ç ã€‚å½“æˆ‘ä»¬å®ä¾‹åŒ–`DistillationTrainer`æ—¶ï¼Œæˆ‘ä»¬ä¼ é€’äº†ä¸€ä¸ªå·²ç»åœ¨æˆ‘ä»¬çš„ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¾®è°ƒçš„è€å¸ˆæ¨¡å‹ã€‚æ¥ä¸‹æ¥ï¼Œåœ¨`compute_loss()`æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬ä»å­¦ç”Ÿå’Œè€å¸ˆé‚£é‡Œæå–logitsï¼Œé€šè¿‡æ¸©åº¦å¯¹å®ƒä»¬è¿›è¡Œç¼©æ”¾ï¼Œç„¶ååœ¨ä¼ é€’ç»™PyTorchçš„`nn.KLDivLoss()`å‡½æ•°ä¹‹å‰ï¼Œä½¿ç”¨softmaxå¯¹å®ƒä»¬è¿›è¡Œå½’ä¸€åŒ–ä»¥è®¡ç®—KLæ•£åº¦ã€‚`nn.KLDivLoss()`çš„ä¸€ä¸ªæ€ªç™–æ˜¯ï¼Œå®ƒæœŸæœ›è¾“å…¥ä»¥å¯¹æ•°æ¦‚ç‡çš„å½¢å¼ï¼Œæ ‡ç­¾ä»¥æ­£å¸¸æ¦‚ç‡çš„å½¢å¼ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä½¿ç”¨`F.log_softmax()`å‡½æ•°å¯¹å­¦ç”Ÿçš„logitsè¿›è¡Œå½’ä¸€åŒ–ï¼Œè€Œè€å¸ˆçš„logitsåˆ™ä½¿ç”¨æ ‡å‡†softmaxè½¬æ¢ä¸ºæ¦‚ç‡ã€‚`nn.KLDivLoss()`ä¸­çš„`reduction=batchmean`å‚æ•°æŒ‡å®šæˆ‘ä»¬åœ¨æ‰¹ç»´åº¦ä¸Šå¹³å‡æŸå¤±ã€‚
- en: Tip
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: æç¤º
- en: You can also perform knowledge distillation with the Keras API of the ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers library. To do this, youâ€™ll need to implement a custom `Distiller`
    class that overrides the `train_step()`, `test_step()`, and `compile()` methods
    of `tf.keras.Model()`. See the [Keras documentation](https://oreil.ly/6qp0F) for
    an example of how to do this.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨è¿˜å¯ä»¥ä½¿ç”¨![nlpt_pin01](Images/nlpt_pin01.png) Transformersåº“çš„Keras APIè¿›è¡ŒçŸ¥è¯†è’¸é¦ã€‚ä¸ºæ­¤ï¼Œæ‚¨éœ€è¦å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„`Distiller`ç±»ï¼Œè¦†ç›–`tf.keras.Model()`çš„`train_step()`ã€`test_step()`å’Œ`compile()`æ–¹æ³•ã€‚è¯·å‚é˜…[Kerasæ–‡æ¡£](https://oreil.ly/6qp0F)äº†è§£å¦‚ä½•å®ç°ã€‚
- en: Choosing a Good Student Initialization
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: é€‰æ‹©ä¸€ä¸ªå¥½çš„å­¦ç”Ÿåˆå§‹åŒ–
- en: Now that we have our custom trainer, the first question you might have is which
    pretrained language model should we pick for the student? In general we should
    pick a smaller model for the student to reduce the latency and memory footprint.
    A good rule of thumb from the literature is that knowledge distillation works
    best when the teacher and student are of the same *model type*.^([9](ch08.xhtml#idm46238707933552))
    One possible reason for this is that different model types, say BERT and RoBERTa,
    can have different output embedding spaces, which hinders the ability of the student
    to mimic the teacher. In our case study the teacher is BERT, so DistilBERT is
    a natural candidate to initialize the student with since it has 40% fewer parameters
    and has been shown to achieve strong results on downstream tasks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†è‡ªå®šä¹‰çš„è®­ç»ƒå™¨ï¼Œæ‚¨å¯èƒ½ä¼šé—®çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬åº”è¯¥ä¸ºå­¦ç”Ÿé€‰æ‹©å“ªä¸ªé¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Ÿä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬åº”è¯¥ä¸ºå­¦ç”Ÿé€‰æ‹©ä¸€ä¸ªè¾ƒå°çš„æ¨¡å‹ï¼Œä»¥å‡å°‘å»¶è¿Ÿå’Œå†…å­˜å ç”¨ã€‚ä»æ–‡çŒ®ä¸­å¾—å‡ºçš„ä¸€ä¸ªå¾ˆå¥½çš„ç»éªŒæ³•åˆ™æ˜¯ï¼Œå½“è€å¸ˆå’Œå­¦ç”Ÿæ˜¯ç›¸åŒçš„*æ¨¡å‹ç±»å‹*æ—¶ï¼ŒçŸ¥è¯†è’¸é¦æ•ˆæœæœ€å¥½ã€‚^([9](ch08.xhtml#idm46238707933552))è¿™æ ·åšçš„ä¸€ä¸ªå¯èƒ½åŸå› æ˜¯ï¼Œä¸åŒçš„æ¨¡å‹ç±»å‹ï¼Œæ¯”å¦‚BERTå’ŒRoBERTaï¼Œå¯èƒ½å…·æœ‰ä¸åŒçš„è¾“å‡ºåµŒå…¥ç©ºé—´ï¼Œè¿™ä¼šå¦¨ç¢å­¦ç”Ÿæ¨¡ä»¿è€å¸ˆçš„èƒ½åŠ›ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ç ”ç©¶ä¸­ï¼Œè€å¸ˆæ˜¯BERTï¼Œå› æ­¤DistilBERTæ˜¯ä¸€ä¸ªè‡ªç„¶çš„å€™é€‰ï¼Œå› ä¸ºå®ƒçš„å‚æ•°å°‘äº†40%ï¼Œå¹¶ä¸”å·²ç»åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸­å–å¾—äº†è‰¯å¥½çš„ç»“æœã€‚
- en: 'First weâ€™ll need to tokenize and encode our queries, so letâ€™s instantiate the
    tokenizer from DistilBERT and create a simple `tokenize_text()` function to take
    care of the preprocessing:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å¯¹æˆ‘ä»¬çš„æŸ¥è¯¢è¿›è¡Œæ ‡è®°åŒ–å’Œç¼–ç ï¼Œå› æ­¤è®©æˆ‘ä»¬å®ä¾‹åŒ–æ¥è‡ªDistilBERTçš„æ ‡è®°å™¨ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªç®€å•çš„`tokenize_text()`å‡½æ•°æ¥å¤„ç†é¢„å¤„ç†ï¼š
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here weâ€™ve removed the `text` column since we no longer need it, and weâ€™ve also
    renamed the `intent` column to `labels` so it can be automatically detected by
    the trainer.^([10](ch08.xhtml#idm46238707834608))
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å·²ç»åˆ é™¤äº†`text`åˆ—ï¼Œå› ä¸ºæˆ‘ä»¬ä¸å†éœ€è¦å®ƒï¼Œæˆ‘ä»¬è¿˜å°†`intent`åˆ—é‡å‘½åä¸º`labels`ï¼Œä»¥ä¾¿è®­ç»ƒå™¨å¯ä»¥è‡ªåŠ¨æ£€æµ‹åˆ°å®ƒã€‚^([10](ch08.xhtml#idm46238707834608))
- en: 'Now that weâ€™ve processed our texts, the next thing we need to do is define
    the hyperparameters and `compute_metrics()` function for our `DistillationTrainer`.
    Weâ€™ll also push all of our models to the Hugging Face Hub, so letâ€™s start by logging
    in to our account:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å¤„ç†äº†æˆ‘ä»¬çš„æ–‡æœ¬ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦åšçš„æ˜¯ä¸ºæˆ‘ä»¬çš„`DistillationTrainer`å®šä¹‰è¶…å‚æ•°å’Œ`compute_metrics()`å‡½æ•°ã€‚æˆ‘ä»¬è¿˜å°†æŠŠæ‰€æœ‰çš„æ¨¡å‹æ¨é€åˆ°Hugging
    Face Hubï¼Œæ‰€ä»¥è®©æˆ‘ä»¬é¦–å…ˆç™»å½•åˆ°æˆ‘ä»¬çš„è´¦æˆ·ï¼š
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, weâ€™ll define the metrics to track during training. As we did in the performance
    benchmark, weâ€™ll use accuracy as the main metric. This means we can reuse our
    `accuracy_score()` function in the `compute_metrics()` function that weâ€™ll include
    in `DistillationTrainer`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å®šä¹‰è®­ç»ƒè¿‡ç¨‹ä¸­è¦è·Ÿè¸ªçš„æŒ‡æ ‡ã€‚å°±åƒæˆ‘ä»¬åœ¨æ€§èƒ½åŸºå‡†æµ‹è¯•ä¸­æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨å‡†ç¡®æ€§ä½œä¸ºä¸»è¦æŒ‡æ ‡ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥åœ¨`compute_metrics()`å‡½æ•°ä¸­é‡ç”¨æˆ‘ä»¬çš„`accuracy_score()`å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°å°†åŒ…å«åœ¨`DistillationTrainer`ä¸­ï¼š
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this function, the predictions from the sequence modeling head come in the
    form of logits, so we use the `np.argmax()` function to find the most confident
    class prediction and compare that against the ground truth label.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œåºåˆ—å»ºæ¨¡å¤´éƒ¨çš„é¢„æµ‹ä»¥logitsçš„å½¢å¼å‡ºç°ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨`np.argmax()`å‡½æ•°æ‰¾åˆ°æœ€æœ‰ä¿¡å¿ƒçš„ç±»åˆ«é¢„æµ‹ï¼Œå¹¶å°†å…¶ä¸åœ°é¢çœŸç›¸æ ‡ç­¾è¿›è¡Œæ¯”è¾ƒã€‚
- en: 'Next we need to define the training arguments. To warm up, weâ€™ll set <math
    alttext="alpha equals 1"><mrow><mi>Î±</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    to see how well DistilBERT performs without any signal from the teacher.^([11](ch08.xhtml#idm46238707732144))
    Then we will push our fine-tuned model to a new repository called `distilbert-base-uncased-finetuned-clinc`,
    so we just need to specify that in the `output_dir` argument of `DistillationTrainingArguments`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦å®šä¹‰è®­ç»ƒå‚æ•°ã€‚ä¸ºäº†çƒ­èº«ï¼Œæˆ‘ä»¬å°†è®¾ç½®<math alttext="alpha equals 1"><mrow><mi>Î±</mi> <mo>=</mo>
    <mn>1</mn></mrow></math>ï¼Œä»¥æŸ¥çœ‹DistilBERTåœ¨æ²¡æœ‰æ¥è‡ªæ•™å¸ˆçš„ä»»ä½•ä¿¡å·çš„æƒ…å†µä¸‹çš„è¡¨ç°ã€‚^([11](ch08.xhtml#idm46238707732144))ç„¶åæˆ‘ä»¬å°†æˆ‘ä»¬çš„å¾®è°ƒæ¨¡å‹æ¨é€åˆ°ä¸€ä¸ªåä¸º`distilbert-base-uncased-finetuned-clinc`çš„æ–°å­˜å‚¨åº“ï¼Œæ‰€ä»¥æˆ‘ä»¬åªéœ€è¦åœ¨`DistillationTrainingArguments`çš„`output_dir`å‚æ•°ä¸­æŒ‡å®šå®ƒï¼š
- en: '[PRE25]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Weâ€™ve also tweaked a few of the default hyperparameter values, like the number
    of epochs, the weight decay, and the learning rate. The next thing to do is initialize
    a student model. Since we will be doing multiple runs with the trainer, weâ€™ll
    create a `student_init()` function to initialize the model with each new run.
    When we pass this function to the `DistillationTrainer`, this will ensure we initialize
    a new model each time we call the `train()` method.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜è°ƒæ•´äº†ä¸€äº›é»˜è®¤è¶…å‚æ•°å€¼ï¼Œæ¯”å¦‚epochsçš„æ•°é‡ï¼Œæƒé‡è¡°å‡å’Œå­¦ä¹ ç‡ã€‚æ¥ä¸‹æ¥è¦åšçš„æ˜¯åˆå§‹åŒ–ä¸€ä¸ªå­¦ç”Ÿæ¨¡å‹ã€‚ç”±äºæˆ‘ä»¬å°†ä½¿ç”¨è®­ç»ƒå™¨è¿›è¡Œå¤šæ¬¡è¿è¡Œï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ª`student_init()`å‡½æ•°ï¼Œä»¥ä¾¿åœ¨æ¯æ¬¡è°ƒç”¨`train()`æ–¹æ³•æ—¶åˆå§‹åŒ–ä¸€ä¸ªæ–°æ¨¡å‹ã€‚å½“æˆ‘ä»¬å°†è¿™ä¸ªå‡½æ•°ä¼ é€’ç»™`DistillationTrainer`æ—¶ï¼Œè¿™å°†ç¡®ä¿æˆ‘ä»¬æ¯æ¬¡è°ƒç”¨`train()`æ–¹æ³•æ—¶åˆå§‹åŒ–ä¸€ä¸ªæ–°æ¨¡å‹ã€‚
- en: 'One other thing we need to do is provide the student model with the mappings
    between each intent and label ID. These mappings can be obtained from our BERT-base
    model that we downloaded in the pipeline:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜éœ€è¦åšçš„å¦ä¸€ä»¶äº‹æ˜¯ä¸ºå­¦ç”Ÿæ¨¡å‹æä¾›æ¯ä¸ªæ„å›¾å’Œæ ‡ç­¾IDä¹‹é—´çš„æ˜ å°„ã€‚è¿™äº›æ˜ å°„å¯ä»¥ä»æˆ‘ä»¬åœ¨æµæ°´çº¿ä¸­ä¸‹è½½çš„BERT-baseæ¨¡å‹ä¸­è·å¾—ï¼š
- en: '[PRE26]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'With these mappings, we can now create a custom model configuration with the
    `AutoConfig` class hat we encountered in Chapters [3](ch03.xhtml#chapter_anatomy)
    and [4](ch04.xhtml#chapter_ner). Letâ€™s use this to create a configuration for
    our student with the information about the label mappings:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº†è¿™äº›æ˜ å°„ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ä½¿ç”¨`AutoConfig`ç±»åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰æ¨¡å‹é…ç½®ï¼Œè¿™æ˜¯æˆ‘ä»¬åœ¨ç¬¬[3](ch03.xhtml#chapter_anatomy)ç« å’Œç¬¬[4](ch04.xhtml#chapter_ner)ç« ä¸­é‡åˆ°çš„ã€‚è®©æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªä¸ºæˆ‘ä»¬çš„å­¦ç”Ÿåˆ›å»ºä¸€ä¸ªåŒ…å«æ ‡ç­¾æ˜ å°„ä¿¡æ¯çš„é…ç½®ï¼š
- en: '[PRE27]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here weâ€™ve also specified the number of classes our model should expect. We
    can then provide this configuration to the `from_pretrained()` function of the
    `AutoModelForSequenceClassification` class as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è¿˜æŒ‡å®šäº†æˆ‘ä»¬çš„æ¨¡å‹åº”è¯¥æœŸæœ›çš„ç±»çš„æ•°é‡ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªé…ç½®æä¾›ç»™`AutoModelForSequenceClassification`ç±»çš„`from_pretrained()`å‡½æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE28]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We now have all the ingredients needed for our distillation trainer, so letâ€™s
    load the teacher and fine-tune:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»æ‹¥æœ‰äº†æˆ‘ä»¬çš„è’¸é¦è®­ç»ƒå™¨æ‰€éœ€çš„æ‰€æœ‰è¦ç´ ï¼Œè®©æˆ‘ä»¬åŠ è½½æ•™å¸ˆå¹¶è¿›è¡Œå¾®è°ƒï¼š
- en: '[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '| Epoch | Training Loss | Validation Loss | Accuracy |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: Epoch | Training Loss | Validation Loss | Accuracy
- en: '| --- | --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '--- | --- | --- | ---'
- en: '| 1 | 4.2923 | 3.289337 | 0.742258 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: 1 | 4.2923 | 3.289337 | 0.742258
- en: '| 2 | 2.6307 | 1.883680 | 0.828065 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: 2 | 2.6307 | 1.883680 | 0.828065
- en: '| 3 | 1.5483 | 1.158315 | 0.896774 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: 3 | 1.5483 | 1.158315 | 0.896774
- en: '| 4 | 1.0153 | 0.861815 | 0.909355 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: 4 | 1.0153 | 0.861815 | 0.909355
- en: '| 5 | 0.7958 | 0.777289 | 0.917419 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: 5 | 0.7958 | 0.777289 | 0.917419
- en: 'The 92% accuracy on the validation set looks quite good compared to the 94%
    that the BERT-base teacher achieves. Now that weâ€™ve fine-tuned DistilBERT, letâ€™s
    push the model to the Hub so we can reuse it later:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: éªŒè¯é›†ä¸Šçš„92%å‡†ç¡®ç‡çœ‹èµ·æ¥ç›¸å½“ä¸é”™ï¼Œä¸BERT-baseæ•™å¸ˆå®ç°çš„94%ç›¸æ¯”ã€‚ç°åœ¨æˆ‘ä»¬å·²ç»å¯¹DistilBERTè¿›è¡Œäº†å¾®è°ƒï¼Œè®©æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ°Hubï¼Œä»¥ä¾¿ä»¥åé‡ç”¨ï¼š
- en: '[PRE31]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'With our model now safely stored on the Hub, we can immediately use it in a
    pipeline for our performance benchmark:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çš„æ¨¡å‹å·²ç»å®‰å…¨åœ°å­˜å‚¨åœ¨Hubä¸Šï¼Œæˆ‘ä»¬å¯ä»¥ç«‹å³åœ¨æ€§èƒ½åŸºå‡†æµ‹è¯•çš„æµæ°´çº¿ä¸­ä½¿ç”¨å®ƒï¼š
- en: '[PRE32]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can then pass this pipeline to our `PerformanceBenchmark` class to compute
    the metrics associated with this model:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªæµæ°´çº¿ä¼ é€’ç»™æˆ‘ä»¬çš„`PerformanceBenchmark`ç±»ï¼Œä»¥è®¡ç®—ä¸è¿™ä¸ªæ¨¡å‹ç›¸å…³çš„æŒ‡æ ‡ï¼š
- en: '[PRE33]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To compare these results against our baseline, letâ€™s create a scatter plot
    of the accuracy against the latency, with the radius of each point corresponding
    to the size of the model on disk. The following function does what we need and
    marks the current optimization type as a dashed circle to aid the comparison to
    previous results:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å°†è¿™äº›ç»“æœä¸æˆ‘ä»¬çš„åŸºå‡†è¿›è¡Œæ¯”è¾ƒï¼Œè®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæ•£ç‚¹å›¾ï¼Œæ˜¾ç¤ºå‡†ç¡®æ€§ä¸å»¶è¿Ÿä¹‹é—´çš„å…³ç³»ï¼Œæ¯ä¸ªç‚¹çš„åŠå¾„å¯¹åº”äºç£ç›˜ä¸Šæ¨¡å‹çš„å¤§å°ã€‚ä»¥ä¸‹å‡½æ•°å¯ä»¥æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚ï¼Œå¹¶å°†å½“å‰ä¼˜åŒ–ç±»å‹æ ‡è®°ä¸ºè™šçº¿åœ†åœˆï¼Œä»¥ä¾¿ä¸ä»¥å‰çš„ç»“æœè¿›è¡Œæ¯”è¾ƒï¼š
- en: '[PRE35]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](Images/nlpt_08in01.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_08in01.png)'
- en: From the plot we can see that by using a smaller model weâ€™ve managed to significantly
    decrease the average latency. And all this at the price of just over a 1% reduction
    in accuracy! Letâ€™s see if we can close that last gap by including the distillation
    loss of the teacher and finding good values for <math alttext="alpha"><mi>Î±</mi></math>
    and *T*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œé€šè¿‡ä½¿ç”¨ä¸€ä¸ªæ›´å°çš„æ¨¡å‹ï¼Œæˆ‘ä»¬æˆåŠŸåœ°æ˜¾è‘—é™ä½äº†å¹³å‡å»¶è¿Ÿã€‚è€Œè¿™ä¸€åˆ‡åªéœ€ç‰ºç‰²äº†ç•¥å¾®è¶…è¿‡1%çš„å‡†ç¡®æ€§ï¼è®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦å¯ä»¥é€šè¿‡åŒ…æ‹¬æ•™å¸ˆçš„è’¸é¦æŸå¤±å¹¶æ‰¾åˆ°<math
    alttext="alpha"><mi>Î±</mi></math>å’Œ*T*çš„è‰¯å¥½å€¼æ¥ç¼©å°æœ€åçš„å·®è·ã€‚
- en: Finding Good Hyperparameters with Optuna
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ä½¿ç”¨Optunaæ‰¾åˆ°è‰¯å¥½çš„è¶…å‚æ•°
- en: 'To find good values for <math alttext="alpha"><mi>Î±</mi></math> and *T*, we
    could do a grid search over the 2D parameter space. But a much better alternative
    is to use *Optuna*,^([12](ch08.xhtml#idm46238706954496)) which is an optimization
    framework designed for just this type of task. Optuna formulates the search problem
    in terms of an objective function that is optimized through multiple *trials*.
    For example, suppose we wished to minimize Rosenbrockâ€™s [â€œbanana functionâ€](https://oreil.ly/hPk8h):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†æ‰¾åˆ°<math alttext="alpha"><mi>Î±</mi></math>å’Œ*T*çš„è‰¯å¥½å€¼ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨2Då‚æ•°ç©ºé—´ä¸Šè¿›è¡Œç½‘æ ¼æœç´¢ã€‚ä½†ä¸€ä¸ªæ›´å¥½çš„é€‰æ‹©æ˜¯ä½¿ç”¨*Optuna*ï¼Œ^([12](ch08.xhtml#idm46238706954496))è¿™æ˜¯ä¸€ä¸ªä¸“ä¸ºè¿™ç§ä»»åŠ¡è®¾è®¡çš„ä¼˜åŒ–æ¡†æ¶ã€‚Optunaé€šè¿‡å¤šæ¬¡*trials*ä¼˜åŒ–ç›®æ ‡å‡½æ•°æ¥åˆ¶å®šæœç´¢é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œå‡è®¾æˆ‘ä»¬å¸Œæœ›æœ€å°åŒ–Rosenbrockçš„[â€œé¦™è•‰å‡½æ•°â€](https://oreil.ly/hPk8h)ï¼š
- en: <math alttext="f left-parenthesis x comma y right-parenthesis equals left-parenthesis
    1 minus x right-parenthesis squared plus 100 left-parenthesis y minus x squared
    right-parenthesis squared" display="block"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>x</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>100</mn> <msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="f left-parenthesis x comma y right-parenthesis equals left-parenthesis
    1 minus x right-parenthesis squared plus 100 left-parenthesis y minus x squared
    right-parenthesis squared" display="block"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>x</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>100</mn> <msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
- en: which is a famous test case for optimization frameworks. As shown in [FigureÂ 8-5](#banana-function),
    the function gets its name from the curved contours and has a global minimum at
    <math alttext="left-parenthesis x comma y right-parenthesis equals left-parenthesis
    1 comma 1 right-parenthesis"><mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>)</mo> <mo>=</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math>
    . Finding the valley is an easy optimization problem, but converging to the global
    minimum is not.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªè‘—åçš„ä¼˜åŒ–æ¡†æ¶çš„æµ‹è¯•æ¡ˆä¾‹ã€‚å¦‚[å›¾8-5](#banana-function)æ‰€ç¤ºï¼Œè¯¥å‡½æ•°å› å…¶æ›²çº¿è½®å»“è€Œå¾—åï¼Œå¹¶ä¸”åœ¨<math alttext="left-parenthesis
    x comma y right-parenthesis equals left-parenthesis 1 comma 1 right-parenthesis"><mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo> <mo>=</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo>
    <mn>1</mn> <mo>)</mo></mrow></math>å¤„æœ‰ä¸€ä¸ªå…¨å±€æœ€å°å€¼ã€‚æ‰¾åˆ°è¿™ä¸ªè°·æ˜¯ä¸€ä¸ªç®€å•çš„ä¼˜åŒ–é—®é¢˜ï¼Œä½†æ”¶æ•›åˆ°å…¨å±€æœ€å°å€¼å´ä¸æ˜¯ã€‚
- en: '![A banana plot](Images/nlpt_0805.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![é¦™è•‰å›¾](Images/nlpt_0805.png)'
- en: Figure 8-5\. Plot of the Rosenbrock function of two variables
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾8-5ã€‚ä¸¤ä¸ªå˜é‡çš„Rosenbrockå‡½æ•°çš„ç»˜å›¾
- en: 'In Optuna, we can find the minimum of <math alttext="f left-parenthesis x comma
    y right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>)</mo></mrow></math> by defining an `objective()` function that returns the
    value of <math alttext="f left-parenthesis x comma y right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></math> :'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Optunaä¸­ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å®šä¹‰ä¸€ä¸ª`objective()`å‡½æ•°æ¥æ‰¾åˆ°<math alttext="f left-parenthesis x comma
    y right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>)</mo></mrow></math>çš„æœ€å°å€¼ï¼Œè¯¥å‡½æ•°è¿”å›<math alttext="f left-parenthesis x comma y
    right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>)</mo></mrow></math>çš„å€¼ï¼š
- en: '[PRE36]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `trial.suggest_float` object specifies the parameter ranges to sample uniformly
    from; Optuna also provides `suggest_int` and `suggest_categorical` for integer
    and categorical parameters, respectively. Optuna collects multiple trials as a
    *study*, so to create one we just pass the `objective()` function to `study.optimize()`
    as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`trial.suggest_float`å¯¹è±¡æŒ‡å®šè¦å‡åŒ€é‡‡æ ·çš„å‚æ•°èŒƒå›´ï¼›Optunaè¿˜æä¾›`suggest_int`å’Œ`suggest_categorical`ç”¨äºæ•´æ•°å’Œåˆ†ç±»å‚æ•°ã€‚Optunaå°†å¤šä¸ªè¯•éªŒæ”¶é›†ä¸ºä¸€ä¸ª*study*ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€å°†`objective()`å‡½æ•°ä¼ é€’ç»™`study.optimize()`æ¥åˆ›å»ºä¸€ä¸ªå¦‚ä¸‹ï¼š'
- en: '[PRE37]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Once the study is completed, we can then find the best parameters as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ç ”ç©¶å®Œæˆï¼Œæˆ‘ä»¬å°±å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼æ‰¾åˆ°æœ€ä½³å‚æ•°ï¼š
- en: '[PRE38]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We see that with one thousand trials, Optuna has managed to find values for
    *x* and *y* that are reasonably close to the global minimum. To use Optuna in
    ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, we use similar logic by first
    defining the hyperparameter space that we wish to optimize over. In addition to
    <math alttext="alpha"> <mi>Î±</mi> </math> and *T*, weâ€™ll include the number of
    training epochs as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä¸€åƒæ¬¡è¯•éªŒï¼ŒOptunaå·²ç»æˆåŠŸæ‰¾åˆ°äº†* x *å’Œ* y *çš„å€¼ï¼Œè¿™äº›å€¼ä¸å…¨å±€æœ€å°å€¼ç›¸å½“æ¥è¿‘ã€‚è¦åœ¨![nlpt_pin01](Images/nlpt_pin01.png)
    Transformersä¸­ä½¿ç”¨Optunaï¼Œæˆ‘ä»¬é¦–å…ˆå®šä¹‰è¦ä¼˜åŒ–çš„è¶…å‚æ•°ç©ºé—´ã€‚é™¤äº†<math alttext="alpha"> <mi>Î±</mi> </math>å’Œ*T*ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å°†åŒ…æ‹¬è®­ç»ƒå‘¨æœŸçš„æ•°é‡å¦‚ä¸‹ï¼š
- en: '[PRE40]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Running the hyperparameter search with the `Trainer` is then quite simple;
    we just need to specify the number of trials to run and a direction to optimize
    for. Because we want the best possible accuracy, we specify `direction="maximize"`
    in the `hyperâ€‹paraâ meter_â€‹search()` method of the trainer and pass the hyperparameter
    search space as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨`Trainer`è¿›è¡Œè¶…å‚æ•°æœç´¢éå¸¸ç®€å•ï¼›æˆ‘ä»¬åªéœ€è¦æŒ‡å®šè¦è¿è¡Œçš„è¯•éªŒæ¬¡æ•°å’Œè¦ä¼˜åŒ–çš„æ–¹å‘ã€‚å› ä¸ºæˆ‘ä»¬å¸Œæœ›è·å¾—æœ€ä½³å‡†ç¡®åº¦ï¼Œæ‰€ä»¥åœ¨è®­ç»ƒå™¨çš„`hyperâ€‹paraâ meter_â€‹search()`æ–¹æ³•ä¸­æŒ‡å®š`direction="maximize"`ï¼Œå¹¶æŒ‰å¦‚ä¸‹æ–¹å¼ä¼ é€’è¶…å‚æ•°æœç´¢ç©ºé—´ï¼š
- en: '[PRE41]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The `hyperparameter_search()` method returns a `BestRun` object, which contains
    the value of the objective that was maximized (by default, the sum of all metrics)
    and the hyperparameters it used for that run:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`hyperparameter_search()`æ–¹æ³•è¿”å›ä¸€ä¸ª`BestRun`å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«äº†è¢«æœ€å¤§åŒ–çš„ç›®æ ‡å€¼ï¼ˆé»˜è®¤ä¸ºæ‰€æœ‰æŒ‡æ ‡çš„æ€»å’Œï¼‰å’Œè¯¥è¿è¡Œæ‰€ä½¿ç”¨çš„è¶…å‚æ•°ï¼š'
- en: '[PRE42]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This value of <math alttext="alpha"><mi>Î±</mi></math> tells us that most of
    the training signal is coming from the knowledge distillation term. Letâ€™s update
    our training arguments with these values and run the final training run:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ª<math alttext="alpha"><mi>Î±</mi></math>çš„å€¼å‘Šè¯‰æˆ‘ä»¬ï¼Œå¤§éƒ¨åˆ†çš„è®­ç»ƒä¿¡å·æ¥è‡ªçŸ¥è¯†è’¸é¦é¡¹ã€‚è®©æˆ‘ä»¬ä½¿ç”¨è¿™äº›å€¼æ›´æ–°æˆ‘ä»¬çš„è®­ç»ƒå‚æ•°ï¼Œå¹¶è¿è¡Œæœ€ç»ˆçš„è®­ç»ƒï¼š
- en: '[PRE44]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '| Epoch | Training Loss | Validation Loss | Accuracy |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: Epoch | Training Loss | Validation Loss | Accuracy |
- en: '| --- | --- | --- | --- |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '--- | --- | --- | ---'
- en: '| 1 | 0.9031 | 0.574540 | 0.736452 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: 1 | 0.9031 | 0.574540 | 0.736452 |
- en: '| 2 | 0.4481 | 0.285621 | 0.874839 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: 2 | 0.4481 | 0.285621 | 0.874839
- en: '| 3 | 0.2528 | 0.179766 | 0.918710 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: 3 | 0.2528 | 0.179766 | 0.918710
- en: '| 4 | 0.1760 | 0.139828 | 0.929355 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: 4 | 0.1760 | 0.139828 | 0.929355
- en: '| 5 | 0.1416 | 0.121053 | 0.934839 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: 5 | 0.1416 | 0.121053 | 0.934839
- en: '| 6 | 0.1243 | 0.111640 | 0.934839 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: 6 | 0.1243 | 0.111640 | 0.934839
- en: '| 7 | 0.1133 | 0.106174 | 0.937742 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: 7 | 0.1133 | 0.106174 | 0.937742
- en: '| 8 | 0.1075 | 0.103526 | 0.938710 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: 8 | 0.1075 | 0.103526 | 0.938710
- en: '| 9 | 0.1039 | 0.101432 | 0.938065 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: 9 | 0.1039 | 0.101432 | 0.938065
- en: '| 10 | 0.1018 | 0.100493 | 0.939355 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: 10 | 0.1018 | 0.100493 | 0.939355
- en: 'Remarkably, weâ€™ve been able to train the student to match the accuracy of the
    teacher, despite it having almost half the number of parameters! Letâ€™s push the
    model to the Hub for future use:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå°½ç®¡å‚æ•°æ•°é‡å‡ ä¹å‡å°‘äº†ä¸€åŠï¼Œæˆ‘ä»¬å·²ç»æˆåŠŸè®­ç»ƒå‡ºå­¦ç”Ÿæ¨¡å‹ä¸æ•™å¸ˆæ¨¡å‹çš„å‡†ç¡®åº¦ç›¸åŒ¹é…ï¼è®©æˆ‘ä»¬å°†æ¨¡å‹æ¨é€åˆ°Hubä»¥ä¾›å°†æ¥ä½¿ç”¨ï¼š
- en: '[PRE45]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Benchmarking Our Distilled Model
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: åŸºå‡†æµ‹è¯•æˆ‘ä»¬çš„ç²¾ç‚¼æ¨¡å‹
- en: 'Now that we have an accurate student, letâ€™s create a pipeline and redo our
    benchmark to see how we perform on the test set:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To put these results in context, letâ€™s also visualize them with our `plot_metrics()`
    function:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](Images/nlpt_08in02.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: As expected, the model size and latency remain essentially unchanged compared
    to the DistilBERT benchmark, but the accuracy has improved and even surpassed
    the performance of the teacher! One way to interpret this surprising result is
    that the teacher has likely not been fine-tuned as systematically as the student.
    This is great, but we can actually compress our distilled model even further using
    a technique known as quantization. Thatâ€™s the topic of the next section.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Making Models Faster with Quantization
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Weâ€™ve now seen that with knowledge distillation we can reduce the computational
    and memory cost of running inference by transferring the information from a teacher
    into a smaller student. Quantization takes a different approach; instead of reducing
    the number of computations, it makes them much more efficient by representing
    the weights and activations with low-precision data types like 8-bit integer (INT8)
    instead of the usual 32-bit floating point (FP32). Reducing the number of bits
    means the resulting model requires less memory storage, and operations like matrix
    multiplication can be performed much faster with integer arithmetic. Remarkably,
    these performance gains can be realized with little to no loss in accuracy!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea behind quantization is that we can â€œdiscretizeâ€ the floating-point
    values *f* in each tensor by mapping their range [ <math alttext="f Subscript
    normal m normal a normal x Baseline comma f Subscript normal m normal i normal
    n Baseline"><mrow><msub><mi>f</mi> <mi>max</mi></msub> <mo>,</mo> <msub><mi>f</mi>
    <mi>min</mi></msub></mrow></math> ] into a smaller one [ <math alttext="q Subscript
    normal m normal a normal x Baseline comma q Subscript normal m normal i normal
    n Baseline"><mrow><msub><mi>q</mi> <mi>max</mi></msub> <mo>,</mo> <msub><mi>q</mi>
    <mi>min</mi></msub></mrow></math> ] of fixed-point numbers <math alttext="q"><mi>q</mi></math>
    , and linearly distributing all values in between. Mathematically, this mapping
    is described by the following equation:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="f equals left-parenthesis StartFraction f Subscript normal m
    normal a normal x Baseline minus f Subscript normal m normal i normal n Baseline
    Over q Subscript normal m normal a normal x Baseline minus q Subscript normal
    m normal i normal n Baseline EndFraction right-parenthesis left-parenthesis q
    minus upper Z right-parenthesis equals upper S left-parenthesis q minus upper
    Z right-parenthesis" display="block"><mrow><mi>f</mi> <mo>=</mo> <mfenced open="("
    close=")"><mfrac><mrow><msub><mi>f</mi> <mi>max</mi></msub> <mo>-</mo><msub><mi>f</mi>
    <mi>min</mi></msub></mrow> <mrow><msub><mi>q</mi> <mi>max</mi></msub> <mo>-</mo><msub><mi>q</mi>
    <mi>min</mi></msub></mrow></mfrac></mfenced> <mrow><mo>(</mo> <mi>q</mi> <mo>-</mo>
    <mi>Z</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>S</mi> <mrow><mo>(</mo> <mi>q</mi>
    <mo>-</mo> <mi>Z</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: where the scale factor <math alttext="upper S"><mi>S</mi></math> is a positive
    floating-point number and the constant <math alttext="upper Z"><mi>Z</mi></math>
    has the same type as <math alttext="q"><mi>q</mi></math> and is called the *zero
    point* because it corresponds to the quantized value of the floating-point value
    <math alttext="f equals 0"><mrow><mi>f</mi> <mo>=</mo> <mn>0</mn></mrow></math>
    . Note that the map needs to be *affine* so that we get back floating-point numbers
    when we dequantize the fixed-point ones.^([13](ch08.xhtml#idm46238706293280))
    An illustration of the conversion is shown in [FigureÂ 8-6](#fp32toint8).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Mapping floating-point numbers to 8-bit integers](Images/nlpt_0806.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. Quantizing floating-point numbers as unsigned 8-bit integers (courtesy
    of Manas Sahni)
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, one of the main reasons why transformers (and deep neural networks more
    generally) are prime candidates for quantization is that the weights and activations
    tend to take values in relatively small ranges. This means we donâ€™t have to squeeze
    the whole range of possible FP32 numbers into, say, the <math alttext="2 Superscript
    8 Baseline equals 256"><mrow><msup><mn>2</mn> <mn>8</mn></msup> <mo>=</mo> <mn>256</mn></mrow></math>
    numbers represented by INT8\. To see this, letâ€™s pick out one of the attention
    weight matrices from our distilled model and plot the frequency distribution of
    the values:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![](Images/nlpt_08in03.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, the values of the weights are distributed in the small range
    [ <math alttext="negative 0.1 comma 0.1"><mrow><mo>-</mo> <mn>0</mn> <mo>.</mo>
    <mn>1</mn> <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>1</mn></mrow></math> ] around
    zero. Now, suppose we want to quantize this tensor as a signed 8-bit integer.
    In that case, the range of possible values for our integers is [ <math alttext="q
    Subscript normal m normal a normal x Baseline comma q Subscript normal m normal
    i normal n Baseline"><mrow><msub><mi>q</mi> <mi>max</mi></msub> <mo>,</mo> <msub><mi>q</mi>
    <mi>min</mi></msub></mrow></math> ] = [ <math alttext="negative 128 comma 127"><mrow><mo>-</mo>
    <mn>128</mn> <mo>,</mo> <mn>127</mn></mrow></math> ]. The zero point coincides
    with the zero of FP32 and the scale factor is calculated according to the previous
    equation:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'To obtain the quantized tensor, we just need to invert the mapping <math alttext="q
    equals f slash upper S plus upper Z"><mrow><mi>q</mi> <mo>=</mo> <mi>f</mi> <mo>/</mo>
    <mi>S</mi> <mo>+</mo> <mi>Z</mi></mrow></math> , clamp the values, round them
    to the nearest integer, and represent the result in the `torch.int8` data type
    using the `Tensor.char()` function:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Great, weâ€™ve just quantized our first tensor! In PyTorch we can simplify the
    conversion by using the `quantize_per_tensor()` function together with a quantized
    data type, `torch.qint`, that is optimized for integer arithmetic operations:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The plot in [FigureÂ 8-7](#weight-quantization) shows very clearly the discretization
    thatâ€™s induced by only mapping some of the weight values precisely and rounding
    the rest.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of quantization on a transformer''s weights](Images/nlpt_0807.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. Effect of quantization on a transformerâ€™s weights
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To round out our little analysis, letâ€™s compare how long it takes to compute
    the multiplication of two weight tensors with FP32 and INT8 values. For the FP32
    tensors, we can multiply them using PyTorchâ€™s nifty `@` operator:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'For the quantized tensors we need the `QFunctional` wrapper class so that we
    can perform operations with the special `torch.qint8` data type:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This class supports various elementary operations, like addition, and in our
    case we can time the multiplication of our quantized tensors as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Compared to our FP32 computation, using the INT8 tensors is almost 100 times
    faster! Even larger gains can be obtained by using dedicated backends for running
    quantized operators efficiently. As of this bookâ€™s writing, PyTorch supports:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: x86 CPUs with AVX2 support or higher
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ARM CPUs (typically found in mobile/embedded devices)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since INT8 numbers have four times fewer bits than FP32 numbers, quantization
    also reduces the memory storage requirements by up to a factor of four. In our
    simple example we can verify this by comparing the underlying storage size of
    our weight tensor and its quantized cousin by using the `Tensor.storage()` function
    and the `getsizeof()` function from Pythonâ€™s `sys` module:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: For a full-scale transformer, the actual compression rate depends on which layers
    are quantized (as weâ€™ll see in the next section it is only the linear layers that
    typically get quantized).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'So whatâ€™s the catch with quantization? Changing the precision for all computations
    in our model introduces small disturbances at each point in the modelâ€™s computational
    graph, which can compound and affect the modelâ€™s performance. There are several
    ways to quantize a model, which all have pros and cons. For deep neural networks,
    there are typically three main approaches to quantization:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic quantization
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: When using dynamic quantization nothing is changed during training and the adaptations
    are only performed during inference. Like with all the quantization methods we
    will discuss, the weights of the model are converted to INT8 ahead of inference
    time. In addition to the weights, the modelâ€™s activations are also quantized.
    This approach is dynamic because the quantization happens on the fly. This means
    that all the matrix multiplications can be calculated with highly optimized INT8
    functions. Of all the quantization methods discussed here, dynamic quantization
    is the simplest one. However, with dynamic quantization the activations are written
    and read to memory in floating-point format. This conversion between integer and
    floating point can be a performance bottleneck.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Static quantization
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of computing the quantization of the activations on the fly, we can
    avoid the conversion to floating point by precomputing the quantization scheme.
    Static quantization achieves this by observing the activation patterns on a representative
    sample of the data ahead of inference time. The ideal quantization scheme is calculated
    and then saved. This enables us to skip the conversion between INT8 and FP32 values
    and speeds up the computations. However, it requires access to a good data sample
    and introduces an additional step in the pipeline, since we now need to train
    and determine the quantization scheme before we can perform inference. There is
    also one aspect that static quantization does not address: the discrepancy between
    the precision during training and inference, which leads to a performance drop
    in the modelâ€™s metrics (e.g., accuracy).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Quantization-aware training
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: The effect of quantization can be effectively simulated during training by â€œfakeâ€
    quantization of the FP32 values. Instead of using INT8 values during training,
    the FP32 values are rounded to mimic the effect of quantization. This is done
    during both the forward and the backward pass and improves performance in terms
    of model metrics over static and dynamic quantization.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: The main bottleneck for running inference with transformers is the compute and
    memory bandwidth associated with the enormous numbers of weights in these models.
    For this reason, dynamic quantization is currently the best approach for transformer-based
    models in NLP. In smaller computer vision models the limiting factor is the memory
    bandwidth of the activations, which is why static quantization is generally used
    (or quantization-aware training in cases where the performance drops are too significant).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing dynamic quantization in PyTorch is quite simple and can be done
    with a single line of code:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Here we pass to `quantize_dynamic()` the full-precision model and specify the
    set of PyTorch layer classes in that model that we want to quantize. The `dtype`
    argument specifies the target precision and can be `fp16` or `qint8`. A good practice
    is to pick the lowest precision that you can tolerate with respect to your evaluation
    metrics. In this chapter weâ€™ll use INT8, which as weâ€™ll soon see has little impact
    on our modelâ€™s accuracy.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking Our Quantized Model
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With our model now quantized, letâ€™s pass it through the benchmark and visualize
    the results:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '![](Images/nlpt_08in04.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: Nice, the quantized model is almost half the size of our distilled one and has
    even gained a slight accuracy boost! Letâ€™s see if we can push our optimization
    to the limit with a powerful framework called the ONNX Runtime.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Inference with ONNX and the ONNX Runtime
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[ONNX](https://onnx.ai) is an open standard that defines a common set of operators
    and a common file format to represent deep learning models in a wide variety of
    frameworks, including PyTorch and TensorFlow.^([14](ch08.xhtml#idm46238705705264))
    When a model is exported to the ONNX format, these operators are used to construct
    a computational graph (often called an *intermediate representation*) that represents
    the flow of data through the neural network. An example of such a graph for BERT-base
    is shown in [FigureÂ 8-8](#bert-onnx), where each node receives some input, applies
    an operation like `Add` or `Squeeze`, and then feeds the output to the next set
    of nodes.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[ONNX](https://onnx.ai)æ˜¯ä¸€ä¸ªå¼€æ”¾æ ‡å‡†ï¼Œå®šä¹‰äº†ä¸€ç»„é€šç”¨çš„æ“ä½œç¬¦å’Œä¸€ç§é€šç”¨çš„æ–‡ä»¶æ ¼å¼ï¼Œç”¨äºåœ¨å„ç§æ¡†æ¶ä¸­è¡¨ç¤ºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼ŒåŒ…æ‹¬PyTorchå’ŒTensorFlowã€‚^([14](ch08.xhtml#idm46238705705264))å½“æ¨¡å‹å¯¼å‡ºä¸ºONNXæ ¼å¼æ—¶ï¼Œè¿™äº›æ“ä½œç¬¦ç”¨äºæ„å»ºä¸€ä¸ªè®¡ç®—å›¾ï¼ˆé€šå¸¸ç§°ä¸º*ä¸­é—´è¡¨ç¤º*ï¼‰ï¼Œè¡¨ç¤ºæ•°æ®é€šè¿‡ç¥ç»ç½‘ç»œçš„æµåŠ¨ã€‚ä¾‹å¦‚ï¼ŒBERT-baseçš„è¿™æ ·ä¸€ä¸ªå›¾ç¤ºä¾‹æ˜¾ç¤ºåœ¨[å›¾8-8](#bert-onnx)ä¸­ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹æ¥æ”¶ä¸€äº›è¾“å…¥ï¼Œåº”ç”¨æ“ä½œå¦‚`Add`æˆ–`Squeeze`ï¼Œç„¶åå°†è¾“å‡ºé¦ˆé€åˆ°ä¸‹ä¸€ç»„èŠ‚ç‚¹ã€‚'
- en: '![Example ONNX graph](Images/nlpt_0808.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![ONNXå›¾ç¤ºä¾‹](Images/nlpt_0808.png)'
- en: Figure 8-8\. A section of the ONNX graph for BERT-base, visualized in Netron
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾8-8\. BERT-baseçš„ONNXå›¾çš„ä¸€ä¸ªéƒ¨åˆ†ï¼Œåœ¨Netronä¸­å¯è§†åŒ–
- en: By exposing a graph with standardized operators and data types, ONNX makes it
    easy to switch between frameworks. For example, a model trained in PyTorch can
    be exported to ONNX format and then imported in TensorFlow (and vice versa).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å…¬å¼€å…·æœ‰æ ‡å‡†åŒ–æ“ä½œç¬¦å’Œæ•°æ®ç±»å‹çš„å›¾ï¼ŒONNXä½¿å¾—åœ¨ä¸åŒæ¡†æ¶ä¹‹é—´åˆ‡æ¢å˜å¾—å®¹æ˜“ã€‚ä¾‹å¦‚ï¼Œåœ¨PyTorchä¸­è®­ç»ƒçš„æ¨¡å‹å¯ä»¥å¯¼å‡ºä¸ºONNXæ ¼å¼ï¼Œç„¶ååœ¨TensorFlowä¸­å¯¼å…¥ï¼ˆåä¹‹äº¦ç„¶ï¼‰ã€‚
- en: Where ONNX really shines is when it is coupled with a dedicated accelerator
    like [ONNX Runtime](https://onnxruntime.ai), or ORT for short.^([15](ch08.xhtml#idm46238705675632))
    ORT provides tools to optimize the ONNX graph through techniques like operator
    fusion and constant folding,^([16](ch08.xhtml#idm46238705672032)) and defines
    an interface to *execution providers* that allow you to run the model on different
    types of hardware. This is a powerful abstraction. [FigureÂ 8-9](#onnx-ort) shows
    the high-level architecture of the ONNX and ORT ecosystem.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ONNXä¸ä¸“ç”¨åŠ é€Ÿå™¨å¦‚[ONNX Runtime](https://onnxruntime.ai)æˆ–ORTé…åˆä½¿ç”¨æ—¶ï¼Œå®ƒçš„ä¼˜åŠ¿å°±æ˜¾ç°å‡ºæ¥äº†ã€‚^([15](ch08.xhtml#idm46238705675632))ORTé€šè¿‡æ“ä½œç¬¦èåˆå’Œå¸¸é‡æŠ˜å ç­‰æŠ€æœ¯æä¾›äº†ä¼˜åŒ–ONNXå›¾çš„å·¥å…·ï¼Œ^([16](ch08.xhtml#idm46238705672032))å¹¶å®šä¹‰äº†ä¸€ä¸ªæ¥å£ï¼Œå…è®¸æ‚¨åœ¨ä¸åŒç±»å‹çš„ç¡¬ä»¶ä¸Šè¿è¡Œæ¨¡å‹ã€‚è¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æŠ½è±¡ã€‚[å›¾8-9](#onnx-ort)æ˜¾ç¤ºäº†ONNXå’ŒORTç”Ÿæ€ç³»ç»Ÿçš„é«˜çº§æ¶æ„ã€‚
- en: '![Architecture of the ONNX and ONNX Runtime ecosystem](Images/nlpt_0809.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![ONNXå’ŒONNX Runtimeç”Ÿæ€ç³»ç»Ÿçš„æ¶æ„](Images/nlpt_0809.png)'
- en: Figure 8-9\. Architecture of the ONNX and ONNX Runtime ecosystem (courtesy of
    the ONNX Runtime team)
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾8-9\. ONNXå’ŒONNX Runtimeç”Ÿæ€ç³»ç»Ÿçš„æ¶æ„ï¼ˆç”±ONNX Runtimeå›¢é˜Ÿæä¾›ï¼‰
- en: 'To see ORT in action, the first thing we need to do is convert our distilled
    model into the ONNX format. The ![nlpt_pin01](Images/nlpt_pin01.png) Transformers
    library has a built-in function called `conâ vert_graph_to_onnx.convert()` that
    simplifies the process by taking the following steps:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: è¦çœ‹åˆ°ORTçš„è¿è¡Œæƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯å°†æˆ‘ä»¬çš„ç²¾ç‚¼æ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼ã€‚![nlpt_pin01](Images/nlpt_pin01.png)
    Transformersåº“æœ‰ä¸€ä¸ªå†…ç½®å‡½æ•°å«åš`conâ vert_graph_to_onnx.convert()`ï¼Œå®ƒç®€åŒ–äº†è¿™ä¸ªè¿‡ç¨‹ï¼Œé‡‡å–ä»¥ä¸‹æ­¥éª¤ï¼š
- en: Initialize the model as a `Pipeline`.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ¨¡å‹åˆå§‹åŒ–ä¸º`Pipeline`ã€‚
- en: Run placeholder inputs through the pipeline so that ONNX can record the computational
    graph.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é€šè¿‡ç®¡é“è¿è¡Œå ä½ç¬¦è¾“å…¥ï¼Œä»¥ä¾¿ONNXå¯ä»¥è®°å½•è®¡ç®—å›¾ã€‚
- en: Define dynamic axes to handle dynamic sequence lengths.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å®šä¹‰åŠ¨æ€è½´ä»¥å¤„ç†åŠ¨æ€åºåˆ—é•¿åº¦ã€‚
- en: Save the graph with network parameters.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¿å­˜å…·æœ‰ç½‘ç»œå‚æ•°çš„å›¾ã€‚
- en: 'To use this function, we first need to set some [OpenMP](https://openmp.org)
    environment variables for ONNX:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: è¦ä½¿ç”¨è¿™ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ä¸ºONNXè®¾ç½®ä¸€äº›[OpenMP](https://openmp.org)ç¯å¢ƒå˜é‡ï¼š
- en: '[PRE66]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: OpenMP is an API designed for developing highly parallelized applications. The
    `OMP_NUM_THREADS` environment variable sets the number of threads to use for parallel
    computations in the ONNX Runtime, while `OMP_WAIT_POLICY=ACTIVE` specifies that
    waiting threads should be active (i.e., using CPU processor cycles).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: OpenMPæ˜¯ä¸€ä¸ªä¸ºå¼€å‘é«˜åº¦å¹¶è¡ŒåŒ–åº”ç”¨ç¨‹åºè€Œè®¾è®¡çš„APIã€‚`OMP_NUM_THREADS`ç¯å¢ƒå˜é‡è®¾ç½®å¹¶è¡Œè®¡ç®—ä¸­ä½¿ç”¨çš„çº¿ç¨‹æ•°ï¼Œåœ¨ONNX Runtimeä¸­ï¼Œ`OMP_WAIT_POLICY=ACTIVE`æŒ‡å®šç­‰å¾…çº¿ç¨‹åº”å¤„äºæ´»åŠ¨çŠ¶æ€ï¼ˆå³ä½¿ç”¨CPUå¤„ç†å™¨å‘¨æœŸï¼‰ã€‚
- en: 'Next, letâ€™s convert our distilled model to the ONNX format. Here we need to
    specify the argument `pipeline_name="text-classification"` since `convert()` wraps
    the model in a ![nlpt_pin01](Images/nlpt_pin01.png) Transformers `pipeline()`
    function during the conversion. In addition to the `model_ckpt`, we also pass
    the tokenizer to initialize the pipeline:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬å°†æˆ‘ä»¬çš„ç²¾ç‚¼æ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬éœ€è¦æŒ‡å®šå‚æ•°`pipeline_name="text-classification"`ï¼Œå› ä¸º`convert()`åœ¨è½¬æ¢è¿‡ç¨‹ä¸­å°†æ¨¡å‹åŒ…è£…åœ¨ä¸€ä¸ª![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers `pipeline()`å‡½æ•°ä¸­ã€‚é™¤äº†`model_ckpt`ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜ä¼ é€’äº†tokenizeræ¥åˆå§‹åŒ–ç®¡é“ï¼š
- en: '[PRE67]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: ONNX uses *operator sets* to group together immutable operator specifications,
    so `opset=12` corresponds to a specific version of the ONNX library.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ONNXä½¿ç”¨*æ“ä½œç¬¦é›†*æ¥å°†ä¸å¯å˜çš„æ“ä½œç¬¦è§„èŒƒåˆ†ç»„åœ¨ä¸€èµ·ï¼Œå› æ­¤`opset=12`å¯¹åº”äºONNXåº“çš„ç‰¹å®šç‰ˆæœ¬ã€‚
- en: 'Now that we have our model saved, we need to create an `InferenceSession` instance
    to feed inputs to the model:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»ä¿å­˜äº†æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ª`InferenceSession`å®ä¾‹æ¥å‘æ¨¡å‹è¾“å…¥æ•°æ®ï¼š
- en: '[PRE68]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now when we call `onnx_model.run()`, we can get the class logits from the ONNX
    model. Letâ€™s test this out with an example from the test set. Since the output
    from `convert()` tells us that ONNX expects just the `input_ids` and `attention_mask`
    as inputs, we need to drop the `label` column from our sample:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å½“æˆ‘ä»¬è°ƒç”¨`onnx_model.run()`æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä»ONNXæ¨¡å‹ä¸­è·å–ç±»åˆ«å¯¹æ•°ã€‚è®©æˆ‘ä»¬ç”¨æµ‹è¯•é›†ä¸­çš„ä¸€ä¸ªä¾‹å­æ¥æµ‹è¯•ä¸€ä¸‹ã€‚ç”±äº`convert()`çš„è¾“å‡ºå‘Šè¯‰æˆ‘ä»¬ONNXåªæœŸæœ›`input_ids`å’Œ`attention_mask`ä½œä¸ºè¾“å…¥ï¼Œæˆ‘ä»¬éœ€è¦ä»æˆ‘ä»¬çš„æ ·æœ¬ä¸­åˆ é™¤`label`åˆ—ï¼š
- en: '[PRE70]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Once we have the logits, we can easily get the predicted label by taking the
    argmax:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬æœ‰äº†å¯¹æ•°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡å–argmaxè½»æ¾è·å¾—é¢„æµ‹çš„æ ‡ç­¾ï¼š
- en: '[PRE72]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'which indeed agrees with the ground truth label:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¡®å®ä¸åœ°é¢çœŸå®æ ‡ç­¾ä¸€è‡´ï¼š
- en: '[PRE74]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The ONNX model is not compatible with the `text-classification` pipeline, so
    weâ€™ll create our own class that mimics the core behavior:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ONNXæ¨¡å‹ä¸`text-classification`ç®¡é“ä¸å…¼å®¹ï¼Œå› æ­¤æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªæ¨¡ä»¿æ ¸å¿ƒè¡Œä¸ºçš„è‡ªå®šä¹‰ç±»ï¼š
- en: '[PRE76]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'We can then test this on our simple query to see if we recover the `car_rental`
    intent:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥æµ‹è¯•è¿™ä¸ªç®€å•çš„æŸ¥è¯¢ï¼Œçœ‹çœ‹æˆ‘ä»¬æ˜¯å¦æ¢å¤äº†`car_rental`æ„å›¾ï¼š
- en: '[PRE77]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Great, our pipeline works as expected. The next step is to create a performance
    benchmark for ONNX models. Here we can build on the work we did with the `Perâ formanceBenchmark`
    class by simply overriding the `compute_size()` method and leaving the `compute_accuracy()`
    and `time_pipeline()` methods intact. The reason we need to override the `compute_size()`
    method is that we cannot rely on the `state_dict` and `torch.save()` to measure
    a modelâ€™s size, since `onnx_model` is technically an ONNX `InferenceSession` object
    that doesnâ€™t have access to the attributes of PyTorchâ€™s `nn.Module`. In any case,
    the resulting logic is simple and can be implemented as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œæˆ‘ä»¬çš„æµæ°´çº¿æŒ‰é¢„æœŸå·¥ä½œã€‚ä¸‹ä¸€æ­¥æ˜¯ä¸ºONNXæ¨¡å‹åˆ›å»ºæ€§èƒ½åŸºå‡†æµ‹è¯•ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥å€Ÿé‰´æˆ‘ä»¬ä¸`Perâ formanceBenchmark`ç±»ä¸€èµ·å®Œæˆçš„å·¥ä½œï¼Œåªéœ€é‡å†™`compute_size()`æ–¹æ³•ï¼Œä¿ç•™`compute_accuracy()`å’Œ`time_pipeline()`æ–¹æ³•ã€‚æˆ‘ä»¬éœ€è¦é‡å†™`compute_size()`æ–¹æ³•çš„åŸå› æ˜¯ï¼Œæˆ‘ä»¬ä¸èƒ½ä¾èµ–`state_dict`å’Œ`torch.save()`æ¥æµ‹é‡æ¨¡å‹çš„å¤§å°ï¼Œå› ä¸º`onnx_model`åœ¨æŠ€æœ¯ä¸Šæ˜¯ä¸€ä¸ªONNX`InferenceSession`å¯¹è±¡ï¼Œæ— æ³•è®¿é—®PyTorchçš„`nn.Module`çš„å±æ€§ã€‚æ— è®ºå¦‚ä½•ï¼Œç»“æœé€»è¾‘å¾ˆç®€å•ï¼Œå¯ä»¥å®ç°å¦‚ä¸‹ï¼š
- en: '[PRE79]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'With our new benchmark, letâ€™s see how our distilled model performs when converted
    to ONNX format:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡æˆ‘ä»¬çš„æ–°åŸºå‡†æµ‹è¯•ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬çš„è’¸é¦æ¨¡å‹è½¬æ¢ä¸ºONNXæ ¼å¼åçš„æ€§èƒ½ï¼š
- en: '[PRE80]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '![](Images/nlpt_08in05.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_08in05.png)'
- en: Remarkably, converting to the ONNX format and using the ONNX Runtime has given
    our distilled model (i.e. the â€œDistillationâ€ circle in the plot) a boost in latency!
    Letâ€™s see if we can squeeze out a bit more performance by adding quantization
    to the mix.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè½¬æ¢ä¸ºONNXæ ¼å¼å¹¶ä½¿ç”¨ONNX Runtimeä¸ºæˆ‘ä»¬çš„è’¸é¦æ¨¡å‹ï¼ˆå³å›¾ä¸­çš„â€œè’¸é¦â€åœˆï¼‰æä¾›äº†å»¶è¿Ÿå¢ç›Šï¼è®©æˆ‘ä»¬çœ‹çœ‹æ˜¯å¦å¯ä»¥é€šè¿‡æ·»åŠ é‡åŒ–æ¥æŒ¤å‡ºæ›´å¤šæ€§èƒ½ã€‚
- en: 'Similar to PyTorch, ORT offers three ways to quantize a model: dynamic, static,
    and quantization-aware training. As we did with PyTorch, weâ€™ll apply dynamic quantization
    to our distilled model. In ORT, the quantization is applied through the `quanâ tize_dynamic()`
    function, which requires a path to the ONNX model to quantize, a target path to
    save the quantized model to, and the data type to reduce the weights to:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸PyTorchç±»ä¼¼ï¼ŒORTæä¾›äº†ä¸‰ç§æ¨¡å‹é‡åŒ–çš„æ–¹å¼ï¼šåŠ¨æ€é‡åŒ–ã€é™æ€é‡åŒ–å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒã€‚ä¸PyTorchä¸€æ ·ï¼Œæˆ‘ä»¬å°†å¯¹æˆ‘ä»¬çš„è’¸é¦æ¨¡å‹åº”ç”¨åŠ¨æ€é‡åŒ–ã€‚åœ¨ORTä¸­ï¼Œé‡åŒ–æ˜¯é€šè¿‡`quanâ tize_dynamic()`å‡½æ•°åº”ç”¨çš„ï¼Œè¯¥å‡½æ•°éœ€è¦ä¸€ä¸ªONNXæ¨¡å‹çš„è·¯å¾„è¿›è¡Œé‡åŒ–ï¼Œä¸€ä¸ªç›®æ ‡è·¯å¾„æ¥ä¿å­˜é‡åŒ–åçš„æ¨¡å‹ï¼Œä»¥åŠè¦å°†æƒé‡å‡å°‘åˆ°çš„æ•°æ®ç±»å‹ï¼š
- en: '[PRE83]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Now that the model is quantized, letâ€™s run it through our benchmark:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ¨¡å‹å·²ç»è¢«é‡åŒ–ï¼Œè®©æˆ‘ä»¬é€šè¿‡æˆ‘ä»¬çš„åŸºå‡†æµ‹è¯•è¿è¡Œå®ƒï¼š
- en: '[PRE84]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '![](Images/nlpt_08in06.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/nlpt_08in06.png)'
- en: ORT quantization has reduced the model size and latency by around 30% compared
    to the model obtained from PyTorch quantization (the distillation + quantization
    blob). One reason for this is that PyTorch only optimizes the `nn.Linear` modules,
    while ONNX quantizes the embedding layer as well. From the plot we can also see
    that applying ORT quantization to our distilled model has provided an almost three-fold
    gain compared to our BERT baseline!
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸PyTorché‡åŒ–è·å¾—çš„æ¨¡å‹ç›¸æ¯”ï¼ŒORTé‡åŒ–å·²ç»å°†æ¨¡å‹å¤§å°å’Œå»¶è¿Ÿå‡å°‘äº†çº¦30%ï¼ˆè’¸é¦+é‡åŒ–blobï¼‰ã€‚å…¶ä¸­ä¸€ä¸ªåŸå› æ˜¯PyTorchåªä¼˜åŒ–`nn.Linear`æ¨¡å—ï¼Œè€ŒONNXè¿˜é‡åŒ–äº†åµŒå…¥å±‚ã€‚ä»å›¾ä¸­æˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°ï¼Œå°†ORTé‡åŒ–åº”ç”¨äºæˆ‘ä»¬çš„è’¸é¦æ¨¡å‹ä¸æˆ‘ä»¬çš„BERTåŸºçº¿ç›¸æ¯”ï¼Œæä¾›äº†è¿‘ä¸‰å€çš„å¢ç›Šï¼
- en: This concludes our analysis of techniques to speed up transformers for inference.
    We have seen that methods such as quantization reduce the model size by reducing
    the precision of the representation. Another strategy to reduce the size is to
    remove some weights altogether. This technique is called *weight pruning*, and
    itâ€™s the focus of the next section.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç»“æŸäº†æˆ‘ä»¬å¯¹åŠ é€Ÿå˜å‹å™¨è¿›è¡Œæ¨æ–­çš„æŠ€æœ¯çš„åˆ†æã€‚æˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œè¯¸å¦‚é‡åŒ–ä¹‹ç±»çš„æ–¹æ³•é€šè¿‡é™ä½è¡¨ç¤ºçš„ç²¾åº¦æ¥å‡å°æ¨¡å‹å¤§å°ã€‚å¦ä¸€ç§å‡å°å¤§å°çš„ç­–ç•¥æ˜¯å½»åº•åˆ é™¤ä¸€äº›æƒé‡ã€‚è¿™ç§æŠ€æœ¯ç§°ä¸º*æƒé‡ä¿®å‰ª*ï¼Œå¹¶ä¸”æ˜¯ä¸‹ä¸€èŠ‚çš„é‡ç‚¹ã€‚
- en: Making Models Sparser with Weight Pruning
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æƒé‡ä¿®å‰ªä½¿æ¨¡å‹æ›´ç¨€ç–
- en: So far weâ€™ve seen that knowledge distillation and weight quantization are quite
    effective at producing faster models for inference, but in some cases you might
    also have strong constraints on the memory footprint of your model. For example,
    if our product manager suddenly decides that our text assistant needs to be deployed
    on a mobile device, then weâ€™ll need our intent classifier to take up as little
    storage space as possible. To round out our survey of compression methods, letâ€™s
    take a look at how we can shrink the number of parameters in our model by identifying
    and removing the least important weights in the network.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬å·²ç»çœ‹åˆ°çŸ¥è¯†è’¸é¦å’Œæƒé‡é‡åŒ–åœ¨äº§ç”Ÿæ›´å¿«çš„æ¨æ–­æ¨¡å‹æ–¹é¢éå¸¸æœ‰æ•ˆï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæ‚¨å¯èƒ½è¿˜å¯¹æ¨¡å‹çš„å†…å­˜å ç”¨æœ‰å¾ˆå¼ºçš„çº¦æŸã€‚ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬çš„äº§å“ç»ç†çªç„¶å†³å®šæˆ‘ä»¬çš„æ–‡æœ¬åŠ©æ‰‹éœ€è¦éƒ¨ç½²åœ¨ç§»åŠ¨è®¾å¤‡ä¸Šï¼Œé‚£ä¹ˆæˆ‘ä»¬éœ€è¦æˆ‘ä»¬çš„æ„å›¾åˆ†ç±»å™¨å°½å¯èƒ½å°‘åœ°å ç”¨å­˜å‚¨ç©ºé—´ã€‚ä¸ºäº†å®Œæˆæˆ‘ä»¬å¯¹å‹ç¼©æ–¹æ³•çš„è°ƒæŸ¥ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•é€šè¿‡è¯†åˆ«å’Œåˆ é™¤ç½‘ç»œä¸­æœ€ä¸é‡è¦çš„æƒé‡æ¥å‡å°‘æ¨¡å‹å‚æ•°çš„æ•°é‡ã€‚
- en: Sparsity in Deep Neural Networks
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„ç¨€ç–æ€§
- en: As shown in [FigureÂ 8-10](#network-pruning), the main idea behind pruning is
    to gradually remove weight connections (and potentially neurons) during training
    such that the model becomes progressively sparser. The resulting pruned model
    has a smaller number of nonzero parameters, which can then be stored in a compact
    sparse matrix format. Pruning can be also combined with quantization to obtain
    further compression.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚[å›¾8-10](#network-pruning)æ‰€ç¤ºï¼Œä¿®å‰ªçš„ä¸»è¦æ€æƒ³æ˜¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸ç§»é™¤æƒé‡è¿æ¥ï¼ˆå¯èƒ½è¿˜æœ‰ç¥ç»å…ƒï¼‰ï¼Œä½¿æ¨¡å‹é€æ¸å˜å¾—æ›´ç¨€ç–ã€‚ç»“æœä¿®å‰ªåçš„æ¨¡å‹å…·æœ‰æ›´å°‘çš„éé›¶å‚æ•°ï¼Œç„¶åå¯ä»¥ä»¥ç´§å‡‘çš„ç¨€ç–çŸ©é˜µæ ¼å¼å­˜å‚¨ã€‚ä¿®å‰ªä¹Ÿå¯ä»¥ä¸é‡åŒ–ç»“åˆä»¥è·å¾—è¿›ä¸€æ­¥çš„å‹ç¼©ã€‚
- en: '![Network Pruning](Images/nlpt_0810.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![ç½‘ç»œä¿®å‰ª](Images/nlpt_0810.png)'
- en: Figure 8-10\. Weights and neurons before and after pruning (courtesy of Song
    Han)
  id: totrans-295
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾8-10ã€‚ä¿®å‰ªå‰åçš„æƒé‡å’Œç¥ç»å…ƒï¼ˆç”±Song Hanæä¾›ï¼‰
- en: Weight Pruning Methods
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æƒé‡ä¿®å‰ªæ–¹æ³•
- en: 'Mathematically, the way most weight pruning methods work is to calculate a
    matrix <math alttext="bold upper S"><mi>ğ’</mi></math> of *importance scores* and
    then select the top <math alttext="k"><mi>k</mi></math> percent of weights by
    importance:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•°å­¦ä¸Šï¼Œå¤§å¤šæ•°æƒé‡ä¿®å‰ªæ–¹æ³•çš„å·¥ä½œæ–¹å¼æ˜¯è®¡ç®—ä¸€ä¸ª*é‡è¦æ€§åˆ†æ•°*çŸ©é˜µ<math alttext="bold upper S"><mi>ğ’</mi></math>ï¼Œç„¶åæŒ‰é‡è¦æ€§é€‰æ‹©å‰<math
    alttext="k"><mi>k</mi></math>ç™¾åˆ†æ¯”çš„æƒé‡ï¼š
- en: <math alttext="normal upper T normal o normal p Subscript k Baseline left-parenthesis
    bold upper S right-parenthesis Subscript i j Baseline equals StartLayout Enlarged
    left-brace 1st Row 1st Column 1 2nd Column Blank 3rd Column normal i normal f
    upper S Subscript i j Baseline normal i normal n normal t normal o normal p k
    percent-sign 2nd Row 1st Column 0 2nd Column Blank 3rd Column normal o normal
    t normal h normal e normal r normal w normal i normal s normal e EndLayout" display="block"><mrow><msub><mi>Top</mi>
    <mi>k</mi></msub> <msub><mrow><mo>(</mo><mi>ğ’</mi><mo>)</mo></mrow> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mn>1</mn></mrow></mtd>
    <mtd><mrow><mi>if</mi> <msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mi>in</mi> <mi>top</mi> <mi>k</mi> <mo>%</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd>
    <mtd><mi>otherwise</mi></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'In effect, <math alttext="k"><mi>k</mi></math> acts as a new hyperparameter
    to control the amount of sparsity in the modelâ€”that is, the proportion of weights
    that are zero-valued. Lower values of <math alttext="k"><mi>k</mi></math> correspond
    to sparser matrices. From these scores we can then define a *mask matrix* <math
    alttext="bold upper M"><mi>ğŒ</mi></math> that masks the weights <math alttext="upper
    W Subscript i j"><msub><mi>W</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>
    during the forward pass with some input <math alttext="x Subscript i"><msub><mi>x</mi>
    <mi>i</mi></msub></math> and effectively creates a sparse network of activations
    <math alttext="a Subscript i"><msub><mi>a</mi> <mi>i</mi></msub></math> :'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="a Subscript i Baseline equals sigma-summation Underscript k Endscripts
    upper W Subscript i k Baseline upper M Subscript i k Baseline x Subscript k" display="block"><mrow><msub><mi>a</mi>
    <mi>i</mi></msub> <mo>=</mo> <munder><mo>âˆ‘</mo> <mi>k</mi></munder> <msub><mi>W</mi>
    <mrow><mi>i</mi><mi>k</mi></mrow></msub> <msub><mi>M</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub>
    <msub><mi>x</mi> <mi>k</mi></msub></mrow></math>
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed in the tongue-in-cheek â€œOptimal Brain Surgeonâ€ paper,^([17](ch08.xhtml#idm46238704674352))
    at the heart of each pruning method are a set of questions that need to be considered:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Which weights should be eliminated?
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should the remaining weights be adjusted for best performance?
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can such network pruning be done in a computationally efficient way?
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The answers to these questions inform how the score matrix <math alttext="bold
    upper S"><mi>ğ’</mi></math> is computed, so letâ€™s begin by looking at one of the
    earliest and most popular pruning methods: magnitude pruning.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Magnitude pruning
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name suggests, magnitude pruning calculates the scores according to the
    magnitude of the weights <math alttext="bold upper S equals left-parenthesis bar
    upper W Subscript i j Baseline bar right-parenthesis Subscript 1 less-than-or-equal-to
    j comma j less-than-or-equal-to n"><mrow><mi>ğ’</mi> <mo>=</mo> <msub><mfenced
    separators="" open="(" close=")"><mo>âˆ£</mo> <msub><mi>W</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>âˆ£</mo></mfenced> <mrow><mn>1</mn><mo>â‰¤</mo><mi>j</mi><mo>,</mo><mi>j</mi><mo>â‰¤</mo><mi>n</mi></mrow></msub></mrow></math>
    and then derives the masks from <math alttext="bold upper M equals normal upper
    T normal o normal p Subscript k Baseline left-parenthesis bold upper S right-parenthesis"><mrow><mi>ğŒ</mi>
    <mo>=</mo> <msub><mi>Top</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi>ğ’</mi> <mo>)</mo></mrow></mrow></math>
    . In the literature it is common to apply magnitude pruning in an iterative fashion
    by first training the model to learn which connections are important and pruning
    the weights of least importance.^([18](ch08.xhtml#idm46238704650928)) The sparse
    model is then retrained and the process repeated until the desired sparsity is
    reached.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'One drawback with this approach is that it is computationally demanding: at
    every step of pruning we need to train the model to convergence. For this reason
    it is generally better to gradually increase the initial sparsity <math alttext="s
    Subscript i"><msub><mi>s</mi> <mi>i</mi></msub></math> (which is usually zero)
    to a final value <math alttext="s Subscript f"><msub><mi>s</mi> <mi>f</mi></msub></math>
    after some number of steps <math alttext="upper N"><mi>N</mi></math> :^([19](ch08.xhtml#idm46238704643440))'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–¹æ³•çš„ä¸€ä¸ªç¼ºç‚¹æ˜¯è®¡ç®—éœ€æ±‚é‡å¤§ï¼šåœ¨æ¯ä¸€æ­¥ä¿®å‰ªä¸­ï¼Œæˆ‘ä»¬éƒ½éœ€è¦å°†æ¨¡å‹è®­ç»ƒåˆ°æ”¶æ•›ã€‚å› æ­¤ï¼Œé€šå¸¸æœ€å¥½é€æ¸å¢åŠ åˆå§‹ç¨€ç–åº¦<math alttext="s Subscript
    i"><msub><mi>s</mi> <mi>i</mi></msub></math>ï¼ˆé€šå¸¸ä¸ºé›¶ï¼‰åˆ°ä¸€å®šæ­¥æ•°<math alttext="upper N"><mi>N</mi></math>åçš„æœ€ç»ˆå€¼<math
    alttext="s Subscript f"><msub><mi>s</mi> <mi>f</mi></msub></math>ã€‚^([19](ch08.xhtml#idm46238704643440))
- en: <math alttext="s Subscript t Baseline equals s Subscript f Baseline plus left-parenthesis
    s Subscript i Baseline minus s Subscript f Baseline right-parenthesis left-parenthesis
    1 minus StartFraction t minus t 0 Over upper N normal upper Delta t EndFraction
    right-parenthesis cubed normal f normal o normal r t element-of StartSet t 0 comma
    t 0 plus normal upper Delta t comma ellipsis comma t 0 plus upper N normal upper
    Delta t EndSet" display="block"><mrow><msub><mi>s</mi> <mi>t</mi></msub> <mo>=</mo>
    <msub><mi>s</mi> <mi>f</mi></msub> <mo>+</mo> <mrow><mo>(</mo> <msub><mi>s</mi>
    <mi>i</mi></msub> <mo>-</mo> <msub><mi>s</mi> <mi>f</mi></msub> <mo>)</mo></mrow>
    <msup><mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mfrac><mrow><mi>t</mi><mo>-</mo><msub><mi>t</mi>
    <mn>0</mn></msub></mrow> <mrow><mi>N</mi><mi>Î”</mi><mi>t</mi></mrow></mfrac></mfenced>
    <mn>3</mn></msup> <mi>for</mi> <mi>t</mi> <mo>âˆˆ</mo> <mrow><mo>{</mo> <msub><mi>t</mi>
    <mn>0</mn></msub> <mo>,</mo> <msub><mi>t</mi> <mn>0</mn></msub> <mo>+</mo> <mi>Î”</mi>
    <mi>t</mi> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>t</mi> <mn>0</mn></msub>
    <mo>+</mo> <mi>N</mi> <mi>Î”</mi> <mi>t</mi> <mo>}</mo></mrow></mrow></math>
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="s Subscript t Baseline equals s Subscript f Baseline plus left-parenthesis
    s Subscript i Baseline minus s Subscript f Baseline right-parenthesis left-parenthesis
    1 minus StartFraction t minus t 0 Over upper N normal upper Delta t EndFraction
    right-parenthesis cubed normal f normal o normal r t element-of StartSet t 0 comma
    t 0 plus normal upper Delta t comma ellipsis comma t 0 plus upper N normal upper
    Delta t EndSet" display="block"><mrow><msub><mi>s</mi> <mi>t</mi></msub> <mo>=</mo>
    <msub><mi>s</mi> <mi>f</mi></msub> <mo>+</mo> <mrow><mo>(</mo> <msub><mi>s</mi>
    <mi>i</mi></msub> <mo>-</mo> <msub><mi>s</mi> <mi>f</mi></msub> <mo>)</mo></mrow>
    <msup><mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mfrac><mrow><mi>t</mi><mo>-</mo><msub><mi>t</mi>
    <mn>0</mn></msub></mrow> <mrow><mi>N</mi><mi>Î”</mi><mi>t</mi></mrow></mfrac></mfenced>
    <mn>3</mn></msup> <mi>for</mi> <mi>t</mi> <mo>âˆˆ</mo> <mrow><mo>{</mo> <msub><mi>t</mi>
    <mn>0</mn></msub> <mo>,</mo> <msub><mi>t</mi> <mn>0</mn></msub> <mo>+</mo> <mi>Î”</mi>
    <mi>t</mi> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>t</mi> <mn>0</mn></msub>
    <mo>+</mo> <mi>N</mi> <mi>Î”</mi> <mi>t</mi> <mo>}</mo></mrow></mrow></math>
- en: Here the idea is to update the binary masks <math alttext="bold upper M"><mi>ğŒ</mi></math>
    every <math alttext="normal upper Delta t"><mrow><mi>Î”</mi> <mi>t</mi></mrow></math>
    steps to allow masked weights to reactivate during training and recover from any
    potential loss in accuracy that is induced by the pruning process. As shown in
    [FigureÂ 8-11](#sparsity-scheduler), the cubic factor implies that the rate of
    weight pruning is highest in the early phases (when the number of redundant weights
    is large) and gradually tapers off.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„æƒ³æ³•æ˜¯æ¯éš”<math alttext="normal upper Delta t"><mrow><mi>Î”</mi> <mi>t</mi></mrow></math>æ­¥æ›´æ–°ä¸€æ¬¡äºŒè¿›åˆ¶æ©ç <math
    alttext="bold upper M"><mi>ğŒ</mi></math>ï¼Œä»¥å…è®¸è¢«å±è”½çš„æƒé‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡æ–°æ¿€æ´»ï¼Œå¹¶ä»ä¿®å‰ªè¿‡ç¨‹ä¸­å¯èƒ½å¯¼è‡´çš„ä»»ä½•ç²¾åº¦æŸå¤±ä¸­æ¢å¤è¿‡æ¥ã€‚å¦‚[å›¾8-11](#sparsity-scheduler)æ‰€ç¤ºï¼Œç«‹æ–¹å› å­æ„å‘³ç€æƒé‡ä¿®å‰ªçš„é€Ÿç‡åœ¨æ—©æœŸé˜¶æ®µæœ€é«˜ï¼ˆå½“å†—ä½™æƒé‡æ•°é‡è¾ƒå¤§æ—¶ï¼‰ï¼Œå¹¶é€æ¸å‡å°ã€‚
- en: '![Sparsity scheduler](Images/nlpt_0811.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![ç¨€ç–è°ƒåº¦å™¨](Images/nlpt_0811.png)'
- en: Figure 8-11\. The cubic sparsity scheduler used for pruning
  id: totrans-312
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾8-11ã€‚ç”¨äºä¿®å‰ªçš„ç«‹æ–¹ç¨€ç–è°ƒåº¦å™¨ã€‚
- en: One problem with magnitude pruning is that it is really designed for pure supervised
    learning, where the importance of each weight is directly related to the task
    at hand. By contrast, in transfer learning the importance of the weights is primarily
    determined by the pretraining phase, so magnitude pruning can remove connections
    that are important for the fine-tuning task. Recently, an adaptive approach called
    movement pruning has been proposed by Hugging Face researchersâ€”letâ€™s take a look.^([20](ch08.xhtml#idm46238704604576))
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: å¹…åº¦ä¿®å‰ªçš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œå®ƒå®é™…ä¸Šæ˜¯ä¸ºçº¯ç›‘ç£å­¦ä¹ è€Œè®¾è®¡çš„ï¼Œå…¶ä¸­æ¯ä¸ªæƒé‡çš„é‡è¦æ€§ä¸æ‰‹å¤´çš„ä»»åŠ¡ç›´æ¥ç›¸å…³ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œåœ¨è¿ç§»å­¦ä¹ ä¸­ï¼Œæƒé‡çš„é‡è¦æ€§ä¸»è¦ç”±é¢„è®­ç»ƒé˜¶æ®µç¡®å®šï¼Œå› æ­¤å¹…åº¦ä¿®å‰ªå¯èƒ½ä¼šç§»é™¤å¯¹å¾®è°ƒä»»åŠ¡é‡è¦çš„è¿æ¥ã€‚æœ€è¿‘ï¼ŒHugging
    Faceçš„ç ”ç©¶äººå‘˜æå‡ºäº†ä¸€ç§ç§°ä¸ºç§»åŠ¨ä¿®å‰ªçš„è‡ªé€‚åº”æ–¹æ³•â€”â€”è®©æˆ‘ä»¬æ¥çœ‹ä¸€ä¸‹ã€‚^([20](ch08.xhtml#idm46238704604576))
- en: Movement pruning
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ç§»åŠ¨ä¿®å‰ª
- en: The basic idea behind movement pruning is to *gradually* remove weights during
    fine-tuning such that the model becomes progressively *sparser*. The key novelty
    is that both the weights and the scores are learned during fine-tuning. So, instead
    of being derived directly from the weights (like with magnitude pruning), the
    scores in movement pruning are arbitrary and are learned through gradient descent
    like any other neural network parameter. This implies that in the backward pass,
    we also track the gradient of the loss <math alttext="upper L"><mi>L</mi></math>
    with respect to the scores <math alttext="upper S Subscript i j"><msub><mi>S</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub></math> .
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ç§»åŠ¨ä¿®å‰ªèƒŒåçš„åŸºæœ¬æ€æƒ³æ˜¯*é€æ¸*åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ç§»é™¤æƒé‡ï¼Œä½¿æ¨¡å‹é€æ¸å˜å¾—*æ›´ç¨€ç–*ã€‚å…³é”®çš„æ–°é¢–ä¹‹å¤„åœ¨äºï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œæƒé‡å’Œåˆ†æ•°éƒ½æ˜¯å¯å­¦ä¹ çš„ã€‚å› æ­¤ï¼Œä¸å¹…åº¦ä¿®å‰ªç›´æ¥ä»æƒé‡æ´¾ç”Ÿï¼ˆå¦‚å¹…åº¦ä¿®å‰ªï¼‰ä¸åŒï¼Œç§»åŠ¨ä¿®å‰ªä¸­çš„åˆ†æ•°æ˜¯ä»»æ„çš„ï¼Œå¹¶ä¸”é€šè¿‡æ¢¯åº¦ä¸‹é™å­¦ä¹ ï¼Œå°±åƒä»»ä½•å…¶ä»–ç¥ç»ç½‘ç»œå‚æ•°ä¸€æ ·ã€‚è¿™æ„å‘³ç€åœ¨åå‘ä¼ æ’­ä¸­ï¼Œæˆ‘ä»¬è¿˜è¦è·Ÿè¸ªæŸå¤±<math
    alttext="upper L"><mi>L</mi></math>ç›¸å¯¹äºåˆ†æ•°<math alttext="upper S Subscript i j"><msub><mi>S</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>çš„æ¢¯åº¦ã€‚
- en: Once the scores are learned, it is then straightforward to generate the binary
    mask using <math alttext="bold upper M equals normal upper T normal o normal p
    Subscript k Baseline left-parenthesis bold upper S right-parenthesis"><mrow><mi>ğŒ</mi>
    <mo>=</mo> <msub><mi>Top</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi>ğ’</mi> <mo>)</mo></mrow></mrow></math>
    .^([21](ch08.xhtml#idm46238704590336))
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦å­¦ä¹ äº†åˆ†æ•°ï¼Œå°±å¾ˆå®¹æ˜“ä½¿ç”¨<math alttext="bold upper M equals normal upper T normal o normal
    p Subscript k Baseline left-parenthesis bold upper S right-parenthesis"><mrow><mi>ğŒ</mi>
    <mo>=</mo> <msub><mi>Top</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi>ğ’</mi> <mo>)</mo></mrow></mrow></math>ç”ŸæˆäºŒè¿›åˆ¶æ©ç ã€‚^([21](ch08.xhtml#idm46238704590336))
- en: The intuition behind movement pruning is that the weights that are â€œmovingâ€
    the most from zero are the most important ones to keep. In other words, the positive
    weights increase during fine-tuning (and vice versa for the negative weights),
    which is equivalent to saying that the scores increase as the weights move away
    from zero. As shown in [FigureÂ 8-12](#magnitude-vs-movement), this behavior differs
    from magnitude pruning, which selects as the most important weights those that
    are *furthest* from zero.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: è¿åŠ¨å‰ªæèƒŒåçš„ç›´è§‰æ˜¯ï¼Œâ€œç§»åŠ¨â€ç¦»é›¶æœ€è¿œçš„æƒé‡æ˜¯æœ€é‡è¦çš„ã€‚æ¢å¥è¯è¯´ï¼Œæ­£æƒé‡åœ¨ç²¾ç»†è°ƒæ•´æœŸé—´å¢åŠ ï¼ˆè´Ÿæƒé‡ç›¸åï¼‰ï¼Œè¿™ç›¸å½“äºè¯´åˆ†æ•°éšç€æƒé‡è¿œç¦»é›¶è€Œå¢åŠ ã€‚å¦‚[å›¾8-12](#magnitude-vs-movement)æ‰€ç¤ºï¼Œè¿™ç§è¡Œä¸ºä¸å¹…å€¼å‰ªæä¸åŒï¼Œåè€…é€‰æ‹©ç¦»é›¶æœ€è¿œçš„æƒé‡ä½œä¸ºæœ€é‡è¦çš„æƒé‡ã€‚
- en: '![Magnitude vs Movement Pruning](Images/nlpt_0812.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![å¹…å€¼ä¸è¿åŠ¨å‰ªæ](Images/nlpt_0812.png)'
- en: Figure 8-12\. Comparison of weights removed during magnitude pruning (left)
    and movement pruning (right)
  id: totrans-319
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾8-12ã€‚å¹…å€¼å‰ªæï¼ˆå·¦ï¼‰å’Œè¿åŠ¨å‰ªæï¼ˆå³ï¼‰ä¸­ç§»é™¤çš„æƒé‡çš„æ¯”è¾ƒ
- en: These differences between the two pruning methods are also evident in the distribution
    of the remaining weights. As shown in [FigureÂ 8-13](#pruning-dists), magnitude
    pruning produces two clusters of weights, while movement pruning produces a smoother
    distribution.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ç§å‰ªææ–¹æ³•ä¹‹é—´çš„å·®å¼‚ä¹Ÿåœ¨å‰©ä½™æƒé‡çš„åˆ†å¸ƒä¸­æ˜¾è€Œæ˜“è§ã€‚å¦‚[å›¾8-13](#pruning-dists)æ‰€ç¤ºï¼Œå¹…å€¼å‰ªæäº§ç”Ÿä¸¤ä¸ªæƒé‡ç°‡ï¼Œè€Œè¿åŠ¨å‰ªæäº§ç”Ÿæ›´å¹³æ»‘çš„åˆ†å¸ƒã€‚
- en: As of this bookâ€™s writing, ![nlpt_pin01](Images/nlpt_pin01.png) Transformers
    does not support pruning methods out of the box. Fortunately, there is a nifty
    library called [*Neural Networks Block Movement Pruning*](https://oreil.ly/aHEvD)
    that implements many of these ideas, and we recommend checking it out if memory
    constraints are a concern.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: æˆªè‡³æœ¬ä¹¦æ’°å†™æ—¶ï¼Œ![nlpt_pin01](Images/nlpt_pin01.png) Transformersä¸æ”¯æŒå¼€ç®±å³ç”¨çš„å‰ªææ–¹æ³•ã€‚å¹¸è¿çš„æ˜¯ï¼Œæœ‰ä¸€ä¸ªåä¸º[*ç¥ç»ç½‘ç»œå—è¿åŠ¨å‰ªæ*](https://oreil.ly/aHEvD)çš„å·§å¦™åº“å®ç°äº†è®¸å¤šè¿™äº›æƒ³æ³•ï¼Œå¦‚æœå†…å­˜é™åˆ¶æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å»ºè®®æŸ¥çœ‹å®ƒã€‚
- en: '![Pruning Distributions](Images/nlpt_0813.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![å‰ªæåˆ†å¸ƒ](Images/nlpt_0813.png)'
- en: Figure 8-13\. Distribution of remaining weights for magnitude pruning (MaP)
    and movement pruning (MvP)
  id: totrans-323
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾8-13ã€‚å‰©ä½™æƒé‡çš„åˆ†å¸ƒï¼Œç”¨äºå¹…å€¼å‰ªæï¼ˆMaPï¼‰å’Œè¿åŠ¨å‰ªæï¼ˆMvPï¼‰
- en: Conclusion
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç»“è®º
- en: 'Weâ€™ve seen that optimizing transformers for deployment in production environments
    involves compression along two dimensions: latency and memory footprint. Starting
    from a fine-tuned model, we applied distillation, quantization, and optimizations
    through ORT to significantly reduce both of these. In particular, we found that
    quantization and conversion in ORT gave the largest gains with minimal effort.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»çœ‹åˆ°ï¼Œä¼˜åŒ–å˜å‹å™¨ä»¥éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒä¸­æ¶‰åŠæ²¿ä¸¤ä¸ªç»´åº¦çš„å‹ç¼©ï¼šå»¶è¿Ÿå’Œå†…å­˜å ç”¨ã€‚ä»ç»è¿‡ç²¾ç»†è°ƒæ•´çš„æ¨¡å‹å¼€å§‹ï¼Œæˆ‘ä»¬åº”ç”¨äº†è’¸é¦ã€é‡åŒ–å’ŒORTä¼˜åŒ–ï¼Œæ˜¾è‘—å‡å°‘äº†è¿™ä¸¤è€…ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å‘ç°é‡åŒ–å’ŒORTä¸­çš„è½¬æ¢ç»™å‡ºäº†æœ€å¤§çš„æ”¶ç›Šï¼Œè€Œä»˜å‡ºçš„åŠªåŠ›æœ€å°ã€‚
- en: Although pruning is an effective strategy for reducing the storage size of transformer
    models, current hardware is not optimized for sparse matrix operations, which
    limits the usefulness of this technique. However, this is an active area of research,
    and by the time this book hits the shelves many of these limitations may have
    been resolved.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡å‰ªææ˜¯å‡å°‘å˜å‹å™¨æ¨¡å‹å­˜å‚¨å¤§å°çš„æœ‰æ•ˆç­–ç•¥ï¼Œä½†å½“å‰çš„ç¡¬ä»¶å¹¶æœªé’ˆå¯¹ç¨€ç–çŸ©é˜µè¿ç®—è¿›è¡Œä¼˜åŒ–ï¼Œè¿™é™åˆ¶äº†è¿™ç§æŠ€æœ¯çš„å®ç”¨æ€§ã€‚ç„¶è€Œï¼Œè¿™æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œåˆ°æœ¬ä¹¦ä¸Šå¸‚æ—¶ï¼Œè®¸å¤šè¿™äº›é™åˆ¶å¯èƒ½å·²ç»å¾—åˆ°è§£å†³ã€‚
- en: So where to from here? All of the techniques in this chapter can be adapted
    to other tasks, such as question answering, named entity recognition, or language
    modeling. If you find yourself struggling to meet the latency requirements or
    your model is eating up all your compute budget, we suggest giving one of them
    a try.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆæ¥ä¸‹æ¥å‘¢ï¼Ÿæœ¬ç« ä¸­çš„æ‰€æœ‰æŠ€æœ¯éƒ½å¯ä»¥åº”ç”¨åˆ°å…¶ä»–ä»»åŠ¡ä¸­ï¼Œæ¯”å¦‚é—®ç­”ã€å‘½åå®ä½“è¯†åˆ«æˆ–è¯­è¨€å»ºæ¨¡ã€‚å¦‚æœæ‚¨å‘ç°è‡ªå·±éš¾ä»¥æ»¡è¶³å»¶è¿Ÿè¦æ±‚ï¼Œæˆ–è€…æ‚¨çš„æ¨¡å‹å ç”¨äº†æ‰€æœ‰çš„è®¡ç®—é¢„ç®—ï¼Œæˆ‘ä»¬å»ºè®®å°è¯•å…¶ä¸­ä¹‹ä¸€ã€‚
- en: 'In the next chapter, weâ€™ll switch gears away from performance optimization
    and explore every data scientistâ€™s worst nightmare: dealing with few to no labels.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†æ‘†è„±æ€§èƒ½ä¼˜åŒ–ï¼Œæ¢è®¨æ¯ä¸ªæ•°æ®ç§‘å­¦å®¶çš„å™©æ¢¦ï¼šå¤„ç†å°‘é‡æˆ–æ²¡æœ‰æ ‡ç­¾çš„æƒ…å†µã€‚
- en: ^([1](ch08.xhtml#idm46238709482512-marker)) S. Larson et al., [â€œAn Evaluation
    Dataset for Intent Classification and Out-of-Scope Predictionâ€](https://arxiv.org/abs/1909.02027),
    (2019).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch08.xhtml#idm46238709482512-marker)) S. Larsonç­‰äººï¼Œ[â€œæ„å›¾åˆ†ç±»å’Œè¶…å‡ºèŒƒå›´é¢„æµ‹çš„è¯„ä¼°æ•°æ®é›†â€](https://arxiv.org/abs/1909.02027)ï¼Œï¼ˆ2019å¹´ï¼‰ã€‚
- en: ^([2](ch08.xhtml#idm46238709422384-marker)) As described by Emmanuel Ameisen
    in *Building Machine Learning Powered Applications* (Oâ€™Reilly), business or product
    metrics are the *most* important ones to consider. After all, it doesnâ€™t matter
    how accurate your model is if it doesnâ€™t solve a problem your business cares about.
    In this chapter weâ€™ll assume that you have already defined the metrics that matter
    for your application and focus on optimizing the model metrics.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch08.xhtml#idm46238709422384-marker)) æ­£å¦‚Emmanuel Ameisenåœ¨*æ„å»ºæœºå™¨å­¦ä¹ é©±åŠ¨çš„åº”ç”¨*ï¼ˆO'Reillyï¼‰ä¸­æ‰€æè¿°çš„ï¼Œä¸šåŠ¡æˆ–äº§å“æŒ‡æ ‡æ˜¯*æœ€*é‡è¦çš„è€ƒè™‘å› ç´ ã€‚æ¯•ç«Ÿï¼Œå¦‚æœæ‚¨çš„æ¨¡å‹ä¸èƒ½è§£å†³ä¸šåŠ¡å…³å¿ƒçš„é—®é¢˜ï¼Œé‚£ä¹ˆå®ƒçš„å‡†ç¡®æ€§å°±æ— å…³ç´§è¦ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†å‡è®¾æ‚¨å·²ç»ä¸ºåº”ç”¨ç¨‹åºå®šä¹‰äº†é‡è¦çš„æŒ‡æ ‡ï¼Œå¹¶ä¸“æ³¨äºä¼˜åŒ–æ¨¡å‹æŒ‡æ ‡ã€‚
- en: '^([3](ch08.xhtml#idm46238708497152-marker)) C. BuciluÄƒ et al., â€œModel Compression,â€
    *Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining* (August 2006): 535â€“541, [*https://doi.org/10.1145/1150402.1150464*](https://doi.org/10.1145/1150402.1150464).'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch08.xhtml#idm46238708497152-marker)) C. BuciluÄƒç­‰äººï¼Œâ€œæ¨¡å‹å‹ç¼©â€ï¼Œ*ç¬¬12å±ŠACM SIGKDDå›½é™…çŸ¥è¯†å‘ç°å’Œæ•°æ®æŒ–æ˜ä¼šè®®è®ºæ–‡é›†*ï¼ˆ2006å¹´8æœˆï¼‰ï¼š535-541ï¼Œ[*https://doi.org/10.1145/1150402.1150464*](https://doi.org/10.1145/1150402.1150464)ã€‚
- en: ^([4](ch08.xhtml#idm46238708494384-marker)) G. Hinton, O. Vinyals, and J. Dean,
    [â€œDistilling the Knowledge in a Neural Networkâ€](https://arxiv.org/abs/1503.02531),
    (2015).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch08.xhtml#idm46238708494384-marker)) G. Hinton, O. Vinyalså’ŒJ. Deanï¼Œ[â€œè’¸é¦ç¥ç»ç½‘ç»œä¸­çš„çŸ¥è¯†â€](https://arxiv.org/abs/1503.02531)ï¼Œï¼ˆ2015å¹´ï¼‰ã€‚
- en: '^([5](ch08.xhtml#idm46238708466160-marker)) W. Fedus, B. Zoph, and N. Shazeer,
    [â€œSwitch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient
    Sparsityâ€](https://arxiv.org/abs/2101.03961), (2021).'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch08.xhtml#idm46238708457120-marker)) Geoff Hinton coined this term in
    a [talk](https://oreil.ly/OkHGp) to refer to the observation that softened probabilities
    reveal the hidden knowledge of the teacher.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch08.xhtml#idm46238708428448-marker)) We also encountered temperature
    in the context of text generation in [ChapterÂ 5](ch05.xhtml#chapter_generation).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch08.xhtml#idm46238708343392-marker)) V. Sanh et al., [â€œDistilBERT, a
    Distilled Version of BERT: Smaller, Faster, Cheaper and Lighterâ€](https://arxiv.org/abs/1910.01108),
    (2019).'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch08.xhtml#idm46238707933552-marker)) Y. Kim and H. Awadalla, [â€œFastFormers:
    Highly Efficient Transformer Models for Natural Language Understandingâ€](https://arxiv.org/abs/2010.13382),
    (2020).'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch08.xhtml#idm46238707834608-marker)) By default, the `Trainer` looks
    for a column called `labels` when fine-tuning on classification tasks. You can
    also override this behavior by specifying the `label_names` argument of `TrainingArguments`.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch08.xhtml#idm46238707732144-marker)) This approach of fine-tuning a
    general-purpose, distilled language model is sometimes referred to as â€œtask-agnosticâ€
    distillation.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '^([12](ch08.xhtml#idm46238706954496-marker)) T. Akiba et al., [â€œOptuna: A Next-Generation
    Hyperparameter Optimization Frameworkâ€](https://arxiv.org/abs/1907.10902), (2019).'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch08.xhtml#idm46238706293280-marker)) An affine map is just a fancy name
    for the <math alttext="y equals upper A x plus b"><mrow><mi>y</mi> <mo>=</mo>
    <mi>A</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi></mrow></math> map that youâ€™re familiar
    with in the linear layers of a neural network.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch08.xhtml#idm46238705705264-marker)) There is a separate standard called
    ONNX-ML that is designed for traditional machine learning models like random forests
    and frameworks like Scikit-learn.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch08.xhtml#idm46238705675632-marker)) Other popular accelerators include
    [NVIDIAâ€™s TensorRT](https://oreil.ly/HnNZx) and [Apache TVM](https://oreil.ly/7KUyt).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch08.xhtml#idm46238705672032-marker)) A fused operation involves merging
    one operator (usually an activation function) into another so that they can be
    executed together. For example, suppose we want to apply an activation *f* to
    a matrix product *A* Ã— *B*. Normally the result of the product needs to be written
    back to the GPU memory before the activation is computed. Operator fusion allows
    as to compute <math alttext="f left-parenthesis upper A times upper B right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>A</mi> <mo>Ã—</mo> <mi>B</mi> <mo>)</mo></mrow></math> in a single
    step. Constant folding refers to the process of evaluating constant expressions
    at compile time instead of runtime.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '^([17](ch08.xhtml#idm46238704674352-marker)) B. Hassibi and D. Stork, â€œSecond
    Order Derivatives for Network Pruning: Optimal Brain Surgeon,â€ *Proceedings of
    the 5th International Conference on Neural Information Processing Systems* (November
    1992): 164â€“171, [*https://papers.nips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html*](https://papers.nips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html).'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch08.xhtml#idm46238704650928-marker)) S. Han et al., [â€œLearning Both
    Weights and Connections for Efficient Neural Networksâ€](https://arxiv.org/abs/1506.02626),
    (2015).
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '^([19](ch08.xhtml#idm46238704643440-marker)) M. Zhu and S. Gupta, [â€œTo Prune,
    or Not to Prune: Exploring the Efficacy of Pruning for Model Compressionâ€](https://arxiv.org/abs/1710.01878),
    (2017).'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '^([20](ch08.xhtml#idm46238704604576-marker)) V. Sanh, T. Wolf, and A.M. Rush,
    [â€œMovement Pruning: Adaptive Sparsity by Fine-Tuningâ€](https://arxiv.org/abs/2005.07683),
    (2020).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '^([21](ch08.xhtml#idm46238704590336-marker)) There is also a â€œsoftâ€ version
    of movement pruning where instead of picking the top <math alttext="k"><mi>k</mi></math>
    % of weights, one uses a global threshold <math alttext="tau"><mi>Ï„</mi></math>
    to define the binary mask: <math alttext="bold upper M equals left-parenthesis
    bold upper S greater-than tau right-parenthesis"><mrow><mi>ğŒ</mi> <mo>=</mo> <mo>(</mo>
    <mi>ğ’</mi> <mo>></mo> <mi>Ï„</mi> <mo>)</mo></mrow></math> .'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch08.xhtml#idm46238704590336-marker)) è¿˜æœ‰ä¸€ç§â€œè½¯â€ç‰ˆæœ¬çš„ç§»åŠ¨ä¿®å‰ªï¼Œå…¶ä¸­ä¸æ˜¯é€‰æ‹©æƒé‡çš„å‰<math
    alttext="k"><mi>k</mi></math> %ï¼Œè€Œæ˜¯ä½¿ç”¨å…¨å±€é˜ˆå€¼<math alttext="tau"><mi>Ï„</mi></math>æ¥å®šä¹‰äºŒè¿›åˆ¶æ©ç ï¼š<math
    alttext="bold upper M equals left-parenthesis bold upper S greater-than tau right-parenthesis"><mrow><mi>ğŒ</mi>
    <mo>=</mo> <mo>(</mo> <mi>ğ’</mi> <mo>></mo> <mi>Ï„</mi> <mo>)</mo></mrow></math>ã€‚
