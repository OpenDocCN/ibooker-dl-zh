- en: Chapter 8\. Making Transformers Efficient in Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, you’ve seen how transformers can be fine-tuned to
    produce great results on a wide range of tasks. However, in many situations accuracy
    (or whatever metric you’re optimizing for) is not enough; your state-of-the-art
    model is not very useful if it’s too slow or large to meet the business requirements
    of your application. An obvious alternative is to train a faster and more compact
    model, but the reduction in model capacity is often accompanied by a degradation
    in performance. So what can you do when you need a fast, compact, yet highly accurate
    model?
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will explore four complementary techniques that can be used
    to speed up the predictions and reduce the memory footprint of your transformer
    models: *knowledge distillation*, *quantization*, *pruning*, and *graph optimization*
    with the Open Neural Network Exchange (ONNX) format and ONNX Runtime (ORT). We’ll
    also see how some of these techniques can be combined to produce significant performance
    gains. For example, this was the approach taken by the Roblox engineering team
    in their article [“How We Scaled Bert to Serve 1+ Billion Daily Requests on CPUs”](https://oreil.ly/QdNIk),
    who as shown in [Figure 8-1](#roblox) found that combining knowledge distillation
    and quantization enabled them to improve the latency and throughput of their BERT
    classifier by over a factor of 30!'
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling BERT at Roblox](Images/nlpt_0801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. How Roblox scaled BERT with knowledge distillation, dynamic padding,
    and weight quantization (photo courtesy of Roblox employees Quoc N. Le and Kip
    Kaehler)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To illustrate the benefits and trade-offs associated with each technique, we’ll
    use intent detection as a case study; this is an important component of text-based
    assistants, where low latencies are critical for maintaining a conversation in
    real time. Along the way you’ll learn how to create custom trainers, perform efficient
    hyperparameter search, and gain a sense of what it takes to implement cutting-edge
    research with ![nlpt_pin01](Images/nlpt_pin01.png) Transformers. Let’s dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Intent Detection as a Case Study
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s suppose that we’re trying to build a text-based assistant for our company’s
    call center so that customers can request their account balance or make bookings
    without needing to speak with a human agent. In order to understand the goals
    of a customer, our assistant will need to be able to classify a wide variety of
    natural language text into a set of predefined actions or *intents*. For example,
    a customer might send a message like the following about an upcoming trip:'
  prefs: []
  type: TYPE_NORMAL
- en: Hey, I’d like to rent a vehicle from Nov 1st to Nov 15th in Paris and I need
    a 15 passenger van
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: and our intent classifier could automatically categorize this as a *Car Rental*
    intent, which then triggers an action and response. To be robust in a production
    environment, our classifier will also need to be able to handle *out-of-scope*
    queries, where a customer makes a query that doesn’t belong to any of the predefined
    intents and the system should yield a fallback response. For example, in the second
    case shown in [Figure 8-2](#oos), a customer asks a question about sports (which
    is out of scope), and the text assistant mistakenly classifies it as one of the
    known in-scope intents and returns the payday response. In the third case, the
    text assistant has been trained to detect out-of-scope queries (usually labeled
    as a separate class) and informs the customer about which topics it can answer
    questions about.
  prefs: []
  type: TYPE_NORMAL
- en: '![Out of Scope Query](Images/nlpt_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Three exchanges between a human (right) and a text-based assistant
    (left) for personal finance (courtesy of Stefan Larson et al.)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As a baseline, we’ve fine-tuned a BERT-base model that achieves around 94% accuracy
    on the CLINC150 dataset.^([1](ch08.xhtml#idm46238709482512)) This dataset includes
    22,500 in-scope queries across 150 intents and 10 domains like banking and travel,
    and also includes 1,200 out-of-scope queries that belong to an `oos` intent class.
    In practice we would also gather our own in-house dataset, but using public data
    is a great way to iterate quickly and generate preliminary results.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, let’s download our fine-tuned model from the Hugging Face Hub
    and wrap it in a pipeline for text classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a pipeline, we can pass a query to get the predicted intent
    and confidence score from the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Great, the `car_rental` intent makes sense. Let’s now look at creating a benchmark
    that we can use to evaluate the performance of our baseline model.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Performance Benchmark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like other machine learning models, deploying transformers in production environments
    involves a trade-off among several constraints, the most common being:^([2](ch08.xhtml#idm46238709422384))
  prefs: []
  type: TYPE_NORMAL
- en: '*Model performance*'
  prefs: []
  type: TYPE_NORMAL
- en: How well does our model perform on a well-crafted test set that reflects production
    data? This is especially important when the cost of making errors is large (and
    best mitigated with a human in the loop), or when we need to run inference on
    millions of examples and small improvements to the model metrics can translate
    into large gains in aggregate.
  prefs: []
  type: TYPE_NORMAL
- en: '*Latency*'
  prefs: []
  type: TYPE_NORMAL
- en: How fast can our model deliver predictions? We usually care about latency in
    real-time environments that deal with a lot of traffic, like how Stack Overflow
    needed a classifier to quickly [detect unwelcome comments on the website](https://oreil.ly/cf7QX).
  prefs: []
  type: TYPE_NORMAL
- en: '*Memory*'
  prefs: []
  type: TYPE_NORMAL
- en: How can we deploy billion-parameter models like GPT-2 or T5 that require gigabytes
    of disk storage and RAM? Memory plays an especially important role in mobile or
    edge devices, where a model has to generate predictions without access to a powerful
    cloud server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Failing to address these constraints can have a negative impact on the user
    experience of your application. More commonly, it can lead to ballooning costs
    from running expensive cloud servers that may only need to handle a few requests.
    To explore how each of these constraints can be optimized with various compression
    techniques, let’s begin by creating a simple benchmark that measures each quantity
    for a given pipeline and test set. A skeleton of what we’ll need is given by the
    following class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We’ve defined an `optim_type` parameter to keep track of the different optimization
    techniques that we’ll cover in this chapter. We’ll use the `run_benchmark()` method
    to collect all the metrics in a dictionary, with keys given by `optim_type`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now put some flesh on the bones of this class by computing the model
    accuracy on the test set. First we need some data to test on, so let’s download
    the CLINC150 dataset that was used to fine-tune our baseline model. We can get
    the dataset from the Hub with ![nlpt_pin01](Images/nlpt_pin01.png) Datasets as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the `plus` configuration refers to the subset that contains the out-of-scope
    training examples. Each example in the CLINC150 dataset consists of a query in
    the `text` column and its corresponding intent. We’ll use the test set to benchmark
    our models, so let’s take a look at one of the dataset’s examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The intents are provided as IDs, but we can easily get the mapping to strings
    (and vice versa) by accessing the `features` attribute of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a basic understanding of the contents in the CLINC150 dataset,
    let’s implement the `compute_accuracy()` method of `PerformanceBenchmark`. Since
    the dataset is balanced across the intent classes, we’ll use accuracy as our metric.
    We can load this metric with ![nlpt_pin01](Images/nlpt_pin01.png) Datasets as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The accuracy metric expects the predictions and references (i.e., the ground
    truth labels) to be integers. We can use the pipeline to extract the predictions
    from the `text` field and then use the `str2int()` method of our `intents` object
    to map each prediction to its corresponding ID. The following code collects all
    the predictions and labels in lists before returning the accuracy on the dataset.
    Let’s also add it to our `Perform​an⁠ce​Benchmark` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s compute the size of our model by using the `torch.save()` function
    from PyTorch to serialize the model to disk. Under the hood, `torch.save()` uses
    Python’s `pickle` module and can be used to save anything from models to tensors
    to ordinary Python objects. In PyTorch, the recommended way to save a model is
    by using its `state_dict`, which is a Python dictionary that maps each layer in
    a model to its learnable parameters (i.e., weights and biases). Let’s see what
    is stored in the `state_dict` of our baseline model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can clearly see that each key/value pair corresponds to a specific layer
    and tensor in BERT. So if we save our model with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'we can then use the `Path.stat()` function from Python’s `pathlib` module to
    get information about the underlying files. In particular, `Path(​"model.​pt").​stat().​st_size`
    will give us the model size in bytes. Let’s put this all together in the `com⁠pute_​size()`
    function and add it to `PerformanceBenchmark`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Finally let’s implement the `time_pipeline()` function so that we can time the
    average latency per query. For this application, latency refers to the time it
    takes to feed a text query to the pipeline and return the predicted intent from
    the model. Under the hood the pipeline also tokenizes the text, but this is around
    one thousand times faster than generating the predictions and thus adds a negligible
    contribution to the overall latency. A simple way to measure the execution time
    of a code snippet is to use the `perf_counter()` function from Python’s `time`
    module. This function has a better time resolution than the `time.time()` function
    and is well suited for getting precise results.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `perf_counter()` to time our pipeline by passing our test query
    and calculating the time difference in milliseconds between the start and end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'These results exhibit quite some spread in the latencies and suggest that timing
    a single pass through the pipeline can give wildly different results each time
    we run the code. So instead, we’ll collect the latencies over many runs and then
    use the resulting distribution to calculate the mean and standard deviation, which
    will give us an idea about the spread in values. The following code does what
    we need and includes a phase to warm up the CPU before performing the actual timed
    run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: To keeps things simple, we’ll use the same `query` value to benchmark all our
    models. In general, the latency will depend on the query length, and a good practice
    is to benchmark your models with queries that they’re likely to encounter in production
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our `PerformanceBenchmark` class is complete, let’s give it a spin!
    Let’s start by benchmarking our BERT baseline. For the baseline model, we just
    need to pass the pipeline and the dataset we wish to perform the benchmark on.
    We’ll collect the results in the `perf_metrics` dictionary to keep track of each
    model’s performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a reference point, let’s look at our first compression technique:
    knowledge distillation.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The average latency values will differ depending on what type of hardware you
    are running on. For example, you can usually get better performance by running
    inference on a GPU since it enables batch processing. For the purposes of this
    chapter, what’s important is the relative difference in latencies between models.
    Once we have determined the best-performing model, we can then explore different
    backends to reduce the absolute latency if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Making Models Smaller via Knowledge Distillation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knowledge distillation is a general-purpose method for training a smaller *student*
    model to mimic the behavior of a slower, larger, but better-performing *teacher*.
    Originally introduced in 2006 in the context of ensemble models,^([3](ch08.xhtml#idm46238708497152))
    it was later popularized in a famous 2015 paper that generalized the method to
    deep neural networks and applied it to image classification and automatic speech
    recognition.^([4](ch08.xhtml#idm46238708494384))
  prefs: []
  type: TYPE_NORMAL
- en: Given the trend toward pretraining language models with ever-increasing parameter
    counts (the largest at the time of writing having over one trillion parameters),^([5](ch08.xhtml#idm46238708466160))
    knowledge distillation has also become a popular strategy to compress these huge
    models and make them more suitable for building practical applications.
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge Distillation for Fine-Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So how is knowledge actually “distilled” or transferred from the teacher to
    the student during training? For supervised tasks like fine-tuning, the main idea
    is to augment the ground truth labels with a distribution of “soft probabilities”
    from the teacher which provide complementary information for the student to learn
    from. For example, if our BERT-base classifier assigns high probabilities to multiple
    intents, then this could be a sign that these intents lie close to each other
    in the feature space. By training the student to mimic these probabilities, the
    goal is to distill some of this “dark knowledge”^([6](ch08.xhtml#idm46238708457120))
    that the teacher has learned—that is, knowledge that is not available from the
    labels alone.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the way this works is as follows. Suppose we feed an input
    sequence *x* to the teacher to generate a vector of logits <math alttext="bold
    z left-parenthesis x right-parenthesis"><mrow><mi>𝐳</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></math> = [ <math alttext="z 1 left-parenthesis x right-parenthesis
    comma ellipsis comma z Subscript upper N Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>z</mi>
    <mn>1</mn></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>z</mi> <mi>N</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    ]. We can convert these logits into probabilities by applying a softmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis
    x right-parenthesis right-parenthesis Over sigma-summation Underscript j Endscripts
    exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    right-parenthesis EndFraction" display="block"><mrow><mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow>
    <mrow><msub><mo>∑</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This isn’t quite what we want, though, because in many cases the teacher will
    assign a high probability to one class, with all other class probabilities close
    to zero. When that happens, the teacher doesn’t provide much additional information
    beyond the ground truth labels, so instead we “soften” the probabilities by scaling
    the logits with a temperature hyperparameter *T* before applying the softmax:^([7](ch08.xhtml#idm46238708428448))
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="p Subscript i Baseline left-parenthesis x right-parenthesis equals
    StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    slash upper T right-parenthesis Over sigma-summation Underscript j Endscripts
    exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    slash upper T right-parenthesis EndFraction" display="block"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mo
    form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi> <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow>
    <mrow><msub><mo>∑</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: As shown in [Figure 8-3](#soft-probs), higher values of *T* produce a softer
    probability distribution over the classes and reveal much more information about
    the decision boundary that the teacher has learned for each training example.
    When <math alttext="upper T equals 1"><mrow><mi>T</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    we recover the original softmax distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Soft Probabilities](Images/nlpt_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. Comparison of a hard label that is one-hot encoded (left), softmax
    probabilities (middle), and softened class probabilities (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Since the student also produces softened probabilities <math alttext="q Subscript
    i Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>i</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> of its own, we can
    use the [Kullback–Leibler (KL)](https://oreil.ly/8nKQG) divergence to measure
    the difference between the two probability distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper D Subscript upper K upper L Baseline left-parenthesis p
    comma q right-parenthesis equals sigma-summation Underscript i Endscripts p Subscript
    i Baseline left-parenthesis x right-parenthesis log StartFraction p Subscript
    i Baseline left-parenthesis x right-parenthesis Over q Subscript i Baseline left-parenthesis
    x right-parenthesis EndFraction" display="block"><mrow><msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo> <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munder><mo>∑</mo> <mi>i</mi></munder> <msub><mi>p</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mfrac><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>q</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'With the KL divergence we can calculate how much is lost when we approximate
    the probability distribution of the teacher with the student. This allows us to
    define a knowledge distillation loss:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper L Subscript upper K upper D Baseline equals upper T squared
    upper D Subscript upper K upper L" display="block"><mrow><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub>
    <mo>=</mo> <msup><mi>T</mi> <mn>2</mn></msup> <msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'where <math alttext="upper T squared"><msup><mi>T</mi> <mn>2</mn></msup></math>
    is a normalization factor to account for the fact that the magnitude of the gradients
    produced by soft labels scales as <math alttext="1 slash upper T squared"><mrow><mn>1</mn>
    <mo>/</mo> <msup><mi>T</mi> <mn>2</mn></msup></mrow></math> . For classification
    tasks, the student loss is then a weighted average of the distillation loss with
    the usual cross-entropy loss <math alttext="upper L Subscript upper C upper E"><msub><mi>L</mi>
    <mrow><mi>C</mi><mi>E</mi></mrow></msub></math> of the ground truth labels:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper L Subscript normal s normal t normal u normal d normal
    e normal n normal t Baseline equals alpha upper L Subscript upper C upper E Baseline
    plus left-parenthesis 1 minus alpha right-parenthesis upper L Subscript upper
    K upper D" display="block"><mrow><msub><mi>L</mi> <mi>student</mi></msub> <mo>=</mo>
    <mi>α</mi> <msub><mi>L</mi> <mrow><mi>C</mi><mi>E</mi></mrow></msub> <mo>+</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>α</mi> <mo>)</mo></mrow> <msub><mi>L</mi>
    <mrow><mi>K</mi><mi>D</mi></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="alpha"><mi>α</mi></math> is a hyperparameter that controls
    the relative strength of each loss. A diagram of the whole process is shown in
    [Figure 8-4](#kd); the temperature is set to 1 at inference time to recover the
    standard softmax probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '![Knowledge distillation](Images/nlpt_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. The knowledge distillation process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Knowledge Distillation for Pretraining
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Knowledge distillation can also be used during pretraining to create a general-purpose
    student that can be subsequently fine-tuned on downstream tasks. In this case,
    the teacher is a pretrained language model like BERT, which transfers its knowledge
    about masked language modeling to the student. For example, in the DistilBERT
    paper,^([8](ch08.xhtml#idm46238708343392)) the masked language modeling loss <math
    alttext="upper L Subscript m l m"><msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub></math>
    is augmented with a term from knowledge distillation and a cosine embedding loss
    <math alttext="upper L Subscript c o s Baseline equals 1 minus cosine left-parenthesis
    h Subscript s Baseline comma h Subscript t Baseline right-parenthesis"><mrow><msub><mi>L</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub> <mo>=</mo> <mn>1</mn> <mo>-</mo>
    <mo form="prefix">cos</mo> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>s</mi></msub>
    <mo>,</mo> <msub><mi>h</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
    to align the directions of the hidden state vectors between the teacher and student:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper L Subscript normal upper D normal i normal s normal t normal
    i normal l normal upper B normal upper E normal upper R normal upper T Baseline
    equals alpha upper L Subscript m l m Baseline plus beta upper L Subscript upper
    K upper D Baseline plus gamma upper L Subscript c o s" display="block"><mrow><msub><mi>L</mi>
    <mi>DistilBERT</mi></msub> <mo>=</mo> <mi>α</mi> <msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub>
    <mo>+</mo> <mi>β</mi> <msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub>
    <mo>+</mo> <mi>γ</mi> <msub><mi>L</mi> <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Since we already have a fine-tuned BERT-base model, let’s see how we can use
    knowledge distillation to fine-tune a smaller and faster model. To do that we’ll
    need a way to augment the cross-entropy loss with an <math alttext="upper L Subscript
    upper K upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math>
    term. Fortunately we can do this by creating our own trainer!
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Knowledge Distillation Trainer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To implement knowledge distillation we need to add a few things to the `Trainer`
    base class:'
  prefs: []
  type: TYPE_NORMAL
- en: The new hyperparameters <math alttext="alpha"><mi>α</mi></math> and *T*, which
    control the relative weight of the distillation loss and how much the probability
    distribution of the labels should be smoothed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fine-tuned teacher model, which in our case is BERT-base
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A new loss function that combines the cross-entropy loss with the knowledge
    distillation loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adding the new hyperparameters is quite simple, since we just need to subclass
    `TrainingArguments` and include them as new attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'For the trainer itself, we need a new loss function. The way to implement this
    is by subclassing `Trainer` and overriding the `compute_loss()` method to include
    the knowledge distillation loss term <math alttext="upper L Subscript upper K
    upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Let’s unpack this code a bit. When we instantiate `DistillationTrainer` we pass
    a `teacher_model` argument with a teacher that has already been fine-tuned on
    our task. Next, in the `compute_loss()` method we extract the logits from the
    student and teacher, scale them by the temperature, and then normalize them with
    a softmax before passing them to PyTorch’s `nn.KLDivLoss()` function for computing
    the KL divergence. One quirk with `nn.KLDivLoss()` is that it expects the inputs
    in the form of log probabilities and the labels as normal probabilities. That’s
    why we’ve used the `F.log_softmax()` function to normalize the student’s logits,
    while the teacher’s logits are converted to probabilities with a standard softmax.
    The `reduction=batchmean` argument in `nn.KLDivLoss()` specifies that we average
    the losses over the batch dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also perform knowledge distillation with the Keras API of the ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers library. To do this, you’ll need to implement a custom `Distiller`
    class that overrides the `train_step()`, `test_step()`, and `compile()` methods
    of `tf.keras.Model()`. See the [Keras documentation](https://oreil.ly/6qp0F) for
    an example of how to do this.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing a Good Student Initialization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have our custom trainer, the first question you might have is which
    pretrained language model should we pick for the student? In general we should
    pick a smaller model for the student to reduce the latency and memory footprint.
    A good rule of thumb from the literature is that knowledge distillation works
    best when the teacher and student are of the same *model type*.^([9](ch08.xhtml#idm46238707933552))
    One possible reason for this is that different model types, say BERT and RoBERTa,
    can have different output embedding spaces, which hinders the ability of the student
    to mimic the teacher. In our case study the teacher is BERT, so DistilBERT is
    a natural candidate to initialize the student with since it has 40% fewer parameters
    and has been shown to achieve strong results on downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'First we’ll need to tokenize and encode our queries, so let’s instantiate the
    tokenizer from DistilBERT and create a simple `tokenize_text()` function to take
    care of the preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Here we’ve removed the `text` column since we no longer need it, and we’ve also
    renamed the `intent` column to `labels` so it can be automatically detected by
    the trainer.^([10](ch08.xhtml#idm46238707834608))
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve processed our texts, the next thing we need to do is define
    the hyperparameters and `compute_metrics()` function for our `DistillationTrainer`.
    We’ll also push all of our models to the Hugging Face Hub, so let’s start by logging
    in to our account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll define the metrics to track during training. As we did in the performance
    benchmark, we’ll use accuracy as the main metric. This means we can reuse our
    `accuracy_score()` function in the `compute_metrics()` function that we’ll include
    in `DistillationTrainer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In this function, the predictions from the sequence modeling head come in the
    form of logits, so we use the `np.argmax()` function to find the most confident
    class prediction and compare that against the ground truth label.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we need to define the training arguments. To warm up, we’ll set <math
    alttext="alpha equals 1"><mrow><mi>α</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    to see how well DistilBERT performs without any signal from the teacher.^([11](ch08.xhtml#idm46238707732144))
    Then we will push our fine-tuned model to a new repository called `distilbert-base-uncased-finetuned-clinc`,
    so we just need to specify that in the `output_dir` argument of `DistillationTrainingArguments`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We’ve also tweaked a few of the default hyperparameter values, like the number
    of epochs, the weight decay, and the learning rate. The next thing to do is initialize
    a student model. Since we will be doing multiple runs with the trainer, we’ll
    create a `student_init()` function to initialize the model with each new run.
    When we pass this function to the `DistillationTrainer`, this will ensure we initialize
    a new model each time we call the `train()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'One other thing we need to do is provide the student model with the mappings
    between each intent and label ID. These mappings can be obtained from our BERT-base
    model that we downloaded in the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'With these mappings, we can now create a custom model configuration with the
    `AutoConfig` class hat we encountered in Chapters [3](ch03.xhtml#chapter_anatomy)
    and [4](ch04.xhtml#chapter_ner). Let’s use this to create a configuration for
    our student with the information about the label mappings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we’ve also specified the number of classes our model should expect. We
    can then provide this configuration to the `from_pretrained()` function of the
    `AutoModelForSequenceClassification` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have all the ingredients needed for our distillation trainer, so let’s
    load the teacher and fine-tune:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '| Epoch | Training Loss | Validation Loss | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 4.2923 | 3.289337 | 0.742258 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2.6307 | 1.883680 | 0.828065 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.5483 | 1.158315 | 0.896774 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.0153 | 0.861815 | 0.909355 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.7958 | 0.777289 | 0.917419 |'
  prefs: []
  type: TYPE_TB
- en: 'The 92% accuracy on the validation set looks quite good compared to the 94%
    that the BERT-base teacher achieves. Now that we’ve fine-tuned DistilBERT, let’s
    push the model to the Hub so we can reuse it later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'With our model now safely stored on the Hub, we can immediately use it in a
    pipeline for our performance benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then pass this pipeline to our `PerformanceBenchmark` class to compute
    the metrics associated with this model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'To compare these results against our baseline, let’s create a scatter plot
    of the accuracy against the latency, with the radius of each point corresponding
    to the size of the model on disk. The following function does what we need and
    marks the current optimization type as a dashed circle to aid the comparison to
    previous results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_08in01.png)'
  prefs: []
  type: TYPE_IMG
- en: From the plot we can see that by using a smaller model we’ve managed to significantly
    decrease the average latency. And all this at the price of just over a 1% reduction
    in accuracy! Let’s see if we can close that last gap by including the distillation
    loss of the teacher and finding good values for <math alttext="alpha"><mi>α</mi></math>
    and *T*.
  prefs: []
  type: TYPE_NORMAL
- en: Finding Good Hyperparameters with Optuna
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To find good values for <math alttext="alpha"><mi>α</mi></math> and *T*, we
    could do a grid search over the 2D parameter space. But a much better alternative
    is to use *Optuna*,^([12](ch08.xhtml#idm46238706954496)) which is an optimization
    framework designed for just this type of task. Optuna formulates the search problem
    in terms of an objective function that is optimized through multiple *trials*.
    For example, suppose we wished to minimize Rosenbrock’s [“banana function”](https://oreil.ly/hPk8h):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="f left-parenthesis x comma y right-parenthesis equals left-parenthesis
    1 minus x right-parenthesis squared plus 100 left-parenthesis y minus x squared
    right-parenthesis squared" display="block"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>x</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>100</mn> <msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: which is a famous test case for optimization frameworks. As shown in [Figure 8-5](#banana-function),
    the function gets its name from the curved contours and has a global minimum at
    <math alttext="left-parenthesis x comma y right-parenthesis equals left-parenthesis
    1 comma 1 right-parenthesis"><mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>)</mo> <mo>=</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math>
    . Finding the valley is an easy optimization problem, but converging to the global
    minimum is not.
  prefs: []
  type: TYPE_NORMAL
- en: '![A banana plot](Images/nlpt_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. Plot of the Rosenbrock function of two variables
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In Optuna, we can find the minimum of <math alttext="f left-parenthesis x comma
    y right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>)</mo></mrow></math> by defining an `objective()` function that returns the
    value of <math alttext="f left-parenthesis x comma y right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The `trial.suggest_float` object specifies the parameter ranges to sample uniformly
    from; Optuna also provides `suggest_int` and `suggest_categorical` for integer
    and categorical parameters, respectively. Optuna collects multiple trials as a
    *study*, so to create one we just pass the `objective()` function to `study.optimize()`
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the study is completed, we can then find the best parameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We see that with one thousand trials, Optuna has managed to find values for
    *x* and *y* that are reasonably close to the global minimum. To use Optuna in
    ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, we use similar logic by first
    defining the hyperparameter space that we wish to optimize over. In addition to
    <math alttext="alpha"> <mi>α</mi> </math> and *T*, we’ll include the number of
    training epochs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the hyperparameter search with the `Trainer` is then quite simple;
    we just need to specify the number of trials to run and a direction to optimize
    for. Because we want the best possible accuracy, we specify `direction="maximize"`
    in the `hyper​para⁠meter_​search()` method of the trainer and pass the hyperparameter
    search space as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The `hyperparameter_search()` method returns a `BestRun` object, which contains
    the value of the objective that was maximized (by default, the sum of all metrics)
    and the hyperparameters it used for that run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This value of <math alttext="alpha"><mi>α</mi></math> tells us that most of
    the training signal is coming from the knowledge distillation term. Let’s update
    our training arguments with these values and run the final training run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '| Epoch | Training Loss | Validation Loss | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.9031 | 0.574540 | 0.736452 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.4481 | 0.285621 | 0.874839 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.2528 | 0.179766 | 0.918710 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.1760 | 0.139828 | 0.929355 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.1416 | 0.121053 | 0.934839 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0.1243 | 0.111640 | 0.934839 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.1133 | 0.106174 | 0.937742 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.1075 | 0.103526 | 0.938710 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.1039 | 0.101432 | 0.938065 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.1018 | 0.100493 | 0.939355 |'
  prefs: []
  type: TYPE_TB
- en: 'Remarkably, we’ve been able to train the student to match the accuracy of the
    teacher, despite it having almost half the number of parameters! Let’s push the
    model to the Hub for future use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Benchmarking Our Distilled Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have an accurate student, let’s create a pipeline and redo our
    benchmark to see how we perform on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'To put these results in context, let’s also visualize them with our `plot_metrics()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_08in02.png)'
  prefs: []
  type: TYPE_IMG
- en: As expected, the model size and latency remain essentially unchanged compared
    to the DistilBERT benchmark, but the accuracy has improved and even surpassed
    the performance of the teacher! One way to interpret this surprising result is
    that the teacher has likely not been fine-tuned as systematically as the student.
    This is great, but we can actually compress our distilled model even further using
    a technique known as quantization. That’s the topic of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Making Models Faster with Quantization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve now seen that with knowledge distillation we can reduce the computational
    and memory cost of running inference by transferring the information from a teacher
    into a smaller student. Quantization takes a different approach; instead of reducing
    the number of computations, it makes them much more efficient by representing
    the weights and activations with low-precision data types like 8-bit integer (INT8)
    instead of the usual 32-bit floating point (FP32). Reducing the number of bits
    means the resulting model requires less memory storage, and operations like matrix
    multiplication can be performed much faster with integer arithmetic. Remarkably,
    these performance gains can be realized with little to no loss in accuracy!
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea behind quantization is that we can “discretize” the floating-point
    values *f* in each tensor by mapping their range [ <math alttext="f Subscript
    normal m normal a normal x Baseline comma f Subscript normal m normal i normal
    n Baseline"><mrow><msub><mi>f</mi> <mi>max</mi></msub> <mo>,</mo> <msub><mi>f</mi>
    <mi>min</mi></msub></mrow></math> ] into a smaller one [ <math alttext="q Subscript
    normal m normal a normal x Baseline comma q Subscript normal m normal i normal
    n Baseline"><mrow><msub><mi>q</mi> <mi>max</mi></msub> <mo>,</mo> <msub><mi>q</mi>
    <mi>min</mi></msub></mrow></math> ] of fixed-point numbers <math alttext="q"><mi>q</mi></math>
    , and linearly distributing all values in between. Mathematically, this mapping
    is described by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="f equals left-parenthesis StartFraction f Subscript normal m
    normal a normal x Baseline minus f Subscript normal m normal i normal n Baseline
    Over q Subscript normal m normal a normal x Baseline minus q Subscript normal
    m normal i normal n Baseline EndFraction right-parenthesis left-parenthesis q
    minus upper Z right-parenthesis equals upper S left-parenthesis q minus upper
    Z right-parenthesis" display="block"><mrow><mi>f</mi> <mo>=</mo> <mfenced open="("
    close=")"><mfrac><mrow><msub><mi>f</mi> <mi>max</mi></msub> <mo>-</mo><msub><mi>f</mi>
    <mi>min</mi></msub></mrow> <mrow><msub><mi>q</mi> <mi>max</mi></msub> <mo>-</mo><msub><mi>q</mi>
    <mi>min</mi></msub></mrow></mfrac></mfenced> <mrow><mo>(</mo> <mi>q</mi> <mo>-</mo>
    <mi>Z</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>S</mi> <mrow><mo>(</mo> <mi>q</mi>
    <mo>-</mo> <mi>Z</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where the scale factor <math alttext="upper S"><mi>S</mi></math> is a positive
    floating-point number and the constant <math alttext="upper Z"><mi>Z</mi></math>
    has the same type as <math alttext="q"><mi>q</mi></math> and is called the *zero
    point* because it corresponds to the quantized value of the floating-point value
    <math alttext="f equals 0"><mrow><mi>f</mi> <mo>=</mo> <mn>0</mn></mrow></math>
    . Note that the map needs to be *affine* so that we get back floating-point numbers
    when we dequantize the fixed-point ones.^([13](ch08.xhtml#idm46238706293280))
    An illustration of the conversion is shown in [Figure 8-6](#fp32toint8).
  prefs: []
  type: TYPE_NORMAL
- en: '![Mapping floating-point numbers to 8-bit integers](Images/nlpt_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. Quantizing floating-point numbers as unsigned 8-bit integers (courtesy
    of Manas Sahni)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, one of the main reasons why transformers (and deep neural networks more
    generally) are prime candidates for quantization is that the weights and activations
    tend to take values in relatively small ranges. This means we don’t have to squeeze
    the whole range of possible FP32 numbers into, say, the <math alttext="2 Superscript
    8 Baseline equals 256"><mrow><msup><mn>2</mn> <mn>8</mn></msup> <mo>=</mo> <mn>256</mn></mrow></math>
    numbers represented by INT8\. To see this, let’s pick out one of the attention
    weight matrices from our distilled model and plot the frequency distribution of
    the values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_08in03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, the values of the weights are distributed in the small range
    [ <math alttext="negative 0.1 comma 0.1"><mrow><mo>-</mo> <mn>0</mn> <mo>.</mo>
    <mn>1</mn> <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>1</mn></mrow></math> ] around
    zero. Now, suppose we want to quantize this tensor as a signed 8-bit integer.
    In that case, the range of possible values for our integers is [ <math alttext="q
    Subscript normal m normal a normal x Baseline comma q Subscript normal m normal
    i normal n Baseline"><mrow><msub><mi>q</mi> <mi>max</mi></msub> <mo>,</mo> <msub><mi>q</mi>
    <mi>min</mi></msub></mrow></math> ] = [ <math alttext="negative 128 comma 127"><mrow><mo>-</mo>
    <mn>128</mn> <mo>,</mo> <mn>127</mn></mrow></math> ]. The zero point coincides
    with the zero of FP32 and the scale factor is calculated according to the previous
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'To obtain the quantized tensor, we just need to invert the mapping <math alttext="q
    equals f slash upper S plus upper Z"><mrow><mi>q</mi> <mo>=</mo> <mi>f</mi> <mo>/</mo>
    <mi>S</mi> <mo>+</mo> <mi>Z</mi></mrow></math> , clamp the values, round them
    to the nearest integer, and represent the result in the `torch.int8` data type
    using the `Tensor.char()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, we’ve just quantized our first tensor! In PyTorch we can simplify the
    conversion by using the `quantize_per_tensor()` function together with a quantized
    data type, `torch.qint`, that is optimized for integer arithmetic operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The plot in [Figure 8-7](#weight-quantization) shows very clearly the discretization
    that’s induced by only mapping some of the weight values precisely and rounding
    the rest.
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of quantization on a transformer''s weights](Images/nlpt_0807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. Effect of quantization on a transformer’s weights
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To round out our little analysis, let’s compare how long it takes to compute
    the multiplication of two weight tensors with FP32 and INT8 values. For the FP32
    tensors, we can multiply them using PyTorch’s nifty `@` operator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'For the quantized tensors we need the `QFunctional` wrapper class so that we
    can perform operations with the special `torch.qint8` data type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'This class supports various elementary operations, like addition, and in our
    case we can time the multiplication of our quantized tensors as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Compared to our FP32 computation, using the INT8 tensors is almost 100 times
    faster! Even larger gains can be obtained by using dedicated backends for running
    quantized operators efficiently. As of this book’s writing, PyTorch supports:'
  prefs: []
  type: TYPE_NORMAL
- en: x86 CPUs with AVX2 support or higher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ARM CPUs (typically found in mobile/embedded devices)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since INT8 numbers have four times fewer bits than FP32 numbers, quantization
    also reduces the memory storage requirements by up to a factor of four. In our
    simple example we can verify this by comparing the underlying storage size of
    our weight tensor and its quantized cousin by using the `Tensor.storage()` function
    and the `getsizeof()` function from Python’s `sys` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: For a full-scale transformer, the actual compression rate depends on which layers
    are quantized (as we’ll see in the next section it is only the linear layers that
    typically get quantized).
  prefs: []
  type: TYPE_NORMAL
- en: 'So what’s the catch with quantization? Changing the precision for all computations
    in our model introduces small disturbances at each point in the model’s computational
    graph, which can compound and affect the model’s performance. There are several
    ways to quantize a model, which all have pros and cons. For deep neural networks,
    there are typically three main approaches to quantization:'
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic quantization
  prefs: []
  type: TYPE_NORMAL
- en: When using dynamic quantization nothing is changed during training and the adaptations
    are only performed during inference. Like with all the quantization methods we
    will discuss, the weights of the model are converted to INT8 ahead of inference
    time. In addition to the weights, the model’s activations are also quantized.
    This approach is dynamic because the quantization happens on the fly. This means
    that all the matrix multiplications can be calculated with highly optimized INT8
    functions. Of all the quantization methods discussed here, dynamic quantization
    is the simplest one. However, with dynamic quantization the activations are written
    and read to memory in floating-point format. This conversion between integer and
    floating point can be a performance bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: Static quantization
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of computing the quantization of the activations on the fly, we can
    avoid the conversion to floating point by precomputing the quantization scheme.
    Static quantization achieves this by observing the activation patterns on a representative
    sample of the data ahead of inference time. The ideal quantization scheme is calculated
    and then saved. This enables us to skip the conversion between INT8 and FP32 values
    and speeds up the computations. However, it requires access to a good data sample
    and introduces an additional step in the pipeline, since we now need to train
    and determine the quantization scheme before we can perform inference. There is
    also one aspect that static quantization does not address: the discrepancy between
    the precision during training and inference, which leads to a performance drop
    in the model’s metrics (e.g., accuracy).'
  prefs: []
  type: TYPE_NORMAL
- en: Quantization-aware training
  prefs: []
  type: TYPE_NORMAL
- en: The effect of quantization can be effectively simulated during training by “fake”
    quantization of the FP32 values. Instead of using INT8 values during training,
    the FP32 values are rounded to mimic the effect of quantization. This is done
    during both the forward and the backward pass and improves performance in terms
    of model metrics over static and dynamic quantization.
  prefs: []
  type: TYPE_NORMAL
- en: The main bottleneck for running inference with transformers is the compute and
    memory bandwidth associated with the enormous numbers of weights in these models.
    For this reason, dynamic quantization is currently the best approach for transformer-based
    models in NLP. In smaller computer vision models the limiting factor is the memory
    bandwidth of the activations, which is why static quantization is generally used
    (or quantization-aware training in cases where the performance drops are too significant).
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing dynamic quantization in PyTorch is quite simple and can be done
    with a single line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Here we pass to `quantize_dynamic()` the full-precision model and specify the
    set of PyTorch layer classes in that model that we want to quantize. The `dtype`
    argument specifies the target precision and can be `fp16` or `qint8`. A good practice
    is to pick the lowest precision that you can tolerate with respect to your evaluation
    metrics. In this chapter we’ll use INT8, which as we’ll soon see has little impact
    on our model’s accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking Our Quantized Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With our model now quantized, let’s pass it through the benchmark and visualize
    the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_08in04.png)'
  prefs: []
  type: TYPE_IMG
- en: Nice, the quantized model is almost half the size of our distilled one and has
    even gained a slight accuracy boost! Let’s see if we can push our optimization
    to the limit with a powerful framework called the ONNX Runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Inference with ONNX and the ONNX Runtime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[ONNX](https://onnx.ai) is an open standard that defines a common set of operators
    and a common file format to represent deep learning models in a wide variety of
    frameworks, including PyTorch and TensorFlow.^([14](ch08.xhtml#idm46238705705264))
    When a model is exported to the ONNX format, these operators are used to construct
    a computational graph (often called an *intermediate representation*) that represents
    the flow of data through the neural network. An example of such a graph for BERT-base
    is shown in [Figure 8-8](#bert-onnx), where each node receives some input, applies
    an operation like `Add` or `Squeeze`, and then feeds the output to the next set
    of nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Example ONNX graph](Images/nlpt_0808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. A section of the ONNX graph for BERT-base, visualized in Netron
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By exposing a graph with standardized operators and data types, ONNX makes it
    easy to switch between frameworks. For example, a model trained in PyTorch can
    be exported to ONNX format and then imported in TensorFlow (and vice versa).
  prefs: []
  type: TYPE_NORMAL
- en: Where ONNX really shines is when it is coupled with a dedicated accelerator
    like [ONNX Runtime](https://onnxruntime.ai), or ORT for short.^([15](ch08.xhtml#idm46238705675632))
    ORT provides tools to optimize the ONNX graph through techniques like operator
    fusion and constant folding,^([16](ch08.xhtml#idm46238705672032)) and defines
    an interface to *execution providers* that allow you to run the model on different
    types of hardware. This is a powerful abstraction. [Figure 8-9](#onnx-ort) shows
    the high-level architecture of the ONNX and ORT ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture of the ONNX and ONNX Runtime ecosystem](Images/nlpt_0809.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. Architecture of the ONNX and ONNX Runtime ecosystem (courtesy of
    the ONNX Runtime team)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To see ORT in action, the first thing we need to do is convert our distilled
    model into the ONNX format. The ![nlpt_pin01](Images/nlpt_pin01.png) Transformers
    library has a built-in function called `con⁠vert_graph_to_onnx.convert()` that
    simplifies the process by taking the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the model as a `Pipeline`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run placeholder inputs through the pipeline so that ONNX can record the computational
    graph.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define dynamic axes to handle dynamic sequence lengths.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the graph with network parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To use this function, we first need to set some [OpenMP](https://openmp.org)
    environment variables for ONNX:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: OpenMP is an API designed for developing highly parallelized applications. The
    `OMP_NUM_THREADS` environment variable sets the number of threads to use for parallel
    computations in the ONNX Runtime, while `OMP_WAIT_POLICY=ACTIVE` specifies that
    waiting threads should be active (i.e., using CPU processor cycles).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s convert our distilled model to the ONNX format. Here we need to
    specify the argument `pipeline_name="text-classification"` since `convert()` wraps
    the model in a ![nlpt_pin01](Images/nlpt_pin01.png) Transformers `pipeline()`
    function during the conversion. In addition to the `model_ckpt`, we also pass
    the tokenizer to initialize the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: ONNX uses *operator sets* to group together immutable operator specifications,
    so `opset=12` corresponds to a specific version of the ONNX library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our model saved, we need to create an `InferenceSession` instance
    to feed inputs to the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Now when we call `onnx_model.run()`, we can get the class logits from the ONNX
    model. Let’s test this out with an example from the test set. Since the output
    from `convert()` tells us that ONNX expects just the `input_ids` and `attention_mask`
    as inputs, we need to drop the `label` column from our sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the logits, we can easily get the predicted label by taking the
    argmax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'which indeed agrees with the ground truth label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The ONNX model is not compatible with the `text-classification` pipeline, so
    we’ll create our own class that mimics the core behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then test this on our simple query to see if we recover the `car_rental`
    intent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Great, our pipeline works as expected. The next step is to create a performance
    benchmark for ONNX models. Here we can build on the work we did with the `Per⁠formanceBenchmark`
    class by simply overriding the `compute_size()` method and leaving the `compute_accuracy()`
    and `time_pipeline()` methods intact. The reason we need to override the `compute_size()`
    method is that we cannot rely on the `state_dict` and `torch.save()` to measure
    a model’s size, since `onnx_model` is technically an ONNX `InferenceSession` object
    that doesn’t have access to the attributes of PyTorch’s `nn.Module`. In any case,
    the resulting logic is simple and can be implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'With our new benchmark, let’s see how our distilled model performs when converted
    to ONNX format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_08in05.png)'
  prefs: []
  type: TYPE_IMG
- en: Remarkably, converting to the ONNX format and using the ONNX Runtime has given
    our distilled model (i.e. the “Distillation” circle in the plot) a boost in latency!
    Let’s see if we can squeeze out a bit more performance by adding quantization
    to the mix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to PyTorch, ORT offers three ways to quantize a model: dynamic, static,
    and quantization-aware training. As we did with PyTorch, we’ll apply dynamic quantization
    to our distilled model. In ORT, the quantization is applied through the `quan⁠tize_dynamic()`
    function, which requires a path to the ONNX model to quantize, a target path to
    save the quantized model to, and the data type to reduce the weights to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the model is quantized, let’s run it through our benchmark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '![](Images/nlpt_08in06.png)'
  prefs: []
  type: TYPE_IMG
- en: ORT quantization has reduced the model size and latency by around 30% compared
    to the model obtained from PyTorch quantization (the distillation + quantization
    blob). One reason for this is that PyTorch only optimizes the `nn.Linear` modules,
    while ONNX quantizes the embedding layer as well. From the plot we can also see
    that applying ORT quantization to our distilled model has provided an almost three-fold
    gain compared to our BERT baseline!
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our analysis of techniques to speed up transformers for inference.
    We have seen that methods such as quantization reduce the model size by reducing
    the precision of the representation. Another strategy to reduce the size is to
    remove some weights altogether. This technique is called *weight pruning*, and
    it’s the focus of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Making Models Sparser with Weight Pruning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we’ve seen that knowledge distillation and weight quantization are quite
    effective at producing faster models for inference, but in some cases you might
    also have strong constraints on the memory footprint of your model. For example,
    if our product manager suddenly decides that our text assistant needs to be deployed
    on a mobile device, then we’ll need our intent classifier to take up as little
    storage space as possible. To round out our survey of compression methods, let’s
    take a look at how we can shrink the number of parameters in our model by identifying
    and removing the least important weights in the network.
  prefs: []
  type: TYPE_NORMAL
- en: Sparsity in Deep Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As shown in [Figure 8-10](#network-pruning), the main idea behind pruning is
    to gradually remove weight connections (and potentially neurons) during training
    such that the model becomes progressively sparser. The resulting pruned model
    has a smaller number of nonzero parameters, which can then be stored in a compact
    sparse matrix format. Pruning can be also combined with quantization to obtain
    further compression.
  prefs: []
  type: TYPE_NORMAL
- en: '![Network Pruning](Images/nlpt_0810.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-10\. Weights and neurons before and after pruning (courtesy of Song
    Han)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Weight Pruning Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Mathematically, the way most weight pruning methods work is to calculate a
    matrix <math alttext="bold upper S"><mi>𝐒</mi></math> of *importance scores* and
    then select the top <math alttext="k"><mi>k</mi></math> percent of weights by
    importance:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal upper T normal o normal p Subscript k Baseline left-parenthesis
    bold upper S right-parenthesis Subscript i j Baseline equals StartLayout Enlarged
    left-brace 1st Row 1st Column 1 2nd Column Blank 3rd Column normal i normal f
    upper S Subscript i j Baseline normal i normal n normal t normal o normal p k
    percent-sign 2nd Row 1st Column 0 2nd Column Blank 3rd Column normal o normal
    t normal h normal e normal r normal w normal i normal s normal e EndLayout" display="block"><mrow><msub><mi>Top</mi>
    <mi>k</mi></msub> <msub><mrow><mo>(</mo><mi>𝐒</mi><mo>)</mo></mrow> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mn>1</mn></mrow></mtd>
    <mtd><mrow><mi>if</mi> <msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mi>in</mi> <mi>top</mi> <mi>k</mi> <mo>%</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd>
    <mtd><mi>otherwise</mi></mtd></mtr></mtable></mfenced></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In effect, <math alttext="k"><mi>k</mi></math> acts as a new hyperparameter
    to control the amount of sparsity in the model—that is, the proportion of weights
    that are zero-valued. Lower values of <math alttext="k"><mi>k</mi></math> correspond
    to sparser matrices. From these scores we can then define a *mask matrix* <math
    alttext="bold upper M"><mi>𝐌</mi></math> that masks the weights <math alttext="upper
    W Subscript i j"><msub><mi>W</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>
    during the forward pass with some input <math alttext="x Subscript i"><msub><mi>x</mi>
    <mi>i</mi></msub></math> and effectively creates a sparse network of activations
    <math alttext="a Subscript i"><msub><mi>a</mi> <mi>i</mi></msub></math> :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="a Subscript i Baseline equals sigma-summation Underscript k Endscripts
    upper W Subscript i k Baseline upper M Subscript i k Baseline x Subscript k" display="block"><mrow><msub><mi>a</mi>
    <mi>i</mi></msub> <mo>=</mo> <munder><mo>∑</mo> <mi>k</mi></munder> <msub><mi>W</mi>
    <mrow><mi>i</mi><mi>k</mi></mrow></msub> <msub><mi>M</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub>
    <msub><mi>x</mi> <mi>k</mi></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed in the tongue-in-cheek “Optimal Brain Surgeon” paper,^([17](ch08.xhtml#idm46238704674352))
    at the heart of each pruning method are a set of questions that need to be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: Which weights should be eliminated?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should the remaining weights be adjusted for best performance?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can such network pruning be done in a computationally efficient way?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The answers to these questions inform how the score matrix <math alttext="bold
    upper S"><mi>𝐒</mi></math> is computed, so let’s begin by looking at one of the
    earliest and most popular pruning methods: magnitude pruning.'
  prefs: []
  type: TYPE_NORMAL
- en: Magnitude pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name suggests, magnitude pruning calculates the scores according to the
    magnitude of the weights <math alttext="bold upper S equals left-parenthesis bar
    upper W Subscript i j Baseline bar right-parenthesis Subscript 1 less-than-or-equal-to
    j comma j less-than-or-equal-to n"><mrow><mi>𝐒</mi> <mo>=</mo> <msub><mfenced
    separators="" open="(" close=")"><mo>∣</mo> <msub><mi>W</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>∣</mo></mfenced> <mrow><mn>1</mn><mo>≤</mo><mi>j</mi><mo>,</mo><mi>j</mi><mo>≤</mo><mi>n</mi></mrow></msub></mrow></math>
    and then derives the masks from <math alttext="bold upper M equals normal upper
    T normal o normal p Subscript k Baseline left-parenthesis bold upper S right-parenthesis"><mrow><mi>𝐌</mi>
    <mo>=</mo> <msub><mi>Top</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi>𝐒</mi> <mo>)</mo></mrow></mrow></math>
    . In the literature it is common to apply magnitude pruning in an iterative fashion
    by first training the model to learn which connections are important and pruning
    the weights of least importance.^([18](ch08.xhtml#idm46238704650928)) The sparse
    model is then retrained and the process repeated until the desired sparsity is
    reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'One drawback with this approach is that it is computationally demanding: at
    every step of pruning we need to train the model to convergence. For this reason
    it is generally better to gradually increase the initial sparsity <math alttext="s
    Subscript i"><msub><mi>s</mi> <mi>i</mi></msub></math> (which is usually zero)
    to a final value <math alttext="s Subscript f"><msub><mi>s</mi> <mi>f</mi></msub></math>
    after some number of steps <math alttext="upper N"><mi>N</mi></math> :^([19](ch08.xhtml#idm46238704643440))'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="s Subscript t Baseline equals s Subscript f Baseline plus left-parenthesis
    s Subscript i Baseline minus s Subscript f Baseline right-parenthesis left-parenthesis
    1 minus StartFraction t minus t 0 Over upper N normal upper Delta t EndFraction
    right-parenthesis cubed normal f normal o normal r t element-of StartSet t 0 comma
    t 0 plus normal upper Delta t comma ellipsis comma t 0 plus upper N normal upper
    Delta t EndSet" display="block"><mrow><msub><mi>s</mi> <mi>t</mi></msub> <mo>=</mo>
    <msub><mi>s</mi> <mi>f</mi></msub> <mo>+</mo> <mrow><mo>(</mo> <msub><mi>s</mi>
    <mi>i</mi></msub> <mo>-</mo> <msub><mi>s</mi> <mi>f</mi></msub> <mo>)</mo></mrow>
    <msup><mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mfrac><mrow><mi>t</mi><mo>-</mo><msub><mi>t</mi>
    <mn>0</mn></msub></mrow> <mrow><mi>N</mi><mi>Δ</mi><mi>t</mi></mrow></mfrac></mfenced>
    <mn>3</mn></msup> <mi>for</mi> <mi>t</mi> <mo>∈</mo> <mrow><mo>{</mo> <msub><mi>t</mi>
    <mn>0</mn></msub> <mo>,</mo> <msub><mi>t</mi> <mn>0</mn></msub> <mo>+</mo> <mi>Δ</mi>
    <mi>t</mi> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>t</mi> <mn>0</mn></msub>
    <mo>+</mo> <mi>N</mi> <mi>Δ</mi> <mi>t</mi> <mo>}</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Here the idea is to update the binary masks <math alttext="bold upper M"><mi>𝐌</mi></math>
    every <math alttext="normal upper Delta t"><mrow><mi>Δ</mi> <mi>t</mi></mrow></math>
    steps to allow masked weights to reactivate during training and recover from any
    potential loss in accuracy that is induced by the pruning process. As shown in
    [Figure 8-11](#sparsity-scheduler), the cubic factor implies that the rate of
    weight pruning is highest in the early phases (when the number of redundant weights
    is large) and gradually tapers off.
  prefs: []
  type: TYPE_NORMAL
- en: '![Sparsity scheduler](Images/nlpt_0811.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-11\. The cubic sparsity scheduler used for pruning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One problem with magnitude pruning is that it is really designed for pure supervised
    learning, where the importance of each weight is directly related to the task
    at hand. By contrast, in transfer learning the importance of the weights is primarily
    determined by the pretraining phase, so magnitude pruning can remove connections
    that are important for the fine-tuning task. Recently, an adaptive approach called
    movement pruning has been proposed by Hugging Face researchers—let’s take a look.^([20](ch08.xhtml#idm46238704604576))
  prefs: []
  type: TYPE_NORMAL
- en: Movement pruning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The basic idea behind movement pruning is to *gradually* remove weights during
    fine-tuning such that the model becomes progressively *sparser*. The key novelty
    is that both the weights and the scores are learned during fine-tuning. So, instead
    of being derived directly from the weights (like with magnitude pruning), the
    scores in movement pruning are arbitrary and are learned through gradient descent
    like any other neural network parameter. This implies that in the backward pass,
    we also track the gradient of the loss <math alttext="upper L"><mi>L</mi></math>
    with respect to the scores <math alttext="upper S Subscript i j"><msub><mi>S</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub></math> .
  prefs: []
  type: TYPE_NORMAL
- en: Once the scores are learned, it is then straightforward to generate the binary
    mask using <math alttext="bold upper M equals normal upper T normal o normal p
    Subscript k Baseline left-parenthesis bold upper S right-parenthesis"><mrow><mi>𝐌</mi>
    <mo>=</mo> <msub><mi>Top</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi>𝐒</mi> <mo>)</mo></mrow></mrow></math>
    .^([21](ch08.xhtml#idm46238704590336))
  prefs: []
  type: TYPE_NORMAL
- en: The intuition behind movement pruning is that the weights that are “moving”
    the most from zero are the most important ones to keep. In other words, the positive
    weights increase during fine-tuning (and vice versa for the negative weights),
    which is equivalent to saying that the scores increase as the weights move away
    from zero. As shown in [Figure 8-12](#magnitude-vs-movement), this behavior differs
    from magnitude pruning, which selects as the most important weights those that
    are *furthest* from zero.
  prefs: []
  type: TYPE_NORMAL
- en: '![Magnitude vs Movement Pruning](Images/nlpt_0812.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-12\. Comparison of weights removed during magnitude pruning (left)
    and movement pruning (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These differences between the two pruning methods are also evident in the distribution
    of the remaining weights. As shown in [Figure 8-13](#pruning-dists), magnitude
    pruning produces two clusters of weights, while movement pruning produces a smoother
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: As of this book’s writing, ![nlpt_pin01](Images/nlpt_pin01.png) Transformers
    does not support pruning methods out of the box. Fortunately, there is a nifty
    library called [*Neural Networks Block Movement Pruning*](https://oreil.ly/aHEvD)
    that implements many of these ideas, and we recommend checking it out if memory
    constraints are a concern.
  prefs: []
  type: TYPE_NORMAL
- en: '![Pruning Distributions](Images/nlpt_0813.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-13\. Distribution of remaining weights for magnitude pruning (MaP)
    and movement pruning (MvP)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ve seen that optimizing transformers for deployment in production environments
    involves compression along two dimensions: latency and memory footprint. Starting
    from a fine-tuned model, we applied distillation, quantization, and optimizations
    through ORT to significantly reduce both of these. In particular, we found that
    quantization and conversion in ORT gave the largest gains with minimal effort.'
  prefs: []
  type: TYPE_NORMAL
- en: Although pruning is an effective strategy for reducing the storage size of transformer
    models, current hardware is not optimized for sparse matrix operations, which
    limits the usefulness of this technique. However, this is an active area of research,
    and by the time this book hits the shelves many of these limitations may have
    been resolved.
  prefs: []
  type: TYPE_NORMAL
- en: So where to from here? All of the techniques in this chapter can be adapted
    to other tasks, such as question answering, named entity recognition, or language
    modeling. If you find yourself struggling to meet the latency requirements or
    your model is eating up all your compute budget, we suggest giving one of them
    a try.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we’ll switch gears away from performance optimization
    and explore every data scientist’s worst nightmare: dealing with few to no labels.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch08.xhtml#idm46238709482512-marker)) S. Larson et al., [“An Evaluation
    Dataset for Intent Classification and Out-of-Scope Prediction”](https://arxiv.org/abs/1909.02027),
    (2019).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch08.xhtml#idm46238709422384-marker)) As described by Emmanuel Ameisen
    in *Building Machine Learning Powered Applications* (O’Reilly), business or product
    metrics are the *most* important ones to consider. After all, it doesn’t matter
    how accurate your model is if it doesn’t solve a problem your business cares about.
    In this chapter we’ll assume that you have already defined the metrics that matter
    for your application and focus on optimizing the model metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch08.xhtml#idm46238708497152-marker)) C. Buciluă et al., “Model Compression,”
    *Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining* (August 2006): 535–541, [*https://doi.org/10.1145/1150402.1150464*](https://doi.org/10.1145/1150402.1150464).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch08.xhtml#idm46238708494384-marker)) G. Hinton, O. Vinyals, and J. Dean,
    [“Distilling the Knowledge in a Neural Network”](https://arxiv.org/abs/1503.02531),
    (2015).
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch08.xhtml#idm46238708466160-marker)) W. Fedus, B. Zoph, and N. Shazeer,
    [“Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient
    Sparsity”](https://arxiv.org/abs/2101.03961), (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch08.xhtml#idm46238708457120-marker)) Geoff Hinton coined this term in
    a [talk](https://oreil.ly/OkHGp) to refer to the observation that softened probabilities
    reveal the hidden knowledge of the teacher.
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch08.xhtml#idm46238708428448-marker)) We also encountered temperature
    in the context of text generation in [Chapter 5](ch05.xhtml#chapter_generation).
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch08.xhtml#idm46238708343392-marker)) V. Sanh et al., [“DistilBERT, a
    Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter”](https://arxiv.org/abs/1910.01108),
    (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch08.xhtml#idm46238707933552-marker)) Y. Kim and H. Awadalla, [“FastFormers:
    Highly Efficient Transformer Models for Natural Language Understanding”](https://arxiv.org/abs/2010.13382),
    (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch08.xhtml#idm46238707834608-marker)) By default, the `Trainer` looks
    for a column called `labels` when fine-tuning on classification tasks. You can
    also override this behavior by specifying the `label_names` argument of `TrainingArguments`.
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch08.xhtml#idm46238707732144-marker)) This approach of fine-tuning a
    general-purpose, distilled language model is sometimes referred to as “task-agnostic”
    distillation.
  prefs: []
  type: TYPE_NORMAL
- en: '^([12](ch08.xhtml#idm46238706954496-marker)) T. Akiba et al., [“Optuna: A Next-Generation
    Hyperparameter Optimization Framework”](https://arxiv.org/abs/1907.10902), (2019).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch08.xhtml#idm46238706293280-marker)) An affine map is just a fancy name
    for the <math alttext="y equals upper A x plus b"><mrow><mi>y</mi> <mo>=</mo>
    <mi>A</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi></mrow></math> map that you’re familiar
    with in the linear layers of a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch08.xhtml#idm46238705705264-marker)) There is a separate standard called
    ONNX-ML that is designed for traditional machine learning models like random forests
    and frameworks like Scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch08.xhtml#idm46238705675632-marker)) Other popular accelerators include
    [NVIDIA’s TensorRT](https://oreil.ly/HnNZx) and [Apache TVM](https://oreil.ly/7KUyt).
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch08.xhtml#idm46238705672032-marker)) A fused operation involves merging
    one operator (usually an activation function) into another so that they can be
    executed together. For example, suppose we want to apply an activation *f* to
    a matrix product *A* × *B*. Normally the result of the product needs to be written
    back to the GPU memory before the activation is computed. Operator fusion allows
    as to compute <math alttext="f left-parenthesis upper A times upper B right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>A</mi> <mo>×</mo> <mi>B</mi> <mo>)</mo></mrow></math> in a single
    step. Constant folding refers to the process of evaluating constant expressions
    at compile time instead of runtime.
  prefs: []
  type: TYPE_NORMAL
- en: '^([17](ch08.xhtml#idm46238704674352-marker)) B. Hassibi and D. Stork, “Second
    Order Derivatives for Network Pruning: Optimal Brain Surgeon,” *Proceedings of
    the 5th International Conference on Neural Information Processing Systems* (November
    1992): 164–171, [*https://papers.nips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html*](https://papers.nips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch08.xhtml#idm46238704650928-marker)) S. Han et al., [“Learning Both
    Weights and Connections for Efficient Neural Networks”](https://arxiv.org/abs/1506.02626),
    (2015).
  prefs: []
  type: TYPE_NORMAL
- en: '^([19](ch08.xhtml#idm46238704643440-marker)) M. Zhu and S. Gupta, [“To Prune,
    or Not to Prune: Exploring the Efficacy of Pruning for Model Compression”](https://arxiv.org/abs/1710.01878),
    (2017).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([20](ch08.xhtml#idm46238704604576-marker)) V. Sanh, T. Wolf, and A.M. Rush,
    [“Movement Pruning: Adaptive Sparsity by Fine-Tuning”](https://arxiv.org/abs/2005.07683),
    (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([21](ch08.xhtml#idm46238704590336-marker)) There is also a “soft” version
    of movement pruning where instead of picking the top <math alttext="k"><mi>k</mi></math>
    % of weights, one uses a global threshold <math alttext="tau"><mi>τ</mi></math>
    to define the binary mask: <math alttext="bold upper M equals left-parenthesis
    bold upper S greater-than tau right-parenthesis"><mrow><mi>𝐌</mi> <mo>=</mo> <mo>(</mo>
    <mi>𝐒</mi> <mo>></mo> <mi>τ</mi> <mo>)</mo></mrow></math> .'
  prefs: []
  type: TYPE_NORMAL
