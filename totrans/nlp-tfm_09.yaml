- en: Chapter 8\. Making Transformers Efficient in Production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, you’ve seen how transformers can be fine-tuned to
    produce great results on a wide range of tasks. However, in many situations accuracy
    (or whatever metric you’re optimizing for) is not enough; your state-of-the-art
    model is not very useful if it’s too slow or large to meet the business requirements
    of your application. An obvious alternative is to train a faster and more compact
    model, but the reduction in model capacity is often accompanied by a degradation
    in performance. So what can you do when you need a fast, compact, yet highly accurate
    model?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will explore four complementary techniques that can be used
    to speed up the predictions and reduce the memory footprint of your transformer
    models: *knowledge distillation*, *quantization*, *pruning*, and *graph optimization*
    with the Open Neural Network Exchange (ONNX) format and ONNX Runtime (ORT). We’ll
    also see how some of these techniques can be combined to produce significant performance
    gains. For example, this was the approach taken by the Roblox engineering team
    in their article [“How We Scaled Bert to Serve 1+ Billion Daily Requests on CPUs”](https://oreil.ly/QdNIk),
    who as shown in [Figure 8-1](#roblox) found that combining knowledge distillation
    and quantization enabled them to improve the latency and throughput of their BERT
    classifier by over a factor of 30!'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: '![Scaling BERT at Roblox](Images/nlpt_0801.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. How Roblox scaled BERT with knowledge distillation, dynamic padding,
    and weight quantization (photo courtesy of Roblox employees Quoc N. Le and Kip
    Kaehler)
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To illustrate the benefits and trade-offs associated with each technique, we’ll
    use intent detection as a case study; this is an important component of text-based
    assistants, where low latencies are critical for maintaining a conversation in
    real time. Along the way you’ll learn how to create custom trainers, perform efficient
    hyperparameter search, and gain a sense of what it takes to implement cutting-edge
    research with ![nlpt_pin01](Images/nlpt_pin01.png) Transformers. Let’s dive in!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Intent Detection as a Case Study
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s suppose that we’re trying to build a text-based assistant for our company’s
    call center so that customers can request their account balance or make bookings
    without needing to speak with a human agent. In order to understand the goals
    of a customer, our assistant will need to be able to classify a wide variety of
    natural language text into a set of predefined actions or *intents*. For example,
    a customer might send a message like the following about an upcoming trip:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: Hey, I’d like to rent a vehicle from Nov 1st to Nov 15th in Paris and I need
    a 15 passenger van
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: and our intent classifier could automatically categorize this as a *Car Rental*
    intent, which then triggers an action and response. To be robust in a production
    environment, our classifier will also need to be able to handle *out-of-scope*
    queries, where a customer makes a query that doesn’t belong to any of the predefined
    intents and the system should yield a fallback response. For example, in the second
    case shown in [Figure 8-2](#oos), a customer asks a question about sports (which
    is out of scope), and the text assistant mistakenly classifies it as one of the
    known in-scope intents and returns the payday response. In the third case, the
    text assistant has been trained to detect out-of-scope queries (usually labeled
    as a separate class) and informs the customer about which topics it can answer
    questions about.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: '![Out of Scope Query](Images/nlpt_0802.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Three exchanges between a human (right) and a text-based assistant
    (left) for personal finance (courtesy of Stefan Larson et al.)
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As a baseline, we’ve fine-tuned a BERT-base model that achieves around 94% accuracy
    on the CLINC150 dataset.^([1](ch08.xhtml#idm46238709482512)) This dataset includes
    22,500 in-scope queries across 150 intents and 10 domains like banking and travel,
    and also includes 1,200 out-of-scope queries that belong to an `oos` intent class.
    In practice we would also gather our own in-house dataset, but using public data
    is a great way to iterate quickly and generate preliminary results.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基准，我们微调了一个BERT-base模型，在CLINC150数据集上达到了约94%的准确性。这个数据集包括150个意图和10个领域（如银行和旅行）中的22,500个范围内查询，还包括属于`oos`意图类别的1,200个范围外查询。在实践中，我们还会收集自己的内部数据集，但使用公共数据是快速迭代和生成初步结果的好方法。
- en: 'To get started, let’s download our fine-tuned model from the Hugging Face Hub
    and wrap it in a pipeline for text classification:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从Hugging Face Hub下载我们微调的模型，并将其包装成文本分类的管道：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that we have a pipeline, we can pass a query to get the predicted intent
    and confidence score from the model:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个管道，我们可以传递一个查询以从模型获取预测的意图和置信度分数：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Great, the `car_rental` intent makes sense. Let’s now look at creating a benchmark
    that we can use to evaluate the performance of our baseline model.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，`car_rental`意图是有意义的。现在让我们看看创建一个基准，我们可以用来评估我们基准模型的性能。
- en: Creating a Performance Benchmark
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建性能基准
- en: Like other machine learning models, deploying transformers in production environments
    involves a trade-off among several constraints, the most common being:^([2](ch08.xhtml#idm46238709422384))
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他机器学习模型一样，在生产环境中部署transformers涉及在几个约束条件之间进行权衡，最常见的是：
- en: '*Model performance*'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型性能*'
- en: How well does our model perform on a well-crafted test set that reflects production
    data? This is especially important when the cost of making errors is large (and
    best mitigated with a human in the loop), or when we need to run inference on
    millions of examples and small improvements to the model metrics can translate
    into large gains in aggregate.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在反映生产数据的精心设计的测试集上表现如何？当错误的成本很高时（最好通过人为干预来减轻），或者当我们需要对数百万个示例进行推断，并且模型指标的小幅改进可以转化为大幅增益时，这一点尤为重要。
- en: '*Latency*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*延迟*'
- en: How fast can our model deliver predictions? We usually care about latency in
    real-time environments that deal with a lot of traffic, like how Stack Overflow
    needed a classifier to quickly [detect unwelcome comments on the website](https://oreil.ly/cf7QX).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型能够多快地提供预测？我们通常关心实时环境中的延迟，这些环境处理大量流量，就像Stack Overflow需要一个分类器来快速[检测网站上不受欢迎的评论](https://oreil.ly/cf7QX)一样。
- en: '*Memory*'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*内存*'
- en: How can we deploy billion-parameter models like GPT-2 or T5 that require gigabytes
    of disk storage and RAM? Memory plays an especially important role in mobile or
    edge devices, where a model has to generate predictions without access to a powerful
    cloud server.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何部署像GPT-2或T5这样需要占用几GB磁盘存储和内存的百亿参数模型？内存在移动设备或边缘设备中扮演着特别重要的角色，因为模型必须在没有强大的云服务器的情况下生成预测。
- en: 'Failing to address these constraints can have a negative impact on the user
    experience of your application. More commonly, it can lead to ballooning costs
    from running expensive cloud servers that may only need to handle a few requests.
    To explore how each of these constraints can be optimized with various compression
    techniques, let’s begin by creating a simple benchmark that measures each quantity
    for a given pipeline and test set. A skeleton of what we’ll need is given by the
    following class:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 未能解决这些约束条件可能会对应用程序的用户体验产生负面影响。更常见的是，可能会导致运行昂贵的云服务器的成本激增，而这些服务器可能只需要处理少量请求。为了探索如何使用各种压缩技术优化这些约束条件，让我们从创建一个简单的基准开始，该基准可以测量给定管道和测试集的每个数量：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We’ve defined an `optim_type` parameter to keep track of the different optimization
    techniques that we’ll cover in this chapter. We’ll use the `run_benchmark()` method
    to collect all the metrics in a dictionary, with keys given by `optim_type`.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个`optim_type`参数，以跟踪我们在本章中将涵盖的不同优化技术。我们将使用`run_benchmark()`方法将所有指标收集到一个字典中，键由`optim_type`给出。
- en: 'Let’s now put some flesh on the bones of this class by computing the model
    accuracy on the test set. First we need some data to test on, so let’s download
    the CLINC150 dataset that was used to fine-tune our baseline model. We can get
    the dataset from the Hub with ![nlpt_pin01](Images/nlpt_pin01.png) Datasets as
    follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在通过在测试集上计算模型的准确性来为这个类添加一些具体内容。首先，我们需要一些数据进行测试，所以让我们下载用于微调基准模型的CLINC150数据集。我们可以通过以下方式从Hub获取数据集：![nlpt_pin01](Images/nlpt_pin01.png)。
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, the `plus` configuration refers to the subset that contains the out-of-scope
    training examples. Each example in the CLINC150 dataset consists of a query in
    the `text` column and its corresponding intent. We’ll use the test set to benchmark
    our models, so let’s take a look at one of the dataset’s examples:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`plus`配置是指包含超出范围的训练示例的子集。CLINC150数据集中的每个示例都包括`text`列中的查询及其对应的意图。我们将使用测试集来对我们的模型进行基准测试，所以让我们看一下数据集的一个示例：
- en: '[PRE5]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The intents are provided as IDs, but we can easily get the mapping to strings
    (and vice versa) by accessing the `features` attribute of the dataset:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 意图以ID的形式提供，但我们可以通过访问数据集的`features`属性轻松获取到字符串的映射（反之亦然）：
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that we have a basic understanding of the contents in the CLINC150 dataset,
    let’s implement the `compute_accuracy()` method of `PerformanceBenchmark`. Since
    the dataset is balanced across the intent classes, we’ll use accuracy as our metric.
    We can load this metric with ![nlpt_pin01](Images/nlpt_pin01.png) Datasets as
    follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对CLINC150数据集的内容有了基本的了解，让我们实现`PerformanceBenchmark`的`compute_accuracy()`方法。由于数据集在意图类别上是平衡的，我们将使用准确性作为我们的度量标准。我们可以通过以下方式使用![nlpt_pin01](Images/nlpt_pin01.png)数据集加载这个度量标准：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The accuracy metric expects the predictions and references (i.e., the ground
    truth labels) to be integers. We can use the pipeline to extract the predictions
    from the `text` field and then use the `str2int()` method of our `intents` object
    to map each prediction to its corresponding ID. The following code collects all
    the predictions and labels in lists before returning the accuracy on the dataset.
    Let’s also add it to our `Perform​an⁠ce​Benchmark` class:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度指标期望预测和参考（即，真实标签）是整数。我们可以使用管道从“text”字段中提取预测，然后使用我们的“intents”对象的“str2int（）”方法将每个预测映射到其相应的ID。以下代码在返回数据集的准确度之前收集所有的预测和标签。让我们也将其添加到我们的“PerformanceBenchmark”类中：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, let’s compute the size of our model by using the `torch.save()` function
    from PyTorch to serialize the model to disk. Under the hood, `torch.save()` uses
    Python’s `pickle` module and can be used to save anything from models to tensors
    to ordinary Python objects. In PyTorch, the recommended way to save a model is
    by using its `state_dict`, which is a Python dictionary that maps each layer in
    a model to its learnable parameters (i.e., weights and biases). Let’s see what
    is stored in the `state_dict` of our baseline model:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用PyTorch的“torch.save（）”函数来计算我们模型的大小，将模型序列化到磁盘上。在内部，“torch.save（）”使用Python的“pickle”模块，可以用来保存从模型到张量到普通Python对象的任何东西。在PyTorch中，保存模型的推荐方式是使用它的“state_dict”，这是一个Python字典，将模型中的每一层映射到它的可学习参数（即，权重和偏置）。让我们看看我们基准模型的“state_dict”中存储了什么：
- en: '[PRE11]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can clearly see that each key/value pair corresponds to a specific layer
    and tensor in BERT. So if we save our model with:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到每个键/值对对应于BERT中的特定层和张量。因此，如果我们用以下方式保存我们的模型：
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'we can then use the `Path.stat()` function from Python’s `pathlib` module to
    get information about the underlying files. In particular, `Path(​"model.​pt").​stat().​st_size`
    will give us the model size in bytes. Let’s put this all together in the `com⁠pute_​size()`
    function and add it to `PerformanceBenchmark`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Python的“pathlib”模块中的“Path.stat（）”函数来获取有关底层文件的信息。特别是，“Path（"model.​pt"）.​stat（）.​st_size”将给出模型的大小（以字节为单位）。让我们将所有这些放在“compute_​size（）”函数中，并将其添加到“PerformanceBenchmark”中：
- en: '[PRE14]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Finally let’s implement the `time_pipeline()` function so that we can time the
    average latency per query. For this application, latency refers to the time it
    takes to feed a text query to the pipeline and return the predicted intent from
    the model. Under the hood the pipeline also tokenizes the text, but this is around
    one thousand times faster than generating the predictions and thus adds a negligible
    contribution to the overall latency. A simple way to measure the execution time
    of a code snippet is to use the `perf_counter()` function from Python’s `time`
    module. This function has a better time resolution than the `time.time()` function
    and is well suited for getting precise results.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们实现“time_pipeline（）”函数，以便我们可以计算每个查询的平均延迟时间。对于这个应用程序，延迟时间指的是将文本查询输入到管道中并从模型返回预测意图所需的时间。在内部，管道还会对文本进行标记化，但这比生成预测快了大约一千倍，因此对整体延迟时间的贡献可以忽略不计。衡量代码片段的执行时间的一个简单方法是使用Python的“time”模块中的“perf_counter（）”函数。这个函数比“time.time（）”函数具有更好的时间分辨率，非常适合获取精确的结果。
- en: 'We can use `perf_counter()` to time our pipeline by passing our test query
    and calculating the time difference in milliseconds between the start and end:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用“perf_counter（）”通过传递我们的测试查询来计时我们的管道，并计算开始和结束之间的毫秒时间差：
- en: '[PRE15]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'These results exhibit quite some spread in the latencies and suggest that timing
    a single pass through the pipeline can give wildly different results each time
    we run the code. So instead, we’ll collect the latencies over many runs and then
    use the resulting distribution to calculate the mean and standard deviation, which
    will give us an idea about the spread in values. The following code does what
    we need and includes a phase to warm up the CPU before performing the actual timed
    run:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果展示了延迟时间的相当大的差异，并且表明通过管道的单次计时可能每次运行代码时都会得到完全不同的结果。因此，我们将收集多次运行的延迟时间，然后使用得到的分布来计算均值和标准差，这将让我们对数值的差异有一个概念。以下代码实现了我们需要的功能，并包括了在执行实际计时运行之前预热CPU的阶段：
- en: '[PRE17]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To keeps things simple, we’ll use the same `query` value to benchmark all our
    models. In general, the latency will depend on the query length, and a good practice
    is to benchmark your models with queries that they’re likely to encounter in production
    environments.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，我们将使用相同的“query”值来对我们所有的模型进行基准测试。一般来说，延迟时间将取决于查询长度，一个好的做法是使用模型可能在生产环境中遇到的查询来对模型进行基准测试。
- en: 'Now that our `PerformanceBenchmark` class is complete, let’s give it a spin!
    Let’s start by benchmarking our BERT baseline. For the baseline model, we just
    need to pass the pipeline and the dataset we wish to perform the benchmark on.
    We’ll collect the results in the `perf_metrics` dictionary to keep track of each
    model’s performance:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的“PerformanceBenchmark”类已经完成，让我们来试一试吧！让我们从对我们的BERT基准模型进行基准测试开始。对于基准模型，我们只需要传递管道和我们希望进行基准测试的数据集。我们将在“perf_metrics”字典中收集结果，以跟踪每个模型的性能：
- en: '[PRE18]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that we have a reference point, let’s look at our first compression technique:
    knowledge distillation.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个参考点，让我们来看看我们的第一个压缩技术：知识蒸馏。
- en: Note
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The average latency values will differ depending on what type of hardware you
    are running on. For example, you can usually get better performance by running
    inference on a GPU since it enables batch processing. For the purposes of this
    chapter, what’s important is the relative difference in latencies between models.
    Once we have determined the best-performing model, we can then explore different
    backends to reduce the absolute latency if needed.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 平均延迟值将取决于您所运行的硬件类型。例如，通常可以通过在GPU上运行推断来获得更好的性能，因为它可以实现批处理。对于本章的目的，重要的是模型之间延迟时间的相对差异。一旦确定了性能最佳的模型，我们可以探索不同的后端来减少绝对延迟时间（如果需要）。
- en: Making Models Smaller via Knowledge Distillation
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过知识蒸馏使模型变得更小
- en: Knowledge distillation is a general-purpose method for training a smaller *student*
    model to mimic the behavior of a slower, larger, but better-performing *teacher*.
    Originally introduced in 2006 in the context of ensemble models,^([3](ch08.xhtml#idm46238708497152))
    it was later popularized in a famous 2015 paper that generalized the method to
    deep neural networks and applied it to image classification and automatic speech
    recognition.^([4](ch08.xhtml#idm46238708494384))
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏是一种通用方法，用于训练一个较小的“学生”模型来模仿速度较慢、更大但性能更好的“教师”模型的行为。最初是在2006年在集成模型的背景下引入的，后来在一篇著名的2015年论文中将该方法推广到深度神经网络，并将其应用于图像分类和自动语音识别。
- en: Given the trend toward pretraining language models with ever-increasing parameter
    counts (the largest at the time of writing having over one trillion parameters),^([5](ch08.xhtml#idm46238708466160))
    knowledge distillation has also become a popular strategy to compress these huge
    models and make them more suitable for building practical applications.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于预训练语言模型参数数量不断增加的趋势（撰写时最大的模型参数超过一万亿），知识蒸馏也成为压缩这些庞大模型并使其更适合构建实际应用的流行策略。
- en: Knowledge Distillation for Fine-Tuning
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调的知识蒸馏
- en: So how is knowledge actually “distilled” or transferred from the teacher to
    the student during training? For supervised tasks like fine-tuning, the main idea
    is to augment the ground truth labels with a distribution of “soft probabilities”
    from the teacher which provide complementary information for the student to learn
    from. For example, if our BERT-base classifier assigns high probabilities to multiple
    intents, then this could be a sign that these intents lie close to each other
    in the feature space. By training the student to mimic these probabilities, the
    goal is to distill some of this “dark knowledge”^([6](ch08.xhtml#idm46238708457120))
    that the teacher has learned—that is, knowledge that is not available from the
    labels alone.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 那么在训练过程中，知识实际上是如何从教师传递给学生的呢？对于微调等监督任务，主要思想是用教师的“软概率”分布来增强地面真实标签，为学生提供补充信息。例如，如果我们的BERT-base分类器为多个意图分配高概率，那么这可能表明这些意图在特征空间中相互靠近。通过训练学生模仿这些概率，目标是蒸馏教师学到的一些“暗知识”——也就是，仅从标签中无法获得的知识。
- en: 'Mathematically, the way this works is as follows. Suppose we feed an input
    sequence *x* to the teacher to generate a vector of logits <math alttext="bold
    z left-parenthesis x right-parenthesis"><mrow><mi>𝐳</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></math> = [ <math alttext="z 1 left-parenthesis x right-parenthesis
    comma ellipsis comma z Subscript upper N Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>z</mi>
    <mn>1</mn></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>z</mi> <mi>N</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    ]. We can convert these logits into probabilities by applying a softmax function:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，这是如何工作的。假设我们将输入序列*x*提供给教师，以生成一个对数向量<math alttext="bold z left-parenthesis
    x right-parenthesis"><mrow><mi>𝐳</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    = [ <math alttext="z 1 left-parenthesis x right-parenthesis comma ellipsis comma
    z Subscript upper N Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>z</mi>
    <mn>1</mn></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>z</mi> <mi>N</mi></sub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    ]。我们可以通过应用softmax函数将这些对数转换为概率：
- en: <math alttext="StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis
    x right-parenthesis right-parenthesis Over sigma-summation Underscript j Endscripts
    exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    right-parenthesis EndFraction" display="block"><mrow><mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow>
    <mrow><msub><mo>∑</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis
    x right-parenthesis right-parenthesis Over sigma-summation Underscript j Endscripts
    exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    right-parenthesis EndFraction" display="block"><mrow><mfrac><mrow><mo form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow>
    <mrow><msub><mo>∑</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: This isn’t quite what we want, though, because in many cases the teacher will
    assign a high probability to one class, with all other class probabilities close
    to zero. When that happens, the teacher doesn’t provide much additional information
    beyond the ground truth labels, so instead we “soften” the probabilities by scaling
    the logits with a temperature hyperparameter *T* before applying the softmax:^([7](ch08.xhtml#idm46238708428448))
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是我们想要的，因为在许多情况下，教师会为一个类分配高概率，而其他类的概率接近于零。当发生这种情况时，教师除了地面真实标签外并没有提供太多额外信息，因此我们会在应用softmax之前，通过一个温度超参数*T*来缩放对数，从而“软化”概率。
- en: <math alttext="p Subscript i Baseline left-parenthesis x right-parenthesis equals
    StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    slash upper T right-parenthesis Over sigma-summation Underscript j Endscripts
    exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    slash upper T right-parenthesis EndFraction" display="block"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mo
    form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi> <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow>
    <mrow><msub><mo>∑</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="p Subscript i Baseline left-parenthesis x right-parenthesis equals
    StartFraction exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    slash upper T right-parenthesis Over sigma-summation Underscript j Endscripts
    exp left-parenthesis z Subscript i Baseline left-parenthesis x right-parenthesis
    slash upper T right-parenthesis EndFraction" display="block"><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mo
    form="prefix">exp</mo><mo>(</mo><msub><mi>z</mi> <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow>
    <mrow><msub><mo>∑</mo> <mi>j</mi></msub> <mo form="prefix">exp</mo><mrow><mo>(</mo><msub><mi>z</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>/</mo><mi>T</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: As shown in [Figure 8-3](#soft-probs), higher values of *T* produce a softer
    probability distribution over the classes and reveal much more information about
    the decision boundary that the teacher has learned for each training example.
    When <math alttext="upper T equals 1"><mrow><mi>T</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    we recover the original softmax distribution.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图8-3](#soft-probs)所示，*T*的值越高，类别上的软化概率分布就越软，可以更多地揭示老师对每个训练示例学习的决策边界。当<math
    alttext="upper T equals 1"><mrow><mi>T</mi> <mo>=</mo> <mn>1</mn></mrow></math>时，我们恢复了原始的softmax分布。
- en: '![Soft Probabilities](Images/nlpt_0803.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![软概率](Images/nlpt_0803.png)'
- en: Figure 8-3\. Comparison of a hard label that is one-hot encoded (left), softmax
    probabilities (middle), and softened class probabilities (right)
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3。一个使用one-hot编码的硬标签（左）、softmax概率（中）和软化类别概率（右）的比较。
- en: 'Since the student also produces softened probabilities <math alttext="q Subscript
    i Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>i</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> of its own, we can
    use the [Kullback–Leibler (KL)](https://oreil.ly/8nKQG) divergence to measure
    the difference between the two probability distributions:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于学生还产生了自己的软化概率<math alttext="q Subscript i Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>，我们可以使用[Kullback-Leibler（KL）](https://oreil.ly/8nKQG)散度来衡量两个概率分布之间的差异：
- en: <math alttext="upper D Subscript upper K upper L Baseline left-parenthesis p
    comma q right-parenthesis equals sigma-summation Underscript i Endscripts p Subscript
    i Baseline left-parenthesis x right-parenthesis log StartFraction p Subscript
    i Baseline left-parenthesis x right-parenthesis Over q Subscript i Baseline left-parenthesis
    x right-parenthesis EndFraction" display="block"><mrow><msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo> <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munder><mo>∑</mo> <mi>i</mi></munder> <msub><mi>p</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mfrac><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>q</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper D Subscript upper K upper L Baseline left-parenthesis p
    comma q right-parenthesis equals sigma-summation Underscript i Endscripts p Subscript
    i Baseline left-parenthesis x right-parenthesis log StartFraction p Subscript
    i Baseline left-parenthesis x right-parenthesis Over q Subscript i Baseline left-parenthesis
    x right-parenthesis EndFraction" display="block"><mrow><msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>p</mi> <mo>,</mo> <mi>q</mi> <mo>)</mo></mrow> <mo>=</mo>
    <munder><mo>∑</mo> <mi>i</mi></munder> <msub><mi>p</mi> <mi>i</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo form="prefix">log</mo> <mfrac><mrow><msub><mi>p</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>q</mi>
    <mi>i</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac></mrow></math>
- en: 'With the KL divergence we can calculate how much is lost when we approximate
    the probability distribution of the teacher with the student. This allows us to
    define a knowledge distillation loss:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过KL散度，我们可以计算当我们用学生来近似老师的概率分布时损失了多少。这使我们能够定义知识蒸馏损失：
- en: <math alttext="upper L Subscript upper K upper D Baseline equals upper T squared
    upper D Subscript upper K upper L" display="block"><mrow><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub>
    <mo>=</mo> <msup><mi>T</mi> <mn>2</mn></msup> <msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub></mrow></math>
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L Subscript upper K upper D Baseline equals upper T squared
    upper D Subscript upper K upper L" display="block"><mrow><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub>
    <mo>=</mo> <msup><mi>T</mi> <mn>2</mn></msup> <msub><mi>D</mi> <mrow><mi>K</mi><mi>L</mi></mrow></msub></mrow></math>
- en: 'where <math alttext="upper T squared"><msup><mi>T</mi> <mn>2</mn></msup></math>
    is a normalization factor to account for the fact that the magnitude of the gradients
    produced by soft labels scales as <math alttext="1 slash upper T squared"><mrow><mn>1</mn>
    <mo>/</mo> <msup><mi>T</mi> <mn>2</mn></msup></mrow></math> . For classification
    tasks, the student loss is then a weighted average of the distillation loss with
    the usual cross-entropy loss <math alttext="upper L Subscript upper C upper E"><msub><mi>L</mi>
    <mrow><mi>C</mi><mi>E</mi></mrow></msub></math> of the ground truth labels:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="upper T squared"><msup><mi>T</mi> <mn>2</mn></msup></math>是一个归一化因子，用于考虑软标签产生的梯度大小按<math
    alttext="1 slash upper T squared"><mrow><mn>1</mn> <mo>/</mo> <msup><mi>T</mi>
    <mn>2</mn></msup></mrow></math>缩放的事实。对于分类任务，学生的损失是蒸馏损失和地面真实标签的交叉熵损失<math alttext="upper
    L Subscript upper C upper E"><msub><mi>L</mi> <mrow><mi>C</mi><mi>E</mi></mrow></msub></math>的加权平均：
- en: <math alttext="upper L Subscript normal s normal t normal u normal d normal
    e normal n normal t Baseline equals alpha upper L Subscript upper C upper E Baseline
    plus left-parenthesis 1 minus alpha right-parenthesis upper L Subscript upper
    K upper D" display="block"><mrow><msub><mi>L</mi> <mi>student</mi></msub> <mo>=</mo>
    <mi>α</mi> <msub><mi>L</mi> <mrow><mi>C</mi><mi>E</mi></mrow></msub> <mo>+</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>α</mi> <mo>)</mo></mrow> <msub><mi>L</mi>
    <mrow><mi>K</mi><mi>D</mi></mrow></msub></mrow></math>
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L Subscript normal s normal t normal u normal d normal
    e normal n normal t Baseline equals alpha upper L Subscript upper C upper E Baseline
    plus left-parenthesis 1 minus alpha right-parenthesis upper L Subscript upper
    K upper D" display="block"><mrow><msub><mi>L</mi> <mi>student</mi></msub> <mo>=</mo>
    <mi>α</mi> <msub><mi>L</mi> <mrow><mi>C</mi><mi>E</mi></mrow></msub> <mo>+</mo>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>α</mi> <mo>)</mo></mrow> <msub><mi>L</mi>
    <mrow><mi>K</mi><mi>D</mi></mrow></msub></mrow></math>
- en: where <math alttext="alpha"><mi>α</mi></math> is a hyperparameter that controls
    the relative strength of each loss. A diagram of the whole process is shown in
    [Figure 8-4](#kd); the temperature is set to 1 at inference time to recover the
    standard softmax probabilities.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="alpha"><mi>α</mi></math>是一个控制每个损失相对强度的超参数。整个过程的图表如[图8-4](#kd)所示；在推断时，温度被设置为1，以恢复标准的softmax概率。
- en: '![Knowledge distillation](Images/nlpt_0804.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![知识蒸馏](Images/nlpt_0804.png)'
- en: Figure 8-4\. The knowledge distillation process
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4。知识蒸馏过程
- en: Knowledge Distillation for Pretraining
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练的知识蒸馏
- en: 'Knowledge distillation can also be used during pretraining to create a general-purpose
    student that can be subsequently fine-tuned on downstream tasks. In this case,
    the teacher is a pretrained language model like BERT, which transfers its knowledge
    about masked language modeling to the student. For example, in the DistilBERT
    paper,^([8](ch08.xhtml#idm46238708343392)) the masked language modeling loss <math
    alttext="upper L Subscript m l m"><msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub></math>
    is augmented with a term from knowledge distillation and a cosine embedding loss
    <math alttext="upper L Subscript c o s Baseline equals 1 minus cosine left-parenthesis
    h Subscript s Baseline comma h Subscript t Baseline right-parenthesis"><mrow><msub><mi>L</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub> <mo>=</mo> <mn>1</mn> <mo>-</mo>
    <mo form="prefix">cos</mo> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>s</mi></msub>
    <mo>,</mo> <msub><mi>h</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
    to align the directions of the hidden state vectors between the teacher and student:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏也可以在预训练期间使用，以创建一个通用的学生模型，随后可以在下游任务上进行精细调整。在这种情况下，教师是一个预训练的语言模型，如BERT，它将其关于掩码语言建模的知识转移到学生身上。例如，在DistilBERT论文中，^([8](ch08.xhtml#idm46238708343392))掩码语言建模损失<math
    alttext="upper L Subscript m l m"><msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub></math>被知识蒸馏的一个项和余弦嵌入损失<math
    alttext="upper L Subscript c o s Baseline equals 1 minus cosine left-parenthesis
    h Subscript s Baseline comma h Subscript t Baseline right-parenthesis"><mrow><msub><mi>L</mi>
    <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub> <mo>=</mo> <mn>1</mn> <mo>-</mo>
    <mo form="prefix">cos</mo> <mrow><mo>(</mo> <msub><mi>h</mi> <mi>s</mi></msub>
    <mo>,</mo> <msub><mi>h</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>来对齐教师和学生之间的隐藏状态向量的方向：
- en: <math alttext="upper L Subscript normal upper D normal i normal s normal t normal
    i normal l normal upper B normal upper E normal upper R normal upper T Baseline
    equals alpha upper L Subscript m l m Baseline plus beta upper L Subscript upper
    K upper D Baseline plus gamma upper L Subscript c o s" display="block"><mrow><msub><mi>L</mi>
    <mi>DistilBERT</mi></msub> <mo>=</mo> <mi>α</mi> <msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub>
    <mo>+</mo> <mi>β</mi> <msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub>
    <mo>+</mo> <mi>γ</mi> <msub><mi>L</mi> <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper L Subscript normal upper D normal i normal s normal t normal
    i normal l normal upper B normal upper E normal upper R normal upper T Baseline
    equals alpha upper L Subscript m l m Baseline plus beta upper L Subscript upper
    K upper D Baseline plus gamma upper L Subscript c o s" display="block"><mrow><msub><mi>L</mi>
    <mi>DistilBERT</mi></msub> <mo>=</mo> <mi>α</mi> <msub><mi>L</mi> <mrow><mi>m</mi><mi>l</mi><mi>m</mi></mrow></msub>
    <mo>+</mo> <mi>β</mi> <msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub>
    <mo>+</mo> <mi>γ</mi> <msub><mi>L</mi> <mrow><mi>c</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow></math>
- en: Since we already have a fine-tuned BERT-base model, let’s see how we can use
    knowledge distillation to fine-tune a smaller and faster model. To do that we’ll
    need a way to augment the cross-entropy loss with an <math alttext="upper L Subscript
    upper K upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math>
    term. Fortunately we can do this by creating our own trainer!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经有了一个经过精细调整的BERT-base模型，让我们看看如何使用知识蒸馏来对一个更小更快的模型进行精细调整。为了做到这一点，我们需要一种方法来将交叉熵损失与<math
    alttext="upper L Subscript upper K upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math>项相结合。幸运的是，我们可以通过创建自己的训练器来实现这一点！
- en: Creating a Knowledge Distillation Trainer
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建知识蒸馏训练器
- en: 'To implement knowledge distillation we need to add a few things to the `Trainer`
    base class:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现知识蒸馏，我们需要向`Trainer`基类添加一些内容：
- en: The new hyperparameters <math alttext="alpha"><mi>α</mi></math> and *T*, which
    control the relative weight of the distillation loss and how much the probability
    distribution of the labels should be smoothed
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新的超参数<math alttext="alpha"><mi>α</mi></math>和*T*，它们控制蒸馏损失的相对权重以及标签的概率分布应该被平滑的程度
- en: The fine-tuned teacher model, which in our case is BERT-base
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经过精细调整的教师模型，我们的情况下是BERT-base
- en: A new loss function that combines the cross-entropy loss with the knowledge
    distillation loss
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合交叉熵损失和知识蒸馏损失的新损失函数
- en: 'Adding the new hyperparameters is quite simple, since we just need to subclass
    `TrainingArguments` and include them as new attributes:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 添加新的超参数非常简单，因为我们只需要对`TrainingArguments`进行子类化，并将它们包含为新的属性：
- en: '[PRE20]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For the trainer itself, we need a new loss function. The way to implement this
    is by subclassing `Trainer` and overriding the `compute_loss()` method to include
    the knowledge distillation loss term <math alttext="upper L Subscript upper K
    upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math> :'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练器本身，我们需要一个新的损失函数。实现这一点的方法是通过对`Trainer`进行子类化，并覆盖`compute_loss()`方法，以包括知识蒸馏损失项<math
    alttext="upper L Subscript upper K upper D"><msub><mi>L</mi> <mrow><mi>K</mi><mi>D</mi></mrow></msub></math>：
- en: '[PRE21]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Let’s unpack this code a bit. When we instantiate `DistillationTrainer` we pass
    a `teacher_model` argument with a teacher that has already been fine-tuned on
    our task. Next, in the `compute_loss()` method we extract the logits from the
    student and teacher, scale them by the temperature, and then normalize them with
    a softmax before passing them to PyTorch’s `nn.KLDivLoss()` function for computing
    the KL divergence. One quirk with `nn.KLDivLoss()` is that it expects the inputs
    in the form of log probabilities and the labels as normal probabilities. That’s
    why we’ve used the `F.log_softmax()` function to normalize the student’s logits,
    while the teacher’s logits are converted to probabilities with a standard softmax.
    The `reduction=batchmean` argument in `nn.KLDivLoss()` specifies that we average
    the losses over the batch dimension.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解开一下这段代码。当我们实例化`DistillationTrainer`时，我们传递了一个已经在我们的任务上进行了微调的老师模型。接下来，在`compute_loss()`方法中，我们从学生和老师那里提取logits，通过温度对它们进行缩放，然后在传递给PyTorch的`nn.KLDivLoss()`函数之前，使用softmax对它们进行归一化以计算KL散度。`nn.KLDivLoss()`的一个怪癖是，它期望输入以对数概率的形式，标签以正常概率的形式。这就是为什么我们使用`F.log_softmax()`函数对学生的logits进行归一化，而老师的logits则使用标准softmax转换为概率。`nn.KLDivLoss()`中的`reduction=batchmean`参数指定我们在批维度上平均损失。
- en: Tip
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can also perform knowledge distillation with the Keras API of the ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers library. To do this, you’ll need to implement a custom `Distiller`
    class that overrides the `train_step()`, `test_step()`, and `compile()` methods
    of `tf.keras.Model()`. See the [Keras documentation](https://oreil.ly/6qp0F) for
    an example of how to do this.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用![nlpt_pin01](Images/nlpt_pin01.png) Transformers库的Keras API进行知识蒸馏。为此，您需要实现一个自定义的`Distiller`类，覆盖`tf.keras.Model()`的`train_step()`、`test_step()`和`compile()`方法。请参阅[Keras文档](https://oreil.ly/6qp0F)了解如何实现。
- en: Choosing a Good Student Initialization
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择一个好的学生初始化
- en: Now that we have our custom trainer, the first question you might have is which
    pretrained language model should we pick for the student? In general we should
    pick a smaller model for the student to reduce the latency and memory footprint.
    A good rule of thumb from the literature is that knowledge distillation works
    best when the teacher and student are of the same *model type*.^([9](ch08.xhtml#idm46238707933552))
    One possible reason for this is that different model types, say BERT and RoBERTa,
    can have different output embedding spaces, which hinders the ability of the student
    to mimic the teacher. In our case study the teacher is BERT, so DistilBERT is
    a natural candidate to initialize the student with since it has 40% fewer parameters
    and has been shown to achieve strong results on downstream tasks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了自定义的训练器，您可能会问的第一个问题是，我们应该为学生选择哪个预训练语言模型？一般来说，我们应该为学生选择一个较小的模型，以减少延迟和内存占用。从文献中得出的一个很好的经验法则是，当老师和学生是相同的*模型类型*时，知识蒸馏效果最好。^([9](ch08.xhtml#idm46238707933552))这样做的一个可能原因是，不同的模型类型，比如BERT和RoBERTa，可能具有不同的输出嵌入空间，这会妨碍学生模仿老师的能力。在我们的案例研究中，老师是BERT，因此DistilBERT是一个自然的候选，因为它的参数少了40%，并且已经在下游任务中取得了良好的结果。
- en: 'First we’ll need to tokenize and encode our queries, so let’s instantiate the
    tokenizer from DistilBERT and create a simple `tokenize_text()` function to take
    care of the preprocessing:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要对我们的查询进行标记化和编码，因此让我们实例化来自DistilBERT的标记器，并创建一个简单的`tokenize_text()`函数来处理预处理：
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here we’ve removed the `text` column since we no longer need it, and we’ve also
    renamed the `intent` column to `labels` so it can be automatically detected by
    the trainer.^([10](ch08.xhtml#idm46238707834608))
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经删除了`text`列，因为我们不再需要它，我们还将`intent`列重命名为`labels`，以便训练器可以自动检测到它。^([10](ch08.xhtml#idm46238707834608))
- en: 'Now that we’ve processed our texts, the next thing we need to do is define
    the hyperparameters and `compute_metrics()` function for our `DistillationTrainer`.
    We’ll also push all of our models to the Hugging Face Hub, so let’s start by logging
    in to our account:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经处理了我们的文本，接下来我们需要做的是为我们的`DistillationTrainer`定义超参数和`compute_metrics()`函数。我们还将把所有的模型推送到Hugging
    Face Hub，所以让我们首先登录到我们的账户：
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we’ll define the metrics to track during training. As we did in the performance
    benchmark, we’ll use accuracy as the main metric. This means we can reuse our
    `accuracy_score()` function in the `compute_metrics()` function that we’ll include
    in `DistillationTrainer`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义训练过程中要跟踪的指标。就像我们在性能基准测试中所做的那样，我们将使用准确性作为主要指标。这意味着我们可以在`compute_metrics()`函数中重用我们的`accuracy_score()`函数，这个函数将包含在`DistillationTrainer`中：
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In this function, the predictions from the sequence modeling head come in the
    form of logits, so we use the `np.argmax()` function to find the most confident
    class prediction and compare that against the ground truth label.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个函数中，序列建模头部的预测以logits的形式出现，因此我们使用`np.argmax()`函数找到最有信心的类别预测，并将其与地面真相标签进行比较。
- en: 'Next we need to define the training arguments. To warm up, we’ll set <math
    alttext="alpha equals 1"><mrow><mi>α</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    to see how well DistilBERT performs without any signal from the teacher.^([11](ch08.xhtml#idm46238707732144))
    Then we will push our fine-tuned model to a new repository called `distilbert-base-uncased-finetuned-clinc`,
    so we just need to specify that in the `output_dir` argument of `DistillationTrainingArguments`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We’ve also tweaked a few of the default hyperparameter values, like the number
    of epochs, the weight decay, and the learning rate. The next thing to do is initialize
    a student model. Since we will be doing multiple runs with the trainer, we’ll
    create a `student_init()` function to initialize the model with each new run.
    When we pass this function to the `DistillationTrainer`, this will ensure we initialize
    a new model each time we call the `train()` method.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'One other thing we need to do is provide the student model with the mappings
    between each intent and label ID. These mappings can be obtained from our BERT-base
    model that we downloaded in the pipeline:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'With these mappings, we can now create a custom model configuration with the
    `AutoConfig` class hat we encountered in Chapters [3](ch03.xhtml#chapter_anatomy)
    and [4](ch04.xhtml#chapter_ner). Let’s use this to create a configuration for
    our student with the information about the label mappings:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here we’ve also specified the number of classes our model should expect. We
    can then provide this configuration to the `from_pretrained()` function of the
    `AutoModelForSequenceClassification` class as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We now have all the ingredients needed for our distillation trainer, so let’s
    load the teacher and fine-tune:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '| Epoch | Training Loss | Validation Loss | Accuracy |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
- en: '| 1 | 4.2923 | 3.289337 | 0.742258 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
- en: '| 2 | 2.6307 | 1.883680 | 0.828065 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
- en: '| 3 | 1.5483 | 1.158315 | 0.896774 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
- en: '| 4 | 1.0153 | 0.861815 | 0.909355 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.7958 | 0.777289 | 0.917419 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
- en: 'The 92% accuracy on the validation set looks quite good compared to the 94%
    that the BERT-base teacher achieves. Now that we’ve fine-tuned DistilBERT, let’s
    push the model to the Hub so we can reuse it later:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'With our model now safely stored on the Hub, we can immediately use it in a
    pipeline for our performance benchmark:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can then pass this pipeline to our `PerformanceBenchmark` class to compute
    the metrics associated with this model:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To compare these results against our baseline, let’s create a scatter plot
    of the accuracy against the latency, with the radius of each point corresponding
    to the size of the model on disk. The following function does what we need and
    marks the current optimization type as a dashed circle to aid the comparison to
    previous results:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](Images/nlpt_08in01.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: From the plot we can see that by using a smaller model we’ve managed to significantly
    decrease the average latency. And all this at the price of just over a 1% reduction
    in accuracy! Let’s see if we can close that last gap by including the distillation
    loss of the teacher and finding good values for <math alttext="alpha"><mi>α</mi></math>
    and *T*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Finding Good Hyperparameters with Optuna
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To find good values for <math alttext="alpha"><mi>α</mi></math> and *T*, we
    could do a grid search over the 2D parameter space. But a much better alternative
    is to use *Optuna*,^([12](ch08.xhtml#idm46238706954496)) which is an optimization
    framework designed for just this type of task. Optuna formulates the search problem
    in terms of an objective function that is optimized through multiple *trials*.
    For example, suppose we wished to minimize Rosenbrock’s [“banana function”](https://oreil.ly/hPk8h):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="f left-parenthesis x comma y right-parenthesis equals left-parenthesis
    1 minus x right-parenthesis squared plus 100 left-parenthesis y minus x squared
    right-parenthesis squared" display="block"><mrow><mi>f</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow> <mo>=</mo> <msup><mrow><mo>(</mo><mn>1</mn><mo>-</mo><mi>x</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <mn>100</mn> <msup><mrow><mo>(</mo><mi>y</mi><mo>-</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow> <mn>2</mn></msup></mrow></math>
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: which is a famous test case for optimization frameworks. As shown in [Figure 8-5](#banana-function),
    the function gets its name from the curved contours and has a global minimum at
    <math alttext="left-parenthesis x comma y right-parenthesis equals left-parenthesis
    1 comma 1 right-parenthesis"><mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>)</mo> <mo>=</mo> <mo>(</mo> <mn>1</mn> <mo>,</mo> <mn>1</mn> <mo>)</mo></mrow></math>
    . Finding the valley is an easy optimization problem, but converging to the global
    minimum is not.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![A banana plot](Images/nlpt_0805.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: Figure 8-5\. Plot of the Rosenbrock function of two variables
  id: totrans-144
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In Optuna, we can find the minimum of <math alttext="f left-parenthesis x comma
    y right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>)</mo></mrow></math> by defining an `objective()` function that returns the
    value of <math alttext="f left-parenthesis x comma y right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></math> :'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `trial.suggest_float` object specifies the parameter ranges to sample uniformly
    from; Optuna also provides `suggest_int` and `suggest_categorical` for integer
    and categorical parameters, respectively. Optuna collects multiple trials as a
    *study*, so to create one we just pass the `objective()` function to `study.optimize()`
    as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Once the study is completed, we can then find the best parameters as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We see that with one thousand trials, Optuna has managed to find values for
    *x* and *y* that are reasonably close to the global minimum. To use Optuna in
    ![nlpt_pin01](Images/nlpt_pin01.png) Transformers, we use similar logic by first
    defining the hyperparameter space that we wish to optimize over. In addition to
    <math alttext="alpha"> <mi>α</mi> </math> and *T*, we’ll include the number of
    training epochs as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Running the hyperparameter search with the `Trainer` is then quite simple;
    we just need to specify the number of trials to run and a direction to optimize
    for. Because we want the best possible accuracy, we specify `direction="maximize"`
    in the `hyper​para⁠meter_​search()` method of the trainer and pass the hyperparameter
    search space as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The `hyperparameter_search()` method returns a `BestRun` object, which contains
    the value of the objective that was maximized (by default, the sum of all metrics)
    and the hyperparameters it used for that run:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This value of <math alttext="alpha"><mi>α</mi></math> tells us that most of
    the training signal is coming from the knowledge distillation term. Let’s update
    our training arguments with these values and run the final training run:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '| Epoch | Training Loss | Validation Loss | Accuracy |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.9031 | 0.574540 | 0.736452 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.4481 | 0.285621 | 0.874839 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.2528 | 0.179766 | 0.918710 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.1760 | 0.139828 | 0.929355 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0.1416 | 0.121053 | 0.934839 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0.1243 | 0.111640 | 0.934839 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0.1133 | 0.106174 | 0.937742 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.1075 | 0.103526 | 0.938710 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.1039 | 0.101432 | 0.938065 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.1018 | 0.100493 | 0.939355 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: 'Remarkably, we’ve been able to train the student to match the accuracy of the
    teacher, despite it having almost half the number of parameters! Let’s push the
    model to the Hub for future use:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Benchmarking Our Distilled Model
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have an accurate student, let’s create a pipeline and redo our
    benchmark to see how we perform on the test set:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To put these results in context, let’s also visualize them with our `plot_metrics()`
    function:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](Images/nlpt_08in02.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: As expected, the model size and latency remain essentially unchanged compared
    to the DistilBERT benchmark, but the accuracy has improved and even surpassed
    the performance of the teacher! One way to interpret this surprising result is
    that the teacher has likely not been fine-tuned as systematically as the student.
    This is great, but we can actually compress our distilled model even further using
    a technique known as quantization. That’s the topic of the next section.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Making Models Faster with Quantization
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve now seen that with knowledge distillation we can reduce the computational
    and memory cost of running inference by transferring the information from a teacher
    into a smaller student. Quantization takes a different approach; instead of reducing
    the number of computations, it makes them much more efficient by representing
    the weights and activations with low-precision data types like 8-bit integer (INT8)
    instead of the usual 32-bit floating point (FP32). Reducing the number of bits
    means the resulting model requires less memory storage, and operations like matrix
    multiplication can be performed much faster with integer arithmetic. Remarkably,
    these performance gains can be realized with little to no loss in accuracy!
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea behind quantization is that we can “discretize” the floating-point
    values *f* in each tensor by mapping their range [ <math alttext="f Subscript
    normal m normal a normal x Baseline comma f Subscript normal m normal i normal
    n Baseline"><mrow><msub><mi>f</mi> <mi>max</mi></msub> <mo>,</mo> <msub><mi>f</mi>
    <mi>min</mi></msub></mrow></math> ] into a smaller one [ <math alttext="q Subscript
    normal m normal a normal x Baseline comma q Subscript normal m normal i normal
    n Baseline"><mrow><msub><mi>q</mi> <mi>max</mi></msub> <mo>,</mo> <msub><mi>q</mi>
    <mi>min</mi></msub></mrow></math> ] of fixed-point numbers <math alttext="q"><mi>q</mi></math>
    , and linearly distributing all values in between. Mathematically, this mapping
    is described by the following equation:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="f equals left-parenthesis StartFraction f Subscript normal m
    normal a normal x Baseline minus f Subscript normal m normal i normal n Baseline
    Over q Subscript normal m normal a normal x Baseline minus q Subscript normal
    m normal i normal n Baseline EndFraction right-parenthesis left-parenthesis q
    minus upper Z right-parenthesis equals upper S left-parenthesis q minus upper
    Z right-parenthesis" display="block"><mrow><mi>f</mi> <mo>=</mo> <mfenced open="("
    close=")"><mfrac><mrow><msub><mi>f</mi> <mi>max</mi></msub> <mo>-</mo><msub><mi>f</mi>
    <mi>min</mi></msub></mrow> <mrow><msub><mi>q</mi> <mi>max</mi></msub> <mo>-</mo><msub><mi>q</mi>
    <mi>min</mi></msub></mrow></mfrac></mfenced> <mrow><mo>(</mo> <mi>q</mi> <mo>-</mo>
    <mi>Z</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>S</mi> <mrow><mo>(</mo> <mi>q</mi>
    <mo>-</mo> <mi>Z</mi> <mo>)</mo></mrow></mrow></math>
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: where the scale factor <math alttext="upper S"><mi>S</mi></math> is a positive
    floating-point number and the constant <math alttext="upper Z"><mi>Z</mi></math>
    has the same type as <math alttext="q"><mi>q</mi></math> and is called the *zero
    point* because it corresponds to the quantized value of the floating-point value
    <math alttext="f equals 0"><mrow><mi>f</mi> <mo>=</mo> <mn>0</mn></mrow></math>
    . Note that the map needs to be *affine* so that we get back floating-point numbers
    when we dequantize the fixed-point ones.^([13](ch08.xhtml#idm46238706293280))
    An illustration of the conversion is shown in [Figure 8-6](#fp32toint8).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '![Mapping floating-point numbers to 8-bit integers](Images/nlpt_0806.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. Quantizing floating-point numbers as unsigned 8-bit integers (courtesy
    of Manas Sahni)
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, one of the main reasons why transformers (and deep neural networks more
    generally) are prime candidates for quantization is that the weights and activations
    tend to take values in relatively small ranges. This means we don’t have to squeeze
    the whole range of possible FP32 numbers into, say, the <math alttext="2 Superscript
    8 Baseline equals 256"><mrow><msup><mn>2</mn> <mn>8</mn></msup> <mo>=</mo> <mn>256</mn></mrow></math>
    numbers represented by INT8\. To see this, let’s pick out one of the attention
    weight matrices from our distilled model and plot the frequency distribution of
    the values:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![](Images/nlpt_08in03.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, the values of the weights are distributed in the small range
    [ <math alttext="negative 0.1 comma 0.1"><mrow><mo>-</mo> <mn>0</mn> <mo>.</mo>
    <mn>1</mn> <mo>,</mo> <mn>0</mn> <mo>.</mo> <mn>1</mn></mrow></math> ] around
    zero. Now, suppose we want to quantize this tensor as a signed 8-bit integer.
    In that case, the range of possible values for our integers is [ <math alttext="q
    Subscript normal m normal a normal x Baseline comma q Subscript normal m normal
    i normal n Baseline"><mrow><msub><mi>q</mi> <mi>max</mi></msub> <mo>,</mo> <msub><mi>q</mi>
    <mi>min</mi></msub></mrow></math> ] = [ <math alttext="negative 128 comma 127"><mrow><mo>-</mo>
    <mn>128</mn> <mo>,</mo> <mn>127</mn></mrow></math> ]. The zero point coincides
    with the zero of FP32 and the scale factor is calculated according to the previous
    equation:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'To obtain the quantized tensor, we just need to invert the mapping <math alttext="q
    equals f slash upper S plus upper Z"><mrow><mi>q</mi> <mo>=</mo> <mi>f</mi> <mo>/</mo>
    <mi>S</mi> <mo>+</mo> <mi>Z</mi></mrow></math> , clamp the values, round them
    to the nearest integer, and represent the result in the `torch.int8` data type
    using the `Tensor.char()` function:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Great, we’ve just quantized our first tensor! In PyTorch we can simplify the
    conversion by using the `quantize_per_tensor()` function together with a quantized
    data type, `torch.qint`, that is optimized for integer arithmetic operations:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The plot in [Figure 8-7](#weight-quantization) shows very clearly the discretization
    that’s induced by only mapping some of the weight values precisely and rounding
    the rest.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '![Effect of quantization on a transformer''s weights](Images/nlpt_0807.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. Effect of quantization on a transformer’s weights
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To round out our little analysis, let’s compare how long it takes to compute
    the multiplication of two weight tensors with FP32 and INT8 values. For the FP32
    tensors, we can multiply them using PyTorch’s nifty `@` operator:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'For the quantized tensors we need the `QFunctional` wrapper class so that we
    can perform operations with the special `torch.qint8` data type:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This class supports various elementary operations, like addition, and in our
    case we can time the multiplication of our quantized tensors as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Compared to our FP32 computation, using the INT8 tensors is almost 100 times
    faster! Even larger gains can be obtained by using dedicated backends for running
    quantized operators efficiently. As of this book’s writing, PyTorch supports:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: x86 CPUs with AVX2 support or higher
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ARM CPUs (typically found in mobile/embedded devices)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since INT8 numbers have four times fewer bits than FP32 numbers, quantization
    also reduces the memory storage requirements by up to a factor of four. In our
    simple example we can verify this by comparing the underlying storage size of
    our weight tensor and its quantized cousin by using the `Tensor.storage()` function
    and the `getsizeof()` function from Python’s `sys` module:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: For a full-scale transformer, the actual compression rate depends on which layers
    are quantized (as we’ll see in the next section it is only the linear layers that
    typically get quantized).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'So what’s the catch with quantization? Changing the precision for all computations
    in our model introduces small disturbances at each point in the model’s computational
    graph, which can compound and affect the model’s performance. There are several
    ways to quantize a model, which all have pros and cons. For deep neural networks,
    there are typically three main approaches to quantization:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic quantization
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: When using dynamic quantization nothing is changed during training and the adaptations
    are only performed during inference. Like with all the quantization methods we
    will discuss, the weights of the model are converted to INT8 ahead of inference
    time. In addition to the weights, the model’s activations are also quantized.
    This approach is dynamic because the quantization happens on the fly. This means
    that all the matrix multiplications can be calculated with highly optimized INT8
    functions. Of all the quantization methods discussed here, dynamic quantization
    is the simplest one. However, with dynamic quantization the activations are written
    and read to memory in floating-point format. This conversion between integer and
    floating point can be a performance bottleneck.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Static quantization
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of computing the quantization of the activations on the fly, we can
    avoid the conversion to floating point by precomputing the quantization scheme.
    Static quantization achieves this by observing the activation patterns on a representative
    sample of the data ahead of inference time. The ideal quantization scheme is calculated
    and then saved. This enables us to skip the conversion between INT8 and FP32 values
    and speeds up the computations. However, it requires access to a good data sample
    and introduces an additional step in the pipeline, since we now need to train
    and determine the quantization scheme before we can perform inference. There is
    also one aspect that static quantization does not address: the discrepancy between
    the precision during training and inference, which leads to a performance drop
    in the model’s metrics (e.g., accuracy).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Quantization-aware training
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: The effect of quantization can be effectively simulated during training by “fake”
    quantization of the FP32 values. Instead of using INT8 values during training,
    the FP32 values are rounded to mimic the effect of quantization. This is done
    during both the forward and the backward pass and improves performance in terms
    of model metrics over static and dynamic quantization.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: The main bottleneck for running inference with transformers is the compute and
    memory bandwidth associated with the enormous numbers of weights in these models.
    For this reason, dynamic quantization is currently the best approach for transformer-based
    models in NLP. In smaller computer vision models the limiting factor is the memory
    bandwidth of the activations, which is why static quantization is generally used
    (or quantization-aware training in cases where the performance drops are too significant).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing dynamic quantization in PyTorch is quite simple and can be done
    with a single line of code:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Here we pass to `quantize_dynamic()` the full-precision model and specify the
    set of PyTorch layer classes in that model that we want to quantize. The `dtype`
    argument specifies the target precision and can be `fp16` or `qint8`. A good practice
    is to pick the lowest precision that you can tolerate with respect to your evaluation
    metrics. In this chapter we’ll use INT8, which as we’ll soon see has little impact
    on our model’s accuracy.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Benchmarking Our Quantized Model
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With our model now quantized, let’s pass it through the benchmark and visualize
    the results:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '![](Images/nlpt_08in04.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
- en: Nice, the quantized model is almost half the size of our distilled one and has
    even gained a slight accuracy boost! Let’s see if we can push our optimization
    to the limit with a powerful framework called the ONNX Runtime.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Inference with ONNX and the ONNX Runtime
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[ONNX](https://onnx.ai) is an open standard that defines a common set of operators
    and a common file format to represent deep learning models in a wide variety of
    frameworks, including PyTorch and TensorFlow.^([14](ch08.xhtml#idm46238705705264))
    When a model is exported to the ONNX format, these operators are used to construct
    a computational graph (often called an *intermediate representation*) that represents
    the flow of data through the neural network. An example of such a graph for BERT-base
    is shown in [Figure 8-8](#bert-onnx), where each node receives some input, applies
    an operation like `Add` or `Squeeze`, and then feeds the output to the next set
    of nodes.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '![Example ONNX graph](Images/nlpt_0808.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. A section of the ONNX graph for BERT-base, visualized in Netron
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By exposing a graph with standardized operators and data types, ONNX makes it
    easy to switch between frameworks. For example, a model trained in PyTorch can
    be exported to ONNX format and then imported in TensorFlow (and vice versa).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Where ONNX really shines is when it is coupled with a dedicated accelerator
    like [ONNX Runtime](https://onnxruntime.ai), or ORT for short.^([15](ch08.xhtml#idm46238705675632))
    ORT provides tools to optimize the ONNX graph through techniques like operator
    fusion and constant folding,^([16](ch08.xhtml#idm46238705672032)) and defines
    an interface to *execution providers* that allow you to run the model on different
    types of hardware. This is a powerful abstraction. [Figure 8-9](#onnx-ort) shows
    the high-level architecture of the ONNX and ORT ecosystem.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture of the ONNX and ONNX Runtime ecosystem](Images/nlpt_0809.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. Architecture of the ONNX and ONNX Runtime ecosystem (courtesy of
    the ONNX Runtime team)
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To see ORT in action, the first thing we need to do is convert our distilled
    model into the ONNX format. The ![nlpt_pin01](Images/nlpt_pin01.png) Transformers
    library has a built-in function called `con⁠vert_graph_to_onnx.convert()` that
    simplifies the process by taking the following steps:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the model as a `Pipeline`.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run placeholder inputs through the pipeline so that ONNX can record the computational
    graph.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define dynamic axes to handle dynamic sequence lengths.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save the graph with network parameters.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To use this function, we first need to set some [OpenMP](https://openmp.org)
    environment variables for ONNX:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: OpenMP is an API designed for developing highly parallelized applications. The
    `OMP_NUM_THREADS` environment variable sets the number of threads to use for parallel
    computations in the ONNX Runtime, while `OMP_WAIT_POLICY=ACTIVE` specifies that
    waiting threads should be active (i.e., using CPU processor cycles).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s convert our distilled model to the ONNX format. Here we need to
    specify the argument `pipeline_name="text-classification"` since `convert()` wraps
    the model in a ![nlpt_pin01](Images/nlpt_pin01.png) Transformers `pipeline()`
    function during the conversion. In addition to the `model_ckpt`, we also pass
    the tokenizer to initialize the pipeline:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: ONNX uses *operator sets* to group together immutable operator specifications,
    so `opset=12` corresponds to a specific version of the ONNX library.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our model saved, we need to create an `InferenceSession` instance
    to feed inputs to the model:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Now when we call `onnx_model.run()`, we can get the class logits from the ONNX
    model. Let’s test this out with an example from the test set. Since the output
    from `convert()` tells us that ONNX expects just the `input_ids` and `attention_mask`
    as inputs, we need to drop the `label` column from our sample:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Once we have the logits, we can easily get the predicted label by taking the
    argmax:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'which indeed agrees with the ground truth label:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The ONNX model is not compatible with the `text-classification` pipeline, so
    we’ll create our own class that mimics the core behavior:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'We can then test this on our simple query to see if we recover the `car_rental`
    intent:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Great, our pipeline works as expected. The next step is to create a performance
    benchmark for ONNX models. Here we can build on the work we did with the `Per⁠formanceBenchmark`
    class by simply overriding the `compute_size()` method and leaving the `compute_accuracy()`
    and `time_pipeline()` methods intact. The reason we need to override the `compute_size()`
    method is that we cannot rely on the `state_dict` and `torch.save()` to measure
    a model’s size, since `onnx_model` is technically an ONNX `InferenceSession` object
    that doesn’t have access to the attributes of PyTorch’s `nn.Module`. In any case,
    the resulting logic is simple and can be implemented as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'With our new benchmark, let’s see how our distilled model performs when converted
    to ONNX format:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '![](Images/nlpt_08in05.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
- en: Remarkably, converting to the ONNX format and using the ONNX Runtime has given
    our distilled model (i.e. the “Distillation” circle in the plot) a boost in latency!
    Let’s see if we can squeeze out a bit more performance by adding quantization
    to the mix.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to PyTorch, ORT offers three ways to quantize a model: dynamic, static,
    and quantization-aware training. As we did with PyTorch, we’ll apply dynamic quantization
    to our distilled model. In ORT, the quantization is applied through the `quan⁠tize_dynamic()`
    function, which requires a path to the ONNX model to quantize, a target path to
    save the quantized model to, and the data type to reduce the weights to:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Now that the model is quantized, let’s run it through our benchmark:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: '[PRE85]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: '[PRE86]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '![](Images/nlpt_08in06.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
- en: ORT quantization has reduced the model size and latency by around 30% compared
    to the model obtained from PyTorch quantization (the distillation + quantization
    blob). One reason for this is that PyTorch only optimizes the `nn.Linear` modules,
    while ONNX quantizes the embedding layer as well. From the plot we can also see
    that applying ORT quantization to our distilled model has provided an almost three-fold
    gain compared to our BERT baseline!
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our analysis of techniques to speed up transformers for inference.
    We have seen that methods such as quantization reduce the model size by reducing
    the precision of the representation. Another strategy to reduce the size is to
    remove some weights altogether. This technique is called *weight pruning*, and
    it’s the focus of the next section.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Making Models Sparser with Weight Pruning
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we’ve seen that knowledge distillation and weight quantization are quite
    effective at producing faster models for inference, but in some cases you might
    also have strong constraints on the memory footprint of your model. For example,
    if our product manager suddenly decides that our text assistant needs to be deployed
    on a mobile device, then we’ll need our intent classifier to take up as little
    storage space as possible. To round out our survey of compression methods, let’s
    take a look at how we can shrink the number of parameters in our model by identifying
    and removing the least important weights in the network.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: Sparsity in Deep Neural Networks
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As shown in [Figure 8-10](#network-pruning), the main idea behind pruning is
    to gradually remove weight connections (and potentially neurons) during training
    such that the model becomes progressively sparser. The resulting pruned model
    has a smaller number of nonzero parameters, which can then be stored in a compact
    sparse matrix format. Pruning can be also combined with quantization to obtain
    further compression.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![Network Pruning](Images/nlpt_0810.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: Figure 8-10\. Weights and neurons before and after pruning (courtesy of Song
    Han)
  id: totrans-295
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Weight Pruning Methods
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Mathematically, the way most weight pruning methods work is to calculate a
    matrix <math alttext="bold upper S"><mi>𝐒</mi></math> of *importance scores* and
    then select the top <math alttext="k"><mi>k</mi></math> percent of weights by
    importance:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal upper T normal o normal p Subscript k Baseline left-parenthesis
    bold upper S right-parenthesis Subscript i j Baseline equals StartLayout Enlarged
    left-brace 1st Row 1st Column 1 2nd Column Blank 3rd Column normal i normal f
    upper S Subscript i j Baseline normal i normal n normal t normal o normal p k
    percent-sign 2nd Row 1st Column 0 2nd Column Blank 3rd Column normal o normal
    t normal h normal e normal r normal w normal i normal s normal e EndLayout" display="block"><mrow><msub><mi>Top</mi>
    <mi>k</mi></msub> <msub><mrow><mo>(</mo><mi>𝐒</mi><mo>)</mo></mrow> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>=</mo> <mfenced separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mn>1</mn></mrow></mtd>
    <mtd><mrow><mi>if</mi> <msub><mi>S</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mi>in</mi> <mi>top</mi> <mi>k</mi> <mo>%</mo></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd>
    <mtd><mi>otherwise</mi></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'In effect, <math alttext="k"><mi>k</mi></math> acts as a new hyperparameter
    to control the amount of sparsity in the model—that is, the proportion of weights
    that are zero-valued. Lower values of <math alttext="k"><mi>k</mi></math> correspond
    to sparser matrices. From these scores we can then define a *mask matrix* <math
    alttext="bold upper M"><mi>𝐌</mi></math> that masks the weights <math alttext="upper
    W Subscript i j"><msub><mi>W</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub></math>
    during the forward pass with some input <math alttext="x Subscript i"><msub><mi>x</mi>
    <mi>i</mi></msub></math> and effectively creates a sparse network of activations
    <math alttext="a Subscript i"><msub><mi>a</mi> <mi>i</mi></msub></math> :'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="a Subscript i Baseline equals sigma-summation Underscript k Endscripts
    upper W Subscript i k Baseline upper M Subscript i k Baseline x Subscript k" display="block"><mrow><msub><mi>a</mi>
    <mi>i</mi></msub> <mo>=</mo> <munder><mo>∑</mo> <mi>k</mi></munder> <msub><mi>W</mi>
    <mrow><mi>i</mi><mi>k</mi></mrow></msub> <msub><mi>M</mi> <mrow><mi>i</mi><mi>k</mi></mrow></msub>
    <msub><mi>x</mi> <mi>k</mi></msub></mrow></math>
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'As discussed in the tongue-in-cheek “Optimal Brain Surgeon” paper,^([17](ch08.xhtml#idm46238704674352))
    at the heart of each pruning method are a set of questions that need to be considered:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Which weights should be eliminated?
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should the remaining weights be adjusted for best performance?
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can such network pruning be done in a computationally efficient way?
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The answers to these questions inform how the score matrix <math alttext="bold
    upper S"><mi>𝐒</mi></math> is computed, so let’s begin by looking at one of the
    earliest and most popular pruning methods: magnitude pruning.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Magnitude pruning
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As the name suggests, magnitude pruning calculates the scores according to the
    magnitude of the weights <math alttext="bold upper S equals left-parenthesis bar
    upper W Subscript i j Baseline bar right-parenthesis Subscript 1 less-than-or-equal-to
    j comma j less-than-or-equal-to n"><mrow><mi>𝐒</mi> <mo>=</mo> <msub><mfenced
    separators="" open="(" close=")"><mo>∣</mo> <msub><mi>W</mi> <mrow><mi>i</mi><mi>j</mi></mrow></msub>
    <mo>∣</mo></mfenced> <mrow><mn>1</mn><mo>≤</mo><mi>j</mi><mo>,</mo><mi>j</mi><mo>≤</mo><mi>n</mi></mrow></msub></mrow></math>
    and then derives the masks from <math alttext="bold upper M equals normal upper
    T normal o normal p Subscript k Baseline left-parenthesis bold upper S right-parenthesis"><mrow><mi>𝐌</mi>
    <mo>=</mo> <msub><mi>Top</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi>𝐒</mi> <mo>)</mo></mrow></mrow></math>
    . In the literature it is common to apply magnitude pruning in an iterative fashion
    by first training the model to learn which connections are important and pruning
    the weights of least importance.^([18](ch08.xhtml#idm46238704650928)) The sparse
    model is then retrained and the process repeated until the desired sparsity is
    reached.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'One drawback with this approach is that it is computationally demanding: at
    every step of pruning we need to train the model to convergence. For this reason
    it is generally better to gradually increase the initial sparsity <math alttext="s
    Subscript i"><msub><mi>s</mi> <mi>i</mi></msub></math> (which is usually zero)
    to a final value <math alttext="s Subscript f"><msub><mi>s</mi> <mi>f</mi></msub></math>
    after some number of steps <math alttext="upper N"><mi>N</mi></math> :^([19](ch08.xhtml#idm46238704643440))'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="s Subscript t Baseline equals s Subscript f Baseline plus left-parenthesis
    s Subscript i Baseline minus s Subscript f Baseline right-parenthesis left-parenthesis
    1 minus StartFraction t minus t 0 Over upper N normal upper Delta t EndFraction
    right-parenthesis cubed normal f normal o normal r t element-of StartSet t 0 comma
    t 0 plus normal upper Delta t comma ellipsis comma t 0 plus upper N normal upper
    Delta t EndSet" display="block"><mrow><msub><mi>s</mi> <mi>t</mi></msub> <mo>=</mo>
    <msub><mi>s</mi> <mi>f</mi></msub> <mo>+</mo> <mrow><mo>(</mo> <msub><mi>s</mi>
    <mi>i</mi></msub> <mo>-</mo> <msub><mi>s</mi> <mi>f</mi></msub> <mo>)</mo></mrow>
    <msup><mfenced separators="" open="(" close=")"><mn>1</mn> <mo>-</mo> <mfrac><mrow><mi>t</mi><mo>-</mo><msub><mi>t</mi>
    <mn>0</mn></msub></mrow> <mrow><mi>N</mi><mi>Δ</mi><mi>t</mi></mrow></mfrac></mfenced>
    <mn>3</mn></msup> <mi>for</mi> <mi>t</mi> <mo>∈</mo> <mrow><mo>{</mo> <msub><mi>t</mi>
    <mn>0</mn></msub> <mo>,</mo> <msub><mi>t</mi> <mn>0</mn></msub> <mo>+</mo> <mi>Δ</mi>
    <mi>t</mi> <mo>,</mo> <mo>...</mo> <mo>,</mo> <msub><mi>t</mi> <mn>0</mn></msub>
    <mo>+</mo> <mi>N</mi> <mi>Δ</mi> <mi>t</mi> <mo>}</mo></mrow></mrow></math>
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Here the idea is to update the binary masks <math alttext="bold upper M"><mi>𝐌</mi></math>
    every <math alttext="normal upper Delta t"><mrow><mi>Δ</mi> <mi>t</mi></mrow></math>
    steps to allow masked weights to reactivate during training and recover from any
    potential loss in accuracy that is induced by the pruning process. As shown in
    [Figure 8-11](#sparsity-scheduler), the cubic factor implies that the rate of
    weight pruning is highest in the early phases (when the number of redundant weights
    is large) and gradually tapers off.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '![Sparsity scheduler](Images/nlpt_0811.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
- en: Figure 8-11\. The cubic sparsity scheduler used for pruning
  id: totrans-312
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One problem with magnitude pruning is that it is really designed for pure supervised
    learning, where the importance of each weight is directly related to the task
    at hand. By contrast, in transfer learning the importance of the weights is primarily
    determined by the pretraining phase, so magnitude pruning can remove connections
    that are important for the fine-tuning task. Recently, an adaptive approach called
    movement pruning has been proposed by Hugging Face researchers—let’s take a look.^([20](ch08.xhtml#idm46238704604576))
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Movement pruning
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The basic idea behind movement pruning is to *gradually* remove weights during
    fine-tuning such that the model becomes progressively *sparser*. The key novelty
    is that both the weights and the scores are learned during fine-tuning. So, instead
    of being derived directly from the weights (like with magnitude pruning), the
    scores in movement pruning are arbitrary and are learned through gradient descent
    like any other neural network parameter. This implies that in the backward pass,
    we also track the gradient of the loss <math alttext="upper L"><mi>L</mi></math>
    with respect to the scores <math alttext="upper S Subscript i j"><msub><mi>S</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub></math> .
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Once the scores are learned, it is then straightforward to generate the binary
    mask using <math alttext="bold upper M equals normal upper T normal o normal p
    Subscript k Baseline left-parenthesis bold upper S right-parenthesis"><mrow><mi>𝐌</mi>
    <mo>=</mo> <msub><mi>Top</mi> <mi>k</mi></msub> <mrow><mo>(</mo> <mi>𝐒</mi> <mo>)</mo></mrow></mrow></math>
    .^([21](ch08.xhtml#idm46238704590336))
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: The intuition behind movement pruning is that the weights that are “moving”
    the most from zero are the most important ones to keep. In other words, the positive
    weights increase during fine-tuning (and vice versa for the negative weights),
    which is equivalent to saying that the scores increase as the weights move away
    from zero. As shown in [Figure 8-12](#magnitude-vs-movement), this behavior differs
    from magnitude pruning, which selects as the most important weights those that
    are *furthest* from zero.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '![Magnitude vs Movement Pruning](Images/nlpt_0812.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
- en: Figure 8-12\. Comparison of weights removed during magnitude pruning (left)
    and movement pruning (right)
  id: totrans-319
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: These differences between the two pruning methods are also evident in the distribution
    of the remaining weights. As shown in [Figure 8-13](#pruning-dists), magnitude
    pruning produces two clusters of weights, while movement pruning produces a smoother
    distribution.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: As of this book’s writing, ![nlpt_pin01](Images/nlpt_pin01.png) Transformers
    does not support pruning methods out of the box. Fortunately, there is a nifty
    library called [*Neural Networks Block Movement Pruning*](https://oreil.ly/aHEvD)
    that implements many of these ideas, and we recommend checking it out if memory
    constraints are a concern.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![Pruning Distributions](Images/nlpt_0813.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: Figure 8-13\. Distribution of remaining weights for magnitude pruning (MaP)
    and movement pruning (MvP)
  id: totrans-323
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ve seen that optimizing transformers for deployment in production environments
    involves compression along two dimensions: latency and memory footprint. Starting
    from a fine-tuned model, we applied distillation, quantization, and optimizations
    through ORT to significantly reduce both of these. In particular, we found that
    quantization and conversion in ORT gave the largest gains with minimal effort.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Although pruning is an effective strategy for reducing the storage size of transformer
    models, current hardware is not optimized for sparse matrix operations, which
    limits the usefulness of this technique. However, this is an active area of research,
    and by the time this book hits the shelves many of these limitations may have
    been resolved.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: So where to from here? All of the techniques in this chapter can be adapted
    to other tasks, such as question answering, named entity recognition, or language
    modeling. If you find yourself struggling to meet the latency requirements or
    your model is eating up all your compute budget, we suggest giving one of them
    a try.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we’ll switch gears away from performance optimization
    and explore every data scientist’s worst nightmare: dealing with few to no labels.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch08.xhtml#idm46238709482512-marker)) S. Larson et al., [“An Evaluation
    Dataset for Intent Classification and Out-of-Scope Prediction”](https://arxiv.org/abs/1909.02027),
    (2019).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch08.xhtml#idm46238709422384-marker)) As described by Emmanuel Ameisen
    in *Building Machine Learning Powered Applications* (O’Reilly), business or product
    metrics are the *most* important ones to consider. After all, it doesn’t matter
    how accurate your model is if it doesn’t solve a problem your business cares about.
    In this chapter we’ll assume that you have already defined the metrics that matter
    for your application and focus on optimizing the model metrics.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch08.xhtml#idm46238708497152-marker)) C. Buciluă et al., “Model Compression,”
    *Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining* (August 2006): 535–541, [*https://doi.org/10.1145/1150402.1150464*](https://doi.org/10.1145/1150402.1150464).'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch08.xhtml#idm46238708494384-marker)) G. Hinton, O. Vinyals, and J. Dean,
    [“Distilling the Knowledge in a Neural Network”](https://arxiv.org/abs/1503.02531),
    (2015).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch08.xhtml#idm46238708466160-marker)) W. Fedus, B. Zoph, and N. Shazeer,
    [“Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient
    Sparsity”](https://arxiv.org/abs/2101.03961), (2021).'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch08.xhtml#idm46238708457120-marker)) Geoff Hinton coined this term in
    a [talk](https://oreil.ly/OkHGp) to refer to the observation that softened probabilities
    reveal the hidden knowledge of the teacher.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch08.xhtml#idm46238708428448-marker)) We also encountered temperature
    in the context of text generation in [Chapter 5](ch05.xhtml#chapter_generation).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '^([8](ch08.xhtml#idm46238708343392-marker)) V. Sanh et al., [“DistilBERT, a
    Distilled Version of BERT: Smaller, Faster, Cheaper and Lighter”](https://arxiv.org/abs/1910.01108),
    (2019).'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch08.xhtml#idm46238707933552-marker)) Y. Kim and H. Awadalla, [“FastFormers:
    Highly Efficient Transformer Models for Natural Language Understanding”](https://arxiv.org/abs/2010.13382),
    (2020).'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch08.xhtml#idm46238707834608-marker)) By default, the `Trainer` looks
    for a column called `labels` when fine-tuning on classification tasks. You can
    also override this behavior by specifying the `label_names` argument of `TrainingArguments`.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch08.xhtml#idm46238707732144-marker)) This approach of fine-tuning a
    general-purpose, distilled language model is sometimes referred to as “task-agnostic”
    distillation.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '^([12](ch08.xhtml#idm46238706954496-marker)) T. Akiba et al., [“Optuna: A Next-Generation
    Hyperparameter Optimization Framework”](https://arxiv.org/abs/1907.10902), (2019).'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch08.xhtml#idm46238706293280-marker)) An affine map is just a fancy name
    for the <math alttext="y equals upper A x plus b"><mrow><mi>y</mi> <mo>=</mo>
    <mi>A</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi></mrow></math> map that you’re familiar
    with in the linear layers of a neural network.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: ^([14](ch08.xhtml#idm46238705705264-marker)) There is a separate standard called
    ONNX-ML that is designed for traditional machine learning models like random forests
    and frameworks like Scikit-learn.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch08.xhtml#idm46238705675632-marker)) Other popular accelerators include
    [NVIDIA’s TensorRT](https://oreil.ly/HnNZx) and [Apache TVM](https://oreil.ly/7KUyt).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch08.xhtml#idm46238705672032-marker)) A fused operation involves merging
    one operator (usually an activation function) into another so that they can be
    executed together. For example, suppose we want to apply an activation *f* to
    a matrix product *A* × *B*. Normally the result of the product needs to be written
    back to the GPU memory before the activation is computed. Operator fusion allows
    as to compute <math alttext="f left-parenthesis upper A times upper B right-parenthesis"><mrow><mi>f</mi>
    <mo>(</mo> <mi>A</mi> <mo>×</mo> <mi>B</mi> <mo>)</mo></mrow></math> in a single
    step. Constant folding refers to the process of evaluating constant expressions
    at compile time instead of runtime.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '^([17](ch08.xhtml#idm46238704674352-marker)) B. Hassibi and D. Stork, “Second
    Order Derivatives for Network Pruning: Optimal Brain Surgeon,” *Proceedings of
    the 5th International Conference on Neural Information Processing Systems* (November
    1992): 164–171, [*https://papers.nips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html*](https://papers.nips.cc/paper/1992/hash/303ed4c69846ab36c2904d3ba8573050-Abstract.html).'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: ^([18](ch08.xhtml#idm46238704650928-marker)) S. Han et al., [“Learning Both
    Weights and Connections for Efficient Neural Networks”](https://arxiv.org/abs/1506.02626),
    (2015).
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '^([19](ch08.xhtml#idm46238704643440-marker)) M. Zhu and S. Gupta, [“To Prune,
    or Not to Prune: Exploring the Efficacy of Pruning for Model Compression”](https://arxiv.org/abs/1710.01878),
    (2017).'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '^([20](ch08.xhtml#idm46238704604576-marker)) V. Sanh, T. Wolf, and A.M. Rush,
    [“Movement Pruning: Adaptive Sparsity by Fine-Tuning”](https://arxiv.org/abs/2005.07683),
    (2020).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '^([21](ch08.xhtml#idm46238704590336-marker)) There is also a “soft” version
    of movement pruning where instead of picking the top <math alttext="k"><mi>k</mi></math>
    % of weights, one uses a global threshold <math alttext="tau"><mi>τ</mi></math>
    to define the binary mask: <math alttext="bold upper M equals left-parenthesis
    bold upper S greater-than tau right-parenthesis"><mrow><mi>𝐌</mi> <mo>=</mo> <mo>(</mo>
    <mi>𝐒</mi> <mo>></mo> <mi>τ</mi> <mo>)</mo></mrow></math> .'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
