- en: 9 Deep learning best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to the Kuala Lumpur real estate dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing the dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the deep learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the deep learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercising the deep learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In chapter 8 we examined a set of stacks for doing deep learning with tabular
    data. In this chapter, we use one of these stacks, Keras, to explore some best
    practices for deep learning with tabular data, including how to prepare the data,
    how to design the model, and how to train the model. We introduce a new problem
    to demonstrate all these best practices: predicting whether real estate properties
    in Kuala Lumpur will have a price above or below the median price for the market.
    We selected this dataset because it is messier and more challenging to prepare
    than the Airbnb NYC dataset we have used so far. Consequently, we’ll be able to
    demonstrate a wider range of techniques for applying deep learning to tabular
    datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are new to training deep learning models, the examples in this chapter
    will help you learn some best practices. If you already have extensive experience
    with defining and training deep learning architectures, this chapter could be
    beneficial for you as a review of principles.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Introduction to the Kuala Lumpur real estate dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we will use the Kuala Lumpur real estate dataset to explain
    the best practices for deep learning with tabular data. This dataset consists
    of records that describe properties sold in Kuala Lumpur, the capital of Malaysia.
    Figure 9.1 presents a sample of the records in this dataset from the output of
    `df.head()`. The code illustrated in this chapter is at [https://mng.bz/yWQp](https://mng.bz/yWQp).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F01_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 Sample of the Kuala Lumpur real estate dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we’ll go through the steps we need to take to clean up
    each of the columns in this dataset. To prepare for those descriptions, let’s
    first review what’s in each column of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Location`—The neighborhood in which the property is located.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Price`—The listed price for the property in Ringgit, including RM, the conventional
    symbol for Malaysian currency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Rooms`—The number of rooms in the property. Values like “2 + 1” in this column
    mean the property has two bedrooms and one room that cannot be classified as a
    bedroom.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Bathrooms`—The number of washrooms in the property.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Car Parks`—The number of parking spaces on the property.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Property Type`—The category of property, such as “Condominium,” “Serviced
    Residence,” etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Size`—The dimensions of the property. There are several aspects of the property
    that values in this column could refer to, including the overall land area or
    the built-up area within the property.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Furnishing`—Whether the property is furnished or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the basic questions we need to answer about this dataset is what columns
    are continuous or categorical. By looking at figure 9.1, we can spot a subset
    of columns that contain numeric values. Let’s take a closer look at this subset
    of the dataset to see if we can determine which columns are continuous. Figure
    9.2 shows values from the subset of columns in the dataset that appear to contain
    numeric data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F02_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 Subset of columns that look like they contain numeric data
  prefs: []
  type: TYPE_NORMAL
- en: 'We can validate which of these columns have numeric data by using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This command returns descriptive statistics for all numeric columns in the DataFrame,
    providing insights into the data distribution, central tendency, and spread within
    each numeric column. By examining the output of this command, you can identify
    which columns indeed contain numerical values. The output of this command is shown
    in figure 9.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F03_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 Output of `describe()` for this dataset
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3 indicates that only `Bathrooms` and `Car Parks` are numeric columns
    and that `Price`, `Rooms`, and `Size` are not numeric columns even though they
    contain some data that looks numeric. In the next section, as part of processing
    the dataset, we will describe the steps to extract the numeric data from the `Price`,
    `Rooms`, and `Size` features and make it available to train a model.
  prefs: []
  type: TYPE_NORMAL
- en: Another way of evaluating which columns are categorical or continuous is to
    count the number of unique values in each column. If a column contains a large
    number of unique values, that may be an indication that we should treat it as
    continuous, and if it contains a relatively small number of values, that may be
    an indication that we should treat it as categorical. In fact, features presenting
    few unique values are often considered categorical because they typically represent
    discrete categories or groups rather than continuous numerical measurements. This
    is not a hard and fast rule, as we shall see. The output of the `df.unique()`
    command gives the number of unique values in each column in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.1 Getting the count of unique values in each column
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① Returns the number of unique values in each column of the dataframe df
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of the command in listing 9.1 looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ① The limited number of values in this column reinforces our intuition that
    this column is categorical.
  prefs: []
  type: TYPE_NORMAL
- en: ② This column is continuous.
  prefs: []
  type: TYPE_NORMAL
- en: ③ This column requires further investigation.
  prefs: []
  type: TYPE_NORMAL
- en: ④ This column is continuous.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ This column is categorical.
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ This column is continuous, but it requires special treatment, as we will see
    later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ This column is categorical.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize what the output of the command in listing 9.1 tells us about the
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Columns that appear to be categorical*—`Location,` `Property` `Type,` `Furnishing.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Columns that appear to be continuous*—`Price,` `Bathrooms,` `Car` `Parks,`
    `Size.` Of these columns, `Size` also requires further investigation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Columns that need further investigation to determine whether they should be
    treated as continuous or categorical*—`Rooms.`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Two of these columns require further investigation: `Rooms` and `Size.` In
    the following section on processing the dataset, we will dig deeper into these
    two columns to determine how to deal with them.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a sense of the columns in the dataset and what kind of information
    they offer, let’s explore some more aspects of the dataset. First, let’s check
    the dimensions of the dataset, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.2 Code to check the dimensions of the dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① Statement to get the dimensions of the input dataframe
  prefs: []
  type: TYPE_NORMAL
- en: ② Output of the statement
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.2 shows that this dataset has over 53,000 rows and eight columns.
    In chapter 12, we will examine the relationship between the number of rows in
    a dataset, the nature of the columns in a dataset, and the applicability of a
    deep learning model to the data. For now, it is safe to say that while this dataset
    is on the small side, it is big enough for us to have a decent chance of training
    a deep learning model with it.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing contains the statements that list the number of missing
    values in each column of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.3 Statements to list missing values in each column
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ① The output of this statement is a count of the number of missing values by
    column.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output of the commands in listing 9.3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ① Location is the only column with no missing values.
  prefs: []
  type: TYPE_NORMAL
- en: ② Car Parks is the column with the most missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the output of the command in listing 9.3, we can see that all but one
    of the columns in this dataset have missing values. This is an early sign of some
    of the problems we will need to correct to get this dataset ready to train with
    a deep learning model. In comparison, the Airbnb NYC dataset only had missing
    values in four columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ① Missing values in the name column
  prefs: []
  type: TYPE_NORMAL
- en: ② Missing values in the host_name column
  prefs: []
  type: TYPE_NORMAL
- en: ③ Missing values in the last_review column
  prefs: []
  type: TYPE_NORMAL
- en: ④ Missing values in the reviews_per_month column
  prefs: []
  type: TYPE_NORMAL
- en: As shown here, almost every column in the Kuala Lumpur dataset is missing values,
    which warns us that, as often happens with real-world data, we will have to perform
    a lot of work to clean up this dataset before we can start using it with a model.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have taken a first look at the Kuala Lumpur real estate
    dataset. In the next section, we will review the process of preparing this dataset
    to train a deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Processing the dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we’ve looked at the Kuala Lumpur real estate dataset and seen that
    it has a large number of missing values, we have some idea that it will require
    a good deal of processing before we can use it to train a model. In this section,
    we will look at the features of the dataset one by one to describe the cleanup
    required.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we haven’t decided yet if we are going to use a subset or the
    full set of columns. Initially, our approach defaults to utilizing all available
    features. As we progress and examine the model’s performance and behavior, we
    may decide to exclude some features, for example, because these features have
    undetected invalid values that effect the model’s performance. It is not just
    because we cannot anticipate what features will work and what won’t that we strive
    to obtain the full set of clean features available to the model for training.
    It is also a way to get the most out of data, as we will learn more about the
    dataset through processing it entirely and rendering it reusable for other projects
    as well. This comprehensive approach ensures better results than we would have
    obtained if we only cleaned up the features that we ultimately use to train the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The code highlighted in this section is available at [https://mng.bz/MDBQ](https://mng.bz/MDBQ)
    and the config file is at [https://mng.bz/av1j](https://mng.bz/av1j).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by addressing columns that only involve handling missing values:
    `Bathrooms,` `Car` `Parks,` `Furnishing,` `Property Type,` `Location`. Next, we
    will describe the process for the columns that require more cleanup than simply
    dealing with missing information: `Price`, `Rooms`, and `Size`.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1 Processing Bathrooms, Car Parks, Furnishing, Property Type, and Location
    columns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For a subset of the columns in the dataset (`Bathrooms,` `Car` `Parks,` `Furnishing,
    Property` `Type,` `Location`), we can accomplish an effective cleanup by simply
    dealing with missing values. The config file contains default values to replace
    missing values for these columns that we determined based on their characteristics
    and domain knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.4 Defining the default replacement values for missing values
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: ① For the Bathrooms columns, sets the median value for the column as the default
    value
  prefs: []
  type: TYPE_NORMAL
- en: ② For the Car Parks column, sets the default value to zero
  prefs: []
  type: TYPE_NORMAL
- en: ③ For the categorical columns, sets a placeholder category as the default value
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.4 shows that we set missing values for the `Bathrooms` column in this
    dictionary to the median value for the column. For the `Car` `Parks` column, we
    set missing values to zero. The reason for the difference between these is due
    to the specific use case of real estate listings. Residential properties will
    rarely have no washrooms, so picking the median value as the default for `Bathrooms`
    makes sense. On the other hand, many properties will have no parking spaces. If
    a property does have a parking space, it is in the best interests of the seller
    and the seller’s agent to include this in the listing to ensure they get the best
    selling price for the property. Thus, when `Car` `Parks` is missing a value, it
    makes sense to assume that this property does not have any parking spaces, so
    we set missing values in this column to zero.
  prefs: []
  type: TYPE_NORMAL
- en: In this dictionary, we also have distinct placeholder category values for the
    categorical columns. The code for obtaining this simple cleanup is placed in the
    data cleanup notebook’s function `clean_up_misc_cols()`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.5 Function to replace general missing values
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ① Iterates through the columns that have simple data cleanup
  prefs: []
  type: TYPE_NORMAL
- en: ② Replaces missing values with the median for the column where specified
  prefs: []
  type: TYPE_NORMAL
- en: ③ For the other columns, replaces missing values in the column with the default
    value for that column
  prefs: []
  type: TYPE_NORMAL
- en: In the `clean_up_misc_cols()` function shown in listing 9.5, the dictionary
    defined in the config file, as shown in listing 9.4, is used to replace missing
    values in the columns that require a simple cleanup.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have described how the cleanup is done for the columns that only
    require missing values to be dealt with, the subsequent subsections in this section
    will describe the more intensive data operations that are required for the remaining
    three columns: `Price`, `Rooms`, and `Size`.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2 Processing the Price column
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we get into what needs to be cleaned up in the `Price` column, let’s
    review some examples of values in this column, as shown in figure 9.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F04_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 Examples of values in the Price column
  prefs: []
  type: TYPE_NORMAL
- en: 'The values in figure 9.4 show a few items that need to be dealt with in the
    `Price` column:'
  prefs: []
  type: TYPE_NORMAL
- en: Values including the symbol “RM”, representing Ringgit, the Malaysian currency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values needing to be converted to float
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 9.6 presents the `clean_up_price_col()` function, which contains a code
    snippet to effectively clean up the `Price` column.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.6 Function to clean up the `Price` column
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: ① Drops rows in the dataframe that are missing a value in the Price column
  prefs: []
  type: TYPE_NORMAL
- en: ② Removes the currency symbol
  prefs: []
  type: TYPE_NORMAL
- en: ③ Removes commas and convert the values to float
  prefs: []
  type: TYPE_NORMAL
- en: As shown in listing 9.6, the `clean_up_price_col()` function removes rows with
    missing `Price` values. The rationale for removing these rows (as opposed to replacing
    missing `Price` values with some placeholder) is that `Price` is the target for
    our model; hence it won’t work to keep rows where such a value is missing. The
    output of the `clean_up_price_col()` function is a dataframe with valid numeric
    values in all rows for the `Price` column.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.3 Processing the Rooms column
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we get into what needs to be cleaned up in the `Rooms` column, let’s
    review what some of the values in this column look like, as shown in figure 9.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F05_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 Examples of values in the `Rooms` column
  prefs: []
  type: TYPE_NORMAL
- en: 'The values in figure 9.5 present some examples of the problems that need to
    be dealt with in the `Price` column:'
  prefs: []
  type: TYPE_NORMAL
- en: Missing values.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Double-barrelled values that contain more than one constituent value. In the
    Kuala Lumpur real estate dataset, some such values include string expressions
    such as “4 + 1.” These values require parsing to extract usable values for the
    model training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As for the missing values, we can opt to replace the `NaN` values with zero.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this point, we have the urge to make a choice about how to deal with the
    `Rooms` column overall: should we treat it as a categorical column or a continuous
    column? To help us decide, let’s review the count of unique values in the `Rooms`
    column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: With around 40 values in the `Rooms` column, which is a not-too-large number
    of categories, we could convert it into a categorical column if we wished. Suppose
    we opt to treat it as a numeric column; let’s check the necessary steps to take.
    To begin with, let’s look at the first few unique values and their counts.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.7 Count of the most common values in the `Rooms` column
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ① Example of a value that is immediately convertible to a numeric value
  prefs: []
  type: TYPE_NORMAL
- en: ② Example of a value that can be turned into a numeric value by treating it
    as an equation
  prefs: []
  type: TYPE_NORMAL
- en: ③ Example of a value that is a string that is not convertible to a numeric value
  prefs: []
  type: TYPE_NORMAL
- en: ④ Example of a value that could be converted to a numeric value with some extrapolation
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to treat `Rooms` as a continuous column, we can deal with the representative
    examples shown in listing 9.7 in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Values like `3` that can be converted directly to numeric: convert to numeric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values like `3` `+` `1` that we can use the built-in `eval()` Python function
    to evaluate the string value as if it were an equation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Values like `Studio`: replace them with a reasonable numeric value, such as
    1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values like `6+` should be treated as if they were `6+1`. This is not a perfect
    approach—the dataset does not clarify whether `6+` is a short form of `6+1` or
    if it means “6 plus an unspecified number of additional rooms.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that the `treat_rooms_as_numeric` setting in the config file for data
    preparation controls whether `Rooms` is prepared as a continuous column or a categorical
    column. If you set this value to `True`, `Rooms` is prepared as a continuous column,
    and if you set it to `False` then `Rooms` is prepared as a categorical column.
    In addition to updating the data preparation config file, you also need to ensure
    that `Rooms` is in the appropriate list in the model training config file so that
    the model training notebook knows whether to treat `Rooms` as categorical or continuous,
    as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have looked at the transformations we would make to treat `Rooms`
    as a numeric column, we can look at the `clean_up_rooms_col()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.8 Count of the most common values in the `Rooms` column
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ① Checks the parameter to determine whether Rooms will be treated as a continuous
    column
  prefs: []
  type: TYPE_NORMAL
- en: ② Deals with values like “6+”
  prefs: []
  type: TYPE_NORMAL
- en: ③ If a value ends with +, adds 1 to the end of the string.
  prefs: []
  type: TYPE_NORMAL
- en: ④ If the value is Studio, replaces it with 1.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Example of a value that could be converted to a numeric value with some extrapolation
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Replaces strings that are valid equations with the numeric result of the equation
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Converts all values in the column to numeric
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ If any NaNs have been introduced in these transformations, replaces them with
    0.
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ If the column is being treated as categorical, replaces missing values with
    a placeholder value.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.8 shows that we need to perform many transformations if we want to
    treat the `Rooms` column as a continuous column. In particular, we need to replace
    one-off nonnumeric values (`Studio`, `20` `Above`) with our best guess of the
    corresponding numeric value, and we need to replace values that include `+` with
    the evaluation of the string as if it were an equation. For values that end with
    `+`, we assume it’s valid for the string to end with `+1` so that the value can
    be treated as an equation by the `eval()` function. We’ve made some assumptions
    about what values like `6+`, `Studio`, and `20 Above` mean.
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world scenario, we might have access to a subject matter expert, or
    we may have to make similar guesses to see whether we can get a signal out of
    these values. Given the expected importance of the `Rooms` column (more rooms
    typically mean more surface area, which can often contribute to higher property
    values), the acid test will be when we train the model and compare the resulting
    performance based on treating this column differently, either as categorical or
    continuous. When we get to train the model later in the chapter, we will try it
    using both variations of the `Rooms` column to determine which one produces the
    best results.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.4 Processing the Size column
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Among all the potential features for the Kuala Lumpur real estate price prediction
    problem, the `Size` column is the most problematic. Before going into the details
    of how to clean up this specific column, let’s review some samples of values in
    the `Size` column, as shown in figure 9.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F06_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 Examples of values in the `Size` column
  prefs: []
  type: TYPE_NORMAL
- en: Along with some missing values, this column contains as a string the classification
    of the size type (`Built-up` or `Land` `area`) as well as the area of the property
    and the area metric (“sq. ft”). But that’s not all. As you can see in figure 9.7,
    there are entries in the `Size` column that express the area as length by width.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F07_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 Examples of values in the `Size` column with area expressed in length
    by width
  prefs: []
  type: TYPE_NORMAL
- en: There is still more to be done with the `Size` column. Figure 9.8 shows examples
    of values in the `Size` column where the area of the property is expressed in
    various equations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F08_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 Examples of values in the `Size` column with area expressed in complex
    equations
  prefs: []
  type: TYPE_NORMAL
- en: 'So it appears that the `Size` column combines three or more different pieces
    of information for each entry:'
  prefs: []
  type: TYPE_NORMAL
- en: Size type (`Land area` or `Built` `up`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Area, which can be formatted as a numeric value (e.g., `6900`), length by width
    (e.g., `20` `×` `80`), or length by width with an equation in one or both dimensions
    (e.g., `10` `+` `24` `×` `80`). We are assuming that the property is rectangular.
    Note that this assumption needs to be validated by a real estate professional,
    and this is an essential point. To conduct a thorough analysis of a dataset, it’s
    crucial to have access to a subject matter expert who can verify assumptions.
    For instance, an expert could suggest using missing value replacements such as
    the median number of bathrooms, which is a good default value to replace missing
    values in the `Bathrooms` column, or 0, which is a good replacement for missing
    values in the `Car` `Parks` column. We have set up config files for the data preparation
    and model training notebooks to make it easy to make changes if our assumptions
    don’t match what the subject matter expert says. By putting parameters like this
    in config files, we can update the behavior of the system without touching the
    Python code and run experiments methodically.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Area metric (e.g., `sq.` `ft.`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may be asking now how we were able to identify that the `Size` column contained
    all these anomalies and find a remedy. The answer is through trial and error.
    First, we separated the size type from the area and area metric. Next, we removed
    the area metric (since it is always the same). Then, we iterated through the remaining
    area values, removing or replacing characters until every area value could be
    applied to `eval()`.
  prefs: []
  type: TYPE_NORMAL
- en: This meticulous iterative process is not uncommon for real-world datasets. We
    chose the Kuala Lumpur real estate dataset for this section of the book because
    it presents these kinds of real-world challenges, and working through them illustrates
    the approaches that need to be taken, some systematic and some tactical, to squeeze
    as much useful signal as possible out of a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We need to separate the three kinds of values in the `Size` column. We will
    discard the area metric because it is always a variation of “square feet” and
    create a new categorical column that combines the size type value with an identifier
    for the bin that the area value falls into. To do this, we will need to
  prefs: []
  type: TYPE_NORMAL
- en: Convert the area values to numeric values by discarding extraneous characters
    and using the `eval()` function to convert the string representation of equations
    into the numeric result of evaluating the equation. That is, 20 × 80 is replaced
    with 160.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get bins for the resulting numeric area values and add a new column to the dataset
    with the bin value for each row.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a new categorical column that combines size type and bin number.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following listing shows the `clean_up_size_col()` function, which implements
    the changes we have described so far in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.9 Code to clean up the `Size` column
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: ① Removes rows that are missing Size values
  prefs: []
  type: TYPE_NORMAL
- en: ② Changes all values in the Size column to lowercase
  prefs: []
  type: TYPE_NORMAL
- en: ③ Splits the Size column by moving the size type values into a new column”
  prefs: []
  type: TYPE_NORMAL
- en: ④ Replaces any remaining missing values with 0
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Removes any rows where the size column contains no digits
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Removes any rows where the size column cannot be converted to a numeric value
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Replaces characters that will cause problems treating multiplications correctly
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Removes remaining characters that will cause problems treating the remaining
    Size values as numeric
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Removes extraneous characters following spaces
  prefs: []
  type: TYPE_NORMAL
- en: ⑩ Evaluates the remaining Size values as equations
  prefs: []
  type: TYPE_NORMAL
- en: ⑪ Defines bins for the Size values
  prefs: []
  type: TYPE_NORMAL
- en: ⑫ Defines bin names
  prefs: []
  type: TYPE_NORMAL
- en: ⑬ Creates a new column that contains the bin value for the Size value in that
    row
  prefs: []
  type: TYPE_NORMAL
- en: ⑭ Creates a new categorical column that combines the Size type value and the
    Size_bin value
  prefs: []
  type: TYPE_NORMAL
- en: The `clean_up_size_col()` function shown in listing 9.9 comprises a series of
    transformations that allow the evaluation of the area information from the `Size`
    column as a numeric value. To do this, we need to remove the rows where the area
    cannot be interpreted. The area of the property is so fundamental to determining
    its pricing value that it does not make sense to train a model using examples
    from listings where it is impossible to extract the area information; hence, we
    simply drop those rows. Next, we need to clean up the remaining area values so
    that they can be evaluated as numeric values, either because they can be directly
    converted to numeric values or because they can be interpreted as equations that
    can be applied to the `eval()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Given the significant influence of a listing’s area on its price, it’s worthwhile
    to invest a lot of effort into extracting the area information in any way we can.
    For this purpose, we create new categorical columns containing the bin number
    corresponding to the size of the listing. Utilizing this bin value makes it possible
    to evaluate the area portion of the `Size` column as a numeric value. In doing
    this, we need to remove rows where the area cannot be interpreted. The area of
    the property is so fundamental to its value that it does not make sense to train
    the model with the data from listings where it is impossible to extract the area
    information because it is concatenated with the size type (`built-up` or `land`
    `area`).
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.9 displays examples of values in the revised `Size` column along with
    the new columns (`Size_type`, `Size_bin`, and `Size_type_bin`) created by the
    function in listing 9.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F09_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 Examples of values in the new columns generated from the `Size` column
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at each column in figure 9.9:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Size`—Replacing the original values in the `Size` column are single continuous
    values that correspond to the land area from the original `Size` column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Size_type`—This new column contains the size type (`built-up` or `land` `area`)
    portion of the original `Size` column value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Size_bin`—This new column contains the bin number that the `Size` column value
    belongs to. Notice that for the examples in figure 9.9, the row with the smallest
    `Size` value has the smallest bin number, and the rows with the largest `Size`
    values have the largest bin number.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Size_type_bin`—This new column contains a combination of the values in the
    other two new columns.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider a specific value from the original `Size` column and how it gets processed
    to create the values in the new `Size`, `Size_type`, `Size_bin`, and `Size_type_bin`
    columns, as shown in figure 9.10.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F10_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 Original `Size` column and values in the new columns
  prefs: []
  type: TYPE_NORMAL
- en: 'We started with a `Size` column that contained a jumble of two kinds of critical
    information jammed into a single column and that had numeric data (the area of
    the property), sometimes as a number and sometimes as an equation. After applying
    the cleanup steps, we have split the `Size` column into four columns: one continuous
    (`Size`) and three categorical (`Size_type`, `Size_bin`, and `Size_type_bin`)
    that we can choose from to train the model.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Defining the deep learning model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will go through the code that defines the deep learning
    model for the Kuala Lumpur real estate price prediction model. First, we’ll compare
    the approach for defining the model used in this chapter, with Keras preprocessing
    layers, to the approach that we saw in chapter 3, which used custom layers. Then
    we’ll review in detail the code that makes up the model definition. Finally, we’ll
    wrap up this section by discussing the rationale for using Keras preprocessing
    layers as a best practice for deep learning with tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: The code in this section is available at [https://mng.bz/gaBe](https://mng.bz/gaBe),
    and the config file is at [https://mng.bz/ey19](https://mng.bz/ey19).
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.1 Contrasting the custom layer and Keras preprocessing layer approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Starting in chapter 3, we have examined the Keras-based solution for the Airbnb
    NYC price prediction problem. In that solution, we created a deep learning model
    and the associated pipelines “from scratch.” That is, we didn’t use any functions
    from TensorFlow or Keras specifically designed for tabular data. In the solution
    we propose for the Kuala Lumpur real estate problem, we are going to switch gears
    and take advantage of Keras preprocessing layers to make it easier to handle the
    tabular dataset. To provide some context on the differences between the two approaches,
    let’s compare:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Using custom layers*—This is the approach we used for the Airbnb NYC price
    prediction problem in chapter 3 and the baseline for comparison with other deep
    learning approaches in chapter 8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Using Keras preprocessing layers*—This is the approach we will use for this
    chapter and the subsequent chapters in this book.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See table 9.1.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.1 Comparison of the Kuala Lumpur real solutions using custom classes
    and using Keras preprocessing layers
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Using custom layers | Using Keras preprocessing layers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Pipeline | Custom pipeline classes based on Scikit-learn Pipeline class ([https://mng.bz/pKP5](https://mng.bz/pKP5));
    requires classes to be defined in a separate file so they can be used at training
    and inference ([https://mng.bz/OBxK](https://mng.bz/OBxK)); complex code to define,
    train, invoke, and save the pipelines. | The standard, off-the-shelf Keras preprocessing
    layers, available at [https://mng.bz/YD1o](https://mng.bz/YD1o). The code to define,
    train, and invoke the pipeline is much simpler and more robust. |'
  prefs: []
  type: TYPE_TB
- en: '| Model definition | A complex set of layers, specified to work with categorical,
    continuous, and text inputs | A simple set of layers, capable of working with
    generic categorical and continuous inputs |'
  prefs: []
  type: TYPE_TB
- en: Figure 9.11 shows the layers that make up the Kuala Lumpur real estate price
    prediction model using Keras preprocessing layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F11_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 Kuala Lumpur real estate price prediction model with Keras preprocessing
    layers
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.12 shows the layers that make up the Kuala Lumpur real estate price
    prediction model using custom layers; the code is available at [https://mng.bz/KGeP](https://mng.bz/KGeP).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F12_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 Kuala Lumpur real estate price prediction model with custom layers
  prefs: []
  type: TYPE_NORMAL
- en: The overall structure for the Keras model with custom layers in figure 9.12
    looks more complex than the structure in figure 9.11 for the model with Keras
    preprocessing layers, but there is a lot going on, so it’s not easy to see the
    details. Let’s zoom into the layers that just process the `Size` column to get
    a more specific idea of how these two architectures differ. Figure 9.13 shows
    the layers for the `Size` column in the model with Keras preprocessing layers.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F13_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 Layers for the `Size` column with Keras preprocessing layers
  prefs: []
  type: TYPE_NORMAL
- en: We can see that there are four layers between the input layer for `Size` and
    the final output layer. Compare this to the layers for the `Size` column for the
    Keras model with custom layers, as shown in figure 9.14.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F14_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 Layers for the `Size` column with custom layers
  prefs: []
  type: TYPE_NORMAL
- en: For the Keras model defined with custom layers, there are seven layers between
    the input layer for `Size` and the final output layer, compared to four layers
    for the model that uses Keras preprocessing layers. Because of the way that the
    layers are chained together in the model with customer layers, there is a series
    of individual concatenation operations layer by layer. In contrast, the model
    with Keras preprocessing layers has a single concatenation layer that pulls together
    all layers coming in from each input. This difference in the number of intermediate
    layers between the input of the `Size` column and the final layer reflects the
    overall additional complexity of the Keras model with custom layers compared to
    the model with Keras preprocessing layers.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.2 Examining the code for model definition using Keras preprocessing layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, the code for defining and training the model using Keras preprocessing
    is simpler and more streamlined than the deep learning code you read about in
    previous chapters that used custom layers and Scikit-learn based pipelines. However,
    a small price is to be paid in exchange for the increased simplicity of a model
    based on Keras preprocessing layers. To avoid getting an error when saving the
    model using `model.save()` and in the model saving callback, we need to ensure
    that all column names are lowercase and contain no spaces (this can be achieved
    using the `snake_case` naming convention instead) for the model using Keras preprocessing
    layers. The code in the following listing automatically does this.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.10 Lowercase column names and replacing spaces
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: ① For columns in the input dataframe, replaces spaces in column names with underscores
  prefs: []
  type: TYPE_NORMAL
- en: ② Lowercase column names in the input dataframe
  prefs: []
  type: TYPE_NORMAL
- en: ③ For the list of categorical column names, replaces spaces with underscores
  prefs: []
  type: TYPE_NORMAL
- en: ④ For the list of continuous column names, replaces spaces with underscores
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Lowercases the list of categorical column names
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Lowercases the list of continuous column names
  prefs: []
  type: TYPE_NORMAL
- en: The code shown in listing 9.10 replaces spaces in the column names in the input
    DataFrame and the lists of categorical and continuous column names. For example,
    the variable name `Car` `Parks` becomes `Car_Parks`.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing presents the definition of the `df_to_dataset` function,
    which creates an input pipeline for the model that uses Keras preprocessing layers.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.11 Function to create an input pipeline
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: ① Gets the target column from the local copy of the dataframe
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a new dictionary df with the same keys and values but with a new axis
    added to it
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a TensorFlow Dataset ds using the from_tensor_slices method
  prefs: []
  type: TYPE_NORMAL
- en: ④ Shuffles the elements of the dataset to avoid overfitting if the data has
    some intrinsic sorting
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Groups the elements of the dataset into batches of size batch_size
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Applies prefetch() to the dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The `df_to_dataset` function shown in listing 9.11, which is taken directly
    from [https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers),
    will be applied to the training, validation, and test datasets to convert them
    into `tf.data.Dataset` objects, then shuffle and batch the datasets. Note that
    the definition of the dataset `ds` takes two arguments: `dict(df)`, a dictionary
    version of the input dataframe, and `labels`, the target values from the input
    dataframe. Also, note that applying `prefetch()` to the dataset allows the dataset
    to be processed more efficiently by overlaying the preprocessing and model execution
    of one batch while the next batch is being loaded.'
  prefs: []
  type: TYPE_NORMAL
- en: The following listing defines the `get_normalization_layer()` function, which
    defines a normalization layer for a given feature.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.12 Creating normalization layers for continuous columns
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: ① Defines a normalization object
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a dataset from the input dataset with only the input feature
  prefs: []
  type: TYPE_NORMAL
- en: ③ Trains the normalizer using the specified input feature
  prefs: []
  type: TYPE_NORMAL
- en: The `get_normalization_layer()` function defined in listing 9.12, which is taken
    directly from [https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers),
    will be applied to all the continuous columns that we want to use to train the
    model. This function scales input values with a distribution centered around 0
    with a standard deviation of 1\. For details about the normalization object defined
    in this function, see [https://mng.bz/9YDx](https://mng.bz/9YDx). Note that for
    gradient boosting solutions, normalization would not be needed.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing presents the definition of the `get_category_encoding_layer()`
    function, which specifies an encoding layer for a given categorical column.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.13 Creating encoding layers for categorical columns
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates layers for the column depending on whether or not it’s a string column
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a dataset from the input dataset with only the input feature
  prefs: []
  type: TYPE_NORMAL
- en: ③ Learns the set of possible values for the column and assigns them a fixed
    numeric index
  prefs: []
  type: TYPE_NORMAL
- en: ④ Encodes the numeric indices
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Applies multi-hot encoding to the indices
  prefs: []
  type: TYPE_NORMAL
- en: The `get_category_encoding_layer()` function defined in listing 9.13, which
    is taken directly from [https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers),
    will be applied to all the categorical columns that we want to use to train the
    model. In this function, if a column is a string column, a layer is generated
    that turns strings into numeric indices; otherwise, a layer is created that turns
    integer values into numeric indices.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows the code to apply the `df_to_dataset()` function
    to the training, validation, and testing datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.14 Applying `df_to_dataset()` to train, validate, and test datasets
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: ① Generates training dataset
  prefs: []
  type: TYPE_NORMAL
- en: ② Generates validation dataset
  prefs: []
  type: TYPE_NORMAL
- en: ③ Generates test dataset
  prefs: []
  type: TYPE_NORMAL
- en: Once the code in listing 9.14 is applied, we will obtain the datasets and be
    ready to start the training process.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows the code to apply the `df_to_dataset()` function
    to the training, validation, and testing datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.15 Defining layers for continuous and categorical columns
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: ① List for input features
  prefs: []
  type: TYPE_NORMAL
- en: ② List for encoded features
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a normalization layer for each continuous column
  prefs: []
  type: TYPE_NORMAL
- en: ④ Creates an encoding layer for each categorical column
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code in listing 9.15, for each continuous column, `get_normalization_layer()`
    defines a normalization layer for the column. The new layer is added to the `encoded_features`
    list, and the column name is appended to the `all_inputs` list. For each categorical
    column, `get_category_encoding_layer()` defines an encoding layer for the column.
    The new layer is added to the `encoded_features` list, and the column name is
    appended to the `all_inputs` list. Once the code in listing 9.15 is run, the `all_features`
    list contains the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'If we look at the settings in the config file that specify the continuous and
    categorical columns used to train the model, we see that they match the layers
    specified in the `all_features` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have defined the layers that correspond with the columns of the
    input dataset, we can define the model. The following listing shows the code that
    defines the model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.16 Code to define the model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: ① Concatenates the features from the encoded_features list created in the code
    in listing 9.15
  prefs: []
  type: TYPE_NORMAL
- en: ② Adds a dense layer to the model. It corresponds with layer 2 in figure 9.15.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Adds a dropout layer to the model. It corresponds with layer 3 in figure 9.15.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Adds a dense layer to the model. It corresponds with layer 4 in figure 9.15.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.16 includes concatenating the features from the `encoded_features`
    list with the `concatenate()` function. It corresponds with layer 1 in figure
    9.15.
  prefs: []
  type: TYPE_NORMAL
- en: Once the code in listing 9.16 is applied, the set of layers for the model is
    defined. As shown in figure 9.15, every categorical column has an input layer,
    a `StringLookup` layer, and a `CategoryEncoding` layer, and every continuous column
    has an input layer and a normalization layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F15_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 Diagram of the layers in the model
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.15 shows the layers rendered in the default vertical arrangement.
    To get a horizontal arrangement, we can use the `rankdir=''LR''` parameter in
    the `plot_model` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, note that the architecture shown in figure 9.15 is simple enough
    for the modest number of features in the Kuala Lumpur dataset. The architecture
    will get complicated if you are working with a dataset with a large number of
    features. In addition, the approach described in this chapter for categorical
    columns depends on one-hot encoding. This approach works out for the Kuala Lumpur
    real estate dataset since the maximum number of unique values in any categorical
    column is just a bit over 100:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: However, if we had a dataset with hundreds or thousands of values in some of
    its categorical columns, using one-hot encoding could cause memory problems. In
    that case, we may have to consider using embeddings for categorical columns, as
    we did for the categorical columns in the Keras model with custom layers.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have reviewed the code to define the model. In the next
    section, we will go over the code to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Training the deep learning model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we examined the code to define the model and examined
    how the layers in the model were built up based on the input columns. In this
    section, we’ll describe the process of training the model that we defined in the
    previous section.
  prefs: []
  type: TYPE_NORMAL
- en: The code in this section is available at [https://mng.bz/gaBe](https://mng.bz/gaBe)
    and the config file at [https://mng.bz/ey19](https://mng.bz/ey19). The following
    listing shows the code necessary to compile and train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.17 Code to compile and train the model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: ① Compiles the model defined in listing 9.16 using the hyperparameters from
    the config file
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates the list of callbacks and uses it in the call to fit to train the
    model
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calls fit to train the model without the callback list
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.17 shows that the hyperparameter values are from the config file.
    By defining the hyperparameters in the config file, we can adjust them and rerun
    the model training notebook without touching the Python code. Maintaining the
    config values in a file separate from the Python code reduces the chance of regressions
    caused by touching the code and makes it easier to track the results of experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Note also that we can call `fit()` to train the model with callbacks. The code
    in listing 9.17 shows that we can call `fit()` using a set of callbacks if we
    want to control the training process or without callbacks to let the training
    process run through all the specified epochs without interruption. Using callbacks
    to control the training process is a best practice for deep learning models with
    Keras because it allows us to use the resources to train the model more efficiently.
    Instead of running through all the specified epochs and getting the performance
    of the model from whatever it happened to be in the last epoch, we can use callbacks
    to stop the training process if it stops improving for a given number of epochs.
    We can ensure that the outcome of the training process is optimal for the whole
    training process.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the model has been trained, the following listing presents how to perform
    a quick evaluation of the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.18 Code to get a quick evaluation of the trained model
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: ① Uses the evaluate() function to get the loss and accuracy for the trained
    model with the test dataset
  prefs: []
  type: TYPE_NORMAL
- en: ② Tests loss for this training run
  prefs: []
  type: TYPE_NORMAL
- en: ③ Tests accuracy for this training run
  prefs: []
  type: TYPE_NORMAL
- en: The output from running listing 9.18 presents decent results on the test set
    using the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.1 Cross-validation in the training process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In chapter 4 we highlighted cross-validation (that is, segmenting the dataset
    and repeatedly training the model on different subsets of the data with different
    holdouts to validate the model) as a best practice for classic machine learning
    approaches. Do we need to worry about cross-validation for deep learning with
    tabular data? The short answer is “no.” In Keras, by default, when we do repeated
    training runs with proportions of the dataset specified to use for training, validation,
    and testing, the subsets of the dataset that get put in each category are randomized,
    so we will get the benefits of cross-validation with Keras naturally if we do
    repeated training runs ([https://mng.bz/jpPz](https://mng.bz/jpPz)).
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.2 Regularization in the training process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another technique we highlighted in chapter 4 was regularization (that is, preventing
    overfitting by reducing the complexity of the model to improve its generalization
    performance). Do we need to be concerned the same about overfitting using deep
    learning as we do with classic machine learning? The answer is “absolutely,” and
    you can see in figure 9.16 a subset of the diagram for the model that highlights
    the part of the model that is specifically concerned with avoiding overfitting,
    the dropout layer. The dropout layer randomly sets inputs to 0 to reduce overfitting
    ([https://mng.bz/W2z4](https://mng.bz/W2z4)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F16_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.16 Regularization in the deep learning model
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.3 Normalization in the training process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normalization refers to adjusting continuous values across features so that
    their values fall into a consistent range. In chapter 4, we covered using this
    technique (also known as standardization) in classic machine learning. In listing
    9.12, we showed the Kuala Lumpur solution code that uses normalization layers.
  prefs: []
  type: TYPE_NORMAL
- en: There are examples of continuous columns in the Kuala Lumpur dataset whose values
    have significantly different ranges. For example, in the Kuala Lumpur dataset,
    the `Bathrooms` feature ranges between 0 and 20 while `Size` ranges from 0 to
    11 million. Leaving the continuous columns with such disparate ranges can make
    the training process less efficient. Figure 9.17 shows the normalization layers
    that make the ranges of values for the continuous features fall within more consistent
    ranges.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F17_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.17 Normalization in the deep learning model
  prefs: []
  type: TYPE_NORMAL
- en: The normalization layers adjust the values in each of the continuous columns
    into a distribution centered around 0 with a standard deviation of 1 ([https://mng.bz/0QKp](https://mng.bz/0QKp)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we have reviewed the code for training the Kuala Lumpur real
    estate price prediction model, along with a set of best practices demonstrated
    in the model: regularization to avoid overfitting, and normalization to bring
    the values of continuous columns into consistent ranges. In the next section,
    we will put all our work together by exercising the trained deep learning model
    with brand-new data points.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.5 Exercising the deep learning model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far in this chapter, we have prepared the Kuala Lumpur real estate dataset,
    defined a deep learning model using Keras preprocessing layers, and trained the
    model. In this section, we’ll exercise the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: The code in this section is available at [https://mng.bz/gaBe](https://mng.bz/gaBe)
    and the config file at [https://mng.bz/ey19](https://mng.bz/ey19).
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.1 Rationale for exercising the trained model on some new data points
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The process of deploying a model is complex, and we will examine it in more
    depth in chapters 10 and 11\. It can be a big investment to complete the initial
    deployment, so we want to ensure that deployment goes smoothly and we don’t have
    to backtrack to earlier points in the process if we don’t have to. If we exercise
    the trained model on a few real-world data points as soon as possible, we can
    save more problems from appearing later in the process. There are two key benefits
    to exercising the trained model on new data points as soon as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s an easy way of detecting data leakage (see [https://mng.bz/zZXw](https://mng.bz/zZXw)).
    It can take months between the start of a data science project and the deployment
    of the trained model, particularly if you are dealing with the demands of deep
    learning. During that time, it’s possible to lose track of exactly what data will
    be available to the trained model once it has been deployed. Exercising the trained
    model with a few data points is an easy way of uncovering potential data leakage
    before the project has gone too far. The reason for this is that exercising the
    model will force you to think about what a new data point looks like.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides a way to validate the performance of the model on the test dataset.
    For example, if you exercise the trained model with some brand-new data points
    from the Kuala Lumpur real estate market, you get a sense of whether the performance
    of the model on the test dataset is consistent with its performance with brand-new
    data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To illustrate how exercising the trained model on a few new, real-world examples
    can help to prevent data leakage, let’s look at how we would exercise the trained
    model to predict whether a brand-new listing would have a selling price above
    or below the median for Kuala Lumpur. For the Kuala Lumpur real estate dataset,
    suppose there was a column called `Weeks` `on` `the` `market` that had the number
    of weeks the property had been on the market before it was sold. If we included
    such a column in the training of the model, what would happen when we tried to
    exercise the trained model with a handful of new data points? Figure 9.18 illustrates
    the problem we would run into.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F18_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.18 Validating the features used to train the model with a new data
    point
  prefs: []
  type: TYPE_NORMAL
- en: On the left side of figure 9.18, we have the list of features used to train
    the model as they would be specified in the config file, including the `Weeks`
    `on` `the` `market` feature. On the right is an extract from a real Kuala Lumpur
    real estate listing. If we want to be able to use the trained model to get a price
    prediction for actual real estate listings, then we should be able to find values
    for all the features on the left in the listing on the right.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown by the numbers in figure 9.18, for most of the features we will use
    to train the model, there is a value in the real Kuala Lumpur real estate listing.
    However, there are two features where we can’t get the values from the real estate
    listing:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Car` `Parks`—This particular listing mentions covered parking for the building
    overall but doesn’t include any information about parking spaces for this particular
    unit. For the sake of a quick test, we can assume that the value of `Car` `Parks`
    for this listing should be 0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Weeks` `on` `the` `market`—The listing does not include the number of weeks
    it was on the market before it was sold because we won’t know this value until
    after the listing has sold. We cannot include `Weeks` `on` `the` `market` in the
    training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have explained why it’s important to exercise the trained model
    on a handful of real data points, we will review the code for exercising the model
    in the next subsection.
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.2 Exercising the trained model on some new data points
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have established the benefits of exercising the trained model on
    new data points, let’s review the code for doing this.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows the code to save the model to the file system, define
    a new data point by specifying values for all the features used to train and exercise
    the model on the new data point.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.19 Code to exercise the trained model on a new data point
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: ① Saves the trained model to the file system
  prefs: []
  type: TYPE_NORMAL
- en: ② Reloads the saved model
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines a new data point point
  prefs: []
  type: TYPE_NORMAL
- en: ④ Puts the new data point into the format expected by the model
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Gets a prediction for the new data point
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the code shown in listing 9.19, we get an output like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The model seems certain that this listing will be over the median price. Can
    you think of a reason why? It could be because the size shown in this property
    is huge—over 16,000 square feet. Suppose we rerun this code with a size value
    of 1,500 and leave all the other values the same. We get output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This seems roughly in line with our expectations. When we reduce the size to
    a more reasonable value and leave all the other feature values the same, we get
    a prediction that is roughly what we expect. With the code shown in listing 9.19,
    we can exercise a wide variety of data points. For example, a data point for the
    real Kuala Lumpur listing shown in figure 9.18 would look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'And if we get a prediction for this data point, the output looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This prediction seems to be on the low side. We can check the median price
    value from the input dataset to see how it compares with the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: So, the median price from the input dataset is RM 980,000, and the list price
    for the brand-new property is RM 450,000\. This one data point does not prove
    that the model is good—the dataset was collected four years prior to this new
    listing, and the list price for this new listing could be well below what the
    listing actually sells for. However, by trying out several data points, including
    some with extremes in an important feature (i.e., size, along with a brand-new
    real-world data point), we can get some assurance that the model’s performance
    on the test set is not a fluke. By exercising a real-world example of a real estate
    listing, we have proved that the pipeline in the model is capable of handling
    real-world data and that the model has not been trained on features that are not
    available at the point when we want to get a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: There is one complication with getting the prediction for the brand-new listing
    that goes back to the way that we created the `size_type_bin` column when we prepared
    the data. Recall that the values in this column are a combination of the `Size_type`
    value (`built-up` or `land` `area`) and the bin value for the area. We reasonably
    guess the `Size_type` value for the new listing, but getting the `bin` value requires
    some work. Now that we have seen a real-world value, one thing to consider is
    whether we need this combined feature. Perhaps we could refactor this feature
    so that it is only the `Size_type` value without the `bin` value for the area.
    After all, a signal for the area of the listing is already in the `Size` column.
    In the next chapter, we will revisit this problem when we go through the end-to-end
    process for a deep learning solution to a tabular data problem.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train a model effectively, it is necessary to clean up the dataset. In this
    chapter, we examined a range of processes used to clean up the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning algorithms cannot deal with missing values, so we must fill in
    values (such as the mean value for a continuous column) or eliminate records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strings that express numeric values (such as `24` `x` `12`) can be converted
    into numeric values using the built-in Python `eval()` function. By converting
    such strings into numeric values, we can extract useful information that can improve
    the performance of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Columns that contain multiple types of data (such as the original `Size` column)
    can be separated into columns with distinct categories of data. By separating
    such columns into distinct columns with just one kind of data, we can use each
    of the new columns as a feature to train the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By taking advantage of the postprocessing layers that are built into Keras,
    we can define a deep learning model that is simpler and easier to maintain than
    a Keras model, where the tabular data characteristics are coded from scratch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras callbacks allow us to control the training process and ensure that we
    don’t waste resources on training iterations when the model is no longer improving.
    Callbacks also ensure that the model we get at the end of the training run is
    the peak performance achieved during training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many of the best practices from classic machine learning also apply to deep
    learning, including regularization to avoid overfitting and normalization to adjust
    values in continuous columns to fall within consistent ranges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercising the trained model on a handful of real-world examples helps avoid
    data leakage and validate the model’s performance on the test dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
