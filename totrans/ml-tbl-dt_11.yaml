- en: 9 Deep learning best practices
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 深度学习最佳实践
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: An introduction to the Kuala Lumpur real estate dataset
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吉隆坡房地产数据集简介
- en: Processing the dataset
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理数据集
- en: Defining the deep learning model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义深度学习模型
- en: Training the deep learning model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练深度学习模型
- en: Exercising the deep learning model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 练习深度学习模型
- en: 'In chapter 8 we examined a set of stacks for doing deep learning with tabular
    data. In this chapter, we use one of these stacks, Keras, to explore some best
    practices for deep learning with tabular data, including how to prepare the data,
    how to design the model, and how to train the model. We introduce a new problem
    to demonstrate all these best practices: predicting whether real estate properties
    in Kuala Lumpur will have a price above or below the median price for the market.
    We selected this dataset because it is messier and more challenging to prepare
    than the Airbnb NYC dataset we have used so far. Consequently, we’ll be able to
    demonstrate a wider range of techniques for applying deep learning to tabular
    datasets.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 8 章中，我们考察了一系列用于表格数据深度学习的堆栈。在本章中，我们使用这些堆栈之一，Keras，来探索表格数据深度学习的最佳实践，包括如何准备数据、如何设计模型以及如何训练模型。我们引入了一个新问题来展示所有这些最佳实践：预测吉隆坡的房地产价格是否高于或低于市场中位数价格。我们选择这个数据集是因为它比我们迄今为止使用的
    Airbnb 纽约市数据集更复杂、更具挑战性。因此，我们将能够展示更广泛的将深度学习应用于表格数据集的技术。
- en: If you are new to training deep learning models, the examples in this chapter
    will help you learn some best practices. If you already have extensive experience
    with defining and training deep learning architectures, this chapter could be
    beneficial for you as a review of principles.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你刚开始训练深度学习模型，本章中的示例将帮助你学习一些最佳实践。如果你在定义和训练深度学习架构方面已有丰富的经验，那么本章作为原则的复习对你可能有益。
- en: 9.1 Introduction to the Kuala Lumpur real estate dataset
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 吉隆坡房地产数据集简介
- en: In this chapter, we will use the Kuala Lumpur real estate dataset to explain
    the best practices for deep learning with tabular data. This dataset consists
    of records that describe properties sold in Kuala Lumpur, the capital of Malaysia.
    Figure 9.1 presents a sample of the records in this dataset from the output of
    `df.head()`. The code illustrated in this chapter is at [https://mng.bz/yWQp](https://mng.bz/yWQp).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用吉隆坡房地产数据集来解释表格数据深度学习的最佳实践。该数据集包含描述在马来西亚首都吉隆坡出售的物业的记录。图 9.1 展示了使用 `df.head()`
    输出的数据集中记录的样本。本章中展示的代码可在 [https://mng.bz/yWQp](https://mng.bz/yWQp) 找到。
- en: '![](../Images/CH09_F01_Ryan2.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F01_Ryan2.png)'
- en: Figure 9.1 Sample of the Kuala Lumpur real estate dataset
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 马来西亚吉隆坡房地产数据集样本
- en: 'In the next section, we’ll go through the steps we need to take to clean up
    each of the columns in this dataset. To prepare for those descriptions, let’s
    first review what’s in each column of the dataset:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍我们需要采取的步骤来清理数据集中每一列。为了准备这些描述，让我们首先回顾数据集中每一列的内容：
- en: '`Location`—The neighborhood in which the property is located.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`位置`—该物业所在的街区。'
- en: '`Price`—The listed price for the property in Ringgit, including RM, the conventional
    symbol for Malaysian currency.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`价格`—该物业在林吉特（RM，马来西亚货币的传统符号）中的标价。'
- en: '`Rooms`—The number of rooms in the property. Values like “2 + 1” in this column
    mean the property has two bedrooms and one room that cannot be classified as a
    bedroom.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`房间数`—该物业的房间数量。此列中的值如“2 + 1”表示该物业有两个卧室和一个不能归类为卧室的房间。'
- en: '`Bathrooms`—The number of washrooms in the property.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`浴室数`—该物业中的洗手间数量。'
- en: '`Car Parks`—The number of parking spaces on the property.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`停车位`—该物业上的停车位数量。'
- en: '`Property Type`—The category of property, such as “Condominium,” “Serviced
    Residence,” etc.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`物业类型`—物业的分类，例如“公寓”、“服务式住宅”等。'
- en: '`Size`—The dimensions of the property. There are several aspects of the property
    that values in this column could refer to, including the overall land area or
    the built-up area within the property.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`面积`—该物业的尺寸。此列中的值可能指代物业的几个方面，包括整体土地面积或物业内的建筑面积。'
- en: '`Furnishing`—Whether the property is furnished or not.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`装修情况`—该物业是否装修。'
- en: One of the basic questions we need to answer about this dataset is what columns
    are continuous or categorical. By looking at figure 9.1, we can spot a subset
    of columns that contain numeric values. Let’s take a closer look at this subset
    of the dataset to see if we can determine which columns are continuous. Figure
    9.2 shows values from the subset of columns in the dataset that appear to contain
    numeric data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要回答关于这个数据集的基本问题之一是哪些列是连续的还是分类的。通过查看图9.1，我们可以发现包含数值值的列的子集。让我们仔细看看这个数据集的子集，看看我们是否能确定哪些列是连续的。图9.2显示了数据集中看起来包含数值数据的列的子集的值。
- en: '![](../Images/CH09_F02_Ryan2.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F02_Ryan2.png)'
- en: Figure 9.2 Subset of columns that look like they contain numeric data
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 看起来包含数值数据的列的子集
- en: 'We can validate which of these columns have numeric data by using the following
    command:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令验证这些列中哪些包含数值数据：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This command returns descriptive statistics for all numeric columns in the DataFrame,
    providing insights into the data distribution, central tendency, and spread within
    each numeric column. By examining the output of this command, you can identify
    which columns indeed contain numerical values. The output of this command is shown
    in figure 9.3.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令返回DataFrame中所有数值列的描述性统计信息，提供了关于每个数值列的数据分布、集中趋势和变异性的洞察。通过检查此命令的输出，您可以确定哪些列确实包含数值值。此命令的输出显示在图9.3中。
- en: '![](../Images/CH09_F03_Ryan2.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F03_Ryan2.png)'
- en: Figure 9.3 Output of `describe()` for this dataset
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 该数据集`describe()`的输出
- en: Figure 9.3 indicates that only `Bathrooms` and `Car Parks` are numeric columns
    and that `Price`, `Rooms`, and `Size` are not numeric columns even though they
    contain some data that looks numeric. In the next section, as part of processing
    the dataset, we will describe the steps to extract the numeric data from the `Price`,
    `Rooms`, and `Size` features and make it available to train a model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3表明只有`Bathrooms`和`Car Parks`是数值列，而`Price`、`Rooms`和`Size`不是数值列，尽管它们包含一些看起来是数值的数据。在下一节中，作为处理数据集的一部分，我们将描述从`Price`、`Rooms`和`Size`特征中提取数值数据并使其可用于训练模型的步骤。
- en: Another way of evaluating which columns are categorical or continuous is to
    count the number of unique values in each column. If a column contains a large
    number of unique values, that may be an indication that we should treat it as
    continuous, and if it contains a relatively small number of values, that may be
    an indication that we should treat it as categorical. In fact, features presenting
    few unique values are often considered categorical because they typically represent
    discrete categories or groups rather than continuous numerical measurements. This
    is not a hard and fast rule, as we shall see. The output of the `df.unique()`
    command gives the number of unique values in each column in the dataset.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 评估哪些列是分类的还是连续的另一种方法是计算每列的唯一值数量。如果一个列包含大量唯一值，这可能表明我们应该将其视为连续的，如果它包含相对较少的值，这可能表明我们应该将其视为分类的。实际上，具有少量唯一值的特征通常被认为是分类的，因为它们通常代表离散的分类或组，而不是连续的数值测量。但这并不是一个绝对规则，正如我们将看到的那样。`df.unique()`命令的输出给出了数据集中每列的唯一值数量。
- en: Listing 9.1 Getting the count of unique values in each column
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.1 获取每个列中唯一值的数量
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Returns the number of unique values in each column of the dataframe df
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ① 返回数据框df中每列的唯一值数量
- en: 'The output of the command in listing 9.1 looks like the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.1中命令的输出如下所示：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① The limited number of values in this column reinforces our intuition that
    this column is categorical.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这一列中有限数量的值强化了我们的直觉，即这一列是分类的。
- en: ② This column is continuous.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ② 这一列是连续的。
- en: ③ This column requires further investigation.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 这一列需要进一步调查。
- en: ④ This column is continuous.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 这一列是连续的。
- en: ⑤ This column is categorical.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 这一列是分类的。
- en: ⑥ This column is continuous, but it requires special treatment, as we will see
    later in this chapter.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 这一列是连续的，但需要特殊处理，正如我们将在本章后面看到的那样。
- en: ⑦ This column is categorical.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 这一列是分类的。
- en: 'To summarize what the output of the command in listing 9.1 tells us about the
    dataset:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结列表9.1中命令的输出关于数据集的信息：
- en: '*Columns that appear to be categorical*—`Location,` `Property` `Type,` `Furnishing.`'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*看似分类的列*——`Location,` `Property` `Type,` `Furnishing.`'
- en: '*Columns that appear to be continuous*—`Price,` `Bathrooms,` `Car` `Parks,`
    `Size.` Of these columns, `Size` also requires further investigation.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*看起来是连续的列*——`价格`、`浴室`、`停车场`、`大小`。在这些列中，`大小`也需要进一步调查。'
- en: '*Columns that need further investigation to determine whether they should be
    treated as continuous or categorical*—`Rooms.`'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*需要进一步调查以确定是否应将其视为连续或分类的列*——`房间数`。'
- en: 'Two of these columns require further investigation: `Rooms` and `Size.` In
    the following section on processing the dataset, we will dig deeper into these
    two columns to determine how to deal with them.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中两列需要进一步调查：“房间数”和“大小”。在下面的数据集处理部分，我们将深入调查这两列，以确定如何处理它们。
- en: Now that we have a sense of the columns in the dataset and what kind of information
    they offer, let’s explore some more aspects of the dataset. First, let’s check
    the dimensions of the dataset, as shown in the following listing.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对数据集中的列以及它们提供的信息有了大致的了解，让我们进一步探索数据集的一些其他方面。首先，让我们检查数据集的维度，如下面的列表所示。
- en: Listing 9.2 Code to check the dimensions of the dataset
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2检查数据集维度的代码
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Statement to get the dimensions of the input dataframe
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ① 获取输入数据框维度的语句
- en: ② Output of the statement
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ② 命令的输出
- en: Listing 9.2 shows that this dataset has over 53,000 rows and eight columns.
    In chapter 12, we will examine the relationship between the number of rows in
    a dataset, the nature of the columns in a dataset, and the applicability of a
    deep learning model to the data. For now, it is safe to say that while this dataset
    is on the small side, it is big enough for us to have a decent chance of training
    a deep learning model with it.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2显示了该数据集有超过53,000行和八个列。在第12章中，我们将检查数据集行数、数据集列的性质以及深度学习模型对数据的适用性之间的关系。目前，可以安全地说，虽然这个数据集相对较小，但它足够大，我们可以有机会用它来训练一个深度学习模型。
- en: The following listing contains the statements that list the number of missing
    values in each column of the dataset.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表包含了列出数据集中每个列缺失值数量的语句。
- en: Listing 9.3 Statements to list missing values in each column
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.3列出每个列缺失值的语句
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① The output of this statement is a count of the number of missing values by
    column.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ① 此语句的输出是按列计算的缺失值数量。
- en: 'The following is the output of the commands in listing 9.3:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的命令输出是列表9.3的结果：
- en: '[PRE5]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Location is the only column with no missing values.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ① “位置”是唯一没有缺失值的列。
- en: ② Car Parks is the column with the most missing values.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ② 缺失值最多的列是“停车场”列。
- en: 'From the output of the command in listing 9.3, we can see that all but one
    of the columns in this dataset have missing values. This is an early sign of some
    of the problems we will need to correct to get this dataset ready to train with
    a deep learning model. In comparison, the Airbnb NYC dataset only had missing
    values in four columns:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 从9.3列表中的命令输出中，我们可以看到这个数据集中除了一个列之外的所有列都有缺失值。这是我们需要纠正的一些问题的早期迹象，以便使这个数据集准备好用深度学习模型进行训练。相比之下，Airbnb纽约数据集只有四个列有缺失值：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① Missing values in the name column
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ① “name”列中的缺失值
- en: ② Missing values in the host_name column
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ② “host_name”列中的缺失值
- en: ③ Missing values in the last_review column
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ③ “last_review”列中的缺失值
- en: ④ Missing values in the reviews_per_month column
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ④ “reviews_per_month”列中的缺失值
- en: As shown here, almost every column in the Kuala Lumpur dataset is missing values,
    which warns us that, as often happens with real-world data, we will have to perform
    a lot of work to clean up this dataset before we can start using it with a model.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如此所示，吉隆坡数据集几乎每个列都有缺失值，这警告我们，正如现实世界数据经常发生的那样，在我们可以开始使用模型之前，我们将不得不进行大量工作来清理这个数据集。
- en: In this section, we have taken a first look at the Kuala Lumpur real estate
    dataset. In the next section, we will review the process of preparing this dataset
    to train a deep learning model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首次查看了吉隆坡房地产数据集。在下一节中，我们将回顾准备此数据集以训练深度学习模型的过程。
- en: 9.2 Processing the dataset
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 处理数据集
- en: Now that we’ve looked at the Kuala Lumpur real estate dataset and seen that
    it has a large number of missing values, we have some idea that it will require
    a good deal of processing before we can use it to train a model. In this section,
    we will look at the features of the dataset one by one to describe the cleanup
    required.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经查看了吉隆坡房地产数据集，并看到它有大量的缺失值，我们有一些想法，在我们可以用它来训练模型之前，它将需要大量的处理。在本节中，我们将逐个查看数据集的特征，以描述所需的清理工作。
- en: At this point, we haven’t decided yet if we are going to use a subset or the
    full set of columns. Initially, our approach defaults to utilizing all available
    features. As we progress and examine the model’s performance and behavior, we
    may decide to exclude some features, for example, because these features have
    undetected invalid values that effect the model’s performance. It is not just
    because we cannot anticipate what features will work and what won’t that we strive
    to obtain the full set of clean features available to the model for training.
    It is also a way to get the most out of data, as we will learn more about the
    dataset through processing it entirely and rendering it reusable for other projects
    as well. This comprehensive approach ensures better results than we would have
    obtained if we only cleaned up the features that we ultimately use to train the
    model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们还没有决定是使用子集还是完整集的列。最初，我们的方法默认使用所有可用特征。随着我们前进并检查模型的性能和行为，我们可能会决定排除一些特征，例如，因为这些特征有未检测到的无效值，这影响了模型的性能。我们努力获得模型训练时可用的一整套干净特征，不仅仅是因为我们无法预测哪些特征会起作用，哪些不会。这也是充分利用数据的一种方式，因为我们将在完全处理数据集的过程中了解更多关于数据集的信息，并将其用于其他项目，使其可重用。这种全面的方法确保了比我们仅清理最终用于训练模型的特征所能获得的结果更好的结果。
- en: The code highlighted in this section is available at [https://mng.bz/MDBQ](https://mng.bz/MDBQ)
    and the config file is at [https://mng.bz/av1j](https://mng.bz/av1j).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中突出显示的代码可在[https://mng.bz/MDBQ](https://mng.bz/MDBQ)找到，配置文件可在[https://mng.bz/av1j](https://mng.bz/av1j)找到。
- en: 'We will begin by addressing columns that only involve handling missing values:
    `Bathrooms,` `Car` `Parks,` `Furnishing,` `Property Type,` `Location`. Next, we
    will describe the process for the columns that require more cleanup than simply
    dealing with missing information: `Price`, `Rooms`, and `Size`.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先解决只涉及处理缺失值的列：`浴室`、`车位`、`装修`、`物业类型`、`位置`。接下来，我们将描述需要比简单地处理缺失信息更多的清理过程的列：`价格`、`房间数`和`面积`。
- en: 9.2.1 Processing Bathrooms, Car Parks, Furnishing, Property Type, and Location
    columns
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 处理浴室、车位、装修、物业类型和位置列
- en: For a subset of the columns in the dataset (`Bathrooms,` `Car` `Parks,` `Furnishing,
    Property` `Type,` `Location`), we can accomplish an effective cleanup by simply
    dealing with missing values. The config file contains default values to replace
    missing values for these columns that we determined based on their characteristics
    and domain knowledge.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据集中的一部分列（`浴室`、`车位`、`装修`、`物业类型`、`位置`），我们可以通过简单地处理缺失值来实现有效的清理。配置文件包含默认值，用于替换这些列的缺失值，这些默认值是基于它们的特性和领域知识确定的。
- en: Listing 9.4 Defining the default replacement values for missing values
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.4 定义缺失值的默认替换值
- en: '[PRE7]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① For the Bathrooms columns, sets the median value for the column as the default
    value
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ① 对于浴室列，将该列的中位数作为默认值
- en: ② For the Car Parks column, sets the default value to zero
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ② 对于车位列，将默认值设为零
- en: ③ For the categorical columns, sets a placeholder category as the default value
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 对于分类列，设置一个占位符类别作为默认值
- en: Listing 9.4 shows that we set missing values for the `Bathrooms` column in this
    dictionary to the median value for the column. For the `Car` `Parks` column, we
    set missing values to zero. The reason for the difference between these is due
    to the specific use case of real estate listings. Residential properties will
    rarely have no washrooms, so picking the median value as the default for `Bathrooms`
    makes sense. On the other hand, many properties will have no parking spaces. If
    a property does have a parking space, it is in the best interests of the seller
    and the seller’s agent to include this in the listing to ensure they get the best
    selling price for the property. Thus, when `Car` `Parks` is missing a value, it
    makes sense to assume that this property does not have any parking spaces, so
    we set missing values in this column to zero.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.4显示，我们在该字典中将`浴室`列的缺失值设置为列的中位数。对于`车位`列，我们将缺失值设为零。这些差异的原因是由于房地产列表的具体使用案例。住宅物业很少没有洗手间，因此将中位数作为`浴室`的默认值是有意义的。另一方面，许多物业可能没有停车位。如果一个物业确实有停车位，那么卖方和卖方代理的最佳利益是将其包含在列表中，以确保他们为该物业获得最佳售价。因此，当`车位`缺失值时，假设该物业没有停车位是有意义的，所以我们在这个列中将缺失值设为零。
- en: In this dictionary, we also have distinct placeholder category values for the
    categorical columns. The code for obtaining this simple cleanup is placed in the
    data cleanup notebook’s function `clean_up_misc_cols()`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.5 Function to replace general missing values
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Iterates through the columns that have simple data cleanup
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: ② Replaces missing values with the median for the column where specified
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: ③ For the other columns, replaces missing values in the column with the default
    value for that column
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: In the `clean_up_misc_cols()` function shown in listing 9.5, the dictionary
    defined in the config file, as shown in listing 9.4, is used to replace missing
    values in the columns that require a simple cleanup.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have described how the cleanup is done for the columns that only
    require missing values to be dealt with, the subsequent subsections in this section
    will describe the more intensive data operations that are required for the remaining
    three columns: `Price`, `Rooms`, and `Size`.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2 Processing the Price column
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we get into what needs to be cleaned up in the `Price` column, let’s
    review some examples of values in this column, as shown in figure 9.4.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F04_Ryan2.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 Examples of values in the Price column
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'The values in figure 9.4 show a few items that need to be dealt with in the
    `Price` column:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Values including the symbol “RM”, representing Ringgit, the Malaysian currency
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing values
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Values needing to be converted to float
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 9.6 presents the `clean_up_price_col()` function, which contains a code
    snippet to effectively clean up the `Price` column.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.6 Function to clean up the `Price` column
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Drops rows in the dataframe that are missing a value in the Price column
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: ② Removes the currency symbol
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: ③ Removes commas and convert the values to float
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: As shown in listing 9.6, the `clean_up_price_col()` function removes rows with
    missing `Price` values. The rationale for removing these rows (as opposed to replacing
    missing `Price` values with some placeholder) is that `Price` is the target for
    our model; hence it won’t work to keep rows where such a value is missing. The
    output of the `clean_up_price_col()` function is a dataframe with valid numeric
    values in all rows for the `Price` column.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.3 Processing the Rooms column
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we get into what needs to be cleaned up in the `Rooms` column, let’s
    review what some of the values in this column look like, as shown in figure 9.5.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F05_Ryan2.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 Examples of values in the `Rooms` column
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'The values in figure 9.5 present some examples of the problems that need to
    be dealt with in the `Price` column:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: Missing values.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Double-barrelled values that contain more than one constituent value. In the
    Kuala Lumpur real estate dataset, some such values include string expressions
    such as “4 + 1.” These values require parsing to extract usable values for the
    model training.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As for the missing values, we can opt to replace the `NaN` values with zero.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至于缺失值，我们可以选择用零替换 `NaN` 值。
- en: 'At this point, we have the urge to make a choice about how to deal with the
    `Rooms` column overall: should we treat it as a categorical column or a continuous
    column? To help us decide, let’s review the count of unique values in the `Rooms`
    column:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们有选择如何处理 `Rooms` 列的整体选择的冲动：我们应该将其视为分类列还是连续列？为了帮助我们做出决定，让我们回顾 `Rooms`
    列中唯一值的计数：
- en: '[PRE10]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: With around 40 values in the `Rooms` column, which is a not-too-large number
    of categories, we could convert it into a categorical column if we wished. Suppose
    we opt to treat it as a numeric column; let’s check the necessary steps to take.
    To begin with, let’s look at the first few unique values and their counts.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `Rooms` 列大约有40个值的情况下，这是一个不算太大的类别数量，如果我们愿意，可以将其转换为分类列。假设我们选择将其视为数值列；让我们检查需要采取的必要步骤。首先，让我们看看前几个唯一值及其计数。
- en: Listing 9.7 Count of the most common values in the `Rooms` column
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.7 `Rooms` 列中最常见值的计数
- en: '[PRE11]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Example of a value that is immediately convertible to a numeric value
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ① 一个可以立即转换为数值的值的示例
- en: ② Example of a value that can be turned into a numeric value by treating it
    as an equation
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ② 一个可以通过将其视为等式转换为数值的值的示例
- en: ③ Example of a value that is a string that is not convertible to a numeric value
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 一个无法转换为数值的字符串值的示例
- en: ④ Example of a value that could be converted to a numeric value with some extrapolation
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 一个可以通过一些外推转换为数值的值的示例
- en: 'If we want to treat `Rooms` as a continuous column, we can deal with the representative
    examples shown in listing 9.7 in the following ways:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想将 `Rooms` 视为连续列，我们可以按照列表9.7中显示的代表性示例进行处理：
- en: 'Values like `3` that can be converted directly to numeric: convert to numeric.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以直接转换为数值的值，如 `3`：转换为数值。
- en: Values like `3` `+` `1` that we can use the built-in `eval()` Python function
    to evaluate the string value as if it were an equation.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用内置的 `eval()` Python 函数将值 `3` `+` `1` 评估为等式。
- en: 'Values like `Studio`: replace them with a reasonable numeric value, such as
    1.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像这样的值 `Studio`：用合理的数值替换，例如1。
- en: Values like `6+` should be treated as if they were `6+1`. This is not a perfect
    approach—the dataset does not clarify whether `6+` is a short form of `6+1` or
    if it means “6 plus an unspecified number of additional rooms.”
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 像这样的值 `6+` 应该被视为 `6+1`。这不是一个完美的方法——数据集没有明确说明 `6+` 是 `6+1` 的缩写，还是表示“6加上一些未指定的额外房间数”。
- en: 'Note that the `treat_rooms_as_numeric` setting in the config file for data
    preparation controls whether `Rooms` is prepared as a continuous column or a categorical
    column. If you set this value to `True`, `Rooms` is prepared as a continuous column,
    and if you set it to `False` then `Rooms` is prepared as a categorical column.
    In addition to updating the data preparation config file, you also need to ensure
    that `Rooms` is in the appropriate list in the model training config file so that
    the model training notebook knows whether to treat `Rooms` as categorical or continuous,
    as shown in the following:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，数据准备配置文件中的 `treat_rooms_as_numeric` 设置控制 `Rooms` 是否作为连续列或分类列准备。如果您将此值设置为
    `True`，则 `Rooms` 将作为连续列准备；如果设置为 `False`，则 `Rooms` 将作为分类列准备。除了更新数据准备配置文件外，您还需要确保
    `Rooms` 在模型训练配置文件中的适当列表中，以便模型训练笔记本知道是否将 `Rooms` 视为分类或连续，如下所示：
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that we have looked at the transformations we would make to treat `Rooms`
    as a numeric column, we can look at the `clean_up_rooms_col()` function.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经查看了对 `Rooms` 作为数值列进行处理的转换，我们可以查看 `clean_up_rooms_col()` 函数。
- en: Listing 9.8 Count of the most common values in the `Rooms` column
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.8 `Rooms` 列中最常见值的计数
- en: '[PRE13]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① Checks the parameter to determine whether Rooms will be treated as a continuous
    column
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ① 检查参数以确定是否将 `Rooms` 视为连续列
- en: ② Deals with values like “6+”
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ② 处理像“6+”这样的值
- en: ③ If a value ends with +, adds 1 to the end of the string.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 如果一个值以加号结尾，则在字符串末尾加1。
- en: ④ If the value is Studio, replaces it with 1.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 如果值是工作室，则将其替换为1。
- en: ⑤ Example of a value that could be converted to a numeric value with some extrapolation
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 一个可以通过一些外推转换为数值的值的示例
- en: ⑥ Replaces strings that are valid equations with the numeric result of the equation
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 将有效的等式字符串替换为等式的数值结果
- en: ⑦ Converts all values in the column to numeric
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 将列中的所有值转换为数值
- en: ⑧ If any NaNs have been introduced in these transformations, replaces them with
    0.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 如果在这些转换中引入了任何NaN值，请用0替换它们。
- en: ⑨ If the column is being treated as categorical, replaces missing values with
    a placeholder value.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 如果该列被处理为分类，则用占位符值替换缺失值。
- en: Listing 9.8 shows that we need to perform many transformations if we want to
    treat the `Rooms` column as a continuous column. In particular, we need to replace
    one-off nonnumeric values (`Studio`, `20` `Above`) with our best guess of the
    corresponding numeric value, and we need to replace values that include `+` with
    the evaluation of the string as if it were an equation. For values that end with
    `+`, we assume it’s valid for the string to end with `+1` so that the value can
    be treated as an equation by the `eval()` function. We’ve made some assumptions
    about what values like `6+`, `Studio`, and `20 Above` mean.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.8显示，如果我们想将“房间”列作为连续列处理，我们需要执行许多转换。特别是，我们需要用我们最好的猜测值替换一次性非数值值（“工作室”，“20以上”），并且我们需要用字符串作为方程的评估替换包含“+”的值。对于以“+”结尾的值，我们假设字符串以“+1”结尾是有效的，这样值就可以通过`eval()`函数作为方程处理。我们对像“6+”，“工作室”和“20以上”这样的值的意义做了一些假设。
- en: In a real-world scenario, we might have access to a subject matter expert, or
    we may have to make similar guesses to see whether we can get a signal out of
    these values. Given the expected importance of the `Rooms` column (more rooms
    typically mean more surface area, which can often contribute to higher property
    values), the acid test will be when we train the model and compare the resulting
    performance based on treating this column differently, either as categorical or
    continuous. When we get to train the model later in the chapter, we will try it
    using both variations of the `Rooms` column to determine which one produces the
    best results.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的场景中，我们可能能够访问到主题专家，或者我们可能需要做出类似的猜测，以查看我们是否可以从这些值中获取信号。鉴于“房间”列预期的的重要性（通常房间越多意味着面积越大，这往往可以增加物业价值），真正的考验将是当我们训练模型时，根据将此列作为分类或连续处理的不同方式，比较产生的性能。当我们在本章的后面部分训练模型时，我们将尝试使用“房间”列的两种变体来确定哪一种产生最佳结果。
- en: 9.2.4 Processing the Size column
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.4 处理“大小”列
- en: Among all the potential features for the Kuala Lumpur real estate price prediction
    problem, the `Size` column is the most problematic. Before going into the details
    of how to clean up this specific column, let’s review some samples of values in
    the `Size` column, as shown in figure 9.6.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在吉隆坡房地产价格预测问题的所有潜在特征中，“大小”列是最有问题的。在详细说明如何清理这个特定列之前，让我们回顾一下“大小”列中的一些值样本，如图9.6所示。
- en: '![](../Images/CH09_F06_Ryan2.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F06_Ryan2.png)'
- en: Figure 9.6 Examples of values in the `Size` column
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 “大小”列中值的示例
- en: Along with some missing values, this column contains as a string the classification
    of the size type (`Built-up` or `Land` `area`) as well as the area of the property
    and the area metric (“sq. ft”). But that’s not all. As you can see in figure 9.7,
    there are entries in the `Size` column that express the area as length by width.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 除了缺失值外，该列还包含作为字符串的大小类型分类（“建成”或“土地面积”）以及物业面积和面积度量（“平方英尺”）。但这还不是全部。如图9.7所示，在“大小”列中，有一些条目用长度乘以宽度表示面积。
- en: '![](../Images/CH09_F07_Ryan2.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F07_Ryan2.png)'
- en: Figure 9.7 Examples of values in the `Size` column with area expressed in length
    by width
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 “大小”列中用长度乘以宽度表示面积值的示例
- en: There is still more to be done with the `Size` column. Figure 9.8 shows examples
    of values in the `Size` column where the area of the property is expressed in
    various equations.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: “大小”列还有更多的工作要做。图9.8显示了“大小”列中值的示例，其中物业面积用各种方程表示。
- en: '![](../Images/CH09_F08_Ryan2.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F08_Ryan2.png)'
- en: Figure 9.8 Examples of values in the `Size` column with area expressed in complex
    equations
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 “大小”列中用复杂方程表示面积值的示例
- en: 'So it appears that the `Size` column combines three or more different pieces
    of information for each entry:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，看起来“大小”列将每个条目中的三个或更多不同信息组合在一起：
- en: Size type (`Land area` or `Built` `up`).
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大小类型（“土地面积”或“建成”）。
- en: Area, which can be formatted as a numeric value (e.g., `6900`), length by width
    (e.g., `20` `×` `80`), or length by width with an equation in one or both dimensions
    (e.g., `10` `+` `24` `×` `80`). We are assuming that the property is rectangular.
    Note that this assumption needs to be validated by a real estate professional,
    and this is an essential point. To conduct a thorough analysis of a dataset, it’s
    crucial to have access to a subject matter expert who can verify assumptions.
    For instance, an expert could suggest using missing value replacements such as
    the median number of bathrooms, which is a good default value to replace missing
    values in the `Bathrooms` column, or 0, which is a good replacement for missing
    values in the `Car` `Parks` column. We have set up config files for the data preparation
    and model training notebooks to make it easy to make changes if our assumptions
    don’t match what the subject matter expert says. By putting parameters like this
    in config files, we can update the behavior of the system without touching the
    Python code and run experiments methodically.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面积可以格式化为数值（例如，`6900`），长度乘以宽度（例如，`20` `×` `80`），或者在一维或两维中包含方程的长度乘以宽度（例如，`10`
    `+` `24` `×` `80`）。我们假设该属性是矩形的。请注意，这个假设需要由房地产专业人士进行验证，这是一个关键点。为了对数据集进行彻底的分析，必须能够访问一个可以验证假设的主题专家。例如，专家可能会建议使用缺失值替换，如浴室数量的中位数，这是替换`Bathrooms`列缺失值的良好默认值，或者0，这是替换`Car
    Parks`列缺失值的良好替换。我们已经为数据准备和模型训练笔记本设置了配置文件，以便在假设与主题专家的说法不符时轻松进行更改。通过将这些参数放入配置文件中，我们可以在不接触Python代码的情况下更新系统的行为，并系统地运行实验。
- en: Area metric (e.g., `sq.` `ft.`).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面积度量（例如，`sq.` `ft.`）。
- en: You may be asking now how we were able to identify that the `Size` column contained
    all these anomalies and find a remedy. The answer is through trial and error.
    First, we separated the size type from the area and area metric. Next, we removed
    the area metric (since it is always the same). Then, we iterated through the remaining
    area values, removing or replacing characters until every area value could be
    applied to `eval()`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可能想知道我们是如何能够识别出`Size`列包含所有这些异常并找到补救措施的。答案是通过对错误进行尝试和修正。首先，我们将尺寸类型从面积和面积度量中分离出来。然后，我们移除了面积度量（因为它总是相同的）。接着，我们迭代剩余的面积值，移除或替换字符，直到每个面积值都可以应用于`eval()`。
- en: This meticulous iterative process is not uncommon for real-world datasets. We
    chose the Kuala Lumpur real estate dataset for this section of the book because
    it presents these kinds of real-world challenges, and working through them illustrates
    the approaches that need to be taken, some systematic and some tactical, to squeeze
    as much useful signal as possible out of a dataset.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这种细致的迭代过程在现实世界的数据集中并不少见。我们选择吉隆坡房地产数据集作为本书这一章节的内容，因为它展示了这些现实世界的挑战，通过解决这些挑战，说明了需要采取的方法，有些是系统的，有些是战术性的，以尽可能从数据集中提取有用的信号。
- en: We need to separate the three kinds of values in the `Size` column. We will
    discard the area metric because it is always a variation of “square feet” and
    create a new categorical column that combines the size type value with an identifier
    for the bin that the area value falls into. To do this, we will need to
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将`Size`列中的三种值分开。我们将丢弃面积度量，因为它总是“平方英尺”的变体，并创建一个新分类列，该列结合了尺寸类型值和面积值所属分箱的标识符。为此，我们需要
- en: Convert the area values to numeric values by discarding extraneous characters
    and using the `eval()` function to convert the string representation of equations
    into the numeric result of evaluating the equation. That is, 20 × 80 is replaced
    with 160.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过丢弃无关字符并使用`eval()`函数将方程的字符串表示转换为方程的数值结果，将面积值转换为数值。也就是说，20 × 80被替换为160。
- en: Get bins for the resulting numeric area values and add a new column to the dataset
    with the bin value for each row.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取结果数值面积值的分箱，并为数据集中的每一行添加一个包含分箱值的新的列。
- en: Create a new categorical column that combines size type and bin number.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个新的分类列，该列结合了尺寸类型和分箱编号。
- en: The following listing shows the `clean_up_size_col()` function, which implements
    the changes we have described so far in this section.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了`clean_up_size_col()`函数，该函数实现了本节中描述的更改。
- en: Listing 9.9 Code to clean up the `Size` column
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.9 清理`Size`列的代码
- en: '[PRE14]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① Removes rows that are missing Size values
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ① 移除缺失尺寸值的行
- en: ② Changes all values in the Size column to lowercase
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将`Size`列中的所有值转换为小写
- en: ③ Splits the Size column by moving the size type values into a new column”
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: ④ Replaces any remaining missing values with 0
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Removes any rows where the size column contains no digits
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Removes any rows where the size column cannot be converted to a numeric value
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Replaces characters that will cause problems treating multiplications correctly
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Removes remaining characters that will cause problems treating the remaining
    Size values as numeric
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Removes extraneous characters following spaces
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: ⑩ Evaluates the remaining Size values as equations
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: ⑪ Defines bins for the Size values
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: ⑫ Defines bin names
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: ⑬ Creates a new column that contains the bin value for the Size value in that
    row
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: ⑭ Creates a new categorical column that combines the Size type value and the
    Size_bin value
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: The `clean_up_size_col()` function shown in listing 9.9 comprises a series of
    transformations that allow the evaluation of the area information from the `Size`
    column as a numeric value. To do this, we need to remove the rows where the area
    cannot be interpreted. The area of the property is so fundamental to determining
    its pricing value that it does not make sense to train a model using examples
    from listings where it is impossible to extract the area information; hence, we
    simply drop those rows. Next, we need to clean up the remaining area values so
    that they can be evaluated as numeric values, either because they can be directly
    converted to numeric values or because they can be interpreted as equations that
    can be applied to the `eval()` function.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Given the significant influence of a listing’s area on its price, it’s worthwhile
    to invest a lot of effort into extracting the area information in any way we can.
    For this purpose, we create new categorical columns containing the bin number
    corresponding to the size of the listing. Utilizing this bin value makes it possible
    to evaluate the area portion of the `Size` column as a numeric value. In doing
    this, we need to remove rows where the area cannot be interpreted. The area of
    the property is so fundamental to its value that it does not make sense to train
    the model with the data from listings where it is impossible to extract the area
    information because it is concatenated with the size type (`built-up` or `land`
    `area`).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.9 displays examples of values in the revised `Size` column along with
    the new columns (`Size_type`, `Size_bin`, and `Size_type_bin`) created by the
    function in listing 9.9.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F09_Ryan2.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 Examples of values in the new columns generated from the `Size` column
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at each column in figure 9.9:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '`Size`—Replacing the original values in the `Size` column are single continuous
    values that correspond to the land area from the original `Size` column.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Size_type`—This new column contains the size type (`built-up` or `land` `area`)
    portion of the original `Size` column value.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Size_bin`—This new column contains the bin number that the `Size` column value
    belongs to. Notice that for the examples in figure 9.9, the row with the smallest
    `Size` value has the smallest bin number, and the rows with the largest `Size`
    values have the largest bin number.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Size_type_bin`—This new column contains a combination of the values in the
    other two new columns.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider a specific value from the original `Size` column and how it gets processed
    to create the values in the new `Size`, `Size_type`, `Size_bin`, and `Size_type_bin`
    columns, as shown in figure 9.10.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F10_Ryan2.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 Original `Size` column and values in the new columns
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'We started with a `Size` column that contained a jumble of two kinds of critical
    information jammed into a single column and that had numeric data (the area of
    the property), sometimes as a number and sometimes as an equation. After applying
    the cleanup steps, we have split the `Size` column into four columns: one continuous
    (`Size`) and three categorical (`Size_type`, `Size_bin`, and `Size_type_bin`)
    that we can choose from to train the model.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Defining the deep learning model
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will go through the code that defines the deep learning
    model for the Kuala Lumpur real estate price prediction model. First, we’ll compare
    the approach for defining the model used in this chapter, with Keras preprocessing
    layers, to the approach that we saw in chapter 3, which used custom layers. Then
    we’ll review in detail the code that makes up the model definition. Finally, we’ll
    wrap up this section by discussing the rationale for using Keras preprocessing
    layers as a best practice for deep learning with tabular data.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: The code in this section is available at [https://mng.bz/gaBe](https://mng.bz/gaBe),
    and the config file is at [https://mng.bz/ey19](https://mng.bz/ey19).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.1 Contrasting the custom layer and Keras preprocessing layer approaches
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Starting in chapter 3, we have examined the Keras-based solution for the Airbnb
    NYC price prediction problem. In that solution, we created a deep learning model
    and the associated pipelines “from scratch.” That is, we didn’t use any functions
    from TensorFlow or Keras specifically designed for tabular data. In the solution
    we propose for the Kuala Lumpur real estate problem, we are going to switch gears
    and take advantage of Keras preprocessing layers to make it easier to handle the
    tabular dataset. To provide some context on the differences between the two approaches,
    let’s compare:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '*Using custom layers*—This is the approach we used for the Airbnb NYC price
    prediction problem in chapter 3 and the baseline for comparison with other deep
    learning approaches in chapter 8.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Using Keras preprocessing layers*—This is the approach we will use for this
    chapter and the subsequent chapters in this book.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See table 9.1.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Table 9.1 Comparison of the Kuala Lumpur real solutions using custom classes
    and using Keras preprocessing layers
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Using custom layers | Using Keras preprocessing layers |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
- en: '| Pipeline | Custom pipeline classes based on Scikit-learn Pipeline class ([https://mng.bz/pKP5](https://mng.bz/pKP5));
    requires classes to be defined in a separate file so they can be used at training
    and inference ([https://mng.bz/OBxK](https://mng.bz/OBxK)); complex code to define,
    train, invoke, and save the pipelines. | The standard, off-the-shelf Keras preprocessing
    layers, available at [https://mng.bz/YD1o](https://mng.bz/YD1o). The code to define,
    train, and invoke the pipeline is much simpler and more robust. |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '| Model definition | A complex set of layers, specified to work with categorical,
    continuous, and text inputs | A simple set of layers, capable of working with
    generic categorical and continuous inputs |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: Figure 9.11 shows the layers that make up the Kuala Lumpur real estate price
    prediction model using Keras preprocessing layers.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F11_Ryan2.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 Kuala Lumpur real estate price prediction model with Keras preprocessing
    layers
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.12 shows the layers that make up the Kuala Lumpur real estate price
    prediction model using custom layers; the code is available at [https://mng.bz/KGeP](https://mng.bz/KGeP).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F12_Ryan2.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 Kuala Lumpur real estate price prediction model with custom layers
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: The overall structure for the Keras model with custom layers in figure 9.12
    looks more complex than the structure in figure 9.11 for the model with Keras
    preprocessing layers, but there is a lot going on, so it’s not easy to see the
    details. Let’s zoom into the layers that just process the `Size` column to get
    a more specific idea of how these two architectures differ. Figure 9.13 shows
    the layers for the `Size` column in the model with Keras preprocessing layers.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F13_Ryan2.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 Layers for the `Size` column with Keras preprocessing layers
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: We can see that there are four layers between the input layer for `Size` and
    the final output layer. Compare this to the layers for the `Size` column for the
    Keras model with custom layers, as shown in figure 9.14.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F14_Ryan2.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 Layers for the `Size` column with custom layers
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: For the Keras model defined with custom layers, there are seven layers between
    the input layer for `Size` and the final output layer, compared to four layers
    for the model that uses Keras preprocessing layers. Because of the way that the
    layers are chained together in the model with customer layers, there is a series
    of individual concatenation operations layer by layer. In contrast, the model
    with Keras preprocessing layers has a single concatenation layer that pulls together
    all layers coming in from each input. This difference in the number of intermediate
    layers between the input of the `Size` column and the final layer reflects the
    overall additional complexity of the Keras model with custom layers compared to
    the model with Keras preprocessing layers.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.2 Examining the code for model definition using Keras preprocessing layers
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, the code for defining and training the model using Keras preprocessing
    is simpler and more streamlined than the deep learning code you read about in
    previous chapters that used custom layers and Scikit-learn based pipelines. However,
    a small price is to be paid in exchange for the increased simplicity of a model
    based on Keras preprocessing layers. To avoid getting an error when saving the
    model using `model.save()` and in the model saving callback, we need to ensure
    that all column names are lowercase and contain no spaces (this can be achieved
    using the `snake_case` naming convention instead) for the model using Keras preprocessing
    layers. The code in the following listing automatically does this.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.10 Lowercase column names and replacing spaces
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① For columns in the input dataframe, replaces spaces in column names with underscores
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: ② Lowercase column names in the input dataframe
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: ③ For the list of categorical column names, replaces spaces with underscores
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: ④ For the list of continuous column names, replaces spaces with underscores
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Lowercases the list of categorical column names
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Lowercases the list of continuous column names
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: The code shown in listing 9.10 replaces spaces in the column names in the input
    DataFrame and the lists of categorical and continuous column names. For example,
    the variable name `Car` `Parks` becomes `Car_Parks`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: The following listing presents the definition of the `df_to_dataset` function,
    which creates an input pipeline for the model that uses Keras preprocessing layers.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.11 Function to create an input pipeline
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ① Gets the target column from the local copy of the dataframe
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a new dictionary df with the same keys and values but with a new axis
    added to it
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a TensorFlow Dataset ds using the from_tensor_slices method
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: ④ Shuffles the elements of the dataset to avoid overfitting if the data has
    some intrinsic sorting
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Groups the elements of the dataset into batches of size batch_size
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Applies prefetch() to the dataset
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'The `df_to_dataset` function shown in listing 9.11, which is taken directly
    from [https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers),
    will be applied to the training, validation, and test datasets to convert them
    into `tf.data.Dataset` objects, then shuffle and batch the datasets. Note that
    the definition of the dataset `ds` takes two arguments: `dict(df)`, a dictionary
    version of the input dataframe, and `labels`, the target values from the input
    dataframe. Also, note that applying `prefetch()` to the dataset allows the dataset
    to be processed more efficiently by overlaying the preprocessing and model execution
    of one batch while the next batch is being loaded.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: The following listing defines the `get_normalization_layer()` function, which
    defines a normalization layer for a given feature.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.12 Creating normalization layers for continuous columns
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ① Defines a normalization object
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a dataset from the input dataset with only the input feature
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: ③ Trains the normalizer using the specified input feature
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: The `get_normalization_layer()` function defined in listing 9.12, which is taken
    directly from [https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers),
    will be applied to all the continuous columns that we want to use to train the
    model. This function scales input values with a distribution centered around 0
    with a standard deviation of 1\. For details about the normalization object defined
    in this function, see [https://mng.bz/9YDx](https://mng.bz/9YDx). Note that for
    gradient boosting solutions, normalization would not be needed.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: The following listing presents the definition of the `get_category_encoding_layer()`
    function, which specifies an encoding layer for a given categorical column.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.13 Creating encoding layers for categorical columns
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ① Creates layers for the column depending on whether or not it’s a string column
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a dataset from the input dataset with only the input feature
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: ③ Learns the set of possible values for the column and assigns them a fixed
    numeric index
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: ④ Encodes the numeric indices
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Applies multi-hot encoding to the indices
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: The `get_category_encoding_layer()` function defined in listing 9.13, which
    is taken directly from [https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers](https://www.tensorflow.org/tutorials/structured_data/preprocessing_layers),
    will be applied to all the categorical columns that we want to use to train the
    model. In this function, if a column is a string column, a layer is generated
    that turns strings into numeric indices; otherwise, a layer is created that turns
    integer values into numeric indices.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows the code to apply the `df_to_dataset()` function
    to the training, validation, and testing datasets.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.14 Applying `df_to_dataset()` to train, validate, and test datasets
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ① Generates training dataset
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: ② Generates validation dataset
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: ③ Generates test dataset
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: Once the code in listing 9.14 is applied, we will obtain the datasets and be
    ready to start the training process.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows the code to apply the `df_to_dataset()` function
    to the training, validation, and testing datasets.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.15 Defining layers for continuous and categorical columns
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ① List for input features
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: ② List for encoded features
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a normalization layer for each continuous column
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: ④ Creates an encoding layer for each categorical column
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code in listing 9.15, for each continuous column, `get_normalization_layer()`
    defines a normalization layer for the column. The new layer is added to the `encoded_features`
    list, and the column name is appended to the `all_inputs` list. For each categorical
    column, `get_category_encoding_layer()` defines an encoding layer for the column.
    The new layer is added to the `encoded_features` list, and the column name is
    appended to the `all_inputs` list. Once the code in listing 9.15 is run, the `all_features`
    list contains the following values:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'If we look at the settings in the config file that specify the continuous and
    categorical columns used to train the model, we see that they match the layers
    specified in the `all_features` list:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Now that we have defined the layers that correspond with the columns of the
    input dataset, we can define the model. The following listing shows the code that
    defines the model.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.16 Code to define the model
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ① Concatenates the features from the encoded_features list created in the code
    in listing 9.15
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: ② Adds a dense layer to the model. It corresponds with layer 2 in figure 9.15.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: ③ Adds a dropout layer to the model. It corresponds with layer 3 in figure 9.15.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: ④ Adds a dense layer to the model. It corresponds with layer 4 in figure 9.15.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.16 includes concatenating the features from the `encoded_features`
    list with the `concatenate()` function. It corresponds with layer 1 in figure
    9.15.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Once the code in listing 9.16 is applied, the set of layers for the model is
    defined. As shown in figure 9.15, every categorical column has an input layer,
    a `StringLookup` layer, and a `CategoryEncoding` layer, and every continuous column
    has an input layer and a normalization layer.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F15_Ryan2.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 Diagram of the layers in the model
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9.15 shows the layers rendered in the default vertical arrangement.
    To get a horizontal arrangement, we can use the `rankdir=''LR''` parameter in
    the `plot_model` function:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In addition, note that the architecture shown in figure 9.15 is simple enough
    for the modest number of features in the Kuala Lumpur dataset. The architecture
    will get complicated if you are working with a dataset with a large number of
    features. In addition, the approach described in this chapter for categorical
    columns depends on one-hot encoding. This approach works out for the Kuala Lumpur
    real estate dataset since the maximum number of unique values in any categorical
    column is just a bit over 100:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: However, if we had a dataset with hundreds or thousands of values in some of
    its categorical columns, using one-hot encoding could cause memory problems. In
    that case, we may have to consider using embeddings for categorical columns, as
    we did for the categorical columns in the Keras model with custom layers.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have reviewed the code to define the model. In the next
    section, we will go over the code to train the model.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Training the deep learning model
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we examined the code to define the model and examined
    how the layers in the model were built up based on the input columns. In this
    section, we’ll describe the process of training the model that we defined in the
    previous section.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: The code in this section is available at [https://mng.bz/gaBe](https://mng.bz/gaBe)
    and the config file at [https://mng.bz/ey19](https://mng.bz/ey19). The following
    listing shows the code necessary to compile and train the model.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.17 Code to compile and train the model
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ① Compiles the model defined in listing 9.16 using the hyperparameters from
    the config file
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates the list of callbacks and uses it in the call to fit to train the
    model
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: ③ Calls fit to train the model without the callback list
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.17 shows that the hyperparameter values are from the config file.
    By defining the hyperparameters in the config file, we can adjust them and rerun
    the model training notebook without touching the Python code. Maintaining the
    config values in a file separate from the Python code reduces the chance of regressions
    caused by touching the code and makes it easier to track the results of experiments.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Note also that we can call `fit()` to train the model with callbacks. The code
    in listing 9.17 shows that we can call `fit()` using a set of callbacks if we
    want to control the training process or without callbacks to let the training
    process run through all the specified epochs without interruption. Using callbacks
    to control the training process is a best practice for deep learning models with
    Keras because it allows us to use the resources to train the model more efficiently.
    Instead of running through all the specified epochs and getting the performance
    of the model from whatever it happened to be in the last epoch, we can use callbacks
    to stop the training process if it stops improving for a given number of epochs.
    We can ensure that the outcome of the training process is optimal for the whole
    training process.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Now that the model has been trained, the following listing presents how to perform
    a quick evaluation of the trained model.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.18 Code to get a quick evaluation of the trained model
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ① Uses the evaluate() function to get the loss and accuracy for the trained
    model with the test dataset
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: ② Tests loss for this training run
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: ③ Tests accuracy for this training run
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: The output from running listing 9.18 presents decent results on the test set
    using the trained model.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.1 Cross-validation in the training process
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In chapter 4 we highlighted cross-validation (that is, segmenting the dataset
    and repeatedly training the model on different subsets of the data with different
    holdouts to validate the model) as a best practice for classic machine learning
    approaches. Do we need to worry about cross-validation for deep learning with
    tabular data? The short answer is “no.” In Keras, by default, when we do repeated
    training runs with proportions of the dataset specified to use for training, validation,
    and testing, the subsets of the dataset that get put in each category are randomized,
    so we will get the benefits of cross-validation with Keras naturally if we do
    repeated training runs ([https://mng.bz/jpPz](https://mng.bz/jpPz)).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.2 Regularization in the training process
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another technique we highlighted in chapter 4 was regularization (that is, preventing
    overfitting by reducing the complexity of the model to improve its generalization
    performance). Do we need to be concerned the same about overfitting using deep
    learning as we do with classic machine learning? The answer is “absolutely,” and
    you can see in figure 9.16 a subset of the diagram for the model that highlights
    the part of the model that is specifically concerned with avoiding overfitting,
    the dropout layer. The dropout layer randomly sets inputs to 0 to reduce overfitting
    ([https://mng.bz/W2z4](https://mng.bz/W2z4)).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F16_Ryan2.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
- en: Figure 9.16 Regularization in the deep learning model
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 9.4.3 Normalization in the training process
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Normalization refers to adjusting continuous values across features so that
    their values fall into a consistent range. In chapter 4, we covered using this
    technique (also known as standardization) in classic machine learning. In listing
    9.12, we showed the Kuala Lumpur solution code that uses normalization layers.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: There are examples of continuous columns in the Kuala Lumpur dataset whose values
    have significantly different ranges. For example, in the Kuala Lumpur dataset,
    the `Bathrooms` feature ranges between 0 and 20 while `Size` ranges from 0 to
    11 million. Leaving the continuous columns with such disparate ranges can make
    the training process less efficient. Figure 9.17 shows the normalization layers
    that make the ranges of values for the continuous features fall within more consistent
    ranges.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F17_Ryan2.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: Figure 9.17 Normalization in the deep learning model
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: The normalization layers adjust the values in each of the continuous columns
    into a distribution centered around 0 with a standard deviation of 1 ([https://mng.bz/0QKp](https://mng.bz/0QKp)).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we have reviewed the code for training the Kuala Lumpur real
    estate price prediction model, along with a set of best practices demonstrated
    in the model: regularization to avoid overfitting, and normalization to bring
    the values of continuous columns into consistent ranges. In the next section,
    we will put all our work together by exercising the trained deep learning model
    with brand-new data points.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 9.5 Exercising the deep learning model
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far in this chapter, we have prepared the Kuala Lumpur real estate dataset,
    defined a deep learning model using Keras preprocessing layers, and trained the
    model. In this section, we’ll exercise the trained model.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: The code in this section is available at [https://mng.bz/gaBe](https://mng.bz/gaBe)
    and the config file at [https://mng.bz/ey19](https://mng.bz/ey19).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.1 Rationale for exercising the trained model on some new data points
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The process of deploying a model is complex, and we will examine it in more
    depth in chapters 10 and 11\. It can be a big investment to complete the initial
    deployment, so we want to ensure that deployment goes smoothly and we don’t have
    to backtrack to earlier points in the process if we don’t have to. If we exercise
    the trained model on a few real-world data points as soon as possible, we can
    save more problems from appearing later in the process. There are two key benefits
    to exercising the trained model on new data points as soon as possible:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: It’s an easy way of detecting data leakage (see [https://mng.bz/zZXw](https://mng.bz/zZXw)).
    It can take months between the start of a data science project and the deployment
    of the trained model, particularly if you are dealing with the demands of deep
    learning. During that time, it’s possible to lose track of exactly what data will
    be available to the trained model once it has been deployed. Exercising the trained
    model with a few data points is an easy way of uncovering potential data leakage
    before the project has gone too far. The reason for this is that exercising the
    model will force you to think about what a new data point looks like.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides a way to validate the performance of the model on the test dataset.
    For example, if you exercise the trained model with some brand-new data points
    from the Kuala Lumpur real estate market, you get a sense of whether the performance
    of the model on the test dataset is consistent with its performance with brand-new
    data points.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To illustrate how exercising the trained model on a few new, real-world examples
    can help to prevent data leakage, let’s look at how we would exercise the trained
    model to predict whether a brand-new listing would have a selling price above
    or below the median for Kuala Lumpur. For the Kuala Lumpur real estate dataset,
    suppose there was a column called `Weeks` `on` `the` `market` that had the number
    of weeks the property had been on the market before it was sold. If we included
    such a column in the training of the model, what would happen when we tried to
    exercise the trained model with a handful of new data points? Figure 9.18 illustrates
    the problem we would run into.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F18_Ryan2.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
- en: Figure 9.18 Validating the features used to train the model with a new data
    point
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: On the left side of figure 9.18, we have the list of features used to train
    the model as they would be specified in the config file, including the `Weeks`
    `on` `the` `market` feature. On the right is an extract from a real Kuala Lumpur
    real estate listing. If we want to be able to use the trained model to get a price
    prediction for actual real estate listings, then we should be able to find values
    for all the features on the left in the listing on the right.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown by the numbers in figure 9.18, for most of the features we will use
    to train the model, there is a value in the real Kuala Lumpur real estate listing.
    However, there are two features where we can’t get the values from the real estate
    listing:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '`Car` `Parks`—This particular listing mentions covered parking for the building
    overall but doesn’t include any information about parking spaces for this particular
    unit. For the sake of a quick test, we can assume that the value of `Car` `Parks`
    for this listing should be 0.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Weeks` `on` `the` `market`—The listing does not include the number of weeks
    it was on the market before it was sold because we won’t know this value until
    after the listing has sold. We cannot include `Weeks` `on` `the` `market` in the
    training process.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have explained why it’s important to exercise the trained model
    on a handful of real data points, we will review the code for exercising the model
    in the next subsection.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 9.5.2 Exercising the trained model on some new data points
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have established the benefits of exercising the trained model on
    new data points, let’s review the code for doing this.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: The following listing shows the code to save the model to the file system, define
    a new data point by specifying values for all the features used to train and exercise
    the model on the new data point.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.19 Code to exercise the trained model on a new data point
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ① Saves the trained model to the file system
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: ② Reloads the saved model
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: ③ Defines a new data point point
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: ④ Puts the new data point into the format expected by the model
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Gets a prediction for the new data point
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the code shown in listing 9.19, we get an output like the following:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The model seems certain that this listing will be over the median price. Can
    you think of a reason why? It could be because the size shown in this property
    is huge—over 16,000 square feet. Suppose we rerun this code with a size value
    of 1,500 and leave all the other values the same. We get output as follows:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This seems roughly in line with our expectations. When we reduce the size to
    a more reasonable value and leave all the other feature values the same, we get
    a prediction that is roughly what we expect. With the code shown in listing 9.19,
    we can exercise a wide variety of data points. For example, a data point for the
    real Kuala Lumpur listing shown in figure 9.18 would look something like the following:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'And if we get a prediction for this data point, the output looks like the following:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This prediction seems to be on the low side. We can check the median price
    value from the input dataset to see how it compares with the prediction:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: So, the median price from the input dataset is RM 980,000, and the list price
    for the brand-new property is RM 450,000\. This one data point does not prove
    that the model is good—the dataset was collected four years prior to this new
    listing, and the list price for this new listing could be well below what the
    listing actually sells for. However, by trying out several data points, including
    some with extremes in an important feature (i.e., size, along with a brand-new
    real-world data point), we can get some assurance that the model’s performance
    on the test set is not a fluke. By exercising a real-world example of a real estate
    listing, we have proved that the pipeline in the model is capable of handling
    real-world data and that the model has not been trained on features that are not
    available at the point when we want to get a prediction.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: There is one complication with getting the prediction for the brand-new listing
    that goes back to the way that we created the `size_type_bin` column when we prepared
    the data. Recall that the values in this column are a combination of the `Size_type`
    value (`built-up` or `land` `area`) and the bin value for the area. We reasonably
    guess the `Size_type` value for the new listing, but getting the `bin` value requires
    some work. Now that we have seen a real-world value, one thing to consider is
    whether we need this combined feature. Perhaps we could refactor this feature
    so that it is only the `Size_type` value without the `bin` value for the area.
    After all, a signal for the area of the listing is already in the `Size` column.
    In the next chapter, we will revisit this problem when we go through the end-to-end
    process for a deep learning solution to a tabular data problem.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-361
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train a model effectively, it is necessary to clean up the dataset. In this
    chapter, we examined a range of processes used to clean up the dataset.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning algorithms cannot deal with missing values, so we must fill in
    values (such as the mean value for a continuous column) or eliminate records.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strings that express numeric values (such as `24` `x` `12`) can be converted
    into numeric values using the built-in Python `eval()` function. By converting
    such strings into numeric values, we can extract useful information that can improve
    the performance of the model.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Columns that contain multiple types of data (such as the original `Size` column)
    can be separated into columns with distinct categories of data. By separating
    such columns into distinct columns with just one kind of data, we can use each
    of the new columns as a feature to train the model.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By taking advantage of the postprocessing layers that are built into Keras,
    we can define a deep learning model that is simpler and easier to maintain than
    a Keras model, where the tabular data characteristics are coded from scratch.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras callbacks allow us to control the training process and ensure that we
    don’t waste resources on training iterations when the model is no longer improving.
    Callbacks also ensure that the model we get at the end of the training run is
    the peak performance achieved during training.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many of the best practices from classic machine learning also apply to deep
    learning, including regularization to avoid overfitting and normalization to adjust
    values in continuous columns to fall within consistent ranges.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exercising the trained model on a handful of real-world examples helps avoid
    data leakage and validate the model’s performance on the test dataset.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
