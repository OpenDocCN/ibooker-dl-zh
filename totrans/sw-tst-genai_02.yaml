- en: 2 Large language models and prompt engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 大型语言模型与提示工程
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: The fundamentals of how large language models work
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型语言模型工作原理的基础
- en: The risks of using large language models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用大型语言模型的风险
- en: A definition of prompt engineering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程的一个定义
- en: Experimenting with prompt engineering to return various outputs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过提示工程进行实验以返回各种输出
- en: How to solve problems using prompt engineering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用提示工程解决问题
- en: In the previous chapter, we learned that it’s important to take time and familiarize
    ourselves with new tools, and it’s the very same mindset we’ll be adopting in
    this chapter. Throughout this book, we’ll be exploring how to use generative AI
    tools such as Open AI’s ChatGPT and GitHub Copilot, which are built on large language
    models, or LLMs. There are many ways in which AI can be employed in testing, but
    what makes LLMs so interesting is their adaptability to different situations—hence,
    their rise in popularity. So, before we look at how we can incorporate LLM tools
    into our everyday testing, let’s first learn a bit about what LLMs are, how they
    work, and how to get the most out of them by learning about the concept of prompt
    engineering.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解到花时间熟悉新工具是很重要的，而这就是我们在本章中将要采取的心态。在整个书中，我们将探讨如何使用基于大型语言模型（LLM）的生成式AI工具，如OpenAI的ChatGPT和GitHub
    Copilot。AI在测试中有很多应用方式，但LLM之所以有趣，在于它们对不同情况的适应性——因此，它们越来越受欢迎。所以，在我们探讨如何将LLM工具融入日常测试之前，让我们先了解一下LLM是什么，它们是如何工作的，以及如何通过学习提示工程的概念来最大限度地发挥它们的作用。
- en: 'What has made LLMs such as ChatGPT dominate tech headlines throughout 2023?
    Consider this sample interaction with ChatGPT that I had:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 什么使得像ChatGPT这样的LLM在2023年一直占据科技头条？考虑以下我与ChatGPT的样本交互：
- en: '|'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-MW.png)'
- en: '| Hello ChatGPT, can you introduce yourself briefly? |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 你好，ChatGPT，你能简单介绍一下你自己吗？ |'
- en: '|'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-openai.png)'
- en: '| Of course! I’m ChatGPT, an AI language model developed by OpenAI. I’m based
    on the GPT-3.5 architecture, which means I’m designed to understand and generate
    human-like text based on the prompts I receive. I’ve been trained on a diverse
    range of data sources, including books, articles, websites, and other text documents,
    up until September 2021\. I can assist you with a wide array of topics, including
    answering questions, providing explanations, creative writing, and much more.
    Just let me know how I can help you! |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 当然！我是ChatGPT，由OpenAI开发的AI语言模型。我基于GPT-3.5架构，这意味着我被设计成根据我收到的提示来理解和生成类似人类的文本。我在包括书籍、文章、网站和其他文本文档在内的多种数据源上接受了训练，直到2021年9月。我可以帮助您解决各种话题，包括回答问题、提供解释、创意写作等等。请告诉我您需要我如何帮助您！
    |'
- en: '|'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-MW.png)'
- en: '| Nice to meet you. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 很高兴见到你。 |'
- en: '|'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-openai.png)'
- en: '| Nice to meet you too! I’m here to assist you with any questions or tasks
    you have. How may I help you today? |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 很高兴见到你！我在这里帮助你解答任何问题或完成任何任务。今天我能帮您做什么？ |'
- en: '|'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-MW.png)'
- en: '| I’m just getting to know you. |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 我刚开始了解你。 |'
- en: '|'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-openai.png)'
- en: '| That’s perfectly fine! Feel free to ask me anything you’d like to know, whether
    it’s about my capabilities, how I work, or anything else you’re curious about.
    I’m here to provide information and help you out to the best of my abilities.
    Fire away with your questions! |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 没问题！请随意问我任何您想了解的问题，无论是关于我的能力、我的工作方式，还是您好奇的其他任何事情。我在这里提供信息，尽我所能帮助您。请随意提问！
    |'
- en: Based on this quick conversation, we can see that
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这次简短的对话，我们可以看到
- en: It is possible to interact with ChatGPT using natural language. No traditional
    programming experience was required to get results from ChatGPT in this example.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自然语言与ChatGPT进行交互是可能的。在这个例子中，不需要传统的编程经验就可以从ChatGPT获得结果。
- en: The output from ChatGPT is also in natural language. It’s easy to understand
    and react to.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGPT的输出也是自然语言。它易于理解和反应。
- en: Advocates of LLMs are celebrating that these types of AI tools have democratized
    the use of AI, allowing anyone to use it to get results. However, this democratization
    is a double-edged sword. The nature in which we interact with LLMs can give us
    the illusion that we’re talking with a machine that reasons in the same way we
    humans do. But making that assumption can affect our ability to get the most out
    of an LLM. So, to get the best results out of tools such as ChatGPT, we should
    know how they work (at least in general terms) to better understand how they can
    fit into our testing activities and how to extract the most value from them.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的支持者正在庆祝这些类型的AI工具使AI的使用民主化，允许任何人使用它来获得结果。然而，这种民主化是一把双刃剑。我们与LLMs互动的方式可能会让我们产生一种错觉，认为我们正在与一个以与我们人类相同的方式进行推理的机器交谈。但做出这种假设可能会影响我们充分利用LLM的能力。因此，为了从ChatGPT等工具中获得最佳结果，我们应该了解它们的工作原理（至少是一般术语），以便更好地理解它们如何融入我们的测试活动以及如何从中提取最大价值。
- en: 2.1 LLMs explained
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 LLMs explained
- en: How does a person with relatively little experience in building AI systems explain
    how a complex LLM system works? Fortunately, in the Computerphile video “AI Language
    Models & Transformers” ([www.youtube.com/watch?v=rURRYI66E54](https://www.youtube.com/watch?v=rURRYI66E54)),
    Rob Miles offers an example that can help us gain a fundamental grasp on what
    LLMs do. (I strongly recommend watching all his videos on AI.)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一个在构建AI系统方面经验相对较少的人如何解释一个复杂的LLM系统的工作原理？幸运的是，在Computerphile视频“AI语言模型与Transformer”([www.youtube.com/watch?v=rURRYI66E54](https://www.youtube.com/watch?v=rURRYI66E54))中，Rob
    Miles提供了一个可以帮助我们获得LLMs基本理解的例子。（我强烈推荐观看他关于AI的所有视频。）
- en: 'Take out your phone and open a messaging app, or any other app that makes your
    keyboard appear. Above the keyboard, you’ll likely see a range of suggested words
    to insert into your message. For example, my keyboard offers the following suggestions:
    *I,* *I am,* and *The.* Selecting one of these options, such as *I am,* causes
    the suggestions to update. For me, it offered the options *away,* *away for,*
    and *now.* Selecting the option *away* *for* once again updates the available
    options. So, how does the keyboard know which options to show?'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 拿出你的手机并打开一个消息应用，或者任何使键盘出现的其他应用。在键盘上方，你可能会看到一系列建议的单词，可以插入到你的消息中。例如，我的键盘提供了以下建议：*我*，*我是*和*那*。选择这些选项之一，例如*我是*，会导致建议更新。对我来说，它提供了以下选项：*离开*，*离开去*和*现在*。选择*离开去*选项再次更新了可用的选项。那么，键盘是如何知道显示哪些选项的呢？
- en: In your keyboard, there is an AI model that behaves in a manner resembling LLMs.
    This description is an oversimplification, but at its core, the keyboard on your
    phone is applying the same machine learning approach as an LLM by using probability.
    Language is a complex and fluid set of rules, meaning any attempt to codify relationships
    explicitly is almost impossible. So instead, a model is trained on massive data
    sets to implicitly learn the relationships in language and create a probability
    distribution that is used to predict what the next word might be. This can best
    be described by visualizing the options available from the keyboard example, as
    shown in figure 2.1
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的键盘上，有一个类似于LLMs的行为的AI模型。这种描述是一种过度简化，但核心在于，你手机上的键盘通过使用概率应用了与LLM相同的机器学习方法。语言是一套复杂且流动的规则，这意味着任何试图明确编码关系的尝试几乎都是不可能的。因此，模型在大量数据集上训练，以隐式学习语言中的关系，并创建一个概率分布，用于预测下一个单词可能是什么。这可以通过可视化键盘示例中可用的选项来最好地描述，如图2.1所示。
- en: '![](../../OEBPS/Images/CH02_F01_Winteringham2.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/CH02_F01_Winteringham2.png)'
- en: Figure 2.1 Probability distribution in action
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 概率分布的实际应用
- en: As we can see, when we select the term *I am,* the model in our keyboard has
    been trained to assign probabilities to a vast range of words. Some of these will
    have a high probability of coming after *I am*, such as *away*, and some will
    have a low probability, such as *sandalwood*. As mentioned before, these probabilities
    come from a model that has completed a training process, known as unsupervised
    learning, in which vast amounts of data have been sent to an algorithm for processing.
    It’s from that training process that a model is created with complex weights and
    balances that provide the model with its predictive abilities.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，当我们选择“我”这个词时，我们键盘中的模型已经被训练去为大量词汇分配概率。其中一些词汇有很高的概率会跟在“我”之后，比如“不在”，而有些词汇的概率则较低，比如“沉香木”。正如之前提到的，这些概率来自一个已经完成训练过程的模型，这个过程被称为无监督学习，其中大量数据被发送到算法进行处理。正是从这个训练过程中，一个具有复杂权重和平衡的模型被创建出来，为模型提供了预测能力。
- en: Supervised learning and unsupervised learning
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习
- en: When training AI, two of the more dominant techniques to use are supervised
    and unsupervised learning. Which learning approach is used will determine how
    data has been structured and sent to an algorithm. *Supervised* learning uses
    data that has been organized, labeled, and paired with an output. For example,
    a medical data set might contain labeled data that includes BMI, age, and sex,
    for example, which is paired with a labeled outcome about whether an individual
    suffered a specific illness—say, a heart attack or stroke. *Unsupervised* learning,
    conversely, uses data that isn’t labeled, and it has no output data. The idea
    is that when an algorithm is trained on this type of data, it learns the implicit
    patterns within the data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练AI时，使用较为普遍的技术是监督学习和无监督学习。所使用的哪种学习方法将决定数据是如何被结构化和发送到算法中的。“监督”学习使用已经组织、标记并配对输出的数据。例如，一个医疗数据集可能包含标记的数据，包括BMI、年龄和性别等，这些数据与标记的结果配对，比如一个人是否患有特定的疾病——比如说心脏病发作或中风。“无监督”学习则相反，使用未标记的数据，并且没有输出数据。其理念是，当算法在这种类型的数据上被训练时，它会学习数据中的隐含模式。
- en: Chances are good that if you play around with the predictive function on your
    keyboard, the output will differ from mine—even if we have the same phone and
    operating system. This is because once the model has been trained and is utilized
    in our phones, it’s still being fine-tuned by what we type into our phones. I
    travel for work, so I must let people know when I am away and when I’m available.
    (It is perhaps a damning indictment of my work–life balance!) So, words such as
    *I am* and *away* have an increased probability as they are words I use more regularly.
    This is known as Reinforcement Learning with Human Feedback, or RLHF.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能，如果你在你的键盘上玩弄预测功能，输出结果将与我不同——即使我们使用的是同一部手机和操作系统。这是因为一旦模型在我们的手机上被训练并投入使用，它仍然会根据我们输入手机的内容进行微调。我因工作需要出差，所以我必须让人们知道我何时不在，何时可用。（这或许是对我的工作与生活平衡的一种严厉批评！）因此，像“我”和“不在”这样的词出现的概率会增加，因为它们是我更经常使用的词。这被称为带有人类反馈的强化学习，或简称RLHF。
- en: Again, comparing predictive messaging on a phone to an LLM is an oversimplification,
    but the comparison holds true. LLMs also use unsupervised learning and RLHF. The
    difference, however, is that although an AI model on a phone can look at perhaps
    the last five words typed to predict the next, LLMs use cutting-edge techniques,
    such as
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，将手机上的预测消息与LLM（大型语言模型）进行比较是一种过度简化，但这个比较是成立的。LLM也使用无监督学习和RLHF。然而，区别在于，尽管手机上的AI模型可以查看可能最后输入的五個字来预测下一个字，但LLM使用最前沿的技术，例如
- en: Generative pretrained transformers (which is what makes the GPT abbreviation
    in ChatGPT)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成式预训练转换器（这是ChatGPT中GPT缩写的原因）
- en: Powerful hardware infrastructure using thousands of servers
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数千台服务器的强大硬件基础设施
- en: Training data on a scale that would dwarf what our humble keyboard model will
    have been trained on
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据量将远远超过我们谦逊的键盘模型所训练的数据量
- en: Do we need to know the intricacies of each of these points? Not really, but
    it helps us appreciate a key aspect of LLMs. The output of LLMs, no matter how
    powerful, is probabilistic. LLMs are not a repository of information—there is
    structured knowledge stored within them like we would see on the wider internet.
    This means that how it comes to conclusions differs from how we humans do (probability
    rather than experience), which is what makes them so powerful but also risky to
    use if we aren’t vigilant about how we use them.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是否需要了解这些点的每一个细节？实际上并不需要，但这有助于我们理解LLMs的关键方面。LLMs的输出，无论多么强大，都是概率性的。LLMs不是一个信息库——它们内部存储的结构化知识就像我们在更广泛的互联网上看到的那样。这意味着它得出结论的方式与我们人类不同（基于概率而非经验），这也是它们如此强大但如果我们不警惕地使用它们，使用起来也具有风险的原因。
- en: 2.2 Avoiding the risks of using LLMs
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 避免使用LLMs的风险
- en: Having an AI predict what word goes after another isn’t an easy task, and although
    current LLMs have seen an explosion in ability, there are risks we need to be
    aware of. Let’s take a look at a few of them.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让AI预测一个词之后应该跟什么词并不容易，尽管当前的LLMs在能力上有了爆炸性的增长，但我们仍需意识到存在的风险。让我们来看看其中的一些。
- en: 2.2.1 Hallucinations
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 幻觉
- en: 'The challenge with text prediction is to ensure that the output of an LLM makes
    sense and is rooted in reality. For example, back in chapter 1, when I asked ChatGPT
    to write an introduction to this book, it shared the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 文本预测的挑战在于确保LLMs的输出是有意义并且基于现实的。例如，在第一章中，当我要求ChatGPT为这本书写一个引言时，它分享了以下内容：
- en: '|'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-openai.png)'
- en: '| Regarding the book, “How AI can be used to help support various software
    testing activities,” I would recommend “AI-Driven Testing: Adding Intelligence
    to Your Software Testing Practice” by Julian Harty and Mahesh Sharma. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 关于这本书，“如何使用AI来帮助支持各种软件测试活动”，我推荐“AI-Driven Testing: Adding Intelligence to
    Your Software Testing Practice”由Julian Harty和Mahesh Sharma所著。 |'
- en: Initially, when LLMs were being developed, their output didn’t make much sense.
    The text would be readable, but it lacked structure or grammatical sense. If we
    read this example, it parses perfectly well, and it makes sense. However, as I
    mentioned, the book that ChatGPT describes doesn’t exist. In the context of an
    LLM, this is known as a *hallucination.* The LLM is able to output a clear statement
    in a way that grants it some authority, but what has been written is false.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs最初开发时，它们的输出并没有太多意义。文本是可读的，但缺乏结构或语法上的合理性。如果我们阅读这个例子，它解析得非常完美，并且有道理。然而，正如我提到的，ChatGPT所描述的那本书并不存在。在LLMs的语境中，这被称为*幻觉*。LLMs能够以一种赋予其一定权威的方式输出清晰的陈述，但所写的内容是错误的。
- en: 'LLM hallucinations can be caused by a range of factors, such as the quality
    of the data it was trained on, the degree to which a model has been trained on
    data (overfitting), or a model’s propensity for giving an answer regardless of
    whether it is correct. One of the challenges of working with LLMs is that they
    act like a black box. It’s difficult to monitor how an LLM reached a specific
    conclusion, which is compounded by its indeterminate nature. Just because I got
    an output that contained a hallucination doesn’t mean that others will do the
    same in the future. (This is where the RLHF helps combat hallucinations: we can
    inform the model whether its output is false, and it will learn from that.)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的幻觉可能由多种因素引起，例如训练数据的质量、模型在数据上的训练程度（过拟合），或者模型倾向于给出无论正确与否的答案。与LLMs一起工作的一个挑战是它们就像一个黑盒。很难监控LLMs如何得出一个特定的结论，这由于其不确定性而加剧。仅仅因为我得到了包含幻觉的输出，并不意味着其他人将来会做同样的事情。（这就是RLHF帮助对抗幻觉的地方：我们可以告知模型其输出是否错误，它会从中学到东西。）
- en: The risk of hallucinations means we must always maintain an element of skepticism
    when interpreting the output of an LLM. We need to be mindful that what is being
    returned from an LLM is predictive and not always correct. We can’t turn off our
    critical thinking just because a tool appears to be behaving in a way that mimics
    human behavior.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 幻觉的风险意味着我们在解释LLMs的输出时必须始终保持一定的怀疑态度。我们需要意识到从LLMs返回的内容是预测性的，并不总是正确的。我们不能因为一个工具看起来在以模仿人类行为的方式行事就关闭我们的批判性思维。
- en: 2.2.2 Data provenance
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 数据来源
- en: For most LLM users, it’s not just how the model works precisely that is a black
    box to us, but also the data it has been trained on. Since ChatGPT’s explosion
    in popularity, the conversation around data ownership and copyright has intensified.
    Companies such as X (formerly known as Twitter) and Reddit have accused OpenAI
    of stealing their data wholesale, and at the time of writing, a class action lawsuit
    against OpenAI has been filed by a collection of authors who accuse the company
    of breaching copyright law by training models on their works ([https://mng.bz/1aBZ](https://mng.bz/1aBZ)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数LLM用户来说，对我们来说是一个黑盒的不仅仅是模型的工作原理，还有它所训练的数据。自从ChatGPT的流行爆发以来，关于数据所有权和版权的讨论已经加剧。例如，X（以前称为Twitter）和Reddit等公司指责OpenAI大规模窃取他们的数据，在撰写本文时，一群作者已经对OpenAI提起集体诉讼，指控该公司通过在他们的作品上训练模型违反版权法([https://mng.bz/1aBZ](https://mng.bz/1aBZ))。
- en: The results from these debates are yet to be seen, but if we bring this topic
    back to the world of software development, we must be mindful of what material
    an LLM has been trained on. For example, ChatGPT, at one point, would return nonsensical
    responses when specific phrases were sent to it, all because it had been trained
    on data from the subreddit r/counting, which is full of data that is seemingly
    nonsensical itself. You can learn more about this weird behavior from Computerphile
    at [www.youtube.com/watch?v=WO2X3oZEJOA](https://www.youtube.com/watch?v=WO2X3oZEJOA)).
    If an LLM has been trained on garbage, it will output garbage.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些辩论的结果尚未可知，但如果我们把这个话题带回软件开发的世界，我们必须注意LLM训练所使用的材料。例如，ChatGPT在某个时刻，当发送特定短语给它时，会返回无意义的响应，这仅仅是因为它被训练在来自r/counting
    subreddits的数据上，这个subreddit充满了看似无意义的数据。你可以从Computerphile了解更多关于这种奇怪行为的信息，请参阅[www.youtube.com/watch?v=WO2X3oZEJOA](https://www.youtube.com/watch?v=WO2X3oZEJOA))。如果一个LLM被垃圾数据训练，它将输出垃圾。
- en: This becomes important when we consider tools such as GitHub Copilot, which
    uses the same GPT model that ChatGPT uses. Copilot has been fine-tuned differently,
    using the billions of lines of code stored in GitHub so that it can act as an
    assistant and suggest code snippets as we develop our codebase. We’ll explore
    in later chapters how we can put Copilot to good use, but again, we should be
    critical of what it suggests and not blindly accept everything it offers as a
    suggestion. Why? Ask yourself, are you happy with the code you’ve created in the
    past? Do you trust all the code others have created? If a large population of
    engineers is prone to implementing bad patterns, then that is what tools like
    Copilot will have been trained on. The point is a little hyperbolic because a
    lot of good developers and testers out there do good work—good work that Copilot
    is trained on. But it’s a thought exercise worth considering every now and then
    just to ensure that we remember who is in the driver’s seat when building applications
    with LLMs.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑像GitHub Copilot这样的工具时，这一点变得尤为重要。Copilot使用的是与ChatGPT相同的GPT模型。Copilot经过不同的微调，利用GitHub存储的数十亿行代码，以便在我们开发代码库时充当助手并建议代码片段。我们将在后面的章节中探讨如何充分利用Copilot，但再次强调，我们应该对它提出的建议持批判态度，不要盲目接受它提供的所有建议。为什么？问问自己，你对过去创建的代码满意吗？你信任其他人创建的所有代码吗？如果大量工程师倾向于实施不良模式，那么这就是像Copilot这样的工具所训练的内容。这个观点有点夸张，因为许多优秀的开发人员和测试人员确实在做好工作——这是Copilot所训练的。但这是一个值得不时考虑的思维练习，以确保我们在使用LLM构建应用程序时记住谁是驾驶员。
- en: 2.2.3 Data privacy
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 数据隐私
- en: Just as we need to be mindful of what an LLM outputs, we also have to consider
    what we enter into it. The temptation to share material with LLMs to find answers
    to problems we’re facing will be strong. But we have to ask ourselves, where is
    the data we send being stored? As mentioned earlier, LLMs are being tweaked continuously
    through the RLFH feedback. Companies such as OpenAI and GitHub will take the information
    we share, store it, and use it for future model training (GitHub does offer some
    privacy controls over what it can store, though).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们需要注意LLM输出的内容一样，我们也要考虑我们输入的内容。与LLM分享材料以寻找我们面临的问题的答案的诱惑将会很强。但我们必须问自己，我们发送的数据将被存储在哪里？如前所述，LLM正在通过RLFH反馈不断调整。例如，OpenAI和GitHub这样的公司将会获取我们分享的信息，存储它，并将其用于未来的模型训练（尽管GitHub提供了一些关于它可以存储的隐私控制）。
- en: 'This can be problematic when working for companies (or for ourselves) who want
    to keep their intellectual property private. Take Samsung, for example, whose
    employees accidentally leaked confidential material through the use of ChatGPT,
    as described here by TechRadar (you can read about it at [https://mng.bz/PN52](https://mng.bz/PN52)):'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当为希望保持其知识产权私有的公司（或为我们自己）工作时，这可能会成为一个问题。以三星为例，其员工意外地通过使用ChatGPT泄露了机密材料，正如TechRadar所描述的（你可以在[https://mng.bz/PN52](https://mng.bz/PN52)上了解更多信息）：
- en: The company allowed engineers at its semiconductor arm to use the AI writer
    to help fix problems with their source code. But in doing so, the workers entered
    confidential data, such as the source code itself for a new program and internal
    meeting notes data relating to their hardware.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 公司允许其半导体部门的工程师使用AI写作工具来帮助他们修复源代码中的问题。但在这样做的时候，工人们输入了机密数据，例如新程序的源代码本身以及与他们的硬件相关的内部会议笔记数据。
- en: As the adoption of LLMs begins to increase across organizations, we may begin
    to see an increase in policies that restrict what we can and can’t use LLMs for.
    Some may ban the use of third-party LLMs, and some organizations will opt to train
    and deploy their own internal LLMs for internal use (a topic we will explore in
    part 3). The result of those decisions will be highly contextual, but they will
    affect what type of LLMs we use and what data we can and cannot send, underlying
    our need to be mindful of what we send to LLMs.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLMs在组织中的采用开始增加，我们可能会看到越来越多的政策出台，限制我们可以和不可以使用LLMs的情况。一些可能会禁止使用第三方LLMs，而一些组织可能会选择训练和部署他们自己的内部LLMs以供内部使用（我们将在第3部分探讨这个话题）。这些决策的结果将非常具体，但它们将影响我们使用的LLM类型以及我们可以和不可以发送的数据，这使我们有必要注意我们发送给LLMs的内容。
- en: It’s also important to keep customer privacy in mind as we have an obligation
    not only to the companies we work for (especially for those who sign nondisclosure
    agreements) but also to our users. We have a legal and moral duty to protect user
    data from being spread into the wild, where we have no oversight.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们不仅对我们工作的公司（特别是那些签署保密协议的公司）有义务（特别是对于那些签署保密协议的公司），而且对我们用户也有义务的情况下，保持客户隐私同样重要。我们有法律和道德责任保护用户数据不被传播到我们无法监管的野外。
- en: In conclusion, although LLMs provide a wealth of opportunities, we must avoid
    the trap of anthropomorphizing them. Treating LLMs as if they have come to conclusions
    in the same way as we humans do is a fallacy. It can entrench a level of trust
    in the output that is dangerous and likely means that we aren’t getting the most
    benefit out of them. However, if we learn to use the probabilistic nature of LLMs
    when we instruct them, we can increase our chances of creating outputs that can
    help us improve efficiency—which is where prompt engineering can help us.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，尽管LLMs提供了丰富的机会，我们必须避免将它们拟人化的陷阱。将LLMs视为以与我们人类相同的方式得出结论是错误的。这可能会在输出中建立一种危险的信任水平，并且很可能意味着我们没有从它们中获得最大的好处。然而，如果我们学会在指导它们时利用LLMs的概率性质，我们可以增加创建有助于我们提高效率的输出的机会——这就是提示工程能帮助我们的地方。
- en: 2.3 Improving results with prompt engineering
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 使用提示工程改进结果
- en: We use natural language to prompt an LLM to return a desired outcome, but because
    they are probabilistic, we can communicate with them in a way that differs from
    normal interaction with humans. As LLMs have developed, a new field of engineering
    has appeared known as *prompt engineering,* which contains a collection of patterns
    and techniques that we can use to increase the likelihood of getting a desired
    output from an LLM.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用自然语言来提示一个LLM返回期望的结果，但由于它们是概率性的，我们可以以一种与正常人类互动不同的方式与他们沟通。随着LLMs的发展，一个新的工程领域出现了，被称为*提示工程*，它包含了一系列我们可以用来增加从LLM获得期望输出的可能性的模式和技巧。
- en: What is a prompt?
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是提示？
- en: 'In this book, we’ll use the term *prompt* regularly as this will be our primary
    means of communicating with LLMs. When we use this term, we are simply referring
    to the natural language input that is sent to an LLM. For example, in the first
    example of this chapter, I sent the following prompt:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将经常使用*提示*这个术语，因为这将是我们的主要沟通手段。当我们使用这个术语时，我们只是在指发送给LLM的自然语言输入。例如，在本章的第一个例子中，我发送了以下提示：
- en: '![](../../OEBPS/Images/logo-MW.png)   Hello ChatGPT, can you introduce yourself
    briefly?'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/logo-MW.png)   嗨，ChatGPT，你能简单介绍一下你自己吗？'
- en: 'which prompted the LLM to return this response to me:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这个提示促使LLM给了我以下回应：
- en: '![](../../OEBPS/Images/logo-openai.png)   Hello! I’m ChatGPT, an AI language
    model developed by OpenAI. I can help with answering questions, providing information,
    brainstorming ideas, and more. Feel free to ask me anything!'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../../OEBPS/Images/logo-openai.png)   你好！我是ChatGPT，由OpenAI开发的AI语言模型。我可以帮助你回答问题、提供信息、头脑风暴想法等等。请随时向我提问！'
- en: In the coming chapters, we will use prompt engineering heavily to trigger LLMs
    to deliver a range of useful content for various testing activities. But before
    we begin, it’s worthwhile learning the fundamentals of prompt engineering so that
    we can see how prompts are built to maximize output from LLMs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将大量使用提示工程来触发LLM，以提供各种测试活动的有用内容。但在我们开始之前，学习提示工程的基本原理是很有价值的，这样我们就可以看到提示是如何构建以最大化LLM输出的。
- en: 'To help us better understand what prompt engineering is, consider these two
    prompts sent to an LLM. The first is a general question:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们更好地理解提示工程是什么，考虑这两个发送给一个LLM的提示。第一个是一个一般性的问题：
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-MW.png)'
- en: '| What risks should I test for when testing a Contact Us form? |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 在测试“联系我们”表单时，我应该测试哪些风险？ |'
- en: 'The second is a more detailed prompt:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个是一个更详细的提示：
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-MW.png)'
- en: '| Assume the role of a professional software tester and suggest a list of risks
    for a Contact Us form using the following constraints:# Focus on the quality characteristics
    of accessibility, compliance and usability# Number each risk# Provide which quality
    characteristic the risk relates to# Use no more than 30 words to describe each
    riskFor example:   1.  Semantic structure - Accessibility - The HTML structure
    of the form should follow WCAG A standards |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 假设你是专业软件测试员，根据以下约束条件为“联系我们”表单提出一份风险列表：# 专注于可访问性、合规性和易用性的质量特性# 对每个风险进行编号#
    提供与哪个质量特性相关的风险# 每个风险的描述不超过30个字例如：   1. 语义结构 - 可访问性 - 表单的HTML结构应遵循WCAG A标准 |'
- en: Comparing the two, we can see that the second example is more detailed, with
    explicit requests and examples to outline what we might expect the LLM to return.
    Although the intention is similar, the output from each is drastically different.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 比较这两个示例，我们可以看到第二个示例更加详细，有明确的请求和示例来概述我们可能期望LLM返回的内容。尽管意图相似，但每个示例的输出却截然不同。
- en: 'Compare the output from the first example:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 比较第一个示例的输出：
- en: '|'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-openai.png)'
- en: '| Validation and Error Handling: Verify that the form correctly validates user
    input, such as email addresses, phone numbers, and required fields. Test for appropriate
    error messages when users submit incorrect or incomplete information.Security:
    Ensure that the contact form is secure and protected against common vulnerabilities
    such as cross-site scripting (XSS) and SQL injection attacks. Validate that the
    form sanitizes or escapes user input to prevent malicious code execution.Usability:
    Test the form’s usability by evaluating factors such as the clarity of form fields,
    user instructions, and overall user experience. Make sure the form is intuitive,
    easy to navigate, and visually appealing. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 验证和错误处理：验证表单是否正确验证用户输入，例如电子邮件地址、电话号码和必填字段。测试用户提交不正确或不完整信息时的适当错误消息。安全性：确保联系表单安全并受到常见漏洞的保护，如跨站脚本（XSS）和SQL注入攻击。验证表单对用户输入进行清理或转义以防止恶意代码执行。易用性：通过评估表单字段、用户说明和整体用户体验等因素来测试表单的易用性。确保表单直观、易于导航且视觉上吸引人。
    |'
- en: 'against the second example:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与第二个示例相反：
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-openai.png)'
- en: '|    1.  Keyboard accessibility - Accessibility - Ensure the form can be easily
    navigated and filled out using only a keyboard....   6.  Consistent design - Usability
    - Verify that the form follows a consistent design pattern and matches the overall
    website design.... 13.  Consent checkboxes - Compliance - Ensure any consent checkboxes
    or privacy policy links are clearly presented and function correctly. |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 1. 键盘可访问性 - 可访问性 - 确保表单可以通过仅使用键盘轻松导航和填写。   6. 一致的设计 - 易用性 - 验证表单是否遵循一致的设计模式并与整体网站设计相匹配。... 13.
    同意复选框 - 合规性 - 确保任何同意复选框或隐私政策链接都清晰呈现且功能正常。 |'
- en: Imagine that we want to use the proposed risks in each example to guide our
    testing. The first example has output suggestions that are vague and abstract.
    We would still need to do a fair bit of work to break down the large topics, such
    as security risks, whereas, with the second example, we have specific, actionable
    risks that we could use easily. And the goal of using tools such as LLM is to
    *reduce* the workload, not increase it.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们想要利用每个示例中提出的风险来指导我们的测试。第一个示例的输出建议模糊且抽象。我们仍然需要做大量工作来分解大型主题，例如安全风险，而第二个示例则提供了具体、可操作的风险，我们可以轻松地使用。使用像LLM这样的工具的目标是*减少*工作量，而不是增加。
- en: Our second prompt yields better results because the instructions it gives have
    been considered and are detailed and clear, which is what prompt engineering is
    essentially about. Although both prompts use natural language, with prompt engineering,
    we are aware of how an LLM works and what we want it to return to understand how
    to write a prompt so that we maximize the chances of a desired outcome. When using
    prompt engineering, we can see that although an LLM communicates in plain language,
    how it processes our request differs from how a human might do so, so we can adopt
    specific techniques to steer an LLM in the direction we want.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个提示产生了更好的结果，因为它给出的指令经过考虑，详细且清晰，这正是提示工程的核心所在。尽管两个提示都使用了自然语言，但在提示工程中，我们了解LLM的工作方式以及我们希望它返回的内容，以便了解如何编写提示以最大化期望结果的机会。在使用提示工程时，我们可以看到，尽管LLM使用普通语言进行交流，但它处理我们的请求的方式与人类可能的方式不同，因此我们可以采用特定的技术来引导LLM走向我们想要的方向。
- en: 2.4 Examining the principles of prompt engineering
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 探讨提示工程的原则
- en: As LLMs have developed, so have the patterns and techniques of prompt engineering.
    Many courses and blog posts have been written around prompt engineering, but one
    notable collection of principles, which we’ll explore shortly, has been created
    by Isa Fulford and Andrew Ng and their respective teams. A collaboration between
    OpenAI’s LLM knowledge and Deeplearning.ai’s teaching platform has created a course
    called ChatGPT Prompt Engineering for Developers, which features a series of principles
    and tactics that can be used in prompts to get the most out of LLMs. If you have
    the time, I encourage you to take the short course available at [https://mng.bz/JNGp](https://mng.bz/JNGp)
    ([https://www.promptingguide.ai/](https://www.promptingguide.ai/) is also a useful
    reference.) Although the course references ChatGPT specifically, the principles
    taught there can be applied across many LLMs. So let’s explore these principles
    and tactics to get comfortable with prompting LLMs.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM的发展，提示工程的模式和技术也相应发展。围绕提示工程已经编写了许多课程和博客文章，但由Isa Fulford和Andrew Ng及其各自团队创建的一套引人注目的原则，我们将在稍后探讨，已经形成。OpenAI的LLM知识和Deeplearning.ai的教学平台之间的合作创造了一门名为“开发者ChatGPT提示工程”的课程，该课程介绍了一系列可用于提示以充分利用LLM的原则和策略。如果您有时间，我鼓励您参加可在[https://mng.bz/JNGp](https://mng.bz/JNGp)
    ([https://www.promptingguide.ai/](https://www.promptingguide.ai/)也是一个有用的参考资料。)提供的短期课程。尽管该课程专门提到了ChatGPT，但其中教授的原则可以应用于许多LLM。因此，让我们探讨这些原则和策略，以便我们能够熟练地提示LLM。
- en: '2.4.1 Principle 1: Write clear and specific instructions'
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 原则 1：编写清晰和具体的指令
- en: 'This first principle might seem obvious at first glance—it’s always sensible
    to provide instructions to others that are clear and specific. However, what this
    principle suggests is that we write prompts that are clear and specific *for an
    LLM.* And that means something different from what might be clear and specific
    to a human. To implement this concept, Fulford and Ng teach four tactics to achieve
    clear and specific prompts: use delimiters, ask for structured output, check for
    assumptions, and use few-shot prompting. In the next few sections, we’ll examine
    each one in more detail.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这个第一个原则乍一看可能显得很显然——向他人提供清晰和具体的指令总是明智的。然而，这个原则所建议的是，我们为LLM编写清晰和具体的提示。这意味着与对人类来说清晰和具体的东西是不同的。为了实现这一概念，Fulford和Ng教授了四种实现清晰和具体提示的策略：使用分隔符、请求结构化输出、检查假设和使用少样本提示。在接下来的几节中，我们将更详细地考察每一个策略。
- en: '2.4.2 Tactic 1: Use delimiters'
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 策略 1：使用分隔符
- en: 'When writing prompts, we may need to provide different content and data that
    serve different purposes. For example, the start of our prompt might include instructions
    on what we want an LLM to produce, whereas the end of our prompt might include
    raw data that we want to process. LLMs can guess our intentions for different
    sections of our prompts, but because our goal is to be as clear as possible, we
    can aid the process by using *delimiters,* which are characters used to separate
    strings, to state our intentions for different parts of our prompt. Take this
    prompt as an example. We open the prompt with a message that sets expectations:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写提示时，我们可能需要提供不同内容和数据，以服务于不同的目的。例如，我们提示的开始部分可能包括我们希望 LLM 生成的内容的说明，而提示的末尾可能包括我们希望处理的原数据。LLMs
    可以猜测我们提示不同部分的目的，但鉴于我们的目标是尽可能清晰，我们可以通过使用 *分隔符*（用于分隔字符串的字符）来明确我们提示不同部分的目的。以下是一个提示示例。我们以一个设置期望的消息开始提示：
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![MW 标志](../../OEBPS/Images/logo-MW.png)'
- en: '| You are a SQL data generator. Generate five rows of SQL for a MySQL database.
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 你是一个 SQL 数据生成器。为 MySQL 数据库生成五行 SQL。 |'
- en: 'We use the * character to delimit rules:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 * 字符来分隔规则：
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![MW 标志](../../OEBPS/Images/logo-MW.png)'
- en: '|    *   The table name is identified with a # sign.   *   Each table column
    is identified with a % sign   *   Each column is described in order of name, data
    type and data options using the &#124; sign   *   If a column data option says
    random, randomize data based on the suggested format and column name |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|   *   表名用 # 符号标识。   *   每个表列用 % 符号标识   *   每个列按名称、数据类型和数据选项的顺序描述，使用 | 符号  
    *   如果列数据选项为随机，则根据建议的格式和列名随机化数据 |'
- en: 'We then use the #, %, and | delimiters that we set in rules to provide instructions:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '我们随后使用在规则中设置的 #、% 和 | 分隔符来提供指令：'
- en: '|'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![MW 标志](../../OEBPS/Images/logo-MW.png)'
- en: '| Here are the instructions:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '| 这里是指令：'
- en: '[PRE0]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '|'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'When running this prompt within ChatGPT, I received the following output:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ChatGPT 中运行此提示时，我收到了以下输出：
- en: '|'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![OpenAI 标志](../../OEBPS/Images/logo-openai.png)'
- en: '|'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE1]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '|'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: As we can see, through the use of delimiters the correct table name `rooms`
    has been added, and the column names, formats, and ranges have been correctly
    randomized. This is made possible by the clear expectations and rules we set that
    are distinguished by various delimiters throughout the prompt.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，通过使用分隔符，正确的表名 `rooms` 已被添加，列名、格式和范围已被正确随机化。这是通过我们在提示中通过各种分隔符区分的清晰期望和规则实现的。
- en: The delimiters help to make the prompt clearer, but they also make it easy to
    modify. For example, if we want to reconfigure the generated data, we could enter
    another line, such as
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔符有助于使提示更清晰，但它们也使得修改变得容易。例如，如果我们想重新配置生成数据，我们可以输入另一行，例如
- en: '|'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![MW 标志](../../OEBPS/Images/logo-MW.png)'
- en: '|'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE2]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '|'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: So, when writing prompts that contain a large amount of data that changes context,
    we can use delimiters to make clear what is being provided in a prompt at a specific
    point.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们编写包含大量数据且上下文变化的提示时，我们可以使用分隔符来明确在特定点提示中提供的内容。
- en: Activity 2.1
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 活动二点一
- en: Using the delimiter prompt example, we create new instructions—this time, for
    a booking that would include information about who made the booking, contact details,
    and check-in and check-out dates.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分隔符提示示例，我们创建新的指令——这次是为一个预订，该预订将包含预订人信息、联系详情以及入住和退房日期。
- en: '2.4.3 Tactic 2: Ask for structured output'
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 策略 2：请求结构化输出
- en: 'One facet of LLMs that makes them useful is that they have the ability to provide
    outputs in structured formats. As we make our way through this book, we’ll explore
    just how useful this ability is, but as a rule, we must always remember that we
    need to be clear in a prompt about what structured format we want to see used.
    Take this prompt as an example:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs（大型语言模型）的一个使其有用的方面是它们能够以结构化格式提供输出。随着我们阅读本书，我们将探讨这一能力有多么有用，但作为一个规则，我们必须始终记住，在提示中我们必须清楚地说明我们希望使用哪种结构化格式。以下是一个提示示例：
- en: '|'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![MW 标志](../../OEBPS/Images/logo-MW.png)'
- en: '| Create a JSON object with random data that contains the following fields:
    firstname, lastname, totalprice, deposit paid. Also, include an object called
    booking dates that contains checkin and checkout dates. |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 创建一个包含以下字段的随机数据 JSON 对象：firstname，lastname，totalprice，deposit paid。还包括一个名为
    booking dates 的对象，包含 checkin 和 checkout 日期。 |'
- en: 'We begin the prompt by clearly stating what format we want to see our object
    in, in this case, JSON, before we start outlining the structure of the object.
    When I sent this prompt to ChatGPT, the following result was returned:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在开始概述对象结构之前，明确指出我们希望看到对象以何种格式呈现，在这种情况下是 JSON。当我将此提示发送到 ChatGPT 时，返回以下结果：
- en: '|'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![OpenAI 标志](../../OEBPS/Images/logo-openai.png)'
- en: '|'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE3]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '|'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: As the prompt clearly states, the expected format and structure are two distinct
    instructions, meaning we can modify our instructions to quickly change the format
    by sending an additional prompt, such as
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如提示中明确指出的，期望的格式和结构是两个不同的指令，这意味着我们可以通过发送额外的提示来快速更改指令，例如
- en: '|'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![MW 标志](../../OEBPS/Images/logo-MW.png)'
- en: '| Create this in a YAML format |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 请以 YAML 格式创建此内容 |'
- en: 'Sending this prompt to ChatGPT returned the following output:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 将此提示发送到 ChatGPT 返回以下输出：
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![OpenAI 标志](../../OEBPS/Images/logo-openai.png)'
- en: '|'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE4]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '|'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Notice the structure is the same (as well as the randomized data). By explicitly
    stating what format we desire, we can instruct an LLM with exactly what format
    we want at a given time and then alternate formats with ease.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意结构是相同的（以及随机化数据）。通过明确说明我们期望的格式，我们可以指导 LLM 在特定时间确切知道我们想要的格式，然后轻松地交替格式。
- en: Activity 2.2
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 活动第 2.2 部分
- en: Attempt to create an object that contains multiple parameters with different
    data types in different formats. Try prompting an LLM to convert your object from
    one format to another—for example, from JSON to XML.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试创建一个包含不同数据类型和不同格式的多个参数的对象。尝试提示一个大型语言模型（LLM）将您的对象从一种格式转换为另一种格式——例如，从 JSON 转换为
    XML。
- en: '2.4.4 Tactic 3: Check for assumptions'
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.4 策略 3：检查假设
- en: 'As we learned earlier, LLMs can hallucinate by generating output that contains
    information that is incorrect or disconnected from reality. It goes without saying
    that we want to reduce the risk of producing hallucinations, which is where our
    third tactic, checking for assumptions, can help. LLMs are more likely to hallucinate
    if they are provided with prompts that are focused on edge cases around a problem
    we want to solve. If an LLM is not properly instructed, it’s more likely to make
    a guess at an answer than to outright inform us that it cannot provide a useful
    answer. So, if we want to avoid guesswork, we need to provide instructions in
    our prompt to allow the LLM to bail out if it cannot execute our request. Consider
    the following example. It provides initial instructions for the prompt, which
    includes a delimiter:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所学的，LLM 可以通过生成包含错误或不与现实脱节的信息的输出来进行幻觉。不言而喻，我们希望减少产生幻觉的风险，这就是我们的第三种策略——检查假设——可以发挥作用的地方。如果
    LLM 被提供了关注我们想要解决的问题边缘情况的提示，它们更有可能产生幻觉。如果 LLM 没有得到适当的指导，它更有可能猜测答案，而不是直接告诉我们它无法提供有用的答案。因此，如果我们想要避免猜测，我们需要在提示中提供指令，以便
    LLM 在无法执行我们的请求时退出。考虑以下示例。它提供了提示的初始指令，其中包括一个分隔符：
- en: '|'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![MW 标志](../../OEBPS/Images/logo-MW.png)'
- en: '| You will be provided with a JSON object delimited by three hashes. Extract
    all emails that end with .com and write them out as a list. |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 你将获得一个由三个井号分隔的 JSON 对象。提取所有以 .com 结尾的电子邮件，并将它们列出来。 |'
- en: 'We then check for assumptions and allow the LLM to bail:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后检查假设，并允许 LLM 退出：
- en: '|'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![MW 标志](../../OEBPS/Images/logo-MW.png)'
- en: '| If no email addresses with a .com email address exist, simply write “No .com
    emails found” |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 如果不存在以 .com 结尾的电子邮件地址，只需简单地写出 “未找到 .com 电子邮件” |'
- en: 'Finally, we provide the data to process using ### as a delimiter:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用###作为分隔符提供要处理的数据：
- en: '|'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![MW 标志](../../OEBPS/Images/logo-MW.png)'
- en: '|'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE5]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Running this prompt with a collection of email addresses that can be extracted
    returned the following output:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用可以提取的电子邮件地址集合运行此提示后返回以下输出：
- en: '|'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![OpenAI 标志](../../OEBPS/Images/logo-openai.png)'
- en: '| Here are the email addresses that end with “.com”:   *   baveray0@apple.com  
    *   pyandell4@ning.com   *   gpentecost6@yelp.com |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 这里是以 “.com” 结尾的电子邮件地址：   *   baveray0@apple.com   *   pyandell4@ning.com  
    *   gpentecost6@yelp.com |'
- en: 'But here’s what happened when I ran the prompt again without email addresses:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 但当我再次运行提示而不包含电子邮件地址时，发生了以下情况：
- en: '|'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![MW 标志](../../OEBPS/Images/logo-MW.png)'
- en: '| You will be provided with a JSON object delimited by three hashes. Extract
    all emails that end with .com and write them out as a list.If no email addresses
    with a .com email address exist, simply write “No .com emails found”'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '| 你将获得一个由三个井号分隔的JSON对象。提取所有以.com结尾的电子邮件，并将它们列出来。如果不存在以.com结尾的电子邮件地址，只需写下“未找到以.com结尾的电子邮件地址”'
- en: '[PRE6]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '|'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'I received the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我收到了以下信息：
- en: '|'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/logo-openai.png)'
- en: '| Here is the list of email addresses that end with .com extracted from the
    provided JSON object:No .com emails found |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 从提供的JSON对象中提取的以.com结尾的电子邮件地址列表如下：未找到以.com结尾的电子邮件地址 |'
- en: 'This response from the LLM was a direct reference to this section of the prompt:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的此响应直接引用了提示的这一部分：
- en: '|'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/logo-MW.png)'
- en: '| If no email addresses with a .com email address exist, simply write “No .com
    emails found.” |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 如果不存在以.com结尾的电子邮件地址，只需写下“未找到以.com结尾的电子邮件地址。” |'
- en: 'which prevented the LLM from providing an output that was incorrect. For example,
    when I ran the prompt without the assumption check, the following was returned:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这阻止了LLM提供错误输出。例如，当我运行没有假设检查的提示时，返回了以下内容：
- en: '|'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/logo-openai.png)'
- en: '| Sure! I can help you with that. Here are the email addresses that end with
    “.com” from the provided JSON object:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '| 当然！我可以帮你。以下是提供的JSON对象中以“ .com”结尾的电子邮件地址：'
- en: '[PRE7]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Please note that I have used example.com as a placeholder domain for all the
    email addresses. Replace it with the appropriate domain if you have the actual
    email addresses. |
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我已经使用example.com作为所有电子邮件地址的占位符域名。如果您有实际的电子邮件地址，请将其替换为适当的域名。 |
- en: This is an excellent example of an LLM hallucination. The object we provided
    contained no email addresses, so the LLM used guesswork and incorrectly started
    generating new email addresses based on existing data. However, with the assumption
    check in place, we prevented the hallucination from occurring.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个LLM幻觉的绝佳例子。我们提供的对象中不包含电子邮件地址，因此LLM使用了猜测并错误地根据现有数据开始生成新的电子邮件地址。然而，由于假设检查的存在，我们防止了幻觉的发生。
- en: Activity 2.3
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 活动二点三
- en: Create a prompt that filters specific data out of a list of raw data and then
    outputs the results (for example, filter fruit from a list of different foodstuffs).
    Next, modify the data to include an edge case (for example, incorrect or missing
    data). Observe what the LLM outputs, and then attempt to correct the output by
    adding to the prompt some instructions that follow the check-for-assumptions prompt.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个提示，从原始数据列表中过滤出特定数据，然后输出结果（例如，从不同食品列表中过滤水果）。接下来，修改数据以包括边缘情况（例如，错误或缺失的数据）。观察LLM的输出，然后尝试通过在提示中添加一些遵循检查假设提示的指令来纠正输出。
- en: '2.4.5 Tactic 4: Few-shot prompting'
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.5 策略4：少样本提示
- en: 'As we’ve progressed through this chapter and learned new concepts about LLMs
    and prompting, key points have been clarified using examples. They are essential
    tools when it comes to not just teaching but also communicating with one another.
    This is no different for LLMs. *Few-shot prompting* basically means providing
    explicit examples to clarify instructions (The word *few* in this context indicates
    how many examples you share. A prompt with no examples would be a *zero-shot prompt*).
    What follows is a prompt using examples. First, we provide the instructions in
    the prompt outlining the output format:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们进入本章并学习关于LLM和提示的新概念，关键点通过示例得到了阐明。它们不仅是教学工具，也是相互沟通的重要工具。对于LLM来说，也是如此。*少样本提示*基本上意味着提供明确的示例以阐明指令（在这个上下文中，“少”这个词表示你分享了几个示例。没有示例的提示将是*零样本提示*）。以下是一个使用示例的提示。首先，我们在提示中提供指令，概述输出格式：
- en: '|'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/logo-MW.png)'
- en: '| You are an expert exploratory tester. Create three test charters for a booking
    system that focus on the risks around booking dates. The format should follow:
    Explore <Target> using <Resource> to discover <Information> |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 你是一位专家探索性测试员。为预订系统创建三个测试章程，重点关注预订日期的风险。格式应遵循：探索<目标>使用<资源>以发现<信息> |'
- en: 'We provide examples to clarify the output format for the LLM:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供示例以阐明LLM的输出格式：
- en: '|'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/logo-MW.png)'
- en: '| For example:   *   Explore user permissions using different users to discover
    permission risks   *   Explore browser animations using different devices to discover
    how animations render |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 例如：   *   使用不同的用户探索用户权限以发现权限风险   *   使用不同的设备探索浏览器动画以发现动画的渲染方式|'
- en: 'Sending this prompt to ChatGPT returned the following output:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 将此提示发送到ChatGPT返回以下输出：
- en: '|'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![OpenAI 标志](../../OEBPS/Images/logo-openai.png)'
- en: '| Explore date validation using various date formats to discover potential
    input errors.Explore time zone handling using different geographical locations
    to discover discrepancies in booking times.Explore booking conflicts using overlapping
    reservation requests to discover potential double-booking risks. |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 使用各种日期格式探索日期验证以发现潜在的输入错误。使用不同的地理位置探索时区处理以发现预订时间的差异。使用重叠的预订请求探索预订冲突以发现潜在的重复预订风险。|'
- en: Consider the format of “Explore <Target> using <Resource> to discover <Information>.”
    It is rule based, but it leans heavily on natural language to communicate the
    rules (unlike JSON or XML structures that rely on explicit delimiter rules). By
    providing examples in our prompt, we can help contextualize what we mean by `<Target>`,
    `<Resource>`, and `<Information>` and reduce the risk of the LLM guessing what
    that means.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑“使用<资源>探索<目标>以发现<信息>”的格式。它是基于规则的，但它在传达规则方面高度依赖自然语言（与依赖于显式分隔符规则的JSON或XML结构不同）。通过在我们的提示中提供示例，我们可以帮助说明我们所说的“<目标>”、“<资源>”和“<信息>”，并降低LLM猜测其含义的风险。
- en: Activity 2.4
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 活动二.4
- en: Create a prompt that uses the few-shot tactic. In your prompt, provide the instructions
    you expect the LLM to follow and then add at least two examples to help guide
    it in providing a desired outcome.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个使用少量样本策略的提示。在你的提示中，提供你期望LLM遵循的指令，然后添加至少两个示例以帮助指导它提供期望的结果。
- en: '2.4.6 Principle 2: Give the model time to “think”'
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.6 原则2：给模型思考时间
- en: 'It may seem a little unusual, given that we’ve been considering LLMs as probabilistic
    machines and not entities that can think, to see a principle that encourages us
    to give a model time to think. However, the best way to understand this principle
    is that we’re not actually giving an LLM time to think but rather emulating how
    we would give someone time to think about a task in our prompt. In the ChatGPT
    Prompt Engineering for Developers course, Fulford gives a useful analogy as she
    introduces this principle: “If you give a model a task that’s too complex to do
    in a short amount of time, . . . it may make up a guess which is likely to be
    incorrect.”'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们一直将LLM视为概率机器，而不是能够思考的实体，看到鼓励我们给模型思考时间的原则可能显得有些不寻常。然而，理解这一原则的最好方式是，我们实际上并没有给LLM时间来思考，而是在提示中模拟我们如何给某人时间来思考任务。在ChatGPT提示工程开发课程中，Fulford在介绍这一原则时提供了一个有用的类比：“如果你给模型一个在短时间内无法完成的任务，它可能会做出一个可能是错误的猜测。”
- en: She explains that the same situation would happen if humans were given a complex
    task with limited time to respond. We’d rely on educated guesswork and likely
    come up with a less-than-satisfactory answer. Thus, principle 2 offers tactics
    to help us write prompts that break down tasks and encourage LLMs to evaluate
    output to once again maximize the chances of a desired response.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 她解释说，如果人类被赋予一个复杂任务，且只有有限的时间来响应，同样的情况也会发生。我们会依赖有根据的猜测，并可能得出一个不尽如人意的答案。因此，原则2提供了策略，帮助我们编写将任务分解并鼓励LLM评估输出以再次最大化期望响应机会的提示。
- en: '2.4.7 Tactic 1: Specify the steps to complete the task'
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.7 策略1：指定完成任务步骤
- en: 'The first tactic is relatively straightforward once we are confident about
    using the tactics of principle 1 (write clear and specific instructions). By using
    delimiters, we can break a complex task into individual steps for an LLM to take
    to solve the larger task. Take a look at this prompt, where we first notify the
    LLM that a set of instructions is coming, followed by delimited data:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对使用原则1的策略（编写清晰和具体的指令）有信心，第一个策略相对简单。通过使用分隔符，我们可以将复杂任务分解成LLM可以采取的单独步骤来解决更大的任务。看看这个提示，我们首先通知LLM即将到来一系列指令，然后是分隔的数据：
- en: '|'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![MW 标志](../../OEBPS/Images/logo-MW.png)'
- en: '| You are going to be given a list of instructions to follow. |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 你将被给出一系列要遵循的指令。|'
- en: 'We then list each subtask clearly to instruct the LLM on what to do:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后明确列出每个子任务，以指导LLM执行以下操作：
- en: '|'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![MW 标志](../../OEBPS/Images/logo-MW.png)'
- en: '|    1.  Identify functional risks that might impact the text delimited by
    three hashes   2.  Convert the risks into test charters   3.  Format each charter
    into a Discover <feature> using <resource> to discover <information> format |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|   1.  识别可能影响由三个井号分隔的文本的功能性风险   2.  将风险转换为测试章程   3.  将每个章程格式化为“使用 <资源> 发现
    <信息> 以发现 <功能>”格式 |'
- en: 'Finally, we provide the data we would like to see processed:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们提供我们希望看到处理的数据：
- en: '|'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![MW 标志](../../OEBPS/Images/logo-MW.png)'
- en: '| ###As a userI want to be able to calculate my taxSo I know what tax I have
    to pay### |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| ### 作为用户，我希望能够计算我的税额，以便我知道我需要支付多少税额### |'
- en: 'When sent to ChatGPT, it returned this result:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当发送到 ChatGPT 时，它返回了以下结果：
- en: '|'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![OpenAI 标志](../../OEBPS/Images/logo-openai.png)'
- en: '| Functional risks:   •   Calculation accuracy: There is a risk that the tax
    calculation may be inaccurate, leading to incorrect tax amounts being calculated.Test
    charters based on the identified risks:   •   Test Charter 1: Validate the accuracy
    of tax calculations by inputting different test scenarios with known tax amounts
    and verifying that the calculated tax matches the expected values.Formatting the
    charters into a “Discover <feature> using <resource> to discover <information>”
    format:   •   Discover tax calculation accuracy using test scenarios with known
    tax amounts to validate the accuracy of the calculated tax. |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 功能性风险：   •   计算准确性：存在税务计算可能不准确的风险，导致计算出的税额不正确。基于已识别风险的测试章程：   •   测试章程 1：通过输入已知税额的不同测试场景并验证计算出的税额是否与预期值相符，来验证税务计算的准确性。将章程格式化为“使用
    <资源> 发现 <信息> 以发现 <功能>”格式：   •   使用已知税额的测试场景来发现税务计算准确性，以验证计算出的税额的准确性。 |'
- en: This is a snippet from the output I received. It created many more risks and
    charters, but the first example from each section demonstrates the LLM responding
    to each subtask, one by one, to create an output.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这是接收到的输出片段。它创建了更多风险和章程，但每个部分的第一个例子都展示了 LLM 逐个响应每个子任务以创建输出。
- en: Activity 2.5
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 活动第 2.5 部分
- en: Create a prompt that requires a complex task to be carried out. Attempt to break
    out the complex task into multiple subtasks that the LLM can carry out.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个需要执行复杂任务的提示。尝试将复杂任务分解成多个子任务，LLM 可以执行这些子任务。
- en: '2.4.8 Tactic 2: Instruct the model to work out its own solution first'
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.8 策略 2：指示模型首先自行解决问题
- en: 'Our final tactic focuses not on the process of creating an output, but rather
    on evaluating the output itself. Similar to checking for assumptions, ask an LLM
    to evaluate the output to confirm that it aligns with what it has been instructed
    to produce. Let’s check out an example to gain a better understanding of how this
    would work. First, take a look at a prompt that doesn’t ask an LLM to work out
    its solution first but sets out the instructions for the LLM to follow:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后的策略不是关注创建输出的过程，而是关注评估输出本身。类似于检查假设，要求 LLM 评估输出以确认其是否符合其被指示产生的结果。让我们通过一个例子来更好地了解这将如何运作。首先，看看一个不要求
    LLM 首先计算出解决方案，而是列出 LLM 应遵循的指示的提示：
- en: '|'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![MW 标志](../../OEBPS/Images/logo-MW.png)'
- en: '| You are a software developer in test that is experienced in writing Java.
    Create a unit test for the following method: |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 你是一位有经验的 Java 软件开发测试人员，请为以下方法创建单元测试： |'
- en: 'We then provide the code it needs to provide unit tests for:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们提供它需要的代码来提供单元测试：
- en: '|'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![MW 标志](../../OEBPS/Images/logo-MW.png)'
- en: '|'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE8]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '|'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Sending this to ChatGPT, the following result was returned:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 将此发送到 ChatGPT，返回以下结果：
- en: '|'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![OpenAI 标志](../../OEBPS/Images/logo-openai.png)'
- en: '|'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE9]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: This looks like reasonable Java code for a unit test, but if this were added
    to a suite of unit checks, it would fail because the method `authDB.deleteToken`
    in the production code provided has not been handled correctly. Specifically,
    if we wanted this unit check to work, we would need to mock `authDB.deleteToken`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来像是合理的单元测试 Java 代码，但如果将其添加到单元检查套件中，它将失败，因为生产代码中提供的 `authDB.deleteToken` 方法没有被正确处理。具体来说，如果我们想让这个单元检查工作，我们需要模拟
    `authDB.deleteToken`。
- en: 'Now, if we run the prompt again but this time have it evaluate its solution
    before outputting a final answer, we get a different result. So first we change
    the prompt to a new prompt that asks the LLM to confirm that the result of the
    check works:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们再次运行提示，但这次在输出最终答案之前先评估其解决方案，我们会得到不同的结果。所以首先我们将提示更改为一个新的提示，要求 LLM 确认检查的结果是否有效：
- en: '|'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-MW.png)'
- en: '| You are a software developer in test that is experienced in writing Java.
    Create a unit test for the following method that will be shared. Before outputting
    the unit tests, check to see that the assertion of each unit check will return
    a deterministic result. |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 你是一位有经验的 Java 开发者，在测试中编写单元测试。为以下方法创建一个单元测试，该测试将被共享。在输出单元测试之前，检查每个单元检查的断言是否返回一个确定的结果。
    |'
- en: 'We then provide the code for which a unit test will be written:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后提供了将要编写单元测试的代码：
- en: '|'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-MW.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-MW.png)'
- en: '|'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE10]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '|'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Sending this to ChatGPT returned this result:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 将此发送到 ChatGPT 返回了以下结果：
- en: '|'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![](../../OEBPS/Images/logo-openai.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/logo-openai.png)'
- en: '|'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE11]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '|'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: This time, we can see that because we asked the LLM to evaluate its solution
    before returning a result, the output unit check uses `Mockito` to mock the `authDB.deleteToken`.
    So, if we observe problems with LLMs outputting erroneous solutions or they start
    hallucinating, we can add an instruction to evaluate solutions first to minimize
    the occurrence of hallucinations.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们可以看到，因为我们要求 LLM 在返回结果之前评估其解决方案，所以输出单元检查使用 `Mockito` 模拟 `authDB.deleteToken`。因此，如果我们观察到
    LLMs 输出错误解决方案或它们开始产生幻觉，我们可以添加一个指令先评估解决方案，以最大限度地减少幻觉的发生。
- en: Activity 2.6
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 活动 2.6
- en: Create a prompt that requires an LLM to work out a solution to a problem. Observe
    its output and see if the solution it produces is correct. Then add instructions
    to have the LLM evaluate the solution. What happens? Does the solution change?
    Is it an improvement?
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个提示，要求 LLM 解决一个问题。观察其输出，看看它产生的解决方案是否正确。然后添加指令让 LLM 评估该解决方案。发生了什么？解决方案是否改变？这是否是一个改进？
- en: 2.5 Working with various LLMs
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 与各种大型语言模型（LLMs）合作
- en: So far, we’ve talked about LLMs in a broad sense while using OpenAI’s ChatGPT
    in the previous examples to demonstrate how they work in general. However, ChatGPT
    is just one of the many different LLMs that we can use. So, before we conclude
    the chapter, let’s familiarize ourselves with the ways in which LLMs differ from
    one another and learn about some of the currently popular models and communities
    so that we can increase our chances of finding the right LLM for the job.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在广义上讨论 LLMs，而在之前的例子中使用 OpenAI 的 ChatGPT 来演示它们的一般工作方式。然而，ChatGPT 只是我们可以使用的许多不同
    LLMs 之一。因此，在我们结束本章之前，让我们熟悉一下 LLMs 之间的不同之处，并了解一些目前流行的模型和社区，以便我们增加找到适合工作的正确 LLM
    的机会。
- en: 2.5.1 Comparing LLMs
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 比较LLMs
- en: What makes an LLM good? How do we determine whether a model is worth using?
    These are not easy questions to answer. The complex nature of LLMs, how they’re
    trained, and what data was used close these systems off to deep analysis, compromising
    an area that some researchers are trying to improve or shed light upon. However,
    that doesn’t mean we shouldn’t educate ourselves on some of the key aspects of
    LLMs and how they affect them. We might not all be AI researchers attempting to
    explore the deep inner workings of LLMs, but we are or will be their users and
    will want to know that what we spend resources on is giving us value. So, to help
    us break down some of the jargon and give us some grounding on how LLMs differ,
    let’s go through some key attributes discussed in the world of LLMs.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 什么使一个 LLM 变得优秀？我们如何确定一个模型是否值得使用？这些问题并不容易回答。LLMs 的复杂性质、它们的训练方式以及所使用的数据使得这些系统对深入分析关闭，这损害了一些研究人员试图改进或阐明的一个领域。然而，这并不意味着我们不应该在
    LLMs 的关键方面以及它们如何影响它们方面进行自我教育。我们可能不是试图探索 LLMs 深层内部运作的 AI 研究人员，但我们现在是或将成为它们的用户，并希望知道我们投入的资源是否在给我们带来价值。因此，为了帮助我们分解一些术语，并给我们一些关于
    LLMs 如何不同的基础，让我们回顾一下在 LLMs 领域中讨论的一些关键属性。
- en: Parameter count
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 参数数量
- en: If you take a look at different LLMs, you’ll likely see talk of LLMs having
    a 175-billion- or 1-trillion-parameter count. It can sometimes feel like marketing
    speak, but parameter count does not affect LLMs’ performance. The parameter count
    essentially refers to the amount of statistical weights that exist in a model.
    Each individual weight provides a piece of the statistical puzzle that makes up
    an LLM. So, roughly speaking, the more parameters an LLM has, the better it will
    perform. The parameter count can also give us a sense of cost. The higher the
    parameter count, the more expensive it is to run, and there is a cost that may
    be, in part, handed down to users.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看不同的LLM，你可能会听到关于LLM拥有1750亿或1万亿参数数量的讨论。这有时可能感觉像是营销术语，但参数数量并不影响LLM的性能。参数数量本质上指的是模型中存在的统计权重数量。每个单独的权重都提供了构成LLM的统计谜题的一部分。所以，粗略地说，一个LLM拥有的参数越多，其性能越好。参数数量也可以让我们对成本有所感知。参数数量越高，运行成本就越高，并且可能会有部分成本转嫁给用户。
- en: Training data
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据
- en: LLMs require huge quantities of data to be trained on, so the size and quality
    of data will have an effect on the quality of an LLM. If we want an LLM to be
    accurate in how it responds to requests, it’s not enough to just throw as much
    data as possible. It needs to be data that can help influence the probability
    of a model in a sensible manner. For example, the Reddit example we explored earlier
    in this chapter, in which the subreddit r/counting used to train ChatGPT caused
    it to hallucinate in strange ways, demonstrates that more isn’t necessarily better.
    Still, similar to parameter count, the more high-quality data an LLM has been
    trained on, the better it will likely perform. The challenge is in knowing what
    data an LLM has been trained on—something that corporate creators of AI are keen
    on keeping a secret.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: LLM需要大量的数据进行训练，因此数据的大小和质量将对LLM的质量产生影响。如果我们希望LLM在响应请求时准确无误，仅仅投入尽可能多的数据是不够的。它需要的是能够以合理方式影响模型概率的数据。例如，我们在本章前面探讨的Reddit示例，其中r/counting子版块被用来训练ChatGPT，导致它以奇怪的方式产生幻觉，这表明更多的数据并不一定是更好的。然而，与参数数量类似，一个LLM训练时所使用的高质量数据越多，其性能可能越好。挑战在于了解LLM训练所使用的数据——这是企业AI创造者热衷于保密的事情。
- en: Extensibility and integration
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性和集成
- en: Just like with any other tool, the value of an LLM can be increased further
    if it can offer other features beyond its core abilities, such as integrating
    into existing systems or training models further for our specific needs. What
    features are available to integrate and extend LLMs depends largely on who was
    responsible for training.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 就像任何其他工具一样，如果LLM能够提供其核心能力之外的其他功能，其价值可以进一步提高，例如集成到现有系统或进一步训练以满足我们的特定需求。可集成和扩展LLM的功能很大程度上取决于谁负责了训练。
- en: For example, OpenAI offers paid-for API access to their models. But beyond an
    instruction feature that allows you to tweak output with a simple prompt, there
    is no ability to further fine-tune and deploy one of their GPT models for private
    use. Compare this to Meta’s LlaMa model, which has been open sourced, allowing
    the AI community to download and further train to their own requirements, although
    they must build their own infrastructure to deploy the model.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，OpenAI提供了付费API访问其模型的服务。但除了允许你通过简单的提示调整输出的指令功能外，没有进一步微调和部署其GPT模型用于私人使用的功能。与此相比，Meta的LlaMa模型已被开源，允许AI社区下载并根据自身需求进一步训练，尽管他们必须建立自己的基础设施来部署该模型。
- en: As LLM platforms grow, we will see advances in not just their ability to respond
    to prompts but also the features around them and their access. Thus, it’s necessary
    to keep said features in mind when evaluating what to work with.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLM）平台的增长，我们将看到它们不仅对提示的响应能力有所提升，而且其周边功能和访问性也将得到改进。因此，在评估要与之合作的内容时，有必要牢记这些功能。
- en: Quality of responses
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 响应质量
- en: Arguably, the most important factor to consider is whether an LLM provides responses
    that are legible, useful, and free (or as close to free) of hallucination as possible.
    Although criteria such as parameter count and training data are useful indicators
    of an LLM’s performance, it’s up to us to understand what we want to use an LLM
    for and then determine how each responds to our prompts and helps solve our specific
    problems. Not all challenges we face need the largest, most expensive LLM in the
    market. Thus, it’s important that we take time to try out different models, compare
    their outputs, and then make a judgment for ourselves. For example, GPT models
    from OpenAI are found to perform better with code examples than Google Gemini.
    These details have been discovered through experimentation and observation.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 不可否认，最重要的考虑因素是LLM提供的响应是否易于阅读、有用，并且尽可能没有幻觉（或尽可能接近免费）。尽管参数数量和训练数据等标准是衡量LLM性能的有用指标，但了解我们想要使用LLM做什么，然后确定每个LLM如何响应我们的提示并帮助我们解决具体问题，这取决于我们。我们面临的挑战并不都需要市场上最大、最昂贵的LLM。因此，花时间尝试不同的模型、比较它们的输出，然后自己做出判断是很重要的。例如，OpenAI的GPT模型在代码示例方面的表现优于Google
    Gemini。这些细节是通过实验和观察发现的。
- en: The criteria we’ve explored are by no means an exhaustive list, but they demonstrate
    that there is more to consider about LLMs once we get past the initial glamour
    of how they respond. Different LLMs perform differently, helping us with various
    challenges. So let’s take a look at some of the more popular currently available
    models and platforms.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所探讨的标准绝对不是详尽的列表，但它们表明，一旦我们超越了LLMs最初的光环，还有更多关于LLMs需要考虑的地方。不同的LLMs表现不同，帮助我们应对各种挑战。因此，让我们来看看一些目前更受欢迎的模型和平台。
- en: 2.5.2 Examining popular LLMs
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 检查流行的LLMs
- en: Since OpenAI’s launch of ChatGPT, there has been an explosion in releases of
    LLMs from various organizations. It’s not to say that these models and related
    work weren’t around before ChatGPT’s release, but the public focus has certainly
    intensified, and an increasing number of marketing and release announcements have
    focused on companies releasing their LLM offerings. Here are some of the more
    common/popular LLMs that have been released since the end of 2022.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 自从OpenAI发布ChatGPT以来，各种组织发布的LLMs数量激增。这并不是说这些模型和相关工作在ChatGPT发布之前不存在，但公众的关注确实加剧了，越来越多的营销和发布公告都集中在发布LLM产品的公司。以下是一些自2022年底以来发布的更常见/流行的LLMs。
- en: Keeping up with LLMs
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪大型语言模型（LLMs）
- en: It’s worth noting that the situation with the launch of LLMs and their related
    features is extremely fluid and has grown at quite a fast pace. Therefore, it’s
    likely that some of what we’ll explore will differ from the time of writing in
    mid-2024 to the time you are reading this book. Fortunately, sites such as LLM
    Models ([https://llmmodels.org/](https://llmmodels.org/)) share up-to-date lists
    to review. However, this list demonstrates that some of the bigger names in the
    LLM space are worth exploring.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，LLMs及其相关功能的发布情况极其灵活，发展速度相当快。因此，我们可能会发现，从2024年中期写作时到您阅读这本书的时间，我们将探索的内容可能会有所不同。幸运的是，像LLM
    Models ([https://llmmodels.org/](https://llmmodels.org/))这样的网站分享了最新的列表以供审查。然而，这个列表表明，LLM领域的一些知名公司值得探索。
- en: OpenAI
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI
- en: At the time of writing, OpenAI is the most ubiquitous of organizations offering
    LLMs for use. Although OpenAI has been working on LLM models for quite some time,
    releasing their GPT-3 model in 2020, it was their release of ChatGPT in November
    2022 that kick-started the popular wave of interest and use of LLMs.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在写作时，OpenAI是提供LLMs使用的最普遍的组织。尽管OpenAI已经研究LLM模型有一段时间了，2020年发布了GPT-3模型，但直到2022年11月发布ChatGPT，才引发了人们对LLMs的兴趣和使用的热潮。
- en: OpenAI offers a range of different LLM models, but the two that stand out are
    GPT-3.5-Turbo and GPT-4o, which you can learn about more at [https://platform.openai.com/docs/models/overview](https://platform.openai.com/docs/models/overview).
    These two models are used as *foundation* models or models that can be trained
    further for specific purposes, for a range of products such as ChatGPT, GitHub
    Copilot, and Microsoft Bing AI.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI提供了一系列不同的LLM模型，但最突出的是GPT-3.5-Turbo和GPT-4o，您可以在[https://platform.openai.com/docs/models/overview](https://platform.openai.com/docs/models/overview)了解更多信息。这两个模型被用作*基础*模型或可以进一步训练以用于特定目的的模型，用于ChatGPT、GitHub
    Copilot和Microsoft Bing AI等一系列产品。
- en: In addition to their models, OpenAI has offered a range of features such as
    API access to their direct GPT-3.5-Turbo and GPT-4 models and a collection of
    apps that integrate with ChatGPT (if you subscribe to their plus membership).
    It’s by far the most popular LLM (for now) and has kick-started a race with organizations
    to release their own LLMs. Although we’ve already explored some prompts with ChatGPT,
    you can always access and experiment with ChatGPT at [https://chatgpt.com/](https://chatgpt.com/).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 除了他们的模型，OpenAI还提供了一系列功能，例如直接访问他们的GPT-3.5-Turbo和GPT-4模型的API，以及与ChatGPT集成的应用程序集合（如果您订阅了他们的plus会员）。到目前为止，它是最受欢迎的LLM（至少目前是这样），并引发了一波组织发布他们自己的LLM的竞赛。尽管我们已经探索了一些与ChatGPT相关的提示，但您始终可以访问并实验ChatGPT，网址为[https://ChatGPT.com/](https://ChatGPT.com/)。
- en: Sticking with OpenAI
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 坚持使用OpenAI
- en: Although there are many different LLMs that I encourage you to use, for the
    sake of consistency, we will stick with ChatGPT-3.5-Turbo. It’s not necessarily
    the most powerful LLM at this time, but it is the most ubiquitous—and free. That
    said, if you want to try out these prompts with other LLM models, feel free to
    do so. However, keep in mind that their responses will likely differ from what
    is shared in this book.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多不同的LLM（大型语言模型）我鼓励您使用，但为了保持一致性，我们将坚持使用ChatGPT-3.5-Turbo。它不一定是最强大的LLM，但它是分布最广泛的——而且是免费的。话虽如此，如果您想尝试使用其他LLM模型来测试这些提示，请随意。然而，请记住，它们的响应可能会与本书中分享的内容不同。
- en: Gemini
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: Gemini
- en: Unsurprisingly, Google also has a stake in the Generative AI market with their
    own range of LLM models known as Gemini. Currently at the time of writing their
    most powerful model is Gemini 1.5 Pro, but they also offer other versions of their
    models such as Gemini 1.5 Flash and Gemini 1.0 Pro. Given it's Google the parameter
    counts for each model is not public knowledge, but they are relatively comparable
    in performance to other LLM models.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 无怪乎，Google也在生成式AI市场中占有一席之地，他们有一系列名为Gemini的LLM模型。截至撰写本文时，他们最强大的模型是Gemini 1.5
    Pro，但他们还提供其他版本的模型，如Gemini 1.5 Flash和Gemini 1.0 Pro。鉴于是Google，每个模型的参数数量不是公开信息，但它们与其他LLM模型在性能上相对可比较。
- en: Similar to OpenAI, Google offers access to their Gemini models via their Google
    Cloud platform ([https://ai.google.dev/](https://ai.google.dev/)) and has recently
    started offering apps that work similarly to OpenAI’s ChatGPT apps, with the added
    integration into other Google Suite tools such as Google Drive and Gmail. You
    can access and experiment with Gemini at [https://gemini.google.com/app](https://gemini.google.com/app).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 与OpenAI类似，Google通过他们的Google Cloud平台提供对Gemini模型的访问（[https://ai.google.dev/](https://ai.google.dev/）），并且最近开始提供类似于OpenAI
    ChatGPT应用程序的应用程序，增加了与其他Google Suite工具（如Google Drive和Gmail）的集成。您可以在[https://gemini.google.com/app](https://gemini.google.com/app)访问并实验Gemini。
- en: LLaMa
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMa
- en: 'LLaMa, which is the name for a collection of models, was first released by
    Meta in July 2023\. What sets LLaMa apart from OpenAI’s GPT models and Google’s
    Gemini is that LLaMa is open source. In addition to the open source license, LLaMa
    comes in a range of sizes: 8 and 70 billion parameters, respectively. The combination
    of these sizes and their access means that LLaMa has been adopted by the AI community
    as a popular foundational model. The flip side of this access, though, is that
    Meta doesn’t provide a public platform to train and run versions of LLaMa. So,
    data sets and infrastructure must be personally sourced for use.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMa这个名字代表一系列模型，首次由Meta在2023年7月发布。LLaMa与OpenAI的GPT模型和Google的Gemini模型的不同之处在于LLaMa是开源的。除了开源许可外，LLaMa还提供多种大小：分别是80亿和700亿参数。这些大小及其可访问性意味着LLaMa已被AI社区采纳为流行的基础模型。然而，这种可访问性的另一面是，Meta不提供公共平台来训练和运行LLaMa的版本。因此，必须个人获取数据集和基础设施才能使用。
- en: 'More details on LLaMa can be found at the following links:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 关于LLaMa的更多详细信息可以在以下链接中找到：
- en: '[https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)'
- en: '[https://www.llama.com/llama-downloads/](https://www.llama.com/llama-downloads/)'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.llama.com/llama-downloads/](https://www.llama.com/llama-downloads/)'
- en: Hugging Face
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face
- en: Unlike the other entries in our list, Hugging Face offers no proprietary model
    but instead facilitates an AI community that contains a wide variety of different
    models, most of which are open source. Looking at their index page of models available
    at [https://huggingface.co/models](https://huggingface.co/models), we can see
    hundreds of thousands of differently trained models that have come from different
    companies and research labs. Hugging Face also offers datasets for training, apps,
    and documentation that allows the reader to dive deeper into how models are built.
    All of these resources are available so that the AI community can access pretrained
    models, tweak them, and further train them for a specific use, which is something
    that we’ll be exploring further in part 3 of this book.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们列表中的其他条目不同，Hugging Face不提供专有模型，而是促进了一个包含各种不同模型的AI社区，其中大多数是开源的。查看他们提供的模型索引页[https://huggingface.co/models](https://huggingface.co/models)，我们可以看到来自不同公司和研究实验室的数十万个不同训练的模型。Hugging
    Face还提供了用于训练的数据集、应用程序和文档，允许读者深入了解模型是如何构建的。所有这些资源都是为了使AI社区能够访问预训练模型，对其进行调整，并针对特定用途进行进一步训练，这是我们将在本书的第三部分进一步探讨的内容。
- en: The marketplace for LLMs has sizably grown in a short amount of time, both commercially
    and in open source, and similar to other areas of software development, being
    proactive in what new LLMs are appearing can be beneficial. However, it can also
    be overwhelming and not necessarily feasible to keep up with everything that is
    happening at once. So, instead of attempting to keep abreast of all the comings
    and goings in the AI community, we can opt to explore LLMs when we want to use
    LLMs to solve specific problems. Having a problem can help frame our criteria
    around which tools work best for us.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的市场在短时间内显著增长，无论是商业上还是开源领域，这与其他软件开发领域类似，积极关注新出现的LLM可能是有益的。然而，这可能会让人感到不知所措，并且不一定能够跟上所有同时发生的事情。因此，我们不妨选择在我们想要使用LLM解决特定问题时去探索LLM。拥有问题可以帮助我们确定哪些工具最适合我们。
- en: Activity 2.7
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 活动二.7
- en: Either select an earlier prompt from this chapter or create one of your own
    and submit it to different LLMs. Note how each responds and compares. Do some
    of them feel more conversational? How do they handle receiving or sending code
    examples? Which ones provide the best response in your opinion?
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 要么选择本章中的一个早期提示词，要么创建自己的提示词并提交给不同的LLM。注意每个LLM的响应，并进行比较。其中一些是否感觉更像是对话？它们如何处理接收或发送代码示例？在你看来，哪一些提供了最好的响应？
- en: 2.6 Creating a library of prompts
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 创建提示词库
- en: 'One of the benefits of prompts is that once created, they can be used repeatedly.
    Consequently, a lot of collections of prompts for different roles and tasks are
    appearing online. For example, here are a few collections that I’ve seen shared
    recently:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 提示词的一个好处是，一旦创建，就可以重复使用。因此，针对不同角色和任务的提示词集合在网上大量出现。例如，以下是一些我最近看到的分享的集合：
- en: Awesome ChatGPT Prompts, GitHub ([https://github.com/f/awesome-chatgpt-prompts](https://github.com/f/awesome-chatgpt-prompts))
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Awesome ChatGPT Prompts，GitHub ([https://github.com/f/awesome-ChatGPT-prompts](https://github.com/f/awesome-ChatGPT-prompts))
- en: 50 ChatGPT Prompts for Developers, Dev.to ([https://mng.bz/w5D7](https://mng.bz/w5D7))
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 50个面向开发者的ChatGPT提示词，Dev.to ([https://mng.bz/w5D7](https://mng.bz/w5D7))
- en: ChatGPT Cheat Sheet, Hackr.io ([https://mng.bz/q0PK](https://mng.bz/q0PK))
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ChatGPT速查表，Hackr.io ([https://mng.bz/q0PK](https://mng.bz/q0PK))
- en: This list is not at all exhaustive, and the sample collections aren’t necessarily
    related to testing, but they are worth looking through to learn how others have
    created prompts, as well as giving us the opportunity to determine which prompts
    would be effective and which wouldn’t.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表远非详尽无遗，样本集合也不一定与测试相关，但它们值得浏览，以了解他人是如何创建提示词的，同时也为我们提供了确定哪些提示词有效、哪些无效的机会。
- en: Although prompt collections shared publicly can be useful, it’s likely we’ll
    end up creating prompts that are used for specific contexts. So, it’s worthwhile
    getting into the habit of storing prompts that prove to be beneficial in some
    sort of repository for us and others to quickly use. Where you store these will
    depend on what and who they are used for. If they’re for public use, then sharing
    a repository of prompts or adding to existing collections might be valuable. If
    we’re creating and using them while developing company products, then we need
    to treat them in the same way as our production code and store them somewhere
    private so that we don’t violate any policies around intellectual property. Finally,
    we may also consider version control so that we can tweak and track prompts as
    we learn more about working with LLMs and as the LLMs themselves evolve.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然公开共享的提示集合可能很有用，但我们很可能会创建出适用于特定情境的提示。因此，养成将有益的提示存储在某种类型的存储库中，以便我们和其他人可以快速使用的习惯是值得的。你将存储这些提示的位置将取决于它们的使用目的和对象。如果是供公众使用，那么共享提示存储库或添加到现有集合中可能是有价值的。如果我们是在开发公司产品的同时创建和使用它们，那么我们需要像对待我们的生产代码一样对待它们，并将它们存储在私密的地方，以免违反任何关于知识产权的政策。最后，我们还可以考虑版本控制，这样我们就可以随着我们对与LLMs合作的学习以及LLMs本身的演变来调整和跟踪提示。
- en: Wherever they are stored, the idea is to create a repository of prompts that
    are quick and easy to access so that once a prompt has been created for a specific
    activity, it can be reused multiple times rapidly so that we can get as much value
    from them to improve our productivity.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 无论它们存储在哪里，目的是创建一个提示存储库，这些提示快速且易于访问，以便一旦为特定活动创建了一个提示，就可以快速多次重用，从而我们可以从它们中获得尽可能多的价值来提高我们的生产力。
- en: Activity 2.8
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 活动二.8
- en: Create a space where you can store future prompts for you and your team to use.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个可以存储未来提示的空间供你和你的团队使用。
- en: Using prompts from this book
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 使用本书中的提示
- en: In the spirit of storing prompts for future use and to help you, the reader,
    with trying out the prompt examples in this book, you can find each prompt example
    at [https://mng.bz/75mx](https://mng.bz/75mx).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 为了存储提示以备将来使用，并帮助读者尝试本书中的提示示例，你可以在[https://mng.bz/75mx](https://mng.bz/75mx)找到每个提示示例。
- en: This will enable you to quickly copy and paste the prompts into your chosen
    LLM as we go through each chapter, saving you the task of having to type the whole
    prompt manually. There will be sections in certain prompts where you will need
    to add your own custom content or context to use them. To make them clear, instructions
    on what is required to add to the prompt are provided in the prompt and will be
    formatted in all caps and inside square brackets.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使你能够在我们浏览每一章时快速复制和粘贴提示，从而节省你手动输入整个提示的任务。在某些提示的部分，你可能需要添加你自己的自定义内容或上下文才能使用它们。为了使它们清晰，提示中提供了需要添加到提示中的说明，并将以大写字母和方括号格式呈现。
- en: 2.7 Solving problems by using prompts
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.7 通过使用提示解决问题
- en: The tactics and tooling we’ve learned about in this chapter help provide us
    with a framework to use LLMs and design specific prompts for specific testing
    activities. We should be mindful, though, that although these tactics improve
    our chances of getting desired results, they are not foolproof. For example, when
    we ask an LLM to evaluate its output, the LLM isn’t evaluating its output like
    a traditional application might. It’s simply moving the predictive needle further
    toward an output that aligns with our requirements.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中学到的策略和工具可以帮助我们构建一个框架，用于使用大型语言模型（LLMs）并为特定的测试活动设计特定的提示。然而，我们应该意识到，尽管这些策略提高了我们获得期望结果的机会，但它们并不是万无一失的。例如，当我们要求一个LLM评估其输出时，LLM并不是像传统应用程序那样评估其输出。它只是简单地将预测指针进一步移动到符合我们要求的结果。
- en: Single prompting vs. multi-prompting
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 单一提示与多提示
- en: Throughout this chapter, we’ve explored how to use principles and tactics to
    create individual prompts that are as effective as possible at maximizing desired
    output from an LLM. However, tools such as ChatGPT, Gemini, and Claude allow us
    to conduct conversations with LLMs, and the history of the conversations influences
    the output of future responses in said conversation. This raises the question
    of whether it would be easier to try multiple prompts in a conversation to tweak
    output. Although this can be effective, we do run the risk that the longer a conversation
    progresses, the higher the risk of hallucinations occurring as an LLM attempts
    to overfit responses to our requests. This is why tools such as BingAI are limited
    in the number of responses they can give in a conversation. However, more importantly,
    more doesn’t necessarily mean better. The garbage in, garbage out rule is valid
    for both single and multiple prompts. Relying on multiple prompts in one conversation
    means we become less clear and precise in what we are asking for, which adds delays
    and increases hallucination, thus negating the value of using an LLM in the first
    place. In conclusion, whether we want to send a single prompt to get what we want
    or send multiple prompts, adopting the principles and tactics created by Isa Fulford
    and Andrew Ng will increase our productivity with LLMs.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了如何运用原则和策略来创建尽可能有效的个人提示，以最大化从LLM（大型语言模型）中获得期望的输出。然而，像ChatGPT、Gemini和Claude这样的工具使我们能够与LLM进行对话，而对话的历史会影响该对话中未来响应的输出。这引发了一个问题：在对话中尝试多个提示以调整输出是否会更简单。尽管这可能有效，但我们确实面临着一个风险，那就是随着对话的进行，LLM试图过度拟合我们的请求，从而发生幻觉的风险会越来越高。这就是为什么像BingAI这样的工具在对话中可以给出的响应数量有限。然而，更重要的是，更多并不一定意味着更好。垃圾输入，垃圾输出的规则既适用于单个提示也适用于多个提示。在单个对话中依赖多个提示意味着我们在请求时变得不那么清晰和精确，这增加了延迟并增加了幻觉，从而抵消了最初使用LLM的价值。总之，无论我们是想发送单个提示以获得我们想要的，还是发送多个提示，采用Isa
    Fulford和Andrew Ng创建的原则和策略都将提高我们在LLM上的生产力。
- en: Therefore, it is necessary to develop the skills to write prompts that help
    us solve our problems effectively and in a way that doesn’t diminish the time
    saved using LLMs (for example, we don’t want to spend hours tweaking prompts).
    This means being able to identify specific problems that LLMs can help with and
    then utilizing prompt engineering to maximize the chances of extracting valuable
    information from an LLM. This is what we’ll explore throughout the rest of this
    book—when and how to use LLMs.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，有必要培养编写提示的技能，帮助我们有效地解决问题，并且不会减少使用LLM节省的时间（例如，我们不希望花费数小时调整提示）。这意味着能够识别LLM可以帮助解决的具体问题，然后利用提示工程来最大化从LLM中提取有价值信息的机会。这就是本书余下部分将要探讨的——何时以及如何使用LLM。
- en: As we progress, we’ll also learn that prompts come in many shapes and sizes.
    Throughout this chapter, we’ve looked at prompts that are manually written by
    us humans. But, as we’ll learn, tools such as GitHub Copilot auto-generate prompts
    as we write our code. That doesn’t mean we can’t still infuse the principles and
    tactics into our ways of working, but it does take time, awareness, and practice
    to develop.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们不断前进，我们还将了解到提示有多种形状和大小。在本章中，我们查看了我们人类手动编写的提示。但，正如我们将学到的，GitHub Copilot等工具在我们编写代码时会自动生成提示。这并不意味着我们不能再将原则和策略融入我们的工作方式中，但这确实需要时间、意识和实践来发展。
- en: Activity 2.9
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 活动2.9
- en: Before you continue reading this book and learn about different types of prompts
    for different testing activities, use the knowledge from chapters 1 and 2, consider
    a specific testing task that you do, and try to build a prompt that can help you
    with your work.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续阅读本书并了解不同测试活动的不同类型提示之前，利用第1章和第2章的知识，考虑一个你做的特定测试任务，并尝试构建一个可以帮助你工作的提示。
- en: Summary
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: LLMs are trained on vast amounts of data using sophisticated algorithms to analyze
    our requests and predict an output.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM使用复杂的算法在大量数据上进行训练，以分析我们的请求并预测输出。
- en: The predictive nature of LLMs makes them quite adaptable but also means they
    come with some risks.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM的预测性使它们相当适应，但也意味着它们带来了一些风险。
- en: LLMs can sometimes output *hallucinations,* or text that sounds authoritative
    and correct when, in fact, it is false.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM有时会输出*幻觉*，或者听起来权威且正确，但实际上是错误的文本。
- en: The data that LLMs are trained on may contain errors, gaps, and assumptions,
    and we must keep this in mind when using them.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM训练所使用的数据可能包含错误、缺失和假设，我们在使用它们时必须牢记这一点。
- en: We must also be mindful of the data we share with LLMs so as not to cause unauthorized
    leaks of business or user information.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们还必须注意与我们分享给LLM的数据，以免造成业务或用户信息的未经授权泄露。
- en: Prompt engineering is a collection of principles and tactics used to maximize
    the chances of an LLM returning a desired output.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程是一系列用于最大化语言模型（LLM）返回所需输出的概率的原则和策略。
- en: We can use the knowledge that LLMs are predictive in nature and benefit from
    it by implementing prompt engineering.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以利用LLM本质上是预测性的这一知识，通过实施提示工程从中受益。
- en: Using delimiters can help us clarify instructions and parameters in a prompt.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分隔符可以帮助我们澄清提示中的指令和参数。
- en: An LLM can output data in various formats, but it requires us to explicitly
    state which structure format we want in a prompt.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLM可以以各种格式输出数据，但它需要我们明确指出在提示中我们想要哪种结构格式。
- en: We can reduce hallucinations from LLMs by using the check-for-assumption tactic.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用检查假设的策略，我们可以减少LLM的幻觉。
- en: Providing examples in a prompt can help ensure that an LLM provides an output
    in a desired format or context.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示中提供示例可以帮助确保LLM以所需格式或上下文提供输出。
- en: Specifying specific subtasks in a prompt can help an LLM process complex tasks
    successfully.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示中指定具体的子任务可以帮助LLM成功处理复杂任务。
- en: Asking LLMs to evaluate solutions to problems can also reduce errors and maximize
    outcomes.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求LLM评估问题的解决方案也可以减少错误并最大化结果。
- en: Knowing when to use LLMs and developing skills with prompt engineering is the
    key to success, regardless of the tool we use.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知道何时使用LLM以及通过提示工程发展技能是成功的关键，无论我们使用什么工具。
