- en: 2 Large language models and prompt engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: The fundamentals of how large language models work
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The risks of using large language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A definition of prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimenting with prompt engineering to return various outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to solve problems using prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous chapter, we learned that it’s important to take time and familiarize
    ourselves with new tools, and it’s the very same mindset we’ll be adopting in
    this chapter. Throughout this book, we’ll be exploring how to use generative AI
    tools such as Open AI’s ChatGPT and GitHub Copilot, which are built on large language
    models, or LLMs. There are many ways in which AI can be employed in testing, but
    what makes LLMs so interesting is their adaptability to different situations—hence,
    their rise in popularity. So, before we look at how we can incorporate LLM tools
    into our everyday testing, let’s first learn a bit about what LLMs are, how they
    work, and how to get the most out of them by learning about the concept of prompt
    engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 'What has made LLMs such as ChatGPT dominate tech headlines throughout 2023?
    Consider this sample interaction with ChatGPT that I had:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Hello ChatGPT, can you introduce yourself briefly? |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Of course! I’m ChatGPT, an AI language model developed by OpenAI. I’m based
    on the GPT-3.5 architecture, which means I’m designed to understand and generate
    human-like text based on the prompts I receive. I’ve been trained on a diverse
    range of data sources, including books, articles, websites, and other text documents,
    up until September 2021\. I can assist you with a wide array of topics, including
    answering questions, providing explanations, creative writing, and much more.
    Just let me know how I can help you! |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Nice to meet you. |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Nice to meet you too! I’m here to assist you with any questions or tasks
    you have. How may I help you today? |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| I’m just getting to know you. |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| That’s perfectly fine! Feel free to ask me anything you’d like to know, whether
    it’s about my capabilities, how I work, or anything else you’re curious about.
    I’m here to provide information and help you out to the best of my abilities.
    Fire away with your questions! |'
  prefs: []
  type: TYPE_TB
- en: Based on this quick conversation, we can see that
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to interact with ChatGPT using natural language. No traditional
    programming experience was required to get results from ChatGPT in this example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output from ChatGPT is also in natural language. It’s easy to understand
    and react to.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advocates of LLMs are celebrating that these types of AI tools have democratized
    the use of AI, allowing anyone to use it to get results. However, this democratization
    is a double-edged sword. The nature in which we interact with LLMs can give us
    the illusion that we’re talking with a machine that reasons in the same way we
    humans do. But making that assumption can affect our ability to get the most out
    of an LLM. So, to get the best results out of tools such as ChatGPT, we should
    know how they work (at least in general terms) to better understand how they can
    fit into our testing activities and how to extract the most value from them.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 LLMs explained
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How does a person with relatively little experience in building AI systems explain
    how a complex LLM system works? Fortunately, in the Computerphile video “AI Language
    Models & Transformers” ([www.youtube.com/watch?v=rURRYI66E54](https://www.youtube.com/watch?v=rURRYI66E54)),
    Rob Miles offers an example that can help us gain a fundamental grasp on what
    LLMs do. (I strongly recommend watching all his videos on AI.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Take out your phone and open a messaging app, or any other app that makes your
    keyboard appear. Above the keyboard, you’ll likely see a range of suggested words
    to insert into your message. For example, my keyboard offers the following suggestions:
    *I,* *I am,* and *The.* Selecting one of these options, such as *I am,* causes
    the suggestions to update. For me, it offered the options *away,* *away for,*
    and *now.* Selecting the option *away* *for* once again updates the available
    options. So, how does the keyboard know which options to show?'
  prefs: []
  type: TYPE_NORMAL
- en: In your keyboard, there is an AI model that behaves in a manner resembling LLMs.
    This description is an oversimplification, but at its core, the keyboard on your
    phone is applying the same machine learning approach as an LLM by using probability.
    Language is a complex and fluid set of rules, meaning any attempt to codify relationships
    explicitly is almost impossible. So instead, a model is trained on massive data
    sets to implicitly learn the relationships in language and create a probability
    distribution that is used to predict what the next word might be. This can best
    be described by visualizing the options available from the keyboard example, as
    shown in figure 2.1
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH02_F01_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 Probability distribution in action
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, when we select the term *I am,* the model in our keyboard has
    been trained to assign probabilities to a vast range of words. Some of these will
    have a high probability of coming after *I am*, such as *away*, and some will
    have a low probability, such as *sandalwood*. As mentioned before, these probabilities
    come from a model that has completed a training process, known as unsupervised
    learning, in which vast amounts of data have been sent to an algorithm for processing.
    It’s from that training process that a model is created with complex weights and
    balances that provide the model with its predictive abilities.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning and unsupervised learning
  prefs: []
  type: TYPE_NORMAL
- en: When training AI, two of the more dominant techniques to use are supervised
    and unsupervised learning. Which learning approach is used will determine how
    data has been structured and sent to an algorithm. *Supervised* learning uses
    data that has been organized, labeled, and paired with an output. For example,
    a medical data set might contain labeled data that includes BMI, age, and sex,
    for example, which is paired with a labeled outcome about whether an individual
    suffered a specific illness—say, a heart attack or stroke. *Unsupervised* learning,
    conversely, uses data that isn’t labeled, and it has no output data. The idea
    is that when an algorithm is trained on this type of data, it learns the implicit
    patterns within the data.
  prefs: []
  type: TYPE_NORMAL
- en: Chances are good that if you play around with the predictive function on your
    keyboard, the output will differ from mine—even if we have the same phone and
    operating system. This is because once the model has been trained and is utilized
    in our phones, it’s still being fine-tuned by what we type into our phones. I
    travel for work, so I must let people know when I am away and when I’m available.
    (It is perhaps a damning indictment of my work–life balance!) So, words such as
    *I am* and *away* have an increased probability as they are words I use more regularly.
    This is known as Reinforcement Learning with Human Feedback, or RLHF.
  prefs: []
  type: TYPE_NORMAL
- en: Again, comparing predictive messaging on a phone to an LLM is an oversimplification,
    but the comparison holds true. LLMs also use unsupervised learning and RLHF. The
    difference, however, is that although an AI model on a phone can look at perhaps
    the last five words typed to predict the next, LLMs use cutting-edge techniques,
    such as
  prefs: []
  type: TYPE_NORMAL
- en: Generative pretrained transformers (which is what makes the GPT abbreviation
    in ChatGPT)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Powerful hardware infrastructure using thousands of servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training data on a scale that would dwarf what our humble keyboard model will
    have been trained on
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do we need to know the intricacies of each of these points? Not really, but
    it helps us appreciate a key aspect of LLMs. The output of LLMs, no matter how
    powerful, is probabilistic. LLMs are not a repository of information—there is
    structured knowledge stored within them like we would see on the wider internet.
    This means that how it comes to conclusions differs from how we humans do (probability
    rather than experience), which is what makes them so powerful but also risky to
    use if we aren’t vigilant about how we use them.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Avoiding the risks of using LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having an AI predict what word goes after another isn’t an easy task, and although
    current LLMs have seen an explosion in ability, there are risks we need to be
    aware of. Let’s take a look at a few of them.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Hallucinations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The challenge with text prediction is to ensure that the output of an LLM makes
    sense and is rooted in reality. For example, back in chapter 1, when I asked ChatGPT
    to write an introduction to this book, it shared the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Regarding the book, “How AI can be used to help support various software
    testing activities,” I would recommend “AI-Driven Testing: Adding Intelligence
    to Your Software Testing Practice” by Julian Harty and Mahesh Sharma. |'
  prefs: []
  type: TYPE_TB
- en: Initially, when LLMs were being developed, their output didn’t make much sense.
    The text would be readable, but it lacked structure or grammatical sense. If we
    read this example, it parses perfectly well, and it makes sense. However, as I
    mentioned, the book that ChatGPT describes doesn’t exist. In the context of an
    LLM, this is known as a *hallucination.* The LLM is able to output a clear statement
    in a way that grants it some authority, but what has been written is false.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM hallucinations can be caused by a range of factors, such as the quality
    of the data it was trained on, the degree to which a model has been trained on
    data (overfitting), or a model’s propensity for giving an answer regardless of
    whether it is correct. One of the challenges of working with LLMs is that they
    act like a black box. It’s difficult to monitor how an LLM reached a specific
    conclusion, which is compounded by its indeterminate nature. Just because I got
    an output that contained a hallucination doesn’t mean that others will do the
    same in the future. (This is where the RLHF helps combat hallucinations: we can
    inform the model whether its output is false, and it will learn from that.)'
  prefs: []
  type: TYPE_NORMAL
- en: The risk of hallucinations means we must always maintain an element of skepticism
    when interpreting the output of an LLM. We need to be mindful that what is being
    returned from an LLM is predictive and not always correct. We can’t turn off our
    critical thinking just because a tool appears to be behaving in a way that mimics
    human behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Data provenance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For most LLM users, it’s not just how the model works precisely that is a black
    box to us, but also the data it has been trained on. Since ChatGPT’s explosion
    in popularity, the conversation around data ownership and copyright has intensified.
    Companies such as X (formerly known as Twitter) and Reddit have accused OpenAI
    of stealing their data wholesale, and at the time of writing, a class action lawsuit
    against OpenAI has been filed by a collection of authors who accuse the company
    of breaching copyright law by training models on their works ([https://mng.bz/1aBZ](https://mng.bz/1aBZ)).
  prefs: []
  type: TYPE_NORMAL
- en: The results from these debates are yet to be seen, but if we bring this topic
    back to the world of software development, we must be mindful of what material
    an LLM has been trained on. For example, ChatGPT, at one point, would return nonsensical
    responses when specific phrases were sent to it, all because it had been trained
    on data from the subreddit r/counting, which is full of data that is seemingly
    nonsensical itself. You can learn more about this weird behavior from Computerphile
    at [www.youtube.com/watch?v=WO2X3oZEJOA](https://www.youtube.com/watch?v=WO2X3oZEJOA)).
    If an LLM has been trained on garbage, it will output garbage.
  prefs: []
  type: TYPE_NORMAL
- en: This becomes important when we consider tools such as GitHub Copilot, which
    uses the same GPT model that ChatGPT uses. Copilot has been fine-tuned differently,
    using the billions of lines of code stored in GitHub so that it can act as an
    assistant and suggest code snippets as we develop our codebase. We’ll explore
    in later chapters how we can put Copilot to good use, but again, we should be
    critical of what it suggests and not blindly accept everything it offers as a
    suggestion. Why? Ask yourself, are you happy with the code you’ve created in the
    past? Do you trust all the code others have created? If a large population of
    engineers is prone to implementing bad patterns, then that is what tools like
    Copilot will have been trained on. The point is a little hyperbolic because a
    lot of good developers and testers out there do good work—good work that Copilot
    is trained on. But it’s a thought exercise worth considering every now and then
    just to ensure that we remember who is in the driver’s seat when building applications
    with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Data privacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just as we need to be mindful of what an LLM outputs, we also have to consider
    what we enter into it. The temptation to share material with LLMs to find answers
    to problems we’re facing will be strong. But we have to ask ourselves, where is
    the data we send being stored? As mentioned earlier, LLMs are being tweaked continuously
    through the RLFH feedback. Companies such as OpenAI and GitHub will take the information
    we share, store it, and use it for future model training (GitHub does offer some
    privacy controls over what it can store, though).
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be problematic when working for companies (or for ourselves) who want
    to keep their intellectual property private. Take Samsung, for example, whose
    employees accidentally leaked confidential material through the use of ChatGPT,
    as described here by TechRadar (you can read about it at [https://mng.bz/PN52](https://mng.bz/PN52)):'
  prefs: []
  type: TYPE_NORMAL
- en: The company allowed engineers at its semiconductor arm to use the AI writer
    to help fix problems with their source code. But in doing so, the workers entered
    confidential data, such as the source code itself for a new program and internal
    meeting notes data relating to their hardware.
  prefs: []
  type: TYPE_NORMAL
- en: As the adoption of LLMs begins to increase across organizations, we may begin
    to see an increase in policies that restrict what we can and can’t use LLMs for.
    Some may ban the use of third-party LLMs, and some organizations will opt to train
    and deploy their own internal LLMs for internal use (a topic we will explore in
    part 3). The result of those decisions will be highly contextual, but they will
    affect what type of LLMs we use and what data we can and cannot send, underlying
    our need to be mindful of what we send to LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also important to keep customer privacy in mind as we have an obligation
    not only to the companies we work for (especially for those who sign nondisclosure
    agreements) but also to our users. We have a legal and moral duty to protect user
    data from being spread into the wild, where we have no oversight.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, although LLMs provide a wealth of opportunities, we must avoid
    the trap of anthropomorphizing them. Treating LLMs as if they have come to conclusions
    in the same way as we humans do is a fallacy. It can entrench a level of trust
    in the output that is dangerous and likely means that we aren’t getting the most
    benefit out of them. However, if we learn to use the probabilistic nature of LLMs
    when we instruct them, we can increase our chances of creating outputs that can
    help us improve efficiency—which is where prompt engineering can help us.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Improving results with prompt engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We use natural language to prompt an LLM to return a desired outcome, but because
    they are probabilistic, we can communicate with them in a way that differs from
    normal interaction with humans. As LLMs have developed, a new field of engineering
    has appeared known as *prompt engineering,* which contains a collection of patterns
    and techniques that we can use to increase the likelihood of getting a desired
    output from an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: What is a prompt?
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, we’ll use the term *prompt* regularly as this will be our primary
    means of communicating with LLMs. When we use this term, we are simply referring
    to the natural language input that is sent to an LLM. For example, in the first
    example of this chapter, I sent the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)   Hello ChatGPT, can you introduce yourself
    briefly?'
  prefs: []
  type: TYPE_NORMAL
- en: 'which prompted the LLM to return this response to me:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)   Hello! I’m ChatGPT, an AI language
    model developed by OpenAI. I can help with answering questions, providing information,
    brainstorming ideas, and more. Feel free to ask me anything!'
  prefs: []
  type: TYPE_NORMAL
- en: In the coming chapters, we will use prompt engineering heavily to trigger LLMs
    to deliver a range of useful content for various testing activities. But before
    we begin, it’s worthwhile learning the fundamentals of prompt engineering so that
    we can see how prompts are built to maximize output from LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help us better understand what prompt engineering is, consider these two
    prompts sent to an LLM. The first is a general question:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| What risks should I test for when testing a Contact Us form? |'
  prefs: []
  type: TYPE_TB
- en: 'The second is a more detailed prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Assume the role of a professional software tester and suggest a list of risks
    for a Contact Us form using the following constraints:# Focus on the quality characteristics
    of accessibility, compliance and usability# Number each risk# Provide which quality
    characteristic the risk relates to# Use no more than 30 words to describe each
    riskFor example:   1.  Semantic structure - Accessibility - The HTML structure
    of the form should follow WCAG A standards |'
  prefs: []
  type: TYPE_TB
- en: Comparing the two, we can see that the second example is more detailed, with
    explicit requests and examples to outline what we might expect the LLM to return.
    Although the intention is similar, the output from each is drastically different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compare the output from the first example:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Validation and Error Handling: Verify that the form correctly validates user
    input, such as email addresses, phone numbers, and required fields. Test for appropriate
    error messages when users submit incorrect or incomplete information.Security:
    Ensure that the contact form is secure and protected against common vulnerabilities
    such as cross-site scripting (XSS) and SQL injection attacks. Validate that the
    form sanitizes or escapes user input to prevent malicious code execution.Usability:
    Test the form’s usability by evaluating factors such as the clarity of form fields,
    user instructions, and overall user experience. Make sure the form is intuitive,
    easy to navigate, and visually appealing. |'
  prefs: []
  type: TYPE_TB
- en: 'against the second example:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    1.  Keyboard accessibility - Accessibility - Ensure the form can be easily
    navigated and filled out using only a keyboard....   6.  Consistent design - Usability
    - Verify that the form follows a consistent design pattern and matches the overall
    website design.... 13.  Consent checkboxes - Compliance - Ensure any consent checkboxes
    or privacy policy links are clearly presented and function correctly. |'
  prefs: []
  type: TYPE_TB
- en: Imagine that we want to use the proposed risks in each example to guide our
    testing. The first example has output suggestions that are vague and abstract.
    We would still need to do a fair bit of work to break down the large topics, such
    as security risks, whereas, with the second example, we have specific, actionable
    risks that we could use easily. And the goal of using tools such as LLM is to
    *reduce* the workload, not increase it.
  prefs: []
  type: TYPE_NORMAL
- en: Our second prompt yields better results because the instructions it gives have
    been considered and are detailed and clear, which is what prompt engineering is
    essentially about. Although both prompts use natural language, with prompt engineering,
    we are aware of how an LLM works and what we want it to return to understand how
    to write a prompt so that we maximize the chances of a desired outcome. When using
    prompt engineering, we can see that although an LLM communicates in plain language,
    how it processes our request differs from how a human might do so, so we can adopt
    specific techniques to steer an LLM in the direction we want.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Examining the principles of prompt engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As LLMs have developed, so have the patterns and techniques of prompt engineering.
    Many courses and blog posts have been written around prompt engineering, but one
    notable collection of principles, which we’ll explore shortly, has been created
    by Isa Fulford and Andrew Ng and their respective teams. A collaboration between
    OpenAI’s LLM knowledge and Deeplearning.ai’s teaching platform has created a course
    called ChatGPT Prompt Engineering for Developers, which features a series of principles
    and tactics that can be used in prompts to get the most out of LLMs. If you have
    the time, I encourage you to take the short course available at [https://mng.bz/JNGp](https://mng.bz/JNGp)
    ([https://www.promptingguide.ai/](https://www.promptingguide.ai/) is also a useful
    reference.) Although the course references ChatGPT specifically, the principles
    taught there can be applied across many LLMs. So let’s explore these principles
    and tactics to get comfortable with prompting LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.1 Principle 1: Write clear and specific instructions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This first principle might seem obvious at first glance—it’s always sensible
    to provide instructions to others that are clear and specific. However, what this
    principle suggests is that we write prompts that are clear and specific *for an
    LLM.* And that means something different from what might be clear and specific
    to a human. To implement this concept, Fulford and Ng teach four tactics to achieve
    clear and specific prompts: use delimiters, ask for structured output, check for
    assumptions, and use few-shot prompting. In the next few sections, we’ll examine
    each one in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.2 Tactic 1: Use delimiters'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When writing prompts, we may need to provide different content and data that
    serve different purposes. For example, the start of our prompt might include instructions
    on what we want an LLM to produce, whereas the end of our prompt might include
    raw data that we want to process. LLMs can guess our intentions for different
    sections of our prompts, but because our goal is to be as clear as possible, we
    can aid the process by using *delimiters,* which are characters used to separate
    strings, to state our intentions for different parts of our prompt. Take this
    prompt as an example. We open the prompt with a message that sets expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a SQL data generator. Generate five rows of SQL for a MySQL database.
    |'
  prefs: []
  type: TYPE_TB
- en: 'We use the * character to delimit rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    *   The table name is identified with a # sign.   *   Each table column
    is identified with a % sign   *   Each column is described in order of name, data
    type and data options using the &#124; sign   *   If a column data option says
    random, randomize data based on the suggested format and column name |'
  prefs: []
  type: TYPE_TB
- en: 'We then use the #, %, and | delimiters that we set in rules to provide instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Here are the instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'When running this prompt within ChatGPT, I received the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, through the use of delimiters the correct table name `rooms`
    has been added, and the column names, formats, and ranges have been correctly
    randomized. This is made possible by the clear expectations and rules we set that
    are distinguished by various delimiters throughout the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: The delimiters help to make the prompt clearer, but they also make it easy to
    modify. For example, if we want to reconfigure the generated data, we could enter
    another line, such as
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: So, when writing prompts that contain a large amount of data that changes context,
    we can use delimiters to make clear what is being provided in a prompt at a specific
    point.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 2.1
  prefs: []
  type: TYPE_NORMAL
- en: Using the delimiter prompt example, we create new instructions—this time, for
    a booking that would include information about who made the booking, contact details,
    and check-in and check-out dates.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.3 Tactic 2: Ask for structured output'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'One facet of LLMs that makes them useful is that they have the ability to provide
    outputs in structured formats. As we make our way through this book, we’ll explore
    just how useful this ability is, but as a rule, we must always remember that we
    need to be clear in a prompt about what structured format we want to see used.
    Take this prompt as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Create a JSON object with random data that contains the following fields:
    firstname, lastname, totalprice, deposit paid. Also, include an object called
    booking dates that contains checkin and checkout dates. |'
  prefs: []
  type: TYPE_TB
- en: 'We begin the prompt by clearly stating what format we want to see our object
    in, in this case, JSON, before we start outlining the structure of the object.
    When I sent this prompt to ChatGPT, the following result was returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: As the prompt clearly states, the expected format and structure are two distinct
    instructions, meaning we can modify our instructions to quickly change the format
    by sending an additional prompt, such as
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Create this in a YAML format |'
  prefs: []
  type: TYPE_TB
- en: 'Sending this prompt to ChatGPT returned the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Notice the structure is the same (as well as the randomized data). By explicitly
    stating what format we desire, we can instruct an LLM with exactly what format
    we want at a given time and then alternate formats with ease.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 2.2
  prefs: []
  type: TYPE_NORMAL
- en: Attempt to create an object that contains multiple parameters with different
    data types in different formats. Try prompting an LLM to convert your object from
    one format to another—for example, from JSON to XML.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.4 Tactic 3: Check for assumptions'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we learned earlier, LLMs can hallucinate by generating output that contains
    information that is incorrect or disconnected from reality. It goes without saying
    that we want to reduce the risk of producing hallucinations, which is where our
    third tactic, checking for assumptions, can help. LLMs are more likely to hallucinate
    if they are provided with prompts that are focused on edge cases around a problem
    we want to solve. If an LLM is not properly instructed, it’s more likely to make
    a guess at an answer than to outright inform us that it cannot provide a useful
    answer. So, if we want to avoid guesswork, we need to provide instructions in
    our prompt to allow the LLM to bail out if it cannot execute our request. Consider
    the following example. It provides initial instructions for the prompt, which
    includes a delimiter:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You will be provided with a JSON object delimited by three hashes. Extract
    all emails that end with .com and write them out as a list. |'
  prefs: []
  type: TYPE_TB
- en: 'We then check for assumptions and allow the LLM to bail:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| If no email addresses with a .com email address exist, simply write “No .com
    emails found” |'
  prefs: []
  type: TYPE_TB
- en: 'Finally, we provide the data to process using ### as a delimiter:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running this prompt with a collection of email addresses that can be extracted
    returned the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Here are the email addresses that end with “.com”:   *   baveray0@apple.com  
    *   pyandell4@ning.com   *   gpentecost6@yelp.com |'
  prefs: []
  type: TYPE_TB
- en: 'But here’s what happened when I ran the prompt again without email addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You will be provided with a JSON object delimited by three hashes. Extract
    all emails that end with .com and write them out as a list.If no email addresses
    with a .com email address exist, simply write “No .com emails found”'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'I received the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Here is the list of email addresses that end with .com extracted from the
    provided JSON object:No .com emails found |'
  prefs: []
  type: TYPE_TB
- en: 'This response from the LLM was a direct reference to this section of the prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| If no email addresses with a .com email address exist, simply write “No .com
    emails found.” |'
  prefs: []
  type: TYPE_TB
- en: 'which prevented the LLM from providing an output that was incorrect. For example,
    when I ran the prompt without the assumption check, the following was returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Sure! I can help you with that. Here are the email addresses that end with
    “.com” from the provided JSON object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Please note that I have used example.com as a placeholder domain for all the
    email addresses. Replace it with the appropriate domain if you have the actual
    email addresses. |
  prefs: []
  type: TYPE_NORMAL
- en: This is an excellent example of an LLM hallucination. The object we provided
    contained no email addresses, so the LLM used guesswork and incorrectly started
    generating new email addresses based on existing data. However, with the assumption
    check in place, we prevented the hallucination from occurring.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 2.3
  prefs: []
  type: TYPE_NORMAL
- en: Create a prompt that filters specific data out of a list of raw data and then
    outputs the results (for example, filter fruit from a list of different foodstuffs).
    Next, modify the data to include an edge case (for example, incorrect or missing
    data). Observe what the LLM outputs, and then attempt to correct the output by
    adding to the prompt some instructions that follow the check-for-assumptions prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.5 Tactic 4: Few-shot prompting'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we’ve progressed through this chapter and learned new concepts about LLMs
    and prompting, key points have been clarified using examples. They are essential
    tools when it comes to not just teaching but also communicating with one another.
    This is no different for LLMs. *Few-shot prompting* basically means providing
    explicit examples to clarify instructions (The word *few* in this context indicates
    how many examples you share. A prompt with no examples would be a *zero-shot prompt*).
    What follows is a prompt using examples. First, we provide the instructions in
    the prompt outlining the output format:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are an expert exploratory tester. Create three test charters for a booking
    system that focus on the risks around booking dates. The format should follow:
    Explore <Target> using <Resource> to discover <Information> |'
  prefs: []
  type: TYPE_TB
- en: 'We provide examples to clarify the output format for the LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| For example:   *   Explore user permissions using different users to discover
    permission risks   *   Explore browser animations using different devices to discover
    how animations render |'
  prefs: []
  type: TYPE_TB
- en: 'Sending this prompt to ChatGPT returned the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Explore date validation using various date formats to discover potential
    input errors.Explore time zone handling using different geographical locations
    to discover discrepancies in booking times.Explore booking conflicts using overlapping
    reservation requests to discover potential double-booking risks. |'
  prefs: []
  type: TYPE_TB
- en: Consider the format of “Explore <Target> using <Resource> to discover <Information>.”
    It is rule based, but it leans heavily on natural language to communicate the
    rules (unlike JSON or XML structures that rely on explicit delimiter rules). By
    providing examples in our prompt, we can help contextualize what we mean by `<Target>`,
    `<Resource>`, and `<Information>` and reduce the risk of the LLM guessing what
    that means.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 2.4
  prefs: []
  type: TYPE_NORMAL
- en: Create a prompt that uses the few-shot tactic. In your prompt, provide the instructions
    you expect the LLM to follow and then add at least two examples to help guide
    it in providing a desired outcome.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.6 Principle 2: Give the model time to “think”'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It may seem a little unusual, given that we’ve been considering LLMs as probabilistic
    machines and not entities that can think, to see a principle that encourages us
    to give a model time to think. However, the best way to understand this principle
    is that we’re not actually giving an LLM time to think but rather emulating how
    we would give someone time to think about a task in our prompt. In the ChatGPT
    Prompt Engineering for Developers course, Fulford gives a useful analogy as she
    introduces this principle: “If you give a model a task that’s too complex to do
    in a short amount of time, . . . it may make up a guess which is likely to be
    incorrect.”'
  prefs: []
  type: TYPE_NORMAL
- en: She explains that the same situation would happen if humans were given a complex
    task with limited time to respond. We’d rely on educated guesswork and likely
    come up with a less-than-satisfactory answer. Thus, principle 2 offers tactics
    to help us write prompts that break down tasks and encourage LLMs to evaluate
    output to once again maximize the chances of a desired response.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.7 Tactic 1: Specify the steps to complete the task'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first tactic is relatively straightforward once we are confident about
    using the tactics of principle 1 (write clear and specific instructions). By using
    delimiters, we can break a complex task into individual steps for an LLM to take
    to solve the larger task. Take a look at this prompt, where we first notify the
    LLM that a set of instructions is coming, followed by delimited data:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are going to be given a list of instructions to follow. |'
  prefs: []
  type: TYPE_TB
- en: 'We then list each subtask clearly to instruct the LLM on what to do:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    1.  Identify functional risks that might impact the text delimited by
    three hashes   2.  Convert the risks into test charters   3.  Format each charter
    into a Discover <feature> using <resource> to discover <information> format |'
  prefs: []
  type: TYPE_TB
- en: 'Finally, we provide the data we would like to see processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| ###As a userI want to be able to calculate my taxSo I know what tax I have
    to pay### |'
  prefs: []
  type: TYPE_TB
- en: 'When sent to ChatGPT, it returned this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Functional risks:   •   Calculation accuracy: There is a risk that the tax
    calculation may be inaccurate, leading to incorrect tax amounts being calculated.Test
    charters based on the identified risks:   •   Test Charter 1: Validate the accuracy
    of tax calculations by inputting different test scenarios with known tax amounts
    and verifying that the calculated tax matches the expected values.Formatting the
    charters into a “Discover <feature> using <resource> to discover <information>”
    format:   •   Discover tax calculation accuracy using test scenarios with known
    tax amounts to validate the accuracy of the calculated tax. |'
  prefs: []
  type: TYPE_TB
- en: This is a snippet from the output I received. It created many more risks and
    charters, but the first example from each section demonstrates the LLM responding
    to each subtask, one by one, to create an output.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 2.5
  prefs: []
  type: TYPE_NORMAL
- en: Create a prompt that requires a complex task to be carried out. Attempt to break
    out the complex task into multiple subtasks that the LLM can carry out.
  prefs: []
  type: TYPE_NORMAL
- en: '2.4.8 Tactic 2: Instruct the model to work out its own solution first'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our final tactic focuses not on the process of creating an output, but rather
    on evaluating the output itself. Similar to checking for assumptions, ask an LLM
    to evaluate the output to confirm that it aligns with what it has been instructed
    to produce. Let’s check out an example to gain a better understanding of how this
    would work. First, take a look at a prompt that doesn’t ask an LLM to work out
    its solution first but sets out the instructions for the LLM to follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a software developer in test that is experienced in writing Java.
    Create a unit test for the following method: |'
  prefs: []
  type: TYPE_TB
- en: 'We then provide the code it needs to provide unit tests for:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sending this to ChatGPT, the following result was returned:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: This looks like reasonable Java code for a unit test, but if this were added
    to a suite of unit checks, it would fail because the method `authDB.deleteToken`
    in the production code provided has not been handled correctly. Specifically,
    if we wanted this unit check to work, we would need to mock `authDB.deleteToken`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we run the prompt again but this time have it evaluate its solution
    before outputting a final answer, we get a different result. So first we change
    the prompt to a new prompt that asks the LLM to confirm that the result of the
    check works:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are a software developer in test that is experienced in writing Java.
    Create a unit test for the following method that will be shared. Before outputting
    the unit tests, check to see that the assertion of each unit check will return
    a deterministic result. |'
  prefs: []
  type: TYPE_TB
- en: 'We then provide the code for which a unit test will be written:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sending this to ChatGPT returned this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: This time, we can see that because we asked the LLM to evaluate its solution
    before returning a result, the output unit check uses `Mockito` to mock the `authDB.deleteToken`.
    So, if we observe problems with LLMs outputting erroneous solutions or they start
    hallucinating, we can add an instruction to evaluate solutions first to minimize
    the occurrence of hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 2.6
  prefs: []
  type: TYPE_NORMAL
- en: Create a prompt that requires an LLM to work out a solution to a problem. Observe
    its output and see if the solution it produces is correct. Then add instructions
    to have the LLM evaluate the solution. What happens? Does the solution change?
    Is it an improvement?
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Working with various LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve talked about LLMs in a broad sense while using OpenAI’s ChatGPT
    in the previous examples to demonstrate how they work in general. However, ChatGPT
    is just one of the many different LLMs that we can use. So, before we conclude
    the chapter, let’s familiarize ourselves with the ways in which LLMs differ from
    one another and learn about some of the currently popular models and communities
    so that we can increase our chances of finding the right LLM for the job.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.1 Comparing LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What makes an LLM good? How do we determine whether a model is worth using?
    These are not easy questions to answer. The complex nature of LLMs, how they’re
    trained, and what data was used close these systems off to deep analysis, compromising
    an area that some researchers are trying to improve or shed light upon. However,
    that doesn’t mean we shouldn’t educate ourselves on some of the key aspects of
    LLMs and how they affect them. We might not all be AI researchers attempting to
    explore the deep inner workings of LLMs, but we are or will be their users and
    will want to know that what we spend resources on is giving us value. So, to help
    us break down some of the jargon and give us some grounding on how LLMs differ,
    let’s go through some key attributes discussed in the world of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter count
  prefs: []
  type: TYPE_NORMAL
- en: If you take a look at different LLMs, you’ll likely see talk of LLMs having
    a 175-billion- or 1-trillion-parameter count. It can sometimes feel like marketing
    speak, but parameter count does not affect LLMs’ performance. The parameter count
    essentially refers to the amount of statistical weights that exist in a model.
    Each individual weight provides a piece of the statistical puzzle that makes up
    an LLM. So, roughly speaking, the more parameters an LLM has, the better it will
    perform. The parameter count can also give us a sense of cost. The higher the
    parameter count, the more expensive it is to run, and there is a cost that may
    be, in part, handed down to users.
  prefs: []
  type: TYPE_NORMAL
- en: Training data
  prefs: []
  type: TYPE_NORMAL
- en: LLMs require huge quantities of data to be trained on, so the size and quality
    of data will have an effect on the quality of an LLM. If we want an LLM to be
    accurate in how it responds to requests, it’s not enough to just throw as much
    data as possible. It needs to be data that can help influence the probability
    of a model in a sensible manner. For example, the Reddit example we explored earlier
    in this chapter, in which the subreddit r/counting used to train ChatGPT caused
    it to hallucinate in strange ways, demonstrates that more isn’t necessarily better.
    Still, similar to parameter count, the more high-quality data an LLM has been
    trained on, the better it will likely perform. The challenge is in knowing what
    data an LLM has been trained on—something that corporate creators of AI are keen
    on keeping a secret.
  prefs: []
  type: TYPE_NORMAL
- en: Extensibility and integration
  prefs: []
  type: TYPE_NORMAL
- en: Just like with any other tool, the value of an LLM can be increased further
    if it can offer other features beyond its core abilities, such as integrating
    into existing systems or training models further for our specific needs. What
    features are available to integrate and extend LLMs depends largely on who was
    responsible for training.
  prefs: []
  type: TYPE_NORMAL
- en: For example, OpenAI offers paid-for API access to their models. But beyond an
    instruction feature that allows you to tweak output with a simple prompt, there
    is no ability to further fine-tune and deploy one of their GPT models for private
    use. Compare this to Meta’s LlaMa model, which has been open sourced, allowing
    the AI community to download and further train to their own requirements, although
    they must build their own infrastructure to deploy the model.
  prefs: []
  type: TYPE_NORMAL
- en: As LLM platforms grow, we will see advances in not just their ability to respond
    to prompts but also the features around them and their access. Thus, it’s necessary
    to keep said features in mind when evaluating what to work with.
  prefs: []
  type: TYPE_NORMAL
- en: Quality of responses
  prefs: []
  type: TYPE_NORMAL
- en: Arguably, the most important factor to consider is whether an LLM provides responses
    that are legible, useful, and free (or as close to free) of hallucination as possible.
    Although criteria such as parameter count and training data are useful indicators
    of an LLM’s performance, it’s up to us to understand what we want to use an LLM
    for and then determine how each responds to our prompts and helps solve our specific
    problems. Not all challenges we face need the largest, most expensive LLM in the
    market. Thus, it’s important that we take time to try out different models, compare
    their outputs, and then make a judgment for ourselves. For example, GPT models
    from OpenAI are found to perform better with code examples than Google Gemini.
    These details have been discovered through experimentation and observation.
  prefs: []
  type: TYPE_NORMAL
- en: The criteria we’ve explored are by no means an exhaustive list, but they demonstrate
    that there is more to consider about LLMs once we get past the initial glamour
    of how they respond. Different LLMs perform differently, helping us with various
    challenges. So let’s take a look at some of the more popular currently available
    models and platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.2 Examining popular LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since OpenAI’s launch of ChatGPT, there has been an explosion in releases of
    LLMs from various organizations. It’s not to say that these models and related
    work weren’t around before ChatGPT’s release, but the public focus has certainly
    intensified, and an increasing number of marketing and release announcements have
    focused on companies releasing their LLM offerings. Here are some of the more
    common/popular LLMs that have been released since the end of 2022.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping up with LLMs
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that the situation with the launch of LLMs and their related
    features is extremely fluid and has grown at quite a fast pace. Therefore, it’s
    likely that some of what we’ll explore will differ from the time of writing in
    mid-2024 to the time you are reading this book. Fortunately, sites such as LLM
    Models ([https://llmmodels.org/](https://llmmodels.org/)) share up-to-date lists
    to review. However, this list demonstrates that some of the bigger names in the
    LLM space are worth exploring.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, OpenAI is the most ubiquitous of organizations offering
    LLMs for use. Although OpenAI has been working on LLM models for quite some time,
    releasing their GPT-3 model in 2020, it was their release of ChatGPT in November
    2022 that kick-started the popular wave of interest and use of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI offers a range of different LLM models, but the two that stand out are
    GPT-3.5-Turbo and GPT-4o, which you can learn about more at [https://platform.openai.com/docs/models/overview](https://platform.openai.com/docs/models/overview).
    These two models are used as *foundation* models or models that can be trained
    further for specific purposes, for a range of products such as ChatGPT, GitHub
    Copilot, and Microsoft Bing AI.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to their models, OpenAI has offered a range of features such as
    API access to their direct GPT-3.5-Turbo and GPT-4 models and a collection of
    apps that integrate with ChatGPT (if you subscribe to their plus membership).
    It’s by far the most popular LLM (for now) and has kick-started a race with organizations
    to release their own LLMs. Although we’ve already explored some prompts with ChatGPT,
    you can always access and experiment with ChatGPT at [https://chatgpt.com/](https://chatgpt.com/).
  prefs: []
  type: TYPE_NORMAL
- en: Sticking with OpenAI
  prefs: []
  type: TYPE_NORMAL
- en: Although there are many different LLMs that I encourage you to use, for the
    sake of consistency, we will stick with ChatGPT-3.5-Turbo. It’s not necessarily
    the most powerful LLM at this time, but it is the most ubiquitous—and free. That
    said, if you want to try out these prompts with other LLM models, feel free to
    do so. However, keep in mind that their responses will likely differ from what
    is shared in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Gemini
  prefs: []
  type: TYPE_NORMAL
- en: Unsurprisingly, Google also has a stake in the Generative AI market with their
    own range of LLM models known as Gemini. Currently at the time of writing their
    most powerful model is Gemini 1.5 Pro, but they also offer other versions of their
    models such as Gemini 1.5 Flash and Gemini 1.0 Pro. Given it's Google the parameter
    counts for each model is not public knowledge, but they are relatively comparable
    in performance to other LLM models.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to OpenAI, Google offers access to their Gemini models via their Google
    Cloud platform ([https://ai.google.dev/](https://ai.google.dev/)) and has recently
    started offering apps that work similarly to OpenAI’s ChatGPT apps, with the added
    integration into other Google Suite tools such as Google Drive and Gmail. You
    can access and experiment with Gemini at [https://gemini.google.com/app](https://gemini.google.com/app).
  prefs: []
  type: TYPE_NORMAL
- en: LLaMa
  prefs: []
  type: TYPE_NORMAL
- en: 'LLaMa, which is the name for a collection of models, was first released by
    Meta in July 2023\. What sets LLaMa apart from OpenAI’s GPT models and Google’s
    Gemini is that LLaMa is open source. In addition to the open source license, LLaMa
    comes in a range of sizes: 8 and 70 billion parameters, respectively. The combination
    of these sizes and their access means that LLaMa has been adopted by the AI community
    as a popular foundational model. The flip side of this access, though, is that
    Meta doesn’t provide a public platform to train and run versions of LLaMa. So,
    data sets and infrastructure must be personally sourced for use.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More details on LLaMa can be found at the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.llama.com/llama-downloads/](https://www.llama.com/llama-downloads/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hugging Face
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the other entries in our list, Hugging Face offers no proprietary model
    but instead facilitates an AI community that contains a wide variety of different
    models, most of which are open source. Looking at their index page of models available
    at [https://huggingface.co/models](https://huggingface.co/models), we can see
    hundreds of thousands of differently trained models that have come from different
    companies and research labs. Hugging Face also offers datasets for training, apps,
    and documentation that allows the reader to dive deeper into how models are built.
    All of these resources are available so that the AI community can access pretrained
    models, tweak them, and further train them for a specific use, which is something
    that we’ll be exploring further in part 3 of this book.
  prefs: []
  type: TYPE_NORMAL
- en: The marketplace for LLMs has sizably grown in a short amount of time, both commercially
    and in open source, and similar to other areas of software development, being
    proactive in what new LLMs are appearing can be beneficial. However, it can also
    be overwhelming and not necessarily feasible to keep up with everything that is
    happening at once. So, instead of attempting to keep abreast of all the comings
    and goings in the AI community, we can opt to explore LLMs when we want to use
    LLMs to solve specific problems. Having a problem can help frame our criteria
    around which tools work best for us.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 2.7
  prefs: []
  type: TYPE_NORMAL
- en: Either select an earlier prompt from this chapter or create one of your own
    and submit it to different LLMs. Note how each responds and compares. Do some
    of them feel more conversational? How do they handle receiving or sending code
    examples? Which ones provide the best response in your opinion?
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Creating a library of prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the benefits of prompts is that once created, they can be used repeatedly.
    Consequently, a lot of collections of prompts for different roles and tasks are
    appearing online. For example, here are a few collections that I’ve seen shared
    recently:'
  prefs: []
  type: TYPE_NORMAL
- en: Awesome ChatGPT Prompts, GitHub ([https://github.com/f/awesome-chatgpt-prompts](https://github.com/f/awesome-chatgpt-prompts))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 50 ChatGPT Prompts for Developers, Dev.to ([https://mng.bz/w5D7](https://mng.bz/w5D7))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT Cheat Sheet, Hackr.io ([https://mng.bz/q0PK](https://mng.bz/q0PK))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This list is not at all exhaustive, and the sample collections aren’t necessarily
    related to testing, but they are worth looking through to learn how others have
    created prompts, as well as giving us the opportunity to determine which prompts
    would be effective and which wouldn’t.
  prefs: []
  type: TYPE_NORMAL
- en: Although prompt collections shared publicly can be useful, it’s likely we’ll
    end up creating prompts that are used for specific contexts. So, it’s worthwhile
    getting into the habit of storing prompts that prove to be beneficial in some
    sort of repository for us and others to quickly use. Where you store these will
    depend on what and who they are used for. If they’re for public use, then sharing
    a repository of prompts or adding to existing collections might be valuable. If
    we’re creating and using them while developing company products, then we need
    to treat them in the same way as our production code and store them somewhere
    private so that we don’t violate any policies around intellectual property. Finally,
    we may also consider version control so that we can tweak and track prompts as
    we learn more about working with LLMs and as the LLMs themselves evolve.
  prefs: []
  type: TYPE_NORMAL
- en: Wherever they are stored, the idea is to create a repository of prompts that
    are quick and easy to access so that once a prompt has been created for a specific
    activity, it can be reused multiple times rapidly so that we can get as much value
    from them to improve our productivity.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 2.8
  prefs: []
  type: TYPE_NORMAL
- en: Create a space where you can store future prompts for you and your team to use.
  prefs: []
  type: TYPE_NORMAL
- en: Using prompts from this book
  prefs: []
  type: TYPE_NORMAL
- en: In the spirit of storing prompts for future use and to help you, the reader,
    with trying out the prompt examples in this book, you can find each prompt example
    at [https://mng.bz/75mx](https://mng.bz/75mx).
  prefs: []
  type: TYPE_NORMAL
- en: This will enable you to quickly copy and paste the prompts into your chosen
    LLM as we go through each chapter, saving you the task of having to type the whole
    prompt manually. There will be sections in certain prompts where you will need
    to add your own custom content or context to use them. To make them clear, instructions
    on what is required to add to the prompt are provided in the prompt and will be
    formatted in all caps and inside square brackets.
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 Solving problems by using prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The tactics and tooling we’ve learned about in this chapter help provide us
    with a framework to use LLMs and design specific prompts for specific testing
    activities. We should be mindful, though, that although these tactics improve
    our chances of getting desired results, they are not foolproof. For example, when
    we ask an LLM to evaluate its output, the LLM isn’t evaluating its output like
    a traditional application might. It’s simply moving the predictive needle further
    toward an output that aligns with our requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Single prompting vs. multi-prompting
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this chapter, we’ve explored how to use principles and tactics to
    create individual prompts that are as effective as possible at maximizing desired
    output from an LLM. However, tools such as ChatGPT, Gemini, and Claude allow us
    to conduct conversations with LLMs, and the history of the conversations influences
    the output of future responses in said conversation. This raises the question
    of whether it would be easier to try multiple prompts in a conversation to tweak
    output. Although this can be effective, we do run the risk that the longer a conversation
    progresses, the higher the risk of hallucinations occurring as an LLM attempts
    to overfit responses to our requests. This is why tools such as BingAI are limited
    in the number of responses they can give in a conversation. However, more importantly,
    more doesn’t necessarily mean better. The garbage in, garbage out rule is valid
    for both single and multiple prompts. Relying on multiple prompts in one conversation
    means we become less clear and precise in what we are asking for, which adds delays
    and increases hallucination, thus negating the value of using an LLM in the first
    place. In conclusion, whether we want to send a single prompt to get what we want
    or send multiple prompts, adopting the principles and tactics created by Isa Fulford
    and Andrew Ng will increase our productivity with LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it is necessary to develop the skills to write prompts that help
    us solve our problems effectively and in a way that doesn’t diminish the time
    saved using LLMs (for example, we don’t want to spend hours tweaking prompts).
    This means being able to identify specific problems that LLMs can help with and
    then utilizing prompt engineering to maximize the chances of extracting valuable
    information from an LLM. This is what we’ll explore throughout the rest of this
    book—when and how to use LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: As we progress, we’ll also learn that prompts come in many shapes and sizes.
    Throughout this chapter, we’ve looked at prompts that are manually written by
    us humans. But, as we’ll learn, tools such as GitHub Copilot auto-generate prompts
    as we write our code. That doesn’t mean we can’t still infuse the principles and
    tactics into our ways of working, but it does take time, awareness, and practice
    to develop.
  prefs: []
  type: TYPE_NORMAL
- en: Activity 2.9
  prefs: []
  type: TYPE_NORMAL
- en: Before you continue reading this book and learn about different types of prompts
    for different testing activities, use the knowledge from chapters 1 and 2, consider
    a specific testing task that you do, and try to build a prompt that can help you
    with your work.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs are trained on vast amounts of data using sophisticated algorithms to analyze
    our requests and predict an output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The predictive nature of LLMs makes them quite adaptable but also means they
    come with some risks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can sometimes output *hallucinations,* or text that sounds authoritative
    and correct when, in fact, it is false.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The data that LLMs are trained on may contain errors, gaps, and assumptions,
    and we must keep this in mind when using them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We must also be mindful of the data we share with LLMs so as not to cause unauthorized
    leaks of business or user information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering is a collection of principles and tactics used to maximize
    the chances of an LLM returning a desired output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use the knowledge that LLMs are predictive in nature and benefit from
    it by implementing prompt engineering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using delimiters can help us clarify instructions and parameters in a prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An LLM can output data in various formats, but it requires us to explicitly
    state which structure format we want in a prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can reduce hallucinations from LLMs by using the check-for-assumption tactic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing examples in a prompt can help ensure that an LLM provides an output
    in a desired format or context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifying specific subtasks in a prompt can help an LLM process complex tasks
    successfully.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asking LLMs to evaluate solutions to problems can also reduce errors and maximize
    outcomes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowing when to use LLMs and developing skills with prompt engineering is the
    key to success, regardless of the tool we use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
