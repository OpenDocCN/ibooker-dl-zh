<html><head></head><body>
  <div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">1</span> </span><span class="chapter-title-text">How AI works</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">The way LLMs process inputs and generate outputs</li> 
    <li class="readable-text" id="p3">The transformer architecture that powers LLMs</li> 
    <li class="readable-text" id="p4">Different types of machine learning</li> 
    <li class="readable-text" id="p5">How LLMs and other AI models learn from data</li> 
    <li class="readable-text" id="p6">How convolutional neural networks are used to process different types of media with AI</li> 
    <li class="readable-text" id="p7">Combining different types of data (e.g., producing images from text)</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p8"> 
   <p>This chapter clarifies how AI works, discussing many foundational AI topics. Since the latest AI boom, many of these topics (e.g., “embeddings” and “temperature”) are now widely discussed, not just by AI practitioners but also by businesspeople and the general public. This chapter demystifies them.</p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>Instead of just piling up definitions and writing textbook explanations, this chapter is a bit more opinionated. It points out common AI problems, misconceptions, and limitations based on my experience working in the field, as well as discussing some interesting insights you might not be aware of. For example, we’ll discuss why language generation is more expensive in French than in English and how OpenAI hires armies of human workers to manually help train ChatGPT. So, even if you are already familiar with all the topics covered in this chapter, reading it might provide you with a different perspective. </p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>The first part of this chapter is a high-level explanation of how <em>large language models </em>(LLMs) such as ChatGPT work. Its sections are ordered to roughly mimic how LLMs themselves turn inputs into outputs one step at a time. </p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>The middle part of this chapter discusses <em>machine learning,</em> which is the technique that makes computers learn from data to create LLMs and other types of AI. Note that AI and machine learning don’t mean the same. AI is a research field that tries to create computer programs to perform tasks in a way similar to humans. Machine learning may or may not be used for that goal. However, machine learning has been the preferred methodology in AI for at least two decades. So, you might hear people use the terms AI and machine learning interchangeably. When I speak of AI in this book, I mean current AI methods, and these methods involve the use of machine learning.</p> 
  </div> 
  <div class="readable-text intended-text" id="p12"> 
   <p>The last third of this chapter discusses how AI works outside language generation. Specifically, I give an overview of how AI analyzes and generates images or combinations of text and images. We also comment on current developments in AI-based video generation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p13"> 
   <p>Enjoy the ride!</p> 
  </div> 
  <div class="readable-text" id="p14"> 
   <h2 class=" readable-text-h2">How LLMs work</h2> 
  </div> 
  <div class="readable-text" id="p15"> 
   <p>Language models are computer programs that try to represent the structure of human language. A large language model, or LLM, is a language model on steroids. Its sheer size lets the LLM perform complex analyses of sentences and generate new text with impressive performance. Examples of LLMs are Open­AI’s GPT-4o, Meta’s Llama-3, Anthropic’s Claude 3.5 Sonnet, Google’s Gemini 1.5 Pro, and Mistral AI’s Mixtral 8x7b.</p> 
  </div> 
  <div class="readable-text intended-text" id="p16"> 
   <p>Current LLMs are designed to perform one specific task—guess the next word given an input sentence. The input sentence is known as the <em>prompt.</em> Suppose I asked you to predict the word that comes after the incomplete sentence “The Eiffel.” You’re very likely to suggest that “Tower” is the most logical choice. This is the exact job LLMs are designed to do. So, we can think of LLMs as sophisticated autocomplete programs. Officially, we say that LLMs are <em>autoregressive, </em>which means that they’re designed to produce a single extra piece of content based on previous content.</p> 
  </div> 
  <div class="readable-text intended-text" id="p17"> 
   <p>The autocomplete task may seem simple at first, but it is far-reaching. Consider the following prompt: “How much is 2 + 5? It is. . .” Autocompleting this kind of sentence requires knowing how to perform arithmetic operations. So, the task of performing arithmetic operations is included in the autocomplete task.</p> 
  </div> 
  <div class="readable-text intended-text" id="p18"> 
   <p>Now, consider the following prompt: “How do you say ‘umbrella’ in French?” To accurately autocomplete this kind of sentence, you’d need to be capable of translating French to English. So, at least in theory, the autocomplete task encompasses all sorts of tasks. </p> 
  </div> 
  <div class="readable-text intended-text" id="p19"> 
   <p>LLMs are created using machine learning, a process in which a computer analyzes a huge amount of data—pretty much a snapshot of the entire public internet—to automatically put the LLM together. The resulting LLM is a self-contained piece of software, meaning that it doesn’t access any external information to generate its outputs. For example, it doesn’t browse the web to make its next-word predictions. In addition, the LLM is static, so it must be periodically updated with new data if we want it to speak about recent events.</p> 
  </div> 
  <div class="readable-text intended-text" id="p20"> 
   <p>When we interact with LLMs, we don’t usually do so directly. Instead, we use an intermediary piece of software that processes our requests and manages the underlying LLM. Let’s call it the <em>LLM wrapper.</em> The wrapper uses tricks to provide further functionality to the user than just guessing the next word like the bare LLM would do. For example, the wrapper generates entire sentences, responds in a chatty way, and answers with real-time information, such as the current date. </p> 
  </div> 
  <div class="readable-text intended-text" id="p21"> 
   <p>An example of an LLM wrapper is ChatGPT, which is OpenAI’s customer-facing application. This application manages our interactions with the underlying LLM, such as GPT-4 and GPT-4o. Note that it is common to just use the term LLM to refer to the whole AI system, including the wrapper. </p> 
  </div> 
  <div class="readable-text intended-text" id="p22"> 
   <p>The next few sections discuss examples of how LLM wrappers use tricks to enhance the capabilities of their underlying, next-word guessing LLMs. </p> 
  </div> 
  <div class="readable-text" id="p23"> 
   <h3 class=" readable-text-h3"> Text generation</h3> 
  </div> 
  <div class="readable-text" id="p24"> 
   <p>We typically use LLMs to output entire sentences instead of just guessing a single word. The LLM wrapper achieves this through a simple trick: it makes the LLM eat its own output repeatedly. Suppose we give an LLM the prompt “The Eiffel.” The LLM guesses the most likely continuation of the sentence: “Tower.” The LLM wrapper then attaches this word to the initial prompt, which leads to the new prompt: “The Eiffel Tower.” It then uses the LLM to guess the following word, say, “is” and attaches it to the prompt again. The process is repeated (see figure 1.1) to generate entire sentences such as “The Eiffel Tower is in Paris.”</p> 
  </div> 
  <div class="browsable-container figure-container " id="p25">  
   <img alt="" src="../Images/CH01_F01_Maggiori.png" style="width: 100%; max-width: max-content;"/> 
   <h5 class=" figure-container-h5"><span class="">Figure 1.1</span><span class=""> </span><span class="">To generate full sentences, the LLM wrapper used the LLM to generate one word, then attached that word to the initial prompt, then used the LLM again to generate one more word, and so on.</span></h5>
  </div> 
  <div class="readable-text" id="p26"> 
   <h3 class=" readable-text-h3"> End of text</h3> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>In addition to outputting regular words, LLMs are designed to output a handful of special code words. One of them, often stylized as “&lt;|end of text|&gt;” in the literature, is a code word that signals the end of the text. When the LLM is built, it is exposed to examples of sentences containing this special code word to indicate their end. So, the LLM gains the capability of guessing that the next best word is actually an indication of its ending. </p> 
  </div> 
  <div class="readable-text intended-text" id="p28"> 
   <p>When the LLM wrapper encounters this special code word, it stops the process of having the LLM eat its own output to generate more text, as explained in the previous section. </p> 
  </div> 
  <div class="readable-text" id="p29"> 
   <h3 class=" readable-text-h3"> Chat</h3> 
  </div> 
  <div class="readable-text" id="p30"> 
   <p>Have a look at what happened when I inputted the prompt “The Eiffel” to ChatGPT:</p> 
  </div> 
  <div class="callout-container sidebar-container prompt-response-sidebar"> 
   <div class="readable-text" id="p31"> 
    <p><strong class="response-head-image"><img alt="image" src="../Images/Response-Chatgpt.png" width="undefined"/></strong><span class="CharOverride-2"> </span>It seems that your sentence was cut off. Were you asking about the Eiffel Tower?</p> 
   </div> 
   <div class="readable-text" id="p32"> 
    <p>The Eiffel Tower is one of the most iconic landmarks in the world, located in Paris, France. It was designed by Gustave Eiffel and completed in 1889 for the Exposition Universelle, a world’s fair held to celebrate the 100th anniversary of the French Revolution. Originally criticized by some of Paris's leading artists and intellectuals for its design, the tower has become a global cultural icon of France and one of the most recognizable structures in the world.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>By the looks of it, the LLM is not just autocompleting my input, or else it would have continued with “Tower” instead of telling me that my sentence was cut off.</p> 
  </div> 
  <div class="readable-text intended-text" id="p34"> 
   <p>What happened here was that, behind the scenes, the LLM wrapper inserted a special code word to indicate the end of my prompt. This code word is often stylized as “&lt;|end of prompt|&gt;” in the literature. So, the actual input sent to the LLM by the wrapper was “The Eiffel&lt;|end of prompt|&gt;”. During its construction, the underlying LLM was fed examples of dialog ending with “&lt;|end of prompt|&gt;”, which is why it recognized that in my sentence, the code word was located at an unusual place. </p> 
  </div> 
  <div class="readable-text intended-text" id="p35"> 
   <p>As we continue chatting back and forth with ChatGPT, the wrapper sends the LLM a history of the entire conversation each time, filled with “&lt;|end of prompt|&gt;” and “&lt;|end of text|&gt;” code words to clearly delimit the beginning and end of each interaction. This helps ChatGPT converse effectively.</p> 
  </div> 
  <div class="readable-text intended-text" id="p36"> 
   <p>OpenAI lets software developers use a more naked version of the LLM wrapper, which doesn’t insert the “&lt;|end of prompt|&gt;” code word after the user’s prompt. This is available through the API (an API is a way to connect two pieces of software—in this case, your software with OpenAI’s software) or the online platform for software developers (see <a href="https://platform.openai.com"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/platform.openai.com</span></a>) by using the <em>completions</em> mode of the API instead of the default <em>chat</em> mode.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p37"> 
   <p><span class="print-book-callout-head">NOTE</span> OpenAI’s documentation contains a warning sign telling us that the completions mode is “now considered legacy,” but it is still available for use as of this writing.</p> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>Let’s see what happened when I inputted “The Eiffel” to the LLM using the completions mode instead:</p> 
  </div> 
  <div class="callout-container sidebar-container prompt-response-sidebar"> 
   <div class="readable-text" id="p39"> 
    <p><strong class="response-head-image"><img alt="image" src="../Images/Response-Chatgpt.png" width="4%"/></strong><span class="CharOverride-2"> </span>Tower was inaugurated on March 31st in 1889. It was originally built as the entrance arch for the 1889 World’s Fair, and was only intended to stand for 20 years. However, its unique design and popularity among Parisians and tourists alike led to its preservation and it has become an iconic symbol of France and one of the most recognizable monuments in the world. The idea for the Eiffel Tower came from businessman Gustave Eiffel, who wanted to create a structure that would showcase the advancements in engineering and design during the Industrial Revolution. He and his team spent two years designing and constructing the tower, using over 18,000 individual pieces of wrought iron and 2.5 million rivets.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p40"> 
   <p><span class="CharOverride-3">We can see that the API simply autocompleted my input with a sensible continuation, which is why it started with “ Tower” (the word </span><em class="CharOverride-4">Tower</em><span class="CharOverride-3"> with a leading space) instead of telling me that my prompt was incomplete. </span> </p> 
  </div> 
  <div class="readable-text" id="p41"> 
   <h3 class=" readable-text-h3"> The system prompt</h3> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p>I asked, “What is today’s date?” The response was</p> 
  </div> 
  <div class="callout-container sidebar-container prompt-response-sidebar"> 
   <div class="readable-text" id="p43"> 
    <p><strong class="response-head-image"><img alt="image" src="../Images/Response-Chatgpt.png" width="4%"/></strong><span class="CharOverride-2"> </span>Today’s date is May 30, 2024.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>This was the correct response at the time of me asking. This is a bit surprising because, as LLMs simply analyze sentences to guess the next word, they don’t have access to real-time data. </p> 
  </div> 
  <div class="readable-text intended-text" id="p45"> 
   <p>What happened here was that ChatGPT secretly inserted additional text before my prompt to provide contextual information to the LLM. This is known as the <em>system prompt.</em> We don’t know the exact details, but the rumor is that ChatGPT’s system prompt is as follows (see <a href="https://mng.bz/RVOv"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/RVOv</span></a>):</p> 
  </div> 
  <div class="readable-text prompt" id="p46"> 
   <p><strong class="prompt-head-image"><img alt="image" src="../Images/Prompt-Icon.png" width="4%"/></strong> You are ChatGPT, a large language model trained by Open­AI. Answer as concisely as possible. Knowledge cutoff: [knowledge cutoff] Current date: [current date and time]</p> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>This prompt is secretly inserted every time you start a chat with ChatGPT. Because the date appears in ChatGPT’s system prompt, the chatbot can answer questions about the current date, as in the previous example. Note that the knowledge cutoff date is also inserted, which helps ChatGPT inform the user that it cannot answer questions about events that took place after a certain date.</p> 
  </div> 
  <div class="readable-text intended-text" id="p48"> 
   <p>Software developers can interact with OpenAI’s LLMs via an API instead of using the customer-facing ChatGPT. The API lets you define what the system prompt is, which is inserted before your initial interactions with the LLM. Figure 1.2 shows a visual interface provided by OpenAI to help developers try out the API. We can see a box dedicated to the system prompt.</p> 
  </div> 
  <div class="readable-text intended-text" id="p49"> 
   <p>I asked the GPT-4o LLM about the current date using OpenAI’s API, while leaving the system prompt empty. In figure 1.2, we can see that the LLM refused to answer about the date.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p50">  
   <img alt="" src="../Images/CH01_F02_Maggiori.png" style="width: 100%; max-width: max-content;"/> 
   <h5 class=" figure-container-h5"><span class="">Figure 1.2</span><span class=""> </span><span class="">OpenAI’s API lets users define a system prompt, which is a piece of text inserted into the beginning of the user’s prompt. </span></h5>
  </div> 
  <div class="readable-text" id="p51"> 
   <p>Figure 1.3 shows that the LLM does answer with the date if it is given as part of the system prompt, like ChatGPT would do.</p> 
  </div> 
  <div class="readable-text" id="p52"> 
   <h3 class=" readable-text-h3"> Calling external software functions</h3> 
  </div> 
  <div class="readable-text" id="p53"> 
   <p>I asked ChatGPT about the current weather in London. ChatGPT’s user interface showed a sign that said, “Searching the web.” A second later, the sign turned into “Searching current weather in London.” Afterward, it told me what the weather in London was like (see figure 1.4).</p> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>The trick here is to describe in the system prompt a list of software functions that the LLM can suggest the wrapper to call if it needs to gather external information. If the LLM suggests calling one of those functions, it is the job of the LLM wrapper to call it and then insert the result into the prompt. </p> 
  </div> 
  <div class="browsable-container figure-container " id="p55">  
   <img alt="" src="../Images/CH01_F03_Maggiori.png" style="width: 100%; max-width: max-content;"/> 
   <h5 class=" figure-container-h5"><span class="">Figure 1.3</span><span class=""> </span><span class="">When the current date is supplied as part of the system prompt, the LLM can answer questions about the current date.</span></h5>
  </div> 
  <div class="readable-text" id="p56"> 
   <p>Suppose a developer wants to create a chatbot app that can seamlessly answer questions about current events, such as the weather, the value of stocks, and trending news topics. The developer could explain in the system prompt that, if the current weather in London is required, the LLM should output <code>"current_weather(London)"</code>, if the value of Apple stock is needed, it should output <code>"stock_value(Apple)"</code>, and so on. When these special messages are outputted, the developer will call software functions to gather the necessary information and add it to the prompt. This will give the end user the impression of seamless access to real-time data.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p57">  
   <img alt="" src="../Images/CH01_F04_Maggiori.png" style="width: 100%; max-width: max-content;"/> 
   <h5 class=" figure-container-h5"><span class="">Figure 1.4</span><span class=""> </span><span class="">ChatGPT called a function to search the web behind the scenes and inserted the results into the user’s prompt. This creates the illusion that the LLM browses the web.</span></h5>
  </div> 
  <div class="readable-text" id="p58"> 
   <p>OpenAI has created a framework that lets a developer easily define a list of functions that the LLM could suggest calling. Here’s an example of how to define a <code>"get_current_weather"</code> function, as described in the official documentation (see <a href="https://mng.bz/2y4a"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/2y4a</span></a>):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p59"> 
   <div class="code-area-container"> 
    <pre class="code-area">tools = [
    {
        "type": "function",
        "function": {
            "name": "get_current_weather",
            "description": "Get the current weather",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, 
                         e.g. San Francisco, CA",
                    },
                    "format": {
                        "type": "string",
                        "enum": ["celsius", "fahrenheit"],
                        "description": "The temperature unit 
                         to use. Infer this from the users 
                         location.",
                    },
                },
                "required": ["location", "format"],
            },
        }
    }
]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>Note that the code of the weather-fetching function is not part of this declaration. Only a description of the function and its inputs is provided. The LLM wrapper inserts the description of this function into the system prompt so that the underlying LLM can suggest calling it if needed.</p> 
  </div> 
  <div class="readable-text intended-text" id="p61"> 
   <p>When the wrapper detects that the LLM suggests calling the function, it notifies the user. Here’s an example of the API response object, using OpenAI’s Python SDK, that resulted after the user asked about the weather in London:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p62"> 
   <div class="code-area-container"> 
    <pre class="code-area">ChatCompletionMessage(
    content=None, 
    role='assistant', 
    tool_calls=[
    ChatCompletionMessageToolCall(
        id='call_Dn2RJJSxzDm49vlVTehseJ0k', 
        function=Function(
            arguments='{"location":"London, United Kindgdom",
             "format":"celsius"}', 
            name='get_current_weather'
        ), 
    type='function')
    ]
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>The software developer must code the "<code>get_current_weather"</code> function, run it, and then insert the response into the following prompt (“Weather in London, United Kingdom: 20 degrees Celsius, rainy”). The LLM can then use this newly added information. The app end user gets the impression that the LLM itself was capable of answering about the weather in real time. In reality, the LLM is still a self-contained program; the enhanced functionality is achieved outside the LLM.</p> 
  </div> 
  <div class="readable-text" id="p64"> 
   <h3 class=" readable-text-h3"> Retrieval-augmented generation</h3> 
  </div> 
  <div class="readable-text" id="p65"> 
   <p>Sometimes users want the LLM to analyze documents that aren’t present in the training data. For example, a business may want to answer questions about its internal documents, or an app may want to analyze the content of up-to-date webpages. Retrieval-augmented generation, or RAG, is a popular way of doing that (you can learn more in <em class="CharOverride-4">A Simple Guide to Retrieval Augmented Generation</em> by Abhinav Kimothi, available at <a href="https://mng.bz/yWpe"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/yWpe</span></a>). When the user submits a prompt, the LLM wrapper first searches for relevant documents in a database. For example, it may extract keywords from the prompt and find documents that match the keywords. This is known as <em>retrieval.</em><em/></p> 
  </div> 
  <div class="readable-text intended-text" id="p66"> 
   <p>Afterward, the LLM wrapper inserts the content of these documents into the prompt. So, the prompt is said to be <em>augmented</em> with additional, relevant information.</p> 
  </div> 
  <div class="readable-text intended-text" id="p67"> 
   <p>When the LLM generates text, it has access to these documents as part of the prompt, so it can use their content to enhance its predictions. RAG is a popular approach to creating an in-house chatbot adapted to a specific business. In addition, it is commonly used to create the illusion that an LLM can access up-to-date web content in real time. RAG can also help identify specific sources used by the LLM to generate its output and thus cite references. </p> 
  </div> 
  <div class="readable-text intended-text" id="p68"> 
   <p>One of the challenges of the RAG approach is finding relevant documents based on the prompt. Many algorithms have been used for a long time by search engines to index and retrieve content, and researchers are studying specific retrieval techniques for RAG (see <a href="https://arxiv.org/abs/2405.06211"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/abs/2405.06211</span></a>). Another challenge is that prompts can become quite long with the added documents. LLMs only accept a maximum prompt length (more on this in the following), so you must make sure that the documents inserted into the prompt fit the maximum allowed length. In addition, longer prompts incur higher costs as AI providers charge fees that depend on the amount of text inputted and outputted.</p> 
  </div> 
  <div class="readable-text" id="p69"> 
   <h2 class=" readable-text-h2">The concept of tokens</h2> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>We’ve been saying that LLMs guess the next word from an input prompt, but this isn’t quite accurate. Let’s now refine our understanding.  </p> 
  </div> 
  <div class="readable-text intended-text" id="p71"> 
   <p>LLMs contain a fixed-size internal vocabulary. These are the words that LLMs can read and generate. An LLM’s vocabulary typically contains </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p72">Common words (e.g., “dog”)</li> 
   <li class="readable-text" id="p73">Common pieces of words (e.g., “ish”)</li> 
   <li class="readable-text" id="p74">Common Latin characters (e.g., “a” and “b”)</li> 
   <li class="readable-text" id="p75">Special symbols from a text-encoding standard called UTF-8, which are combined together to represent non-Latin characters and other symbols (e.g., “á,” “æ,” and “<span class="CharOverride-5">你</span>”)</li> 
   <li class="readable-text" id="p76">Special code words such as “&lt;end of text&gt;” and “&lt;end of prompt&gt;”</li> 
  </ul> 
  <div class="readable-text" id="p77"> 
   <p><span class="CharOverride-3">Each element in the vocabulary is known as a </span><em>token.</em><span class="CharOverride-3"> We can think of a token as a common piece of text. Using tokens instead of entire words, lets LLMs read and produce words that aren’t in the dictionary (e.g., “hungryish”) by combining common pieces of words (“hungry” + “ish”). It also lets LLMs read and produce non-Latin text and invent new words. </span><span class="CharOverride-3"/></p> 
  </div> 
  <div class="readable-text intended-text" id="p78"> 
   <p>Current LLMs’ vocabularies contain roughly 100,000 different possible tokens. For example, some of OpenAI’s LLMs, including GPT-3.5 and GPT-4, have a vocabulary with 100,261 possible tokens. </p> 
  </div> 
  <div class="readable-text intended-text" id="p79"> 
   <p>Note that many tokens represent common words with a leading space attached to them. For example, both “dog” and “ dog” are tokens in the vocabulary of OpenAI’s LLMs. So, the LLM is often spared from having to use the dedicated whitespace token. From now on, whenever I speak of an individual token in this book, such as the “dog” token, bear in mind there might be a leading space attached to it. (I won’t be writing the space every time, as it’s a bit ugly to read.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p80"> 
   <p>The vocabulary of an LLM is created by running an automated analysis over thousands of documents to identify the most common text patterns (the algorithm usually used for this is called byte pair encoding. You can find more details and a step-by-step example in a blog article I wrote at <a href="https://emaggiori.com/chatgpt-vocabulary/"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/emaggiori.com/chatgpt-vocabulary/</span></a>). OpenAI stopped disclosing how it creates LLMs’ vocabularies, but we do know how they did it with older models. For example, GPT-3’s vocabulary was created by automatically following links from popular Reddit discussions, collecting the text from the linked webpages, and identifying the most common words and combinations of characters in them (Redford et al., “Language Models are Unsupervised Multitask Learners,” 2019).</p> 
  </div> 
  <div class="readable-text" id="p81"> 
   <h3 class=" readable-text-h3"> One token at a time</h3> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>LLMs are designed to read a sequence of valid tokens from their vocabulary. So, the LLM wrapper first subdivides the input prompt into valid tokens. For example, when using GPT-3.5, the prompt “The dog’s bark was barely” is subdivided as follows by the LLM wrapper before passing it to the LLM:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p83"> 
   <div class="code-area-container"> 
    <pre class="code-area">The| dog|'s| bark| was| barely</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>The subdivision is performed using an algorithm that roughly tries to split the input using the largest possible tokens from the vocabulary.</p> 
  </div> 
  <div class="readable-text intended-text" id="p85"> 
   <p>OpenAI provides a webpage where you can input text and see how it’s tokenized before being fed into a model. You can find it at <a href="https://platform.openai.com/tokenizer"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/platform.openai.com/tokenizer</span></a>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p86"> 
   <p>LLMs don’t read raw text. Instead, the LLM wrapper first converts the input prompt into a list of integers indicating the ID of each token, which is its position in the vocabulary:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p87"> 
   <div class="code-area-container"> 
    <pre class="code-area">[791, 5679, 596, 54842, 574, 20025]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>Afterward, the wrapper uses the LLM to predict the ID of the most likely next token. In the previous example, the LLM outputs that the token with ID 80415 is the most likely continuation of the input prompt. This token corresponds to “audible”.The LLM wrapper then attaches that token to the input:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p89"> 
   <div class="code-area-container"> 
    <pre class="code-area">The| dog|'s| bark| was| barely| audible</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>Next, the LLM wrapper feeds this new prompt (as a list of integers, <code>[791,</code> <code>5679,</code> <code>596,</code> <code>54842,</code> <code>574,</code> <code>20025,</code> <code>80415]</code>) to the LLM to have it “eat its own output” and generate one more token. This process is repeated many times to generate more tokens:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p91"> 
   <div class="code-area-container"> 
    <pre class="code-area">The| dog|'s| bark| was| barely| audible| above| the| roar| of| the| city| traffic|.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>In this example, after a few more paragraphs of mumbo jumbo regarding dogs and noise, the LLM decided that the token with ID 100276 was the most likely continuation of the prompt. This token is code for “&lt;|end of text|&gt;”. So, the LLM deemed this a good place to end the text. Upon stumbling on this token, the LLM wrapper heeded the LLM’s recommendation and stopped generating more text. </p> 
  </div> 
  <div class="readable-text intended-text" id="p93"> 
   <p>Have a look at how GPT-3.5 explained to me the meaning of the word “hungryish”, token by token:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p94"> 
   <div class="code-area-container"> 
    <pre class="code-area">If| you| say| "|I|'m | hungry|ish|,"| you| mean| you|'re | feeling| somewhat| hungry|,| but| not| extremely| so|.| It|'s | a| mild|er| form| of| hunger|.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>We can see that, even though the word “hungryish” isn’t part of GPT-3.5’s vocabulary, it managed to generate it using a sequence of two tokens, “hungry” and “ish.” Note that the words “milder,” “I’m,” “you’re,” and “It’s” were also produced using two tokens each.</p> 
  </div> 
  <div class="readable-text" id="p96"> 
   <h3 class=" readable-text-h3"> Billed by the token</h3> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>Most LLM APIs, which let software developers use LLMs programmatically, bill users by the number of tokens inputted and outputted to the LLM. Thus, longer prompts and longer responses incur higher costs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p98"> 
   <p>As of today, for example, GPT-4o costs US$5 per million input tokens plus US$15 per million output tokens. For reference, the entire Shakespearean play <em>Romeo and Juliet</em> requires 40,000 tokens, so inputting it to GPT-4o would cost $0.20, and generating it would cost $0.60. This doesn’t sound like a lot, but bills can easily add up if you use LLMs repeatedly. For example, if you send a long prompt to an LLM every time a user visits your website, you could spend thousands a month.</p> 
  </div> 
  <div class="readable-text intended-text" id="p99"> 
   <p>Note that when you chat back and forth with an LLM, you must include your entire chat history on every interaction with it, or at least you must do so if you want the LLM to be able to analyze the previous conversation when generating new outputs. So, the prompt becomes increasingly expensive as your chat history becomes longer.</p> 
  </div> 
  <div class="readable-text" id="p100"> 
   <h3 class=" readable-text-h3"> What about languages other than English?</h3> 
  </div> 
  <div class="readable-text" id="p101"> 
   <p>LLM’s vocabularies tend to be optimized for the English language. For example, they contain a “dog” token but not one to represent the French word for dog. So, words not in English tend to be split into many tokens, often covering one or two letters at a time, as the vocabulary doesn’t contain as many tokens to represent entire words.</p> 
  </div> 
  <div class="readable-text intended-text" id="p102"> 
   <p>Have a look at how the preamble of the U.S. Constitution is tokenized before being inputted into GPT-4:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p103"> 
   <div class="code-area-container"> 
    <pre class="code-area">We| the| People| of| the| United| States|,| in| Order| to| form| a| more| perfect| Union|,| establish| Justice|,| insure| domestic| Tran|qu|ility|,| provide| for| the| common| defense|,| promote| the| general| Welfare|,| and| secure| the| Bless|ings| of| Liberty| to| ourselves| and| our| Poster|ity|,| do| ord|ain| and| establish| this| Constitution| for| the| United| States| of| America|.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p104"> 
   <p>And now, have a look at its French translation:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p105"> 
   <div class="code-area-container"> 
    <pre class="code-area">Nous|,| le| Pe|uple| des| É|t|ats|-Un|is|,| en| vue| de| former| une| Union| plus| par|fa|ite|,| d|'é |tabl|ir| la| justice|,| de| faire| rég|ner| la| pa|ix| int|érie|ure|,| de| pour|voir| à| la| déf|ense| commune|,| de| dévelop|per| le|  bien|-être| général| et| d|' |ass|urer| les| bien|fa|its| de| la| libert|é| à| nous|-m|ê|mes| et| à| notre| post|é|rit|é|,| nous| dé|cr|é|tons| et| é|tab|lis|sons| cette| Constitution| pour| les| É|t|ats|-Un|is| d|' |Am|érique|.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>The French text takes more than twice the number of tokens than its English counterpart. In addition, the subdivision of words in French doesn’t make much sense. For example, “États-Unis d’Amérique” (United States of America) is chopped up into many meaningless pieces such as “ats” and “-Un.”</p> 
  </div> 
  <div class="readable-text intended-text" id="p107"> 
   <p>This problem gets even more serious with non-Latin alphabets. An extreme example, widely discussed around the internet, is the word for “woman” in Telugu, one of the languages spoken in India: <span class="CharOverride-6">స్త్రీ. </span>This word is made up of a combination of six characters arranged horizontally and vertically. GPT-4 requires a whopping 18 tokens to represent this word using special UTF-8 tokens. </p> 
  </div> 
  <div class="readable-text intended-text" id="p108"> 
   <p>As LLMs are billed by the token, the higher number of tokens can make them more expensive to use in other languages compared to English. In addition, it can be more challenging for the LLM to analyze the prompt because individual inputs, such as an “é” token, don’t carry much meaning by themselves; the LLM must work extra hard to contextualize adjacent tokens and derive meaning from them.</p> 
  </div> 
  <div class="readable-text intended-text" id="p109"> 
   <p>The bias toward a specific language—English in the most popular LLMs—may not be easily removed. To better tokenize words in other languages, the vocabulary would have to be extended to include words or common pieces of words in, say, French, Chinese, Telugu, and so on. This would multiply the vocabulary size, well beyond the current 100,000 mark, which could turn LLMs ineffective and slow. </p> 
  </div> 
  <div class="readable-text intended-text" id="p110"> 
   <p>OpenAI has been working on improving its LLMs’ internal vocabularies to better handle non-English text. The details haven’t been disclosed yet as of this writing, but its creators shared a few illustrative cases with the new vocabulary used by GPT-4o (see <a href="https://openai.com/index/hello-gpt-4o/"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/openai.com/index/hello-gpt-4o/</span></a>). For example, a snippet of text in Telugu requires 3.5× fewer tokens than before, but it still requires twice as many as its English counterpart. </p> 
  </div> 
  <div class="readable-text" id="p111"> 
   <h3 class=" readable-text-h3"> Why do LLMs need tokens anyway?</h3> 
  </div> 
  <div class="readable-text" id="p112"> 
   <p><span class="CharOverride-3">One may wonder why tokens are needed at all; that is, why not have the LLM directly read and generate individual characters instead? As we’ll discuss soon, LLMs try to internally describe the </span><em>meaning</em><span class="CharOverride-3"> of each individual input. Describing the meaning of a token such as “Paris” is quite easy. For instance, we could describe it as “capital of France.” However, describing the meaning of a token such as “P” is much harder, as we don’t know what the letter refers to unless we analyze the context. That’s why it’s much more straightforward to take “Paris” as a single token in one go. The same goes for generating text—it’s much more straightforward to let the LLM output a token such as “Paris,” which carries a strong meaning by itself, instead of having it output the same word one character at a time. </span><span class="CharOverride-3"/></p> 
  </div> 
  <div class="readable-text intended-text" id="p113"> 
   <p>We could take this idea to the extreme and create a huge vocabulary that includes all sorts of words and their derivatives, such as “Parisian,” “Parisians,” “Parisian weather,” and “Emily in Paris.” But this would go too far—the vocabulary would become huge, and it would be wasteful because many tokens would represent closely related ideas. The current setup, with tokens representing the most common words and pieces of words, is an in-between solution that works well in practice. </p> 
  </div> 
  <div class="readable-text" id="p114"> 
   <h2 class=" readable-text-h2">Embeddings: A way to represent meaning</h2> 
  </div> 
  <div class="readable-text" id="p115"> 
   <p>One of the greatest challenges of AI is finding an effective way to represent high-level concepts, meaning, and ideas. When designing an LLM, we want the model to internally represent the meaning of a token instead of its letters. For example, we want the token “dog” to be represented by a description of what a dog is (say, a friendly, four-legged animal). </p> 
  </div> 
  <div class="readable-text intended-text" id="p116"> 
   <p>An <em>embedding</em> is one of the most common ways of representing meaning. It is used by LLMs and other types of AI<em>.</em> An embedding is a list (or “vector”) of numbers. The number of elements in the vector is known as the embedding’s <em>dimension.</em><em/></p> 
  </div> 
  <div class="readable-text intended-text" id="p117"> 
   <p>We can think of each position in this vector as a measure of how much a token matches a certain topic. Let’s have a look at an example. Imagine an embedding vector of length five represents the following five topics: “Animal,” “Cat,” “Large,” “Scary,” and “Four legs.” Suppose we want to represent the meaning of the “dog” token using these topics. Figure 1.5 provides an (imagined) solution.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p118">  
   <img alt="" src="../Images/CH01_UN01_Maggiori.png" style="width: 100%; max-width: max-content;"/> 
   <h5 class=" figure-container-h5">Figure 1.5 Each token is mapped to a vector of numbers. We can imagine that each number in the vector represents a topic. Here’s an imaginary list of topics and their respective numbers for the “dog” token.</h5>
  </div> 
  <div class="readable-text" id="p119"> 
   <p>In this illustration, the token was mapped to five numbers, each of them indicating how much the meaning of the token matches each topic. We can see that the token scores a high value with respect to the “Animal” topic, as a dog is certainly an animal. The token scores a negative value with respect to the “Cat” topic, as a dog is sometimes seen as the opposite of a cat. It scores a neutral value of zero with respect to “Large” because we don’t typically think of a dog as being a particularly large or small object. Figure 1.6 shows how we could imagine the embedding for the “elephant” token.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p120">  
   <img alt="" src="../Images/CH01_UN02_Maggiori.png" style="width: 100%; max-width: max-content;"/> 
   <h5 class=" figure-container-h5">Figure 1.6 An imaginary embedding vector for the “elephant” token</h5>
  </div> 
  <div class="readable-text" id="p121"> 
   <p>In this case, the embedding vector is neutral with respect to “Cat” and highly positive with respect to “Large.” </p> 
  </div> 
  <div class="readable-text intended-text" id="p122"> 
   <p>LLMs are all about embeddings. LLMs go to great lengths to try to find a good, contextualized representation of tokens by using embeddings. At the end of many layers of processing, the embeddings are very good at representing the true meaning of the input tokens, which makes it easy for the LLM to do the job of guessing the next token. </p> 
  </div> 
  <div class="readable-text intended-text" id="p123"> 
   <p>LLMs use much longer embedding vectors than in the above example, which lets them represent a huge number of topics. For example, GPT-3 uses 12,288-dimensional embeddings, so each input token is represented by 12,288 numbers. The smallest model in the Llama 3 family, developed by Meta, uses embeddings of 4,096 dimensions, and the largest one uses embeddings of 16,384 dimensions (<a href="https://arxiv.org/abs/2407.21783"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/abs/2407.21783</span></a>).</p> 
  </div> 
  <div class="readable-text" id="p124"> 
   <h3 class=" readable-text-h3"> Machine learning and embeddings</h3> 
  </div> 
  <div class="readable-text" id="p125"> 
   <p><span class="CharOverride-3">Designing long embeddings by hand would be very difficult. Thus, we use </span><em>machine learning</em><span class="CharOverride-3"> to do the job instead. This means that we make a computer analyze a large amount of data, such as text collected from the internet, to come up with useful embeddings. </span><span class="CharOverride-3"/></p> 
  </div> 
  <div class="readable-text intended-text" id="p126"> 
   <p>When AI engineers use, say, 12,288-dimensional embeddings inside an LLM, what they do is leave room for 12,288 topics. However, it is up to the machine to select and organize the topics to best attain its objectives.</p> 
  </div> 
  <div class="readable-text intended-text" id="p127"> 
   <p>As embeddings are created automatically, it is very hard to know which topics are represented by each of their dimensions. In addition, the topics may not be as clear-cut as “Large” and “Cat.” So, by using machine learning, we can create effective embeddings—the proof being that LLMs work well—but we can’t understand exactly how they work. <em>Explainability</em> is sacrificed in the name of predictive power.</p> 
  </div> 
  <div class="readable-text" id="p128"> 
   <h3 class=" readable-text-h3"> Visualizing embeddings</h3> 
  </div> 
  <div class="readable-text" id="p129"> 
   <p>A location on Earth can be determined by its latitude and longitude. We can equally think of each number inside an embedding vector as coordinates that help us figure out where the token is inside a space of meaning. Figure 1.7 illustrates an example of the space of meaning defined by a 2D embedding vector with the topics “Scary” and “Large.” Every token is placed inside this space according to its “Scary” and “Large” values in the embedding vector. </p> 
  </div> 
  <div class="browsable-container figure-container " id="p130">  
   <img alt="" src="../Images/CH01_F05_Maggiori.png" style="width: 100%; max-width: max-content;"/> 
   <h5 class=" figure-container-h5">Figure 1.7 We can think the numbers in an embedding vector as coordinates that place the token in a multidimensional “meaning space.”</h5>
  </div> 
  <div class="readable-text" id="p131"> 
   <p>You can see that similar objects tend to group together; that’s why the tokens “anaconda” and “snake” are close together in this space and so are “beetle” and “ladybug,” but “anaconda” and “ladybug” are far apart.</p> 
  </div> 
  <div class="readable-text intended-text" id="p132"> 
   <p>Well-designed, useful embeddings are such that tokens that are closely related in terms of meaning are also placed close together within this imaginary embedding space. If embedding vectors do a bad job at representing the true meaning of tokens, then related tokens will not be close together in this imaginary embedding space. </p> 
  </div> 
  <div class="readable-text intended-text" id="p133"> 
   <p>As embedding vectors are usually very long, the embedding space is high-dimensional. We can’t draw it, but we can still imagine that, in this high-dimensional space, related tokens are physically clustered together.</p> 
  </div> 
  <div class="readable-text" id="p134"> 
   <h3 class=" readable-text-h3"> Why embeddings are useful</h3> 
  </div> 
  <div class="readable-text" id="p135"> 
   <p><span class="CharOverride-3">Embedding vectors are particularly useful because it’s possible to compare them or extract information from them very easily, just by performing simple, linear calculations. Suppose you want to compare the meaning of two tokens. You can do that by calculating their physical distance in the imaginary embedding space. One popular way of doing that is calculating the </span><em>dot product </em><span class="CharOverride-3">between the two vectors, which produces a sort of “signed distance” between them. If the result is positive, the tokens are close enough in the embedding space and thus their meanings are related. If it’s zero, they are unrelated. If it’s negative, their meanings are opposed, such as in “large” and “small.” </span><span class="CharOverride-3"/></p> 
  </div> 
  <div class="readable-text print-book-callout" id="p136"> 
   <p><span class="print-book-callout-head">NOTE</span> The dot product is calculated by multiplying the numbers in one vector by their corresponding numbers in the other vector (at the same position) and then adding the results. </p> 
  </div> 
  <div class="readable-text" id="p137"> 
   <p><span class="CharOverride-3">Now, suppose you want to extract a limited amount of information of interest from a much more expressive embedding vector. For example, you may want to extract animal-related topics and dump everything else. We can think of this as squashing the multidimensional embedding space into a lower-dimensional space, such as flattening the 3D space to turn it into a thin plate, thus discarding uninformative dimensions. We could imagine, for instance, squashing the entire 12,288-dimensional space into, say, a 100-dimentional space that only focuses on animal-related topics (e.g., “Barks,” “Mammal,” “Pet”). The mathematical operation to perform such a squashing is known as a </span><em>projection.</em><em/></p> 
  </div> 
  <div class="readable-text intended-text" id="p138"> 
   <p>A projection is performed by multiplying a matrix by the embedding vector. The matrix represents the direction in which we want to squeeze the embedding space. Note that, as we don’t usually understand how embeddings encode meaning, we don’t understand how meaning is represented in the squeezed embedding space. Just like with the embeddings, the projections into squeezed spaces are also determined through machine learning and not designed by hand.</p> 
  </div> 
  <div class="readable-text intended-text" id="p139"> 
   <p>In addition to their use within LLMs and other types of AI, it has become popular for engineers to use third-party tools to generate embeddings for all sorts of content-retrieval applications. For example, you can use an embeddings API to generate embeddings that represent the meaning of text documents, and then you compare documents by calculating the dot product of their embeddings. Specifically, OpenAI provides an embeddings API that helps generate an embedding for a text document.</p> 
  </div> 
  <div class="readable-text intended-text" id="p140"> 
   <p>In addition, some APIs generate embeddings for different input modalities, such as text and images. One example is Google Clouds’ embeddings API (see <a href="https://mng.bz/1Xvq"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/1Xvq</span></a>). The generated embeddings can be directly compared. For example, a piece of text speaking about cats and a picture of a cat are mapped to closely related embedding vectors. Thus, you can use dot products to find the image that best matches a description. </p> 
  </div> 
  <div class="readable-text" id="p141"> 
   <h3 class=" readable-text-h3"> Why LLMs struggle to analyze individual letters </h3> 
  </div> 
  <div class="readable-text" id="p142"> 
   <p>LLMs are notorious for struggling to correctly analyze the individual letters in words, such as counting the number of occurrences of a letter. They also struggle to follow instructions that require generating text with certain letters in it. Figure 1.8 shows an example of this problem using GPT-4o.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p143">  
   <img alt="" src="../Images/CH01_F06_Maggiori.png" style="width: 100%; max-width: max-content;"/> 
   <h5 class=" figure-container-h5"><span class="">Figure 1.8</span><span class=""> </span><span class="">LLMs often struggle to analyze individual letters in words.</span></h5>
  </div> 
  <div class="readable-text" id="p144"> 
   <p>If you remember, LLMs receive tokens as inputs, not letters. So, the exact letters of a word are not inputted to the model. In the example of figure 1.8, the token “berry” is inputted to the LLM in one go.</p> 
  </div> 
  <div class="readable-text intended-text" id="p145"> 
   <p>Each token is then mapped to an embedding vector to represent its meaning. So, any references to individual letters are likely to be completely lost at this stage, as it’d be wasteful to devote space in the embedding vector to represent topics such as “token with two times the letter a,” when there are much more useful topics to represent instead.</p> 
  </div> 
  <div class="readable-text intended-text" id="p146"> 
   <p>As people have been widely mocking LLMs’ terrible performance at analyzing letters, it’s likely AI engineers will take ad hoc measures to directly address this problem. For example, the LLM wrapper may augment the prompt with words’ spellings if it detects that there are questions about individual letters. Maybe some of this has already been done, as newer LLMs seem to struggle less to analyze individual letters. However, the problem persists in even the most recent LLMs as of this writing, so it hasn’t been fully solved yet. </p> 
  </div> 
  <div class="readable-text" id="p147"> 
   <h2 class=" readable-text-h2">The transformer architecture </h2> 
  </div> 
  <div class="readable-text" id="p148"> 
   <p><span class="CharOverride-3">The methodology that powers current LLMs was invented by a group of Google researchers. It was described in a famous paper, published in 2017, titled “</span>Attention Is All You Need” (available at <a href="https://arxiv.org/abs/1706.03762"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/abs/1706.03762</span></a><span class="CharOverride-3">). The paper proposed a new way of designing language models, which became known as the </span><em>transformer architecture</em><span class="CharOverride-3"> or just </span><em>transformers.</em><span class="CharOverride-3"> </span><span class="CharOverride-3"/></p> 
  </div> 
  <div class="readable-text intended-text" id="p149"> 
   <p>If you remember, when I asked an LLM to complete the sentence “the dog’s bark was barely,” it correctly outputted “audible.” Despite its apparent simplicity, this sentence is challenging because the word “bark” has two distinct meanings—the noise made by a dog and the coating of a tree. If I asked an LLM to continue the sentence “the tree’s bark was barely,” then “audible” would be a poor choice. I tried it, and the LLM outputted “visible” instead of “audible.” The LLM managed to correctly disambiguate the word “bark” based on whether “dog” or “tree” appeared earlier in the sentence. The transformer architecture was especially designed to effectively disambiguate tokens based on their context. </p> 
  </div> 
  <div class="readable-text intended-text" id="p150"> 
   <p>Before the transformer architecture, the most popular language models were based on a type of AI model known as LSTM (long short-term memory). LSTMs try to predict the next token based on the following two things:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p151">The last token in the input prompt (“barely” if the input is “the dog’s bark was barely”)</li> 
   <li class="readable-text" id="p152">A single embedding that summarizes the meaning of all the previous tokens (a single embedding vector that represents “the dog’s bark was”)</li> 
  </ul> 
  <div class="readable-text" id="p153"> 
   <p>These two pieces of information are used to predict the next token (“audible” in this case). As the entire context before the last token is squashed into a single, fixed-sized embedding vector, LSTMs can process inputs of varying lengths without any complications. This is one of the reasons they became so popular. But this is also LSTMs’ Achilles’ heel—by squashing such a large context into a single vector, they often lose important, fine-grained contextual information necessary to properly guess the next word. </p> 
  </div> 
  <div class="readable-text intended-text" id="p154"> 
   <p>The transformer architecture solved this problem by processing the previous tokens in a different way, without squashing them all. The process, which follows three steps, is depicted in figure 1.9. </p> 
  </div> 
  <div class="browsable-container figure-container " id="p155">  
   <img alt="" src="../Images/CH01_F07_Maggiori.png" style="width: 100%; max-width: max-content;"/> 
   <h5 class=" figure-container-h5"><span class="">Figure 1.9</span><span class=""> </span><span class="">LLM overview. In step 1, the tokens are mapped to embeddings one by one. In step 2, each embedding is improved by contextualizing it using the previous tokens in the prompt. In step 3, <br/>the much-improved embeddings are used to make predictions about <br/>the next token.</span></h5>
  </div> 
  <div class="readable-text" id="p156"> 
   <p>First, the model maps each token in the input prompt to an embedding vector that seeks to represent its meaning. This is performed on each token separately, so no contextual information is used—each token is processed as if the other ones didn’t exist. While these embeddings can be okay sometimes, they can’t be too good because in many cases, it’s hard to know the true meaning of a token without looking at the context. For example, the embedding generated for a token such as “bark” will be poor because the model can’t know if it refers to dogs or trees. </p> 
  </div> 
  <div class="readable-text intended-text" id="p157"> 
   <p>In the second step, the LLM improves the embedding of each individual token by analyzing its previous tokens—each token is <em>transformed</em> by taking its context into account. Note that, compared to LSTMs, the transformer architecture does not squash embeddings together to summarize the entire prompt. </p> 
  </div> 
  <div class="readable-text intended-text" id="p158"> 
   <p>The LLM uses a fixed number of previous tokens to contextualize each token, which is known as the <em>context window.</em> For example, suppose an LLM has a context window of 10,000 tokens. Each token is contextualized by analyzing its previous 9,999 tokens. If the user’s prompt is shorter than 10,000 tokens, then the beginning of the prompt is padded with dummy values like zeros until it reaches 10,000 tokens. If the user’s prompt is longer than 10,000 tokens, then the LLM wrapper rejects the user request or drops the beginning of the prompt.</p> 
  </div> 
  <div class="readable-text intended-text" id="p159"> 
   <p>You need to carefully consider the context window before using an LLM. If you want to, say, ask an LLM to summarize an entire novel, you need to make sure that it fits within the context window, or the LLM won’t be able to summarize the entire novel at once. In addition, if you use a RAG approach to insert the content of relevant documents into a user’s prompt, you also need to make sure the context window can fit them all. Moreover, when you chat back and forth with an LLM-based app, the entire history of the conversation is usually included in each prompt, making the prompt longer as you converse with the chatbot.</p> 
  </div> 
  <div class="readable-text intended-text" id="p160"> 
   <p>Earlier LLMs had very limited context windows. For example, GPT-3’s context window was 2,048 tokens. Therefore, their capabilities to analyze long inputs were limited.</p> 
  </div> 
  <div class="readable-text intended-text" id="p161"> 
   <p>Over time, the context window has grown. As of this writing, OpenAI’s latest model, GPT-4o, has a context window of 128,000 tokens. And one of Google’s models, Gemini 1.5 Pro, offers a context window of 1 million tokens to its enterprise customers. The size of the context window is specified in an LLM’s official documentation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p162"> 
   <p>After the end of this contextualization step, the embeddings associated with each input token are much more accurate and thus useful than the initial ones, thanks to contextualization. For example, we could imagine that the embedding for “bark” becomes more animal-like at the end of step 2 if the word “dog” appears before. Conversely, its embedding would become more tree-like if the context contains tree references.</p> 
  </div> 
  <div class="readable-text intended-text" id="p163"> 
   <p>The third step in the transformer architecture (see figure 1.9) is to predict the next token based on the enhanced, contextualized embeddings generated in step 2. This is performed through a very simple mathematical operation because it is assumed that step 2 produced really good embeddings that can help guess the next word very easily. </p> 
  </div> 
  <div class="readable-text intended-text" id="p164"> 
   <p>In the next few sections, we describe each of the three steps in more detail, and we explain how machine learning enters the picture.</p> 
  </div> 
  <div class="readable-text" id="p165"> 
   <h3 class=" readable-text-h3"> Step 1: Initial embeddings</h3> 
  </div> 
  <div class="readable-text" id="p166"> 
   <p>The initial embeddings are obtained very easily. The LLM contains an internal dictionary that maps each possible token to its corresponding embedding. We could imagine it as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p167"> 
   <div class="code-area-container"> 
    <pre class="code-area">"a"        -&gt; [0, -1, 2, 3, 1, …]
"b"        -&gt; [1, -2, 0, 4, 0, …]
…
" bark"    -&gt; [1, 0, -1, 3, 1, …]
…
" dog"      -&gt; [3, -1, 0, 2, 3, …]
…</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p168"> 
   <p>The initial embeddings are created by looking up each token in the dictionary and replacing it with its corresponding embedding. The result is an initial set of embeddings, created one by one without context, which concludes step 1 (see figure 1.9).</p> 
  </div> 
  <div class="readable-text intended-text" id="p169"> 
   <p>The numbers inside the dictionary are not defined by hand. These numbers are all <em>learnable parameters</em> of the model. This means that the AI engineer leaves them as blanks in the code and lets the computer fill in their values later, when the learning algorithm runs. We can think of the previous dictionary as follows from the point of view of the AI engineer:  </p> 
  </div> 
  <div class="browsable-container listing-container" id="p170"> 
   <div class="code-area-container"> 
    <pre class="code-area">"a"     -&gt; [?, ?, ?, ?, ?, …]
"b"     -&gt; [?, ?, ?, ?, ?, …]
…
"bark"    -&gt; [?, ?, ?, ?, ?, …]
…
"dog"     -&gt; [?, ?, ?, ?, ?, …]
…</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p171"> 
   <p><span class="CharOverride-3">When the computer fills in the blanks, which is known as </span><em>learning</em><span class="CharOverride-3"> or </span><em>training,</em><span class="CharOverride-3"> it designs its own embedding space. So, the computer is free to organize tokens and pick topics as it wishes to attain its goal of effectively guessing the next word.</span></p> 
  </div> 
  <div class="readable-text intended-text" id="p172"> 
   <p>Consider a model whose vocabulary contains 100,000 different tokens and whose embeddings contain 10,000 dimensions, as is the case with many LLMs. The dictionary would contain 100,000 entries, and each entry would contain 10,000 numbers, which are question marks. The total number of learnable parameters (the question marks) would be 100,000 × 10,000 = 1 billion. That’s a lot of learnable parameters! And it’s just the beginning.</p> 
  </div> 
  <div class="readable-text" id="p173"> 
   <h3 class=" readable-text-h3"> Step 2: Contextualization</h3> 
  </div> 
  <div class="readable-text" id="p174"> 
   <p>In the second step, the LLM contextualizes each of the input tokens, one by one, by considering its previous tokens (within the context window). Let’s see, for example, how the LLM would contextualize the token “bark” in “dog’s bark”.</p> 
  </div> 
  <div class="readable-text intended-text" id="p175"> 
   <p>Contextualization starts by calculating an attention score for each token in the context. The attention score indicates how it’s best to divide attention among all the tokens in the context window to disambiguate the last one. For example, to contextualize “bark”, it’s worth focusing most of your attention on “dog,” followed by “bark” itself, and finally by “’s”. Figure 1.10 represents this operation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p176"> 
   <p>The calculation of attention scores, known as the <em>attention mechanism,</em> is performed through a series of mathematical operations, such as projections on the embedding vectors (see section 1.3.3). We won’t cover the details here, so let’s just say that these operations are specially designed to let the LLM extract meaning from the embeddings and compare them.  </p> 
  </div> 
  <div class="browsable-container figure-container " id="p177">  
   <img alt="" src="../Images/CH01_F08_Maggiori.png" style="width: 100%; max-width: max-content;"/> 
   <h5 class=" figure-container-h5"><span class="">Figure 1.10</span><span class=""> </span><span class="">The attention mechanism calculates the relative relevance of all tokens in the context window to contextualize or disambiguate the last token.</span></h5>
  </div> 
  <div class="readable-text" id="p178"> 
   <p>The AI engineer determines the type of number of operations but leaves blanks that are determined later using machine learning. For example, the numbers inside projection matrixes, which configure what projections do, are left as blanks. Thus, we can picture a projection matrix as</p> 
  </div> 
  <div class="browsable-container equation-container" id="p179"> 
   <p style="display: flex; align-items: center; justify-content: center;  text-align: center;">[[ ? ? ? ... ?],<br/>[ ? ? ? ... ?],<br/>...<br/>[ ? ? ? ... ?]]</p> 
  </div> 
  <div class="readable-text" id="p180"> 
   <p>So, the AI engineer tells the computer how to disambiguate tokens—by using projections to compare embeddings, and so on—but lets the machine fill in the details. The machine discovers by itself useful ways of analyzing the embeddings to disambiguate problematic tokens like “bark”. Projection matrices are rather large, so this step can easily add a few hundred million, if not billions of learnable parameters to the model. </p> 
  </div> 
  <div class="readable-text intended-text" id="p181"> 
   <p>Once the LLM has calculated attention scores, it uses the resulting values to guide the contextualization of tokens’ embedding vectors. We can think of this step as letting information from tokens rub off onto other tokens using the attention score for guidance. </p> 
  </div> 
  <div class="readable-text intended-text" id="p182"> 
   <p>For example, a lot of information from “dog” rubs off on “bark”, as its attention mechanism determined that the token “dog” was relevant to the meaning of “bark”. As a consequence of this step, the embedding for “bark” becomes more animal-like, as opposed to tree-like. Conversely, very little information from “’s” rubs off on “bark”, as the attention score deems it rather irrelevant. The process of updating the embeddings based on the context is known as the <em>feed-forward</em> step of the transformer.</p> 
  </div> 
  <div class="readable-text intended-text" id="p183"> 
   <p>In the previous example, the end result of the attention and feed-forward mechanisms is an improved version of the embedding for “bark”. The same process is applied to contextualize all the tokens in input the prompt, using their previous ones, which leads to a new generation of improved embeddings, as illustrated in step 2 of figure 1.9.</p> 
  </div> 
  <div class="readable-text intended-text" id="p184"> 
   <p>At the end of this process, the LLM is in a much better position to make a guess about the next token, as it contains an improved, contextualized representation of the meaning of the entire input prompt.</p> 
  </div> 
  <div class="readable-text" id="p185"> 
   <h4 class=" readable-text-h4">Multilayer architecture</h4> 
  </div> 
  <div class="readable-text" id="p186"> 
   <p><span class="CharOverride-3">The contextualization step we just described (step 2) is usually applied multiple times. So, the embedding vectors are improved many times. This is known as a </span><em>multilayer</em><span class="CharOverride-3"> transformer. Most LLMs contain at least a few tens of layers of transformers applied in sequence. Each transformer layer has its own set of learnable parameters, so each layer can specialize in different contextualization tasks. </span><span class="CharOverride-3"/></p> 
  </div> 
  <div class="readable-text intended-text" id="p187"> 
   <p>GPT-3, for example, has 96 transformer layers. This leads to a whopping total of 175 billion learnable parameters inside the model. </p> 
  </div> 
  <div class="readable-text" id="p188"> 
   <h4 class=" readable-text-h4">Multiheaded attention</h4> 
  </div> 
  <div class="readable-text" id="p189"> 
   <p>The attention mechanism is often subdivided into different heads, meaning that it analyzes different parts of the embedding vectors separately, one chunk at a time. This forces the LLM to design embedding vectors with highly specialized segments. For example, we could imagine that one segment is dedicated to all things animal related and another one to all things tree related, although we still can’t usually understand the embedding vectors. This has been observed to work better in practice than having a single head that processes the entire embedding vector at once.</p> 
  </div> 
  <div class="readable-text" id="p190"> 
   <h3 class=" readable-text-h3"> Step 3: Predictions</h3> 
  </div> 
  <div class="readable-text" id="p191"> 
   <p>The last step, step 3 in figure 1.9, is to make a prediction about the most likely next token, which is the LLM’s ultimate job. This is performed through projections over the contextualized embeddings generated in step 2.</p> 
  </div> 
  <div class="readable-text intended-text" id="p192"> 
   <p>While we’ve been saying that LLMs predict the most likely next token, that’s not quite accurate. In reality, they calculate a probability value for each possible token in the vocabulary. So, the LLM’s output is a vector with as many numbers as tokens in the vocabulary. Each position refers to one possible token, as shown in table 1.1. In this example, the token “audible” receives a high probability of 0.8, meaning that the LLM deems it a highly likely next token. </p> 
  </div> 
  <div class="browsable-container browsable-table-container" id="p193"> 
   <h5 class=" browsable-container-h5">Table 1.1<span class="CharOverride-8"> </span>In the last step, the LLM assigns a probability value <br/>for each possible token in the vocabulary. All the values add to 1.</h5> 
   <table> 
    <colgroup> 
     <col class="_idGenTableRowColumn-1"/> 
     <col class="_idGenTableRowColumn-1"/> 
     <col class="_idGenTableRowColumn-2"/> 
     <col class="_idGenTableRowColumn-1"/> 
     <col class="_idGenTableRowColumn-3"/> 
     <col class="_idGenTableRowColumn-1"/> 
    </colgroup> 
    <tbody> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style"> <p class="TableBody">0.01</p> </td> 
      <td class="No-Table-Style"> <p class="TableBody">0.0</p> </td> 
      <td class="No-Table-Style"> <p class="TableBody">0.05</p> </td> 
      <td class="No-Table-Style"> <p class="TableBody">…</p> </td> 
      <td class="No-Table-Style"> <p class="TableBody">0.8</p> </td> 
      <td class="No-Table-Style"> <p class="TableBody">…</p> </td> 
     </tr> 
     <tr class="No-Table-Style _idGenTableRowColumn-4"> 
      <td class="No-Table-Style"> <p class="TableBody">“a”</p> </td> 
      <td class="No-Table-Style"> <p class="TableBody">“b”</p> </td> 
      <td class="No-Table-Style"> <p class="TableBody">“c”</p> </td> 
      <td class="No-Table-Style"> </td> 
      <td class="No-Table-Style"> <p class="TableBody">“audible”</p> </td> 
      <td class="No-Table-Style"> </td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p194"> 
   <p>The LLM wrapper picks the next token based on the LLM’s output probabilities. One way to do this is to pick the token with the highest probability according to the LLM (in the unlikely event that two tokens have the exact same probability, either one can be picked at random). However, there are other ways to do this, which lets the LLM get more adventurous. We will discuss this next.</p> 
  </div> 
  <div class="readable-text" id="p195"> 
   <h3 class=" readable-text-h3"> Temperature</h3> 
  </div> 
  <div class="readable-text" id="p196"> 
   <p><span class="CharOverride-3">As discussed in the previous section, LLMs output a probability for each token that describes how likely it is to come right after the input prompt. One way to select the next token from the vocabulary is to pick the one with the highest probability according to the LLM. However, this encourages the LLM to play it a bit too safe—sometimes we want a more adventurous output. So, instead, the next token is often selected by randomly </span><em>sampling</em><span class="CharOverride-3"> a token from the vocabulary using the LLM’s output probabilities. For example, if the LLM outputs a probability of 0.9 for the “audible” token, then the sampler picks that token with 90% probability and other ones with 10% probability.</span><span class="CharOverride-3"/></p> 
  </div> 
  <div class="readable-text intended-text" id="p197"> 
   <p>The user can usually regulate how adventurous the output should be by adjusting a setting known as the <em>temperature. </em>This setting squeezes or smooths out the LLM’s output probabilities. A low temperature pushes the highest sampling probabilities upward and lowers the others. For example, a probability of 0.9 may be transformed into 0.95, while a probability of 0.05 may be transformed into 0.01. This makes it more likely for the LLM wrapper to pick tokens at the top of the ranking. We can think of this as making the LLM wrapper more conservative, as it becomes more prone to select the most obvious tokens at the top of the ranking and less prone to pick alternative ones. </p> 
  </div> 
  <div class="readable-text intended-text" id="p198"> 
   <p>Conversely, a high temperature smooths out probabilities. For example, a probability of 0.9 may be transformed to 0.8, and a probability of 0.01 may be transformed to 0.05. This makes the output more creative by making lower-ranked tokens more likely to be picked. Each LLM wrapper offers its own range of temperature values. OpenAI’s API, for example, allows users to set the temperature to a value between zero (conservative) and two (creative).</p> 
  </div> 
  <div class="readable-text intended-text" id="p199"> 
   <p>In the following paragraphs, we describe two alternative ways of setting how adventurous we want our output to be. </p> 
  </div> 
  <div class="readable-text" id="p200"> 
   <h4 class=" readable-text-h4">Top-p</h4> 
  </div> 
  <div class="readable-text" id="p201"> 
   <p><span class="CharOverride-3">An alternative setting known as </span><em>Top-p</em><span class="CharOverride-3"> is a cutoff level of cumulative probability. If we set Top-p to, say, 0.8, then we only sample from the top tokens that cover 80% of the probability. The tokens covering the bottom 20% of probability are ignored.</span><span class="CharOverride-3"/></p> 
  </div> 
  <div class="readable-text" id="p202"> 
   <h4 class=" readable-text-h4">Top k</h4> 
  </div> 
  <div class="readable-text" id="p203"> 
   <p><span class="CharOverride-3">The </span><em>Top-k</em><span class="CharOverride-3"> setting imposes a limit on the number of top tokens we can sample from. For example, if we set Top-k to 20, the LLM wrapper is only allowed to pick a token among the top-20 tokens. If we set Top-k to 1, we force the LLM to pick the top token every time.</span><span class="CharOverride-3"/></p> 
  </div> 
  <div class="readable-text intended-text" id="p204"> 
   <p>Note that not all LLM wrappers let users configure all these settings—sometimes only one or two of them are available. For example, as of today, OpenAI lets users set temperature and Top-p but not Top-k. The available settings are described in the documentation. </p> 
  </div> 
  <div class="readable-text" id="p205"> 
   <h3 class=" readable-text-h3"> Can you get an LLM to always output the same thing?</h3> 
  </div> 
  <div class="readable-text" id="p206"> 
   <p><span class="CharOverride-3">It is sometimes desirable to generate </span><em>reproducible </em><span class="CharOverride-3">outputs with an LLM, meaning that the LLM generates the exact same output every time it’s given the same input prompt. This can be useful to benchmark the performance of LLMs or share examples of LLMs’ outputs that others can replicate. </span><span class="CharOverride-3"/></p> 
  </div> 
  <div class="readable-text intended-text" id="p207"> 
   <p>It is theoretically possible to have an LLM generate reproducible outputs. For example, this could be achieved by using a top-1 sampling strategy, in which we always pick the token with the highest probability, thus making sure that all mathematical calculations inside the LLM are performed exactly the same way on different runs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p208"> 
   <p>However, while this is theoretically possible, it is not always the case in practice. As of today, for example, it’s not possible to guarantee that OpenAI’s LLMs will generate the exact same output on different runs. There is official guidance on how to configure settings to produce mostly reproducible outputs, but they’re not guaranteed to be exactly alike (see <a href="https://mng.bz/PdeR"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/PdeR</span></a>).</p> 
  </div> 
  <div class="readable-text intended-text" id="p209"> 
   <p>This probably happens because popular AI and arithmetic libraries divide a calculation into multiple threads which can be executed in different orders every time (see <a href="https://news.ycombinator.com/item?id=37006224"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/news.ycombinator.com/item?id=37006224</span></a>). This can cause slight differences in outputs due to round-off errors when adding the same numbers in different orders (see <a href="https://mng.bz/JYQZ"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/JYQZ</span></a>). In the future, if these problems are fixed, it will be possible to generate reproducible outputs with popular LLM APIs.</p> 
  </div> 
  <div class="readable-text" id="p210"> 
   <h3 class=" readable-text-h3"> Where to learn more</h3> 
  </div> 
  <div class="readable-text" id="p211"> 
   <p>In this section, we’ve covered the gist of how LLMs work. We haven’t discussed the implementation details, such as the exact calculations performed inside the LLM, but we did discuss the overall process LLMs follow to make their predictions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p212"> 
   <p>If you want to know the details, I recommend you to directly have a look at the publicly available source code of GPT-2 (<a href="https://mng.bz/wJR5"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/wJR5</span></a>). The file called models.py is the most important one; it defines the entire model in a very compact way (just 174 lines). The code is moderately easy to follow if you understand some Python coding and the TensorFlow library and start from the bottom of the file. I also recommend you read a guide called <em>The Illustrated Transformer</em> (<a href="https://mng.bz/qxlx"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/qxlx</span></a>) to learn the details of the architecture.</p> 
  </div> 
  <div class="readable-text intended-text" id="p213"> 
   <p>Even if you don’t want to go through all the code, a quick skim through it reveals that the LLM is genuinely just a sequence of simple mathematical operations. As you can see in the code, each layer (called a “block”) first calculates the attention scores (“attn”) and then uses them to update the embeddings (“mlp”). Projections (“matmul”) are among the most common operations performed by the model.</p> 
  </div> 
  <div class="readable-text intended-text" id="p214"> 
   <p>We’ve now covered how LLMs generate their predictions and mentioned that their details are filled in using machine learning. We haven’t, however, described how learning unfolds. That’s where we move next.</p> 
  </div> 
  <div class="readable-text" id="p215"> 
   <h2 class=" readable-text-h2">Machine learning </h2> 
  </div> 
  <div class="readable-text" id="p216"> 
   <p>In traditional software development, the engineer writes every single line of code to tell the computer exactly what to do. Machine learning, or ML, is a different way of creating programs (these programs are known as <em>models </em>in ML jargon). </p> 
  </div> 
  <div class="readable-text intended-text" id="p217"> 
   <p>The ML approach comprises two steps. The first step is designing the <em>architecture</em> of the solution, which in ML means a template of the steps the program will follow to accomplish the task. Have a look at a piece of Python code using the popular ML library PyTorch:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p218"> 
   <div class="code-area-container"> 
    <pre class="code-area">import torch
embedding = <span class=" link-like">torch.nn</span>.Embedding(num_embeddings=100000, embedding_dim=10000)
projection = <span class=" link-like">torch.nn</span>.Linear(10000, 2000)
model = <span class=" link-like">torch.nn</span>.Sequential(embedding, projection)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p219"> 
   <p>In the first line, the engineer defines an embedding operation that maps a vocabulary of 100,000 tokens to embedding vectors of length 10,000, similar to what LLMs do. In the second line, the engineer defines a projection to transform an embedding vector of length 10,000 into one of length 2,000. The third line applies each of those operations sequentially, first the embedding and then the projection. </p> 
  </div> 
  <div class="readable-text intended-text" id="p220"> 
   <p>We can see that the engineer puts together the building blocks of the model manually. However, the model has blanks in it, known as <em>parameters,</em> which are not defined by hand. In the above example, the embedding block contains 1 billion parameters (100,000 × 10,000) which are not defined by hand. The second building block, the projection, contains over 20 million parameters (I’ll leave the math to you).</p> 
  </div> 
  <div class="readable-text intended-text" id="p221"> 
   <p>Note that the architecture of a machine learning model is designed carefully—the building blocks are introduced with a specific intention in mind and in a way that is tailor-made to the application. For example, the transformer architecture is designed to contextualize words. </p> 
  </div> 
  <div class="readable-text intended-text" id="p222"> 
   <p>The following step in the ML approach is known as <em>training</em> or <em>learning. </em>The choice between these two words is down to grammar—you typically say that a person trains a model and that the machine learns. </p> 
  </div> 
  <div class="readable-text intended-text" id="p223"> 
   <p>During training, the engineer runs an algorithm that tries to find the best way of setting the model’s parameters (filling in the blanks in the template) to accomplish the desired task. The training algorithm uses data for guidance—usually lots of it—to find promising ways of adjusting the parameter values to improve the model’s performance. </p> 
  </div> 
  <div class="readable-text intended-text" id="p224"> 
   <p>The training step is time-consuming and data-hungry, but, if all goes well, the resulting model is often seen to perform much better than if we tried to write the entire program by hand. This is mainly due to the following reasons:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p225">The process is data-driven, so we rely on evidence to build the best model instead of intuition.</li> 
   <li class="readable-text" id="p226">The model can be millions of times larger than a manually written program.</li> 
   <li class="readable-text" id="p227">The training process can identify serendipitous ways of solving the problem that engineers wouldn’t rely on if writing the program manually.</li> 
  </ul> 
  <div class="readable-text" id="p228"> 
   <p>Throughout this section, we’ll dig a bit deeper into how machine learning works and discuss common terminology. </p> 
  </div> 
  <div class="readable-text" id="p229"> 
   <h3 class=" readable-text-h3"> Deep learning</h3> 
  </div> 
  <div class="readable-text" id="p230"> 
   <p>In earlier machine learning, the engineer would first write a dedicated piece of software to extract representative <em>features</em> from the input. For example, the engineer would write a dedicated algorithm to extract keywords from text or detect lines in an image. Afterward, a small ML model would be used to make predictions from these manually engineered features. This process can be summarized as follows:</p> 
  </div> 
  <div class="readable-text centered-text" id="p231"> 
   <p>Raw input -&gt; Feature engineering -&gt; Model -&gt; Prediction</p> 
  </div> 
  <div class="readable-text" id="p232"> 
   <p>In deep learning, which is a type of machine learning, the model processes much rawer inputs, such as tokens or an unprocessed input image:</p> 
  </div> 
  <div class="readable-text centered-text" id="p233"> 
   <p>Raw input -&gt; Model -&gt; Prediction</p> 
  </div> 
  <div class="readable-text" id="p234"> 
   <p>In deep learning, the model itself learns a useful way to represent the input—it performs its own feature engineering. We saw that in action with LLMs: the machine works hard to produce contextualized embeddings to represent the meaning of the input tokens. To process rawer inputs, the model usually contains multiple layers of processing stacked on top of each other, which is where the name “deep” comes from.</p> 
  </div> 
  <div class="readable-text intended-text" id="p235"> 
   <p>In many applications within text generation and image analysis, deep learning is much more accurate than the previous two-step process with manually engineered features. This requires, however, devising an effective architecture for the task, such as the transformer architecture.</p> 
  </div> 
  <div class="readable-text intended-text" id="p236"> 
   <p>Note that there’s still a place for old-school, “shallow” learning. Whenever your input is already abstract and informative—say, patient records with their age, blood type, and so on—then all you need is a shallow ML model on top. In addition, deep learning models are too large to understand, so it’s hard to know exactly how they produce outputs. We need to trust them based on their high performance. But sometimes you want to have an explainable model that you can fully understand. In that case, a more explainable model over manually engineered features may be the right choice.</p> 
  </div> 
  <div class="readable-text" id="p237"> 
   <h3 class=" readable-text-h3"> Types of machine learning</h3> 
  </div> 
  <div class="readable-text" id="p238"> 
   <p>In this section, we discuss the four most common ML paradigms. These paradigms differ in terms of how they formulate the task and process the training data. Afterward, we discuss which of these paradigms is used by LLMs.</p> 
  </div> 
  <div class="readable-text" id="p239"> 
   <h4 class=" readable-text-h4">Supervised learning</h4> 
  </div> 
  <div class="readable-text" id="p240"> 
   <p>Most ML models learn by example. You supply the computer with a large—or even huge—number of examples of how to do the job you want it to do. This is known as <em>supervised learning.</em> In supervised learning, each example is a pair of an input and its corresponding <em>label,</em> which is the “true” output we’d like the model to learn how to produce.  </p> 
  </div> 
  <div class="readable-text intended-text" id="p241"> 
   <p>In the case of LLMs, training examples are sentences labeled with the “correct” next-token guess, such as</p> 
  </div> 
  <div class="readable-text list-body-item" id="p242"> 
   <p>“Better safe than” / “sorry”<br/>“The Eiffel” / “Tower”</p> 
  </div> 
  <div class="readable-text" id="p243"> 
   <p>This way, the LLM is shown examples of how to perform the exact task it is expected to perform. All the examples supplied to the machine make up its <em>training data.</em><em/></p> 
  </div> 
  <div class="readable-text intended-text" id="p244"> 
   <p>To cite another example, in the case of a model for automated image categorization, the training data contains thousands of sample images, each of them labeled with their right category (“strawberry,” “plane,” “dog,” and so on). </p> 
  </div> 
  <div class="readable-text intended-text" id="p245"> 
   <p>Gathering labeled data often requires manual work. For example, to create an ML model for image categorization, people are often hired to manually label tens of thousands of images with their respective categories. Sometimes, there is no way to escape this, and data labeling becomes a costly and time-consuming bottleneck. In other cases, it’s possible to use tricks to generate labels automatically by analyzing existing data, which we will discuss soon. </p> 
  </div> 
  <div class="readable-text intended-text" id="p246"> 
   <p>Ideally, the machine will learn a general process to perform the required task. So, it will also work well with inputs not exactly present in the training data, such as new sentences or new images. When this happens, the model is said to <em>generalize.</em> </p> 
  </div> 
  <div class="readable-text intended-text" id="p247"> 
   <p>In some unfortunate cases, the model memorizes specific training examples instead of learning a general process to perform the task. So, it doesn’t work well when it must do its job on data not seen during training. This is known as <em>overfitting.</em> In other cases, a model might learn a process that is too simple, so it doesn’t work effectively on training data or other data. This is known as <em>underfitting.</em><em/></p> 
  </div> 
  <div class="readable-text" id="p248"> 
   <h4 class=" readable-text-h4">A note on simulated data</h4> 
  </div> 
  <div class="readable-text" id="p249"> 
   <p>As of late, people have been asking me why they can’t just run a computer program to generate simulated training samples (also known as synthetic data) instead of going through the painstaking process of collecting and manually labeling data. Imagine you had a program that could generate training examples for an LLM. That program would have to be able to correctly guess the next word given a prompt to generate examples such as “Better safe than” / “sorry”. But that program would already be an LLM. If you had such a program to effectively generate correctly labeled training examples, then you wouldn’t need to build an LLM in the first place!</p> 
  </div> 
  <div class="readable-text intended-text" id="p250"> 
   <p>The confusion about simulated data seems to arise from the fact that, in a few narrow scenarios, it is indeed possible to create training data by simulation. This was the case with AlphaZero, the famous ML model that beat a human player at the game of Go. Its creators had a computer play Go against itself to generate millions of simulated games and generate training examples. But this was only possible because it’s easy to calculate the end result of a game—you can easily tell who won. This isn’t the case with most applications outside game-playing. For example, you can’t easily tell what the next token is unless you already have an LLM, and you can’t easily tell an image’s category unless you already have an effective image categorization model. </p> 
  </div> 
  <div class="readable-text intended-text" id="p251"> 
   <p>Some people also suggest augmenting your existing training data by automatically creating new training examples from combinations of existing ones. One technique called SMOTE (<em class="CharOverride-4">synthetic minority oversampling technique</em>), for instance, is sometimes used to generate more examples of an underrepresented category. Suppose you’re trying to train an ML model to detect whether a credit card transaction is fraudulent. The training data may contain very few instances of transactions labeled as fraud because (hopefully) fraud doesn’t happen all that often. By using SMOTE, the AI engineer creates additional examples of fraudulent transactions by combining existing ones. However, this doesn’t add any <em>new</em> information to the training data. So, the machine cannot learn anything new with this extra data that it couldn’t learn before (for a more detailed discussion, see <a href="https://mng.bz/7paQ"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/7paQ</span></a>). I advise you to be careful if anyone suggests you should concoct fake data to improve the performance of your model. In most cases, such fake data is used to compensate for a poor formulation of the task and not a necessity.</p> 
  </div> 
  <div class="readable-text" id="p252"> 
   <h4 class=" readable-text-h4">Self-supervised learning</h4> 
  </div> 
  <div class="readable-text" id="p253"> 
   <p>In some applications, it’s possible to generate a huge number of labeled examples by automatically extracting information from existing data. This is known as <em>self-supervised learning.</em> </p> 
  </div> 
  <div class="readable-text intended-text" id="p254"> 
   <p>Imagine that an AI engineer collects a huge amount of text from the internet. The engineer then extracts thousands of sentences from it and removes the last token from each, turning it into the label. The result is a large number of examples of how to guess the next token from the previous ones, which is exactly what LLMs need. </p> 
  </div> 
  <div class="readable-text intended-text" id="p255"> 
   <p>Suppose a sentence in the data is “The Eiffel Tower is in Paris.” The engineer generates the training examples by using the previous process, as shown in figure 1.10.</p> 
  </div> 
  <div class="browsable-container figure-container " id="p256">  
   <img alt="" src="../Images/CH01_UN03_Maggiori.png" style="width: 100%; max-width: max-content;"/> 
   <h5 class=" figure-container-h5">Figure 1.10 Training examples are generated by subdividing existing sentences and turning the last token in each into the desired autocomplete label.</h5>
  </div> 
  <div class="readable-text" id="p257"> 
   <p>Afterward, supervised machine learning is used to train the LLM from these examples. Machine learning is still supervised, because it’s based on examples of how to do the job. But we say it’s “self” supervised to indicate that the labels were generated automatically from our original data source as opposed to obtained elsewhere, such as through manual labeling.</p> 
  </div> 
  <div class="readable-text intended-text" id="p258"> 
   <p>This trick works only when we can formulate the task as learning to reconstruct a corrupted input. In the case of LLMs, we artificially corrupt the input by removing the last token and then ask the LLM to reconstruct it by guessing that token. The fact that we can use this trick is probably one of the main reasons for LLMs’ success, as it’s possible to generate a huge number of training examples without manual labeling. </p> 
  </div> 
  <div class="readable-text intended-text" id="p259"> 
   <p>This isn’t the case, however, with most ML applications. For example, when building a model for image categorization, we cannot use the self-supervised trick. Suppose our data contains a picture of a strawberry. The label “strawberry” is not available inside the picture, so we can’t remove it and then ask the model to guess it as we do with LLMs. </p> 
  </div> 
  <div class="readable-text" id="p260"> 
   <h4 class=" readable-text-h4">Reinforcement learning</h4> 
  </div> 
  <div class="readable-text" id="p261"> 
   <p>In an alternative, less common ML paradigm, the computer learns by trial and error. The training algorithm picks random actions, tries them out, and learns from feedback collected afterward. For example, suppose an advertising platform wants to create a model of a user’s interests using machine learning. The advertiser first shows random ads every time the user visits a webpage and registers whether the user clicked on the ad or not—this is known as <em>exploration.</em> Over time, the training algorithm identifies the kinds of things the user is interested in based on their clicks. Once the advertiser has an idea of the user’s interests, it starts showing relevant ads to them—this is known as <em>exploitation. </em><em/></p> 
  </div> 
  <div class="readable-text intended-text" id="p262"> 
   <p>The technique of learning by trial and error is known as <em>reinforcement learning, </em>or <em>RL</em><em>.</em> One of the major research topics in this field is how to balance exploitation and exploration over time. For example, after user preferences are discovered, the advertiser may still want to sometimes show random ads to the user to discover new preferences.</p> 
  </div> 
  <div class="readable-text intended-text" id="p263"> 
   <p>While RL has been successful in some applications, its use in a commercial setting is rare. This is probably because learning by trial and error is a rather wasteful way of learning compared to supervised learning, in which we directly provide the machine with examples of how to do the job. </p> 
  </div> 
  <div class="readable-text intended-text" id="p264"> 
   <p>With the rise of LLMs, there is a new flavor of RL that has become popular, called <em>reinforcement learning with human feedback,</em> or RLHF. This technique is used to improve an existing LLM. It works as follows: an army of human workers are asked to manually create thousands of imaginary LLM prompts and pairs of alternative LLM outputs, and they are asked to label the alternative outputs based on preference (“best” versus “not best”). Afterward, AI engineers train a supervised ML model to guess whether an LLM output is good or bad based on these manually labeled examples. The result is an LM model, called the <em>reward model,</em> which is especially designed to determine whether an LLM’s output is good or bad.</p> 
  </div> 
  <div class="readable-text intended-text" id="p265"> 
   <p>Afterward, the AI engineers run a reinforcement learning algorithm to refine an existing LLM. The algorithm generates random LLM outputs and determines how good they are using the reward model. The feedback from the reward model is used to slightly improve the LLM. This algorithm progressively refines the LLM by better aligning it with what the human labelers considered good outputs.</p> 
  </div> 
  <div class="readable-text" id="p266"> 
   <h4 class=" readable-text-h4">Unsupervised learning</h4> 
  </div> 
  <div class="readable-text" id="p267"> 
   <p>Our final machine learning category is <em>unsupervised learning.</em> In this paradigm, we do not supply the machine with examples of the “right output.” In fact, there is no such notion because the task doesn’t have a single right answer. Unsupervised learning is typically used to explore data and find patterns in it. </p> 
  </div> 
  <div class="readable-text intended-text" id="p268"> 
   <p>The most common example of unsupervised learning is <em>clustering</em>, in which we try to group similar data points together. For example, we may want to group similar patients together based on their medical records to create a handful of imaginary representative patients and analyze them. There is no notion of the “right group” a patient should belong to. We could group them into two, three, or five clusters, and there is no conclusive way of determining which number of clusters is the right one. </p> 
  </div> 
  <div class="readable-text intended-text" id="p269"> 
   <p>Because there is no uniquely right model, we cannot measure the success of an unsupervised learning algorithm in a clear-cut way. That’s why people often suggest a multitude of rules of thumb to use unsupervised learning. Some of them are poorly defined. For example, they suggest creating many different models, calculating a metric for each, plotting a curve with the results, and finally, picking the model at the “knee” or “elbow” of the curve. The popular book <em>The Elements of Statistical Learning </em>(2nd ed., Penguin, 2009) by Hastie et al. explains the conundrum as follows:</p> 
  </div> 
  <div class="readable-text" id="p270"> 
   <blockquote>
    <div>
     <span class="CharOverride-9">With supervised learning there is a clear measure of success, or lack thereof, that can be used to judge adequacy in particular situations and to compare the effectiveness of different methods over various situations. . . . In the context of unsupervised learning, there is no such direct measure of success. It is difficult to ascertain the validity of inferences drawn from the output of most unsupervised learning algorithms. One must resort to heuristic arguments not only for motivating the algorithms, as is often the case in supervised learning as well, but also for judgments as to the quality of the results. This uncomfortable situation has led to heavy proliferation of proposed methods, since effectiveness is a matter of opinion and cannot be verified directly. </span> 
     <span class="CharOverride-10">(p. 486)</span>
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p271"> 
   <p>In my experience, many of the people who try to use unsupervised learning need supervised learning instead. </p> 
  </div> 
  <div class="readable-text intended-text" id="p272"> 
   <p>For example, I know an engineer from a hospital who was trying to predict the severity of a patient’s disease. He used a clustering algorithm to automatically group patients together into a handful of representative patients. Afterward, when a new patient arrived, he tried to triage them based on their closest cluster.</p> 
  </div> 
  <div class="readable-text intended-text" id="p273"> 
   <p>It didn’t work well, and the engineer was quite frustrated. He’d tried several popular approaches to create good clusters. He asked me, “How can I find high-quality clusters, so that triage works well?” I explained to him that there is no such thing; you cannot evaluate the quality of clusters independently of what you want to use them for. What he really needed was supervised learning trained on pairs of patient records with their expected triage outcomes. </p> 
  </div> 
  <div class="readable-text intended-text" id="p274"> 
   <p>Note that sometimes people use the term “unsupervised learning” to refer to supervised learning without manually generated labels, which only adds extra confusion to the matter. </p> 
  </div> 
  <div class="readable-text" id="p275"> 
   <h3 class=" readable-text-h3"> How LLMs are trained (and tamed)</h3> 
  </div> 
  <div class="readable-text" id="p276"> 
   <p>The first LLMs were built using only self-supervised learning. The AI engineers collected a huge amount of text from the internet and generated training examples automatically using the process described above (“Better safe than sorry” / “Better safe than” / “Sorry”). One popular source of data was Common Crawl, a database that contains a huge amount of text gathered from all over the internet. Another popular source of text was Books3, a database of 190,000 books. Note that a lot of this data was collected without authorization from its authors; we’ll return to this controversial topic later. </p> 
  </div> 
  <div class="readable-text intended-text" id="p277"> 
   <p>In a 2018 paper, OpenAI researchers revealed that their largest model until then, GPT-2, managed to perform impressive tasks just by using self-supervised learning (Redford et al., 2019). This promising result made them very ambitious about this approach. They speculated that the large amount of data available on the internet combined with self-supervised learning could lead to LLMs that learned to perform all sorts of tasks:</p> 
  </div> 
  <div class="readable-text" id="p278"> 
   <blockquote>
    <div>
     <span class="CharOverride-9">The internet contains a vast amount of information that is passively available without the need for interactive communication. Our speculation is that a language model with sufficient capacity will begin to learn to infer and perform the tasks demonstrated in natural language sequences [e.g., asking the LLM to translate or summarize text] in order to better predict them [guess the next word], regardless of their method of procurement.</span>
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p279"> 
   <p>In addition, they argued that the task of guessing the next word encompassed many other tasks, so it was generally enough to build really powerful LLMs. By using jargon from the mathematical optimization field, they explained that the “global minimum” (the best solution) to the next-token-prediction task coincided with the “global minimum” (the best solution) to perform all sorts of other tasks. So, striving to find the best solution to the next-token-prediction task was equivalent to striving to find the best solution to other tasks. </p> 
  </div> 
  <div class="readable-text intended-text" id="p280"> 
   <p>But enthusiasm didn’t last long. While LLMs trained using the self-supervised approach worked very well in many cases, they also erred badly in others. In addition, sometimes they generated inappropriate outputs. Researchers from OpenAI discussed the problem in a 2022 paper (available at <a href="https://arxiv.org/pdf/2203.02155"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/pdf/2203.02155</span></a><span class="Hyperlink">)</span>:</p> 
  </div> 
  <div class="readable-text" id="p281"> 
   <blockquote>
    <div>
     These models often express unintended behaviors such as making up facts, generating biased or toxic text, or simply not following user instructions. This is because the language modeling objective used for many recent large LMs—predicting the next token on a webpage from the internet—is different from the objective “follow the user’s instructions helpfully and safely.”
    </div>
   </blockquote> 
  </div> 
  <div class="readable-text" id="p282"> 
   <p>As this quote illustrates, the researchers attributed the problem to a misalignment between what we <em>really </em>want from LLMs—produce factual, appropriate text—and what they’re trained to do—guess the next token according to text collected from the internet. </p> 
  </div> 
  <div class="readable-text intended-text" id="p283"> 
   <p>The OpenAI researchers proposed a solution to this problem, called InstructGPT (<a href="https://arxiv.org/abs/2203.02155"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/abs/2203.02155</span></a><span class="Hyperlink">)</span>, which trains the LLM in four steps. First, the model is trained the usual way by using internet data in a self-supervised way. </p> 
  </div> 
  <div class="readable-text intended-text" id="p284"> 
   <p>Second, human workers are hired to manually write thousands of examples of input prompts and their corresponding desired outputs. These manually written examples provide extra training data to help improve and “tame” the model, for example, by showing it how to perform popular tasks, have two-way conversations, and refuse to answer inappropriate questions. According to a <em>Time</em> article, “OpenAI used Kenyan Workers on less than $2 per hour” for the job of labeling data (<a href="https://mng.bz/mGP8"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/mGP8</span></a><span class="Hyperlink">)</span>. This goes to show that training high-performing LLMs is more manual than it seems at first sight.</p> 
  </div> 
  <div class="readable-text intended-text" id="p285"> 
   <p>Third, the existing LLM is <em>fine-tuned</em> using the manually generated data. This means that its parameters are slightly adjusted through a few extra rounds of training with the new examples. </p> 
  </div> 
  <div class="readable-text intended-text" id="p286"> 
   <p>The fourth step is to use reinforcement learning with human feedback to refine the LLM even further (see the explanation under <span class="CharOverride-3">“</span><em>Reinforcement learning”</em>). In this case, humans are asked to manually rank alternative LLM outputs based on their quality, which provides feedback to the training algorithm to improve the LLM. </p> 
  </div> 
  <div class="readable-text intended-text" id="p287"> 
   <p>ChatGPT was the first popular model trained using steps 1–4. This turn of events may have caused some serious disappointment among those who believed that the highest performing LLMs would be created just from data collected from the internet, without any manual labeling. </p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p288"> 
    <h5 class=" callout-container-h5 readable-text-h5">A note on privacy</h5> 
   </div> 
   <div class="readable-text" id="p289"> 
    <p>As people use LLMs, their conversations may be recorded by the LLM provider. The resulting data may be used to improve models, either automatically—by generating new training data and fine-tuning the model—or manually—by having employees identify recurring problems faced by users and come up with ways of fixing them. Some apps such as ChatGPT let users rate answers with a thumbs up or thumbs down, and they sometimes ask users to rank alternative answers, which might be later used to improve the LLMs. </p> 
   </div> 
   <div class="readable-text" id="p290"> 
    <p>You should be careful if you include sensitive information within an LLM’s prompt, as it might be seen or used by the staff who works on creating and improving LLMs. You’ll probably be able to opt out from your prompts being recorded. For example, OpenAI’s website explains, “When you use our services for individuals such as ChatGPT, we may use your content to train our models. You can opt out of training through our privacy portal. . . . We do not use content from our business offerings such as ChatGPT Team or ChatGPT Enterprise to train our models.”</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p291"> 
   <h3 class=" readable-text-h3"> Loss</h3> 
  </div> 
  <div class="readable-text" id="p292"> 
   <p>Let’s move on to the topic of how ML models learn. The first ingredient is a way of assessing the quality of a given model, known as the <em>loss</em> or <em>cost.</em> This is used by the learning algorithm to compare alternative models (with different parameter values) and find opportunities for improvement. </p> 
  </div> 
  <div class="readable-text intended-text" id="p293"> 
   <p>The loss calculates how inaccurate the outputs of a model are compared to the training examples—the higher the value, the worse the model. </p> 
  </div> 
  <div class="readable-text intended-text" id="p294"> 
   <p>Consider a training example, “The Eiffel” paired with its corresponding label “Tower”, which is used to train an LLM. Our goal is to calculate a loss value that measures how far off the LLM’s output is when given the input “The Eiffel”.</p> 
  </div> 
  <div class="readable-text intended-text" id="p295"> 
   <p>The loss is calculated by looking at the probability the LLM assigns to the right token, such as “Tower” in this case. If the probability is high, the loss is low, and vice versa. This is calculated by taking the negative logarithm of the probability, which is known as the <em>cross-entropy loss</em> or <em>log loss. </em>The loss is zero if the probability of “ Tower” is 1.0 (−log(1) = 0), and it takes an increasingly higher value the lower the probability assigned to “Tower” (e.g., −log(0.2) = 1.6 and −log(0.1) = 2.3).</p> 
  </div> 
  <div class="readable-text intended-text" id="p296"> 
   <p>The loss over the entire dataset is calculated by adding the individual losses of each of the training examples. The better the model is at guessing the correct next token according to the training data, the higher the probabilities it assigns to them, and the lower the loss. Mission accomplished.</p> 
  </div> 
  <div class="readable-text intended-text" id="p297"> 
   <p>Note, however, that the loss measures the performance of the model on <em>training</em> data. The AI engineer hopes that a lower loss will translate to a higher performance on unseen, new data. But this isn’t always the case; if the model suffers from overfitting, it memorizes individual instances of the training data, thus achieving a low loss, but it doesn’t work well with other data. </p> 
  </div> 
  <div class="readable-text" id="p298"> 
   <h3 class=" readable-text-h3"> Stochastic gradient descent</h3> 
  </div> 
  <div class="readable-text" id="p299"> 
   <p>So far, we’ve described the following ML ingredients:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p300">The architecture of a model, which contains learnable parameters (“blanks”)</li> 
   <li class="readable-text" id="p301">Training examples</li> 
   <li class="readable-text" id="p302">A way to measure the quality of a model (the loss) according to the training examples</li> 
  </ul> 
  <div class="readable-text" id="p303"> 
   <p>The only remaining ingredient is an algorithm to find the best way to adjust the parameters, so that the model yields the lowest loss. </p> 
  </div> 
  <div class="readable-text intended-text" id="p304"> 
   <p>The most common algorithm for this, used to build LLMs and many other ML models, is <span class="CharOverride-4">stochastic gradient descent</span> (SGD). It works as follows. First, all the parameters inside the model are initialized using random values. So, this first version of the model is completely useless at the task at hand—for example, the next-token predictions of the LLM are nonsensical.</p> 
  </div> 
  <div class="readable-text intended-text" id="p305"> 
   <p>Afterward, the training algorithm selects a small number of training samples, called a <em>batch</em> or <em>minibatch,</em> to calculate a promising way of slightly modifying the model’s parameters to reduce the loss on that batch. In calculus jargon, this amounts to computing the <em>gradient</em> of the loss. We can think of this as wiggling the parameters a little bit to find a promising direction of change. Think of an optometrist slightly varying your glasses prescription and asking you if you see better than before. Afterward, the training algorithm slightly modifies the model’s parameters according to the promising direction it just found, hoping this will slightly improve the model. </p> 
  </div> 
  <div class="readable-text intended-text" id="p306"> 
   <p>Note that only a batch of training examples is used for this calculation, instead of the entire training data. This is why the algorithm is said to be <em>stochastic</em>, because you estimate the gradient based on a sample of the data instead of all the data. This makes the process much quicker.</p> 
  </div> 
  <div class="readable-text intended-text" id="p307"> 
   <p>The next step is to repeat the above operation using a second batch of examples extracted from the training data. The parameters are again slightly updated in the direction of the gradient calculated on that batch. This process is repeated, one batch at a time. At some point, the algorithm makes a full pass over the entire training data, which is known as an <em>epoch. </em>Usually, training is performed for several epochs, so there are multiple passes over the entire training data. We don’t know the exact number of epochs used to train popular LLMs, but OpenAI once revealed training a model for 100 epochs (see <a href="https://mng.bz/5gy7"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/5gy7</span></a>). </p> 
  </div> 
  <div class="readable-text intended-text" id="p308"> 
   <p>The training process is very time-consuming. It can take days to complete and multiple GPUs working in unison to do all the number crunching. </p> 
  </div> 
  <div class="readable-text intended-text" id="p309"> 
   <p>Stochastic gradient descent helps progressively improve the model, but it doesn’t guarantee finding the best possible model of all. This is because making slight improvements in the direction of the gradient can get the model stuck in a <em>local minimum.</em> This means that the model cannot be improved any further by making <em>small</em> changes to parameter values. There may be a better model, perhaps the globally best one, if parameters were changed widely from their current ones, but this is like finding a needle in a haystack. </p> 
  </div> 
  <div class="readable-text intended-text" id="p310"> 
   <p>It is kind of crazy that we can create a good LLM following this process, as we must find effective values for billions of parameters starting from completely random ones. It is wild! The reason it works is that the model’s architecture is laser-focused and tailor-made to the task (e.g., it enforces a multi-headed attention mechanism with simple, linear projections and dot products). So, the model’s parameter values are guided in the right direction thanks to their specialization to perform the task in a human-prescribed way.</p> 
  </div> 
  <div class="readable-text intended-text" id="p311"> 
   <p>Note that using an existing model is much faster than training it. All the parameters are already defined, so you just need to use the model once to calculate its outputs from its inputs. Using an already created model is often described as <em>inference time</em> to distinguish it from the much lengthier <em>training time.</em><em/> </p> 
  </div> 
  <div class="readable-text intended-text" id="p312"> 
   <p>So far, we’ve covered AI within the context of LLMs. Understanding the gist of how AI works with other inputs, such as images, isn’t a big leap from what we’ve already discussed. In the next couple of sections, we’ll briefly comment on how AI processes images and combinations of different data types. We start with convolutional neural networks, which are a type of architecture that did for image analysis what transformers did for text analysis.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p313"> 
    <h5 class=" callout-container-h5 readable-text-h5">Generative AI (and are LLMs generative?)</h5> 
   </div> 
   <div class="readable-text" id="p314"> 
    <p>Since the proliferation of LLMs, the term “generative AI” has become a popular way of describing any AI model used to generate new content, such as text and images. In this sense, LLMs are generative.</p> 
   </div> 
   <div class="readable-text" id="p315"> 
    <p>However, that’s not what “generative” used to mean in the technical ML literature, so you may find conflicting uses of the word.</p> 
   </div> 
   <div class="readable-text" id="p316"> 
    <p>In ML, a model is said to be <em class="CharOverride-4">discriminative</em> when it calculates the probability of a label given the input. We can describe this mathematically as the conditional probability P(Label | Input). This is exactly what LLMs calculate—the probability of the next token given the previous ones—so they’re technically discriminative models. </p> 
   </div> 
   <div class="readable-text" id="p317"> 
    <p>By contrast, a <em class="CharOverride-4">generative</em> model in the ML literature is one that calculates the probability of stumbling upon a certain piece of data—both input and label. For example, if you give the generative model a picture of a cat paired with the label “cat,” it tells you how likely you are to ever find such an image paired with such a label. So, it also assesses the plausibility of the cat image itself. If you give the model a picture of a blue cat paired with the label “cat,” it will probably output a low probability, as you’re unlikely to find pictures of blue cats. In mathematical terms, a generative model calculates P(Input, Label), the joint probability of stumbling upon a specific input/label training example. LLMs are not designed to do this, so, strictly speaking, they’re not generative models (see discussion at <a href="https://mng.bz/6eMR"><span class="Hyperlink">https://mng.bz/6eMR</span></a>). </p> 
   </div> 
  </div> 
  <div class="readable-text" id="p318"> 
   <h2 class=" readable-text-h2">Convolutions (images, video, and audio)</h2> 
  </div> 
  <div class="readable-text" id="p319"> 
   <p><span class="CharOverride-3">Let’s now step away from text generation and take a quick look at how AI models process other data types like images. In the 2010s, an ML model architecture known as </span><em class="CharOverride-4">convolutional neural network</em><span class="CharOverride-3">, or CNN, became extremely popular for image categorization. The input to a CNN is an image—represented as a table of numbers, or </span><em>pixels</em><span class="CharOverride-3">—and the output is a prediction of the image’s category, such as “strawberry.”</span><span class="CharOverride-3"/> </p> 
  </div> 
  <div class="readable-text intended-text" id="p320"> 
   <p>CNNs were specifically designed to exploit a strong assumption about image categorization: objects can be detected by the presence of their parts (e.g., a cat can be identified by the presence of a tail, eyes, whiskers), but we don’t care so much about the exact location of the parts (e.g., the direction in which a cat’s tail points is irrelevant to recognize that it’s a cat).</p> 
  </div> 
  <div class="readable-text intended-text" id="p321"> 
   <p>A CNN applies a series of transformations to the input image. The first transformation is a <em>convolution,</em> which is a simple mathematical operation that filters the image and produces a slightly modified version of it. Convolutions can be configured to do things such as</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p322">Blur the image</li> 
   <li class="readable-text" id="p323">Highlight areas of a specific color</li> 
   <li class="readable-text" id="p324">Highlight areas of sharp color changes in given directions (e.g., diagonal lines)</li> 
  </ul> 
  <div class="readable-text" id="p325"> 
   <p>The exact filter applied by a convolution is configured by defining the numbers in a small matrix. These values are learnable parameters of the CNN, so the model decides which filters to apply at training time instead of the engineer defining them beforehand.</p> 
  </div> 
  <div class="readable-text intended-text" id="p326"> 
   <p>The CNN performs multiple convolutions simultaneously and combines the results into a new image. Afterward, the image in <em>downsampled,</em> meaning that it is spatially shrunk. For example, an image of size 1024 × 1024 pixels might be shrunk to a size of 512 × 512 pixels by averaging the values of quadruplets of neighboring pixels. The effect of downsampling is to make this image more abstract by removing objects’ precise locations (as we said above, we assumed precise locations to be unimportant in the context of image categorization).</p> 
  </div> 
  <div class="readable-text intended-text" id="p327"> 
   <p>New convolutions are applied to the resulting image, followed by another round of downsampling. This is then done again and again. As filters are applied over already filtered images, the CNN can detect progressively complicated patterns. We could imagine, for example, that at first, the CNN uses convolutions to detect simple lines, then it detects pairs of parallel lines, then groups of parallel lines, then whiskers from those lines, and finally, it detects cats from their whiskers. As the exact filters are determined through machine learning, it’s hard to understand the exact strategy used by CNNs to make predictions. </p> 
  </div> 
  <div class="readable-text intended-text" id="p328"> 
   <p>The end result of this process is an embedding that effectively represents the content of the image in an abstract way. This embedding is used to predict the probability of the image belonging to each possible category. Mission accomplished. </p> 
  </div> 
  <div class="readable-text intended-text" id="p329"> 
   <p>CNNs are also used to transform images into other images of the same size. This is useful, for example, when reconstructing a damaged image or making any picture look like a Van Gogh painting. A popular architecture, called U-Net, achieves this in two steps. First, a usual CNN performs the above-described transformations to shrink the input image into a smaller, more abstract representation of its content. Afterward, another CNN-like structure extracts the intermediate images produced by the CNN and “stiches” them together to reconstruct a full-size image in a different style. </p> 
  </div> 
  <div class="readable-text intended-text" id="p330"> 
   <p>CNNs have also become popular to process audio and video. The principle is the same—the input goes through a series of convolutions and downsampling operations until it’s transformed into a more abstract representation. </p> 
  </div> 
  <div class="readable-text intended-text" id="p331"> 
   <p>Transformers have become the go-to architecture to process text, and CNNs have become the go-to architecture to process images, video, and audio. In the next section, we see how transformer and CNNs are combined in multimodal AI.</p> 
  </div> 
  <div class="readable-text" id="p332"> 
   <h2 class=" readable-text-h2">Multimodal AI</h2> 
  </div> 
  <div class="readable-text" id="p333"> 
   <p>Some AI models, known as <em>multimodal,</em> are capable of consuming or producing combinations of text, image, and audio. One example is AI that generates images from a textual description, such as the popular Midjourney and DALL-E.</p> 
  </div> 
  <div class="readable-text intended-text" id="p334"> 
   <p>Multimodal AI models are architected by combining LLMs and CNNs. There are myriad ways of combining them, so we’ll only briefly describe two approaches, one to generate text from images and one to generate images from text.</p> 
  </div> 
  <div class="readable-text intended-text" id="p335"> 
   <p>A popular image-to-text architecture uses an independently trained CNN to generate an embedding for the input image. The embedding is then transformed through a linear projection to make it comparable to the LLM’s embedding. For example, the embedding generated by the CNN for an image of a cat is turned into the embedding the LLM uses for the “cat” token. The new embedding is then injected inside the LLM. <em>Voilà!</em></p> 
  </div> 
  <div class="readable-text intended-text" id="p336"> 
   <p>Let’s now turn to a highly popular text-to-image approach, known as a <em>conditional diffusion</em> model. In this approach, a U-Net type of CNN is trained to reconstruct an image from a corrupted version of the image and its textual caption (see figure 1.11).</p> 
  </div> 
  <div class="browsable-container figure-container " id="p337">  
   <img alt="" src="../Images/CH01_F09_Maggiori.png" style="width: 100%; max-width: max-content;"/> 
   <h5 class=" figure-container-h5"><span class="">Figure 1.11</span><span class=""> </span><span class="">A diffusion model is trained to improve a corrupted image paired with its caption.</span></h5>
  </div> 
  <div class="readable-text" id="p338"> 
   <p>The model takes two inputs:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p339">A corrupted image (often called a “noisy” image)</li> 
   <li class="readable-text" id="p340">An embedding that represents the meaning of the text caption (e.g., generated using a language model)</li> 
  </ul> 
  <div class="readable-text" id="p341"> 
   <p>The text embedding is inserted into the model as an additional input. This is often done, for example, by using an embedding that matches the image size and inserting it as an additional color channel, on top of red, green, and blue.</p> 
  </div> 
  <div class="readable-text intended-text" id="p342"> 
   <p>The CNN is trained to repair the damaged image. This is performed in a supervised way. This requires a database with numerous examples of corrupted images, their corresponding captions, and their uncorrupted versions. The corrupted image is generated automatically by artificially corrupting a higher-quality image, and the captions are generated manually. </p> 
  </div> 
  <div class="readable-text intended-text" id="p343"> 
   <p>Once this model is trained, it is capable of slightly improving a bad image using the caption for guidance. Let’s see how this model is used to create a brand-new image from a description, as we do with Midjourney.</p> 
  </div> 
  <div class="readable-text intended-text" id="p344"> 
   <p>First, the model is fed a totally random image, which resembles the static noise in a faulty TV set, together with the caption of the desired image (see figure 1.12). The model <span class="CharOverride-3">then produces a slightly “improved” version of this image, where we see the desired object slightly pop up from the noise.</span></p> 
  </div> 
  <div class="browsable-container figure-container " id="p345">  
   <img alt="" src="../Images/CH01_F10_Maggiori.png" style="width: 100%; max-width: max-content;"/> 
   <h5 class=" figure-container-h5"><span class="">Figure 1.12</span><span class=""> </span><span class="">A diffusion model is used repeatedly to have a desired image emerge from Gaussian noise.</span></h5>
  </div> 
  <div class="readable-text" id="p346"> 
   <p>The model is then used repeatedly on its own output, which progressively enhances the image. After doing this many times, the image becomes nice and sharp. This is usually performed a fixed number of times decided in advance through experimentation—the number of steps is set to be large enough to guarantee that most images will be sharp by the end. Some people are studying techniques to vary the number of steps depending on the prompt (<a href="https://arxiv.org/abs/2408.02054"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/arxiv.org/abs/2408.02054</span></a><span class="Hyperlink">)</span>. We can think of this process as diffusing away the “noise,” hence the term “diffusion model.” This technique powers the most popular text-to-image models. Diffusion is also the cornerstone of text-to-video models, which is a hot research topic. For example, OpenAI’s video-generating model called Sora uses diffusion (<a href="https://mng.bz/oKlD"><span class="Hyperlink">https:</span><span class="Hyperlink">/</span><span class="Hyperlink">/mng.bz/oKlD</span></a>). Instead of denoising an image, it is designed to denoise a <em>patch,</em> which is a representation of a small piece of video over space and time. A patch covers a small portion of the screen, such as the top-left corner, across a few contiguous frames. Just like with images, the model is used repeatedly to progressively enhance patches using the prompt for guidance, starting from random noise. As of this writing, the model hasn’t yet been released to the public.</p> 
  </div> 
  <div class="readable-text intended-text" id="p347"> 
   <p>This brings us to the end of our (relatively) quick rundown of some of the fundamental elements of AI. Let’s draw things to a close with a high-level reflection about machine learning before we move on to the next chapter.</p> 
  </div> 
  <div class="readable-text" id="p348"> 
   <h2 class=" readable-text-h2">No free lunch</h2> 
  </div> 
  <div class="readable-text" id="p349"> 
   <p>I’d like to wrap things up with a reflection about machine learning. As we’ve seen throughout this chapter, ML requires designing a dedicated architecture to each problem. For example, transformers are used to generate text, CNNs are used to analyze images, and creative combinations of the two are used in a multimodal setting. Each model’s architecture is based on assumptions of how to best solve the problem at hand. For example, transformers force the model to calculate attention scores, and CNNs impose using convolutions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p350"> 
   <p>Every ML milestone has been attained thanks to the invention of a new type of architecture that does a better job than previous ones at the task at hand. For example, transformers replaced LSTMs, and there was a boom in AI’s performance at text generation. Progress is made when we tailor architectures to specific tasks in a creative and useful way. So, current AI is about designing tailored solutions to each problem and not about devising a general approach that works on everything.</p> 
  </div> 
  <div class="readable-text intended-text" id="p351"> 
   <p>In fact, the No Free Lunch Theorem of machine learning says, in simple terms, that there is no universally best architecture that is optimal for all problems (see David Wolpert, 1996, “The lack of a priori distinctions between learning algorithms,” <em class="CharOverride-4">Neural Computation</em> 8.7: 1341–1390). Instead, each problem requires a dedicated architecture. </p> 
  </div> 
  <div class="readable-text intended-text" id="p352"> 
   <p>Sometimes, we get the impression that machines learn by themselves and that current AI is a general approach. In reality, we help the machine learn. And we help a lot.</p> 
  </div> 
  <div class="readable-text" id="p353"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p354">LLMs are designed to guess the best next word that completes an input prompt.</li> 
   <li class="readable-text" id="p355">LLMs subdivide inputs into valid tokens (common words or pieces of words) from an internal vocabulary.</li> 
   <li class="readable-text" id="p356">LLMs calculate the probability that each possible token is the one that comes next after the input. </li> 
   <li class="readable-text" id="p357">A wrapper around the LLM enhances its capabilities. For examples, it makes the LLM eat its own output repeatedly to generate full outputs, one token at a time.</li> 
   <li class="readable-text" id="p358">Current LLMs represent information using embedding vector, which are lists of numbers.</li> 
   <li class="readable-text" id="p359">Current LLMs follow the transformer architecture, which is a method to progressively contextualize input tokens.</li> 
   <li class="readable-text" id="p360">LLMs are created using machine learning, meaning that data is used to define missing parameters inside the model.</li> 
   <li class="readable-text" id="p361">There are different types of machine learning, including supervised, self-supervised, and unsupervised learning.</li> 
   <li class="readable-text" id="p362">In supervised learning, the computer learns by example—it is fed with examples of how to perform the task. In the case of self-supervised learning, these examples are generated automatically by scanning data.</li> 
   <li class="readable-text" id="p363">Popular LLMs were first trained in a self-supervised way using publicly available data, and then, they were refined using manually generated data to align them to the users’ objectives.</li> 
   <li class="readable-text" id="p364">CNNs are a popular architecture to process other types of data, such as images.</li> 
   <li class="readable-text" id="p365">CNNs are combined with transformers to create multimodal AI. </li> 
  </ul>
 </body></html>