- en: '6 Dynamic graphs: Spatiotemporal GNNs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 动态图：时空GNN
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Introducing memory into your deep learning models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将记忆引入你的深度学习模型
- en: Understanding the different ways to model temporal relations using graph neural
    networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解使用图神经网络建模时序关系的不同方法
- en: Implementing dynamic graph neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现动态图神经网络
- en: Evaluating your temporal graph neural network models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估你的时序图神经网络模型
- en: So far, all of our models and data have been single snapshots in time. In practice,
    the world is dynamic and in constant flux. Objects can move physically, following
    a trajectory in front of our eyes, and we’re able to predict their future positions
    based on these observed trajectories. Traffic flow, weather patterns, and the
    spread of diseases across networks of people are all examples where more information
    can be gained when modeled with spatiotemporal graphs instead of static graphs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所有的模型和数据都只是时间上的单一快照。在实践中，世界是动态的，并且处于不断变化之中。物体可以物理移动，在我们眼前沿着轨迹移动，我们能够根据这些观察到的轨迹预测它们的未来位置。交通流量、天气模式和疾病在人群网络中的传播都是当使用时空图而不是静态图建模时可以获取更多信息的情况。
- en: Models that we build today might quickly lose performance and accuracy as we
    deploy them in the real world. These are problems intrinsic to any deep learning
    (and machine learning) model, known as *out-of-distribution (OOD) generalization,*
    that is, how well models generalize to entirely unseen data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天构建的模型，一旦部署到现实世界中，可能会迅速失去性能和准确性。这些问题是任何深度学习（以及机器学习）模型固有的，被称为*分布外（OOD）泛化*问题，即模型对完全未见过的数据的泛化能力如何。
- en: In this chapter, we consider how to make models that are suitable for dynamic
    events. While this doesn’t mean they can deal with OOD data, our dynamic models
    will be able to make predictions about unseen events in the future using the recent
    past.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们考虑如何构建适合动态事件的模型。虽然这并不意味着它们可以处理OOD数据，但我们的动态模型将能够利用最近过去的数据对未来未见的事件进行预测。
- en: To build our dynamic graph-based learning model, we’ll consider the problem
    of pose estimation. *Pose estimation* relates to those classes of problems that
    predict how bodies (human, animal, or robotic) move over time. In this chapter,
    we’ll consider a body walking and build several models that learn how to predict
    the next step from a series of video frames. To do this, we’ll first explain the
    problem in more detail and how to understand this as a relational problem before
    jumping in to see how graph-based learning approaches this problem. As with the
    rest of our book, further technical details are left to section 6.5 at the end
    of the chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的基于动态图的 学习模型，我们将考虑姿态估计的问题。*姿态估计*与那些预测身体（人类、动物或机器人）随时间移动的类问题相关。在本章中，我们将考虑一个行走的人体，并构建几个模型来学习如何从一系列视频帧中预测下一步。为此，我们首先将更详细地解释这个问题，以及如何将其理解为一个关系问题，然后再深入探讨基于图的学习方法是如何处理这个问题的。与本书的其他部分一样，更详细的技术细节留到本章末尾的6.5节。
- en: We’ll use much of the material that we’ve already covered in the book. If you’ve
    skipped ahead to this chapter, make sure you have a good understanding of the
    concepts described in the “Building on what you’ve learned” sidebar.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用本书中已经覆盖的大部分材料。如果你已经跳到了这一章，请确保你对“建立在所学知识的基础上”侧边栏中描述的概念有很好的理解。
- en: Note  Code from this chapter can be found in notebook form at the GitHub repository
    ([https://mng.bz/4a8D](https://mng.bz/4a8D)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：本章的代码以笔记本形式可在GitHub仓库（[https://mng.bz/4a8D](https://mng.bz/4a8D)）中找到。
- en: Building on what you’ve learned
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 建立在所学知识的基础上
- en: 'To introduce temporal updates into our GNN, we can build on some of the concepts
    that we’ve learned in previous chapters. As a quick refresher, we’ve summarized
    some of the main important features from each chapter:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要将时序更新引入我们的GNN，我们可以基于之前章节中学到的某些概念。作为一个快速回顾，我们已经总结了每个章节的一些主要重要特性：
- en: '*Message passing*—In chapter 2, you learned that the main method used by GNNs
    to learn from relational data is by combining message passing with artificial
    neural networks. Each layer of a GNN can be understood as one step of message
    passing.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*消息传递*——在第2章中，你了解到GNN（图神经网络）从关系数据中学习的主要方法是结合消息传递与人工神经网络。GNN的每一层都可以理解为消息传递的一个步骤。'
- en: '*Graph convolutional networks (GCNs)*—In chapter 3, you saw that message passing
    itself can be understood as the relational form of the convolution operator (as
    in convolutional neural networks [CNNs]), and this is the central idea behind
    GCNs. Messages can also be averaged across neighborhoods by only sampling a subset
    of nearest neighbors. This is used for GraphSAGE and can considerably reduce the
    total compute needed.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图卷积网络（GCNs）**—在第3章中，你看到消息传递本身可以理解为卷积算子的关系形式（如卷积神经网络 [CNNs] 中的那样），这是GCNs背后的核心思想。消息也可以通过仅采样最近邻的子集来在邻域内平均。这用于GraphSAGE，并且可以显著减少所需的总体计算量。'
- en: '*Attention*—In chapter 4, we showed how the aggregation function for message
    passing doesn’t need to be restricted to only summing, averaging, or max operations
    (though the operation must be permutation invariant). Attention allows for a weighting
    to be learned during training to give more flexible message-passing aggregation
    functions. Using a graph attention network (GAT) is the basic form of adding attention
    to message passing.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意**—在第4章中，我们展示了消息传递的聚合函数不需要仅限于求和、平均或最大操作（尽管操作必须是排列不变的）。注意力机制允许在训练过程中学习权重，从而为消息传递聚合函数提供更大的灵活性。使用图注意力网络（GAT）是向消息传递添加注意力的基本形式。'
- en: '*Generative models*—While discriminative models seek to learn separations between
    data classes, generative models attempt to learn the underlying data-generating
    process. The autoencoder is one of the most popular frameworks for designing generative
    models, where data is passed through a neural network bottleneck to create a low-dimensional
    representation of the data, also called the latent space. These are commonly implemented
    as graph autoencoders (GAEs) or variational graph autoencoders (VGAEs) for graphs,
    as we discussed in chapter 5\.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成模型**—虽然判别模型试图学习数据类之间的分离，但生成模型试图学习底层的数据生成过程。自动编码器是设计生成模型中最受欢迎的框架之一，其中数据通过神经网络瓶颈传递以创建数据的低维表示，也称为潜在空间。这些通常作为图自动编码器（GAEs）或变分图自动编码器（VGAEs）在图中实现，正如我们在第5章中讨论的那样。'
- en: '6.1 Temporal models: Relations through time'
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 时间模型：通过时间的关系
- en: Almost every data problem will, in some way, also be a dynamic problem. In many
    cases, we can ignore changes in time and build models that are suitable for snapshots
    of the data that we’ve collected. For example, image segmentation methods rarely
    consider video footage to train models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每个数据问题在某种程度上也会是一个动态问题。在许多情况下，我们可以忽略时间的变化，并构建适合我们所收集的数据快照的模型。例如，图像分割方法很少考虑视频素材来训练模型。
- en: In chapter 3, we used a GCN to predict suitable products to recommend to customers
    using data on a customer-purchaser network. We used a toy dataset that had been
    collected over a period of several years. However, in reality, we’ll often have
    constant streams of data and want to make up-to-date predictions that account
    for both customer and cultural habit changes. Similarly, when we applied a GAT
    to a fraud-detection problem, the data we used was a single snapshot of financial
    records that was collected over a period of several years. However, we didn’t
    account for how financial behaviors changed over time in our model. Again, we
    would likely want to use this information to predict where an individual’s spending
    behavior abruptly changes to help us detect fraudulent activity.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们使用GCN根据客户购买网络上的数据预测向客户推荐的产品。我们使用了一个跨越数年的玩具数据集。然而，在现实中，我们通常会拥有持续的数据流，并希望做出最新的预测，这些预测要考虑到客户和文化习惯的变化。同样，当我们将GAT应用于欺诈检测问题时，我们所使用的数据是在数年内收集的金融记录的单一快照。然而，我们没有在我们的模型中考虑到金融行为随时间的变化。再次，我们可能会希望使用这些信息来预测个人的消费行为突然改变的位置，以帮助我们检测欺诈活动。
- en: These are just a few of the many different dynamic problems that we’re faced
    with every day (see figure 6.1). GNNs are unique in that they can model both dynamic
    and relational changes. This is very important as many of the networks that operate
    around us are also moving in time. Take, for example, a social network. Our friendships
    change, mature, and sadly (or fortunately!) weaken over time. We might become
    stronger friends with work colleagues or friends of friends and see friends from
    our hometown less frequently. Making predictions for social networks need to account
    for this.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是我们每天面临的大量不同动态问题中的一小部分（见图6.1）。GNN的独特之处在于它们可以模拟动态和关系变化。这一点非常重要，因为围绕我们的许多网络也在随时间移动。以社交网络为例。我们的友谊会变化、成熟，并且不幸地（或者幸运地！）会减弱。我们可能会与工作同事或朋友的朋友变得更亲密，而与家乡的朋友见面的频率会降低。对社交网络进行预测需要考虑到这一点。
- en: As another example, we often make predictions about which way to go and when
    we might arrive based on our knowledge of the roads, traffic patterns, and how
    much of a rush we’re in. A dynamic GNN can also be used to help make use of this
    data, by treating the road network as a graph and making temporal predictions
    on how this network will change. Finally, we can consider predicting how two or
    more objects move together, that is, by estimating their future trajectories.
    While this might seem less useful than making friends or getting to work on time,
    predicting trajectories of interacting bodies, such as molecules, cells, objects,
    or even stars, is vital to many sciences as well as for robotic planning. Again,
    dynamic GNNs can help us both predict these trajectories and infer new equations
    or rules that explain them.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，我们经常根据我们对道路、交通模式和我们的紧迫感的了解来预测我们将走向何方以及我们何时可能到达。动态GNN也可以用来帮助利用这些数据，通过将道路网络视为图，并对该网络如何变化进行时间预测。最后，我们可以考虑预测两个或更多物体如何一起移动，即通过估计它们的未来轨迹。虽然这可能不如交朋友或按时到达工作地点有用，但预测相互作用物体的轨迹，如分子、细胞、物体甚至恒星，对于许多科学以及机器人规划都是至关重要的。同样，动态GNN可以帮助我们预测这些轨迹并推断解释它们的新的方程或规则。
- en: '![figure](../Images/6-1.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/6-1.png)'
- en: Figure 6.1 Examples of different dynamic problems
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1 不同动态问题的示例
- en: 'These examples are just the tip of the iceberg for applications where we need
    to model temporal changes. In fact, we’re sure that you can think of many others.
    Given the importance of knowing how to combine relational learning with temporal
    learning, we’ll cover three different methods for building dynamic models, two
    of which use GNNs: a recurrent neural network (RNN) model, a GAT model, and a
    neural relational inference (NRI) model. We’ll build machine learning models that
    “learn to walk” by estimating how a human pose changes over time. These models
    are often deployed in, for example, medical consultations, remote home security
    services, and filmmaking. The models are also a great toy problem for us to learn
    to walk before we can run. In that spirit, let’s first learn more about the data
    and build our first benchmark model.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子只是我们需要对时间变化进行建模的应用的冰山一角。实际上，我们确信你们可以想到很多其他的例子。鉴于了解如何结合关系学习和时间学习的重要性，我们将介绍三种构建动态模型的不同方法，其中两种使用GNN：一个循环神经网络（RNN）模型，一个GAT模型，以及一个神经关系推理（NRI）模型。我们将通过估计人类姿态随时间的变化来构建“学习走路”的机器学习模型。这些模型通常被部署在例如医疗咨询、远程家庭安全服务和电影制作中。这些模型也是我们在能够奔跑之前学习走路的绝佳玩具问题。本着这种精神，让我们首先更多地了解数据并构建我们的第一个基准模型。
- en: '6.2 Problem definition: Pose estimation'
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 问题定义：姿态估计
- en: 'In this chapter, we’ll solve a “dynamic relational” problem with one set of
    data: preprocessed segmentation of a body walking. This is a useful dataset to
    explore these techniques, as a moving body is a textbook example of an interacting
    system: our foot moves because our knee moves because a leg moves, and our arms
    and torso will all move too. This means that there is a temporal component to
    our problem.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用一组数据解决一个“动态关系”问题：一个行走身体的预处理分割。这是一个探索这些技术的有用数据集，因为移动的身体是相互作用系统的教科书式例子：我们的脚移动是因为膝盖移动，因为腿移动，而我们的手臂和躯干也会移动。这意味着我们的问题有一个时间成分。
- en: In a nutshell, our pose estimation problem is about path prediction. More precisely,
    we want to know where, for example, a foot will move having followed the rest
    of the body for some number of previous timesteps. This type of object tracking
    is something that we do every day, for example, when we play sports, catch something
    that’s falling, or watch a television show. We learn this skill as a child and
    often take it for granted. However, as you’ll see, teaching a machine to perform
    this object tracking was a significant challenge up until the emergence of spatiotemporal
    GNNs.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: The skills that we’ll use for path prediction are important for many other tasks.
    Predicting events in the future is useful when we want to predict the next purchase
    of a customer or understand how weather patterns will change based on geospatial
    data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll be using the Carnegie Mellon University (CMU) Motion Capture Database
    ([http://mocap.cs.cmu.edu/](http://mocap.cs.cmu.edu/)), which contains many examples
    of different dynamic poses, including walking, running, jumping, and performing
    sports moves, as well as multiple people interacting [1]. Throughout this chapter,
    we’ll use the same dataset of subject #35 walking. At each timestep, the subject
    has 41 sensors that each follow a single joint, ranging from the toes up to the
    neck. An example of the data from this database is shown in figure 6.2\. These
    sensors track the movement of part of the body across snapshots of their motion.
    In this chapter, we won’t follow the entire motion and consider only a small subset
    of the motion. We’ll use the first 49 frames for our training and validation datasets
    and 99 frames for our test set. In total, there are 31 different examples of this
    subject walking. We’ll discuss more about the structure of our data in the next
    section.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-2.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 Snapshots in time (t = time in seconds) of a human subject walking.
    The dots represent sensors placed on key joints on the human’s body. These snapshots
    are across 30 seconds. To represent these figures as a graph, the sensor placements
    (joints) can be represented as nodes, and the body’s connections between the joints
    are the edges.
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.2.1 Setting up the problem
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our aim is to predict the dynamics for all the individual joints. Clearly, we
    can construct this as a graph because all the joints are connected through edges,
    as shown previously in figure 6.2\. Therefore, it makes sense to use GNNs to solve
    this problem. However, we’ll first compare another approach, which doesn’t account
    for the graph data, to benchmark our GNN models.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the data
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve included the steps to download and preprocess the data in our code repository.
    The data is contained within a zip file where each of the different trials is
    saved as an advanced systems format (.asf) file. These .asf files are basically
    just text files that contain the label for each sensor and their xyz coordinates
    at each timestep. In the following listing, we show a snippet of the text.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 Example of the sensor data text files
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, the first number is the frame number, and `root` is specific to the sensors
    and can be ignored. `lowerback`, `upperback`, `thorax`, `lowerneck`, and `upperneck`
    denote the positions of the sensors. In total, there are 31 sensors mapping the
    movement of a man walking. To convert this sensor data into trajectories, we need
    to calculate the change in position for each sensor. This becomes quite a complicated
    task, as we need to account for both translational movements and angular rotations
    for the various sensors between each frame. Here, we’ll use the same data files
    as in the NRI paper [2]. We can use these to map out the trajectories of each
    individual sensor in x, y, and z, or look at how the sensors are moving in two
    dimensions to get intuition about how the entire body is moving. Examples of this
    are shown in figure 6.3, where we focus on the movement of a foot sensor in x,
    y, and z, as well as the overall movement of the body over time (with the sensor
    shown as solid black stars).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-3.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 Preconstructed spatial trajectories of sensors
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Along with the spatial data, we can also calculate the velocity data. This data
    is provided as separate files for each of the movie frames. An example of the
    change in velocity data is shown in figure 6.4\. As you can see, the velocity
    data varies around a smaller range. Both spatial and velocity data will be used
    as the features in our machine learning problem. Here, we now have six features
    across 50 frames for each of our 31 sensors and across 33 different trials. We
    can understand this as a multivariate time series problem. We’re trying to predict
    the future evolution of a six-dimensional (three spatial and three velocity) object
    (each sensor). Our first approach will treat these as independent, looking to
    predict future positions and velocity based on past sensor data. We’ll then switch
    to treating this as a graph, where we can couple all sensors together.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-4.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 Preconstructed velocity data of sensors
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Currently, this is a relational problem, but we’re only considering the node
    data and not the edge data. Where there is node data and no edge data, we have
    to be careful not to make too many assumptions. For example, if we chose to connect
    nodes based on their distance from one another, then we might end up with a very
    strange-looking skeleton, as shown in figure 6.5\. Luckily, we have the edge data
    as well, which has been built using the CMU dataset and is included in the data
    provided. This serves as a cautionary tale that GNNs are only as powerful as the
    graphs they’re trained on and that we must take care to ensure that the graph
    structure is correct. However, if edge data is entirely lacking, then we can attempt
    to infer the edge data from the node data itself. While we won’t be doing this
    here, note that the NRI model we’ll be using has this capability.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-5.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 Sensor networks showing the error of wrongly inferring graph structures.
    The nodes are human skeletal connections. The left figure shows a network with
    edges inferred from node proximity (closest nodes connected to one another). This
    figure does not reflect a real human skeleton. The true set of edges is shown
    in the right figure.
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We now have all of our data loaded. In total, we have three datasets (training,
    validation, testing) that each contain 31 individual sensor positions. Each of
    these sensors contain six features (spatial coordinates) and are connected by
    an adjacency matrix that is constant in time. The sensor graph is undirected,
    and the edges are unweighted. The training and validation sets contain 49 frames,
    and the test sets contain 99 frames.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Building models with memory
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that our problem is defined and our data is loaded, let’s consider how
    we might approach the problem of predicting the joint dynamics. First, we need
    to think about what the underlying aim is. At its core, we’ll be involved in sequence
    prediction, just like autocomplete on a phone or search tool. These types of problems
    are often approached using networks, such as transformers, for which we use an
    attention mechanism as in chapter 4\. However, before attention-based networks,
    many deep learning practitioners instead approached sequence prediction tasks
    by introducing memory into their models [3]. This makes intuitive sense: if we
    want to predict the future, we need to remember the past.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Let’s build a simple model that predicts the next location for all the individual
    sensors using past events. Essentially, this means we’ll build a model that predicts
    the position of nodes without edge data. An example of what we’ll be attempting
    is shown in figure 6.6\. Here, we’ll start by preprocessing and preparing our
    data to be passed to a model that can predict how the data evolves over time.
    This allows us to predict the changes in the pose given a few input frames.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-6.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 Predicting future positions using only sensor data
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To introduce memory to our neural networks, we’ll start by considering a recurrent
    neural network (RNN). Similar to convolutional and attention neural networks,
    RNNs are a broad class of architectures that are fundamental tools for researchers
    and practitioners alike. For more information about RNNs, see, for example *Machine
    Learning with TensorFlow* (Manning, 2020, [https://mng.bz/VVOW](https://mng.bz/VVOW)).
    RNNs can be considered as multiple individual networks that link together. These
    repeating subnetworks allow for past information to be “remembered” and the effect
    from past data to affect future predictions. After initializing, each subnetwork
    takes in input data as well as the output of the last subnetwork, and these are
    used to make new predictions. In other words, each subnetwork takes input and
    information from the recent past to build inferences about the data. However,
    a vanilla RNN will only ever remember the preceding step. They have *very* short-term
    memory. To improve the effect of the past on the future, we need something stronger.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将记忆引入我们的神经网络，我们首先考虑循环神经网络（RNN）。与卷积和注意力神经网络类似，RNN是一类广泛的架构，是研究人员和实践者共同的基本工具。有关RNN的更多信息，请参阅例如《用TensorFlow进行机器学习》（Manning，2020年，[https://mng.bz/VVOW](https://mng.bz/VVOW)）。RNN可以被视为多个相互连接的独立网络。这些重复的子网络允许“记住”过去的信息，以及过去数据对未来预测的影响。初始化后，每个子网络都会接收输入数据以及最后一个子网络的输出，并使用这些信息进行新的预测。换句话说，每个子网络都会接收来自最近过去的信息和输入，以构建关于数据的推理。然而，普通的RNN只会记住前一步。它们的记忆非常短暂。为了增强过去对未来影响的效果，我们需要更强大的东西。
- en: Long short-term memory (LSTM) networks are another extremely popular neural
    network architecture for modeling and predicting temporal or sequential information.
    These networks are special cases of RNN that similarly link multiple subnetworks
    together. The difference is that LSTMs introduce more complex dependencies in
    the subnetwork structure. LSTMs are particularly useful for sequential data as
    they resolve the problem of vanishing gradients that is observed for RNNs. Put
    simply, *vanishing gradients* refers to where the gradient that we use to train
    our neural network using gradient descent approaches zero. This is especially
    likely to happen when we train an RNN that has many layers. (We won’t go into
    the reasons for this here, but if you’re interested, read *Deep Learning with
    Python* (Manning, 2024, [https://mng.bz/xKag](https://mng.bz/xKag)) for more information.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）网络是另一种用于建模和预测时间或序列信息的极受欢迎的神经网络架构。这些网络是RNN的特殊情况，类似于将多个子网络链接在一起。不同之处在于，LSTMs在子网络结构中引入了更复杂的依赖关系。LSTMs对于序列数据特别有用，因为它们解决了RNN中观察到的梯度消失问题。简单来说，*梯度消失*指的是我们使用梯度下降法训练神经网络时，梯度变为零的情况。当我们训练具有许多层的RNN时，这种情况尤其可能发生。（我们在这里不会深入探讨其原因，但如果您对此感兴趣，请阅读《用Python进行深度学习》（Manning，2024年，[https://mng.bz/xKag](https://mng.bz/xKag)）以获取更多信息。）
- en: Gated recurrent unit networks (GRUs) also resolve the problem of vanishing gradients
    by allowing new information to be added to the memory store about the recent past.
    This is achieved through a gating structure, where gates within the model architecture
    help to control the flow of information. These gates also add a new design element
    to how we can build and adapt our neural networks. We won’t consider LSTM here
    as it’s outside the scope of the book, but again we recommend that you check out
    *Deep Learning with Python* (Manning, 2024, [https://mng.bz/xKag](https://mng.bz/xKag))
    for more information.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 门控循环单元网络（GRUs）通过允许将关于最近过去的新信息添加到记忆存储中，解决了梯度消失的问题。这是通过一个门控结构实现的，模型架构中的门控帮助控制信息的流动。这些门控还为我们构建和调整神经网络添加了一个新的设计元素。在这里我们不会考虑LSTM，因为它超出了本书的范围，但再次建议您查阅《用Python进行深度学习》（Manning，2024年，[https://mng.bz/xKag](https://mng.bz/xKag)）以获取更多信息。
- en: Constructing a recurrent neural network
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 构建循环神经网络
- en: Let’s now look at how to use an RNN to predict the trajectories of the body
    sensors over time, which will act as one of our baselines for future performance
    gains. We won’t go into the details of RNNs and GRU architectures but additional
    information is provided at the end of the chapter in section 6.5\.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看如何使用RNN来预测随时间变化的身体传感器的轨迹，这将成为我们未来性能提升的基准之一。我们不会深入探讨RNN和GRU架构的细节，但有关信息可在本章6.5节末尾找到。
- en: The idea for this model is that our RNN will predict the future positions for
    sensors without taking into account relational data. When we start to introduce
    our graph models, we’ll see how this can be improved.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the same standard training loop for deep learning, as shown in figure
    6.7\. Once we define our model and define a training and test loop, we use these
    to train and then test the model. As always, we’ll keep the training and testing
    data completely separate and include a validation set of data to make sure our
    model isn’t over-fitting during training.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-7.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 Standard process for training a deep learning model that we’ll follow
    throughout this chapter
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The training loop used here is fairly standard, so we’ll describe it first.
    In the training loop definition shown in listing 6.2, we follow the same convention
    as in previous chapters, looping through model prediction and loss updates over
    a fixed number of epochs. Here, our loss will be contained in our criterion function,
    which we define as a simple mean standard error (MSE) loss. We will use a learning
    rate scheduler, which will reduce the learning rate parameter after our validation
    loss starts to plateau. We initialize the best loss as infinity and lower the
    learning rate after the validation loss is less than our best loss for `N` steps.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.2 Training loop
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Initializes loss and accuracy variables'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Begins the training loop'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Zeros the parameter gradients'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Forward + backward + optimize'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Updates training loss, multiplying by the number of samples in the current
    mini-batch'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Begins the validation loop'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Checks for early stopping'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Steps the scheduler'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Calculates and stores losses'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Both layers are trained (using our training loop in listing 6.3) for a specific
    task. For both the RNN and the GRU, the format for the data will be the individual
    trials or videos, the frame timestamp, the number of sensors, and the features
    of the sensors. By providing the data broken up into individual snapshots of time,
    the model is able to use the temporal aspects to learn from. Here, we use the
    RNN to predict the future position for each individual sensor, given the 40 previous
    frames. For all of our calculations, we’ll normalize the data based on the node
    features (position and velocity) using min-max scaling.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: After we finish our training loop, we test our network. As always, we don’t
    want to update the parameters of our network, so we make sure that there is no
    backpropagated gradient (by selecting `torch.no_grad()`). Note that we choose
    a sequence length of 40 so that our testing loop is able to see the first 40 frames
    and then attempt to infer the final 10 frames.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.3 Testing loop
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Sets the model to evaluation mode'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Updates inputs for the next prediction'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Computes the loss for this sequence'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Converts predictions to a NumPy array for easier manipulation'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Computes the average test loss'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: Once our models are defined, we can next use the training loop given in listing
    6.3 to train our model. At this point, you might be wondering how we’ll amend
    the training loop to correctly account for the temporal element when backpropagating.
    The good news is that this is handled automatically by PyTorch. We find that the
    RNN model is able to predict the future positions with 70% accuracy for the validation
    data and 60% accuracy for the test data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: We also tried a GRU model to predict the future steps taken and found this model
    is able to get an accuracy of 75% using the validation data. This is quite low
    but not as low as it might be given the simplicity of the model and the little
    amount of information that we’ve passed it. However, when we test the model performance
    on our test data, we can see that performance falls to 65%. A few example outputs
    from our model are shown in figure 6.8\. Clearly, the model quickly degrades,
    and the estimated pose position starts to vary widely. For better accuracy, we’ll
    need to use some of the relational inductive biases in the pose data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-8.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 Predicting future movements using an RNN. Here, figures on the left
    represent the true data, and those on the right represent the predicted data.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.3 Dynamic graph neural networks
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To predict the future evolution of the graph, we need to restructure our data
    to account for temporal data. Specifically, dynamic GNNs connect different sequential
    snapshots of the graph’s evolution and learn to predict future evolutions [4–6].
    One method for doing so is to combine them into a single graph. This temporal
    graph now contains both per-timestep data and the temporal connections encoded
    as nodes with temporal edges. We’ll first approach the task of pose estimation
    by taking a naive approach to modeling graph evolution. We’ll look at how we can
    combine our temporal data into one large graph and then predict the future evolution
    by masking the nodes of interest. We’ll use the same GAT network that you saw
    in chapter 3\. Then, in section 6.4, we’ll show another method for solving the
    pose estimation problem by instead encoding each snapshot of the graph and predicting
    the evolution using a combination of variational autoencoders (VAEs) and RNNs,
    which is the NRI method [2].
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Graph attention network for dynamic graphs
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll look at how to convert our pose estimation problem into a graph-based
    problem. To do this, we need to construct an adjacency matrix that accounts for
    temporal information. First, we need to load our data in as a PyTorch Geometric
    (PyG) data object. We’ll use the same location and velocity data that we used
    to train our RNN. The difference here is that we’ll construct a single graph that
    contains all the data. The code snippet in listing 6.4 shows how we initialize
    our dataset. We pass the paths for where the location and velocity data are as
    well as where the edge data is located. We also pass whether we need to transform
    our data and the mask and window size that we’ll predict over.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.4 Loading the data as a graph
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Loads the data from .npy files'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Determines the mask size'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Determines the window size'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: For all our dataset objects, we need a `get` method inside the class to describe
    how to retrieve this data, which is shown in listing 6.5\. This method combines
    the location and velocity data into node features. We also provide an option to
    transform the data using a `normalize_array` function.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.5 Set up node features using location and velocity data
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '#1 Concatenates location and velocity data for each node'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Determines the mask size'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Applies normalization if transform is True'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Repeats the edges for the total number of timesteps (past + future)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Applies the shift to the edge indices'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Flattens the edge indices into two dimensions'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Converts everything to PyTorch tensors'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Calculates the indices of the masked nodes'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: We next want to combine all nodes across the different timesteps into one large
    graph containing all individual frames. This gives an adjacency matrix that covers
    all different timesteps. (For further details on the idea of temporal adjacency
    matrices, see section 6.5 at the end of this chapter.) To do this for our pose
    estimation data, we first construct the adjacency matrix for each timestep, as
    shown in listing 6.6 and included in listing 6.5\.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: As shown in figure 6.9, the process begins by representing the graph data across
    multiple timesteps, where each timestep is treated as a distinct layer (Step 1).
    All nodes have node feature data (not shown in the figure). For our application,
    the node feature data consists of location and velocity information.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Nodes within a timestep are connected to each other using intra-timestep edges,
    that is, connections between nodes on the same timestep layer (Step 2). These
    edges ensure that each graph at a specific timestep is internally consistent.
    The nodes are not yet connected across timesteps.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: To incorporate temporal relationships, inter-timestep edges (i.e., connections
    between nodes on different timestep layers) are added to connect corresponding
    nodes across adjacent timesteps (Step 3). These edges allow information to flow
    between nodes in different timesteps, enabling temporal modeling of the graph
    data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: In preparation for predicting future values, the nodes in the last timestep
    are masked to represent unknown data (Step 4). These masked nodes are treated
    as the target of the prediction task. Their values are unknown, but they can be
    inferred by leveraging the features and relationships of the unmasked nodes in
    earlier timesteps.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The inference process (Step 5) involves using the known features of unmasked
    nodes from previous timesteps (t = 0 and t = 1) to predict the features of the
    masked nodes in t = 2\. Dotted arrows illustrate how information flows from unmasked
    nodes to masked nodes, showing the dependency of the predictions on earlier graph
    data. This transforms the task into a node prediction problem, where the goal
    is to estimate the features of the masked nodes based on the relationships and
    features of the unmasked nodes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-9.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 Illustration of the spatiotemporal graph construction and inference
    process. Step 1 shows the sequence of graphs across timesteps with nodes representing
    entities at each timestep. Step 2 highlights intra-timestep edges (solid lines)
    connecting nodes within the same graph layer. Step 3 introduces inter-timestep
    edges (dotted lines) that encode temporal dependencies by linking corresponding
    nodes across adjacent timesteps. In Step 4, nodes at the final timestep are masked
    (gray) to represent unknown values for prediction. Step 5 demonstrates the inference
    process (dashed arrows), where information from unmasked nodes in earlier timesteps
    is used to estimate the features of masked nodes. The legend clarifies the types
    of nodes and edges used in the graph representation.
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Listing 6.6 Constructing the adjacency matrix
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Repeats the edges for the total number of timesteps (past + future)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates a shift for each timestep'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Applies the shift to the edge indices'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Flattens the edge indices into two dimensions'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the adjacency matrix, the next step is to build a model that
    can predict future timesteps. Here, we’ll use a GAT model, introduced in chapter
    4 [7]. We choose this GNN because it can be more expressive than other GNNs, and
    we want something that is able to account for the different temporal and spatial
    information. The model architecture is provided in listing 6.7\.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.7 Defining the GAT model
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 First GAT layer'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '#2 BatchNorm layer for the first GAT layer'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Intermediate GAT layers'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '#4 BatchNorm layers for intermediate GAT layers'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Last GAT layer'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Don’t apply batch normalization and dropout to the output of the last GAT
    layer.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Only outputs the last frame'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: This model follows the basic structure outlined in chapter 4\. We define the
    number of layers and heads for our model as well as the relevant input size, which
    depends on the number of features that we’re predicting. Each of our GAT layers
    has a hidden size and we include dropout and batch normalization to improve performance.
    We then loop through the number of layers in our model, ensuring that the dimensions
    are correct to match our target output. We also define our forward function, which
    predicts the node features for the masked nodes. By unwrapping each timestep into
    a larger graph, we start to introduce temporal effects as additional network structures
    that our model can learn.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: With both model and dataset defined, let’s start training our model and see
    how it performs. Recall that the RNN and GRU achieved 60% and 65% in test accuracy,
    respectively. In listing 6.8, we show the training loop for our GAT model. This
    training loop follows the same structure as that used in previous chapters. We
    use the MSE as our loss functions and set the learning rate to 0.0005\. We calculate
    the node features of the masked nodes using our GAT and then compare these to
    the true data, which is stored in `data`. We first train our model and then compare
    the model predictions using our validation set. Note that because of the multiple
    graph sequences we’re now predicting, this training loop takes more time than
    previous models. On a V100 GPU through Google Colab, this took under an hour to
    train.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.8 GAT training loop
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Initializes loss and optimizer with learning rate'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Generates the model’s predictions for the input'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Computes the loss between the outputs and the targets'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Validation loop'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Generates the model’s predictions for the input'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Computes the loss between the outputs and the targets'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we test our trained model using the test set and code shown in the
    following listing.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.9 GAT test loop
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Generates the model’s predictions for the input'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Computes the loss between the outputs and the targets'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: We find that this naive approach is unable to predict the poses. Our overall
    test accuracy is 55%, and the predicted graphs look very different from our expectation
    of the pose’s appearance. This is due to the large amount of data that we’re now
    holding in a single graph. We’re compressing both node features and temporal data
    into one graph, and we’re not emphasizing the temporal property when defining
    our model. There are ways to improve this, such as by using temporal encodings
    to extract the edge data that is unused, as in the temporal GAT (TGAT) model.
    TGAT treats edges as dynamic rather than static, such that each edge also encodes
    a timestamp.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: However, without this time data, our model has become too expressive such that
    the overall structure of the pose has diverged significantly from the original
    structure, as shown with the predicted poses in figure 6.10\. Next, we’ll investigate
    how to combine the best of both approaches into a GNN that uses RNN-based predictions
    by learning on each graph snapshot.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-10.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 Output from the GAT model
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.4 Neural relational inference
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our RNN focused entirely on the temporal data but ignored the underlying relational
    data. This resulted in a model that was able to move in the right direction on
    average but didn’t really alter the individual sensor positions very well. On
    the other hand, our GAT model ignored temporal data by encoding all individual
    temporal graphs into a single graph and attempting node prediction on the unknown
    future graphs. The model caused the sensors to move dramatically, and our resulting
    graphs looked very unlike how we would expect a human to move.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Neural relational inference (NRI), as mentioned earlier, is a slightly different
    approach that uses a more complex encoding framework to combine the best of both
    RNN and GNNs [2]. The architecture for this model is shown in figure 6.11\. Specifically,
    NRI uses an autoencoder structure to embed the information at each timestep. Therefore,
    the embedding architecture is applied to the entire graph in a similar way to
    GAE, which we discussed in chapter 5\. This encoded graph data is then updated
    using an RNN. One key point is that NRI evolves the latent representation of the
    embeddings.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-11.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11 Schematic for NRI (Source: Kipf et al. [2]). The model consists
    of an encoder and decoder layer and several message-passing steps. However, here
    the messages are passed in the encoder from node to edge, back from edge to node,
    and then back from node to edge again. For the decoder, messages are passed from
    node to edge and then from edge to node. The final step takes the latent representation
    and is used to predict the next step in the temporal evolution of the body.'
  id: totrans-155
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s explore how this model applies to our problem of pose estimation so that
    we can best understand the different components in the model. We’ll use the same
    format of masking some data during training and then using the test day to identify
    these masked nodes. Recall that this is equivalent to inferring the future frames
    in our video. However, we now need to change both the model architecture and the
    loss. We need to change the model architecture to account for the new autoencoder
    structure, and we need to adjust the loss to include minimizing the reconstruction
    loss as well as the Kullbeck-Liebler divergence (KL divergence). For more information
    on the NRI model and relevant changes, see section 6.5 at the end of the chapter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: The code for the base class of an NRI model is provided in listing 6.10\. As
    is clear in the code, we need to define an encoder and decoder when calling this
    class. Along with the encoder and decoder, there are some other model-specific
    details we need to be aware of. First, we need to define the number of variables.
    This relates to the number nodes in our graph rather than the number of features
    for each node. In our case, this will be 31, corresponding to each of the different
    sensors tracking a joint position. We also need to define the different types
    of edges between the nodes. This will be either 1 or 0, representing whether an
    edge exists.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: We’ll assume that the way the nodes, or sensors, connect doesn’t change, that
    is, that the graph structure is static. Note that this model also allows for dynamic
    graphs where the connectivity changes over time, for example, when different players
    move around a basketball court. The total number of players is fixed but the number
    of players that can be passed to changes. In fact, this model was also used to
    predict how different players would pass using footage from the NBA.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this model needs some hyperparameters to be set, including the Gumbel
    temperature and the prior variance. *Gumbel temperature* controls the tradeoff
    between exploration and exploitation when performing discrete sampling. Here,
    we need to use a discrete probability distribution to predict the edge type. We
    discuss this in more detail in section 6.5\. *Prior variance* reflects how uncertain
    we are on the connectivity of the graph before we start. We need to set this because
    the model assumes we *don’t* know the connectivity. In fact, the model learns
    the connectivity that best helps it to improve its predictions. This is exactly
    what we’re setting when we call the `_initialize_log_prior` function. We’re telling
    the model what our best guess is for a likely connectivity pattern. For example,
    if we were to apply this model to a sports team, we might use a Gaussian distribution
    with a high mean for edges between players that frequently pass to each other
    or even to players on the same team.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate our model, we’re instead going to assume a uniform prior, which
    means that all edges are as likely as all others, or in everyday terms “we don’t
    know.” The prior variance sets our uncertainty bound for each of the edges. In
    the following listing, we set it to be 5 × 10^(–5) for numerical stability, but
    given that our prior is uniform, it shouldn’t have much effect.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.10 Base class for the NRI model
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Number of variables in the mode'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Encoder neural network'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Decoder neural network'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Gumbel temperature for sampling categorical variables'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Prior variance'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Fills the prior tensor with uniform probabilities'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Takes the log and adds two singleton dimensions'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: As we discovered in chapter 5, VAEs have a two-component loss—the reconstruction
    error and the error in representing the distributional properties of the data—captured
    by the KL-divergence. The total loss function is given in listing 6.11.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Our encoder is passed edge embeddings and then outputs log probabilities of
    an edge type. The Gumbel-Softmax function converts these discrete logits into
    a differentiable continuous distribution. The decoder takes this distribution
    and the edge representations and then converts these back into node data. At this
    point, we’re ready to use the standard loss machinery for VAEs, so we calculate
    the reconstruction loss as MSE and the KL divergence. For further insight into
    VAE losses and how the KL divergence is calculated, revisit chapter 5\.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.11 Loss for the NRI model
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 Calculates Gumbel-Softmax using PyTorch''s functional API, imported as F
    in code'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Negative log likelihood (NLL) for Gaussian distribution'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Adds a small constant to avoid taking the logarithm of zero'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '#4 KL divergence with a uniform categorical distribution'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need our model to be able to predict the future trajectories of
    the sensors. The code for predicting the future state of the graph is given in
    listing 6.12\. This is a relatively simple function once we have our encoder and
    decoder trained. We pass the encoder the current graph, and this returns a latent
    representation of whether an edge exists. We then convert these probabilities
    into a suitable distribution using Gumbel-Softmax and pass this to our decoder.
    The output from the decoder is our predictions. We can either get the predictions
    directly or get both predictions and whether an edge exists.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.12 Predicting the future
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Runs the encoder to get logits for edge types'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Applies Gumbel-Softmax to the edges'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Runs the decoder to get the initial predictions and decoder state'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Uses the last input and decoder state to predict future steps'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Concatenates initial and future predictions if needed'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Returns predictions and edges if specified'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: This is the basis of the NRI model. We have an encoder that converts our initial
    node data into edge probabilities. The edge probabilities get passed to our decoder,
    and the decoder predicts future trajectories conditional on the most likely graph
    representation. Our encoder will be a simple multilayer perceptron (MLP) that
    works on graph data. Our decoder needs to be able to make future predictions,
    so we’ll use an RNN to do this, specifically the same GRU model we discussed in
    section 6.2.2\. Let’s next meet our encoder and decoder networks so we can apply
    our model to the data and see how it performs.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.1 Encoding pose data
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we know the different parts of our NRI model, let’s define our encoder.
    This encoder will act as the bottleneck to make our problem simpler. After encoding,
    we’ll be left with a low-dimensional representation of the edge data, so we don’t
    need to worry about temporal data at this stage. However, by providing our temporal
    data together, we’re transferring temporal structure into our latent space. Specifically,
    the encoder takes the temporal patterns and relationships from the input data
    and preserves this in the compressed, low-dimensional representations. This makes
    it easier to decode from, making our pose prediction problem easier to solve.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: There are several subsets to implementing the encoder. First, we pass the input
    data, which comprises the different sensors at different frames, across different
    experiments. The encoder then takes this data, *x*, and performs a message-passing
    step to transform edge data into node data and then back into edge data. The edge
    data is then converted to node data again before being encoded in the latent space.
    This is equivalent to three message-passing steps, from edges to nodes, edges
    to edges, and edges to nodes again. The repeated transformations are useful for
    information aggregation through repeated message passing and capturing high-order
    interactions in the graph. By repeatedly transforming between nodes and edges,
    the model becomes aware of both local and global structure information.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we’ve explored how to use message passing to convert node
    or edge features into complex representations of nodes or edges. These are at
    the core of all GNN methods. The NRI model is slightly different from the methods
    that we’ve explored before because messages are passed between nodes and edges,
    rather than node to node or edge to edge. To make explicit what these steps are
    doing, we’ll depart from PyG and code our model in plain PyTorch instead.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: In listing 6.13, we show the base class for our encoder, which requires several
    key features. First, note that we haven’t described the actual neural network
    that will be used to encode the data. We’ll introduce this shortly. Instead, we
    have two message-passing functions, `edge2node` and `node2edge`, as well as an
    encoding function, `one_hot_recv`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.13 Encoder base class
  id: totrans-193
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 Creates a matrix representing edges between variables'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Finds the indices where edges exist'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Creates a one-hot representation for receiving edges'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Creates a parameter tensor for edge-to-node transformation'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Extracts sender and receiver embeddings'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Concatenates sender and receiver embeddings'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Multiplies edge embeddings with edge-to-node matrix'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Normalizes the incoming embeddings'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: The first step in our encoder class is to build an adjacency matrix. Here, we
    assume that the graph is fully connected, such that all nodes are connected to
    all other nodes but not to themselves. The `node2edge` function takes node embedding
    data and identifies the direction that these messages have been sent. Figure 6.12
    shows an example of how we’re building the adjacency matrix.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-12.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 Example of creating an adjacency matrix for a fully connected graph
    with three nodes. The matrix on the left represents a fully connected graph, the
    matrix in the middle represents the identity matrix, and the matrix on the right
    shows the final adjacency matrix after subtracting the identity matrix. This results
    in a graph where each node is connected to every other node with no self-loops.
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The next function call then determines which nodes are sending or receiving
    data by returning two vectors that contain rows and columns for connected nodes.
    Recall that in an adjacency matrix, the rows represent receiving nodes and the
    columns represent sending nodes. The output is then
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can interpret this as saying that the node at row 0 sends data to nodes at
    columns 1 and 2, and so on. This allows us to extract edges between nodes. Once
    we construct our node embeddings, we then use the sending and receiving data to
    convert our node data to edges. This is the principle of the `node2edge` function.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: The next function we need is how to build `edge2node` based on our `edge_ embeddings`.
    We first construct an `edge2node` matrix. Here, we’re using a one-hot encoding
    method that converts our receiving edges into a one-hot encoded representation.
    Specifically, we create a matrix where each row denotes whether that category
    (receiving node) exists. For our simple three-node case, the one-hot encoding
    method for the receiving edges is shown in figure 6.13\.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: We then transpose this to switch rows and columns, so that the dimension will
    be (number of nodes, number of edges), and we convert it into a PyTorch parameter
    so that we can differentiate over it. Once we have our `edge2node` matrix, we
    multiple this by our edge embeddings. Our edge embeddings will be of shape (number
    of edges, embedding size) so that multiplying the `edge2node` matrix by the edge
    embeddings gives us an object of shape (number of nodes, embedding size). These
    are our new node embeddings! Finally, we normalize this matrix by the number of
    possible nodes for numerical stability.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: This section is key to understanding the message-passing step in the model.
    (For further information on message passing, revisit chapter 2 and 3.) As discussed
    there, once we have a principled way to pass messages between nodes, edges, or
    some combination of both, we then apply neural networks to these embeddings to
    get nonlinear representations. To do so, we need to define our embedding architecture.
    The code for the complete encoder is given in listing 6.14\.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-13.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 The one-hot encoding matrix representing incoming edges for each
    node in a fully connected graph with three nodes is shown on the left. Each row
    corresponds to an edge, and each column corresponds to a node. A 1 in position
    (i, j) indicates that edge i is directed toward node j. This matrix is used to
    transform edge embeddings to node embeddings in the `edge2node` function of the
    encoder base class, enabling the model to aggregate information from incoming
    edges for each node. In this graph structure, nodes 0, 1, and 2 each send messages
    to the other two nodes, resulting in a total of six directed edges. The diagram
    of the three-node graph is shown on the right.
  id: totrans-213
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The `RefMLPEncoder` is shown in listing 6.14\. This encoder uses four MLPs for
    message processing, each featuring exponential linear unit (ELU) activation and
    batch normalization (defined in `RefNRIMLP`, shown in the chapter’s code repository).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Note  The exponential linear unit (ELU) is an activation function that is useful
    in smoothing outputs across multiple layers and preventing vanishing gradients.
    In contrast to ReLUs, ELUs have a smoother gradient built in for negative inputs
    and allows for negative outputs.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: The final part of the network (`self.fc_out`) is a sequence of linear layers
    with ELU activations between them, ending with a linear layer that outputs the
    desired embeddings or predictions. The final layer of this sequence is a fully
    connected layer.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.14 NRI MLP encoder
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Defines MLP layers. RefNRIMLP is a 2-layer fully connected ELU net with
    batch norm.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Defines the final fully connected layer'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Here, we define architectural details related to the encoder. As discussed earlier,
    there are 31 sensors that we represent using the `num_vars` variable. The number
    of features is 6, which is the `input_size` for our network. The number of timesteps
    for our training and validation set is still 50, and our encoder network size
    will be 256\. The number of `edge_types` is 2, and we assume no dropout of the
    weights. We then initialize our networks, which are typical MLPs, described in
    our shared repository. The networks include a batch normalization layer and two
    fully connected layers. Once the network is defined, we also pre-initialize the
    weights, as shown in listing 6.15\. Here, we loop through all the different layers
    and then initialize the weights using the Xavier initialization approach. This
    ensures that the gradients in the layers are all approximately of similar scale,
    which reduces the risk of our loss rapidly diverging—known as blow-up. This is
    an important step when combining multiple networks with different architectures
    as we do here. We also set the initial bias to 0.1, which further helps with the
    stability of training.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.15 Weight initialization
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 Only applies to linear layers'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Initializes weights using Xavier normal initialization'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sets bias to 0.1'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need to define our forward pass method, as shown in listing 6.16\.
    This is where our message-passing step occurs.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.16 Encoder forward pass
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 New shape: [num_sims, num_atoms, num_timesteps*num_dims]'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Passes through first MLP layer (two-layer ELU network per node)'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Converts node embeddings to edge embeddings'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Passes through the second MLP layer'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Converts edge embeddings back to node embeddings'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Converts node embeddings to edge embeddings again'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Final fully connected layer to get the logits'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Our encoder lets our model transform different sets of frames of our sensor
    graphs into latent representation of edge probabilities. Next, let’s explore how
    to construct a decoder that transforms the latent edge probabilities into trajectory
    using the recent sensor data.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.2 Decoding pose data using a GRU
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To transform the latent representations into future frames, we need to account
    for the temporal evolution of the trajectories. To do so, we train a decoder network.
    Here, we’ll follow the original structure of the NRI paper [2] and use a GRU as
    our RNN.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: We introduced the concept of a GRU in section 6.2.2 earlier. As a quick reminder,
    gated recurrent units (GRUs) are a type of RNN that uses a gated process to allow
    RNNs to capture long-term behaviors in the data. They are composed of two types
    of gates—reset gates and update gates.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: For the NRI model, we’ll apply GRUs to our edges, rather than across the entire
    graph. The update gates will be used to determine how much of the node’s hidden
    state should be updated, given the receiving data, and the reset gate decides
    how much should be erased or “forgotten.” To put it another way, we’ll use a GRU
    to predict what the future state of a node should be based on the edge type probabilities
    from our encoder network.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how we construct this step-by-step. The initialization code for
    our decoder is given in listing 6.17\. First, we note some of the variables passed
    to this network. We again define the number of variables or nodes in our graphs,
    31, and the number of input features, 6\. We assume there is no dropout of the
    weights and the hidden size for each layer is 64\. Again, we need to make clear
    that our decoder should be predicting two different types of edges. We’ll also
    skip the first edge type when making predictions as this denotes that there is
    no edge.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the input parameters defined, we can introduce the network architecture.
    The first layer is a simple linear network that needs to have twice the input
    dimension to account for the mean and variance provided by our encoder, and we
    define this network for each of the edge types. We then define a second layer
    to further increase the expressivity of our network. The output from these two
    linear layers is passed to our RNN, which is a GRU. Here, we have to use a custom
    GRU to account for both node data and edge data. The output from the GRU is passed
    to three more neural network layers to provide the future predictions. Finally,
    we need to define our `edge2node` matrix and sending and receiving nodes, as we
    did with our encoder.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.17 RNN decoder
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '#1 Edge-related layers'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '#2 GRU layers'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Fully connected layers'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'In listing 6.18, we provide the architecture for our GRU. The first overall
    architecture for this network is the same structure as a typical GRU. We define
    three hidden layers which represent the reset gates defined by `hidden_r` and
    `input_r`, the update gates defined by `hidden_i` and `input_i`, and the activation
    networks defined by `hidden_h` and `input_h`. The forward network, however, needs
    to account for the aggregated messages from the message-passing output of our
    encoder. This is shown in the forward pass. We’ll pass the edge probabilities
    in `agg_msgs`, along with the input node data, and these combine to return future
    predictions. This can be seen in the `predict_future` code in our base NRI class:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Our decoder gets passed the last time frame of our graphs. The edge data that
    is output from our encoder is also passed to the decoder.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.18 Custom GRU network
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '#1 Defines hidden layer transformations for reset, input, and new gates'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Defines input layer transformations for reset, input, and new gates'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Computes reset gate activations'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Computes input gate activations'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Computes new gate activations'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Updates hidden state'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'The output from the decoder network is then the future prediction timesteps.
    To better understand this, let’s look at the forward pass method for our decoder,
    given in listing 6.19\. Our forward pass is given the inputs and sampled edges
    to build a prediction. There are also four additional arguments that help control
    the behavior. First, we define a `teacher_forcing` variable. Teaching forcing
    is a typical method used when training sequential models, such as RNNs. If this
    is true, we use the ground truth (the real graph) to predict the next time frame.
    When this is false, we use the output from the model’s previous timestep. This
    makes sure that the model isn’t led astray by incorrect predictions during training.
    Next, we include a `return_state` variable, which allows us to access the hidden
    representations given by the decoder network. We use this when we predict the
    future graph evolution, as shown here:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Let’s now discuss the prediction process. First, we predict a temporary prediction
    set. Then, we use the hidden representations to predict as many steps in the future
    as is needed. This is particularly useful when we want to predict more than one
    timestep, as we show in the testing phase of this model. This is controlled by
    the `prediction_steps` variable, which tells us how many times to loop through
    our RNN, that is, how many timesteps in the future we want to predict. Finally,
    we have a `state` variable, which is used to control the information being passed
    to our decoder. When it’s left empty, we initialize a tensor of zeros so that
    there is no information being passed. Otherwise, we’ll use information from previous
    timesteps.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.19 Decoder forward pass
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#1 Determines the number of prediction steps'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Expands the sampled_edges tensor if needed'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Initializes the hidden state if not provided'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Determines the number of steps to apply teacher forcing to'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Decides the input for this step based on teacher forcing'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Performs a single forward step using the ins calculated from inputs or pred_all
    (see the previous comment)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Returns predictions and the hidden state'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: To predict timesteps into the future, we make an additional forward pass that
    is based on a single timestep, as defined in listing 6.20\. This is where our
    network performs additional message-passing steps. We take our receiver nodes
    and sending nodes, which are defined from the edge probabilities from our encoder.
    We ignore the first edges, as these are unconnected nodes, and the network then
    loops through the different networks for the different edge types to get all edge-dependent
    messages from the network. This is the critical step that makes our predictions
    dependent on the graph data. Our GRU then takes the messages from the connected
    node to inform its predictions of the trajectories. At this step, we’re learning
    to predict how the body is walking from what we’ve learned about how the body
    is connected. The output is both the predicted trajectories of the sensors on
    the body as well as the network data for why it made these predictions, encoded
    in the hidden weights. This completes the NRI model for estimating poses.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.20 Decoder single step forward
  id: totrans-273
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '#1 Node-to-edge step'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Message of size: [batch, num_edges, 2*msg_out]'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Runs a separate MLP for every edge type'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Sums all the messages per node'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '#5 GRU-style gated aggregation'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Builds output MLP'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.3 Training the NRI model
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we’ve defined the different parts of our model, let’s train the model
    and see how it performs. To train our model, we’ll take the following steps:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Train an encoder that converts sensor data into a representation of edge probabilities,
    indicating whether a sensor is connected to another or not.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a decoder to predict future trajectories, conditional on the probability
    of there being an edge connecting the different sensors.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the decoder to predict the future trajectories using a GRU, which is passed
    the edge probabilities.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reduce the loss based on the reconstructed poses. This loss has two components:
    the reconstruction loss and the KL divergence.'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1 through 4 until training converges.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is also shown in figure 6.14, and the training loop is given in listing
    6.21.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-14.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 Pipeline for the NRI model
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Listing 6.21 NRI training loop
  id: totrans-291
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '#1 Training loop'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Update the weights.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Zero gradients for the validation pass'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: We’ll train for 50 epochs with a learning rate of 0.0005, a learning rate scheduler
    that reduces the learning rate by a factor of 0.5 after 500 forward passes, and
    a batch size of 8\. Most of the training is based on the `calculate_loss` method
    call, which we defined earlier in listing 6.14\. We find that our model loss falls
    along with the validation loss, reaching a validation loss of 1.21 based on the
    negative log likelihood (`nll`). This looks good but let’s see how it performs
    on the test data, where it needs to predict multiple steps into the future. To
    do so, we need to define a new function, given in the following listing.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.22 Evaluating future predictions
  id: totrans-297
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This function loads our test data and then calculates the MSE for our predictions
    given different time horizons. When we test our model, we find that it’s able
    to predict the next timestep with an MSE of 0.00008\. Even better, it predicts
    40 timesteps into the future with an accuracy of 94%. This is significantly better
    than our LSTM and GAT models, which achieved 65% and 55%, respectively. The reduction
    in accuracy over future timesteps is shown in figure 6.15, and the example output
    is given in figure 6.16.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-15.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 Reduction in accuracy as we predict into the future
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/6-16.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 Example output from the NRI model
  id: totrans-303
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We’ve covered all the core components for the NRI model, with the full working
    code provided in the GitHub repository ([https://mng.bz/4a8D](https://mng.bz/4a8D)).
    The accuracy is impressive and highlights the power of combining generative and
    graph-based methods with temporal models. This is shown in figure 6.15, where
    we see good agreement with the predicted pose and the resulting estimated pose.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, this method is robust at not just predicting graphs but also learning
    the underlying structure even when all the graph data isn’t available. In this
    problem, we knew what interaction network to expect. However, there are many instances
    where we don’t know the interaction network. One example is particles that are
    moving in a confined space. When they are within some interaction radius, then
    they will influence each other, but not when they are farther away. This is true
    of organisms from cells to sports players. In fact, the majority of the world
    involves interacting agents with secret interaction networks. NRI models provide
    a tool to not only predict the behavior and movement of these agents but also
    learn about their interaction patterns with other agents. Indeed, the original
    NRI paper demonstrated this using video tracking data of basketball games and
    showed that the model can learn typical patterns between ball, ball handler, screener,
    and defensive matchups for the different players. (For more information, refer
    to Kipf et al. [2].)
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Under the hood
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we showed how to tackle temporal or dynamic problems. Here,
    we go into more detail for some of the key model components that we used.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.1 Recurrent neural networks
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In figure 6.16, we showed a schematic for RNN models. The main difference for
    RNN models compared to all the other models that we’ve seen is that the model
    can cope with sequential data. This means that each timestep has a hidden layer,
    and output from this hidden layer is combined with new input at subsequent timesteps.
    In figure 6.17, this is shown in two ways. First, on the left side, we show the
    temporal updates as a single self-loop denoted by Whh. To get a better understanding
    of what this self-loop is doing, we’ve “unfolded” the model in time so that we
    can explicitly see how our model updates. Here, we change our input, output, and
    hidden layers (x, y, h) to be temporal variables (xt, yt, ht). At our initial
    step, t, we update our current hidden layer with input data from xt and the weights
    from our previous hidden layer ht–1 and then use this to output yt. The weights
    from ht are then passed to ht+1 along with the new input at xt+1 to infer yt+1\.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: One of the key features for this model is that when we backpropagate to update
    our weights, we need to backpropagate through time (BPTT). This is a specific
    feature for all RNNs. However, most modern deep learning packages make this very
    straightforward to do and hide all the difficult computational details for the
    practitioner.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-17.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 Structure for an RNN. Temporal updates as a single self-loop denoted
    by Whh (left). An unfolded model in time showing the model updates (right). Here,
    we change our input, output, and hidden layers (x, y, h) to be temporal variables
    (x[t], y[t], h[t]). At our initial step, t, we update our current hidden layer
    with input data from x[t] and the weights from our previous hidden layer h[t–1]
    and then use this to output y[t]. The weights from ht are then passed to h[t+1]
    along with the new input at x[t+1] to infer y[t+1].
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s see how to implement an RNN using PyTorch. This is as straightforward
    as defining a neural network class and then introducing specific RNN layers within
    the network. For example, in listing 6.23, we show the code for defining a network
    with a single RNN layer. This is a very basic definition of an RNN, given there
    is only one hidden layer. However, it’s useful to see this example to get some
    solid intuition on how a model can be trained. For each timestep, our input is
    passed both to the hidden layer and the output. When we perform a forward pass,
    the output goes back to output and the hidden layer. Finally, we need to initialize
    our hidden layer with something, so we’re using a fully connected layer.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.23 Defining an RNN
  id: totrans-314
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#1 RNN layer'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Fully connected layer'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sets the initial hidden and cell states'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Forward propagates the RNN'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Passes the output of the last timestep to the fully connected layer'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we often want to use more complicated RNNs. This includes extensions
    to RNNs such as LSTM networks or GRU networks. We can even stack RNNs, LSTMs,
    and GRUs together using our deep learning library of choice. A GRU is similar
    to an RNN in that it’s useful for sequences of data. They were specifically designed
    to resolve one of the key drawbacks of RNNs, the vanishing gradient problem. It
    uses two gates, which determine both how much past information to keep (the update
    gates) and how much to forget or throw away (the reset gates). We show an example
    design for a GRU in figure 6.18\. Here, *z**[t]* denotes the update gates, and
    r*[t]* denotes the reset gates. The *~h**[t]* term is known as the candidate activation
    and reflects a candidate for the new state of the representations, while the *h**[t]*
    term is the actual hidden state.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-18.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 Design of the GRU layer, where r*[t]* represents the reset gate,
    z*[t]* is the update gate, ~h*[t]* is the candidate function, and h*[t]* is the
    final actual hidden state
  id: totrans-323
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In listing 6.24, we show how to build a model with GRU layers. Here, the majority
    of the implementation is handled by PyTorch, where the layer is imported from
    the standard PyTorch library. The rest of the model definition is a typical neural
    network.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.24 GRU
  id: totrans-325
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '#1 GRU layer'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Fully connected layer'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sets the initial hidden state'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Forward propagates the GRU'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Passes the output of the last timestep to the fully connected layer'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.2 Temporal adjacency matrices
  id: totrans-332
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When considering temporal graphs, we might start with two nodes connected by
    one edge, then at each subsequent timestep, another few nodes and/or edges are
    added. This results in several distinct graphs, each with a differently sized
    adjacency matrix.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: This might present a difficulty when designing our GNN. First, we have different
    sized graphs at each timestep. This means we won’t be able to use node embeddings
    because the number of nodes will keep changing across input data. One method is
    to use graph embeddings at each timestep to store the entire graph as a low-dimensional
    representation. This method is at the heart of many temporal approaches, where
    graph embeddings are evolved in time rather than the actual graph. We can even
    use more complex transformations on our graph, such as using an autoencoder model
    as in our NRI model.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can transform all the individual graphs at each timestep into
    one single larger graph by creating a temporal adjacency matrix. This involves
    wrapping each timestep into a single graph that spans both per-timestep data as
    well as dynamic temporal data. Temporal adjacency matrices can be useful if a
    graph is small and we’re only interested in a few timesteps in the future. However,
    they can often become very large and difficult to work with. On the other hand,
    using temporal embedding methods can often involve multiple complicated subcomponents
    and become difficult to train. Unfortunately, there is no one-size-fits-all temporal
    graph, and the best approach is almost always problem specific.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.3 Combining autoencoders with RNNs
  id: totrans-336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, to build intuition around the NRI model, we’ll summarize its
    components and illustrate its application in predicting graph structures and node
    trajectories. To start, in figure 6.19, we repeat the schematic for the NRI model.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-19.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.19 Schematic for NRI (Source: Kipf et al. [2]). The model consists
    of an encoder and decoder layer and several message-passing steps. However, here
    the messages are passed in the encoder from node to edge, back from edge to node,
    and then back from node to edge again. For the decoder, messages are passed from
    node to edge and then from edge to node. The final step takes the latent representation
    and is used to predict the next step in the temporal evolution of the body.'
  id: totrans-339
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this model, there are two key components. First, we train an encoder to encode
    the graphs from each frame into the latent space. Explicitly, we use the encoder
    to predict the probability distribution, q[j](z|x) over the latent interactions
    (z), given the initial graphs (x). Once we’ve trained the encoder, we then use
    the decoder to convert samples from this probability distribution into trajectories
    using the latent encoding as well as previous timesteps. In practice, we use the
    encoder-decoder structure to infer the trajectories of nodes with different interaction
    types (or edges).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ve only considered two edge types: where there is or isn’t
    a physical connection between sensors. However, this method can be scaled to consider
    many different connections, all changing with time. Additionally, the decoder
    model needs an RNN to effectively capture the temporal data in our graph. To build
    some intuition around the NRI model, let’s repeat the process once more.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '*Input *—Node data.'
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Encoding *—'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The encoder receives the node data.
  id: totrans-344
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The encoder converts the node data into edge data.
  id: totrans-345
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The encoder represents the edge data in a latent space.
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Latent space *—The latent space represents probabilities of different edge
    types. Here, we have two edge types (connected and not connected), though multiple
    edge types are possible for more complex relationships. We always need to include
    at least two types as otherwise the model would assume all the nodes are connected
    or, worse, none of them are.'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Decoding *—'
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The decoder takes the edge type probabilities from the latent space.
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The decoder learns to reconstruct the future graph state based on these probabilities.
  id: totrans-350
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Prediction *—The model predicts future trajectories by learning to predict
    graph connectivity.'
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that this model gives us graph and trajectory predictions simultaneously!
    While this might not be helpful for our problem, for cases where we don’t know
    the underlying graph structure such as social media networks or sports teams,
    this can provide ways to discover new interaction patterns in a system.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.4 Gumbel-Softmax
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the NRI model, there is an additional step before calculating both of these
    losses, which is calculating the probability of an edge using Gumbel-Softmax.
    The key reason we need to introduce Gumbel-Softmax is that our autoencoder is
    learning to predict the adjacency matrix representing our edges, that is, the
    network connectivity, rather than the nodes and their features. Therefore, the
    end predictions for the autoencoder have to be discrete. However, we’re also inferring
    a probability. Gumbel-Softmax is a popular approach whenever probability data
    needs to be made discrete.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have two discrete types of edges, that is, whether something is or
    isn’t connected. This means that our data is *categorical *—each edge is either
    in category 0 (isn’t connected) or category 1 (connected). Gumbel-Softmax is used
    to draw and score samples from a categorical distribution. In practice, Gumbel-Softmax
    will approximate the output from our encoder, which comes in the form of log probabilities
    or *logits*, as a Gumbel distribution, which is an extreme value distribution.
    This approximates the continuous distribution of our data as a discrete one (edge
    types) and allows us to then apply a loss function to the distribution.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: The temperature of a Gumbel distribution, one of our hyperparameters, reflects
    the “sharpness” of the distribution, similar to how variance controls the sharpness
    of a Gaussian distribution. In this chapter, we used a temperature of 0.5, which
    is about medium sharpness. We also specify `Hard` as a hyperparameter, which denotes
    whether one or more categories exist. As discussed, we want it to have two categories
    when training to represent whether an edge exists. This allows us to approximate
    the distribution as a continuous one, and then we can backpropagate this through
    our network as a loss. However, when testing, we can set `Hard` to `True`, which
    means that there is only one category. This makes the distribution fully discrete,
    meaning we can’t optimize using the loss, as discrete variables are nondifferentiable
    by definition. This is a useful control to make sure that our test loop doesn’t
    propagate any gradients.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While some systems can use single snapshots of data to make predictions, others
    need to consider changes in time to avoid errors or vulnerabilities.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spatiotemporal GNNs consider previous timesteps to model how graphs evolve over
    time.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spatiotemporal GNNs can solve pose-estimation problems where we predict the
    next position of the body given some data on how the body position was in the
    recent past. In this case, nodes represent sensors placed on body joints, and
    edges represent the body connections between joints.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjacency matrices can be adapted to consider temporal information by concatenating
    different adjacency matrices along the diagonal.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory can be introduced into models, including GNNs, such as by using a recurrent
    neural network (RNN) or a gated recurrent unit network (GRU).
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The neural relational inference (NRI) model combines recurrent networks such
    as a GRU with autoencoder GNNs. These models can infer temporal patterns, even
    where adjacency information is unknown.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
