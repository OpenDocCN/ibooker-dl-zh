- en: '6 Dynamic graphs: Spatiotemporal GNNs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Introducing memory into your deep learning models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the different ways to model temporal relations using graph neural
    networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing dynamic graph neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating your temporal graph neural network models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So far, all of our models and data have been single snapshots in time. In practice,
    the world is dynamic and in constant flux. Objects can move physically, following
    a trajectory in front of our eyes, and we’re able to predict their future positions
    based on these observed trajectories. Traffic flow, weather patterns, and the
    spread of diseases across networks of people are all examples where more information
    can be gained when modeled with spatiotemporal graphs instead of static graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Models that we build today might quickly lose performance and accuracy as we
    deploy them in the real world. These are problems intrinsic to any deep learning
    (and machine learning) model, known as *out-of-distribution (OOD) generalization,*
    that is, how well models generalize to entirely unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we consider how to make models that are suitable for dynamic
    events. While this doesn’t mean they can deal with OOD data, our dynamic models
    will be able to make predictions about unseen events in the future using the recent
    past.
  prefs: []
  type: TYPE_NORMAL
- en: To build our dynamic graph-based learning model, we’ll consider the problem
    of pose estimation. *Pose estimation* relates to those classes of problems that
    predict how bodies (human, animal, or robotic) move over time. In this chapter,
    we’ll consider a body walking and build several models that learn how to predict
    the next step from a series of video frames. To do this, we’ll first explain the
    problem in more detail and how to understand this as a relational problem before
    jumping in to see how graph-based learning approaches this problem. As with the
    rest of our book, further technical details are left to section 6.5 at the end
    of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use much of the material that we’ve already covered in the book. If you’ve
    skipped ahead to this chapter, make sure you have a good understanding of the
    concepts described in the “Building on what you’ve learned” sidebar.
  prefs: []
  type: TYPE_NORMAL
- en: Note  Code from this chapter can be found in notebook form at the GitHub repository
    ([https://mng.bz/4a8D](https://mng.bz/4a8D)).
  prefs: []
  type: TYPE_NORMAL
- en: Building on what you’ve learned
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'To introduce temporal updates into our GNN, we can build on some of the concepts
    that we’ve learned in previous chapters. As a quick refresher, we’ve summarized
    some of the main important features from each chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Message passing*—In chapter 2, you learned that the main method used by GNNs
    to learn from relational data is by combining message passing with artificial
    neural networks. Each layer of a GNN can be understood as one step of message
    passing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Graph convolutional networks (GCNs)*—In chapter 3, you saw that message passing
    itself can be understood as the relational form of the convolution operator (as
    in convolutional neural networks [CNNs]), and this is the central idea behind
    GCNs. Messages can also be averaged across neighborhoods by only sampling a subset
    of nearest neighbors. This is used for GraphSAGE and can considerably reduce the
    total compute needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Attention*—In chapter 4, we showed how the aggregation function for message
    passing doesn’t need to be restricted to only summing, averaging, or max operations
    (though the operation must be permutation invariant). Attention allows for a weighting
    to be learned during training to give more flexible message-passing aggregation
    functions. Using a graph attention network (GAT) is the basic form of adding attention
    to message passing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Generative models*—While discriminative models seek to learn separations between
    data classes, generative models attempt to learn the underlying data-generating
    process. The autoencoder is one of the most popular frameworks for designing generative
    models, where data is passed through a neural network bottleneck to create a low-dimensional
    representation of the data, also called the latent space. These are commonly implemented
    as graph autoencoders (GAEs) or variational graph autoencoders (VGAEs) for graphs,
    as we discussed in chapter 5\.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '6.1 Temporal models: Relations through time'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Almost every data problem will, in some way, also be a dynamic problem. In many
    cases, we can ignore changes in time and build models that are suitable for snapshots
    of the data that we’ve collected. For example, image segmentation methods rarely
    consider video footage to train models.
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 3, we used a GCN to predict suitable products to recommend to customers
    using data on a customer-purchaser network. We used a toy dataset that had been
    collected over a period of several years. However, in reality, we’ll often have
    constant streams of data and want to make up-to-date predictions that account
    for both customer and cultural habit changes. Similarly, when we applied a GAT
    to a fraud-detection problem, the data we used was a single snapshot of financial
    records that was collected over a period of several years. However, we didn’t
    account for how financial behaviors changed over time in our model. Again, we
    would likely want to use this information to predict where an individual’s spending
    behavior abruptly changes to help us detect fraudulent activity.
  prefs: []
  type: TYPE_NORMAL
- en: These are just a few of the many different dynamic problems that we’re faced
    with every day (see figure 6.1). GNNs are unique in that they can model both dynamic
    and relational changes. This is very important as many of the networks that operate
    around us are also moving in time. Take, for example, a social network. Our friendships
    change, mature, and sadly (or fortunately!) weaken over time. We might become
    stronger friends with work colleagues or friends of friends and see friends from
    our hometown less frequently. Making predictions for social networks need to account
    for this.
  prefs: []
  type: TYPE_NORMAL
- en: As another example, we often make predictions about which way to go and when
    we might arrive based on our knowledge of the roads, traffic patterns, and how
    much of a rush we’re in. A dynamic GNN can also be used to help make use of this
    data, by treating the road network as a graph and making temporal predictions
    on how this network will change. Finally, we can consider predicting how two or
    more objects move together, that is, by estimating their future trajectories.
    While this might seem less useful than making friends or getting to work on time,
    predicting trajectories of interacting bodies, such as molecules, cells, objects,
    or even stars, is vital to many sciences as well as for robotic planning. Again,
    dynamic GNNs can help us both predict these trajectories and infer new equations
    or rules that explain them.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 Examples of different dynamic problems
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'These examples are just the tip of the iceberg for applications where we need
    to model temporal changes. In fact, we’re sure that you can think of many others.
    Given the importance of knowing how to combine relational learning with temporal
    learning, we’ll cover three different methods for building dynamic models, two
    of which use GNNs: a recurrent neural network (RNN) model, a GAT model, and a
    neural relational inference (NRI) model. We’ll build machine learning models that
    “learn to walk” by estimating how a human pose changes over time. These models
    are often deployed in, for example, medical consultations, remote home security
    services, and filmmaking. The models are also a great toy problem for us to learn
    to walk before we can run. In that spirit, let’s first learn more about the data
    and build our first benchmark model.'
  prefs: []
  type: TYPE_NORMAL
- en: '6.2 Problem definition: Pose estimation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll solve a “dynamic relational” problem with one set of
    data: preprocessed segmentation of a body walking. This is a useful dataset to
    explore these techniques, as a moving body is a textbook example of an interacting
    system: our foot moves because our knee moves because a leg moves, and our arms
    and torso will all move too. This means that there is a temporal component to
    our problem.'
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, our pose estimation problem is about path prediction. More precisely,
    we want to know where, for example, a foot will move having followed the rest
    of the body for some number of previous timesteps. This type of object tracking
    is something that we do every day, for example, when we play sports, catch something
    that’s falling, or watch a television show. We learn this skill as a child and
    often take it for granted. However, as you’ll see, teaching a machine to perform
    this object tracking was a significant challenge up until the emergence of spatiotemporal
    GNNs.
  prefs: []
  type: TYPE_NORMAL
- en: The skills that we’ll use for path prediction are important for many other tasks.
    Predicting events in the future is useful when we want to predict the next purchase
    of a customer or understand how weather patterns will change based on geospatial
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll be using the Carnegie Mellon University (CMU) Motion Capture Database
    ([http://mocap.cs.cmu.edu/](http://mocap.cs.cmu.edu/)), which contains many examples
    of different dynamic poses, including walking, running, jumping, and performing
    sports moves, as well as multiple people interacting [1]. Throughout this chapter,
    we’ll use the same dataset of subject #35 walking. At each timestep, the subject
    has 41 sensors that each follow a single joint, ranging from the toes up to the
    neck. An example of the data from this database is shown in figure 6.2\. These
    sensors track the movement of part of the body across snapshots of their motion.
    In this chapter, we won’t follow the entire motion and consider only a small subset
    of the motion. We’ll use the first 49 frames for our training and validation datasets
    and 99 frames for our test set. In total, there are 31 different examples of this
    subject walking. We’ll discuss more about the structure of our data in the next
    section.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 Snapshots in time (t = time in seconds) of a human subject walking.
    The dots represent sensors placed on key joints on the human’s body. These snapshots
    are across 30 seconds. To represent these figures as a graph, the sensor placements
    (joints) can be represented as nodes, and the body’s connections between the joints
    are the edges.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.2.1 Setting up the problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our aim is to predict the dynamics for all the individual joints. Clearly, we
    can construct this as a graph because all the joints are connected through edges,
    as shown previously in figure 6.2\. Therefore, it makes sense to use GNNs to solve
    this problem. However, we’ll first compare another approach, which doesn’t account
    for the graph data, to benchmark our GNN models.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We’ve included the steps to download and preprocess the data in our code repository.
    The data is contained within a zip file where each of the different trials is
    saved as an advanced systems format (.asf) file. These .asf files are basically
    just text files that contain the label for each sensor and their xyz coordinates
    at each timestep. In the following listing, we show a snippet of the text.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 Example of the sensor data text files
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Here, the first number is the frame number, and `root` is specific to the sensors
    and can be ignored. `lowerback`, `upperback`, `thorax`, `lowerneck`, and `upperneck`
    denote the positions of the sensors. In total, there are 31 sensors mapping the
    movement of a man walking. To convert this sensor data into trajectories, we need
    to calculate the change in position for each sensor. This becomes quite a complicated
    task, as we need to account for both translational movements and angular rotations
    for the various sensors between each frame. Here, we’ll use the same data files
    as in the NRI paper [2]. We can use these to map out the trajectories of each
    individual sensor in x, y, and z, or look at how the sensors are moving in two
    dimensions to get intuition about how the entire body is moving. Examples of this
    are shown in figure 6.3, where we focus on the movement of a foot sensor in x,
    y, and z, as well as the overall movement of the body over time (with the sensor
    shown as solid black stars).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 Preconstructed spatial trajectories of sensors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Along with the spatial data, we can also calculate the velocity data. This data
    is provided as separate files for each of the movie frames. An example of the
    change in velocity data is shown in figure 6.4\. As you can see, the velocity
    data varies around a smaller range. Both spatial and velocity data will be used
    as the features in our machine learning problem. Here, we now have six features
    across 50 frames for each of our 31 sensors and across 33 different trials. We
    can understand this as a multivariate time series problem. We’re trying to predict
    the future evolution of a six-dimensional (three spatial and three velocity) object
    (each sensor). Our first approach will treat these as independent, looking to
    predict future positions and velocity based on past sensor data. We’ll then switch
    to treating this as a graph, where we can couple all sensors together.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 Preconstructed velocity data of sensors
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Currently, this is a relational problem, but we’re only considering the node
    data and not the edge data. Where there is node data and no edge data, we have
    to be careful not to make too many assumptions. For example, if we chose to connect
    nodes based on their distance from one another, then we might end up with a very
    strange-looking skeleton, as shown in figure 6.5\. Luckily, we have the edge data
    as well, which has been built using the CMU dataset and is included in the data
    provided. This serves as a cautionary tale that GNNs are only as powerful as the
    graphs they’re trained on and that we must take care to ensure that the graph
    structure is correct. However, if edge data is entirely lacking, then we can attempt
    to infer the edge data from the node data itself. While we won’t be doing this
    here, note that the NRI model we’ll be using has this capability.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 Sensor networks showing the error of wrongly inferring graph structures.
    The nodes are human skeletal connections. The left figure shows a network with
    edges inferred from node proximity (closest nodes connected to one another). This
    figure does not reflect a real human skeleton. The true set of edges is shown
    in the right figure.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We now have all of our data loaded. In total, we have three datasets (training,
    validation, testing) that each contain 31 individual sensor positions. Each of
    these sensors contain six features (spatial coordinates) and are connected by
    an adjacency matrix that is constant in time. The sensor graph is undirected,
    and the edges are unweighted. The training and validation sets contain 49 frames,
    and the test sets contain 99 frames.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Building models with memory
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that our problem is defined and our data is loaded, let’s consider how
    we might approach the problem of predicting the joint dynamics. First, we need
    to think about what the underlying aim is. At its core, we’ll be involved in sequence
    prediction, just like autocomplete on a phone or search tool. These types of problems
    are often approached using networks, such as transformers, for which we use an
    attention mechanism as in chapter 4\. However, before attention-based networks,
    many deep learning practitioners instead approached sequence prediction tasks
    by introducing memory into their models [3]. This makes intuitive sense: if we
    want to predict the future, we need to remember the past.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s build a simple model that predicts the next location for all the individual
    sensors using past events. Essentially, this means we’ll build a model that predicts
    the position of nodes without edge data. An example of what we’ll be attempting
    is shown in figure 6.6\. Here, we’ll start by preprocessing and preparing our
    data to be passed to a model that can predict how the data evolves over time.
    This allows us to predict the changes in the pose given a few input frames.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 Predicting future positions using only sensor data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To introduce memory to our neural networks, we’ll start by considering a recurrent
    neural network (RNN). Similar to convolutional and attention neural networks,
    RNNs are a broad class of architectures that are fundamental tools for researchers
    and practitioners alike. For more information about RNNs, see, for example *Machine
    Learning with TensorFlow* (Manning, 2020, [https://mng.bz/VVOW](https://mng.bz/VVOW)).
    RNNs can be considered as multiple individual networks that link together. These
    repeating subnetworks allow for past information to be “remembered” and the effect
    from past data to affect future predictions. After initializing, each subnetwork
    takes in input data as well as the output of the last subnetwork, and these are
    used to make new predictions. In other words, each subnetwork takes input and
    information from the recent past to build inferences about the data. However,
    a vanilla RNN will only ever remember the preceding step. They have *very* short-term
    memory. To improve the effect of the past on the future, we need something stronger.
  prefs: []
  type: TYPE_NORMAL
- en: Long short-term memory (LSTM) networks are another extremely popular neural
    network architecture for modeling and predicting temporal or sequential information.
    These networks are special cases of RNN that similarly link multiple subnetworks
    together. The difference is that LSTMs introduce more complex dependencies in
    the subnetwork structure. LSTMs are particularly useful for sequential data as
    they resolve the problem of vanishing gradients that is observed for RNNs. Put
    simply, *vanishing gradients* refers to where the gradient that we use to train
    our neural network using gradient descent approaches zero. This is especially
    likely to happen when we train an RNN that has many layers. (We won’t go into
    the reasons for this here, but if you’re interested, read *Deep Learning with
    Python* (Manning, 2024, [https://mng.bz/xKag](https://mng.bz/xKag)) for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Gated recurrent unit networks (GRUs) also resolve the problem of vanishing gradients
    by allowing new information to be added to the memory store about the recent past.
    This is achieved through a gating structure, where gates within the model architecture
    help to control the flow of information. These gates also add a new design element
    to how we can build and adapt our neural networks. We won’t consider LSTM here
    as it’s outside the scope of the book, but again we recommend that you check out
    *Deep Learning with Python* (Manning, 2024, [https://mng.bz/xKag](https://mng.bz/xKag))
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Constructing a recurrent neural network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Let’s now look at how to use an RNN to predict the trajectories of the body
    sensors over time, which will act as one of our baselines for future performance
    gains. We won’t go into the details of RNNs and GRU architectures but additional
    information is provided at the end of the chapter in section 6.5\.
  prefs: []
  type: TYPE_NORMAL
- en: The idea for this model is that our RNN will predict the future positions for
    sensors without taking into account relational data. When we start to introduce
    our graph models, we’ll see how this can be improved.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll use the same standard training loop for deep learning, as shown in figure
    6.7\. Once we define our model and define a training and test loop, we use these
    to train and then test the model. As always, we’ll keep the training and testing
    data completely separate and include a validation set of data to make sure our
    model isn’t over-fitting during training.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 Standard process for training a deep learning model that we’ll follow
    throughout this chapter
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The training loop used here is fairly standard, so we’ll describe it first.
    In the training loop definition shown in listing 6.2, we follow the same convention
    as in previous chapters, looping through model prediction and loss updates over
    a fixed number of epochs. Here, our loss will be contained in our criterion function,
    which we define as a simple mean standard error (MSE) loss. We will use a learning
    rate scheduler, which will reduce the learning rate parameter after our validation
    loss starts to plateau. We initialize the best loss as infinity and lower the
    learning rate after the validation loss is less than our best loss for `N` steps.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.2 Training loop
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Initializes loss and accuracy variables'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Begins the training loop'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Zeros the parameter gradients'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Forward + backward + optimize'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Updates training loss, multiplying by the number of samples in the current
    mini-batch'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Begins the validation loop'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Checks for early stopping'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Steps the scheduler'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Calculates and stores losses'
  prefs: []
  type: TYPE_NORMAL
- en: Both layers are trained (using our training loop in listing 6.3) for a specific
    task. For both the RNN and the GRU, the format for the data will be the individual
    trials or videos, the frame timestamp, the number of sensors, and the features
    of the sensors. By providing the data broken up into individual snapshots of time,
    the model is able to use the temporal aspects to learn from. Here, we use the
    RNN to predict the future position for each individual sensor, given the 40 previous
    frames. For all of our calculations, we’ll normalize the data based on the node
    features (position and velocity) using min-max scaling.
  prefs: []
  type: TYPE_NORMAL
- en: After we finish our training loop, we test our network. As always, we don’t
    want to update the parameters of our network, so we make sure that there is no
    backpropagated gradient (by selecting `torch.no_grad()`). Note that we choose
    a sequence length of 40 so that our testing loop is able to see the first 40 frames
    and then attempt to infer the final 10 frames.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.3 Testing loop
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Sets the model to evaluation mode'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Updates inputs for the next prediction'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Computes the loss for this sequence'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Converts predictions to a NumPy array for easier manipulation'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Computes the average test loss'
  prefs: []
  type: TYPE_NORMAL
- en: Once our models are defined, we can next use the training loop given in listing
    6.3 to train our model. At this point, you might be wondering how we’ll amend
    the training loop to correctly account for the temporal element when backpropagating.
    The good news is that this is handled automatically by PyTorch. We find that the
    RNN model is able to predict the future positions with 70% accuracy for the validation
    data and 60% accuracy for the test data.
  prefs: []
  type: TYPE_NORMAL
- en: We also tried a GRU model to predict the future steps taken and found this model
    is able to get an accuracy of 75% using the validation data. This is quite low
    but not as low as it might be given the simplicity of the model and the little
    amount of information that we’ve passed it. However, when we test the model performance
    on our test data, we can see that performance falls to 65%. A few example outputs
    from our model are shown in figure 6.8\. Clearly, the model quickly degrades,
    and the estimated pose position starts to vary widely. For better accuracy, we’ll
    need to use some of the relational inductive biases in the pose data.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 Predicting future movements using an RNN. Here, figures on the left
    represent the true data, and those on the right represent the predicted data.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.3 Dynamic graph neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To predict the future evolution of the graph, we need to restructure our data
    to account for temporal data. Specifically, dynamic GNNs connect different sequential
    snapshots of the graph’s evolution and learn to predict future evolutions [4–6].
    One method for doing so is to combine them into a single graph. This temporal
    graph now contains both per-timestep data and the temporal connections encoded
    as nodes with temporal edges. We’ll first approach the task of pose estimation
    by taking a naive approach to modeling graph evolution. We’ll look at how we can
    combine our temporal data into one large graph and then predict the future evolution
    by masking the nodes of interest. We’ll use the same GAT network that you saw
    in chapter 3\. Then, in section 6.4, we’ll show another method for solving the
    pose estimation problem by instead encoding each snapshot of the graph and predicting
    the evolution using a combination of variational autoencoders (VAEs) and RNNs,
    which is the NRI method [2].
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Graph attention network for dynamic graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll look at how to convert our pose estimation problem into a graph-based
    problem. To do this, we need to construct an adjacency matrix that accounts for
    temporal information. First, we need to load our data in as a PyTorch Geometric
    (PyG) data object. We’ll use the same location and velocity data that we used
    to train our RNN. The difference here is that we’ll construct a single graph that
    contains all the data. The code snippet in listing 6.4 shows how we initialize
    our dataset. We pass the paths for where the location and velocity data are as
    well as where the edge data is located. We also pass whether we need to transform
    our data and the mask and window size that we’ll predict over.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.4 Loading the data as a graph
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Loads the data from .npy files'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Determines the mask size'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Determines the window size'
  prefs: []
  type: TYPE_NORMAL
- en: For all our dataset objects, we need a `get` method inside the class to describe
    how to retrieve this data, which is shown in listing 6.5\. This method combines
    the location and velocity data into node features. We also provide an option to
    transform the data using a `normalize_array` function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.5 Set up node features using location and velocity data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Concatenates location and velocity data for each node'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Determines the mask size'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Applies normalization if transform is True'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Repeats the edges for the total number of timesteps (past + future)'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Applies the shift to the edge indices'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Flattens the edge indices into two dimensions'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Converts everything to PyTorch tensors'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Calculates the indices of the masked nodes'
  prefs: []
  type: TYPE_NORMAL
- en: We next want to combine all nodes across the different timesteps into one large
    graph containing all individual frames. This gives an adjacency matrix that covers
    all different timesteps. (For further details on the idea of temporal adjacency
    matrices, see section 6.5 at the end of this chapter.) To do this for our pose
    estimation data, we first construct the adjacency matrix for each timestep, as
    shown in listing 6.6 and included in listing 6.5\.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in figure 6.9, the process begins by representing the graph data across
    multiple timesteps, where each timestep is treated as a distinct layer (Step 1).
    All nodes have node feature data (not shown in the figure). For our application,
    the node feature data consists of location and velocity information.
  prefs: []
  type: TYPE_NORMAL
- en: Nodes within a timestep are connected to each other using intra-timestep edges,
    that is, connections between nodes on the same timestep layer (Step 2). These
    edges ensure that each graph at a specific timestep is internally consistent.
    The nodes are not yet connected across timesteps.
  prefs: []
  type: TYPE_NORMAL
- en: To incorporate temporal relationships, inter-timestep edges (i.e., connections
    between nodes on different timestep layers) are added to connect corresponding
    nodes across adjacent timesteps (Step 3). These edges allow information to flow
    between nodes in different timesteps, enabling temporal modeling of the graph
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In preparation for predicting future values, the nodes in the last timestep
    are masked to represent unknown data (Step 4). These masked nodes are treated
    as the target of the prediction task. Their values are unknown, but they can be
    inferred by leveraging the features and relationships of the unmasked nodes in
    earlier timesteps.
  prefs: []
  type: TYPE_NORMAL
- en: The inference process (Step 5) involves using the known features of unmasked
    nodes from previous timesteps (t = 0 and t = 1) to predict the features of the
    masked nodes in t = 2\. Dotted arrows illustrate how information flows from unmasked
    nodes to masked nodes, showing the dependency of the predictions on earlier graph
    data. This transforms the task into a node prediction problem, where the goal
    is to estimate the features of the masked nodes based on the relationships and
    features of the unmasked nodes.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 Illustration of the spatiotemporal graph construction and inference
    process. Step 1 shows the sequence of graphs across timesteps with nodes representing
    entities at each timestep. Step 2 highlights intra-timestep edges (solid lines)
    connecting nodes within the same graph layer. Step 3 introduces inter-timestep
    edges (dotted lines) that encode temporal dependencies by linking corresponding
    nodes across adjacent timesteps. In Step 4, nodes at the final timestep are masked
    (gray) to represent unknown values for prediction. Step 5 demonstrates the inference
    process (dashed arrows), where information from unmasked nodes in earlier timesteps
    is used to estimate the features of masked nodes. The legend clarifies the types
    of nodes and edges used in the graph representation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Listing 6.6 Constructing the adjacency matrix
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Repeats the edges for the total number of timesteps (past + future)'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates a shift for each timestep'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Applies the shift to the edge indices'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Flattens the edge indices into two dimensions'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the adjacency matrix, the next step is to build a model that
    can predict future timesteps. Here, we’ll use a GAT model, introduced in chapter
    4 [7]. We choose this GNN because it can be more expressive than other GNNs, and
    we want something that is able to account for the different temporal and spatial
    information. The model architecture is provided in listing 6.7\.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.7 Defining the GAT model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 First GAT layer'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 BatchNorm layer for the first GAT layer'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Intermediate GAT layers'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 BatchNorm layers for intermediate GAT layers'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Last GAT layer'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Don’t apply batch normalization and dropout to the output of the last GAT
    layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Only outputs the last frame'
  prefs: []
  type: TYPE_NORMAL
- en: This model follows the basic structure outlined in chapter 4\. We define the
    number of layers and heads for our model as well as the relevant input size, which
    depends on the number of features that we’re predicting. Each of our GAT layers
    has a hidden size and we include dropout and batch normalization to improve performance.
    We then loop through the number of layers in our model, ensuring that the dimensions
    are correct to match our target output. We also define our forward function, which
    predicts the node features for the masked nodes. By unwrapping each timestep into
    a larger graph, we start to introduce temporal effects as additional network structures
    that our model can learn.
  prefs: []
  type: TYPE_NORMAL
- en: With both model and dataset defined, let’s start training our model and see
    how it performs. Recall that the RNN and GRU achieved 60% and 65% in test accuracy,
    respectively. In listing 6.8, we show the training loop for our GAT model. This
    training loop follows the same structure as that used in previous chapters. We
    use the MSE as our loss functions and set the learning rate to 0.0005\. We calculate
    the node features of the masked nodes using our GAT and then compare these to
    the true data, which is stored in `data`. We first train our model and then compare
    the model predictions using our validation set. Note that because of the multiple
    graph sequences we’re now predicting, this training loop takes more time than
    previous models. On a V100 GPU through Google Colab, this took under an hour to
    train.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.8 GAT training loop
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Initializes loss and optimizer with learning rate'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Generates the model’s predictions for the input'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Computes the loss between the outputs and the targets'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Validation loop'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Generates the model’s predictions for the input'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Computes the loss between the outputs and the targets'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we test our trained model using the test set and code shown in the
    following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.9 GAT test loop
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Generates the model’s predictions for the input'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Computes the loss between the outputs and the targets'
  prefs: []
  type: TYPE_NORMAL
- en: We find that this naive approach is unable to predict the poses. Our overall
    test accuracy is 55%, and the predicted graphs look very different from our expectation
    of the pose’s appearance. This is due to the large amount of data that we’re now
    holding in a single graph. We’re compressing both node features and temporal data
    into one graph, and we’re not emphasizing the temporal property when defining
    our model. There are ways to improve this, such as by using temporal encodings
    to extract the edge data that is unused, as in the temporal GAT (TGAT) model.
    TGAT treats edges as dynamic rather than static, such that each edge also encodes
    a timestamp.
  prefs: []
  type: TYPE_NORMAL
- en: However, without this time data, our model has become too expressive such that
    the overall structure of the pose has diverged significantly from the original
    structure, as shown with the predicted poses in figure 6.10\. Next, we’ll investigate
    how to combine the best of both approaches into a GNN that uses RNN-based predictions
    by learning on each graph snapshot.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 Output from the GAT model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 6.4 Neural relational inference
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our RNN focused entirely on the temporal data but ignored the underlying relational
    data. This resulted in a model that was able to move in the right direction on
    average but didn’t really alter the individual sensor positions very well. On
    the other hand, our GAT model ignored temporal data by encoding all individual
    temporal graphs into a single graph and attempting node prediction on the unknown
    future graphs. The model caused the sensors to move dramatically, and our resulting
    graphs looked very unlike how we would expect a human to move.
  prefs: []
  type: TYPE_NORMAL
- en: Neural relational inference (NRI), as mentioned earlier, is a slightly different
    approach that uses a more complex encoding framework to combine the best of both
    RNN and GNNs [2]. The architecture for this model is shown in figure 6.11\. Specifically,
    NRI uses an autoencoder structure to embed the information at each timestep. Therefore,
    the embedding architecture is applied to the entire graph in a similar way to
    GAE, which we discussed in chapter 5\. This encoded graph data is then updated
    using an RNN. One key point is that NRI evolves the latent representation of the
    embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11 Schematic for NRI (Source: Kipf et al. [2]). The model consists
    of an encoder and decoder layer and several message-passing steps. However, here
    the messages are passed in the encoder from node to edge, back from edge to node,
    and then back from node to edge again. For the decoder, messages are passed from
    node to edge and then from edge to node. The final step takes the latent representation
    and is used to predict the next step in the temporal evolution of the body.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s explore how this model applies to our problem of pose estimation so that
    we can best understand the different components in the model. We’ll use the same
    format of masking some data during training and then using the test day to identify
    these masked nodes. Recall that this is equivalent to inferring the future frames
    in our video. However, we now need to change both the model architecture and the
    loss. We need to change the model architecture to account for the new autoencoder
    structure, and we need to adjust the loss to include minimizing the reconstruction
    loss as well as the Kullbeck-Liebler divergence (KL divergence). For more information
    on the NRI model and relevant changes, see section 6.5 at the end of the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The code for the base class of an NRI model is provided in listing 6.10\. As
    is clear in the code, we need to define an encoder and decoder when calling this
    class. Along with the encoder and decoder, there are some other model-specific
    details we need to be aware of. First, we need to define the number of variables.
    This relates to the number nodes in our graph rather than the number of features
    for each node. In our case, this will be 31, corresponding to each of the different
    sensors tracking a joint position. We also need to define the different types
    of edges between the nodes. This will be either 1 or 0, representing whether an
    edge exists.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll assume that the way the nodes, or sensors, connect doesn’t change, that
    is, that the graph structure is static. Note that this model also allows for dynamic
    graphs where the connectivity changes over time, for example, when different players
    move around a basketball court. The total number of players is fixed but the number
    of players that can be passed to changes. In fact, this model was also used to
    predict how different players would pass using footage from the NBA.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this model needs some hyperparameters to be set, including the Gumbel
    temperature and the prior variance. *Gumbel temperature* controls the tradeoff
    between exploration and exploitation when performing discrete sampling. Here,
    we need to use a discrete probability distribution to predict the edge type. We
    discuss this in more detail in section 6.5\. *Prior variance* reflects how uncertain
    we are on the connectivity of the graph before we start. We need to set this because
    the model assumes we *don’t* know the connectivity. In fact, the model learns
    the connectivity that best helps it to improve its predictions. This is exactly
    what we’re setting when we call the `_initialize_log_prior` function. We’re telling
    the model what our best guess is for a likely connectivity pattern. For example,
    if we were to apply this model to a sports team, we might use a Gaussian distribution
    with a high mean for edges between players that frequently pass to each other
    or even to players on the same team.
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate our model, we’re instead going to assume a uniform prior, which
    means that all edges are as likely as all others, or in everyday terms “we don’t
    know.” The prior variance sets our uncertainty bound for each of the edges. In
    the following listing, we set it to be 5 × 10^(–5) for numerical stability, but
    given that our prior is uniform, it shouldn’t have much effect.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.10 Base class for the NRI model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Number of variables in the mode'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Encoder neural network'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Decoder neural network'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Gumbel temperature for sampling categorical variables'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Prior variance'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Fills the prior tensor with uniform probabilities'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Takes the log and adds two singleton dimensions'
  prefs: []
  type: TYPE_NORMAL
- en: As we discovered in chapter 5, VAEs have a two-component loss—the reconstruction
    error and the error in representing the distributional properties of the data—captured
    by the KL-divergence. The total loss function is given in listing 6.11.
  prefs: []
  type: TYPE_NORMAL
- en: Our encoder is passed edge embeddings and then outputs log probabilities of
    an edge type. The Gumbel-Softmax function converts these discrete logits into
    a differentiable continuous distribution. The decoder takes this distribution
    and the edge representations and then converts these back into node data. At this
    point, we’re ready to use the standard loss machinery for VAEs, so we calculate
    the reconstruction loss as MSE and the KL divergence. For further insight into
    VAE losses and how the KL divergence is calculated, revisit chapter 5\.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.11 Loss for the NRI model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Calculates Gumbel-Softmax using PyTorch''s functional API, imported as F
    in code'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Negative log likelihood (NLL) for Gaussian distribution'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Adds a small constant to avoid taking the logarithm of zero'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 KL divergence with a uniform categorical distribution'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need our model to be able to predict the future trajectories of
    the sensors. The code for predicting the future state of the graph is given in
    listing 6.12\. This is a relatively simple function once we have our encoder and
    decoder trained. We pass the encoder the current graph, and this returns a latent
    representation of whether an edge exists. We then convert these probabilities
    into a suitable distribution using Gumbel-Softmax and pass this to our decoder.
    The output from the decoder is our predictions. We can either get the predictions
    directly or get both predictions and whether an edge exists.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.12 Predicting the future
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Runs the encoder to get logits for edge types'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Applies Gumbel-Softmax to the edges'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Runs the decoder to get the initial predictions and decoder state'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Uses the last input and decoder state to predict future steps'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Concatenates initial and future predictions if needed'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Returns predictions and edges if specified'
  prefs: []
  type: TYPE_NORMAL
- en: This is the basis of the NRI model. We have an encoder that converts our initial
    node data into edge probabilities. The edge probabilities get passed to our decoder,
    and the decoder predicts future trajectories conditional on the most likely graph
    representation. Our encoder will be a simple multilayer perceptron (MLP) that
    works on graph data. Our decoder needs to be able to make future predictions,
    so we’ll use an RNN to do this, specifically the same GRU model we discussed in
    section 6.2.2\. Let’s next meet our encoder and decoder networks so we can apply
    our model to the data and see how it performs.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.1 Encoding pose data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we know the different parts of our NRI model, let’s define our encoder.
    This encoder will act as the bottleneck to make our problem simpler. After encoding,
    we’ll be left with a low-dimensional representation of the edge data, so we don’t
    need to worry about temporal data at this stage. However, by providing our temporal
    data together, we’re transferring temporal structure into our latent space. Specifically,
    the encoder takes the temporal patterns and relationships from the input data
    and preserves this in the compressed, low-dimensional representations. This makes
    it easier to decode from, making our pose prediction problem easier to solve.
  prefs: []
  type: TYPE_NORMAL
- en: There are several subsets to implementing the encoder. First, we pass the input
    data, which comprises the different sensors at different frames, across different
    experiments. The encoder then takes this data, *x*, and performs a message-passing
    step to transform edge data into node data and then back into edge data. The edge
    data is then converted to node data again before being encoded in the latent space.
    This is equivalent to three message-passing steps, from edges to nodes, edges
    to edges, and edges to nodes again. The repeated transformations are useful for
    information aggregation through repeated message passing and capturing high-order
    interactions in the graph. By repeatedly transforming between nodes and edges,
    the model becomes aware of both local and global structure information.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we’ve explored how to use message passing to convert node
    or edge features into complex representations of nodes or edges. These are at
    the core of all GNN methods. The NRI model is slightly different from the methods
    that we’ve explored before because messages are passed between nodes and edges,
    rather than node to node or edge to edge. To make explicit what these steps are
    doing, we’ll depart from PyG and code our model in plain PyTorch instead.
  prefs: []
  type: TYPE_NORMAL
- en: In listing 6.13, we show the base class for our encoder, which requires several
    key features. First, note that we haven’t described the actual neural network
    that will be used to encode the data. We’ll introduce this shortly. Instead, we
    have two message-passing functions, `edge2node` and `node2edge`, as well as an
    encoding function, `one_hot_recv`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.13 Encoder base class
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates a matrix representing edges between variables'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Finds the indices where edges exist'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Creates a one-hot representation for receiving edges'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Creates a parameter tensor for edge-to-node transformation'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Extracts sender and receiver embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Concatenates sender and receiver embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Multiplies edge embeddings with edge-to-node matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Normalizes the incoming embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: The first step in our encoder class is to build an adjacency matrix. Here, we
    assume that the graph is fully connected, such that all nodes are connected to
    all other nodes but not to themselves. The `node2edge` function takes node embedding
    data and identifies the direction that these messages have been sent. Figure 6.12
    shows an example of how we’re building the adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-12.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 Example of creating an adjacency matrix for a fully connected graph
    with three nodes. The matrix on the left represents a fully connected graph, the
    matrix in the middle represents the identity matrix, and the matrix on the right
    shows the final adjacency matrix after subtracting the identity matrix. This results
    in a graph where each node is connected to every other node with no self-loops.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The next function call then determines which nodes are sending or receiving
    data by returning two vectors that contain rows and columns for connected nodes.
    Recall that in an adjacency matrix, the rows represent receiving nodes and the
    columns represent sending nodes. The output is then
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: We can interpret this as saying that the node at row 0 sends data to nodes at
    columns 1 and 2, and so on. This allows us to extract edges between nodes. Once
    we construct our node embeddings, we then use the sending and receiving data to
    convert our node data to edges. This is the principle of the `node2edge` function.
  prefs: []
  type: TYPE_NORMAL
- en: The next function we need is how to build `edge2node` based on our `edge_ embeddings`.
    We first construct an `edge2node` matrix. Here, we’re using a one-hot encoding
    method that converts our receiving edges into a one-hot encoded representation.
    Specifically, we create a matrix where each row denotes whether that category
    (receiving node) exists. For our simple three-node case, the one-hot encoding
    method for the receiving edges is shown in figure 6.13\.
  prefs: []
  type: TYPE_NORMAL
- en: We then transpose this to switch rows and columns, so that the dimension will
    be (number of nodes, number of edges), and we convert it into a PyTorch parameter
    so that we can differentiate over it. Once we have our `edge2node` matrix, we
    multiple this by our edge embeddings. Our edge embeddings will be of shape (number
    of edges, embedding size) so that multiplying the `edge2node` matrix by the edge
    embeddings gives us an object of shape (number of nodes, embedding size). These
    are our new node embeddings! Finally, we normalize this matrix by the number of
    possible nodes for numerical stability.
  prefs: []
  type: TYPE_NORMAL
- en: This section is key to understanding the message-passing step in the model.
    (For further information on message passing, revisit chapter 2 and 3.) As discussed
    there, once we have a principled way to pass messages between nodes, edges, or
    some combination of both, we then apply neural networks to these embeddings to
    get nonlinear representations. To do so, we need to define our embedding architecture.
    The code for the complete encoder is given in listing 6.14\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-13.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 The one-hot encoding matrix representing incoming edges for each
    node in a fully connected graph with three nodes is shown on the left. Each row
    corresponds to an edge, and each column corresponds to a node. A 1 in position
    (i, j) indicates that edge i is directed toward node j. This matrix is used to
    transform edge embeddings to node embeddings in the `edge2node` function of the
    encoder base class, enabling the model to aggregate information from incoming
    edges for each node. In this graph structure, nodes 0, 1, and 2 each send messages
    to the other two nodes, resulting in a total of six directed edges. The diagram
    of the three-node graph is shown on the right.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The `RefMLPEncoder` is shown in listing 6.14\. This encoder uses four MLPs for
    message processing, each featuring exponential linear unit (ELU) activation and
    batch normalization (defined in `RefNRIMLP`, shown in the chapter’s code repository).
  prefs: []
  type: TYPE_NORMAL
- en: Note  The exponential linear unit (ELU) is an activation function that is useful
    in smoothing outputs across multiple layers and preventing vanishing gradients.
    In contrast to ReLUs, ELUs have a smoother gradient built in for negative inputs
    and allows for negative outputs.
  prefs: []
  type: TYPE_NORMAL
- en: The final part of the network (`self.fc_out`) is a sequence of linear layers
    with ELU activations between them, ending with a linear layer that outputs the
    desired embeddings or predictions. The final layer of this sequence is a fully
    connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.14 NRI MLP encoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Defines MLP layers. RefNRIMLP is a 2-layer fully connected ELU net with
    batch norm.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Defines the final fully connected layer'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we define architectural details related to the encoder. As discussed earlier,
    there are 31 sensors that we represent using the `num_vars` variable. The number
    of features is 6, which is the `input_size` for our network. The number of timesteps
    for our training and validation set is still 50, and our encoder network size
    will be 256\. The number of `edge_types` is 2, and we assume no dropout of the
    weights. We then initialize our networks, which are typical MLPs, described in
    our shared repository. The networks include a batch normalization layer and two
    fully connected layers. Once the network is defined, we also pre-initialize the
    weights, as shown in listing 6.15\. Here, we loop through all the different layers
    and then initialize the weights using the Xavier initialization approach. This
    ensures that the gradients in the layers are all approximately of similar scale,
    which reduces the risk of our loss rapidly diverging—known as blow-up. This is
    an important step when combining multiple networks with different architectures
    as we do here. We also set the initial bias to 0.1, which further helps with the
    stability of training.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.15 Weight initialization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Only applies to linear layers'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Initializes weights using Xavier normal initialization'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sets bias to 0.1'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we need to define our forward pass method, as shown in listing 6.16\.
    This is where our message-passing step occurs.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.16 Encoder forward pass
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '#1 New shape: [num_sims, num_atoms, num_timesteps*num_dims]'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Passes through first MLP layer (two-layer ELU network per node)'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Converts node embeddings to edge embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Passes through the second MLP layer'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Converts edge embeddings back to node embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Converts node embeddings to edge embeddings again'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Final fully connected layer to get the logits'
  prefs: []
  type: TYPE_NORMAL
- en: Our encoder lets our model transform different sets of frames of our sensor
    graphs into latent representation of edge probabilities. Next, let’s explore how
    to construct a decoder that transforms the latent edge probabilities into trajectory
    using the recent sensor data.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.2 Decoding pose data using a GRU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To transform the latent representations into future frames, we need to account
    for the temporal evolution of the trajectories. To do so, we train a decoder network.
    Here, we’ll follow the original structure of the NRI paper [2] and use a GRU as
    our RNN.
  prefs: []
  type: TYPE_NORMAL
- en: We introduced the concept of a GRU in section 6.2.2 earlier. As a quick reminder,
    gated recurrent units (GRUs) are a type of RNN that uses a gated process to allow
    RNNs to capture long-term behaviors in the data. They are composed of two types
    of gates—reset gates and update gates.
  prefs: []
  type: TYPE_NORMAL
- en: For the NRI model, we’ll apply GRUs to our edges, rather than across the entire
    graph. The update gates will be used to determine how much of the node’s hidden
    state should be updated, given the receiving data, and the reset gate decides
    how much should be erased or “forgotten.” To put it another way, we’ll use a GRU
    to predict what the future state of a node should be based on the edge type probabilities
    from our encoder network.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at how we construct this step-by-step. The initialization code for
    our decoder is given in listing 6.17\. First, we note some of the variables passed
    to this network. We again define the number of variables or nodes in our graphs,
    31, and the number of input features, 6\. We assume there is no dropout of the
    weights and the hidden size for each layer is 64\. Again, we need to make clear
    that our decoder should be predicting two different types of edges. We’ll also
    skip the first edge type when making predictions as this denotes that there is
    no edge.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the input parameters defined, we can introduce the network architecture.
    The first layer is a simple linear network that needs to have twice the input
    dimension to account for the mean and variance provided by our encoder, and we
    define this network for each of the edge types. We then define a second layer
    to further increase the expressivity of our network. The output from these two
    linear layers is passed to our RNN, which is a GRU. Here, we have to use a custom
    GRU to account for both node data and edge data. The output from the GRU is passed
    to three more neural network layers to provide the future predictions. Finally,
    we need to define our `edge2node` matrix and sending and receiving nodes, as we
    did with our encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.17 RNN decoder
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Edge-related layers'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 GRU layers'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Fully connected layers'
  prefs: []
  type: TYPE_NORMAL
- en: 'In listing 6.18, we provide the architecture for our GRU. The first overall
    architecture for this network is the same structure as a typical GRU. We define
    three hidden layers which represent the reset gates defined by `hidden_r` and
    `input_r`, the update gates defined by `hidden_i` and `input_i`, and the activation
    networks defined by `hidden_h` and `input_h`. The forward network, however, needs
    to account for the aggregated messages from the message-passing output of our
    encoder. This is shown in the forward pass. We’ll pass the edge probabilities
    in `agg_msgs`, along with the input node data, and these combine to return future
    predictions. This can be seen in the `predict_future` code in our base NRI class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Our decoder gets passed the last time frame of our graphs. The edge data that
    is output from our encoder is also passed to the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.18 Custom GRU network
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Defines hidden layer transformations for reset, input, and new gates'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Defines input layer transformations for reset, input, and new gates'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Computes reset gate activations'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Computes input gate activations'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Computes new gate activations'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Updates hidden state'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output from the decoder network is then the future prediction timesteps.
    To better understand this, let’s look at the forward pass method for our decoder,
    given in listing 6.19\. Our forward pass is given the inputs and sampled edges
    to build a prediction. There are also four additional arguments that help control
    the behavior. First, we define a `teacher_forcing` variable. Teaching forcing
    is a typical method used when training sequential models, such as RNNs. If this
    is true, we use the ground truth (the real graph) to predict the next time frame.
    When this is false, we use the output from the model’s previous timestep. This
    makes sure that the model isn’t led astray by incorrect predictions during training.
    Next, we include a `return_state` variable, which allows us to access the hidden
    representations given by the decoder network. We use this when we predict the
    future graph evolution, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Let’s now discuss the prediction process. First, we predict a temporary prediction
    set. Then, we use the hidden representations to predict as many steps in the future
    as is needed. This is particularly useful when we want to predict more than one
    timestep, as we show in the testing phase of this model. This is controlled by
    the `prediction_steps` variable, which tells us how many times to loop through
    our RNN, that is, how many timesteps in the future we want to predict. Finally,
    we have a `state` variable, which is used to control the information being passed
    to our decoder. When it’s left empty, we initialize a tensor of zeros so that
    there is no information being passed. Otherwise, we’ll use information from previous
    timesteps.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.19 Decoder forward pass
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Determines the number of prediction steps'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Expands the sampled_edges tensor if needed'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Initializes the hidden state if not provided'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Determines the number of steps to apply teacher forcing to'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Decides the input for this step based on teacher forcing'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Performs a single forward step using the ins calculated from inputs or pred_all
    (see the previous comment)'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Returns predictions and the hidden state'
  prefs: []
  type: TYPE_NORMAL
- en: To predict timesteps into the future, we make an additional forward pass that
    is based on a single timestep, as defined in listing 6.20\. This is where our
    network performs additional message-passing steps. We take our receiver nodes
    and sending nodes, which are defined from the edge probabilities from our encoder.
    We ignore the first edges, as these are unconnected nodes, and the network then
    loops through the different networks for the different edge types to get all edge-dependent
    messages from the network. This is the critical step that makes our predictions
    dependent on the graph data. Our GRU then takes the messages from the connected
    node to inform its predictions of the trajectories. At this step, we’re learning
    to predict how the body is walking from what we’ve learned about how the body
    is connected. The output is both the predicted trajectories of the sensors on
    the body as well as the network data for why it made these predictions, encoded
    in the hidden weights. This completes the NRI model for estimating poses.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.20 Decoder single step forward
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Node-to-edge step'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Message of size: [batch, num_edges, 2*msg_out]'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Runs a separate MLP for every edge type'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Sums all the messages per node'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 GRU-style gated aggregation'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Builds output MLP'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.3 Training the NRI model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we’ve defined the different parts of our model, let’s train the model
    and see how it performs. To train our model, we’ll take the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Train an encoder that converts sensor data into a representation of edge probabilities,
    indicating whether a sensor is connected to another or not.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a decoder to predict future trajectories, conditional on the probability
    of there being an edge connecting the different sensors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the decoder to predict the future trajectories using a GRU, which is passed
    the edge probabilities.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Reduce the loss based on the reconstructed poses. This loss has two components:
    the reconstruction loss and the KL divergence.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1 through 4 until training converges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is also shown in figure 6.14, and the training loop is given in listing
    6.21.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-14.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 Pipeline for the NRI model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Listing 6.21 NRI training loop
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Training loop'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Update the weights.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Zero gradients for the validation pass'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll train for 50 epochs with a learning rate of 0.0005, a learning rate scheduler
    that reduces the learning rate by a factor of 0.5 after 500 forward passes, and
    a batch size of 8\. Most of the training is based on the `calculate_loss` method
    call, which we defined earlier in listing 6.14\. We find that our model loss falls
    along with the validation loss, reaching a validation loss of 1.21 based on the
    negative log likelihood (`nll`). This looks good but let’s see how it performs
    on the test data, where it needs to predict multiple steps into the future. To
    do so, we need to define a new function, given in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.22 Evaluating future predictions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This function loads our test data and then calculates the MSE for our predictions
    given different time horizons. When we test our model, we find that it’s able
    to predict the next timestep with an MSE of 0.00008\. Even better, it predicts
    40 timesteps into the future with an accuracy of 94%. This is significantly better
    than our LSTM and GAT models, which achieved 65% and 55%, respectively. The reduction
    in accuracy over future timesteps is shown in figure 6.15, and the example output
    is given in figure 6.16.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-15.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 Reduction in accuracy as we predict into the future
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![figure](../Images/6-16.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 Example output from the NRI model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We’ve covered all the core components for the NRI model, with the full working
    code provided in the GitHub repository ([https://mng.bz/4a8D](https://mng.bz/4a8D)).
    The accuracy is impressive and highlights the power of combining generative and
    graph-based methods with temporal models. This is shown in figure 6.15, where
    we see good agreement with the predicted pose and the resulting estimated pose.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, this method is robust at not just predicting graphs but also learning
    the underlying structure even when all the graph data isn’t available. In this
    problem, we knew what interaction network to expect. However, there are many instances
    where we don’t know the interaction network. One example is particles that are
    moving in a confined space. When they are within some interaction radius, then
    they will influence each other, but not when they are farther away. This is true
    of organisms from cells to sports players. In fact, the majority of the world
    involves interacting agents with secret interaction networks. NRI models provide
    a tool to not only predict the behavior and movement of these agents but also
    learn about their interaction patterns with other agents. Indeed, the original
    NRI paper demonstrated this using video tracking data of basketball games and
    showed that the model can learn typical patterns between ball, ball handler, screener,
    and defensive matchups for the different players. (For more information, refer
    to Kipf et al. [2].)
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Under the hood
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we showed how to tackle temporal or dynamic problems. Here,
    we go into more detail for some of the key model components that we used.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.1 Recurrent neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In figure 6.16, we showed a schematic for RNN models. The main difference for
    RNN models compared to all the other models that we’ve seen is that the model
    can cope with sequential data. This means that each timestep has a hidden layer,
    and output from this hidden layer is combined with new input at subsequent timesteps.
    In figure 6.17, this is shown in two ways. First, on the left side, we show the
    temporal updates as a single self-loop denoted by Whh. To get a better understanding
    of what this self-loop is doing, we’ve “unfolded” the model in time so that we
    can explicitly see how our model updates. Here, we change our input, output, and
    hidden layers (x, y, h) to be temporal variables (xt, yt, ht). At our initial
    step, t, we update our current hidden layer with input data from xt and the weights
    from our previous hidden layer ht–1 and then use this to output yt. The weights
    from ht are then passed to ht+1 along with the new input at xt+1 to infer yt+1\.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key features for this model is that when we backpropagate to update
    our weights, we need to backpropagate through time (BPTT). This is a specific
    feature for all RNNs. However, most modern deep learning packages make this very
    straightforward to do and hide all the difficult computational details for the
    practitioner.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-17.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 Structure for an RNN. Temporal updates as a single self-loop denoted
    by Whh (left). An unfolded model in time showing the model updates (right). Here,
    we change our input, output, and hidden layers (x, y, h) to be temporal variables
    (x[t], y[t], h[t]). At our initial step, t, we update our current hidden layer
    with input data from x[t] and the weights from our previous hidden layer h[t–1]
    and then use this to output y[t]. The weights from ht are then passed to h[t+1]
    along with the new input at x[t+1] to infer y[t+1].
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Let’s see how to implement an RNN using PyTorch. This is as straightforward
    as defining a neural network class and then introducing specific RNN layers within
    the network. For example, in listing 6.23, we show the code for defining a network
    with a single RNN layer. This is a very basic definition of an RNN, given there
    is only one hidden layer. However, it’s useful to see this example to get some
    solid intuition on how a model can be trained. For each timestep, our input is
    passed both to the hidden layer and the output. When we perform a forward pass,
    the output goes back to output and the hidden layer. Finally, we need to initialize
    our hidden layer with something, so we’re using a fully connected layer.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.23 Defining an RNN
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '#1 RNN layer'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Fully connected layer'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sets the initial hidden and cell states'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Forward propagates the RNN'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Passes the output of the last timestep to the fully connected layer'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we often want to use more complicated RNNs. This includes extensions
    to RNNs such as LSTM networks or GRU networks. We can even stack RNNs, LSTMs,
    and GRUs together using our deep learning library of choice. A GRU is similar
    to an RNN in that it’s useful for sequences of data. They were specifically designed
    to resolve one of the key drawbacks of RNNs, the vanishing gradient problem. It
    uses two gates, which determine both how much past information to keep (the update
    gates) and how much to forget or throw away (the reset gates). We show an example
    design for a GRU in figure 6.18\. Here, *z**[t]* denotes the update gates, and
    r*[t]* denotes the reset gates. The *~h**[t]* term is known as the candidate activation
    and reflects a candidate for the new state of the representations, while the *h**[t]*
    term is the actual hidden state.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-18.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 Design of the GRU layer, where r*[t]* represents the reset gate,
    z*[t]* is the update gate, ~h*[t]* is the candidate function, and h*[t]* is the
    final actual hidden state
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In listing 6.24, we show how to build a model with GRU layers. Here, the majority
    of the implementation is handled by PyTorch, where the layer is imported from
    the standard PyTorch library. The rest of the model definition is a typical neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.24 GRU
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '#1 GRU layer'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Fully connected layer'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sets the initial hidden state'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Forward propagates the GRU'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Passes the output of the last timestep to the fully connected layer'
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.2 Temporal adjacency matrices
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When considering temporal graphs, we might start with two nodes connected by
    one edge, then at each subsequent timestep, another few nodes and/or edges are
    added. This results in several distinct graphs, each with a differently sized
    adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: This might present a difficulty when designing our GNN. First, we have different
    sized graphs at each timestep. This means we won’t be able to use node embeddings
    because the number of nodes will keep changing across input data. One method is
    to use graph embeddings at each timestep to store the entire graph as a low-dimensional
    representation. This method is at the heart of many temporal approaches, where
    graph embeddings are evolved in time rather than the actual graph. We can even
    use more complex transformations on our graph, such as using an autoencoder model
    as in our NRI model.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can transform all the individual graphs at each timestep into
    one single larger graph by creating a temporal adjacency matrix. This involves
    wrapping each timestep into a single graph that spans both per-timestep data as
    well as dynamic temporal data. Temporal adjacency matrices can be useful if a
    graph is small and we’re only interested in a few timesteps in the future. However,
    they can often become very large and difficult to work with. On the other hand,
    using temporal embedding methods can often involve multiple complicated subcomponents
    and become difficult to train. Unfortunately, there is no one-size-fits-all temporal
    graph, and the best approach is almost always problem specific.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.3 Combining autoencoders with RNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, to build intuition around the NRI model, we’ll summarize its
    components and illustrate its application in predicting graph structures and node
    trajectories. To start, in figure 6.19, we repeat the schematic for the NRI model.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/6-19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.19 Schematic for NRI (Source: Kipf et al. [2]). The model consists
    of an encoder and decoder layer and several message-passing steps. However, here
    the messages are passed in the encoder from node to edge, back from edge to node,
    and then back from node to edge again. For the decoder, messages are passed from
    node to edge and then from edge to node. The final step takes the latent representation
    and is used to predict the next step in the temporal evolution of the body.'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In this model, there are two key components. First, we train an encoder to encode
    the graphs from each frame into the latent space. Explicitly, we use the encoder
    to predict the probability distribution, q[j](z|x) over the latent interactions
    (z), given the initial graphs (x). Once we’ve trained the encoder, we then use
    the decoder to convert samples from this probability distribution into trajectories
    using the latent encoding as well as previous timesteps. In practice, we use the
    encoder-decoder structure to infer the trajectories of nodes with different interaction
    types (or edges).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ve only considered two edge types: where there is or isn’t
    a physical connection between sensors. However, this method can be scaled to consider
    many different connections, all changing with time. Additionally, the decoder
    model needs an RNN to effectively capture the temporal data in our graph. To build
    some intuition around the NRI model, let’s repeat the process once more.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Input *—Node data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Encoding *—'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The encoder receives the node data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The encoder converts the node data into edge data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The encoder represents the edge data in a latent space.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Latent space *—The latent space represents probabilities of different edge
    types. Here, we have two edge types (connected and not connected), though multiple
    edge types are possible for more complex relationships. We always need to include
    at least two types as otherwise the model would assume all the nodes are connected
    or, worse, none of them are.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Decoding *—'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The decoder takes the edge type probabilities from the latent space.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The decoder learns to reconstruct the future graph state based on these probabilities.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Prediction *—The model predicts future trajectories by learning to predict
    graph connectivity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that this model gives us graph and trajectory predictions simultaneously!
    While this might not be helpful for our problem, for cases where we don’t know
    the underlying graph structure such as social media networks or sports teams,
    this can provide ways to discover new interaction patterns in a system.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.4 Gumbel-Softmax
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the NRI model, there is an additional step before calculating both of these
    losses, which is calculating the probability of an edge using Gumbel-Softmax.
    The key reason we need to introduce Gumbel-Softmax is that our autoencoder is
    learning to predict the adjacency matrix representing our edges, that is, the
    network connectivity, rather than the nodes and their features. Therefore, the
    end predictions for the autoencoder have to be discrete. However, we’re also inferring
    a probability. Gumbel-Softmax is a popular approach whenever probability data
    needs to be made discrete.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have two discrete types of edges, that is, whether something is or
    isn’t connected. This means that our data is *categorical *—each edge is either
    in category 0 (isn’t connected) or category 1 (connected). Gumbel-Softmax is used
    to draw and score samples from a categorical distribution. In practice, Gumbel-Softmax
    will approximate the output from our encoder, which comes in the form of log probabilities
    or *logits*, as a Gumbel distribution, which is an extreme value distribution.
    This approximates the continuous distribution of our data as a discrete one (edge
    types) and allows us to then apply a loss function to the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The temperature of a Gumbel distribution, one of our hyperparameters, reflects
    the “sharpness” of the distribution, similar to how variance controls the sharpness
    of a Gaussian distribution. In this chapter, we used a temperature of 0.5, which
    is about medium sharpness. We also specify `Hard` as a hyperparameter, which denotes
    whether one or more categories exist. As discussed, we want it to have two categories
    when training to represent whether an edge exists. This allows us to approximate
    the distribution as a continuous one, and then we can backpropagate this through
    our network as a loss. However, when testing, we can set `Hard` to `True`, which
    means that there is only one category. This makes the distribution fully discrete,
    meaning we can’t optimize using the loss, as discrete variables are nondifferentiable
    by definition. This is a useful control to make sure that our test loop doesn’t
    propagate any gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While some systems can use single snapshots of data to make predictions, others
    need to consider changes in time to avoid errors or vulnerabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spatiotemporal GNNs consider previous timesteps to model how graphs evolve over
    time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spatiotemporal GNNs can solve pose-estimation problems where we predict the
    next position of the body given some data on how the body position was in the
    recent past. In this case, nodes represent sensors placed on body joints, and
    edges represent the body connections between joints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adjacency matrices can be adapted to consider temporal information by concatenating
    different adjacency matrices along the diagonal.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory can be introduced into models, including GNNs, such as by using a recurrent
    neural network (RNN) or a gated recurrent unit network (GRU).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The neural relational inference (NRI) model combines recurrent networks such
    as a GRU with autoencoder GNNs. These models can infer temporal patterns, even
    where adjacency information is unknown.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
