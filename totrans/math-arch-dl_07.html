<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="ch-training-neural-networks">8 Training neural networks: Forward propagation and backpropagation</h1>
<p class="co-summary-head">This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">Sigmoid functions as differential surrogates for Heaviside step functions</li>
<li class="co-summary-bullet">Layering in neural networks: expressing linear layers as matrix-vector multiplication</li>
<li class="co-summary-bullet">Regression loss, forward and backward propagation, and their math</li>
</ul>
<p class="body"><a id="marker-272"/>So far, we have seen that neural networks make complicated real-life decisions by modeling the decision-making process with mathematical functions. These functions can become arbitrarily involved, but fortunately, we have a simple building block called a <i class="fm-italics">perceptron</i> that can be repeated systematically to model any arbitrary function. We need not even explicitly know the function being modeled in closed form. All we need is a reasonably sized set of sample inputs and corresponding correct outputs. This collection of input and output pairs is known as <i class="fm-italics">training data</i>. Armed with this training data, we can <i class="fm-italics">train</i> a multilayer perceptron (MLP, aka neural network) to emit reasonably correct outputs on inputs it has never seen before.</p>
<p class="body">Such neural networks, where we need to know the output for each input in the training data set, are known as <i class="fm-italics">supervised</i> neural networks. The correct output for the training inputs is typically generated via a manual process called <i class="fm-italics">labeling</i>. Labeling is expensive and time-consuming. Much research is going on toward unsupervised, semi-supervised, and self-supervised networks, eliminating or minimizing the labeling process. But as of now, the accuracy of unsupervised and self-supervised networks in general does not match that of supervised networks. In this chapter, we focus on supervised neural networks. In chapter <a class="url" href="../Text/14.xhtml#ch-ae-vae">14</a>, we will study unsupervised networks.</p>
<p class="body">What is this process of ‚Äútraining‚Äù a neural network? It essentially estimates the parameter values that would make the network emit output values as close as possible to the known correct outputs on the training inputs. In this chapter, we discuss how this is done. But before that, we have to learn a few other things.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> The complete PyTorch code for this chapter is available at <a class="url" href="http://mng.bz/YAXa">http://mng.bz/YAXa</a> in the form of fully functional and executable Jupyter notebooks.</p>
<h2 class="fm-head" id="sec-sigmoid-etc">8.1 Differentiable step-like functions</h2>
<p class="body"><a id="marker-273"/>In equation <a class="url" href="../Text/07.xhtml#eq-perceptron">7.3</a>, we expressed the perceptron as a combination of a Heaviside step function <i class="timesitalic">œï</i> and an affine transformation <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> + <i class="fm-italics">b</i></span>. This is the perceptron we used throughout chapter <a class="url" href="../Text/07.xhtml#ch-func-approx">7</a> and with which we were able to express (model) pretty much all functions of interest.</p>
<p class="body">Despite its expressive power, the Heaviside step function has a drawback: it has a discontinuity at <span class="math"><i class="fm-italics">x</i> = 0</span> and is <i class="fm-italics">not differentiable</i>. Why is differentiability important? As we shall see in chapter <a class="url" href="../Text/08.xhtml#ch-training-neural-networks">8</a> (and got a glimpse of in section <a class="url" href="../Text/03.xhtml#sec-grad">3.3</a>), optimal training of a neural network requires evaluation of the gradient vector of a loss function with respect to weights. Since the gradient is nothing but a vector of partial derivatives, differentiability is needed for training.</p>
<p class="body">In this section, we discuss a few functions that are differentiable and yet can mimic the Heaviside step function. The most significant among them is the sigmoid function.</p>
<h3 class="fm-head1" id="sec-sigmoid">8.1.1 Sigmoid function</h3>
<p class="body">The sigmoid function is named after its characteristic S-shaped curve figure <a class="url" href="../Text/08.xhtml#fig-sigmoid1d">8.1</a>). The corresponding equation is</p><!--<p class="Body"><span class="times">$$\sigma\left(x\right) = \frac{1}{1 + e^{-x}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="43" src="../../OEBPS/Images/eq_08-01.png" width="116"/></p>
</div>
<p class="fm-equation-caption">Equation 8.1 <span class="calibre" id="eq-sigmoid"/></p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="431" id="fig-sigmoid1d" src="../../OEBPS/Images/CH08_F01_Chaudhury.png" width="890"/></p>
<p class="figurecaption">Figure 8.1 Graph of a <span class="math">1</span>D sigmoid function</p>
</div>
<p class="fm-head2" id="parameterized-sigmoid-function">Parameterized sigmoid function</p>
<p class="body">We can parametrize equation <a class="url" href="../Text/01.xhtml#eq-sigmoid">1.5</a> as</p><!--<p class="Body"><span class="times">$$\sigma\left(x\right) = \frac{1}{1 + e^{-\left(wx+b\right)}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_08-02.png" width="164"/></p>
</div>
<p class="fm-equation-caption">Equation 8.2 <span class="calibre" id="eq-sigmoid-parameterized"/></p>
<p class="body">This allows us to</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Adjust the steepness of the linear portion of the S curve by changing <i class="timesitalic">w</i></p>
</li>
<li class="fm-list-bullet">
<p class="list">Adjust the position of the curve by changing <i class="timesitalic">b</i></p>
</li>
</ul>
<p class="body"><a id="marker-274"/>Figure <a class="url" href="#fig-sigmoid-parameterized">8.2</a> shows how the parametrized sigmoid curve changes with different values for the parameters <i class="timesitalic">w</i> and <i class="timesitalic">b</i>. In particular, note that for large values of <i class="timesitalic">w</i>, the parameterized sigmoid is virtually indistinguishable from the Heaviside step function (compare the dotted curve in figure <a class="url" href="#fig-sigmoid-parameterized-various-ws">8.2a</a> with figure <a class="url" href="../Text/07.xhtml#fig-step-1D">7.7</a>), even though it remains differentiable. This is exactly what we desire in neural networks.</p>
<div class="figure" id="fig-sigmoid-parameterized">
<p class="figure1" id="fig-sigmoid-parameterized-various-ws"><img alt="" class="calibre8" height="416" src="../../OEBPS/Images/CH08_F02_Chaudhury.png" width="1023"/></p>
<p class="figurecaption" id="fig-sigmoid-parameterized-various-bs">Figure 8.2 Sigmoid curves corresponding to various parameter values in equation <a class="url" href="../Text/08.xhtml#eq-sigmoid-parameterized">8.2</a></p>
</div>
<p class="fm-head2" id="some-properties-of-the-sigmoid-function">Some properties of the sigmoid function</p>
<p class="body">The sigmoid function has several interesting properties, some of which are listed here with proof outlines.</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Expression with positive x</i>:</p>
</li>
</ul><!--<p class="body-ind"><span class="times">$$\sigma\left(x\right) = \frac{e^{x}}{1 + e^x}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="48" src="../../OEBPS/Images/eq_08-03.png" width="111"/></p>
</div>
<p class="fm-equation-caption">Equation 8.3</p>
<p class="body-ind">This expression can be easily proved by multiplying both the numerator and denominator of equation <a class="url" href="../Text/01.xhtml#eq-sigmoid">1.5</a> by <i class="timesitalic">e<sub class="fm-subscript">x</sub></i>.</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Sigmoid of negative x</i>:</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\sigma\left(-x\right) = \frac{1}{1 + e^{x}} =
\frac{e^{-x}}{1 + e^{-x}} = 1 - \frac{1}{1 + e^{-x}} = 1 -
\sigma\left(x\right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="48" src="../../OEBPS/Images/eq_08-04.png" width="379"/></p>
</div>
<p class="fm-equation-caption">Equation 8.4</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Derivative of sigmoid</i>:</p>
</li>
</ul><!--<p class="FM-Equation"><span class="times">$$\begin{aligned}
\begin{split}
&amp;\frac{d\sigma\left(x\right)}{dx}
= \frac{d}{dx}\left( \left( 1 + e^{-x} \right)^{-1} \right) =
\left( \frac{-1}{\left( 1 + e^{-x} \right)^{2} } \right)
\frac{d}{dx}\left(  \left( 1 + e^{-x} \right) \right) \\[6pt]
&amp;\quad = \left( \frac{-1}{\left( 1 + e^{-x} \right)^{2} } \right)
\left( -e^{-x} \right)
= \left( \frac{1}{1 + e^{-x}} \right) \left(  \frac{e^{-x}}{1 + e^{-x}}
\right) = \sigma\left(x\right)\left( 1 - \sigma\left(x\right) \right)
\end{split}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="64" src="../../OEBPS/Images/eq_08-05.png" width="902"/></p>
</div>
<p class="fm-equation-caption">Equation 8.5 <span class="calibre" id="eq-sigmoid-derivative"/></p>
<p class="body-ind"><a id="marker-275"/>Figure <a class="url" href="#fig-sigmoid1d-with-derivative">8.3</a> shows the graph of the derivative of the sigmoid superimposed on the sigmoid graph itself. As expected, the derivative has its maximum value at the middle of the sigmoid curve (where the sigmoid is climbing more or less linearly) and is near zero at both ends (where the sigmoid is saturated and flat, hardly changing).</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="709" id="fig-sigmoid1d-with-derivative" src="../../OEBPS/Images/CH08_F03_Chaudhury.png" width="900"/></p>
<p class="figurecaption">Figure 8.3 Graph of a <span class="math">1</span>D sigmoid function (solid curve) and its derivative (dashed curve)</p>
</div>
<h3 class="fm-head1" id="sec-tanh">8.1.2 Tanh function</h3>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="425" id="fig-tanh1d" src="../../OEBPS/Images/CH08_F04_Chaudhury.png" width="891"/></p>
<p class="figurecaption">Figure 8.4 Graph of a <span class="math">1</span>D tanh function.</p>
</div>
<p class="body">An alternative to the sigmoid function is the <i class="fm-italics">hyperbolic tangent</i> tanh function, shown in figure <a class="url" href="#fig-tanh1d">8.4</a>. It is very similar to the sigmoid function, but the range of output values is from <span class="math">[‚àí1,1]</span> as opposed to <span class="math">[0,1]</span>. In essence, it is the sigmoid function stretched and shifted so it is centered around 0. The equation of the tanh function is given by</p><!--<p class="Body"><span class="times">$$tanh\left(x\right) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="46" src="../../OEBPS/Images/eq_08-06.png" width="148"/></p>
</div>
<p class="fm-equation-caption">Equation 8.6</p>
<p class="body"><a id="marker-276"/>Why is tanh preferred over sigmoid? To understand this, consider figure <a class="url" href="#fig-derivative-sigmoid-tanh-1d">8.5</a>. It compares the derivatives of the sigmoid and tanh functions. As the plot shows, the derivative (gradient) of the function near <span class="math"><i class="fm-italics">x</i> = 0</span> is much higher for tanh than for sigmoid. Stronger gradients mean faster convergence, as the weight updates happen in larger steps. Note that this holds mainly when the data is centered around 0: in most preprocessing steps, we standardize the data (make it 0 mean) before feeding it into the neural network.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre18" height="431" id="fig-derivative-sigmoid-tanh-1d" src="../../OEBPS/Images/CH08_F05_Chaudhury.png" width="890"/></p>
<p class="figurecaption">Figure 8.5 Graph of the derivatives of <span class="math">1</span>D sigmoid and tanh functions</p>
</div>
<h2 class="fm-head" id="sec-linlayer-forwardprop">8.2 Why layering?</h2>
<p class="body">In section <a class="url" href="../Text/07.xhtml#sec-layering">7.5</a>, we encountered the idea of <i class="fm-italics">layering</i> as the preferred way to organize multiple perceptrons. The main property of a layered network is that neurons in any layer take their input only from the outputs of the preceding layer. This means connections exist only between successive layers. No other connection exists in the MLP, which greatly simplifies the evaluation and training of the network, which will become apparent as we discuss forward propagation and backpropagation.</p>
<p class="body">Why have layers at all? We have seen that multiple perceptrons allow us to model problems that cannot be solved by a single perceptron (such as the XOR problem discussed in section <a class="url" href="../Text/07.xhtml#sec-mlp-xor">7.4.1</a>). In theory, it is possible to model all mathematical functions (and hence solve all quantifiable problems) with neurons organized in a single hidden layer (see Cybenko‚Äôs theorem and proof in section <a class="url" href="../Text/07.xhtml#sec-cybenko-uat">7.5.3</a>). However, that does not mean a single hidden layer is the most <i class="fm-italics">efficient</i> way of doing all modelings. We can often model complicated problems with <i class="fm-italics">fewer</i> perceptrons if we organize them in more than one layer.</p>
<p class="body">Why do extra layers help? The primary reason is the extra nonlinearity. Each layer brings in its own nonlinear (such as sigmoid) function. Nonlinear functions, with proper parametrization, can model more complicated functions. Hence, a larger count of nonlinear functions in the model typically implies greater expressive power.</p>
<h2 class="fm-head" id="sec-linear-layer">8.3 Linear layers</h2>
<p class="body"><a id="marker-277"/>Various types of layers are used in popular neural network architectures. In subsequent chapters, we shall look at different kinds of layers, such as convolution layers. But in this section, we examine the simplest and most basic type of layer: the <i class="fm-italics">linear</i> layer. Here every perceptron from the previous layer is connected to every perceptron in the next layer. Such a layer is also known as <i class="fm-italics">fully connected</i> layer. Thus if the previous layer has <i class="timesitalic">m</i> neurons and the next layer has <i class="timesitalic">n</i> neurons, there are <i class="timesitalic">mn</i> connections, each with its own weight.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> We use the words <i class="fm-italics">neuron</i> and <i class="fm-italics">perceptron</i> interchangeably.</p>
<p class="body">Figure <a class="url" href="#fig-linear-layer">8.6</a> shows a linear layer that is a slice of a bigger MLP. Figure <a class="url" href="#fig-full-MLP-with-linear-layer">8.7</a> shows a bigger MLP with a linear layer. Consistent with previous chapters, we have used superscripts for layer IDs and subscripts for source and destination IDs.</p>
<p class="body">The weight of the connection from the <i class="timesitalic">k</i>th neuron in layer <span class="math">(<i class="fm-italics">l</i> ‚àí 1)</span> to the <i class="timesitalic">j</i>th neuron in layer <i class="timesitalic">l</i> is denoted <span class="math"><i class="fm-italics">w<sub class="fm-subscript">jk</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>. Here the subscript ordering is the destination (<i class="timesitalic">j</i>) followed by the source (<i class="timesitalic">k</i>). This is slightly counterintuitive but universally followed because it simplifies the matrix notation (described shortly). Note the following:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">We have split a single perceptron (weighted sum followed by sigmoid) into two separate layers, weighted sum and sigmoid.</p>
</li>
<li class="fm-list-bullet">
<p class="list">We have used sigmoid instead of Heaviside as the nonlinear function.</p>
</li>
</ul>
<h3 class="fm-head1" id="sec-linlayer-matmult">8.3.1 Linear layers expressed as matrix-vector multiplication</h3>
<p class="body">Let‚Äôs revisit the perceptron in the context of the MLP. As we saw in equation <a class="url" href="../Text/07.xhtml#eq-perceptron">7.3</a>, a single perceptron takes a weighted sum of its inputs and then performs a step function on the result. In an MLP, the inputs to any perceptron in the <i class="timesitalic">l</i>th layer come from the previous layer: the <span class="math">(<i class="fm-italics">l</i> ‚àí 1)</span>th layer.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre17" height="723" id="fig-linear-layer" src="../../OEBPS/Images/CH08_F06_Chaudhury.png" width="792"/></p>
<p class="figurecaption">Figure 8.6 Linear layer outputting layer <i class="timesitalic">l</i> from layer <span class="math">(<i class="fm-italics">l</i> ‚àí 1)</span>. The weights belonging to row <span class="math">1</span> of the weight matrix (coming from all the input neurons, layer <span class="math">(<i class="fm-italics">l</i> ‚àí 1)</span>, which sum together to form output neuron <span class="math">1</span>) are shown in bold.<a id="marker-278"/></p>
</div>
<div class="figure">
<p class="figure1"><img alt="" class="calibre30" height="539" id="fig-full-MLP-with-linear-layer" src="../../OEBPS/Images/CH08_F07_Chaudhury.png" width="1023"/></p>
<p class="figurecaption">Figure 8.7 Multilayered neural networks: This is a complete deep neural network, a slice of which is shown in figure <a class="url" href="#fig-linear-layer">8.6</a>.</p>
</div>
<p class="body">Let <span class="math"><i class="fm-italics">a</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">(<i class="fm-italics1">l</i> ‚àí 1)</sup></span>, <span class="math"><i class="fm-italics">a</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">(<i class="fm-italics1">l</i> ‚àí 1)</sup></span>, <span class="math">‚ãØ</span>, <span class="math"><i class="fm-italics">a<sub class="fm-subscript">m</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i> ‚àí 1)</sup></span> denote the outputs of the <i class="timesitalic">m</i> neurons in layer <span class="math">(<i class="fm-italics">l</i> ‚àí 1)</span> (the left-most input column of nodes in figure <a class="url" href="#fig-linear-layer">8.6</a>). And let <span class="math"><i class="fm-italics">a</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>, <span class="math"><i class="fm-italics">a</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>, <span class="math">‚ãØ</span>, <span class="math"><i class="fm-italics">a<sub class="fm-subscript">n</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span> denote the outputs of the <i class="timesitalic">n</i> neurons in layer <i class="timesitalic">l</i>. Note that we typically use the symbol <i class="timesitalic">a</i>, standing for activation, to denote the output of individual neurons. Now consider the <i class="timesitalic">j</i>th neuron in layer <i class="timesitalic">l</i>. For instance, check <span class="math"><i class="fm-italics">z</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span> in figure <a class="url" href="#fig-linear-layer">8.6</a>: note the weights going into it and the activations at their source. Its output is <span class="math"><i class="fm-italics">a<sub class="fm-subscript">j</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>, where<a id="marker-279"/></p><!--<p class="Body"><span class="times">$$\begin{rcases}
\begin{aligned} z^{\left(l\right)}_{j} &amp;= \sum_{k=0}^{m} w^{\left(l\right)}_{jk} a^{\left(l - 1\right)}_{k} + b^{\left( l \right)}_{j} \nonumber\\[6pt] a^{\left(l\right)}_{j} &amp;= \sigma\left( z^{\left(l\right)}_{j}
\right)
\end{aligned}
\end{rcases}\text{for } j = 0 \cdots n$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="114" src="../../OEBPS/Images/eq_08-06-a.png" width="305"/></p>
</div>
<p class="body">We can rewrite the summation in these equations as a dot product between the weight and activation vectors:</p><!--<p class="Body"><span class="times">$$\begin{rcases}
\begin{aligned}
&amp;z^{\left(l\right)}_{j} =
\begin{bmatrix}
&amp;w^{\left(l\right)}_{j0} &amp;w^{\left(l\right)}_{j1} &amp;\cdots
&amp;w^{\left(l\right)}_{jm}
\end{bmatrix}
\begin{bmatrix} a^{\left(l - 1\right)}_{0} \\[2pt] a^{\left(l - 1\right)}_{1} \\[2pt]
\cdots \\[2pt] a^{\left(l - 1\right)}_{m}
\end{bmatrix}
+ b^{\left( l \right)}_{j}
\nonumber\\[2pt]
&amp;a^{\left(l\right)}_{j} = \sigma\left( z^{\left(l\right)}_{j}
\right)
\end{aligned}
\end{rcases}\text{for } j = 0 \cdots n$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="194" src="../../OEBPS/Images/eq_08-06-b.png" width="468"/></p>
</div>
<p class="body">The complete set of equations for all <i class="timesitalic">j</i>s together can be written in a super-compact way using matrix-vector multiplication,</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\vec{z}^{\left(l\right)} = W^{\left(l\right)}  \vec{a}^{\left(l-1\right)} +
\vec{b}^{\left(l\right)} \nonumber\\
&amp; \vec{a}^{\left(l\right)} = \sigma\left( \vec{z}^{\left(l\right)}
\right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="73" src="../../OEBPS/Images/eq_08-07.png" width="173"/></p>
</div>
<p class="fm-equation-caption">Equation 8.7 <span class="calibre" id="eq-linlayer-forwardprop"/></p>
<p class="body">where</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><span class="math"><i class="fm-italics">W</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span> is an <span class="math"><i class="fm-italics">n</i> √ó <i class="fm-italics">m</i></span> matrix representing the weights of <i class="fm-italics">all connections from layer <span class="math"><i class="fm-italics">l</i> ‚àí 1</span> to layer l</i>:</p>
</li>
</ul><!--<p class="Body"><span class="times">$$W^{\left(l\right)} =
\begin{bmatrix}
&amp;w^{\left(l\right)}_{00} &amp;w^{\left(l\right)}_{01} &amp;\cdots
&amp;w^{\left(l\right)}_{0m}\\[2pt]
&amp;w^{\left(l\right)}_{10} &amp;w^{\left(l\right)}_{11} &amp;\cdots
&amp;w^{\left(l\right)}_{1m}\\[2pt]
&amp;\vdots \\[2pt]
&amp;w^{\left(l\right)}_{j0} &amp;w^{\left(l\right)}_{j1} &amp;\cdots
&amp;w^{\left(l\right)}_{jm}\\[2pt]
&amp;\vdots \\[2pt]
&amp;w^{\left(l\right)}_{n0} &amp;w^{\left(l\right)}_{n1} &amp;\cdots
&amp;w^{\left(l\right)}_{nm}
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="226" src="../../OEBPS/Images/eq_08-08.png" width="261"/></p>
</div>
<p class="fm-equation-caption">Equation 8.8 <span class="calibre" id="eq-MLP-weight-matrix"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span> represents the activations for the entire layer <i class="timesitalic">l</i>. Applying the sigmoid function to a vector is equivalent to applying it individually to each element of the vector:</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\vec{a}^{\left(l\right)} =
\begin{bmatrix} a^{\left(l\right)}_{0} \\ a^{\left(l\right)}_{1} \\ \cdots \\ a^{\left(l\right)}_{n}
\end{bmatrix}
\qquad \qquad \qquad
&amp;\vec{a}^{\left(l - 1\right)} =
\begin{bmatrix} a^{\left(l - 1\right)}_{0} \\ a^{\left(l - 1\right)}_{1} \\ \cdots \\
\cdots \\ a^{\left(l - 1\right)}_{m}
\end{bmatrix} \\[6pt]
&amp;\vec{z}^{\left(l\right)} =
\begin{bmatrix} z^{\left(l\right)}_{0} \\ z^{\left(l\right)}_{1} \\ \cdots \\ z^{\left(l\right)}_{n}
\end{bmatrix}
\qquad \qquad \qquad
&amp;\sigma\left(\vec{z}^{\left(l\right)}\right) =
\begin{bmatrix}
\sigma\left(z^{\left(l\right)}_{0}\right) \\
\sigma\left(z^{\left(l\right)}_{1}\right) \\ \cdots \\
\sigma\left(z^{\left(l\right)}_{n}\right)
\end{bmatrix}\end{aligned}$$</span> </span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="327" src="../../OEBPS/Images/eq_08-08-a.png" width="282"/></p>
</div>
<p class="body">The matrix-vector notation saves us from dealing with subscripts by working with all the weights, biases, activations, and so on in a <i class="fm-italics">global fashion</i>.</p>
<h3 class="fm-head1" id="sec-layered-forwardprop">8.3.2 Forward propagation and grand output functions for an MLP of linear layers</h3>
<p class="body"><a id="marker-280"/>Equation <a class="url" href="../Text/08.xhtml#eq-linlayer-forwardprop">8.7</a> describes the forward propagation of a single linear layer. The final output of an MLP with fully connected (aka linear) layers <span class="math">0‚ãØ<i class="fm-italics">L</i></span> on input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> can be obtained by repeated application of this equation:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">MLP</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>) = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup> = <i class="fm-italics">y</i> = <i class="fm-italics">œÉ</i>(<i class="fm-italics">W</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup>‚Ä¶<i class="fm-italics">œÉ</i>(<i class="fm-italics">W</i><sup class="fm-superscript">(1)</sup><i class="fm-italics">œÉ</i>(<i class="fm-italics">W</i><sup class="fm-superscript">(0)</sup><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> + <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span><sup class="fm-superscript">(0)</sup>)+<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span><sup class="fm-superscript">(1)</sup>)‚ãØ+<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup>)</span></p>
<p class="fm-equation-caption">Equation 8.9 <span class="calibre" id="eq-MLP-out-nosubscript"/></p><!-- <span class="times">$$\begin{aligned}
&amp;\vec{a}^{0} = \sigma\left(W^{\left(0\right)} \vec{x} +
\vec{b}^{\left(0\right)} \right) \nonumber  \\
&amp;\vec{a}^{1} = \sigma\left(W^{\left(1\right)} \vec{a}^{0} +
\vec{b}^{\left(1\right)} \right) \nonumber  \\
&amp;\cdots \nonumber  \\
&amp;\vec{a}^{L} = \sigma\left(W^{\left(L\right)} \vec{a}^{L - 1} +
\vec{b}^{\left(L\right)} \right)\end{aligned}$$</span></p>-->
<p class="body">In a computer implementation, this expression is evaluated step by step by repeated application of the linear layer:</p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="153" src="../../OEBPS/Images/eq_08-10.png" width="199"/></p>
</div>
<p class="fm-equation-caption">Equation 8.10 <span class="calibre" id="eq-forwardprop-layered"/></p>
<p class="body">It‚Äôs easy to see that equation <a class="url" href="#eq-forwardprop-layered">8.10</a> is a restatement of equation <a class="url" href="../Text/08.xhtml#eq-linlayer-forwardprop">8.7</a>.</p>
<p class="body">Close examination of these equations reveals a beautiful property. The complicated equation <a class="url" href="#eq-MLP-out-nosubscript">8.9</a> is <i class="fm-italics">never explicitly evaluated</i>. Instead, we evaluate the outputs of successive layers, one layer at a time, as per equation <a class="url" href="#eq-forwardprop-layered">8.10</a>. Every layer can be evaluated by taking the previous layer‚Äôs output as input. No other input is necessary. That is to say, we can evaluate <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(0)</sup></span> directly from the input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, then <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(1)</sup></span> from <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(0)</sup></span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(2)</sup></span> from <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(1)</sup></span>, and so forth, all the way to <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup></span> (which is the grand output of the MLP). During the evaluation, we need to keep only the previous and current layers in memory at any given time. This process greatly simplifies the implementation as well as the conceptualization and is known as <i class="fm-italics">forward propagation</i>.</p>
<p class="fm-code-listing-caption" id="listing-8.1-pytorch-code-for-forward-propagation">Listing 8.1 PyTorch code for forward propagation</p>
<pre class="programlisting">def Z(x, W_l, b_l):                    <span class="fm-combinumeral">‚ë†</span>
    return torch.matmul(W_l, x) + b_l

def A(z_l):                            <span class="fm-combinumeral">‚ë°</span>
    return torch.sigmoid(z_l)




def forward(x, W, b):                  <span class="fm-combinumeral">‚ë¢</span>
    L = len(W) - 1
    a_l = x
    for l in range(0, L + 1):          <span class="fm-combinumeral">‚ë£</span>
        z_l = Z(a_l, W[l], b[l])       <span class="fm-combinumeral">‚ë§</span>
        a_l = A(z_l)                   <span class="fm-combinumeral">‚ë•</span>
    return a_l</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> x: activation of layer l-1 (1-d vector) <i class="timesitalic">W<sub class="fm-subscript">l</sub></i>: Weight matrix of layer l <i class="timesitalic">b<sub class="fm-subscript">l</sub></i>: Bias vector of layer l</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Sigmoid activation function (nonlinear layer)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¢</span> x: 1-d input vector W: list of matrices for layers 0 to L. b: list of vectors for layers 0 to L</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë£</span> Loops through layers 0 to L</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë§</span> Computes Z</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë•</span> Computes activation</p>
<h2 class="fm-head" id="training-and-backpropagation">8.4 Training and backpropagation</h2>
<p class="body"><a id="marker-281"/>Throughout the book, we have been discussing bits and pieces of this process. In sections <a class="url" href="../Text/01.xhtml#alg-supervised_training">1.1</a> and <a class="url" href="../Text/03.xhtml#sec-grad">3.3</a> (specifically, algorithm 3.1 ), we saw an overview of the process for training a supervised model (you are encouraged to reread those if necessary). Training is an iterative process by which the parameters of the neural network are estimated. The goal is to estimate the parameters (weights and biases) such that on the training inputs, the neural network outputs are close as possible to the known ground-truth outputs.</p>
<p class="body">In general, iterative processes improve (get closer to the goal) gradually. In each iteration, we make small adjustments to the parameters. Here, <i class="fm-italics">parameter</i> refers to the weights and biases of the MLP, the <span class="math"><i class="fm-italics">w<sub class="fm-subscript">jk</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>s and <span class="math"><i class="fm-italics">b<sub class="fm-subscript">j</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>s from section <a class="url" href="#sec-linlayer-forwardprop">8.2</a>. We keep adjusting the parameters so that in every iteration, the outputs on training data inputs come a little closer to the ground truth (GT). Eventually, after many iterations, we hopefully converge to optimal values. Note that there is no guarantee that the iterative process will converge to the best possible parameter values. The training might go completely astray or get stuck in a local minimum. (Local minima are explained in section <a class="url" href="../Text/03.xhtml#sec-locglob-minima">3.6</a>; you are encouraged to reread it if necessary.) There is no good way to know whether we have reached optimal values (global minima) for the weights and biases. We typically run the neural network on test data, and if the results are satisfactory, we stop training. Test data should be <i class="fm-italics">held back</i> during training, meaning we should never use test data to train. In the unfortunate event that the network has not reached the desired level of accuracy, we typically throw in more training data and/or try a modified loss function and/or a different architecture. Simply retraining the network from a different random start may also work. This is an experimental science with a lot of trial and error.</p>
<p class="body">How do we know how to adjust the parameter values in each iteration? We define a loss (aka error) function. There are many popular formulations of loss functions, and we review many of them later, but their common property is that when the neural network output agrees more with the known output (GT), the loss becomes lower, and vice versa. Thus if <i class="fm-italics">y</i> denotes the output of the neural network and <i class="timesitalic">»≥</i> is the GT, a reasonable expression for the loss is the <i class="fm-italics">mean squared error</i> (MSE) function <span class="math">(<i class="fm-italics">y</i> ‚àí <i class="fm-italics">»≥</i>)<sup class="fm-superscript">2</sup></span>. For now, we use the MSE loss as our representative loss function. Later we discuss others.</p>
<p class="body">Once the loss function is defined, we have a crisp, quantitative definition of the goal of neural network training. The goal is to minimize the total loss over the entire training data set. Note the clause <i class="fm-italics">entire training data set</i>: we do not want to do well on one or two input instances at the cost of doing badly over the rest. If we have to choose between a solution that gives <span class="math">10%</span> error on all of, say, 100 training input instances versus one that yields <span class="math">0%</span> error on 50 training input instances but <span class="math">40%</span> on the remaining <span class="math">50</span>, we prefer the former.</p>
<p class="body"><a id="marker-282"/>Each weight in the MLP, <span class="math"><i class="fm-italics">w<sub class="fm-subscript">jk</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>, is adjusted by an amount proportional to <span class="math"><i class="fm-italics">Œ¥w<sub class="fm-subscript">jk</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>. Similarly, each bias <span class="math"><i class="fm-italics">b<sub class="fm-subscript">j</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span> is adjusted by an amount proportional to <span class="math"><i class="fm-italics">Œ¥b<sub class="fm-subscript">j</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>. We can denote all this compactly by saying we have a weight vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> and bias vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>. In each iteration, we change <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> by amount <span class="math"><i class="fm-italics">Œ¥</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> by <span class="math"><i class="fm-italics">Œ¥</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> so that their new values are <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> ‚àí <i class="fm-italics">rŒ¥</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span></span> and <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> ‚àí <i class="fm-italics">rŒ¥</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span> <i class="timesitalic">r</i> is a constant known as the <i class="fm-italics">learning rate</i> that needs to be set at the beginning of training). In this context, it is worthwhile to note that in section <a class="url" href="#sec-linlayer-matmult">8.3.1</a>, we expressed the collection of weights in an MLP with a matrix, while here we are referring to the same thing as a vector. These are not incompatible because we can always rasterize the elements of a matrix (that is, walk over the elements of the matrix from top to bottom and from left to right) into a vector.</p>
<p class="body">How do we estimate the adjustment amounts <span class="math"><i class="fm-italics">Œ¥</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span></span> and <span class="math"><i class="fm-italics">Œ¥</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></span>? This is where the notion of gradients comes in. These were discussed in detail in sections <a class="url" href="../Text/03.xhtml#sec-gradient">3.3.1</a>, <a class="url" href="../Text/03.xhtml#subsec-level-surf">3.3.2</a>, and <a class="url" href="../Text/03.xhtml#sec-gradient-descent">3.5</a> (again, you are encouraged to reread if necessary). In general, if a loss, denoted <span class="segoe">ùïÉ</span>, is expressed as a function of the parameters, such as <span class="math"><span class="segoe">ùïÉ</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>)</span>, then the change in the parameters that optimally takes us toward lower loss is yielded by the gradient of the loss with respect to the parameters <span class="math">‚àá<sub class="fm-subscript"><span class="infigure1"><img alt="" class="calibre21" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics1">b</i></sub><span class="segoe">ùïÉ</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i>)</span>. The high-level process is described later in the chapter in algorithm <a class="url" href="../Text/03.xhtml#alg-supervised_training_detailed">3.2</a>. Here we look at the guts of it.</p>
<h3 class="fm-head1" id="sec-backprop-guts">8.4.1 Loss and its minimization: Goal of training</h3>
<p class="body">Given a training data set <span class="math">ùïã</span> that is a set of &lt;input, GT output&gt; pairs <span class="math">ùïã = {<span class="segoe">‚ü®</span><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <i class="fm-italics">»≥</i><span class="segoe">‚ü©</span>}</span>, the loss can be expressed as</p><!--<p class="Body"><span class="times">$$\mathbb{L} =
\frac{1}{2}\sum_{x \in \mathbb{T}} \left( \vec{y} - \bar{y}
\right)^{2}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="56" src="../../OEBPS/Images/eq_08-11.png" width="139"/></p>
</div>
<p class="fm-equation-caption">Equation 8.11 <span class="calibre" id="eq-mse-loss"/></p>
<p class="body">where</p>
<p class="fm-equation"><span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> = <i class="fm-italics">MLP</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span></p>
<p class="body">as per equation <a class="url" href="#eq-MLP-out-nosubscript">8.9</a>.</p>
<p class="body">Now consider equation <a class="url" href="../Text/08.xhtml#eq-linlayer-forwardprop">8.7</a> again. We can rasterize each layer‚Äôs weight matrix <span class="math"><i class="fm-italics">W</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span> into a vector and then concatenate all these vectors from successive layers to form a giant weight vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, the vector of all weights in the MLP:</p><!--<p class="Body"><span class="times">$$\vec{w} =
\begin{bmatrix}
&amp;w^{\left( 0 \right)}_{00} &amp;w^{\left( 0 \right)}_{01}
&amp;\cdots &amp;w^{\left( 1 \right)}_{00}  &amp;w^{\left( 1
\right)}_{01} &amp;\cdots  &amp;w^{\left( L
\right)}_{00}  &amp;w^{\left( L \right)}_{01} &amp;\cdots
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="37" src="../../OEBPS/Images/eq_08-11-a.png" width="474"/></p>
</div>
<p class="body">Similarly, we can form a giant vector of all biases in the MLP:</p><!--<p class="Body"><span class="times">$$\vec{b} =
\begin{bmatrix}
&amp;b^{\left( 0 \right)}_{0} &amp;b^{\left( 0 \right)}_{1} &amp;\cdots
&amp;b^{\left( 1 \right)}_{0} &amp;b^{\left( 1 \right)}_{1}  &amp;\cdots
&amp;b^{\left( L \right)}_{0} &amp;b^{\left( L \right)}_{1}  &amp;\cdots
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="40" src="../../OEBPS/Images/eq_08-11-b.png" width="444"/></p>
</div>
<p class="body">The ultimate goal of training is to find <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> that will minimize the loss <span class="segoe">ùïÉ</span>. In chapter <a class="url" href="../Text/03.xhtml#chapter-intro-vec-mat">3</a>, we saw that we can solve for the minimum by setting the gradients <span class="math">‚àá<sub class="fm-subscript"><span class="infigure1"><img alt="" class="calibre21" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span></sub><span class="segoe">ùïÉ</span> = 0</span> and <span class="math">‚àá<sub class="fm-subscript"><span class="infigure1"><img alt="" class="calibre21" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></sub><span class="segoe">ùïÉ</span> = 0</span>. Computing the loss gradient from a combination of equations <a class="url" href="#eq-MLP-out-nosubscript">8.9</a> and <a class="url" href="../Text/08.xhtml#eq-mse-loss">8.11</a> is intractable. Instead, we go for an iterative solution: <i class="fm-italics">gradient descent</i> on the loss surface, as described in the next section.</p>
<p class="fm-code-listing-caption" id="listing-8.2-pytorch-code-for-mse-loss">Listing 8.2 PyTorch code for MSE loss</p>
<pre class="programlisting">def mse_loss(a_L, y):                      <span class="fm-combinumeral">‚ë†</span>
    return 1./ 2 * torch.pow((a_L - y), 2) <span class="fm-combinumeral">‚ë°</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> a: Activation of layer L (1D vector) : Ground truth (1D vector)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> See equation <a class="url" href="../Text/08.xhtml#eq-mse-loss">8.11</a>.</p>
<h3 class="fm-head1" id="sec-loss-surface-graddesc">8.4.2 Loss surface and gradient descent</h3>
<p class="body"><a id="marker-283"/>Geometrically, the loss function <span class="math"><span class="segoe">ùïÉ</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>)</span> can be viewed as a surface in a high-dimensional space. The domain of this space corresponds to all the dimensions in <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> plus all the dimensions in <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>. This is shown in figure <a class="url" href="../Text/08.xhtml#fig-gradient-descent-3d">8.8</a> with a <span class="math">2</span>D domain. In chapter <a class="url" href="../Text/03.xhtml#chapter-intro-vec-mat">3</a>, we also saw that given a function <span class="math"><span class="segoe">ùïÉ</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>)</span>, the best way to progress toward the minimum is to walk on the parameter space along the negative gradient. We adopt this approach to minimize the loss. We compute the gradients of the loss function with respect to weights and biases and update the weights and bias vectors by an amount proportional to the (negative) of these gradients. Doing this repeatedly takes us close to the minimum. In figure <a class="url" href="../Text/08.xhtml#fig-gradient-descent-3d">8.8</a>, the gradient descent path is shown with solid arrows, while an arbitrary non-optimal path to the minimum is shown with dashed arrows.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre13" height="581" id="fig-gradient-descent-3d" src="../../OEBPS/Images/CH08_F08_Chaudhury.png" width="629"/></p>
<p class="figurecaption">Figure 8.8 A representative loss <span class="math"><span class="segoe">ùïÉ</span>(<i class="fm-italics">w</i>, <i class="fm-italics">b</i>)</span>. Note that <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> and <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> have each been reduced to 1D for this .</p>
</div>
<p class="body">Thus the equations for updating weights and biases in gradient descent are</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\vec{w} = \vec{w} - r \nabla_{\vec{w}} \mathbb{L} \nonumber \\
&amp;\vec{b} = \vec{b} - r \nabla_{\vec{b}}
\mathbb{L}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="66" src="../../OEBPS/Images/eq_08-12.png" width="120"/></p>
</div>
<p class="fm-equation-caption">Equation 8.12 <span class="calibre" id="eq-wtsbiases-update-vector"/></p>
<p class="body">where <i class="timesitalic">r</i> is a constant. Here</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\nabla_{\vec{w}} \mathbb{L} = \left[ \frac{\partial
\mathbb{L}}{\partial w^{\left( l \right)}_{jk}}
\;\;\;\;  \text{      for all }  l, j, k \right] \nonumber \\
&amp; \nabla_{\vec{b}} \mathbb{L} = \left[ \frac{\partial
\mathbb{L}}{\partial b^{\left(l\right)}_{j}}  \;\;\;\;  \text{      for all }  l, j \right]\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="145" src="../../OEBPS/Images/eq_08-13.png" width="223"/></p>
</div>
<p class="fm-equation-caption">Equation 8.13</p>
<p class="body">The vector update equation <a class="url" href="../Text/08.xhtml#eq-wtsbiases-update-vector">8.12</a> can be expressed in terms of the scalar components as</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;w^{\left( l \right)}_{jk} =  w^{\left( l \right)}_{jk} - r
\frac{\partial \mathbb{L}}{\partial w^{\left( l \right)}_{jk}}
\;\;\;\;  \text{      for all }  l, j, k  \nonumber \\
&amp;b^{\left( l \right)}_{j} =  b^{\left( l \right)}_{j} - r
\frac{\partial \mathbb{L}}{\partial b^{\left( l \right)}_{j}}
\;\;\;\;   \text{      for all }  l, j\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="120" src="../../OEBPS/Images/eq_08-14.png" width="265"/></p>
</div>
<p class="fm-equation-caption">Equation 8.14 <span class="calibre" id="eq-wtsbiases-update-scalar"/></p>
<p class="body"><a id="marker-284"/>Note that we have to reevaluate these partial derivatives in each iteration since their values will change in every iteration.</p>
<h3 class="fm-head1" id="why-a-gradient-provides-the-best-direction-for-descent">8.4.3 Why a gradient provides the best direction for descent</h3>
<p class="body">Why does updating along the gradient reduce the function optimally? This is discussed in detail in chapter <a class="url" href="../Text/03.xhtml#chapter-intro-vec-mat">3</a>. Here we briefly recap the idea. Using multidimensional Taylor expansion, we can evaluate a function in the neighborhood of a known point. For instance, we can evaluate <span class="math"><span class="segoe">ùïÉ</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> + <i class="fm-italics">Œ¥</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span> for small offset <span class="math"><i class="fm-italics">Œ¥</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span></span> from <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> as follows</p><!--<p class="Body"><span class="times">$$\mathbb{L}\left(\vec{w} - \vec{\delta w}\right) = \mathbb{L}\left(\vec{w}\right) -
\frac{1}{1!}\left(\vec{\delta w}\right)^{T} \nabla_{\vec{w}}
\mathbb{L}  +  \frac{1}{2!}\left(\vec{\delta w}\right)^{T} H
\left(\vec{\delta w}\right) + \cdots$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="52" src="../../OEBPS/Images/eq_08-15.png" width="462"/></p>
</div>
<p class="fm-equation-caption">Equation 8.15 <span class="calibre" id="eq-taylor-multidim-loss"/></p>
<p class="body">where <i class="timesitalic">H</i>, called the <i class="fm-italics">Hessian matrix</i>, is defined as in equation <a class="url" href="../Text/03.xhtml#eq-hessian">3.9</a>. Since we are not going too far from <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <span class="math">||<i class="fm-italics">Œ¥</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>||</span> is small. This means the quadratic and higher-order terms are negligibly small, and we can drop them (the approximation is perfect in the limit when <span class="math">||<i class="fm-italics">Œ¥</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>|| ‚Üí 0</span>):</p><!--<p class="Body"><span class="times">$$\mathbb{L}\left(\vec{w} - \vec{\delta w}\right)
\approx \mathbb{L}\left(\vec{w}\right) - \frac{1}{1!}\left(\vec{\delta w}\right)^{T} \nabla_{\vec{w}} \mathbb{L}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="43" src="../../OEBPS/Images/eq_08-15-a.png" width="276"/></p>
</div>
<p class="body">But we know the dot product <span class="math">(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i> ‚ñΩ<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><span class="segoe">ùïÉ</span></span> will attain its maximum value when both the vectors point in the same direction: that is,</p><!--<p class="FM-Equation"><span class="times"><span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_delta.png" /></span> = <i class="fm-italics">r</em>  ‚ñΩ<span class="inFigure"><img alt="" height="15px" src="imgs/icons/AR_w.png" /></span><span class="segoe">ùïÉ</span></span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="31" src="../../OEBPS/Images/eq_08-15-b.png" width="89"/></p>
</div>
<p class="body">for some constant of proportionality <i class="timesitalic">r</i>.</p>
<p class="body">In implementation, <i class="timesitalic">r</i> is called the <i class="fm-italics">learning rate</i>. A higher learning rate causes the optimization to progress more rapidly but also runs the risk of overshooting the minimum. We learn about these in more detail later. For now, simply note that <i class="timesitalic">r</i> is a <i class="fm-italics">tunable hyperparameter</i> of the system.</p>
<p class="body">Thus, the largest decrease in value from <span class="math"><span class="segoe">ùïÉ</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>)</span> to <span class="math"><span class="segoe">ùïÉ</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> ‚Äì <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span>)</span> happens when <span class="math"><i class="fm-italics">Œ¥</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span></span> is along the negative gradient. This is why we move toward the negative gradient in gradient descent: it is the fastest way to reach the minimum. The straight arrows in figure <a class="url" href="../Text/08.xhtml#fig-gradient-descent-3d">8.8</a> illustrate the direction of the gradient. The dashed arrows show an arbitrary nongradient path for comparison.<a id="marker-285"/></p>
<p class="body">We can deal with the bias vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span> similarly.</p>
<h3 class="fm-head1" id="sec-graddesc_local_minima">8.4.4 Gradient descent and local minima</h3>
<p class="body">We should note that gradient descent can get stuck in a <i class="fm-italics">local minimum</i>. Figure <a class="url" href="../Text/08.xhtml#fig-non-convex-local-minima">8.9</a> shows this.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="552" id="fig-non-convex-local-minima" src="../../OEBPS/Images/CH08_F09_Chaudhury.png" width="803"/></p>
<p class="figurecaption">Figure 8.9 A nonconvex function with local and global minima. Depending on the point, gradient descent will take us to one or the other.</p>
</div>
<p class="body">In earlier eras, optimization techniques tried hard to avoid local minima and converge to the global minimum. Techniques like simulated annealing and tunneling were carefully designed to avoid local minima. Modern-day neural networks have adopted a different attitude: they do not try very hard to avoid local minima. Sometimes a local minimum is an acceptable (accurate enough) solution. Otherwise, we can retrain the neural network: it will start from a random position, so this time it may go to a better minimum.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre8" height="538" id="fig-MLP-one-neuron-per-layer" src="../../OEBPS/Images/CH08_F10_Chaudhury.png" width="1023"/></p>
<p class="figurecaption">Figure 8.10 MLP with layers <span class="math">0</span>, <span class="math">‚Ä¶</span>, <i class="timesitalic">L</i>, one neuron per layer. Again, we have split every layer into a weighted sum and a sigmoid.</p>
</div>
<h3 class="fm-head1" id="the-backpropagation-algorithm">8.4.5 The backpropagation algorithm</h3>
<p class="body"><a id="marker-286"/>We have seen that gradient descent progresses by repeatedly updating the weights and biases via equation <a class="url" href="../Text/08.xhtml#eq-wtsbiases-update-vector">8.12</a>. This is equivalent to repeatedly updating individual weights and biases using individual partial derivatives via equation <a class="url" href="#eq-wtsbiases-update-scalar">8.14</a>.</p>
<p class="body">Obtaining a closed-form solution for the gradients <span class="math">‚àá<sub class="fm-subscript"><span class="infigure1"><img alt="" class="calibre21" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span></sub><span class="segoe">ùïÉ</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>)</span>, <span class="math">‚àá<sub class="fm-subscript"><span class="infigure1"><img alt="" class="calibre21" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span></sub><span class="segoe">ùïÉ</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span>)</span> from equations <a class="url" href="#eq-MLP-out-nosubscript">8.9</a> and <a class="url" href="../Text/08.xhtml#eq-mse-loss">8.11</a>‚Äîor, equivalently, obtaining a closed-form solution for the partial derivatives <span class="math"><i class="fm-italics">‚àÇ</i><span class="segoe">ùïÉ</span><span class="large">/</span><i class="fm-italics">‚àÇw<sub class="fm-subscript">jk</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>, <span class="math"><i class="fm-italics">‚àÇ</i><span class="segoe">ùïÉ</span><span class="large">/</span><i class="fm-italics">‚àÇb<sub class="fm-subscript">j</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>, ‚Äîis very difficult. Backpropagation is an algorithm that allows us to evaluate the gradients and update the weights and biases one layer at a time, like forward propagation (equation <a class="url" href="#eq-forwardprop-layered">8.10</a>).</p>
<p class="fm-head2" id="sec-backprop-simplenet">Backpropagation algorithm on a simple network</p>
<p class="body">We first discuss backpropagation on a simple MLP with only a single neuron per layer. The main simplification resulting from this is that individual weights and biases no longer need subscripts, with only one weight and one bias between two successive layers. They still need superscripts to indicate layer IDs, however. Figure <a class="url" href="#fig-MLP-one-neuron-per-layer">8.10</a> shows this MLP. We use MSE loss (equation <a class="url" href="../Text/08.xhtml#eq-mse-loss">8.11</a>), but we work on a single input-output pair <span class="math"><i class="fm-italics">x<sub class="fm-subscript">i</sub></i>, <i class="fm-italics">y<sub class="fm-subscript">i</sub></i></span>. The total loss (summation over all the training data instances) can easily be derived by repeating the same steps.</p>
<p class="body">We first define an auxiliary variable:</p><!--<p class="Body"><span class="times">$$\delta^{ \left( l \right) } = \frac{ \partial
\mathbb{L} }{ \partial z^{ \left( l \right )} }\;\;\;\;\text{  for } l
\in \left\lbrace 0, L \right\rbrace$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="48" src="../../OEBPS/Images/eq_08-15-c.png" width="203"/></p>
</div>
<p class="body">The physical significance of <span class="math"><i class="fm-italics">Œ¥</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span> is that it is the rate of change of the loss with the (pre-activation) output of layer <i class="timesitalic">l</i> (remember, in this network, layer <i class="timesitalic">l</i> has a single neuron).</p>
<p class="body">Let‚Äôs establish a few important equations for the MLP in figure <a class="url" href="#fig-MLP-one-neuron-per-layer">8.10</a>:<a id="marker-287"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Forward propagation for an arbitrary layer</i> <span class="math"><i class="fm-italics">l</i> <span class="cambria">‚àà</span> {0, <i class="fm-italics">L</i>}</span></p>
</li>
</ul><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;z^{ \left( l \right) } = w^{\left( l \right) } a^{ \left( l - 1
\right) } + b^{ \left( l \right) } \\[4pt]\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="31" src="../../OEBPS/Images/eq_08-16.png" width="169"/></p>
</div>
<p class="fm-equation-caption">Equation 8.16 <span class="calibre" id="eq-forwardprop-z-simple"/></p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;a^{ \left( l \right) } = \sigma\left( z^{\left( l \right) }
\right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="41" src="../../OEBPS/Images/eq_08-17.png" width="108"/></p>
</div>
<p class="fm-equation-caption">Equation 8.17 <span class="calibre" id="eq-forwardprop-a-simple"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Loss</i>‚ÄîHere we are working with a single training data instance, <i class="timesitalic">x<sub class="fm-subscript">i</sub></i>, whose GT output is <i class="timesitalic">»≥<sub class="fm-subscript">i</sub></i>:</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\mathbb{L} = \frac{1}{2} \left( a^{ \left( L
\right) } - \bar{y}_{i} \right)^{2}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_08-17-a.png" width="137"/></p>
</div>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Partial derivative of loss with respect to the weight and bias in terms of an auxiliary variable for the last layer, L</i>‚ÄîUsing the chain rule for partial derivatives,</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\frac{ \partial
\mathbb{L} }{ \partial w^{ \left( L \right) } } = \frac{ \partial
\mathbb{L} }{ \partial z^{ \left( L \right) } }  \;\;  \frac{ \partial z^{ \left( L \right) } }{  \partial w^{ \left( L \right)
}  }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="51" src="../../OEBPS/Images/eq_08-17-b.png" width="168"/></p>
</div>
<p class="body-ind">Examining the terms on the right, we see</p><!--<p class="Body"><span class="times">$$\frac{ \partial \mathbb{L} }{ \partial z^{ \left( L \right) } } = \delta^{ \left( L \right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_08-17-c.png" width="96"/></p>
</div>
<p class="body-ind">(auxiliary variable for layer <i class="timesitalic">L</i>). And using the forward propagation equations,</p><!--<p class="Body"><span class="times">$$\frac{
\partial z^{ \left( L \right) } }{  \partial w^{ \left( L \right) }  } = a^{ \left( L - 1 \right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="53" src="../../OEBPS/Images/eq_08-17-d.png" width="119"/></p>
</div>
<p class="body-ind">Together, they lead to</p><!--<p class="Body"><span class="times">$$\frac{ \partial \mathbb{L} }{ \partial w^{ \left( L \right) } } = \delta^{ \left( L \right) } \cdot a^{ \left( L - 1
\right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="45" src="../../OEBPS/Images/eq_08-17-e.png" width="160"/></p>
</div>
<p class="body-ind">Similarly,</p><!--<p class="Body"><span class="times">$$\frac{
\partial \mathbb{L} }{ \partial b^{ \left( L \right) } } =  \frac{
\partial \mathbb{L} }{ \partial z^{ \left( L \right) } }  \;\;  \frac{
\partial z^{ \left( L \right) } }{  \partial b^{ \left( L \right) }  } =
\delta^{ \left( L \right) } \cdot 1$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="49" src="../../OEBPS/Images/eq_08-17-f.png" width="235"/></p>
</div>
<p class="body-ind">Consequently, we have the following pair of equations expressing the partial derivative of loss with respect to weight and bias, respectively, in terms of the auxiliary variable for the last layer:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\frac{ \partial \mathbb{L} }{ \partial w^{ \left( L \right) } } =
\delta^{ \left( L \right) } \cdot a^{ \left( L - 1 \right)
}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="47" src="../../OEBPS/Images/eq_08-18.png" width="162"/></p>
</div>
<p class="fm-equation-caption">Equation 8.18 <span class="calibre" id="eq-dw-aux-lastlayer-simple"/></p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\frac{ \partial \mathbb{L} }{ \partial b^{ \left( L \right) } } =
\delta^{ \left( L \right) }\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="49" src="../../OEBPS/Images/eq_08-19.png" width="99"/></p>
</div>
<p class="fm-equation-caption">Equation 8.19 <span class="calibre" id="eq-db-aux-lastlayer-simple"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Auxiliary variable for the last layer, L</i>‚ÄîUsing the chain rule for partial derivatives,</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\delta^{ \left( L
\right) } = \frac{ \partial \mathbb{L} }{ \partial z^{ \left( L \right
)} } =  \frac{ \partial \mathbb{L} }{ \partial a^{ \left( L \right )} }
\;\; \frac{ \partial a^{ \left( L \right )} }{ \partial z^{ \left( L
\right )} } = \left( a^{ \left( L \right )} - \bar{y}_{i} \right) \;\;
\frac{ d \sigma\left( z^{ \left( L \right ) } \right) }{d z^{ \left( L
\right ) } }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="66" src="../../OEBPS/Images/eq_08-19-a.png" width="402"/></p>
</div>
<p class="body-ind">Using equation <a class="url" href="../Text/08.xhtml#eq-sigmoid-derivative">8.5</a> for the derivative of a sigmoid, we get</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">Œ¥</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup> = (<i class="fm-italics">a</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup>‚àí<i class="fm-italics">»≥<sub class="fm-subscript">i</sub></i>) <i class="fm-italics">œÉ</i>(<i class="fm-italics">z</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup>)(1‚àí<i class="fm-italics">œÉ</i>(<i class="fm-italics">z</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup>))</span></p>
<p class="body-ind">which, using equation <a class="url" href="#eq-forwardprop-a-simple">8.17</a>, leads to</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">Œ¥</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup> = (<i class="fm-italics">a</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup>‚àí<i class="fm-italics">»≥<sub class="fm-subscript">i</sub></i>) <i class="fm-italics">a</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup>(1‚àí<i class="fm-italics">a</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup>)</span></p>
<p class="fm-equation-caption">Equation 8.20 <span class="calibre" id="eq-aux-lastlayer-simple"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Partial derivative of the loss with respect to the weight and bias in terms of an auxiliary variable for an arbitrary layer l</i>‚ÄîUsing the chain rule for partial derivatives,<a id="marker-288"/></p>
</li>
</ul><!--<p class="Body"><span class="times">$$\frac{ \partial
\mathbb{L} }{ \partial w^{ \left( l \right) } } = \frac{ \partial
\mathbb{L} }{ \partial z^{ \left( l \right) } }  \;\;  \frac{ \partial z^{ \left( l \right) } }{  \partial w^{ \left( l \right) }  }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="52" src="../../OEBPS/Images/eq_08-20-a.png" width="157"/></p>
</div>
<p class="body-ind">Using the definition of the auxiliary variable and the forward propagation equation <a class="url" href="#eq-forwardprop-z-simple">8.16</a>, this leads to</p><!--<p class="Body"><span class="times">$$\frac{ \partial \mathbb{L} }{
\partial w^{ \left( l \right) } } = \delta^{ \left( l \right) } \;\; a^{
\left( l - 1 \right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="48" src="../../OEBPS/Images/eq_08-21.png" width="144"/></p>
</div>
<p class="fm-equation-caption">Equation 8.21 <span class="calibre" id="eq-dw-aux-simple"/></p>
<p class="body-ind">Similarly,</p><!--<p class="Body"><span class="times">$$\frac{ \partial \mathbb{L} }{
\partial b^{ \left( l \right) } } = \frac{ \partial \mathbb{L} }{
\partial z^{ \left( l \right) } }  \;\;  \frac{ \partial z^{ \left( l
\right) } }{  \partial b^{ \left( l \right) }  }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="47" src="../../OEBPS/Images/eq_08-21-a.png" width="145"/></p>
</div>
<p class="body-ind">Using the definition of the auxiliary variable and the forward propagation equation <a class="url" href="#eq-forwardprop-z-simple">8.16</a>, this leads to</p><!--<p class="Body"><span class="times">$$\frac{ \partial \mathbb{L} }{
\partial b^{ \left( l \right) } } = \delta^{ \left( l \right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="46" src="../../OEBPS/Images/eq_08-22.png" width="91"/></p>
</div>
<p class="fm-equation-caption">Equation 8.22 <span class="calibre" id="eq-db-aux-simple"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Auxiliary variable for an arbitrary layer, l</i>‚ÄîUsing the chain rule for partial derivatives,</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\delta^{ \left( l
\right) } = \frac{ \partial \mathbb{L} }{ \partial z^{ \left( l \right
)} } =  \frac{ \partial \mathbb{L} }{ \partial z^{ \left( l+1 \right )}
} \;\; \frac{ \partial z^{ \left( l+1 \right )} }{ \partial a^{ \left( l
\right )} } \;\; \frac{  \partial a^{ \left( l \right )} }{ \partial z^{
\left( l \right )} }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="50" src="../../OEBPS/Images/eq_08-22-a.png" width="272"/></p>
</div>
<p class="body-ind">Using the definition of the auxiliary variable and the forward propagation equation <a class="url" href="#eq-forwardprop-z-simple">8.16</a>, this leads to</p><!--<p class="Body"><span class="times">$$\delta^{ \left( l \right) } =
\delta^{ \left( l+1 \right) } \;\; w^{ \left( l+1 \right) } \;\; \frac{ d \sigma\left( z^{ \left( l \right ) } \right) }{d z^{ \left( l \right )
} } =  \delta^{ \left( l+1 \right) } \;\; w^{ \left( l+1 \right) } \;\;
\sigma\left( z^{ \left( l \right ) } \right) \left( 1 - \sigma\left( z^{
\left( l \right ) } \right) \right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="64" src="../../OEBPS/Images/eq_08-22-b.png" width="504"/></p>
</div>
<p class="body-ind">which yields</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">Œ¥</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup> = <i class="fm-italics">Œ¥</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>+1)</sup> <i class="fm-italics">w</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>+1)</sup> <i class="fm-italics">a</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup>(1‚àí<i class="fm-italics">a</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup>)</span></p>
<p class="fm-equation-caption">Equation 8.23 <span class="calibre" id="eq-aux-simple"/></p>
<p class="body">We first encountered the one-layer-at-a-time property in section <a class="url" href="#sec-layered-forwardprop">8.3.2</a> in connection with the forward propagation equations. Let‚Äôs recap that in the context of training our simple network. Consider equations <a class="url" href="#eq-forwardprop-z-simple">8.16</a> and <a class="url" href="#eq-forwardprop-a-simple">8.17</a>. We initialize the system with some values of weights <span class="math"><i class="fm-italics">w</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span> and biases <span class="math"><i class="fm-italics">b</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>. Using those, we can evaluate the layer <span class="math">0</span> outputs. For starters, we can evaluate <span class="math"><i class="fm-italics">z</i><sup class="fm-superscript">(0)</sup></span> and <span class="math"><i class="fm-italics">a</i><sup class="fm-superscript">(0)</sup></span> easily (since all the inputs are known):</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;z^{ \left( 0 \right) } = w^{\left( 0 \right) } x + b^{ \left( 0
\right) }  \\*
&amp;a^{ \left( 0 \right) } = \sigma\left( z^{\left( 0 \right) }
\right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="78" src="../../OEBPS/Images/eq_08-23-a.png" width="142"/></p>
</div>
<p class="body">Once we have <span class="math"><i class="fm-italics">z</i><sup class="fm-superscript">(0)</sup></span> and <span class="math"><i class="fm-italics">a</i><sup class="fm-superscript">(0)</sup></span>, we can use them to evaluate <span class="math"><i class="fm-italics">z</i><sup class="fm-superscript">(1)</sup></span> and <span class="math"><i class="fm-italics">a</i><sup class="fm-superscript">(1)</sup></span> via equations <a class="url" href="#eq-forwardprop-z-simple">8.16</a> and <a class="url" href="#eq-forwardprop-a-simple">8.17</a>. But if we have <span class="math"><i class="fm-italics">z</i><sup class="fm-superscript">(1)</sup></span> and <span class="math"><i class="fm-italics">a</i><sup class="fm-superscript">(1)</sup></span>, we can use them to evaluate <span class="math"><i class="fm-italics">z</i><sup class="fm-superscript">(2)</sup></span> and <span class="math"><i class="fm-italics">a</i><sup class="fm-superscript">(2)</sup></span> via equations <a class="url" href="#eq-forwardprop-z-simple">8.16</a> and <a class="url" href="#eq-forwardprop-a-simple">8.17</a> again. And we can proceed in this fashion up to layer <i class="timesitalic">L</i> to obtain <span class="math"><i class="fm-italics">a</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup></span>, which is the grand output of the MLP. In other words, we can iteratively evaluate the outputs of successive layers using <i class="fm-italics">only</i> the outputs from the previous layer. No other layers need to be known. At any given iteration, we only have to keep the previous layer in memory: we can build the current layer from that. A single sequence of applications of equations <a class="url" href="#eq-forwardprop-z-simple">8.16</a> and <a class="url" href="#eq-forwardprop-a-simple">8.17</a> for layers <span class="math">0</span> to <i class="timesitalic">L</i> is known as a <i class="fm-italics">forward pass</i>.<a id="marker-289"/></p>
<p class="body">A similar trick can be applied to evaluate the auxiliary variables, except we go <i class="fm-italics">backward</i>. We can evaluate the auxiliary variable for the last layer, <span class="math"><i class="fm-italics">Œ¥</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup></span>, via equation <a class="url" href="#eq-aux-lastlayer-simple">8.20</a>. But once we have <span class="math"><i class="fm-italics">Œ¥</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup></span>, we can evaluate <span class="math"><i class="fm-italics">Œ¥</i><sup class="fm-superscript">(<i class="fm-italics1">L</i> ‚àí 1)</sup></span> via equation <a class="url" href="#eq-aux-simple">8.23</a>. From that, we can evaluate <span class="math"><i class="fm-italics">Œ¥</i><sup class="fm-superscript">(<i class="fm-italics1">L</i> ‚àí 2)</sup></span>. We can proceed in this fashion all the way to layer <span class="math">0</span>, evaluating successively <span class="math"><i class="fm-italics">Œ¥</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup></span>, <span class="math"><i class="fm-italics">Œ¥</i><sup class="fm-superscript">(<i class="fm-italics1">L</i> ‚àí 1)</sup></span>, <span class="math">‚ãØ</span>, <span class="math"><i class="fm-italics">Œ¥</i><sup class="fm-superscript">(0)</sup></span>. Every time we evaluate a <span class="math"><i class="fm-italics">Œ¥</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>, we can also evaluate the <span class="math"><i class="fm-italics">‚àÇ</i><span class="segoe">ùïÉ</span><b class="fm-bold">/</b><i class="fm-italics">‚àÇw</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span> and <span class="math"><i class="fm-italics">‚àÇ</i><span class="segoe">ùïÉ</span><b class="fm-bold">/</b><i class="fm-italics">‚àÇb</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span> for the same layer via equations <a class="url" href="#eq-dw-aux-simple">8.21</a> and <a class="url" href="#eq-db-aux-simple">8.22</a>, respectively. We can also update the weight and bias of that layer right there using the just estimated partial derivatives, since the current values will never be needed again during training. Thus, starting from the last layer, we can update the weights and biases of all layers until layer <span class="math">0</span> in this fashion. This is <i class="fm-italics">backpropagation</i>.</p>
<p class="body">Of course, we have to proceed in tandem: one forward propagation which sets the values of <i class="timesitalic">z</i>s and <i class="timesitalic">a</i>s) for layers <span class="math">0</span> to <i class="timesitalic">L</i>, followed by a backpropagation layer for <i class="timesitalic">L</i> to <span class="math">0</span>. Repeat these steps until convergence.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for forward propagation, MSE loss, and backpropagation, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/pJrw">http://mng.bz/pJrw</a>.</p>
<p class="fm-code-listing-caption" id="listing-8.3-pytorch-code-for-forward-and-backward-propagation">Listing 8.3 PyTorch code for forward and backward propagation</p>
<pre class="programlisting">def forward_backward(x, y, W, b):
    L = len(W) - 1
    a = []
    for l in range(0, L+1):
        a_prev = x if l == 0 else a[l-1]              <span class="fm-combinumeral">‚ë†</span>
        z_l = Z(a_prev, W[l], b[l])
        a_l = A(z_l)
        a.append(a_l)

    loss = mse_loss(a[L], y)                          <span class="fm-combinumeral">‚ë°</span>

    deltas = [None for _ in range(L + 1)]
    W_grads = [None for _ in range(L + 1)]            <span class="fm-combinumeral">‚ë¢</span>
    b_grads = [None for _ in range(L + 1)]

    a_L = a[L]                                        <span class="fm-combinumeral">‚ë£</span>
    deltas[L] = (a_L - y) * a_L * (1 - a_L)
    W_grads[L] = torch.matmul(deltas[L], a[L - 1].T)  <span class="fm-combinumeral">‚ë§</span>
    b_grads[L] = deltas[L]

    for l in range(L-1, -1, -1):                      <span class="fm-combinumeral">‚ë•</span>
        a_l = a[l]
        deltas[l] =  torch.matmul(W[l+1].T, deltas[l + 1]) * a_l * (1 - a_l)
        W_grads[l] = torch.matmul(deltas[l], a[l - 1].T)
        b_grads[l] = deltas[l]

    return loss, W_grads, b_grads</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Forward propagation</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Computes MSE loss</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¢</span> Arrays to store <span class="math"><i class="fm-italics">Œ¥</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>, <span class="math"><i class="fm-italics">‚àÇ</i><span class="segoe">ùïÉ</span><b class="fm-bold">/</b><i class="fm-italics">‚àÇw</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>, <span class="math"><i class="fm-italics">‚àÇ</i><span class="segoe">ùïÉ</span><b class="fm-bold">/</b><i class="fm-italics">‚àÇb</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span> for layers <span class="math">0</span> to <i class="timesitalic">L</i></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë£</span> Activation of the last layer - <span class="math"><i class="fm-italics">a</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup></span></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë§</span> Computes the <i class="timesitalic">Œ¥</i> and gradients for layer <i class="timesitalic">L</i></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë•</span> Computes the <i class="timesitalic">Œ¥</i> and gradients for layers <span class="math">0</span> to <span class="math"><i class="fm-italics">L</i> ‚àí 1</span></p>
<p class="fm-head2" id="backpropagation-algorithm-on-an-arbitrary-network-of-linear-layers">Backpropagation algorithm on an arbitrary network of linear layers</p>
<p class="body"><a id="marker-290"/>In section <a class="url" href="#sec-backprop-simplenet">8.4.5.1</a>, we saw a simple network with only one neuron per layer. There was only one connection and hence one weight, one activation, and one auxiliary variable per layer. Consequently, we could drop the subscripts (although we had to keep the superscript indicating the layer) of all these variables. Now we examine a more generic network consisting of linear layers <span class="math">0</span>, <span class="math">‚ãØ</span>, <i class="timesitalic">L</i>. An arbitrary slice of this network is shown in figure <a class="url" href="#fig-linear-layer">8.6</a>.</p>
<p class="body">The ultimate goal is to evaluate the partial derivatives of the loss with respect to the weights and biases. Using them, we can update the current weights and biases to optimally reduce the loss.</p>
<p class="body">Our overall strategy is as follows. We use the auxiliary variables again. We first derive expressions that allow us to compute the auxiliary variable for the last layer. Then we derive an expression that allows us to compute auxiliary variables for an arbitrary layer, <i class="timesitalic">l</i>, given the auxiliary variables for layer <span class="math"><i class="fm-italics">l</i> + 1</span>. Since we can directly compute auxiliary variables for the last layer, <i class="timesitalic">L</i>, we can use this expression to compute auxiliary variables for the second-to-last layer <span class="math"><i class="fm-italics">L</i> ‚àí 1</span>. But once we have them, we can compute auxiliary variables for layer <span class="math"><i class="fm-italics">L</i> ‚àí 2</span>. We proceed like this until we reach layer <span class="math">0</span>. Thus we can compute all the auxiliary variables. We also derive expressions that allow us to compute, from the auxiliary variables, the partial derivatives of loss with respect to weights and biases. This gives us everything we need. Since we start by computing things pertaining to the last layer and proceed iteratively toward the initial layer, the process is called <i class="fm-italics">backpropagation</i>.</p>
<p class="body">You will notice the similarity between the expressions derived next and those derived for the one-neuron-per-layer network. The differences are explained:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Forward propagation (arbitrary layer l)</i>‚ÄîForward propagation through this network has already been described in section <a class="url" href="#sec-linlayer-matmult">8.3.1</a> and can be succinctly represented by equation <a class="url" href="../Text/08.xhtml#eq-linlayer-forwardprop">8.7</a> repeated here for handy reference). On the left are the scalar equations, for one neuron at a time; and on the right are the vector equations, for the entire layer. They are equivalent:</p>
</li>
</ul><!--<div class="flalign">
<p class="Body">&amp;z^(l)</span>_j</span> =
_k=0</span>^m</span> w^(l)</span>_jk</span> a^(l - 1)</span>_k</span> + b^( l
)</span>_j</span> &amp;^(l)</span> = W^(l)</span>
^(l-1)</span> + ^(l)</span>
&amp;a^(l)</span>_j</span> = ( z^(l)</span>_j</span> ) &amp;^(l)</span> = (
^(l)</span> )</p>
</div>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="110" src="../../OEBPS/Images/eq_08-24.png" width="426"/></p>
</div>
<p class="fm-equation-caption">Equation 8.24 <span class="calibre" id="eq-MLP-out-nosubscript-1"/></p>
<p class="body">Indices <i class="timesitalic">j</i> and <i class="timesitalic">k</i> iterate over all the neurons in the relevant layer. By convention, we always use these variables for arbitrary neurons in a layer. The variable <i class="timesitalic">l</i> is used to index the layers. When indexing weights, we typically use <i class="timesitalic">j</i> to indicate the destination and <i class="timesitalic">k</i> to indicate the source‚Äîremember that weights are indexed (destination, source) somewhat unexpectedly to simplify the math. Typically, vectors correspond to entire layers. Individual vector elements correspond to specific neurons and are indexed by <i class="timesitalic">j</i> or <i class="timesitalic">k</i>.</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Loss</i>‚ÄîUnlike the simple network, here, the final <i class="timesitalic">L</i>th layer can have multiple neurons. Hence the loss function becomes</p>
</li>
</ul><!--<p class="FM-Equation-Caption"><span class="times">$$\mathbb{L}
= \frac{1}{2} \| \vec{a}^{ \left( L \right) } - \bar{y} \|^{2} =
\frac{1}{2} \sum_{j} \left( a^{ \left( L \right) }_{j} - \bar{y}_{j}
\right)^{2}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="61" src="../../OEBPS/Images/eq_08-25.png" width="281"/></p>
</div>
<p class="fm-equation-caption">Equation 8.25 <span class="calibre" id="eq-loss-mlp"/></p>
<p class="body-ind"><a id="marker-291"/>where the summation happens over all neurons in the last layer. Note that <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup></span> is the output of the MLP: that is, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup> = <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> = <i class="fm-italics">MLP</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> for the training input <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> (see equation <a class="url" href="#eq-forwardprop-layered">8.10</a>). The GT output corresponding to <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is the constant vector <i class="timesitalic">»≥</i>. The closer <span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span> is to <i class="timesitalic">»≥</i>, the smaller the loss. Note that we need to average the loss over the entire training data set. Here we are showing the loss computation for a single training data instance. The computation simply needs to be replicated for each training data instance, and the results averaged.</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Auxiliary variables</i>‚ÄîNow that a layer has multiple neurons, we have one auxiliary variable per neuron. Thus the auxiliary variable has a subscript identifying the specific neuron in that layer. It continues to have a superscript indicating its layer. We define</p>
</li>
</ul><!--<p class="body-ind"><span class="times">$$\delta^{\left( l\right)}_{j} = \frac{ \partial
\mathbb{L} }{ \partial z^{ \left( l \right) }_{j} }\;\;\;\forall  j  \in
\{ 0 \cdots \text{ number of neurons in layer $l$} \}, \forall l \in \{ 0 \cdots L \}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="64" src="../../OEBPS/Images/eq_08-25-a.png" width="530"/></p>
</div>
<div class="sgc">
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Auxiliary variable</i> <i class="fm-italics">for the last layer</i></p>
</li>
</ul>
</div><!--<p class="body-ind2"><span class="times">$$\begin{aligned}
\delta^{\left( L\right)}_{j} = \frac{ \partial \mathbb{L} }{ \partial z^{ \left( L \right) }_{j} } = \frac{ \partial \mathbb{L} }{ \partial a^{ \left( L \right) }_{j} } \frac{ \partial a^{ \left( L \right) }_{j}
}{ \partial z^{ \left( L \right) }_{j} }\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure4"><img alt="" class="calibre5" height="77" src="../../OEBPS/Images/eq_08-25-b.png" width="208"/></p>
</div>
<p class="body-ind1">Using equation <a class="url" href="#eq-loss-mlp">8.25</a> and observing that only one of the terms in the summation‚Äîthe <i class="timesitalic">j</i>th term‚Äîwill survive the differentiation with respect to <span class="math"><i class="fm-italics">a<sub class="fm-subscript">j</sub></i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup></span> since the <i class="timesitalic">a<sub class="fm-subscript">j</sub></i>s are independent of each other), we get</p><!--<p class="body-ind2"><span class="times">$$\frac{
\partial \mathbb{L} } { \partial a^{ \left( L \right) }_{j} } = \left( a^{ \left( L \right) }_{j}  - \bar{y}_{j} \right)$$</span></p>-->
<div class="figure">
<p class="figure4"><img alt="" class="calibre5" height="57" src="../../OEBPS/Images/eq_08-25-c.png" width="148"/></p>
</div>
<p class="body-ind1">Also, using the lower-left equation from <a class="url" href="../Text/08.xhtml#eq-MLP-out-nosubscript-1">8.24</a> and equation <a class="url" href="../Text/08.xhtml#eq-sigmoid-derivative">8.5</a>, we get</p><!--<p class="body-ind2"><span class="times">$$\frac{ \partial a^{ \left( L \right)
}_{j} }{ \partial z^{ \left( L \right) }_{j} } = \frac{d \sigma\left( z^{\left( L \right)}_{j} \right) }{d z^{ \left( L \right) }_{j}} = a^{\left( L \right)}_{j}  \left( 1 - a^{\left( L
\right)}_{j}   \right)$$</span></p>-->
<div class="figure">
<p class="figure4"><img alt="" class="calibre5" height="74" src="../../OEBPS/Images/eq_08-25-d.png" width="273"/></p>
</div>
<p class="body-ind1">Combining these, we get</p><!--<p class="body-ind2"><span class="times">$$\begin{aligned}
&amp;\delta^{\left( L \right)}_{j} = \left( a^{ \left( L \right)
}_{j}  - \bar{y}_{j} \right) a^{ \left( L \right) }_{j} \left( 1 - a^{\left( L \right)}_{j}  \right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure4"><img alt="" class="calibre5" height="40" src="../../OEBPS/Images/eq_08-26.png" width="248"/></p>
</div>
<p class="fm-equation-caption1">Equation 8.26 <span class="calibre" id="eq-auxvar-last-layer-scalar"/></p><!--<p class="body-ind2"><span class="times">$$\begin{aligned}
&amp;\vec{\delta}^{\left( L \right)} = \left( \vec{a}^{ \left( L \right)
}  - \bar{y} \right) \circ \vec{a}^{\left( L \right)} \circ \left(
\vec{1} - \vec{a}^{\left( L \right)}  \right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure4"><img alt="" class="calibre5" height="43" src="../../OEBPS/Images/eq_08-27.png" width="267"/></p>
</div>
<p class="fm-equation-caption1">Equation 8.27 <span class="calibre" id="eq-auxvar-last-layer-vector"/></p>
<p class="body-ind1">Here, <span class="math">‚àò</span> denotes the Hadamard product between two vectors. It is basically a vector of elementwise products of corresponding vector elements. Thus,</p><!--<p class="body-ind2"><span class="times">$$\begin{aligned}
\vec{a} &amp;= \begin{bmatrix} a_{0}\\  a_{1} \\ \vdots \\ a_{n}
\end{bmatrix}\qquad\qquad\qquad
\vec{b} = \begin{bmatrix} b_{0}\\  b_{1} \\ \vdots \\ b_{n}
\end{bmatrix}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure4"><img alt="" class="calibre5" height="135" src="../../OEBPS/Images/eq_08-28.png" width="234"/></p>
</div>
<p class="fm-equation-caption1">Equation 8.28</p><!--<p class="body-ind2"><span class="times">$$\begin{aligned}
\vec{a} \circ \vec{b} &amp;= \begin{bmatrix} a_{0} b_{0} \\  a_{1} b_{1}  \\ \vdots \\ a_{n} b_{n}
\end{bmatrix}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure4"><img alt="" class="calibre5" height="138" src="../../OEBPS/Images/eq_08-29.png" width="101"/></p>
</div>
<p class="fm-equation-caption1">Equation 8.29</p>
<p class="body-ind1"><a id="marker-292"/>Equations <a class="url" href="#eq-auxvar-last-layer-scalar">8.26</a> and <a class="url" href="#eq-auxvar-last-layer-vector">8.27</a> are identical. The former is a scalar equation expressing individual auxiliary variables of the last layer. The latter is a vector equation expressing all the auxiliary variables of the last layer together. We can compute these directly if we have performed a forward pass and have its results, the <span class="math"><i class="fm-italics">a<sub class="fm-subscript">j</sub></i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup></span>s available along with the training data GT.</p>
<div class="sgc">
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Auxiliary variable for an arbitrary layer, l</i>‚ÄîThis is significantly different and harder to understand than the one-neuron-per-layer case. We are trying to evaluate <span class="math"><i class="fm-italics">Œ¥<sub class="fm-subscript">j</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup> = <i class="fm-italics">‚àÇ</i><span class="segoe">ùïÉ</span><b class="fm-bold">/</b><i class="fm-italics">‚àÇz<sub class="fm-subscript">j</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span> in the general case: that is, for an arbitrary layer <i class="timesitalic">l</i>. The loss does not <i class="fm-italics">directly</i> depend on the inner layer variable <span class="math"><i class="fm-italics">z<sub class="fm-subscript">j</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>. The loss directly depends only on the last layer activations, which depend on the previous layer, and so forth. The <i class="timesitalic">z</i>s in any one layer form a <i class="fm-italics">complete</i> dependency set for the loss <span class="segoe">ùïÉ</span>, meaning the loss can be expressed in terms of only these and no other variables. In particular, we can express the loss as <span class="math"><span class="segoe">ùïÉ</span>(<i class="fm-italics">z</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">(<i class="fm-italics1">l</i>+1)</sup>, <i class="fm-italics">z</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">(<i class="fm-italics1">l</i>+1)</sup>, <i class="fm-italics">z</i><sub class="fm-subscript">2</sub><sup class="fm-superscript">(<i class="fm-italics1">l</i>+1)</sup>,‚ãØ)</span>. You can form a mental picture that <span class="math"><i class="fm-italics">z<sub class="fm-subscript">j</sub></i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span> fans out to <span class="segoe">ùïÉ</span> <i class="fm-italics">through</i> all the <i class="timesitalic">z</i>s in the next layer, <span class="math"><i class="fm-italics">z</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">(<i class="fm-italics1">l</i>+1)</sup></span>, <span class="math"><i class="fm-italics">z</i><sub class="fm-subscript">1</sub><sup class="fm-superscript">(<i class="fm-italics1">l</i>+1)</sup></span>, <span class="math"><i class="fm-italics">z</i><sub class="fm-subscript">2</sub><sup class="fm-superscript">(<i class="fm-italics1">l</i>+1)</sup></span>, and so on. Then, using the chain rule of partial differentiation,</p>
</li>
</ul>
</div><!--<p class="body-ind2"><span class="times">$$\begin{aligned}
&amp;\delta^{\left( l\right)}_{j} = \frac{ \partial \mathbb{L}\left( z^{
\left( l + 1 \right) }_{0}, z^{ \left( l + 1 \right) }_{1}, z^{ \left( l
+ 1 \right) }_{2}, \cdots \right)  }{ \partial z^{ \left( l \right)
}_{j} }
= \sum_{k} \frac{ \partial \mathbb{L} }{ \partial z^{ \left( l + 1
\right) }_{k} } \frac{ \partial z^{ \left( l + 1 \right) }_{k}  }{
\partial z^{ \left( l \right) }_{j} } \\
&amp;= \sum_{k} \frac{ \partial \mathbb{L} }{ \partial z^{ \left( l + 1
\right) }_{k} } \frac{ \partial z^{ \left( l + 1 \right) }_{k}  }{
\partial a^{ \left( l \right) }_{j} } \frac{ \partial a^{ \left( l
\right) }_{j}  }{ \partial z^{ \left( l \right) }_{j}
}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure4"><img alt="" class="calibre5" height="79" src="../../OEBPS/Images/eq_08-29-a.png" width="627"/></p>
</div>
<p class="body-ind1">Now, by definition,</p><!--<p class="body-ind2"><span class="times">$$\frac{ \partial \mathbb{L} }{ \partial z^{ \left( l + 1 \right) }_{k} } = \delta^{\left( l + 1 \right)}_{k}$$</span></p>-->
<div class="figure">
<p class="figure4"><img alt="" class="calibre5" height="59" src="../../OEBPS/Images/eq_08-29-b.png" width="121"/></p>
</div>
<p class="body-ind1">And using equation <a class="url" href="../Text/08.xhtml#eq-MLP-out-nosubscript-1">8.24</a>,</p><!--<p class="body-ind2"><span class="times">$$\frac{ \partial z^{ \left( l + 1 \right)
}_{k}  }{ \partial a^{ \left( l \right) }_{j} } = w^{\left( l + 1
\right)}_{kj}$$</span></p>-->
<div class="figure">
<p class="figure4"><img alt="" class="calibre5" height="70" src="../../OEBPS/Images/eq_08-29-c.png" width="121"/></p>
</div>
<p class="body-ind1">while</p><!--<p class="body-ind2"><span class="times">$$\frac{
\partial a^{ \left( l \right) }_{j}  }{ \partial z^{ \left( l \right)
}_{j} } = \frac{d \sigma\left( z^{\left( l \right)}_{j} \right) }{d z^{
\left( l \right) }_{j}} = a^{\left( l \right)}_{j} \left( 1 - a^{\left( l \right)}_{j}  \right)$$</span></p>-->
<div class="figure">
<p class="figure4"><img alt="" class="calibre5" height="79" src="../../OEBPS/Images/eq_08-29-d.png" width="257"/></p>
</div>
<p class="body-ind1">Combining all these, we get the scalar expression for a single auxiliary variable. It is presented here along with its equivalent vector equation for the entire layer:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp; \delta^{\left( l \right)}_{j} = \sum_k \delta^{ \left( l + 1
\right) }_{k}  w^{ \left( l + 1 \right) }_{kj} a^{ \left( l \right)
}_{j} \left( 1 - a^{\left( l \right)}_{j}  \right)\\
&amp; \vec{\delta}^{ \left( l \right) } =
\left(
\left(  W^{ \left( l+1 \right) } \right)^{T}
\vec{ \delta }^{ \left( l+1 \right) }
\right)
\circ
\vec{a}^{ \left( l \right) } \circ \left( \vec{1} -  \vec{a}^{ \left( l
\right) } \right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure4"><img alt="" class="calibre5" height="51" src="../../OEBPS/Images/eq_08-30.png" width="270"/></p>
</div>
<p class="fm-equation-caption1">Equation 8.30 <span class="calibre" id="eq-auxvar-scalar"/></p>
<div class="figure">
<p class="figure4"><img alt="" class="calibre5" height="57" src="../../OEBPS/Images/eq_08-31.png" width="319"/></p>
</div>
<p class="fm-equation-caption1">Equation 8.31 <span class="calibre" id="eq-auxvar-vector"/></p>
<p class="body-ind1"><a id="marker-293"/>Here, <span class="math">‚àò</span> denotes the Hadamard multiplication explained earlier and <span class="math"><i class="fm-italics">W</i><sup class="fm-superscript">(+1<i class="fm-italics1">l</i>)</sup></span> is the matrix representing the weights of <i class="fm-italics">all connections from layer l to layer <span class="math">(<i class="fm-italics">l</i>+1)</span></i> (see equation <a class="url" href="../Text/08.xhtml#eq-MLP-weight-matrix">8.8</a>). Equations <a class="url" href="#eq-auxvar-scalar">8.30</a> and <a class="url" href="../Text/08.xhtml#eq-auxvar-vector">8.31</a> allow us to evaluate <span class="math"><i class="fm-italics">Œ¥</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>s from the <span class="math"><i class="fm-italics">Œ¥</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>+1)</sup></span>s if the results of forward propagation (<i class="timesitalic">a</i>s) are available. We have already shown that the auxiliary variables for the last layer are directly computable from the activations of that layer. Hence, we can evaluate all the layers‚Äô auxiliary variables.</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="fm-italics">Derivatives of loss with respect to weights and biases in terms of auxiliary variables</i>‚ÄîWe have already seen how to compute auxiliary variables. Now we will express the partial derivatives of loss with respect to weights and biases in terms of those. This will provide us with the gradients we need to update the weights and biases along the negative gradient, which is the optimal move to minimize loss:</p>
</li>
</ul><!--<p class="body-ind"><span class="times">$$\begin{aligned}
&amp;\frac{ \partial \mathbb{L} }{ \partial w^{ \left( l \right) }_{jk}
} = \frac{ \partial \mathbb{L} }{ \partial z^{ \left( l \right) }_{j} }
\frac{ \partial z^{ \left( l \right) }_{j}  }{ \partial w^{ \left( l
\right) }_{jk} } =  \delta^{\left( l \right)}_{j} a^{ \left( l-1 \right)
}_{k} \\
&amp; \nabla_{ w^{ \left( l \right) } } \mathbb{L} = \vec{ \delta }^{
\left( l \right) } \left( \vec{ a }^{ \left( l - 1 \right) }
\right)^{T}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="72" src="../../OEBPS/Images/eq_08-32.png" width="239"/></p>
</div>
<p class="fm-equation-caption">Equation 8.32 <span class="calibre" id="eq-partialderiv-loss-wt-scalar"/></p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="48" src="../../OEBPS/Images/eq_08-33.png" width="172"/></p>
</div>
<p class="fm-equation-caption">Equation 8.33 <span class="calibre" id="eq-partialderiv-loss-wt-vector"/></p>
<p class="body-ind">Equations <a class="url" href="#eq-partialderiv-loss-wt-scalar">8.32</a> and <a class="url" href="../Text/08.xhtml#eq-partialderiv-loss-wt-vector">8.33</a> are equivalent. The first is scalar and pertains to individual weights in layer <i class="timesitalic">l</i>, and the second describes the entire layer. Similarly, equations <a class="url" href="#eq-partialderiv-loss-bias-scalar">8.34</a> and <a class="url" href="#eq-partialderiv-loss-bias-vector">8.35</a> are equivalent:</p><!--<p class="body-ind"><span class="times">$$\begin{aligned}
&amp;\frac{ \partial \mathbb{L} }{ \partial b^{ \left( l \right) }_{j} }
= \frac{ \partial \mathbb{L} }{ \partial z^{ \left( l \right) }_{j} }
\frac{ \partial z^{ \left( l \right) }_{j}  }{ \partial b^{ \left( l
\right) }_{j} } =  \delta^{\left( l \right)}_{j}  \\
&amp; \nabla_{ b^{ \left( l \right) } } \mathbb{L} = \vec{ \delta }^{
\left( l \right) }\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="71" src="../../OEBPS/Images/eq_08-34.png" width="189"/></p>
</div>
<p class="fm-equation-caption">Equation 8.34 <span class="calibre" id="eq-partialderiv-loss-bias-scalar"/></p>
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="31" src="../../OEBPS/Images/eq_08-35.png" width="96"/></p>
</div>
<p class="fm-equation-caption">Equation 8.35 <span class="calibre" id="eq-partialderiv-loss-bias-vector"/></p>
<p class="body-ind">The first is scalar and pertains to individual biases in layer <i class="timesitalic">l</i>, and the second describes the entire layer.</p>
<h3 class="fm-head1" id="putting-it-all-together-overall-training-algorithm">8.4.6 Putting it all together: Overall training algorithm</h3>
<p class="body"><a id="marker-294"/>Previously, we discussed forward propagation: passing an input vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> through a sequence of linear layers and generating an output prediction. We learned about MSE loss, <span class="segoe">ùïÉ</span>, which calculates the deviation of the output prediction from the GT, <i class="timesitalic">y</i>. We also learned to compute the gradients of <span class="segoe">ùïÉ</span> with respect to parameters <i class="timesitalic">W</i> and <i class="timesitalic">b</i> using backpropagation. In the following algorithm, we describe how these components come together in the training process:</p>
<div class="calibre3">
<p class="fm-algorithm-caption">Algorithm 8.5 Training a neural network</p>
<p class="algorithm-body">Initialize <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span>, <i class="fm-italics">b</i></span> with random values</p>
<p class="algorithm-body"><b class="fm-bold">while</b> <span class="math"><span class="segoe">ùïÉ</span> &gt;</span> <i class="fm-italics">threshold</i> <b class="fm-bold">do</b></p>
<p class="algorithm-body">¬†¬†<span class="segoe">‚ä≥</span> Forward pass</p>
<p class="algorithm-body">¬†¬†¬†¬†¬†¬†¬†¬†<b class="fm-bold">for</b> <span class="math"><i class="fm-italics">l</i> ‚Üê 0</span> to <i class="timesitalic">L</i> <b class="fm-bold">do</b></p>
<p class="algorithm-body">¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†<span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup> = <i class="fm-italics">W</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">l</i>‚Äì1)</sup> + <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span></p>
<p class="algorithm-body">¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†<span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup> = <i class="fm-italics">œÉ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_z.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup>)</span></p>
<p class="algorithm-body">¬†¬†¬†¬†¬†¬†¬†¬†<b class="fm-bold">end</b> <b class="fm-bold">for</b></p>
<p class="algorithm-body">¬†¬†<span class="segoe">‚ä≥</span> Loss</p>
<p class="algorithm-body">¬†¬†¬†¬†¬†¬†¬†¬†<span class="math"><span class="segoe">ùïÉ</span> = 1/2 ||<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup> ‚Äì <i class="fm-italics">»≥</i>||<sup class="fm-superscript">2</sup></span></p>
<p class="algorithm-body">¬†¬†<span class="segoe">‚ä≥</span> Gradients for the last layer</p>
<p class="algorithm-body">¬†¬†¬†¬†¬†¬†¬†¬†<span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span> <sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup> = (<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup> ‚Äì <i class="fm-italics">»≥</i>) ‚àò <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup> ‚àò (<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_1.png" width="14"/></span> ‚Äì <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup>)</span></p>
<p class="algorithm-body">¬†¬†¬†¬†¬†¬†¬†¬†<span class="math">‚ñΩ<i class="fm-italics">W</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup><span class="segoe">ùïÉ</span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span> <sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">L</i>‚Äì1)</sup>)<sup class="superscript-italic">T</sup></span></p>
<p class="algorithm-body">¬†¬†¬†¬†¬†¬†¬†¬†<span class="math">‚ñΩ<i class="fm-italics">b</i><sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup><span class="segoe">ùïÉ</span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span> <sup class="fm-superscript">(<i class="fm-italics1">L</i>)</sup></span></p>
<p class="algorithm-body">¬†¬†<span class="segoe">‚ä≥</span> Gradients for the remaining layers</p>
<p class="algorithm-body">¬†¬†¬†¬†¬†¬†¬†¬†<b class="fm-bold">for</b> <span class="math"><i class="fm-italics">l</i> ‚Üê <i class="fm-italics">L</i> ‚Äì 1</span> to <span class="math">0</span> <b class="fm-bold">do</b></p>
<p class="algorithm-body">¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†<span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span> <sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup> = <span class="large">(</span>(<i class="fm-italics">W</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>+1)</sup>)<i class="fm-italics"><sup class="fm-superscript">T</sup></i> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span><sup class="fm-superscript">(<i class="fm-italics1">l</i>+1)</sup><span class="large">)</span> <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup> ‚àò (<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_1.png" width="14"/></span> ‚Äì <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup>)</span></p>
<p class="algorithm-body">¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†<span class="math">‚ñΩ<i class="fm-italics">W</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup><span class="segoe">ùïÉ</span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span> <sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup> (<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">l</i>‚Äì1)</sup>)<sup class="superscript-italic">T</sup></span></p>
<p class="algorithm-body">¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†<span class="math">‚ñΩ<i class="fm-italics">b</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup><span class="segoe">ùïÉ</span> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_delta.png" width="26"/></span> <sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span></p>
<p class="algorithm-body">¬†¬†¬†¬†¬†¬†¬†¬†<b class="fm-bold">end</b> <b class="fm-bold">for</b></p>
<p class="algorithm-body">¬†¬†<span class="segoe">‚ä≥</span> Parameter update</p>
<p class="algorithm-body">¬†¬†¬†¬†¬†¬†¬†¬†<span class="math"><i class="fm-italics">W</i> = <i class="fm-italics">W</i> ‚Äì <i class="fm-italics">r</i>‚ñΩ<i class="fm-italics"><sub class="fm-subscript">W</sub></i><span class="segoe">ùïÉ</span></span></p>
<p class="algorithm-body">¬†¬†¬†¬†¬†¬†¬†¬†<span class="math"><i class="fm-italics">b</i> = <i class="fm-italics">b</i> ‚Äì <i class="fm-italics">r</i>‚ñΩ<i class="fm-italics"><sub class="fm-subscript">b</sub></i><span class="segoe">ùïÉ</span></span></p>
<p class="algorithm-body"><b class="fm-bold">end</b> <b class="fm-bold">while</b></p>
</div>
<h2 class="fm-head" id="training-a-neural-network-in-pytorch">8.5 Training a neural network in PyTorch</h2>
<p class="body"><a id="marker-295"/>Now that we‚Äôve seen how the training process works, let‚Äôs look at how this can be implemented in PyTorch. For this purpose, let‚Äôs take the following example. Consider an e-commerce company that‚Äôs trying to solve the problem of demand prediction: the company would like to estimate the number of mobile phones that will be sold in the upcoming week so that it can manage its inventory accordingly. Our goal is to develop a model that can make such a prediction. Let‚Äôs assume that the demand for a given week is a function of three variables: (a) the number of mobile phones sold in the previous week, (b) discounts offered, and (c) the number of weeks to the next holiday. Let‚Äôs call these variables <code class="fm-code-in-text">prev_week_sales</code>, <code class="fm-code-in-text">discount_fraction</code>, and <code class="fm-code-in-text">weeks_to_next_holidays</code>, respectively. This example can be modeled as a regression problem wherein we predict the number of mobile phones sold in the upcoming week from an input vector of the form [<code class="fm-code-in-text">prev_week_sales</code>, <code class="fm-code-in-text">discount_fraction</code>, <code class="fm-code-in-text">weeks_to_next_holidays</code>].</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for this section, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/O1Ra">http://mng.bz/O1Ra</a>.</p>
<p class="body">From historical data, we generate a large data set, <code class="fm-code-in-text">X</code>, that contains the values of the three variables for the last <i class="timesitalic">N</i> weeks. <code class="fm-code-in-text">X</code> is represented as an <i class="timesitalic">N</i> x <span class="math">3</span> matrix, with each row representing an individual training data instance and <i class="timesitalic">N</i> being the total number of data points available. We also have a GT vector <i class="timesitalic">»≥</i> of length <i class="timesitalic">N</i>, containing the actual sales of mobile phones for each of the weeks in the training data set. Table <a class="url" href="#tab-demand-prediction-data-set">8.1</a> shows sample data points from our training set.</p>
<p class="fm-table-caption">Table 8.1 Sample training data for demand prediction</p>
<table border="1" class="contenttable-1-table" id="tab-demand-prediction-data-set" width="100%">
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th">
<p class="fm-table-head">Previous week sales</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Discount fraction (%)</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Weeks to next holidays</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Number of units sold</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">76,440</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">63</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">2</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">94,182</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">41,512</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">50</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">3</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">51,531</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">77,395</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">77</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">9</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">95,938</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">‚Ä¶</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">‚Ä¶</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">‚Ä¶</span></p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body"><span class="math">‚Ä¶</span></p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">21,532</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">70</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">4</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">28,559</p>
</td>
</tr>
</tbody>
</table>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> In this section, <code class="fm-code-in-text">X</code> and <i class="timesitalic">»≥</i> refer to the entire batch of training data instances. This may be infeasible in practical settings because of large data sets. To address this, we typically use mini-batches of <code class="fm-code-in-text">X</code> and <i class="timesitalic">»≥</i>. We introduce the concept of mini-batches formally in the next chapter.</p>
<p class="body">One important point about the data set is that the range of values for each feature is completely different. For example, the previous week‚Äôs sales are expressed as a number on the order of tens of thousands of units, whereas the discount fraction is a percentage number between 0 and 100. In machine learning, it is a good practice to bring all the values to a common scale, because doing so can help improve the speed of training and reduce the chance of getting stuck at a local minimum. For our example, let‚Äôs use min-max normalization to scale all the features to 0‚Äì1. The following code snippet shows how to perform min-max normalization in PyTorch. For the rest of the discussion, we assume that we are operating on the normalized data:<a id="marker-296"/></p>
<pre class="programlisting">def min_max_norm(X, y):
    X, y = X.clone(), y.clone()              <span class="fm-combinumeral">‚ë†</span>
    X_min, X_max = torch.min(X, dim=0)[0], 
            torch.max(X, dim=0)[0]           <span class="fm-combinumeral">‚ë°</span>
    X = (X - X_min) / (X_max - X_min)        <span class="fm-combinumeral">‚ë¢</span>
    y_min, y_max = torch.min(y, dim=0)[0], 
            torch.max(y, dim=0)[0]           <span class="fm-combinumeral">‚ë£</span>
    y = (y - y_min) / (y_max - y_min)        <span class="fm-combinumeral">‚ë§</span>
    return X, y</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Clones the data so as not to mutilate the original data</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Calculates the min and max values of each column of <i class="timesitalic">X</i></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¢</span> Normalizes <i class="timesitalic">X</i></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë£</span> Calculates the min and max values of <i class="timesitalic">y</i></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë§</span> Normalizes <i class="timesitalic">y</i></p>
<p class="body">To solve the regression problem, let‚Äôs first define a two-layer neural network model that can take in <span class="math">3</span>D input vectors of the form [<code class="fm-code-in-text">prev_week_sales</code>, <code class="fm-code-in-text">discount_fraction</code>, <code class="fm-code-in-text">is_holidays_ongoing</code>] and generate output predictions. The following code snippet gives the PyTorch implementation:</p>
<pre class="programlisting">class TwoLayeredNN(torch.nn.Module):
    def __init__(self, input_size, hidden1_size, hidden2_size, output_size):
        super(TwoLayeredNN, self).__init__()

        self.model = torch.nn.Sequential(                  <span class="fm-combinumeral">‚ë†</span>

            torch.nn.Linear(input_size, hidden1_size),     <span class="fm-combinumeral">‚ë°</span>
            torch.nn.Sigmoid(),

            torch.nn.Linear(hidden1_size, hidden2_size),   <span class="fm-combinumeral">‚ë¢</span>
            torch.nn.Sigmoid(),

            torch.nn.Linear(hidden2_size, output_size)     <span class="fm-combinumeral">‚ë£</span>
        )

    def forward(self, X):                                  <span class="fm-combinumeral">‚ë§</span>
        return self.model(X)

nn = TwoLayeredNN(input_size=X.shape[-1], hidden1_size=10,
                                  hidden2_size=5, output_size=1)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Defines the network as a sequence of linear and sigmoid layers</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> First hidden layer with a weight matrix of size input_size <span class="math">√ó</span> hidden1_size)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¢</span> Second hidden layer with a weight matrix of size hidden1_size <span class="math">√ó</span> hidden2_size)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë£</span> Output layer with a weight matrix of size hidden2_size <span class="math">√ó</span> output_size)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë§</span> <i class="timesitalic">X</i> is an <span class="math"><i class="fm-italics">N</i> √ó 3</span> matrix. Each row is a (<span class="math">3</span>D vector) representing a single data point.</p>
<p class="body">Neural network models in PyTorch should subclass <code class="fm-code-in-text">torch.nn.Module</code> and implement the <code class="fm-code-in-text">forward</code> method. Our two-layer neural network contains two linear layers, each followed by a sigmoid (nonlinear) activation layer. Finally, we have a linear layer that converts the final activation into the output prediction. These layers are chained together using the <code class="fm-code-in-text">torch.nn.Sequential</code> class to form the two-layer neural network. Whenever our model is called using <code class="fm-code-in-text">nn(X)</code>, the <code class="fm-code-in-text">forward</code> method is invoked, and the input <code class="fm-code-in-text">X</code> is passed through the individual layers to obtain the final output.</p>
<p class="body">Now that we have defined the neural network and its forward pass, we need to define the loss function. We can use the MSE loss defined in equation <a class="url" href="../Text/08.xhtml#eq-mse-loss">8.11</a>. The loss function essentially compares the demand predicted by the neural network model with the actual demand from the GT and returns larger values when the difference is higher and smaller values when the difference is lower. MSE loss is readily available in PyTorch through the <code class="fm-code-in-text">torch.nn.MSELoss</code> class. The following code snippet shows a sample invocation:</p>
<pre class="programlisting">loss = torch.nn.MSELoss() <span class="fm-combinumeral">‚ë†</span>

loss(y_pred, y_gt)        <span class="fm-combinumeral">‚ë°</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Instantiates the loss function</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> compute loss _pred: Output of the neural network _gt: ground truth</p>
<p class="body">Finally, we need a way to compute the gradients of the loss with respect to the parameters of our model so we can start the training process. Luckily, we don‚Äôt have to explicitly compute the gradients ourselves because PyTorch automatically does this for us using automatic differentiation, aka autograd. (Refer to section <a class="url" href="../Text/03.xhtml#sec3_1">3.1</a> for more details about autograd.) For our current example, we can instruct PyTorch to run backpropagation and compute gradients by calling <code class="fm-code-in-text">loss.backward()</code>. With this, we‚Äôre ready to start training. PyTorch code for training the neural network is shown next.<a id="marker-297"/></p>
<p class="fm-code-listing-caption" id="listing-8.4-training-a-neural-network">Listing 8.4 Training a neural network</p>
<pre class="programlisting">nn = TwoLayeredNN(input_size=X.shape[-1],             <span class="fm-combinumeral">‚ë†</span>
                  hidden1_size=10,
                  hidden2_size=5,
                  output_size=1)
loss = torch.nn.MSELoss()                             <span class="fm-combinumeral">‚ë°</span>
optimizer = torch.optim.SGD(nn.parameters(), lr=0.2,  <span class="fm-combinumeral">‚ë¢</span>
                            momentum=0.9)
num_iters = 1000

for i in range(num_iters):                            <span class="fm-combinumeral">‚ë£</span>
    y_out = nn(X)                                     <span class="fm-combinumeral">‚ë§</span>
    mse_loss = loss(y_out, y)                         <span class="fm-combinumeral">‚ë•</span>
    optimizer.zero_grad()                             <span class="fm-combinumeral">‚ë¶</span>
    mse_loss.backward()                               <span class="fm-combinumeral">‚ëß</span>
    optimizer.step()                                  <span class="fm-combinumeral">‚ë®</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Instantiates the neural network</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Instantiates the loss function</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¢</span> Instantiates the optimizer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë£</span> Training loop</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë§</span> Forward pass</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë•</span> Computes the loss</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¶</span> Clears the gradients and prevents accumulation of gradients from the previous step</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ëß</span> Runs backpropagation (computes gradients)</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë®</span> Updates the weights</p>
<p class="body">In the training loop, we iteratively run the forward pass, compute the loss, calculate the gradients, and update the weights. The neural network is initialized with random weights and hence makes arbitrary predictions for the demand in the early iterations of the training loop. This translates to a high initial loss value. However, as training proceeds, the weights are updated to minimize the loss value, and the predicted demand comes closer to the actual GT. To update the weights, we use what is known as an <i class="fm-italics">optimizer</i>. During training, the gradients are computed by calling the <code class="fm-code-in-text">backward()</code> function on the <code class="fm-code-in-text">loss</code> object. Following that, the <code class="fm-code-in-text">optimizer.step</code> call updates the weights and biases. In this example, we used the stochastic gradient descent‚Äìbased optimizer, which can be invoked using <code class="fm-code-in-text">torch.optim.SGD</code>. PyTorch offers various optimizers, such as Adam, AdaGrad, and so on, which will be discussed in detail in the next chapter. We typically run the training loop until the loss reaches a value low enough to be acceptable. Once the training loop completes, we have a model that can readily take in new data points and generate output predictions.<a id="marker-298"/></p>
<h2 class="fm-head" id="summary-7">Summary</h2>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The sigmoid function <span class="math"><i class="fm-italics">œÉ</i>(<i class="fm-italics">x</i>) = 1/1+<i class="fm-italics">e<sup class="fm-superscript">‚Äìx</sup></i></span> has an S-shaped graph, is a differential version of the Heaviside step function, and, as such, is used in perceptrons. Thus the overall perceptron function becomes <span class="math"><i class="fm-italics">P</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>) ‚â° <i class="fm-italics">œÉ</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup></i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> + <i class="fm-italics">b</i>)</span>. It is parametrized by <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_w.png" width="15"/></span> and <i class="timesitalic">b</i>, which control the slope and position, respectively, of the S-shaped curve.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Neural networks solve real-life problems that require intelligence by approximating the function that solves the problem in question. They are built of multiple perceptrons interconnected by weighted edges. Instead of connecting perceptrons haphazardly, we connect them as layers. In a layered network, a perceptron is only connected to perceptrons from the immediately preceding layer. Intra-layer and other connections are not allowed.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Supervised neural networks have manually generated outputs for a sample set of input values (ground truth). This entire data set consisting of inputs and known outputs is known as the training data set.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Loss is defined as the mismatch between the ground truth and actual output generated by the neural network on training data inputs. The simplest way to compute loss is to take the Euclidean distance between the neural network-generated output and ground-truth vectors. This is called the MSE (mean squared error) loss. Mathematically, given a training data set <span class="math">ùïã</span> that is a set of &lt;input, GT output&gt; pairs <span class="math">ùïã = {<span class="segoe">‚ü®</span><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>, <i class="fm-italics">»≥</i><span class="segoe">‚ü©</span>}</span>, the loss can be expressed as</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\mathbb{L} =\frac{1}{2}\sum_{x_{i} \in \mathbb{T}} \left( \vec{y} - \bar{y}
\right)^{2}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="55" src="../../OEBPS/Images/eq_08-35-a.png" width="138"/></p>
</div>
<p class="body-ind">where the output is <span class="math"><span class="infigure"><img alt="" class="calibre15" height="29" src="../../OEBPS/Images/AR_y.png" width="15"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub></i> = <i class="fm-italics">MLP</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub></i>)</span>.</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Training is the process of optimizing the connection weights and biases of a specific neural network so that the loss is minimal. Note that during inferencing, the neural network typically sees data it has never seen during training. Inferencing outputs are good only if the distribution of training inputs roughly matches the overall input distribution.</p>
</li>
<li class="fm-list-bullet">
<p class="list">We minimize the loss by iteratively adjusting the weights and biases. The quickest way to reach the closest minimum of a multivariate function is to follow the gradient. Hence, we adjust the weights and biases following the gradient of the loss function. Mathematically,</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;W = W  - r \nabla_{W}\mathbb{L} \\
&amp;b = b  - r \nabla_{b}\mathbb{L}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="63" src="../../OEBPS/Images/eq_08-35-b.png" width="127"/></p>
</div>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">A forward pass is the process of generating outputs from inputs with a neural network: more specifically, a multilayer perceptron MLP). Thus an MLP does inferencing via a forward pass. A beautiful property of a layered network is that we can do a forward pass dealing with one layer at a time, proceeding iteratively from layer <span class="math">0</span> (closest to the input) to the output layer. Mathematically,</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\vec{z}^{\left(l\right)} = W^{\left(l\right)}  \vec{a}^{\left(l-1\right)} +
\vec{b}^{\left(l\right)} \\
&amp;\vec{a}^{\left(l\right)} = \sigma\left( \vec{z}^{\left(l\right)}
\right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="79" src="../../OEBPS/Images/eq_08-35-c.png" width="176"/></p>
</div>
<p class="body-ind">where <span class="math"><i class="fm-italics">W</i><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>, <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_b.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span> represent the weights and biases for layer <i class="timesitalic">l</i>, and <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span> represent the output for layer <i class="timesitalic">l</i> activation), which is also the input for layer <span class="math"><i class="fm-italics">l</i> + 1</span>.</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">A backward pass is the process by which the gradients of the loss with respect to all the weights and biases are generated. It relies on the result of the preceding forward pass and proceeds from the output layer toward the input layer. It uses auxiliary variables <span class="math"><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_bw.png" width="15"/></span><sup class="fm-superscript">(<i class="fm-italics1">l</i>)</sup></span>, which can be computed by iterating backward from the last (closest to output) layer to the first (closest to input) layer‚Äîhence the name <i class="fm-italics">backward propagation</i>‚Äîand all the required gradients can be computed from those auxiliary variables. Mathematically,</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\vec{\delta}^{ \left( l \right) } = \left(\left(  W^{ \left( l+1
\right) } \right)^{T}  \vec{ \delta }^{ \left( l+1 \right) }\right)
\circ \vec{a}^{ \left( l \right) } \circ \left( \vec{1} -  \vec{a}^{
\left( l \right) } \right) \\
&amp;\nabla_{ W^{ \left( l \right) } } \mathbb{L} = \vec{ \delta }^{
\left( l \right) } \left( \vec{ a }^{ \left( l - 1 \right) } \right)^{T}
\\
&amp;\nabla_{ b^{ \left( l \right) } } \mathbb{L} = \vec{ \delta }^{
\left( l \right) }\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="125" src="../../OEBPS/Images/eq_08-35-d.png" width="319"/></p>
</div>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">Training progresses by alternating forward and backward passes on the training data set.<a id="marker-299"/></p>
</li>
</ul>
</div></body></html>