["```py\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nimport numpy as np\nimport matplotlib.pylab as plt\nimport os\nfrom datetime import datetime\n\n(train_images, train_labels), (test_images, test_labels) = \ndatasets.cifar10.load_data()\n```", "```py\ntrain_images, test_images = train_images / 255.0, \ntest_images / 255.0\n```", "```py\nnp.unique(train_labels)\n```", "```py\narray([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)\n```", "```py\nCLASS_NAMES = ['airplane', 'automobile', 'bird', 'cat',\n               'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n```", "```py\nvalidation_dataset = tf.data.Dataset.from_tensor_slices(\n(test_images[:500], \n test_labels[:500]))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n(test_images[500:], \n test_labels[500:]))\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n(train_images,\n train_labels))\n```", "```py\ntrain_dataset_size = len(list(train_dataset.as_numpy_iterator()))\nprint('Training data sample size: ', train_dataset_size)\n\nvalidation_dataset_size = len(list(validation_dataset.\nas_numpy_iterator()))\nprint('Validation data sample size: ', \nvalidation_dataset_size)\n\ntest_dataset_size = len(list(test_dataset.as_numpy_iterator()))\nprint('Test data sample size: ', test_dataset_size)\n```", "```py\nTraining data sample size:  50000\nValidation data sample size:  500\nTest data sample size:  9500\n```", "```py\nTRAIN_BATCH_SIZE = 128\ntrain_dataset = train_dataset.shuffle(50000).batch(\nTRAIN_BATCH_SIZE, \ndrop_remainder=True)\n\nvalidation_dataset = validation_dataset.batch(\n validation_dataset_size)\ntest_dataset = test_dataset.batch(test_dataset_size)\n```", "```py\nSTEPS_PER_EPOCH = train_dataset_size // TRAIN_BATCH_SIZE\nVALIDATION_STEPS = 1\n```", "```py\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), \n      activation='relu',\n      kernel_initializer='glorot_uniform', padding='same', \n      input_shape = (32,32,3)),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), \n     activation='relu',\n      kernel_initializer='glorot_uniform', padding='same'),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(256, \n     activation='relu', kernel_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(10, activation='softmax', \n    name = 'custom_class')\n])\nmodel.build([None, 32, 32, 3])\n```", "```py\nmodel.compile(\n          loss=tf.keras.losses.SparseCategoricalCrossentropy(\n               from_logits=True),\n          optimizer='adam',\n          metrics=['accuracy'])\n```", "```py\nMODEL_NAME = 'myCIFAR10-{}'.format(datetime.now().strftime(\n\"%Y%m%d-%H%M%S\"))\nprint(MODEL_NAME)\n```", "```py\ncheckpoint_dir = './' + MODEL_NAME\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt-{epoch}\")\nprint(checkpoint_prefix)\n```", "```py\nmyCheckPoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    monitor='val_accuracy',\n    mode='max')\n```", "```py\nmyCallbacks = [\n    myCheckPoint\n]\n```", "```py\nhist = model.fit(\n    train_dataset,\n    epochs=12,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=validation_dataset,\n    validation_steps=VALIDATION_STEPS,\n    callbacks=myCallbacks).history\n```", "```py\n[0.47200000286102295,\n 0.5680000185966492,\n 0.6000000238418579,\n 0.5899999737739563,\n 0.6119999885559082,\n 0.6019999980926514,\n 0.6100000143051147,\n 0.6380000114440918,\n 0.6100000143051147,\n 0.5699999928474426,\n 0.5619999766349792,\n 0.5960000157356262]\n```", "```py\nmax_value = max(hist['val_accuracy'])\nmax_index = hist['val_accuracy'].index(max_value)\nprint('Best epoch: ', max_index + 1)\n```", "```py\nBest epoch:  8\n```", "```py\n!ls -lrt ./cifar10_training_checkpoints\n```", "```py\n!ls -lrt ./cifar10_training_checkpoints/ckpt_8\n```", "```py\nbest_only_checkpoint_dir = \n './best_only_cifar10_training_checkpoints'\nbest_only_checkpoint_prefix = os.path.join(\nbest_only_checkpoint_dir, \n\"ckpt_{epoch}\")\n\nbestCheckPoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=best_only_checkpoint_prefix,\n    monitor='val_accuracy',\n    mode='max',\n    save_best_only=True)\n```", "```py\n    bestCallbacks = [\n    bestCheckPoint\n]\n```", "```py\nbest_hist = model.fit(\n    train_dataset,\n    epochs=12,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=validation_dataset,\n    validation_steps=VALIDATION_STEPS,\n    callbacks=bestCallbacks).history\n```", "```py\n !ls -lrt ./best_only_cifar10_training_checkpoints\n```", "```py\nmyEarlyStop = tf.keras.callbacks.EarlyStopping(\nmonitor='val_accuracy',\npatience=4)\n```", "```py\nmyCallbacks = [\n    myCheckPoint,\n    myEarlyStop\n]\n```", "```py\nhist = model.fit(\n    train_dataset,\n    epochs=20,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=validation_dataset,\n    validation_steps=VALIDATION_STEPS,\n    callbacks=myCallbacks).history\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nimport numpy as np\nimport matplotlib.pylab as plt\nimport os\nfrom datetime import datetime\n\n(train_images, train_labels), (test_images, test_labels) = \ndatasets.cifar10.load_data()\n\ntrain_images, test_images = train_images / 255.0, \n test_images / 255.0\n```", "```py\nCLASS_NAMES = ['airplane', 'automobile', 'bird', 'cat',\n               'deer','dog', 'frog', 'horse', 'ship', 'truck']\n```", "```py\nvalidation_dataset = tf.data.Dataset.from_tensor_slices(\n(test_images[:500], test_labels[:500]))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices(\n(test_images[500:], test_labels[500:]))\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices(\n(train_images, train_labels))\n```", "```py\ntrain_dataset_size = len(list(train_dataset.as_numpy_iterator()))\nprint('Training data sample size: ', train_dataset_size)\n\nvalidation_dataset_size = len(list(validation_dataset.\nas_numpy_iterator()))\nprint('Validation data sample size: ', \n validation_dataset_size)\n\ntest_dataset_size = len(list(test_dataset.as_numpy_iterator()))\nprint('Test data sample size: ', test_dataset_size)\n```", "```py\nTraining data sample size:  50000\nValidation data sample size:  500\nTest data sample size:  9500\n```", "```py\nTRAIN_BATCH_SIZE = 128\ntrain_dataset = train_dataset.shuffle(50000).batch(\nTRAIN_BATCH_SIZE, \ndrop_remainder=True)\n\nvalidation_dataset = validation_dataset.batch(\nvalidation_dataset_size)\ntest_dataset = test_dataset.batch(test_dataset_size)\n```", "```py\nSTEPS_PER_EPOCH = train_dataset_size // TRAIN_BATCH_SIZE\nVALIDATION_STEPS = 1\n```", "```py\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, \n     kernel_size=(3, 3), \n     activation='relu', \n     name = 'conv_1',\n     kernel_initializer='glorot_uniform', \n     padding='same', input_shape = (32,32,3)),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), \n     activation='relu', name = 'conv_2',\n      kernel_initializer='glorot_uniform', padding='same'),\n    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n    tf.keras.layers.Conv2D(64, \n     kernel_size=(3, 3), \n     activation='relu', \n     name = 'conv_3',\n      kernel_initializer='glorot_uniform', padding='same'),\n    tf.keras.layers.Flatten(name = 'flat_1',),\n    tf.keras.layers.Dense(64, activation='relu',   \n     kernel_initializer='glorot_uniform', \n     name = 'dense_64'),\n    tf.keras.layers.Dense(10, \n     activation='softmax', \n     name = 'custom_class')\n])\nmodel.build([None, 32, 32, 3])\n```", "```py\nmodel.compile(\n          loss=tf.keras.losses.SparseCategoricalCrossentropy(\n               from_logits=True),\n          optimizer='adam',\n          metrics=['accuracy'])\n```", "```py\nMODEL_NAME =\n'myCIFAR10-{}'.format(datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\nprint(MODEL_NAME)\n```", "```py\ncheckpoint_dir = './' + MODEL_NAME\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt-{epoch}\")\nprint(checkpoint_prefix)\n```", "```py\nmyCheckPoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    monitor='val_accuracy',\nmode='max')\n```", "```py\nmyTensorBoard = tf.keras.callbacks.TensorBoard(\nlog_dir='./tensorboardlogs/{}'.format(MODEL_NAME),\nwrite_graph=True,\nwrite_images=True,\nhistogram_freq=1)\n```", "```py\nmyCallbacks = [\n    myCheckPoint,\n    myTensorBoard\n]\n```", "```py\nhist = model.fit(\n    train_dataset,\n    epochs=30,\n    steps_per_epoch=STEPS_PER_EPOCH,\n    validation_data=validation_dataset,\n    validation_steps=VALIDATION_STEPS,\n    callbacks=myCallbacks).history\n```", "```py\n!tensorboard --logdir='./tensorboardlogs/'\n```", "```py\nServing TensorBoard on localhost; to expose to the network, use a\nproxy or pass --bind_all\nTensorBoard 2.3.0 at http://localhost:6006/ (Press CTRL+C to quit)\n```", "```py\ntensorboard --logdir='./tensorboardlogs/'\n```", "```py\n%load_ext tensorboard\n```", "```py\n%tensorboard --logdir ./tensorboardlogs/\n```"]