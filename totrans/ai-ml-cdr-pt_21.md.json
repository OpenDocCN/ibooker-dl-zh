["```py\ngit clone https://github.com/huggingface/diffusers\ncd diffusers\npip install .\n```", "```py\n!git clone https://github.com/huggingface/diffusers\n%cd diffusers\n!pip install .\n```", "```py\n%cd /content/diffusers/examples/text_to_image # or whatever your dir is\n!pip install -r requirements.txt\n```", "```py\n!pip install xformers\n```", "```py\n{ \"file_name\": \"rightprofile-smile.png\", \n               \"prompt\": \"photo of (lora-misato-token), \n               `right` `side` `profile``,` `high` `quality``,` `detailed` `features``,` \n               `smiling``,` `professional` `photo` `\"}` ```", "```py                 `right` `side` `profile``,` `high` `quality``,` `detailed` `features``,`                 `professional` `photo` `\"}` ```", "```py`\n```", "```py `` `That’s pretty much all you need. For training with diffusers, I’ve found it much easier if you publish your dataset on Hugging Face. To do this, when logged in, visit the [Hugging Face website](https://oreil.ly/Ez3Gp). There, you’ll be able to specify the name of the new dataset and whether or not it’s public. Once you’ve done this, you’ll be able to upload the files through the web interface (see [Figure 20-4](#ch20_figure_4_1748550104889591)).    Once you’ve done this, your dataset will be available at [*https://huggingface.co/datasets/*](https://huggingface.co/datasets/)*<yourname>/<datasetname>*. So, for example, my username (see [Figure 20-4](#ch20_figure_4_1748550104889591)) is “lmoroney,” and the dataset name is “misato,” so you can see this dataset at [*https://huggingface.co/datasets/lmoroney/misato*](https://huggingface.co/datasets/lmoroney/misato).  ![](assets/aiml_2004.png)  ###### Figure 20-4\\. Creating a new dataset on Hugging Face` `` ```", "```py```", "````py` ````", "```` ```py````", "```py from accelerate.utils import write_basic_config   write_basic_config() ```", "```py !accelerate launch train_text_to_image_lora.py \\   --pretrained_model_name_or_path=\"stabilityai/stable-diffusion-2\" \\   --dataset_name=\"lmoroney/misato\" \\   --caption_column=\"prompt\" \\   --resolution=512 \\   --random_flip \\   --train_batch_size=1 \\   --num_train_epochs=1000 \\   --checkpointing_steps=5000 \\   --learning_rate=1e-04 \\   --lr_scheduler=\"constant\" \\   --lr_warmup_steps=0 \\   --seed=42 \\   --output_dir=\"/content/lm-misato-lora\" ```", "```py Resolving data files: 100% 22/22 [00:00<00:00, 74.14it/s] 12/30/2024 19:23:48 - INFO - __main__ - ***** Running training ***** 12/30/2024 19:23:48 - INFO - __main__ -   Num examples = 21 12/30/2024 19:23:48 - INFO - __main__ -   Num Epochs = 1000 12/30/2024 19:23:48 - INFO - __main__ -   Instantaneous batch size per device... 12/30/2024 19:23:48 - INFO - __main__ -   Total train batch size (w. parallel... 12/30/2024 19:23:48 - INFO - __main__ -   Gradient Accumulation steps = 1 12/30/2024 19:23:48 - INFO - __main__ -   Total optimization steps = 1000 Steps:  10% 103/1000 [05:03<44:00,  2.94s/it, lr=0.0001, step_loss=0.227] ```", "```py import torch from diffusers import (     StableDiffusionPipeline,     EulerAncestralDiscreteScheduler,   ) ```", "```py model_id = \"stabilityai/stable-diffusion-2\"   # Choose your device device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # 1\\. Pick your scheduler scheduler = EulerAncestralDiscreteScheduler.from_pretrained(     model_id,     subfolder=\"scheduler\" ) ```", "```py # 2\\. Load the pipeline with the chosen scheduler pipe = StableDiffusionPipeline.from_pretrained(     model_id,     scheduler=scheduler,     torch_dtype=torch.float16 ).to(device) ```", "```py # 3\\. (Optional) Load LoRA weights pipe.load_lora_weights(\"lmoroney/finetuned-misato-sd2\") ```", "```py # 4\\. Define prompts and parameters prompt = \"(lora-misato-token) in food ad, billboard sign, 90s, anime,           `japanese` `pop``,` `japanese` `words``,` `front` `view``,` `plain` `background``\"` ```", "```py`` `negative_prompt` `=` `(`     `\"(deformed, distorted, disfigured:1.3), poorly drawn, bad anatomy,` ```", "```py`       `wrong` `anatomy``,` `\"` ```", "```py `\"extra limb, missing limb, floating limbs, (mutated hands and` ```", "```py` `\"disconnected limbs, mutation, mutated, ugly, disgusting, blurry,` ```", "```py ```", "```py`` ```", "```py ```", "```py` ```", "```py`` ```", "```py```", "````` ```py`The negative prompt is very useful in helping you avoid some of the issues with AI-generated visuals, such as deformed hands and faces.    Next up is to define the hyperparameters, such as the number of inference steps, the size of the image, and the seed. There’s also a parameter called *guidance scale*, which controls how imaginative your model is. A guidance scale value of less than 5 gives the model more creative freedom, but the model may not follow your prompt closely. A guidance scale value that’s higher than 7 will make the model adhere more strongly to your prompt, but it can also lead to strange artifacts. The guidance scale value in the middle—6—is a nice balance between freedom and adherence. There’s no hard and fast rule, so feel free to experiment:    ``` num_inference_steps = 50 guidance_scale = 6.0 width = 512 height = 512 seed = 1234567 ```py    Next, you just generate the image as usual:    ``` # 5\\. Create a generator for reproducible results generator = torch.Generator(device=device).manual_seed(seed)   # 6\\. Run the pipeline image = pipe(     prompt,     negative_prompt=negative_prompt,     width=width,     height=height,     num_inference_steps=num_inference_steps,     guidance_scale=guidance_scale,     generator=generator, ).images[0]   # 7\\. Save the result image.save(\"lora-with-negative.png\") ```py    As an experiment, you can try using a different scheduler with the same hyperparameters to yield similar results (see [Figure 20-8](#ch20_figure_8_1748550104889721)):    ``` # For DPMSolver, use: from diffusers import DPMSolverMultistepScheduler   scheduler = DPMSolverMultistepScheduler.from_pretrained(model_id,              subfolder=\"scheduler\", algorithm_type=\"dpmsolver++\") ```py  ![](assets/aiml_2008.png)  ###### Figure 20-8\\. The same prompt and hyperparameters with different schedulers    Note that the text in the image is entirely made up, but given that the prompt is about advertisements, the tone is similar. In the picture on the left, the characters represent “loneliness” and “no,” while in the image on the right, they suggest “husband split?”    What’s most interesting is the consistency in the character! For example, consider [Figure 20-9](#ch20_figure_9_1748550104889755), in which Misato was painted in the styles of Monet and Picasso. We can see that the features learned by LoRA were consistent enough to (mostly) survive the restyling process.  ![](assets/aiml_2009.png)  ###### Figure 20-9\\. Character consistency across styles    This example used Stable Diffusion 2, which is an older model but one that’s easy to tune with LoRA. As you use more advanced models and tune them, you can get much better results, but the time and costs of tuning will be much higher. I’d recommend starting with a simpler model like this one and working on your craft. From there, you can build up to the more advanced models.    Additionally, Misato’s synthetic nature has triggered different features in the LoRA retraining, leading to the new images that have been created from her having a low-res, highly synthetic look. While the images have been close to photoreal to the human eye, they clearly haven’t been to the model, which learned a LoRA that was very CGI in nature and lower resolution than the ones in the training set!```` ```py`` `````", "``````py``` ``````", "```` ```py````", "```py`  ```"]