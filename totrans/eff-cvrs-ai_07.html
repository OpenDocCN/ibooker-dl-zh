<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">6</span> </span> <span class="chapter-title-text">Enhancing responses with retrieval-augmented generation</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Enhancing chatbot responses without coding intents</li> 
    <li class="readable-text" id="p3">Improving weak understanding with RAG</li> 
    <li class="readable-text" id="p4">Evaluating the advantage of using RAG over traditional search models</li> 
    <li class="readable-text" id="p5">Selecting the proper RAG techniques for your conversational AI</li> 
    <li class="readable-text" id="p6">Assessing and improving the performance of RAG in your conversational AI systems</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>In previous chapters, we saw the “chatbot doesn’t understand” pain point for question-answering bots. We first addressed it by helping the chatbot understand more intents, but at some point there are diminishing returns to this strategy. Uncommon questions from the “long tail” may never make sense to implement as intents. This chapter introduces ways to handle that “long tail,” including search and retrieval-augmented generation (RAG). These are great methods for improving a chatbot’s weak understanding.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>We concluded chapter 5 with advice on when to avoid adding new intents, especially when dealing with diverse, infrequent domain-related problems. In this chapter, we’ll add search capabilities to improve weak understanding.</p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>Both search and RAG allow you to improve a chatbot by adding data and documents without programming new intents. This allows you to serve thousands of intents with the simplicity of training just a few. The answers provided by these methods are more straightforward to change—just change the documents rather than changing your chatbot.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>Search and RAG can be easier for you as a builder and efficient for your users. Let’s explore how a chatbot can evolve using search and RAG capabilities.</p> 
  </div> 
  <div class="readable-text" id="p11"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.1</span> Beyond intents: The role of search in conversational AI</h2> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>Traditional conversational AI centers on understanding user intents. Systems are trained to recognize predefined categories of user queries and to provide pre-scripted responses. </p> 
  </div> 
  <div class="readable-text intended-text" id="p13"> 
   <p>Figure 6.1 illustrates a conceptual intent-based chatbot architecture in its simplest form. The chatbot’s classifier detects the intent and determines the appropriate dialogue flow. When the classifier cannot identify the user’s intent, the answer is a generic “I didn’t understand” response style, leading to the “chatbot not understanding me” pain point. Intent-based question-answering is a great way to start handling frequently asked questions—you can define an exact answer to be given for different question types. Initially, this is quite effective, but it breaks down. Users often ask questions that deviate from predefined intents. When the predefined intents are insufficient to handle the user’s questions, the user may receive irrelevant or incorrect responses, leading to frustration. Further, maintaining and evolving these intents requires significant effort.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p14">  
   <img alt="figure" src="../Images/CH06_F01_Freed2.png" width="571" height="94"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.1</span> An intent-based chatbot first detects an intent and then maps it to an answer.</h5>
  </div> 
  <div class="readable-text" id="p15"> 
   <p>There is a tradeoff between the specificity of the answer and the number of variations covered. Figure 6.2 demonstrates an example from the PharmaBot we introduced in chapter 3. The answer is accurate but generic—it detected a question about side effects but did not answer all the nuances in the user’s question.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p16">  
   <img alt="figure" src="../Images/CH06_F02_Freed2.png" width="893" height="99"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.2</span> Intent-based systems identify the main theme of an utterance and often give a static or generic answer.</h5>
  </div> 
  <div class="readable-text intended-text" id="p17"> 
   <p>We can handle this nuance by adding search capabilities. There are two primary methods:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p18"> <em>Traditional search</em> supplies the user with documents or passages relevant to their query. The user uses these documents to find their answer. </li> 
   <li class="readable-text" id="p19"> <em>RAG</em> starts with a search process but expands on (augments) it by summarizing the passages into an answer. </li> 
  </ul> 
  <div class="readable-text" id="p20"> 
   <p>Figure 6.3 illustrates how RAG refines information retrieval by locating relevant document passages and synthesizing them into a specific, contextualized answer. Unlike<span class="aframe-location"/> traditional intent-based systems that often respond with fixed or general answers, RAG dynamically pulls in content to address the user’s unique query, demonstrating how specific passages on ibuprofen and blood pressure are distilled into a targeted recommendation.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p21">  
   <img alt="figure" src="../Images/CH06_F03_Freed2.png" width="925" height="164"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.3</span> RAG finds relevant passages and summarizes them, giving a targeted answer.</h5>
  </div> 
  <div class="readable-text" id="p22"> 
   <p>Let’s dive deeper into how we can add these capabilities effectively.</p> 
  </div> 
  <div class="readable-text" id="p23"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.1.1</span> Using search in conversational AI</h3> 
  </div> 
  <div class="readable-text" id="p24"> 
   <p>User questions follow a “short head, long tail” distribution, as shown in figure 6.4. This distribution has a high frequency of common or popular questions (the short head). Most interactions involve less frequent, niche, or specialized queries (the long tail).<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p25">  
   <img alt="figure" src="../Images/CH06_F04_Freed2.png" width="719" height="250"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.4</span> Distribution of user questions. Intents address the most common, high-volume questions, while low-volume, unique questions may necessitate search integration. </h5>
  </div> 
  <div class="readable-text" id="p26"> 
   <p>For PharmaBot, the short head includes general COVID inquiries, such as vaccine information and appointments. Each bot will have a different short head, but it will cover the most popular questions. When the chatbot is trained well, these questions are recognized with high confidence. Intents afford builders complete control over short head queries, albeit potentially overlooking nuanced distinctions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p27"> 
   <p>We saw earlier that PharmaBot did not handle a nuanced question well because it used a static intent for <code>#side_effects</code>. Figure 6.5 shows PharmaBot handling the same nuanced question using traditional search capability. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p28">  
   <img alt="figure" src="../Images/CH06_F05_Freed2.png" width="897" height="185"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.5</span> Search finds relevant passages and displays those directly to the user, often with links to the source documents.</h5>
  </div> 
  <div class="readable-text" id="p29"> 
   <p>The response includes all nuances from the user’s question in this example. The passages reference vaccine side effects, ibuprofen, blood pressure, and pain. However, the chatbot did not provide a single, cohesive answer. Instead, it offered document links and snippets. Users need to combine the answers from those documents and passages.</p> 
  </div> 
  <div class="readable-text" id="p30"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.1.2</span> Benefits of traditional search</h3> 
  </div> 
  <div class="readable-text" id="p31"> 
   <p>Traditional search can complement an intent-based chatbot by enabling it to retrieve relevant information from a document repository. The approach offers several advantages: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p32"> <em>Breadth</em><em>  </em>—The bot can access various materials in your document repository, giving it answers to different question types. </li> 
   <li class="readable-text" id="p33"> <em>Maintenance</em><em>  </em>—Adding knowledge to your bot can be as easy as adding or editing documents in your repository. </li> 
   <li class="readable-text" id="p34"> <em>Technology</em><em>  </em>—Search is a well-established technology with mature algorithms and implementation methods. It can be implemented with relatively low computational resources and infrastructure. </li> 
   <li class="readable-text" id="p35"> <em>Speed</em><em>  </em>—While slower than a static intent-based response, traditional search executes reasonably quickly. </li> 
  </ul> 
  <div class="readable-text" id="p36"> 
   <p>Thus, search is an excellent complement to an intent-based system. The most common way to combine intents and search is to use a confidence threshold in the chatbot’s natural language understanding (NLU) component. The NLU attempts to detect an intent from the user’s utterance. If an intent is detected with high confidence, an intent-based answer is returned. Otherwise, the user’s utterance is passed to a search component (in some conversational AI systems, this is called a <em>fallback action</em> or <em>inten</em><em>t</em>). The high-level architecture is illustrated in figure 6.6.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p37">  
   <img alt="figure" src="../Images/CH06_F06_Freed2.png" width="745" height="232"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.6</span> Intents and searches have complementary functions. A search-augmented bot uses intent-based answers when it recognizes the utterance with high confidence; otherwise, it defers to search. Using intents and search together improves chatbot capabilities, but this approach still has some limitations.</h5>
  </div> 
  <div class="readable-text" id="p38"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.1.3</span> Drawbacks of traditional search</h3> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>Two fundamental problem areas exist when integrating traditional search with chatbot applications: the quality of search results and the user experience of how the search results are presented.</p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p>One major drawback to search quality is its reliance on keyword matching, which may be inaccurate or brittle, depending on the user’s phrasing. Consider the previous example question: “Can I take ibuprofen with my blood pressure medication? My arms are sore after getting the vaccine.” This may be converted to “ibuprofen blood pressure medication arms sore vaccine,” emphasizing the most relevant keywords but losing the nuance of the question.</p> 
  </div> 
  <div class="readable-text intended-text" id="p41"> 
   <p>Not all search engines limit themselves to keyword matching, but it is ubiquitous. Newer search engines support searching by meaning rather than keywords. This approach is done with vector databases and will be described more fully in the next section. Like traditional searches, vector database searches take an input query and return a set of relevant documents and passages.</p> 
  </div> 
  <div class="readable-text intended-text" id="p42"> 
   <p>The other major drawback of search-based options is the user experience of receiving documents and passages. Some of the user experience limitations derive from the limited space in a chatbot window and the challenges of presenting multiple search results well. These are commonly addressed by showing a small number of results (possibly asking the user if they want to see more).</p> 
  </div> 
  <div class="readable-text intended-text" id="p43"> 
   <p>Screen real estate is sometimes preserved by showing document links (not the passages). In this case, users must leave the chat interface, which disrupts the conversation flow and may lead to the user abandoning the chatbot and continuing where the document links took them. </p> 
  </div> 
  <div class="readable-text intended-text" id="p44"> 
   <p>Search results are also challenging to handle through a voice interface, leading to lengthy readouts and a non-optimal user experience.</p> 
  </div> 
  <div class="readable-text intended-text" id="p45"> 
   <p>Most critically, the search does not result in a cohesive answer. Some users may prefer to construct their answers from relevant documents. Most users, however, are frustrated when a cohesive answer is not given, and they must do the piecing together—“Why did I use the chatbot in the first place? I could have searched on my own.”</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p46"> 
   <p><span class="print-book-callout-head">Note </span> You might consider combining web search with answer synthesis. While a chatbot can create synthesized answers from web search results, this approach also has limitations. It requires more sophisticated processing, but it addresses the user experience limitations by eliminating the need for users to sift through multiple documents. This approach depends heavily on how well individual passages match the query, and the summarization may miss nuances or context if the retrieved data isn’t comprehensive.</p> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>Search with answer synthesis typically relies on rule-based extraction methods, ranking algorithms, keyword matching, or predefined heuristics. The responses are presented by combining information from retrieved documents. While this can efficiently surface relevant information, it may struggle with incomplete or ambiguous queries. The system does not truly “understand” the content. Instead, it selects and reformulates existing text, which can lead to missing context, fragmented responses, or over-reliance on the most prominent retrieved results rather than the most accurate ones. It lacks the flexibility of generative approaches.</p> 
  </div> 
  <div class="readable-text intended-text" id="p48"> 
   <p>This is where<strong> </strong>RAG<strong> </strong>is a powerful alternative. RAG doesn’t just pull text from documents—it combines retrieval with generation, allowing the chatbot to produce a cohesive, contextually aware answer using relevant content from various sources. Unlike traditional search and summarization approaches, RAG can adapt to a broader range of user questions and provide deeper, more accurate responses by using a combination of real-time retrieval and language generation capabilities.</p> 
  </div> 
  <div class="readable-text intended-text" id="p49"> 
   <p>The next section will explore how RAG enhances chatbot responses by improving accuracy and maintaining context, even with complex or nuanced queries.</p> 
  </div> 
  <div class="readable-text" id="p50"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.2</span> Beyond search: Generating answers with RAG</h2> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>The lack of clear answers is a limitation of traditional search methods. To overcome these limitations, we’ll look at RAG as an advanced alternative. At its core, RAG combines the strengths of search-based information retrieval with the flexibility of generative models, offering a more comprehensive approach to understanding and responding to user queries. Most importantly, this response includes an answer.</p> 
  </div> 
  <div class="readable-text" id="p52"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.2.1</span> Using RAG in conversational AI</h3> 
  </div> 
  <div class="readable-text" id="p53"> 
   <p>RAG combines the best of retrieval and generation techniques to enhance the user experience. Like traditional search, it retrieves relevant passages to handle long-tail questions. RAG then feeds the passages and the user’s request to generative AI, which creates the answer. RAG “augments” the retrieved passages by generating an answer, creating a seamless conversational flow, even for complex or long-tail queries.</p> 
  </div> 
  <div class="readable-text intended-text" id="p54"> 
   <p>Figure 6.7 shows PharmaBot answering our familiar example using RAG: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p55"> 
   <div class="code-area-container"> 
    <pre class="code-area">User: “Can I take Ibuprofen with my blood pressure medication? My arms are sore after getting the vaccine?” 
Chatbot: Do not use over-the-counter ibuprofen for pain relief. Instead, use a painkiller less likely to increase your blood pressure, like aspirin.”<span class="aframe-location"/></pre>  
   </div> 
  </div> 
  <div class="browsable-container figure-container" id="p56">  
   <img alt="figure" src="../Images/CH06_F07_Freed2.png" width="807" height="133"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.7</span> RAG retrieves relevant passages and augments the response by synthesizing the information into a grounded answer.</h5>
  </div> 
  <div class="readable-text" id="p57"> 
   <p>PharmaBot may still retrieve the same passages as the traditional search, but now it summarizes them to generate an answer. The answer acknowledges the user’s specific concerns and provides tailored advice. Most importantly, the answer is grounded in PharmaBot’s source documents—not the generative AI’s general knowledge. PharmaBot may provide links to supporting documentation, but it has made the answer prominent rather than the document passages. This is a more effortless experience for users. </p> 
  </div> 
  <div class="readable-text intended-text" id="p58"> 
   <p>RAG empowers chatbots to better understand user questions, and it streamlines development efforts by minimizing the need for explicit intent classification. This shift in approach enhances user satisfaction and future-proofs conversational AI systems against the evolving landscape of human language and user needs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p59"> 
   <p>The use of RAG introduces dynamism and diminishes user effort, contingent upon the avoidance of hallucinations. While RAG inherently reduces hallucinations, it does not eliminate them entirely. Attention should be paid to the quality of the retrieved documents, the generative model’s behavior, and when the retrieval fails to find relevant documents, as we’ll discuss later. </p> 
  </div> 
  <div class="readable-text intended-text" id="p60"> 
   <p>When the user interacts with the conversational AI, the retrieval system connects to the trusted content sources, executes the search (keyword, semantic, or vector), and provides a relevancy score for the retrieved results. A large language model (LLM) then augments its response using the retrieved information. It generates a response from the retrieved content and presents it to the user through the chatbot interface. It may also apply translation if needed.</p> 
  </div> 
  <div class="readable-text intended-text" id="p61"> 
   <p>Like traditional search, RAG can complement intent-based systems. The short head is handled via intents, and RAG handles the long tail. The answers are as current as the document sources. Documents are easier to update than chatbots! </p> 
  </div> 
  <div class="readable-text" id="p62"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.2.2</span> Benefits of RAG</h3> 
  </div> 
  <div class="readable-text" id="p63"> 
   <p>RAG is a great addition to question-answer bots. By retrieving information from your knowledge bases (and not generative AI model knowledge), RAG ensures that answers are grounded in accurate and up-to-date data, thus enhancing the bot’s credibility and reliability. Additionally, RAG expands the versatility of conversational AI systems by enabling them to handle a wide range of user queries and tasks effectively. It also reduces the “bot doesn’t understand” and “too much complexity out on the user” pain points. </p> 
  </div> 
  <div class="readable-text intended-text" id="p64"> 
   <p>Benefits from RAG include enhanced answer relevance, adaptiveness, and improved handover to agents.</p> 
  </div> 
  <div class="readable-text" id="p65"> 
   <h4 class=" readable-text-h4">Enhanced relevance</h4> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>RAG’s essential advantage is its ability to access and integrate the latest information from your knowledge bases. The knowledge bases provide more varied information than pretrained intents do, and LLMs can adapt knowledge from your documents into an answer.</p> 
  </div> 
  <div class="readable-text intended-text" id="p67"> 
   <p>Let’s consider another example from PharmaBot. The initial PharmaBot included several intents, including guidance for dining in restaurants. It responded based on general guidelines and included a reference to “check local regulations.” There is no tailoring of responses to the user—it is a one-size-fits-all answer.</p> 
  </div> 
  <div class="readable-text intended-text" id="p68"> 
   <p>Table 6.1 compares intent-based and RAG-based versions of PharmaBot. Intent-based gives a generic answer, while RAG-based localizes to New York per the user’s question.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p69"> 
   <h5 class=" browsable-container-h5"><span class="num-string">Table 6.1</span> A comparison of answers for intents and RAG</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Interaction with an intent-based bot 
       </div></th> 
      <th> 
       <div>
         Interaction with a RAG-integrated bot 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  <code>User: What are the current COVID-19 guidelines for restaurants in New York?</code> <br/>  <code>PharmaBot: Current COVID-19 guidelines for restaurant dining include limited indoor capacity, mask requirements for staff and patrons when not seated, and enhanced cleaning protocols. Please check local regulations for the most up-to-date information.</code> <br/></td> 
      <td>  <code>User: What are the current COVID-19 guidelines for restaurants in New York?</code> <br/>  <code>PharmaBot: COVID-19 remains a significant concern for New Yorkers. Make sure you are up to date on the updated COVID-19 vaccines for the 2023-2024 season that offer protection against emerging variants and enhance overall immunity. Masks continue to be effective tools in mitigating the spread of COVID-19 and other respiratory viruses, reducing the transmission of droplets during various activities. To minimize the risk of contracting COVID-19, individuals are advised to wear masks in crowded indoor public settings.</code> <br/>  Source: NYC Health <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>The RAG-integrated chatbot acknowledges the user’s location in New York City. It provides more specific information tailored to the local guidelines and regulations, pulled from the current NYC Health Guidelines and summarized for restaurant dining. This personalized approach enhances the user experience by delivering more relevant and actionable guidance based on the user’s context.</p> 
  </div> 
  <div class="readable-text" id="p71"> 
   <h4 class=" readable-text-h4">Adaptiveness</h4> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>On the conversational side, another advantage of RAG is in the way the conversational AI generates the response—RAG can adjust its generated response to the style of the user’s question. The response can be similarly more formal if the user’s tone is more fact-seeking. The responses to the user’s unique question are generated in real time. Sometimes users expect a concise and direct answer (“Yes or no, are there restrictions on dining in restaurants?”<em>), </em>and sometimes they expect a longer and more complex response (“Can I take my extended family to a restaurant, and will we have to wear masks?”). Both questions may use the same source documents but will receive very different answers.</p> 
  </div> 
  <div class="readable-text" id="p73"> 
   <h4 class=" readable-text-h4">Handover to human agent (or not answering)</h4> 
  </div> 
  <div class="readable-text" id="p74"> 
   <p>There are occasions when the conversational AI cannot find a definite response. In these cases, it is better for the bot to answer that it doesn’t know or to transfer the user to a human agent. Figure 6.8 depicts a user asking a question a chatbot can’t answer. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p75">  
   <img alt="figure" src="../Images/CH06_F08_Freed2.png" width="793" height="224"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.8</span> Supplementing RAG with human agents. If the answer has poor semantic overlap with the retrieved documents, send the user to a human agent instead.</h5>
  </div> 
  <div class="browsable-container listing-container" id="p76"> 
   <div class="code-area-container"> 
    <pre class="code-area">Sample chat:
User: I have achalasia. Will my dysphagia get worse if I get a Booster and experience side effects?</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>When a user asks a question, the conversational AI follows a multistep process to retrieve information, generate a response, and determine whether the answer is sufficiently grounded in retrieved evidence before delivering a final response. The following steps illustrate this process, showing how the system retrieves relevant passages, generates a candidate answer, evaluates its accuracy, and ultimately decides whether to respond or transfer the user to a human agent:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p78"> <em>Passage retrieval</em><em> </em>—The system retrieves passages related to achalasia, dysphagia, and general information about vaccine side effects: </li> 
  </ol> 
  <div class="readable-text list-body-item" id="p79"> 
   <p>Example passage 1: <code>"Achalasia</code> <code>is</code> <code>a</code> <code>condition</code> <code>affecting</code> <code>the</code> <code>esophagus,</code> <code>causing</code> <code>difficulty</code> <code>in</code> <code>swallowing."</code></p> 
  </div> 
  <div class="readable-text list-body-item" id="p80"> 
   <p>Example passage 2: <code>"Common</code> <code>side</code> <code>effects</code> <code>of</code> <code>vaccines</code> <code>include</code> <code>soreness, fever,</code> <code>and</code> <code>fatigue."</code></p> 
  </div> 
  <div class="readable-text list-body-item" id="p81"> 
   <p>Example passage 3: <code>"Dysphagia,</code> <code>or</code> <code>difficulty</code> <code>swallowing,</code> <code>can</code> <code>be</code> <code>a symptom</code> <code>of</code> <code>esophageal</code> <code>conditions</code> <code>like</code> <code>achalasia."</code></p> 
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p82"><span class="faux-ol-li-counter">2. </span> <em>Answer generation</em><em> </em>—The LLM generates a candidate response based on the retrieved passages: </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p83"> 
   <div class="code-area-container"> 
    <pre class="code-area">Getting a booster might lead to common side effects, but there is no clear evidence linking it to worsening dysphagia in people with achalasia.</pre>  
   </div> 
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p84"><span class="faux-ol-li-counter">3. </span> <em>Comparison check</em><em> </em>—The system evaluates this generated answer against the retrieved passages and identifies a potential problem: the generated answer contains an element of “no clear evidence linking it to worsening dysphagia” that is not directly supported by the retrieved passages.  </li> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p85"><span class="faux-ol-li-counter">4. </span> <em>Transfer decision</em><em> </em>—Given the low match rate between the answer and the retrieved passages, the conversational AI determines that the answer may lack sufficient grounding and could be misleading. It then offers to transfer the user to a human agent for a more reliable answer: </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p86"> 
   <div class="code-area-container"> 
    <pre class="code-area">Chatbot response: I apologize. I could not find a clear answer to your question in our resources. Let me connect you with a specialist who can provide more detailed information.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>In this scenario, the conversational system searched for relevant document passages and fed them to an LLM. The LLM generated an answer, and the conversational AI then compared the generated answer to the retrieved passages. This comparison includes detecting how many words and phrases in the generated answer appear in the passages. If the percentage is low, the conversational AI decides that the answer is not grounded in the documents. The conversational AI then gracefully acknowledges its inability to provide a suitable grounded response and offers an alternate resolution path. </p> 
  </div> 
  <div class="readable-text intended-text" id="p88"> 
   <p>Alternatively, the search process may not have retrieved any documents. In that case, the conversational AI would not have to invoke the LLM to generate an answer, and it could directly deflect the question. For both scenarios, the conversational AI could instead return an “I don’t know” or other fallback responses. Both options reduce the chance of hallucinated and irrelevant answers.</p> 
  </div> 
  <div class="readable-text" id="p89"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.2.3</span> Combining RAG with other generative AI use cases</h3> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>RAG may also be combined with other generative AI use cases. For instance, RAG can handle informational queries, while other generative AI models specialize in tasks like sentiment analysis or language translation. By using a combination of AI capabilities, conversational AI systems can offer users a comprehensive range of services, further enhancing efficiency and satisfaction.</p> 
  </div> 
  <div class="readable-text intended-text" id="p91"> 
   <p>RAG is only one of several generative AI patterns that enhance conversational AI. When users pose common questions or seek detailed information about a product or service, RAG draws upon the enterprise knowledge base to provide accurate and up-to-date answers. By grounding responses in the organization’s specific domain, RAG ensures that users receive relevant information tailored to their needs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p92"> 
   <p>However, specific user inquiries may require more than informational responses, necessitating actionable steps (information-seeking versus transactional questions). In such cases, the conversational AI system executes transactions and guides users through specific tasks or processes. For instance, users may express an interest in purchasing after receiving information about a product or service from RAG. In response, the conversational AI can seamlessly transition to a transactional action, such as initiating a checkout process or scheduling a vaccination, facilitating a smooth and efficient user journey.</p> 
  </div> 
  <div class="readable-text intended-text" id="p93"> 
   <p>While RAG excels at efficiently and accurately responding to user queries, additional options, such as handing over to human agents or combining RAG with other generative AI use cases, can further optimize the user experience. These options ensure that users receive the support and assistance they need in the most efficient manner possible.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p94"> 
   <p><span class="print-book-callout-head">Note</span>  For scenarios where RAG responses may fall short—such as providing real-time data or fulfilling specific customer requests—function calling can be integrated to retrieve information from external systems dynamically. This approach allows chatbots to identify relevant intents and parameters for real-time responses, extending RAG’s utility in complex interactions. While it is not covered in depth here, the function call is valuable if you are seeking a more dynamic conversational AI system.</p> 
  </div> 
  <div class="readable-text" id="p95"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.2.4</span> Comparing intents, search, and RAG approaches</h3> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>Table 6.2 summarizes the capabilities and performance of three types of chatbots: intent-based chatbots, chatbots integrated with search, and RAG-integrated chatbots. Each chatbot type is evaluated based on the requirements and capabilities users and chatbot creators expect from conversational AI. You can discern the most suitable chatbot solution for your specific needs by comparing these attributes.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p97"> 
   <h5 class=" browsable-container-h5"><span class="num-string">Table 6.2</span> Comparing the capabilities of intent-based chatbots, chatbots integrated with search, and RAG-integrated chatbots</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Requirements 
       </div></th> 
      <th> 
       <div>
         Intent-based chatbots 
       </div></th> 
      <th> 
       <div>
         Chatbots integrated with search 
       </div></th> 
      <th> 
       <div>
         RAG-integrated chatbots 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Flexibility in handling queries <br/></td> 
      <td>  Limited to short-head predefined intents. May ignore nuance. <br/></td> 
      <td>  Handle long-tail queries by returning links and snippets <br/></td> 
      <td>  Handle long-tail queries by returning answers <br/></td> 
     </tr> 
     <tr> 
      <td>  Accuracy and relevance of responses <br/></td> 
      <td>  When an intent is recognized with high confidence, the answers are accurate and precrafted. <br/></td> 
      <td>  Provide contextually relevant and accurate documents that help the user find an answer <br/></td> 
      <td>  Provide contextually relevant and accurate answers grounded in your documents <br/></td> 
     </tr> 
     <tr> 
      <td>  Adding new know-ledge to the bot <br/></td> 
      <td>  Add or revise manually curated intent-response pairs <br/></td> 
      <td>  Add or revise documents in your knowledge base <br/></td> 
      <td>  Add or revise documents in your knowledge base <br/></td> 
     </tr> 
     <tr> 
      <td>  Maintenance and scalability <br/></td> 
      <td>  Extensive regression testing when intent training data is changed <br/></td> 
      <td>  Document repository needs to be maintained by adding new documents and removing stale documents <br/></td> 
      <td>  Document repository needs to be maintained by adding new documents and removing stale documents <br/></td> 
     </tr> 
     <tr> 
      <td>  Response generation quality <br/></td> 
      <td>  Predefined responses are presented. <br/></td> 
      <td>  User must put together their own answer from retrieved passages and documents <br/></td> 
      <td>  Answers are grounded in source documents but adapted to nuance from the question <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p98"> 
   <p>While traditional chatbots help organizations automate simple tasks and provide essential customer support, integrating RAG techniques enhances their ability to deliver more accurate, context-aware responses, ultimately improving the user experience.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p99"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercises</h5> 
   </div> 
   <ol> 
    <li class="readable-text buletless-item" id="p100"> Consider your last chatbot implementation and consider the long-tail concept: 
     <ul> 
      <li> List three examples of niche or uncommon user queries that traditional intent-based chatbots may not adequately address. </li> 
      <li> Discuss how these queries exemplify the long-tail phenomenon in conversational AI. </li> 
     </ul></li> 
    <li class="readable-text" id="p101"> For the same chatbot implementation, consider what answers you can provide with traditional searches versus RAG. </li> 
   </ol> 
  </div> 
  <div class="readable-text" id="p102"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.3</span> How is RAG implemented?</h2> 
  </div> 
  <div class="readable-text" id="p103"> 
   <p>As the “retrieval-augmented generation” name suggests, RAG has two phases: retrieval and generation. In the retrieval phase, algorithms search for and retrieve snippets of information relevant to the user’s prompt or question. In an open-domain consumer setting, those facts can come from indexed documents on the internet; in a closed-domain enterprise setting, a narrower set of private sources are typically used for added security and reliability.</p> 
  </div> 
  <div class="readable-text" id="p104"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.3.1</span> High-level implementation</h3> 
  </div> 
  <div class="readable-text" id="p105"> 
   <p>With RAG, the system searches a knowledge base for information relevant to a question and uses that information to generate a conversational answer. Let’s break down the steps:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p106"> The user asks the chatbot a question. </li> 
   <li class="readable-text buletless-item" id="p107"> The system uses its NLU capabilities to determine the intent of the user’s question: 
    <ul> 
     <li> If it recognizes the question with high confidence—for example, if it is one of the intents it was trained on—it will be able to respond, and a search will not be needed. This ends the flow. </li> 
     <li> If it cannot recognize the query, it will go to search. The system will send the user’s query to the search tool to search the document content and produce and rank search results. </li> 
    </ul></li> 
   <li class="readable-text" id="p108"> It passes back the ranked search results to the chatbot for display. (Before RAG, the ranked link and snippet list would have been passed back to the chatbot—handling the long-tail questions with search results was still more helpful than providing a “sorry, I cannot understand” response.) </li> 
   <li class="readable-text" id="p109"> Instead of simply displaying the results, the original question and the search results are sent to an LLM. The LLM may rerank the search results, but most importantly, it generates a concise, summarized, linguistically correct answer. </li> 
   <li class="readable-text" id="p110"> The answer is then passed back to the system. </li> 
   <li class="readable-text" id="p111"> The answer is presented to the user through the chatbot UI. </li> 
  </ol> 
  <div class="readable-text print-book-callout" id="p112"> 
   <p><span class="print-book-callout-head">Note </span> Neither the original user question nor the generated answer needs to match the documents <em>exactly</em>. While verbatim responses can indeed occur and are sometimes even preferred for legal reasons, the primary focus is on grounding the content in the knowledge base, ensuring that the generated answer is rooted in the curated document set.</p> 
  </div> 
  <div class="readable-text" id="p113"> 
   <p>By their nature, LLMs do not generate consistent results each time a query is processed. These models can produce different responses to the same question depending on subtle variations in context or phrasing. This variability is due to the probabilistic nature of LLMs, which generate text based on learned patterns rather than retrieving fixed responses. While this flexibility allows for more nuanced and contextually appropriate answers, it can also lead to expectations of consistent outputs, which is not how these models function.</p> 
  </div> 
  <div class="readable-text intended-text" id="p114"> 
   <p>Emphasizing this point is essential, because team members unfamiliar with how LLMs work often expect consistent results. This expectation can hinder projects, leading to differing approaches to the problem among team members. Understanding that LLMs prioritize relevance and context over the exact replication of document content can help align expectations and improve collaboration within the team.</p> 
  </div> 
  <div class="readable-text intended-text" id="p115"> 
   <p>In some cases, the generated response may closely resemble or even match the wording in the documents. This can occur when the documents contain relevant and informative passages directly addressing the user’s question. In such instances, the RAG model may include verbatim excerpts from the documents in the generated response to provide the user with the most accurate and relevant information.</p> 
  </div> 
  <div class="readable-text intended-text" id="p116"> 
   <p>In other cases, the RAG model uses the information within the knowledge base to understand the context and relevant concepts related to the user’s question. It then uses this understanding to generate a response that aligns with the content found in the documents, even if the specific wording of the user’s question or the generated answer does not exist verbatim within the documents. This approach allows for greater flexibility and adaptability in developing responses that effectively address user queries while drawing upon the information available in the curated document set.</p> 
  </div> 
  <div class="readable-text" id="p117"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.3.2</span> Preparing your document repository for RAG</h3> 
  </div> 
  <div class="readable-text" id="p118"> 
   <p>Let’s also consider how the document content is retrieved during RAG searches. Figure 6.9 provides more detail about creating appropriate data. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p119">  
   <img alt="figure" src="../Images/CH06_F09_Freed2.png" width="927" height="311"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.9</span> RAG uses a vector database during build time and run time.</h5>
  </div> 
  <div class="readable-text" id="p120"> 
   <p>A systematic preprocessing pipeline ensures that both the raw data and the user’s question (or the LLM’s rephrasing of it) are optimized for use in RAG-based searches. This pipeline is crucial for transforming data into embeddings, enabling the model to match the user’s query with relevant information efficiently. Techniques such as cosine similarity or other methods are then applied to identify the best matches, ensuring accurate and contextually appropriate results. The following list outlines the key steps involved in this pipeline, detailing how data is processed before being used in retrieval: </p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p121"> <em>Preprocessing data</em>—The system (typically a data pipeline, not the LLM itself) processes raw documents to make them searchable. For example, PDF documents are converted to text, or table structures are converted to processable statements. Metadata may be added to enhance the original content. The text is then divided into coherent semantic units, called <em>chunks</em>. For instance, a document may be chunked at paragraph boundaries. Chunking is a common process for identifying and extracting meaningful groups of words (“chunks”) from sentences for further analysis or processing. The chunking strategy impacts the overall results. There are open source tools that can help with visualizing and understanding different chunking or splitting strategies.  </li> 
   <li class="readable-text" id="p122"> <em>Embedding generation</em>—An embedding model converts these chunks into embeddings or numerical representations of words or phrases in a high-dimensional vector space. Embeddings capture semantic relationships between words and documents, enabling a more efficient understanding of the connections. Similar meanings or contexts are mapped nearby in the vector space, and dissimilar meanings are mapped to more distant points. This provides more relevant search results than keyword matches. </li> 
   <li class="readable-text" id="p123"> <em>Storage in a vector database</em>—The generated embeddings are stored in a vector database, which enables efficient similarity searches. Each document chunk is indexed using its vector representation, allowing fast retrieval based on meaning rather than exact word matches. </li> 
   <li class="readable-text" id="p124"> <em>Retrieval and matching at runtime</em>—At run time, the end user interacts with the chatbot. Their question will be converted to a vector using the same embedding model, and that vector will be searched in the vector database to find the most relevant passages (chunks) based on semantic similarity. These retrieved passages are then passed to the LLM, which synthesizes them into a response presented to the user.  </li> 
  </ol> 
  <div class="readable-text" id="p125"> 
   <p>Each of these steps ensures that the retrieval process is optimized, making it possible for the LLM to generate accurate, context-aware responses based on the most relevant retrieved data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p126"> 
   <p>Listing 6.1 shows sample code for an embedding function. You can use any custom embedding function or other vector databases, and the performance may differ depending on the embedding model used. This is the most common approach to RAG: you create a dense vector representation of the knowledge base to calculate the semantic similarity to the user queries. For this sample, we used Chroma as the vector database. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p127"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.1</span> Splitting a file into chunks, embedding it, and storing it in a vector database</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma

loader = TextLoader(filename)
documents = loader.load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings()
docsearch = Chroma.from_documents(texts, embeddings)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p128"> 
   <p>At its core, RAG operates by retrieving relevant documents or passages based on a user’s query and then generating a response using natural language generation techniques. This process can be achieved without explicit chunking or embedding by using other methods for document retrieval and language generation. </p> 
  </div> 
  <div class="readable-text intended-text" id="p129"> 
   <p>For example, using Lucene as an alternative to chunking and embeddings involves using its document indexing and retrieval capabilities. Lucene can handle the retrieval part, fetching the most relevant documents based on the query. After retrieval, the generator part of the RAG can take over to produce coherent responses based on the content of the retrieved documents. Lucene is very efficient at text retrieval, which leaves the complex task of generating human-like responses to the more specialized generative components of the RAG model. This approach can be particularly advantageous in systems emphasizing retrieval accuracy and speed over nuanced understanding.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p130"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercises</h5> 
   </div> 
   <ol> 
    <li class="readable-text buletless-item" id="p131"> Text chunking—In this exercise, you will experiment with different chunking strategies and embeddings: 
     <ul> 
      <li> Choose your sample text data (small text files of your choice). </li> 
      <li> Decide on a chunking strategy (splitting by sentences or words). For your experiments, try a chunking tool. </li> 
      <li> Embed the chunks using an open source embedding model, and then load the chunks into a vector database (Chroma). </li> 
     </ul></li> 
    <li class="readable-text buletless-item" id="p132"> Setting up an ingestion pipeline—This exercise guides you through building a simple ingestion pipeline for processing documents in a RAG system: 
     <ul> 
      <li> Choose a document set relevant to your organization’s domain or a specific use case for the chatbot. Start with simple, text-only documents, i.e., no tables, etc. </li> 
      <li> Build the ingestion pipeline, considering factors such as ease of use and compatibility with RAG. For querying, use open source models, Hugging Face embedding models, and a llama index. </li> 
      <li> Implement the ingestion pipeline to preprocess and structure the dataset for use with RAG. </li> 
      <li> Test the ingestion pipeline with sample data to ensure proper functionality and data integrity. </li> 
     </ul></li> 
   </ol> 
  </div> 
  <div class="readable-text" id="p133"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.4</span> Additional considerations of RAG implementations</h2> 
  </div> 
  <div class="readable-text" id="p134"> 
   <p>Traditional search returns links, passages, or the full text of relevant documents, and the user needs to sift through this information to find their answer. RAG conversely returns the answer directly, and the user can optionally see the documents used. </p> 
  </div> 
  <div class="readable-text" id="p135"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.4.1</span> Can’t we just use an LLM directly? </h3> 
  </div> 
  <div class="readable-text" id="p136"> 
   <p>What if the conversational AI passed the user’s query to an LLM and got the answer? After all, LLMs are trained on vast amounts of data. </p> 
  </div> 
  <div class="readable-text intended-text" id="p137"> 
   <p>First, LLMs trained on internet-scale data have limitations due to the nature of their training data. This data represents a snapshot of the training time from publicly available sources—it does not contain business-specific, personal, or classified information, and it doesn’t contain public data created after the cutoff date. Thus, even the newest LLM’s knowledge can become outdated, leading to inaccurate responses over time. RAG addresses this by offering data to LLMs after they are trained.</p> 
  </div> 
  <div class="readable-text intended-text" id="p138"> 
   <p>Second, as LLMs are trained from extensive datasets, it is challenging to trace their responses to sources, undermining the reliability and trustworthiness of the model’s output. RAG is inherently grounded because you know exactly what data was provided to the LLM for a given question. </p> 
  </div> 
  <div class="readable-text intended-text" id="p139"> 
   <p>The broad domain LLMs cover poses another significant challenge. With access to vast information, they may generate responses with high confidence, even when lacking concrete evidence or context. This tendency to produce plausible but incorrect or unverified information is known as <em>hallucination</em>. In contrast, you want your conversational AI to provide correct and grounded answers. Advanced prompting techniques can help mitigate hallucinations, but providing source data through RAG is more reliable. </p> 
  </div> 
  <div class="readable-text intended-text" id="p140"> 
   <p>You also want to prioritize answers grounded in the specific documents or corpus being indexed, not those on which LLM was trained. A RAG system’s primary focus is to provide responses based on the content and context of your documents, so answers are directly generated from the information within the corpus, promoting accuracy, relevance, and trustworthiness in the responses provided to users. </p> 
  </div> 
  <div class="readable-text intended-text" id="p141"> 
   <p>It is important to consider the training data and domain of a specific LLM before selecting it for your use. If the LLM was trained on generic data and you need domain-specific results, it may not produce the desired outcomes. In such cases, you could explore techniques like model blending, where you combine multiple models to use the strengths of each, enhancing performance in specific domains. If you have the resources and data available, you may also consider fine-tuning the selected model to better suit your needs. However, this can require a significant budget for computational resources and data, so consider prompt-tuning first. While fine-tuning costs are decreasing and will continue to do so, they still need to be carefully considered. Other methods are also emerging for domain-specific training, offering further flexibility. </p> 
  </div> 
  <div class="readable-text" id="p142"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.4.2</span> Keeping answers current and relevant with RAG</h3> 
  </div> 
  <div class="readable-text" id="p143"> 
   <p>RAG represents a significant advantage over directly using LLMs for question answering. While RAG still uses LLM for natural language generation, LLM is crafting accurate responses from the searched documents. Real-time retrieval will find up-to-date information when new or updated documents are added during the build phase. RAG ensures the answer reflects the latest documents from the searched knowledge sources. This real-time integration of enterprise content enhances the relevance and accuracy of responses and instills confidence in users, who know that they are receiving current and reliable information.</p> 
  </div> 
  <div class="readable-text intended-text" id="p144"> 
   <p>Furthermore, RAG goes beyond merely accessing enterprise content. It accesses specific passages and retrieves information from multiple documents. This granularity allows RAG to trace and verify answers to their exact sources, providing users with full transparency and trustworthiness. This facilitates accountability and the verification process for your development team too, ensuring you know what your bot is doing and why.</p> 
  </div> 
  <div class="readable-text intended-text" id="p145"> 
   <p>Moreover, RAG defines the domain of the LLM’s understanding, enabling it to recognize the limits of its knowledge and expertise. Unlike LLMs that are used directly and that may attempt to provide answers outside of their domain, RAG can acknowledge when it encounters queries beyond its scope. This ability to say “I don’t know” prevents it from giving inaccurate answers and fosters transparency in conversational interactions. By establishing clear boundaries for its understanding, RAG empowers developers to build AI systems prioritizing accuracy, reliability, and integrity, ultimately enhancing the overall user experience. </p> 
  </div> 
  <div class="readable-text intended-text" id="p146"> 
   <p>There is a difference between “I don’t understand” and “I cannot find an answer to your question.” While the primary goal of RAG is to generate informative and relevant responses based on the content of the retrieved documents, there are scenarios where the system may not find sufficient or appropriate information to generate a meaningful response. In such cases, it is common for the RAG model to acknowledge its inability to provide a satisfactory answer and communicate this to the user.</p> 
  </div> 
  <div class="readable-text intended-text" id="p147"> 
   <p>However, it’s important to note that a RAG system’s specific behavior, such as returning an “I don’t know” response, can be influenced by the retrieval component’s design, the knowledge base’s quality, and the generation model’s settings or parameters. Additionally, developers may choose to implement specific strategies or fallback mechanisms to handle cases where the system cannot generate a response, such as providing alternative suggestions or prompting the user for more information.</p> 
  </div> 
  <div class="readable-text" id="p148"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.4.3</span> How easy is it to set up the ingestion pipeline?</h3> 
  </div> 
  <div class="readable-text" id="p149"> 
   <p>Setting up an ingestion pipeline that effectively preserves document structure is critical for ensuring accurate search results within a retrieval system for RAG. Several key areas must be considered. Essentially, every architectural decision you make about the components will have an influence on the overall accuracy of the results. </p> 
  </div> 
  <div class="readable-text intended-text" id="p150"> 
   <p>First, you must establish mechanisms to connect existing content stores to the retrieval system or migrate content into a new repository. This will allow the retrieval system to access the necessary data and maintain data integrity.</p> 
  </div> 
  <div class="readable-text intended-text" id="p151"> 
   <p>The next challenge is correctly extracting structures (such as headings, tables, and lists) during ingestion. These formatting elements contribute to the document’s organization and clarity. By retaining this structural information during ingestion, the retrieval system can use it to enhance search accuracy and relevance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p152"> 
   <p>There are also challenges related to chunking. The ability to chunk, split, or partition large documents into representative subdocuments for indexing enhances the retrieval process’s efficiency. This allows for more granular indexing and retrieval of information, facilitating quicker access to specific content within lengthy documents. Additionally, selecting appropriate search methodologies, such as vector, semantic, federated, keyword, or hybrid, further augments the retrieval system’s capabilities.</p> 
  </div> 
  <div class="readable-text intended-text" id="p153"> 
   <p>Using LangChain simplifies setting up the ingestion pipeline. Recall that you will need </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p154"> <em>Document loaders</em><em> </em>—Load data from various formats.  </li> 
   <li class="readable-text" id="p155"> <em>Document transformers</em><em> </em>—Process and structure the data for efficient retrieval. </li> 
   <li class="readable-text" id="p156"> <em>Retrievers</em><em> </em>—Fetch the most relevant document chunks during query time.  </li> 
  </ul> 
  <div class="readable-text" id="p157"> 
   <p><em>Document loaders </em>facilitate the ingestion of diverse document formats. These loaders streamline the workflow, ensuring efficient processing and retrieval of pertinent context for LLMs to deliver precise responses. They load data from the source documents, treating each extracted piece as a document comprising textual content and associated metadata. LangChain provides built-in capabilities for handling various files: all files in a directory, PDF, CSV, JSON, HTML, markdown, txt, and more. </p> 
  </div> 
  <div class="readable-text intended-text" id="p158"> 
   <p>For example, you can load text from a web page, transcripts, or corporate documents. Document loaders provide a <code>load</code> method for loading data as documents from a configured source:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p159"> Text loader: </li> 
  </ul> 
  <div class="browsable-container listing-container" id="p160"> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain_community.document_loaders import TextLoader

# Load text data from a file using TextLoader
loader = TextLoader("./your_data/YourText.txt")
document = loader.load()</pre>  
   </div> 
  </div> 
  <ul> 
   <li class="readable-text" id="p161"> CSV loader: </li> 
  </ul> 
  <div class="browsable-container listing-container" id="p162"> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain_community.document_loaders import CSVLoader

# Load data from a CSV file using CSVLoader
loader = CSVLoader("./your_data/Yourspreadsheet.csv")
document = loader.load()</pre>  
   </div> 
  </div> 
  <div class="readable-text list-body-item" id="p163"> 
   <p>Look at LangChain’s documentation on customizing the CSV parsing and loading. For example, you may want to specify your delimiters, field names, etc. Similarly, LangChain provides a <code>DirectoryLoader</code> for all documents in a directory, an <code>UnstructuredHTMLLoader</code> to load HTML docs, and so on for the common types. It is essential to know the <code>AzureAIDocumentIntelligenceLoader</code>, which is useful for Microsoft Office-type documents. </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p164"> Microsoft Document Loader: </li> 
  </ul> 
  <div class="browsable-container listing-container" id="p165"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">%pip install --upgrade --quiet langchain langchain-community 
<span class="">↪</span>azure-ai-documentintelligence

from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader

file_path = "&lt;your_filepath&gt;"
endpoint = "&lt;Your_endpoint&gt;"
key = "&lt;key&gt;"
loader = AzureAIDocumentIntelligenceLoader(
    api_endpoint=endpoint, api_key=key, file_path=file_path, 
    <span class="">↪</span> sapi_model="prebuilt-layout"
)

documents = loader.load()</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p166"> 
   <p>Once you have loaded the documents, you need to look at the <em>document transformers</em>, which can split a long document into smaller chunks that the selected LLM can process. LLMs have a “context window” property, determining the text length they can effectively process in a single pass, so the chunks must fit into the LLM’s context window. It is easy to assume that setting a more extensive context (i.e., longer text passages) would inherently lead to better performance across various language understanding tasks. However, recent studies have revealed that this isn’t always the case. Evidence suggests that language models can achieve improved performance when presented with less text overall, but text that is highly relevant to the task at hand.</p> 
  </div> 
  <div class="readable-text intended-text" id="p167"> 
   <p>A larger context window allows for including more information in the prompt during inference, but this technique, often called <em>prompt stuffing</em>, comes with trade-offs. Processing more text demands greater computational resources, which slows inference and increases costs—particularly for companies paying by the token, where summarizing lengthy documents, like annual reports or meeting transcripts, can become costly. While larger context windows can improve results to some extent, there are diminishing returns. Like humans, LLMs can experience information overload; when presented with excessive detail, they may overlook critical points. Studies have shown that LLMs are more likely to focus on essential information at the beginning or end of a prompt, potentially missing key insights buried in the middle. </p> 
  </div> 
  <div class="readable-text intended-text" id="p168"> 
   <p>We need document transformers to preprocess the documents, extract relevant information, and transform it into a structured representation that the language model can efficiently use during generation. LangChain has several built-in transformers that make document manipulation easy. </p> 
  </div> 
  <div class="readable-text intended-text" id="p169"> 
   <p>The splitting process is as follows:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p170"> Divide the text into smaller chunks. </li> 
   <li class="readable-text" id="p171"> Combine these smaller chunks into larger chunks of a certain size, usually measurable by some function. </li> 
   <li class="readable-text" id="p172"> Once it reaches that size, it becomes the new unit of the text. Then, you create a new text segment with some overlap to maintain context between the fragments. </li> 
  </ol> 
  <div class="readable-text" id="p173"> 
   <p>You can choose your division rules (characters, words, tokens) and how to measure the chunk size. Again, LangChain offers many different types of splitters in the langchain-text-splitters package. These are some examples of text splitters:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p174"> <em>Recursive</em><em> </em>—Splitting text recursively is the recommended way to start. It aims to keep related pieces of text next to each other. </li> 
   <li class="readable-text" id="p175"> <em>HTML</em><em> </em>—A “structure-aware” chunker splits text based on HTML-specific characters; an example is shown in listing 6.2. It splits at the element level, adding metadata to headers for chunk relevance. It preserves semantic grouping and context-rich information in document structures: 
    <ul> 
     <li> <em>Character</em><em> </em>—It breaks the document at user-defined characters (e.g., "\n\n"). </li> 
     <li> <em>Code</em><em> </em>—It employs code syntax and grammar identifiers for languages like Python and JavaScript (and 13 others), organizing code into logical groups. </li> 
     <li> <em>Markdown</em><em> </em>—It identifies markdown language and organizes the document into a structured format (similar to HTML). </li> 
     <li> <em>Tokens</em><em> </em>—It uses a tokenizer, like tiktoken, to split text based on model-defined token limits in the code. </li> 
    </ul></li> 
  </ol> 
  <div class="browsable-container listing-container" id="p176"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 6.2</span> HTML splitter</h5> 
   <div class="code-area-container"> 
    <pre class="code-area"># Install langchain-text-splitters if not already installed
%pip install -qU langchain-text-splitters

# Import necessary modules
import langchain
import langchain_text_splitters

print("Langchain version:", langchain.__version__)
print("Langchain Text Splitters module loaded successfully!")
from langchain_text_splitters import HTMLHeaderTextSplitter
from langchain.schema import Document  # Ensure Document is properly imported

from bs4 import BeautifulSoup

print("BeautifulSoup is installed successfully!")

# Sample HTML content to be split
html_string = """
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;body&gt;
&lt;div&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;This is the introduction section of the document.&lt;/p&gt;
&lt;div&gt;
&lt;h2&gt;Chapter 1: Getting started&lt;/h2&gt;
&lt;p&gt;This section covers the basics of getting started.&lt;/p&gt;
&lt;h3&gt;Section 1.1 Setup&lt;/h3&gt;
&lt;p&gt;This subsection explains the setup process.&lt;/p&gt;
&lt;h3&gt;Section 1.2 Configuration&lt;/h3&gt;
&lt;p&gt;This subsection details the configuration options.&lt;/p&gt;
&lt;/div&gt;
&lt;div&gt;
&lt;h2&gt;Chapter 2: Advanced Techniques&lt;/h2&gt;
&lt;p&gt;This section dives into more advanced techniques.&lt;/p&gt;
&lt;/div&gt;
&lt;br/&gt;  &lt;!-- Fix: Ensuring self-closing tag is correctly formatted --&gt;

&lt;p&gt;What you learned in the Introduction.&lt;/p&gt;
&lt;/div&gt;
&lt;/body&gt;&lt;/html&gt;
"""

# Define header tags to split on (h1, h2, h3 represent different levels of headers)
headers_to_split_on = [
    ("h1", "Header 1"),  # Top-level headers
    ("h2", "Header 2"),  # Subsection headers
    ("h3", "Header 3"),  # Sub-subsection headers
]

# Initialize the HTML header text splitter with the specified header levels
html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

# Split the HTML document into structured chunks
html_header_splits = html_splitter.split_text(html_string)

# Display structured output
for doc in html_header_splits:
    print(f"Content:\n{doc.page_content}\nMetadata: {doc.metadata}\n{'-'*40}")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p177"> 
   <p>Next, we need to deal with the embeddings. The type of data and the language support requirements govern the selection of embedding models. Furthermore, when you are dealing with specific domain or industry terms, these models may have to be extended. </p> 
  </div> 
  <div class="readable-text intended-text" id="p178"> 
   <p>Embedding models in LangChain transform the text into numerical representations, or embeddings, that can be processed. LangChain integrates with different model providers (OpenAI, Cohere, Hugging Face, and more) to generate embeddings. The <code>OpenAIEmbeddings</code> class, for instance, uses the OpenAI API to create embeddings, and this can be done using either OpenAI’s API key or Azure’s OpenAI API key. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p179"> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain.embeddings import OpenAIEmbeddings
embeddings = OpenAIEmbeddings()

text = "This is a test document."
query_result = embeddings.embed_query(text)
query_result[:5]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p180"> 
   <p>Other integrations include <code>CohereEmbeddings</code>, <code>TensorFlowEmbeddings</code>, and <code>HuggingFaceInferenceEmbeddings</code>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p181"> 
   <p>After you have the embeddings, you must store them in a vector database, such as Chroma, which we used earlier. When selecting the vector database, you’ll want to consider run-time performance, how it scales for the size of your data set, and overall performance. Another important consideration is integrating tools like LangChain, which is continually improving. LangChain enhances the capabilities of vector databases by providing streamlined processes for handling embeddings and integrating with various machine learning and AI workflows. This combination ensures efficient data management and retrieval, making it a robust choice for scalable and high-performance applications.</p> 
  </div> 
  <div class="readable-text intended-text" id="p182"> 
   <p><em>Retrievers</em> bridge the gap between embeddings and user queries. While embeddings store numerical representations of documents in a vector database, retrievers identify and fetch the most relevant chunks based on similarity scoring.</p> 
  </div> 
  <div class="readable-text intended-text" id="p183"> 
   <p>The retriever works as follows:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p184"> The user’s query is embedded using the same embedding model used during ingestion. </li> 
   <li class="readable-text" id="p185"> The vector database searches for the most semantically similar embeddings. </li> 
   <li class="readable-text" id="p186"> The retriever fetches the top matches and passes them to the LLM for response generation. </li> 
  </ol> 
  <div class="readable-text" id="p187"> 
   <p>LangChain includes multiple retrieval methods. For example, there is a similarity-based retriever in LangChain:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p188"> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain.vectorstores import Chroma retriever = vector_db.as_retriever(search_type="similarity", search_kwargs={"k": 5}) retrieved_docs = retriever.get_relevant_documents(query)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p189"> 
   <p>Retrievers play a crucial role in returning only the most relevant document chunks, ensuring the LLM works with focused, high-quality context rather than raw, unprocessed data.</p> 
  </div> 
  <div class="readable-text" id="p190"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.4.4</span> Handling latency</h3> 
  </div> 
  <div class="readable-text" id="p191"> 
   <p>A universal best practice for handling latency has yet to be developed. Long response times are frustrating for users, but these techniques can enhance their experience: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p192"> <em>Use a quality vector store with efficient search.</em> The Facebook AI Similarity Search (FAISS) library allows you to search for similar embeddings quickly. There are many purpose-built vector databases, like Chroma, Milvus, Pinecone, and Weaviate, with many more emerging. Traditional databases and search systems like Elasticsearch provide vector search plugins. Each has unique strengths and can be selected based on your needs, including scalability, functionality, performance, and cost. </li> 
   <li class="readable-text" id="p193"> <em>Preprocess and curate your dataset.</em> Having multiple similar versions of the same document increases search time and lowers search result quality. </li> 
   <li class="readable-text" id="p194"> <em>Inform the user before executing a slow action.</em> An appropriate-toned message, such as “Just need a moment,” may placate the user and bridge the delay. </li> 
   <li class="readable-text" id="p195"> <em>Stream responses to show the user the answer as each token is generated.</em> LLMs may take 1.5 to 5 seconds or more to generate an answer, and searches may take 5 to 10 seconds. The user may think the chatbot is broken if the conversational AI waits for the LLM to finish. </li> 
   <li class="readable-text" id="p196"> <em>Consider caching. </em>By caching each user’s vector database and chat history, commonly accessed information relevant to that user’s interactions can be stored locally. This reduces the need to generate responses from scratch every time, saving the computational resources required. While caching may consume additional tokens, the trade-off is improved efficiency. </li> 
  </ul> 
  <div class="readable-text" id="p197"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.4.5</span> When to use a fallback mechanism and when to search</h3> 
  </div> 
  <div class="readable-text" id="p198"> 
   <p>Determining whether to use RAG’s response or to deflect to a human agent involves several key considerations. For instance, the generation part of RAG should not be invoked if the retrieval does not yield appropriate results. In this case, the conversational AI can gracefully exit the query and respond with the offer to pass the user to a human agent. By bypassing the generation of an answer based on potentially subpar search results, you effectively reduce latency for end users, ensuring they receive prompt responses while saving computational resources.</p> 
  </div> 
  <div class="readable-text intended-text" id="p199"> 
   <p>Figure 6.10 illustrates the decision-making process for determining whether to use the RAG response or to hand over the query to a human agent: </p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p200"> The conversational AI processes the user query and passes it to the retrieval system, which searches the knowledge base to find relevant information. </li> 
   <li class="readable-text" id="p201"> The retrieval results are evaluated to determine if they are appropriate for generating a response. </li> 
   <li class="readable-text" id="p202"> If the retrieval results are deemed appropriate, the system generates a response using the full-on RAG approach.<span class="aframe-location"/> </li> 
  </ol> 
  <div class="browsable-container figure-container" id="p203">  
   <img alt="figure" src="../Images/CH06_F10_Freed2.png" width="920" height="150"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.10</span> When the answer from RAG does not match the retrieved passages, it can be better to offer a human agent instead.</h5>
  </div> 
  <ol class=" faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p204"><span class="faux-ol-li-counter">4. </span> If the retrieval results are inappropriate (e.g., insufficient or no results), the system gracefully offers to pass the query to a human agent. </li> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p205"><span class="faux-ol-li-counter">5. </span> The response (either generated by the system or passed to a human agent) is returned to the user. </li> 
  </ol> 
  <div class="readable-text" id="p206"> 
   <h2 class=" readable-text-h2"><span class="num-string">6.5</span> Evaluating and analyzing RAG performance</h2> 
  </div> 
  <div class="readable-text" id="p207"> 
   <p>Evaluating the capabilities of a RAG model within a conversational AI system is multifaceted. Each capability must be evaluated for an overall result. Most evaluations consider three aspects:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p208"> <em>Was it the right response?</em> Did the answer directly address the user’s question? For example, if the user asked about resetting a password, the response should clearly explain the steps rather than discussing broader account security topics. </li> 
   <li class="readable-text" id="p209"> <em>Was it in the right context for this user?</em> Did the response consider the user’s specific situation or history? For example, if a user previously reported an account problem, the system should provide a tailored follow-up solution instead of generic advice. </li> 
   <li class="readable-text" id="p210"> <em>Was it grounded in the documents (or hallucinated or made up by the generation process)?</em> Did the response accurately reflect the information retrieved from the source documents without fabricating details? For example, a response should correctly reference a company’s policy document when explaining return procedures instead of creating nonexistent policies. </li> 
  </ul> 
  <div class="readable-text" id="p211"> 
   <p>An LLM can score responses based on the key criteria to enhance the evaluation process. This approach works best when combined with human review: the LLM provides an initial assessment, and human evaluators then verify the accuracy and contextual relevance of the responses.</p> 
  </div> 
  <div class="readable-text intended-text" id="p212"> 
   <p>These evaluation criteria help determine the truthfulness of the chatbot’s responses. The generated answers should also be accurate if the source documents are accurate and RAG retrieved the correct documents. The evaluation of the responses can be broken down into assessing the different components of RAG, which can be individually evaluated for overall performance, including the quality of the document index, the effectiveness of the retrieval process, and the accuracy of the answer generation.</p> 
  </div> 
  <div class="readable-text" id="p213"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.5.1</span> Indexing metrics </h3> 
  </div> 
  <div class="readable-text" id="p214"> 
   <p>Indexing metrics provide insights into how efficiently a system can organize, store, and retrieve vast amounts of data. Key considerations include indexing speed, storage requirements, scalability, and how well the system handles high-dimensional data like vectors. Table 6.3 summarizes these important aspects, offering a quick overview and relevant examples. </p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p215"> 
   <h5 class=" browsable-container-h5"><span class="num-string">Table 6.3</span> Critical metrics that influence the efficiency and accuracy of a RAG system’s document index</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Aspect 
       </div></th> 
      <th> 
       <div>
         Summary 
       </div></th> 
      <th> 
       <div>
         Example 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Indexing metrics <br/></td> 
      <td>  Evaluates speed, storage needs, and scalability. Critical for large-scale data systems. <br/></td> 
      <td>  Indexing for a news aggregator where speed and scale are crucial <br/></td> 
     </tr> 
     <tr> 
      <td>  Vector database performance <br/></td> 
      <td>  Measures performance in handling high-dimensional data <br/></td> 
      <td>  For technical support, accurate troubleshooting steps must be assembled for multiple documents. E.g., “Why is my device overheating?” <br/></td> 
     </tr> 
     <tr> 
      <td>  Recall rate <br/></td> 
      <td>  Indicates accuracy in retrieving relevant data. High recall is vital for complete retrieval. <br/></td> 
      <td>  In legal document retrieval, high recall ensures all relevant cases are found. <br/></td> 
     </tr> 
     <tr> 
      <td>  Query complexity <br/></td> 
      <td>  Affects performance based on query specifics, dimensionality, and dataset diversity <br/></td> 
      <td>  Financial databases handling complex queries across multiple data points <br/></td> 
     </tr> 
     <tr> 
      <td>  Benchmarking tools <br/></td> 
      <td>  Tools like ANN-Benchmark compare algorithms on metrics like recall versus QPS. <br/></td> 
      <td>  Evaluating which algorithm best balances speed and accuracy for a video search engine <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p216"> 
   <p>The first critical component to assess is the indexing metrics, which involve evaluating the efficiency and effectiveness of organizing and accessing data in a system’s knowledge base. This includes examining factors such as indexing speed, storage requirements, and the scalability of the indexing process. Efficient indexing is crucial for a RAG system, as it impacts the speed and accuracy of information retrieval. Ineffective indexing can result in slow response times and inaccurate data retrieval, compromising response quality. </p> 
  </div> 
  <div class="readable-text intended-text" id="p217"> 
   <p>Vector database performance is another vital metric specializing in storing and retrieving high-dimensional vectors representing complex data, such as text, images, or embeddings. These databases perform approximate rather than exact match searches, necessitating performance evaluation beyond traditional database performance measures like queries per second (QPS) and latency. While these metrics are important for evaluating system speed and responsiveness, they do not directly capture the accuracy of retrieval results. Therefore, besides QPS and latency, the <em>recall rate</em> is another essential performance metric for vector databases. If the vector database performs well, the RAG model can access high-quality, relevant information, leading to more accurate and contextually appropriate generated content. Conversely, poor performance can result in slow retrieval times and irrelevant or less useful data being used for generation. </p> 
  </div> 
  <div class="readable-text intended-text" id="p218"> 
   <p>Consider a customer support scenario where a chatbot powered by a RAG system is utilized to handle inquiries. With a high recall rate, the chatbot accesses a broad range of information from the knowledge base, effectively resolving customer queries and enhancing satisfaction. However, a low recall rate can lead to missed crucial information, resulting in inadequate responses and increased customer frustration. Therefore, the chatbot’s effectiveness significantly depends on its ability to comprehensively retrieve relevant information, underscoring the importance of a high recall rate in such automated support systems.</p> 
  </div> 
  <div class="readable-text intended-text" id="p219"> 
   <p>Query complexity, influenced by factors like the dimensionality and specificity of the query, as well as the data diversity, also affects vector database performance. Higher-dimensional queries require more computational resources because distance calculations between vectors become more complex. This can lead to increased time and memory usage for retrieval tasks. More specific queries might target very narrow segments of the vector space, which can challenge the indexing system to efficiently isolate and retrieve the relevant vectors, especially in large datasets. High query complexity can strain the system, potentially leading to slower retrieval times and less relevant data being returned. Also, complex queries make distinguishing between relevant and irrelevant results difficult. </p> 
  </div> 
  <div class="readable-text intended-text" id="p220"> 
   <p>Imagine a chatbot on an e-commerce platform designed to help customers find products using complex queries involving multiple attributes like brand, color, size, and user ratings. For example, a customer might ask the chatbot for “6.5-sized blue Adidas running shoes with a minimum of a 4-star rating.” This query presents a multifaceted challenge due to its specificity across several dimensions. Each of these attributes represents a different vector in the database.</p> 
  </div> 
  <div class="readable-text intended-text" id="p221"> 
   <p>Benchmarking tools like ANN-Benchmarks and VectorDBBench help evaluate these aspects by comparing different algorithms and configurations, ensuring the RAG system is built on a robust retrieval foundation for consistently high-quality content generation. ANN-Benchmarks plots the recall rate on the <em>x</em>-axis against QPS on the <em>y</em>-axis, illustrating each algorithm’s performance at different retrieval accuracy levels. VectorDBBench displays QPS and recall rates separately. </p> 
  </div> 
  <div class="readable-text" id="p222"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.5.2</span> Retrieval metrics</h3> 
  </div> 
  <div class="readable-text" id="p223"> 
   <p>The next capability is retrieval metrics, which gauge the system’s ability to fetch relevant information from indexed data. Key aspects include retrieval accuracy, precision, recall, and response time. Effective retrieval metrics ensure users receive accurate and relevant responses, boosting satisfaction and trust in the conversational AI system. Table 6.4 summarizes these important aspects, followed by more detailed explanations. </p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p224"> 
   <h5 class=" browsable-container-h5"><span class="num-string">Table 6.4</span> Critical aspects influencing retrieval metrics</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Aspect 
       </div></th> 
      <th> 
       <div>
         Summary 
       </div></th> 
      <th> 
       <div>
         Example 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Retrieval accuracy <br/></td> 
      <td>  Evaluates the system’s ability to retrieve relevant information from indexed data <br/></td> 
      <td>  Ensuring a chatbot retrieves accurate troubleshooting guides from a large dataset <br/></td> 
     </tr> 
     <tr> 
      <td>  Precision and recall <br/></td> 
      <td>  Precision measures relevance of retrieved docs; recall measures how many relevant docs are retrieved. <br/></td> 
      <td>  Balancing precision and recall when retrieving product recommendations in an e-commerce chatbot <br/></td> 
     </tr> 
     <tr> 
      <td>  Context precision and context recall <br/></td> 
      <td>  Specific to RAG: context precision checks relevance; context recall checks coverage of relevant info <br/></td> 
      <td>  Evaluating how well a generated response in a support chatbot matches the query’s context <br/></td> 
     </tr> 
     <tr> 
      <td>  Parameter optimization <br/></td> 
      <td>  Tuning search parameters and algorithms to improve speed, accuracy, and relevance of results <br/></td> 
      <td>  Adjusting FAISS clusters or Elasticsearch settings to improve document retrieval for legal databases <br/></td> 
     </tr> 
     <tr> 
      <td>  Embedding models <br/></td> 
      <td>  The use of different embeddings impacts the retrieval quality by enhancing precision or recall. <br/></td> 
      <td>  Using BERT for precise context understanding in a legal advice chatbot <br/></td> 
     </tr> 
     <tr> 
      <td>  Filtering and reranking <br/></td> 
      <td>  Strategies to remove noise and rerank results to improve both relevance and accuracy <br/></td> 
      <td>  Filtering out irrelevant articles in a news aggregation chatbot and then reranking top results <br/></td> 
     </tr> 
     <tr> 
      <td>  Normalized Discounted Cumulative Gain (NDCG) <br/></td> 
      <td>  Assesses the ranking quality by considering relevance and position of retrieved documents <br/></td> 
      <td>  Ensuring the most relevant help articles appear at the top in a technical support chatbot <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p225"> 
   <p>These metrics evaluate search quality, document relevance, and how well user queries align with responses. <em>Retrieval accuracy</em> measures the system’s ability to fetch the most relevant information from its indexed data. If retrieval accuracy is low, the chatbot may return responses that are only loosely related to the user’s query or fail to retrieve critical details. </p> 
  </div> 
  <div class="readable-text intended-text" id="p226"> 
   <p>Precision measures the proportion of relevant retrieved documents. Recall measures how many relevant documents are retrieved by the system. A high recall value indicates the system retrieves many relevant documents from the database. In contrast, a high precision value indicates that the retrieved documents are mostly relevant to the user’s query. Balancing recall and precision is crucial to ensure comprehensive coverage of relevant information and minimize irrelevant results.</p> 
  </div> 
  <div class="readable-text intended-text" id="p227"> 
   <p>Specific to RAG models, context precision and context recall evaluate the alignment and coverage of generated responses relative to the user’s query. Context precision measures how precisely the retrieved context matches the user’s query, indicating the relevance and accuracy of the generated response. Context recall measures how comprehensively the generated response covers relevant information from the retrieved context. </p> 
  </div> 
  <div class="readable-text intended-text" id="p228"> 
   <p>You should implement various strategies to enhance the retrieval process:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p229"> Optimizing search parameters </li> 
   <li class="readable-text" id="p230"> Using different embedding models </li> 
   <li class="readable-text" id="p231"> Implementing filtering and reranking </li> 
  </ul> 
  <div class="readable-text" id="p232"> 
   <p>The first strategy is to adjust retrieval parameters to improve both speed and accuracy. Adjusting the search parameters, such as the number of clusters in FAISS or the search query complexity in Elasticsearch, can significantly enhance the precision and recall of retrieved documents. This ensures that the system returns the most relevant documents, increasing precision and retrieving all pertinent documents, boosting recall.</p> 
  </div> 
  <div class="readable-text intended-text" id="p233"> 
   <p>Parameter optimization can also reduce response time, making the system more responsive. Tuning the indexing and query algorithms can lead to faster retrieval times without compromising accuracy.</p> 
  </div> 
  <div class="readable-text intended-text" id="p234"> 
   <p>The second strategy is to utilize different embedding models to find the most effective combination for retrieving relevant documents. Embeddings can drastically affect retrieval quality. For example, BERT and Sentence-BERT can improve the system’s understanding of the context and semantics of user queries, enhancing precision. GPT-3 embeddings can offer a broader contextual grasp, improving recall by retrieving more relevant documents from diverse contexts.</p> 
  </div> 
  <div class="readable-text intended-text" id="p235"> 
   <p>The third strategy is to implement filtering and reranking. Filtering and reranking strategies further enhance precision by removing irrelevant documents and boost recall by prioritizing relevant documents. Techniques like domain-specific filters or context-aware reranking can significantly refine the relevance of top results.</p> 
  </div> 
  <div class="readable-text intended-text" id="p236"> 
   <p>Normalized Discounted Cumulative Gain (NDCG) is useful when the order of retrieved documents matters. It measures the effectiveness of ranking algorithms by evaluating the relevance and position of documents in the search results, providing a comprehensive ranking quality assessment.</p> 
  </div> 
  <div class="readable-text" id="p237"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.5.3</span> Generation metrics</h3> 
  </div> 
  <div class="readable-text" id="p238"> 
   <p>The last area for evaluation is the generation itself: whether the generated output provides a relevant and complete answer. Table 6.5 offers a concise overview of these key metrics, strategies for enhancing generation quality, and practical examples of their application, followed by further details. </p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p239"> 
   <h5 class=" browsable-container-h5"><span class="num-string">Table 6.5</span> Critical aspects influencing generation metrics</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Aspect 
       </div></th> 
      <th> 
       <div>
         Summary 
       </div></th> 
      <th> 
       <div>
         Example 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Faithfulness <br/></td> 
      <td>  Evaluates the factual accuracy of generated output based on retrieved context <br/></td> 
      <td>  Ensuring chatbot answers are factually correct when responding to questions. E.g., “What are the consequences of breaching a contract?” <br/></td> 
     </tr> 
     <tr> 
      <td>  Answer relevancy <br/></td> 
      <td>  Assesses how relevant the generated answer is to the specific user question <br/></td> 
      <td>  “What is the status of my order?” The chatbot, after retrieving the relevant data, responds with, “Your order #12345 is currently in transit and expected to be delivered by August 15th.” This response is directly relevant to the user’s question, providing specific information about the order status without unnecessary details. <br/></td> 
     </tr> 
     <tr> 
      <td>  Fine-tuning <br/></td> 
      <td>  Improves generation by aligning the LLM with domain- specific data, enhancing accuracy and relevance <br/></td> 
      <td>  Fine-tuning an LLM for legal advice, ensuring generated responses are accurate and legally relevant <br/></td> 
     </tr> 
     <tr> 
      <td>  Prompt engineering <br/></td> 
      <td>  Crafting prompts to guide the LLM in generating more contextually appropriate and relevant responses <br/></td> 
      <td>  Using prompt engineering to ensure a healthcare chatbot provides clear, relevant medical suggestions <br/></td> 
     </tr> 
     <tr> 
      <td>  Model blending <br/></td> 
      <td>  Combining specialized models to enhance the quality of generation, balancing accuracy and fluency <br/></td> 
      <td>  Blending a retrieval-focused model with a language-focused model to generate accurate, fluent responses <br/></td> 
     </tr> 
     <tr> 
      <td>  Sensibleness and Specificity Average (SSA) <br/></td> 
      <td>  Measures response quality in open-domain chatbots, ensuring responses are sensible and specific <br/></td> 
      <td>  Assessing open-domain chatbot responses to ensure they make sense and are not overly vague <br/></td> 
     </tr> 
     <tr> 
      <td>  Faithfulness- Evaluator <br/></td> 
      <td>  Assesses whether the generated response avoids hallucinations by aligning with retrieved context <br/></td> 
      <td>  Using FaithfulnessEvaluator to ensure that a finance chatbot’s responses are grounded in retrieved documents <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p240"> 
   <p>Two primary metrics for assessing LLM performance are faithfulness and answer relevancy. Faithfulness evaluates the factual accuracy of the answer based on the retrieved context, while answer relevancy assesses how pertinent the answer is to the given question. An answer may be factually accurate (faithful) but not well-matched to the question (less relevant), or it may be accurate but not based on sourced documents.</p> 
  </div> 
  <div class="readable-text intended-text" id="p241"> 
   <p>Several strategies can be employed to enhance generation metrics. Fine-tuning the LLM on domain-specific data can improve faithfulness and relevancy by aligning the model more closely with the context in which it will operate. This ensures that generated responses are accurate and pertinent to the specific domain. You may also want to incorporate prompt engineering. The generation process becomes more focused and aligned with the user’s intent by explicitly including necessary context or constraints within the prompt. Model blending may also be used. Combining multiple models, each specialized in different aspects of the task, can help enhance generation quality. For example, one model may excel at retrieving accurate information, while another might be better at generating fluent and contextually appropriate language. Blending these models can lead to more balanced and effective output.</p> 
  </div> 
  <div class="readable-text intended-text" id="p242"> 
   <p>Sensibleness and Specificity Average (SSA) metrics evaluate response quality in open-domain chatbots. Sensibleness ensures that responses make contextual sense, while the specificity average ensures comprehensiveness without vagueness. Historically, human interactions were necessary for assigning these ratings.</p> 
  </div> 
  <div class="readable-text intended-text" id="p243"> 
   <p>While avoiding vague responses is essential, preventing LLMs from hallucinating is equally critical. LlamaIndex established a FaithfulnessEvaluator metric to measure hallucination by assessing whether the response aligns with the retrieved context. LlamaIndex was developed to address the challenge of connecting LLMs with various data sources, so LLMs can access, query, and generate insights from structured and unstructured data. It is a framework for building context-augmented generative AI applications. It offers capabilities for data integration, indexing, enrichment query processing, and more. Stay current with the latest updates from LlamaIndex. </p> 
  </div> 
  <div class="readable-text intended-text" id="p244"> 
   <p>RAG-integrated conversational AI efficiently addresses rare or complex queries. Nevertheless, realizing these advantages from RAG necessitates ongoing monitoring of all components, particularly where failures are common, such as during retrieval and generation. </p> 
  </div> 
  <div class="readable-text" id="p245"> 
   <h3 class=" readable-text-h3"><span class="num-string">6.5.4</span> Comparing efficiency of indexing and embedding solutions for RAG</h3> 
  </div> 
  <div class="readable-text" id="p246"> 
   <p>In the previous sections, we introduced the contributing factors to the efficiency of RAG systems. The indexing and embedding components are crucial for effectively retrieving relevant documents and ensuring the system’s responsiveness. When implementing RAG in a conversational AI system, it is important to measure its impact on retrieval accuracy and response relevance. Benchmarking is essential to creating an effective RAG system. You must have a method to evaluate whether changes in the system prompt improve user-query hit rates. Is the improvement 1%, 2%, or more? This is fundamental to knowing if your RAG system is truly effective. </p> 
  </div> 
  <div class="readable-text intended-text" id="p247"> 
   <p>Furthermore, without proper monitoring, validation, and evaluation, it will be difficult to prove the effectiveness of your system. RAG is inherently complex, and typical implementations have 50% to 60% accuracy at first. You’ll want to increase the accuracy to above 80% for a practical solution. </p> 
  </div> 
  <div class="readable-text intended-text" id="p248"> 
   <p>PharmaBot, initially developed as a COVID-19 chatbot to handle general inquiries, such as vaccine information and appointment scheduling, is now set to be enhanced with RAG. The goal is to extend PharmaBot’s capabilities to answer more nuanced questions, such as “Can I take ibuprofen with my blood pressure medication?” and “My arms are sore after getting the vaccine. What should I do?” First, we’ll want to select a dataset of medical articles, research papers, and guidelines from health organizations, all focusing on various health problems and drug and vaccine interactions. Next, we’ll compile a set of representative queries like those previously listed, which are what users might ask. </p> 
  </div> 
  <div class="readable-text intended-text" id="p249"> 
   <p>We’ll select the indexing solutions and the embeddings we want to use. For indexing, we’ll consider several options, such as FAISS and Elasticsearch. The most popular source for the latest performance benchmarks of text embedding models is the MTEB leaderboards hosted by Hugging Face. While MTEB provides a valuable starting point, the displayed results are self-reported, and many models may not perform as accurately when applied to real-world data. BERT, Sentence-BERT, or GPT-3 embeddings are worth considering, as they have been used in many solutions. Then we’ll run our selections to generate embeddings for our content and index our embeddings using the selected methods. Finally, we’ll run our queries and measure our performance. </p> 
  </div> 
  <div class="readable-text intended-text" id="p250"> 
   <p>When evaluating PharmaBot enhanced with RAG, we can use table 6.6 to determine relevant metrics, establish baselines and goals for these metrics to assess improvements introduced by different combinations of indexing (FAISS, Elasticsearch) and embedding (BERT, Sentence-BERT, GPT-3) solutions, and compare solutions. For example, we could compare how the combinations of RAG components perform against business objectives.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p251"> 
   <h5 class=" browsable-container-h5"><span class="num-string">Table 6.6</span> Prioritized metrics for RAG evaluation based on business objectives</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Business objective 
       </div></th> 
      <th> 
       <div>
         Prioritized metrics 
       </div></th> 
      <th> 
       <div>
         Why? 
       </div></th> 
      <th> 
       <div>
         Example 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Customer satisfaction <br/></td> 
      <td>  Response accuracy <br/>  Relevance <br/></td> 
      <td>  Directly impacts user experience and satisfaction <br/></td> 
      <td>  PharmaBot providing accurate answers to users’ queries <br/></td> 
     </tr> 
     <tr> 
      <td>  Operational efficiency <br/></td> 
      <td>  Latency <br/>  Throughput <br/></td> 
      <td>  Ensures the system can handle high query volumes quickly <br/></td> 
      <td>  A customer support chatbot for a large e-commerce platform should prioritize low latency to provide quick responses during peak shopping times. <br/></td> 
     </tr> 
     <tr> 
      <td>  Scalability <br/></td> 
      <td>  Queries processed per second <br/>  Resource utilization <br/></td> 
      <td>  Evaluates system performance under increasing loads <br/></td> 
      <td>  Chatbot for a healthcare provider managing seasonal spikes in appointments and queries <br/></td> 
     </tr> 
     <tr> 
      <td>  Cost-effectiveness <br/></td> 
      <td>  CPU usage <br/>  Memory usage <br/></td> 
      <td>  Ensures high performance without excessive resource consumption <br/></td> 
      <td>  Chatbot for a non-profit organization providing 24/7 mental health support on limited funding <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p252"> 
   <p>You could create a comparison table like the one in table 6.7. These sample numbers are hypothetical and should be adjusted based on actual selected components and benchmarking results. For example, one configuration may be selecting Elasticsearch and then using three different embedding models to arrive at your numbers. </p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p253"> 
   <h5 class=" browsable-container-h5"><span class="num-string">Table 6.7</span> Evaluation of PharmaBot with various configurations</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Metrics 
       </div></th> 
      <th/> 
      <th> 
       <div>
         Configuration 1 
       </div></th> 
      <th> 
       <div>
         Configuration 2 
       </div></th> 
      <th> 
       <div>
         Configuration 3 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Response accuracy <br/></td> 
      <td>  Recall <br/></td> 
      <td>  0.85 <br/></td> 
      <td>  0.87 <br/></td> 
      <td>  0.88 <br/></td> 
     </tr> 
     <tr> 
      <td/> 
      <td>  Precision <br/></td> 
      <td>  0.75 <br/></td> 
      <td>  0.77 <br/></td> 
      <td>  0.78 <br/></td> 
     </tr> 
     <tr> 
      <td/> 
      <td>  F1 score <br/></td> 
      <td>  0.8 <br/></td> 
      <td>  0.82 <br/></td> 
      <td>  0.83 <br/></td> 
     </tr> 
     <tr> 
      <td>  Relevance <br/></td> 
      <td>  Mean reciprocal rank (MMR) <br/></td> 
      <td>  0.70 <br/></td> 
      <td>  0.72 <br/></td> 
      <td>  0.74 <br/></td> 
     </tr> 
     <tr> 
      <td/> 
      <td>  Average precision <br/></td> 
      <td>  0.65 <br/></td> 
      <td>  0.68 <br/></td> 
      <td>  0.70 <br/></td> 
     </tr> 
     <tr> 
      <td>  Latency <br/></td> 
      <td>  Average latency (ms) <br/></td> 
      <td>  50 <br/></td> 
      <td>  55 <br/></td> 
      <td>  60 <br/></td> 
     </tr> 
     <tr> 
      <td>  Throughput <br/></td> 
      <td>  Queries/second <br/></td> 
      <td>  20 <br/></td> 
      <td>  18 <br/></td> 
      <td>  16 <br/></td> 
     </tr> 
     <tr> 
      <td>  Resource utilization <br/></td> 
      <td>  CPU usage (%) <br/></td> 
      <td>  70 <br/></td> 
      <td>  65 <br/></td> 
      <td>  90 <br/></td> 
     </tr> 
     <tr> 
      <td/> 
      <td>  Memory usage <br/></td> 
      <td>  8 <br/></td> 
      <td>  7 <br/></td> 
      <td>  12 <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p254"> 
   <p>When trying to satisfy the overall business requirements for PharmaBot using RAG, various trade-offs must be considered based on the evaluation metrics. An indexing component combined with selected embedding components offers high response accuracy and relevance with lower latency and higher throughput, making them suitable for systems requiring quick and accurate responses. However, these combinations exhibit moderate to high resource utilization, which may increase operational costs. Conversely, integrating another embedding component with various indexing strategies provides superior response accuracy and relevance but at the cost of significantly higher latency and lower throughput due to the computational demands of the selected embeddings. This can impact the system’s ability to handle high query volumes efficiently. The high CPU and memory usage may also strain resources, increasing operational costs. Ultimately, selecting the optimal combination requires balancing the need for high accuracy and relevance with the system’s capacity to handle queries efficiently while managing resource utilization to control costs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p255"> 
   <p>Furthermore, the ongoing re-evaluation and refinement of the RAG system should be considered. Table 6.7 is not comprehensive, but the key point is to decide on key evaluation metrics and then use a structured evaluation approach. Systematic testing and focusing on the RAG component provide a robust RAG evaluation pipeline. The overall goal is to see an upward trend at the end, as illustrated in figure 6.11. Systematically applying the strategies discussed throughout this chapter and then analyzing the results reveal the impact of different configurations on RAG performance. Some tweaks show significant improvements, emphasizing the importance of experimentation and tuning. There is no best approach; exploring multiple directions when tuning your RAG systems is crucial. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p256">  
   <img alt="figure" src="../Images/CH06_F11_Freed2.png" width="790" height="360"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 6.11</span> Different configurations and enhancements improve the accuracy of the RAG system.</h5>
  </div> 
  <div class="readable-text" id="p257"> 
   <p>Additionally, you may want to use RAG evaluation frameworks, from proprietary paid solutions to open source tools. Selecting the right solution requires balancing considerations around ease of maintenance and operational burden, plus how well the metrics observed by the tool map to your RAG pipeline and your business objectives. The following solutions are current examples, but more are being developed, providing even more options:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p258"> <em>Arize</em><em> </em>—A model monitoring platform focusing on precision, recall, and F1 score. It is beneficial in scenarios requiring ongoing performance tracking, ensuring RAG systems consistently meet accuracy thresholds in real-time applications. Arize is a proprietary paid offering that provides robust support and continuous updates for enterprise deployments. </li> 
   <li class="readable-text" id="p259"> <em>RAGAS</em><em> </em>—An open-source tool that offers streamlined, reference-free evaluation focusing on average precision (AP) and custom metrics like faithfulness. It assesses how well the generated content aligns with provided contexts, and it is suitable for initial assessments or when reference data is scarce.  </li> 
  </ul> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p260"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercises</h5> 
   </div> 
   <ol> 
    <li class="readable-text buletless-item" id="p261"> Assess the relevance of responses generated by a RAG model within a conversational AI system: 
     <ul> 
      <li> Define evaluation criteria to measure the relevance of responses generated by the RAG model. </li> 
      <li> Establish a scoring system to quantify the relevance of responses based on factors such as semantic similarity and informativeness. </li> 
      <li> Devise a set of user queries you will evaluate. </li> 
      <li> Create a set of expected responses (manually). </li> 
      <li> Compare the generated responses with the previously created responses to determine the level of relevance. </li> 
      <li> Calculate evaluation metrics such as precision, recall, and F1 score to quantitatively assess the performance of the RAG model in generating relevant responses. </li> 
      <li> Analyze the evaluation results to identify patterns or areas where the RAG model excels or fails to generate relevant responses. </li> 
      <li> Discuss potential factors influencing response relevance and strategies for improving the RAG model’s performance in this aspect. </li> 
     </ul></li> 
    <li class="readable-text buletless-item" id="p262"> Evaluate document grounding with RAG: 
     <ul> 
      <li> Generate responses to user queries using the RAG model, and identify the source documents or passages from which the responses are derived. </li> 
      <li> Assess the degree of grounding by comparing the relevance of the source documents or passages to the corresponding user queries. </li> 
      <li> Develop a scoring mechanism to quantify the RAG model’s grounding effectiveness based on factors such as document relevance and coverage. </li> 
     </ul></li> 
   </ol> 
  </div> 
  <div class="readable-text" id="p263"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p264"> Traditional intent-based chatbots can be greatly enhanced by integrating search functionality. </li> 
   <li class="readable-text" id="p265"> Intents are great for answering common short-head questions, and search is great for long-tail questions. </li> 
   <li class="readable-text" id="p266"> Traditional search returns links or document passages instead of an answer. </li> 
   <li class="readable-text" id="p267"> RAG extends search capability by generating an answer from the documents retrieved by the search. </li> 
   <li class="readable-text" id="p268"> By using RAG, chatbots can provide contextually appropriate responses in real time, reducing user frustration and enhancing the conversational experience. Grounding answers in the organization’s domain also solves intent maintenance and enhancement for developers. </li> 
   <li class="readable-text" id="p269"> RAG implementations must consider several problems, from handling latency to providing fallback mechanisms or handover to human agents to prevent hallucinations. </li> 
   <li class="readable-text" id="p270"> Evaluation of RAG must consider the different components of indexing, retrieval, and generation. </li> 
  </ul>
 </div></div></body></html>