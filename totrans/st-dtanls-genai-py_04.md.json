["```py\npip install pandas nltk\n```", "```py\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\n\n*# Make sure to download Portuguese stopwords*\nnltk.download('stopwords')\nnltk.download('punkt')\n\n*# Load your DataFrame*\ndf = pd.read_csv('olist_order_reviews_dataset.csv')\n\n*# Preprocessing function*\ndef preprocess_text(text):\n *# Tokenize, convert to lowercase, and remove punctuation*\n    tokens = word_tokenize(text.lower())\n    words = [word for word in tokens if word.isalpha()]\n\n *# Remove Portuguese stopwords*\n    stop_words = set(stopwords.words('portuguese'))\n    filtered_words = [word for word in words if word not in stop_words]\n\n    return filtered_words\n\n*# Apply preprocessing to the 'review_comment_message' column*\ndf['processed_review'] = \n↪df['review_comment_message'].apply(preprocess_text)\n\n*# Calculate word frequencies*\nword_freq = Counter()\nfor _, row in df.iterrows():\n    word_freq.update(row['processed_review'])\n\n*# Print the most common words and their frequencies*\nprint(word_freq.most_common(10))\n```", "```py\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom collections import Counter\n\n*# Make sure to download Portuguese stopwords*\nnltk.download('stopwords')\nnltk.download('punkt')\n\n*# Load your DataFrame*\ndf = pd.read_csv('olist_order_reviews_dataset.csv')\n\n*# Preprocessing function*\ndef preprocess_text(text):\n *# Tokenize, convert to lowercase, and remove punctuation*\n    tokens = word_tokenize(text.lower(), language='portuguese')\n    words = [word for word in tokens if word.isalpha()]\n\n *# Remove Portuguese stopwords*\n    stop_words = set(stopwords.words('portuguese'))\n    filtered_words = [word for word in words if word not in stop_words]\n\n    return filtered_words\n\n*# Apply preprocessing to the 'review_comment_message' column*\ndf['processed_review'] = \n↪df['review_comment_message'].apply(preprocess_text)\n\n*# Calculate word frequencies*\nword_freq = Counter()\nfor _, row in df.iterrows():\n    word_freq.update(row['processed_review'])\n\n*# Print the most common words and their frequencies*\nprint(word_freq.most_common(10))\n```", "```py\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.collocations import TrigramAssocMeasures, \n↪TrigramCollocationFinder\nfrom collections import Counter\n\n*# Make sure to download Portuguese stopwords*\nnltk.download('stopwords')\nnltk.download('punkt')\n\n*# Load your DataFrame*\ndf = pd.read_csv('olist_order_reviews_dataset.csv')\n\n*# Preprocessing function*\ndef preprocess_text(text):\n    # Tokenize, convert to lowercase, and remove punctuation\n    tokens = word_tokenize(text.lower(), language='portuguese')\n    words = [word for word in tokens if word.isalpha()]\n\n    # Remove Portuguese stopwords\n    stop_words = set(stopwords.words('portuguese'))\n    filtered_words = [word for word in words if word not in stop_words]\n\n    return filtered_words\n\n*# Apply preprocessing to the 'review_comment_message' column*\ndf['processed_review'] = \n↪df['review_comment_message'].apply(preprocess_text)\n\n*# Calculate trigram frequencies*\ntrigram_measures = TrigramAssocMeasures()\ntrigram_freq = Counter()\n\nfor _, row in df.iterrows():\n    finder = TrigramCollocationFinder.from_words(row['processed_review'])\n    trigram_freq.update(finder.ngram_fd)\n\n*# Print the most common trigrams and their frequencies*\nprint(trigram_freq.most_common(10))\n```", "```py\n('chegou', 'antes', 'prazo') - (arrived, before, deadline)\n('bem', 'antes', 'prazo') - (well, before, deadline)\n('entregue', 'antes', 'prazo') - (delivered, before, deadline)\n('produto', 'chegou', 'antes') - (product, arrived, before)\n('entrega', 'antes', 'prazo') - (delivery, before, deadline)\n('chegou', 'bem', 'antes') - (arrived, well, before)\n('produto', 'entregue', 'antes') - (product, delivered, before)\n('entrega', 'super', 'rápida') - (delivery, super, fast)\n('antes', 'prazo', 'previsto') - (before, deadline, expected)\n('produto', 'ótima', 'qualidade') - (product, great, quality)\n```", "```py\nimport matplotlib.pyplot as plt\n\n*# Frequency analysis results with English translations*\nword_freq = {\n    'product': 18344,\n    'deadline': 8410,\n    'delivery': 6486,\n    'before': 5619,\n    'arrived': 5535,\n    'received': 5262,\n    'good': 4592,\n    'recommend': 4269,\n    'delivered': 3769,\n    'came': 3276\n}\n\n*# Create a word cloud*\nwordcloud = WordCloud(width=800, height=400, \n↪background_color='white').generate_from_frequencies(word_freq)\n\n*# Display the word cloud*\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')  *# Hide the axes*\nplt.show()\n```", "```py\n*# Preprocessing function*\ndef preprocess(text):\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n    words = text.split()\n    words = [word for word in words if word not in \n    ↪stopwords.words('portuguese')]\n    return words\n\n*# Co-occurrence function*\ndef co_occurrence_matrix(corpus, window_size=5):\n    vocab = set(corpus)\n    vocab = list(vocab)\n    vocab_index = {word: i for i, word in enumerate(vocab)}\n    n = len(vocab)\n    co_occurrence_matrix = np.zeros((n, n))\n\n    for row in data['review_comment_message']:\n        tokens = preprocess(row)\n        for i, word in enumerate(tokens):\n            for j in range(max(i - window_size, 0), min(i + window_size + \n            ↪1, len(tokens))):\n                if i != j:\n                    co_occurrence_matrix[vocab_index[word], \n                    ↪vocab_index[tokens[j]]] += 1\n\n    return co_occurrence_matrix, vocab_index\n\n*# Calculate co-occurrence matrix*\ncorpus = [word for row in data['review_comment_message'] for word in \n↪preprocess(row)]\nco_matrix, vocab_index = co_occurrence_matrix(corpus)\n\n*# Visualize the co-occurrence matrix using a heatmap*\nplt.figure(figsize=(15, 15))\nsns.heatmap(co_matrix, xticklabels=vocab_index.keys(), \n↪yticklabels=vocab_index.keys(), cmap=\"YlGnBu\")\nplt.show()\n```", "```py\n<SOME IMPORTS HERE>\nfrom scipy.sparse import lil_matrix\n\n<DATA LOADING>\n\n*# Preprocessing function*\ndef preprocess(text):\n    text = re.sub(r'[^\\w\\s]', '', text).lower()\n    words = text.split()\n    words = [word for word in words if word not in stopwords_set]\n    return words\n\n*# Co-occurrence function*\ndef co_occurrence_matrix(corpus, vocab, window_size=5):\n    vocab_index = {word: i for i, word in enumerate(vocab)}\n    n = len(vocab)\n    co_occurrence_matrix = lil_matrix((n, n), dtype=np.float64)\n\n    for row in data['review_comment_message']:\n        tokens = preprocess(row)\n        for i, word in enumerate(tokens):\n            for j in range(max(i - window_size, 0), min(i + window_size + \n            ↪1, len(tokens))):\n                if i != j and word in vocab_index and tokens[j] in \n                ↪vocab_index:\n                    co_occurrence_matrix[vocab_index[word], \n                    ↪vocab_index[tokens[j]]] += 1\n\n    return co_occurrence_matrix, vocab_index\n\n*# Calculate co-occurrence matrix*\ncorpus = [word for row in data['review_comment_message'] for word in \n↪preprocess(row)]\nword_counts = Counter(corpus)\ntop_n_words = 30  *# Adjust this number according to your needs*\nmost_common_words = [word for word, count in \n↪ word_counts.most_common(top_n_words)]\nco_matrix, vocab_index = co_occurrence_matrix(corpus, most_common_words)\n\n*# English translations of the words (this part was added by ChatGPT when we* \n↪*asked directly for translations)*\ntranslations = {\n    'produto': 'product',\n    'prazo': 'deadline',\n<MORE TRANSLATIONS HERE>\n}\n\n*# Create a list of translated words for visualization*\ntranslated_words = [translations.get(word, word) for word in \n↪vocab_index.keys()]\n\n*# Visualize the co-occurrence matrix using a heatmap*\nplt.figure(figsize=(15, 15))\nsns.heatmap(co_matrix.toarray(), xticklabels=translated_words, \n↪yticklabels=translated_words, cmap='coolwarm', annot=False)\nplt.title(\"Co-occurrence Matrix Heatmap with English Translations\")\nplt.xlabel(\"Words\")\nplt.ylabel(\"Words\")\nplt.show()\n```", "```py\nfrom openai import OpenAI\n\ndef generate_keywords(temperature=0.5, max_tokens=150):\n    client = OpenAI(\n        api_key=\"your_api_key\", *# Make sure to use your actual API key here*\n    )\n\n    prompt = \"\"\"Generate a list of 20 keywords indicating positive \n    ↪sentiment to be used for searching customer reviews in Portuguese.\"\"\"\n\n    try:\n        response = client.chat.completions.create(\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": prompt,\n                }\n            ],\n            model=\"gpt-4-0125-preview\", *# Replace with the model you have* \n            ↪*access to*\n            temperature=temperature,\n            max_tokens=max_tokens,\n        )\n        return(response.choices[0].message.content.split(\"\\n\"))\n\n    except Exception as e:\n        print(f\"An error occurred: {str(e)}\")\n\n*# Example usage*\nprint(\"Generated Keywords:\")\nprint(generate_keywords())\n```", "```py\n*# Importing libraries and data.*\nimport pandas as pd\ndf = pd.read_csv('olist_order_reviews_dataset.csv')\n\n*# List of keywords proposed by ChatGPT.*\nkeywords = [\n    \"excelente\", \"ótimo\", \"maravilhoso\", \"incrível\", \"fantástico\",\n    \"perfeito\", \"bom\", \"eficiente\", \"durável\", \"confiável\",\n    \"rápido\", \"custo-benefício\", \"recomendo\", \"satisfeito\",\n    \"surpreendente\", \"confortável\", \"fácil de usar\", \"funcional\",\n    \"melhor\", \"vale a pena\"\n]\n\n*# Second version of the keyword search function proposed by ChatGPT that* \n↪*copes with NaNs in the input.*\ndef is_positive(review, keywords):\n    if not isinstance(review, str):\n        return False\n\n    for keyword in keywords:\n        if keyword.lower() in review.lower():\n            return True\n    return False\n\n*# Applying the function to the test DataFrame. Variable names were adapted* \n↪*manually.*\ndf['positive_review'] = df['review_comment_message'].apply(lambda x: \n↪is_positive(x, keywords))\n```", "```py\n*# Remove rows that don't have reviews.*\ndf = df.dropna(subset = ['review_comment_message'])\n\n*# Extract records with positive reviews assessed by keywords and by review* \n↪*scores.*\nposrev_keyword = df[df['positive_review']==True]\nposrev_score = df[(df[\"review_score\"]==5)|(df[\"review_score\"]==4)]\n\n*# Perform set operations to determine true positives (TP), false positives* \n↪*(FP), false negatives (FN) and true negatives (TN).*\nTP = pd.merge(posrev_keyword, posrev_score)\nFP = posrev_keyword[posrev_keyword[\"review_id\"].\n↪isin(posrev_score[\"review_id\"]) == False]\nFN = posrev_score[posrev_score[\"review_id\"].\n↪isin(posrev_keyword[\"review_id\"]) == False]\nTN = df[(df[\"review_id\"].isin(posrev_keyword[\"review_id\"]) == False) & \n↪(df[\"review_id\"].isin(posrev_score[\"review_id\"]) == False)]\n\n*# Calculate sensitivity and specificity*\nprint(\"Sensitivity: \", round(len(TP) / (len(TP) + len(FN)),2))\nprint(\"Specificity: \", round(len(TN) / (len(TN) + len(FP)),2))\n```", "```py\nimport string\ndef remove_punctuation(text):\n        return text.translate(str.maketrans(\"\", \"\", string.punctuation))\ndf['cleaned_review_text'] = df['review_text'].apply(remove_punctuation)\n```", "```py\ndf['cleaned_review_text'] = df['cleaned_review_text'].str.lower()\n```", "```py\ndf['cleaned_review_text'] = df['cleaned_review_text'].apply(lambda x: \n↪' '.join(x.split()))\n```", "```py\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nstop_words = set(stopwords.words('portuguese'))\ndef remove_stopwords(text):\n    words = text.split()\n    filtered_words = [word for word in words if word not in stop_words]\n    return ' '.join(filtered_words)\ndf['cleaned_review_text'] = \n↪df['cleaned_review_text'].apply(remove_stopwords)\n```", "```py\n*# Using NLTK for stemming*\nfrom nltk.stem import RSLPStemmer\n\nnltk.download('rslp')\nstemmer = RSLPStemmer()\ndef stem_words(text):\n       words = text.split()\n       stemmed_words = [stemmer.stem(word) for word in words]\n    return ' '.join(stemmed_words)\ndf['cleaned_review_text'] = df['cleaned_review_text'].apply(stem_words)\n```", "```py\ndf['positive_review'] = df['cleaned_review_text'].apply(lambda x: \n↪is_positive(x, keywords))\n```", "```py\nimport string\nimport nltk\nfrom nltk.stem import RSLPStemmer\n\nnltk.download('rslp')\nstemmer = RSLPStemmer()\n\ndef remove_punctuation(text):\n    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n\ndef remove_extra_whitespace(text):\n        return ' '.join(text.split())\n\ndef stem_words(text):\n        words = text.split()\n    stemmed_words = [stemmer.stem(word) for word in words]\n    return ' '.join(stemmed_words)\n\ndef clean_text(text):\n        if not isinstance(text, str):\n                 return \"\"\n    text = text.lower()\n    text = remove_punctuation(text)\n    text = remove_extra_whitespace(text)\n    text = stem_words(text)\n    return text\ndf['cleaned_review_text'] = df['review_text'].apply(clean_text)\n```", "```py\nimport spacy\nnlp = spacy.load(\"pt_core_news_sm\")\ndef lemmatize_text(text):\n    doc = nlp(text)\n    lemmatized_words = [token.lemma_ for token in doc]\n    return ' '.join(lemmatized_words)\ndef clean_text(text):\n    if not isinstance(text, str):\n        return \"\"\n    text = text.lower()\n    text = remove_punctuation(text)\n    text = remove_extra_whitespace(text)\n    text = lemmatize_text(text)\n    return text\ndf['cleaned_review_text'] = df['review_text'].apply(clean_text)\n```", "```py\nimport pandas as pd\n*# Load your dataframe*\ndf = pd.read_csv('olist_order_reviews_dataset.csv')\n\n*# List of positive keywords in Portuguese*\npositive_keywords = ['ótimo', 'excelente', 'bom', 'incrível', \n↪'maravilhoso', 'perfeito', 'gostei', 'satisfeito', 'recomendo', \n↪'amei']\n\n*# Define a function to check if a review contains positive keywords*\ndef is_positive(review):\n    for keyword in positive_keywords:\n        if keyword.lower() in review.lower():\n            return True\n    return False\n\n*# Apply the function to the 'review_comment_message' column*\ndf['is_positive'] = df['review_comment_message'].apply(is_positive)\n\n*# Create a new dataframe with only positive reviews*\npositive_reviews_df = df[df['is_positive']]\n\n*# Save the positive reviews to a new CSV file*\npositive_reviews_df.to_csv('positive_reviews.csv', index=False)\n```", "```py\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\n*# Download resources if needed*\nnltk.download('punkt')\nnltk.download('stopwords/portuguese')\n\n*# Define list of positive keywords*\npositive_keywords = [\"ótimo\", \"excelente\", \"adorei\", \"recomendo\", \n↪\"maravilhoso\"]\n\n*# Define stopwords for language*\nstop_words = stopwords.words('portuguese')\n\n*# Define sentiment analyzer*\nanalyzer = SentimentIntensityAnalyzer()\n\ndef extract_positive_reviews(df):\n  \"\"\"\n  Extracts positive reviews from a dataframe containing \n  ↪\"review_comment_message\" column.\n\n  Args:\n    df: Pandas dataframe with \"review_comment_message\" column.\n\n  Returns:\n    Pandas dataframe with extracted positive reviews.\n  \"\"\"\n\n  positive_reviews = []\n\n  for index, row in df.iterrows():\n    review = row[\"review_comment_message\"]\n\n *# Preprocess review*\n    words = word_tokenize(review.lower())\n    filtered_words = [word for word in words if word not in stop_words]\n\n *# Find keywords*\n    has_keyword = any(keyword in filtered_words for keyword in \n    ↪positive_keywords)\n\n *# Sentiment analysis*\n    sentiment = analyzer.polarity_scores(review)\n\n *# Positive review criteria: Positive keyword present OR positive* \n    ↪*sentiment compound score*\n    if has_keyword or sentiment['compound'] > 0.2:\n      positive_reviews.append(review)\n\n  return pd.DataFrame({\"review_comment_message\": positive_reviews})\n\n*# Extract positive reviews*\npositive_df = extract_positive_reviews(df)\n\n*# Print or use positive_df as needed*\nprint(positive_df)\n```", "```py\nimport pandas as pd\n\n*# Sample Data (Simulate your dataframe)*\ndata = {'review_comment_message': ['Recebi bem antes do prazo estipulado.',\n                                  'Produto incrível! Superou minhas \n                                   ↪expectativas',\n                                  'Produto bom, porém o que veio para mim\n                                   ↪não condiz com a foto do anúncio.',\n                                  'Gostaria de saber porque meu produto \n                                   ↪ainda não chegou.']}\ndf = pd.DataFrame(data)\n\n*# Keywords for positive sentiment in Portuguese*\npositive_keywords = ['bom', 'ótimo', 'excelente', 'incrível', 'gostei', \n↪'satisfeito', 'superou', 'recomendo']\n\ndef extract_positive_reviews(df, keywords):\n    \"\"\"Extracts positive reviews from a dataframe based on a list of \n↪keywords.\"\"\"\n\n    positive_reviews = df[df['review_comment_message'].\n    ↪str.contains('|'.join(keywords), case=False)]\n    return positive_reviews\n\n*# Extract the positive reviews*\npositive_reviews_df = extract_positive_reviews(df, positive_keywords)\n\n*# Print the results*\nprint(positive_reviews_df)\n```", "```py\nimport re\n\n*# Create a list of positive keywords:*\npositive_keywords = ['amazing', 'great', 'excellent', 'love', 'valuable',\n↪'fantastic']\n\n*# Define a function to extract positive reviews:*\ndef filter_positive_reviews(text, keywords):\n *# Create a regex pattern to match the positive keywords:*\n    regex_pattern = \"|\".join(keywords)\n *# Check if the pattern is present in the given text:*\n    if re.search(regex_pattern, text, flags=re.IGNORECASE):\n        return True\n    else:\n        return False\n\n*# Apply the function to the review column and filter positive reviews:*\ndf['positive_reviews'] = df['review_comment_message'].apply(lambda text:\n↪filter_positive_reviews(text, positive_keywords))\n\n*# Extract the positive reviews:*\npositive_reviews = df['review_comment_message'].loc[df['positive_reviews'] \n↪== True].tolist()\n```", "```py\nimport pandas as pd\n\n*# Assuming you have already loaded the dataframe and defined the* \n↪*is_positive function*\n\n*# Define a function to determine if a review is positive based on the* \n↪*product score*\ndef is_positive_based_on_score(score, threshold=4):\n    return score >= threshold\n\n*# Apply the function to the 'product_score' column*\ndf['is_positive_based_on_score'] = \n↪df['product_score'].apply(is_positive_based_on_score)\n\n*# Calculate the confusion matrix*\nTP = sum((df['is_positive'] == True) & (df['is_positive_based_on_score']  \n↪== True))\nFP = sum((df['is_positive'] == True) & (df['is_positive_based_on_score']\n↪== False))\nTN = sum((df['is_positive'] == False) & (df['is_positive_based_on_score']\n↪== False))\nFN = sum((df['is_positive'] == False) & (df['is_positive_based_on_score']\n↪== True))\n\n*# Calculate sensitivity (true positive rate) and specificity (true* \n↪*negative rate)*\nsensitivity = TP / (TP + FN)\nspecificity = TN / (TN + FP)\nprint(f'Sensitivity: {sensitivity:.2f}')\nprint(f'Specificity: {specificity:.2f}')\n```", "```py\nimport pandas as pd\n\n*# Load your dataframe*\ndf = pd.read_csv('olist_order_reviews_dataset.csv')\n\n*# List of positive keywords in Portuguese*\npositive_keywords = ['ótimo', 'excelente', 'bom', 'incrível', \n↪'maravilhoso', 'perfeito', 'gostei', 'satisfeito', 'recomendo', 'amei']\n\n*# Define a function to check if a review contains positive keywords*\ndef is_positive(review):\n    for keyword in positive_keywords:\n        if keyword.lower() in review.lower():\n            return True\n    return False\n\n*# Apply the function to the 'review_comment_message' column*\ndf['is_positive'] = df['review_comment_message'].apply(is_positive)\n\n*# Create a new dataframe with only positive reviews*\npositive_reviews_df = df[df['is_positive']]\n\n*# Calculate the average product score in the original dataframe*\noriginal_avg_score = df['product_score'].mean()\nprint(f\"Original average product score: {original_avg_score}\")\n\n*# Calculate the average product score in the positive reviews dataframe*\npositive_avg_score = positive_reviews_df['product_score'].mean()\nprint(f\"Positive average product score: {positive_avg_score}\")\n\n*# Compare the average scores*\nif positive_avg_score > original_avg_score:\nprint(\"The average product score for positive reviews is higher than the\n↪original dataset. The extraction seems to be working correctly.\")\nElse:\nprint(\"The average product score for positive reviews is not higher than\n↪the original dataset. The extraction might not be accurate.\")\n\n*# Save the positive reviews to a new CSV file*\npositive_reviews_df.to_csv('positive_reviews.csv', index=False)\n```", "```py\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n[...]\n\n*# Visualize the distribution of review scores in the positive reviews* \n↪*dataframe*\nsns.countplot(x='review_score', data=positive_reviews_df)\nplt.title('Distribution of Review Scores in Extracted Positive Reviews')\nplt.xlabel('Review Score')\nplt.ylabel('Count')\nplt.show()\n```", "```py\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom unidecode import unidecode\n\n*# Load your DataFrame*\n# df = pd.read_csv('your_reviews_file.csv')\n\nnltk.download('punkt')\nnltk.download('stopwords')\nstop_words = set(stopwords.words('portuguese'))\n\n*# Function to preprocess text*\ndef preprocess(text):\n    text = unidecode(text.lower())  *# Normalize*\n    tokens = nltk.word_tokenize(text)\n    tokens = [word for word in tokens if word.isalpha() and word not in \n    ↪stop_words]\n    return tokens\n\n*# Assuming SentiLex is in a CSV file with 'word' and 'polarity' columns*\nsentilex_df = pd.read_csv('path_to_sentilex.csv', encoding='utf-8')\nsentilex_dict = pd.Series(sentilex_df.polarity.values, \n↪index=sentilex_df.word).to_dict()\n\n*# Modify the scoring function to use SentiLex*\ndef score_review(tokens):\n    scores = [sentilex_dict.get(word, 0) for word in tokens]\n    return sum(scores)\n\n*# Assuming 'reviews' is the column with text data*\ndf['processed_reviews'] = df['reviews'].apply(preprocess)\ndf['sentiment_score'] = df['processed_reviews'].apply(score_review)\ndf['sentiment'] = df['sentiment_score'].apply(lambda x: 'positive' \n↪if x > 0 else 'negative' if x < 0 else 'neutral')\n```", "```py\nà-vontade.PoS=N;TG=HUM:N0;POL:N0=1;ANOT=MAN\n```", "```py\ndef load_sentilex(filepath):\n    sentilex_dict = {}\n    with open(filepath, 'r', encoding='utf-8') as file:\n        for line in file:\n            parts = line.strip().split(';')\n            word = parts[0].split('.')[0]  *# Get the word before '.PoS='*\n            pol_entry = [part for part in parts if \n            ↪part.startswith('POL:N0=')]\n            if pol_entry:\n *# Assuming there's exactly one 'POL:N0=' entry per line*\n                polarity = int(pol_entry[0].split('=')[1])\n                sentilex_dict[word] = polarity\n    return sentilex_dict\n\n*# Example usage*\nfilepath = 'SentiLex-lem-PT02.txt'\nsentilex_dict = load_sentilex(filepath)\n\n*# Now sentilex_dict is ready to be used as in the previous example.*\n```", "```py\npip install spacy\npython -m spacy download pt_core_news_sm\n```", "```py\nimport spacy\n\n*# Load the Portuguese language model*\nnlp = spacy.load('pt_core_news_sm')\n\ndef preprocess_and_lemmatize(text):\n    text = unidecode(text.lower())  *# Normalize*\n    doc = nlp(text)\n    lemmas = [token.lemma_ for token in doc if token.is_alpha and \n↪       åtoken.lemma_ not in stop_words]\n    return lemmas\n\n*# Update the DataFrame processing line to use the new preprocessing* \n↪*function*\ndf['processed_reviews'] = df['reviews'].apply(preprocess_and_lemmatize)\n```"]