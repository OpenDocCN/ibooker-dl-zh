- en: 10 Agent reasoning and evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using various prompt engineering techniques to extend large language model functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engaging large language models with prompt engineering techniques that engage
    reasoning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Employing an evaluation prompt to narrow and identify the solution to an unknown
    problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we’ve examined the patterns of memory and retrieval that define the
    semantic memory component in agents, we can take a look at the last and most instrumental
    component in agents: planning. Planning encompasses many facets, from reasoning,
    understanding, and evaluation to feedback.'
  prefs: []
  type: TYPE_NORMAL
- en: To explore how LLMs can be prompted to reason, understand, and plan, we’ll demonstrate
    how to engage reasoning through prompt engineering and then expand that to planning.
    The planning solution provided by the Semantic Kernel (SK) encompasses multiple
    planning forms. We’ll finish the chapter by incorporating adaptive feedback into
    a new planner.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 demonstrates the high-level prompt engineering strategies we’ll
    cover in this chapter and how they relate to the various techniques we’ll cover.
    Each of the methods showcased in the figure will be explored in this chapter,
    from the basics of solution/direct prompting, shown in the top-left corner, to
    self-consistency and tree of thought (ToT) prompting, in the bottom right.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 How the two planning prompt engineering strategies align with the
    various techniques
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 10.1 Understanding direct solution prompting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Direct solution prompting* is generally the first form of prompt engineering
    that users employ when asking LLMs questions or solving a particular problem.
    Given any LLM use, these techniques may seem apparent, but they are worth reviewing
    to establish the foundation of thought and planning. In the next section, we’ll
    start from the beginning, asking questions and expecting answers.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 Question-and-answer prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the exercises in this chapter, we’ll employ prompt flow to build and evaluate
    the various techniques. (We already extensively covered this tool in chapter 9,
    so refer to that chapter if you need a review.) Prompt flow is an excellent tool
    for understanding how these techniques work and exploring the flow of the planning
    and reasoning process.
  prefs: []
  type: TYPE_NORMAL
- en: Open Visual Studio Code (VS Code) to the `chapter` `10` source folder. Create
    a new virtual environment for the folder, and install the `requirements.txt` file.
    If you need help setting up a chapter’s Python environment, refer to appendix
    B.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll look at the first flow in the `prompt_flow/question-answering-prompting`
    folder. Open the `flow.dag.yaml` file in the visual editor, as shown in figure
    10.2\. On the right side, you’ll see the flow of components. At the top is the
    `question_answer` LLM prompt, followed by two `Embedding` components and a final
    LLM prompt to do the evaluation called `evaluate`.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 The `flow.dag.yaml` file, open in the visual editor, highlighting
    the various components of the flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The breakdown in listing 10.1 shows the structure and components of the flow
    in more detail using a sort of YAML-shortened pseudocode. You can also see the
    input and outputs to the various components and a sample output from running the
    flow.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.1 `question-answer-prompting` flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Before running this flow, make sure your LLM block is configured correctly.
    This may require you to set up a connection to your chosen LLM. Again, refer to
    chapter 9 if you need a review on how to complete this. You’ll need to configure
    the LLM and `Embedding` blocks with your connection if you’re not using OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: After configuring your LLM connection, run the flow by clicking the Play button
    from the visual editor or using the Test (Shift-F5) link in the YAML editor window.
    If everything is connected and configured correctly, you should see output like
    that in listing 10.1.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `question_answer.jinja2` file in VS Code, as shown in listing 10.2\.
    This listing shows the basic question-and-answer-style prompt. In this style of
    prompt, the system message describes the basic rules and provides the context
    to answer the question. In chapter 4, we explored the retrieval augmented generation
    (RAG) pattern, and this prompt follows a similar pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.2 `question_answer.jinja2`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Replace with the content LLM should answer the question about.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Replace with the question.'
  prefs: []
  type: TYPE_NORMAL
- en: This exercise shows the simple method of using an LLM to ask questions about
    a piece of content. Then, the question response is evaluated using a similarity
    matching score. We can see from the output in listing 10.1 that the LLM does a
    good job of answering a question about the context. In the next section, we’ll
    explore a similar technique that uses direct prompting.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 Implementing few-shot prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Few-shot prompting* is like question-and-answer prompting, but the makeup
    of the prompt is more about providing a few examples than about facts or context.
    This allows the LLM to bend to patterns or content not previously seen. While
    this approach sounds like question and answer, the implementation is quite different,
    and the results can be powerful.'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot, one-shot, and few-shot learning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: One holy grail of machine learning and AI is the ability to train a model on
    as few items as possible. For example, in traditional vision models, millions
    of images are fed into the model to help identify the differences between a cat
    and a dog.
  prefs: []
  type: TYPE_NORMAL
- en: A *one-shot* model is a model that requires only a single image to train it.
    For example, a picture of a cat can be shown, and then the model can identify
    any cat image. A *few-shot* model requires only a few things to train the model.
    And, of course, *zero-shot* indicates the ability to identify something given
    no previous examples. LLMs are efficient learners and can do all three types of
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: Open `prompt_flow/few-shot-prompting/flow.dag.yaml` in VS Code and the visual
    editor. Most of the flow looks like the one pictured earlier in figure 10.2, and
    the differences are highlighted in listing 10.3, which shows a YAML pseudocode
    representation. The main differences between this and the previous flow are the
    inputs and LLM prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.3 `few-shot-prompting` flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Evaluation score represents the similarity between expected and predicted.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Uses sunner in a sentence'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 This is a false statement but the intent is to get the LLM to use the word
    as if it was real.'
  prefs: []
  type: TYPE_NORMAL
- en: Run the flow by pressing Shift-F5 or clicking the Play/Test button from the
    visual editor. You should see output like listing 10.3 where the LLM has used
    the word *sunner* (a made-up term) correctly in a sentence given the initial statement.
  prefs: []
  type: TYPE_NORMAL
- en: This exercise demonstrates the ability to use a prompt to alter the behavior
    of the LLM to be contrary to what it has learned. We’re changing what the LLM
    understands to be accurate. Furthermore, we then use that modified perspective
    to elicit the use of a made-up word.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `few_shot.jinja2` prompt in VS Code, shown in listing 10.4\. This listing
    demonstrates setting up a simple persona, that of an eccentric dictionary maker,
    and then providing examples of words it has defined and used before. The base
    of the prompt allows for the LLM to extend the examples and produce similar results
    using other words.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.4 `few_shot.jinja2`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Demonstrates an example defining a made-up word and using it in a sentence'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Demonstrates another example'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 A rule to prevent the LLM from outputting extra information'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 The input statement defines a new word and asks for the use.'
  prefs: []
  type: TYPE_NORMAL
- en: You may say we’re forcing the LLM to hallucinate here, but this technique is
    the basis for modifying behavior. It allows prompts to be constructed to guide
    an LLM to do everything contrary to what it learned. This foundation of prompting
    also establishes techniques for other forms of altered behavior. From the ability
    to alter the perception and background of an LLM, we’ll move on to demonstrate
    a final example of a direct solution in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.3 Extracting generalities with zero-shot prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Zero-shot prompting or learning* is the ability to generate a prompt in such
    a manner that allows the LLM to generalize. This generalization is embedded within
    the LLM and demonstrated through zero-shot prompting, where no examples are given,
    but instead a set of guidelines or rules are given to guide the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: Employing this technique is simple and works well to guide the LLM to generate
    replies given its internal knowledge and no other contexts. It’s a subtle yet
    powerful technique that applies the knowledge of the LLM to other applications.
    This technique, combined with other prompting strategies, is proving effective
    at replacing other language classification models—models that identify the emotion
    or sentiment in text, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Open `prompt_flow/zero-shot-prompting/flow.dag.yaml` in the VS Code prompt flow
    visual editor. This flow is again almost identical to that shown earlier in figure
    10.1 but differs slightly in implementation, as shown in listing 10.5.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.5 `zero-shot-prompting` flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Shows a perfect evaluation score of 1.0'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The statement we’re asking the LLM to classify'
  prefs: []
  type: TYPE_NORMAL
- en: Run the flow by pressing Shift-F5 within the VS Code prompt flow visual editor.
    You should see output similar to that shown in listing 10.5.
  prefs: []
  type: TYPE_NORMAL
- en: Now open the `zero_shot.jinja2` prompt as shown in listing 10.6\. The prompt
    is simple and uses no examples to extract the sentiment from the text. What is
    especially interesting to note is that the prompt doesn’t even mention the phrase
    sentiment, and the LLM seems to understand the intent.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.6 `zero_shot.jinja2`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Provides essential guidance on performing the classification'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The statement of text to classify'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot prompt engineering is about using the ability of the LLM to generalize
    broadly based on its training material. This exercise demonstrates how knowledge
    within the LLM can be put to work for other tasks. The LLM’s ability to self-contextualize
    and apply knowledge can extend beyond its training. In the next section, we extend
    this concept further by looking at how LLMs can reason.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Reasoning in prompt engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs like ChatGPT were developed to function as chat completion models, where
    text content is fed into the model, whose responses align with completing that
    request. LLMs were never trained to reason, plan, think, or have thoughts.
  prefs: []
  type: TYPE_NORMAL
- en: However, much like we demonstrated with the examples in the previous section,
    LLMs can be prompted to extract their generalities and be extended beyond their
    initial design. While an LLM isn’t designed to reason, the training material fed
    into the model provides an understanding of reasoning, planning, and thought.
    Therefore, by extension, an LLM understands what reasoning is and can employ the
    concept of reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning and planning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Reasoning* is the ability of an intellect, artificial or not, to understand
    the process of thought or thinking through a problem. An intellect can understand
    that actions have outcomes, and it can use this ability to reason through which
    action from a set of actions can be applied to solve a given task.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Planning* is the ability of the intellect to reason out the order of actions
    or tasks and apply the correct parameters to achieve a goal or outcome—the extent
    to which an intellectual plan depends on the scope of the problem. An intellect
    may combine multiple levels of planning, from strategic and tactical to operational
    and contingent.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll look at another set of prompt engineering techniques that allow or mimic
    reasoning behavior to demonstrate this reasoning ability. Typically, when evaluating
    the application of reasoning, we look to having the LLM solve challenging problems
    it wasn’t designed to solve. A good source of such is based on logic, math, and
    word problems.
  prefs: []
  type: TYPE_NORMAL
- en: Using the time travel theme, what class of unique problems could be better to
    solve than understanding time travel? Figure 10.3 depicts one example of a uniquely
    challenging time travel problem. Our goal is to acquire the ability to prompt
    the LLM in a manner that allows it to solve the problem correctly.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 The complexity of the time travel problems we intend to solve using
    LLMs with reasoning and planning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Time travel problems are thought exercises that can be deceptively difficult
    to solve. The example in figure 10.3 is complicated to solve for an LLM, but the
    part it gets wrong may surprise you. The next section will use reasoning in prompts
    to solve these unique problems.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 Chain of thought prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Chain of thought* (CoT)prompting is a prompt engineering technique that employs
    the one-shot or few-shot examples that describe the reasoning and the steps to
    accomplish a desired goal. Through the demonstration of reasoning, the LLM can
    generalize this principle and reason through similar problems and goals. While
    the LLM isn’t trained with the goal of reasoning, we can elicit the model to reason,
    using prompt engineering.'
  prefs: []
  type: TYPE_NORMAL
- en: Open `prompt_flow/chain-of-thought-prompting/flow.dag.yaml` in the VS Code prompt
    flow visual editor. The elements of this flow are simple, as shown in figure 10.4\.
    With only two LLM blocks, the flow first uses a CoT prompt to solve a complex
    question; then, the second LLM prompt evaluates the answer.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 The flow of the CoT
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Listing 10.7 shows the YAML pseudocode that describes the blocks and the inputs/outputs
    of the flow in more detail. The default problem statement in this example isn’t
    the same as in figure 10.3.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.7 `chain-of-thought-prompting` flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The evaluated score for the given solution'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The expected answer for the problem'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The predicted answer shows the reasoning steps and output.'
  prefs: []
  type: TYPE_NORMAL
- en: Dig into the inputs and check the problem statement; try to evaluate the problem
    yourself. Then, run the flow by pressing Shift-F5\. You should see output similar
    to that shown in listing 10.7.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `cot.jinja2` prompt file as shown in listing 10.8\. This prompt gives
    a few examples of time travel problems and then the thought-out and reasoned solution.
    The process of showing the LLM the steps to complete the problem provides the
    reasoning mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.8 `cot.jinja2`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 A few example problem statements'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The solution to the problem statement, output as a sequence of reasoning
    steps'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 A few example problem statements'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 The solution to the problem statement, output as a sequence of reasoning
    steps'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 The problem statement the LLM is directed to solve'
  prefs: []
  type: TYPE_NORMAL
- en: You may note that the solution to figure 10.3 is also provided as an example
    in listing 10.8\. It’s also helpful to go back and review listing 10.7 for the
    reply from the LLM about the problem. From this, you can see the reasoning steps
    the LLM applied to get its final answer.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can look at the prompt that evaluates how well the solution solved the
    problem. Open `evaluate_answer.jinja2`, shown in listing 10.9, to review the prompt
    used. The prompt is simple, uses zero-shot prompting, and allows the LLM to generalize
    how it should score the expected and predicted. We could provide examples and
    scores, thus changing this to an example of a few-shot classification.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.9 `evaluate_answer.jinja2`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The rules for evaluating the solution'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Direction to only return the score and nothing else'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The initial problem statement'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 The expected or grounded answer'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 The output from the CoT prompt earlier'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the LLM output shown earlier in listing 10.7, you can see why the
    evaluation step may get confusing. Perhaps a fix to this could be suggesting to
    the LLM to provide the final answer in a single statement. In the next section,
    we move on to another example of prompt reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 Zero-shot CoT prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As our time travel demonstrates, CoT prompting can be expensive in terms of
    prompt generation for a specific class of problem. While not as effective, there
    are techniques similar to CoT that don’t use examples and can be more generalized.
    This section will examine a straightforward phrase employed to elicit reasoning
    in LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Open `prompt_flow/zero-shot-cot-prompting/flow.dag.yaml` in the VS Code prompt
    flow visual editor. This flow is very similar to the previous CoT, as shown in
    figure 10.4\. The next lsting shows the YAML pseudocode that describes the flow.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.10 `zero-shot-CoT-prompting` flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The final evaluation score'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The expected answer'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The predicted answer (the steps have been omitted showing the final answer)'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 The initial problem statement'
  prefs: []
  type: TYPE_NORMAL
- en: Run/test the flow in VS Code by pressing Shift-F5 while in the visual editor.
    The flow will run, and you should see output similar to that shown in listing
    10.10\. This exercise example performs better than the previous example on the
    same problem.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `cot.jinja2` prompt in VS Code, as shown in listing 10.11\. This is
    a much simpler prompt than the previous example because it only uses zero-shot.
    However, one key phrase turns this simple prompt into a powerful reasoning engine.
    The line in the prompt `Let’s` `think` `step` `by` `step` triggers the LLM to
    consider internal context showing reasoning. This, in turn, directs the LLM to
    reason out the problem in steps.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.11 `cot.jinja2`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 A magic line that formulates reasoning from the LLM'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Asks the LLM to provide a final statement of the answer'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The problem statement the LLM is asked to solve'
  prefs: []
  type: TYPE_NORMAL
- en: Similar phrases asking the LLM to think about the steps or asking it to respond
    in steps also extract reasoning. We’ll demonstrate a similar but more elaborate
    technique in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.3 Step by step with prompt chaining
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can extend the behavior of asking an LLM to think step by step into a chain
    of prompts that force the LLM to solve the problem in steps. In this section,
    we look at a technique called *prompt chaining* that forces an LLM to process
    problems in steps.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `prompt_flow/prompt-chaining/flow.dag.yaml` file in the visual editor,
    as shown in figure 10.5\. Prompt chaining breaks up the reasoning method used
    to solve a problem into chains of prompts. This technique forces the LLM to answer
    the problem in terms of steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 The prompt chaining flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Listing 10.12 shows the YAML pseudocode that describes the flow in a few more
    details. This flow chains the output of the first LLM block into the second and
    then from the second into the third. Forcing the LLM to process the problem this
    way uncovers the reasoning pattern, but it can also be overly verbose.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.12 `prompt-chaining` flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Start of the chain of prompts'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Output from the previous step injected into this step'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Output from two previous steps injected into this step'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 The final solution statement, although wrong, is closer.'
  prefs: []
  type: TYPE_NORMAL
- en: Run the flow by pressing Shift-F5 from the visual editor, and you’ll see the
    output as shown in listing 10.12\. The answer is still not correct for the Alex
    problem, but we can see all the work the LLM is doing to reason out the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open up all three prompts: `decompose_steps.jinja2`, `calculate_steps.jinja2`,
    and `calculate_solution.jinja2` (see listings 10.13, 10.14, and 10.15, respectively).
    All three prompts shown in the listings can be compared to show how outputs chain
    together.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.13 `decompose_steps.jinja2`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Forces the LLM to list only the steps and nothing else'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The initial problem statement'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.14 `calculate_steps.jinja2`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Requests that the LLM not solve the whole problem, just the steps'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Uses the magic statement to extract reasoning'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Injects the steps produced by the decompose_steps step'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.15 `calculate_solution.jinja2`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Requests that the LLM output the final answer and not any steps'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The decomposed steps'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The calculated steps'
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise example, we’re not performing any evaluation and scoring. Without
    the evaluation, we can see that this sequence of prompts still has problems solving
    our more challenging time travel problem shown earlier in figure 10.3\. However,
    that doesn’t mean this technique doesn’t have value, and this prompting format
    solves some complex problems well.
  prefs: []
  type: TYPE_NORMAL
- en: What we want to find, however, is a reasoning and planning methodology that
    can solve such complex problems consistently. The following section moves from
    reasoning to evaluating the best solution.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Employing evaluation for consistent solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we learned that even the best-reasoned plans may not
    always derive the correct solution. Furthermore, we may not always have the answer
    to confirm if that solution is correct. The reality is that we often want to use
    some form of evaluation to determine the efficacy of a solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.6 shows a comparison of the prompt engineering strategies that have
    been devised as a means of getting LLMs to reason and plan. We’ve already covered
    the two on the left: zero-shot direct prompting and CoT prompting. The following
    example exercises in this section will look at self-consistency with the CoT and
    ToT techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 Comparing the various prompt engineering strategies to enable reasoning
    and planning from LLMs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We’ll continue to focus on the complex time travel problem to compare these
    more advanced methods that expand on reasoning and planning with evaluation. In
    the next section, we’ll evaluate self-consistency.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.1 Evaluating self-consistency prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Consistency in prompting is more than just lowering the temperature parameter
    we send to an LLM. Often, we want to generate a consistent plan or solution and
    still use a high temperature to better evaluate all the variations to a plan.
    By evaluating multiple different plans, we can get a better sense of the overall
    value of a solution.
  prefs: []
  type: TYPE_NORMAL
- en: '*Self-consistent prompting* is the technique of generating multiple plans/solutions
    for a given problem. Then, those plans are evaluated, and the more frequent or
    consistent plan is accepted. Imagine three plans generated, where two are similar,
    but the third is different. Using self-consistency, we evaluate the first two
    plans as the more consistent answer.'
  prefs: []
  type: TYPE_NORMAL
- en: Open `prompt_flow/self-consistency-prompting/flow.dag.yaml` in the VS Code prompt
    flow visual editor. The flow diagram shows the simplicity of the prompt generation
    flow in figure 10.7\. Next to it in the diagram is the self-consistency evaluation
    flow.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 The self-consistency prompt generation beside the evaluation flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Prompt flow uses a direct acyclic graph (DAG) format to execute the flow logic.
    DAGs are an excellent way of demonstrating and executing flow logic, but because
    they are *acyclic,* meaning they can’t repeat, they can’t execute loops. However,
    because prompt flow provides a batch processing mechanism, we can use that to
    simulate loops or repetition in a flow.
  prefs: []
  type: TYPE_NORMAL
- en: Referring to figure 10.6, we can see that self-consistency processes the input
    three times before collecting the results and determining the best plan/reply.
    We can apply this same pattern but use batch processing to generate the outputs.
    Then, the evaluation flow will aggregate the results and determine the best answer.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `self-consistency-prompting/cot.jinja2` prompt template in VS Code
    (see listing 10.16). The listing was shortened, as we’ve seen parts before. This
    prompt uses two (few-shot prompt) examples of a CoT to demonstrate the thought
    reasoning to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.16 `self-consistency-prompting/cot.jinja2`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The Sarah time travel problem'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Sample CoT, cut for brevity'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The Max time travel problem'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Sample CoT, cut for brevity'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Final guide and statement to constrain output'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the `self-consistency-prompting/flow.dag.yaml` file in VS Code. Run the
    example in batch mode by clicking Batch Run (the beaker icon) from the visual
    editor. Figure 10.8 shows the process step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: Click Batch Run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the JSON Lines (JSONL) input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select `statements.jsonl`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click the Run link.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/10-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 The step-by-step process of launching a batch process
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: TIP  If you need to review the process, refer to chapter 9, which covers this
    process in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.17 shows the JSON output from executing the flow in batch mode. The
    `statements.jsonl` file has five identical Alex time travel problem entries. Using
    identical entries allows us to simulate the prompt executing five times on the
    duplicate entry.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.17 `self-consistency-prompting` batch execution output
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The path where the flow was executed from'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The folder containing the outputs of the flow (note this path)'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The data used to run the flow in batch'
  prefs: []
  type: TYPE_NORMAL
- en: You can view the flow produced by pressing the Ctrl key and clicking the output
    link, highlighted in listing 10.17\. This will open another instance of VS Code,
    showing a folder with all the output from the run. We now want to check the most
    consistent answer. Fortunately, the evaluation feature in prompt flow can help
    us identify consistent answers using similarity matching.
  prefs: []
  type: TYPE_NORMAL
- en: Open `self-consistency-evaluation/flow.dag.yaml` in VS Code (see figure 10.7).
    This flow embeds the predicted answer and then uses an aggregation to determine
    the most consistent answer.
  prefs: []
  type: TYPE_NORMAL
- en: From the flow, open `consistency.py` in VS Code, as shown in listing 10.18\.
    The code for this tool function calculates the cosine similarity for all pairs
    of answers. Then, it finds the most similar answer, logs it, and outputs that
    as the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.18 `consistency.py`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Calculates the mean of all the embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Calculates cosine similarity for each pair of embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Finds the index of the most similar answer'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Logs the output as a metric'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Returns the text for the most similar answer'
  prefs: []
  type: TYPE_NORMAL
- en: We need to run the evaluation flow in batch mode as well. Open `self-consistency-evaluation/flow.dag.yaml`
    in VS Code and run the flow in batch mode (beaker icon). Then, select Existing
    Run as the flow input, and when prompted, choose the top or the last run you just
    executed as input.
  prefs: []
  type: TYPE_NORMAL
- en: Again, after the flow completes processing, you’ll see an output like that shown
    in listing 10.17\. Ctrl-click on the output folder link to open a new instance
    of VS Code showing the results. Locate and open the `metric.json` file in VS Code,
    as shown in figure 10.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 The VS Code is open to the batch run output folder. Highlighted
    are the `metrics.json` file and the output showing the most similar answer.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The answer shown in figure 10.9 is still incorrect for this run. You can continue
    a few more batch runs of the prompt and/or increase the number of runs in a batch
    and then evaluate flows to see if you get better answers. This technique is generally
    more helpful for more straightforward problems but still demonstrates an inability
    to reason out complex problems.
  prefs: []
  type: TYPE_NORMAL
- en: Self-consistency uses a reflective approach to evaluate the most likely thought.
    However, the most likely thing is certainly not always the best. Therefore, we
    must consider a more comprehensive approach in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.2 Evaluating tree of thought prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned earlier, ToT prompting, as shown in figure 10.6, combines self-evaluation
    and prompt chaining techniques. As such, it breaks down the sequence of planning
    into a chain of prompts, but at each step in the chain, it provides for multiple
    evaluations. This creates a tree that can be executed and evaluated at each level,
    breadth-first, or from top to bottom, depth-first.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.10 shows the difference between executing a tree using breadth-first
    or depth-first. Unfortunately, due to the DAG execution pattern of prompt flow,
    we can’t quickly implement the depth-first method, but breadth-first works just
    fine.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 Breadth-first vs. depth-first execution on a ToT pattern
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Open `tree-of-thought-evaluation/flow.dag.yaml` in VS Code. The visual of the
    flow is shown in figure 10.11\. This flow functions like a breadth-first ToT pattern—the
    flow chains together a series of prompts asking the LLM to return multiple plans
    at each step.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 ToT pattern expressed and prompt flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Because the flow executes in a breadth-first style, each level output of the
    nodes is also evaluated. Each node in the flow uses a pair of semantic functions—one
    to generate the answer and the other to evaluate the answer. The semantic function
    is a custom Python flow block that processes multiple inputs and generates multiple
    outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.19 shows the `semantic_function.py` tool. This general tool is reused
    for multiple blocks in this flow. It also demonstrates the embedding functionality
    from the SK for direct use within prompt flow.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.19 `semantic_function.py`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Uses a union to allow for different types of LLM connections'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Checks to see if the input is empty or None; if so, the function shouldn’t
    be executed.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Sets up the generation function that creates a plan'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Sets up the evaluation function'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Runs the evaluate function and determines if the input is good enough to
    continue'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 If the evaluation score is high enough, generates the next step'
  prefs: []
  type: TYPE_NORMAL
- en: The semantic function tool is used in the tree’s experts, nodes, and answer
    blocks. At each step, the function determines if any text is being input. If there
    is no text, the block returns with no execution. Passing no text to a block means
    that the previous block failed evaluation. By evaluating before each step, ToT
    short-circuits the execution of plans it deems as not being valid.
  prefs: []
  type: TYPE_NORMAL
- en: This may be a complex pattern to grasp at first, so go ahead and run the flow
    in VS Code. Listing 10.20 shows just the answer node output of a run; these results
    may vary from what you see but should be similar. Nodes that return no text either
    failed evaluation or their parents did.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.20 Output from `tree-of-thought-evaluation` flow
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Represents that the first node plans weren’t valid and not executed'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The plan for node 2 and answer 2 failed evaluation and wasn’t run.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The plan for this node failed to evaluate and wasn’t run.'
  prefs: []
  type: TYPE_NORMAL
- en: The output in listing 10.20 shows how only a select set of nodes was evaluated.
    In most cases, the evaluated nodes returned an answer that could be valid. Where
    no output was produced, it means that the node itself or its parent wasn’t valid.
    When sibling nodes all return empty, the parent node fails to evaluate.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, ToT is valid for complex problems but perhaps not very practical.
    The execution of this flow can take up to 27 calls to an LLM to generate an output.
    In practice, it may only do half that many calls, but that’s still a dozen or
    more calls to answer a single problem.
  prefs: []
  type: TYPE_NORMAL
- en: 10.4 Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the following exercises to improve your knowledge of the material:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Exercise 1*—Create Direct Prompting, Few-Shot Prompting, and Zero-Shot Prompting'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Create three different prompts for an LLM to summarize a recent
    scientific article: one using direct prompting, one with few-shot prompting, and
    the last employing zero-shot prompting.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks:*'
  prefs: []
  type: TYPE_NORMAL
- en: Compare the effectiveness of the summaries generated by each approach.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare the accuracy of the summaries generated by each approach.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exercise 2*—Craft Reasoning Prompts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Design a set of prompts that require the LLM to solve logical
    puzzles or riddles.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks:*'
  prefs: []
  type: TYPE_NORMAL
- en: Focus on how the structure of your prompt can influence the LLM’s reasoning
    process.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Focus on how the same can influence the correctness of its answers.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exercise 3*—Evaluation Prompt Techniques'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Develop an evaluation prompt that asks the LLM to predict the
    outcome of a hypothetical experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Task:*'
  prefs: []
  type: TYPE_NORMAL
- en: Create a follow-up prompt that evaluates the LLM’s prediction for accuracy and
    provides feedback on its reasoning process.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Direct solution prompting is a foundational method of using prompts to direct
    LLMs toward solving specific problems or tasks, emphasizing the importance of
    clear question-and-answer structures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few-shot prompting provides LLMs with a few examples to guide them in handling
    new or unseen content, highlighting its power in enabling the model to adapt to
    unfamiliar patterns.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero-shot learning and prompting demonstrate how LLMs can generalize from their
    training to solve problems without needing explicit examples, showcasing their
    inherent ability to understand and apply knowledge in new contexts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chain of thought prompting guides the LLMs through a reasoning process step
    by step to solve complex problems, illustrating how to elicit detailed reasoning
    from the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt chaining breaks down a problem into a series of prompts that build upon
    each other, showing how to structure complex problem-solving processes into manageable
    steps for LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-consistency is a prompt technique that generates multiple solutions to
    a problem and selects the most consistent answer through evaluation, emphasizing
    the importance of consistency in achieving reliable outcomes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tree of thought prompting combines self-evaluation and prompt chaining to create
    a comprehensive strategy for tackling complex problems, allowing for a systematic
    exploration of multiple solution paths.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advanced prompt engineering strategies provide insights into sophisticated techniques
    such as self-consistency with CoT and ToT, offering methods to increase the accuracy
    and reliability of LLM-generated solutions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
