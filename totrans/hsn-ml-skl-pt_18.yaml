- en: Chapter 16\. Vision and Multimodal Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we implemented a transformer from scratch and turned
    it into a translation system, then we explored encoder-only models for NLU, decoder-only
    models for NLG, and we even built a little chatbot—that was quite a journey! Yet,
    there’s still a lot more to say about transformers. In particular, we have only
    dealt with text so far, but transformers actually turned out to be exceptionally
    good at processing all sorts of inputs. In this chapter we will cover *vision
    transformers* (ViTs), capable of processing images, followed by *multimodal transformers*,
    capable of handling multiple modalities, including text, images, audio, videos,
    robot sensors and actuators, and really any kind of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first part of this chapter, we will discuss some of the most influential
    pure-vision transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: DETR (Detection Transformer)
  prefs: []
  type: TYPE_NORMAL
- en: An early encoder-decoder transformer for object detection.
  prefs: []
  type: TYPE_NORMAL
- en: The original ViT (Vision Transformer)
  prefs: []
  type: TYPE_NORMAL
- en: This landmark encoder-only transformer treats image patches like word tokens
    and reaches the state of the art if trained on a large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: DeiT (Data-Efficient Image Transformer)
  prefs: []
  type: TYPE_NORMAL
- en: A more data-efficient ViT trained at scale using distillation.
  prefs: []
  type: TYPE_NORMAL
- en: PVT (Pyramid Vision Transformer)
  prefs: []
  type: TYPE_NORMAL
- en: A hierarchical model that can produce multiscale feature maps for semantic segmentation
    and other dense prediction tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Swin Transformer (Shifted Windows Transformer)
  prefs: []
  type: TYPE_NORMAL
- en: A much faster hierarchical model.
  prefs: []
  type: TYPE_NORMAL
- en: DINO (self-Distillation with NO labels)
  prefs: []
  type: TYPE_NORMAL
- en: This introduced a novel self-supervised technique for visual representation
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second part of this chapter, we will dive into multimodal transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: VideoBERT
  prefs: []
  type: TYPE_NORMAL
- en: A BERT model trained to process both text and video tokens.
  prefs: []
  type: TYPE_NORMAL
- en: ViLBERT (Visio-Linguistic BERT)
  prefs: []
  type: TYPE_NORMAL
- en: A dual-encoder model for image plus text, which introduced co-attention (i.e.,
    two-way cross-attention).
  prefs: []
  type: TYPE_NORMAL
- en: CLIP (Contrastive Language–Image Pretraining)
  prefs: []
  type: TYPE_NORMAL
- en: This is another image plus text dual-encoder model trained using contrastive
    pretraining.
  prefs: []
  type: TYPE_NORMAL
- en: DALL·E (a pun on the names of the artist Salvador Dali and the Pixar character
    Wall-E)
  prefs: []
  type: TYPE_NORMAL
- en: A model capable of generating images from text prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Perceiver
  prefs: []
  type: TYPE_NORMAL
- en: This efficiently compresses any high-resolution modality into a short sequence
    using a cross-attention trick.
  prefs: []
  type: TYPE_NORMAL
- en: Perceiver IO (Input/Output)
  prefs: []
  type: TYPE_NORMAL
- en: Adds a flexible output mechanism to the Perceiver, using a similar cross-attention
    trick.
  prefs: []
  type: TYPE_NORMAL
- en: Flamingo
  prefs: []
  type: TYPE_NORMAL
- en: Rather than starting from scratch, it reuses two large pretrained models—one
    for vision and one for language (both frozen)—and connects them using a Perceiver-style
    adapter named a Resampler. This architecture enables open-ended visual dialogue.
  prefs: []
  type: TYPE_NORMAL
- en: BLIP-2 (Bootstrapping Language-Image Pretraining)
  prefs: []
  type: TYPE_NORMAL
- en: This is another open-ended visual dialogue model that reuses two large pretrained
    models, connects them using a lightweight querying transformer (Q-Former), and
    uses a powerful two-stage training approach with multiple training objectives.
  prefs: []
  type: TYPE_NORMAL
- en: So turn on the lights, transformers are about to open their eyes.
  prefs: []
  type: TYPE_NORMAL
- en: Vision Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Vision transformers didn’t pop out of a vacuum: before they were invented,
    there were RNNs with visual attention, and hybrid CNN-Transformer models. Let’s
    take a look at these ViT ancestors before we dive into some of the most influential
    ViTs.'
  prefs: []
  type: TYPE_NORMAL
- en: RNNs with Visual Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the first applications of attention mechanisms beyond NLP was to generate
    image captions using [visual attention](https://homl.info/visualattention).⁠^([1](ch16.html#id3722))
    Here a convolutional neural network first processes the image and outputs some
    feature maps, then a decoder RNN equipped with an attention mechanism generates
    the caption, one token at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The decoder uses an attention layer at each decoding step to focus on just
    the right part of the image. For example, in [Figure 16-1](#visual_attention_diagram),
    the model generated the caption “A woman is throwing a Frisbee in a park”, and
    you can see what part of the input image the decoder focused its attention on
    when it was about to output the word “Frisbee”: clearly, most of its attention
    was focused on the Frisbee.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A woman in green throws a red Frisbee in a park, and the attention layer
    highlights the Frisbee when producing the word "Frisbee" in a caption.](assets/hmls_1601.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-1\. Visual attention: an input image (left) and the model’s focus
    before producing the word “Frisbee” (right)⁠^([2](ch16.html#id3724))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once transformers were invented, they were quickly applied to visual tasks,
    generally by replacing RNNs in existing architectures (e.g., for image captioning).
    However, the bulk of the visual work was still performed by a CNN, so although
    they were transformers used for visual tasks, we usually don’t consider them as
    ViTs. The *detection transformer* (DETR) is a good example of this.
  prefs: []
  type: TYPE_NORMAL
- en: 'DETR: A CNN-Transformer Hybrid for Object Detection'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In May 2020, a team of Facebook researchers proposed a hybrid CNN–transformer
    architecture for object detection, named [*detection transformer*](https://homl.info/detr)
    (DETR, see [Figure 16-2](#detr_diagram)).⁠^([4](ch16.html#id3740)) The CNN first
    processes the input images and outputs a set of feature maps, then these feature
    maps are turned into a sequence of visual tokens that are fed to an encoder-decoder
    transformer, and finally the transformer outputs a sequence of bounding box predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'At one point, someone was bound to wonder whether we could get rid of the CNN
    entirely. After all, attention is all you need, right? This happened a few months
    after DETR: the original ViT was born.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of the detection transformer (DETR) process illustrating how visual
    tokens and positional encoding are used to identify objects like a temple and
    a tree with confidence levels.](assets/hmls_1602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-2\. The detection transformer (DETR) for object detection
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Original ViT
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In October 2020, a team of Google researchers released [a paper](https://homl.info/vit)⁠^([5](ch16.html#id3741))
    that introduced the first vision transformer without a CNN (see [Figure 16-3](#vit_diagram)).
    It was simply named the *vision transformer* (ViT). The idea is surprisingly simple:
    chop the image into little 16 × 16 patches, and treat the sequence of patches
    as if it is a sequence of word representations. In fact, the paper’s title is
    “An Image Is Worth 16 × 16 Words”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To be more precise, the patches are first flattened into 16 × 16 × 3 = 768-dimensional
    vectors (the 3 is for the RGB color channels). For example, a 224 × 224 image
    gets chopped into 14 × 14 = 196 patches, so we get 196 vectors of 768 dimensions
    each. These vectors then go through a linear layer that projects the vectors to
    the transformer’s embedding size. The resulting sequence of vectors can then be
    treated just like a sequence of word embeddings: add learnable positional embeddings,
    and pass the result to the transformer, which is a regular encoder-only model.
    A class token with a trainable representation is inserted at the start of the
    sequence, and a classification head is added on top of the corresponding output
    (i.e., this is BERT-style classification).'
  prefs: []
  type: TYPE_NORMAL
- en: And that’s it! This model beat the state of the art on ImageNet image classification,
    but to be fair the authors had to use over 300 million additional images for training.
    This makes sense since transformers don’t have as many *inductive biases* as convolution
    neural nets, so they need extra data just to learn things that CNNs implicitly
    assume.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An inductive bias is an implicit assumption made by the model, due to its architecture.
    For example, linear models implicitly assume that the data is, well, linear. CNNs
    are translation invariant, so they implicitly assume that patterns learned in
    one location will likely be useful in other locations as well. They also have
    a strong bias toward locality. RNNs implicitly assume that the inputs are ordered,
    and that recent tokens are more important than older ones. The more inductive
    biases a model has, assuming they are correct, the less training data the model
    will require. But if the implicit assumptions are wrong, then the model may perform
    poorly even if it is trained on a large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the Vision Transformer (ViT) model, showing the process
    of converting image patches into token embeddings for classification.](assets/hmls_1603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-3\. Vision transformer (ViT) for classification
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now you know everything you need to implement a ViT from scratch!
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a ViT from scratch using PyTorch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will start by implementing a custom module to take care of patch embedding.
    For this, we can actually use an `nn.Conv2d` module with `kernel_size` and `stride`
    both set to the patch size (16). This is equivalent to chopping the image into
    patches, flattening them, and passing them through a linear layer (then reshaping
    the result). Just what we need!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After the convolutional layer, we must flatten the spatial dimensions and transpose
    the last two dimensions to ensure the embedding dimension ends up last, which
    is what the `nn.TransformerEncoder` module expects. Now we’re ready to implement
    our ViT model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s go through this code:'
  prefs: []
  type: TYPE_NORMAL
- en: The constructor starts by creating the `PatchEmbedding` module.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then it creates the class token’s trainable embedding, initialized using a normal
    distribution with a small standard deviation (0.02 is common). Its shape is [1,
    1, *E*], where *E* is the embedding dimension.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we initialize the positional embeddings, of shape [1, 1 + *L*, *E*], where
    *L* is the number of patch tokens. We need one more positional embedding for the
    class token, hence the 1 + *L*. Again, we initialize it using a normal distribution
    with a small standard deviation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, we create the other modules: `nn.Dropout`, `nn.TransformerEncoder` (based
    on an `nn.TransformerEncoderLayer`), `nn.LayerNorm`, and the output linear layer
    that we will use as a classification head.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the `forward()` method, we start by creating the patch tokens.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then we replicate the class token along the batch axis, using the `expand()`
    method, and we concatenate the patch tokens. This ensures that each sequence of
    patch tokens starts with the class token.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The rest is straightforward: we add the positional embeddings, apply some dropout,
    run the encoder, keep only the class token’s output (`Z[:, 0]`) and normalize
    it, and lastly pass it through the output layer, which produces the logits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can create the model and test it with a random batch of images, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can then train this model using the `nn.CrossEntropyLoss`, as usual. This
    would take quite a while, however, so unless your image dataset is very domain-specific,
    you’re usually better off downloading a pretrained ViT using the Transformers
    library and then fine-tuning it on your dataset. Let’s see how.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a pretrained ViT using the Transformers library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s download a small pretrained ViT and fine-tune it on the Oxford-IIIT Pet
    dataset, which contains over 7,000 pictures of pets grouped into 37 different
    classes. First, let’s download the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s download the ViT:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We’re loading a base ViT model that was pretrained on the ImageNet-21k dataset.
    This dataset contains roughly 14 million images across over 21,800 classes. We’re
    using the `ViTForImageClassification` class which automatically replaces the original
    classification head with a new one (untrained) for the desired number of classes.
    That’s the part we now need to train.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also loaded the image processor for this model. We will use it to preprocess
    each image as the model expects: it will be rescaled to 224 × 224, pixel values
    will be normalized to range between –1 and 1, and the channel dimension will be
    moved in front of the spatial dimensions. We also set `use_fast=True` because
    a fast implementation of the image processor is available, so we might as well
    use it. The processor takes an image as input and returns a dictionary containing
    a “pixel_values” entry equal to the preprocessed image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need a data collator that will preprocess all the images in a batch
    and return the images and labels as PyTorch tensors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We set `do_convert_rgb=True` because the model expects RGB images, but some
    images in the dataset are RGBA (i.e., they have an extra transparency channel),
    so we must force the conversion to RGB to avoid an error in the middle of training.
    And now we’re ready to train our model using the familiar Hugging Face training
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'By default, the trainer will automatically remove input attributes that are
    not used by the `forward()` method: our model expects `pixel_values` and optionally
    `labels`, but anything else will be dropped, including the `"image"` attribute.
    Since the unused attributes are dropped before the `collate_fn()` function is
    called, the code `example["image"]` will cause an error. This is why we must set
    `remove_unused_columns=False`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After just 3 epochs, our ViT model reaches about 91.8% accuracy. With some
    data augmentation and more training, you could reach 93 to 95% accuracy, which
    is close to the state of the art. Great! But we’re just getting started: ViTs
    have been improved in many ways since 2020\. In particular, it’s possible to train
    them from scratch in a much more efficient way using distillation. Let’s see how.'
  prefs: []
  type: TYPE_NORMAL
- en: Data-Efficient Image Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just two months after Google’s ViT paper was published, a team of Facebook researchers
    released [*data-efficient image transformers*](https://homl.info/deit) (DeiT).⁠^([6](ch16.html#id3760))
    Their DeiT model achieved competitive results on ImageNet without requiring any
    additional data for training. The model’s architecture is virtually the same as
    the original ViT (see [Figure 16-4](#deit_diagram)), but the authors used a distillation
    technique to transfer knowledge from a teacher model to their student ViT model
    (distillation was introduced in [Chapter 15](ch15.html#transformer_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: The authors used a frozen, state-of-the-art CNN as the teacher model. During
    training, they added a special distillation token to the student ViT model. Just
    like the class token, the distillation token representation is trainable, and
    its output goes through a dedicated classification head. Both classification heads
    (for the class token and for the distillation token) are trained simultaneously,
    both using the cross-entropy loss, but the class token’s classification head is
    trained using the normal hard targets (i.e., one-hot vectors), while the distillation
    head is trained using soft targets output by a teacher model. The final loss is
    a weighted sum of both classification losses (typically with equal weights). At
    inference time, the distillation token is dropped, along with its classification
    head. And that’s all there is to it! If you fine-tune a DeiT model on the same
    pets dataset, using `model_id = "facebook/deit-base-distilled-patch16-224"` and
    `DeiTForImageClassification`, you should get around 94.4% validation accuracy
    after just three epochs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the data-efficient image transformer (DeiT) process,
    showing the flow from image patches through linear transformation, encoding with
    position and class tokens, distillation token inclusion, to classifier heads with
    hard and soft target outputs from a CNN.](assets/hmls_1604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-4\. Data-efficient image transformer (DeiT) = ViT + distillation
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So far, we have only used ViTs for classification tasks, but what about dense
    prediction tasks such as object detection or semantic segmentation (introduced
    in [Chapter 12](ch12.html#cnn_chapter))? For this, the ViT architecture needs
    to be tweaked a bit; welcome to hierarchical vision transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Pyramid Vision Transformer for Dense Prediction Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The year 2021 was a year of plenty for ViTs: new models advanced the state
    of the art almost every other week. An important milestone was the release of
    the [Pyramid Vision Transformer (PVT)](https://homl.info/pvt) in February 2021,⁠^([7](ch16.html#id3765))
    developed by a team of researchers from Nanjing University, HKU, IIAI, and SenseTime
    Research. They pointed out that the original ViT architecture was good at classification
    tasks, but not so much at dense prediction tasks, where fine-grained resolution
    is needed. To solve this issue, they proposed a pyramidal architecture in which
    the image is processed into a gradually smaller but deeper image (i.e., semantically
    richer), much like in a CNN. [Figure 16-5](#pvt_diagram) shows how a 256 × 192
    image with 3 channels (RGB) is first turned into a 64 × 48 image with 64 channels,
    then into a 32 × 24 image with 128 channels, then a 16 × 12 image with 320 channels,
    and lastly an 8 × 6 image with 512 channels.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the Pyramid Vision Transformer process, showing how
    an image is transformed through multiple stages with increasing channels and reduced
    spatial resolution, ultimately used for dense prediction tasks.](assets/hmls_1605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-5\. Pyramid Vision Transformer for dense prediction tasks
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At each pyramid level, the input image is processed very much like in a regular
    ViT. It is first chopped into patches and turned into a sequence of patch tokens,
    then trainable positional embeddings are added, and the resulting tokens are passed
    through an encoder-only transformer, composed of multiple encoder layers.
  prefs: []
  type: TYPE_NORMAL
- en: Since the encoder outputs a sequence of vectors (i.e., contextualized embeddings),
    this sequence must be reshaped into a *grid* of vectors, which can then be treated
    as an image (with many channels) and passed on to the next level of the pyramid.
    For example, the encoder at the first level receives a sequence of 3,072 patch
    tokens, since the image was chopped into a 64 × 48 grid of 4 × 4 patches (and
    64 × 48 = 3,072). Each patch token is represented as a 64-dimensional vector.
    The encoder also outputs 3,072 vectors (i.e., contextualized embeddings), each
    64-dimensional, and they are organized into a 64 × 48 grid once again. This gives
    us a 64 × 48 image with 64 channels, which can be passed on to the next level.
    In levels 2, 3, and 4 of the pyramid, the patch tokens are 128-, 320-, and 512-dimensional,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Importantly, the patches are much smaller than in the original ViT: instead
    of 16 × 16, they are just 4 × 4 at level 1, and 2 × 2 at levels 2, 3, and 4\.
    These tiny patches offer a much higher spatial resolution, which is crucial for
    dense prediction tasks. However, this comes at a cost: smaller patches means many
    more of them, and since multi-head attention has quadratic complexity, a naive
    adaptation of ViT would require vastly more computation. This is why the PVT authors
    introduced *spatial reduction attention* (SRA): it’s just like MHA except that
    the keys and values are first spatially reduced (but not the queries). For this,
    the authors proposed a sequence of operations that is usually implemented as a
    strided convolutional layer, followed by layer norm (although some implementations
    use an average pooling layer instead).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the impact of SRA at the first level of the pyramid. There are
    3,072 patch tokens. In regular MHA, each of these tokens would attend to every
    token, so we would have to compute 3,072² attention scores: that’s over 9 million
    scores! In SRA, the query is unchanged so it still involves 3,072 tokens, but
    the keys and values are reduced spatially by a factor of 8, both horizontally
    and vertically (in levels 2, 3, and 4 of the pyramid, the reduction factor is
    4, 2, and 1, respectively). So instead of 3,072 tokens representing a 64 × 48
    grid, the keys and values are only composed of 48 tokens representing an 8 × 6
    grid (because 64 / 8 = 8 and 48 / 8 = 6). So we only need to compute 3,072 × 48
    = 147,456 attention scores: that’s 64 times less computationally expensive. And
    the good news is that this doesn’t affect the output resolution since we didn’t
    reduce the query at all: the encoder still output 3,072 tokens, representing a
    64 × 48 image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'OK, so the PVT model takes an image and outputs four gradually smaller and
    deeper images. Now what? How do we use these multiscale feature maps to implement
    object detection or other dense prediction tasks? Well, no need to reinvent the
    wheel: existing solutions generally involve a CNN backbone that produces multiscale
    feature maps, so we can simply swap out this backbone for a PVT (often pretrained
    on ImageNet). For example, we can use an FCN approach for semantic segmentation
    (introduced at the end of [Chapter 12](ch12.html#cnn_chapter)) by upscaling and
    combining the multiscale feature maps output by the PVT, and add a final classification
    head to output one class per pixel. Similarly, we can use a Mask R-CNN for object
    detection and instance segmentation, replacing its CNN backbone with a PVT.'
  prefs: []
  type: TYPE_NORMAL
- en: In short, the PVT’s hierarchical structure was a big milestone for vision transformers,
    but despite spatial reduction attention, it’s still computationally expensive.
    The Swin Transformer, released one month later, is much more scalable. Let’s see
    why.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Swin Transformer: A Fast and Versatile ViT'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In March 2021, a team of Microsoft researchers released the [Swin Transformer](https://homl.info/swin).⁠^([8](ch16.html#id3776))
    Just like PVT, it has a hierarchical structure, producing multiscale feature maps
    which can be used for dense prediction tasks. But Swin uses a very different variant
    of multi-head attention: each patch only attends to patches located within the
    same window. This is called *window-based multi-head self-attention* (W-MSA),
    and it allows the cost of self-attention to scale linearly with the image size
    (meaning its area), instead of quadratically.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, on the lefthand side of [Figure 16-6](#swin_diagram), the image
    is chopped into a 28 × 28 grid of patches, and these patches are grouped into
    nonoverlapping windows. At the first level of the Swin pyramid, the patches are
    usually 4 × 4 pixels, and each window contains a 7 × 7 grid of patches. So there’s
    a total of 784 patch tokens (28 × 28), but each token only attends to 49 tokens
    (7 × 7), so the W-MSA layer only needs to compute 784 × 49 = 38,416 attention
    scores, instead of 784² = 614,656 scores for regular MHA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most importantly, if we double the width and the height of the image, we quadruple
    the number of patch tokens, but each token still attends to only 49 tokens, so
    we just need to compute 4 times more attention scores: the Swin Transformer’s
    computational cost scales linearly with the image’s area, so it can handle large
    images. Conversely, ViT, DeiT, and PVT all scale quadratically: if you double
    the image width and height, the area is quadrupled, and the computational cost
    is multiplied by 16! As a result, these models are way too slow for very large
    images, meaning you must first downsample the image, which may hurt the model’s
    accuracy, especially for dense prediction tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of the Swin Transformer architecture showing regular multi-head self-attention
    (S-MSA), shifted window multi-head self-attention (SW-MSA), and optimized SW-MSA,
    highlighting how different window configurations cover the same image.](assets/hmls_1606.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-6\. Swin Transformer: alternates W-MSA (left) and SW-MSA (center);
    SW-MSA can be optimized to require the same number of windows as W-MSA (right)'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'But wait a minute! If each token only attends to patches within the same window,
    how can we hope to capture long-range patterns? The answer is in the name of the
    architecture, Swin, which stands for *shifted windows*: every other encoder layer
    uses *shifted W-MSA* (SW-MSA), which is just like W-MSA except the windows are
    offset by half a window size. As you can see in the middle of [Figure 16-6](#swin_diagram),
    the windows are shifted by 3 patches toward the bottom right (because half of
    7 is 3.5, which we round down to 3). Why does this help? Well, nearby patches
    that were in separate windows in the previous layer are now in the same window,
    so they can see each other. By alternating W-MSA and SW-MSA, information from
    any part of the image can gradually propagate throughout the whole image. Moreover,
    since the architecture is hierarchical, the patches get coarser and coarser as
    we go up the pyramid, so the information can propagate faster and faster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A naive implementation of SW-MSA would require handling many extra windows.
    For example, if you compare W-MSA and SW-MSA in [Figure 16-6](#swin_diagram),
    you can see that W-MSA uses 16 windows, while SW-MSA uses 25 (at least in this
    example). To avoid this extra cost, the authors proposed an optimized implementation:
    instead of shifting the windows, we shift the image itself and wrap it around
    the borders, as shown in the righthand side of [Figure 16-6](#swin_diagram). This
    way, we’re back to 16 windows. However, this requires careful masking for the
    border windows that contain the wrapped patches; for example, the regions labeled
    ①, ②, ③, ④ should not see each other, even though they are within the same window,
    so an appropriate attention mask must be applied.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, Swin is harder to implement than PVT, but its linear scaling and excellent
    performance make it one of the best vision transformers out there. But the year
    2021 wasn’t over: [Swin v2](https://homl.info) was released in November 2021.⁠^([9](ch16.html#id3781))
    It improved Swin across the board: more stable training for large ViTs, easier
    to fine-tune on large images, reduced need for labeled data, and more. Swin v2
    is still widely used in vision tasks today.'
  prefs: []
  type: TYPE_NORMAL
- en: Our toolbox now contains vision transformers for classification (e.g., ViT and
    DeiT) and for dense prediction tasks (e.g., PVT and Swin). Let’s now explore one
    last pure-vision transformer, DINO, which introduced a revolutionary self-supervision
    technique for visual representation learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'DINO: Self-Supervised Visual Representation Learning'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In April 2021, Mathilde Caron et al. introduced [DINO](https://homl.info/dino),^([10](ch16.html#id3788))
    an impressive self-supervised training technique that produces models capable
    of generating excellent image representations. These representations can then
    be used for classification and other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s how it works: the model is duplicated during training, with one network
    acting as a teacher and the other acting as a student (see [Figure 16-7](#dino_diagram)).
    Gradient descent only affects the student, while the teacher’s weights are just
    an exponential moving average (EMA) of the student’s weights. This is called a
    *momentum teacher*. The student is trained to match the teacher’s predictions:
    since they’re almost the same model, this is called *self-distillation* (hence
    the name of the model: self-**di**stillation with **no** labels).'
  prefs: []
  type: TYPE_NORMAL
- en: 'At each training step, the input images are augmented in various ways: color
    jitter, grayscale, Gaussian blur, horizontal flipping, and more. Importantly,
    they are augmented in different ways for the teacher and the student: the teacher
    always sees the full image, only slightly augmented, while the student often sees
    only a zoomed-in section of the image, with stronger augmentations. In short,
    the teacher and the student don’t see the same variant of the original image,
    yet their predictions must still match. This forces them to agree on high-level
    representations.'
  prefs: []
  type: TYPE_NORMAL
- en: With this mechanism, however, there’s a strong risk of *mode collapse*. This
    is when both the student and the teacher always output the exact same thing, completely
    ignoring the input images. To prevent this, DINO keeps track of a moving average
    of the teacher’s predicted logits, and it subtracts this average from the predicted
    logits. This is called *centering*, forcing the teacher to distribute its predictions
    evenly across all classes (on average, over time).
  prefs: []
  type: TYPE_NORMAL
- en: 'But centering alone might cause the teacher to simply output the same probability
    for every class, all the time, still ignoring the image. To avoid this, DINO also
    forces the teacher to have high confidence in its highest predictions: this is
    called *sharpening*. It’s implemented by applying a low temperature to the teacher’s
    logits (i.e., dividing them by a temperature smaller than 1). Together, centering
    and sharpening preserve the diversity in the teacher’s outputs; this leaves no
    easy shortcut for the model. It must base its predictions on the actual content
    of the image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the DINO model''s use of teacher and student Vision
    Transformers (ViTs) with centering and sharpening to enhance predictive accuracy
    without labels.](assets/hmls_1607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-7\. DINO, or self-distillation with no labels
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'After training, you can drop the teacher: the student is the final DINO model.
    If you feed it a new image, it will output a sequence of contextualized patch
    embeddings. These can be used in various ways. For example, you can train a classifier
    head on top of the class token’s output embedding. In fact, you don’t even need
    a new classifier head: you can run DINO on every training image to get their representation
    (i.e., the output of the class token), then compute the mean representation per
    class. Then, when given a new image, use DINO to compute its representation and
    look for the class with the nearest mean representation. This simple approach
    reaches 78.3% top-1 accuracy on ImageNet, which is pretty impressive.'
  prefs: []
  type: TYPE_NORMAL
- en: But it’s not just about classification! Interestingly, the DINO authors noticed
    that the class token’s attention maps in the last layer often focus on the main
    object of interest in the image, even though they were trained entirely without
    labels! In fact, each attention head seems to focus on a different part of the
    object, as you can see in [Figure 16-8](#unsupervised_segmentation_diagram).⁠^([11](ch16.html#id3797))
    See the notebook for a code example that uses DINO to plot a similar attention
    map.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing unsupervised image segmentation using DINO, with different
    attention heads highlighting various parts of objects like vegetables, a zebra,
    and a truck.](assets/hmls_1608.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-8\. Unsupervised image segmentation using DINO—different attention
    heads attend to different parts of the main object
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Later techniques such as [TokenCut](https://homl.info/tokencut)⁠^([12](ch16.html#id3799))
    built upon DINO to detect and segment objects in images and videos. Then, in April
    2023, Meta released [DINOv2](https://homl.info/dino2),⁠^([13](ch16.html#id3801))
    which was trained on a curated and much larger dataset, and was tweaked to output
    per-patch features, making it a great foundation model not just for classification,
    but also for dense prediction tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s step back: we’ve covered CNN-based transformers such as DETR, followed
    by the original ViT (image patches through an encoder), DeiT (a distilled ViT),
    PVT (a hierarchical ViT with spatial reduction attention), Swin (a hierarchical
    ViT with window-based attention), and DINO (self-distillation with no labels).
    Before we move on to multimodal transformers, let’s quickly go through a few more
    pure-vision transformer models and techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Other Major Vision Models and Techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Progress in vision transformers has continued steadily to this day. Here is
    a brief overview of some landmark papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[“Scaling Vision Transformers”](https://homl.info/scalingvits),⁠^([14](ch16.html#id3806))
    June 2021'
  prefs: []
  type: TYPE_NORMAL
- en: 'Google researchers showed how to scale ViTs up or down, depending on the amount
    of available data. They managed to create a huge 2 billion parameter model that
    reached over 90.4% top-1 accuracy on ImageNet. Conversely, they also trained a
    scaled-down model that reached over 84.8% top-1 accuracy on ImageNet, using only
    10,000 images: that’s just 10 images per class!'
  prefs: []
  type: TYPE_NORMAL
- en: '[“BEiT: BERT Pre-Training of Image Transformers”](https://homl.info/beit),⁠^([15](ch16.html#id3808))
    June 2021'
  prefs: []
  type: TYPE_NORMAL
- en: Hangbo Bao et al. proposed a *masked image modeling* (MIM) approach inspired
    from BERT’s masked language modeling (MLM). BEiT is pretrained to reconstruct
    masked image patches from the visible ones. This pretraining technique significantly
    improves downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Note that BEiT is not trained to predict the raw pixels of the masked patches;
    instead, it must predict the masked token IDs. But where do these token IDs come
    from? Well, the original image is passed through a *discrete variational autoencoder*
    (dVAE, see [Chapter 18](ch18.html#autoencoders_chapter)) which encodes each patch
    into a visual token ID (an integer), from a fixed vocabulary. These are the IDs
    that BEiT tries to predict. The goal is to avoid wasting the model’s capacity
    on unnecessary details.
  prefs: []
  type: TYPE_NORMAL
- en: '[“Masked Autoencoders Are Scalable Vision Learners”](https://homl.info/mae),⁠^([16](ch16.html#id3816))
    November 2021'
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper by a team of Facebook researchers (led by the prolific Kaiming He)
    also proposes a pretraining technique based on masked image modeling, but it removes
    the complexity of BEiT’s dVAE: masked autoencoder (MAE) directly predicts raw
    pixel values. Crucially, it uses an asymmetric encoder-decoder architecture: a
    large encoder processes only the visible patches, while a lightweight decoder
    reconstructs the entire image. Since 75% of patches are masked, this design dramatically
    reduces computational cost and allows MAE to be pretrained on very large datasets.
    This leads to strong performance on downstream tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '[“Model Soups”](https://homl.info/modelsoups),⁠^([17](ch16.html#id3820)) March
    2022'
  prefs: []
  type: TYPE_NORMAL
- en: This paper demonstrated that it’s possible to first train multiple transformers,
    then average their weights to create a new and improved model. This is similar
    to an ensemble (see [Chapter 6](ch06.html#ensembles_chapter)), except there’s
    just one model in the end, which means there’s no inference cost.
  prefs: []
  type: TYPE_NORMAL
- en: '[“EVA: Exploring the Limits of Masked Visual Representation Learning at Scale”](https://homl.info/eva),⁠^([18](ch16.html#id3821))
    May 2022'
  prefs: []
  type: TYPE_NORMAL
- en: EVA is a family of large ViTs pretrained at scale, using enhanced MAE and strong
    augmentations. It’s one of the leading foundation models for ViTs. EVA-02, released
    in March 2023, does just as well or better despite having fewer parameters. The
    large variant has 304M parameters and reaches an impressive 90.0% on ImageNet.
  prefs: []
  type: TYPE_NORMAL
- en: '[I-JEPA](https://homl.info/ijepa),⁠^([19](ch16.html#id3823)) January 2023'
  prefs: []
  type: TYPE_NORMAL
- en: Yann LeCun proposed the joint-embedding predictive architecture (JEPA) in a
    [2022 paper](https://homl.info/jepa),⁠^([20](ch16.html#id3826)) as part of his
    world-model framework, which aims to deepen AI’s understanding of the world and
    improve the reliability of its predictions. I-JEPA is an implementation of JEPA
    for images. It was soon followed by [V-JEPA](https://homl.info/vjepa) in 2024,
    and [V-JEPA 2](https://homl.info/vjepa2) in 2025, both of which process videos.
  prefs: []
  type: TYPE_NORMAL
- en: 'During training, JEPA involves two encoders and a predictor: the teacher encoder
    sees the full input (e.g., a photo of a cat) while the student encoder sees only
    part of the input (e.g., the same cat photo but without the ears). Both encoders
    convert their inputs to embeddings, then the predictor tries to predict the teacher
    embedding for the missing part (e.g., the ears) given the student embeddings for
    the rest of the input (e.g., the cat without ears). The student encoder and the
    predictor are trained jointly, while the teacher encoder is just a moving average
    of the student encoder (much like in DINO). JEPA mostly works in embedding space
    rather than pixel space, which makes it fast, parameter efficient, and more semantic.'
  prefs: []
  type: TYPE_NORMAL
- en: After training, the teacher encoder and the predictor are no longer needed,
    but the student encoder can be used to generate excellent, meaningful representations
    for downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The list could go on and on:'
  prefs: []
  type: TYPE_NORMAL
- en: NesT or DeiT-III for image classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MobileViT, EfficientFormer, EfficientViT, or TinyViT for small and efficient
    image classification models (e.g., for mobile devices)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical transformers like Twins-SVT, FocalNet, MaxViT, and InternImage,
    often used as backbones for dense prediction tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mask2Former or OneFormer for general-purpose segmentation, SEEM for universal
    segmentation, and SAM or MobileSAM for interactive segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ViTDet or RT-DETR for object detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TimeSformer, VideoMAE, or OmniMAE for video understanding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also techniques like *token merging* (ToMe) which speeds up inference
    by merging similar tokens on the fly, *token pruning* to drop unimportant tokens
    during processing (i.e., with low attention scores), *early exiting* to only compute
    deep layers for the most important tokens, *patch selection* to select only the
    most informative patches for processing, and self-supervised training techniques
    like SimMIM, iBOT, CAE, or DINOv2, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully we’ve covered a wide enough variety of models and techniques for you
    to be able to explore further on your own.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Some of these vision-only models were pretrained on multimodal data (e.g.,
    image-text pairs or input prompts): OmniMAE, SEEM, SAM, MobileSAM, and DINOv2\.
    Which leads us nicely to the second part of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: We already had transformers that could read and write (and chat!), and now we
    have vision transformers that can see. It’s time to build transformers that can
    handle both text and images at the same time, as well as other modalities.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Humans are multimodal creatures: we perceive the world through multiple senses—sight,
    hearing, smell, taste, touch, sense of balance, proprioception (i.e., sense of
    body position), and several others—and we act upon the world through movement,
    speech, writing, etc. Each of these modalities can be considered at a very low
    level (e.g., sound waves) or at a higher level (e.g., words, intonations, melody).
    Importantly, modalities are heterogeneous: one modality may be continuous while
    another is discrete, one may be temporal while the other is spatial, one may be
    high-resolution (e.g., 48 kHz audio) while the other is not (e.g., text), one
    may be noisy while the other is clean, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, modalities may interact in various ways. For example, when we chat
    with someone, we may listen to their voice while also watching the movement of
    their lips: these two modalities (auditory and visual) carry overlapping information,
    which helps our brain better parse words. But multimodality is not just about
    improving the signal/noise ratio: facial expressions may carry their own meaning
    (e.g., smiles and frowns), and different modalities may combine to produce a new
    meaning. For example, if you say “he’s an expert” while rolling your eyes or gesturing
    air quotes, you’re clearly being ironic, which inverts the meaning of your sentence
    and conveys extra information (e.g., humor or disdain) which neither modality
    possesses on its own.'
  prefs: []
  type: TYPE_NORMAL
- en: So multimodal machine learning requires designing models that can handle very
    heterogeneous data and capture their interactions. There are two main challenges
    for this. The first is called *fusion*, and it’s about finding a way to combine
    different modalities, for example, by encoding them into the same representation
    space. The second is called *alignment*, where the goal is to discover the relationships
    between modalities. For example, perhaps you have a recording of a speech, as
    well as a text transcription, and you want to find the timestamp of each word.
    Or you want to find the most relevant object in an image given a text query such
    as “the dog next to the tree” (this is called *visual grounding*). Many other
    common tasks involve two or more modalities, such as image captioning, image search,
    visual question answering (VQA), speech-to-text (STT), text-to-speech (TTS), embodied
    AI (i.e., a model capable of physically interacting with the environment), and
    much more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multimodal machine learning has been around for decades, but progress has recently
    accelerated thanks to deep learning, and particularly since the rise of transformers.
    Indeed, transformers can ingest pretty much any modality, as long as you can chop
    it into a sequence of meaningful tokens (e.g., text into words, images into small
    patches, audio or video into short clips, etc.). Once you have prepared a sequence
    of token embeddings, you’re ready to feed it to a transformer. Embeddings from
    different modalities can be fused in various ways: summed up, concatenated, passed
    through a fusion encoder, and more. This can take care of the fusion problem.
    And transformers also have multi-head attention, which is a powerful tool to detect
    and exploit complex patterns, both within and across modalities. This can take
    care of the alignment problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Researchers quickly understood the potential of transformers for multimodal
    architectures. The first multimodal transformers were released just months after
    the original Transformer paper was released in early 2018 with image captioning,
    video captioning, and more. Let’s look at some of the most impactful multimodal
    transformer architectures, starting with VideoBERT.
  prefs: []
  type: TYPE_NORMAL
- en: 'VideoBERT: A BERT Variant for Text plus Video'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In April 2019, Google researchers released [VideoBERT](https://homl.info/videobert).⁠^([21](ch16.html#id3840))
    As its name suggests, this model is very similar to BERT, except it can handle
    both text and videos. In fact, the authors just took a pretrained BERT-large model,
    extended its embedding matrix to allow for extra video tokens (more on this shortly),
    and continued training the model using self-supervision on a text plus video training
    set. This dataset was built from a large collection of instructional YouTube videos,
    particularly cooking videos. These videos typically involve someone describing
    a sequence of actions while performing them (e.g., “Cut the tomatoes into thin
    slices like this”). To feed these videos to VideoBERT, the authors had to encode
    the videos into both text and visual sequences (see [Figure 16-9](#videobert_encoding_diagram)):'
  prefs: []
  type: TYPE_NORMAL
- en: For the visual modality, they extracted nonoverlapping 1.5-second clips at 20
    frames per second (i.e., 30 frames each), and they passed these clips through
    a 3D CNN named S3D. This CNN is based on Inception modules and separable convolutions
    (see [Chapter 12](ch12.html#cnn_chapter)), and it was pretrained on the Kinetics
    dataset composed of many YouTube videos of people performing a wide range of actions.
    The authors added a 3D average pooling layer on top of S3D to get a 1,024-dimensional
    vector for each video clip. Each vector encodes fairly high-level information
    about the video clip.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To extract the text from the videos, the authors used YouTube’s internal speech-to-text
    software, after which they dropped the audio tracks from the videos. Then they
    separated the text into sentences by adding punctuation using an off-the-shelf
    LSTM model. Finally, they preprocessed and tokenized the text just like for BERT.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Diagram illustrating the VideoBERT process, showing how actions in a video
    are converted into text and visual tokens using S3D for video clips and speech-to-text
    for audio.](assets/hmls_1609.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-9\. VideoBERT—encoding a video into a text sequence and a visual sequence
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Great! We now have a text token sequence describing some actions, and a sequence
    of vectors representing video clips of these actions. However, we have a problem.
    Recall that BERT is pretrained using MLM, where the model must predict masked
    tokens from a fixed vocabulary. We do have a fixed vocabulary for the text tokens,
    but not for the video tokens. So let’s build one! For this, the authors gathered
    all the visual vectors produced by S3D over their training set, and they clustered
    these vectors into *k* = 12 clusters using *k*-means (see [Chapter 8](ch08.html#unsupervised_learning_chapter)).
    Then they used *k*-means again on each cluster to get 12² = 144 clusters, then
    again and again to get 12⁴ = 20,736 clusters. This process is called *hierarchical
    k-means*, and it’s much faster than running *k*-means just once using *k* = 20,736,
    plus it typically produces much better clusters. Now each vector can be replaced
    with its cluster ID: this way, each video clip is represented by a single ID from
    a fixed visual vocabulary, so the whole video is now represented as one sequence
    of visual token IDs (e.g., 194, 3912, …​), exactly like tokenized text. In short,
    we’ve gone from a continuous 1,024-dimensional space down to a discrete space
    with just 20,736 possible values. There’s a lot of information loss at this step,
    but VideoBERT’s excellent performance suggests that much of the important information
    remains.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since the authors used a pretrained BERT-large model, the text token embeddings
    were already excellent before VideoBERT’s additional training even started. For
    the visual token embeddings, rather than using trainable embeddings initialized
    from scratch, the authors used frozen embeddings initialized using the 1,024-dimensional
    vector representations of the *k*-means cluster centroids.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors used three different training regimes: text-only, video-only, and
    text plus video. In text-only and video-only modes, VideoBERT was fed a single
    modality and trained to predict masked tokens (either text tokens or video tokens).
    For text plus video, the model was fed both text tokens and video tokens, simply
    concatenated (plus an unimportant separation token in between), and it had to
    predict whether the text tokens and video tokens came from the same part of the
    original video. This is called *linguistic-visual alignment*. For this, the authors
    added a binary classification head on top of the class token’s output (this replaces
    BERT’s next sentence prediction head). For negative examples, the authors just
    sampled random sentences and video segments. [Figure 16-10](#videobert_pretraining_diagram)
    shows all three modes at once, but keep in mind that they are actually separate.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the VideoBERT model, highlighting masked token prediction
    and linguistic-visual alignment processes.](assets/hmls_1610.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-10\. VideoBERT—pretraining using masked token prediction and linguistic-visual
    alignment (shown together but actually separate)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Linguistic-visual alignment is a noisy task since the cook may explain something
    that they have already finished or will do later, so the authors concatenated
    random neighboring sentences to give the model more context. The authors had a
    few more tricks up their sleeves, such as randomly changing the video sampling
    rate to make the model more robust to different action speeds, since some cooks
    are faster than others; see the paper for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'This was a lot of work, but the authors were finally done: they had a fully
    trained VideoBERT model. To demonstrate its effectiveness, they evaluated VideoBERT
    on some downstream tasks, including:'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot action classification
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a video clip, figure out which action is performed, without fine-tuning
    VideoBERT. The authors achieved this by feeding the video to VideoBERT, along
    with the following masked sentence: “Now let me show you how to [MASK] the [MASK]”.
    Then they looked at the output probabilities for both masked tokens, for each
    possible pair of verb and noun. If the video shows a cook slicing some tomatoes,
    then the probability of “slice” and “tomatoes” will be much higher than “bake”
    and “cake” or “boil” and “egg”.'
  prefs: []
  type: TYPE_NORMAL
- en: Video captioning
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a video clip, generate a caption. To do this, the authors used the earliest
    [video-captioning transformer architecture](https://homl.info/videocaption),⁠^([22](ch16.html#id3846))
    but they replaced the input to the encoder with visual features output by VideoBERT.
    More specifically, they took an average of VideoBERT’s final output representations,
    including the representations of all of the visual tokens and the masked-out text
    tokens. The masked sentence they used was: “now let’s [MASK] the [MASK] to the
    [MASK], and then [MASK] the [MASK]”. After fine-tuning this new model, they obtained
    improved results over the original captioning model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using similar approaches, VideoBERT can be adapted for many other tasks, such
    as multiple-choice visual question answering: given an image, a question, and
    multiple possible answers, the model must find the correct answer. For example:
    “What is the cook doing?” → “Slicing tomatoes”. For this, one approach is to simply
    run VideoBERT on each possible answer, along with the video, and compare the linguistic-visual
    alignment scores: the correct answer should have the highest score.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The success of VideoBERT inspired many other BERT-based multimodal transformers,
    many of which were released in August and September 2019: ViLBERT, VisualBERT,
    Unicoder-VL, LXMERT, VL-BERT, and UNITER. Most of these are single-stream models
    like VideoBERT, meaning that the modalities are fused very early in the network,
    typically by simply concatenating the sequences. However, ViLBERT and LXMERT are
    dual-stream transformers, meaning that each modality is processed by its own encoder,
    with a mechanism allowing the encoders to influence each other. This lets the
    model better understand each modality before trying to make sense of the interactions
    between them. VilBERT was particularly influential, so let’s look at it more closely.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ViLBERT: A Dual-Stream Transformer for Text plus Image'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ViLBERT was [proposed in August 2019](https://homl.info/vilbert)⁠^([23](ch16.html#id3852))
    by a team of researchers from the Georgia Institute of Technology, Facebook AI
    Research, and Oregon State University. They pointed out that the single-stream
    approach (used by VideoBERT and many others) treats both modalities identically,
    even though they may require different levels of processing. For example, if the
    visual features come from a deep CNN, then we already have good high-level visual
    features, whereas the text will need much more processing before the model has
    access to high-level text features. Moreover, the researchers hypothesized that
    “image regions may have weaker relations than words in a sentence”.⁠^([24](ch16.html#id3853))
    Lastly, BERT was initially pretrained using text only, so forcing it to process
    other modalities may give suboptimal results and even damage its weights during
    multimodal training.
  prefs: []
  type: TYPE_NORMAL
- en: 'So the authors chose a dual-stream approach instead: each modality goes through
    its own encoder, and in the upper layers the two encoders are connected and exchange
    information through a new bidirectional cross-attention mechanism called *co-attention*
    (see [Figure 16-11](#co_attention_diagram)). Specifically, in each pair of connected
    encoder layers, the MHA query of one encoder is used as the MHA key/value by the
    other encoder.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of connected vision and text co-encoder layers using co-attention,
    where the multi-head attention (MHA) query from one encoder is used as the key/value
    by the other encoder.](assets/hmls_1611.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-11\. Two encoder layers connected through co-attention: the MHA query
    of one encoder is used as the MHA key/value by the other encoder'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The lower layers of the text encoder are initialized with BERT’s weights (the
    authors used a BERT base, which has 12 layers), and 6 co-attention layers sit
    on top (see the lower-right quadrant of [Figure 16-12](#vilbert_diagram)). The
    visual features are produced by a pretrained and frozen Faster R-CNN model, and
    it is assumed that these features are sufficiently high level so no further processing
    is needed; therefore, the visual encoder is exclusively composed of six co-attention
    layers, paired up with the text encoder’s six co-attention layers (see the lower-left
    quadrant of the figure). The Faster R-CNN model’s outputs go through a mean pooling
    layer for each detected region, so we get one feature vector per region, and low-confidence
    regions are dropped: each image ends up represented by 10 to 36 vectors.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the ViLBERT model''s pretraining process for masked
    token prediction and linguistic-visual alignment, highlighting the interaction
    between the visual and text encoders and their integration for classification
    tasks.](assets/hmls_1612.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16-12\. ViLBert pretraining using masked token prediction and linguistic-visual
    alignment (again, shown together but actually separate)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Since regions don’t have a natural order like words do, the visual encoder
    does not use positional encoding. Instead, it uses spatial encodings that are
    computed like this: each region’s bounding box is encoded as a 5D vector containing
    the normalized upper-left and lower-right coordinates, and the ratio of the image
    covered by the bounding box. Then this 5D vector is linearly projected up to the
    same dimensionality as the visual vector, and simply added to it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, a special [IMG] token is prepended to the visual sequence: it serves
    the same purpose as the class token (i.e., to produce a representation of the
    whole sequence), but instead of being a trainable embedding, it’s computed as
    the average of the feature vectors (before spatial encoding), plus the spatial
    encoding for a bounding box covering the whole image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now on to training! Similar to VideoBERT, the authors used masked token prediction
    and linguistic-visual alignment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For masked token prediction, the authors used regular BERT-like MLM for the
    text encoder. However, for the visual encoder, since ViLBERT does not use a fixed-size
    visual vocabulary (there’s no clustering step), the model is trained to predict
    the class distribution that the CNN predicts for the given image region (this
    is a soft target). The authors chose this task rather than predicting raw pixels
    because the regions can be quite large and there’s typically not enough information
    in the surrounding regions and in the text to reconstruct the masked region correctly:
    it’s better to aim for a higher-level target.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For linguistic-visual alignment, the model takes the outputs of the [IMG] and
    [CLS] tokens, then computes their itemwise product and passes the result to a
    binary classification head that must predict whether the text and image match.
    Multiplication is preferred over addition because it amplifies features that are
    strong in both representations (a bit like a logical AND gate), so it better captures
    alignment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And that’s it. This model significantly beat the state of the art for several
    downstream tasks, including image grounding, caption-based image retrieval (even
    zero-shot), visual question answering, and *visual commonsense reasoning* (VCR)
    which involves answering a multiple-choice question about an image (like VQA),
    then selecting the appropriate justification. For example, given an image of a
    waiter serving some pancakes at a table, along with the question “Why is person
    #4 pointing at person #1”, the model must choose the correct answer “He is telling
    person #3 that person #1 ordered the pancakes”, then it must choose the justification
    “Person #3 is serving food and they might not know whose order is whose”.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ViLBERT had a strong influence on the field of multimodal machine learning
    thanks to its dual-stream architecture, the invention of co-attention, and its
    excellent results on many downstream tasks. It was another great demonstration
    of the power of large-scale self-supervised pretraining using transformers. The
    next major milestone came in 2021, and it approached the problem very differently,
    using contrastive pretraining: meet CLIP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'CLIP: A Dual-Encoder Text plus Image Model Trained with Contrastive Pretraining'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'OpenAI’s January 2021 release of [contrastive language–image pretraining (CLIP)](https://homl.info/clip)⁠^([25](ch16.html#id3870))
    was a major breakthrough, not just for its astounding capabilities, but also because
    of its surprisingly straightforward approach based on *contrastive learning*:
    the model learns to encode text and images into vector representations that are
    similar when the text and image match, and dissimilar when they don’t match.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once trained, the model can be used for many tasks, particularly zero-shot
    image classification. For example, CLIP can be used as an insect classifier without
    any additional training: just start by feeding all the possible class names to
    CLIP, such as “cricket”, “ladybug”, “spider”, and so on, to get one vector representation
    for each class name. Then, whenever you want to classify an image, feed it to
    CLIP to get a vector representation, and find the most similar class name representation
    using cosine similarity. This usually works even better if the text resembles
    typical image captions found on the web, since this is what CLIP was trained on,
    for example, “This is a photo of a ladybug” instead of just “ladybug”. A bit of
    prompt engineering can help (i.e., experimenting with various prompt templates).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The good news is that CLIP is fully open source,⁠^([26](ch16.html#id3873)),
    several pretrained models are available on the Hugging Face Hub, and the Transformers
    library provides a convenient pipeline for zero-shot image classification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we provided a prompt template, so the model will actually encode
    “This is a photo of a ladybug”, not just “ladybug” (if you don’t provide any template,
    the pipeline actually defaults to “This is a photo of a {}”.). Now let’s look
    at the results, which are sorted by score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Great! CLIP predicts ladybug with over 99.7% confidence. Now if you want a flower
    classifier instead, just replace the candidate labels with names of flowers. If
    you include “dandelion” in the list and classify the same image, the model should
    choose “dandelion” with high confidence (ignoring the ladybug). Impressive!
  prefs: []
  type: TYPE_NORMAL
- en: 'So how does this magic work? Well, CLIP’s architecture is based on a regular
    text encoder and a regular vision encoder, no co-attention or anything fancy (see
    [Figure 16-13](#clip_diagram)). You can actually use pretty much any text and
    vision encoders you want, as long as they can produce a vector representation
    of the text or image. The authors experimented with various encoders, including
    several ResNet and ViT models for vision, and a GPT-2-like model for text, all
    trained from scratch. What’s that I hear you say, GPT-2 is not an encoder? That’s
    true, it’s a decoder-only model, but we’re not pretraining it for next token prediction,
    so the last token’s output is free to be used as a representation of the entire
    input sequence, which is what CLIP does. You may wonder why we’re not using a
    regular text encoder like BERT? Well, we could, but OpenAI created GPT—Alex Radford
    is the lead author of both GPT and CLIP—so that’s most likely why GPT-2 was chosen:
    the authors simply had more experience with this model and a good training infrastructure
    already in place. Using a causal encoder also makes it possible to cache the intermediate
    state of the model when multiple texts start in the same way; for example, “This
    is a photo of a”.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the CLIP model with text and vision encoders processing
    image-caption pairs into vectors to match corresponding pairs and differentiate
    mismatched ones.](assets/hmls_1613.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16-13\. CLIP: a batch of image-caption pairs is encoded as vectors,
    then matching pairs are pulled closer while mismatched pairs are pushed away'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Also note that a pooling layer is added on top of the vision encoder to ensure
    it outputs a single vector for the whole image instead of feature maps. Moreover,
    a linear layer is added on top of each encoder to project the final representation
    into the same output space (i.e., with the same number of dimensions). So given
    a batch of *m* image-caption pairs, we get *m* vector representations for the
    images and *m* vector representations for the captions, and all vectors have the
    same number of dimensions. [Figure 16-13](#clip_diagram) shows *m* = 4, but the
    authors used a shockingly large batch size of *m* = 2^(15) = 32,768 during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model was then pretrained on a large dataset of 400 million image-caption
    pairs scraped from the internet, using a contrastive loss⁠^([27](ch16.html#id3874))
    that pulls together the representations of matching pairs, while also pushing
    apart representations of mismatched pairs. Here’s how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'All vectors are first ℓ[2] normalized, meaning they are rescaled to unit vectors:
    we only care about their orientation, not their length.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we compute the cosine similarity of the image representation and the text
    representation for every possible image-caption pair. The result is an *m* × *m*
    matrix containing numbers between –1 for opposite vectors, and +1 for identical
    vectors. In [Figure 16-13](#clip_diagram), this matrix is represented by the 4
    × 4 grid (black is +1, white is –1). Each column measures how much each image
    in the batch matches a given caption in the same batch, while each row measures
    how much each caption matches a given image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the *i*^(th) image corresponds to the *i*^(th) caption, we want the main
    diagonal of this matrix to contain similarity scores close to +1, while all other
    scores should be close to 0\. Why not close to –1? Well, if an image and a text
    are totally unrelated, we can think of their representations as two random vectors.
    Recall that two random high-dimensional vectors are highly likely to be close
    to orthogonal (as discussed in [Chapter 7](ch07.html#dimensionality_chapter)),
    so their cosine similarity will be close to 0, not –1\. In other words, it makes
    sense to assume that the text and image representations of a mismatched pair are
    unrelated (score close to 0), not opposite (score close to –1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the *i*^(th) row, we know that the matching caption is in the *i*^(th) column,
    so we want the model to produce a high similarity score in that column, and a
    low score elsewhere. This resembles a classification task where the target class
    is the *i*^(th) class. Indeed, we can treat each similarity score as class logit
    and simply compute the cross-entropy loss for that row with *i* as the target.
    We can follow the exact same rationale for each column. If we compute the cross-entropy
    loss for each row and each column (using class *i* as the target for the *i*^(th)
    row and the *i*^(th) column), and evaluate the mean, we get the final loss.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There’s just one extra technical detail: the similarity scores range between
    –1 and +1, which is unlikely to be the ideal logit scale for the task, so CLIP
    divides all the similarity scores by a trainable temperature (a scalar) before
    computing the loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This loss requires a large batch size to ensure the model sees enough negative
    examples to contrast with the positive examples, or else it could overfit details
    in the positive examples. CLIP’s success is due in part to the gigantic batch
    size that the authors were able to implement.
  prefs: []
  type: TYPE_NORMAL
- en: The authors evaluated CLIP on many image classification datasets, and for roughly
    60% of these, it performed better without any extra training (i.e., zero-shot)
    than a *linear probe* trained on ResNet-50 features (that’s a linear classifier
    trained on features output by a pretrained and frozen ResNet-50 model), including
    on ImageNet, despite the fact that the ResNet-50 model was actually pretrained
    on ImageNet. CLIP is particularly strong on datasets with few examples per class,
    with pictures of everyday scenes (i.e., the kind of pictures you find on the web).
    In fact, CLIP even beat the state of the art on the Stanford Cars dataset, ahead
    of the best ViTs specifically trained on this dataset, because pictures of cars
    are very common on the web and the dataset doesn’t have many examples per class.
    However, CLIP doesn’t perform as well on domain-specific images, such as satellite
    or medical images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Importantly, the visual features output by CLIP are also highly robust to perturbations,
    making them excellent for downstream tasks, such as image retrieval: if you store
    images in a vector database, indexing them by their CLIP-encoded visual features,
    you can then search for them given either a text query or an image query. For
    this, just run the query through CLIP to get a vector representation, then search
    the database for images with a similar representation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get the text and visual features using the Transformers library, you must
    run the CLIP model directly, without going through a pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you need to encode the images and text separately, you can use the CLIP model’s
    `get_image_features()` and `get_text_features()` methods. You must first tokenize
    the text using a `CLIPTokenizer` and process the images using a `CLIPImageProcessor`.
    The resulting features are not ℓ[2] normalized, so you must divide them by `features.norm(dim=1,
    keepdim=True)` (see the notebook for a code example).
  prefs: []
  type: TYPE_NORMAL
- en: 'The features are already ℓ[2] normalized, so if you want to compute similarity
    scores, a single matrix multiplication is all you need:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11] [PRE12]`py  [PRE13]py`` [PRE14]py It’s a good description of the photo,
    but it would nicer without the special tokens, so let’s get rid of them when decoding
    the model’s output:    [PRE15]py`` `...` [PRE16]` [PRE17] `` `Perfect!    ######
    Tip    Also check out InstructBLIP, a BLIP-2 model with vision-language instruction
    tuning.` `` [PRE18][PRE19][PRE20][PRE21][PRE22]  [PRE23]'
  prefs: []
  type: TYPE_NORMAL
