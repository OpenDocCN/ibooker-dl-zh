- en: Chapter 16\. Vision and Multimodal Transformers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第16章\. 视觉和多模态Transformer
- en: In the previous chapter, we implemented a transformer from scratch and turned
    it into a translation system, then we explored encoder-only models for NLU, decoder-only
    models for NLG, and we even built a little chatbot—that was quite a journey! Yet,
    there’s still a lot more to say about transformers. In particular, we have only
    dealt with text so far, but transformers actually turned out to be exceptionally
    good at processing all sorts of inputs. In this chapter we will cover *vision
    transformers* (ViTs), capable of processing images, followed by *multimodal transformers*,
    capable of handling multiple modalities, including text, images, audio, videos,
    robot sensors and actuators, and really any kind of data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们从零开始实现了一个Transformer，并将其转变为一个翻译系统，然后我们探索了仅编码器的NLU模型，仅解码器的NLG模型，甚至构建了一个小型的聊天机器人——这是一段相当漫长的旅程！然而，关于Transformer还有很多话要说。特别是，我们迄今为止只处理了文本，但Transformer实际上在处理各种输入方面表现出色。在本章中，我们将介绍*视觉Transformer*（ViTs），能够处理图像，随后是*多模态Transformer*，能够处理包括文本、图像、音频、视频、机器人传感器和执行器以及任何类型的数据在内的多种模态。
- en: 'In the first part of this chapter, we will discuss some of the most influential
    pure-vision transformers:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，我们将讨论一些最有影响力的纯视觉Transformer：
- en: DETR (Detection Transformer)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: DETR（检测Transformer）
- en: An early encoder-decoder transformer for object detection.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于目标检测的早期编码器-解码器Transformer。
- en: The original ViT (Vision Transformer)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 原始ViT（视觉Transformer）
- en: This landmark encoder-only transformer treats image patches like word tokens
    and reaches the state of the art if trained on a large dataset.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这个标志性的仅编码器Transformer将图像块视为词标记，如果在大数据集上训练，可以达到最先进的状态。
- en: DeiT (Data-Efficient Image Transformer)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: DeiT（数据高效图像Transformer）
- en: A more data-efficient ViT trained at scale using distillation.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 使用蒸馏在大规模上训练的更具数据效率的ViT。
- en: PVT (Pyramid Vision Transformer)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: PVT（金字塔视觉Transformer）
- en: A hierarchical model that can produce multiscale feature maps for semantic segmentation
    and other dense prediction tasks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可以生成多尺度特征图以进行语义分割和其他密集预测任务的层次模型。
- en: Swin Transformer (Shifted Windows Transformer)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Swin Transformer（移位窗口Transformer）
- en: A much faster hierarchical model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 一个速度更快的层次模型。
- en: DINO (self-Distillation with NO labels)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: DINO（无标签的自蒸馏）
- en: This introduced a novel self-supervised technique for visual representation
    learning.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这引入了一种新颖的自监督视觉表示学习方法。
- en: 'In the second part of this chapter, we will dive into multimodal transformers:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，我们将深入探讨多模态Transformer：
- en: VideoBERT
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: VideoBERT
- en: A BERT model trained to process both text and video tokens.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个训练以处理文本和视频标记的BERT模型。
- en: ViLBERT (Visio-Linguistic BERT)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ViLBERT（视觉语言BERT）
- en: A dual-encoder model for image plus text, which introduced co-attention (i.e.,
    two-way cross-attention).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一种用于图像加文本的双编码器模型，它引入了共注意力（即双向交叉注意力）。
- en: CLIP (Contrastive Language–Image Pretraining)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: CLIP（对比语言-图像预训练）
- en: This is another image plus text dual-encoder model trained using contrastive
    pretraining.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这又是一个使用对比预训练训练的图像加文本双编码器模型。
- en: DALL·E (a pun on the names of the artist Salvador Dali and the Pixar character
    Wall-E)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: DALL·E（萨尔瓦多·达利和皮克斯角色瓦力名字的双关语）
- en: A model capable of generating images from text prompts.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一个能够从文本提示生成图像的模型。
- en: Perceiver
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Perceiver
- en: This efficiently compresses any high-resolution modality into a short sequence
    using a cross-attention trick.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这通过交叉注意力技巧有效地将任何高分辨率模态压缩成短序列。
- en: Perceiver IO (Input/Output)
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Perceiver IO（输入/输出）
- en: Adds a flexible output mechanism to the Perceiver, using a similar cross-attention
    trick.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 向Perceiver添加了一个灵活的输出机制，使用类似的交叉注意力技巧。
- en: Flamingo
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Flamingo
- en: Rather than starting from scratch, it reuses two large pretrained models—one
    for vision and one for language (both frozen)—and connects them using a Perceiver-style
    adapter named a Resampler. This architecture enables open-ended visual dialogue.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是从头开始，它重用了两个大型预训练模型——一个用于视觉，一个用于语言（两者都冻结）——并通过一个名为Resampler的Perceiver风格适配器将它们连接起来。这种架构使得开放式视觉对话成为可能。
- en: BLIP-2 (Bootstrapping Language-Image Pretraining)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: BLIP-2（自举语言-图像预训练）
- en: This is another open-ended visual dialogue model that reuses two large pretrained
    models, connects them using a lightweight querying transformer (Q-Former), and
    uses a powerful two-stage training approach with multiple training objectives.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个另一个开放式视觉对话模型，它重用了两个大型预训练模型，使用轻量级的查询Transformer（Q-Former）将它们连接起来，并使用具有多个训练目标的有力两阶段训练方法。
- en: So turn on the lights, transformers are about to open their eyes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，打开灯吧，Transformer即将睁开眼睛。
- en: Vision Transformers
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉Transformer
- en: 'Vision transformers didn’t pop out of a vacuum: before they were invented,
    there were RNNs with visual attention, and hybrid CNN-Transformer models. Let’s
    take a look at these ViT ancestors before we dive into some of the most influential
    ViTs.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉Transformer并非凭空出现：在它们被发明之前，已经有了带有视觉注意力的RNN和混合CNN-Transformer模型。在我们深入研究一些最有影响力的ViT之前，让我们看看这些ViT的祖先。
- en: RNNs with Visual Attention
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 带有视觉注意力的RNN
- en: One of the first applications of attention mechanisms beyond NLP was to generate
    image captions using [visual attention](https://homl.info/visualattention).⁠^([1](ch16.html#id3722))
    Here a convolutional neural network first processes the image and outputs some
    feature maps, then a decoder RNN equipped with an attention mechanism generates
    the caption, one token at a time.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制在NLP之外的第一项应用之一是使用[视觉注意力](https://homl.info/visualattention)生成图像字幕。⁠^([1](ch16.html#id3722))
    在这里，卷积神经网络首先处理图像并输出一些特征图，然后一个配备注意力机制的解码器RNN逐个生成字幕。
- en: 'The decoder uses an attention layer at each decoding step to focus on just
    the right part of the image. For example, in [Figure 16-1](#visual_attention_diagram),
    the model generated the caption “A woman is throwing a Frisbee in a park”, and
    you can see what part of the input image the decoder focused its attention on
    when it was about to output the word “Frisbee”: clearly, most of its attention
    was focused on the Frisbee.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器在每个解码步骤中使用注意力层来专注于图像的恰当部分。例如，在[图16-1](#visual_attention_diagram)中，模型生成了字幕“一位女士在公园里扔飞盘”，你可以看到当解码器即将输出“飞盘”这个词时，它关注了输入图像的哪个部分：很明显，大部分注意力都集中在飞盘上。
- en: '![A woman in green throws a red Frisbee in a park, and the attention layer
    highlights the Frisbee when producing the word "Frisbee" in a caption.](assets/hmls_1601.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![一位穿绿色衣服的女士在公园里扔一个红色的飞盘，当在字幕中产生“飞盘”这个词时，注意力层突出显示了飞盘。](assets/hmls_1601.png)'
- en: 'Figure 16-1\. Visual attention: an input image (left) and the model’s focus
    before producing the word “Frisbee” (right)⁠^([2](ch16.html#id3724))'
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-1\. 视觉注意力：输入图像（左）和模型在输出“飞盘”这个词之前关注的模型焦点（右）⁠^([2](ch16.html#id3724))
- en: Once transformers were invented, they were quickly applied to visual tasks,
    generally by replacing RNNs in existing architectures (e.g., for image captioning).
    However, the bulk of the visual work was still performed by a CNN, so although
    they were transformers used for visual tasks, we usually don’t consider them as
    ViTs. The *detection transformer* (DETR) is a good example of this.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦发明了Transformer，它们很快就被应用于视觉任务，通常是通过替换现有架构中的RNN（例如，用于图像字幕）。然而，大部分视觉工作仍然由CNN完成，因此尽管它们是用于视觉任务的Transformer，我们通常不把它们视为ViT。*检测Transformer*（DETR）是这种情况的一个很好的例子。
- en: 'DETR: A CNN-Transformer Hybrid for Object Detection'
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DETR：用于目标检测的CNN-Transformer混合架构
- en: In May 2020, a team of Facebook researchers proposed a hybrid CNN–transformer
    architecture for object detection, named [*detection transformer*](https://homl.info/detr)
    (DETR, see [Figure 16-2](#detr_diagram)).⁠^([4](ch16.html#id3740)) The CNN first
    processes the input images and outputs a set of feature maps, then these feature
    maps are turned into a sequence of visual tokens that are fed to an encoder-decoder
    transformer, and finally the transformer outputs a sequence of bounding box predictions.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年5月，一支Facebook研究团队提出了一种混合CNN-Transformer架构，用于目标检测，命名为[*检测Transformer*](https://homl.info/detr)（DETR，见[图16-2](#detr_diagram)）。⁠^([4](ch16.html#id3740))
    CNN首先处理输入图像并输出一组特征图，然后这些特征图被转换成一系列视觉标记，这些标记被输入到编码器-解码器Transformer中，最后Transformer输出一系列边界框预测。
- en: 'At one point, someone was bound to wonder whether we could get rid of the CNN
    entirely. After all, attention is all you need, right? This happened a few months
    after DETR: the original ViT was born.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个时候，肯定有人会想知道我们是否可以完全去掉CNN。毕竟，注意力就是一切，对吧？在DETR几个月后，原始的ViT诞生了。
- en: '![Diagram of the detection transformer (DETR) process illustrating how visual
    tokens and positional encoding are used to identify objects like a temple and
    a tree with confidence levels.](assets/hmls_1602.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![检测Transformer（DETR）过程图解，展示了如何使用视觉标记和位置编码来以置信度识别寺庙和树木等物体。](assets/hmls_1602.png)'
- en: Figure 16-2\. The detection transformer (DETR) for object detection
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-2\. 用于目标检测的检测Transformer（DETR）
- en: The Original ViT
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 原始ViT
- en: 'In October 2020, a team of Google researchers released [a paper](https://homl.info/vit)⁠^([5](ch16.html#id3741))
    that introduced the first vision transformer without a CNN (see [Figure 16-3](#vit_diagram)).
    It was simply named the *vision transformer* (ViT). The idea is surprisingly simple:
    chop the image into little 16 × 16 patches, and treat the sequence of patches
    as if it is a sequence of word representations. In fact, the paper’s title is
    “An Image Is Worth 16 × 16 Words”.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 2020年10月，一组谷歌研究人员发布了一篇[论文](https://homl.info/vit)⁠^([5](ch16.html#id3741))，介绍了第一个没有CNN的视觉转换器（见[图16-3](#vit_diagram)）。它简单地被命名为*视觉转换器*（ViT）。这个想法非常简单：将图像切割成小的16
    × 16补丁，并将补丁序列视为一个单词表示序列。实际上，论文的标题是“一张图片胜过16 × 16个单词”。
- en: 'To be more precise, the patches are first flattened into 16 × 16 × 3 = 768-dimensional
    vectors (the 3 is for the RGB color channels). For example, a 224 × 224 image
    gets chopped into 14 × 14 = 196 patches, so we get 196 vectors of 768 dimensions
    each. These vectors then go through a linear layer that projects the vectors to
    the transformer’s embedding size. The resulting sequence of vectors can then be
    treated just like a sequence of word embeddings: add learnable positional embeddings,
    and pass the result to the transformer, which is a regular encoder-only model.
    A class token with a trainable representation is inserted at the start of the
    sequence, and a classification head is added on top of the corresponding output
    (i.e., this is BERT-style classification).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 更精确地说，这些补丁首先被展平成16 × 16 × 3 = 768维度的向量（3代表RGB颜色通道）。例如，一个224 × 224的图像被切割成14 ×
    14 = 196个补丁，所以我们得到196个768维度的向量。这些向量随后通过一个线性层，将向量投影到转换器的嵌入大小。得到的向量序列可以像处理单词嵌入序列一样处理：添加可学习的位置嵌入，然后将结果传递给转换器，这是一个常规的仅编码器模型。在序列的开始插入一个具有可训练表示的类别标记，并在相应的输出上方添加一个分类头（即这是BERT风格的分类）。
- en: And that’s it! This model beat the state of the art on ImageNet image classification,
    but to be fair the authors had to use over 300 million additional images for training.
    This makes sense since transformers don’t have as many *inductive biases* as convolution
    neural nets, so they need extra data just to learn things that CNNs implicitly
    assume.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！这个模型在ImageNet图像分类上击败了最先进的技术，但公平地说，作者们不得不使用超过3亿张额外的图像进行训练。这很有道理，因为与卷积神经网络相比，转换器没有那么多*归纳偏差*，所以它们需要额外的数据来学习CNN隐含假设的事情。
- en: Note
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: An inductive bias is an implicit assumption made by the model, due to its architecture.
    For example, linear models implicitly assume that the data is, well, linear. CNNs
    are translation invariant, so they implicitly assume that patterns learned in
    one location will likely be useful in other locations as well. They also have
    a strong bias toward locality. RNNs implicitly assume that the inputs are ordered,
    and that recent tokens are more important than older ones. The more inductive
    biases a model has, assuming they are correct, the less training data the model
    will require. But if the implicit assumptions are wrong, then the model may perform
    poorly even if it is trained on a large dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 归纳偏差是模型由于其架构而做出的隐含假设。例如，线性模型隐含地假设数据是线性的。CNN具有平移不变性，因此它们隐含地假设在一个位置学习到的模式在其他位置也可能有用。它们还强烈倾向于局部性。RNN隐含地假设输入是有序的，并且最近的标记比旧的标记更重要。一个模型具有的归纳偏差越多，假设它们是正确的，那么模型所需的训练数据就越少。但如果隐含的假设是错误的，那么即使在大数据集上训练，模型也可能表现不佳。
- en: '![Diagram illustrating the Vision Transformer (ViT) model, showing the process
    of converting image patches into token embeddings for classification.](assets/hmls_1603.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![说明视觉转换器（ViT）模型图，展示了将图像补丁转换为分类标记嵌入的过程。](assets/hmls_1603.png)'
- en: Figure 16-3\. Vision transformer (ViT) for classification
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-3\. 用于分类的视觉转换器（ViT）
- en: Now you know everything you need to implement a ViT from scratch!
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道从头实现ViT所需的一切了！
- en: Implementing a ViT from scratch using PyTorch
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PyTorch从头实现ViT
- en: We will start by implementing a custom module to take care of patch embedding.
    For this, we can actually use an `nn.Conv2d` module with `kernel_size` and `stride`
    both set to the patch size (16). This is equivalent to chopping the image into
    patches, flattening them, and passing them through a linear layer (then reshaping
    the result). Just what we need!
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先实现一个自定义模块来处理补丁嵌入。为此，我们可以实际使用一个`nn.Conv2d`模块，将`kernel_size`和`stride`都设置为补丁大小（16）。这相当于将图像切割成补丁，将它们展平，并通过一个线性层（然后重塑结果）。这正是我们所需要的！
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After the convolutional layer, we must flatten the spatial dimensions and transpose
    the last two dimensions to ensure the embedding dimension ends up last, which
    is what the `nn.TransformerEncoder` module expects. Now we’re ready to implement
    our ViT model:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积层之后，我们必须展平空间维度并转置最后两个维度，以确保嵌入维度最终位于最后，这是`nn.TransformerEncoder`模块所期望的。现在我们已经准备好实现我们的ViT模型：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s go through this code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这段代码：
- en: The constructor starts by creating the `PatchEmbedding` module.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数首先创建`PatchEmbedding`模块。
- en: Then it creates the class token’s trainable embedding, initialized using a normal
    distribution with a small standard deviation (0.02 is common). Its shape is [1,
    1, *E*], where *E* is the embedding dimension.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后它创建类别标记的可训练嵌入，使用具有小标准差（0.02是常见的）的正态分布进行初始化。其形状为[1, 1, *E*]，其中*E*是嵌入维度。
- en: Next, we initialize the positional embeddings, of shape [1, 1 + *L*, *E*], where
    *L* is the number of patch tokens. We need one more positional embedding for the
    class token, hence the 1 + *L*. Again, we initialize it using a normal distribution
    with a small standard deviation.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们初始化位置嵌入，形状为[1, 1 + *L*, *E*]，其中*L*是补丁标记的数量。我们需要一个额外的位置嵌入用于类别标记，因此是1 +
    *L*。同样，我们使用具有小标准差的正态分布来初始化它。
- en: 'Next, we create the other modules: `nn.Dropout`, `nn.TransformerEncoder` (based
    on an `nn.TransformerEncoderLayer`), `nn.LayerNorm`, and the output linear layer
    that we will use as a classification head.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们创建其他模块：`nn.Dropout`，`nn.TransformerEncoder`（基于`nn.TransformerEncoderLayer`），`nn.LayerNorm`，以及我们将用作分类头的输出线性层。
- en: In the `forward()` method, we start by creating the patch tokens.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`forward()`方法中，我们首先创建补丁标记。
- en: Then we replicate the class token along the batch axis, using the `expand()`
    method, and we concatenate the patch tokens. This ensures that each sequence of
    patch tokens starts with the class token.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们使用`expand()`方法在批次轴上复制类别标记，并将补丁标记连接起来。这确保了每个补丁标记序列都以类别标记开头。
- en: 'The rest is straightforward: we add the positional embeddings, apply some dropout,
    run the encoder, keep only the class token’s output (`Z[:, 0]`) and normalize
    it, and lastly pass it through the output layer, which produces the logits.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其余部分都很直接：我们添加位置嵌入，应用一些dropout，运行编码器，只保留类别标记的输出（`Z[:, 0]`）并对其进行归一化，最后通过输出层，该层产生logits。
- en: 'You can create the model and test it with a random batch of images, like this:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以创建模型并使用随机批次图像进行测试，如下所示：
- en: '[PRE2]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can then train this model using the `nn.CrossEntropyLoss`, as usual. This
    would take quite a while, however, so unless your image dataset is very domain-specific,
    you’re usually better off downloading a pretrained ViT using the Transformers
    library and then fine-tuning it on your dataset. Let’s see how.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`nn.CrossEntropyLoss`来训练这个模型，就像通常一样。然而，这需要相当长的时间，除非您的图像数据集非常特定于领域，否则您通常最好下载一个使用Transformers库预训练的ViT，然后在您的数据集上微调它。让我们看看如何。
- en: Fine-tuning a pretrained ViT using the Transformers library
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Transformers库微调预训练的ViT
- en: 'Let’s download a small pretrained ViT and fine-tune it on the Oxford-IIIT Pet
    dataset, which contains over 7,000 pictures of pets grouped into 37 different
    classes. First, let’s download the dataset:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们下载一个小型预训练的ViT，并在包含超过7000张宠物图片的Oxford-IIIT Pet数据集上对其进行微调，这些图片被分为37个不同的类别。首先，让我们下载这个数据集：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, let’s download the ViT:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们下载ViT：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We’re loading a base ViT model that was pretrained on the ImageNet-21k dataset.
    This dataset contains roughly 14 million images across over 21,800 classes. We’re
    using the `ViTForImageClassification` class which automatically replaces the original
    classification head with a new one (untrained) for the desired number of classes.
    That’s the part we now need to train.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在加载一个在ImageNet-21k数据集上预训练的基础ViT模型。这个数据集包含大约1400万张图片，跨越21800多个类别。我们使用`ViTForImageClassification`类，该类会自动用新的（未训练的）分类头替换原始分类头，以适应所需的类别数量。这就是我们现在需要训练的部分。
- en: 'We also loaded the image processor for this model. We will use it to preprocess
    each image as the model expects: it will be rescaled to 224 × 224, pixel values
    will be normalized to range between –1 and 1, and the channel dimension will be
    moved in front of the spatial dimensions. We also set `use_fast=True` because
    a fast implementation of the image processor is available, so we might as well
    use it. The processor takes an image as input and returns a dictionary containing
    a “pixel_values” entry equal to the preprocessed image.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还为这个模型加载了图像处理器。我们将使用它来预处理每张图像，正如模型所期望的那样：它将被缩放到 224 × 224，像素值将被归一化到 -1 和 1
    之间，通道维度将被移动到空间维度之前。我们还设置了 `use_fast=True`，因为有一个快速的图像处理器实现可用，所以我们不妨使用它。处理器接收一个图像作为输入，并返回一个包含“pixel_values”条目的字典，该条目等于预处理后的图像。
- en: 'Next, we need a data collator that will preprocess all the images in a batch
    and return the images and labels as PyTorch tensors:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个数据合并器，它将预处理批处理中的所有图像，并将图像和标签作为 PyTorch 张量返回：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We set `do_convert_rgb=True` because the model expects RGB images, but some
    images in the dataset are RGBA (i.e., they have an extra transparency channel),
    so we must force the conversion to RGB to avoid an error in the middle of training.
    And now we’re ready to train our model using the familiar Hugging Face training
    API:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置了 `do_convert_rgb=True`，因为模型期望 RGB 图像，但数据集中的一些图像是 RGBA（即，它们有一个额外的透明度通道），因此我们必须强制转换为
    RGB 以避免训练过程中出现错误。现在我们已准备好使用熟悉的 Hugging Face 训练 API 来训练我们的模型：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Warning
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'By default, the trainer will automatically remove input attributes that are
    not used by the `forward()` method: our model expects `pixel_values` and optionally
    `labels`, but anything else will be dropped, including the `"image"` attribute.
    Since the unused attributes are dropped before the `collate_fn()` function is
    called, the code `example["image"]` will cause an error. This is why we must set
    `remove_unused_columns=False`.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，训练器将自动删除 `forward()` 方法未使用的输入属性：我们的模型期望 `pixel_values` 和可选的 `labels`，但任何其他内容都将被丢弃，包括
    `"image"` 属性。由于未使用的属性在调用 `collate_fn()` 函数之前被删除，因此 `example["image"]` 代码将导致错误。这就是为什么我们必须设置
    `remove_unused_columns=False`。
- en: 'After just 3 epochs, our ViT model reaches about 91.8% accuracy. With some
    data augmentation and more training, you could reach 93 to 95% accuracy, which
    is close to the state of the art. Great! But we’re just getting started: ViTs
    have been improved in many ways since 2020\. In particular, it’s possible to train
    them from scratch in a much more efficient way using distillation. Let’s see how.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 仅经过 3 个训练周期后，我们的 ViT 模型就达到了约 91.8% 的准确率。通过一些数据增强和更多的训练，你可能会达到 93% 到 95% 的准确率，这接近了当前的最佳水平。太棒了！但我们才刚刚开始：自
    2020 年以来，ViT 在许多方面都得到了改进。特别是，现在可以使用蒸馏技术以更有效的方式从头开始训练它们。让我们看看如何。
- en: Data-Efficient Image Transformer
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据高效图像变换器
- en: Just two months after Google’s ViT paper was published, a team of Facebook researchers
    released [*data-efficient image transformers*](https://homl.info/deit) (DeiT).⁠^([6](ch16.html#id3760))
    Their DeiT model achieved competitive results on ImageNet without requiring any
    additional data for training. The model’s architecture is virtually the same as
    the original ViT (see [Figure 16-4](#deit_diagram)), but the authors used a distillation
    technique to transfer knowledge from a teacher model to their student ViT model
    (distillation was introduced in [Chapter 15](ch15.html#transformer_chapter)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google 的 ViT 论文发布后的仅仅两个月，Facebook 研究人员团队发布了[*数据高效图像变换器*](https://homl.info/deit)（DeiT）。⁠^([6](ch16.html#id3760))
    他们的 DeiT 模型在 ImageNet 上取得了有竞争力的结果，而无需为训练提供任何额外的数据。该模型的架构几乎与原始 ViT 相同（参见 [图 16-4](#deit_diagram)），但作者使用了一种蒸馏技术将知识从教师模型传递到他们的学生
    ViT 模型（蒸馏在 [第 15 章](ch15.html#transformer_chapter) 中介绍）。
- en: The authors used a frozen, state-of-the-art CNN as the teacher model. During
    training, they added a special distillation token to the student ViT model. Just
    like the class token, the distillation token representation is trainable, and
    its output goes through a dedicated classification head. Both classification heads
    (for the class token and for the distillation token) are trained simultaneously,
    both using the cross-entropy loss, but the class token’s classification head is
    trained using the normal hard targets (i.e., one-hot vectors), while the distillation
    head is trained using soft targets output by a teacher model. The final loss is
    a weighted sum of both classification losses (typically with equal weights). At
    inference time, the distillation token is dropped, along with its classification
    head. And that’s all there is to it! If you fine-tune a DeiT model on the same
    pets dataset, using `model_id = "facebook/deit-base-distilled-patch16-224"` and
    `DeiTForImageClassification`, you should get around 94.4% validation accuracy
    after just three epochs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了一个冻结的、最先进的CNN作为教师模型。在训练过程中，他们向学生ViT模型添加了一个特殊的蒸馏标记。就像类别标记一样，蒸馏标记的表示是可训练的，其输出经过一个专门的分类头。两个分类头（对于类别标记和蒸馏标记）同时训练，都使用交叉熵损失，但类别标记的分类头使用正常硬目标（即one-hot向量）进行训练，而蒸馏头使用教师模型输出的软目标进行训练。最终的损失是两个分类损失的加权总和（通常权重相等）。在推理时间，蒸馏标记及其分类头被丢弃。这就是全部！如果你在相同的宠物数据集上微调一个DeiT模型，使用`model_id
    = "facebook/deit-base-distilled-patch16-224"`和`DeiTForImageClassification`，你只需三个epoch就应该能达到大约94.4%的验证准确率。
- en: '![Diagram illustrating the data-efficient image transformer (DeiT) process,
    showing the flow from image patches through linear transformation, encoding with
    position and class tokens, distillation token inclusion, to classifier heads with
    hard and soft target outputs from a CNN.](assets/hmls_1604.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![说明数据高效图像变换器（DeiT）过程的图解，展示了从图像块通过线性变换、编码位置和类别标记、包含蒸馏标记，到具有CNN硬和软目标输出的分类头的流程。](assets/hmls_1604.png)'
- en: Figure 16-4\. Data-efficient image transformer (DeiT) = ViT + distillation
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-4\. 数据高效图像变换器（DeiT）= ViT + 知识蒸馏
- en: So far, we have only used ViTs for classification tasks, but what about dense
    prediction tasks such as object detection or semantic segmentation (introduced
    in [Chapter 12](ch12.html#cnn_chapter))? For this, the ViT architecture needs
    to be tweaked a bit; welcome to hierarchical vision transformers.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只使用了ViTs进行分类任务，那么对于密集预测任务，如目标检测或语义分割（在第12章中介绍）怎么办呢？为此，ViT架构需要稍作调整；欢迎来到层次视觉变换器。
- en: Pyramid Vision Transformer for Dense Prediction Tasks
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于密集预测任务的金字塔视觉变换器
- en: 'The year 2021 was a year of plenty for ViTs: new models advanced the state
    of the art almost every other week. An important milestone was the release of
    the [Pyramid Vision Transformer (PVT)](https://homl.info/pvt) in February 2021,⁠^([7](ch16.html#id3765))
    developed by a team of researchers from Nanjing University, HKU, IIAI, and SenseTime
    Research. They pointed out that the original ViT architecture was good at classification
    tasks, but not so much at dense prediction tasks, where fine-grained resolution
    is needed. To solve this issue, they proposed a pyramidal architecture in which
    the image is processed into a gradually smaller but deeper image (i.e., semantically
    richer), much like in a CNN. [Figure 16-5](#pvt_diagram) shows how a 256 × 192
    image with 3 channels (RGB) is first turned into a 64 × 48 image with 64 channels,
    then into a 32 × 24 image with 128 channels, then a 16 × 12 image with 320 channels,
    and lastly an 8 × 6 image with 512 channels.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年是ViTs的一年：新的模型几乎每隔一周就推进了当前的最佳水平。一个重要的里程碑是2021年2月发布的[金字塔视觉变换器（PVT）](https://homl.info/pvt)，由南京大学、HKU、IIAI和SenseTime
    Research的研究团队开发。他们指出，原始的ViT架构在分类任务上表现良好，但在需要精细分辨率的密集预测任务上表现不佳。为了解决这个问题，他们提出了一种金字塔架构，其中图像被处理成逐渐变小但更深（即语义更丰富）的图像，这与CNN非常相似。[图16-5](#pvt_diagram)展示了如何将一个256
    × 192像素、3通道（RGB）的图像首先转换成一个64 × 48像素、64通道的图像，然后转换成一个32 × 24像素、128通道的图像，接着是一个16
    × 12像素、320通道的图像，最后是一个8 × 6像素、512通道的图像。
- en: '![Diagram illustrating the Pyramid Vision Transformer process, showing how
    an image is transformed through multiple stages with increasing channels and reduced
    spatial resolution, ultimately used for dense prediction tasks.](assets/hmls_1605.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图解金字塔视觉变换器过程，展示了图像如何通过多个阶段进行转换，通道数增加，空间分辨率降低，最终用于密集预测任务。](assets/hmls_1605.png)'
- en: Figure 16-5\. Pyramid Vision Transformer for dense prediction tasks
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 16-5. 用于密集预测任务的金字塔视觉变换器
- en: At each pyramid level, the input image is processed very much like in a regular
    ViT. It is first chopped into patches and turned into a sequence of patch tokens,
    then trainable positional embeddings are added, and the resulting tokens are passed
    through an encoder-only transformer, composed of multiple encoder layers.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在金字塔的每个级别上，输入图像的处理方式与常规 ViT 非常相似。它首先被切割成补丁，并转换成一个补丁标记的序列，然后添加可训练的位置嵌入，最终标记通过一个仅编码器的变压器传递，由多个编码器层组成。
- en: Since the encoder outputs a sequence of vectors (i.e., contextualized embeddings),
    this sequence must be reshaped into a *grid* of vectors, which can then be treated
    as an image (with many channels) and passed on to the next level of the pyramid.
    For example, the encoder at the first level receives a sequence of 3,072 patch
    tokens, since the image was chopped into a 64 × 48 grid of 4 × 4 patches (and
    64 × 48 = 3,072). Each patch token is represented as a 64-dimensional vector.
    The encoder also outputs 3,072 vectors (i.e., contextualized embeddings), each
    64-dimensional, and they are organized into a 64 × 48 grid once again. This gives
    us a 64 × 48 image with 64 channels, which can be passed on to the next level.
    In levels 2, 3, and 4 of the pyramid, the patch tokens are 128-, 320-, and 512-dimensional,
    respectively.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于编码器输出一系列向量（即，上下文化的嵌入），这个序列必须被重塑为一个 *网格* 的向量，然后可以将其视为一个图像（具有许多通道）并传递到金字塔的下一级。例如，第一层的编码器接收一个包含
    3,072 个补丁标记的序列，因为图像被切割成一个 64 × 48 的 4 × 4 补丁网格（64 × 48 = 3,072）。每个补丁标记表示为一个 64
    维向量。编码器还输出 3,072 个向量（即，上下文化的嵌入），每个 64 维，并且它们再次组织成一个 64 × 48 的网格。这给我们提供了一个 64 ×
    48 的图像，具有 64 个通道，可以传递到下一级。在金字塔的第二、三、四级中，补丁标记分别是 128 维、320 维和 512 维。
- en: 'Importantly, the patches are much smaller than in the original ViT: instead
    of 16 × 16, they are just 4 × 4 at level 1, and 2 × 2 at levels 2, 3, and 4\.
    These tiny patches offer a much higher spatial resolution, which is crucial for
    dense prediction tasks. However, this comes at a cost: smaller patches means many
    more of them, and since multi-head attention has quadratic complexity, a naive
    adaptation of ViT would require vastly more computation. This is why the PVT authors
    introduced *spatial reduction attention* (SRA): it’s just like MHA except that
    the keys and values are first spatially reduced (but not the queries). For this,
    the authors proposed a sequence of operations that is usually implemented as a
    strided convolutional layer, followed by layer norm (although some implementations
    use an average pooling layer instead).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，这些补丁比原始的 ViT 小得多：在第一层，它们只是 4 × 4，而在第二、三、四层则是 2 × 2。这些微小的补丁提供了更高的空间分辨率，这对于密集预测任务至关重要。然而，这也带来了代价：较小的补丁意味着需要更多的它们，而且由于多头注意力的二次复杂性，对
    ViT 的简单适配将需要大量的计算。这就是为什么 PVT 作者引入了 *空间减少注意力*（SRA）：它就像 MHA 一样，只是键和值首先在空间上进行了减少（但不是查询）。为此，作者提出了一系列操作，通常实现为一个步长卷积层，随后是层归一化（尽管一些实现使用平均池化层代替）。
- en: 'Let’s look at the impact of SRA at the first level of the pyramid. There are
    3,072 patch tokens. In regular MHA, each of these tokens would attend to every
    token, so we would have to compute 3,072² attention scores: that’s over 9 million
    scores! In SRA, the query is unchanged so it still involves 3,072 tokens, but
    the keys and values are reduced spatially by a factor of 8, both horizontally
    and vertically (in levels 2, 3, and 4 of the pyramid, the reduction factor is
    4, 2, and 1, respectively). So instead of 3,072 tokens representing a 64 × 48
    grid, the keys and values are only composed of 48 tokens representing an 8 × 6
    grid (because 64 / 8 = 8 and 48 / 8 = 6). So we only need to compute 3,072 × 48
    = 147,456 attention scores: that’s 64 times less computationally expensive. And
    the good news is that this doesn’t affect the output resolution since we didn’t
    reduce the query at all: the encoder still output 3,072 tokens, representing a
    64 × 48 image.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 SRA 在金字塔第一层的影响。这里有 3,072 个 patch tokens。在常规的 MHA 中，每个这些 tokens 都会关注到每一个
    tokens，因此我们需要计算 3,072² 个注意力分数：这超过 900 万个分数！在 SRA 中，查询保持不变，因此仍然涉及 3,072 个 tokens，但键和值在空间上减少了
    8 倍，水平和垂直方向上都是（在金字塔的 2、3 和 4 层，减少因子分别是 4、2 和 1）。因此，不再是代表 64 × 48 网格的 3,072 个 tokens，键和值仅由
    48 个 tokens 组成，代表一个 8 × 6 的网格（因为 64 / 8 = 8 和 48 / 8 = 6）。因此，我们只需要计算 3,072 × 48
    = 147,456 个注意力分数：这比之前的计算成本降低了 64 倍。好消息是，这不会影响输出分辨率，因为我们根本就没有减少查询：编码器仍然输出 3,072
    个 tokens，代表一个 64 × 48 的图像。
- en: 'OK, so the PVT model takes an image and outputs four gradually smaller and
    deeper images. Now what? How do we use these multiscale feature maps to implement
    object detection or other dense prediction tasks? Well, no need to reinvent the
    wheel: existing solutions generally involve a CNN backbone that produces multiscale
    feature maps, so we can simply swap out this backbone for a PVT (often pretrained
    on ImageNet). For example, we can use an FCN approach for semantic segmentation
    (introduced at the end of [Chapter 12](ch12.html#cnn_chapter)) by upscaling and
    combining the multiscale feature maps output by the PVT, and add a final classification
    head to output one class per pixel. Similarly, we can use a Mask R-CNN for object
    detection and instance segmentation, replacing its CNN backbone with a PVT.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以 PVT 模型接收一个图像并输出四个逐渐缩小和深入处理的图像。那么接下来呢？我们如何使用这些多尺度特征图来实现目标检测或其他密集预测任务呢？嗯，没有必要重新发明轮子：现有的解决方案通常涉及一个生成多尺度特征图的
    CNN 主干网络，因此我们可以简单地将其替换为 PVT（通常在 ImageNet 上预训练）。例如，我们可以使用全卷积网络（FCN）方法进行语义分割（在第
    12 章末尾介绍），通过上采样和组合 PVT 输出的多尺度特征图，并添加一个最终的分类头以输出每个像素的一个类别。同样，我们可以使用 Mask R-CNN
    进行目标检测和实例分割，用 PVT 替换其 CNN 主干网络。
- en: In short, the PVT’s hierarchical structure was a big milestone for vision transformers,
    but despite spatial reduction attention, it’s still computationally expensive.
    The Swin Transformer, released one month later, is much more scalable. Let’s see
    why.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，PVT 的层次结构是视觉 Transformer 的一大里程碑，尽管有空间减少注意力，但它仍然计算成本高昂。一个月后发布的 Swin Transformer，其可扩展性要高得多。让我们看看原因。
- en: 'The Swin Transformer: A Fast and Versatile ViT'
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Swin Transformer：快速且通用的 ViT
- en: 'In March 2021, a team of Microsoft researchers released the [Swin Transformer](https://homl.info/swin).⁠^([8](ch16.html#id3776))
    Just like PVT, it has a hierarchical structure, producing multiscale feature maps
    which can be used for dense prediction tasks. But Swin uses a very different variant
    of multi-head attention: each patch only attends to patches located within the
    same window. This is called *window-based multi-head self-attention* (W-MSA),
    and it allows the cost of self-attention to scale linearly with the image size
    (meaning its area), instead of quadratically.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 2021 年 3 月，一支微软研究团队发布了 [Swin Transformer](https://homl.info/swin)。⁠^([8](ch16.html#id3776))
    就像 PVT 一样，它具有层次结构，生成可用于密集预测任务的多尺度特征图。但 Swin 使用了一种非常不同的多头注意力变体：每个 patch 只关注同一窗口内的其他
    patch。这被称为 *基于窗口的多头自注意力*（W-MSA），它允许自注意力的成本与图像大小（即面积）线性缩放，而不是平方缩放。
- en: For example, on the lefthand side of [Figure 16-6](#swin_diagram), the image
    is chopped into a 28 × 28 grid of patches, and these patches are grouped into
    nonoverlapping windows. At the first level of the Swin pyramid, the patches are
    usually 4 × 4 pixels, and each window contains a 7 × 7 grid of patches. So there’s
    a total of 784 patch tokens (28 × 28), but each token only attends to 49 tokens
    (7 × 7), so the W-MSA layer only needs to compute 784 × 49 = 38,416 attention
    scores, instead of 784² = 614,656 scores for regular MHA.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[图16-6](#swin_diagram)的左侧，图像被分割成28 × 28的补丁网格，并且这些补丁被分组为非重叠窗口。在Swin金字塔的第一层，补丁通常是4
    × 4像素，每个窗口包含一个7 × 7的补丁网格。因此，总共有784个补丁标记（28 × 28），但每个标记只关注49个标记（7 × 7），所以W-MSA层只需要计算784
    × 49 = 38,416个注意力分数，而不是常规MHA的784² = 614,656个分数。
- en: 'Most importantly, if we double the width and the height of the image, we quadruple
    the number of patch tokens, but each token still attends to only 49 tokens, so
    we just need to compute 4 times more attention scores: the Swin Transformer’s
    computational cost scales linearly with the image’s area, so it can handle large
    images. Conversely, ViT, DeiT, and PVT all scale quadratically: if you double
    the image width and height, the area is quadrupled, and the computational cost
    is multiplied by 16! As a result, these models are way too slow for very large
    images, meaning you must first downsample the image, which may hurt the model’s
    accuracy, especially for dense prediction tasks.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，如果我们将图像的宽度和高度加倍，补丁标记的数量将增加到四倍，但每个标记仍然只关注49个标记，所以我们只需要计算4倍更多的注意力分数：Swin
    Transformer的计算成本与图像面积成线性关系，因此它可以处理大图像。相反，ViT、DeiT和PVT都呈二次方增长：如果你将图像的宽度和高度加倍，面积将增加到四倍，计算成本将乘以16！因此，这些模型对于非常大的图像来说速度太慢，这意味着你必须首先下采样图像，这可能会损害模型的准确性，尤其是对于密集预测任务。
- en: '![Diagram of the Swin Transformer architecture showing regular multi-head self-attention
    (S-MSA), shifted window multi-head self-attention (SW-MSA), and optimized SW-MSA,
    highlighting how different window configurations cover the same image.](assets/hmls_1606.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![Swin Transformer架构图，展示了常规多头自注意力（S-MSA）、移位窗口多头自注意力（SW-MSA）和优化SW-MSA，突出不同窗口配置如何覆盖相同的图像。](assets/hmls_1606.png)'
- en: 'Figure 16-6\. Swin Transformer: alternates W-MSA (left) and SW-MSA (center);
    SW-MSA can be optimized to require the same number of windows as W-MSA (right)'
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-6\. Swin Transformer：交替W-MSA（左）和SW-MSA（中）；SW-MSA可以优化为需要与W-MSA相同数量的窗口（右）
- en: 'But wait a minute! If each token only attends to patches within the same window,
    how can we hope to capture long-range patterns? The answer is in the name of the
    architecture, Swin, which stands for *shifted windows*: every other encoder layer
    uses *shifted W-MSA* (SW-MSA), which is just like W-MSA except the windows are
    offset by half a window size. As you can see in the middle of [Figure 16-6](#swin_diagram),
    the windows are shifted by 3 patches toward the bottom right (because half of
    7 is 3.5, which we round down to 3). Why does this help? Well, nearby patches
    that were in separate windows in the previous layer are now in the same window,
    so they can see each other. By alternating W-MSA and SW-MSA, information from
    any part of the image can gradually propagate throughout the whole image. Moreover,
    since the architecture is hierarchical, the patches get coarser and coarser as
    we go up the pyramid, so the information can propagate faster and faster.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 但是等等！如果每个标记只关注同一窗口内的补丁，我们如何希望捕捉到长距离模式呢？答案就在架构的名字中，Swin，代表*移位窗口*：每隔一个编码器层使用*移位W-MSA*（SW-MSA），这就像W-MSA一样，只是窗口偏移了半个窗口大小。如图16-6的中间部分所示，窗口向右下角移动了3个补丁（因为7的一半是3.5，我们将其四舍五入到3）。这有什么帮助呢？好吧，在上一层中分别位于不同窗口的邻近补丁现在在同一窗口中，因此它们可以相互看到。通过交替W-MSA和SW-MSA，图像任何部分的信息可以逐渐传播到整个图像。此外，由于架构是分层的，随着我们向上金字塔移动，补丁变得越来越粗糙，因此信息可以更快地传播。
- en: 'A naive implementation of SW-MSA would require handling many extra windows.
    For example, if you compare W-MSA and SW-MSA in [Figure 16-6](#swin_diagram),
    you can see that W-MSA uses 16 windows, while SW-MSA uses 25 (at least in this
    example). To avoid this extra cost, the authors proposed an optimized implementation:
    instead of shifting the windows, we shift the image itself and wrap it around
    the borders, as shown in the righthand side of [Figure 16-6](#swin_diagram). This
    way, we’re back to 16 windows. However, this requires careful masking for the
    border windows that contain the wrapped patches; for example, the regions labeled
    ①, ②, ③, ④ should not see each other, even though they are within the same window,
    so an appropriate attention mask must be applied.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: SW-MSA 的简单实现需要处理许多额外的窗口。例如，如果你比较[图16-6](#swin_diagram)中的 W-MSA 和 SW-MSA，你可以看到
    W-MSA 使用了16个窗口，而 SW-MSA 使用了25个（至少在这个例子中）。为了避免这种额外成本，作者提出了一个优化的实现：不是移动窗口，而是移动图像本身并将其包裹在边缘，如[图16-6](#swin_diagram)的右侧所示。这样，我们回到了16个窗口。然而，这需要对包含包裹补丁的边缘窗口进行仔细的掩码处理；例如，标记为①、②、③、④的区域不应该相互看到，即使它们位于同一个窗口内，因此必须应用适当的注意力掩码。
- en: 'Overall, Swin is harder to implement than PVT, but its linear scaling and excellent
    performance make it one of the best vision transformers out there. But the year
    2021 wasn’t over: [Swin v2](https://homl.info) was released in November 2021.⁠^([9](ch16.html#id3781))
    It improved Swin across the board: more stable training for large ViTs, easier
    to fine-tune on large images, reduced need for labeled data, and more. Swin v2
    is still widely used in vision tasks today.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，Swin 的实现比 PVT 更困难，但它的线性扩展和优秀性能使其成为最好的视觉转换器之一。但2021年还没有结束：[Swin v2](https://homl.info)于2021年11月发布。⁠^([9](ch16.html#id3781))它在各个方面都改进了
    Swin：对大型 ViT 的更稳定训练，更容易在大图像上进行微调，减少了对标记数据的需要，等等。Swin v2至今仍在视觉任务中得到广泛应用。
- en: Our toolbox now contains vision transformers for classification (e.g., ViT and
    DeiT) and for dense prediction tasks (e.g., PVT and Swin). Let’s now explore one
    last pure-vision transformer, DINO, which introduced a revolutionary self-supervision
    technique for visual representation learning.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工具箱现在包含了用于分类（例如，ViT 和 DeiT）和密集预测任务（例如，PVT 和 Swin）的视觉转换器。现在让我们探索最后一个纯视觉转换器，DINO，它引入了一种革命性的自监督技术，用于视觉表示学习。
- en: 'DINO: Self-Supervised Visual Representation Learning'
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DINO：自监督视觉表示学习
- en: In April 2021, Mathilde Caron et al. introduced [DINO](https://homl.info/dino),^([10](ch16.html#id3788))
    an impressive self-supervised training technique that produces models capable
    of generating excellent image representations. These representations can then
    be used for classification and other tasks.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 2021年4月，Mathilde Caron 等人介绍了 [DINO](https://homl.info/dino)，^([10](ch16.html#id3788))一种令人印象深刻的自监督训练技术，能够生成出色的图像表示。这些表示可以用于分类和其他任务。
- en: 'Here’s how it works: the model is duplicated during training, with one network
    acting as a teacher and the other acting as a student (see [Figure 16-7](#dino_diagram)).
    Gradient descent only affects the student, while the teacher’s weights are just
    an exponential moving average (EMA) of the student’s weights. This is called a
    *momentum teacher*. The student is trained to match the teacher’s predictions:
    since they’re almost the same model, this is called *self-distillation* (hence
    the name of the model: self-**di**stillation with **no** labels).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这样工作的：在训练过程中，模型被复制，其中一个网络充当教师，另一个网络充当学生（见[图16-7](#dino_diagram)）。梯度下降只影响学生，而教师的权重只是学生权重的指数移动平均（EMA）。这被称为*动量教师*。学生被训练以匹配教师的预测：由于它们几乎是相同的模型，这被称为*自蒸馏*（因此模型的名称为：无标签的自**蒸馏**）。
- en: 'At each training step, the input images are augmented in various ways: color
    jitter, grayscale, Gaussian blur, horizontal flipping, and more. Importantly,
    they are augmented in different ways for the teacher and the student: the teacher
    always sees the full image, only slightly augmented, while the student often sees
    only a zoomed-in section of the image, with stronger augmentations. In short,
    the teacher and the student don’t see the same variant of the original image,
    yet their predictions must still match. This forces them to agree on high-level
    representations.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个训练步骤中，输入图像以各种方式进行增强：颜色抖动、灰度、高斯模糊、水平翻转等。重要的是，它们以不同的方式增强教师和学生：教师总是看到完整图像，只有轻微的增强，而学生通常只看到图像的放大部分，具有更强的增强。简而言之，教师和学生看到的原始图像的变体不同，但他们的预测必须仍然匹配。这迫使他们就高级表示达成一致。
- en: With this mechanism, however, there’s a strong risk of *mode collapse*. This
    is when both the student and the teacher always output the exact same thing, completely
    ignoring the input images. To prevent this, DINO keeps track of a moving average
    of the teacher’s predicted logits, and it subtracts this average from the predicted
    logits. This is called *centering*, forcing the teacher to distribute its predictions
    evenly across all classes (on average, over time).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个机制存在一个强烈的**模式崩溃**风险。这指的是学生和教师总是输出完全相同的内容，完全忽略了输入图像。为了防止这种情况，DINO会跟踪教师预测的logits的移动平均值，并从预测的logits中减去这个平均值。这被称为**中心化**，迫使教师将其预测均匀地分布在所有类别上（平均而言，随着时间的推移）。
- en: 'But centering alone might cause the teacher to simply output the same probability
    for every class, all the time, still ignoring the image. To avoid this, DINO also
    forces the teacher to have high confidence in its highest predictions: this is
    called *sharpening*. It’s implemented by applying a low temperature to the teacher’s
    logits (i.e., dividing them by a temperature smaller than 1). Together, centering
    and sharpening preserve the diversity in the teacher’s outputs; this leaves no
    easy shortcut for the model. It must base its predictions on the actual content
    of the image.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 但仅靠中心化可能会导致教师始终为每个类别输出相同的概率，仍然忽略图像。为了避免这种情况，DINO还迫使教师对其最高预测有很高的信心：这被称为**锐化**。它是通过将教师logits应用于低温实现的（即，除以小于1的温度）。中心化和锐化共同保留了教师输出的多样性；这为模型留下了没有捷径。它必须基于图像的实际内容进行预测。
- en: '![Diagram illustrating the DINO model''s use of teacher and student Vision
    Transformers (ViTs) with centering and sharpening to enhance predictive accuracy
    without labels.](assets/hmls_1607.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![说明DINO模型使用带有中心化和锐化以无标签增强预测精度的教师和学生Vision Transformers (ViTs)的示意图](assets/hmls_1607.png)'
- en: Figure 16-7\. DINO, or self-distillation with no labels
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-7。DINO，或无标签的自蒸馏
- en: 'After training, you can drop the teacher: the student is the final DINO model.
    If you feed it a new image, it will output a sequence of contextualized patch
    embeddings. These can be used in various ways. For example, you can train a classifier
    head on top of the class token’s output embedding. In fact, you don’t even need
    a new classifier head: you can run DINO on every training image to get their representation
    (i.e., the output of the class token), then compute the mean representation per
    class. Then, when given a new image, use DINO to compute its representation and
    look for the class with the nearest mean representation. This simple approach
    reaches 78.3% top-1 accuracy on ImageNet, which is pretty impressive.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，你可以丢弃教师：学生就是最终的DINO模型。如果你给它一个新图像，它将输出一系列上下文化的补丁嵌入。这些可以以各种方式使用。例如，你可以在类别标记的输出嵌入上训练一个分类头。实际上，你甚至不需要一个新的分类头：你可以在每个训练图像上运行DINO以获取它们的表示（即类别标记的输出），然后计算每个类别的平均表示。然后，当给定一个新图像时，使用DINO计算其表示并寻找与最近平均表示最接近的类别。这种方法在ImageNet上达到了78.3%的top-1准确率，这相当令人印象深刻。
- en: But it’s not just about classification! Interestingly, the DINO authors noticed
    that the class token’s attention maps in the last layer often focus on the main
    object of interest in the image, even though they were trained entirely without
    labels! In fact, each attention head seems to focus on a different part of the
    object, as you can see in [Figure 16-8](#unsupervised_segmentation_diagram).⁠^([11](ch16.html#id3797))
    See the notebook for a code example that uses DINO to plot a similar attention
    map.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 但这不仅仅是关于分类！有趣的是，DINO的作者们注意到，在最后一层的类别标记的注意力图中，通常关注图像中感兴趣的主要对象，尽管它们完全是在没有标签的情况下训练的！实际上，每个注意力头似乎关注对象的某个不同部分，正如您可以在[图16-8](#unsupervised_segmentation_diagram)⁠^([11](ch16.html#id3797))中看到的那样。查看笔记本以获取使用DINO绘制类似注意力图的代码示例。
- en: '![Diagram showing unsupervised image segmentation using DINO, with different
    attention heads highlighting various parts of objects like vegetables, a zebra,
    and a truck.](assets/hmls_1608.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![使用DINO进行无监督图像分割的图表，不同的注意力头突出显示各种对象的部分，如蔬菜、斑马和卡车。](assets/hmls_1608.png)'
- en: Figure 16-8\. Unsupervised image segmentation using DINO—different attention
    heads attend to different parts of the main object
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-8\. 使用DINO进行无监督图像分割——不同的注意力头关注主对象的不同部分
- en: Later techniques such as [TokenCut](https://homl.info/tokencut)⁠^([12](ch16.html#id3799))
    built upon DINO to detect and segment objects in images and videos. Then, in April
    2023, Meta released [DINOv2](https://homl.info/dino2),⁠^([13](ch16.html#id3801))
    which was trained on a curated and much larger dataset, and was tweaked to output
    per-patch features, making it a great foundation model not just for classification,
    but also for dense prediction tasks.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 之后的技术，如[TokenCut](https://homl.info/tokencut)⁠^([12](ch16.html#id3799))，在DINO的基础上构建，用于检测和分割图像和视频中的对象。然后，在2023年4月，Meta发布了[DINOv2](https://homl.info/dino2)⁠^([13](ch16.html#id3801))，该模型在精心挑选且规模更大的数据集上进行了训练，并调整以输出每个补丁的特征，使其不仅成为分类的强大基础模型，也适用于密集预测任务。
- en: 'Let’s step back: we’ve covered CNN-based transformers such as DETR, followed
    by the original ViT (image patches through an encoder), DeiT (a distilled ViT),
    PVT (a hierarchical ViT with spatial reduction attention), Swin (a hierarchical
    ViT with window-based attention), and DINO (self-distillation with no labels).
    Before we move on to multimodal transformers, let’s quickly go through a few more
    pure-vision transformer models and techniques.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下：我们已经涵盖了基于CNN的Transformer，如DETR，接着是原始的ViT（通过编码器传递图像补丁），DeiT（蒸馏ViT），PVT（具有空间减少注意力的分层ViT），Swin（基于窗口的注意力的分层ViT）和DINO（无标签的自蒸馏）。在我们继续到多模态Transformer之前，让我们快速浏览一些其他纯视觉Transformer模型和技术。
- en: Other Major Vision Models and Techniques
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他主要视觉模型和技术
- en: 'Progress in vision transformers has continued steadily to this day. Here is
    a brief overview of some landmark papers:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉Transformer的进展一直持续到今天。以下是一些里程碑论文的简要概述：
- en: '[“Scaling Vision Transformers”](https://homl.info/scalingvits),⁠^([14](ch16.html#id3806))
    June 2021'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[“Scaling Vision Transformers”](https://homl.info/scalingvits)⁠^([14](ch16.html#id3806))
    2021年6月'
- en: 'Google researchers showed how to scale ViTs up or down, depending on the amount
    of available data. They managed to create a huge 2 billion parameter model that
    reached over 90.4% top-1 accuracy on ImageNet. Conversely, they also trained a
    scaled-down model that reached over 84.8% top-1 accuracy on ImageNet, using only
    10,000 images: that’s just 10 images per class!'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Google研究人员展示了如何根据可用数据量放大或缩小ViTs。他们成功创建了一个包含20亿参数的巨大模型，在ImageNet上达到了超过90.4%的top-1准确率。相反，他们还训练了一个缩小后的模型，在ImageNet上达到了超过84.8%的top-1准确率，仅使用了10,000张图片：这仅仅是每个类别10张图片！
- en: '[“BEiT: BERT Pre-Training of Image Transformers”](https://homl.info/beit),⁠^([15](ch16.html#id3808))
    June 2021'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[“BEiT: BERT Pre-Training of Image Transformers”](https://homl.info/beit)⁠^([15](ch16.html#id3808))
    2021年6月'
- en: Hangbo Bao et al. proposed a *masked image modeling* (MIM) approach inspired
    from BERT’s masked language modeling (MLM). BEiT is pretrained to reconstruct
    masked image patches from the visible ones. This pretraining technique significantly
    improves downstream tasks.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 傅博等提出了受BERT的掩码语言建模（MLM）启发的*掩码图像建模*（MIM）方法。BEiT通过预训练从可见图像补丁重建掩码图像补丁。这种预训练技术显著提高了下游任务。
- en: Note that BEiT is not trained to predict the raw pixels of the masked patches;
    instead, it must predict the masked token IDs. But where do these token IDs come
    from? Well, the original image is passed through a *discrete variational autoencoder*
    (dVAE, see [Chapter 18](ch18.html#autoencoders_chapter)) which encodes each patch
    into a visual token ID (an integer), from a fixed vocabulary. These are the IDs
    that BEiT tries to predict. The goal is to avoid wasting the model’s capacity
    on unnecessary details.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，BEiT 并未训练去预测被遮挡区域的原始像素；相反，它必须预测被遮挡的标记 ID。但这些标记 ID 从何而来呢？嗯，原始图像通过一个 *离散变分自动编码器*（dVAE，见第
    18 章[链接](ch18.html#autoencoders_chapter)）被编码成视觉标记 ID（一个整数），来自一个固定词汇表。这些就是 BEiT
    尝试预测的 ID。目标是避免在不需要的细节上浪费模型的能力。
- en: '[“Masked Autoencoders Are Scalable Vision Learners”](https://homl.info/mae),⁠^([16](ch16.html#id3816))
    November 2021'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[“Masked Autoencoders Are Scalable Vision Learners”](https://homl.info/mae)，⁠^([16](ch16.html#id3816))
    2021年11月'
- en: 'This paper by a team of Facebook researchers (led by the prolific Kaiming He)
    also proposes a pretraining technique based on masked image modeling, but it removes
    the complexity of BEiT’s dVAE: masked autoencoder (MAE) directly predicts raw
    pixel values. Crucially, it uses an asymmetric encoder-decoder architecture: a
    large encoder processes only the visible patches, while a lightweight decoder
    reconstructs the entire image. Since 75% of patches are masked, this design dramatically
    reduces computational cost and allows MAE to be pretrained on very large datasets.
    This leads to strong performance on downstream tasks.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇由 Facebook 研究团队（由多产的 Kaiming He 领导）撰写的论文也提出了一种基于遮挡图像建模的预训练技术，但它去除了 BEiT 的
    dVAE 的复杂性：遮挡自动编码器（MAE）直接预测原始像素值。关键的是，它使用了一个非对称的编码器-解码器架构：一个大的编码器仅处理可见的补丁，而一个轻量级的解码器重建整个图像。由于
    75% 的补丁被遮挡，这种设计大大降低了计算成本，并允许 MAE 在非常大的数据集上进行预训练。这导致了在下游任务上的强大性能。
- en: '[“Model Soups”](https://homl.info/modelsoups),⁠^([17](ch16.html#id3820)) March
    2022'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[“模型汤”（Model Soups）](https://homl.info/modelsoups)，⁠^([17](ch16.html#id3820))
    2022年3月'
- en: This paper demonstrated that it’s possible to first train multiple transformers,
    then average their weights to create a new and improved model. This is similar
    to an ensemble (see [Chapter 6](ch06.html#ensembles_chapter)), except there’s
    just one model in the end, which means there’s no inference cost.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文证明了首先训练多个变压器，然后平均它们的权重以创建一个新且改进的模型是可能的。这与集成（见第 6 章[链接](ch06.html#ensembles_chapter)）类似，但最终只有一个模型，这意味着没有推理成本。
- en: '[“EVA: Exploring the Limits of Masked Visual Representation Learning at Scale”](https://homl.info/eva),⁠^([18](ch16.html#id3821))
    May 2022'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[“EVA: Exploring the Limits of Masked Visual Representation Learning at Scale”](https://homl.info/eva)，⁠^([18](ch16.html#id3821))
    2022年5月'
- en: EVA is a family of large ViTs pretrained at scale, using enhanced MAE and strong
    augmentations. It’s one of the leading foundation models for ViTs. EVA-02, released
    in March 2023, does just as well or better despite having fewer parameters. The
    large variant has 304M parameters and reaches an impressive 90.0% on ImageNet.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: EVA 是一系列在规模上预训练的大 ViT，使用增强的 MAE 和强大的增强。它是 ViT 的领先基础模型之一。2023 年 3 月发布的 EVA-02，尽管参数较少，表现同样出色或更好。大型变体有
    304M 个参数，在 ImageNet 上达到了令人印象深刻的 90.0%。
- en: '[I-JEPA](https://homl.info/ijepa),⁠^([19](ch16.html#id3823)) January 2023'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '[I-JEPA](https://homl.info/ijepa)，⁠^([19](ch16.html#id3823)) 2023年1月'
- en: Yann LeCun proposed the joint-embedding predictive architecture (JEPA) in a
    [2022 paper](https://homl.info/jepa),⁠^([20](ch16.html#id3826)) as part of his
    world-model framework, which aims to deepen AI’s understanding of the world and
    improve the reliability of its predictions. I-JEPA is an implementation of JEPA
    for images. It was soon followed by [V-JEPA](https://homl.info/vjepa) in 2024,
    and [V-JEPA 2](https://homl.info/vjepa2) in 2025, both of which process videos.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Yann LeCun 在一篇 [2022 年的论文](https://homl.info/jepa)，⁠^([20](ch16.html#id3826))
    中提出了联合嵌入预测架构（JEPA），作为其世界模型框架的一部分，旨在加深人工智能对世界的理解并提高其预测的可靠性。I-JEPA 是针对图像的 JEPA 的实现。2024
    年很快出现了 [V-JEPA](https://homl.info/vjepa)，2025 年出现了 [V-JEPA 2](https://homl.info/vjepa2)，两者都处理视频。
- en: 'During training, JEPA involves two encoders and a predictor: the teacher encoder
    sees the full input (e.g., a photo of a cat) while the student encoder sees only
    part of the input (e.g., the same cat photo but without the ears). Both encoders
    convert their inputs to embeddings, then the predictor tries to predict the teacher
    embedding for the missing part (e.g., the ears) given the student embeddings for
    the rest of the input (e.g., the cat without ears). The student encoder and the
    predictor are trained jointly, while the teacher encoder is just a moving average
    of the student encoder (much like in DINO). JEPA mostly works in embedding space
    rather than pixel space, which makes it fast, parameter efficient, and more semantic.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，JEPA涉及两个编码器和预测器：教师编码器看到完整的输入（例如，一张猫的照片），而学生编码器只看到输入的一部分（例如，同一张猫的照片但没有耳朵）。两个编码器都将它们的输入转换为嵌入表示，然后预测器试图根据输入其余部分的嵌入（例如，没有耳朵的猫）预测缺失部分（例如，耳朵）的教师嵌入。学生编码器和预测器是联合训练的，而教师编码器仅仅是学生编码器的一个移动平均值（类似于DINO）。JEPA主要在嵌入空间而不是像素空间中工作，这使得它快速、参数高效且更具语义性。
- en: After training, the teacher encoder and the predictor are no longer needed,
    but the student encoder can be used to generate excellent, meaningful representations
    for downstream tasks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，不再需要教师编码器和预测器，但学生编码器可以用来为下游任务生成优秀、有意义的表示。
- en: 'The list could go on and on:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表可以一直继续：
- en: NesT or DeiT-III for image classification
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NesT或DeiT-III用于图像分类
- en: MobileViT, EfficientFormer, EfficientViT, or TinyViT for small and efficient
    image classification models (e.g., for mobile devices)
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MobileViT、EfficientFormer、EfficientViT或TinyViT，用于小型高效的图像分类模型（例如，用于移动设备）
- en: Hierarchical transformers like Twins-SVT, FocalNet, MaxViT, and InternImage,
    often used as backbones for dense prediction tasks
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于Twins-SVT、FocalNet、MaxViT和InternImage这样的分层Transformer，常被用作密集预测任务的骨干网络
- en: Mask2Former or OneFormer for general-purpose segmentation, SEEM for universal
    segmentation, and SAM or MobileSAM for interactive segmentation
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mask2Former或OneFormer用于通用分割，SEEM用于通用分割，SAM或MobileSAM用于交互式分割
- en: ViTDet or RT-DETR for object detection
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ViTDet或RT-DETR用于目标检测
- en: TimeSformer, VideoMAE, or OmniMAE for video understanding
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TimeSformer、VideoMAE或OmniMAE用于视频理解
- en: There are also techniques like *token merging* (ToMe) which speeds up inference
    by merging similar tokens on the fly, *token pruning* to drop unimportant tokens
    during processing (i.e., with low attention scores), *early exiting* to only compute
    deep layers for the most important tokens, *patch selection* to select only the
    most informative patches for processing, and self-supervised training techniques
    like SimMIM, iBOT, CAE, or DINOv2, and more.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一些技术，如*token merging*（ToMe），通过在运行时合并相似标记来加速推理，*token pruning*在处理过程中删除不重要的标记（即，具有低注意力分数的标记），*early
    exiting*仅对最重要的标记计算深层层，*patch selection*仅选择最有信息量的块进行处理，以及像SimMIM、iBOT、CAE或DINOv2这样的自监督训练技术，等等。
- en: Hopefully we’ve covered a wide enough variety of models and techniques for you
    to be able to explore further on your own.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 希望我们已经涵盖了足够多的模型和技术，以便您能够进一步探索。
- en: Tip
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Some of these vision-only models were pretrained on multimodal data (e.g.,
    image-text pairs or input prompts): OmniMAE, SEEM, SAM, MobileSAM, and DINOv2\.
    Which leads us nicely to the second part of this chapter.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些仅用于视觉的模型在多模态数据上进行了预训练（例如，图像-文本对或输入提示）：OmniMAE、SEEM、SAM、MobileSAM和DINOv2。这很自然地引出了本章的第二部分。
- en: We already had transformers that could read and write (and chat!), and now we
    have vision transformers that can see. It’s time to build transformers that can
    handle both text and images at the same time, as well as other modalities.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了能够阅读和写入（以及聊天！）的Transformer，现在我们有能够“看到”的视觉Transformer。是时候构建能够同时处理文本和图像，以及其他模态的Transformer了。
- en: Multimodal Transformers
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态Transformer
- en: 'Humans are multimodal creatures: we perceive the world through multiple senses—sight,
    hearing, smell, taste, touch, sense of balance, proprioception (i.e., sense of
    body position), and several others—and we act upon the world through movement,
    speech, writing, etc. Each of these modalities can be considered at a very low
    level (e.g., sound waves) or at a higher level (e.g., words, intonations, melody).
    Importantly, modalities are heterogeneous: one modality may be continuous while
    another is discrete, one may be temporal while the other is spatial, one may be
    high-resolution (e.g., 48 kHz audio) while the other is not (e.g., text), one
    may be noisy while the other is clean, and so on.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 人类是多模态的生物：我们通过多个感官感知世界——视觉、听觉、嗅觉、味觉、触觉、平衡感、本体感觉（即身体位置感）以及几个其他感官——并通过运动、言语、写作等方式作用于世界。这些模态中的每一个都可以在非常低级（例如，声波）或高级（例如，单词、语调、旋律）层面上进行考虑。重要的是，模态是异质的：一个模态可能是连续的，而另一个是离散的；一个可能是时间的，而另一个是空间的；一个可能是高分辨率的（例如，48
    kHz音频），而另一个不是（例如，文本）；一个可能是嘈杂的，而另一个是干净的，等等。
- en: 'Moreover, modalities may interact in various ways. For example, when we chat
    with someone, we may listen to their voice while also watching the movement of
    their lips: these two modalities (auditory and visual) carry overlapping information,
    which helps our brain better parse words. But multimodality is not just about
    improving the signal/noise ratio: facial expressions may carry their own meaning
    (e.g., smiles and frowns), and different modalities may combine to produce a new
    meaning. For example, if you say “he’s an expert” while rolling your eyes or gesturing
    air quotes, you’re clearly being ironic, which inverts the meaning of your sentence
    and conveys extra information (e.g., humor or disdain) which neither modality
    possesses on its own.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模态之间可能以各种方式相互作用。例如，当我们与人交谈时，我们可能一边听他们的声音，一边观察他们嘴唇的运动：这两种模态（听觉和视觉）携带重叠的信息，这有助于我们的大脑更好地解析单词。但多模态不仅仅是关于提高信号/噪声比：面部表情可能带有自己的意义（例如，微笑和皱眉），不同的模态可能结合产生新的意义。例如，如果你说“他是个专家”的同时翻着白眼或做出引号手势，你显然是在讽刺，这颠倒了你的句子的意义，并传达了额外信息（例如，幽默或轻蔑），这些信息任何一个模态单独都不具备。
- en: So multimodal machine learning requires designing models that can handle very
    heterogeneous data and capture their interactions. There are two main challenges
    for this. The first is called *fusion*, and it’s about finding a way to combine
    different modalities, for example, by encoding them into the same representation
    space. The second is called *alignment*, where the goal is to discover the relationships
    between modalities. For example, perhaps you have a recording of a speech, as
    well as a text transcription, and you want to find the timestamp of each word.
    Or you want to find the most relevant object in an image given a text query such
    as “the dog next to the tree” (this is called *visual grounding*). Many other
    common tasks involve two or more modalities, such as image captioning, image search,
    visual question answering (VQA), speech-to-text (STT), text-to-speech (TTS), embodied
    AI (i.e., a model capable of physically interacting with the environment), and
    much more.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，多模态机器学习需要设计能够处理非常异质数据并捕捉它们之间相互作用的模型。这个任务有两个主要挑战。第一个被称为**融合**，它涉及到找到一种方法来结合不同的模态，例如，通过将它们编码到同一个表示空间中。第二个被称为**对齐**，其目标是发现模态之间的关系。例如，也许你有一段语音录音，以及相应的文本转录，你想要找到每个单词的时间戳。或者，你想要根据文本查询“树旁的狗”找到图像中最相关的对象（这被称为**视觉定位**）。许多其他常见任务涉及两个或更多模态，例如图像标题、图像搜索、视觉问答（VQA）、语音转文本（STT）、文本转语音（TTS）、具身人工智能（即能够与物理环境进行物理交互的模型）等等。
- en: 'Multimodal machine learning has been around for decades, but progress has recently
    accelerated thanks to deep learning, and particularly since the rise of transformers.
    Indeed, transformers can ingest pretty much any modality, as long as you can chop
    it into a sequence of meaningful tokens (e.g., text into words, images into small
    patches, audio or video into short clips, etc.). Once you have prepared a sequence
    of token embeddings, you’re ready to feed it to a transformer. Embeddings from
    different modalities can be fused in various ways: summed up, concatenated, passed
    through a fusion encoder, and more. This can take care of the fusion problem.
    And transformers also have multi-head attention, which is a powerful tool to detect
    and exploit complex patterns, both within and across modalities. This can take
    care of the alignment problem.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态机器学习已经存在了几十年，但由于深度学习，尤其是transformers的兴起，最近进展加速。确实，只要你能将其切割成一系列有意义的标记（例如，文本切割成单词，图像切割成小块，音频或视频切割成短剪辑等），transformers几乎可以处理任何模态。一旦你准备好了标记嵌入的序列，你就可以将其输入到transformers中。不同模态的嵌入可以以各种方式融合：相加、连接、通过融合编码器传递等。这可以解决融合问题。并且transformers还有多头注意力，这是一个强大的工具，可以检测和利用复杂模式，无论是模态内部还是跨模态。这可以解决对齐问题。
- en: Researchers quickly understood the potential of transformers for multimodal
    architectures. The first multimodal transformers were released just months after
    the original Transformer paper was released in early 2018 with image captioning,
    video captioning, and more. Let’s look at some of the most impactful multimodal
    transformer architectures, starting with VideoBERT.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员很快理解了transformers在多模态架构中的潜力。在2018年初原Transformer论文发布后的几个月内，就发布了第一个多模态transformers，包括图像标题、视频标题等。让我们看看一些最有影响力的多模态transformers架构，从VideoBERT开始。
- en: 'VideoBERT: A BERT Variant for Text plus Video'
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VideoBERT：一种用于文本加视频的BERT变体
- en: 'In April 2019, Google researchers released [VideoBERT](https://homl.info/videobert).⁠^([21](ch16.html#id3840))
    As its name suggests, this model is very similar to BERT, except it can handle
    both text and videos. In fact, the authors just took a pretrained BERT-large model,
    extended its embedding matrix to allow for extra video tokens (more on this shortly),
    and continued training the model using self-supervision on a text plus video training
    set. This dataset was built from a large collection of instructional YouTube videos,
    particularly cooking videos. These videos typically involve someone describing
    a sequence of actions while performing them (e.g., “Cut the tomatoes into thin
    slices like this”). To feed these videos to VideoBERT, the authors had to encode
    the videos into both text and visual sequences (see [Figure 16-9](#videobert_encoding_diagram)):'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年4月，谷歌研究人员发布了[VideoBERT](https://homl.info/videobert)。⁠^([21](ch16.html#id3840))
    如其名所示，这个模型与BERT非常相似，但它可以处理文本和视频。实际上，作者只是取了一个预训练的BERT-large模型，扩展了其嵌入矩阵以允许额外的视频标记（稍后将有更多介绍），并继续使用文本加视频训练集上的自监督训练模型。这个数据集是从大量教学YouTube视频中构建的，尤其是烹饪视频。这些视频通常涉及某人描述执行一系列动作的同时进行（例如，“像这样把番茄切成薄片”）。为了将这些视频输入到VideoBERT中，作者必须将这些视频编码成文本和视觉序列（见[图16-9](#videobert_encoding_diagram)）：
- en: For the visual modality, they extracted nonoverlapping 1.5-second clips at 20
    frames per second (i.e., 30 frames each), and they passed these clips through
    a 3D CNN named S3D. This CNN is based on Inception modules and separable convolutions
    (see [Chapter 12](ch12.html#cnn_chapter)), and it was pretrained on the Kinetics
    dataset composed of many YouTube videos of people performing a wide range of actions.
    The authors added a 3D average pooling layer on top of S3D to get a 1,024-dimensional
    vector for each video clip. Each vector encodes fairly high-level information
    about the video clip.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于视觉模态，他们从每秒20帧（即每段30帧）中提取了非重叠的1.5秒剪辑，并将这些剪辑通过一个名为S3D的3D CNN。这个CNN基于Inception模块和可分离卷积（见[第12章](ch12.html#cnn_chapter)），并在包含大量YouTube视频中人们执行各种动作的Kinetics数据集上进行了预训练。作者在S3D之上添加了一个3D平均池化层，为每个视频剪辑得到一个1,024维度的向量。每个向量编码了关于视频剪辑相当高级的信息。
- en: To extract the text from the videos, the authors used YouTube’s internal speech-to-text
    software, after which they dropped the audio tracks from the videos. Then they
    separated the text into sentences by adding punctuation using an off-the-shelf
    LSTM model. Finally, they preprocessed and tokenized the text just like for BERT.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了从视频中提取文本，作者使用了YouTube的内部语音转文本软件，之后他们从视频中删除了音频轨道。然后，他们通过添加标点符号使用现成的LSTM模型将文本分离成句子。最后，他们像对BERT一样预处理和标记化文本。
- en: '![Diagram illustrating the VideoBERT process, showing how actions in a video
    are converted into text and visual tokens using S3D for video clips and speech-to-text
    for audio.](assets/hmls_1609.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![说明VideoBERT过程的图解，展示了如何使用S3D对视频片段进行编码以及使用语音转文本对音频进行编码，将视频中的动作转换为文本和视觉标记。](assets/hmls_1609.png)'
- en: Figure 16-9\. VideoBERT—encoding a video into a text sequence and a visual sequence
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-9\. VideoBERT—将视频编码为文本序列和视觉序列
- en: 'Great! We now have a text token sequence describing some actions, and a sequence
    of vectors representing video clips of these actions. However, we have a problem.
    Recall that BERT is pretrained using MLM, where the model must predict masked
    tokens from a fixed vocabulary. We do have a fixed vocabulary for the text tokens,
    but not for the video tokens. So let’s build one! For this, the authors gathered
    all the visual vectors produced by S3D over their training set, and they clustered
    these vectors into *k* = 12 clusters using *k*-means (see [Chapter 8](ch08.html#unsupervised_learning_chapter)).
    Then they used *k*-means again on each cluster to get 12² = 144 clusters, then
    again and again to get 12⁴ = 20,736 clusters. This process is called *hierarchical
    k-means*, and it’s much faster than running *k*-means just once using *k* = 20,736,
    plus it typically produces much better clusters. Now each vector can be replaced
    with its cluster ID: this way, each video clip is represented by a single ID from
    a fixed visual vocabulary, so the whole video is now represented as one sequence
    of visual token IDs (e.g., 194, 3912, …​), exactly like tokenized text. In short,
    we’ve gone from a continuous 1,024-dimensional space down to a discrete space
    with just 20,736 possible values. There’s a lot of information loss at this step,
    but VideoBERT’s excellent performance suggests that much of the important information
    remains.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们现在有一个描述一些动作的文本标记序列，以及代表这些动作视频剪辑的向量序列。然而，我们遇到了一个问题。回想一下，BERT是使用MLM进行预训练的，其中模型必须从固定词汇表中预测被掩码的标记。我们确实有一个用于文本标记的固定词汇表，但不是用于视频标记的。所以，让我们构建一个吧！为此，作者收集了S3D在他们的训练集中产生的所有视觉向量，并使用k-means将它们聚类成*k*
    = 12个簇（见第8章[第8章](ch08.html#unsupervised_learning_chapter)）。然后，他们在每个簇上再次使用k-means，得到12²
    = 144个簇，然后又再次这样做，得到12⁴ = 20,736个簇。这个过程被称为**分层k-means**，它比只使用*k* = 20,736运行一次k-means要快得多，而且通常会产生更好的簇。现在，每个向量都可以用其簇ID来替换：这样，每个视频剪辑就由一个来自固定视觉词汇表的单一ID来表示，因此整个视频现在就表示为一个视觉标记ID的序列（例如，194，3912，…），就像标记化文本一样。简而言之，我们已经从1,024维的连续空间下降到只有20,736个可能值的离散空间。在这个步骤中会有大量的信息损失，但VideoBERT出色的性能表明，大部分重要信息仍然保留。
- en: Note
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Since the authors used a pretrained BERT-large model, the text token embeddings
    were already excellent before VideoBERT’s additional training even started. For
    the visual token embeddings, rather than using trainable embeddings initialized
    from scratch, the authors used frozen embeddings initialized using the 1,024-dimensional
    vector representations of the *k*-means cluster centroids.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于作者使用了预训练的BERT-large模型，因此在VideoBERT开始额外训练之前，文本标记嵌入已经非常优秀了。对于视觉标记嵌入，作者并没有使用从零开始初始化的可训练嵌入，而是使用了使用k-means簇质心的1,024维向量表示初始化的冻结嵌入。
- en: 'The authors used three different training regimes: text-only, video-only, and
    text plus video. In text-only and video-only modes, VideoBERT was fed a single
    modality and trained to predict masked tokens (either text tokens or video tokens).
    For text plus video, the model was fed both text tokens and video tokens, simply
    concatenated (plus an unimportant separation token in between), and it had to
    predict whether the text tokens and video tokens came from the same part of the
    original video. This is called *linguistic-visual alignment*. For this, the authors
    added a binary classification head on top of the class token’s output (this replaces
    BERT’s next sentence prediction head). For negative examples, the authors just
    sampled random sentences and video segments. [Figure 16-10](#videobert_pretraining_diagram)
    shows all three modes at once, but keep in mind that they are actually separate.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用了三种不同的训练模式：仅文本、仅视频和文本加视频。在仅文本和仅视频模式下，VideoBERT只接受单一模态输入，并训练预测掩码标记（文本标记或视频标记）。对于文本加视频，模型同时接受文本标记和视频标记，简单连接（中间加上一个不重要的分隔标记），并需要预测文本标记和视频标记是否来自原始视频的同一部分。这被称为*语言-视觉对齐*。为此，作者在类别标记的输出上添加了一个二分类头（这取代了BERT的下一句预测头）。对于负例，作者只是随机采样句子和视频片段。[图16-10](#videobert_pretraining_diagram)同时显示了所有三种模式，但请注意，它们实际上是分开的。
- en: '![Diagram illustrating the VideoBERT model, highlighting masked token prediction
    and linguistic-visual alignment processes.](assets/hmls_1610.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![说明VideoBERT模型，突出显示掩码标记预测和语言-视觉对齐过程。](assets/hmls_1610.png)'
- en: Figure 16-10\. VideoBERT—pretraining using masked token prediction and linguistic-visual
    alignment (shown together but actually separate)
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-10\. VideoBERT—使用掩码标记预测和语言-视觉对齐进行预训练（显示在一起但实际上是分开的）
- en: Linguistic-visual alignment is a noisy task since the cook may explain something
    that they have already finished or will do later, so the authors concatenated
    random neighboring sentences to give the model more context. The authors had a
    few more tricks up their sleeves, such as randomly changing the video sampling
    rate to make the model more robust to different action speeds, since some cooks
    are faster than others; see the paper for more details.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 语言-视觉对齐是一个有噪声的任务，因为厨师可能会解释他们已经完成或将要做的某事，因此作者将随机相邻的句子连接起来，以给模型提供更多上下文。作者们还有一些其他的技巧，例如随机改变视频采样率，使模型对不同的动作速度更加鲁棒，因为有些厨师比其他人更快；更多细节请参阅论文。
- en: 'This was a lot of work, but the authors were finally done: they had a fully
    trained VideoBERT model. To demonstrate its effectiveness, they evaluated VideoBERT
    on some downstream tasks, including:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一项大量工作，但作者们最终完成了：他们拥有了一个完全训练好的VideoBERT模型。为了展示其有效性，他们在一些下游任务上评估了VideoBERT，包括：
- en: Zero-shot action classification
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 零样本动作分类
- en: 'Given a video clip, figure out which action is performed, without fine-tuning
    VideoBERT. The authors achieved this by feeding the video to VideoBERT, along
    with the following masked sentence: “Now let me show you how to [MASK] the [MASK]”.
    Then they looked at the output probabilities for both masked tokens, for each
    possible pair of verb and noun. If the video shows a cook slicing some tomatoes,
    then the probability of “slice” and “tomatoes” will be much higher than “bake”
    and “cake” or “boil” and “egg”.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个视频片段，无需微调VideoBERT即可判断出执行了哪个动作。作者通过将视频输入到VideoBERT中，并附带以下掩码句子：“现在让我给你演示如何**做**”。然后他们检查了每个可能的动词和名词配对中掩码标记的输出概率。如果视频中显示厨师正在切一些番茄，那么“slice”和“tomatoes”的概率将远高于“bake”和“cake”或“boil”和“egg”。
- en: Video captioning
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 视频字幕
- en: 'Given a video clip, generate a caption. To do this, the authors used the earliest
    [video-captioning transformer architecture](https://homl.info/videocaption),⁠^([22](ch16.html#id3846))
    but they replaced the input to the encoder with visual features output by VideoBERT.
    More specifically, they took an average of VideoBERT’s final output representations,
    including the representations of all of the visual tokens and the masked-out text
    tokens. The masked sentence they used was: “now let’s [MASK] the [MASK] to the
    [MASK], and then [MASK] the [MASK]”. After fine-tuning this new model, they obtained
    improved results over the original captioning model.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Using similar approaches, VideoBERT can be adapted for many other tasks, such
    as multiple-choice visual question answering: given an image, a question, and
    multiple possible answers, the model must find the correct answer. For example:
    “What is the cook doing?” → “Slicing tomatoes”. For this, one approach is to simply
    run VideoBERT on each possible answer, along with the video, and compare the linguistic-visual
    alignment scores: the correct answer should have the highest score.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'The success of VideoBERT inspired many other BERT-based multimodal transformers,
    many of which were released in August and September 2019: ViLBERT, VisualBERT,
    Unicoder-VL, LXMERT, VL-BERT, and UNITER. Most of these are single-stream models
    like VideoBERT, meaning that the modalities are fused very early in the network,
    typically by simply concatenating the sequences. However, ViLBERT and LXMERT are
    dual-stream transformers, meaning that each modality is processed by its own encoder,
    with a mechanism allowing the encoders to influence each other. This lets the
    model better understand each modality before trying to make sense of the interactions
    between them. VilBERT was particularly influential, so let’s look at it more closely.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'ViLBERT: A Dual-Stream Transformer for Text plus Image'
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ViLBERT was [proposed in August 2019](https://homl.info/vilbert)⁠^([23](ch16.html#id3852))
    by a team of researchers from the Georgia Institute of Technology, Facebook AI
    Research, and Oregon State University. They pointed out that the single-stream
    approach (used by VideoBERT and many others) treats both modalities identically,
    even though they may require different levels of processing. For example, if the
    visual features come from a deep CNN, then we already have good high-level visual
    features, whereas the text will need much more processing before the model has
    access to high-level text features. Moreover, the researchers hypothesized that
    “image regions may have weaker relations than words in a sentence”.⁠^([24](ch16.html#id3853))
    Lastly, BERT was initially pretrained using text only, so forcing it to process
    other modalities may give suboptimal results and even damage its weights during
    multimodal training.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'So the authors chose a dual-stream approach instead: each modality goes through
    its own encoder, and in the upper layers the two encoders are connected and exchange
    information through a new bidirectional cross-attention mechanism called *co-attention*
    (see [Figure 16-11](#co_attention_diagram)). Specifically, in each pair of connected
    encoder layers, the MHA query of one encoder is used as the MHA key/value by the
    other encoder.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，作者选择了一种双流方法：每个模态都通过自己的编码器，在上层，两个编码器通过一个新的双向交叉注意力机制连接并交换信息，称为*共注意力*（参见[图16-11](#co_attention_diagram)）。具体来说，在每一对连接的编码器层中，一个编码器的MHA查询被另一个编码器用作MHA键/值。
- en: '![Diagram of connected vision and text co-encoder layers using co-attention,
    where the multi-head attention (MHA) query from one encoder is used as the key/value
    by the other encoder.](assets/hmls_1611.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![连接视觉和文本共编码层的图解，其中使用共注意力，其中一个编码器的多头注意力（MHA）查询被另一个编码器用作键/值。](assets/hmls_1611.png)'
- en: 'Figure 16-11\. Two encoder layers connected through co-attention: the MHA query
    of one encoder is used as the MHA key/value by the other encoder'
  id: totrans-184
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-11. 通过共注意力连接的两个编码器层：一个编码器的MHA查询被另一个编码器用作MHA键/值
- en: 'The lower layers of the text encoder are initialized with BERT’s weights (the
    authors used a BERT base, which has 12 layers), and 6 co-attention layers sit
    on top (see the lower-right quadrant of [Figure 16-12](#vilbert_diagram)). The
    visual features are produced by a pretrained and frozen Faster R-CNN model, and
    it is assumed that these features are sufficiently high level so no further processing
    is needed; therefore, the visual encoder is exclusively composed of six co-attention
    layers, paired up with the text encoder’s six co-attention layers (see the lower-left
    quadrant of the figure). The Faster R-CNN model’s outputs go through a mean pooling
    layer for each detected region, so we get one feature vector per region, and low-confidence
    regions are dropped: each image ends up represented by 10 to 36 vectors.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 文本编码器的底层使用BERT的权重初始化（作者使用了BERT base，它有12层），并在其上方放置了6个共注意力层（参见[图16-12](#vilbert_diagram)的右下象限）。视觉特征由预训练并冻结的Faster
    R-CNN模型产生，并假设这些特征足够高级，因此不需要进一步处理；因此，视觉编码器仅由六个共注意力层组成，与文本编码器的六个共注意力层配对（参见图的下左象限）。Faster
    R-CNN模型的输出为每个检测到的区域通过一个均值池化层，因此我们得到每个区域的特征向量，并丢弃低置信度区域：每个图像最终由10到36个向量表示。
- en: '![Diagram illustrating the ViLBERT model''s pretraining process for masked
    token prediction and linguistic-visual alignment, highlighting the interaction
    between the visual and text encoders and their integration for classification
    tasks.](assets/hmls_1612.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![展示ViLBERT模型进行掩码标记预测和语言-视觉对齐预训练过程的图解，突出视觉编码器和文本编码器之间的交互以及它们在分类任务中的集成。](assets/hmls_1612.png)'
- en: Figure 16-12\. ViLBert pretraining using masked token prediction and linguistic-visual
    alignment (again, shown together but actually separate)
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-12. 使用掩码标记预测和语言-视觉对齐进行ViLBert预训练（再次，显示在一起但实际上是分开的）
- en: 'Since regions don’t have a natural order like words do, the visual encoder
    does not use positional encoding. Instead, it uses spatial encodings that are
    computed like this: each region’s bounding box is encoded as a 5D vector containing
    the normalized upper-left and lower-right coordinates, and the ratio of the image
    covered by the bounding box. Then this 5D vector is linearly projected up to the
    same dimensionality as the visual vector, and simply added to it.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于区域没有像单词那样的自然顺序，视觉编码器不使用位置编码。相反，它使用这种计算方式的空间编码：每个区域的边界框被编码为一个包含归一化左上角和右下角坐标以及边界框覆盖图像比例的5D向量。然后，这个5D向量被线性投影到与视觉向量相同的维度，并简单地添加到其中。
- en: 'Lastly, a special [IMG] token is prepended to the visual sequence: it serves
    the same purpose as the class token (i.e., to produce a representation of the
    whole sequence), but instead of being a trainable embedding, it’s computed as
    the average of the feature vectors (before spatial encoding), plus the spatial
    encoding for a bounding box covering the whole image.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个特殊的[IMG]标记被添加到视觉序列的开头：它具有与类别标记相同的作用（即产生整个序列的表示），但它不是可训练的嵌入，而是计算为特征向量的平均值（在空间编码之前），加上覆盖整个图像的边界框的空间编码。
- en: 'Now on to training! Similar to VideoBERT, the authors used masked token prediction
    and linguistic-visual alignment:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是训练阶段！与VideoBERT类似，作者使用了掩码标记预测和语言-视觉对齐：
- en: 'For masked token prediction, the authors used regular BERT-like MLM for the
    text encoder. However, for the visual encoder, since ViLBERT does not use a fixed-size
    visual vocabulary (there’s no clustering step), the model is trained to predict
    the class distribution that the CNN predicts for the given image region (this
    is a soft target). The authors chose this task rather than predicting raw pixels
    because the regions can be quite large and there’s typically not enough information
    in the surrounding regions and in the text to reconstruct the masked region correctly:
    it’s better to aim for a higher-level target.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于掩码标记预测，作者为文本编码器使用了类似于BERT的常规MLM。然而，对于视觉编码器，由于ViLBERT不使用固定大小的视觉词汇表（没有聚类步骤），该模型被训练来预测CNN对给定图像区域预测的类别分布（这是一个软目标）。作者选择这个任务而不是预测原始像素，因为区域可以相当大，并且通常周围区域和文本中的信息不足以正确重建掩码区域：目标是追求更高层次的目标。
- en: For linguistic-visual alignment, the model takes the outputs of the [IMG] and
    [CLS] tokens, then computes their itemwise product and passes the result to a
    binary classification head that must predict whether the text and image match.
    Multiplication is preferred over addition because it amplifies features that are
    strong in both representations (a bit like a logical AND gate), so it better captures
    alignment.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于语言-视觉对齐，该模型取[IMG]和[CLS]标记的输出，然后计算它们的逐项乘积，并将结果传递给一个二分类头，该头必须预测文本和图像是否匹配。乘法优于加法，因为它放大了两种表示中都强大的特征（有点像逻辑AND门），因此更好地捕捉了对齐。
- en: 'And that’s it. This model significantly beat the state of the art for several
    downstream tasks, including image grounding, caption-based image retrieval (even
    zero-shot), visual question answering, and *visual commonsense reasoning* (VCR)
    which involves answering a multiple-choice question about an image (like VQA),
    then selecting the appropriate justification. For example, given an image of a
    waiter serving some pancakes at a table, along with the question “Why is person
    #4 pointing at person #1”, the model must choose the correct answer “He is telling
    person #3 that person #1 ordered the pancakes”, then it must choose the justification
    “Person #3 is serving food and they might not know whose order is whose”.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。该模型在包括图像定位、基于描述的图像检索（甚至零样本）、视觉问答和*视觉常识推理*（VCR）等多个下游任务上显著超越了现有技术，其中VCR涉及回答关于图像的多选题（如VQA），然后选择适当的理由。例如，给定一张服务员在桌子上为一些煎饼服务的图片，以及问题“为什么第4个人指向第1个人”，模型必须选择正确的答案“他正在告诉第3个人第1个人点了煎饼”，然后它必须选择理由“第3个人正在上菜，他们可能不知道谁的订单是谁的”。
- en: 'ViLBERT had a strong influence on the field of multimodal machine learning
    thanks to its dual-stream architecture, the invention of co-attention, and its
    excellent results on many downstream tasks. It was another great demonstration
    of the power of large-scale self-supervised pretraining using transformers. The
    next major milestone came in 2021, and it approached the problem very differently,
    using contrastive pretraining: meet CLIP.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ViLBERT凭借其双流架构、协同注意力机制的创新以及在其众多下游任务上取得的出色成果，对多模态机器学习领域产生了深远影响。这是大规模自监督预训练使用transformers的强大能力的又一例证。2021年迎来了下一个重大里程碑，它采取了非常不同的方法，使用了对比预训练：迎来了CLIP。
- en: 'CLIP: A Dual-Encoder Text plus Image Model Trained with Contrastive Pretraining'
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CLIP：使用对比预训练的双编码器文本加图像模型
- en: 'OpenAI’s January 2021 release of [contrastive language–image pretraining (CLIP)](https://homl.info/clip)⁠^([25](ch16.html#id3870))
    was a major breakthrough, not just for its astounding capabilities, but also because
    of its surprisingly straightforward approach based on *contrastive learning*:
    the model learns to encode text and images into vector representations that are
    similar when the text and image match, and dissimilar when they don’t match.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI于2021年1月发布的[对比语言-图像预训练（CLIP）](https://homl.info/clip)⁠^([25](ch16.html#id3870))是一个重大突破，不仅因为其惊人的能力，还因为其基于*对比学习*的出人意料简单的方法：模型学习将文本和图像编码成向量表示，当文本和图像匹配时相似，不匹配时不相似。
- en: 'Once trained, the model can be used for many tasks, particularly zero-shot
    image classification. For example, CLIP can be used as an insect classifier without
    any additional training: just start by feeding all the possible class names to
    CLIP, such as “cricket”, “ladybug”, “spider”, and so on, to get one vector representation
    for each class name. Then, whenever you want to classify an image, feed it to
    CLIP to get a vector representation, and find the most similar class name representation
    using cosine similarity. This usually works even better if the text resembles
    typical image captions found on the web, since this is what CLIP was trained on,
    for example, “This is a photo of a ladybug” instead of just “ladybug”. A bit of
    prompt engineering can help (i.e., experimenting with various prompt templates).'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，该模型可以用于许多任务，尤其是零样本图像分类。例如，CLIP可以用作昆虫分类器，而无需任何额外的训练：只需将所有可能的类名输入到CLIP中，例如“蟋蟀”、“瓢虫”、“蜘蛛”等等，为每个类名得到一个向量表示。然后，无论何时你想对图像进行分类，只需将其输入到CLIP中，得到一个向量表示，然后使用余弦相似度找到最相似类名表示。如果文本类似于网络上常见的典型图像标题，这通常效果更好，因为CLIP就是基于这些进行训练的，例如，“这是一张瓢虫的照片”而不是仅仅“瓢虫”。一点提示工程可以帮助（即，尝试各种提示模板）。
- en: 'The good news is that CLIP is fully open source,⁠^([26](ch16.html#id3873)),
    several pretrained models are available on the Hugging Face Hub, and the Transformers
    library provides a convenient pipeline for zero-shot image classification:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，CLIP是完全开源的⁠^([26](ch16.html#id3873))，Hugging Face Hub上有几个预训练模型可用，Transformers库提供了一个方便的零样本图像分类管道：
- en: '[PRE7]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that we provided a prompt template, so the model will actually encode
    “This is a photo of a ladybug”, not just “ladybug” (if you don’t provide any template,
    the pipeline actually defaults to “This is a photo of a {}”.). Now let’s look
    at the results, which are sorted by score:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们提供了一个提示模板，因此模型实际上会编码“这是一张瓢虫的照片”，而不仅仅是“瓢虫”（如果你没有提供任何模板，管道实际上默认为“这是一张{}的照片”。）现在让我们看看结果，这些结果按分数排序：
- en: '[PRE8]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Great! CLIP predicts ladybug with over 99.7% confidence. Now if you want a flower
    classifier instead, just replace the candidate labels with names of flowers. If
    you include “dandelion” in the list and classify the same image, the model should
    choose “dandelion” with high confidence (ignoring the ladybug). Impressive!
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！CLIP以超过99.7%的置信度预测了瓢虫。现在，如果你想有一个花卉分类器，只需将候选标签替换为花卉名称。如果你在列表中包含“蒲公英”并分类相同的图像，模型应该以高置信度选择“蒲公英”（忽略瓢虫）。令人印象深刻！
- en: 'So how does this magic work? Well, CLIP’s architecture is based on a regular
    text encoder and a regular vision encoder, no co-attention or anything fancy (see
    [Figure 16-13](#clip_diagram)). You can actually use pretty much any text and
    vision encoders you want, as long as they can produce a vector representation
    of the text or image. The authors experimented with various encoders, including
    several ResNet and ViT models for vision, and a GPT-2-like model for text, all
    trained from scratch. What’s that I hear you say, GPT-2 is not an encoder? That’s
    true, it’s a decoder-only model, but we’re not pretraining it for next token prediction,
    so the last token’s output is free to be used as a representation of the entire
    input sequence, which is what CLIP does. You may wonder why we’re not using a
    regular text encoder like BERT? Well, we could, but OpenAI created GPT—Alex Radford
    is the lead author of both GPT and CLIP—so that’s most likely why GPT-2 was chosen:
    the authors simply had more experience with this model and a good training infrastructure
    already in place. Using a causal encoder also makes it possible to cache the intermediate
    state of the model when multiple texts start in the same way; for example, “This
    is a photo of a”.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这个魔法是如何工作的呢？嗯，CLIP的架构基于一个常规文本编码器和常规视觉编码器，没有共注意力或任何花哨的东西（见[图16-13](#clip_diagram)）。实际上，你可以使用几乎任何你想要的文本和视觉编码器，只要它们能够产生文本或图像的向量表示。作者们尝试了各种编码器，包括几个用于视觉的ResNet和ViT模型，以及一个类似GPT-2的文本模型，所有这些模型都是从零开始训练的。你听到我说什么，GPT-2不是一个编码器？这是真的，它是一个仅解码器模型，但我们不是为下一个标记预测进行预训练，所以最后一个标记的输出可以自由地用作整个输入序列的表示，这正是CLIP所做的事情。你可能想知道为什么我们不使用像BERT这样的常规文本编码器？嗯，我们可以，但OpenAI创建了GPT——Alex
    Radford是GPT和CLIP的首席作者——所以这很可能是选择GPT-2的原因：作者们对这个模型有更多的经验，并且已经建立了一个良好的训练基础设施。使用因果编码器还使得在多个文本以相同方式开始时缓存模型的中间状态成为可能；例如，“这是一张瓢虫的照片”。
- en: '![Diagram illustrating the CLIP model with text and vision encoders processing
    image-caption pairs into vectors to match corresponding pairs and differentiate
    mismatched ones.](assets/hmls_1613.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![说明CLIP模型中文本和视觉编码器将图像-文本对处理成向量以匹配对应对并区分不匹配对的图。](assets/hmls_1613.png)'
- en: 'Figure 16-13\. CLIP: a batch of image-caption pairs is encoded as vectors,
    then matching pairs are pulled closer while mismatched pairs are pushed away'
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图16-13\. CLIP：将一批图像-文本对编码为向量，然后匹配对被拉近，不匹配对被推开
- en: Also note that a pooling layer is added on top of the vision encoder to ensure
    it outputs a single vector for the whole image instead of feature maps. Moreover,
    a linear layer is added on top of each encoder to project the final representation
    into the same output space (i.e., with the same number of dimensions). So given
    a batch of *m* image-caption pairs, we get *m* vector representations for the
    images and *m* vector representations for the captions, and all vectors have the
    same number of dimensions. [Figure 16-13](#clip_diagram) shows *m* = 4, but the
    authors used a shockingly large batch size of *m* = 2^(15) = 32,768 during training.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在视觉编码器之上添加了一个池化层，以确保它输出整个图像的单个向量，而不是特征图。此外，在每个编码器之上添加了一个线性层，将最终表示投影到相同的输出空间（即具有相同数量的维度）。因此，给定一个*m*个图像-文本对的批次，我们得到*m*个图像的向量表示和*m*个文本的向量表示，所有向量具有相同数量的维度。[图16-13](#clip_diagram)显示*m*
    = 4，但在训练期间，作者使用了令人震惊的大批次大小*m* = 2^(15) = 32,768。
- en: 'The model was then pretrained on a large dataset of 400 million image-caption
    pairs scraped from the internet, using a contrastive loss⁠^([27](ch16.html#id3874))
    that pulls together the representations of matching pairs, while also pushing
    apart representations of mismatched pairs. Here’s how it works:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，该模型在从互联网上抓取的4亿个图像-文本对的大型数据集上进行了预训练，使用对比损失⁠^([27](ch16.html#id3874))将匹配对的表示拉近，同时将不匹配对的表示推开。以下是它是如何工作的：
- en: 'All vectors are first ℓ[2] normalized, meaning they are rescaled to unit vectors:
    we only care about their orientation, not their length.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有向量首先进行ℓ[2]归一化，这意味着它们被缩放为单位向量：我们只关心它们的方向，而不是它们的长度。
- en: Next, we compute the cosine similarity of the image representation and the text
    representation for every possible image-caption pair. The result is an *m* × *m*
    matrix containing numbers between –1 for opposite vectors, and +1 for identical
    vectors. In [Figure 16-13](#clip_diagram), this matrix is represented by the 4
    × 4 grid (black is +1, white is –1). Each column measures how much each image
    in the batch matches a given caption in the same batch, while each row measures
    how much each caption matches a given image.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们计算每个可能的图像-文本对图像表示和文本表示之间的余弦相似度。结果是包含介于-1（对于相反向量）和+1（对于相同向量）之间的数字的*m* ×
    *m*矩阵。在[图16-13](#clip_diagram)中，这个矩阵由4 × 4网格表示（黑色为+1，白色为-1）。每一列衡量每个批次中的图像与同一批次中给定文本的匹配程度，而每一行衡量每个文本与给定图像的匹配程度。
- en: Since the *i*^(th) image corresponds to the *i*^(th) caption, we want the main
    diagonal of this matrix to contain similarity scores close to +1, while all other
    scores should be close to 0\. Why not close to –1? Well, if an image and a text
    are totally unrelated, we can think of their representations as two random vectors.
    Recall that two random high-dimensional vectors are highly likely to be close
    to orthogonal (as discussed in [Chapter 7](ch07.html#dimensionality_chapter)),
    so their cosine similarity will be close to 0, not –1\. In other words, it makes
    sense to assume that the text and image representations of a mismatched pair are
    unrelated (score close to 0), not opposite (score close to –1).
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于第*i*个图像对应于第*i*个文本，我们希望这个矩阵的主对角线包含接近+1的相似度分数，而所有其他分数应接近0。为什么不接近-1呢？好吧，如果一个图像和文本完全不相关，我们可以将它们的表示视为两个随机向量。回想一下，两个随机的高维向量很可能接近正交（如第7章所述），因此它们的余弦相似度将接近0，而不是-1。换句话说，假设不匹配对的文本和图像表示不相关（分数接近0），而不是相反（分数接近-1）是有意义的。
- en: In the *i*^(th) row, we know that the matching caption is in the *i*^(th) column,
    so we want the model to produce a high similarity score in that column, and a
    low score elsewhere. This resembles a classification task where the target class
    is the *i*^(th) class. Indeed, we can treat each similarity score as class logit
    and simply compute the cross-entropy loss for that row with *i* as the target.
    We can follow the exact same rationale for each column. If we compute the cross-entropy
    loss for each row and each column (using class *i* as the target for the *i*^(th)
    row and the *i*^(th) column), and evaluate the mean, we get the final loss.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第*i*行，我们知道匹配的标题在第*i*列，因此我们希望模型在该列产生一个高相似度分数，在其他地方产生低分数。这类似于一个目标类别为*i*类的分类任务。实际上，我们可以将每个相似度分数视为类logit，并简单地计算该行的交叉熵损失，其中*i*为目标。我们可以对每一列使用完全相同的推理。如果我们为每一行和每一列计算交叉熵损失（使用*i*作为*i*行的目标和*i*列的目标），并计算平均值，我们得到最终的损失。
- en: 'There’s just one extra technical detail: the similarity scores range between
    –1 and +1, which is unlikely to be the ideal logit scale for the task, so CLIP
    divides all the similarity scores by a trainable temperature (a scalar) before
    computing the loss.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有一个额外的技术细节：相似度分数的范围在-1到+1之间，这不太可能是任务的理想logit尺度，因此CLIP在计算损失之前将所有相似度分数除以一个可训练的温度（一个标量）。
- en: Warning
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This loss requires a large batch size to ensure the model sees enough negative
    examples to contrast with the positive examples, or else it could overfit details
    in the positive examples. CLIP’s success is due in part to the gigantic batch
    size that the authors were able to implement.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这个损失需要一个大的批量大小来确保模型看到足够的负例与正例进行对比，否则它可能会过度拟合正例的细节。CLIP的成功部分归因于作者能够实现的巨大批量大小。
- en: The authors evaluated CLIP on many image classification datasets, and for roughly
    60% of these, it performed better without any extra training (i.e., zero-shot)
    than a *linear probe* trained on ResNet-50 features (that’s a linear classifier
    trained on features output by a pretrained and frozen ResNet-50 model), including
    on ImageNet, despite the fact that the ResNet-50 model was actually pretrained
    on ImageNet. CLIP is particularly strong on datasets with few examples per class,
    with pictures of everyday scenes (i.e., the kind of pictures you find on the web).
    In fact, CLIP even beat the state of the art on the Stanford Cars dataset, ahead
    of the best ViTs specifically trained on this dataset, because pictures of cars
    are very common on the web and the dataset doesn’t have many examples per class.
    However, CLIP doesn’t perform as well on domain-specific images, such as satellite
    or medical images.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 作者在许多图像分类数据集上评估了CLIP，对于大约60%的这些数据集，它在没有任何额外训练（即零样本）的情况下表现优于在ResNet-50特征上训练的*线性探针*（这是一个在预训练和冻结的ResNet-50模型输出的特征上训练的线性分类器），包括在ImageNet上，尽管ResNet-50模型实际上是在ImageNet上预训练的。CLIP在每类样本较少的数据集上特别强大，例如日常场景的图片（即你在网上找到的那种图片）。事实上，CLIP甚至在斯坦福汽车数据集上击败了当时最先进的ViTs，因为网上的汽车图片非常常见，而这个数据集每类的样本也不多。然而，CLIP在特定领域的图像上表现不佳，例如卫星或医学图像。
- en: 'Importantly, the visual features output by CLIP are also highly robust to perturbations,
    making them excellent for downstream tasks, such as image retrieval: if you store
    images in a vector database, indexing them by their CLIP-encoded visual features,
    you can then search for them given either a text query or an image query. For
    this, just run the query through CLIP to get a vector representation, then search
    the database for images with a similar representation.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，CLIP输出的视觉特征对扰动也非常鲁棒，这使得它们非常适合下游任务，例如图像检索：如果你将图像存储在向量数据库中，通过它们的CLIP编码视觉特征进行索引，那么你可以通过文本查询或图像查询来搜索它们。为此，只需将查询通过CLIP以获取向量表示，然后在具有相似表示的数据库中搜索图像。
- en: 'To get the text and visual features using the Transformers library, you must
    run the CLIP model directly, without going through a pipeline:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用Transformers库获取文本和视觉特征，你必须直接运行CLIP模型，而不是通过管道：
- en: '[PRE9]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Tip
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you need to encode the images and text separately, you can use the CLIP model’s
    `get_image_features()` and `get_text_features()` methods. You must first tokenize
    the text using a `CLIPTokenizer` and process the images using a `CLIPImageProcessor`.
    The resulting features are not ℓ[2] normalized, so you must divide them by `features.norm(dim=1,
    keepdim=True)` (see the notebook for a code example).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要分别对图像和文本进行编码，可以使用CLIP模型的`get_image_features()`和`get_text_features()`方法。您必须首先使用`CLIPTokenizer`对文本进行分词，并使用`CLIPImageProcessor`处理图像。得到的特征没有经过ℓ[2]归一化，因此您必须将它们除以`features.norm(dim=1,
    keepdim=True)`（请参阅笔记本中的代码示例）。
- en: 'The features are already ℓ[2] normalized, so if you want to compute similarity
    scores, a single matrix multiplication is all you need:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 特征已经经过ℓ[2]归一化，因此如果您想计算相似度分数，只需要进行一次矩阵乘法即可：
- en: '[PRE10]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11] [PRE12]`py  [PRE13]py`` [PRE14]py It’s a good description of the photo,
    but it would nicer without the special tokens, so let’s get rid of them when decoding
    the model’s output:    [PRE15]py`` `...` [PRE16]` [PRE17] `` `Perfect!    ######
    Tip    Also check out InstructBLIP, a BLIP-2 model with vision-language instruction
    tuning.` `` [PRE18][PRE19][PRE20][PRE21][PRE22]  [PRE23]'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE11] [PRE12]`py  [PRE13]py`` [PRE14]py 它是对照片的良好描述，但如果没有特殊标记会更好，所以让我们在解码模型的输出时去掉它们：    [PRE15]py``
    `...` [PRE16]` [PRE17] `` `完美！    ###### 小贴士    还可以查看InstructBLIP，这是一个具有视觉-语言指令调整的BLIP-2模型。`
    `` [PRE18][PRE19][PRE20][PRE21][PRE22]  [PRE23]'
