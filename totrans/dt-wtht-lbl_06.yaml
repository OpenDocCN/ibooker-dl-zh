- en: 5 Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 聚类
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Spectral clustering
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谱聚类
- en: Fuzzy clustering
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模糊聚类
- en: Gaussian mixture models clustering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯混合模型聚类
- en: Out of complexity, find simplicity.—Einstein
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 从复杂中寻找简单。——爱因斯坦
- en: Sometimes life is very simple, and sometimes we experience quite complex situations.
    We sail through both situations and change our approach as needed.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 有时生活非常简单，有时我们却会经历相当复杂的情况。我们可以在这两种情况下航行，并根据需要改变我们的方法。
- en: In part 1, we covered the fundamentals to prepare you for the journey ahead.
    We are now in part 2, which is slightly more complex than part 1\. Part 3 will
    be more advanced than the first two parts. So please give careful attention to
    the coming chapters, as the skills and knowledge gained here will prepare you
    for the later chapters in the book.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，我们介绍了基础知识，为您的旅程做准备。我们现在处于第二部分，它比第一部分稍微复杂一些。第三部分将比前两部分更高级。因此，请仔细关注即将到来的章节，因为在这里获得的技能和知识将为本书后面的章节做好准备。
- en: Before starting this chapter, we should refresh our memory on what we covered
    in chapter 2\. We studied clustering algorithms in part 1 of the book. In chapter
    2, we learned that clustering is an unsupervised learning technique where we wish
    to group the data points by discovering interesting patterns in the datasets.
    We went through the meaning of clustering solutions and different categories of
    clustering algorithms and looked at a case study. In that chapter, we explored
    k-means clustering, hierarchical clustering, and DBSCAN clustering in depth. We
    went through the mathematical background, process, and Python implementation and
    the pros and cons of each algorithm.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始本章之前，我们应该回顾一下我们在第二章中学到的内容。在本书的第一部分，我们研究了聚类算法。在第二章中，我们了解到聚类是一种无监督学习技术，我们希望通过在数据集中发现有趣的模式来对数据点进行分组。我们探讨了聚类解决方案的含义以及不同类别的聚类算法，并查看了一个案例研究。在那个章节中，我们深入探讨了k-means聚类、层次聚类和DBSCAN聚类。我们了解了每个算法的数学背景、过程和Python实现，以及每个算法的优缺点。
- en: 'You may often encounter datasets that do not conform to a simple shape and
    form. Moreover, we have to find the best fit before making a choice of the final
    algorithm we wish to implement. Here we might need help with more complex clustering
    algorithms—the topic of this chapter. In this chapter, we are going to again study
    three such complex clustering algorithms: spectral clustering, fuzzy clustering,
    and Gaussian mixture models (GMM) clustering. As always, Python implementation
    will follow the mathematical and theoretical concepts. This chapter is slightly
    heavy on mathematical concepts. There is no need to be an advanced student of
    mathematics, but it is sometimes important to understand how the algorithms work
    in the background. At the same time, you will be surprised to find that Python
    implementation of such algorithms is not tedious. This chapter does not have a
    case study.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会遇到不符合简单形状和形式的数据库。此外，在做出选择最终要实现的算法之前，我们必须找到最佳匹配。在这种情况下，我们可能需要更复杂的聚类算法的帮助——这正是本章的主题。在本章中，我们将再次研究三种这样的复杂聚类算法：谱聚类、模糊聚类和高斯混合模型（GMM）聚类。像往常一样，Python实现将遵循数学和理论概念。本章在数学概念上稍微有些繁重。不需要成为高级数学学生，但有时理解算法在背后的工作方式是很重要的。同时，您可能会惊讶地发现，这些算法的Python实现并不繁琐。本章没有案例研究。
- en: Welcome to the fifth chapter, and all the very best!
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到第五章，祝您一切顺利！
- en: 5.1 Technical toolkit
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 技术工具箱
- en: We will continue to use the same version of Python and Jupyter Notebook as we
    have used so far. The codes and datasets used in this chapter have been checked
    in at GitHub ([https://mng.bz/6epo](https://mng.bz/6epo)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用迄今为止我们所使用的相同版本的Python和Jupyter Notebook。本章中使用的代码和数据集已在GitHub上检查过（[https://mng.bz/6epo](https://mng.bz/6epo)）。
- en: 'We are going to use the regular Python libraries we have used so far: `numpy`,
    `pandas`, `sklearn`, `seaborn`,  `matplotlib`, etc. You need to install two other
    Python libraries in this chapter: `skfuzzy` and `network`. Using libraries, we
    can implement the algorithms very quickly. Otherwise, coding these algorithms
    is quite a time-consuming and painstaking task.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用迄今为止我们已经使用的常规Python库：`numpy`、`pandas`、`sklearn`、`seaborn`、`matplotlib`等。在本章中，您还需要安装两个其他的Python库：`skfuzzy`和`network`。使用这些库，我们可以非常快速地实现算法。否则，编写这些算法是一项相当耗时且费力的任务。
- en: Let’s get started with a refresh of clustering!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从对聚类的回顾开始吧！
- en: '5.2 Clustering: A brief recap'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 聚类：简要回顾
- en: Recall from chapter 2, clustering is used to group similar objects or data points.
    It is an unsupervised learning technique where we intend to find natural grouping
    in the data, as shown in figure 5.1.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从第二章回顾，聚类用于将相似的对象或数据点分组。这是一种无监督学习技术，我们的目的是在数据中找到自然的分组，如图5.1所示。
- en: Here, we can observe that on the left side, we have ungrouped data, and on the
    right side, the data points have been grouped into logical groups. We can also
    observe that there can be two methodologies to do the grouping or clustering,
    and both result in different clusters. Clustering as a technique is quite heavily
    used in business solutions like customer segmentation, market segmentation, etc.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以观察到左侧是未分组的数据，而右侧数据点已经被分组到逻辑组中。我们还可以观察到有两种方法可以进行分组或聚类，并且两者都会产生不同的簇。作为一项技术，聚类在商业解决方案中得到了广泛的应用，如客户细分、市场细分等。
- en: We learned about k-means and hierarchical and DBSCAN clustering in chapter 2\.
    We also covered various distance measurement techniques and indicators to measure
    the performance of clustering algorithms. You are advised to revisit the concepts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第二章学习了k-means、层次聚类和DBSCAN聚类。我们还介绍了各种距离测量技术以及衡量聚类算法性能的指标。建议您重新回顾这些概念。
- en: In this chapter, we focus on advanced clustering methods. We start with spectral
    clustering in the next section.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于高级聚类方法。下一节我们将从谱聚类开始。
- en: '![figure](../Images/CH05_F01_Verdhan.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F01_Verdhan.png)'
- en: Figure 5.1 Clustering of objected results into natural grouping
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.1 对象结果聚类成自然分组
- en: 5.3 Spectral clustering
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 谱聚类
- en: Spectral clustering is one of the unique clustering algorithms, and a lot of
    research has been done in this field. Revered researchers include Prof. Andrew
    Yang, Prof. Michael Jordan, Prof. Yair Weiss, Prof. Jianbo Shi, and Prof. Jitendra
    Malik, to name a few. We provide links to some of their papers at the end of the
    chapter.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类是一种独特的聚类算法，该领域已经进行了大量的研究。尊敬的研究者包括杨安德鲁教授、迈克尔·乔丹教授、亚伊尔·维斯教授、石建波教授和吉滕德拉·马利克教授等。我们在本章末尾提供了他们一些论文的链接。
- en: Spectral clustering works on the affinity and not the absolute location of the
    data points for clustering. When we consider the absolute location of the points,
    the similarity is simply based on the distances between the points, whereas affinity
    considers the similarity between the points. If the affinity is 0 between the
    points, they are dissimilar, whereas if the affinity is 1, they are very similar.
    Hence, wherever the data is in complicated shapes (i.e., some kind of special
    relationship exists between the data points), spectral clustering is the answer.
    We show a few examples in figure 5.2 where spectral clustering can provide a logical
    solution.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类在聚类数据点时考虑的是亲和力而非数据点的绝对位置。当我们考虑点的绝对位置时，相似性仅仅是基于点之间的距离，而亲和力考虑的是点之间的相似性。如果点之间的亲和力为0，则它们是不相似的，而如果亲和力为1，则它们是非常相似的。因此，无论数据处于何种复杂形状（即数据点之间存在某种特殊关系），谱聚类都是解决方案。我们在图5.2中展示了几个谱聚类可以提供逻辑解决方案的示例。
- en: '![figure](../Images/CH05_F02_Verdhan.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F02_Verdhan.png)'
- en: Figure 5.2 Examples of various complex data shapes that can be clustered using
    spectral clustering
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.2 谱聚类可以聚类的各种复杂数据形状示例
- en: For figure 5.2, we could have used other algorithms like k-means clustering
    too. But they might not be able to do justice to such complicated shapes of data.
    You can see from figure 5.2 that the various data points are in a certain pattern.
    Algorithms like k-means clustering utilize the compactness of the data points
    and are driven by centroids of the respective clusters. In other words, the closeness
    of the points to each other and compactness toward the cluster center drive the
    clustering in k-means. On the other hand, in spectral clustering, *connectivity*
    is the driving logic. In connectivity, either the data points are immediately
    close to one another or they are connected in some way. Some examples of such
    connectivity-based clustering are depicted in figure 5.2\. The points in the inner
    circle belong to one cluster while those in the outer circle belong to another
    cluster.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图5.2，我们也可以使用其他算法，如k-means聚类。但它们可能无法公正地处理这样复杂的数据形状。您可以从图5.2中看到，各种数据点遵循某种模式。像k-means聚类这样的算法利用数据点的紧凑性，并由各个簇的中心点驱动。换句话说，点之间的接近性和对簇中心的紧凑性驱动了k-means聚类。另一方面，在谱聚类中，*连通性*是驱动逻辑。在连通性中，数据点要么彼此立即接近，要么以某种方式连接。图5.2中展示了基于连通性的聚类的一些示例。内圈中的点属于一个簇，而外圈中的点属于另一个簇。
- en: Now look at the first diagram in figure 5.3, where the data points are in a
    doughnut pattern. There can be data points that follow this doughnut pattern.
    We need to cluster this data, and it is indeed a complex pattern. Imagine that
    by using a clustering method, the circles inside a square are made a part of the
    same cluster, which is shown in the middle diagram in figure 5.3\. After all,
    they are close to each other. But if we look closely, the points are in a circle
    and in a pattern, and hence, the actual cluster should be as shown in the far
    right diagram in figure 5.3\.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看看图5.3中的第一个图，其中数据点呈环形模式。可能有遵循这种环形模式的数据点。我们需要聚类这些数据，这确实是一个复杂的模式。想象一下，通过使用聚类方法，方框内的圆圈被纳入同一个簇，如图5.3中间的图所示。毕竟，它们彼此接近。但如果我们仔细观察，点呈圆形和某种模式，因此实际的簇应该如图5.3最右边的图所示。
- en: '![figure](../Images/CH05_F03_Verdhan.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F03_Verdhan.png)'
- en: Figure 5.3 We can have a complex representation of data points that need to
    be clustered. Observe the doughnut shape (left). An explanation can be that the
    dots in a square are a part of the same cluster as what would be based on the
    distance only, but clearly, they are not part of the same cluster (middle). We
    have two circles here. The points in the inner circle belong to one cluster, whereas
    the outer points belong to another cluster (right).
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3 我们可以对需要聚类的数据点进行复杂表示。观察左边的环形形状。一种解释可以是，方框内的点基于距离应该是同一个簇的一部分，但显然它们不是同一个簇的一部分（中间）。这里有两个圆圈。内圈中的点属于一个簇，而外圈中的点属于另一个簇（右边）。
- en: The example shown in figure 5.3 depicts the advantages of spectral clustering
    as opposed to k-means clustering. In the second figure, the dots in red (those
    in the square in the print book) will be incorrectly clustered into a different
    cluster, and in the third figure, the correct clustering is shown. Spectral clustering
    may group the data from the inner circle in a separate cluster.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3所示的示例展示了谱聚类相对于k-means聚类的优势。在第二幅图中，红色点（印刷书中方框内的点）将被错误地聚类到不同的簇中，而在第三幅图中显示了正确的聚类。谱聚类可能会将内圈的数据分组到单独的簇中。
- en: As we said earlier, spectral clustering utilizes the connectivity approach.
    In spectral clustering, data points that are immediately next to each other are
    identified in a graph. These data points are sometimes referred to as *nodes*.
    These data points or nodes are then mapped to a low-dimensional space. A low-dimensional
    space is one that has a fewer number of input features. During this process, spectral
    clustering uses eigenvalues, affinity matrix, Laplacian matrix, and degree matrix
    derived from the dataset. The low-dimensional space can then be segregated into
    clusters.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所述，谱聚类利用了连通性方法。在谱聚类中，在图中识别出相邻的数据点。这些数据点有时被称为*节点*。然后，这些数据点或节点被映射到低维空间。低维空间是指具有较少输入特征的空间。在这个过程中，谱聚类使用从数据集中派生的特征值、亲和矩阵、拉普拉斯矩阵和度矩阵。然后，低维空间可以被分割成簇。
- en: NOTE  Spectral clustering utilizes the connectivity approach for clustering.
    It relies on graph theory, wherein we identify clusters of nodes based on the
    edges connecting them.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：谱聚类利用连接方法进行聚类。它依赖于图论，其中我们根据连接它们的边来识别节点簇。
- en: We will study the process in detail. But first, there are a few important mathematical
    concepts that form the foundation of spectral clustering, which we will cover
    now.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将详细研究这个过程。但首先，有一些重要的数学概念构成了谱聚类的基础，我们将现在介绍这些概念。
- en: 5.3.1 Building blocks of spectral clustering
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 谱聚类的构建块
- en: We know that the goal of clustering is to group data points that are similar
    into one cluster and the data points that are not similar into another. One important
    mathematical concept is similarity graphs, which are a representation of data
    points.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，聚类的目标是把相似的数据点分组到一个簇中，把不相似的数据点分组到另一个簇中。一个重要的数学概念是相似性图，它是数据点的表示。
- en: Similarity graphs
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 相似性图
- en: A graph is one of the intuitive methods to represent data points. The first
    diagram in figure 5.4 shows an example of a graph that is simply a connection
    between data points represented by the edge. Two data points are connected if
    the similarity between them is positive or it is above a certain threshold, which
    is shown in the second diagram. Instead of absolute values for the similarity,
    we can use weights. So in the second diagram in figure 5.4, as point 1 and 2 are
    similar compared to points 1 and 3, the connection between points 1 and 2 has
    a higher weight than points 1 and 3\.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图形是表示数据点的直观方法之一。图5.4中的第一个图表展示了一个图形的例子，它仅仅是表示数据点的边之间的连接。如果两个数据点之间的相似性为正或者超过某个阈值，这两个数据点就是相连的，这在第二个图表中有展示。我们不仅可以使用相似性的绝对值，还可以使用权重。因此，在图5.4的第二个图表中，由于点1和点2比点1和点3更相似，点1和点2之间的连接权重比点1和点3之间的连接权重更高。
- en: '![figure](../Images/CH05_F04_Verdhan.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F04_Verdhan.png)'
- en: Figure 5.4 A graph is a simple representation of data points. The points or
    nodes are connected by edges if they are very similar (left). The weight is higher
    if the similarity between data points is high; for dissimilar data points, the
    weight is less (right).
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4 图形是数据点的简单表示。如果点或节点非常相似，它们将通过边连接（左）。如果数据点之间的相似性高，权重就高；对于不相似的数据点，权重就低（右）。
- en: So, we can say that, using similarity graphs, we wish to cluster the data points
    such that the edges of the data points have
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以这样说，使用相似性图，我们希望将数据点聚类，使得数据点的边具有
- en: Higher weight values and hence are similar to each other and so are in the same
    cluster
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较高的权重值因此彼此相似，并且因此属于同一个簇
- en: Lower values of weight and hence are not similar to each other and so are in
    different clusters
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较低的权重值，因此彼此不相似，并且因此属于不同的簇
- en: Apart from similarity graphs, we should also know the concept of eigenvalues
    and eigenvectors, which we covered in detail in the previous chapter. You are
    advised to refresh your memory on it should you need to.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 除了相似性图之外，我们还应该了解特征值和特征向量的概念，这些我们在上一章中已经详细介绍了。如果你需要的话，建议你复习一下。
- en: Adjacency matrix
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 邻接矩阵
- en: 'Have a close look at figure 5.5\. We can see those various points from 1 to
    5 are connected. We represent the connection in a matrix. That matrix is called
    an *adjacency matrix*. In an adjacency matrix, the rows and columns are the respective
    nodes. The values inside the matrix represent the connection: if the value is
    0, that means there is no connection, and if the value is 1, it means there is
    a connection.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察图5.5。我们可以看到从1到5的这些不同点都是相连的。我们用矩阵来表示这种连接。这个矩阵被称为**邻接矩阵**。在邻接矩阵中，行和列分别是相应的节点。矩阵内的值代表连接：如果值是0，这意味着没有连接，如果值是1，这意味着有连接。
- en: '![figure](../Images/CH05_F05_Verdhan.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F05_Verdhan.png)'
- en: Figure 5.5 An adjacency matrix represents the connection between various nodes.
    There is a connection between node 1 and node 5; hence the value is 1\. There
    is no connection between node 1 and node 4; hence the corresponding value is 0.
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5 邻接矩阵表示了各个节点之间的连接。节点1和节点5之间存在连接；因此，对应的值是1。节点1和节点4之间没有连接；因此，相应的值是0。
- en: So, for an adjacency matrix, we are only concerned if there is a connection
    between two data points. With the way that we are defining the edges (as nonoriented),
    the matrix is always symmetric. This is because if there is a connection from
    1 to 2, there must also be a connection from 2 to 1, and if there is no connection
    between 3 and 1, there is no connection between 1 and 3 either. If we extend the
    concept of the adjacency matrix, we get a degree matrix, which is our next topic.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于邻接矩阵，我们只关心两个数据点之间是否有连接。按照我们定义边的方式（作为非定向的），矩阵总是对称的。这是因为如果从1到2有连接，那么从2到1也必须有连接，如果没有3和1之间的连接，那么1和3之间也没有连接。如果我们扩展邻接矩阵的概念，我们得到度矩阵，这是我们接下来要讨论的主题。
- en: Degree matrix
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 度矩阵
- en: A degree matrix is a diagonal matrix, where the degree of a node along the diagonal
    is the number of edges connected to it. If we use the same example as previously,
    we get the degree matrix shown in figure 5.6\. Nodes 3 and 5 have three connections
    each, so they have values of 3 along the diagonal; the other nodes have only two
    connections each, so they have 2 as the value along the diagonal.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 度矩阵是一个对角矩阵，其中对角线上的节点度数是与之相连的边的数量。如果我们使用之前相同的例子，我们得到图5.6所示的度矩阵。节点3和5各有三个连接，所以它们在对角线上的值是3；其他节点每个只有两个连接，所以它们在对角线上的值是2。
- en: '![figure](../Images/CH05_F06_Verdhan.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F06_Verdhan.png)'
- en: Figure 5.6 While an adjacency matrix represents the connection between various
    nodes, a degree matrix is for the number of connections each node has. It is shown
    on the diagonal of the matrix. For example, node 5 has three connections and hence
    has a value of 3 in the adjacency matrix, while node 1 has only two connections
    and so has a value of 2.
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6 虽然邻接矩阵表示了各个节点之间的连接，但度矩阵是表示每个节点连接数的。它显示在矩阵的对角线上。例如，节点5有三个连接，因此在邻接矩阵中的值是3，而节点1只有两个连接，所以它的值是2。
- en: 'You might be wondering: Why do we use these matrices? Matrices provide an elegant
    representation of the data and can clearly depict the relationships between two
    points. Also, computers can more easily deal with matrix representation than alternative
    ways for manipulating the graph.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想：我们为什么要使用这些矩阵呢？矩阵提供了数据的一种优雅的表示形式，并且可以清楚地描绘两点之间的关系。此外，计算机更容易处理矩阵表示，而不是其他操作图的方法。
- en: Now that we have covered both the adjacency matrix and degree matrix, we can
    move to the Laplacian matrix.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了邻接矩阵和度矩阵，我们可以转向拉普拉斯矩阵。
- en: Laplacian matrix
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 拉普拉斯矩阵
- en: There are quite a few variants of the Laplacian matrix, but if we take the simplest
    form, it is nothing but a subtraction of the adjacency matrix from the degree
    matrix—in other words, L = D – A. We can demonstrate it as shown in figure 5.7.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉斯矩阵有许多变体，但如果我们采用最简单的形式，它不过是邻接矩阵从度矩阵中减去——换句话说，L = D – A。我们可以像图5.7中所示的那样演示它。
- en: '![figure](../Images/CH05_F07_Verdhan.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F07_Verdhan.png)'
- en: Figure 5.7 The Laplacian matrix is quite simple to understand. To get a Laplacian
    matrix, we can simply subtract an adjacency matrix from the degree matrix as shown
    in the example here. Here, D represents the degree matrix, A is the adjacency
    matrix, and L is the Laplacian matrix.
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.7 拉普拉斯矩阵非常容易理解。要得到拉普拉斯矩阵，我们可以简单地从度矩阵中减去邻接矩阵，如图例所示。在这里，D代表度矩阵，A是邻接矩阵，L是拉普拉斯矩阵。
- en: 'The Laplacian matrix is an important concept, and we use the eigenvalues of
    L to develop spectral clustering. Once we get the eigenvalues and eigenvectors,
    we can define two other values: spectral gap and Fielder value. The very first
    nonzero eigenvalue is the *spectral gap,* which defines the density of the graph.
    The *Fielder value* is the second eigenvalue; it provides an approximation of
    the minimum cut required to separate the graph into two components. The corresponding
    vector for the Fielder value is called the *Fielder vector*.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉斯矩阵是一个重要的概念，我们利用L的特征值来发展谱聚类。一旦我们得到了特征值和特征向量，我们还可以定义另外两个值：谱间隙和Fielder值。第一个非零特征值是**谱间隙**，它定义了图的密度。**Fielder值**是第二个特征值；它提供了将图分割成两个部分所需的最小割的近似。与Fielder值对应的向量被称为**Fielder向量**。
- en: NOTE  The Fielder vector has both negative and positive components, and their
    resultant sum is zero.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Fielder向量既有负分量也有正分量，它们的代数和为零。
- en: We will use this concept once we study the process of spectral clustering in
    detail in the next section. We cover one more concept—the affinity matrix—before
    moving on to the process of spectral clustering.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们详细研究下一节中光谱聚类的过程时，我们将使用这个概念。在继续到光谱聚类的过程之前，我们再介绍一个概念——亲和矩阵。
- en: Affinity matrix
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 亲和矩阵
- en: In the adjacency matrix, if we replace the number of connections with the similarity
    of the weights, we will get the affinity matrix. If the points are completely
    dissimilar, the affinity will be 0; if they are completely similar, the affinity
    will be 1\. The values in the matrix represent different levels of similarity
    between data points.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在邻接矩阵中，如果我们用权重的相似度代替连接的数量，我们将得到亲和矩阵。如果点完全不相似，亲和度将为0；如果它们完全相似，亲和度将为1。矩阵中的值表示数据点之间相似度的不同级别。
- en: Exercise 5.1
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习5.1
- en: 'Answer these questions to check your understanding:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题以检查你的理解：
- en: The degree matrix is created by counting the number of connections. True or
    False?
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 度矩阵是通过计算连接的数量来创建的。对还是错？
- en: Laplacian is a transpose of the division of degree and adjacency matrix. True
    or False?
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拉普拉斯矩阵是度矩阵和邻接矩阵除法的转置。对还是错？
- en: Draw a graph on paper and then derive its adjacency and degree matrix.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在纸上画一个图，然后推导出其邻接矩阵和度矩阵。
- en: 5.3.2 The process of spectral clustering
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 光谱聚类过程
- en: 'Now we have covered all the building blocks for spectral clustering. At a high
    level, the various steps can be noted as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了光谱聚类的所有构建块。从高层次来看，各个步骤可以总结如下：
- en: We get the dataset and calculate its degree matrix and adjacency matrix.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们获取数据集并计算其度矩阵和邻接矩阵。
- en: Using them, we calculate the Laplacian matrix.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用它们，我们计算拉普拉斯矩阵。
- en: Then we calculate the first *k* eigenvectors of the Laplacian matrix. The *k*
    eigenvectors are the ones that correspond to the *k* smallest eigenvalues.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们计算拉普拉斯矩阵的前k个特征向量。k个特征向量对应于k个最小的特征值。
- en: The resultant matrix formed is used to cluster the data points in k-dimensional
    space.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 形成的矩阵用于在k维空间中对数据点进行聚类。
- en: NOTE  For more clarity on eigenvalues, the affinity matrix, and the Laplacian
    matrix, refer to the appendix.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：关于特征值、亲和矩阵和拉普拉斯矩阵的更多清晰信息，请参阅附录。
- en: We cover the process of spectral clustering using an example, as shown in figure
    5.8\. These steps are generally not done in real-world implementation, as we have
    packages and libraries to achieve them, but they are covered here to give you
    an idea of how the algorithm can be developed from scratch and how it works so
    that you have a better understanding on how to effectively utilize it. For the
    Python solution, we will use the libraries and packages only. Though it is possible
    to develop an implementation from scratch, it is not time-efficient to reinvent
    the wheel.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个示例来介绍光谱聚类的过程，如图5.8所示。这些步骤在现实世界的实现中通常不会执行，因为我们有包和库来实现这些功能，但在这里介绍它们是为了让你了解算法是如何从头开始开发的，以及它是如何工作的，以便你更好地理解如何有效地利用它。对于Python解决方案，我们将仅使用库和包。虽然从头开始开发实现是可能的，但重新发明轮子并不节省时间。
- en: '![figure](../Images/CH05_F08_Verdhan.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F08_Verdhan.png)'
- en: Figure 5.8 Consider the example shown where we have some data points and they
    are connected. We will perform spectral clustering on this data.
  id: totrans-79
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.8 考虑图5.8所示的示例，其中我们有一些数据点，并且它们是连接的。我们将对此数据进行光谱聚类。
- en: 'When we wish to perform the spectral clustering on this data, we follow these
    steps:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们希望对此数据进行光谱聚类时，我们将遵循以下步骤：
- en: Create the adjacency matrix and degree matrix. We will leave this step up to
    you.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建邻接矩阵和度矩阵。我们将把这个步骤留给你。
- en: Create the Laplacian matrix (see figure 5.9).
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建拉普拉斯矩阵（见图5.9）。
- en: '![figure](../Images/CH05_F09_Verdhan.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F09_Verdhan.png)'
- en: Figure 5.9 The Laplacian matrix of the data. You are advised to create the degree
    and adjacency matrix and check the output.
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.9 数据的拉普拉斯矩阵。建议你创建度矩阵和邻接矩阵，并检查输出。
- en: 3\. Create the Fielder vector, as shown in figure 5.10, for the preceding Laplacian
    matrix. We create the Fielder vector as described in the Laplacian Matrix section.
    Observe how the sum of the matrix is zero.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 创建Fielder向量，如图5.10所示，用于前面的拉普拉斯矩阵。我们创建Fielder向量，如拉普拉斯矩阵部分所述。观察矩阵的总和为零。
- en: '![figure](../Images/CH05_F10_Verdhan.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F10_Verdhan.png)'
- en: Figure 5.10 The Fielder vector is the output for the Laplacian matrix.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.10 Fielder向量是拉普拉斯矩阵的输出。
- en: 4\. We can see that there are a few positive values and a few negative values.
    Based on the positive or negative values, we can create two distinct clusters.
    Figure 5.11 illustrates the process of spectral clustering.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 我们可以看到有一些正值和一些负值。基于这些正值或负值，我们可以创建两个不同的簇。图5.11展示了谱聚类的过程。
- en: '![figure](../Images/CH05_F11_Verdhan.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F11_Verdhan.png)'
- en: Figure 5.11 The two clusters are identified. This is a very simple example to
    illustrate the process of spectral clustering.
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.11 识别出两个簇。这是一个非常简单的例子，用于说明谱聚类的过程。
- en: Spectral clustering is useful for image segmentation, speech analysis, text
    analytics, entity resolution, etc. The method does not make any assumptions about
    the shape of the data. Methods like k-means assume that the points are in a spherical
    form around the center of the cluster, whereas there is no such strong assumption
    in spectral clustering.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类在图像分割、语音分析、文本分析、实体解析等方面非常有用。这种方法不对数据的形状做出任何假设。像k-means这样的方法假设点围绕簇的中心呈球形分布，而谱聚类中并没有这样的强假设。
- en: Another significant difference is that in spectral clustering the data points
    need not have convex boundaries as compared to other methods where compactness
    drives clustering. Spectral clustering is sometimes slow since various matrices
    and their eigenvalues, Laplacians, etc., have to be calculated. With a large dataset,
    the complexity increases, and hence, spectral clustering can become slow, but
    it is a fast method when we have a sparse dataset.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个显著的不同之处在于，与那些以紧凑性驱动聚类的其他方法相比，在谱聚类中，数据点不需要有凸边界。由于需要计算各种矩阵及其特征值、拉普拉斯矩阵等，谱聚类有时会较慢。对于大数据集，复杂性增加，因此谱聚类可能会变慢，但当我们拥有稀疏数据集时，它是一种快速的方法。
- en: Spectral clustering requires building a matrix that nominally has the size of
    the number of items in a dataset squared because there is one column and one row
    for each element. For example, a modest dataset of a few million elements will
    require a matrix of several trillion elements! Storing that matrix verbatim requires
    terabytes of RAM and is something that is at the edge of what a very powerful
    and expensive server could do. There are techniques to mitigate the memory needs
    (such as not storing every single element separately), but they make working with
    the matrix more complicated. Moreover, finding the eigenvalues and even one eigenvector
    of such a large matrix is very time-intense. As such, spectral clustering is a
    viable approach generally for small datasets.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类需要构建一个矩阵，其大小理论上等于数据集中元素数量的平方，因为每个元素都有一个列和一个行。例如，一个包含几百万个元素的中等数据集将需要一个包含数万亿元素的矩阵！直接存储这样一个矩阵需要数TB的RAM，这是非常强大且昂贵的服务器所能做到的边缘。有一些技术可以减轻内存需求（例如，不单独存储每个元素），但它们使得与矩阵的工作变得更加复杂。此外，找到这样一个大矩阵的特征值甚至一个特征向量都是非常耗时的。因此，对于小数据集，谱聚类是一种可行的解决方案。
- en: We will now proceed to the Python solution of the spectral clustering algorithm.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将进入对谱聚类算法的Python解决方案。
- en: 5.4 Python implementation of spectral clustering
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 谱聚类的Python实现
- en: 'We have covered the details of spectral clustering—it is time to get into the
    code. For this, we will create an artificial dataset and run a k-means algorithm
    and then spectral clustering to compare the results. The steps are as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了谱聚类的细节——现在是时候进入代码部分了。为此，我们将创建一个人工数据集，运行k-means算法和谱聚类来比较结果。步骤如下：
- en: 'Import all the necessary libraries. These libraries are standard, except for
    a few that we will cover. `sklearn` is one of the most famous and sought-after
    libraries, and from `sklearn` we import `SpectralClustering`, `make_blobs`, and
    `make_circles`:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的库。这些库是标准的，除了我们将要介绍的一些。`sklearn`是最著名和最受欢迎的库之一，我们从`sklearn`中导入`SpectralClustering`、`make_blobs`和`make_circles`：
- en: '[PRE0]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '2\. Curate a dataset. We will use the `make_circles` method. Here, we take
    2,000 samples and represent them in a circle. The output is as follows (see figure
    5.12):'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 整理数据集。我们将使用`make_circles`方法。在这里，我们取2000个样本并将它们表示为一个圆圈。输出如下（见图5.12）：
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![figure](../Images/CH05_F12_Verdhan.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F12_Verdhan.png)'
- en: Figure 5.12 Curating a dataset using the `make_circles` method
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.12 使用`make_circles`方法整理数据集
- en: '3\. Test this dataset with k-means clustering. The two colors show two different
    clusters, which overlap each other. The print version of the book will not show
    the colors, but the output of the Python code will. The same output is available
    in the GitHub repository (see figure 5.13):'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 使用k-means聚类测试这个数据集。两种颜色表示两个不同的簇，它们相互重叠。本书的打印版不会显示颜色，但Python代码的输出会显示。相同的输出可以在GitHub仓库中找到（见图5.13）：
- en: '[PRE2]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![figure](../Images/CH05_F13_Verdhan.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F13_Verdhan.png)'
- en: Figure 5.13 Testing the dataset with k-means clustering
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.13 使用k-means聚类测试数据集
- en: '4\. Run the same data with spectral clustering. We find that the two clusters
    are being handled separately here (see figure 5.14):'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 使用谱聚类运行相同的数据。我们发现这里（见图5.14）两个簇被分别处理：
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![figure](../Images/CH05_F14_Verdhan.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F14_Verdhan.png)'
- en: Figure 5.14 The two clusters are being handled separately when using spectral
    clustering.
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.14 在使用谱聚类时，两个簇被分别处理。
- en: We can observe here that the same dataset is handled differently by the two
    algorithms. Spectral clustering handles the dataset arguably better, as the circles
    that are separate are depicted separately.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，两个算法以不同的方式处理相同的数据集。谱聚类在处理数据集方面更有优势，因为分离的圆被单独表示。
- en: 5\. Simulate various cases by changing the values in the dataset and running
    the algorithms. Observe the different outputs for comparison.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 通过改变数据集中的值并运行算法来模拟各种情况。观察不同的输出以进行比较。
- en: 5.5 Fuzzy clustering
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 模糊聚类
- en: 'So far we have covered quite a few clustering algorithms. Did you wonder why
    a data point should belong to only one cluster? Why can’t a data point belong
    to more than one cluster? Have a look at figure 5.15: the red points in the right
    image (shown with an x in the print version) can belong to more than one cluster.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了很多聚类算法。你是否想知道为什么数据点应该只属于一个簇？为什么数据点不能属于多个簇？看看图5.15：右侧图像中的红色点（打印版本中用x表示）可以属于多个簇。
- en: '![figure](../Images/CH05_F15_Verdhan.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F15_Verdhan.png)'
- en: Figure 5.15 The figure on the left represents all the data points. The red points
    (those with an x in the print version) can belong to more than one cluster. In
    fact, we can allocate more than one cluster to each point. A probability score
    can be given for a point to belong to a particular cluster.
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.15 左侧的图表示所有数据点。红色点（打印版本中带有x的点）可以属于多个簇。实际上，我们可以为每个点分配多个簇。可以为点分配一个概率分数，以表示它属于特定的簇。
- en: We know that clustering is used to group items in cohesive groups based on their
    similarities. The items that are similar are in one cluster, whereas the items
    that are dissimilar are in different clusters. The idea of clustering is to ensure
    the items in the same cluster are similar. When the items can be only in one cluster,
    it is called *hard clustering.* K-means clustering is a classic example of hard
    clustering. But if we reflect on figure 5.15, we can observe that an item can
    belong to more than one cluster. This is called *soft clustering.*
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，聚类是根据相似性将项目分组到凝聚性组中的。相似的项目在一个簇中，而不相似的项目在不同的簇中。聚类的想法是确保同一簇中的项目相似。当项目只能属于一个簇时，这被称为*硬聚类*。K-means聚类是硬聚类的经典例子。但如果我们回顾图5.15，我们可以观察到项目可以属于多个簇。这被称为*软聚类*。
- en: NOTE  It is computationally cheaper to create fuzzy boundaries than to create
    hard clusters.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：创建模糊边界比创建硬簇在计算上更便宜。
- en: In fuzzy clustering, an item can be assigned to more than one cluster. The items
    that are closer to the center of a cluster will have a stronger belongingness
    to that cluster as compared to the points that are at the edge of the cluster.
    This is referred to as *membership*. It employs the least-square algorithm to
    find the most optimal location of an item. The optimal location that we derive
    from the least-square algorithm will be the probability space between two or more
    clusters. We will examine this concept in detail later.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在模糊聚类中，一个项目可以被分配到多个簇中。与位于簇边缘的点相比，靠近簇中心的点将对该簇有更强的归属感。这被称为*隶属度*。它使用最小二乘算法来找到项目最理想的位置。我们从最小二乘算法中得出的理想位置将是两个或多个簇之间的概率空间。我们将在稍后详细探讨这个概念。
- en: 5.5.1 Types of fuzzy clustering
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.1 模糊聚类的类型
- en: Fuzzy clustering can be further divided into classical fuzzy algorithms and
    shape-based fuzzy algorithms. See figure 5.16\.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 模糊聚类可以进一步分为经典模糊算法和基于形状的模糊算法。见图 5.16。
- en: '![figure](../Images/CH05_F16_Verdhan.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F16_Verdhan.png)'
- en: Figure 5.16 Fuzzy algorithms can be divided into the classical fuzzy algorithm
    and the shape-based fuzzy algorithm.
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.16 模糊算法可以分为经典模糊算法和基于形状的模糊算法。
- en: 'We will cover the fuzzy c-means (FCM) algorithm in detail next, but first we
    will review the rest of the algorithms briefly:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节详细介绍模糊 c-均值（FCM）算法，但首先我们将简要回顾其他算法：
- en: The Gustafson-Kessel algorithm, sometimes called the GK algorithm, works by
    associating an item with a cluster and a matrix. GK results in elliptical clusters,
    and to modify as per varied structures in the datasets, GK uses the covariance
    matrix. It allows the algorithm to capture the elliptical properties of the cluster.
    GK can result in narrower clusters, and wherever the number of items is higher,
    those areas can be thinner.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gustafson-Kessel 算法，有时称为 GK 算法，通过将一个项目与一个簇和一个矩阵关联起来来工作。GK 导致椭圆形簇，为了根据数据集中的不同结构进行修改，GK
    使用协方差矩阵。它允许算法捕捉簇的椭圆形特性。GK 可以导致更窄的簇，并且当项目数量更多时，这些区域可以更薄。
- en: The Gath-Geva algorithm is not based on an objective function. The clusters
    can result in any shape, because it is a fuzzification of statistical estimators.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gath-Geva 算法不是基于目标函数的。簇可以形成任何形状，因为它是对统计估计量的模糊化。
- en: The shape-based clustering algorithms are self-explanatory as per their names.
    A circular fuzzy clustering algorithm will result in circular-shaped clusters
    and so on.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于形状的聚类算法根据其名称是自解释的。一个圆形模糊聚类算法将导致圆形簇，依此类推。
- en: The FCM algorithm is the most popular fuzzy clustering algorithm. It was initially
    developed in 1973 by J.C. Dunn, and it has been improved multiple times. It is
    quite similar to k-means clustering.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: FCM 算法是最受欢迎的模糊聚类算法。它最初由 J.C. Dunn 在 1973 年开发，并且已经多次改进。它与 k-means 聚类非常相似。
- en: Refer to figure 5.17\. In the first part of the figure (left), we have some
    items or data points. These data points can be a part of a clustering dataset
    like customer transactions, etc. In the second part of the figure (middle), we
    create a cluster for these data points. While this cluster is created, membership
    grades are allocated to each of the data points. These membership grades suggest
    the degree or the level to which a data point belongs to a cluster. We will shortly
    examine the mathematical function to calculate these values.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 参考图 5.17。在图的第一个部分（左侧），我们有某些项目或数据点。这些数据点可以是聚类数据集的一部分，如客户交易等。在图的第二部分（中间），我们为这些数据点创建一个簇。在创建这个簇的同时，为每个数据点分配隶属度等级。这些隶属度等级表明数据点属于簇的程度或水平。我们将很快检查计算这些值的数学函数。
- en: TIP  Do not get confused by the degree and the probabilities. If we sum these
    degrees, we may not get 1, as these values are normalized between 0 and 1 for
    all the items.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: TIP  不要被度数和概率弄混淆。如果我们把这些度数加起来，我们可能不会得到 1，因为这些值对于所有项目都是归一化的，介于 0 和 1 之间。
- en: In the third part of the figure (right), we can see that point 1 is closer to
    the cluster center and thus belongs to the cluster to a higher degree than point
    2, which is closer to the boundary or the edge of the cluster.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在图的三部分（右侧）中，我们可以看到点 1 更接近簇中心，因此比点 2 更有可能属于簇，点 2 更接近簇的边界或边缘。
- en: '![figure](../Images/CH05_F17_Verdhan.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F17_Verdhan.png)'
- en: Figure 5.17 Data points that can be clustered (left). The data points can be
    grouped into two clusters. For the first cluster, the cluster centroid is represented
    using a + sign (middle). Point 1 is much closer to the cluster center as compared
    to point 2\. So we can conclude that point 1 belongs to this cluster to a higher
    degree than cluster 2.
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.17 可聚类的数据点（左侧）。数据点可以分成两个簇。对于第一个簇，簇中心用加号表示（中间）。与点 2 相比，点 1 更接近簇中心。因此，我们可以得出结论，点
    1 比簇 2 更有可能属于这个簇。
- en: We will now venture into the technical details of the algorithm. This can get
    a little mathematically heavy.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将深入探讨算法的技术细节。这可能会变得有点数学化。
- en: 'Consider we have a set of *n* items (equation 5.1):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑我们有一组 *n* 个项目（方程式 5.1）：
- en: (5.1)
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: （5.1）
- en: '*x* = {*x*[1], *x*[2], *x*[3], *x*[4], *x*[5], . . ., *x*[*n*]}'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*x* = {*x*[1], *x*[2], *x*[3], *x*[4], *x*[5], . . ., *x*[*n*]}'
- en: 'We apply the FCM algorithm to these items. These *n* items are clustered into
    *c* fuzzy clusters based on some criteria. Let’s say that we will get from the
    algorithm a list of *c* cluster centers (equation 5.2):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些项目应用于FCM算法。这些 *n* 个项目根据某些标准被聚类成 *c* 个模糊簇。比如说，我们将从算法中得到一个包含 *c* 个簇中心的列表（方程5.2）：
- en: (5.2)
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (5.2)
- en: '*c* = {*c*[1], *c*[2], *c*[3], *c*[4], *c*[5], . . ., *c*[*c*]}'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*c* = {*c*[1], *c*[2], *c*[3], *c*[4], *c*[5], . . ., *c*[*c*]}'
- en: 'The algorithm also returns a partition matrix, which can be defined as equation
    5.3:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 算法还返回一个划分矩阵，它可以定义为方程5.3：
- en: '![figure](../Images/verdhan-ch5-eqs-2x.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch5-eqs-2x.png)'
- en: (5.3)
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (5.3)
- en: Here, each of the elements in *w*[*i*][,][*j*] is the degree to which each of
    the elements in *X* belong to cluster *c*[*j*]. This is the purpose of the partition
    matrix.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*w*[*i*][,][*j*] 中的每个元素是 *X* 中每个元素属于簇 *c*[*j*] 的程度。这就是划分矩阵的目的。
- en: Mathematically, we can get *w*[*i*][,][*j*]as shown in equation 5.4\. The proof
    of the equation is beyond the scope of this book.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们可以根据方程5.4得到 *w*[*i*][,][*j*]。方程的证明超出了本书的范围。
- en: '![figure](../Images/verdhan-ch5-eqs-3x.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch5-eqs-3x.png)'
- en: (5.4)
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (5.4)
- en: 'The algorithm generates centroids for the clusters too. The centroid of a cluster
    is the mean of all the points in that cluster, and the mean is weighted by their
    respective degrees of belonging to that cluster. If we represent it mathematically,
    we can write it like in equation 5.5:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 算法也会为簇生成质心。簇的质心是该簇中所有点的平均值，平均值由它们各自属于该簇的程度加权。如果我们用数学方式表示，我们可以写成方程5.5中的样子：
- en: '![figure](../Images/verdhan-ch5-eqs-4x.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch5-eqs-4x.png)'
- en: (5.5)
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (5.5)
- en: 'In equations 5.4 and 5.5, we have a very important term: *m*. *m* is the hyperparameter
    used to control the fuzziness of the clusters. The values of *m* ≥ 1 and can be
    kept as 2 (a typically used value).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在方程5.4和5.5中，我们有一个非常重要的项：*m*。*m* 是用于控制簇的模糊度的超参数。*m* 的值 ≥ 1，通常可以保持为2（一个常用的值）。
- en: NOTE  The higher the value of *m*, the fuzzier the clusters.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: NOTE：*m* 的值越高，簇的模糊度就越大。
- en: 'We now examine the step-by-step process in the FCM algorithm:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们检查FCM算法的逐步过程：
- en: Start as we start in k-means clustering by choosing the number of clusters we
    wish to have in the output.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像在k-means聚类中开始一样，选择我们希望在输出中拥有的簇的数量。
- en: Allocate the weights randomly to each of the data points.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将权重随机分配给每个数据点。
- en: The algorithm iterates until it has converged. Recall how the k-means algorithm
    converges, wherein we initiate the process by randomly allocating the centroids
    of clusters. And then iteratively we refine the centroids for each of the clusters
    until we get convergence. This is how k-means works. For FCM, we will utilize
    a similar process albeit with slight differences. We have added a membership value
    *w*[*i*][,][*j*] and *m*.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 算法会迭代直到收敛。回想一下k-means算法是如何收敛的，其中我们通过随机分配簇的质心来开始这个过程。然后我们迭代地细化每个簇的质心，直到我们得到收敛。这就是k-means算法的工作原理。对于FCM，我们将利用一个类似的过程，尽管有一些细微的差别。我们添加了一个成员值
    *w*[*i*][,][*j*] 和 *m*。
- en: 'For FCM, for the algorithm to converge we calculate the centroid for each of
    the clusters as per equation 5.6:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于FCM，为了算法收敛，我们根据方程5.6计算每个簇的中心：
- en: '![figure](../Images/verdhan-ch5-eqs-4x.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch5-eqs-4x.png)'
- en: (5.6)
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (5.6)
- en: 5\. For each of the data points, we also calculate its respective coefficient
    for being in that particular cluster. We will use equation 5.4\.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 对于每个数据点，我们也计算其属于该特定簇的相应系数。我们将使用方程5.4。
- en: '6\. Now we should iterate until the FCM algorithm has converged. The cost function
    that we wish to minimize is given by equation 5.7:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. 现在我们应该迭代，直到FCM算法收敛。我们希望最小化的成本函数由方程5.7给出：
- en: '![figure](../Images/verdhan-ch5-eqs-5x.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch5-eqs-5x.png)'
- en: (5.7)
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (5.7)
- en: Once this function has been minimized, we can conclude that the FCM algorithm
    has converged. In other words, we can stop the process as the algorithm has finished
    processing.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦这个函数被最小化，我们就可以得出结论，FCM算法已经收敛。换句话说，我们可以停止这个过程，因为算法已经完成了处理。
- en: This is a good time to compare this with the k-means algorithm. In k-means,
    we have a strict objective function that will allow only one cluster membership,
    while for FCM clustering, we can get different clustering membership based on
    the probability scores.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是对比k-means算法的好时机。在k-means算法中，我们有一个严格的客观函数，它只允许一个簇成员资格，而对于FCM聚类，我们可以根据概率分数得到不同的聚类成员资格。
- en: FCM is very useful for business cases where the boundary between clusters is
    not clear and stringent. Consider the field of bioinformatics, wherein a gene
    can belong to more than one cluster of genes. Another example is when we have
    overlapping datasets like in the fields of the marketing analytics or image segmentation
    where we might have a lot of complex, overlapping, and confusing datasets. FCM
    can give comparatively more robust results than k-means.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: FCM对于聚类边界不清晰且严格的商业案例非常有用。考虑生物信息学领域，其中基因可以属于多个基因簇。另一个例子是我们有重叠数据集，如市场营销分析或图像分割领域，我们可能有很多复杂、重叠和令人困惑的数据集。FCM比k-means可以得到更稳健的结果。
- en: We will now proceed to the Python solution of FCM clustering using the libraries.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用库来求解FCM聚类问题的Python解决方案。
- en: Exercise 5.2
  id: totrans-168
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习5.2
- en: 'Answer these questions to check your understanding:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题以检查你的理解：
- en: Fuzzy clustering allows us to create overlapping clusters. True or False?
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模糊聚类使我们能够创建重叠簇。对还是错？
- en: A data point can belong to one and only one cluster. True or False?
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个数据点只能属于一个簇。对还是错？
- en: If the value of *m* is lower, we get clusters with more precise boundaries.
    True or False?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果*m*的值较低，我们得到的簇边界更精确。对还是错？
- en: 5.5.2 Python implementation of FCM
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.2 FCM的Python实现
- en: 'We have covered the process of FCM. We will now work on the Python implementation
    of FCM by following these steps:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了FCM的过程。现在我们将按照以下步骤在Python中实现FCM：
- en: 'Import the necessary libraries:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库：
- en: '[PRE4]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '2\. Declare a color palette, which will be used later for color coding the
    clusters:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 声明一个调色板，稍后将用于对簇进行着色：
- en: '[PRE5]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '3\. Define the cluster centers:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 定义聚类中心：
- en: '[PRE6]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '4\. Assign the weights:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 分配权重：
- en: '[PRE7]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '5\. Set the seed and then loop through the cluster centers:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 设置随机种子，然后遍历聚类中心：
- en: '[PRE8]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '6\. We will represent the data points first. See figure 5.18:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. 我们首先表示数据点。见图5.18：
- en: '[PRE9]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![figure](../Images/CH05_F18_Verdhan.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F18_Verdhan.png)'
- en: Figure 5.18 Representation of the data points
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.18 数据点的表示
- en: '7\. Iterate different outputs with different values of cluster values and FPC
    (see figure 5.19):'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 7. 使用不同的聚类值和FPC（见图5.19）迭代不同的输出：
- en: '![figure](../Images/CH05_F19_Verdhan.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F19_Verdhan.png)'
- en: Figure 5.19 The output of the FCM algorithm
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.19 FCM算法的输出
- en: '[PRE10]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Observe the output of the code, where for the same datasets you can see the
    different clusters with different positions of the centers. To appreciate the
    colors, you will have to run the code.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 观察代码的输出，对于相同的数据集，你可以看到不同位置的中心的不同簇。要欣赏颜色，你必须运行代码。
- en: 5.6 Gaussian mixture model
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 高斯混合模型
- en: Next, we continue our discussion of soft clustering. Recall we introduced the
    GMM at the start of the chapter. Now we will study the concept and see the Python
    implementation of it.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们继续讨论软聚类。回想一下，我们在本章开头介绍了GMM。现在我们将研究这个概念，并查看其Python实现。
- en: First, let’s get an understanding of the *Gaussian distribution* or what is
    sometimes called *normal distribution*. You might recognize it as a bell curve;
    it usually refers to the same thing.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们了解高斯分布，有时也称为正态分布。你可能认识它为钟形曲线；它通常指的是同一件事。
- en: In figure 5.20, observe that the distribution where the *µ* (mean) is 0 and
    *σ*² (standard deviation) is 1\. It is a perfect normal distribution curve. Compare
    the distribution in different curves here.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在图5.20中，观察均值*μ*为0且标准差*σ*²为1的分布。这是一个完美的正态分布曲线。比较这里不同曲线的分布。
- en: '![figure](../Images/CH05_F20_Verdhan.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F20_Verdhan.png)'
- en: Figure 5.20 A Gaussian distribution is one of the most famous distributions.
    Observe how the values of mean and standard deviation are changed and their effect
    on the corresponding curve.
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.20 高斯分布是最著名的分布之一。观察均值和标准差的变化及其对相应曲线的影响。
- en: The mathematical expression for Gaussian distribution is
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯分布的数学表达式是
- en: '![figure](../Images/verdhan-ch5-eqs-6x.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch5-eqs-6x.png)'
- en: '![figure](../Images/verdhan-ch5-eqs-7x.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch5-eqs-7x.png)'
- en: (5.8)
  id: totrans-203
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (5.8)
- en: The equation is also called the probability density function. In figure 5.20,
    observe the shape of the probability distribution where the *µ* is 0 and *σ*²
    is 1\. It is a perfect normal distribution curve. Compare the distribution in
    different curves in figure 5.20 where, by changing the values of the mean and
    standard distribution, we get different graphs.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程也称为概率密度函数。在图5.20中，观察*µ*为0和*σ*²为1的概率分布的形状。这是一个完美的正态分布曲线。比较图5.20中不同曲线的分布，通过改变均值和标准分布的值，我们得到不同的图形。
- en: You might be wondering why we are using Gaussian distribution here. There is
    a very famous statistical theorem called the *central limit theorem*. The theorem
    states that if the variability of the data is due to a large number of unrelated
    causes, then the distribution can be approximated by a Gaussian curve. Also, the
    approximation becomes more and more accurate the more data is collected; that
    is, the more data we collect, the more Gaussian the distribution. This normal
    distribution can be observed across all walks of life and in chemistry, physics,
    mathematics, biology, or any other branch of science. That is the beauty of Gaussian
    distribution.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么我们在这里使用高斯分布。有一个非常著名的统计定理叫做**中心极限定理**。该定理表明，如果数据的变异性是由大量无关的原因引起的，那么分布可以被近似为高斯曲线。此外，随着收集的数据越来越多，近似变得越来越准确；也就是说，我们收集的数据越多，分布就越接近高斯分布。这种正态分布可以在生活的各个方面以及化学、物理、数学、生物学或任何其他科学分支中观察到。这就是高斯分布的美丽之处。
- en: The plot shown in figure 5.20 is 2D. We can have multidimensional Gaussian distribution
    too. In the case of a multidimensional Gaussian distribution, we will get a 3D
    figure as shown in figure 5.21\. Our input was a scalar in 1D. Now, instead of
    scalar, our input is a vector; the mean is also a vector and represents the center
    of the data. Hence, the mean has the same dimensionality as the input data. The
    variance is now the covariance matrix ∑. This matrix not only tells us the variance
    in the inputs but also comments on the relationship between different variables—for
    example, how the values of *y* are affected if the value of *x* is changed. Have
    a look at figure 5.21\. We can understand the relationship between the *x* and
    *y* variables here.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.20中所示的是2D图。我们也可以有多维高斯分布。在多维高斯分布的情况下，我们将得到如图5.21所示的3D图。我们的输入是一维的标量。现在，我们的输入不再是标量，而是一个向量；均值也是一个向量，代表数据的中心。因此，均值具有与输入数据相同的维度。方差现在是对角矩阵Σ。这个矩阵不仅告诉我们输入中的方差，还评论了不同变量之间的关系——例如，如果改变*x*的值，*y*的值会如何受到影响。看看图5.21。我们可以在这里理解*x*和*y*变量之间的关系。
- en: '![figure](../Images/CH05_F21_Verdhan.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F21_Verdhan.png)'
- en: Figure 5.21 3D representation of a Gaussian distribution
  id: totrans-208
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.21高斯分布的3D表示
- en: NOTE  Covariance plays a significant role here. K-means does not consider the
    covariance of a dataset, which is used in the GMM model.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：协方差在这里起着重要的作用。K-means没有考虑数据集的协方差，这在GMM模型中是使用的。
- en: Let’s examine the process of GMM clustering. Imagine we have a dataset with
    *n* items. When we use GMM clustering, we do not find the clusters using the centroid
    method; instead, we fit a set of *k* Gaussian distributions to the dataset at
    hand. In other words, we have *k* clusters. We should determine the parameters
    for each of these Gaussian distributions, which are mean, variance, and weight
    of a cluster. Once the parameters for each of the distributions are determined,
    then we can find the respective probability for each of the *n* items to belong
    to *k* clusters.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考察GMM聚类的过程。想象我们有一个包含*n*个项目的数据集。当我们使用GMM聚类时，我们不是使用质心方法来找到簇；相反，我们将一组*k*个高斯分布拟合到当前的数据集中。换句话说，我们有*k*个簇。我们应该确定每个高斯分布的参数，这些参数是簇的均值、方差和权重。一旦确定了每个分布的参数，我们就可以找到每个*n*个项目属于*k*个簇的相应概率。
- en: Mathematically, we can calculate the probability as shown in equation 5.9\.
    The equation is used so we know that a particular point *x* is a linear combination
    of *k* Gaussians. The term *f*[*j*] is used to represent the strength of the Gaussian,
    and it can be seen in the second equation that the sum of such strength is equal
    to 1.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们可以根据方程5.9计算概率。该方程用于我们知道一个特定的点*x*是*k*个高斯函数的线性组合。术语*f*[*j*]用于表示高斯函数的强度，可以在第二个方程中看到，这种强度的总和等于1。
- en: '![figure](../Images/verdhan-ch5-eqs-8x.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch5-eqs-8x.png)'
- en: '![figure](../Images/verdhan-ch5-eqs-9x.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/verdhan-ch5-eqs-9x.png)'
- en: (5.9)
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (5.9)
- en: For spectral clustering, we must identify the values of *f*, ∑, and *µ*. As
    you can imagine, getting the values of these parameters can be tricky. It is indeed
    a slightly complex process called the expectation-maximization (EM) technique,
    which we will cover next. This section is quite heavy on mathematical concepts
    and is optional. It is recommended for readers interested in understanding the
    deeper workings of the techniques.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对于谱聚类，我们必须确定*f*、∑和*µ*的值。正如你可以想象的那样，获取这些参数的值可能很棘手。这确实是一个稍微复杂的过程，称为期望最大化（EM）技术，我们将在下一节中介绍。本节数学概念较多，是可选的。建议对理解技术深层工作原理感兴趣的读者阅读。
- en: 5.6.1 EM technique
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.1 EM技术
- en: EM is a statistical method to determine the correct parameters for a model.
    There are quite a few techniques that are popular; maximum likelihood estimation
    might be the most famous. But at the same time, there could be a few challenges
    with maximum likelihood. The dataset might have missing values or, in other words,
    be incomplete. Or it is possible that a point in the dataset is generated by two
    different Gaussian distributions. Hence, it will be very difficult to determine
    which distribution generated that data point. Here, EM can be helpful.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: EM是一种确定模型正确参数的统计方法。有许多技术很受欢迎；最大似然估计可能是最著名的。但与此同时，最大似然可能也有一些挑战。数据集可能存在缺失值，或者换句话说，是不完整的。或者，数据集中的某个点可能是由两个不同的高斯分布生成的。因此，很难确定哪个分布生成了该数据点。在这里，EM可能会有所帮助。
- en: NOTE  K-means uses only mean while GMM utilizes both mean and variance of the
    data.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：K-means只使用均值，而GMM利用数据的均值和方差。
- en: The variables that are generated in the process are called *latent variables*.
    Since we do not know the exact values of these latent variables, EM first estimates
    their optimum values using the current data. Once this is done, then the model
    parameters are estimated. Using these model parameters, the latent variables are
    again determined. And, using these new latent variables, new model parameters
    are derived. The process continues until a good enough set of latent values and
    model parameters are achieved that fit the data well. Let’s study that in more
    detail now. We will use the same example as in the last section.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在过程中生成的变量被称为*潜在变量*。由于我们不知道这些潜在变量的确切值，EM首先使用当前数据估计它们的最佳值。一旦完成，然后估计模型参数。使用这些模型参数，再次确定潜在变量。然后，使用这些新的潜在变量，推导出新的模型参数。这个过程会一直持续，直到获得一组足够好的潜在值和模型参数，使得数据拟合良好。现在让我们更详细地研究一下。我们将使用与上一节相同的例子。
- en: Imagine we have a dataset with *n* items. As mentioned, when we use GMM clustering,
    we do not find the clusters using the centroid method; instead, we fit a set of
    *k* Gaussian distributions to the dataset at hand. In other words, we have *k*
    clusters. We determine the parameters for each of these Gaussian distributions
    (mean, variance, and weight). Let’s say that mean is *µ*[1], *µ*[2], *µ*[3], *µ*[4]….
    *µ*[*k*] and covariance is ∑[1], ∑[2], ∑[3], ∑[4]…. ∑[*k*]. We can also have one
    more parameter to represent the density or strength of the distribution, and it
    can be represented by *f*.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个包含*n*个项目的数据集。如前所述，当我们使用GMM聚类时，我们不是使用质心方法找到簇；相反，我们将一组*k*个高斯分布拟合到当前的数据集中。换句话说，我们有*k*个簇。我们确定每个高斯分布的参数（均值、方差和权重）。比如说，均值是*µ*[1]、*µ*[2]、*µ*[3]、*µ*[4]……*µ*[*k*]，协方差是∑[1]、∑[2]、∑[3]、∑[4]……∑[*k*]。我们还可以有一个额外的参数来表示分布的密度或强度，它可以表示为*f*。
- en: We start with the expectation, or the E step. In this step, each data point
    is assigned to a cluster probabilistically. So, for each point, we calculate its
    probability of belonging to a cluster; if this value is high, the point is in
    the correct cluster; otherwise, the point is in the wrong cluster. In other words,
    we calculate the probability that each data point is generated by each of the
    *k* Gaussians.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从期望，或者说E步骤开始。在这个步骤中，每个数据点以概率分配到簇中。因此，对于每个点，我们计算它属于簇的概率；如果这个值很高，那么这个点就在正确的簇中；否则，这个点就在错误的簇中。换句话说，我们计算每个数据点是由每个*k*个高斯分布生成的概率。
- en: NOTE  Since we are calculating probabilities, these are called soft assignments.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于我们正在计算概率，这些被称为软分配。
- en: The probability is calculated using the formula in equation 5.10\. If we look
    closely, the numerator is the probability, and then we normalize by the denominator.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 概率是使用方程5.10中的公式计算的。如果我们仔细观察，分子是概率，然后我们通过分母进行归一化。
- en: '![figure](../Images/verdhan-ch5-eqs-10x.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch5-eqs-10x.png)'
- en: (5.10)
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (5.10)
- en: In the expectation step, for a data point *x*[*i*][,][*j*], where *i* is the
    row and *j* is the column, we are getting a matrix where rows are represented
    by the data points and columns are their respective Gaussian values.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在期望步骤中，对于一个数据点 *x*[*i*][,][*j*]，其中 *i* 是行，*j* 是列，我们得到一个矩阵，其中行由数据点表示，列是它们各自的高斯值。
- en: When the expectation step is finished, we will perform the maximization or the
    M step. In this step, we will update the values of *µ*, ∑, and *f* using the formula
    in equation 5.7\. Recall, in k-means clustering, we simply take the mean of the
    data points and move ahead. We do something similar here albeit use the probability
    or the expectation we calculated in the last step.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当期望步骤完成后，我们将执行最大化步骤或M步骤。在这个步骤中，我们将使用方程5.7中的公式更新 *µ*、∑ 和 *f* 的值。回想一下，在k-means聚类中，我们只是简单地取数据点的均值然后继续。这里我们做的是类似的事情，尽管我们使用了上一步计算的概率或期望。
- en: The three values can be calculated using the equations below. Equation 5.7 is
    the calculation of the covariances ∑[*j*], of all the points, which is then weighted
    by the probability of that point being generated by Gaussian *j* as shown in equation
    5.11\. The mathematical proofs are beyond the scope of this book.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个值可以使用以下方程计算。方程5.7是计算所有点的协方差 ∑[*j*]，然后按照方程5.11中所示，由该点由高斯 *j* 生成的概率进行加权。数学证明超出了本书的范围。
- en: '![figure](../Images/verdhan-ch5-eqs-11x.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch5-eqs-11x.png)'
- en: (5.11)
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (5.11)
- en: The mean *µ*[*j*], is determined by equation 5.12\. Here, we determine the mean
    for all the points, weighted by the probability of that point being generated
    by Gaussian *j*.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 均值 *µ*[*j*]，由方程5.12确定。在这里，我们确定所有点的均值，并按照该点由高斯 *j* 生成的概率进行加权。
- en: '![figure](../Images/verdhan-ch5-eqs-12x.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch5-eqs-12x.png)'
- en: (5.12)
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (5.12)
- en: Similarly, the density or the strength is calculated by equation 5.13, where
    we add all the probabilities for each point to be generated by Gaussian *j* and
    then divide by the total number of points *N*.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，密度或强度是通过方程5.13计算的，其中我们将每个点由高斯 *j* 生成的所有概率相加，然后除以点的总数 *N*。
- en: '![figure](../Images/verdhan-ch5-eqs-13x.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/verdhan-ch5-eqs-13x.png)'
- en: (5.13)
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: (5.13)
- en: Based on these values, new values for ∑, *µ*, and *f* are derived, and the process
    continues until the model converges. We stop when we can maximize the log-likelihood
    function.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些值，推导出 ∑、*µ* 和 *f* 的新值，这个过程会一直持续到模型收敛。当我们能够最大化似然函数时停止。
- en: It is a complex mathematical process. We have covered it to give you an in-depth
    understanding of what happens in the background of the statistical algorithm.
    The Python implementation is much more straightforward than the mathematical concept.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个复杂的过程。我们介绍了它，以便您深入了解统计算法背后的发生情况。Python实现比数学概念要简单得多。
- en: Exercise 5.3
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习5.3
- en: 'Answer these questions to check your understanding:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 回答这些问题来检查你的理解：
- en: Gaussian distribution has a mean equal to 1 and a standard deviation equal to
    0\. True or False?
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高斯分布的均值等于1，标准差等于0。对还是错？
- en: GMM models do not consider the covariance of the data. True or False?
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GMM模型不考虑数据的协方差。对还是错？
- en: 5.6.2 Python implementation of GMM
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.2 GMM的Python实现
- en: 'We will first import the data, and then we will compare the results using k-means
    and GMM. We follow these steps:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入数据，然后使用k-means和GMM进行比较。我们遵循以下步骤：
- en: 'Import all the libraries and the dataset:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有库和数据集：
- en: '[PRE11]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '2\. Drop any NA from the dataset:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 2. 从数据集中删除任何NA值：
- en: '[PRE12]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '3\. Fit a `kmeans` algorithm. We are keeping the number of clusters as 5\.
    Please note that we are not saying that this is an ideal number of clusters. The
    number of clusters is only for illustrative purposes. We declare a variable k-means
    and then use five clusters. The dataset is fit next:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 3. 配置一个 `kmeans` 算法。我们将簇的数量保持为5。请注意，我们并没有说这是簇的理想数量。簇的数量只是为了说明目的。我们声明一个变量 k-means
    并使用五个簇。接下来拟合数据集：
- en: '[PRE13]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 4\. Plot the clusters. First, a prediction is made on the dataset, and then
    the values are added to the data frame as a new column. The data is then plotted
    with different colors representing different clusters. The print version of the
    book will not show the different colors, but the output of the Python code will.
    The same output is available in the GitHub repository.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 4. 绘制聚类。首先，对数据集进行预测，然后将值添加到数据框作为一个新列。然后，用不同颜色表示不同聚类的数据绘制。书的打印版不会显示不同的颜色，但Python代码的输出会。同样的输出也可以在GitHub仓库中找到。
- en: 'The output is as follows (see figure 5.22):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下（见图5.22）：
- en: '[PRE14]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![figure](../Images/CH05_F22_Verdhan.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F22_Verdhan.png)'
- en: Figure 5.22 Outcome of plotting the clusters after fitting the `kmeans` algorithm
  id: totrans-255
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.22 配置`kmeans`算法后绘制聚类的结果
- en: '5\. Fit a GMM model. Note that the code is the same as the k-means algorithm,
    only the algorithm’s name has changed from k-means to `GaussianMixture`:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 5. 配置一个GMM模型。请注意，代码与k-means算法相同，只是算法的名称从k-means更改为`GaussianMixture`：
- en: '[PRE15]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '6\. Plot the results. The output is as follows (figure 5.23):'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 6. 绘制结果。输出如下（见图5.23）：
- en: '[PRE16]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![figure](../Images/CH05_F23_Verdhan.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_F23_Verdhan.png)'
- en: Figure 5.23 Outcome of plotting the clusters after fitting a GMM algorithm
  id: totrans-261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.23 配置GMM算法后绘制聚类的结果
- en: 7\. Run the code with different values of clusters to observe the difference.
    In the following plots, the left one is k-means with two clusters, while the right
    is GMM with two clusters. There are a few points that are classified differently
    in the two clustering approaches. The print version of the book will not show
    the different colors, but the output of the Python code will. The same output
    is available in the GitHub repository, too (see figure 5.24).
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 7. 使用不同的聚类值运行代码以观察差异。在下面的图中，左边的是具有两个聚类的k-means，而右边的是具有两个聚类的GMM。在这两种聚类方法中，有几个点被分类为不同的类别。书的打印版不会显示不同的颜色，但Python代码的输出会。同样的输出也可以在GitHub仓库中找到（见图5.24）。
- en: '![figure](../Images/CH05_UN01_Verdhan.png)![figure](../Images/CH05_F24_Verdhan.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH05_UN01_Verdhan.png)![figure](../Images/CH05_F24_Verdhan.png)'
- en: Figure 5.24 K-means with two clusters (left) and GMM with two clusters (right)
  id: totrans-264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.24 具有两个聚类的k-means（左）和具有两个聚类的GMM（右）
- en: Gaussian distribution is one of the most widely used data distributions used.
    If we compare k-means and the GMM model, we see that k-means does not consider
    the normal distribution of the data. The relationship of various data points is
    also not considered in k-means.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯分布是使用最广泛的数据分布之一。如果我们比较k-means和GMM模型，我们会看到k-means不考虑数据的正态分布。在k-means中，也没有考虑各种数据点之间的关系。
- en: NOTE  K-means is a distance-based algorithm; GMM is a distribution-based algorithm.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：k-means是一种基于距离的算法；GMM是一种基于分布的算法。
- en: In short, it is advantageous to use GMM models for creating clusters, particularly
    when we have overlapping datasets. It is a useful technique for financial and
    price modeling, natural language processing-based solutions, etc.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，使用GMM模型创建聚类是有利的，尤其是在我们拥有重叠数据集时。它对于金融和价格建模、基于自然语言处理解决方案等都是一种有用的技术。
- en: 5.7 Concluding thoughts
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.7 总结思考
- en: In this chapter, we have explored three complex clustering algorithms. You might
    have felt the mathematical concepts were a bit heavy. They are indeed, but they
    provide a deeper understanding of the process. These algorithms are not necessarily
    the best ones for every problem. Ideally, in a real-world business problem, we
    start with classical clustering algorithms (k-means, hierarchical, and DBSCAN).
    If we do not get acceptable results, we can try the more complex algorithms.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了三种复杂的聚类算法。你可能觉得数学概念有点沉重。确实如此，但它们提供了对过程的更深入理解。这些算法并不一定是每个问题的最佳选择。理想情况下，在现实世界的商业问题中，我们首先从经典的聚类算法（k-means、层次和DBSCAN）开始。如果我们没有得到可接受的结果，我们可以尝试更复杂的算法。
- en: Many times, a data science problem is equated to the choice of algorithm, which
    it is not. The algorithm is certainly an important ingredient of the entire solution,
    but it is not the only one. In real-world datasets, there are a lot of variables,
    and the amount of data is also quite high. The data has a lot of noise. We should
    account for all of these factors when we shortlist an algorithm. Algorithm maintenance
    and refreshing are also considerations. All of these aspects are covered in detail
    in the last chapter of the book.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 许多时候，数据科学问题被等同于算法的选择，这其实并不正确。算法确实是整个解决方案的重要组成部分，但并非唯一。在现实世界的数据集中，有很多变量，数据量也相当大。数据中有很多噪声。在选择算法时，我们应该考虑到所有这些因素。算法的维护和更新也是需要考虑的因素。所有这些方面都在本书的最后一章中详细讨论。
- en: 5.8 Practical next steps and suggested readings
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.8 实践步骤和推荐阅读
- en: 'The following provides suggestions for what to do next and offers some helpful
    reading:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以下提供了一些下一步行动的建议和一些有用的阅读材料：
- en: In chapter 2, we did clustering using various techniques. Use the datasets from
    there and perform spectral clustering, GMM, and FCM clustering to compare the
    results. Datasets provided at the end of chapter 2 can be used for clustering.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二章中，我们使用了各种技术进行聚类。使用那里的数据集，执行谱聚类、GMM和FCM聚类以比较结果。第二章末尾提供的数据集可用于聚类。
- en: Get the credit card dataset for clustering from Kaggle ([https://mng.bz/oKwd](https://mng.bz/oKwd))
    and data from the famous Iris dataset, which we used earlier ([https://www.kaggle.com/uciml/iris](https://www.kaggle.com/uciml/iris)).
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Kaggle获取聚类用信用卡数据集([https://mng.bz/oKwd](https://mng.bz/oKwd))和之前使用过的著名Iris数据集的数据([https://www.kaggle.com/uciml/iris](https://www.kaggle.com/uciml/iris))。
- en: Refer to the book *Computational Network Science* by Henry Hexmoor to study
    the mathematical concepts.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参考亨利·赫克斯莫尔的书籍《计算网络科学》来学习数学概念。
- en: 'Get spectral clustering papers from the following links and study them:'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从以下链接获取谱聚类论文并学习：
- en: 'On spectral clustering: analysis and an algorithm: [https://mng.bz/nRwa](https://mng.bz/nRwa)'
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于谱聚类：分析和算法：[https://mng.bz/nRwa](https://mng.bz/nRwa)
- en: 'Spectral clustering with eigenvalue selection: [https://mng.bz/vKw7](https://mng.bz/vKw7)'
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于特征值选择的谱聚类：[https://mng.bz/vKw7](https://mng.bz/vKw7)
- en: 'The mathematics behind spectral clustering and the equivalence to principal
    component analysis: [https://arxiv.org/pdf/2103.00733v1.pdf](https://arxiv.org/pdf/2103.00733v1.pdf)'
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谱聚类的数学原理及其与主成分分析的等价性：[https://arxiv.org/pdf/2103.00733v1.pdf](https://arxiv.org/pdf/2103.00733v1.pdf)
- en: 'Get GMM papers from the following links and explore them:'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从以下链接获取GMM论文并探索：
- en: '“GMM Estimation for High Dimensional Panel Data Models”: [https://mng.bz/4agw](https://mng.bz/4agw)'
  id: totrans-281
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “高维面板数据模型的GMM估计法”：[https://mng.bz/4agw](https://mng.bz/4agw)
- en: '“Application of Compound Gaussian Mixture Model in the Data Stream”: [https://ieeexplore.ieee.org/document/5620507](https://ieeexplore.ieee.org/document/5620507)'
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “复合高斯混合模型在数据流中的应用”：[https://ieeexplore.ieee.org/document/5620507](https://ieeexplore.ieee.org/document/5620507)
- en: 'Get FCM papers from the following links and study them:'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从以下链接获取FCM论文并学习：
- en: '“FCM: The Fuzzy c-Means *Clustering* Algorithm”: [https://mng.bz/QDXG](https://mng.bz/QDXG)'
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “FCM：模糊c均值聚类算法”：[https://mng.bz/QDXG](https://mng.bz/QDXG)
- en: 'A Survey on Fuzzy c-Means Clustering Techniques: [https://www.ijedr.org/papers/IJEDR1704186.pdf](https://www.ijedr.org/papers/IJEDR1704186.pdf)'
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模糊c均值聚类技术综述：[https://www.ijedr.org/papers/IJEDR1704186.pdf](https://www.ijedr.org/papers/IJEDR1704186.pdf)
- en: '“Implementation of Fuzzy C-Means and Possibilistic C-Means Clustering Algorithms,
    Cluster Tendency Analysis and Cluster Validation”: [https://arxiv.org/pdf/1809.08417.pdf](https://arxiv.org/pdf/1809.08417.pdf)'
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: “模糊C均值和可能性C均值聚类算法的实现、聚类趋势分析和聚类验证”：[https://arxiv.org/pdf/1809.08417.pdf](https://arxiv.org/pdf/1809.08417.pdf)
- en: Summary
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Spectral clustering focuses on data point affinity rather than location for
    clustering. It works well with complex data shapes where traditional algorithms
    like k-means may not suffice.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谱聚类关注聚类中的数据点亲和力而非位置。它适用于复杂的数据形状，而传统的算法如k-means可能不足以满足需求。
- en: Spectral clustering utilizes graph theory and connectivity, relying on eigenvalues,
    the Laplacian matrix, and the affinity matrix.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谱聚类利用图理论和连通性，依赖于特征值、拉普拉斯矩阵和亲和矩阵。
- en: The process includes calculating degree, adjacency, Laplacian matrices, and
    the Fielder vector for clustering.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该过程包括计算度、邻接矩阵、拉普拉斯矩阵和Fielder向量以进行聚类。
- en: K-means clustering uses centroids, whereas spectral clustering’s focus is on
    connectivity and data point similarities.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means聚类使用质心，而谱聚类的重点是连通性和数据点相似性。
- en: Spectral clustering can require substantial computational resources due to matrix
    operations and is suitable for smaller datasets.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于矩阵运算，谱聚类可能需要大量的计算资源，因此适用于较小的数据集。
- en: Fuzzy clustering allows data points to belong to multiple clusters, introducing
    “membership” for data items.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模糊聚类允许数据点属于多个簇，为数据项引入了“隶属度”。
- en: FCM is a key algorithm in fuzzy clustering, utilizing membership degrees and
    controlling fuzziness through hyperparameter *m*.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FCM是模糊聚类中的一个关键算法，它通过隶属度和通过超参数*m*来控制模糊度。
- en: GMM employs Gaussian distributions for soft clustering, factoring in dataset
    covariance.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GMM使用高斯分布进行软聚类，并考虑数据集的协方差。
- en: GMM is suitable for overlapping datasets and considers the relationship between
    data points, unlike k-means.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GMM适用于重叠数据集，并且与k-means不同，它考虑数据点之间的关系。
- en: The EM technique is used in GMM to estimate parameters iteratively.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在GMM中，EM技术被用于迭代估计参数。
- en: GMM models are advantageous for financial modeling, natural language processing,
    and cases with overlapping data.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GMM模型在金融建模、自然语言处理以及重叠数据的案例中具有优势。
- en: Fuzzy and GMM are soft clustering methods, allowing detailed membership and
    probability assignment to data points.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模糊聚类和GMM是软聚类方法，允许对数据点进行详细的隶属度和概率分配。
- en: Spectral clustering supports applications in image segmentation, speech analysis,
    and text analytics without assuming data shape constraints.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谱聚类支持图像分割、语音分析和文本分析等应用，而不需要假设数据形状约束。
