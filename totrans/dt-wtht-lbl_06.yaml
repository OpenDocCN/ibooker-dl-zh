- en: 5 Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spectral clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fuzzy clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaussian mixture models clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out of complexity, find simplicity.—Einstein
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Sometimes life is very simple, and sometimes we experience quite complex situations.
    We sail through both situations and change our approach as needed.
  prefs: []
  type: TYPE_NORMAL
- en: In part 1, we covered the fundamentals to prepare you for the journey ahead.
    We are now in part 2, which is slightly more complex than part 1\. Part 3 will
    be more advanced than the first two parts. So please give careful attention to
    the coming chapters, as the skills and knowledge gained here will prepare you
    for the later chapters in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting this chapter, we should refresh our memory on what we covered
    in chapter 2\. We studied clustering algorithms in part 1 of the book. In chapter
    2, we learned that clustering is an unsupervised learning technique where we wish
    to group the data points by discovering interesting patterns in the datasets.
    We went through the meaning of clustering solutions and different categories of
    clustering algorithms and looked at a case study. In that chapter, we explored
    k-means clustering, hierarchical clustering, and DBSCAN clustering in depth. We
    went through the mathematical background, process, and Python implementation and
    the pros and cons of each algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may often encounter datasets that do not conform to a simple shape and
    form. Moreover, we have to find the best fit before making a choice of the final
    algorithm we wish to implement. Here we might need help with more complex clustering
    algorithms—the topic of this chapter. In this chapter, we are going to again study
    three such complex clustering algorithms: spectral clustering, fuzzy clustering,
    and Gaussian mixture models (GMM) clustering. As always, Python implementation
    will follow the mathematical and theoretical concepts. This chapter is slightly
    heavy on mathematical concepts. There is no need to be an advanced student of
    mathematics, but it is sometimes important to understand how the algorithms work
    in the background. At the same time, you will be surprised to find that Python
    implementation of such algorithms is not tedious. This chapter does not have a
    case study.'
  prefs: []
  type: TYPE_NORMAL
- en: Welcome to the fifth chapter, and all the very best!
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Technical toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will continue to use the same version of Python and Jupyter Notebook as we
    have used so far. The codes and datasets used in this chapter have been checked
    in at GitHub ([https://mng.bz/6epo](https://mng.bz/6epo)).
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to use the regular Python libraries we have used so far: `numpy`,
    `pandas`, `sklearn`, `seaborn`,  `matplotlib`, etc. You need to install two other
    Python libraries in this chapter: `skfuzzy` and `network`. Using libraries, we
    can implement the algorithms very quickly. Otherwise, coding these algorithms
    is quite a time-consuming and painstaking task.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started with a refresh of clustering!
  prefs: []
  type: TYPE_NORMAL
- en: '5.2 Clustering: A brief recap'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall from chapter 2, clustering is used to group similar objects or data points.
    It is an unsupervised learning technique where we intend to find natural grouping
    in the data, as shown in figure 5.1.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can observe that on the left side, we have ungrouped data, and on the
    right side, the data points have been grouped into logical groups. We can also
    observe that there can be two methodologies to do the grouping or clustering,
    and both result in different clusters. Clustering as a technique is quite heavily
    used in business solutions like customer segmentation, market segmentation, etc.
  prefs: []
  type: TYPE_NORMAL
- en: We learned about k-means and hierarchical and DBSCAN clustering in chapter 2\.
    We also covered various distance measurement techniques and indicators to measure
    the performance of clustering algorithms. You are advised to revisit the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we focus on advanced clustering methods. We start with spectral
    clustering in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F01_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 Clustering of objected results into natural grouping
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5.3 Spectral clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spectral clustering is one of the unique clustering algorithms, and a lot of
    research has been done in this field. Revered researchers include Prof. Andrew
    Yang, Prof. Michael Jordan, Prof. Yair Weiss, Prof. Jianbo Shi, and Prof. Jitendra
    Malik, to name a few. We provide links to some of their papers at the end of the
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Spectral clustering works on the affinity and not the absolute location of the
    data points for clustering. When we consider the absolute location of the points,
    the similarity is simply based on the distances between the points, whereas affinity
    considers the similarity between the points. If the affinity is 0 between the
    points, they are dissimilar, whereas if the affinity is 1, they are very similar.
    Hence, wherever the data is in complicated shapes (i.e., some kind of special
    relationship exists between the data points), spectral clustering is the answer.
    We show a few examples in figure 5.2 where spectral clustering can provide a logical
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F02_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 Examples of various complex data shapes that can be clustered using
    spectral clustering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For figure 5.2, we could have used other algorithms like k-means clustering
    too. But they might not be able to do justice to such complicated shapes of data.
    You can see from figure 5.2 that the various data points are in a certain pattern.
    Algorithms like k-means clustering utilize the compactness of the data points
    and are driven by centroids of the respective clusters. In other words, the closeness
    of the points to each other and compactness toward the cluster center drive the
    clustering in k-means. On the other hand, in spectral clustering, *connectivity*
    is the driving logic. In connectivity, either the data points are immediately
    close to one another or they are connected in some way. Some examples of such
    connectivity-based clustering are depicted in figure 5.2\. The points in the inner
    circle belong to one cluster while those in the outer circle belong to another
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now look at the first diagram in figure 5.3, where the data points are in a
    doughnut pattern. There can be data points that follow this doughnut pattern.
    We need to cluster this data, and it is indeed a complex pattern. Imagine that
    by using a clustering method, the circles inside a square are made a part of the
    same cluster, which is shown in the middle diagram in figure 5.3\. After all,
    they are close to each other. But if we look closely, the points are in a circle
    and in a pattern, and hence, the actual cluster should be as shown in the far
    right diagram in figure 5.3\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F03_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 We can have a complex representation of data points that need to
    be clustered. Observe the doughnut shape (left). An explanation can be that the
    dots in a square are a part of the same cluster as what would be based on the
    distance only, but clearly, they are not part of the same cluster (middle). We
    have two circles here. The points in the inner circle belong to one cluster, whereas
    the outer points belong to another cluster (right).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The example shown in figure 5.3 depicts the advantages of spectral clustering
    as opposed to k-means clustering. In the second figure, the dots in red (those
    in the square in the print book) will be incorrectly clustered into a different
    cluster, and in the third figure, the correct clustering is shown. Spectral clustering
    may group the data from the inner circle in a separate cluster.
  prefs: []
  type: TYPE_NORMAL
- en: As we said earlier, spectral clustering utilizes the connectivity approach.
    In spectral clustering, data points that are immediately next to each other are
    identified in a graph. These data points are sometimes referred to as *nodes*.
    These data points or nodes are then mapped to a low-dimensional space. A low-dimensional
    space is one that has a fewer number of input features. During this process, spectral
    clustering uses eigenvalues, affinity matrix, Laplacian matrix, and degree matrix
    derived from the dataset. The low-dimensional space can then be segregated into
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Spectral clustering utilizes the connectivity approach for clustering.
    It relies on graph theory, wherein we identify clusters of nodes based on the
    edges connecting them.
  prefs: []
  type: TYPE_NORMAL
- en: We will study the process in detail. But first, there are a few important mathematical
    concepts that form the foundation of spectral clustering, which we will cover
    now.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Building blocks of spectral clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We know that the goal of clustering is to group data points that are similar
    into one cluster and the data points that are not similar into another. One important
    mathematical concept is similarity graphs, which are a representation of data
    points.
  prefs: []
  type: TYPE_NORMAL
- en: Similarity graphs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A graph is one of the intuitive methods to represent data points. The first
    diagram in figure 5.4 shows an example of a graph that is simply a connection
    between data points represented by the edge. Two data points are connected if
    the similarity between them is positive or it is above a certain threshold, which
    is shown in the second diagram. Instead of absolute values for the similarity,
    we can use weights. So in the second diagram in figure 5.4, as point 1 and 2 are
    similar compared to points 1 and 3, the connection between points 1 and 2 has
    a higher weight than points 1 and 3\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F04_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 A graph is a simple representation of data points. The points or
    nodes are connected by edges if they are very similar (left). The weight is higher
    if the similarity between data points is high; for dissimilar data points, the
    weight is less (right).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: So, we can say that, using similarity graphs, we wish to cluster the data points
    such that the edges of the data points have
  prefs: []
  type: TYPE_NORMAL
- en: Higher weight values and hence are similar to each other and so are in the same
    cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lower values of weight and hence are not similar to each other and so are in
    different clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apart from similarity graphs, we should also know the concept of eigenvalues
    and eigenvectors, which we covered in detail in the previous chapter. You are
    advised to refresh your memory on it should you need to.
  prefs: []
  type: TYPE_NORMAL
- en: Adjacency matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Have a close look at figure 5.5\. We can see those various points from 1 to
    5 are connected. We represent the connection in a matrix. That matrix is called
    an *adjacency matrix*. In an adjacency matrix, the rows and columns are the respective
    nodes. The values inside the matrix represent the connection: if the value is
    0, that means there is no connection, and if the value is 1, it means there is
    a connection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F05_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 An adjacency matrix represents the connection between various nodes.
    There is a connection between node 1 and node 5; hence the value is 1\. There
    is no connection between node 1 and node 4; hence the corresponding value is 0.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: So, for an adjacency matrix, we are only concerned if there is a connection
    between two data points. With the way that we are defining the edges (as nonoriented),
    the matrix is always symmetric. This is because if there is a connection from
    1 to 2, there must also be a connection from 2 to 1, and if there is no connection
    between 3 and 1, there is no connection between 1 and 3 either. If we extend the
    concept of the adjacency matrix, we get a degree matrix, which is our next topic.
  prefs: []
  type: TYPE_NORMAL
- en: Degree matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A degree matrix is a diagonal matrix, where the degree of a node along the diagonal
    is the number of edges connected to it. If we use the same example as previously,
    we get the degree matrix shown in figure 5.6\. Nodes 3 and 5 have three connections
    each, so they have values of 3 along the diagonal; the other nodes have only two
    connections each, so they have 2 as the value along the diagonal.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F06_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 While an adjacency matrix represents the connection between various
    nodes, a degree matrix is for the number of connections each node has. It is shown
    on the diagonal of the matrix. For example, node 5 has three connections and hence
    has a value of 3 in the adjacency matrix, while node 1 has only two connections
    and so has a value of 2.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'You might be wondering: Why do we use these matrices? Matrices provide an elegant
    representation of the data and can clearly depict the relationships between two
    points. Also, computers can more easily deal with matrix representation than alternative
    ways for manipulating the graph.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered both the adjacency matrix and degree matrix, we can
    move to the Laplacian matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Laplacian matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are quite a few variants of the Laplacian matrix, but if we take the simplest
    form, it is nothing but a subtraction of the adjacency matrix from the degree
    matrix—in other words, L = D – A. We can demonstrate it as shown in figure 5.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F07_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 The Laplacian matrix is quite simple to understand. To get a Laplacian
    matrix, we can simply subtract an adjacency matrix from the degree matrix as shown
    in the example here. Here, D represents the degree matrix, A is the adjacency
    matrix, and L is the Laplacian matrix.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The Laplacian matrix is an important concept, and we use the eigenvalues of
    L to develop spectral clustering. Once we get the eigenvalues and eigenvectors,
    we can define two other values: spectral gap and Fielder value. The very first
    nonzero eigenvalue is the *spectral gap,* which defines the density of the graph.
    The *Fielder value* is the second eigenvalue; it provides an approximation of
    the minimum cut required to separate the graph into two components. The corresponding
    vector for the Fielder value is called the *Fielder vector*.'
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  The Fielder vector has both negative and positive components, and their
    resultant sum is zero.
  prefs: []
  type: TYPE_NORMAL
- en: We will use this concept once we study the process of spectral clustering in
    detail in the next section. We cover one more concept—the affinity matrix—before
    moving on to the process of spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Affinity matrix
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the adjacency matrix, if we replace the number of connections with the similarity
    of the weights, we will get the affinity matrix. If the points are completely
    dissimilar, the affinity will be 0; if they are completely similar, the affinity
    will be 1\. The values in the matrix represent different levels of similarity
    between data points.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5.1
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: The degree matrix is created by counting the number of connections. True or
    False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Laplacian is a transpose of the division of degree and adjacency matrix. True
    or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a graph on paper and then derive its adjacency and degree matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 5.3.2 The process of spectral clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now we have covered all the building blocks for spectral clustering. At a high
    level, the various steps can be noted as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We get the dataset and calculate its degree matrix and adjacency matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using them, we calculate the Laplacian matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then we calculate the first *k* eigenvectors of the Laplacian matrix. The *k*
    eigenvectors are the ones that correspond to the *k* smallest eigenvalues.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The resultant matrix formed is used to cluster the data points in k-dimensional
    space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NOTE  For more clarity on eigenvalues, the affinity matrix, and the Laplacian
    matrix, refer to the appendix.
  prefs: []
  type: TYPE_NORMAL
- en: We cover the process of spectral clustering using an example, as shown in figure
    5.8\. These steps are generally not done in real-world implementation, as we have
    packages and libraries to achieve them, but they are covered here to give you
    an idea of how the algorithm can be developed from scratch and how it works so
    that you have a better understanding on how to effectively utilize it. For the
    Python solution, we will use the libraries and packages only. Though it is possible
    to develop an implementation from scratch, it is not time-efficient to reinvent
    the wheel.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F08_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 Consider the example shown where we have some data points and they
    are connected. We will perform spectral clustering on this data.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'When we wish to perform the spectral clustering on this data, we follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create the adjacency matrix and degree matrix. We will leave this step up to
    you.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create the Laplacian matrix (see figure 5.9).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F09_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 The Laplacian matrix of the data. You are advised to create the degree
    and adjacency matrix and check the output.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 3\. Create the Fielder vector, as shown in figure 5.10, for the preceding Laplacian
    matrix. We create the Fielder vector as described in the Laplacian Matrix section.
    Observe how the sum of the matrix is zero.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F10_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.10 The Fielder vector is the output for the Laplacian matrix.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 4\. We can see that there are a few positive values and a few negative values.
    Based on the positive or negative values, we can create two distinct clusters.
    Figure 5.11 illustrates the process of spectral clustering.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F11_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.11 The two clusters are identified. This is a very simple example to
    illustrate the process of spectral clustering.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Spectral clustering is useful for image segmentation, speech analysis, text
    analytics, entity resolution, etc. The method does not make any assumptions about
    the shape of the data. Methods like k-means assume that the points are in a spherical
    form around the center of the cluster, whereas there is no such strong assumption
    in spectral clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Another significant difference is that in spectral clustering the data points
    need not have convex boundaries as compared to other methods where compactness
    drives clustering. Spectral clustering is sometimes slow since various matrices
    and their eigenvalues, Laplacians, etc., have to be calculated. With a large dataset,
    the complexity increases, and hence, spectral clustering can become slow, but
    it is a fast method when we have a sparse dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Spectral clustering requires building a matrix that nominally has the size of
    the number of items in a dataset squared because there is one column and one row
    for each element. For example, a modest dataset of a few million elements will
    require a matrix of several trillion elements! Storing that matrix verbatim requires
    terabytes of RAM and is something that is at the edge of what a very powerful
    and expensive server could do. There are techniques to mitigate the memory needs
    (such as not storing every single element separately), but they make working with
    the matrix more complicated. Moreover, finding the eigenvalues and even one eigenvector
    of such a large matrix is very time-intense. As such, spectral clustering is a
    viable approach generally for small datasets.
  prefs: []
  type: TYPE_NORMAL
- en: We will now proceed to the Python solution of the spectral clustering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Python implementation of spectral clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have covered the details of spectral clustering—it is time to get into the
    code. For this, we will create an artificial dataset and run a k-means algorithm
    and then spectral clustering to compare the results. The steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary libraries. These libraries are standard, except for
    a few that we will cover. `sklearn` is one of the most famous and sought-after
    libraries, and from `sklearn` we import `SpectralClustering`, `make_blobs`, and
    `make_circles`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Curate a dataset. We will use the `make_circles` method. Here, we take
    2,000 samples and represent them in a circle. The output is as follows (see figure
    5.12):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH05_F12_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.12 Curating a dataset using the `make_circles` method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '3\. Test this dataset with k-means clustering. The two colors show two different
    clusters, which overlap each other. The print version of the book will not show
    the colors, but the output of the Python code will. The same output is available
    in the GitHub repository (see figure 5.13):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH05_F13_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.13 Testing the dataset with k-means clustering
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '4\. Run the same data with spectral clustering. We find that the two clusters
    are being handled separately here (see figure 5.14):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH05_F14_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.14 The two clusters are being handled separately when using spectral
    clustering.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can observe here that the same dataset is handled differently by the two
    algorithms. Spectral clustering handles the dataset arguably better, as the circles
    that are separate are depicted separately.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Simulate various cases by changing the values in the dataset and running
    the algorithms. Observe the different outputs for comparison.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 5.5 Fuzzy clustering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far we have covered quite a few clustering algorithms. Did you wonder why
    a data point should belong to only one cluster? Why can’t a data point belong
    to more than one cluster? Have a look at figure 5.15: the red points in the right
    image (shown with an x in the print version) can belong to more than one cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F15_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.15 The figure on the left represents all the data points. The red points
    (those with an x in the print version) can belong to more than one cluster. In
    fact, we can allocate more than one cluster to each point. A probability score
    can be given for a point to belong to a particular cluster.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We know that clustering is used to group items in cohesive groups based on their
    similarities. The items that are similar are in one cluster, whereas the items
    that are dissimilar are in different clusters. The idea of clustering is to ensure
    the items in the same cluster are similar. When the items can be only in one cluster,
    it is called *hard clustering.* K-means clustering is a classic example of hard
    clustering. But if we reflect on figure 5.15, we can observe that an item can
    belong to more than one cluster. This is called *soft clustering.*
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  It is computationally cheaper to create fuzzy boundaries than to create
    hard clusters.
  prefs: []
  type: TYPE_NORMAL
- en: In fuzzy clustering, an item can be assigned to more than one cluster. The items
    that are closer to the center of a cluster will have a stronger belongingness
    to that cluster as compared to the points that are at the edge of the cluster.
    This is referred to as *membership*. It employs the least-square algorithm to
    find the most optimal location of an item. The optimal location that we derive
    from the least-square algorithm will be the probability space between two or more
    clusters. We will examine this concept in detail later.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.1 Types of fuzzy clustering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fuzzy clustering can be further divided into classical fuzzy algorithms and
    shape-based fuzzy algorithms. See figure 5.16\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F16_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.16 Fuzzy algorithms can be divided into the classical fuzzy algorithm
    and the shape-based fuzzy algorithm.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We will cover the fuzzy c-means (FCM) algorithm in detail next, but first we
    will review the rest of the algorithms briefly:'
  prefs: []
  type: TYPE_NORMAL
- en: The Gustafson-Kessel algorithm, sometimes called the GK algorithm, works by
    associating an item with a cluster and a matrix. GK results in elliptical clusters,
    and to modify as per varied structures in the datasets, GK uses the covariance
    matrix. It allows the algorithm to capture the elliptical properties of the cluster.
    GK can result in narrower clusters, and wherever the number of items is higher,
    those areas can be thinner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Gath-Geva algorithm is not based on an objective function. The clusters
    can result in any shape, because it is a fuzzification of statistical estimators.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The shape-based clustering algorithms are self-explanatory as per their names.
    A circular fuzzy clustering algorithm will result in circular-shaped clusters
    and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The FCM algorithm is the most popular fuzzy clustering algorithm. It was initially
    developed in 1973 by J.C. Dunn, and it has been improved multiple times. It is
    quite similar to k-means clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to figure 5.17\. In the first part of the figure (left), we have some
    items or data points. These data points can be a part of a clustering dataset
    like customer transactions, etc. In the second part of the figure (middle), we
    create a cluster for these data points. While this cluster is created, membership
    grades are allocated to each of the data points. These membership grades suggest
    the degree or the level to which a data point belongs to a cluster. We will shortly
    examine the mathematical function to calculate these values.
  prefs: []
  type: TYPE_NORMAL
- en: TIP  Do not get confused by the degree and the probabilities. If we sum these
    degrees, we may not get 1, as these values are normalized between 0 and 1 for
    all the items.
  prefs: []
  type: TYPE_NORMAL
- en: In the third part of the figure (right), we can see that point 1 is closer to
    the cluster center and thus belongs to the cluster to a higher degree than point
    2, which is closer to the boundary or the edge of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F17_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.17 Data points that can be clustered (left). The data points can be
    grouped into two clusters. For the first cluster, the cluster centroid is represented
    using a + sign (middle). Point 1 is much closer to the cluster center as compared
    to point 2\. So we can conclude that point 1 belongs to this cluster to a higher
    degree than cluster 2.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We will now venture into the technical details of the algorithm. This can get
    a little mathematically heavy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider we have a set of *n* items (equation 5.1):'
  prefs: []
  type: TYPE_NORMAL
- en: (5.1)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*x* = {*x*[1], *x*[2], *x*[3], *x*[4], *x*[5], . . ., *x*[*n*]}'
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply the FCM algorithm to these items. These *n* items are clustered into
    *c* fuzzy clusters based on some criteria. Let’s say that we will get from the
    algorithm a list of *c* cluster centers (equation 5.2):'
  prefs: []
  type: TYPE_NORMAL
- en: (5.2)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*c* = {*c*[1], *c*[2], *c*[3], *c*[4], *c*[5], . . ., *c*[*c*]}'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm also returns a partition matrix, which can be defined as equation
    5.3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch5-eqs-2x.png)'
  prefs: []
  type: TYPE_IMG
- en: (5.3)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Here, each of the elements in *w*[*i*][,][*j*] is the degree to which each of
    the elements in *X* belong to cluster *c*[*j*]. This is the purpose of the partition
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, we can get *w*[*i*][,][*j*]as shown in equation 5.4\. The proof
    of the equation is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch5-eqs-3x.png)'
  prefs: []
  type: TYPE_IMG
- en: (5.4)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'The algorithm generates centroids for the clusters too. The centroid of a cluster
    is the mean of all the points in that cluster, and the mean is weighted by their
    respective degrees of belonging to that cluster. If we represent it mathematically,
    we can write it like in equation 5.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch5-eqs-4x.png)'
  prefs: []
  type: TYPE_IMG
- en: (5.5)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In equations 5.4 and 5.5, we have a very important term: *m*. *m* is the hyperparameter
    used to control the fuzziness of the clusters. The values of *m* ≥ 1 and can be
    kept as 2 (a typically used value).'
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  The higher the value of *m*, the fuzzier the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now examine the step-by-step process in the FCM algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Start as we start in k-means clustering by choosing the number of clusters we
    wish to have in the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Allocate the weights randomly to each of the data points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The algorithm iterates until it has converged. Recall how the k-means algorithm
    converges, wherein we initiate the process by randomly allocating the centroids
    of clusters. And then iteratively we refine the centroids for each of the clusters
    until we get convergence. This is how k-means works. For FCM, we will utilize
    a similar process albeit with slight differences. We have added a membership value
    *w*[*i*][,][*j*] and *m*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For FCM, for the algorithm to converge we calculate the centroid for each of
    the clusters as per equation 5.6:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch5-eqs-4x.png)'
  prefs: []
  type: TYPE_IMG
- en: (5.6)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 5\. For each of the data points, we also calculate its respective coefficient
    for being in that particular cluster. We will use equation 5.4\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '6\. Now we should iterate until the FCM algorithm has converged. The cost function
    that we wish to minimize is given by equation 5.7:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch5-eqs-5x.png)'
  prefs: []
  type: TYPE_IMG
- en: (5.7)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Once this function has been minimized, we can conclude that the FCM algorithm
    has converged. In other words, we can stop the process as the algorithm has finished
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: This is a good time to compare this with the k-means algorithm. In k-means,
    we have a strict objective function that will allow only one cluster membership,
    while for FCM clustering, we can get different clustering membership based on
    the probability scores.
  prefs: []
  type: TYPE_NORMAL
- en: FCM is very useful for business cases where the boundary between clusters is
    not clear and stringent. Consider the field of bioinformatics, wherein a gene
    can belong to more than one cluster of genes. Another example is when we have
    overlapping datasets like in the fields of the marketing analytics or image segmentation
    where we might have a lot of complex, overlapping, and confusing datasets. FCM
    can give comparatively more robust results than k-means.
  prefs: []
  type: TYPE_NORMAL
- en: We will now proceed to the Python solution of FCM clustering using the libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5.2
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: Fuzzy clustering allows us to create overlapping clusters. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A data point can belong to one and only one cluster. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the value of *m* is lower, we get clusters with more precise boundaries.
    True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 5.5.2 Python implementation of FCM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have covered the process of FCM. We will now work on the Python implementation
    of FCM by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Declare a color palette, which will be used later for color coding the
    clusters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '3\. Define the cluster centers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '4\. Assign the weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '5\. Set the seed and then loop through the cluster centers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '6\. We will represent the data points first. See figure 5.18:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH05_F18_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.18 Representation of the data points
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '7\. Iterate different outputs with different values of cluster values and FPC
    (see figure 5.19):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F19_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.19 The output of the FCM algorithm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Observe the output of the code, where for the same datasets you can see the
    different clusters with different positions of the centers. To appreciate the
    colors, you will have to run the code.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Gaussian mixture model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we continue our discussion of soft clustering. Recall we introduced the
    GMM at the start of the chapter. Now we will study the concept and see the Python
    implementation of it.
  prefs: []
  type: TYPE_NORMAL
- en: First, let’s get an understanding of the *Gaussian distribution* or what is
    sometimes called *normal distribution*. You might recognize it as a bell curve;
    it usually refers to the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: In figure 5.20, observe that the distribution where the *µ* (mean) is 0 and
    *σ*² (standard deviation) is 1\. It is a perfect normal distribution curve. Compare
    the distribution in different curves here.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F20_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.20 A Gaussian distribution is one of the most famous distributions.
    Observe how the values of mean and standard deviation are changed and their effect
    on the corresponding curve.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The mathematical expression for Gaussian distribution is
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch5-eqs-6x.png)'
  prefs: []
  type: TYPE_IMG
- en: '![figure](../Images/verdhan-ch5-eqs-7x.png)'
  prefs: []
  type: TYPE_IMG
- en: (5.8)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The equation is also called the probability density function. In figure 5.20,
    observe the shape of the probability distribution where the *µ* is 0 and *σ*²
    is 1\. It is a perfect normal distribution curve. Compare the distribution in
    different curves in figure 5.20 where, by changing the values of the mean and
    standard distribution, we get different graphs.
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why we are using Gaussian distribution here. There is
    a very famous statistical theorem called the *central limit theorem*. The theorem
    states that if the variability of the data is due to a large number of unrelated
    causes, then the distribution can be approximated by a Gaussian curve. Also, the
    approximation becomes more and more accurate the more data is collected; that
    is, the more data we collect, the more Gaussian the distribution. This normal
    distribution can be observed across all walks of life and in chemistry, physics,
    mathematics, biology, or any other branch of science. That is the beauty of Gaussian
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The plot shown in figure 5.20 is 2D. We can have multidimensional Gaussian distribution
    too. In the case of a multidimensional Gaussian distribution, we will get a 3D
    figure as shown in figure 5.21\. Our input was a scalar in 1D. Now, instead of
    scalar, our input is a vector; the mean is also a vector and represents the center
    of the data. Hence, the mean has the same dimensionality as the input data. The
    variance is now the covariance matrix ∑. This matrix not only tells us the variance
    in the inputs but also comments on the relationship between different variables—for
    example, how the values of *y* are affected if the value of *x* is changed. Have
    a look at figure 5.21\. We can understand the relationship between the *x* and
    *y* variables here.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_F21_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.21 3D representation of a Gaussian distribution
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: NOTE  Covariance plays a significant role here. K-means does not consider the
    covariance of a dataset, which is used in the GMM model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine the process of GMM clustering. Imagine we have a dataset with
    *n* items. When we use GMM clustering, we do not find the clusters using the centroid
    method; instead, we fit a set of *k* Gaussian distributions to the dataset at
    hand. In other words, we have *k* clusters. We should determine the parameters
    for each of these Gaussian distributions, which are mean, variance, and weight
    of a cluster. Once the parameters for each of the distributions are determined,
    then we can find the respective probability for each of the *n* items to belong
    to *k* clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematically, we can calculate the probability as shown in equation 5.9\.
    The equation is used so we know that a particular point *x* is a linear combination
    of *k* Gaussians. The term *f*[*j*] is used to represent the strength of the Gaussian,
    and it can be seen in the second equation that the sum of such strength is equal
    to 1.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch5-eqs-8x.png)'
  prefs: []
  type: TYPE_IMG
- en: '![figure](../Images/verdhan-ch5-eqs-9x.png)'
  prefs: []
  type: TYPE_IMG
- en: (5.9)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For spectral clustering, we must identify the values of *f*, ∑, and *µ*. As
    you can imagine, getting the values of these parameters can be tricky. It is indeed
    a slightly complex process called the expectation-maximization (EM) technique,
    which we will cover next. This section is quite heavy on mathematical concepts
    and is optional. It is recommended for readers interested in understanding the
    deeper workings of the techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6.1 EM technique
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: EM is a statistical method to determine the correct parameters for a model.
    There are quite a few techniques that are popular; maximum likelihood estimation
    might be the most famous. But at the same time, there could be a few challenges
    with maximum likelihood. The dataset might have missing values or, in other words,
    be incomplete. Or it is possible that a point in the dataset is generated by two
    different Gaussian distributions. Hence, it will be very difficult to determine
    which distribution generated that data point. Here, EM can be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  K-means uses only mean while GMM utilizes both mean and variance of the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The variables that are generated in the process are called *latent variables*.
    Since we do not know the exact values of these latent variables, EM first estimates
    their optimum values using the current data. Once this is done, then the model
    parameters are estimated. Using these model parameters, the latent variables are
    again determined. And, using these new latent variables, new model parameters
    are derived. The process continues until a good enough set of latent values and
    model parameters are achieved that fit the data well. Let’s study that in more
    detail now. We will use the same example as in the last section.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine we have a dataset with *n* items. As mentioned, when we use GMM clustering,
    we do not find the clusters using the centroid method; instead, we fit a set of
    *k* Gaussian distributions to the dataset at hand. In other words, we have *k*
    clusters. We determine the parameters for each of these Gaussian distributions
    (mean, variance, and weight). Let’s say that mean is *µ*[1], *µ*[2], *µ*[3], *µ*[4]….
    *µ*[*k*] and covariance is ∑[1], ∑[2], ∑[3], ∑[4]…. ∑[*k*]. We can also have one
    more parameter to represent the density or strength of the distribution, and it
    can be represented by *f*.
  prefs: []
  type: TYPE_NORMAL
- en: We start with the expectation, or the E step. In this step, each data point
    is assigned to a cluster probabilistically. So, for each point, we calculate its
    probability of belonging to a cluster; if this value is high, the point is in
    the correct cluster; otherwise, the point is in the wrong cluster. In other words,
    we calculate the probability that each data point is generated by each of the
    *k* Gaussians.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Since we are calculating probabilities, these are called soft assignments.
  prefs: []
  type: TYPE_NORMAL
- en: The probability is calculated using the formula in equation 5.10\. If we look
    closely, the numerator is the probability, and then we normalize by the denominator.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch5-eqs-10x.png)'
  prefs: []
  type: TYPE_IMG
- en: (5.10)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the expectation step, for a data point *x*[*i*][,][*j*], where *i* is the
    row and *j* is the column, we are getting a matrix where rows are represented
    by the data points and columns are their respective Gaussian values.
  prefs: []
  type: TYPE_NORMAL
- en: When the expectation step is finished, we will perform the maximization or the
    M step. In this step, we will update the values of *µ*, ∑, and *f* using the formula
    in equation 5.7\. Recall, in k-means clustering, we simply take the mean of the
    data points and move ahead. We do something similar here albeit use the probability
    or the expectation we calculated in the last step.
  prefs: []
  type: TYPE_NORMAL
- en: The three values can be calculated using the equations below. Equation 5.7 is
    the calculation of the covariances ∑[*j*], of all the points, which is then weighted
    by the probability of that point being generated by Gaussian *j* as shown in equation
    5.11\. The mathematical proofs are beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch5-eqs-11x.png)'
  prefs: []
  type: TYPE_IMG
- en: (5.11)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The mean *µ*[*j*], is determined by equation 5.12\. Here, we determine the mean
    for all the points, weighted by the probability of that point being generated
    by Gaussian *j*.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch5-eqs-12x.png)'
  prefs: []
  type: TYPE_IMG
- en: (5.12)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Similarly, the density or the strength is calculated by equation 5.13, where
    we add all the probabilities for each point to be generated by Gaussian *j* and
    then divide by the total number of points *N*.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/verdhan-ch5-eqs-13x.png)'
  prefs: []
  type: TYPE_IMG
- en: (5.13)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Based on these values, new values for ∑, *µ*, and *f* are derived, and the process
    continues until the model converges. We stop when we can maximize the log-likelihood
    function.
  prefs: []
  type: TYPE_NORMAL
- en: It is a complex mathematical process. We have covered it to give you an in-depth
    understanding of what happens in the background of the statistical algorithm.
    The Python implementation is much more straightforward than the mathematical concept.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 5.3
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Answer these questions to check your understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian distribution has a mean equal to 1 and a standard deviation equal to
    0\. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GMM models do not consider the covariance of the data. True or False?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 5.6.2 Python implementation of GMM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will first import the data, and then we will compare the results using k-means
    and GMM. We follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the libraries and the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '2\. Drop any NA from the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '3\. Fit a `kmeans` algorithm. We are keeping the number of clusters as 5\.
    Please note that we are not saying that this is an ideal number of clusters. The
    number of clusters is only for illustrative purposes. We declare a variable k-means
    and then use five clusters. The dataset is fit next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 4\. Plot the clusters. First, a prediction is made on the dataset, and then
    the values are added to the data frame as a new column. The data is then plotted
    with different colors representing different clusters. The print version of the
    book will not show the different colors, but the output of the Python code will.
    The same output is available in the GitHub repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output is as follows (see figure 5.22):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH05_F22_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.22 Outcome of plotting the clusters after fitting the `kmeans` algorithm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '5\. Fit a GMM model. Note that the code is the same as the k-means algorithm,
    only the algorithm’s name has changed from k-means to `GaussianMixture`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '6\. Plot the results. The output is as follows (figure 5.23):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![figure](../Images/CH05_F23_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.23 Outcome of plotting the clusters after fitting a GMM algorithm
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 7\. Run the code with different values of clusters to observe the difference.
    In the following plots, the left one is k-means with two clusters, while the right
    is GMM with two clusters. There are a few points that are classified differently
    in the two clustering approaches. The print version of the book will not show
    the different colors, but the output of the Python code will. The same output
    is available in the GitHub repository, too (see figure 5.24).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH05_UN01_Verdhan.png)![figure](../Images/CH05_F24_Verdhan.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.24 K-means with two clusters (left) and GMM with two clusters (right)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Gaussian distribution is one of the most widely used data distributions used.
    If we compare k-means and the GMM model, we see that k-means does not consider
    the normal distribution of the data. The relationship of various data points is
    also not considered in k-means.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  K-means is a distance-based algorithm; GMM is a distribution-based algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In short, it is advantageous to use GMM models for creating clusters, particularly
    when we have overlapping datasets. It is a useful technique for financial and
    price modeling, natural language processing-based solutions, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Concluding thoughts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we have explored three complex clustering algorithms. You might
    have felt the mathematical concepts were a bit heavy. They are indeed, but they
    provide a deeper understanding of the process. These algorithms are not necessarily
    the best ones for every problem. Ideally, in a real-world business problem, we
    start with classical clustering algorithms (k-means, hierarchical, and DBSCAN).
    If we do not get acceptable results, we can try the more complex algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Many times, a data science problem is equated to the choice of algorithm, which
    it is not. The algorithm is certainly an important ingredient of the entire solution,
    but it is not the only one. In real-world datasets, there are a lot of variables,
    and the amount of data is also quite high. The data has a lot of noise. We should
    account for all of these factors when we shortlist an algorithm. Algorithm maintenance
    and refreshing are also considerations. All of these aspects are covered in detail
    in the last chapter of the book.
  prefs: []
  type: TYPE_NORMAL
- en: 5.8 Practical next steps and suggested readings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following provides suggestions for what to do next and offers some helpful
    reading:'
  prefs: []
  type: TYPE_NORMAL
- en: In chapter 2, we did clustering using various techniques. Use the datasets from
    there and perform spectral clustering, GMM, and FCM clustering to compare the
    results. Datasets provided at the end of chapter 2 can be used for clustering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Get the credit card dataset for clustering from Kaggle ([https://mng.bz/oKwd](https://mng.bz/oKwd))
    and data from the famous Iris dataset, which we used earlier ([https://www.kaggle.com/uciml/iris](https://www.kaggle.com/uciml/iris)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Refer to the book *Computational Network Science* by Henry Hexmoor to study
    the mathematical concepts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Get spectral clustering papers from the following links and study them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On spectral clustering: analysis and an algorithm: [https://mng.bz/nRwa](https://mng.bz/nRwa)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spectral clustering with eigenvalue selection: [https://mng.bz/vKw7](https://mng.bz/vKw7)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The mathematics behind spectral clustering and the equivalence to principal
    component analysis: [https://arxiv.org/pdf/2103.00733v1.pdf](https://arxiv.org/pdf/2103.00733v1.pdf)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Get GMM papers from the following links and explore them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“GMM Estimation for High Dimensional Panel Data Models”: [https://mng.bz/4agw](https://mng.bz/4agw)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '“Application of Compound Gaussian Mixture Model in the Data Stream”: [https://ieeexplore.ieee.org/document/5620507](https://ieeexplore.ieee.org/document/5620507)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Get FCM papers from the following links and study them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '“FCM: The Fuzzy c-Means *Clustering* Algorithm”: [https://mng.bz/QDXG](https://mng.bz/QDXG)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Survey on Fuzzy c-Means Clustering Techniques: [https://www.ijedr.org/papers/IJEDR1704186.pdf](https://www.ijedr.org/papers/IJEDR1704186.pdf)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '“Implementation of Fuzzy C-Means and Possibilistic C-Means Clustering Algorithms,
    Cluster Tendency Analysis and Cluster Validation”: [https://arxiv.org/pdf/1809.08417.pdf](https://arxiv.org/pdf/1809.08417.pdf)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spectral clustering focuses on data point affinity rather than location for
    clustering. It works well with complex data shapes where traditional algorithms
    like k-means may not suffice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectral clustering utilizes graph theory and connectivity, relying on eigenvalues,
    the Laplacian matrix, and the affinity matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process includes calculating degree, adjacency, Laplacian matrices, and
    the Fielder vector for clustering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means clustering uses centroids, whereas spectral clustering’s focus is on
    connectivity and data point similarities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectral clustering can require substantial computational resources due to matrix
    operations and is suitable for smaller datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fuzzy clustering allows data points to belong to multiple clusters, introducing
    “membership” for data items.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FCM is a key algorithm in fuzzy clustering, utilizing membership degrees and
    controlling fuzziness through hyperparameter *m*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GMM employs Gaussian distributions for soft clustering, factoring in dataset
    covariance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GMM is suitable for overlapping datasets and considers the relationship between
    data points, unlike k-means.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The EM technique is used in GMM to estimate parameters iteratively.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GMM models are advantageous for financial modeling, natural language processing,
    and cases with overlapping data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fuzzy and GMM are soft clustering methods, allowing detailed membership and
    probability assignment to data points.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spectral clustering supports applications in image segmentation, speech analysis,
    and text analytics without assuming data shape constraints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
