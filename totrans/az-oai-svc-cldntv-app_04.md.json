["```py\nPOST https://{your-resource-name}.openai.azure.com/openai/deployments/\n  {deployment-id}/***completions***?api-version={api-version}\n```", "```py\ncurl https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/\\\n  YOUR_DEPLOYMENT_NAME/completions?api-version=YYYY-MM-DD\\\n  -H \"Content-Type: application/json\" \\\n  -H \"api-key: YOUR_API_KEY\" \\\n  -d \"{\n        \\\"prompt\\\": \\\"The best thing in life\\\",\n        \\\"max_tokens\\\": 5,  \n        \\\"n\\\": 1\n      }\"\n```", "```py\n{ `\"id\"``:` `\"cmpl-4kGh7iXtjW4lc9eGhff6Hp8C7btdQ\"``,` ```", "```py```", "````py `\"object\"``:` `\"text_completion\"``,` ````", "`````` `\"created\"``:` `1646932609``,` ```py``````", "``````py```` `\"choices\"``:` `[` ```py``````", "```py```", "```py```", "```py```", "````` `\"finish_reason\"``:` `\"length\"` ```py` `}` ``` `]` `` `}` `` ```py ```` ```py`` `````", "``````py` ``````", "``````py``` ``````", "```` ```py````", "```py` ```", "```py```", "``` ```", "```````py```\n```\n\n ```py` ``` ``The answers (completions) contain the `finish_reason` parameter. `finish_reason` defines why the model stopped generating more information; for most cases this will be due to `max_tokens`, which stops the model once it reaches the limit. However, there is another option that we will explore in [Chapter 4](ch04.html#additional_cloud_and_ai_capabilities) that stops the model due to what we call *content filters*.`` ```py ```` \n\n ```py``` ````` ```py`[Chat completions](https://oreil.ly/ZJOLp)      Dedicated API for chat scenarios (and the only supported one for future model versions), including the configuration parameters we previously reviewed with the Chat playground. This includes input parameters we discussed for the Azure OpenAI Playground, such as `temperature` and `max_tokens`. There is one important parameter for chat messages, known as the [ChatRole](https://oreil.ly/WLv1g). This allows you to split the interactions based on different roles:    System      Helps you set the behavior of the assistant.      User      Provides input for chat completions.      Assistant      Provides responses to system-instructed, user-prompted input.      Function      Provides function results for chat completions. We will explore this concept later in this chapter, after we cover the different Azure OpenAI APIs.      The sequence for a typical chat scenario follows these steps:    1\\. Resource creation      Using a similar structure to what you have seen in a regular completion API call (including the date as the API version). The regular POST operation for chat completion is:    ``` POST https://{your-resource-name}.openai.azure.com/openai/deployments/   {deployment-id}/chat/completions?api-version={api-version} ```py      2\\. System message      This is how you set the context of the chat engine, by defining the scope of the discussion, allowed or forbidden topics, etc. The system message is also called the context prompt or *meta-prompt*. The [`messages` parameter](https://oreil.ly/vFEYS), along with the [`role` subparameter](https://oreil.ly/y5HFq), is the place where you will define your system message, using:    ``` {   \"messages\": [     {       \"role\": `\"system\"`,       \"content\": `\"the context and system message to add to your chat\"`     }   ] } ```py      3\\. User-assistant interaction      This leverages the same `messages` parameter, with the *user* and *assistant* roles. The structure for both roles is similar to what we have discussed for the system message, and the response includes the same `finish-reason` parameter that will give you a hint about the result (i.e., if the completion has finished due to the `max_tokens` assigned to the answer, or if there is a filtering reason due to negative topic detection).        [Image generation](https://oreil.ly/bm-7a)      The API call to generate images based on text-to-image DALL·E models. As with the visual playground, the input parameters include the text-based prompt, and two optional inputs such as the number `n` of desired images (if you don’t include it, the system will generate only one image), and the size (by default 1024×1024, with alternative 256×256 and 512×512 options). The POST operation to create an image generation resource is:    ``` POST https://{your-resource-name}.openai.azure.com/openai/\\   images/generations:submit?api-version={api-version} ```py    Here is an example of a [curl (command-line tool for downloading and uploading files from various protocols and servers)](https://oreil.ly/xApmg) request:    ``` curl -X POST \\   https://{your-resource-name}.openai.azure.com/openai/deployments/\\   {deployment-id}/images/generations?api-version=2023-12-01-preview \\   -H \"Content-Type: application/json\" \\   -H \"api-key: YOUR_API_KEY\" \\   -d '{         \"prompt\": \"An avocado chair\",         \"size\": \"1024x1024\",         \"n\": 3,         \"quality\": \"hd\",         \"style\": \"vivid\"       }' ```py    The end-to-end process includes three different steps:    1.  *Request*the image generation ([via POST operation](https://oreil.ly/kPf-m)), which helps you pre-generate the images based on the text-based input prompt. It returns an operation ID that you will leverage for the next step.           2.  *Get*the result of the image generation ([GET operation](https://oreil.ly/lxX0B)), which allows you to recover the pre-generated images for the specific operation ID.           3.  *Delete*the previously loaded images ([DELETE operation](https://oreil.ly/5UfTB)) from the server, for the specific Azure OpenAI resource, and the existing operation ID. If you don’t use this option, the images will be automatically deleted after 24 hours.                [Speech to text](https://oreil.ly/hKakE)      Based on the [Azure OpenAI Whisper model](https://oreil.ly/SJNcT), these APIs allow you create transcriptions from audio pieces, for a variety of languages and accents, with great performance and the possibility to combine it with other Azure OpenAI models. You can specify the input audio file, language, discussion style, output format (by default a JSON file), etc. This Azure OpenAI speech-to-text (S2T) feature has a limitation of 25 MB for the input audio file, but you can leverage the [batch transcription mode of Azure AI Speech](https://oreil.ly/NnMTz) (not Azure OpenAI, but the [Azure AI Speech services for voice ↔ text features](https://oreil.ly/-HLPL)) to transcribe bigger files. The POST operation looks similar to the previous APIs:        ``` POST https://{your-resource-name}.openai.azure.com/openai/deployments/   {deployment-id}/audio/transcriptions?api-version={api-version} ```py    The corresponding curl request (illustrative example):    ``` curl $AZURE_OPENAI_ENDPOINT/openai/deployments/MyDeploymentName/\\   audio/transcriptions?api-version=2023-09-01-preview \\   -H \"api-key: $AZURE_OPENAI_KEY\" \\   -H \"Content-Type: multipart/form-data\" \\   -F file=\"@./wikipediaOcelot.wav\" ```py      [Embeddings](https://oreil.ly/imKOS)      This API call allows you to generate embeddings from specific text inputs, from some of the architectures you will see in this chapter. The model and its specific input length will depend on [model availability](https://oreil.ly/gvAHr) at the time of your implementation. The POST operation is similar to the previous ones, and the dynamic is as simple as [requesting the embeddings](https://oreil.ly/xFJTh) for a text input and [obtaining a JSON response](https://oreil.ly/yYCuU) with the generated embeddings, for you to store (we will see several vector store/database options by the end of the chapter) and leverage them later:    ``` POST https://{your-resource-name}.openai.azure.com/openai/deployments/   {deployment-id}/embeddings?api-version={api-version} ```py    And the corresponding curl example:    ``` curl https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/\\   YOUR_DEPLOYMENT_NAME/embeddings?api-version=2023-05-15\\   -H 'Content-Type: application/json' \\   -H 'api-key: YOUR_API_KEY' \\   -d '{\"input\": \"Sample Document goes here\"}' ```py      [Fine-tuning](https://oreil.ly/1pqcT)      As we reviewed at the beginning of this chapter, one of the implementation options includes the ability to fine-tune pre-built models with your specific, available information. We will see more details later in this chapter, but for now keep in mind that if you choose this option, there is a specific set of APIs that you can leverage to create, manage, explore, and delete new fine-tuning “jobs.” Also, you will handle your own input files for the fine-tuned models.      Other relevant APIs      Other relevant APIs include the following:    [Bing Search](https://oreil.ly/2yZuu)      The Bing Search API allows you to leverage Microsoft Bing’s search engine for your own development. You can extend the capabilities of your Azure OpenAI–enabled implementations with live search functionalities.      [Form Recognizer (currently known as Azure AI Document Intelligence)](https://oreil.ly/vxtJA)      This helps you transform information from forms and images into structured data. It includes advanced optical character recognition (OCR) functionalities that will support your Azure OpenAI development with specific data sources such as PDF or DOC files.      [Azure AI Search (previously known as Azure Cognitive Search)](https://oreil.ly/wp6r8)      One of the most important elements for RAG architectures, for both vectors and index approaches.      In addition to these APIs, there is an Azure [OpenAI library for .NET developers](https://oreil.ly/9XMBN) and the [OpenAI library for Python](https://oreil.ly/-cwGH), which essentially replicates the features of the official API for a .NET development environment. It provides an interface with the rest of the Azure SDK ecosystem, and it facilitates the connection to Azure OpenAI resources or to non–Azure OpenAI endpoints.```` ```py`` ``````py  ``` `` `This set of visual and development interfaces are your toolkit for most of the Azure OpenAI implementations out there. They are rapidly evolving, but the links to the official documentation will help you access updated information any time. Now, before moving on to the implementation approaches, let’s take a look at a powerful feature that will enable your generative AI systems to interact with other external APIs: function calling.` `` ```py  ``` `` `### Interoperability features: Function calling and “JSONization”    The [Azure OpenAI function calling](https://oreil.ly/bQdsv) option is a way to leverage language models to generate API calls and structure data outputs based on a specific target format. Technically, it is one of the options within the Chat Completion API—the [function](https://oreil.ly/WLv1g) chat role. You can see [several samples](https://oreil.ly/0nhYM) on how to use this functionality, but it essentially relies on the following steps:    1.  Calling the Chat Completions API, including the functions (based on the official [FunctionDefinition format](https://oreil.ly/5Q-8c)) and the user’s input           2.  Using the model’s chat response to call your API or function           3.  Calling the Chat Completions API again, including the response from your function, to get a final response              This is a relatively new functionality, so you can expect some feature improvements over time. You can always check the [official documentation](https://oreil.ly/UAYNH) to get the latest details and advice. Additionally, you can also explore the [JSON mode](https://oreil.ly/Fi3-l) for Azure OpenAI, as it allows you to get a JSON object from the Chat Completions API answer, a powerful feature for interoperability purposes.    This completes the first part of this section. You have learned about the knowledge domains, how to leverage different building blocks to improve and increase the level of knowledge of your generative AI solutions, and the availability tools you will use for implementation. Now, we will move to the next part of this chapter, in which we will explore some of the most relevant development approaches, based on the industry’s best practices. Let’s get started.` `` ```py  ``````py```````", "````` ```py`````", "```````py ``````py```````", "``` ## Potential Implementation Approaches    There are several ways to implement generative AI applications with Azure OpenAI Service. The type of implementations you use will mostly depend on your specific use case, as well as the technical and financial context for adoption. This means there are situations where the most expensive option is not always the best, or other options may have limitations, such as when we don’t have specific data besides our website, etc. Let’s explore the primary implementation types, based on the customization levels of [Figure 3-16](#fig_16_implementation_approaches_with_azure_openai).  ![](assets/aoas_0316.png)  ###### Figure 3-16\\. Implementation approaches with Azure OpenAI Service    As you can see from the figure, you can customize a model by preparing a good meta-prompt, adjusting technical parameters, providing one or a few “shots” as examples to guide the model, and implementing fine-tuning and/or grounding techniques. The next sections will go into the details of how to do all of this.    ### Basic Azure ChatGPT instance    A basic, private GPT type of instance is the simplest kind of implementation, and one of the most popular Azure OpenAI cases nowadays. When companies want to have a private “ChatGPT” for their employees, this is the answer. It keeps your own data safe and private and deploys the instance within your own cloud infrastructure. It’s one of the favorite options for internal use with employees.    The deployment process is relatively simple:    1.  Within your Azure OpenAI Studio, deploy a GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, or GPT-4o model instance. This type of model is technically similar to what ChatGPT is, and it will deliver that level of performance. Remember to choose the specific geographic region that is closest to you.           2.  Once you have created the resource, go to the visual playground. There, you will see a left menu with the option “Chat.”           3.  Once there, you can prepare the [system message](https://oreil.ly/OmKQO) / meta-prompt to contextualize the chatbot by telling it something like “You are an AI assistant for company X, to answer questions from the employees” (internal use) or “You are an AI assistant for company X with website Y. If anyone asks something that is not related to this topic, say you cannot answer” (for clients).           4.  You can also customize parameters such as the max length of the answers or the temperature of the messages, which is a metric between 0 and 1 to define the level of creativity of the model.           5.  Once you have tested performance and you are ready to deploy the model, you can come back to the resource page (Azure portal) and find both the endpoint and the keys for that specific resource. That page contains examples of code to for calling the APIs.              The end-to-end architecture ([Figure 3-17](#fig_17_simplified_azure_chatgpt_architecture)) is pretty simple—a pre-deployed model that we can directly consume from our applications, based on the existing endpoints and APIs.  ![](assets/aoas_0317.png)  ###### Figure 3-17\\. Simplified Azure ChatGPT architecture    This type of implementation is good enough for internal company cases where you don’t require any customization based on private data, for example, internal chatbots for employee productivity based on general internet information, or search engines for intranet sites. For the rest of the cases where there is some custom data involved, we will explore other options. Let’s dig into the first of them next.    ### Minimal customization with one- or few-shot learning    Besides the baseline model, and system message/meta-prompt and parameters customization, there is an option to perform *one- or few-shot learning*, which means providing the LLM with examples of discussions based on the expected output for a specific topic. This is a useful and simple option for small adjustments, and it relies on a very similar architecture to the previous one, with relatively light changes. The main difference when compared to the previous approach is the inclusion of one or few examples to guide the LLM before starting to use it ([Figure 3-18](#fig_18_one_few_shot_learning_architecture)).  ![](assets/aoas_0318.png)  ###### Figure 3-18\\. One/few-shot learning architecture    The one-shot/few-shot learning process can be achieved in several ways:    *   Via APIs (code)               *   Use the Chat Completions API with GPT-4 and other models that are designed to take input formatted in a chat-like transcript. You can provide conversational examples that are used by the model for in-context learning.                       *   Use the Completions API with the GPT-3 models, which can take a string of text with no specific format rules. You can provide a set of training examples as part of the prompt to give additional context to the model.                   *   Via playground (visual)               *   Use the Chat playground to interact with GPT-4, GPT-4o, etc. You can add few-shot examples in the chat transcript and see how the model responds.                       *   Use the Completions playground to interact with the GPT-x models. You can write your prompt with few-shot examples and see how the model completes it.                      Overall, all these customizations are intended to improve the performance of the model versus a regular vanilla “ChatGPT” implementation like the one we previously explored, but there are ways to retrain the model in a deeper way, like the one we will explore next.    ### Fine-tuned GPT models    As mentioned earlier in the chapter, there are different ways to customize an LLM to adjust its knowledge scope. Most of them rely on the orchestration/combination of the LLM with other knowledge pieces, without really combining the data sources (i.e., grounding). In this case, we will focus on the only way to “retrain” an Azure OpenAI model with custom company data: the [Azure OpenAI Service fine-tuning feature](https://oreil.ly/T0GP8).    This approach may have some advantages for companies with very specific and valuable data intellectual property, but its cost (you will need to add hosting cost to the regular API calls for the fine-tuning process) and technical complexity will probably lead you (and most of the adopters out there) to other kinds of grounding approaches with better performance/cost balance.    Also, the fine-tuning feature relies on a very special kind of training process. It is not the regular label-based training process you can do, for example, in classification tasks with traditional AI models. We are talking about a new kind of supervised process that leverages Azure OpenAI’s prompting system to inject information based on the [JSON Lines (JSONL) file format](https://oreil.ly/SdBph).    For example, with GPT-3.5 Turbo, you will leverage the system and user roles to reeducate the model:    ```", "```py```", "````py````", "```py```", "````py````", "```py```", "````py``` `\"content\"``:` `\"Marv is a factual chatbot that is also sarcastic.\"` ````", "`````` `},` ```py``````", "``````py```` `\"role\"``:` `\"user\"``,` ```py``````", "```py```", "```py```", "```py```", "````` `\"content\"``:` `\"Oh, just some guy named William Shakespeare. Heard of him?\"` ```py` `}` ``` `]` `` `}` `` ```py ```` ```py`` `````", "``````py` ``````", "``````py``` ``````", "```` ```py````", "```py` ```", "```py```", "``` ```", "```````py``` ``````py```````", "``````py``````", "```   ```", "``` ```", "```py` Other legacy models such as DaVinci require a prompt/completion format based on a question-answer logic:    ```", "```py   `` `This new way to inject data and knowledge allows us to reeducate the model in a very granular manner, but it is a complex way to do so. You can see the overall architecture in [Figure 3-19](#fig_19_azure_openai_fine_tuning_architecture), in which you will basically customize the model, based on a fine-tuning process that relies on specific organizational data.  ![](assets/aoas_0319.png)  ###### Figure 3-19\\. Azure OpenAI fine-tuning architecture    The steps to perform *fine-tuning* with Azure OpenAI Service are:    1.  *Prepare your dataset* in JSONL format. For recent models such as GPT-3.5 Turbo, GPT-4, and GPT-4o, you will leverage the Chat Completions API structure for system and user messages.           2.  Launch the *custom model wizard* from Azure OpenAI Studio, as shown in [Figure 3-20](#fig_20_azure_openai_custom_model_wizard), to train your new customized model.          ![](assets/aoas_0320.png)          ###### Figure 3-20\\. Azure OpenAI: custom model wizard           3.  *Select a base model* (e.g., GPT-3.5 Turbo), choosing your training data and, optionally, your validation data to evaluate model performance. Those datasets are the JSON files you previously prepared.           4.  Review your choices and *launch the training* of the new customized model. Check the status of your customized model and wait for the training to finish.           5.  *Deploy your customized model* for use in an application or service, via APIs.              All these options can work [depending on the type of application](https://oreil.ly/cK_7b) and the intended scope of the model customization. However, there are ways to combine the LLM with internal data sources, from which you can extract knowledge, and then refer to that information from the Azure OpenAI completion and chat completion models. This is what we call [RAG](https://oreil.ly/26QYs) or grounding, and there are different ways to implement it. The next sections contain different grounding alternatives.` `` ```", "```py`` ```", "```py  ```", "```py  ```", "```py ```", "```py```", "````py``` ````", "```````py```` ```py```````", "```````", "``` `` `# Conclusion    This chapter includes not only the available visual and code-based tools for your Azure OpenAI implementations, but also the recommended implementation approaches, to help you understand the differences between regular, fine-tuned, and grounded LLMs. Once again, there is not a perfect or a single way to do it. All of these approaches try to leverage the existing power of the Azure OpenAI models, and the ability to increase the knowledge scope of your applications with examples, internal data sources, live internet search, etc. In [Chapter 4](ch04.html#additional_cloud_and_ai_capabilities) we’ll take a look at additional building blocks for your generative AI development.` `` ```"]