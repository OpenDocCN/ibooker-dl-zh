["```py\narray(['onehot_encoding__room_type_Entire home/apt',\n       'onehot_encoding__room_type_Hotel room',\n       'onehot_encoding__room_type_Private room',\n       'onehot_encoding__room_type_Shared room',\n       'ordinal_encoding__neighbourhood_more_than_30',\n       'ordinal_encoding__type_of_accommodation',\n       'target_encoding__coordinates', 'numeric__minimum_nights',\n       'numeric__number_of_reviews', 'numeric__days_since_last_review',\n       'numeric__reviews_per_month',\n       'numeric__calculated_host_listings_count',\n       'numeric__availability_365', 'numeric__score',\n       'numeric__number_of_reviews_ltm',\n       'numeric__number_of_reviews_ltm_ratio',\n       'numeric__number_of_bedrooms', 'numeric__number_of_beds',\n       'numeric__number_of_baths', 'numeric__imperial_palace_distance',\n       'numeric__nearest_convenience_store',\n       'numeric__nearest_train_station', 'numeric__nearest_airport',\n       'numeric__nearest_bus_station', 'numeric__nearest_subway',\n       'binary__is_new', 'binary__is_studio', 'binary__has_shared_bath',\n       'binary__has_half_bath'], dtype=object)\n```", "```py\nxgb_params =  {'booster': 'gbtree',                          ①\n               'objective': 'reg:tweedie', \n               'n_estimators': 932, \n               'learning_rate': 0.08588055025922144, \n               'subsample': 0.9566295202123205, \n               'colsample_bytree': 0.6730567082779646, \n               'max_depth': 7, \n               'min_child_weight': 6, \n               'reg_lambda': 6.643211493348415e-06, \n               'reg_alpha': 7.024597970671363e-05, \n               'tweedie_variance_power': 1.6727891016980427}\n\nfrom sklearn.metrics import r2_score  \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error              ②\nfrom XGBoost import XGBRegressor\nimport numpy as np\n\nxgb = XGBRegressor(**xgb_params)                             ③\n\ncv_splits = cv.split(X, y=neighbourhood_more_than_30)        ④\n\nr2_scores = []\nrmse_scores = []\nmae_scores = []\nxgb_oof_preds = np.zeros(len(X))\n\nfor train_index, test_index in cv_splits:                    ⑤\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    xgb.fit(X_train, y_train)\n    y_pred = xgb.predict(X_test)                             ⑥\n    xgb_oof_preds[test_index] = y_pred\n\n    r2_scores.append(r2_score(y_test, y_pred))               ⑦\n    rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n    mae_scores.append(mean_absolute_error(y_test, y_pred))\n\nprint(f\"Mean cv R-squared: {np.mean(r2_scores):.3f}\")        ⑧\nprint(f\"Mean cv RMSE: {np.mean(rmse_scores):.3f}\")\nPrint(f\"Mean cv MAE: {np.mean(mae_scores):.3f}\")\n```", "```py\nfrom fastai.tabular.all import *\n\nprocs = [FillMissing, Normalize, Categorify]\ncat_vars = [\n    col for col in airbnb_tokyo.columns \n    if \"onehot_encoding__\" in col\n    or ordinal_encoding__\" in col \n    or \"binary__\" in col\n]\ncont_vars = [\n    col for col in airbnb_tokyo.columns \n    if \"numeric__\" in col \n    or \"target_encoding__\" in col\n]\ndep_var = 'target'\n\ncv_splits = cv.split(X, y=neighbourhood_more_than_30)\n\nr2_scores = []\nrmse_scores = []\nmae_scores = []\ndnn_oof_preds = np.zeros(len(X))\n\nfor k, (train_index, test_index) in enumerate(cv_splits):\n    X_train = airbnb_tokyo.set_index(\"listing_id\").iloc[train_index].copy()\n    X_test = airbnb_tokyo.set_index(\"listing_id\").iloc[test_index].copy()\n    y_test = airbnb_tokyo[\"target\"].iloc[test_index].copy()\n\n    tab = TabularPandas(\n        X_train, procs, cat_vars, cont_vars, \n        dep_var, y_block=RegressionBlock(),                       ①\n        splits=RandomSplitter(\n             valid_pct=0.2, seed=0)(range_of(X_train)),\n        inplace=True, \n        reduce_memory=True\n    )\n\n    dls = tab.dataloaders(bs=128)                                 ②\n    y_range = torch.tensor([0, X_train['target'].max() * 1.2])\n    tc = tabular_config(ps=[0.001, 0.01], embed_p=0.04, y_range=y_range)\n    learn = tabular_learner(dls, layers=[1000,500],               ③\n                            metrics=mae,\n                            config=tc,\n                            loss_func=L1LossFlat())\n    with learn.no_bar(), learn.no_logging():\n        lr = learn.lr_find(show_plot=False)\n        learn.fit_one_cycle(80, lr.valley)                        ④\n\n    dl = learn.dls.test_dl(X_test)\n    y_pred = (\n        learn.get_preds(dl=dl)[0]\n        .numpy()\n        .ravel()\n    )                                                             ⑤\n    dnn_oof_preds[test_index] = y_pred\n\n    r2_scores.append(r2_score(y_test, y_pred))                    ⑥\n    rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))\n    mae_scores.append(mean_absolute_error(y_test, y_pred))\n    print(f\"CV Fold {k} MAE: {mae_scores[-1]:.3f}\")\n\nprint(f\"\\nMean cv R-squared: {np.mean(r2_scores):.3f}\")\nprint(f\"Mean cv RMSE: {np.mean(rmse_scores):.3f}\")\nprint(f\"Mean cv MAE: {np.mean(mae_scores):.3f}\")\n```", "```py\npredictions = pd.DataFrame(\n    {'xgb': xgb_oof_preds, 'fastai': dnn_oof_preds}\n)                                                             ①\n\navg_fastai_over_70000 = predictions.loc[\n    predictions['xgb'] > 70000, 'fastai'\n].mean()                                                      ②\navg_xgb_over_70000 = predictions.loc[\n    predictions['xgb'] > 70000, 'xgb'\n].mean()\nprint(f\"Average prediction values when xgb > 70000:\",\n      f\"fastai:{avg_fastai_over_70000:0.2f}\",\n      f\"xgb:{avg_xgb_over_70000:0.2f}\")\n\navg_fastai_under_70000 = predictions.loc[predictions['xgb'] <= 70000, 'fastai'].mean()\navg_xgb_under_70000 = predictions.loc[predictions['xgb'] <= 70000, 'xgb'].mean()\nprint(f\"Average prediction values when xgb <= 70000: fastai:{avg_fastai_under_70000:0.2f}    \n      xgb:{avg_xgb_under_70000:0.2f}\")\n```", "```py\nblend_list = [\n    [1., 0.], [0., 1.], [0.25,0.75],\n    [0.75,0.25],[.5, .5]\n]                                                            ①\nfor a, b in blend_list:\n    print(f\"XGBoost weight={a}, DNN weight={b}\")\n    blended_oof_preds = (\n        xgb_oof_preds * a + dnn_oof_preds * b\n    )                                                        ②\n    r2 = r2_score(blended_oof_preds, y)                      ③\n    rmse = np.sqrt(mean_squared_error(blended_oof_preds, y))\n    mae = mean_absolute_error(blended_oof_preds, y)\n    print(f\"blended result for R-squared: {r2:.3f}\")\n    print(f\"blended result for RMSE: {rmse:.3f}\")\n    print(f\"blended result for MAE: {mae:.3f}\\n\")\n```", "```py\nXGBoost weight=1.0, DNN weight=0.0\nblended result for R-squared: 0.599\nblended result for RMSE: 10783.027\nblended result for MAE: 6531.102\n\nXGBoost weight=0.75, DNN weight=0.25\nblended result for R-squared: 0.619\nblended result for RMSE: 10507.904\nblended result for MAE: 6366.257\n\nXGBoost weight=0.5, DNN weight=0.5\nblended result for R-squared: 0.625\nblended result for RMSE: 10527.024\nblended result for MAE: 6384.576\n\nXGBoost weight=0.25, DNN weight=0.75\nblended result for R-squared: 0.618\nblended result for RMSE: 10838.831\nblended result for MAE: 6566.663\n\nXGBoost weight=0.0, DNN weight=1.0\nblended result for R-squared: 0.599\nblended result for RMSE: 11419.374\nblended result for MAE: 6959.540\n```"]