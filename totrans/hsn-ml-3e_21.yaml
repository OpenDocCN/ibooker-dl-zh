- en: Chapter 19\. Training and Deploying TensorFlow Models at Scale
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第19章。在规模上训练和部署TensorFlow模型
- en: 'Once you have a beautiful model that makes amazing predictions, what do you
    do with it? Well, you need to put it in production! This could be as simple as
    running the model on a batch of data, and perhaps writing a script that runs this
    model every night. However, it is often much more involved. Various parts of your
    infrastructure may need to use this model on live data, in which case you will
    probably want to wrap your model in a web service: this way, any part of your
    infrastructure can query the model at any time using a simple REST API (or some
    other protocol), as we discussed in [Chapter 2](ch02.html#project_chapter). But
    as time passes, you’ll need to regularly retrain your model on fresh data and
    push the updated version to production. You must handle model versioning, gracefully
    transition from one model to the next, possibly roll back to the previous model
    in case of problems, and perhaps run multiple different models in parallel to
    perform *A/B experiments*.⁠^([1](ch19.html#idm45720162442960)) If your product
    becomes successful, your service may start to get a large number of of queries
    per second (QPS), and it must scale up to support the load. A great solution to
    scale up your service, as you will see in this chapter, is to use TF Serving,
    either on your own hardware infrastructure or via a cloud service such as Google
    Vertex AI.⁠^([2](ch19.html#idm45720162441360)) It will take care of efficiently
    serving your model, handle graceful model transitions, and more. If you use the
    cloud platform you will also get many extra features, such as powerful monitoring
    tools.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您拥有一个能够做出惊人预测的美丽模型，您会怎么处理呢？嗯，您需要将其投入生产！这可能只是在一批数据上运行模型，也许编写一个每晚运行该模型的脚本。然而，通常情况下会更加复杂。您的基础设施的各个部分可能需要在实时数据上使用该模型，这种情况下，您可能会希望将模型封装在一个Web服务中：这样，您的基础设施的任何部分都可以随时使用简单的REST
    API（或其他协议）查询模型，正如我们在[第2章](ch02.html#project_chapter)中讨论的那样。但随着时间的推移，您需要定期使用新数据对模型进行重新训练，并将更新后的版本推送到生产环境。您必须处理模型版本控制，优雅地从一个模型过渡到另一个模型，可能在出现问题时回滚到上一个模型，并可能并行运行多个不同的模型来执行*A/B实验*。如果您的产品变得成功，您的服务可能会开始每秒收到大量查询（QPS），并且必须扩展以支持负载。如您将在本章中看到的，一个很好的扩展服务的解决方案是使用TF
    Serving，无论是在您自己的硬件基础设施上还是通过诸如Google Vertex AI之类的云服务。它将有效地为您提供模型服务，处理优雅的模型过渡等。如果您使用云平台，您还将获得许多额外功能，例如强大的监控工具。
- en: Moreover, if you have a lot of training data and compute-intensive models, then
    training time may be prohibitively long. If your product needs to adapt to changes
    quickly, then a long training time can be a showstopper (e.g., think of a news
    recommendation system promoting news from last week). Perhaps even more importantly,
    a long training time will prevent you from experimenting with new ideas. In machine
    learning (as in many other fields), it is hard to know in advance which ideas
    will work, so you should try out as many as possible, as fast as possible. One
    way to speed up training is to use hardware accelerators such as GPUs or TPUs.
    To go even faster, you can train a model across multiple machines, each equipped
    with multiple hardware accelerators. TensorFlow’s simple yet powerful distribution
    strategies API makes this easy, as you will see.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你有大量的训练数据和计算密集型模型，那么训练时间可能会变得过长。如果你的产品需要快速适应变化，那么长时间的训练可能会成为一个阻碍因素（例如，想象一下一个新闻推荐系统在推广上周的新闻）。更重要的是，长时间的训练会阻止你尝试新想法。在机器学习（以及许多其他领域），很难事先知道哪些想法会奏效，因此你应该尽可能快地尝试尽可能多的想法。加快训练的一种方法是使用硬件加速器，如GPU或TPU。为了更快地训练，你可以在多台配备多个硬件加速器的机器上训练模型。TensorFlow的简单而强大的分布策略API使这一切变得容易，你将会看到。
- en: In this chapter we will look at how to deploy models, first using TF Serving,
    then using Vertex AI. We will also take a quick look at deploying models to mobile
    apps, embedded devices, and web apps. Then we will discuss how to speed up computations
    using GPUs and how to train models across multiple devices and servers using the
    distribution strategies API. Lastly, we will explore how to train models and fine-tune
    their hyperparameters at scale using Vertex AI. That’s a lot of topics to discuss,
    so let’s dive in!
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将学习如何部署模型，首先使用TF Serving，然后使用Vertex AI。我们还将简要介绍如何将模型部署到移动应用程序、嵌入式设备和Web应用程序。然后我们将讨论如何使用GPU加速计算，以及如何使用分布策略API在多个设备和服务器上训练模型。最后，我们将探讨如何使用Vertex
    AI 在规模上训练模型并微调其超参数。这是很多要讨论的话题，让我们开始吧！
- en: Serving a TensorFlow Model
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 TensorFlow 模型提供服务
- en: 'Once you have trained a TensorFlow model, you can easily use it in any Python
    code: if it’s a Keras model, just call its `predict()` method! But as your infrastructure
    grows, there comes a point where it is preferable to wrap your model in a small
    service whose sole role is to make predictions and have the rest of the infrastructure
    query it (e.g., via a REST or gRPC API).⁠^([3](ch19.html#idm45720162435760)) This
    decouples your model from the rest of the infrastructure, making it possible to
    easily switch model versions or scale the service up as needed (independently
    from the rest of your infrastructure), perform A/B experiments, and ensure that
    all your software components rely on the same model versions. It also simplifies
    testing and development, and more. You could create your own microservice using
    any technology you want (e.g., using the Flask library), but why reinvent the
    wheel when you can just use TF Serving?'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您训练了一个TensorFlow模型，您可以在任何Python代码中轻松地使用它：如果它是一个Keras模型，只需调用它的`predict()`方法！但随着基础设施的增长，会出现一个更好的选择，即将您的模型封装在一个小型服务中，其唯一作用是进行预测，并让基础设施的其余部分查询它（例如，通过REST或gRPC
    API）。这样可以将您的模型与基础设施的其余部分解耦，从而可以轻松地切换模型版本或根据需要扩展服务（独立于您的基础设施的其余部分），执行A/B实验，并确保所有软件组件依赖于相同的模型版本。这也简化了测试和开发等工作。您可以使用任何您想要的技术（例如，使用Flask库）创建自己的微服务，但为什么要重新发明轮子，当您可以直接使用TF
    Serving呢？
- en: Using TensorFlow Serving
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow Serving
- en: TF Serving is a very efficient, battle-tested model server, written in C++.
    It can sustain a high load, serve multiple versions of your models and watch a
    model repository to automatically deploy the latest versions, and more (see [Figure 19-1](#tf_serving_diagram)).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: TF Serving是一个非常高效、经过实战验证的模型服务器，用C++编写。它可以承受高负载，为您的模型提供多个版本，并监视模型存储库以自动部署最新版本，等等（参见[图19-1](#tf_serving_diagram)）。
- en: '![mls3 1901](assets/mls3_1901.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1901](assets/mls3_1901.png)'
- en: Figure 19-1\. TF Serving can serve multiple models and automatically deploy
    the latest version of each model
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-1。TF Serving可以为多个模型提供服务，并自动部署每个模型的最新版本。
- en: So let’s suppose you have trained an MNIST model using Keras, and you want to
    deploy it to TF Serving. The first thing you have to do is export this model to
    the SavedModel format, introduced in [Chapter 10](ch10.html#ann_chapter).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您已经使用Keras训练了一个MNIST模型，并且希望将其部署到TF Serving。您需要做的第一件事是将此模型导出为SavedModel格式，该格式在[第10章](ch10.html#ann_chapter)中介绍。
- en: Exporting SavedModels
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导出SavedModels
- en: 'You already know how to save the model: just call `model.save()`. Now to version
    the model, you just need to create a subdirectory for each model version. Easy!'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经知道如何保存模型：只需调用`model.save()`。现在要对模型进行版本控制，您只需要为每个模型版本创建一个子目录。很简单！
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: It’s usually a good idea to include all the preprocessing layers in the final
    model you export so that it can ingest data in its natural form once it is deployed
    to production. This avoids having to take care of preprocessing separately within
    the application that uses the model. Bundling the preprocessing steps within the
    model also makes it simpler to update them later on and limits the risk of mismatch
    between a model and the preprocessing steps it requires.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通常最好将所有预处理层包含在最终导出的模型中，这样一旦部署到生产环境中，模型就可以以其自然形式摄取数据。这样可以避免在使用模型的应用程序中单独处理预处理工作。将预处理步骤捆绑在模型中也使得以后更新它们更加简单，并限制了模型与所需预处理步骤之间不匹配的风险。
- en: Warning
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Since a SavedModel saves the computation graph, it can only be used with models
    that are based exclusively on TensorFlow operations, excluding the `tf.py_function()`
    operation, which wraps arbitrary Python code.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SavedModel保存了计算图，因此它只能用于基于纯粹的TensorFlow操作的模型，不包括`tf.py_function()`操作，该操作包装任意的Python代码。
- en: 'TensorFlow comes with a small `saved_model_cli` command-line interface to inspect
    SavedModels. Let use it to inspect our exported model:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow带有一个小的`saved_model_cli`命令行界面，用于检查SavedModels。让我们使用它来检查我们导出的模型：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'What does this output mean? Well, a SavedModel contains one or more *metagraphs*.
    A metagraph is a computation graph plus some function signature definitions, including
    their input and output names, types, and shapes. Each metagraph is identified
    by a set of tags. For example, you may want to have a metagraph containing the
    full computation graph, including the training operations: you would typically
    tag this one as `"train"`. And you might have another metagraph containing a pruned
    computation graph with only the prediction operations, including some GPU-specific
    operations: this one might be tagged as `"serve", "gpu"`. You might want to have
    other metagraphs as well. This can be done using TensorFlow’s low-level [SavedModel
    API](https://homl.info/savedmodel). However, when you save a Keras model using
    its `save()` method, it saves a single metagraph tagged as `"serve"`. Let’s inspect
    this `"serve"` tag set:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出是什么意思？嗯，一个SavedModel包含一个或多个*metagraphs*。一个metagraph是一个计算图加上一些函数签名定义，包括它们的输入和输出名称、类型和形状。每个metagraph都由一组标签标识。例如，您可能希望有一个包含完整计算图的metagraph，包括训练操作：您通常会将这个标记为`"train"`。您可能有另一个包含经过修剪的计算图的metagraph，只包含预测操作，包括一些特定于GPU的操作：这个可能被标记为`"serve",
    "gpu"`。您可能还想要其他metagraphs。这可以使用TensorFlow的低级[SavedModel API](https://homl.info/savedmodel)来完成。然而，当您使用Keras模型的`save()`方法保存模型时，它会保存一个标记为`"serve"`的单个metagraph。让我们检查一下这个`"serve"`标签集：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This metagraph contains two signature definitions: an initialization function
    called `"__saved_model_init_op"`, which you do not need to worry about, and a
    default serving function called `"serving_default"`. When saving a Keras model,
    the default serving function is the model’s `call()` method, which makes predictions,
    as you already know. Let’s get more details about this serving function:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个元图包含两个签名定义：一个名为`"__saved_model_init_op"`的初始化函数，您不需要担心，以及一个名为`"serving_default"`的默认服务函数。当保存一个Keras模型时，默认的服务函数是模型的`call()`方法，用于进行预测，这一点您已经知道了。让我们更详细地了解这个服务函数：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the function’s input is named `"flatten_input"`, and the output is
    named `"dense_1"`. These correspond to the Keras model’s input and output layer
    names. You can also see the type and shape of the input and output data. Looks
    good!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，函数的输入被命名为`"flatten_input"`，输出被命名为`"dense_1"`。这些对应于Keras模型的输入和输出层名称。您还可以看到输入和输出数据的类型和形状。看起来不错！
- en: Now that you have a SavedModel, the next step is to install TF Serving.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经有了一个SavedModel，下一步是安装TF Serving。
- en: Installing and starting TensorFlow Serving
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装和启动TensorFlow Serving
- en: 'There are many ways to install TF Serving: using the system’s package manager,
    using a Docker image,⁠^([4](ch19.html#idm45720162234880)) installing from source,
    and more. Since Colab runs on Ubuntu, we can use Ubuntu’s `apt` package manager
    like this:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多安装TF Serving的方法：使用系统的软件包管理器，使用Docker镜像，从源代码安装等。由于Colab运行在Ubuntu上，我们可以像这样使用Ubuntu的`apt`软件包管理器：
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code starts by adding TensorFlow’s package repository to Ubuntu’s list
    of package sources. Then it downloads TensorFlow’s public GPG key and adds it
    to the package manager’s key list so it can verify TensorFlow’s package signatures.
    Next, it uses `apt` to install the `tensorflow-model-server` package. Lastly,
    it installs the `tensorflow-serving-api` library, which we will need to communicate
    with the server.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码首先将TensorFlow的软件包存储库添加到Ubuntu的软件包源列表中。然后它下载TensorFlow的公共GPG密钥，并将其添加到软件包管理器的密钥列表中，以便验证TensorFlow的软件包签名。接下来，它使用`apt`来安装`tensorflow-model-server`软件包。最后，它安装`tensorflow-serving-api`库，这是我们与服务器通信所需的库。
- en: 'Now we want to start the server. The command will require the absolute path
    of the base model directory (i.e., the path to `my_mnist_model`, not `0001`),
    so let’s save that to the `MODEL_DIR` environment variable:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想要启动服务器。该命令将需要基本模型目录的绝对路径（即`my_mnist_model`的路径，而不是`0001`），所以让我们将其保存到`MODEL_DIR`环境变量中：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can then start the server:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以启动服务器：
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In Jupyter or Colab, the `%%bash --bg` magic command executes the cell as a
    bash script, running it in the background. The `>my_server.log 2>&1` part redirects
    the standard output and standard error to the *my_server.log* file. And that’s
    it! TF Serving is now running in the background, and its logs are saved to *my_server.log*.
    It loaded our MNIST model (version 1), and it is now waiting for gRPC and REST
    requests, respectively, on ports 8500 and 8501.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter或Colab中，`%%bash --bg`魔术命令将单元格作为bash脚本执行，在后台运行。`>my_server.log 2>&1`部分将标准输出和标准错误重定向到*my_server.log*文件。就是这样！TF
    Serving现在在后台运行，其日志保存在*my_server.log*中。它加载了我们的MNIST模型（版本1），现在正在分别等待gRPC和REST请求，端口分别为8500和8501。
- en: Now that the server is up and running, let’s query it, first using the REST
    API, then the gRPC API.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在服务器已经启动运行，让我们首先使用REST API，然后使用gRPC API进行查询。
- en: Querying TF Serving through the REST API
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过REST API查询TF Serving
- en: 'Let’s start by creating the query. It must contain the name of the function
    signature you want to call, and of course the input data. Since the request must
    use the JSON format, we have to convert the input images from a NumPy array to
    a Python list:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建查询开始。它必须包含您想要调用的函数签名的名称，当然还有输入数据。由于请求必须使用JSON格式，我们必须将输入图像从NumPy数组转换为Python列表：
- en: '[PRE7]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that the JSON format is 100% text-based. The request string looks like
    this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，JSON格式是100%基于文本的。请求字符串如下所示：
- en: '[PRE8]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now let’s send this request to TF Serving via an HTTP POST request. This can
    be done using the `requests` library (it is not part of Python’s standard library,
    but it is preinstalled on Colab):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过HTTP POST请求将这个请求发送到TF Serving。这可以使用`requests`库来完成（它不是Python标准库的一部分，但在Colab上是预安装的）：
- en: '[PRE9]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If all goes well, the response should be a dictionary containing a single `"predictions"`
    key. The corresponding value is the list of predictions. This list is a Python
    list, so let’s convert it to a NumPy array and round the floats it contains to
    the second decimal:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，响应应该是一个包含单个`"predictions"`键的字典。相应的值是预测列表。这个列表是一个Python列表，所以让我们将其转换为NumPy数组，并将其中包含的浮点数四舍五入到第二位小数：
- en: '[PRE10]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Hurray, we have the predictions! The model is close to 100% confident that the
    first image is a 7, 99% confident that the second image is a 2, and 97% confident
    that the third image is a 1\. That’s correct.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 万岁，我们有了预测！模型几乎100%确信第一张图片是7，99%确信第二张图片是2，97%确信第三张图片是1。这是正确的。
- en: 'The REST API is nice and simple, and it works well when the input and output
    data are not too large. Moreover, just about any client application can make REST
    queries without additional dependencies, whereas other protocols are not always
    so readily available. However, it is based on JSON, which is text-based and fairly
    verbose. For example, we had to convert the NumPy array to a Python list, and
    every float ended up represented as a string. This is very inefficient, both in
    terms of serialization/deserialization time—we have to convert all the floats
    to strings and back—and in terms of payload size: many floats end up being represented
    using over 15 characters, which translates to over 120 bits for 32-bit floats!
    This will result in high latency and bandwidth usage when transferring large NumPy
    arrays.⁠^([6](ch19.html#idm45720161858608)) So, let’s see how to use gRPC instead.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: REST API 简单易用，当输入和输出数据不太大时效果很好。此外，几乎任何客户端应用程序都可以在没有额外依赖的情况下进行 REST 查询，而其他协议并不总是那么容易获得。然而，它基于
    JSON，这是基于文本且相当冗长的。例如，我们不得不将 NumPy 数组转换为 Python 列表，每个浮点数最终都表示为一个字符串。这非常低效，无论是在序列化/反序列化时间方面——我们必须将所有浮点数转换为字符串然后再转回来——还是在有效载荷大小方面：许多浮点数最终使用超过
    15 个字符来表示，这相当于 32 位浮点数超过 120 位！这将导致在传输大型 NumPy 数组时出现高延迟和带宽使用。因此，让我们看看如何改用 gRPC。
- en: Tip
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: When transferring large amounts of data, or when latency is important, it is
    much better to use the gRPC API, if the client supports it, as it uses a compact
    binary format and an efficient communication protocol based on HTTP/2 framing.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在传输大量数据或延迟重要时，最好使用gRPC API，如果客户端支持的话，因为它使用紧凑的二进制格式和基于HTTP/2 framing的高效通信协议。
- en: Querying TF Serving through the gRPC API
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过gRPC API查询TF Serving
- en: 'The gRPC API expects a serialized `PredictRequest` protocol buffer as input,
    and it outputs a serialized `PredictResponse` protocol buffer. These protobufs
    are part of the `tensorflow-serving-api` library, which we installed earlier.
    First, let’s create the request:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: gRPC API期望一个序列化的`PredictRequest`协议缓冲区作为输入，并输出一个序列化的`PredictResponse`协议缓冲区。这些protobufs是`tensorflow-serving-api`库的一部分，我们之前安装过。首先，让我们创建请求：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This code creates a `PredictRequest` protocol buffer and fills in the required
    fields, including the model name (defined earlier), the signature name of the
    function we want to call, and finally the input data, in the form of a `Tensor`
    protocol buffer. The `tf.make_tensor_proto()` function creates a `Tensor` protocol
    buffer based on the given tensor or NumPy array, in this case `X_new`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码创建了一个`PredictRequest`协议缓冲区，并填充了必需的字段，包括模型名称（之前定义的），我们想要调用的函数的签名名称，最后是输入数据，以`Tensor`协议缓冲区的形式。`tf.make_tensor_proto()`函数根据给定的张量或NumPy数组创建一个`Tensor`协议缓冲区，这里是`X_new`。
- en: 'Next, we’ll send the request to the server and get its response. For this,
    we will need the `grpcio` library, which is preinstalled in Colab:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将向服务器发送请求并获取其响应。为此，我们将需要`grpcio`库，该库已预先安装在Colab中：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The code is quite straightforward: after the imports, we create a gRPC communication
    channel to *localhost* on TCP port 8500, then we create a gRPC service over this
    channel and use it to send a request, with a 10-second timeout. Note that the
    call is synchronous: it will block until it receives the response or when the
    timeout period expires. In this example the channel is insecure (no encryption,
    no authentication), but gRPC and TF Serving also support secure channels over
    SSL/TLS.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 代码非常简单：在导入之后，我们在TCP端口8500上创建一个到*localhost*的gRPC通信通道，然后我们在该通道上创建一个gRPC服务，并使用它发送一个带有10秒超时的请求。请注意，调用是同步的：它将阻塞，直到收到响应或超时期限到期。在此示例中，通道是不安全的（没有加密，没有身份验证），但gRPC和TF
    Serving也支持通过SSL/TLS的安全通道。
- en: 'Next, let’s convert the `PredictResponse` protocol buffer to a tensor:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将`PredictResponse`协议缓冲区转换为张量：
- en: '[PRE13]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'If you run this code and print `y_proba.round(2)`, you will get the exact same
    estimated class probabilities as earlier. And that’s all there is to it: in just
    a few lines of code, you can now access your TensorFlow model remotely, using
    either REST or gRPC.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行此代码并打印`y_proba.round(2)`，您将获得与之前完全相同的估计类概率。这就是全部内容：只需几行代码，您现在就可以远程访问您的TensorFlow模型，使用REST或gRPC。
- en: Deploying a new model version
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署新的模型版本
- en: 'Now let’s create a new model version and export a SavedModel, this time to
    the *my_mnist_model/0002* directory:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个新的模型版本并导出一个SavedModel，这次导出到*my_mnist_model/0002*目录：
- en: '[PRE14]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'At regular intervals (the delay is configurable), TF Serving checks the model
    directory for new model versions. If it finds one, it automatically handles the
    transition gracefully: by default, it answers pending requests (if any) with the
    previous model version, while handling new requests with the new version. As soon
    as every pending request has been answered, the previous model version is unloaded.
    You can see this at work in the TF Serving logs (in *my_server.log*):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在固定的时间间隔（延迟可配置），TF Serving会检查模型目录是否有新的模型版本。如果找到一个新版本，它会自动优雅地处理过渡：默认情况下，它会用前一个模型版本回答待处理的请求（如果有的话），同时用新版本处理新请求。一旦每个待处理的请求都得到回答，之前的模型版本就会被卸载。您可以在TF
    Serving日志（*my_server.log*）中看到这个过程：
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Tip
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'If the SavedModel contains some example instances in the *assets/extra* directory,
    you can configure TF Serving to run the new model on these instances before starting
    to use it to serve requests. This is called *model warmup*: it will ensure that
    everything is properly loaded, avoiding long response times for the first requests.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 如果SavedModel包含*assets/extra*目录中的一些示例实例，您可以配置TF Serving在开始使用它来处理请求之前在这些实例上运行新模型。这称为*模型预热*：它将确保一切都被正确加载，避免第一次请求的长响应时间。
- en: This approach offers a smooth transition, but it may use too much RAM—especially
    GPU RAM, which is generally the most limited. In this case, you can configure
    TF Serving so that it handles all pending requests with the previous model version
    and unloads it before loading and using the new model version. This configuration
    will avoid having two model versions loaded at the same time, but the service
    will be unavailable for a short period.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法提供了平稳的过渡，但可能会使用过多的RAM，特别是GPU RAM，通常是最有限的。在这种情况下，您可以配置TF Serving，使其处理所有挂起的请求与先前的模型版本，并在加载和使用新的模型版本之前卸载它。这种配置将避免同时加载两个模型版本，但服务将在短时间内不可用。
- en: As you can see, TF Serving makes it straightforward to deploy new models. Moreover,
    if you discover that version 2 does not work as well as you expected, then rolling
    back to version 1 is as simple as removing the *my_mnist_model/0002* directory.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，TF Serving使部署新模型变得简单。此外，如果您发现第二个版本的效果不如预期，那么回滚到第一个版本就像删除*my_mnist_model/0002*目录一样简单。
- en: Tip
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Another great feature of TF Serving is its automatic batching capability, which
    you can activate using the `--enable_batching` option upon startup. When TF Serving
    receives multiple requests within a short period of time (the delay is configurable),
    it will automatically batch them together before using the model. This offers
    a significant performance boost by leveraging the power of the GPU. Once the model
    returns the predictions, TF Serving dispatches each prediction to the right client.
    You can trade a bit of latency for a greater throughput by increasing the batching
    delay (see the `--batching_parameters_file` option).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: TF Serving的另一个重要特性是其自动批处理能力，您可以在启动时使用`--enable_batching`选项来激活它。当TF Serving在短时间内接收到多个请求时（延迟可配置），它会在使用模型之前自动将它们批处理在一起。通过利用GPU的性能，这将显著提高性能。一旦模型返回预测结果，TF
    Serving会将每个预测结果分发给正确的客户端。通过增加批处理延迟（参见`--batching_parameters_file`选项），您可以在一定程度上牺牲一点延迟以获得更大的吞吐量。
- en: 'If you expect to get many queries per second, you will want to deploy TF Serving
    on multiple servers and load-balance the queries (see [Figure 19-2](#tf_serving_load_balancing_diagram)).
    This will require deploying and managing many TF Serving containers across these
    servers. One way to handle that is to use a tool such as [Kubernetes](https://kubernetes.io),
    which is an open source system for simplifying container orchestration across
    many servers. If you do not want to purchase, maintain, and upgrade all the hardware
    infrastructure, you will want to use virtual machines on a cloud platform such
    as Amazon AWS, Microsoft Azure, Google Cloud Platform, IBM Cloud, Alibaba Cloud,
    Oracle Cloud, or some other platform as a service (PaaS) offering. Managing all
    the virtual machines, handling container orchestration (even with the help of
    Kubernetes), taking care of TF Serving configuration, tuning and monitoring—all
    of this can be a full-time job. Fortunately, some service providers can take care
    of all this for you. In this chapter we will use Vertex AI: it’s the only platform
    with TPUs today; it supports TensorFlow 2, Scikit-Learn, and XGBoost; and it offers
    a nice suite of AI services. There are several other providers in this space that
    are capable of serving TensorFlow models as well, though, such as Amazon AWS SageMaker
    and Microsoft AI Platform, so make sure to check them out too.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望每秒获得许多查询，您将希望在多台服务器上部署TF Serving并负载平衡查询（请参见[图19-2](#tf_serving_load_balancing_diagram)）。这将需要在这些服务器上部署和管理许多TF
    Serving容器。处理这一问题的一种方法是使用诸如[Kubernetes](https://kubernetes.io)之类的工具，它是一个简化跨多台服务器容器编排的开源系统。如果您不想购买、维护和升级所有硬件基础设施，您将希望在云平台上使用虚拟机，如Amazon
    AWS、Microsoft Azure、Google Cloud Platform、IBM Cloud、Alibaba Cloud、Oracle Cloud或其他平台即服务（PaaS）提供商。管理所有虚拟机，处理容器编排（即使借助Kubernetes的帮助），照顾TF
    Serving配置、调整和监控——所有这些都可能成为一项全职工作。幸运的是，一些服务提供商可以为您处理所有这些事务。在本章中，我们将使用Vertex AI：它是今天唯一支持TPUs的平台；它支持TensorFlow
    2、Scikit-Learn和XGBoost；并提供一套不错的人工智能服务。在这个领域还有其他几家提供商也能够提供TensorFlow模型的服务，比如Amazon
    AWS SageMaker和Microsoft AI Platform，所以请确保也查看它们。
- en: '![mls3 1902](assets/mls3_1902.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1902](assets/mls3_1902.png)'
- en: Figure 19-2\. Scaling up TF Serving with load balancing
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-2。使用负载平衡扩展TF Serving
- en: Now let’s see how to serve our wonderful MNIST model on the cloud!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何在云上提供我们出色的MNIST模型！
- en: Creating a Prediction Service on Vertex AI
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Vertex AI上创建一个预测服务
- en: Vertex AI is a platform within Google Cloud Platform (GCP) that offers a wide
    range of AI-related tools and services. You can upload datasets, get humans to
    label them, store commonly used features in a feature store and use them for training
    or in production, and train models across many GPU or TPU servers with automatic
    hyperparameter tuning or model architecture search (AutoML). You can also manage
    your trained models, use them to make batch predictions on large amounts of data,
    schedule multiple jobs for your data workflows, serve your models via REST or
    gRPC at scale, and experiment with your data and models within a hosted Jupyter
    environment called the *Workbench*. There’s even a *Matching Engine* service that
    lets you compare vectors very efficiently (i.e., approximate nearest neighbors).
    GCP also includes other AI services, such as APIs for computer vision, translation,
    speech-to-text, and more.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI是Google Cloud Platform（GCP）内的一个平台，提供各种与人工智能相关的工具和服务。您可以上传数据集，让人类对其进行标记，将常用特征存储在特征存储中，并将其用于训练或生产中，使用多个GPU或TPU服务器进行模型训练，并具有自动超参数调整或模型架构搜索（AutoML）功能。您还可以管理已训练的模型，使用它们对大量数据进行批量预测，为数据工作流程安排多个作业，通过REST或gRPC以规模化方式提供模型服务，并在名为*Workbench*的托管Jupyter环境中对数据和模型进行实验。甚至还有一个*Matching
    Engine*服务，可以非常高效地比较向量（即，近似最近邻）。GCP还包括其他AI服务，例如计算机视觉、翻译、语音转文本等API。
- en: 'Before we start, there’s a little bit of setup to take care of:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，有一些设置需要处理：
- en: Log in to your Google account, and then go to the [Google Cloud Platform console](https://console.cloud.google.com)
    (see [Figure 19-3](#gcp_screenshot)). If you don’t have a Google account, you’ll
    have to create one.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录您的Google账户，然后转到[Google Cloud Platform控制台](https://console.cloud.google.com)（参见[图19-3](#gcp_screenshot)）。如果您没有Google账户，您将需要创建一个。
- en: 'If it’s your first time using GCP, you’ll have to read and accept the terms
    and conditions. New users are offered a free trial, including $300 worth of GCP
    credit that you can use over the course of 90 days (as of May 2022). You’ll only
    need a small portion of that to pay for the services you’ll use in this chapter.
    Upon signing up for a free trial, you’ll still need to create a payment profile
    and enter your credit card number: it’s used for verification purposes—probably
    to avoid people using the free trial multiple times—but you won’t be billed for
    the first $300, and after that you’ll only be charged if you opt in by upgrading
    to a paid account.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果这是您第一次使用GCP，您将需要阅读并接受条款和条件。新用户可以获得免费试用，包括价值300美元的GCP信用，您可以在90天内使用（截至2022年5月）。您只需要其中的一小部分来支付本章中将使用的服务。注册免费试用后，您仍然需要创建一个付款配置文件并输入您的信用卡号码：这是用于验证目的——可能是为了避免人们多次使用免费试用，但您不会被收取前300美元的费用，之后只有在您选择升级到付费账户时才会收费。
- en: '![mls3 1903](assets/mls3_1903.png)'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![mls3 1903](assets/mls3_1903.png)'
- en: Figure 19-3\. Google Cloud Platform console
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-3. Google Cloud Platform控制台
- en: If you have used GCP before and your free trial has expired, then the services
    you will use in this chapter will cost you some money. It shouldn’t be too much,
    especially if you remember to turn off the services when you don’t need them anymore.
    Make sure you understand and agree to the pricing conditions before you run any
    service. I hereby decline any responsibility if services end up costing more than
    you expected! Also make sure your billing account is active. To check, open the
    ☰ navigation menu at the top left and click Billing, then make sure you have set
    up a payment method and that the billing account is active.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您以前使用过GCP并且您的免费试用已经过期，那么您在本章中将使用的服务将会花费一些钱。这不应该太多，特别是如果您记得在不再需要这些服务时关闭它们。在运行任何服务之前，请确保您理解并同意定价条件。如果服务最终花费超出您的预期，我在此不承担任何责任！还请确保您的计费账户是活动的。要检查，请打开左上角的☰导航菜单，点击计费，然后确保您已设置付款方式并且计费账户是活动的。
- en: 'Every resource in GCP belongs to a *project*. This includes all the virtual
    machines you may use, the files you store, and the training jobs you run. When
    you create an account, GCP automatically creates a project for you, called “My
    First Project”. If you want, you can change its display name by going to the project
    settings: in the ☰ navigation menu, select “IAM and admin → Settings”, change
    the project’s display name, and click SAVE. Note that the project also has a unique
    ID and number. You can choose the project ID when you create a project, but you
    cannot change it later. The project number is automatically generated and cannot
    be changed. If you want to create a new project, click the project name at the
    top of the page, then click NEW PROJECT and enter the project name. You can also
    click EDIT to set the project ID. Make sure billing is active for this new project
    so that service fees can be billed (to your free credits, if any).'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: GCP 中的每个资源都属于一个 *项目*。这包括您可能使用的所有虚拟机、存储的文件和运行的训练作业。当您创建一个帐户时，GCP 会自动为您创建一个名为“我的第一个项目”的项目。如果您愿意，可以通过转到项目设置来更改其显示名称：在
    ☰ 导航菜单中，选择“IAM 和管理员 → 设置”，更改项目的显示名称，然后单击“保存”。请注意，项目还有一个唯一的 ID 和编号。您可以在创建项目时选择项目
    ID，但以后无法更改。项目编号是自动生成的，无法更更改。如果您想创建一个新项目，请单击页面顶部的项目名称，然后单击“新项目”并输入项目名称。您还可以单击“编辑”来设置项目
    ID。确保此新项目的计费处于活动状态，以便可以对服务费用进行计费（如果有免费信用）。
- en: Warning
  id: totrans-82
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Always set an alarm to remind yourself to turn services off when you know you
    will only need them for a few hours, or else you might leave them running for
    days or months, incurring potentially significant costs.
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请始终设置提醒，以便在您知道只需要几个小时时关闭服务，否则您可能会让其运行数天或数月，从而产生潜在的显著成本。
- en: Now that you have a GCP account and a project, and billing is activated, you
    must activate the APIs you need. In the ☰ navigation menu, select “APIs and services”,
    and make sure the Cloud Storage API is enabled. If needed, click + ENABLE APIS
    AND SERVICES, find Cloud Storage, and enable it. Also enable the Vertex AI API.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在您已经拥有GCP帐户和项目，并且计费已激活，您必须激活所需的API。在☰导航菜单中，选择“API和服务”，确保启用了Cloud Storage API。如果需要，点击+启用API和服务，找到Cloud
    Storage，并启用它。还要启用Vertex AI API。
- en: 'You could continue to do everything via the GCP console, but I recommend using
    Python instead: this way you can write scripts to automate just about anything
    you want with GCP, and it’s often more convenient than clicking your way through
    menus and forms, especially for common tasks.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以继续通过GCP控制台完成所有操作，但我建议改用Python：这样您可以编写脚本来自动化几乎任何您想要在GCP上完成的任务，而且通常比通过菜单和表单点击更方便，特别是对于常见任务。
- en: 'The first thing you need to do before you can use any GCP service is to authenticate.
    The simplest solution when using Colab is to execute the following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在您使用任何GCP服务之前，您需要做的第一件事是进行身份验证。在使用Colab时最简单的解决方案是执行以下代码：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The authentication process is based on [*OAuth 2.0*](https://oauth.net): a
    pop-up window will ask you to confirm that you want the Colab notebook to access
    your Google credentials. If you accept, you must select the same Google account
    you used for GCP. Then you will be asked to confirm that you agree to give Colab
    full access to all your data on Google Drive and in GCP. If you allow access,
    only the current notebook will have access, and only until the Colab runtime expires.
    Obviously, you should only accept this if you trust the code in the notebook.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 认证过程基于OAuth 2.0：一个弹出窗口会要求您确认您希望Colab笔记本访问您的Google凭据。如果您接受，您必须选择与GCP相同的Google帐户。然后，您将被要求确认您同意授予Colab对Google
    Drive和GCP中所有数据的完全访问权限。如果您允许访问，只有当前笔记本将具有访问权限，并且仅在Colab运行时到期之前。显然，只有在您信任笔记本中的代码时才应接受此操作。
- en: Warning
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'If you are *not* working with the official notebooks from [*https://github.com/ageron/handson-ml3*](https://github.com/ageron/handson-ml3),
    then you should be extra careful: if the notebook’s author is mischievous, they
    could include code to do whatever they want with your data.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您*不*使用来自https://github.com/ageron/handson-ml3的官方笔记本，则应格外小心：如果笔记本的作者心怀不轨，他们可能包含代码来对您的数据进行任何操作。
- en: 'Now let’s create a Google Cloud Storage bucket to store our SavedModels (a
    GCS *bucket* is a container for your data). For this we will use the `google-cloud-storage`
    library, which is preinstalled in Colab. We first create a `Client` object, which
    will serve as the interface with GCS, then we use it to create the bucket:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个Google Cloud Storage存储桶来存储我们的SavedModels（GCS的*存储桶*是您数据的容器）。为此，我们将使用预先安装在Colab中的`google-cloud-storage`库。我们首先创建一个`Client`对象，它将作为与GCS的接口，然后我们使用它来创建存储桶：
- en: '[PRE17]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Tip
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you want to reuse an existing bucket, replace the last line with `bucket
    = storage_client.bucket(bucket_name)`. Make sure `location` is set to the bucket’s
    region.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想重用现有的存储桶，请将最后一行替换为`bucket = storage_client.bucket(bucket_name)`。确保`location`设置为存储桶的地区。
- en: GCS uses a single worldwide namespace for buckets, so simple names like “machine-learning”
    will most likely not be available. Make sure the bucket name conforms to DNS naming
    conventions, as it may be used in DNS records. Moreover, bucket names are public,
    so do not put anything private in the name. It is common to use your domain name,
    your company name, or your project ID as a prefix to ensure uniqueness, or simply
    use a random number as part of the name.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: GCS使用单个全球命名空间用于存储桶，因此像“machine-learning”这样的简单名称很可能不可用。确保存储桶名称符合DNS命名约定，因为它可能在DNS记录中使用。此外，存储桶名称是公开的，因此不要在名称中放入任何私人信息。通常使用您的域名、公司名称或项目ID作为前缀以确保唯一性，或者只需在名称中使用一个随机数字。
- en: You can change the region if you want, but be sure to choose one that supports
    GPUs. Also, you may want to consider the fact that prices vary greatly between
    regions, some regions produce much more CO₂ than others, some regions do not support
    all services, and using a single-region bucket improves performance. See [Google
    Cloud’s list of regions](https://homl.info/regions) and [Vertex AI’s documentation
    on locations](https://homl.info/locations) for more details. If you are unsure,
    it might be best to stick with `"us-central1"`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要，可以更改区域，但请确保选择支持GPU的区域。此外，您可能需要考虑到不同区域之间价格差异很大，一些区域产生的CO₂比其他区域多得多，一些区域不支持所有服务，并且使用单一区域存储桶可以提高性能。有关更多详细信息，请参阅[Google
    Cloud的区域列表](https://homl.info/regions)和[Vertex AI的位置文档](https://homl.info/locations)。如果您不确定，最好选择`"us-central1"`。
- en: 'Next, let’s upload the *my_mnist_model* directory to the new bucket. Files
    in GCS are called *blobs* (or *objects*), and under the hood they are all just
    placed in the bucket without any directory structure. Blob names can be arbitrary
    Unicode strings, and they can even contain forward slashes (`/`). The GCP console
    and other tools use these slashes to give the illusion that there are directories.
    So, when we upload the *my_mnist_model* directory, we only care about the files,
    not the directories:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们将*my_mnist_model*目录上传到新的存储桶。在GCS中，文件被称为*blobs*（或*objects*），在幕后它们都只是放在存储桶中，没有任何目录结构。Blob名称可以是任意的Unicode字符串，甚至可以包含斜杠(`/`)。GCP控制台和其他工具使用这些斜杠来产生目录的幻觉。因此，当我们上传*my_mnist_model*目录时，我们只关心文件，而不是目录。
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This function works fine now, but it would be very slow if there were many
    files to upload. It’s not too hard to speed it up tremendously by multithreading
    it (see the notebook for an implementation). Alternatively, if you have the Google
    Cloud CLI, then you can use following command instead:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数现在运行良好，但如果有很多文件要上传，它会非常慢。通过多线程可以很容易地大大加快速度（请参阅笔记本中的实现）。或者，如果您有Google Cloud
    CLI，则可以使用以下命令：
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, let’s tell Vertex AI about our MNIST model. To communicate with Vertex
    AI, we can use the `google-cloud-aiplatform` library (it still uses the old AI
    Platform name instead of Vertex AI). It’s not preinstalled in Colab, so we need
    to install it. After that, we can import the library and initialize it—just to
    specify some default values for the project ID and the location—then we can create
    a new Vertex AI model: we specify a display name, the GCS path to our model (in
    this case the version 0001), and the URL of the Docker container we want Vertex
    AI to use to run this model. If you visit that URL and navigate up one level,
    you will find other containers you can use. This one supports TensorFlow 2.8 with
    a GPU:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们告诉Vertex AI关于我们的MNIST模型。要与Vertex AI通信，我们可以使用`google-cloud-aiplatform`库（它仍然使用旧的AI
    Platform名称而不是Vertex AI）。它在Colab中没有预安装，所以我们需要安装它。之后，我们可以导入该库并进行初始化——只需指定一些项目ID和位置的默认值——然后我们可以创建一个新的Vertex
    AI模型：我们指定一个显示名称，我们模型的GCS路径（在这种情况下是版本0001），以及我们希望Vertex AI使用的Docker容器的URL来运行此模型。如果您访问该URL并向上导航一个级别，您将找到其他可以使用的容器。这个支持带有GPU的TensorFlow
    2.8：
- en: '[PRE20]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now let’s deploy this model so we can query it via a gRPC or REST API to make
    predictions. For this we first need to create an *endpoint*. This is what client
    applications connect to when they want to access a service. Then we need to deploy
    our model to this endpoint:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们部署这个模型，这样我们就可以通过gRPC或REST API查询它以进行预测。为此，我们首先需要创建一个*端点*。这是客户端应用程序在想要访问服务时连接的地方。然后我们需要将我们的模型部署到这个端点：
- en: '[PRE21]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This code may take a few minutes to run, because Vertex AI needs to set up a
    virtual machine. In this example, we use a fairly basic machine of type `n1-standard-4`
    (see [*https://homl.info/machinetypes*](https://homl.info/machinetypes) for other
    types). We also use a basic GPU of type `NVIDIA_TESLA_K80` (see [*https://homl.info/accelerators*](https://homl.info/accelerators)
    for other types). If you selected another region than `"us-central1"`, then you
    may need to change the machine type or the accelerator type to values that are
    supported in that region (e.g., not all regions have Nvidia Tesla K80 GPUs).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码可能需要几分钟才能运行，因为Vertex AI 需要设置一个虚拟机。在这个例子中，我们使用一个相当基本的 `n1-standard-4` 类型的机器（查看[*https://homl.info/machinetypes*](https://homl.info/machinetypes)
    获取其他类型）。我们还使用了一个基本的 `NVIDIA_TESLA_K80` 类型的 GPU（查看[*https://homl.info/accelerators*](https://homl.info/accelerators)
    获取其他类型）。如果您选择的区域不是 `"us-central1"`，那么您可能需要将机器类型或加速器类型更改为该区域支持的值（例如，并非所有区域都有 Nvidia
    Tesla K80 GPU）。
- en: Note
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Google Cloud Platform enforces various GPU quotas, both worldwide and per region:
    you cannot create thousands of GPU nodes without prior authorization from Google.
    To check your quotas, open “IAM and admin → Quotas” in the GCP console. If some
    quotas are too low (e.g., if you need more GPUs in a particular region), you can
    ask for them to be increased; it often takes about 48 hours.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Platform 实施各种 GPU 配额，包括全球范围和每个地区：您不能在未经 Google 授权的情况下创建成千上万个 GPU
    节点。要检查您的配额，请在 GCP 控制台中打开“IAM 和管理员 → 配额”。如果某些配额太低（例如，如果您需要在特定地区更多的 GPU），您可以要求增加它们；通常需要大约
    48 小时。
- en: 'Vertex AI will initially spawn the minimum number of compute nodes (just one
    in this case), and whenever the number of queries per second becomes too high,
    it will spawn more nodes (up to the maximum number you defined, five in this case)
    and will load-balance the queries between them. If the QPS rate goes down for
    a while, Vertex AI will stop the extra compute nodes automatically. The cost is
    therefore directly linked to the load, as well as the machine and accelerator
    types you selected and the amount of data you store on GCS. This pricing model
    is great for occasional users and for services with important usage spikes. It’s
    also ideal for startups: the price remains low until the startup actually starts
    up.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI将最初生成最少数量的计算节点（在这种情况下只有一个），每当每秒查询次数变得过高时，它将生成更多节点（最多为您定义的最大数量，这种情况下为五个），并在它们之间负载均衡查询。如果一段时间内QPS速率下降，Vertex
    AI将自动停止额外的计算节点。因此，成本直接与负载、您选择的机器和加速器类型以及您在GCS上存储的数据量相关。这种定价模型非常适合偶尔使用者和有重要使用高峰的服务。对于初创公司来说也是理想的：价格保持低延迟到公司真正开始运营。
- en: 'Congratulations, you have deployed your first model to the cloud! Now let’s
    query this prediction service:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，您已经将第一个模型部署到云端！现在让我们查询这个预测服务：
- en: '[PRE22]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We first need to convert the images we want to classify to a Python list, as
    we did earlier when we sent requests to TF Serving using the REST API. The response
    object contains the predictions, represented as a Python list of lists of floats.
    Let’s round them to two decimal places and convert them to a NumPy array:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要将要分类的图像转换为Python列表，就像我们之前使用REST API向TF Serving发送请求时所做的那样。响应对象包含预测结果，表示为Python浮点数列表的列表。让我们将它们四舍五入到两位小数并将它们转换为NumPy数组：
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Yes! We get the exact same predictions as earlier. We now have a nice prediction
    service running on the cloud that we can query from anywhere securely, and which
    can automatically scale up or down depending on the number of QPS. When you are
    done using the endpoint, don’t forget to delete it, to avoid paying for nothing:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 是的！我们得到了与之前完全相同的预测结果。我们现在在云上有一个很好的预测服务，我们可以从任何地方安全地查询，并且可以根据QPS的数量自动扩展或缩小。当您使用完端点后，请不要忘记将其删除，以避免无谓地支付费用：
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now let’s see how to run a job on Vertex AI to make predictions on a potentially
    very large batch of data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何在Vertex AI上运行作业，对可能非常大的数据批次进行预测。
- en: Running Batch Prediction Jobs on Vertex AI
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Vertex AI上运行批量预测作业
- en: 'If we have a large number of predictions to make, then instead of calling our
    prediction service repeatedly, we can ask Vertex AI to run a prediction job for
    us. This does not require an endpoint, only a model. For example, let’s run a
    prediction job on the first 100 images of the test set, using our MNIST model.
    For this, we first need to prepare the batch and upload it to GCS. One way to
    do this is to create a file containing one instance per line, each formatted as
    a JSON value—this format is called *JSON Lines*—then pass this file to Vertex
    AI. So let’s create a JSON Lines file in a new directory, then upload this directory
    to GCS:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要进行大量预测，那么我们可以请求 Vertex AI 为我们运行预测作业，而不是重复调用我们的预测服务。这不需要端点，只需要一个模型。例如，让我们在测试集的前
    100 张图像上运行一个预测作业，使用我们的 MNIST 模型。为此，我们首先需要准备批处理并将其上传到 GCS。一种方法是创建一个文件，每行包含一个实例，每个实例都格式化为
    JSON 值——这种格式称为 *JSON Lines*——然后将此文件传递给 Vertex AI。因此，让我们在一个新目录中创建一个 JSON Lines
    文件，然后将此目录上传到 GCS：
- en: '[PRE25]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now we’re ready to launch the prediction job, specifying the job’s name, the
    type and number of machines and accelerators to use, the GCS path to the JSON
    Lines file we just created, and the path to the GCS directory where Vertex AI
    will save the model’s predictions:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备启动预测作业，指定作业的名称、要使用的机器和加速器的类型和数量，刚刚创建的 JSON Lines 文件的 GCS 路径，以及 Vertex
    AI 将保存模型预测的 GCS 目录的路径：
- en: '[PRE26]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Tip
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For large batches, you can split the inputs into multiple JSON Lines files and
    list them all via the `gcs_source` argument.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大批量数据，您可以将输入拆分为多个JSON Lines文件，并通过`gcs_source`参数列出它们。
- en: 'This will take a few minutes, mostly to spawn the compute nodes on Vertex AI.
    Once this command completes, the predictions will be available in a set of files
    named something like *prediction.results-00001-of-00002*. These files use the
    JSON Lines format by default, and each value is a dictionary containing an instance
    and its corresponding prediction (i.e., 10 probabilities). The instances are listed
    in the same order as the inputs. The job also outputs *prediction-errors** files,
    which can be useful for debugging if something goes wrong. We can iterate through
    all these output files using `batch_prediction_job.iter_outputs()`, so let’s go
    through all the predictions and store them in a `y_probas` array:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要几分钟的时间，主要是为了在Vertex AI上生成计算节点。一旦这个命令完成，预测将会以类似*prediction.results-00001-of-00002*的文件集合中可用。这些文件默认使用JSON
    Lines格式，每个值都是包含实例及其对应预测（即10个概率）的字典。实例按照输入的顺序列出。该作业还会输出*prediction-errors*文件，如果出现问题，这些文件对于调试可能会有用。我们可以使用`batch_prediction_job.iter_outputs()`迭代所有这些输出文件，所以让我们遍历所有的预测并将它们存储在`y_probas`数组中：
- en: '[PRE27]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now let’s see how good these predictions are:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看这些预测有多好：
- en: '[PRE28]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Nice, 98% accuracy!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，98%的准确率！
- en: The JSON Lines format is the default, but when dealing with large instances
    such as images, it is too verbose. Luckily, the `batch_predict()` method accepts
    an `instances_format` argument that lets you choose another format if you want.
    It defaults to `"jsonl"`, but you can change it to `"csv"`, `"tf-record"`, `"tf-record-gzip"`,
    `"bigquery"`, or `"file-list"`. If you set it to `"file-list"`, then the `gcs_source`
    argument should point to a text file containing one input filepath per line; for
    instance, pointing to PNG image files. Vertex AI will read these files as binary,
    encode them using Base64, and pass the resulting byte strings to the model. This
    means that you must add a preprocessing layer in your model to parse the Base64
    strings, using `tf.io.decode_base64()`. If the files are images, you must then
    parse the result using a function like `tf.io.decode_image()` or `tf.io.decode_png()`,
    as discussed in [Chapter 13](ch13.html#data_chapter).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: JSON Lines格式是默认格式，但是当处理大型实例（如图像）时，它太冗长了。幸运的是，`batch_predict()`方法接受一个`instances_format`参数，让您可以选择另一种格式。它默认为`"jsonl"`，但您可以将其更改为`"csv"`、`"tf-record"`、`"tf-record-gzip"`、`"bigquery"`或`"file-list"`。如果将其设置为`"file-list"`，那么`gcs_source`参数应指向一个文本文件，其中每行包含一个输入文件路径；例如，指向PNG图像文件。Vertex
    AI将读取这些文件作为二进制文件，使用Base64对其进行编码，并将生成的字节字符串传递给模型。这意味着您必须在模型中添加一个预处理层来解析Base64字符串，使用`tf.io.decode_base64()`。如果文件是图像，则必须使用类似`tf.io.decode_image()`或`tf.io.decode_png()`的函数来解析结果，如[第13章](ch13.html#data_chapter)中所讨论的。
- en: 'When you’re finished using the model, you can delete it if you want, by running
    `mnist_model.delete()`. You can also delete the directories you created in your
    GCS bucket, optionally the bucket itself (if it’s empty), and the batch prediction
    job:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当您完成使用模型后，如果需要，可以通过运行`mnist_model.delete()`来删除它。您还可以删除在您的GCS存储桶中创建的目录，可选地删除存储桶本身（如果为空），以及批量预测作业。
- en: '[PRE29]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: You now know how to deploy a model to Vertex AI, create a prediction service,
    and run batch prediction jobs. But what if you want to deploy your model to a
    mobile app instead? Or to an embedded device, such as a heating control system,
    a fitness tracker, or a self-driving car?
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在知道如何将模型部署到Vertex AI，创建预测服务，并运行批量预测作业。但是如果您想将模型部署到移动应用程序，或者嵌入式设备，比如加热控制系统、健身追踪器或自动驾驶汽车呢？
- en: Deploying a Model to a Mobile or Embedded Device
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将模型部署到移动设备或嵌入式设备
- en: 'Machine learning models are not limited to running on big centralized servers
    with multiple GPUs: they can run closer to the source of data (this is called
    *edge computing*), for example in the user’s mobile device or in an embedded device.
    There are many benefits to decentralizing the computations and moving them toward
    the edge: it allows the device to be smart even when it’s not connected to the
    internet, it reduces latency by not having to send data to a remote server and
    reduces the load on the servers, and it may improve privacy, since the user’s
    data can stay on the device.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型不仅限于在拥有多个GPU的大型集中式服务器上运行：它们可以更接近数据源运行（这被称为*边缘计算*），例如在用户的移动设备或嵌入式设备中。去中心化计算并将其移向边缘有许多好处：它使设备即使未连接到互联网时也能智能化，通过不必将数据发送到远程服务器来减少延迟并减轻服务器负载，并且可能提高隐私性，因为用户的数据可以保留在设备上。
- en: 'However, deploying models to the edge has its downsides too. The device’s computing
    resources are generally tiny compared to a beefy multi-GPU server. A large model
    may not fit in the device, it may use too much RAM and CPU, and it may take too
    long to download. As a result, the application may become unresponsive, and the
    device may heat up and quickly run out of battery. To avoid all this, you need
    to make a lightweight and efficient model, without sacrificing too much of its
    accuracy. The [TFLite](https://tensorflow.org/lite) library provides several tools⁠^([7](ch19.html#idm45720160648176))
    to help you deploy your models to the edge, with three main objectives:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将模型部署到边缘也有其缺点。与强大的多GPU服务器相比，设备的计算资源通常很少。一个大模型可能无法适应设备，可能使用过多的RAM和CPU，并且可能下载时间过长。结果，应用可能变得无响应，设备可能会发热并迅速耗尽电池。为了避免这一切，您需要制作一个轻量级且高效的模型，而不会牺牲太多准确性。
    [TFLite](https://tensorflow.org/lite)库提供了几个工具，帮助您将模型部署到边缘，主要有三个目标：
- en: Reduce the model size, to shorten download time and reduce RAM usage.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减小模型大小，缩短下载时间并减少RAM使用量。
- en: Reduce the amount of computations needed for each prediction, to reduce latency,
    battery usage, and heating.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少每次预测所需的计算量，以减少延迟、电池使用量和发热。
- en: Adapt the model to device-specific constraints.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使模型适应特定设备的限制。
- en: 'To reduce the model size, TFLite’s model converter can take a SavedModel and
    compress it to a much lighter format based on [FlatBuffers](https://google.github.io/flatbuffers).
    This is an efficient cross-platform serialization library (a bit like protocol
    buffers) initially created by Google for gaming. It is designed so you can load
    FlatBuffers straight to RAM without any preprocessing: this reduces the loading
    time and memory footprint. Once the model is loaded into a mobile or embedded
    device, the TFLite interpreter will execute it to make predictions. Here is how
    you can convert a SavedModel to a FlatBuffer and save it to a *.tflite* file:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减小模型大小，TFLite 的模型转换器可以接受 SavedModel 并将其压缩为基于 [FlatBuffers](https://google.github.io/flatbuffers)
    的更轻量级格式。这是一个高效的跨平台序列化库（有点像协议缓冲区），最初由谷歌为游戏创建。它设计成可以直接将 FlatBuffers 加载到 RAM 中，无需任何预处理：这样可以减少加载时间和内存占用。一旦模型加载到移动设备或嵌入式设备中，TFLite
    解释器将执行它以进行预测。以下是如何将 SavedModel 转换为 FlatBuffer 并保存为 *.tflite* 文件的方法：
- en: '[PRE30]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Tip
  id: totrans-140
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can also save a Keras model directly to a FlatBuffer using `tf.lite.TFLiteConverter.from_keras_model(model)`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 `tf.lite.TFLiteConverter.from_keras_model(model)` 将 Keras 模型直接保存为 FlatBuffer
    格式。
- en: The converter also optimizes the model, both to shrink it and to reduce its
    latency. It prunes all the operations that are not needed to make predictions
    (such as training operations), and it optimizes computations whenever possible;
    for example, 3 × *a* + 4 ×_ a_ + 5 × *a* will be converted to 12 × *a*. Addtionally,
    it tries to fuse operations whenever possible. For example, if possible, batch
    normalization layers end up folded into the previous layer’s addition and multiplication
    operations. To get a good idea of how much TFLite can optimize a model, download
    one of the [pretrained TFLite models](https://homl.info/litemodels), such as *Inception_V1_quant*
    (click *tflite&pb*), unzip the archive, then open the excellent [Netron graph
    visualization tool](https://netron.app) and upload the *.pb* file to view the
    original model. It’s a big, complex graph, right? Next, open the optimized *.tflite*
    model and marvel at its beauty!
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器还优化模型，既缩小模型大小，又减少延迟。它修剪所有不需要进行预测的操作（例如训练操作），并在可能的情况下优化计算；例如，3 × *a* + 4 ×_
    a_ + 5 × *a* 将被转换为12 × *a*。此外，它尝试在可能的情况下融合操作。例如，如果可能的话，批量归一化层最终会合并到前一层的加法和乘法操作中。要了解TFLite可以对模型进行多少优化，可以下载其中一个[预训练的TFLite模型](https://homl.info/litemodels)，例如*Inception_V1_quant*（点击*tflite&pb*），解压缩存档，然后打开优秀的[Netron图形可视化工具](https://netron.app)并上传*.pb*文件以查看原始模型。这是一个庞大而复杂的图形，对吧？接下来，打开优化后的*.tflite*模型，惊叹于其美丽！
- en: 'Another way you can reduce the model size—other than simply using smaller neural
    network architectures—is by using smaller bit-widths: for example, if you use
    half-floats (16 bits) rather than regular floats (32 bits), the model size will
    shrink by a factor of 2, at the cost of a (generally small) accuracy drop. Moreover,
    training will be faster, and you will use roughly half the amount of GPU RAM.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 除了简单地使用较小的神经网络架构之外，您可以减小模型大小的另一种方法是使用较小的位宽：例如，如果您使用半精度浮点数（16位）而不是常规浮点数（32位），模型大小将缩小2倍，代价是（通常很小的）准确度下降。此外，训练速度将更快，您将使用大约一半的GPU内存。
- en: 'TFLite’s converter can go further than that, by quantizing the model weights
    down to fixed-point, 8-bit integers! This leads to a fourfold size reduction compared
    to using 32-bit floats. The simplest approach is called *post-training quantization*:
    it just quantizes the weights after training, using a fairly basic but efficient
    symmetrical quantization technique. It finds the maximum absolute weight value,
    *m*, then it maps the floating-point range –*m* to +*m* to the fixed-point (integer)
    range –127 to +127\. For example, if the weights range from –1.5 to +0.8, then
    the bytes –127, 0, and +127 will correspond to the floats –1.5, 0.0, and +1.5,
    respectively (see [Figure 19-5](#quantization_diagram)). Note that 0.0 always
    maps to 0 when using symmetrical quantization. Also note that the byte values
    +68 to +127 will not be used in this example, since they map to floats greater
    than +0.8.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1905](assets/mls3_1905.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: Figure 19-5\. From 32-bit floats to 8-bit integers, using symmetrical quantization
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-5。从32位浮点数到8位整数，使用对称量化
- en: 'To perform this post-training quantization, simply add `DEFAULT` to the list
    of converter optimizations before calling the `convert()` method:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行这种训练后的量化，只需在调用`convert()`方法之前将`DEFAULT`添加到转换器优化列表中：
- en: '[PRE31]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This technique dramatically reduces the model’s size, which makes it much faster
    to download, and uses less storage space. At runtime the quantized weights get
    converted back to floats before they are used. These recovered floats are not
    perfectly identical to the original floats, but they’re not too far off, so the
    accuracy loss is usually acceptable. To avoid recomputing the float values all
    the time, which would severely slow down the model, TFLite caches them: unfortunately,
    this means that this technique does not reduce RAM usage, and it doesn’t speed
    up the model either. It’s mostly useful to reduce the application’s size.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术显著减小了模型的大小，使得下载速度更快，占用的存储空间更少。在运行时，量化的权重在使用之前会被转换回浮点数。这些恢复的浮点数与原始浮点数并不完全相同，但也不会相差太远，因此精度损失通常是可以接受的。为了避免一直重新计算浮点值，这样会严重减慢模型的速度，TFLite会对其进行缓存：不幸的是，这意味着这种技术并不会减少RAM的使用量，也不会加快模型的速度。它主要用于减小应用程序的大小。
- en: The most effective way to reduce latency and power consumption is to also quantize
    the activations so that the computations can be done entirely with integers, without
    the need for any floating-point operations. Even when using the same bit-width
    (e.g., 32-bit integers instead of 32-bit floats), integer computations use less
    CPU cycles, consume less energy, and produce less heat. And if you also reduce
    the bit-width (e.g., down to 8-bit integers), you can get huge speedups. Moreover,
    some neural network accelerator devices—such as Google’s Edge TPU—can only process
    integers, so full quantization of both weights and activations is compulsory.
    This can be done post-training; it requires a calibration step to find the maximum
    absolute value of the activations, so you need to provide a representative sample
    of training data to TFLite (it does not need to be huge), and it will process
    the data through the model and measure the activation statistics required for
    quantization. This step is typically fast.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 减少延迟和功耗的最有效方法是对激活进行量化，使得计算可以完全使用整数，而无需任何浮点运算。即使使用相同的位宽（例如，32位整数而不是32位浮点数），整数计算使用的CPU周期更少，消耗的能量更少，产生的热量也更少。如果还减少位宽（例如，降至8位整数），可以获得巨大的加速。此外，一些神经网络加速器设备（如Google的Edge
    TPU）只能处理整数，因此权重和激活的完全量化是强制性的。这可以在训练后完成；它需要一个校准步骤来找到激活的最大绝对值，因此您需要向TFLite提供代表性的训练数据样本（不需要很大），它将通过模型处理数据并测量量化所需的激活统计信息。这一步通常很快。
- en: 'The main problem with quantization is that it loses a bit of accuracy: it is
    similar to adding noise to the weights and activations. If the accuracy drop is
    too severe, then you may need to use *quantization-aware training*. This means
    adding fake quantization operations to the model so it can learn to ignore the
    quantization noise during training; the final weights will then be more robust
    to quantization. Moreover, the calibration step can be taken care of automatically
    during training, which simplifies the whole process.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 量化的主要问题是它会失去一点准确性：这类似于在权重和激活中添加噪声。如果准确性下降太严重，那么您可能需要使用*量化感知训练*。这意味着向模型添加虚假量化操作，以便它在训练过程中学会忽略量化噪声；最终的权重将更加稳健地适应量化。此外，校准步骤可以在训练过程中自动处理，这简化了整个过程。
- en: 'I have explained the core concepts of TFLite, but going all the way to coding
    a mobile or embedded application woud require a dedicated book. Fortunately, some
    exist: if you want to learn more about building TensorFlow applications for mobile
    and embedded devices, check out the O’Reilly books [*TinyML: Machine Learning
    with TensorFlow on Arduino and Ultra-Low Power Micro-Controllers*](https://homl.info/tinyml),
    by Pete Warden (former lead of the TFLite team) and Daniel Situnayake and [*AI
    and Machine Learning for On-Device Development*](https://homl.info/ondevice),
    by Laurence Moroney.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '我已经解释了TFLite的核心概念，但要完全编写移动或嵌入式应用程序需要一本专门的书。幸运的是，一些书籍存在：如果您想了解有关为移动和嵌入式设备构建TensorFlow应用程序的更多信息，请查看O''Reilly的书籍[*TinyML:
    Machine Learning with TensorFlow on Arduino and Ultra-Low Power Micro-Controllers*](https://homl.info/tinyml)，作者是Pete
    Warden（TFLite团队的前负责人）和Daniel Situnayake，以及[*AI and Machine Learning for On-Device
    Development*](https://homl.info/ondevice)，作者是Laurence Moroney。'
- en: Now what if you want to use your model in a website, running directly in the
    user’s browser?
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果您想在网站中使用您的模型，在用户的浏览器中直接运行呢？
- en: Running a Model in a Web Page
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在网页中运行模型
- en: 'Running your machine learning model on the client side, in the user’s browser,
    rather than on the server side can be useful in many scenarios, such as:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在客户端，即用户的浏览器中运行您的机器学习模型，而不是在服务器端运行，可以在许多场景下非常有用，例如：
- en: When your web application is often used in situations where the user’s connectivity
    is intermittent or slow (e.g., a website for hikers), so running the model directly
    on the client side is the only way to make your website reliable.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您的网络应用经常在用户的连接不稳定或缓慢的情况下使用（例如，徒步者的网站），因此在客户端直接运行模型是使您的网站可靠的唯一方法。
- en: When you need the model’s responses to be as fast as possible (e.g., for an
    online game). Removing the need to query the server to make predictions will definitely
    reduce the latency and make the website much more responsive.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您需要模型的响应尽可能快时（例如，用于在线游戏）。消除查询服务器进行预测的需要肯定会减少延迟，并使网站更加响应。
- en: When your web service makes predictions based on some private user data, and
    you want to protect the user’s privacy by making the predictions on the client
    side so that the private data never has to leave the user’s machine.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您的网络服务基于一些私人用户数据进行预测，并且您希望通过在客户端进行预测来保护用户的隐私，以便私人数据永远不必离开用户的设备。
- en: 'For all these scenarios, you can use the [TensorFlow.js (TFJS) JavaScript library](https://tensorflow.org/js).
    This library can load a TFLite model and make predictions directly in the user’s
    browser. For example, the following JavaScript module imports the TFJS library,
    downloads a pretrained MobileNet model, and uses this model to classify an image
    and log the predictions. You can play with the code at [*https://homl.info/tfjscode*](https://homl.info/tfjscode),
    using Glitch.com, a website that lets you build web apps in your browser for free;
    click the PREVIEW button in the lower-right corner of the page to see the code
    in action:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有这些场景，您可以使用[TensorFlow.js（TFJS）JavaScript库](https://tensorflow.org/js)。该库可以在用户的浏览器中加载TFLite模型并直接进行预测。例如，以下JavaScript模块导入了TFJS库，下载了一个预训练的MobileNet模型，并使用该模型对图像进行分类并记录预测结果。您可以在[*https://homl.info/tfjscode*](https://homl.info/tfjscode)上尝试这段代码，使用Glitch.com，这是一个允许您免费在浏览器中构建Web应用程序的网站；点击页面右下角的预览按钮查看代码的运行情况：
- en: '[PRE32]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'It’s even possible to turn this website into a *progressive web app* (PWA):
    this is a website that respects a number of criteria⁠^([8](ch19.html#idm45720160353952))
    that allow it to be viewed in any browser, and even installed as a standalone
    app on a mobile device. For example, try visiting [*https://homl.info/tfjswpa*](https://homl.info/tfjswpa)
    on a mobile device: most modern browsers will ask you whether you would like to
    add TFJS Demo to your home screen. If you accept, you will see a new icon in your
    list of applications. Clicking this icon will load the TFJS Demo website inside
    its own window, just like a regular mobile app. A PWA can even be configured to
    work offline, by using a *service worker*: this is a JavaScript module that runs
    in its own separate thread in the browser and intercepts network requests, allowing
    it to cache resources so the PWA can run faster, or even entirely offline. It
    can also deliver push messages, run tasks in the background, and more. PWAs allow
    you to manage a single code base for the web and for mobile devices. They also
    make it easier to ensure that all users run the same version of your application.
    You can play with this TFJS Demo’s PWA code on Glitch.com at [*https://homl.info/wpacode*](https://homl.info/wpacode).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 甚至可以将这个网站转变成一个*渐进式Web应用程序*（PWA）：这是一个遵守一系列标准的网站，使其可以在任何浏览器中查看，甚至可以在移动设备上作为独立应用程序安装。例如，在移动设备上尝试访问[*https://homl.info/tfjswpa*](https://homl.info/tfjswpa)：大多数现代浏览器会询问您是否想要将TFJS演示添加到主屏幕。如果您接受，您将在应用程序列表中看到一个新图标。点击此图标将在其自己的窗口中加载TFJS演示网站，就像常规移动应用程序一样。PWA甚至可以配置为离线工作，通过使用*服务工作者*：这是一个在浏览器中以自己独立线程运行的JavaScript模块，拦截网络请求，使其可以缓存资源，从而使PWA可以更快地运行，甚至完全离线运行。它还可以传递推送消息，在后台运行任务等。PWA允许您管理Web和移动设备的单个代码库。它们还使得更容易确保所有用户运行您应用程序的相同版本。您可以在Glitch.com上玩这个TFJS演示的PWA代码，网址是[*https://homl.info/wpacode*](https://homl.info/wpacode)。
- en: Tip
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Check out many more demos of machine learning models running in your browser
    at [*https://tensorflow.org/js/demos*](https://tensorflow.org/js/demos).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*https://tensorflow.org/js/demos*](https://tensorflow.org/js/demos)上查看更多在您的浏览器中运行的机器学习模型的演示。
- en: TFJS also supports training a model directly in your web browser! And it’s actually
    pretty fast. If your computer has a GPU card, then TFJS can generally use it,
    even if it’s not an Nvidia card. Indeed, TFJS will use WebGL when it’s available,
    and since modern web browsers generally support a wide range of GPU cards, TFJS
    actually supports more GPU cards than regular TensorFlow (which only supports
    Nvidia cards).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: TFJS还支持在您的网络浏览器中直接训练模型！而且速度相当快。如果您的计算机有GPU卡，那么TFJS通常可以使用它，即使它不是Nvidia卡。实际上，TFJS将在可用时使用WebGL，由于现代网络浏览器通常支持各种GPU卡，TFJS实际上支持的GPU卡比常规的TensorFlow更多（后者仅支持Nvidia卡）。
- en: Training a model in a user’s web browser can be especially useful to guarantee
    that this user’s data remains private. A model can be trained centrally, and then
    fine-tuned locally, in the browser, based on that user’s data. If you’re interested
    in this topic, check out [*federated learning*](https://tensorflow.org/federated).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在用户的网络浏览器中训练模型可以特别有用，可以确保用户的数据保持私密。模型可以在中央进行训练，然后在浏览器中根据用户的数据进行本地微调。如果您对这个话题感兴趣，请查看[*联邦学习*](https://tensorflow.org/federated)。
- en: Once again, doing justice to this topic would require a whole book. If you want
    to learn more about TensorFlow.js, check out the O’reilly books [*Practical Deep
    Learning for Cloud, Mobile, and Edge*](https://homl.info/tfjsbook), by Anirudh
    Koul et al., or [*Learning TensorFlow.js*](https://homl.info/tfjsbook2), by Gant
    Laborde.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，要全面涵盖这个主题需要一本完整的书。如果您想了解更多关于TensorFlow.js的内容，请查看O'reilly图书《云端、移动和边缘的实用深度学习》（Anirudh
    Koul等著）或《学习TensorFlow.js》（Gant Laborde著）。
- en: Now that you’ve seen how to deploy TensorFlow models to TF Serving, or to the
    cloud with Vertex AI, or to mobile and embedded devices using TFLite, or to a
    web browser using TFJS, let’s discuss how to use GPUs to speed up computations.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到如何将TensorFlow模型部署到TF Serving，或者通过Vertex AI部署到云端，或者使用TFLite部署到移动和嵌入式设备，或者使用TFJS部署到Web浏览器，让我们讨论如何使用GPU加速计算。
- en: Using GPUs to Speed Up Computations
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GPU加速计算
- en: 'In [Chapter 11](ch11.html#deep_chapter) we looked at several techniques that
    can considerably speed up training: better weight initialization, sophisticated
    optimizers, and so on. But even with all of these techniques, training a large
    neural network on a single machine with a single CPU can take hours, days, or
    even weeks, depending on the task. Thanks to GPUs, this training time can be reduced
    down to minutes or hours. Not only does this save an enormous amount of time,
    but it also means that you can experiment with various models much more easily,
    and frequently retrain your models on fresh data.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11章](ch11.html#deep_chapter)中，我们看了几种可以显著加快训练速度的技术：更好的权重初始化、复杂的优化器等等。但即使使用了所有这些技术，使用单个CPU的单台机器训练大型神经网络可能需要几个小时、几天，甚至几周，具体取决于任务。由于GPU的出现，这种训练时间可以缩短到几分钟或几小时。这不仅节省了大量时间，还意味着您可以更轻松地尝试各种模型，并经常使用新数据重新训练您的模型。
- en: 'In the previous chapters, we used GPU-enabled runtimes on Google Colab. All
    you have to do for this is select “Change runtime type” from the Runtime menu,
    and choose the GPU accelerator type; TensorFlow automatically detects the GPU
    and uses it to speed up computations, and the code is exactly the same as without
    a GPU. Then, in this chapter you saw how to deploy your models to Vertex AI on
    multiple GPU-enabled compute nodes: it’s just a matter of selecting the right
    GPU-enabled Docker image when creating the Vertex AI model, and selecting the
    desired GPU type when calling `endpoint.deploy()`. But what if you want to buy
    your own GPU? And what if you want to distribute the computations across the CPU
    and multiple GPU devices on a single machine (see [Figure 19-6](#multiple_devices_diagram))?
    This is what we will discuss now, then later in this chapter we will discuss how
    to distribute computations across multiple servers.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1906](assets/mls3_1906.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: Figure 19-6\. Executing a TensorFlow graph across multiple devices in parallel
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-6。在多个设备上并行执行TensorFlow图
- en: Getting Your Own GPU
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取自己的GPU
- en: If you know that you’ll be using a GPU heavily and for a long period of time,
    then buying your own can make financial sense. You may also want to train your
    models locally because you do not want to upload your data to the cloud. Or perhaps
    you just want to buy a GPU card for gaming, and you’d like to use it for deep
    learning as well.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道你将会长时间大量使用GPU，那么购买自己的GPU可能是经济上合理的。你可能也想在本地训练模型，因为你不想将数据上传到云端。或者你只是想购买一张用于游戏的GPU卡，并且想将其用于深度学习。
- en: 'If you decide to purchase a GPU card, then take some time to make the right
    choice. You will need to consider the amount of RAM you will need for your tasks
    (e.g., typically at least 10 GB for image processing or NLP), the bandwidth (i.e.,
    how fast you can send data into and out of the GPU), the number of cores, the
    cooling system, etc. Tim Dettmers wrote an [excellent blog post](https://homl.info/66)
    to help you choose: I encourage you to read it carefully. At the time of this
    writing, TensorFlow only supports [Nvidia cards with CUDA Compute Capability 3.5+](https://homl.info/cudagpus)
    (as well as Google’s TPUs, of course), but it may extend its support to other
    manufacturers, so make sure to check [TensorFlow’s documentation](https://tensorflow.org/install)
    to see what devices are supported today.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您决定购买GPU卡，那么请花些时间做出正确的选择。您需要考虑您的任务所需的RAM数量（例如，图像处理或NLP通常至少需要10GB），带宽（即您可以将数据发送到GPU和从GPU中发送数据的速度），核心数量，冷却系统等。Tim
    Dettmers撰写了一篇[优秀的博客文章](https://homl.info/66)来帮助您选择：我鼓励您仔细阅读。在撰写本文时，TensorFlow仅支持[具有CUDA
    Compute Capability 3.5+的Nvidia卡](https://homl.info/cudagpus)（当然还有Google的TPU），但它可能会将其支持扩展到其他制造商，因此请务必查看[TensorFlow的文档](https://tensorflow.org/install)以了解今天支持哪些设备。
- en: If you go for an Nvidia GPU card, you will need to install the appropriate Nvidia
    drivers and several Nvidia libraries.⁠^([9](ch19.html#idm45720160307664)) These
    include the *Compute Unified Device Architecture* library (CUDA) Toolkit, which
    allows developers to use CUDA-enabled GPUs for all sorts of computations (not
    just graphics acceleration), and the *CUDA Deep Neural Network* library (cuDNN),
    a GPU-accelerated library of common DNN computations such as activation layers,
    normalization, forward and backward convolutions, and pooling (see [Chapter 14](ch14.html#cnn_chapter)).
    cuDNN is part of Nvidia’s Deep Learning SDK. Note that you will need to create
    an Nvidia developer account in order to download it. TensorFlow uses CUDA and
    cuDNN to control the GPU cards and accelerate computations (see [Figure 19-7](#cuda_cudnn_diagram)).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择Nvidia GPU卡，您将需要安装适当的Nvidia驱动程序和几个Nvidia库。这些包括*计算统一设备架构*库（CUDA）工具包，它允许开发人员使用支持CUDA的GPU进行各种计算（不仅仅是图形加速），以及*CUDA深度神经网络*库（cuDNN），一个GPU加速的常见DNN计算库，例如激活层、归一化、前向和反向卷积以及池化（参见[第14章](ch14.html#cnn_chapter)）。cuDNN是Nvidia的深度学习SDK的一部分。请注意，您需要创建一个Nvidia开发者帐户才能下载它。TensorFlow使用CUDA和cuDNN来控制GPU卡并加速计算（参见[图19-7](#cuda_cudnn_diagram)）。
- en: '![mls3 1907](assets/mls3_1907.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1907](assets/mls3_1907.png)'
- en: Figure 19-7\. TensorFlow uses CUDA and cuDNN to control GPUs and boost DNNs
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-7. TensorFlow使用CUDA和cuDNN来控制GPU并加速DNNs
- en: 'Once you have installed the GPU card(s) and all the required drivers and libraries,
    you can use the `nvidia-smi` command to check that everything is properly installed.
    This command lists the available GPU cards, as well as all the processes running
    on each card. In this example, it’s an Nvidia Tesla T4 GPU card with about 15
    GB of available RAM, and there are no processes currently running on it:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了GPU卡和所有必需的驱动程序和库之后，您可以使用`nvidia-smi`命令来检查一切是否正确安装。该命令列出了可用的GPU卡，以及每张卡上运行的所有进程。在这个例子中，这是一张Nvidia
    Tesla T4 GPU卡，大约有15GB的可用内存，并且当前没有任何进程在运行：
- en: '[PRE33]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To check that TensorFlow actually sees your GPU, run the following commands
    and make sure the result is not empty:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查TensorFlow是否真正看到您的GPU，请运行以下命令并确保结果不为空：
- en: '[PRE34]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Managing the GPU RAM
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理GPU内存
- en: 'By default TensorFlow automatically grabs almost all the RAM in all available
    GPUs the first time you run a computation. It does this to limit GPU RAM fragmentation.
    This means that if you try to start a second TensorFlow program (or any program
    that requires the GPU), it will quickly run out of RAM. This does not happen as
    often as you might think, as you will most often have a single TensorFlow program
    running on a machine: usually a training script, a TF Serving node, or a Jupyter
    notebook. If you need to run multiple programs for some reason (e.g., to train
    two different models in parallel on the same machine), then you will need to split
    the GPU RAM between these processes more evenly.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，TensorFlow在第一次运行计算时会自动占用几乎所有可用GPU的RAM，以限制GPU RAM的碎片化。这意味着如果您尝试启动第二个TensorFlow程序（或任何需要GPU的程序），它将很快耗尽RAM。这种情况并不像您可能认为的那样经常发生，因为通常您会在一台机器上运行一个单独的TensorFlow程序：通常是一个训练脚本、一个TF
    Serving节点或一个Jupyter笔记本。如果出于某种原因需要运行多个程序（例如，在同一台机器上并行训练两个不同的模型），那么您需要更均匀地在这些进程之间分配GPU
    RAM。
- en: 'If you have multiple GPU cards on your machine, a simple solution is to assign
    each of them to a single process. To do this, you can set the `CUDA_VISIBLE_DEVICES`
    environment variable so that each process only sees the appropriate GPU card(s).
    Also set the `CUDA_DEVICE_ORDER` environment variable to `PCI_BUS_ID` to ensure
    that each ID always refers to the same GPU card. For example, if you have four
    GPU cards, you could start two programs, assigning two GPUs to each of them, by
    executing commands like the following in two separate terminal windows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的机器上有多个GPU卡，一个简单的解决方案是将每个GPU卡分配给单个进程。为此，您可以设置`CUDA_VISIBLE_DEVICES`环境变量，以便每个进程只能看到适当的GPU卡。还要设置`CUDA_DEVICE_ORDER`环境变量为`PCI_BUS_ID`，以确保每个ID始终指向相同的GPU卡。例如，如果您有四个GPU卡，您可以启动两个程序，将两个GPU分配给每个程序，通过在两个单独的终端窗口中执行以下命令来实现：
- en: '[PRE35]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Program 1 will then only see GPU cards 0 and 1, named `"/gpu:0"` and `"/gpu:1"`,
    respectively, in TensorFlow, and program 2 will only see GPU cards 2 and 3, named
    `"/gpu:1"` and `"/gpu:0"`, respectively (note the order). Everything will work
    fine (see [Figure 19-8](#splitting_gpus_diagram)). Of course, you can also define
    these environment variables in Python by setting `os.environ["CUDA_DEVICE_ORDER"]`
    and `os.environ["CUDA_​VISI⁠BLE_DEVICES"]`, as long as you do so before using
    TensorFlow.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 程序1将只看到GPU卡0和1，分别命名为`"/gpu:0"`和`"/gpu:1"`，在TensorFlow中，程序2将只看到GPU卡2和3，分别命名为`"/gpu:1"`和`"/gpu:0"`（注意顺序）。一切都将正常工作（参见[图19-8](#splitting_gpus_diagram)）。当然，您也可以在Python中通过设置`os.environ["CUDA_DEVICE_ORDER"]`和`os.environ["CUDA_VISIBLE_DEVICES"]`来定义这些环境变量，只要在使用TensorFlow之前这样做。
- en: '![mls3 1908](assets/mls3_1908.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1908](assets/mls3_1908.png)'
- en: Figure 19-8\. Each program gets two GPUs
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-8。每个程序获得两个GPU
- en: 'Another option is to tell TensorFlow to grab only a specific amount of GPU
    RAM. This must be done immediately after importing TensorFlow. For example, to
    make TensorFlow grab only 2 GiB of RAM on each GPU, you must create a *logical
    GPU device* (sometimes called a *virtual GPU device*) for each physical GPU device
    and set its memory limit to 2 GiB (i.e., 2,048 MiB):'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '另一个选项是告诉TensorFlow只获取特定数量的GPU RAM。这必须在导入TensorFlow后立即完成。例如，要使TensorFlow只在每个GPU上获取2
    GiB的RAM，您必须为每个物理GPU设备创建一个*逻辑GPU设备*（有时称为*虚拟GPU设备*），并将其内存限制设置为2 GiB（即2,048 MiB）:'
- en: '[PRE36]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Let’s suppose you have four GPUs, each with at least 4 GiB of RAM: in this
    case, two programs like this one can run in parallel, each using all four GPU
    cards (see [Figure 19-9](#sharing_gpus_diagram)). If you run the `nvidia-smi`
    command while both programs are running, you should see that each process holds
    2 GiB of RAM on each card.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有四个GPU，每个GPU至少有4 GiB的RAM：在这种情况下，可以并行运行两个像这样的程序，每个程序使用所有四个GPU卡（请参见[图19-9](#sharing_gpus_diagram)）。如果在两个程序同时运行时运行`nvidia-smi`命令，则应该看到每个进程在每张卡上占用2
    GiB的RAM。
- en: '![mls3 1909](assets/mls3_1909.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1909](assets/mls3_1909.png)'
- en: Figure 19-9\. Each program gets all four GPUs, but with only 2 GiB of RAM on
    each GPU
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-9。每个程序都可以获得四个GPU，但每个GPU只有2 GiB的RAM
- en: 'Yet another option is to tell TensorFlow to grab memory only when it needs
    it. Again, this must be done immediately after importing TensorFlow:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是告诉TensorFlow只在需要时获取内存。同样，在导入TensorFlow后必须立即执行此操作：
- en: '[PRE37]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Another way to do this is to set the `TF_FORCE_GPU_ALLOW_GROWTH` environment
    variable to `true`. With this option, TensorFlow will never release memory once
    it has grabbed it (again, to avoid memory fragmentation), except of course when
    the program ends. It can be harder to guarantee deterministic behavior using this
    option (e.g., one program may crash because another program’s memory usage went
    through the roof), so in production you’ll probably want to stick with one of
    the previous options. However, there are some cases where it is very useful: for
    example, when you use a machine to run multiple Jupyter notebooks, several of
    which use TensorFlow. The `TF_FORCE_GPU_ALLOW_GROWTH` environment variable is
    set to `true` in Colab runtimes.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是将`TF_FORCE_GPU_ALLOW_GROWTH`环境变量设置为`true`。使用这个选项，TensorFlow一旦分配了内存就不会释放它（再次，为了避免内存碎片化），除非程序结束。使用这个选项很难保证确定性行为（例如，一个程序可能会崩溃，因为另一个程序的内存使用量激增），因此在生产环境中，您可能会选择之前的选项之一。然而，有一些情况下它非常有用：例如，当您使用一台机器运行多个Jupyter笔记本时，其中几个使用了TensorFlow。在Colab运行时，`TF_FORCE_GPU_ALLOW_GROWTH`环境变量被设置为`true`。
- en: 'Lastly, in some cases you may want to split a GPU into two or more *logical
    devices*. For example, this is useful if you only have one physical GPU—like in
    a Colab runtime—but you want to test a multi-GPU algorithm. The following code
    splits GPU #0 into two logical devices, with 2 GiB of RAM each (again, this must
    be done immediately after importing TensorFlow):'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在某些情况下，您可能希望将一个GPU分成两个或更多*逻辑设备*。例如，如果您只有一个物理GPU，比如在Colab运行时，但您想要测试一个多GPU算法，这将非常有用。以下代码将GPU＃0分成两个逻辑设备，每个设备有2
    GiB的RAM（同样，在导入TensorFlow后立即执行）：
- en: '[PRE38]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'These two logical devices are called `"/gpu:0"` and `"/gpu:1"`, and you can
    use them as if they were two normal GPUs. You can list all logical devices like
    this:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个逻辑设备被称为`"/gpu:0"`和`"/gpu:1"`, 你可以像使用两个普通GPU一样使用它们。你可以像这样列出所有逻辑设备：
- en: '[PRE39]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now let’s see how TensorFlow decides which devices it should use to place variables
    and execute operations.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看TensorFlow如何决定应该使用哪些设备来放置变量和执行操作。
- en: Placing Operations and Variables on Devices
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将操作和变量放在设备上
- en: 'Keras and tf.data generally do a good job of placing operations and variables
    where they belong, but you can also place operations and variables manually on
    each device, if you want more control:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Keras和tf.data通常会很好地将操作和变量放在它们应该在的位置，但如果您想要更多控制，您也可以手动将操作和变量放在每个设备上：
- en: You generally want to place the data preprocessing operations on the CPU, and
    place the neural network operations on the GPUs.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，您希望将数据预处理操作放在CPU上，并将神经网络操作放在GPU上。
- en: GPUs usually have a fairly limited communication bandwidth, so it is important
    to avoid unnecessary data transfers into and out of the GPUs.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU通常具有相对有限的通信带宽，因此重要的是要避免不必要的数据传输进出GPU。
- en: 'Adding more CPU RAM to a machine is simple and fairly cheap, so there’s usually
    plenty of it, whereas the GPU RAM is baked into the GPU: it is an expensive and
    thus limited resource, so if a variable is not needed in the next few training
    steps, it should probably be placed on the CPU (e.g., datasets generally belong
    on the CPU).'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向机器添加更多的CPU RAM是简单且相对便宜的，因此通常有很多，而GPU RAM是内置在GPU中的：它是一种昂贵且有限的资源，因此如果一个变量在接下来的几个训练步骤中不需要，它可能应该放在CPU上（例如，数据集通常应该放在CPU上）。
- en: By default, all variables and all operations will be placed on the first GPU
    (the one named `"/gpu:0"`), except for variables and operations that don’t have
    a GPU kernel:⁠^([10](ch19.html#idm45720159934800)) these are placed on the CPU
    (always named `"/cpu:0"`). A tensor or variable’s `device` attribute tells you
    which device it was placed on:⁠^([11](ch19.html#idm45720159930608))
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有变量和操作都将放置在第一个GPU上（命名为`"/gpu:0"`），除非变量和操作没有GPU内核：这些将放置在CPU上（始终命名为`"/cpu:0"`）。张量或变量的`device`属性告诉您它被放置在哪个设备上。
- en: '[PRE40]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'You can safely ignore the prefix `/job:localhost/replica:0/task:0` for now;
    we will discuss jobs, replicas, and tasks later in this chapter. As you can see,
    the first variable was placed on GPU #0, which is the default device. However,
    the second variable was placed on the CPU: this is because there are no GPU kernels
    for integer variables, or for operations involving integer tensors, so TensorFlow
    fell back to the CPU.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以安全地忽略前缀`/job:localhost/replica:0/task:0`；我们将在本章后面讨论作业、副本和任务。正如您所看到的，第一个变量被放置在GPU＃0上，这是默认设备。但是，第二个变量被放置在CPU上：这是因为整数变量没有GPU内核，或者涉及整数张量的操作没有GPU内核，因此TensorFlow回退到CPU。
- en: 'If you want to place an operation on a different device than the default one,
    use a `tf.device()` context:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在与默认设备不同的设备上执行操作，请使用`tf.device()`上下文：
- en: '[PRE41]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note
  id: totrans-213
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The CPU is always treated as a single device (`"/cpu:0"`), even if your machine
    has multiple CPU cores. Any operation placed on the CPU may run in parallel across
    multiple cores if it has a multithreaded kernel.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: CPU始终被视为单个设备（`"/cpu:0"`），即使您的计算机有多个CPU核心。放置在CPU上的任何操作，如果具有多线程内核，则可能在多个核心上并行运行。
- en: If you explicitly try to place an operation or variable on a device that does
    not exist or for which there is no kernel, then TensorFlow will silently fall
    back to the device it would have chosen by default. This is useful when you want
    to be able to run the same code on different machines that don’t have the same
    number of GPUs. However, you can run `tf.config.set_soft_device_placement(False)`
    if you prefer to get an exception.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您明确尝试将操作或变量放置在不存在或没有内核的设备上，那么TensorFlow将悄悄地回退到默认选择的设备。当您希望能够在不具有相同数量的GPU的不同机器上运行相同的代码时，这是很有用的。但是，如果您希望获得异常，可以运行`tf.config.set_soft_device_placement(False)`。
- en: Now, how exactly does TensorFlow execute operations across multiple devices?
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，TensorFlow 如何在多个设备上执行操作呢？
- en: Parallel Execution Across Multiple Devices
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跨多个设备并行执行
- en: As we saw in [Chapter 12](ch12.html#tensorflow_chapter), one of the benefits
    of using TF functions is parallelism. Let’s look at this a bit more closely. When
    TensorFlow runs a TF function, it starts by analyzing its graph to find the list
    of operations that need to be evaluated, and it counts how many dependencies each
    of them has. TensorFlow then adds each operation with zero dependencies (i.e.,
    each source operation) to the evaluation queue of this operation’s device (see
    [Figure 19-10](#parallelization_diagram)). Once an operation has been evaluated,
    the dependency counter of each operation that depends on it is decremented. Once
    an operation’s dependency counter reaches zero, it is pushed to the evaluation
    queue of its device. And once all the outputs have been computed, they are returned.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第12章](ch12.html#tensorflow_chapter)中看到的，使用TF函数的一个好处是并行性。让我们更仔细地看一下这一点。当TensorFlow运行一个TF函数时，它首先分析其图形，找到需要评估的操作列表，并计算每个操作的依赖关系数量。然后TensorFlow将每个具有零依赖关系的操作（即每个源操作）添加到该操作设备的评估队列中（参见[图19-10](#parallelization_diagram)）。一旦一个操作被评估，依赖于它的每个操作的依赖计数器都会减少。一旦一个操作的依赖计数器达到零，它就会被推送到其设备的评估队列中。一旦所有输出都被计算出来，它们就会被返回。
- en: '![mls3 1910](assets/mls3_1910.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1910](assets/mls3_1910.png)'
- en: Figure 19-10\. Parallelized execution of a TensorFlow graph
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-10. TensorFlow图的并行执行
- en: 'Operations in the CPU’s evaluation queue are dispatched to a thread pool called
    the *inter-op thread pool*. If the CPU has multiple cores, then these operations
    will effectively be evaluated in parallel. Some operations have multithreaded
    CPU kernels: these kernels split their tasks into multiple suboperations, which
    are placed in another evaluation queue and dispatched to a second thread pool
    called the *intra-op thread pool* (shared by all multithreaded CPU kernels). In
    short, multiple operations and suboperations may be evaluated in parallel on different
    CPU cores.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: CPU的评估队列中的操作被分派到一个称为*inter-op线程池*的线程池中。如果CPU有多个核心，那么这些操作将有效地并行评估。一些操作具有多线程CPU内核：这些内核将其任务分割为多个子操作，这些子操作被放置在另一个评估队列中，并分派到一个称为*intra-op线程池*的第二线程池中（由所有多线程CPU内核共享）。简而言之，多个操作和子操作可能在不同的CPU核心上并行评估。
- en: 'For the GPU, things are a bit simpler. Operations in a GPU’s evaluation queue
    are evaluated sequentially. However, most operations have multithreaded GPU kernels,
    typically implemented by libraries that TensorFlow depends on, such as CUDA and
    cuDNN. These implementations have their own thread pools, and they typically exploit
    as many GPU threads as they can (which is the reason why there is no need for
    an inter-op thread pool in GPUs: each operation already floods most GPU threads).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GPU来说，情况要简单一些。GPU的评估队列中的操作是按顺序评估的。然而，大多数操作都有多线程GPU内核，通常由TensorFlow依赖的库实现，比如CUDA和cuDNN。这些实现有自己的线程池，它们通常会利用尽可能多的GPU线程（这就是为什么GPU不需要一个跨操作线程池的原因：每个操作已经占用了大部分GPU线程）。
- en: 'For example, in [Figure 19-10](#parallelization_diagram), operations A, B,
    and C are source ops, so they can immediately be evaluated. Operations A and B
    are placed on the CPU, so they are sent to the CPU’s evaluation queue, then they
    are dispatched to the inter-op thread pool and immediately evaluated in parallel.
    Operation A happens to have a multithreaded kernel; its computations are split
    into three parts, which are executed in parallel by the intra-op thread pool.
    Operation C goes to GPU #0’s evaluation queue, and in this example its GPU kernel
    happens to use cuDNN, which manages its own intra-op thread pool and runs the
    operation across many GPU threads in parallel. Suppose C finishes first. The dependency
    counters of D and E are decremented and they reach 0, so both operations are pushed
    to GPU #0’s evaluation queue, and they are executed sequentially. Note that C
    only gets evaluated once, even though both D and E depend on it. Suppose B finishes
    next. Then F’s dependency counter is decremented from 4 to 3, and since that’s
    not 0, it does not run yet. Once A, D, and E are finished, then F’s dependency
    counter reaches 0, and it is pushed to the CPU’s evaluation queue and evaluated.
    Finally, TensorFlow returns the requested outputs.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '例如，在[图19-10](#parallelization_diagram)中，操作A、B和C是源操作，因此它们可以立即被评估。操作A和B被放置在CPU上，因此它们被发送到CPU的评估队列，然后被分派到跨操作线程池并立即并行评估。操作A恰好有一个多线程内核；它的计算被分成三部分，在操作线程池中并行执行。操作C进入GPU
    #0的评估队列，在这个例子中，它的GPU内核恰好使用cuDNN，它管理自己的内部操作线程池，并在许多GPU线程之间并行运行操作。假设C先完成。D和E的依赖计数器被减少到0，因此两个操作都被推送到GPU
    #0的评估队列，并按顺序执行。请注意，即使D和E都依赖于C，C也只被评估一次。假设B接下来完成。然后F的依赖计数器从4减少到3，由于不为0，它暂时不运行。一旦A、D和E完成，那么F的依赖计数器达到0，它被推送到CPU的评估队列并被评估。最后，TensorFlow返回请求的输出。'
- en: 'An extra bit of magic that TensorFlow performs is when the TF function modifies
    a stateful resource, such as a variable: it ensures that the order of execution
    matches the order in the code, even if there is no explicit dependency between
    the statements. For example, if your TF function contains `v.assign_add(1)` followed
    by `v.assign(v * 2)`, TensorFlow will ensure that these operations are executed
    in that order.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow执行的另一个神奇之处是当TF函数修改状态资源（例如变量）时：它确保执行顺序与代码中的顺序匹配，即使语句之间没有显式依赖关系。例如，如果您的TF函数包含`v.assign_add(1)`，然后是`v.assign(v
    * 2)`，TensorFlow将确保这些操作按照这个顺序执行。
- en: Tip
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can control the number of threads in the inter-op thread pool by calling
    `tf.config.threading.set_inter_op_parallelism_threads()`. To set the number of
    intra-op threads, use `tf.config.threading.set_intra_op_parallelism_threads()`.
    This is useful if you do not want TensorFlow to use all the CPU cores or if you
    want it to be single-threaded.⁠^([12](ch19.html#idm45720159777008))
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过调用`tf.config.threading.set_inter_op_parallelism_threads()`来控制inter-op线程池中的线程数。要设置intra-op线程数，请使用`tf.config.threading.set_intra_op_parallelism_threads()`。如果您不希望TensorFlow使用所有CPU核心，或者希望它是单线程的，这将非常有用。⁠^([12](ch19.html#idm45720159777008))
- en: 'With that, you have all you need to run any operation on any device, and exploit
    the power of your GPUs! Here are some of the things you could do:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，您就拥有了在任何设备上运行任何操作并利用GPU的能力所需的一切！以下是您可以做的一些事情：
- en: 'You could train several models in parallel, each on its own GPU: just write
    a training script for each model and run them in parallel, setting `CUDA_DEVICE_ORDER`
    and `CUDA_VISIBLE_DEVICES` so that each script only sees a single GPU device.
    This is great for hyperparameter tuning, as you can train in parallel multiple
    models with different hyperparameters. If you have a single machine with two GPUs,
    and it takes one hour to train one model on one GPU, then training two models
    in parallel, each on its own dedicated GPU, will take just one hour. Simple!'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以并行训练多个模型，每个模型都在自己的GPU上：只需为每个模型编写一个训练脚本，并在并行运行时设置`CUDA_DEVICE_ORDER`和`CUDA_VISIBLE_DEVICES`，以便每个脚本只能看到一个GPU设备。这对于超参数调整非常有用，因为您可以并行训练具有不同超参数的多个模型。如果您有一台具有两个GPU的单台机器，并且在一个GPU上训练一个模型需要一个小时，那么并行训练两个模型，每个模型都在自己专用的GPU上，只需要一个小时。简单！
- en: You could train a model on a single GPU and perform all the preprocessing in
    parallel on the CPU, using the dataset’s `prefetch()` method⁠^([13](ch19.html#idm45720159772080))
    to prepare the next few batches in advance so that they are ready when the GPU
    needs them (see [Chapter 13](ch13.html#data_chapter)).
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在单个GPU上训练一个模型，并在CPU上并行执行所有预处理操作，使用数据集的`prefetch()`方法提前准备好接下来的几批数据，以便在GPU需要时立即使用（参见第13章）。
- en: If your model takes two images as input and processes them using two CNNs before
    joining their outputs,⁠^([14](ch19.html#idm45720159768848)) then it will probably
    run much faster if you place each CNN on a different GPU.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的模型接受两个图像作为输入，并在使用两个CNN处理它们之前将它们连接起来，那么如果您将每个CNN放在不同的GPU上，它可能会运行得更快。
- en: 'You can create an efficient ensemble: just place a different trained model
    on each GPU so that you can get all the predictions much faster to produce the
    ensemble’s final prediction.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以创建一个高效的集成：只需在每个GPU上放置一个不同训练过的模型，这样您就可以更快地获得所有预测结果，以生成集成的最终预测。
- en: But what if you want to speed up training by using multiple GPUs?
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果您想通过使用多个GPU加速训练呢？
- en: Training Models Across Multiple Devices
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在多个设备上训练模型
- en: 'There are two main approaches to training a single model across multiple devices:
    *model parallelism*, where the model is split across the devices, and *data parallelism*,
    where the model is replicated across every device, and each replica is trained
    on a different subset of the data. Let’s look at these two options.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 训练单个模型跨多个设备有两种主要方法：*模型并行*，其中模型在设备之间分割，和*数据并行*，其中模型在每个设备上复制，并且每个副本在不同的数据子集上进行训练。让我们看看这两种选择。
- en: Model Parallelism
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型并行
- en: So far we have trained each neural network on a single device. What if we want
    to train a single neural network across multiple devices? This requires chopping
    the model into separate chunks and running each chunk on a different device. Unfortunately,
    such model parallelism turns out to be pretty tricky, and its effectiveness really
    depends on the architecture of your neural network. For fully connected networks,
    there is generally not much to be gained from this approach (see [Figure 19-11](#split_fully_connected_diagram)).
    Intuitively, it may seem that an easy way to split the model is to place each
    layer on a different device, but this does not work because each layer needs to
    wait for the output of the previous layer before it can do anything. So perhaps
    you can slice it vertically—for example, with the left half of each layer on one
    device, and the right part on another device? This is slightly better, since both
    halves of each layer can indeed work in parallel, but the problem is that each
    half of the next layer requires the output of both halves, so there will be a
    lot of cross-device communication (represented by the dashed arrows). This is
    likely to completely cancel out the benefit of the parallel computation, since
    cross-device communication is slow (and even more so when the devices are located
    on different machines).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在单个设备上训练了每个神经网络。如果我们想要在多个设备上训练单个神经网络怎么办？这需要将模型分割成单独的块，并在不同的设备上运行每个块。不幸的是，这种模型并行化实际上非常棘手，其有效性确实取决于神经网络的架构。对于全连接网络，从这种方法中通常无法获得太多好处。直觉上，似乎将模型分割的一种简单方法是将每一层放在不同的设备上，但这并不起作用，因为每一层都需要等待前一层的输出才能执行任何操作。也许你可以垂直切割它——例如，将每一层的左半部分放在一个设备上，右半部分放在另一个设备上？这样稍微好一些，因为每一层的两半确实可以并行工作，但问题在于下一层的每一半都需要上一层两半的输出，因此会有大量的跨设备通信（由虚线箭头表示）。这很可能会完全抵消并行计算的好处，因为跨设备通信速度很慢（当设备位于不同的机器上时更是如此）。
- en: '![mls3 1911](assets/mls3_1911.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1911](assets/mls3_1911.png)'
- en: Figure 19-11\. Splitting a fully connected neural network
  id: totrans-238
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-11。拆分完全连接的神经网络
- en: Some neural network architectures, such as convolutional neural networks (see
    [Chapter 14](ch14.html#cnn_chapter)), contain layers that are only partially connected
    to the lower layers, so it is much easier to distribute chunks across devices
    in an efficient way ([Figure 19-12](#split_partially_connected_diagram)).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一些神经网络架构，如卷积神经网络（参见[第14章](ch14.html#cnn_chapter)），包含仅部分连接到较低层的层，因此更容易以有效的方式在设备之间分发块（参见[图19-12](#split_partially_connected_diagram)）。
- en: '![mls3 1912](assets/mls3_1912.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1912](assets/mls3_1912.png)'
- en: Figure 19-12\. Splitting a partially connected neural network
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-12。拆分部分连接的神经网络
- en: Deep recurrent neural networks (see [Chapter 15](ch15.html#rnn_chapter)) can
    be split a bit more efficiently across multiple GPUs. If you split the network
    horizontally by placing each layer on a different device, and feed the network
    with an input sequence to process, then at the first time step only one device
    will be active (working on the sequence’s first value), at the second step two
    will be active (the second layer will be handling the output of the first layer
    for the first value, while the first layer will be handling the second value),
    and by the time the signal propagates to the output layer, all devices will be
    active simultaneously ([Figure 19-13](#split_rnn_network_diagram)). There is still
    a lot of cross-device communication going on, but since each cell may be fairly
    complex, the benefit of running multiple cells in parallel may (in theory) outweigh
    the communication penalty. However, in practice a regular stack of `LSTM` layers
    running on a single GPU actually runs much faster.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 深度递归神经网络（参见[第15章](ch15.html#rnn_chapter)）可以更有效地跨多个GPU进行分割。如果将网络水平分割，将每一层放在不同的设备上，并将输入序列输入网络进行处理，那么在第一个时间步中只有一个设备会处于活动状态（处理序列的第一个值），在第二个时间步中两个设备会处于活动状态（第二层将处理第一层的输出值，而第一层将处理第二个值），当信号传播到输出层时，所有设备将同时处于活动状态（[图19-13](#split_rnn_network_diagram)）。尽管设备之间仍然存在大量的跨设备通信，但由于每个单元可能相当复杂，理论上并行运行多个单元的好处可能会超过通信惩罚。然而，在实践中，在单个GPU上运行的常规堆叠`LSTM`层实际上运行得更快。
- en: '![mls3 1913](assets/mls3_1913.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1913](assets/mls3_1913.png)'
- en: Figure 19-13\. Splitting a deep recurrent neural network
  id: totrans-244
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-13。拆分深度递归神经网络
- en: 'In short, model parallelism may speed up running or training some types of
    neural networks, but not all, and it requires special care and tuning, such as
    making sure that devices that need to communicate the most run on the same machine.⁠^([15](ch19.html#idm45720159738736))
    Next we’ll look at a much simpler and generally more efficient option: data parallelism.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，模型并行可能会加快某些类型的神经网络的运行或训练速度，但并非所有类型的神经网络都适用，并且需要特别注意和调整，例如确保需要进行通信的设备在同一台机器上运行。接下来我们将看一个更简单且通常更有效的选择：数据并行。
- en: Data Parallelism
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据并行
- en: Another way to parallelize the training of a neural network is to replicate
    it on every device and run each training step simultaneously on all replicas,
    using a different mini-batch for each. The gradients computed by each replica
    are then averaged, and the result is used to update the model parameters. This
    is called *data parallelism*, or sometimes *single program, multiple data* (SPMD).
    There are many variants of this idea, so let’s look at the most important ones.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种并行训练神经网络的方法是在每个设备上复制它，并在所有副本上同时运行每个训练步骤，为每个副本使用不同的小批量。然后对每个副本计算的梯度进行平均，并将结果用于更新模型参数。这被称为*数据并行*，有时也称为*单程序，多数据*（SPMD）。这个想法有许多变体，让我们看看最重要的几种。
- en: Data parallelism using the mirrored strategy
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用镜像策略的数据并行
- en: Arguably the simplest approach is to completely mirror all the model parameters
    across all the GPUs and always apply the exact same parameter updates on every
    GPU. This way, all replicas always remain perfectly identical. This is called
    the *mirrored strategy*, and it turns out to be quite efficient, especially when
    using a single machine (see [Figure 19-14](#mirrored_strategy_diagram)).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，最简单的方法是在所有GPU上完全镜像所有模型参数，并始终在每个GPU上应用完全相同的参数更新。这样，所有副本始终保持完全相同。这被称为*镜像策略*，在使用单台机器时特别高效（参见[图19-14](#mirrored_strategy_diagram)）。
- en: '![mls3 1914](assets/mls3_1914.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1914](assets/mls3_1914.png)'
- en: Figure 19-14\. Data parallelism using the mirrored strategy
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-14. 使用镜像策略的数据并行
- en: The tricky part when using this approach is to efficiently compute the mean
    of all the gradients from all the GPUs and distribute the result across all the
    GPUs. This can be done using an *AllReduce* algorithm, a class of algorithms where
    multiple nodes collaborate to efficiently perform a *reduce operation* (such as
    computing the mean, sum, and max), while ensuring that all nodes obtain the same
    final result. Fortunately, there are off-the-shelf implementations of such algorithms,
    as you will see.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方法的棘手部分是高效地计算所有GPU的所有梯度的平均值，并将结果分布到所有GPU上。这可以使用*AllReduce*算法来完成，这是一类算法，多个节点合作以高效地执行*reduce操作*（例如计算平均值、总和和最大值），同时确保所有节点获得相同的最终结果。幸运的是，有现成的实现这种算法，您将会看到。
- en: Data parallelism with centralized parameters
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 集中式参数的数据并行
- en: Another approach is to store the model parameters outside of the GPU devices
    performing the computations (called *workers*); for example, on the CPU (see [Figure 19-15](#data_parallelism_diagram)).
    In a distributed setup, you may place all the parameters on one or more CPU-only
    servers called *parameter servers*, whose only role is to host and update the
    parameters.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是将模型参数存储在执行计算的GPU设备之外（称为*工作器*）；例如，在CPU上（参见[图19-15](#data_parallelism_diagram)）。在分布式设置中，您可以将所有参数放在一个或多个仅称为*参数服务器*的CPU服务器上，其唯一作用是托管和更新参数。
- en: '![mls3 1915](assets/mls3_1915.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1915](assets/mls3_1915.png)'
- en: Figure 19-15\. Data parallelism with centralized parameters
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-15. 集中式参数的数据并行
- en: Whereas the mirrored strategy imposes synchronous weight updates across all
    GPUs, this centralized approach allows either synchronous or asynchronous updates.
    Let’s take a look at the pros and cons of both options.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 镜像策略强制所有GPU上的权重更新同步进行，而这种集中式方法允许同步或异步更新。让我们来看看这两种选择的优缺点。
- en: Synchronous updates
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 同步更新
- en: With *synchronous updates*, the aggregator waits until all gradients are available
    before it computes the average gradients and passes them to the optimizer, which
    will update the model parameters. Once a replica has finished computing its gradients,
    it must wait for the parameters to be updated before it can proceed to the next
    mini-batch. The downside is that some devices may be slower than others, so the
    fast devices will have to wait for the slow ones at every step, making the whole
    process as slow as the slowest device. Moreover, the parameters will be copied
    to every device almost at the same time (immediately after the gradients are applied),
    which may saturate the parameter servers’ bandwidth.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在*同步更新*中，聚合器会等待所有梯度可用后再计算平均梯度并将其传递给优化器，优化器将更新模型参数。一旦一个副本完成计算其梯度，它必须等待参数更新后才能继续下一个小批量。缺点是一些设备可能比其他设备慢，因此快速设备将不得不在每一步等待慢速设备，使整个过程与最慢设备一样慢。此外，参数将几乎同时复制到每个设备上（在梯度应用后立即），这可能会饱和参数服务器的带宽。
- en: Tip
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: To reduce the waiting time at each step, you could ignore the gradients from
    the slowest few replicas (typically ~10%). For example, you could run 20 replicas,
    but only aggregate the gradients from the fastest 18 replicas at each step, and
    just ignore the gradients from the last 2\. As soon as the parameters are updated,
    the first 18 replicas can start working again immediately, without having to wait
    for the 2 slowest replicas. This setup is generally described as having 18 replicas
    plus 2 *spare replicas*.⁠^([16](ch19.html#idm45720159706016))
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少每个步骤的等待时间，您可以忽略最慢几个副本（通常约10%）的梯度。例如，您可以运行20个副本，但每个步骤只聚合来自最快的18个副本的梯度，并忽略最后2个的梯度。一旦参数更新，前18个副本可以立即开始工作，而无需等待最慢的2个副本。这种设置通常被描述为有18个副本加上2个*备用副本*。
- en: Asynchronous updates
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 异步更新
- en: With asynchronous updates, whenever a replica has finished computing the gradients,
    the gradients are immediately used to update the model parameters. There is no
    aggregation (it removes the “mean” step in [Figure 19-15](#data_parallelism_diagram))
    and no synchronization. Replicas work independently of the other replicas. Since
    there is no waiting for the other replicas, this approach runs more training steps
    per minute. Moreover, although the parameters still need to be copied to every
    device at every step, this happens at different times for each replica, so the
    risk of bandwidth saturation is reduced.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 使用异步更新时，每当一个副本完成梯度计算后，梯度立即用于更新模型参数。没有聚合（它删除了“均值”步骤在[图19-15](#data_parallelism_diagram)中）和没有同步。副本独立于其他副本工作。由于不需要等待其他副本，这种方法每分钟可以运行更多的训练步骤。此外，尽管参数仍然需要在每一步复制到每个设备，但对于每个副本，这发生在不同的时间，因此带宽饱和的风险降低了。
- en: 'Data parallelism with asynchronous updates is an attractive choice because
    of its simplicity, the absence of synchronization delay, and its better use of
    the bandwidth. However, although it works reasonably well in practice, it is almost
    surprising that it works at all! Indeed, by the time a replica has finished computing
    the gradients based on some parameter values, these parameters will have been
    updated several times by other replicas (on average *N* – 1 times, if there are
    *N* replicas), and there is no guarantee that the computed gradients will still
    be pointing in the right direction (see [Figure 19-16](#stale_gradients_diagram)).
    When gradients are severely out of date, they are called *stale gradients*: they
    can slow down convergence, introducing noise and wobble effects (the learning
    curve may contain temporary oscillations), or they can even make the training
    algorithm diverge.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 1916](assets/mls3_1916.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: Figure 19-16\. Stale gradients when using asynchronous updates
  id: totrans-266
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are a few ways you can reduce the effect of stale gradients:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以减少陈旧梯度的影响：
- en: Reduce the learning rate.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低学习率。
- en: Drop stale gradients or scale them down.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃陈旧的梯度或将其缩小。
- en: Adjust the mini-batch size.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整小批量大小。
- en: Start the first few epochs using just one replica (this is called the *warmup
    phase*). Stale gradients tend to be more damaging at the beginning of training,
    when gradients are typically large and the parameters have not settled into a
    valley of the cost function yet, so different replicas may push the parameters
    in quite different directions.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在开始的几个时期只使用一个副本（这被称为*热身阶段*）。在训练开始阶段，梯度通常很大，参数还没有稳定在成本函数的谷底，因此陈旧的梯度可能会造成更大的损害，不同的副本可能会将参数推向完全不同的方向。
- en: A [paper published by the Google Brain team](https://homl.info/68) in 2016⁠^([17](ch19.html#idm45720159687856))
    benchmarked various approaches and found that using synchronous updates with a
    few spare replicas was more efficient than using asynchronous updates, not only
    converging faster but also producing a better model. However, this is still an
    active area of research, so you should not rule out asynchronous updates just
    yet.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年，Google Brain团队发表的一篇论文对各种方法进行了基准测试，发现使用同步更新和一些备用副本比使用异步更新更有效，不仅收敛更快，而且产生了更好的模型。然而，这仍然是一个活跃的研究领域，所以你不应该立刻排除异步更新。
- en: Bandwidth saturation
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带宽饱和
- en: Whether you use synchronous or asynchronous updates, data parallelism with centralized
    parameters still requires communicating the model parameters from the parameter
    servers to every replica at the beginning of each training step, and the gradients
    in the other direction at the end of each training step. Similarly, when using
    the mirrored strategy, the gradients produced by each GPU will need to be shared
    with every other GPU. Unfortunately, there often comes a point where adding an
    extra GPU will not improve performance at all because the time spent moving the
    data into and out of GPU RAM (and across the network in a distributed setup) will
    outweigh the speedup obtained by splitting the computation load. At that point,
    adding more GPUs will just worsen the bandwidth saturation and actually slow down
    training.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您使用同步还是异步更新，具有集中参数的数据并行仍然需要在每个训练步骤开始时将模型参数从参数服务器传递到每个副本，并在每个训练步骤结束时将梯度传递到另一个方向。同样，当使用镜像策略时，每个GPU生成的梯度将需要与每个其他GPU共享。不幸的是，通常会出现这样一种情况，即添加额外的GPU将不会改善性能，因为将数据移入和移出GPU
    RAM（以及在分布式设置中跨网络）所花费的时间将超过通过分割计算负载获得的加速效果。在那一点上，添加更多的GPU将只会加剧带宽饱和，并实际上减慢训练速度。
- en: 'Saturation is more severe for large dense models, since they have a lot of
    parameters and gradients to transfer. It is less severe for small models (but
    the parallelization gain is limited) and for large sparse models, where the gradients
    are typically mostly zeros and so can be communicated efficiently. Jeff Dean,
    initiator and lead of the Google Brain project, [reported](https://homl.info/69)
    typical speedups of 25–40× when distributing computations across 50 GPUs for dense
    models, and a 300× speedup for sparser models trained across 500 GPUs. As you
    can see, sparse models really do scale better. Here are a few concrete examples:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 饱和对于大型密集模型来说更严重，因为它们有很多参数和梯度需要传输。对于小型模型来说，饱和程度较轻（但并行化增益有限），对于大型稀疏模型也较轻，因为梯度通常大部分为零，可以有效传输。Google
    Brain 项目的发起人和负责人 Jeff Dean [报告](https://homl.info/69) 在将计算分布到 50 个 GPU 上时，密集模型的典型加速为
    25-40 倍，而在 500 个 GPU 上训练稀疏模型时，加速为 300 倍。正如你所看到的，稀疏模型确实更好地扩展。以下是一些具体例子：
- en: 'Neural machine translation: 6× speedup on 8 GPUs'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经机器翻译：在 8 个 GPU 上加速 6 倍
- en: 'Inception/ImageNet: 32× speedup on 50 GPUs'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception/ImageNet：在 50 个 GPU 上加速 32 倍
- en: 'RankBrain: 300× speedup on 500 GPUs'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RankBrain：在 500 个 GPU 上加速 300 倍
- en: 'There is plenty of research going on to alleviate the bandwidth saturation
    issue, with the goal of allowing training to scale linearly with the number of
    GPUs available. For example, a [2018 paper](https://homl.info/pipedream)⁠^([18](ch19.html#idm45720159675552))
    by a team of researchers from Carnegie Mellon University, Stanford University,
    and Microsoft Research proposed a system called *PipeDream* that managed to reduce
    network communications by over 90%, making it possible to train large models across
    many machines. They achieved this using a new technique called *pipeline parallelism*,
    which combines model parallelism and data parallelism: the model is chopped into
    consecutive parts, called *stages*, each of which is trained on a different machine.
    This results in an asynchronous pipeline in which all machines work in parallel
    with very little idle time. During training, each stage alternates one round of
    forward propagation and one round of backpropagation (see [Figure 19-17](#pipedream_diagram)):
    it pulls a mini-batch from its input queue, processes it, and sends the outputs
    to the next stage’s input queue, then it pulls one mini-batch of gradients from
    its gradient queue, backpropagates these gradients and updates its own model parameters,
    and pushes the backpropagated gradients to the previous stage’s gradient queue.
    It then repeats the whole process again and again. Each stage can also use regular
    data parallelism (e.g., using the mirrored strategy), independently from the other
    stages.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多研究正在进行，以缓解带宽饱和问题，目标是使训练能够与可用的GPU数量成线性比例扩展。例如，卡内基梅隆大学、斯坦福大学和微软研究团队在2018年提出了一个名为*PipeDream*的系统，成功将网络通信减少了90%以上，使得可以在多台机器上训练大型模型成为可能。他们使用了一种称为*管道并行*的新技术来实现这一目标，该技术结合了模型并行和数据并行：模型被切分成连续的部分，称为*阶段*，每个阶段在不同的机器上进行训练。这导致了一个异步的管道，所有机器都在很少的空闲时间内并行工作。在训练过程中，每个阶段交替进行一轮前向传播和一轮反向传播：它从输入队列中提取一个小批量数据，处理它，并将输出发送到下一个阶段的输入队列，然后从梯度队列中提取一个小批量的梯度，反向传播这些梯度并更新自己的模型参数，并将反向传播的梯度推送到前一个阶段的梯度队列。然后它一遍又一遍地重复整个过程。每个阶段还可以独立地使用常规的数据并行（例如使用镜像策略），而不受其他阶段的影响。
- en: '![mls3 1917](assets/mls3_1917.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1917](assets/mls3_1917.png)'
- en: Figure 19-17\. PipeDream’s pipeline parallelism
  id: totrans-281
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-17。PipeDream的管道并行性
- en: 'However, as it’s presented here, PipeDream would not work so well. To understand
    why, consider mini-batch #5 in [Figure 19-17](#pipedream_diagram): when it went
    through stage 1 during the forward pass, the gradients from mini-batch #4 had
    not yet been backpropagated through that stage, but by the time #5’s gradients
    flow back to stage 1, #4’s gradients will have been used to update the model parameters,
    so #5’s gradients will be a bit stale. As we have seen, this can degrade training
    speed and accuracy, and even make it diverge: the more stages there are, the worse
    this problem becomes. The paper’s authors proposed methods to mitigate this issue,
    though: for example, each stage saves weights during forward propagation and restores
    them during backpropagation, to ensure that the same weights are used for both
    the forward pass and the backward pass. This is called *weight stashing*. Thanks
    to this, PipeDream demonstrates impressive scaling capability, well beyond simple
    data parallelism.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如在这里展示的那样，PipeDream 不会工作得那么好。要理解原因，考虑在 [Figure 19-17](#pipedream_diagram)
    中的第 5 个小批次：当它在前向传递过程中经过第 1 阶段时，来自第 4 个小批次的梯度尚未通过该阶段进行反向传播，但是当第 5 个小批次的梯度流回到第 1
    阶段时，第 4 个小批次的梯度将已经被用来更新模型参数，因此第 5 个小批次的梯度将有点过时。正如我们所看到的，这可能会降低训练速度和准确性，甚至使其发散：阶段越多，这个问题就会变得越糟糕。论文的作者提出了缓解这个问题的方法：例如，每个阶段在前向传播过程中保存权重，并在反向传播过程中恢复它们，以确保相同的权重用于前向传递和反向传递。这被称为*权重存储*。由于这一点，PipeDream
    展示了令人印象深刻的扩展能力，远远超出了简单的数据并行性。
- en: 'The latest breakthrough in this field of research was published in a [2022
    paper](https://homl.info/pathways)⁠^([19](ch19.html#idm45720159666736)) by Google
    researchers: they developed a system called *Pathways* that uses automated model
    parallelism, asynchronous gang scheduling, and other techniques to reach close
    to 100% hardware utilization across thousands of TPUs! *Scheduling* means organizing
    when and where each task must run, and *gang scheduling* means running related
    tasks at the same time in parallel and close to each other to reduce the time
    tasks have to wait for the others’ outputs. As we saw in [Chapter 16](ch16.html#nlp_chapter),
    this system was used to train a massive language model across over 6,000 TPUs,
    with close to 100% hardware utilization: that’s a mindblowing engineering feat.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这个研究领域的最新突破是由谷歌研究人员在一篇[2022年的论文](https://homl.info/pathways)中发表的：他们开发了一个名为*Pathways*的系统，利用自动模型并行、异步团队调度等技术，实现了数千个TPU几乎100%的硬件利用率！*调度*意味着组织每个任务必须运行的时间和位置，*团队调度*意味着同时并行运行相关任务，并且彼此靠近，以减少任务等待其他任务输出的时间。正如我们在[第16章](ch16.html#nlp_chapter)中看到的，这个系统被用来在超过6,000个TPU上训练一个庞大的语言模型，几乎实现了100%的硬件利用率：这是一个令人惊叹的工程壮举。
- en: 'At the time of writing, Pathways is not public yet, but it’s likely that in
    the near future you will be able to train huge models on Vertex AI using Pathways
    or a similar system. In the meantime, to reduce the saturation problem, you’ll
    probably want to use a few powerful GPUs rather than plenty of weak GPUs, and
    if you need to train a model across multiple servers, you should group your GPUs
    on few and very well interconnected servers. You can also try dropping the float
    precision from 32 bits (`tf.float32`) to 16 bits (`tf.bfloat16`). This will cut
    in half the amount of data to transfer, often without much impact on the convergence
    rate or the model’s performance. Lastly, if you are using centralized parameters,
    you can shard (split) the parameters across multiple parameter servers: adding
    more parameter servers will reduce the network load on each server and limit the
    risk of bandwidth saturation.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Pathways尚未公开，但很可能在不久的将来，您将能够使用Pathways或类似系统在Vertex AI上训练大型模型。与此同时，为了减少饱和问题，您可能会希望使用一些强大的GPU，而不是大量的弱GPU，如果您需要在多台服务器上训练模型，您应该将GPU分组在少数且连接非常良好的服务器上。您还可以尝试将浮点精度从32位（`tf.float32`）降低到16位（`tf.bfloat16`）。这将减少一半的数据传输量，通常不会对收敛速度或模型性能产生太大影响。最后，如果您正在使用集中式参数，您可以将参数分片（分割）到多个参数服务器上：增加更多的参数服务器将减少每个服务器上的网络负载，并限制带宽饱和的风险。
- en: OK, now that we’ve gone through all the theory, let’s actually train a model
    across multiple GPUs!
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们已经讨论了所有的理论，让我们实际在多个GPU上训练一个模型！
- en: Training at Scale Using the Distribution Strategies API
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用分布策略API进行规模训练
- en: 'Luckily, TensorFlow comes with a very nice API that takes care of all the complexity
    of distributing your model across multiple devices and machines: the *distribution
    strategies API*. To train a Keras model across all available GPUs (on a single
    machine, for now) using data parallelism with the mirrored strategy, just create
    a `MirroredStrategy` object, call its `scope()` method to get a distribution context,
    and wrap the creation and compilation of your model inside that context. Then
    call the model’s `fit()` method normally:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，TensorFlow带有一个非常好的API，它负责处理将模型分布在多个设备和机器上的所有复杂性：*分布策略API*。要在所有可用的GPU上（暂时只在单台机器上）使用数据并行性和镜像策略训练一个Keras模型，只需创建一个`MirroredStrategy`对象，调用它的`scope()`方法以获取一个分布上下文，并将模型的创建和编译包装在该上下文中。然后正常调用模型的`fit()`方法：
- en: '[PRE42]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Under the hood, Keras is distribution-aware, so in this `MirroredStrategy`
    context it knows that it must replicate all variables and operations across all
    available GPU devices. If you look at the model’s weights, they are of type `MirroredVariable`:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，Keras是分布感知的，因此在这个`MirroredStrategy`上下文中，它知道必须在所有可用的GPU设备上复制所有变量和操作。如果你查看模型的权重，它们是`MirroredVariable`类型的：
- en: '[PRE43]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note that the `fit()` method will automatically split each training batch across
    all the replicas, so it’s preferable to ensure that the batch size is divisible
    by the number of replicas (i.e., the number of available GPUs) so that all replicas
    get batches of the same size. And that’s all! Training will generally be significantly
    faster than using a single device, and the code change was really minimal.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`fit()` 方法会自动将每个训练批次在所有副本之间进行分割，因此最好确保批次大小可以被副本数量（即可用的 GPU 数量）整除，以便所有副本获得相同大小的批次。就是这样！训练通常会比使用单个设备快得多，而且代码更改确实很小。
- en: 'Once you have finished training your model, you can use it to make predictions
    efficiently: call the `predict()` method, and it will automatically split the
    batch across all replicas, making predictions in parallel. Again, the batch size
    must be divisible by the number of replicas. If you call the model’s `save()`
    method, it will be saved as a regular model, *not* as a mirrored model with multiple
    replicas. So when you load it, it will run like a regular model, on a single device:
    by default on GPU #0, or on the CPU if there are no GPUs. If you want to load
    a model and run it on all available devices, you must call `tf.keras.models.load_model()`
    within a distribution context:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型完成后，您可以使用它高效地进行预测：调用`predict()`方法，它会自动将批处理在所有副本之间分割，以并行方式进行预测。再次强调，批处理大小必须能够被副本数量整除。如果调用模型的`save()`方法，它将被保存为常规模型，*而不是*具有多个副本的镜像模型。因此，当您加载它时，它将像常规模型一样运行，在单个设备上：默认情况下在GPU＃0上，如果没有GPU则在CPU上。如果您想加载一个模型并在所有可用设备上运行它，您必须在分发上下文中调用`tf.keras.models.load_model()`：
- en: '[PRE44]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'If you only want to use a subset of all the available GPU devices, you can
    pass the list to the `MirroredStrategy`’s constructor:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只想使用所有可用GPU设备的子集，您可以将列表传递给`MirroredStrategy`的构造函数：
- en: '[PRE45]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: By default, the `MirroredStrategy` class uses the *NVIDIA Collective Communications
    Library* (NCCL) for the AllReduce mean operation, but you can change it by setting
    the `cross_device_ops` argument to an instance of the `tf.distribute.HierarchicalCopyAllReduce`
    class, or an instance of the `tf.distribute.ReductionToOneDevice` class. The default
    NCCL option is based on the `tf.distribute.NcclAllReduce` class, which is usually
    faster, but this depends on the number and types of GPUs, so you may want to give
    the alternatives a try.⁠^([20](ch19.html#idm45720159438096))
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`MirroredStrategy`类使用*NVIDIA Collective Communications Library*（NCCL）进行AllReduce均值操作，但您可以通过将`cross_device_ops`参数设置为`tf.distribute.HierarchicalCopyAllReduce`类的实例或`tf.distribute.ReductionToOneDevice`类的实例来更改它。默认的NCCL选项基于`tf.distribute.NcclAllReduce`类，通常更快，但这取决于GPU的数量和类型，因此您可能想尝试一下其他选择。
- en: 'If you want to try using data parallelism with centralized parameters, replace
    the `MirroredStrategy` with the `CentralStorageStrategy`:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想尝试使用集中式参数的数据并行性，请将`MirroredStrategy`替换为`CentralStorageStrategy`：
- en: '[PRE46]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: You can optionally set the `compute_devices` argument to specify the list of
    devices you want to use as workers—by default it will use all available GPUs—and
    you can optionally set the `parameter_device` argument to specify the device you
    want to store the parameters on. By default it will use the CPU, or the GPU if
    there is just one.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择设置`compute_devices`参数来指定要用作工作器的设备列表-默认情况下将使用所有可用的GPU-您还可以选择设置`parameter_device`参数来指定要存储参数的设备。默认情况下将使用CPU，或者如果只有一个GPU，则使用GPU。
- en: Now let’s see how to train a model across a cluster of TensorFlow servers!
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何在一组TensorFlow服务器上训练模型！
- en: Training a Model on a TensorFlow Cluster
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在TensorFlow集群上训练模型
- en: 'A *TensorFlow cluster* is a group of TensorFlow processes running in parallel,
    usually on different machines, and talking to each other to complete some work—for
    example, training or executing a neural network model. Each TF process in the
    cluster is called a *task*, or a *TF server*. It has an IP address, a port, and
    a type (also called its *role* or its *job*). The type can be either `"worker"`,
    `"chief"`, `"ps"` (parameter server), or `"evaluator"`:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '*TensorFlow集群*是一组在并行运行的TensorFlow进程，通常在不同的机器上，并相互通信以完成一些工作，例如训练或执行神经网络模型。集群中的每个TF进程被称为*任务*或*TF服务器*。它有一个IP地址，一个端口和一个类型（也称为*角色*或*工作*）。类型可以是`"worker"`、`"chief"`、`"ps"`（参数服务器）或`"evaluator"`：'
- en: Each *worker* performs computations, usually on a machine with one or more GPUs.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个*worker*执行计算，通常在一台或多台GPU的机器上。
- en: The *chief* performs computations as well (it is a worker), but it also handles
    extra work such as writing TensorBoard logs or saving checkpoints. There is a
    single chief in a cluster. If no chief is specified explicitly, then by convention
    the first worker is the chief.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首席执行计算任务（它是一个工作者），但也处理额外的工作，比如编写TensorBoard日志或保存检查点。集群中只有一个首席。如果没有明确指定首席，则按照惯例第一个工作者就是首席。
- en: A *parameter server* only keeps track of variable values, and it is usually
    on a CPU-only machine. This type of task is only used with the `ParameterServerStrategy`.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数服务器只跟踪变量值，并且通常在仅有CPU的机器上。这种类型的任务只能与`ParameterServerStrategy`一起使用。
- en: An *evaluator* obviously takes care of evaluation. This type is not used often,
    and when it’s used, there’s usually just one evaluator.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估者显然负责评估。这种类型并不经常使用，当使用时，通常只有一个评估者。
- en: 'To start a TensorFlow cluster, you must first define its specification. This
    means defining each task’s IP address, TCP port, and type. For example, the following
    *cluster specification* defines a cluster with three tasks (two workers and one
    parameter server; see [Figure 19-18](#cluster_diagram)). The cluster spec is a
    dictionary with one key per job, and the values are lists of task addresses (*IP*:*port*):'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动一个TensorFlow集群，必须首先定义其规范。这意味着定义每个任务的IP地址、TCP端口和类型。例如，以下*集群规范*定义了一个有三个任务的集群（两个工作者和一个参数服务器；参见[图19-18](#cluster_diagram)）。集群规范是一个字典，每个作业对应一个键，值是任务地址（*IP*:*port*）的列表：
- en: '[PRE47]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In general there will be a single task per machine, but as this example shows,
    you can configure multiple tasks on the same machine if you want. In this case,
    if they share the same GPUs, make sure the RAM is split appropriately, as discussed
    earlier.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 通常每台机器上会有一个任务，但正如这个示例所示，如果需要，您可以在同一台机器上配置多个任务。在这种情况下，如果它们共享相同的GPU，请确保RAM适当分配，如前面讨论的那样。
- en: Warning
  id: totrans-310
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: By default, every task in the cluster may communicate with every other task,
    so make sure to configure your firewall to authorize all communications between
    these machines on these ports (it’s usually simpler if you use the same port on
    every machine).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，集群中的每个任务可以与其他任务通信，因此请确保配置防火墙以授权这些机器之间这些端口上的所有通信（如果每台机器使用相同的端口，则通常更简单）。
- en: '![mls3 1918](assets/mls3_1918.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1918](assets/mls3_1918.png)'
- en: Figure 19-18\. An example TensorFlow cluster
  id: totrans-313
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图19-18。一个示例TensorFlow集群
- en: 'When you start a task, you must give it the cluster spec, and you must also
    tell it what its type and index are (e.g., worker #0). The simplest way to specify
    everything at once (both the cluster spec and the current task’s type and index)
    is to set the `TF_CONFIG` environment variable before starting TensorFlow. It
    must be a JSON-encoded dictionary containing a cluster specification (under the
    `"cluster"` key) and the type and index of the current task (under the `"task"`
    key). For example, the following `TF_CONFIG` environment variable uses the cluster
    we just defined and specifies that the task to start is worker #0:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '当您开始一个任务时，您必须给它指定集群规范，并且还必须告诉它它的类型和索引是什么（例如，worker #0）。一次性指定所有内容的最简单方法（包括集群规范和当前任务的类型和索引）是在启动TensorFlow之前设置`TF_CONFIG`环境变量。它必须是一个JSON编码的字典，包含集群规范（在`"cluster"`键下）和当前任务的类型和索引（在`"task"`键下）。例如，以下`TF_CONFIG`环境变量使用我们刚刚定义的集群，并指定要启动的任务是worker
    #0：'
- en: '[PRE48]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Tip
  id: totrans-316
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: In general you want to define the `TF_CONFIG` environment variable outside of
    Python, so the code does not need to include the current task’s type and index
    (this makes it possible to use the same code across all workers).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 通常您希望在Python之外定义`TF_CONFIG`环境变量，这样代码就不需要包含当前任务的类型和索引（这样可以在所有工作节点上使用相同的代码）。
- en: 'Now let’s train a model on a cluster! We will start with the mirrored strategy.
    First, you need to set the `TF_CONFIG` environment variable appropriately for
    each task. There should be no parameter server (remove the `"ps"` key in the cluster
    spec), and in general you will want a single worker per machine. Make extra sure
    you set a different task index for each task. Finally, run the following script
    on every worker:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在集群上训练一个模型！我们将从镜像策略开始。首先，您需要为每个任务适当设置`TF_CONFIG`环境变量。集群规范中不应该有参数服务器（删除集群规范中的`"ps"`键），通常每台机器上只需要一个工作节点。确保为每个任务设置不同的任务索引。最后，在每个工作节点上运行以下脚本：
- en: '[PRE49]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: That’s almost the same code you used earlier, except this time you are using
    the `MultiWorkerMirroredStrategy`. When you start this script on the first workers,
    they will remain blocked at the AllReduce step, but training will begin as soon
    as the last worker starts up, and you will see them all advancing at exactly the
    same rate since they synchronize at each step.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎是您之前使用的相同代码，只是这次您正在使用`MultiWorkerMirroredStrategy`。当您在第一个工作节点上启动此脚本时，它们将在AllReduce步骤处保持阻塞，但是一旦最后一个工作节点启动，训练将开始，并且您将看到它们以完全相同的速度前进，因为它们在每一步都进行同步。
- en: Warning
  id: totrans-321
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When using the `MultiWorkerMirroredStrategy`, it’s important to ensure that
    all workers do the same thing, including saving model checkpoints or writing TensorBoard
    logs, even though you will only keep what the chief writes. This is because these
    operations may need to run the AllReduce operations, so all workers must be in
    sync.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用`MultiWorkerMirroredStrategy`时，重要的是确保所有工作人员做同样的事情，包括保存模型检查点或编写TensorBoard日志，即使您只保留主要写入的内容。这是因为这些操作可能需要运行AllReduce操作，因此所有工作人员必须保持同步。
- en: 'There are two AllReduce implementations for this distribution strategy: a ring
    AllReduce algorithm based on gRPC for the network communications, and NCCL’s implementation.
    The best algorithm to use depends on the number of workers, the number and types
    of GPUs, and the network. By default, TensorFlow will apply some heuristics to
    select the right algorithm for you, but you can force NCCL (or RING) like this:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分发策略有两种AllReduce实现方式：基于gRPC的环形AllReduce算法用于网络通信，以及NCCL的实现。要使用哪种最佳算法取决于工作人员数量、GPU数量和类型，以及网络情况。默认情况下，TensorFlow会应用一些启发式方法为您选择合适的算法，但您可以强制使用NCCL（或RING）如下：
- en: '[PRE50]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: If you prefer to implement asynchronous data parallelism with parameter servers,
    change the strategy to `ParameterServerStrategy`, add one or more parameter servers,
    and configure `TF_CONFIG` appropriately for each task. Note that although the
    workers will work asynchronously, the replicas on each worker will work synchronously.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望使用参数服务器实现异步数据并行处理，请将策略更改为`ParameterServerStrategy`，添加一个或多个参数服务器，并为每个任务适当配置`TF_CONFIG`。请注意，虽然工作人员将异步工作，但每个工作人员上的副本将同步工作。
- en: 'Lastly, if you have access to [TPUs on Google Cloud](https://cloud.google.com/tpu)—for
    example, if you use Colab and you set the accelerator type to TPU—then you can
    create a `TPUStrategy` like this:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您可以访问[Google Cloud上的TPU](https://cloud.google.com/tpu)——例如，如果您在Colab中设置加速器类型为TPU——那么您可以像这样创建一个`TPUStrategy`：
- en: '[PRE51]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This needs to be run right after importing TensorFlow. You can then use this
    strategy normally.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要在导入TensorFlow后立即运行。然后您可以正常使用这个策略。
- en: Tip
  id: totrans-329
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: If you are a researcher, you may be eligible to use TPUs for free; see [*https://tensorflow.org/tfrc*](https://tensorflow.org/tfrc)
    for more details.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您是研究人员，您可能有资格免费使用TPU；请查看[*https://tensorflow.org/tfrc*](https://tensorflow.org/tfrc)获取更多详细信息。
- en: 'You can now train models across multiple GPUs and multiple servers: give yourself
    a pat on the back! If you want to train a very large model, however, you will
    need many GPUs, across many servers, which will require either buying a lot of
    hardware or managing a lot of cloud virtual machines. In many cases, it’s less
    hassle and less expensive to use a cloud service that takes care of provisioning
    and managing all this infrastructure for you, just when you need it. Let’s see
    how to do that using Vertex AI.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以跨多个GPU和多个服务器训练模型：给自己一个鼓励！然而，如果您想训练一个非常大的模型，您将需要许多GPU，跨多个服务器，这将要求要么购买大量硬件，要么管理大量云虚拟机。在许多情况下，使用一个云服务来为您提供所有这些基础设施的配置和管理会更方便、更经济，只有在您需要时才会提供。让我们看看如何使用Vertex
    AI来实现这一点。
- en: Running Large Training Jobs on Vertex AI
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Vertex AI上运行大型训练作业
- en: 'Vertex AI allows you to create custom training jobs with your own training
    code. In fact, you can use almost the same training code as you would use on your
    own TF cluster. The main thing you must change is where the chief should save
    the model, the checkpoints, and the TensorBoard logs. Instead of saving the model
    to a local directory, the chief must save it to GCS, using the path provided by
    Vertex AI in the `AIP_MODEL_DIR` environment variable. For the model checkpoints
    and TensorBoard logs, you should use the paths contained in the `AIP_CHECKPOINT_DIR`
    and `AIP_TENSORBOARD_LOG_DIR` environment variables, respectively. Of course,
    you must also make sure that the training data can be accessed from the virtual
    machines, such as on GCS, or another GCP service like BigQuery, or directly from
    the web. Lastly, Vertex AI sets the `"chief"` task type explicitly, so you should
    identify the chief using `resolved.task_type == "chief"` instead of `resolved.task_id
    == 0`:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI允许您使用自己的训练代码创建自定义训练作业。实际上，您可以几乎使用与在自己的TF集群上使用的相同的训练代码。您必须更改的主要内容是首席应该保存模型、检查点和TensorBoard日志的位置。首席必须将模型保存到GCS，使用Vertex
    AI在`AIP_MODEL_DIR`环境变量中提供的路径，而不是将模型保存到本地目录。对于模型检查点和TensorBoard日志，您应该分别使用`AIP_CHECKPOINT_DIR`和`AIP_TENSORBOARD_LOG_DIR`环境变量中包含的路径。当然，您还必须确保训练数据可以从虚拟机访问，例如在GCS上，或者从另一个GCP服务（如BigQuery）或直接从网络上访问。最后，Vertex
    AI明确设置了`"chief"`任务类型，因此您应该使用`resolved.task_type == "chief"`来识别首席，而不是使用`resolved.task_id
    == 0`：
- en: '[PRE52]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Tip
  id: totrans-335
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'If you place the training data on GCS, you can create a `tf.data.TextLineDataset`
    or `tf.data.TFRecordDataset` to access it: just use the GCS paths as the filenames
    (e.g., *gs://my_bucket/data/001.csv*). These datasets rely on the `tf.io.gfile`
    package to access files: it supports both local files and GCS files.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将训练数据放在GCS上，您可以创建一个`tf.data.TextLineDataset`或`tf.data.TFRecordDataset`来访问它：只需将GCS路径作为文件名（例如，*gs://my_bucket/data/001.csv*）。这些数据集依赖于`tf.io.gfile`包来访问文件：它支持本地文件和GCS文件。
- en: 'Now you can create a custom training job on Vertex AI, based on this script.
    You’ll need to specify the job name, the path to your training script, the Docker
    image to use for training, the one to use for predictions (after training), any
    additional Python libraries you may need, and lastly the bucket that Vertex AI
    should use as a staging directory to store the training script. By default, that’s
    also where the training script will save the trained model, as well as the TensorBoard
    logs and model checkpoints (if any). Let’s create the job:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以在Vertex AI上基于这个脚本创建一个自定义训练作业。您需要指定作业名称、训练脚本的路径、用于训练的Docker镜像、用于预测的镜像（训练后）、您可能需要的任何其他Python库，以及最后Vertex
    AI应该使用作为存储训练脚本的暂存目录的存储桶。默认情况下，这也是训练脚本将保存训练模型、TensorBoard日志和模型检查点（如果有的话）的地方。让我们创建这个作业：
- en: '[PRE53]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'And now let’s run it on two workers, each with two GPUs:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在两个拥有两个GPU的工作节点上运行它：
- en: '[PRE54]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'And that’s it: Vertex AI will provision the compute nodes you requested (within
    your quotas), and it will run your training script across them. Once the job is
    complete, the `run()` method will return a trained model that you can use exactly
    like the one you created earlier: you can deploy it to an endpoint, or use it
    to make batch predictions. If anything goes wrong during training, you can view
    the logs in the GCP console: in the ☰ navigation menu, select Vertex AI → Training,
    click on your training job, and click VIEW LOGS. Alternatively, you can click
    the CUSTOM JOBS tab and copy the job’s ID (e.g., 1234), then select Logging from
    the ☰ navigation menu and query `resource.labels.job_id=1234`.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部内容：Vertex AI将为您请求的计算节点进行配置（在您的配额范围内），并在这些节点上运行您的训练脚本。一旦作业完成，`run()`方法将返回一个经过训练的模型，您可以像之前创建的那样使用它：您可以部署到端点，或者用它进行批量预测。如果在训练过程中出现任何问题，您可以在GCP控制台中查看日志：在☰导航菜单中，选择Vertex
    AI → 训练，点击您的训练作业，然后点击查看日志。或者，您可以点击自定义作业选项卡，复制作业的ID（例如，1234），然后从☰导航菜单中选择日志记录，并查询`resource.labels.job_id=1234`。
- en: Tip
  id: totrans-342
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: To visualize the training progress, just start TensorBoard and point its `--logdir`
    to the GCS path of the logs. It will use *application default credentials*, which
    you can set up using `gcloud auth application-default login`. Vertex AI also offers
    hosted TensorBoard servers if you prefer.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化训练进度，只需启动TensorBoard，并将其`--logdir`指向日志的GCS路径。它将使用*应用程序默认凭据*，您可以使用`gcloud
    auth application-default login`进行设置。如果您喜欢，Vertex AI还提供托管的TensorBoard服务器。
- en: If you want to try out a few hyperparameter values, one option is to run multiple
    jobs. You can pass the hyperparameter values to your script as command-line arguments
    by setting the `args` parameter when calling the `run()` method, or you can pass
    them as environment variables using the `environment_variables` parameter.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想尝试一些超参数值，一个选项是运行多个作业。您可以通过在调用`run()`方法时设置`args`参数将超参数值作为命令行参数传递给您的脚本，或者您可以使用`environment_variables`参数将它们作为环境变量传递。
- en: However, if you want to run a large hyperparameter tuning job on the cloud,
    a much better option is to use Vertex AI’s hyperparameter tuning service. Let’s
    see how.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您想在云上运行一个大型的超参数调整作业，一个更好的选择是使用Vertex AI的超参数调整服务。让我们看看如何做。
- en: Hyperparameter Tuning on Vertex AI
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Vertex AI上的超参数调整
- en: 'Vertex AI’s hyperparameter tuning service is based on a Bayesian optimization
    algorithm, capable of quickly finding optimal combinations of hyperparameters.
    To use it, you first need to create a training script that accepts hyperparameter
    values as command-line arguments. For example, your script could use the `argparse`
    standard library like this:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI的超参数调整服务基于贝叶斯优化算法，能够快速找到最佳的超参数组合。要使用它，首先需要创建一个接受超参数值作为命令行参数的训练脚本。例如，您的脚本可以像这样使用`argparse`标准库：
- en: '[PRE55]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The hyperparameter tuning service will call your script multiple times, each
    time with different hyperparameter values: each run is called a *trial*, and the
    set of trials is called a *study*. Your training script must then use the given
    hyperparameter values to build and compile a model. You can use a mirrored distribution
    strategy if you want, in case each trial runs on a multi-GPU machine. Then the
    script can load the dataset and train the model. For example:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整服务将多次调用您的脚本，每次使用不同的超参数值：每次运行称为*trial*，一组试验称为*study*。然后，您的训练脚本必须使用给定的超参数值来构建和编译模型。如果需要，您可以使用镜像分发策略，以便每个试验在多GPU机器上运行。然后脚本可以加载数据集并训练模型。例如：
- en: '[PRE56]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Tip
  id: totrans-351
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: You can use the `AIP_*` environment variables we mentioned earlier to determine
    where to save the checkpoints, the TensorBoard logs, and the final model.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用我们之前提到的`AIP_*`环境变量来确定在哪里保存检查点、TensorBoard日志和最终模型。
- en: 'Lastly, the script must report the model’s performance back to Vertex AI’s
    hyperparameter tuning service, so it can decide which hyperparameters to try next.
    For this, you must use the `hypertune` library, which is automatically installed
    on Vertex AI training VMs:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，脚本必须将模型的性能报告给Vertex AI的超参数调整服务，以便它决定尝试哪些超参数。为此，您必须使用`hypertune`库，在Vertex
    AI训练VM上自动安装：
- en: '[PRE57]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now that your training script is ready, you need to define the type of machine
    you would like to run it on. For this, you must define a custom job, which Vertex
    AI will use as a template for each trial:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的训练脚本已准备就绪，您需要定义要在其上运行的机器类型。为此，您必须定义一个自定义作业，Vertex AI 将使用它作为每个试验的模板：
- en: '[PRE58]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Finally, you’re ready to create and run the hyperparameter tuning job:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您准备好创建并运行超参数调整作业：
- en: '[PRE59]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Here, we tell Vertex AI to maximize the metric named `"accuracy"`: this name
    must match the name of the metric reported by the training script. We also define
    the search space, using a log scale for the learning rate and a linear (i.e.,
    uniform) scale for the other hyperparameters. The hyperparameter names must match
    the command-line arguments of the training script. Then we set the maximum number
    of trials to 100, and the maximum number of trials running in parallel to 20\.
    If you increase the number of parallel trials to (say) 60, the total search time
    will be reduced significantly, by a factor of up to 3\. But the first 60 trials
    will be started in parallel, so they will not benefit from the other trials’ feedback.
    Therefore, you should increase the max number of trials to compensate—for example,
    up to about 140.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们告诉 Vertex AI 最大化名为 `"accuracy"` 的指标：这个名称必须与训练脚本报告的指标名称匹配。我们还定义了搜索空间，使用对数尺度来设置学习率，使用线性（即均匀）尺度来设置其他超参数。超参数的名称必须与训练脚本的命令行参数匹配。然后我们将最大试验次数设置为
    100，同时最大并行运行的试验次数设置为 20。如果你将并行试验的数量增加到（比如）60，总搜索时间将显著减少，最多可减少到 3 倍。但前 60 个试验将同时开始，因此它们将无法从其他试验的反馈中受益。因此，您应该增加最大试验次数来补偿，例如增加到大约
    140。
- en: 'This will take quite a while. Once the job is completed, you can fetch the
    trial results using `hp_job.trials`. Each trial result is represented as a protobuf
    object, containing the hyperparameter values and the resulting metrics. Let’s
    find the best trial:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要相当长的时间。一旦作业完成，您可以使用 `hp_job.trials` 获取试验结果。每个试验结果都表示为一个protobuf对象，包含超参数值和结果指标。让我们找到最佳试验：
- en: '[PRE60]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Now let’s look at this trial’s accuracy, and its hyperparameter values:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看这个试验的准确率，以及其超参数值：
- en: '[PRE61]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: That’s it! Now you can get this trial’s SavedModel, optionally train it a bit
    more, and deploy it to production.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！现在您可以获取这个试验的SavedModel，可选择性地再训练一下，并将其部署到生产环境中。
- en: Tip
  id: totrans-365
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Vertex AI also includes an AutoML service, which completely takes care of finding
    the right model architecture and training it for you. All you need to do is upload
    your dataset to Vertex AI using a special format that depends on the type of dataset
    (images, text, tabular, video, etc.), then create an AutoML training job, pointing
    to the dataset and specifying the maximum number of compute hours you’re willing
    to spend. See the notebook for an example.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI还包括一个AutoML服务，完全负责为您找到合适的模型架构并为您进行训练。您只需要将数据集以特定格式上传到Vertex AI，这取决于数据集的类型（图像、文本、表格、视频等），然后创建一个AutoML训练作业，指向数据集并指定您愿意花费的最大计算小时数。请参阅笔记本中的示例。
- en: 'Now you have all the tools and knowledge you need to create state-of-the-art
    neural net architectures and train them at scale using various distribution strategies,
    on your own infrastructure or on the cloud, and then deploy them anywhere. In
    other words, you now have superpowers: use them well!'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你拥有了所有需要创建最先进的神经网络架构并使用各种分布策略进行规模化训练的工具和知识，可以在自己的基础设施或云上部署它们，然后在任何地方部署它们。换句话说，你现在拥有超能力：好好利用它们！
- en: Exercises
  id: totrans-368
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: What does a SavedModel contain? How do you inspect its content?
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: SavedModel包含什么？如何检查其内容？
- en: When should you use TF Serving? What are its main features? What are some tools
    you can use to deploy it?
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么时候应该使用TF Serving？它的主要特点是什么？有哪些工具可以用来部署它？
- en: How do you deploy a model across multiple TF Serving instances?
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何在多个TF Serving实例上部署模型？
- en: When should you use the gRPC API rather than the REST API to query a model served
    by TF Serving?
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在查询由TF Serving提供的模型时，何时应该使用gRPC API而不是REST API？
- en: What are the different ways TFLite reduces a model’s size to make it run on
    a mobile or embedded device?
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: TFLite通过哪些不同的方式减小模型的大小，使其能在移动设备或嵌入式设备上运行？
- en: What is quantization-aware training, and why would you need it?
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是量化感知训练，为什么需要它？
- en: What are model parallelism and data parallelism? Why is the latter generally
    recommended?
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是模型并行和数据并行？为什么通常推荐后者？
- en: When training a model across multiple servers, what distribution strategies
    can you use? How do you choose which one to use?
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在多台服务器上训练模型时，您可以使用哪些分发策略？您如何选择使用哪种？
- en: Train a model (any model you like) and deploy it to TF Serving or Google Vertex
    AI. Write the client code to query it using the REST API or the gRPC API. Update
    the model and deploy the new version. Your client code will now query the new
    version. Roll back to the first version.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个模型（任何您喜欢的模型）并部署到TF Serving或Google Vertex AI。编写客户端代码，使用REST API或gRPC API查询它。更新模型并部署新版本。您的客户端代码现在将查询新版本。回滚到第一个版本。
- en: Train any model across multiple GPUs on the same machine using the `MirroredStrategy`
    (if you do not have access to GPUs, you can use Google Colab with a GPU runtime
    and create two logical GPUs). Train the model again using the `CentralStorageStrategy`
    and compare the training time.
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一台机器上使用`MirroredStrategy`在多个GPU上训练任何模型（如果您无法访问GPU，可以使用带有GPU运行时的Google Colab并创建两个逻辑GPU）。再次使用`CentralStorageStrategy`训练模型并比较训练时间。
- en: Fine-tune a model of your choice on Vertex AI, using either Keras Tuner or Vertex
    AI’s hyperparameter tuning service.
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Vertex AI上微调您选择的模型，使用Keras Tuner或Vertex AI的超参数调整服务。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可以在本章笔记本的末尾找到，网址为[*https://homl.info/colab3*](https://homl.info/colab3)。
- en: Thank You!
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谢谢！
- en: Before we close the last chapter of this book, I would like to thank you for
    reading it up to the last paragraph. I truly hope that you had as much fun reading
    this book as I had writing it, and that it will be useful for your projects, big
    or small.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束这本书的最后一章之前，我想感谢您读到最后一段。我真诚地希望您阅读这本书和我写作时一样开心，并且它对您的项目，无论大小，都有用。
- en: If you find errors, please send feedback. More generally, I would love to know
    what you think, so please don’t hesitate to contact me via O’Reilly, through the
    *ageron/handson-ml3* GitHub project, or on Twitter at @aureliengeron.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您发现错误，请发送反馈。更一般地，我很想知道您的想法，所以请不要犹豫通过O'Reilly、*ageron/handson-ml3* GitHub项目或Twitter上的@aureliengeron与我联系。
- en: 'Going forward, my best advice to you is to practice and practice: try going
    through all the exercises (if you have not done so already), play with the notebooks,
    join Kaggle or some other ML community, watch ML courses, read papers, attend
    conferences, and meet experts. Things move fast, so try to keep up to date. Several
    YouTube channels regularly present deep learning papers in great detail, in a
    very approachable way. I particularly recommend the channels by Yannic Kilcher,
    Letitia Parcalabescu, and Xander Steenbrugge. For fascinating ML discussions and
    higher-level insights, make sure to check out ML Street Talk, and Lex Fridman’s
    channel. It also helps tremendously to have a concrete project to work on, whether
    it is for work or for fun (ideally for both), so if there’s anything you have
    always dreamed of building, give it a shot! Work incrementally; don’t shoot for
    the moon right away, but stay focused on your project and build it piece by piece.
    It will require patience and perseverance, but when you have a walking robot,
    or a working chatbot, or whatever else you fancy building, it will be immensely
    rewarding!'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 继续前进，我给你的最好建议是练习和练习：尝试完成所有的练习（如果你还没有这样做），玩一下笔记本电脑，加入Kaggle或其他机器学习社区，观看机器学习课程，阅读论文，参加会议，与专家会面。事情发展迅速，所以尽量保持最新。一些YouTube频道定期以非常易懂的方式详细介绍深度学习论文。我特别推荐Yannic
    Kilcher、Letitia Parcalabescu和Xander Steenbrugge的频道。要了解引人入胜的机器学习讨论和更高层次的见解，请务必查看ML
    Street Talk和Lex Fridman的频道。拥有一个具体的项目要去做也会极大地帮助，无论是为了工作还是为了娱乐（最好两者兼顾），所以如果你一直梦想着建造某样东西，就试一试吧！逐步工作；不要立即朝着月球开火，而是专注于你的项目，一步一步地构建它。这需要耐心和毅力，但当你拥有一个行走的机器人，或一个工作的聊天机器人，或者其他你喜欢的任何东西时，这将是极其有益的！
- en: My greatest hope is that this book will inspire you to build a wonderful ML
    application that will benefit all of us. What will it be?
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 我最大的希望是这本书能激发你构建一个美妙的ML应用程序，使我们所有人受益。它会是什么样的？
- en: —*Aurélien Géron*
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: —*Aurélien Géron*
- en: ^([1](ch19.html#idm45720162442960-marker)) An A/B experiment consists in testing
    two different versions of your product on different subsets of users in order
    to check which version works best and get other insights.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch19.html#idm45720162442960-marker)) A/B实验包括在不同的用户子集上测试产品的两个不同版本，以检查哪个版本效果最好并获得其他见解。
- en: ^([2](ch19.html#idm45720162441360-marker)) Google AI Platform (formerly known
    as Google ML Engine) and Google AutoML merged in 2021 to form Google Vertex AI.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch19.html#idm45720162441360-marker)) Google AI平台（以前称为Google ML引擎）和Google
    AutoML在2021年合并为Google Vertex AI。
- en: ^([3](ch19.html#idm45720162435760-marker)) A REST (or RESTful) API is an API
    that uses standard HTTP verbs, such as GET, POST, PUT, and DELETE, and uses JSON
    inputs and outputs. The gRPC protocol is more complex but more efficient; data
    is exchanged using protocol buffers (see [Chapter 13](ch13.html#data_chapter)).
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch19.html#idm45720162435760-marker)) REST（或RESTful）API是一种使用标准HTTP动词（如GET、POST、PUT和DELETE）以及使用JSON输入和输出的API。gRPC协议更复杂但更高效；数据使用协议缓冲区进行交换（参见[第13章](ch13.html#data_chapter)）。
- en: ^([4](ch19.html#idm45720162234880-marker)) If you are not familiar with Docker,
    it allows you to easily download a set of applications packaged in a *Docker image*
    (including all their dependencies and usually some good default configuration)
    and then run them on your system using a *Docker engine*. When you run an image,
    the engine creates a *Docker container* that keeps the applications well isolated
    from your own system—but you can give it some limited access if you want. It is
    similar to a virtual machine, but much faster and lighter, as the container relies
    directly on the host’s kernel. This means that the image does not need to include
    or run its own kernel.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对Docker不熟悉，它允许您轻松下载一组打包在*Docker镜像*中的应用程序（包括所有依赖项和通常一些良好的默认配置），然后使用*Docker引擎*在您的系统上运行它们。当您运行一个镜像时，引擎会创建一个保持应用程序与您自己系统良好隔离的*Docker容器*，但如果您愿意，可以给它一些有限的访问权限。它类似于虚拟机，但速度更快、更轻，因为容器直接依赖于主机的内核。这意味着镜像不需要包含或运行自己的内核。
- en: ^([5](ch19.html#idm45720162137552-marker)) There are also GPU images available,
    and other installation options. For more details, please check out the official
    [installation instructions](https://homl.info/tfserving).
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 还有GPU镜像可用，以及其他安装选项。有关更多详细信息，请查看官方[安装说明](https://homl.info/tfserving)。
- en: ^([6](ch19.html#idm45720161858608-marker)) To be fair, this can be mitigated
    by serializing the data first and encoding it to Base64 before creating the REST
    request. Moreover, REST requests can be compressed using gzip, which reduces the
    payload size significantly.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 公平地说，这可以通过首先序列化数据，然后将其编码为Base64，然后创建REST请求来减轻。此外，REST请求可以使用gzip进行压缩，从而显著减少有效负载大小。
- en: ^([7](ch19.html#idm45720160648176-marker)) Also check out TensorFlow’s [Graph
    Transform Tool](https://homl.info/tfgtt) for modifying and optimizing computational
    graphs.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 还要查看TensorFlow的[Graph Transform Tool](https://homl.info/tfgtt)，用于修改和优化计算图。
- en: ^([8](ch19.html#idm45720160353952-marker)) For example, a PWA must include icons
    of various sizes for different mobile devices, it must be served via HTTPS, it
    must include a manifest file containing metadata such as the name of the app and
    the background color.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，PWA必须包含不同移动设备大小的图标，必须通过HTTPS提供，必须包含包含应用程序名称和背景颜色等元数据的清单文件。
- en: ^([9](ch19.html#idm45720160307664-marker)) Please check the TensorFlow docs
    for detailed and up-to-date installation instructions, as they change quite often.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看TensorFlow文档，获取详细和最新的安装说明，因为它们经常更改。
- en: ^([10](ch19.html#idm45720159934800-marker)) As we saw in [Chapter 12](ch12.html#tensorflow_chapter),
    a kernel is an operation’s implementation for a specific data type and device
    type. For example, there is a GPU kernel for the `float32` `tf.matmul()` operation,
    but there is no GPU kernel for `int32` `tf.matmul()`, only a CPU kernel.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch19.html#idm45720159934800-marker)) 正如我们在[第12章](ch12.html#tensorflow_chapter)中所看到的，内核是特定数据类型和设备类型的操作实现。例如，`float32`
    `tf.matmul()` 操作有一个 GPU 内核，但 `int32` `tf.matmul()` 没有 GPU 内核，只有一个 CPU 内核。
- en: ^([11](ch19.html#idm45720159930608-marker)) You can also use `tf.debugging.set_log_device_placement(True)`
    to log all device placements.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch19.html#idm45720159930608-marker)) 您还可以使用 `tf.debugging.set_log_device_placement(True)`
    来记录所有设备放置情况。
- en: ^([12](ch19.html#idm45720159777008-marker)) This can be useful if you want to
    guarantee perfect reproducibility, as I explain in [this video](https://homl.info/repro),
    based on TF 1.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch19.html#idm45720159777008-marker)) 如果您想要保证完美的可重现性，这可能很有用，正如我在[这个视频](https://homl.info/repro)中所解释的，基于
    TF 1。
- en: ^([13](ch19.html#idm45720159772080-marker)) At the time of writing, it only
    prefetches the data to the CPU RAM, but use `tf.data.experimental.pre⁠fetch​_to_device()`
    to make it prefetch the data and push it to the device of your choice so that
    the GPU does not waste time waiting for the data to be transferred.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch19.html#idm45720159772080-marker)) 在撰写本文时，它只是将数据预取到 CPU RAM，但使用 `tf.data.experimental.pre⁠fetch​_to_device()`
    可以使其预取数据并将其推送到您选择的设备，以便 GPU 不必等待数据传输而浪费时间。
- en: ^([14](ch19.html#idm45720159768848-marker)) If the two CNNs are identical, then
    it is called a *Siamese neural network*.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 如果两个CNN相同，则称为*孪生神经网络*。
- en: ^([15](ch19.html#idm45720159738736-marker)) If you are interested in going further
    with model parallelism, check out [Mesh TensorFlow](https://github.com/tensorflow/mesh).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您对模型并行性感兴趣，请查看[Mesh TensorFlow](https://github.com/tensorflow/mesh)。
- en: '^([16](ch19.html#idm45720159706016-marker)) This name is slightly confusing
    because it sounds like some replicas are special, doing nothing. In reality, all
    replicas are equivalent: they all work hard to be among the fastest at each training
    step, and the losers vary at every step (unless some devices are really slower
    than others). However, it does mean that if one or two servers crash, training
    will continue just fine.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 这个名字有点令人困惑，因为听起来好像有些副本是特殊的，什么也不做。实际上，所有副本都是等价的：它们都努力成为每个训练步骤中最快的，失败者在每一步都会变化（除非某些设备真的比其他设备慢）。但是，这意味着如果一个或两个服务器崩溃，训练将继续进行得很好。
- en: ^([17](ch19.html#idm45720159687856-marker)) Jianmin Chen et al., “Revisiting
    Distributed Synchronous SGD”, arXiv preprint arXiv:1604.00981 (2016).
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: Jianmin Chen等人，“重新审视分布式同步SGD”，arXiv预印本arXiv:1604.00981（2016）。
- en: '^([18](ch19.html#idm45720159675552-marker)) Aaron Harlap et al., “PipeDream:
    Fast and Efficient Pipeline Parallel DNN Training”, arXiv preprint arXiv:1806.03377
    (2018).'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '^([18](ch19.html#idm45720159675552-marker)) Aaron Harlap等人，“PipeDream: 快速高效的管道并行DNN训练”，arXiv预印本arXiv:1806.03377（2018）。'
- en: '^([19](ch19.html#idm45720159666736-marker)) Paul Barham et al., “Pathways:
    Asynchronous Distributed Dataflow for ML”, arXiv preprint arXiv:2203.12533 (2022).'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '^([19](ch19.html#idm45720159666736-marker)) Paul Barham等人，“Pathways: 异步分布式数据流ML”，arXiv预印本arXiv:2203.12533（2022）。'
- en: ^([20](ch19.html#idm45720159438096-marker)) For more details on AllReduce algorithms,
    read [Yuichiro Ueno’s post](https://homl.info/uenopost) on the technologies behind
    deep learning and [Sylvain Jeaugey’s post](https://homl.info/ncclalgo) on massively
    scaling deep learning training with NCCL.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch19.html#idm45720159438096-marker)) 有关AllReduce算法的更多详细信息，请阅读Yuichiro
    Ueno的文章，该文章介绍了深度学习背后的技术，以及Sylvain Jeaugey的文章，该文章介绍了如何使用NCCL大规模扩展深度学习训练。
