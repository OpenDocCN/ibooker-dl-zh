["```py\ndef extract_relationships(text, lang_model, coref_model):\n  resolved_text = resolve_coreferences(text, coref_model)  #1\n  sentences = get_sentences(resolved_text, lang_model)  #2\n  return resolve_facts(sentences, lang_model) #3\n\ntext = \"\"\"\nData Scientists build machine learning models. They also write code.\nCompanies employ Data Scientists.\nSoftware Engineers also write code. Companies employ Software Engineers.\n\"\"\"\nlang_model = spacy.load(\"en_core_web_sm\")\ncoref_model = spacy.load(\"en_coreference_web_trf\")  #4\ngraph = extract_relationships(text, lang_model, coref_model)\nprint(graph)\n```", "```py\nsentence: Data Scientists build machine learning models.\ndependence_parse: ['nsubj', 'ROOT', 'dobj', 'punct']\n---------------------\nsentence: Data Scientists also write code.\ndependence_parse: ['nsubj', 'advmod', 'ROOT', 'dobj', 'punct']\n---------------------\nsentence: Companies employ Data Scientists.\ndependence_parse: ['nsubj', 'ROOT', 'dobj', 'punct']\n---------------------\nsentence: Software Engineers also write code.\ndependence_parse: ['nsubj', 'advmod', 'ROOT', 'dobj', 'punct']\n---------------------\nsentence: Companies employ Software Engineers.\ndependence_parse: ['nsubj', 'ROOT', 'dobj', 'punct']\n---------------------\n[['Data Scientists', 'build', 'machine learning models'],\n ['Data Scientists', 'write', 'code'],\n ['Companies', 'employ', 'Data Scientists'],\n ['Software Engineers', 'write', 'code'],\n ['Companies', 'employ', 'Software Engineers']]\n```", "```py\nsimple_hearst_patterns = [\n  (\"(NP_\\\\w+ (, )?such as (NP_\\\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n  (\"(such NP_\\\\w+ (, )?as (NP_\\\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n  (\"((NP_\\\\w+ ?(, )?)+(and |or )?other NP_\\\\w+)\", \"last\"),\n  (\"(NP_\\\\w+ (, )?include (NP_\\\\w+ ?(, )?(and |or )?)+)\", \"first\"),\n  (\"(NP_\\\\w+ (, )?especially (NP_\\\\w+ ?(, )?(and |or )?)+)\", \"first\")]\n```", "```py\ntext_content = \"\"\"Many data scientists have skills such as machine learning,\npython, deep learning, apache spark, among others. Job candidates most\nprefer job benefits such as commute time, company culture, and salary.\nGoogle, Apple, or other tech companies might sponsor the conference.\nBig cities such as San Francisco, Miami, and New York often appeal to\nnew graduates. Job roles such as Software Engineer, Registered Nurse,\nand DevOps Engineer are in high demand. There are job benefits including\nhealth insurance and pto.\"\"\"\n\nextracted_relationships = HearstPatterns().find_hyponyms(text_content)\nfacts = [[pair[0], \"is_a\", pair[1]] for pair in extracted_relationships]\nprint(*facts, sep=\"\\n\")\n```", "```py\n['machine learning', 'is_a', 'skill']\n['python', 'is_a', 'skill']\n['deep learning', 'is_a', 'skill']\n['apache spark', 'is_a', 'skill']\n['commute time', 'is_a', 'job benefit']\n['company culture', 'is_a', 'job benefit']\n['salary', 'is_a', 'job benefit']\n['Google', 'is_a', 'tech company']\n['Apple', 'is_a', 'tech company']\n['San Francisco', 'is_a', 'big city']\n['Miami', 'is_a', 'big city']\n['New York', 'is_a', 'big city']\n['Software Engineer', 'is_a', 'Job role']\n['Registered Nurse', 'is_a', 'Job role']\n['DevOps Engineer', 'is_a', 'Job role']\n['health insurance', 'is_a', 'job benefit']\n['pto', 'is_a', 'job benefit']\n```", "```py\nadvil  0.71\nmotrin  0.60\naleve  0.47\nibuprofen  0.38\nalleve  0.37\n```", "```py\nhealth_skg = get_skg(engine.get_collection(\"health\"))\n\nnodes_to_traverse = [{\"field\": \"body\",  #1\n                      \"values\": [\"advil\"]},  #2\n                     {\"field\": \"body\",\n                      \"min_occurrences\": 2,  #3\n                      \"limit\": 8}]  #4\n\ntraversal = health_skg.traverse(*nodes_to_traverse)  #5\nprint_graph(traversal, \"advil\") #6\n```", "```py\nadvil  0.70986\nmotrin  0.59897\naleve  0.4662\nibuprofen  0.38264\nalleve  0.36649\ntylenol  0.33048\nnaproxen  0.31226\nacetaminophen  0.17706\n```", "```py\n{\n  \"limit\": 0,\n  \"params\": {\n    \"q\": \"*\",\n    \"fore\": \"{!${defType} v=$q}\",\n    \"back\": \"*\",\n    \"defType\": \"edismax\",\n    \"f0_0_query\": \"advil\"\n  },\n  \"facet\": {\n    \"f0_0\": {\n      \"type\": \"query\",\n      \"query\": \"{!edismax qf=body v=$f0_0_query}\",\n      \"field\": \"body\",\n      \"sort\": {\"relatedness\": \"desc\"},\n      \"facet\": {\"relatedness\": {\"type\": \"func\",\n                                \"func\": \"relatedness($fore,$back)\"},\n        \"f1_0\": {\n          \"type\": \"terms\",\n          \"mincount\": 2,\n          \"limit\": 8,\n          \"sort\": {\"relatedness\": \"desc\"},\n          \"facet\": {\"relatedness\": {\"type\": \"func\",\n                                    \"func\": \"relatedness($fore,$back)\"}\n}}}}}}\n```", "```py\nstackexchange_skg = get_skg(engine.get_collection(\"stackexchange\"))\n\nquery = \"vibranium\"\nnodes_to_traverse = [{\"field\": \"body\", \"values\": [query]},\n                     {\"field\": \"body\", \"min_occurrences\": 2, \"limit\": 8}]\n\ntraversal = stackexchange_skg.traverse(*nodes_to_traverse)\n\nprint_graph(traversal, query)\n```", "```py\nvibranium  0.94237\nwakandan  0.8197\nadamantium  0.80724\nwakanda  0.79122\nalloy  0.75724\nmaclain  0.75623\nklaw  0.75222\namerica's  0.74002\n```", "```py\nexpansion = \"\"\nfor term, stats in traversal[\"graph\"][0][\"values\"][query] \\\n                       [\"traversals\"][0][\"values\"].items():\n  expansion += f'{term}^{stats[\"relatedness\"]} '\nexpanded_query = f\"{query}^5 \" + expansion\n\nprint(f\"Expanded Query:\\n{expanded_query}\")\n```", "```py\nvibranium^5 vibranium^0.94237 wakandan^0.8197 adamantium^0.80724\nwakanda^0.79122 alloy^0.75724 maclain^0.75623 klaw^0.75222 america's^0.74002\n```", "```py\ndef generate_request(query, min_match=None, boost=None):\n  request = {\"query\": query,\n             \"query_fields\": [\"title\", \"body\"]}\n  if min_match:\n    request[\"min_match\"] = min_match\n  if boost:\n    request[\"query_boosts\"] = boost\n  return request\n\nsimple_expansion = generate_request(f\"{query} {expansion}\", \"1\")\nincreased_conceptual_precision = \\\n  generate_request(f\"{query} {expansion}\", \"30%\")\nincreased_precision_same_recall = \\\n  generate_request(f\"{query} AND ({expansion})\", \"2\")\nslightly_increased_recall = generate_request(f\"{query} {expansion}\", \"2\")\nsame_results_better_ranking = generate_request(query, \"2\", expansion)\n```", "```py\n{\"query\": \"vibranium vibranium^0.94237 wakandan^0.8197 adamantium^0.80724\n         ↪wakanda^0.79122 alloy^0.75724 maclain^0.75623 klaw^0.75222\n         ↪america's^0.74002 \",\n \"query_fields\": [\"title\", \"body\"],\n \"min_match\": \"0%\"}\n```", "```py\n{\"query\": \"vibranium AND (vibranium^0.94237 wakandan^0.8197\n         ↪adamantium^0.80724 wakanda^0.79122 alloy^0.75724\n         ↪maclain^0.75623 klaw^0.75222 america's^0.74002)\",\n \"query_fields\": [\"title\", \"body\"],\n \"min_match\": \"30%\"}\n```", "```py\n{\"query\": \"vibranium AND (vibranium^0.94237 wakandan^0.8197\n         ↪adamantium^0.80724 wakanda^0.79122 alloy^0.75724\n         ↪maclain^0.75623 klaw^0.75222 america's^0.74002)\",\n \"query_fields\": [\"title\", \"body\"],\n \"min_match\": \"2\"}\n```", "```py\n{\"query\": \"vibranium vibranium^0.94237 wakandan^0.8197\n         ↪adamantium^0.80724 wakanda^0.79122 alloy^0.75724\n         ↪maclain^0.75623 klaw^0.75222 america's^0.74002\",\n \"query_fields\": [\"title\", \"body\"],\n \"min_match\": \"2\"}\n```", "```py\n{\"query\": \"vibranium\",\n \"query_fields\": [\"title\", \"body\"],\n \"min_match\": \"2\",\n \"query_boosts\": \"vibranium^0.94237 wakandan^0.8197 adamantium^0.80724\n                ↪wakanda^0.79122 alloy^0.75724 maclain^0.75623\n                ↪klaw^0.75222 america's^0.74002 \"}\n```", "```py\nfrom aips import extract_phrases\n\nstackexchange_skg = get_skg(engine.get_collection(\"stackexchange\"))\n\nclassification = \"star wars\"\ndocument = \"\"\"this doc contains the words luke, magneto, cyclops,\n              darth vader, princess leia, wolverine, apple, banana,\n              galaxy, force, blaster, and chloe.\"\"\"\nparsed_document = extract_phrases(document)\nnodes_to_traverse = [{\"field\": \"body\", \"values\": [classification]},\n                     {\"field\": \"body\", \"values\": parsed_document}]\n\ntraversal = stackexchange_skg.traverse(*nodes_to_traverse)\n\nprint_graph(traversal, classification)\n```", "```py\nluke  0.75212\nforce  0.73248\ndarth vader  0.69378\ngalaxy  0.58693\nprincess leia  0.50491\nblaster  0.47143\nthis  0.19193\nthe  0.17519\nwords  0.10144\nand  0.09709\ncontains  0.03434\ndoc  0.00885\nchloe  0.0\ncyclops  -0.01825\nmagneto  -0.02175\nbanana  -0.0319\nwolverine  -0.03362\napple  -0.03894\n```", "```py\ndef get_scored_terms(traversal):\n  return {term: data[\"relatedness\"]\n          for term, data in traversal[\"graph\"][0][\"values\"][\"star wars\"] \\\n                                [\"traversals\"][0][\"values\"].items()}\n\nrec_query = \" \".join(f'\"{term}\"^{score}'\n                     for term, score in get_scored_terms(traversal).items()\n                     if score > 0.25)\n\nprint(f\"Expanded Query:\\n{rec_query}\")\n```", "```py\n\"luke\"^0.75212 \"force\"^0.73248 \"darth vader\"^0.69378 \"galaxy\"^0.58693\n\"princess leia\"^0.50491 \"blaster\"^0.47143\n```", "```py\nstackexchange_collection = engine.get_collection(\"stackexchange\")\n\nrequest = {\"query\": rec_query,\n           \"query_fields\": [\"title\", \"body\"],\n           \"return_fields\": [\"title\"],\n           \"limit\": 5,\n           \"filters\": [(\"title\", \"*\")]}\n\nresponse = stackexchange_collection.search(**request)\n\nprint(json.dumps(response[\"docs\"], indent=2))\n```", "```py\n[{\"title\": \"At the end of Return of the Jedi, did Darth Vader learn\n          ↪that Princess Leia was his daughter?\"},\n {\"title\": \"Did Luke know the &quot;Chosen One&quot; prophecy?\"},\n {\"title\": \"Was Darth Vader at his strongest during Episode III?\"},\n {\"title\": \"Why couldn't Snoke or Kylo Ren trace Luke using the Force?\"},\n {\"title\": \"Does Kylo Ren know that Darth Vader reconciled with Luke?\"}]\n```", "```py\nscifi_skg = get_skg(engine.get_collection(\"scifi\"))\n\nstarting_node = \"jean grey\"\nrelationship = \"in love with\"\nnodes_to_traverse = [{\"field\": \"body\", \"values\": [starting_node]},\n                     {\"field\": \"body\", \"values\": [relationship],\n                      \"default_operator\": \"OR\"},\n                     {\"field\": \"body\",\n                      \"min_occurrences\": 25, \"limit\": 10}]\n\ntraversal = scifi_skg.traverse(*nodes_to_traverse)\n\nprint_graph(traversal, starting_node, relationship)\n```", "```py\njean  0.84915\ngrey  0.74742\nsummers  0.61021\ncyclops  0.60693\nxavier  0.53004\nwolverine  0.48053\nmutant  0.46532\nx  0.45028\nmutants  0.42568\nmagneto  0.42197\n```"]