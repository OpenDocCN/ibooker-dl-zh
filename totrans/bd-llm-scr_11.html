<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">appendix E</span></span> <span class="chapter-title-text">Parameter-efficient fine-tuning with LoRA</span></h1> 
  </div> 
  <div class="readable-text" id="p2"> 
   <p><em>Low-rank adaptation</em> (<em>LoRA</em>) is one of the most widely used techniques for <em>parameter-efficient fine-tuning</em>. The following discussion is based on the spam classification fine-tuning example given in chapter 6. However, LoRA fine-tuning is also applicable to the supervised <em>instruction fine-tuning</em> discussed in chapter 7.</p> 
  </div> 
  <div class="readable-text" id="p3"> 
   <h2 class=" readable-text-h2"><span class="num-string">E.1</span> Introduction to LoRA</h2> 
  </div> 
  <div class="readable-text" id="p4"> 
   <p>LoRA is a technique that adapts a pretrained model to better suit a specific, often smaller dataset by adjusting only a small subset of the model’s weight parameters. The “low-rank” aspect refers to the mathematical concept of limiting model adjustments to a smaller dimensional subspace of the total weight parameter space. This effectively captures the most influential directions of the weight parameter changes during training. The LoRA method is useful and popular because it enables efficient fine-tuning of large models on task-specific data, significantly cutting down on the computational costs and resources usually required for fine-tuning.</p> 
  </div> 
  <div class="readable-text intended-text" id="p5"> 
   <p>Suppose a large weight matrix <em>W </em>is associated with a specific layer. LoRA can be applied to all linear layers in an LLM. However, we focus on a single layer for illustration purposes.</p> 
  </div> 
  <div class="readable-text intended-text" id="p6"> 
   <p>When training deep neural networks, during backpropagation, we learn a <span class="regular-symbol">D</span><em>W</em> matrix, which contains information on how much we want to update the original weight parameters to minimize the loss function during training. Hereafter, I use the term “weight” as shorthand for the model’s weight parameters.</p> 
  </div> 
  <div class="readable-text intended-text" id="p7"> 
   <p>In regular training and fine-tuning, the weight update is defined as follows:<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p8"> 
   <img alt="figure" src="../Images/Equation-eqs-E-1.png" width="197" height="25"/> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>The LoRA method, proposed by Hu et al. (<a href="https://arxiv.org/abs/2106.09685">https://arxiv.org/abs/2106.09685</a>), offers a more efficient alternative to computing the weight updates <span class="regular-symbol">D</span><em>W</em> by learning an approximation of it:<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p10"> 
   <img alt="figure" src="../Images/Equation-eqs-E-2.png" width="102" height="19"/> 
  </div> 
  <div class="readable-text" id="p11"> 
   <p>where <em>A</em> and <em>B</em> are two matrices much smaller than <em>W</em>, and <em>AB</em> represents the matrix multiplication product between <em>A</em> and <em>B</em>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p12"> 
   <p>Using LoRA, we can then reformulate the weight update we defined earlier:<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p13"> 
   <img alt="figure" src="../Images/Equation-eqs-E-3.png" width="187" height="25"/> 
  </div> 
  <div class="readable-text" id="p14"> 
   <p>Figure E.1 illustrates the weight update formulas for full fine-tuning and LoRA side by side.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p16">  
   <img alt="figure" src="../Images/E-1.png" width="1012" height="404"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure E.1</span> A comparison between weight update methods: regular fine-tuning and LoRA. Regular fine-tuning involves updating the pretrained weight matrix W directly with <span class="regular-symbol">D</span>W (left). LoRA uses two smaller matrices, A and B, to approximate <span class="regular-symbol">D</span>W, where the product AB is added to W, and r denotes the inner dimension, a tunable hyperparameter (right).</h5>
  </div> 
  <div class="readable-text" id="p17"> 
   <p>If you paid close attention, you might have noticed that the visual representations of full fine-tuning and LoRA in figure E.1 differ slightly from the earlier presented formulas. This variation is attributed to the distributive law of matrix multiplication, which allows us to separate the original and updated weights rather than combine them. For example, in the case of regular fine-tuning with <em>x</em> as the input data, we can express the computation as <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p18"> 
   <img alt="figure" src="../Images/Equation-eqs-E-4.png" width="262" height="25"/> 
  </div> 
  <div class="readable-text" id="p19"> 
   <p>Similarly, we can write the following for LoRA:<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p20"> 
   <img alt="figure" src="../Images/Equation-eqs-E-5.png" width="249" height="25"/> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>Besides reducing the number of weights to update during training, the ability to keep the LoRA weight matrices separate from the original model weights makes LoRA even more useful in practice. Practically, this allows for the pretrained model weights to remain unchanged, with the LoRA matrices being applied dynamically after training when using the model.</p> 
  </div> 
  <div class="readable-text intended-text" id="p22"> 
   <p>Keeping the LoRA weights separate is very useful in practice because it enables model customization without needing to store multiple complete versions of an LLM. This reduces storage requirements and improves scalability, as only the smaller LoRA matrices need to be adjusted and saved when we customize LLMs for each specific customer or application.</p> 
  </div> 
  <div class="readable-text intended-text" id="p23"> 
   <p>Next, let’s see how LoRA can be used to fine-tune an LLM for spam classification, similar to the fine-tuning example in chapter 6.</p> 
  </div> 
  <div class="readable-text" id="p24"> 
   <h2 class=" readable-text-h2"><span class="num-string">E.2</span> Preparing the dataset</h2> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>Before applying LoRA to the spam classification example, we must load the dataset and pretrained model we will work with. The code here repeats the data preparation from chapter 6. (Instead of repeating the code, we could open and run the chapter 6 notebook and insert the LoRA code from section E.4 there.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p26"> 
   <p>First, we download the dataset and save it as CSV files.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p27"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing E.1</span> Downloading and preparing the dataset</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from pathlib import Path
import pandas as pd
from ch06 import (
    download_and_unzip_spam_data,
    create_balanced_dataset,
    random_split
)

url = \ 
"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip"
zip_path = "sms_spam_collection.zip"
extracted_path = "sms_spam_collection"
data_file_path = Path(extracted_path) / "SMSSpamCollection.tsv"

download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)

df = pd.read_csv(
    data_file_path, sep="\t", header=None, names=["Label", "Text"]
)
balanced_df = create_balanced_dataset(df)
balanced_df["Label"] = balanced_df["Label"].map({"ham": 0, "spam": 1})

train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)
train_df.to_csv("train.csv", index=None)
validation_df.to_csv("validation.csv", index=None)
test_df.to_csv("test.csv", index=None)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p28"> 
   <p>Next, we create the <code>SpamDataset</code> instances.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p29"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing E.2</span> Instantiating PyTorch datasets</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import torch
from torch.utils.data import Dataset
import tiktoken
from chapter06 import SpamDataset

tokenizer = tiktoken.get_encoding("gpt2")
train_dataset = SpamDataset("train.csv", max_length=None, 
    tokenizer=tokenizer
)
val_dataset = SpamDataset("validation.csv", 
    max_length=train_dataset.max_length, tokenizer=tokenizer
)
test_dataset = SpamDataset(
    "test.csv", max_length=train_dataset.max_length, tokenizer=tokenizer
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p30"> 
   <p>After creating the PyTorch dataset objects, we instantiate the data loaders.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p31"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing E.3</span> Creating PyTorch data loaders</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from torch.utils.data import DataLoader

num_workers = 0
batch_size = 8

torch.manual_seed(123)

train_loader = DataLoader(
    dataset=train_dataset,
    batch_size=batch_size,
    shuffle=True,
    num_workers=num_workers,
    drop_last=True,
)

val_loader = DataLoader(
    dataset=val_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    drop_last=False,
)

test_loader = DataLoader(
    dataset=test_dataset,
    batch_size=batch_size,
    num_workers=num_workers,
    drop_last=False,
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p32"> 
   <p>As a verification step, we iterate through the data loaders and check that the batches contain eight training examples each, where each training example consists of 120 tokens:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p33"> 
   <div class="code-area-container"> 
    <pre class="code-area">print("Train loader:")
for input_batch, target_batch in train_loader:
    pass

print("Input batch dimensions:", input_batch.shape)
print("Label batch dimensions", target_batch.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p34"> 
   <p>The output is </p> 
  </div> 
  <div class="browsable-container listing-container" id="p35"> 
   <div class="code-area-container"> 
    <pre class="code-area">Train loader:
Input batch dimensions: torch.Size([8, 120])
Label batch dimensions torch.Size([8])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>Lastly, we print the total number of batches in each dataset:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p37"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(f"{len(train_loader)} training batches")
print(f"{len(val_loader)} validation batches")
print(f"{len(test_loader)} test batches")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>In this case, we have the following number of batches per dataset:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p39"> 
   <div class="code-area-container"> 
    <pre class="code-area">130 training batches
19 validation batches
38 test batches</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p40"> 
   <h2 class=" readable-text-h2"><span class="num-string">E.3</span> Initializing the model</h2> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>We repeat the code from chapter 6 to load and prepare the pretrained GPT model. We begin by downloading the model weights and loading them into the <code>GPTModel</code> class.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p42"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing E.4</span> Loading a pretrained GPT model</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from gpt_download import download_and_load_gpt2
from chapter04 import GPTModel
from chapter05 import load_weights_into_gpt


CHOOSE_MODEL = "gpt2-small (124M)"
INPUT_PROMPT = "Every effort moves"

BASE_CONFIG = {
    "vocab_size": 50257,        <span class="aframe-location"/> #1
    "context_length": 1024,     <span class="aframe-location"/> #2
    "drop_rate": 0.0,           <span class="aframe-location"/> #3
    "qkv_bias": True            <span class="aframe-location"/> #4
}

model_configs = {
    "gpt2-small (124M)": {"emb_dim": 768, "n_layers": 12, "n_heads": 12},
    "gpt2-medium (355M)": {"emb_dim": 1024, "n_layers": 24, "n_heads": 16},
    "gpt2-large (774M)": {"emb_dim": 1280, "n_layers": 36, "n_heads": 20},
    "gpt2-xl (1558M)": {"emb_dim": 1600, "n_layers": 48, "n_heads": 25},
}

BASE_CONFIG.update(model_configs[CHOOSE_MODEL])

model_size = CHOOSE_MODEL.split(" ")[-1].lstrip("(").rstrip(")")
settings, params = download_and_load_gpt2(
    model_size=model_size, models_dir="gpt2"
)

model = GPTModel(BASE_CONFIG)
load_weights_into_gpt(model, params)
model.eval()</pre> 
    <div class="code-annotations-overlay-container">
     #1 Vocabulary size
     <br/>#2 Context length
     <br/>#3 Dropout rate
     <br/>#4 Query-key-value bias
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p43"> 
   <p>To ensure that the model was loaded corrected, let’s double-check that it generates coherent text:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p44"> 
   <div class="code-area-container"> 
    <pre class="code-area">from chapter04 import generate_text_simple
from chapter05 import text_to_token_ids, token_ids_to_text

text_1 = "Every effort moves you"

token_ids = generate_text_simple(
    model=model,
    idx=text_to_token_ids(text_1, tokenizer),
    max_new_tokens=15,
    context_size=BASE_CONFIG["context_length"]
)

print(token_ids_to_text(token_ids, tokenizer))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>The following output shows that the model generates coherent text, which is an indicator that the model weights are loaded correctly:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p46"> 
   <div class="code-area-container"> 
    <pre class="code-area">Every effort moves you forward.
The first step is to understand the importance of your work</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>Next, we prepare the model for classification fine-tuning, similar to chapter 6, where we replace the output layer:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p48"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
num_classes = 2
model.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p49"> 
   <p>Lastly, we calculate the initial classification accuracy of the not-fine-tuned model (we expect this to be around 50%, which means that the model is not able to distinguish between spam and nonspam messages yet reliably):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p50"> 
   <div class="code-area-container"> 
    <pre class="code-area">from chapter06 import calc_accuracy_loader

torch.manual_seed(123)
train_accuracy = calc_accuracy_loader(
    train_loader, model, device, num_batches=10
)
val_accuracy = calc_accuracy_loader(
    val_loader, model, device, num_batches=10
)
test_accuracy = calc_accuracy_loader(
    test_loader, model, device, num_batches=10
)


print(f"Training accuracy: {train_accuracy*100:.2f}%")
print(f"Validation accuracy: {val_accuracy*100:.2f}%")
print(f"Test accuracy: {test_accuracy*100:.2f}%")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>The initial prediction accuracies are </p> 
  </div> 
  <div class="browsable-container listing-container" id="p52"> 
   <div class="code-area-container"> 
    <pre class="code-area">Training accuracy: 46.25%
Validation accuracy: 45.00%
Test accuracy: 48.75%</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p53"> 
   <h2 class=" readable-text-h2"><span class="num-string">E.4</span> Parameter-efficient fine-tuning with LoRA</h2> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>Next, we modify and fine-tune the LLM using LoRA. We begin by initializing a LoRALayer that creates the matrices <em>A</em> and <em>B</em>, along with the <code>alpha</code> scaling factor and the <code>rank</code> (<em>r</em>) setting. This layer can accept an input and compute the corresponding output, as illustrated in figure E.2.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p55">  
   <img alt="figure" src="../Images/E-2.png" width="480" height="309"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure E.2</span> The LoRA matrices A and B are applied to the layer inputs and are involved in computing the model outputs. The inner dimension r of these matrices serves as a setting that adjusts the number of trainable parameters by varying the sizes of A and B.</h5>
  </div> 
  <div class="readable-text" id="p56"> 
   <p>In code, this LoRA layer can be implemented as follows.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p57"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing E.5</span> Implementing a LoRA layer</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import math

class LoRALayer(torch.nn.Module):
    def __init__(self, in_dim, out_dim, rank, alpha):
        super().__init__()
        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))
        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))   <span class="aframe-location"/> #1
        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))
        self.alpha = alpha

    def forward(self, x):
        x = self.alpha * (x @ self.A @ self.B)
        return x</pre> 
    <div class="code-annotations-overlay-container">
     #1 The same initialization used for Linear layers in PyTorch
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>The <code>rank</code> governs the inner dimension of matrices <em>A</em> and <em>B</em>. Essentially, this setting determines the number of extra parameters introduced by LoRA, which creates balance between the adaptability of the model and its efficiency via the number of parameters used.</p> 
  </div> 
  <div class="readable-text intended-text" id="p59"> 
   <p>The other important setting, <code>alpha</code>, functions as a scaling factor for the output from the low-rank adaptation. It primarily dictates the degree to which the output from the adapted layer can affect the original layer’s output. This can be seen as a way to regulate the effect of the low-rank adaptation on the layer’s output. The <code>LoRALayer</code> class we have implemented so far enables us to transform the inputs of a layer.</p> 
  </div> 
  <div class="readable-text intended-text" id="p60"> 
   <p>In LoRA, the typical goal is to substitute existing <code>Linear</code> layers, allowing weight updates to be applied directly to the pre-existing pretrained weights, as illustrated in figure E.3.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p61">  
   <img alt="figure" src="../Images/E-3.png" width="799" height="399"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure E.3</span> The integration of LoRA into a model layer. The original pretrained weights (W) of a layer are combined with the outputs from LoRA matrices (A and B), which approximate the weight update matrix (<span class="regular-symbol">D</span>W). The final output is calculated by adding the output of the adapted layer (using LoRA weights) to the original output.</h5>
  </div> 
  <div class="readable-text" id="p62"> 
   <p>To integrate the original <code>Linear</code> layer weights, we now create a <code>LinearWithLoRA</code> layer. This layer utilizes the previously implemented <code>LoRALayer</code> and is designed to replace existing <code>Linear</code> layers within a neural network, such as the self-attention modules or feed-forward modules in the <code>GPTModel</code>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p63"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing E.6</span> Replacing <code>Linear</code> layers with <code>LinearWithLora</code> layers</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">class LinearWithLoRA(torch.nn.Module):
    def __init__(self, linear, rank, alpha):
        super().__init__()
        self.linear = linear
        self.lora = LoRALayer(
            linear.in_features, linear.out_features, rank, alpha
        )

    def forward(self, x):
        return self.linear(x) + self.lora(x)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>This code combines a standard <code>Linear</code> layer with the <code>LoRALayer</code>. The <code>forward</code> method computes the output by adding the results from the original linear layer and the LoRA layer.</p> 
  </div> 
  <div class="readable-text intended-text" id="p65"> 
   <p>Since the weight matrix <em>B</em> (<code>self.B</code> in <code>LoRALayer</code>) is initialized with zero values, the product of matrices <em>A </em>and <em>B </em>results in a zero matrix. This ensures that the multiplication does not alter the original weights, as adding zero does not change them.</p> 
  </div> 
  <div class="readable-text intended-text" id="p66"> 
   <p>To apply LoRA to the earlier defined <code>GPTModel</code>, we introduce a <code>replace_linear_ with_lora</code> function. This function will swap all existing <code>Linear</code> layers in the model with the newly created <code>LinearWithLoRA</code> layers:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p67"> 
   <div class="code-area-container"> 
    <pre class="code-area">def replace_linear_with_lora(model, rank, alpha):
    for name, module in model.named_children():
        if isinstance(module, torch.nn.Linear):    <span class="aframe-location"/> #1
            setattr(model, name, LinearWithLoRA(module, rank, alpha))
        else:   <span class="aframe-location"/> #2
            replace_linear_with_lora(module, rank, alpha)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Replaces the Linear layer with LinearWithLoRA
     <br/>#2 Recursively applies the same function to child modules
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>We have now implemented all the necessary code to replace the <code>Linear</code> layers in the <code>GPTModel</code> with the newly developed <code>LinearWithLoRA</code> layers for parameter-efficient fine-tuning. Next, we will apply the <code>LinearWithLoRA</code> upgrade to all <code>Linear</code> layers found in the multihead attention, feed-forward modules, and the output layer of the <code>GPTModel</code>, as shown in figure E.4.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p69">  
   <img alt="figure" src="../Images/E-4.png" width="1014" height="1024"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure E.4</span> The architecture of the GPT model. It highlights the parts of the model where <code>Linear</code> layers are upgraded to <code>LinearWithLoRA</code> layers for parameter-efficient fine-tuning.</h5>
  </div> 
  <div class="readable-text" id="p70"> 
   <p>Before we apply the <code>LinearWithLoRA</code> layer upgrades, we first freeze the original model parameters:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p71"> 
   <div class="code-area-container"> 
    <pre class="code-area">total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total trainable parameters before: {total_params:,}")

for param in model.parameters():
    param.requires_grad = False
total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total trainable parameters after: {total_params:,}")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>Now, we can see that none of the 124 million model parameters are trainable:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p73"> 
   <div class="code-area-container"> 
    <pre class="code-area">Total trainable parameters before: 124,441,346
Total trainable parameters after: 0</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p74"> 
   <p>Next, we use the <code>replace_linear_with_lora</code> to replace the <code>Linear</code> layers:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p75"> 
   <div class="code-area-container"> 
    <pre class="code-area">replace_linear_with_lora(model, rank=16, alpha=16)
total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"Total trainable LoRA parameters: {total_params:,}")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p76"> 
   <p>After adding the LoRA layers, the number of trainable parameters is as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p77"> 
   <div class="code-area-container"> 
    <pre class="code-area">Total trainable LoRA parameters: 2,666,528</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p78"> 
   <p>As we can see, we reduced the number of trainable parameters by almost 50× when using LoRA. A <code>rank</code> and <code>alpha</code> of 16 are good default choices, but it is also common to increase the rank parameter, which in turn increases the number of trainable parameters. Alpha is usually chosen to be half, double, or equal to the rank.</p> 
  </div> 
  <div class="readable-text intended-text" id="p79"> 
   <p>Let’s verify that the layers have been modified as intended by printing the model architecture:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p80"> 
   <div class="code-area-container"> 
    <pre class="code-area">device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
print(model)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p82"> 
   <div class="code-area-container"> 
    <pre class="code-area">GPTModel(
  (tok_emb): Embedding(50257, 768)
  (pos_emb): Embedding(1024, 768)
  (drop_emb): Dropout(p=0.0, inplace=False)
  (trf_blocks): Sequential(
    ...
    (11): TransformerBlock(
      (att): MultiHeadAttention(
        (W_query): LinearWithLoRA(
          (linear): Linear(in_features=768, out_features=768, bias=True)
          (lora): LoRALayer()
        )
        (W_key): LinearWithLoRA(
          (linear): Linear(in_features=768, out_features=768, bias=True)
          (lora): LoRALayer()
        )
        (W_value): LinearWithLoRA(
          (linear): Linear(in_features=768, out_features=768, bias=True)
          (lora): LoRALayer()
        )
        (out_proj): LinearWithLoRA(
          (linear): Linear(in_features=768, out_features=768, bias=True)
          (lora): LoRALayer()
        )
        (dropout): Dropout(p=0.0, inplace=False)
      )
      (ff): FeedForward(
        (layers): Sequential(
          (0): LinearWithLoRA(
            (linear): Linear(in_features=768, out_features=3072, bias=True)
            (lora): LoRALayer()
          )
          (1): GELU()
          (2): LinearWithLoRA(
            (linear): Linear(in_features=3072, out_features=768, bias=True)
            (lora): LoRALayer()
          )
        )
      )
      (norm1): LayerNorm()
      (norm2): LayerNorm()
      (drop_resid): Dropout(p=0.0, inplace=False)
    )
  )
  (final_norm): LayerNorm()
  (out_head): LinearWithLoRA(
    (linear): Linear(in_features=768, out_features=2, bias=True)
    (lora): LoRALayer()
  )
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>The model now includes the new <code>LinearWithLoRA</code> layers, which themselves consist of the original <code>Linear</code> layers, set to nontrainable, and the new LoRA layers, which we will fine-tune.</p> 
  </div> 
  <div class="readable-text intended-text" id="p84"> 
   <p>Before we begin fine-tuning the model, let’s calculate the initial classification accuracy:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p85"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)

train_accuracy = calc_accuracy_loader(
    train_loader, model, device, num_batches=10
)
val_accuracy = calc_accuracy_loader(
    val_loader, model, device, num_batches=10
)
test_accuracy = calc_accuracy_loader(
    test_loader, model, device, num_batches=10
)

print(f"Training accuracy: {train_accuracy*100:.2f}%")
print(f"Validation accuracy: {val_accuracy*100:.2f}%")
print(f"Test accuracy: {test_accuracy*100:.2f}%")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>The resulting accuracy values are </p> 
  </div> 
  <div class="browsable-container listing-container" id="p87"> 
   <div class="code-area-container"> 
    <pre class="code-area">Training accuracy: 46.25%
Validation accuracy: 45.00%
Test accuracy: 48.75%</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>These accuracy values are identical to the values from chapter 6. This result occurs because we initialized the LoRA matrix <em>B</em> with zeros. Consequently, the product of matrices <em>AB</em> results in a zero matrix. This ensures that the multiplication does not alter the original weights since adding zero does not change them.</p> 
  </div> 
  <div class="readable-text intended-text" id="p89"> 
   <p>Now let’s move on to the exciting part—fine-tuning the model using the training function from chapter 6. The training takes about 15 minutes on an M3 MacBook Air laptop and less than half a minute on a V100 or A100 GPU.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p90"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing E.7</span> Fine-tuning a model with LoRA layers</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import time
from chapter06 import train_classifier_simple

start_time = time.time()
torch.manual_seed(123)
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)

num_epochs = 5
train_losses, val_losses, train_accs, val_accs, examples_seen = \
    train_classifier_simple(
        model, train_loader, val_loader, optimizer, device,
        num_epochs=num_epochs, eval_freq=50, eval_iter=5,
        tokenizer=tokenizer
    )

end_time = time.time()
execution_time_minutes = (end_time - start_time) / 60
print(f"Training completed in {execution_time_minutes:.2f} minutes.")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>The output we see during the training is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p92"> 
   <div class="code-area-container"> 
    <pre class="code-area">Ep 1 (Step 000000): Train loss 3.820, Val loss 3.462 
Ep 1 (Step 000050): Train loss 0.396, Val loss 0.364 
Ep 1 (Step 000100): Train loss 0.111, Val loss 0.229 
Training accuracy: 97.50% | Validation accuracy: 95.00% 
Ep 2 (Step 000150): Train loss 0.135, Val loss 0.073 
Ep 2 (Step 000200): Train loss 0.008, Val loss 0.052 
Ep 2 (Step 000250): Train loss 0.021, Val loss 0.179 
Training accuracy: 97.50% | Validation accuracy: 97.50%
Ep 3 (Step 000300): Train loss 0.096, Val loss 0.080 
Ep 3 (Step 000350): Train loss 0.010, Val loss 0.116 
Training accuracy: 97.50% | Validation accuracy: 95.00% 
Ep 4 (Step 000400): Train loss 0.003, Val loss 0.151 
Ep 4 (Step 000450): Train loss 0.008, Val loss 0.077 
Ep 4 (Step 000500): Train loss 0.001, Val loss 0.147 
Training accuracy: 100.00% | Validation accuracy: 97.50%
Ep 5 (Step 000550): Train loss 0.007, Val loss 0.094 
Ep 5 (Step 000600): Train loss 0.000, Val loss 0.056 
Training accuracy: 100.00% | Validation accuracy: 97.50%

Training completed in 12.10 minutes.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>Training the model with LoRA took longer than training it without LoRA (see chapter 6) because the LoRA layers introduce an additional computation during the forward pass. However, for larger models, where backpropagation becomes more costly, models typically train faster with LoRA than without it.</p> 
  </div> 
  <div class="readable-text intended-text" id="p94"> 
   <p>As we can see, the model received perfect training and very high validation accuracy. Let’s also visualize the loss curves to better see whether the training has converged:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p95"> 
   <div class="code-area-container"> 
    <pre class="code-area">from chapter06 import plot_values

epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))
examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))

plot_values(
    epochs_tensor, examples_seen_tensor, 
    train_losses, val_losses, label="loss"
)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>Figure E.5 plots the results.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p97">  
   <img alt="figure" src="../Images/E-5.png" width="542" height="309"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure E.5</span> The training and validation loss curves over six epochs for a machine learning model. Initially, both training and validation loss decrease sharply and then they level off, indicating the model is converging, which means that it is not expected to improve noticeably with further training.</h5>
  </div> 
  <div class="readable-text" id="p98"> 
   <p>In addition to evaluating the model based on the loss curves, let’s also calculate the accuracies on the full training, validation, and test set (during the training, we approximated the training and validation set accuracies from five batches via the <code>eval_iter=5</code> setting):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p99"> 
   <div class="code-area-container"> 
    <pre class="code-area">train_accuracy = calc_accuracy_loader(train_loader, model, device)
val_accuracy = calc_accuracy_loader(val_loader, model, device)
test_accuracy = calc_accuracy_loader(test_loader, model, device)

print(f"Training accuracy: {train_accuracy*100:.2f}%")
print(f"Validation accuracy: {val_accuracy*100:.2f}%")
print(f"Test accuracy: {test_accuracy*100:.2f}%")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>The resulting accuracy values are </p> 
  </div> 
  <div class="browsable-container listing-container" id="p101"> 
   <div class="code-area-container"> 
    <pre class="code-area">Training accuracy: 100.00%
Validation accuracy: 96.64%
Test accuracy: 98.00%</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>These results show that the model performs well across training, validation, and test datasets. With a training accuracy of 100%, the model has perfectly learned the training data. However, the slightly lower validation and test accuracies (96.64% and 97.33%, respectively) suggest a small degree of overfitting, as the model does not generalize quite as well on unseen data compared to the training set. Overall, the results are very impressive, considering we fine-tuned only a relatively small number of model weights (2.7 million LoRA weights instead of the original 124 million model weights).</p> 
  </div>
 </div></div>
<div id="book-content"><div id="sbo-rt-content"><div class="readable-text">
   <h1>index</h1>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">SYMBOLS</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p263">124M parameter</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p126">\[EOS] (end of sequence) token</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p93">.reshape method</a>, <a href="../Text/appendix-a.html#p101">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p78">.to() method</a>, <a href="../Text/appendix-a.html#p293">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p254">.weight attribute</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p221">.eval() mode</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p201">__getitem__ method</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p127">\[PAD] (padding) token</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p102">.T method</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p130">.backward() method</a>, <a href="../Text/appendix-d.html#p45">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p317">%timeit command</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p106">.matmul method</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p264">04_preference-tuning-with-dpo folder</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p131">355M parameter</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p125">\[BOS] (beginning of sequence) token</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p96">\&lt;|unk|&gt; tokens</a>, <a href="../Text/chapter-2.html#p98">2nd</a>, <a href="../Text/chapter-2.html#p100">3rd</a>, <a href="../Text/chapter-2.html#p109">4th</a>, <a href="../Text/chapter-2.html#p147">5th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p293">.view method</a>, <a href="../Text/chapter-3.html#p294">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p172">__init__ constructor</a>, <a href="../Text/chapter-4.html#p170">2nd</a>, <a href="../Text/appendix-a.html#p144">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p89">.shape attribute</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p110">@ operator</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p202">__len__ method</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p146">\&lt;|endoftext|&gt; token</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p242">.pth extension</a></p>
  </div>
  <div class="readable-text">
   <p>&lt;i&gt;Dolma\</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-1.html#p64">An Open Corpus of Three Trillion Tokens for LLM Pretraining Research&lt;/&gt; (Soldaini et al.)</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p253">== comparison operator</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">A</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p267">arXiv</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p172">Alpaca dataset</a>, <a href="../Text/appendix-b.html#p88">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p43">argmax function</a>, <a href="../Text/chapter-5.html#p45">2nd</a>, <a href="../Text/chapter-5.html#p189">3rd</a>, <a href="../Text/chapter-5.html#p195">4th</a>, <a href="../Text/chapter-6.html#p146">5th</a>, <a href="../Text/appendix-a.html#p245">6th</a>, <a href="../Text/appendix-a.html#p249">7th</a></p>
  </div>
  <div class="readable-text">
   <p>attention mechanisms</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-3.html#p1">coding</a>, <a href="../Text/chapter-3.html#p23">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-3.html#p13">problem with modeling long sequences</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p46">attention scores</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p14">AI (artificial intelligence)</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p129">autograd engine</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-e.html#p54">alpha scaling factor</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-1.html#p72">autoregressive model</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p116">attention weights, computing step by step</a>, <a href="../Text/chapter-3.html#p163">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p173">attn_scores</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p270">Axolotl</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-c.html#p114">allowed_max_length</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p156">AdamW optimizer</a>, <a href="../Text/appendix-b.html#p59">2nd</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">B</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p24">Bahdanau attention mechanism</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p63">backpropagation</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-1.html#p48">BERT (bidirectional encoder representations from transformers)</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p129">BPE (byte pair encoding)</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p173">batch_size</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">C</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p262">compute_accuracy function</a>, <a href="../Text/appendix-a.html#p264">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-6.html#p137">causal attention mask</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-d.html#p34">clip_grad_norm_ function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p130">calc_loss_loader function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p77">cross_entropy function</a>, <a href="../Text/chapter-5.html#p80">2nd</a>, <a href="../Text/chapter-5.html#p91">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p187">conversational performance</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p67">custom_collate_draft_1</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p78">custom_collate_draft_2</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-6.html#p157">calc_accuracy_loader function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p132">calc_loss_batch function</a>, <a href="../Text/chapter-6.html#p165">2nd</a>, <a href="../Text/chapter-6.html#p167">3rd</a></p>
  </div>
  <div class="readable-text">
   <p>classification</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-1.html#p41">tasks</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p112">custom_collate_fn function</a>, <a href="../Text/appendix-c.html#p104">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-6.html#p202">classify_review function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p23">context_length</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p150">cfg dictionary</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p62">computing gradients</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p41">context vectors</a>, <a href="../Text/chapter-3.html#p106">2nd</a>, <a href="../Text/chapter-3.html#p280">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p250">CausalAttention class</a>, <a href="../Text/chapter-3.html#p258">2nd</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">D</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p325">DistributedSampler</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p65">dim parameter</a>, <a href="../Text/chapter-4.html#p65">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p181">Dataset class</a>, <a href="../Text/chapter-6.html#p59">2nd</a>, <a href="../Text/appendix-a.html#p193">3rd</a>, <a href="../Text/appendix-a.html#p194">4th</a>, <a href="../Text/appendix-a.html#p200">5th</a>, <a href="../Text/appendix-a.html#p206">6th</a>, <a href="../Text/appendix-a.html#p223">7th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p53">DataLoader class</a>, <a href="../Text/chapter-7.html#p118">2nd</a></p>
  </div>
  <div class="readable-text">
   <p>datasets</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-7.html#p19">downloading</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p261">download_and_load_gpt2 function</a>, <a href="../Text/chapter-5.html#p274">2nd</a>, <a href="../Text/chapter-6.html#p88">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p37">DummyGPTClass</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p333">DistributedDataParallel class</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p35">DummyLayerNorm</a>, <a href="../Text/chapter-4.html#p48">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-4.html#p52">placeholder</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p29">DummyGPTModel</a>, <a href="../Text/chapter-4.html#p31">2nd</a>, <a href="../Text/chapter-4.html#p33">3rd</a>, <a href="../Text/chapter-4.html#p42">4th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p18">deep learning</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p53">dot products</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p322">DDP (DistributedDataParallel) strategy</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p115">device variable</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p142">decode method</a>, <a href="../Text/chapter-2.html#p152">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-6.html#p49">data loaders</a>, <a href="../Text/chapter-6.html#p82">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-c.html#p14">code for</a></p>
  </div>
  <div class="readable-text">
   <p>dropout</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-3.html#p234">defined</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p27">drop_rate</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p213">drop_last parameter</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p163">DummyTransformerBlock</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p22">data list</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p340">ddp_setup function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p312">d_out argument</a>, <a href="../Text/appendix-c.html#p27">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-6.html#p26">DataFrame</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">E</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p78">eps variable</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p150">evaluate_model function</a>, <a href="../Text/chapter-5.html#p150">2nd</a>, <a href="../Text/chapter-5.html#p154">3rd</a>, <a href="../Text/chapter-6.html#p179">4th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p239">embedding size</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-1.html#p77">emergent behavior</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p15">encoder</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p76">encode method</a>, <a href="../Text/chapter-2.html#p138">2nd</a>, <a href="../Text/chapter-2.html#p175">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p24">emb_dim</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-6.html#p196">eval_iter value</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">F</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-d.html#p46">find_highest_gradient function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p188">first_batch variable</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p106">FeedForward module</a>, <a href="../Text/chapter-4.html#p108">2nd</a>, <a href="../Text/chapter-4.html#p112">3rd</a>, <a href="../Text/chapter-4.html#p146">4th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p35">format_input function</a>, <a href="../Text/chapter-7.html#p37">2nd</a>, <a href="../Text/chapter-7.html#p41">3rd</a>, <a href="../Text/chapter-7.html#p230">4th</a>, <a href="../Text/appendix-c.html#p94">5th</a>, <a href="../Text/appendix-c.html#p96">6th</a></p>
  </div>
  <div class="readable-text">
   <p>fine-tuning</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-7.html#p1">LLMs, to follow instructions</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-6.html#p11">categories of</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-6.html#p1">for classification</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p34">forward method</a>, <a href="../Text/chapter-4.html#p121">2nd</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">G</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p152">generate_and_print_sample function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-b.html#p41">GELU (Gaussian error linear unit)</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-4.html#p85">activation function</a>, <a href="../Text/chapter-4.html#p123">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p169">GPTModel</a>, <a href="../Text/chapter-4.html#p192">2nd</a>, <a href="../Text/chapter-4.html#p195">3rd</a>, <a href="../Text/chapter-4.html#p199">4th</a>, <a href="../Text/chapter-5.html#p31">5th</a>, <a href="../Text/appendix-e.html#p66">6th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-4.html#p202">class</a>, <a href="../Text/chapter-6.html#p90">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-5.html#p114">code</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-5.html#p18">instance</a>, <a href="../Text/chapter-5.html#p238">2nd</a>, <a href="../Text/chapter-5.html#p286">3rd</a>, <a href="../Text/chapter-5.html#p300">4th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-1.html#p48">GPT (Generative Pre-trained Transformer)</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-1.html#p69">architecture</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-4.html#p162">coding</a>, <a href="../Text/chapter-4.html#p200">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-4.html#p1">implementing from scratch to generate text</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p180">grad_fn value</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p258">gpt_download.py Python module</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p21">GPT_CONFIG_124M dictionary</a>, <a href="../Text/chapter-4.html#p153">2nd</a>, <a href="../Text/chapter-4.html#p164">3rd</a>, <a href="../Text/chapter-4.html#p173">4th</a>, <a href="../Text/chapter-4.html#p231">5th</a>, <a href="../Text/chapter-5.html#p16">6th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p11">generative text models, evaluating</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-1.html#p16">GenAI (generative AI)</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p200">gpt2-medium355M-sft.pth file</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p180">GPTDatasetV1 class</a>, <a href="../Text/chapter-2.html#p182">2nd</a>, <a href="../Text/chapter-2.html#p184">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p213">generate_text_simple function</a>, <a href="../Text/chapter-4.html#p215">2nd</a>, <a href="../Text/chapter-4.html#p217">3rd</a>, <a href="../Text/chapter-5.html#p42">4th</a>, <a href="../Text/chapter-5.html#p181">5th</a>, <a href="../Text/chapter-5.html#p186">6th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p209">GPT-4</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p18">GPT-2</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-7.html#p158">model</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-6.html#p54">tokenizer</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-1.html#p62">GPT-3</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p244">generate_model_scores function</a>, <a href="../Text/chapter-7.html#p246">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p48">Google Colab</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p235">generate function</a>, <a href="../Text/chapter-5.html#p294">2nd</a>, <a href="../Text/chapter-7.html#p142">3rd</a>, <a href="../Text/chapter-7.html#p177">4th</a>, <a href="../Text/chapter-7.html#p179">5th</a>, <a href="../Text/chapter-7.html#p192">6th</a>, <a href="../Text/appendix-c.html#p60">7th</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">I</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p335">init_process_group function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p10">instruction dataset</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p216">information leakage</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p261">input_embeddings</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p55">InstructionDataset class</a>, <a href="../Text/appendix-c.html#p102">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-1.html#p41">instruction fine-tuning</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-7.html#p110">instruction following, creating data loaders for instruction dataset</a>, <a href="../Text/chapter-7.html#p126">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-7.html#p12">overview</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-7.html#p26">’instruction’ object</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">K</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p64">keepdim parameter</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">L</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p85">logits tensor</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-e.html#p70">LinearWithLoRA layer</a>, <a href="../Text/appendix-e.html#p83">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-e.html#p59">LoRALayer class</a>, <a href="../Text/appendix-e.html#p64">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p26">loss metric</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p7">LLMs (large language models)</a>, <a href="../Text/chapter-2.html#p10">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-1.html#p24">applications of</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-1.html#p31">building and using</a>, <a href="../Text/chapter-1.html#p42">2nd</a>, <a href="../Text/chapter-1.html#p79">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-4.html#p11">coding architecture</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-3.html#p194">coding attention mechanisms, causal attention mechanism</a>, <a href="../Text/chapter-3.html#p262">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-7.html#p149">fine-tuning</a>, <a href="../Text/chapter-7.html#p204">2nd</a>, <a href="../Text/appendix-b.html#p72">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-6.html#p100">fine-tuning for classification</a>, <a href="../Text/chapter-6.html#p140">2nd</a>, <a href="../Text/chapter-6.html#p144">3rd</a>, <a href="../Text/chapter-6.html#p173">4th</a>, <a href="../Text/chapter-6.html#p202">5th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-7.html#p128">instruction fine-tuning, loading pretrained LLMs</a>, <a href="../Text/chapter-7.html#p147">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-1.html#p1">overview of</a>, <a href="../Text/chapter-1.html#p13">2nd</a>, <a href="../Text/chapter-1.html#p22">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-1.html#p57">utilizing large datasets</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-e.html#p60">Linear layers</a>, <a href="../Text/appendix-e.html#p68">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p81">LayerNorm</a>, <a href="../Text/chapter-4.html#p151">2nd</a>, <a href="../Text/chapter-4.html#p171">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-b.html#p90">LIMA dataset</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p51">layer normalization</a>, <a href="../Text/chapter-4.html#p88">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p248">load_state_dict method</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p288">load_weights_into_gpt function</a>, <a href="../Text/chapter-5.html#p290">2nd</a>, <a href="../Text/chapter-5.html#p291">3rd</a>, <a href="../Text/chapter-5.html#p292">4th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p128">loss.backward() function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-e.html#p62">Linear layer weights</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p206">Llama 3 model</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p107">LLama 2 model</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p259">LoRA (low-rank adaptation)</a>, <a href="../Text/appendix-e.html#p2">2nd</a>, <a href="../Text/appendix-e.html#p4">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-e.html#p25">parameter-efficient fine-tuning</a>, <a href="../Text/appendix-e.html#p41">2nd</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">M</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p338">main function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p115">max_length</a>, <a href="../Text/chapter-6.html#p62">2nd</a>, <a href="../Text/appendix-c.html#p82">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p245">model.eval() function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p290">MultiHeadAttention class</a>, <a href="../Text/chapter-3.html#p309">2nd</a>, <a href="../Text/chapter-3.html#p310">3rd</a>, <a href="../Text/chapter-3.html#p314">4th</a>, <a href="../Text/chapter-3.html#p317">5th</a>, <a href="../Text/appendix-b.html#p32">6th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p233">model.train() setting</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p272">MultiHeadAttentionWrapper class</a>, <a href="../Text/chapter-3.html#p274">2nd</a>, <a href="../Text/chapter-3.html#p282">3rd</a>, <a href="../Text/chapter-3.html#p283">4th</a>, <a href="../Text/chapter-3.html#p286">5th</a>, <a href="../Text/chapter-3.html#p287">6th</a>, <a href="../Text/chapter-3.html#p291">7th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p15">machine learning</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p249">multi-head attention</a>, <a href="../Text/chapter-3.html#p265">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p143">Module base class</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p334">multiprocessing submodule</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p194">masked attention</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p191">multinomial function</a>, <a href="../Text/chapter-5.html#p202">2nd</a>, <a href="../Text/chapter-5.html#p203">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p312">macOS</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p198">model_response</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-b.html#p21">minbpe repository</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p278">model_configs table</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p113">mps device</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">N</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p284">NEW_CONFIG dictionary</a></p>
  </div>
  <div class="readable-text">
   <p>neural networks</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-4.html#p92">implementing feed forward network with GELU activations</a>, <a href="../Text/chapter-4.html#p115">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p181">nn.Linear layers</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p25">n_heads</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p178">numel() method</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p295">num_heads dimension</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">O</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-6.html#p103">output layer nodes</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p216">ollama run llama3 command</a>, <a href="../Text/chapter-7.html#p220">2nd</a>, <a href="../Text/chapter-7.html#p225">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p213">ollama serve command</a>, <a href="../Text/chapter-7.html#p214">2nd</a>, <a href="../Text/chapter-7.html#p215">3rd</a>, <a href="../Text/chapter-7.html#p228">4th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p235">optimizer.zero_grad() method</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p207">Ollama application</a>, <a href="../Text/chapter-7.html#p219">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-c.html#p106">Ollama Llama 3 method</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p232">ollama run command</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">P</p>
  </div>
  <div class="readable-text">
   <p>PyTorch</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-a.html#p41">and Torch</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-a.html#p124">automatic differentiation</a>, <a href="../Text/appendix-a.html#p139">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-a.html#p116">computation graphs</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-7.html#p45">data loaders</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-e.html#p30">dataset objects</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-a.html#p192">efficient data loaders</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-a.html#p141">implementing multilayer neural networks</a>, <a href="../Text/appendix-a.html#p190">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-a.html#p24">installing</a>, <a href="../Text/appendix-a.html#p51">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-5.html#p237">loading and saving model weights in</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-a.html#p282">optimizing training performance with GPUs</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-a.html#p2">overview</a>, <a href="../Text/appendix-a.html#p6">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-a.html#p59">with a NumPy-like API</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p132">pip installer</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-b.html#p93">Phi-3 model</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p131">print_gradients function</a>, <a href="../Text/chapter-4.html#p135">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-6.html#p192">plot_values function</a></p>
  </div>
  <div class="readable-text">
   <p>parameters</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-c.html#p34">calculating</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p96">perplexity</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p127">partial derivatives</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p59">print statement</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p166">plot_losses function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p26">Python version</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-b.html#p96">Prometheus model</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p30">prompt styles</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-1.html#p39">pretraining</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-5.html#p102">calculating training and validation set losses</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-5.html#p1">on unlabeled data</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-5.html#p143">training LLMs</a>, <a href="../Text/chapter-5.html#p170">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p205">print_sampled_tokens function</a>, <a href="../Text/appendix-c.html#p51">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p254">pos_embeddings</a>, <a href="../Text/chapter-2.html#p257">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-b.html#p101">preference fine-tuning</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">Q</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p28">qkv_bias</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p235">query_llama function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p239">query_model function</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">R</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p175">responses, extracting and saving</a>, <a href="../Text/chapter-7.html#p202">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p36">re library</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-b.html#p40">RMSNorm</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p58">ReLU (rectified linear unit)</a>, <a href="../Text/chapter-4.html#p93">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p37">re.split command</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-e.html#p74">replace_linear_with_lora function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-1.html#p37">raw text</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p17">retrieval-augmented generation</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p16">RNNs (recurrent neural networks)</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-6.html#p43">random_split function</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">S</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p118">shortcut connections</a>, <a href="../Text/chapter-4.html#p142">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p274">saving and loading models</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p78">SimpleTokenizerV1 class</a>, <a href="../Text/chapter-2.html#p80">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p337">spawn function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p152">Sequential class</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p183">SelfAttention_v2 class</a>, <a href="../Text/chapter-3.html#p189">2nd</a>, <a href="../Text/chapter-3.html#p190">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p68">softmax_naive function</a>, <a href="../Text/chapter-3.html#p70">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p72">sci_mode parameter</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p244">set_printoptions method</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p229">SGD (stochastic gradient descent)</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p171">SelfAttention_v1 class</a>, <a href="../Text/chapter-3.html#p187">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p186">softmax function</a>, <a href="../Text/appendix-a.html#p234">2nd</a>, <a href="../Text/appendix-a.html#p240">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p257">self.register_buffer() call</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p246">state_dict</a>, <a href="../Text/appendix-a.html#p276">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-6.html#p58">SpamDataset class</a>, <a href="../Text/chapter-6.html#p60">2nd</a>, <a href="../Text/chapter-6.html#p65">3rd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p118">special context tokens</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p193">stride setting</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p308">self.out_proj layer</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p19">supervised learning</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-6.html#p175">supervised data, fine-tuning model on</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p145">strip() function</a></p>
  </div>
  <div class="readable-text">
   <p>supervised instruction fine-tuning</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-7.html#p17">preparing dataset for</a>, <a href="../Text/chapter-7.html#p49">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p270">settings dictionary</a>, <a href="../Text/chapter-5.html#p276">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p31">self-attention mechanism</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-3.html#p79">computing attention weights for all input tokens</a>, <a href="../Text/chapter-3.html#p108">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-3.html#p110">implementing with trainable weights</a>, <a href="../Text/chapter-3.html#p192">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-3.html#p37">without trainable weights</a>, <a href="../Text/chapter-3.html#p77">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p269">single-head attention, stacking multiple layers</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p96">SimpleTokenizerV2 class</a>, <a href="../Text/chapter-2.html#p109">2nd</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">T</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p225">text generation function, modifying</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p118">train_ratio</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p1">text data</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-2.html#p96">adding special context tokens</a>, <a href="../Text/chapter-2.html#p128">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-2.html#p208">creating token embeddings</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-2.html#p155">sliding window</a>, <a href="../Text/chapter-2.html#p202">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-2.html#p131">tokenization, byte pair encoding</a>, <a href="../Text/chapter-2.html#p153">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p240">torch.save function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p64">token IDs</a>, <a href="../Text/chapter-2.html#p94">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p11">tensor library</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p149">TransformerBlock class</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p241">token_embedding_layer</a>, <a href="../Text/chapter-2.html#p252">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p204">token embeddings</a>, <a href="../Text/chapter-2.html#p229">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-c.html#p66">train_simple_function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p197">ToyDataset class</a>, <a href="../Text/appendix-a.html#p199">2nd</a></p>
  </div>
  <div class="readable-text">
   <p>training function</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-d.html#p2">enhancing</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-d.html#p55">modified</a>, <a href="../Text/appendix-d.html#p63">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p120">train_data subset</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p204">tril function</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p25">tokenizing text</a>, <a href="../Text/chapter-2.html#p61">2nd</a></p>
  </div>
  <div class="readable-text">
   <p>training, optimizing performance with GPUs</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-a.html#p284">PyTorch computations on GPU devices</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-a.html#p342">selecting available GPUs on multi-GPU machine</a>, <a href="../Text/appendix-a.html#p357">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-a.html#p304">single-GPU training</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-a.html#p320">training with multiple GPUs</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p208">test_loader</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p212">train_loader</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p257">torch.sum method</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p225">training loops</a>, <a href="../Text/appendix-a.html#p271">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-d.html#p24">cosine decay</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-d.html#p33">gradient clipping</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-d.html#p10">learning rate warmup</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-6.html#p181">train_classifier_simple function</a>, <a href="../Text/chapter-6.html#p194">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p51">training batches, organizing data into</a>, <a href="../Text/chapter-7.html#p108">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p204">text generation</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-5.html#p14">using GPT to generate text</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p207">top-k sampling</a>, <a href="../Text/chapter-5.html#p208">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-d.html#p7">text_data</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-1.html#p15">transformer architecture</a>, <a href="../Text/chapter-1.html#p44">2nd</a>, <a href="../Text/chapter-1.html#p55">3rd</a>, <a href="../Text/chapter-3.html#p26">4th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p181">temperature scaling</a>, <a href="../Text/chapter-5.html#p196">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p147">train_model_simple function</a>, <a href="../Text/chapter-5.html#p149">2nd</a>, <a href="../Text/chapter-5.html#p157">3rd</a>, <a href="../Text/chapter-5.html#p245">4th</a>, <a href="../Text/chapter-5.html#p251">5th</a>, <a href="../Text/chapter-6.html#p177">6th</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p67">tensor2d</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p67">tensor3d</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p182">torch.no_grad() context manager</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-7.html#p191">test_set dictionary</a>, <a href="../Text/chapter-7.html#p196">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p59">tensors</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-a.html#p84">common tensor operations</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/appendix-a.html#p69">tensor data types</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p158">torch.nn.Linear layers</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p13">transformer blocks</a>, <a href="../Text/chapter-6.html#p106">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-4.html#p144">connecting attention and linear layers in</a>, <a href="../Text/chapter-4.html#p159">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p29">text generation loss</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p31">torchvision library</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">U</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p80">unbiased parameter</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p172">unlabeled data, decoding strategies to control randomness</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">V</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-5.html#p117">variable-length inputs</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-4.html#p22">vocab_size</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-d.html#p35">v vector</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p66">vectors</a>, <a href="../Text/appendix-a.html#p114">2nd</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">W</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p292">W&lt;.Subscript&gt;q&lt;/&gt; matrix</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p131">weight parameters</a>, <a href="../Text/chapter-5.html#p9">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p13">word embeddings</a>, <a href="../Text/chapter-2.html#p23">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-6.html#p200">weight_decay parameter</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p18">Word2Vec</a></p>
  </div>
  <div class="readable-text">
   <p>weights</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-6.html#p84">initializing model with pretrained weights</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 1em"><a href="../Text/chapter-5.html#p253">loading pretrained weights from OpenAI</a>, <a href="../Text/chapter-5.html#p302">2nd</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-2.html#p231">word positions, encoding</a></p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/chapter-3.html#p285">weight splits</a></p>
  </div>
  <div class="readable-text">
   <p style="font-size: 110%;">X</p>
  </div>
  <div class=" readable-text">
   <p style="margin-left: 0em"><a href="../Text/appendix-a.html#p178">X training example</a></p>
  </div>
 </div></div></body></html>