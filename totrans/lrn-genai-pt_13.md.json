["```py\nimport torch\nimport torch.nn as nn\n\ntorch.manual_seed(42)\nx=torch.randn((1,4,1600))                        ①\nc_attn=nn.Linear(1600,1600*3)                    ②\nB,T,C=x.size()\nq,k,v=c_attn(x).split(1600,dim=2)                ③\nprint(f\"the shape of Q vector is {q.size()}\")\nprint(f\"the shape of K vector is {k.size()}\")\nprint(f\"the shape of V vector is {v.size()}\")    ④\n```", "```py\nthe shape of Q vector is torch.Size([1, 4, 1600])\nthe shape of K vector is torch.Size([1, 4, 1600])\nthe shape of V vector is torch.Size([1, 4, 1600])\n```", "```py\nhs=C//25\nk = k.view(B, T, 25, hs).transpose(1, 2) \nq = q.view(B, T, 25, hs).transpose(1, 2) \nv = v.view(B, T, 25, hs).transpose(1, 2)         ①\nprint(f\"the shape of Q vector is {q.size()}\")\nprint(f\"the shape of K vector is {k.size()}\")\nprint(f\"the shape of V vector is {v.size()}\")    ②\n```", "```py\nthe shape of Q vector is torch.Size([1, 25, 4, 64])\nthe shape of K vector is torch.Size([1, 25, 4, 64])\nthe shape of V vector is torch.Size([1, 25, 4, 64])\n```", "```py\nimport math\nscaled_att = (q @ k.transpose(-2, -1)) *\\\n            (1.0 / math.sqrt(k.size(-1)))\nprint(scaled_att[0,0])\n```", "```py\ntensor([[ 0.2334,  0.1385, -0.1305,  0.2664],\n        [ 0.2916,  0.1044,  0.0095,  0.0993],\n        [ 0.8250,  0.2454,  0.0214,  0.8667],\n        [-0.1557,  0.2034,  0.2172, -0.2740]], grad_fn=<SelectBackward0>)\n```", "```py\nmask=torch.tril(torch.ones(4,4))              ①\nprint(mask)\nmasked_scaled_att=scaled_att.masked_fill(\\\n    mask == 0, float('-inf'))                 ②\nprint(masked_scaled_att[0,0])\n```", "```py\ntensor([[1., 0., 0., 0.],\n        [1., 1., 0., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 1.]])\ntensor([[ 0.2334,    -inf,    -inf,    -inf],\n        [ 0.2916,  0.1044,    -inf,    -inf],\n        [ 0.8250,  0.2454,  0.0214,    -inf],\n        [-0.1557,  0.2034,  0.2172, -0.2740]], grad_fn=<SelectBackward0>)\n```", "```py\nimport torch.nn.functional as F\natt = F.softmax(masked_scaled_att, dim=-1)\nprint(att[0,0])\n```", "```py\ntensor([[1.0000, 0.0000, 0.0000, 0.0000],\n        [0.5467, 0.4533, 0.0000, 0.0000],\n        [0.4980, 0.2790, 0.2230, 0.0000],\n        [0.2095, 0.3001, 0.3042, 0.1862]], grad_fn=<SelectBackward0>)\n```", "```py\ny=att@v\ny = y.transpose(1, 2).contiguous().view(B, T, C)\nprint(y.shape)\n```", "```py\ntorch.Size([1, 4, 1600])\n```", "```py\nfrom utils.bpe import get_encoder\n\nexample=\"This is the original text.\"                       ①\nbpe_encoder=get_encoder()                                  ②\nresponse=bpe_encoder.encode_and_show_work(example)\nprint(response[\"tokens\"])                                  ③\n```", "```py\n['This', ' is', ' the', ' original', ' text', '.']\n```", "```py\nprint(response['bpe_idx'])\n```", "```py\n[1212, 318, 262, 2656, 2420, 13]\n```", "```py\nfrom utils.bpe import BPETokenizer \n\ntokenizer = BPETokenizer()                                  ①\nout=tokenizer.decode(torch.LongTensor(response['bpe_idx'])) ②\nprint(out) \n```", "```py\nThis is the original text.\n```", "```py\nclass GELU(nn.Module):\n    def forward(self, x):\n        return 0.5*x*(1.0+torch.tanh(math.sqrt(2.0/math.pi)*\\\n                       (x + 0.044715 * torch.pow(x, 3.0))))\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ngenu=GELU()\ndef relu(x):                                           ①\n    y=torch.zeros(len(x))\n    for i in range(len(x)):\n        if x[i]>0:\n            y[i]=x[i]\n    return y                 \nxs = torch.linspace(-6,6,300)\nys=relu(xs)\ngs=genu(xs)\nfig, ax = plt.subplots(figsize=(6,4),dpi=300)\nplt.xlim(-3,3)\nplt.ylim(-0.5,3.5)\nplt.plot(xs, ys, color='blue', label=\"ReLU\")           ②\nplt.plot(xs, gs, \"--\", color='red', label=\"GELU\")      ③\nplt.legend(fontsize=15)\nplt.xlabel(\"values of x\")\nplt.ylabel(\"values of $ReLU(x)$ and $GELU(x)$\")\nplt.title(\"The ReLU and GELU Activation Functions\")\nplt.show()\n```", "```py\nclass Config():                                       ①\n    def __init__(self):\n        self.n_layer = 48\n        self.n_head = 25\n        self.n_embd = 1600\n        self.vocab_size = 50257\n        self.block_size = 1024 \n        self.embd_pdrop = 0.1 \n        self.resid_pdrop = 0.1 \n        self.attn_pdrop = 0.1                         ②\n\nconfig=Config()                                       ③\n```", "```py\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n        self.register_buffer(\"bias\", torch.tril(torch.ones(\\\n                   config.block_size, config.block_size))\n             .view(1, 1, config.block_size, config.block_size)) ①\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n    def forward(self, x):\n        B, T, C = x.size() \n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)     ②\n        hs = C // self.n_head\n        k = k.view(B, T, self.n_head, hs).transpose(1, 2) \n        q = q.view(B, T, self.n_head, hs).transpose(1, 2) \n        v = v.view(B, T, self.n_head, hs).transpose(1, 2)       ③\n        att = (q @ k.transpose(-2, -1)) *\\\n            (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, \\\n                              float(‚-inf'))\n        att = F.softmax(att, dim=-1)                            ④\n        att = self.attn_dropout(att)\n        y = att @ v \n        y = y.transpose(1, 2).contiguous().view(B, T, C)        ⑤\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n```", "```py\nclass Block(nn.Module):\n    def __init__(self, config):                                 ①\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = nn.ModuleDict(dict(\n            c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd),\n            c_proj = nn.Linear(4 * config.n_embd, config.n_embd),\n            act    = GELU(),\n            dropout = nn.Dropout(config.resid_pdrop),\n        ))\n        m = self.mlp\n        self.mlpf=lambda x:m.dropout(m.c_proj(m.act(m.c_fc(x)))) \n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))                         ②\n        x = x + self.mlpf(self.ln_2(x))                         ③\n        return x\n```", "```py\nclass GPT2XL(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.block_size = config.block_size\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.embd_pdrop),\n            h = nn.ModuleList([Block(config) \n                               for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd),))\n        self.lm_head = nn.Linear(config.n_embd,\n                                 config.vocab_size, bias=False)\n    def forward(self, idx, targets=None):\n        b, t = idx.size()\n        pos = torch.arange(0,t,dtype=torch.long).unsqueeze(0)\n        tok_emb = self.transformer.wte(idx)    \n        pos_emb = self.transformer.wpe(pos)    \n        x = self.transformer.drop(tok_emb + pos_emb)            ①\n        for block in self.transformer.h:\n            x = block(x)                                        ②\n        x = self.transformer.ln_f(x)                            ③\n        logits = self.lm_head(x)                                ④\n        loss = None\n        if targets is not None:\n            loss=F.cross_entropy(logits.view(-1,logits.size(-1)),\n                           targets.view(-1), ignore_index=-1)\n        return logits, loss\n```", "```py\nmodel=GPT2XL(config)\nnum=sum(p.numel() for p in model.transformer.parameters())\nprint(\"number of parameters: %.2fM\" % (num/1e6,))\n```", "```py\nnumber of parameters: 1557.61M\n```", "```py\nprint(model)\n```", "```py\nGPT2XL(\n  (transformer): ModuleDict(\n    (wte): Embedding(50257, 1600)\n    (wpe): Embedding(1024, 1600)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-47): 48 x Block(\n        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n        (mlp): ModuleDict(\n          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n          (act): GELU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n)\n```", "```py\n!pip install transformers\n```", "```py\nfrom transformers import GPT2LMHeadModel\n\nmodel_hf = GPT2LMHeadModel.from_pretrained('gpt2-xl')         ①\nsd_hf = model_hf.state_dict()                                 ②\nprint(model_hf)                                               ③\n```", "```py\nGPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 1600)\n    (wpe): Embedding(1024, 1600)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-47): 48 x GPT2Block(\n        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()                                   ①\n          (c_proj): Conv1D()                                   ①\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()                                     ①\n          (c_proj): Conv1D()                                   ①\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n)\n```", "```py\nprint(model_hf.transformer.h[0].mlp.c_fc.weight.shape)\n```", "```py\ntorch.Size([1600, 6400])\n```", "```py\nprint(model.transformer.h[0].mlp.c_fc.weight.shape)\n```", "```py\ntorch.Size([6400, 1600])\n```", "```py\nkeys = [k for k in sd_hf if not k.endswith('attn.masked_bias')] \n```", "```py\nsd=model.state_dict()\n```", "```py\ntransposed = ['attn.c_attn.weight', 'attn.c_proj.weight',\n              'mlp.c_fc.weight', 'mlp.c_proj.weight']          ①\nfor k in keys:\n    if any(k.endswith(w) for w in transposed):\n        with torch.no_grad():\n            sd[k].copy_(sd_hf[k].t())                          ②\n    else:\n        with torch.no_grad():\n            sd[k].copy_(sd_hf[k])                              ③\n```", "```py\nmodel.eval()\ndef sample(idx, max_new_tokens, temperature=1.0, top_k=None):\n    for _ in range(max_new_tokens):                            ①\n        if idx.size(1) <= config.block_size:\n            idx_cond = idx  \n        else:\n            idx_cond = idx[:, -config.block_size:]\n        logits, _ = model(idx_cond)                            ②\n        logits = logits[:, -1, :] / temperature\n        if top_k is not None:\n            v, _ = torch.topk(logits, top_k)\n            logits[logits < v[:, [-1]]] = -float('Inf')        ③\n        probs = F.softmax(logits, dim=-1)\n        idx_next = torch.multinomial(probs, num_samples=1)\n        if idx_next.item()==tokenizer.encoder.encoder['<|endoftext|>']:\n            break                                              ④\n        idx = torch.cat((idx, idx_next), dim=1)                ⑤\n    return idx\n```", "```py\ndef generate(prompt, max_new_tokens, temperature=1.0,\n             top_k=None):\n    if prompt == '':\n        x=torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]],\n                         dtype=torch.long)                     ①\n    else:\n        x = tokenizer(prompt)                                  ②\n    y = sample(x, max_new_tokens, temperature, top_k)          ③\n    out = tokenizer.decode(y.squeeze())                        ④\n    print(out)\n```", "```py\nprompt=\"\"\ntorch.manual_seed(42)\ngenerate(prompt, max_new_tokens=100, temperature=1.0,\n             top_k=None)\n```", "```py\n<|endoftext|>Feedback from Ham Radio Recalls\n\nI discovered a tune sticking in my head -- I'd heard it mentioned on several occasions, but hadn't investigated further.\n\nThe tune sounded familiar to a tune I'd previously heard on the 550 micro. \nDuring that same time period I've heard other people's receipients drone on\nthe idea of the DSH-94013, notably Kim Weaver's instructions in her \nInterview on Radio Ham; and both Scott Mcystem and Steve Simmons' concepts.\n```", "```py\nprompt=\"I went to the kitchen and\"\nfor i in range(5):\n    torch.manual_seed(i)\n    generate(prompt, max_new_tokens=10, temperature=1.0,\n                 top_k=None)\n```", "```py\nI went to the kitchen and said, you're not going to believe this.\nI went to the kitchen and noticed a female producer open a drawer in which was\nI went to the kitchen and asked who was going to be right there and A\nI went to the kitchen and took a small vial of bourbon and a little\nI went to the kitchen and found the bottle of wine, and poured it into\n```", "```py\nprompt=\"Lexington is the second largest city in the state of Kentucky\"\ntorch.manual_seed(42)\ngenerate(prompt, max_new_tokens=100, temperature=1.0,\n             top_k=None)\n```", "```py\nLexington is the second largest city in the state of Kentucky. It caters to\nthose who want to make everything in tune with being with friends and \nenjoying a jaunt through the down to Earth lifestyle. To do so, they are \nblessed with several venues large and small to fill their every need while \nresiding micro- cozy with nature within the landmarks of the city.\n\nIn a moment we look at ten up and coming suchache music acts from the \nLexington area to draw upon your attention.\n\nLyrikhop\n\nThis Lexington-based group\n```", "```py\ntorch.manual_seed(42)\ngenerate(prompt, max_new_tokens=100, temperature=0.9,\n             top_k=50)  \n```", "```py\nLexington is the second largest city in the state of Kentucky. It is also \nthe state capital. The population of Lexington was 1,731,947 in the 2011 \nCensus. The city is well-known for its many parks, including Arboretum, \nZoo, Aquarium and the Kentucky Science Center, as well as its restaurants, \nsuch as the famous Kentucky Derby Festival.\n\nIn the United States, there are at least 28 counties in this state with a\npopulation of more than 100,000, according to the 2010 census.\n```"]