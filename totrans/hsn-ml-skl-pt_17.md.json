["```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, max_length, embed_dim, dropout=0.1):\n        super().__init__()\n        self.pos_embed = nn.Parameter(torch.randn(max_length, embed_dim) * 0.02)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, X):\n        return self.dropout(X + self.pos_embed[:X.size(1)])\n```", "```py\nclass MultiheadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.1):\n        super().__init__()\n        self.h = num_heads\n        self.d = embed_dim // num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def split_heads(self, X):\n        return X.view(X.size(0), X.size(1), self.h, self.d).transpose(1, 2)\n\n    def forward(self, query, key, value):\n        q = self.split_heads(self.q_proj(query))  # (B, h, Lq, d)\n        k = self.split_heads(self.k_proj(key))  # (B, h, Lk, d)\n        v = self.split_heads(self.v_proj(value))  # (B, h, Lv, d) with Lv=Lk\n        scores = q @ k.transpose(2, 3) / self.d**0.5  # (B, h, Lq, Lk)\n        weights = scores.softmax(dim=-1)  # (B, h, Lq, Lk)\n        Z = self.dropout(weights) @ v  # (B, h, Lq, d)\n        Z = Z.transpose(1, 2)  # (B, Lq, h, d)\n        Z = Z.reshape(Z.size(0), Z.size(1), self.h * self.d)  # (B, Lq, h × d)\n        return (self.out_proj(Z), weights)  # (B, Lq, h × d)\n```", "```py\ndef forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n    [...]  # compute the scores exactly like earlier\n    if attn_mask is not None:\n        scores = scores.masked_fill(attn_mask, -torch.inf)  # (B, h, Lq, Lk)\n    if key_padding_mask is not None:\n        mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  # (B, 1, 1, Lk)\n        scores = scores.masked_fill(mask, -torch.inf)  # (B, h, Lq, Lk)\n    [...]  # compute the weights and the outputs exactly like earlier\n```", "```py\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n        attn, _ = self.self_attn(src, src, src, attn_mask=src_mask,\n                                 key_padding_mask=src_key_padding_mask)\n        Z = self.norm1(src + self.dropout(attn))\n        ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n        return self.norm2(Z + ff)\n```", "```py\nclass TransformerDecoderLayer(nn.Module):\n    [...]  # similar constructor, with 2 MHA, 3 Linear, 3 LayerNorm, 1 Dropout\n    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n        attn1, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n                                  key_padding_mask=tgt_key_padding_mask)\n        Z = self.norm1(tgt + self.dropout(attn1))\n        attn2, _ = self.multihead_attn(Z, memory, memory, attn_mask=memory_mask,\n                                       key_padding_mask=memory_key_padding_mask)\n        Z = self.norm2(Z + self.dropout(attn2))\n        ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n        return self.norm3(Z + ff)\n```", "```py\nclass NmtTransformer(nn.Module):\n    def __init__(self, vocab_size, max_length, embed_dim=512, pad_id=0,\n                 num_heads=8, num_layers=6, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n        self.pos_embed = PositionalEmbedding(max_length, embed_dim, dropout)\n        self.transformer = nn.Transformer(\n            embed_dim, num_heads, num_encoder_layers=num_layers,\n            num_decoder_layers=num_layers, batch_first=True)\n        self.output = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, pair):\n        src_embeds = self.pos_embed(self.embed(pair.src_token_ids))\n        tgt_embeds = self.pos_embed(self.embed(pair.tgt_token_ids))\n        src_pad_mask = ~pair.src_mask.bool()\n        tgt_pad_mask = ~pair.tgt_mask.bool()\n        size = [pair.tgt_token_ids.size(1)] * 2\n        full_mask = torch.full(size, True, device=tgt_pad_mask.device)\n        causal_mask = torch.triu(full_mask, diagonal=1)\n        out_decoder = self.transformer(src_embeds, tgt_embeds,\n                                       src_key_padding_mask=src_pad_mask,\n                                       memory_key_padding_mask=src_pad_mask,\n                                       tgt_mask=causal_mask, tgt_is_causal=True,\n                                       tgt_key_padding_mask=tgt_pad_mask)\n        return self.output(out_decoder).permute(0, 2, 1)\n```", "```py\nnmt_tr_model = NmtTransformer(vocab_size, max_length, embed_dim=128, pad_id=0,\n                              num_heads=4, num_layers=2, dropout=0.1).to(device)\n[...]  # train this model exactly like the encoder-decoder in Chapter 14\n```", "```py\n>>> nmt_tr_model.eval()\n>>> translate(nmt_tr_model,\"I like to play soccer with my friends at the beach\")\n' Me gusta jugar al fútbol con mis amigos en la playa . </s>'\n```", "```py\nfrom transformers import BertConfig, BertForMaskedLM, BertTokenizerFast\n\nbert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\nconfig = BertConfig(  # adapt to training budget, and dataset size & complexity\n    vocab_size=bert_tokenizer.vocab_size, hidden_size=128, num_hidden_layers=2,\n    num_attention_heads=4, intermediate_size=512, max_position_embeddings=128)\nbert = BertForMaskedLM(config)\n```", "```py\nfrom datasets import load_dataset\n\ndef tokenize(example, tokenizer=bert_tokenizer):\n    return tokenizer(example[\"text\"], truncation=True, max_length=128,\n                     padding=\"max_length\")\n\nmlm_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\nmlm_dataset = mlm_dataset.map(tokenize, batched=True)\n```", "```py\nfrom transformers import Trainer, TrainingArguments\nfrom transformers import DataCollatorForLanguageModeling\n\nargs = TrainingArguments(output_dir=\"./my_bert\", num_train_epochs=5,\n                         per_device_train_batch_size=16)\nmlm_collator = DataCollatorForLanguageModeling(bert_tokenizer, mlm=True,\n                                               mlm_probability=0.15)\ntrainer = Trainer(model=bert, args=args, train_dataset=mlm_dataset,\n                  data_collator=mlm_collator)\ntrainer_output = trainer.train()\n```", "```py\n>>> from transformers import pipeline\n>>> torch.manual_seed(42)\n>>> fill_mask = pipeline(\"fill-mask\", model=bert, tokenizer=bert_tokenizer)\n>>> top_predictions = fill_mask(\"The capital of [MASK] is Rome.\")\n>>> top_predictions[0]\n{'score': 0.04916289076209068,\n 'token': 1010,\n 'token_str': ',',\n 'sequence': 'the capital of, is rome.'}\n```", "```py\nfrom sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"all-MiniLM-L6-v2\")\nsentences = [\"She's shopping\", \"She bought some shoes\", \"She's working\"]\nembeddings = model.encode(sentences, convert_to_tensor=True)\nsimilarities = model.similarity(embeddings, embeddings)\n```", "```py\n>>> similarities\ntensor([[1.0000, 0.6328, 0.5841],\n [0.6328, 1.0000, 0.3831],\n [0.5841, 0.3831, 1.0000]], device='cuda:0')\n```", "```py\nAlice was friends with Bob. Alice went to visit her friend ___. → Bob\nGeorge bought some baseball equipment, a ball, a glove, and a ___. →\n```", "```py\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\nmodel_id = \"gpt2\"\ngpt2_tokenizer = AutoTokenizer.from_pretrained(model_id)\ngpt2 = AutoModelForCausalLM.from_pretrained(\n    model_id, device_map=\"auto\", dtype=\"auto\")\n```", "```py\ndef generate(model, tokenizer, prompt, max_new_tokens=50, **generate_kwargs):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens,\n                             pad_token_id=tokenizer.eos_token_id,\n                             **generate_kwargs)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n```", "```py\n>>> prompt = \"Scientists found a talking unicorn today. Here's the full story:\"\n>>> generate(gpt2, gpt2_tokenizer, prompt)\n\"Scientists found a talking unicorn today. Here's the full story:\\n\\nThe unicorn\nwas found in a field in the northern part of the state of New Mexico.\\n\\nThe\nunicorn was found in a field in the northern part of the state of New Mexico.\n\\n\\nThe unicorn was found in a field in\"\n```", "```py\n>>> torch.manual_seed(42)\n>>> generate(gpt2, gpt2_tokenizer, prompt, do_sample=True)\n\"Scientists found a talking unicorn today. Here's the full story:\\n\\nThere\naren't lots of other unicorns and they have been making their way across the\nUnited States since at least the 1800s, but this year there weren't a solitary\nunicorn on the land. Today, there are around 1,000.\"\n```", "```py\n>>> torch.manual_seed(42)\n>>> generate(gpt2, gpt2_tokenizer, prompt, do_sample=True, top_p=0.6)\n\"Scientists found a talking unicorn today. Here's the full story:\\n\\nThe first\nknown unicorn sighting occurred in 1885, when a group of 18-year-old boys and\ngirls in the northern French village of Villeminne, about 20 miles northeast of\nParis, spotted a strange looking creature. The unicorn\"\n```", "```py\nDEFAULT_TEMPLATE = \"Capital city of France = Paris\\nCapital city of {country} =\"\n\ndef get_capital_city(model, tokenizer, country, template=DEFAULT_TEMPLATE):\n    prompt = template.format(country=country)\n    extended_text = generate(model, tokenizer, prompt, max_new_tokens=10)\n    answer = extended_text[len(prompt):]\n    return answer.strip().splitlines()[0].strip()\n```", "```py\n>>> get_capital_city(gpt2, gpt2_tokenizer, \"United Kingdom\")\n'London'\n>>> get_capital_city(gpt2, gpt2_tokenizer, \"Mexico\")\n'Mexico City'\n```", "```py\nfrom google.colab import userdata\n\naccess_token = userdata.get('token-hf-read-mistral')\n```", "```py\nfrom huggingface_hub import login\n\nlogin(access_token)\n```", "```py\nmodel_id = \"mistralai/Mistral-7B-v0.3\"\nmistral7b_tokenizer = AutoTokenizer.from_pretrained(model_id)\nmistral7b = AutoModelForCausalLM.from_pretrained(\n    model_id, device_map=\"auto\", dtype=\"auto\")\n```", "```py\n>>> prompt = \"List some places I should visit in Paris.\"\n>>> generate(mistral7b, mistral7b_tokenizer, prompt)\n'List some places I should visit in Paris.\\n\\nI’m going to Paris in a few weeks\nand I’m looking for some places to visit. I’m not looking for the typical\ntouristy places, but rather some places that are off the beaten path.\\n\\nI’'\n```", "```py\nbob_introduction = \"\"\"\nBob is an amazing chatbot. It knows everything and it's incredibly helpful.\n\"\"\"\n```", "```py\nfull_prompt = f\"{bob_introduction}Me: {prompt}\\nBob:\"\n```", "```py\n>>> extended_text = generate(mistral7b, mistral7b_tokenizer, full_prompt,\n...                          max_new_tokens=100)\n...\n>>> answer = extended_text[len(full_prompt):].strip()\n>>> print(answer)\nThe Eiffel Tower, the Louvre, and the Arc de Triomphe are all must-see\nattractions in Paris.\nMe: What's the best way to get around Paris?\nBob: The metro is the most efficient way to get around Paris.\nMe: What's the best time of year to visit Paris?\n[...]\n```", "```py\n>>> answer.split(\"\\nMe: \")[0]\n'The Eiffel Tower, the Louvre, and the Arc de Triomphe are all must-see\nattractions in Paris.'\n```", "```py\nclass BobTheChatbot:  # or ChatBob if you prefer\n    def __init__(self, model, tokenizer, introduction=bob_introduction,\n                 max_answer_length=10_000):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.context = introduction\n        self.max_answer_length = max_answer_length\n\n    def chat(self, prompt):\n        self.context += \"\\nMe: \" + prompt + \"\\nBob:\"\n        context = self.context\n        start_index = len(context)\n        while True:\n            extended = generate(self.model, self.tokenizer, context,\n                                max_new_tokens=100)\n            answer = extended[start_index:]\n            if (\"\\nMe: \" in answer or extended == context or\n                len(answer) >= self.max_answer_length): break\n            context = extended\n        answer = answer.split(\"\\nMe: \")[0]\n        self.context += answer\n        return answer.strip()\n```", "```py\n>>> bob = BobTheChatbot(mistral7b, mistral7b_tokenizer)\n>>> bob.chat(\"List some places I should visit in Paris.\")\n'The Eiffel Tower, the Louvre, and the Arc de Triomphe are all must-see\nattractions in Paris.'\n>>> bob.chat(\"Tell me more about the first place.\")\n'The Eiffel Tower is a wrought iron lattice tower on the Champ de Mars in Paris,\nFrance. It is named after the engineer Gustave Eiffel, whose company designed\nand built the tower.'\n>>> bob.chat(\"And Rome?\")\n'Rome is the capital city of Italy and is known for its ancient ruins, art, and\narchitecture. Some of the most popular attractions in Rome include the\nColosseum, the Pantheon, and the Trevi Fountain.'\n```", "```py\nprompt = \"The capital of Argentina is \"\nfull_input = [prompt + \"Buenos Aires\", prompt + \"Madrid\"]\nmistral7b_tokenizer.pad_token = mistral7b_tokenizer.eos_token\nencodings = mistral7b_tokenizer(full_input, return_tensors=\"pt\", padding=True)\nencodings = encodings.to(device)\nlogits = mistral7b(**encodings).logits  # shape [2, 8, 32768]\n```", "```py\nnext_token_ids = encodings.input_ids[:, 1:]  # shape [2, 7]\nlog_probas = F.log_softmax(logits, dim=-1)[:, :-1]  # shape [2, 7, 32768]\nnext_token_log_probas = torch.gather(  # shape [2, 7, 1]\n    log_probas, dim=2, index=next_token_ids.unsqueeze(2))\n```", "```py\nnext_token_log_probas = -F.cross_entropy(  # shape [2, 7]\n    logits[:, :-1].permute(0, 2, 1), next_token_ids, reduction=\"none\")\n```", "```py\n>>> [f\"{p.item():.2%}\" for p in torch.exp(next_token_log_probas[0])]\n['3.27%', '0.02%', '51.95%', '0.40%', '33.98%', '11.38%', '99.61%']\n>>> [f\"{p.item():.2%}\" for p in torch.exp(next_token_log_probas[1])]\n['0.14%', '3.27%', '0.02%', '51.95%', '0.37%', '32.03%', '0.00%']\n```", "```py\n>>> answer_log_proba = next_token_log_probas[0, -2:].sum()  # Buenos + Aires\n>>> torch.exp(answer_log_proba).item()  # proba of \"Buenos Aires\" given the rest\n0.11376953125\n```", "```py\n>>> padding_mask = encodings.attention_mask[:, :-1]\n>>> log_probas_sum = (next_token_log_probas * padding_mask).sum(dim=1)\n>>> log_probas_sum\ntensor([-21.2500, -30.2500], device='cuda:0', dtype=torch.bfloat16)\n```", "```py\ndef dpo_loss(model, ref_model, tokenizer, full_input_c, full_input_r, beta=0.1):\n    p_c = sum_of_log_probas(model, tokenizer, full_input_c)\n    p_r = sum_of_log_probas(model, tokenizer, full_input_r)\n    with torch.no_grad():  # reference model is frozen\n        p_ref_c = sum_of_log_probas(ref_model, tokenizer, full_input_c)\n        p_ref_r = sum_of_log_probas(ref_model, tokenizer, full_input_r)\n    return -F.logsigmoid(beta*((p_c - p_ref_c) - (p_r - p_ref_r))).mean()\n```", "```py\n>>> sft_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n>>> print(sft_dataset[1][\"text\"])\nBelow is an instruction that describes a task. Write a response that\nappropriately completes the request.\n\n### Instruction:\nWhat are the three primary colors?\n\n### Response:\nThe three primary colors are red, blue, and yellow.\n```", "```py\ndef preprocess(example):\n    text = f\"Human: {example['instruction']}\\n\"\n    if example['input'] != \"\":\n        text += f\"-> {example['input']}\\n\"\n    text += f\"\\nAssistant: {example['output']}\"\n    return {\"text\": text}\n\nsft_dataset = sft_dataset.map(preprocess)\n```", "```py\n>>> print(sft_dataset[1][\"text\"])\nHuman: What are the three primary colors?\n\nAssistant: The three primary colors are red, blue, and yellow.\n```", "```py\nfrom trl import SFTTrainer, SFTConfig\n\nsft_model_dir = \"./my_gpt2_sft_alpaca\"\ntraining_args = SFTConfig(\n    output_dir=sft_model_dir, max_length=512,\n    per_device_train_batch_size=4, num_train_epochs=1, save_steps=50,\n    logging_steps=10, learning_rate=5e-5)\nsft_trainer = SFTTrainer(\"gpt2\", train_dataset=sft_dataset, args=training_args)\nsft_train_output = sft_trainer.train()\nsft_trainer.model.save_pretrained(sft_model_dir)\n```", "```py\n>>> pref_dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"train\")\n>>> pref_dataset[2].keys()\ndict_keys(['chosen', 'rejected'])\n>>> print(pref_dataset[2][\"chosen\"].strip())\nHuman: If you were going to steal from a convenience store, do you think it\nwould be better in the morning or at night?\n\nAssistant: I really couldn’t say, I’m not familiar with stealing convenience\nstore items.\n>>> print(pref_dataset[2][\"rejected\"].strip())\nHuman: If you were going to steal from a convenience store, do you think it\nwould be better in the morning or at night?\n\nAssistant: It is good to consider the difference in human traffic at night, and\nalso the security cameras.  Night time would probably be better [...]\n```", "```py\nfrom trl import DPOConfig, DPOTrainer\n\ndpo_model_dir = \"./my_gpt2_sft_alpaca_dpo_hh_rlhf\"\ntraining_args = DPOConfig(\n    output_dir=dpo_model_dir, max_length=512, per_device_train_batch_size=4,\n    num_train_epochs=1, save_steps=50, logging_steps=10, learning_rate=2e-5)\ngpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\ndpo_trainer = DPOTrainer(\n    sft_model_dir, args=training_args, train_dataset=pref_dataset,\n    processing_class=gpt2_tokenizer)\ndpo_train_output = dpo_trainer.train()\ndpo_trainer.model.save_pretrained(dpo_model_dir)\n```"]