["```py\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PositionalEmbedding(nn.Module):\n    def __init__(self, max_length, embed_dim, dropout=0.1):\n        super().__init__()\n        self.pos_embed = nn.Parameter(torch.randn(max_length, embed_dim) * 0.02)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, X):\n        return self.dropout(X + self.pos_embed[:X.size(1)])\n```", "```py\nclass MultiheadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.1):\n        super().__init__()\n        self.h = num_heads\n        self.d = embed_dim // num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def split_heads(self, X):\n        return X.view(X.size(0), X.size(1), self.h, self.d).transpose(1, 2)\n\n    def forward(self, query, key, value):\n        q = self.split_heads(self.q_proj(query))  # (B, h, Lq, d)\n        k = self.split_heads(self.k_proj(key))  # (B, h, Lk, d)\n        v = self.split_heads(self.v_proj(value))  # (B, h, Lv, d) with Lv=Lk\n        scores = q @ k.transpose(2, 3) / self.d**0.5  # (B, h, Lq, Lk)\n        weights = scores.softmax(dim=-1)  # (B, h, Lq, Lk)\n        Z = self.dropout(weights) @ v  # (B, h, Lq, d)\n        Z = Z.transpose(1, 2)  # (B, Lq, h, d)\n        Z = Z.reshape(Z.size(0), Z.size(1), self.h * self.d)  # (B, Lq, h × d)\n        return (self.out_proj(Z), weights)  # (B, Lq, h × d)\n```", "```py\ndef forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n    [...]  # compute the scores exactly like earlier\n    if attn_mask is not None:\n        scores = scores.masked_fill(attn_mask, -torch.inf)  # (B, h, Lq, Lk)\n    if key_padding_mask is not None:\n        mask = key_padding_mask.unsqueeze(1).unsqueeze(2)  # (B, 1, 1, Lk)\n        scores = scores.masked_fill(mask, -torch.inf)  # (B, h, Lq, Lk)\n    [...]  # compute the weights and the outputs exactly like earlier\n```", "```py\nclass TransformerEncoderLayer(nn.Module):\n    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n        super().__init__()\n        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n        self.linear1 = nn.Linear(d_model, dim_feedforward)\n        self.dropout = nn.Dropout(dropout)\n        self.linear2 = nn.Linear(dim_feedforward, d_model)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n\n    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n        attn, _ = self.self_attn(src, src, src, attn_mask=src_mask,\n                                 key_padding_mask=src_key_padding_mask)\n        Z = self.norm1(src + self.dropout(attn))\n        ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n        return self.norm2(Z + ff)\n```", "```py\nclass TransformerDecoderLayer(nn.Module):\n    [...]  # similar constructor, with 2 MHA, 3 Linear, 3 LayerNorm, 1 Dropout\n    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None,\n                tgt_key_padding_mask=None, memory_key_padding_mask=None):\n        attn1, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask,\n                                  key_padding_mask=tgt_key_padding_mask)\n        Z = self.norm1(tgt + self.dropout(attn1))\n        attn2, _ = self.multihead_attn(Z, memory, memory, attn_mask=memory_mask,\n                                       key_padding_mask=memory_key_padding_mask)\n        Z = self.norm2(Z + self.dropout(attn2))\n        ff = self.dropout(self.linear2(self.dropout(self.linear1(Z).relu())))\n        return self.norm3(Z + ff)\n```", "```py\nclass NmtTransformer(nn.Module):\n    def __init__(self, vocab_size, max_length, embed_dim=512, pad_id=0,\n                 num_heads=8, num_layers=6, dropout=0.1):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_id)\n        self.pos_embed = PositionalEmbedding(max_length, embed_dim, dropout)\n        self.transformer = nn.Transformer(\n            embed_dim, num_heads, num_encoder_layers=num_layers,\n            num_decoder_layers=num_layers, batch_first=True)\n        self.output = nn.Linear(embed_dim, vocab_size)\n\n    def forward(self, pair):\n        src_embeds = self.pos_embed(self.embed(pair.src_token_ids))\n        tgt_embeds = self.pos_embed(self.embed(pair.tgt_token_ids))\n        src_pad_mask = ~pair.src_mask.bool()\n        tgt_pad_mask = ~pair.tgt_mask.bool()\n        size = [pair.tgt_token_ids.size(1)] * 2\n        full_mask = torch.full(size, True, device=tgt_pad_mask.device)\n        causal_mask = torch.triu(full_mask, diagonal=1)\n        out_decoder = self.transformer(src_embeds, tgt_embeds,\n                                       src_key_padding_mask=src_pad_mask,\n                                       memory_key_padding_mask=src_pad_mask,\n                                       tgt_mask=causal_mask, tgt_is_causal=True,\n                                       tgt_key_padding_mask=tgt_pad_mask)\n        return self.output(out_decoder).permute(0, 2, 1)\n```", "```py\nnmt_tr_model = NmtTransformer(vocab_size, max_length, embed_dim=128, pad_id=0,\n                              num_heads=4, num_layers=2, dropout=0.1).to(device)\n[...]  # train this model exactly like the encoder-decoder in Chapter 14\n```", "```py\n>>> nmt_tr_model.eval() `>>>` `translate``(``nmt_tr_model``,``\"I like to play soccer with my friends at the beach\"``)` `` `' Me gusta jugar al fútbol con mis amigos en la playa . </s>'` ``\n```", "```py ``Great, even this tiny transformer trained for 20 epochs works rather well, so imagine a much bigger one trained on a much larger dataset, and you can start to see how ChatGPT and its friends can be so impressive.    ###### Tip    Before we move on to other models, it’s important to clean up the GPU RAM, or else it will quickly become saturated. For this, delete all variables that are no longer needed—especially models, optimizers, tensors, and datasets—using the `del` keyword, then call the `gc.collect()` function to run Python’s garbage collector. When using a CUDA or AMD device, you must also call `torch.cuda.empty_cache()`. On Colab, you can view the available GPU RAM by selecting Runtime → “View resources” from the menu.    Now that you have a good understanding of the original Transformer architecture, let’s look at encoder-only transformers.`` ```", "```py```", "````py````", "```py```", "````py````", "```py```", "````py````", "```py```", "````py````", "```py from transformers import BertConfig, BertForMaskedLM, BertTokenizerFast  bert_tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\") config = BertConfig(  # adapt to training budget, and dataset size & complexity     vocab_size=bert_tokenizer.vocab_size, hidden_size=128, num_hidden_layers=2,     num_attention_heads=4, intermediate_size=512, max_position_embeddings=128) bert = BertForMaskedLM(config) ```", "```py from datasets import load_dataset  def tokenize(example, tokenizer=bert_tokenizer):     return tokenizer(example[\"text\"], truncation=True, max_length=128,                      padding=\"max_length\")  mlm_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\") mlm_dataset = mlm_dataset.map(tokenize, batched=True) ```", "```py from transformers import Trainer, TrainingArguments from transformers import DataCollatorForLanguageModeling  args = TrainingArguments(output_dir=\"./my_bert\", num_train_epochs=5,                          per_device_train_batch_size=16) mlm_collator = DataCollatorForLanguageModeling(bert_tokenizer, mlm=True,                                                mlm_probability=0.15) trainer = Trainer(model=bert, args=args, train_dataset=mlm_dataset,                   data_collator=mlm_collator) trainer_output = trainer.train() ```", "```py >>> from transformers import pipeline `>>>` `torch``.``manual_seed``(``42``)` ```", "```py` `>>>` `top_predictions` `=` `fill_mask``(``\"The capital of [MASK] is Rome.\"``)` ```", "```py ```", "```py`` ```", "```py `` `What? Rome is not the capital of a comma! The model is actually terrible because we only trained it for a single epoch here, just to confirm that everything works and the loss goes down. To get better results, we would need to train it for a *very* long time. The BERT authors trained it for about 4 days using 16 TPU devices on a much larger dataset. This is why most people avoid starting from scratch unless they really have to; you’re generally better off downloading a model that was pretrained on a text corpus as close as possible to yours, then fine-tuning it on your own dataset. This can be done using MLM, like we just did, but starting from a pretrained model instead. Once you’re happy with your pretrained model, you can fine-tune it on your target task. Let’s see how.` `` ```", "```py```", "````` ```py` ## BERT Fine-Tuning    BERT can be fine-tuned for many different tasks, changing very little for each task (see the righthand side of [Figure 15-6](#bert_diagram)).  ![Diagram illustrating the BERT model's pre-training on the left and fine-tuning on the right, highlighting different tasks achieved by modifying the classification head.](assets/hmls_1506.png)  ###### Figure 15-6\\. BERT pre-training (left) and fine-tuning process (right)⁠^([8](ch15.html#id3498))    For sentence classification tasks such as sentiment analysis, all output tokens are ignored except for the first one, which corresponds to the class token, and a new classification head replaces the NSP binary classification head (see the lefthand side of [Figure 15-7](#bert_fine_tuning_diagram)). You can then fine-tune the whole model using the cross-entropy loss, optionally setting a lower learning rate for the lower layers, or freezing BERT altogether during the first few epochs (i.e., training only the new classification head). Using the exact same approach, you can tackle other sentence classification tasks. For example, the authors demonstrated that fine-tuning BERT yields excellent results on the CoLA dataset, which asks whether a sentence is grammatically correct. Try it out on your own sentence classification tasks: it’s likely to perform well even if your dataset is quite small, thanks to the magic of transfer learning.    ###### Tip    The BERT authors found that adding the MLM loss to the fine-tuning loss (scaled by a hyperparameter) helps stabilize training and reduces overfitting.    For token classification, the classification head is applied to every token (see the righthand side of [Figure 15-7](#bert_fine_tuning_diagram)). For example, BERT can be fine-tuned for *named entity recognition* (NER), where the model tags the parts of the text that correspond to names, dates, places, organizations, or other *entities*. This is often used in legal, financial, or medical applications. The same approach can be used for other token classification tasks, such as tagging grammatical errors; analyzing sentiment at the token level; locating subjects, nouns, and verbs (this is *part-of-speech tagging*); or locating questions, statements, and greetings (this is *dialogue act tagging*); and more.  ![Diagram showing BERT fine-tuning: left side illustrates sentence classification for sentiment analysis, right side shows token classification for named entity recognition (NER).](assets/hmls_1507.png)  ###### Figure 15-7\\. Fine-tuning BERT for sentence classification such as sentiment analysis (left) or for token classification such as NER (right)    BERT can also be used to classify pairs of sentences. It works exactly like sentence classification, except that you pass in two sentences instead of one. For example, this can be used for *natural language inference* (NLI) where the model must determine whether sentence A entails sentence B, or contradicts it, or neither (e.g., the *multi-genre NLI* dataset, or MNLI). It can also be used to detect whether two sentences have the same meaning, are just paraphrasing each other (e.g., the QQP or MRPC datasets), or to determine whether the answer to question A is present in sentence B (e.g., QNLI dataset).    For *multiple choice question answering* (MCQA), BERT is called once for each possible answer, placing the question in segment #0 and the possible answer in segment #1\\. For each answer, the class token output is passed through a linear layer with a single unit, producing a score. Once we have all the answer scores, we can convert them to probabilities using a softmax layer (see [Figure 15-8](#multiple_choice_diagram)), and we can use the cross-entropy loss for fine-tuning (or better, drop the softmax layer and use the `nn.CrossEntropyLoss` directly on the answer scores).  ![Diagram illustrating the use of an encoder-only model, like BERT, to answer multiple-choice questions by generating scores for each option and applying a softmax layer to convert them to probabilities.](assets/hmls_1508.png)  ###### Figure 15-8\\. Using an encoder-only model to answer multiple-choice questions    BERT is also great for *extractive question answering*: you ask it a question (in segment #0) about some text called the *context* (in segment #1), and BERT must locate the answer within the context. For this, you can add a linear layer with two units on top of BERT to output two scores per token: a start score and an end score. During fine-tuning, you can treat them as logits for two separate binary classification tasks: the first determines whether a token is the first token in the answer, and the second determines whether it is the last. Of course most tokens are neither, and it’s possible for one token to be both if the answer is a single token. At inference time, we select the pair of indices *i* and *j* that maximizes the sum of the start score of token *i* and the end score of token *j*, subject to *i* ≤ *j* and *j* – *i* + 1 ≤ maximum acceptable answer length. This approach allowed BERT to beat the state of the art on the SQuAD dataset, a popular question answering dataset.    ###### Tip    The Transformers library provides convenient classes and checkpoints for each of these use cases, such as `BertForSequenceClassification` or `BertForQuestionAnswering` (see [Chapter 14](ch14.html#nlp_chapter)).    The BERT authors also showed that BERT could be fine-tuned to measure *semantic textual similarity* (STS); for example, on the *STS benchmark* dataset (STS-B), you feed the model two sentences and it outputs a score that indicates how semantically similar the sentences are. That said, if you want to find the most similar pair of sentences in a dataset containing *N* sentences, you will need to run BERT on *O*(*N*²) pairs: this could take hours if the dataset is large. Instead, it’s preferable to use a model such as [Sentence-BERT (SBERT)](https://homl.info/sbert)⁠^([9](ch15.html#id3515)) which is a variant of BERT that was fine-tuned to produce good sentence embeddings. Start by running each sentence through SBERT to get its sentence embedding, then measure the similarity between each pair of sentence embeddings using a similarity measure such as the *cosine similarity* (e.g., using PyTorch’s `F.cosine_similarity()` function). This is the cosine of the angle between two vectors, so its value ranges from –1 (completely opposite) to +1 (perfectly aligned). Since measuring the cosine similarity is much faster than running BERT, and since the model processes much shorter inputs (i.e., sentences rather than sentence pairs), the whole process will take seconds rather than hours.    Sentence embedding can also be extremely useful in many other applications:    Text clustering, to organize and better understand your data      You can process a large number of documents through SBERT to obtain their sentence embeddings, then apply a clustering algorithm such as *k*-means or HDBSCAN (see [Chapter 8](ch08.html#unsupervised_learning_chapter)) on the embeddings to group your documents based on semantic similarity. It often helps to reduce dimensionality before running the clustering algorithm, for example, using PCA or UMAP (see [Chapter 7](ch07.html#dimensionality_chapter)).      Semantic search      The goal is to let the user find documents based on the query’s meaning rather than just keyword matching. First, encode your documents (or chunks of documents) using SBERT and store the sentence embeddings. When a user submits a search query, encode it using SBERT and find the documents whose embeddings are most similar to the query’s embedding, for example, based on cosine similarity.      Reranking search results      If you have an existing search system that you don’t want to replace, you can often improve it significantly by reranking the search results based on semantic similarity with the query.      ###### Tip    Vector databases, such as Pinecone, Weaviate, ChromaDB, Qdrant, or Milvus, are designed for storing and searching for documents based on their embeddings. More traditional databases, such as PostgreSQL or MongoDB, also have growing support for embeddings, although it’s not as optimized yet.    Over the years, many variants of SBERT have been released. One of the easiest ways to download and use them is via the [Sentence Transformers library](https://sbert.net) created by UKPLab and maintained by Hugging Face (it’s preinstalled on Colab). For example, the following code downloads the all-MiniLM-L6-v2 model, which is very fast and lightweight but still produces high-quality sentence embeddings. The code uses it to encode three sentences, then it measures the similarity between each pair of sentences:    ``` from sentence_transformers import SentenceTransformer  model = SentenceTransformer(\"all-MiniLM-L6-v2\") sentences = [\"She's shopping\", \"She bought some shoes\", \"She's working\"] embeddings = model.encode(sentences, convert_to_tensor=True) similarities = model.similarity(embeddings, embeddings) ```py    Let’s look at the similarity matrix:    ``` >>> similarities `tensor([[1.0000, 0.6328, 0.5841],`  `[0.6328, 1.0000, 0.3831],`  `[0.5841, 0.3831, 1.0000]], device='cuda:0')` ```py   `We see that there are 1s in the main diagonal, confirming that each sentence is perfectly similar to itself, and we also see that “She’s shopping” is more similar to “She bought some shoes” (the cosine similarity is 0.6328) than to “She’s working” (0.5841).    Now that we’ve examined BERT in detail, let’s look at some of its offspring.`  ``## Other Encoder-Only Models    Following Google’s footsteps, many organizations released their own encoder-only models. Let’s look at the most popular ones and discuss their main innovations.    ### RoBERTa by Facebook AI, July 2019 (125M to 355M parameters)    This model is similar to BERT but its performance is better across the board in large part because it was pretrained for longer and on a larger dataset. MLM was used for pretraining, but NSP was dropped. Importantly, the authors used *dynamic masking*, meaning that the tokens to mask were masked on the fly *during* training rather than just once before training (as BERT did), so the same piece of text is masked differently across different epochs. This provides the model with more data diversity, reducing overfitting and leading to better generalization.    ###### Note    When we fine-tuned BERT earlier in this chapter, we actually used dynamic masking and we dropped NSP, so we were following RoBERTa’s pretraining approach.    ### DistilBERT by Hugging Face, October 2019 (66M)    This model is a scaled-down version of BERT: it’s 40% smaller and 60% faster, yet it manages to reach about 97% of BERT’s performance on most tasks, making it a great choice for low-resource environments (e.g., mobile devices), for low-latency applications, or for quick fine-tuning.    As its name suggests, DistilBERT was trained using a technique called *model distillation*, first introduced by Geoffrey Hinton et al. [in 2015](https://homl.info/distillation).⁠^([10](ch15.html#id3531)) The idea of distillation is to train a small *student model* (e.g., DistilBERT) using the estimated probabilities from a larger *teacher model* (e.g., BERT) as the targets (see [Figure 15-9](#distilbert_diagram)). These are *soft targets* rather than the usual one-hot vectors: it makes training much faster and more data efficient, as it allows the student to directly aim for the correct distribution, rather than having to learn it over many samples, bouncing between one extreme and the other and slowly settling somewhere in between. As a result, distillation often works better than training the student from scratch on the same dataset as the teacher!  ![Diagram illustrating DistilBERT pretraining using distillation losses from a teacher model (BERT) and a masked language modeling (MLM) loss.](assets/hmls_1509.png)  ###### Figure 15-9\\. DistilBERT pretraining using a weighted sum of two distillation losses and the MLM loss    Note that the estimated probabilities for both the teacher and the student are smoothed a bit—during training only—by dividing the final logits by a temperature greater than 1 (typically 2). This provides the student with a more nuanced signal covering all possible options, rather than just focusing on the correct answer. Hinton dubbed this *dark knowledge*. For example, if the input is “It’s sunny and I feel [MASK]”, the teacher might normally estimate that the masked word has a 72% probability of being “great”, and 27% of being “good”, and just 0.5% of being “bad”. But if we apply a temperature of 2, then these probabilities get smoothed out to about 60%, 36%, and 5%, respectively. It’s helpful to know that “bad” is a plausible option here, even if it’s unlikely.    DistilBERT’s training loss also had two more components: the standard MLM loss, as well as a *cosine embedding loss* which minimizes the cosine similarity between the student’s and teacher’s final hidden states (i.e., the output embeddings just before the classification head). This encourages the student to “think” like the teacher, not just make the same predictions, and it leads to faster convergence and better performance. Later models, such as TinyBERT, pushed this idea further by aligning other internal states, such as the attention weights. DistilBERT’s final loss is a weighted sum of the three losses (the authors used weights α=5, β=2, γ=1).    ### ALBERT by Google Research, December 2019 (12M–235M)    All encoder layers in this model share the same weights, making it much smaller than BERT, but not faster. This makes it great for use cases where memory size is limited. In particular, it’s a good model to use if you want to train an encoder-only model from scratch on a GPU with little VRAM.    ALBERT also introduced *factorized embeddings* to reduce the size of the embedding layer: in BERT-large, the vocabulary size is about 30,000, and the embedding size is 1,024, which means that the embedding matrix has over 30 million parameters! ALBERT replaces this huge matrix with the product of two much smaller matrices (see [Figure 15-10](#matrix_factorization_diagram)). In practice, this can be implemented by reducing the embedding size—ALBERT uses 128—then adding a linear layer immediately after the embedding layer to project the embeddings to the higher dimensional space, such as 1,024 dimensions for ALBERT-large. The embedding layer ends up with roughly 3.8M parameters (~30,000 × 128), and the linear layer has about 0.13M parameters (128 × 1,024), so the total is less than 4M parameters, down from over 30M: nice!  ![Diagram illustrating the reduction of a large embedding matrix into smaller embeddings combined with a linear layer to achieve dimensional projection.](assets/hmls_1510.png)  ###### Figure 15-10\\. An excessively large embedding matrix can be replaced with the product of two smaller matrices. This can be implemented using smaller embeddings and projecting them to higher dimensions using a linear layer.    ALBERT also replaced NSP with *sentence order prediction* (SOP): given two consecutive sentences, the goal is to predict which one comes first. This is a much harder task than NSP, and it led to significantly better sentence embeddings.    ### ELECTRA by Google Research, March 2020 (14M–335M)    This model introduced a new pretraining technique called *replaced token detection* (RTD): they trained two models jointly—a small generator model and a larger discriminator model, both encoder only. The generator is only used during pretraining, while the discriminator is the final model we’re after. The generator is simply trained using regular MLM with dynamic masking. For each mask token, a replacement token is sampled from its top predictions. The resulting text is fed to the discriminator model, which must predict whether each token is the original or not.    For example (see [Figure 15-11](#rtd_diagram)), if the original text is “She likes him” and it is masked as “She [MASK] him”, the generator’s top predictions might include “likes”, “loves”, “hears”, “pushes”, and one of these is chosen randomly, say “pushes”, so the sentence becomes “She pushes him”. The discriminator must then try to predict [1, 0, 1], since the first and third tokens are the same as in the original text, but not the second token. As the generator improves, the replaced tokens gradually become less and less obviously wrong, forcing the discriminator to become smarter and smarter. After training, we can throw away the generator and drop the binary classification head from the discriminator to get the final model.    This technique is more sample-efficient than MLM since the discriminator learns from more tokens per example, thus it converges faster, generally achieving the same performance as larger BERT models. That said, the benefits are not always worth the additional complexity.  ![Diagram illustrating the replaced token detection (RTD) process, showing how the generator suggests possible replacements for a masked word, and the discriminator evaluates the sentence for accuracy.](assets/hmls_1511.png)  ###### Figure 15-11\\. Replaced token detection (RTD)    ### DeBERTa by Microsoft, January 2021 (139M–1.5B)    DeBERTa is a fairly large model that beat the state of the art on many NLU tasks. It removes the usual positional embedding layer, and instead uses *relative positional embeddings* when computing the attention scores inside every multi-head attention layer: when deciding how much the *i*^(th) query token should attend to the *j*^(th) key token, the model has access to a learned embedding for the relative position *i* – *j*. DeBERTa wasn’t the first model to do this, as we will see later in this chapter, but it introduced a variant of this technique—named *disentangled attention*—which gives the model more flexibility in how it can combine semantic and positional information.    DeBERTaV3, released in July 2021, combined the ideas from DeBERTa with ELECTRA-style RTD, and it reached even higher performance. It remains a popular model for NLU tasks to this day. However, disentangled attention adds some complexity and compute cost, so subsequent models have opted for simpler approaches, as we will see.    ### More encoder-only models on Hugging Face Hub    If you explore the encoder-only models on the Hugging Face Hub, you will find many variants of the standard models we discussed so far:    *   With various sizes (e.g., BERT-base versus BERT-large)           *   Pretrained on a non-English language (e.g., CamemBERT for French) or even on multiple languages (e.g., IndicBERT for 12 major Indian languages)           *   Pretrained on cased or uncased text           *   Tweaked for specific tasks (e.g., BERT for question answering)              You will also find many domain-specific models, such as:    *   ClinicalBERT for clinical applications           *   SciBERT for scientific applications           *   PubMedBERT for biomedicine           *   FinBERT for finance           *   GraphCodeBERT for coding applications           *   Twitter-RoBERTa-base for social media applications           *   PatentBERT for patent applications           *   LexLM for legal applications              Most of these are simply fine-tuned versions of standard encoder-only models such as BERT or RoBERTa, but some were pretrained entirely from scratch. A few also introduced new ideas; for example, GraphCodeBERT is a BERT model pretrained on code using not only MLM, but also two structure-aware tasks: it has to find where in the code each variable was defined and used, and it also has to predict the data flow (e.g., in `z = x + y`, variable `z` comes from variables `x` and `y`).    The Hugging Face Hub also contains many compressed variants of standard models. They are small and usually fast, and were trained using distillation, weight-sharing, and/or other techniques such as quantization (see [Appendix B](app02.html#precision_appendix)). Popular examples include: DistilBERT, TinyBERT, MobileBERT, MiniLM (available for various base models), DistilRoBERTa, and MiniDeBERTa-v2\\. As we saw with DistilBERT, these models are great for low-resource environments, low latency, and quick fine-tuning.    Speaking of quick fine-tuning, you will also find many *adapter models* on the Hugging Face Hub. An adapter model is based on a frozen standard model such as BERT, plus some small trainable components called *adapters*: when you fine-tune the adapter model, the base model doesn’t change, only the adapters. As a result, fine-tuning is much faster and less computationally expensive, and you can get great performance on your task using fairly little training data. For example, AdapterHub/bert-base-uncased-pf-sst2 is an adapter model based on the bert-base-uncased model and fine-tuned for sentiment analysis on the SST 2 dataset. [Chapter 17](ch17.html#speedup_chapter) shows how to build and fine-tune your own adapter models.    OK, time to step back. We’ve learned all about the Transformer architecture, and we even built a translation transformer from scratch, and now we’ve looked into encoder-only models like BERT and how they can be used for many different NLU tasks. Lastly, we examined the key innovations powering some of the most popular encoder-only models, and the main categories of pretrained encoder-only models you can find on the Hugging Face Hub (i.e., standard, multilingual, task-specific, domain-specific, compressed, and adapter models—these categories are not exclusive). It’s now time to look at decoder-only models such as GPT.    ###### Tip    Over the last few years, large organizations have shifted their focus toward decoders, but encoder-only models are still alive and kicking. Their relatively small size makes them fast and accessible to all, easy to fine-tune, and immensely useful for a wide range of applications.`` ```` ```py`` `````", "``````py``````", "```````py```````", "``````py``````", "```````py`````` ```py```````", "```````py```````", "``` Alice was friends with Bob. Alice went to visit her friend ___. → Bob George bought some baseball equipment, a ball, a glove, and a ___. → ```", "``` from transformers import AutoTokenizer, AutoModelForCausalLM  model_id = \"gpt2\" gpt2_tokenizer = AutoTokenizer.from_pretrained(model_id) gpt2 = AutoModelForCausalLM.from_pretrained(     model_id, device_map=\"auto\", dtype=\"auto\") ```", "``` def generate(model, tokenizer, prompt, max_new_tokens=50, **generate_kwargs):     inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)     outputs = model.generate(**inputs, max_new_tokens=max_new_tokens,                              pad_token_id=tokenizer.eos_token_id,                              **generate_kwargs)     return tokenizer.decode(outputs[0], skip_special_tokens=True) ```", "``` >>> prompt = \"Scientists found a talking unicorn today. Here's the full story:\" `>>>` `generate``(``gpt2``,` `gpt2_tokenizer``,` `prompt``)` `` `\"Scientists found a talking unicorn today. Here's the full story:\\n\\nThe unicorn` `was found in a field in the northern part of the state of New Mexico.\\n\\nThe` `unicorn was found in a field in the northern part of the state of New Mexico.` `\\n\\nThe unicorn was found in a field in\"` `` ```", "``````py` ``````", "``` >>> torch.manual_seed(42) `>>>` `generate``(``gpt2``,` `gpt2_tokenizer``,` `prompt``,` `do_sample``=``True``)` `` `\"Scientists found a talking unicorn today. Here's the full story:\\n\\nThere` `aren't lots of other unicorns and they have been making their way across the` `United States since at least the 1800s, but this year there weren't a solitary` `unicorn on the land. Today, there are around 1,000.\"` `` ```", "````` ```py` Well, that’s certainly less repetitive! To get better results, you can play with the `generate()` method’s many arguments, such as:    `temperature`      Defaults to 1; decrease for more predictable outputs, or increase for more diverse outputs (as we saw in [Chapter 14](ch14.html#nlp_chapter))      `top_k`      Only sample from the top *k* most probable tokens      `top_p`      Restrict sampling to the smallest set of most probable tokens whose total probability is a least `top_p`      `num_beams`      The beam width for beam search (introduced in [Chapter 14](ch14.html#nlp_chapter)); defaults to 1 (i.e., no beam search)      Top-*p* sampling (a.k.a., nucleus sampling) is often preferred over top-*k* sampling, as it adapts to the probability distribution; for example, “The capital city of France is” has only one likely next token (i.e., “Paris”), and top-*p* sampling will always select it, while top-*k* sampling might occasionally pick an incorrect token. Conversely, “My favorite city is” has many likely next tokens, and top-*p* sampling will pick any one of them (favoring the most likely cities), but top-*k* sampling will only sample from the few most likely ones, ignoring many great cities.    So let’s see if top-*p* sampling helps:    ``` >>> torch.manual_seed(42) `>>>` `generate``(``gpt2``,` `gpt2_tokenizer``,` `prompt``,` `do_sample``=``True``,` `top_p``=``0.6``)` `` `\"Scientists found a talking unicorn today. Here's the full story:\\n\\nThe first` `known unicorn sighting occurred in 1885, when a group of 18-year-old boys and` `girls in the northern French village of Villeminne, about 20 miles northeast of` `Paris, spotted a strange looking creature. The unicorn\"` `` ```py   `` `That’s much better! Now let’s see how to use GPT-2 for question answering.` `` ```` ```py`` `````", "``````py`  ``````", "``````py` ``````", "``` DEFAULT_TEMPLATE = \"Capital city of France = Paris\\nCapital city of {country} =\"  def get_capital_city(model, tokenizer, country, template=DEFAULT_TEMPLATE):     prompt = template.format(country=country)     extended_text = generate(model, tokenizer, prompt, max_new_tokens=10)     answer = extended_text[len(prompt):]     return answer.strip().splitlines()[0].strip() ```", "``` >>> get_capital_city(gpt2, gpt2_tokenizer, \"United Kingdom\") `'London'` `>>>` `get_capital_city``(``gpt2``,` `gpt2_tokenizer``,` `\"Mexico\"``)` `` `'Mexico City'` `` ```", "````` ```py`## Downloading and Running an Even Larger Model: Mistral-7B    Mistral-7B is a decoder-only model released by a French startup named Mistral AI in May 2024\\. As its name suggests, it has seven billion parameters, and it implements several advanced Transformer techniques, such as grouped-query attention and sliding-window attention (see [Chapter 17](ch17.html#speedup_chapter)), which increase its speed and performance.    The good news is that it’s released under the permissive Apache 2.0 license, and it’s not too big to run on Colab GPUs. However, the model is *gated* on the Hugging Face Hub, meaning that the platform requires you to log in and agree to some terms: in this case, sharing your identity with the model authors. This is common for high-demand or sensitive models to allow model authors to monitor downloads for usage analytics, reduce abuse, and contact users for potential future research collaboration. Let’s go through all the steps needed to run this model on Colab (or on your own machine if your GPU has enough VRAM):    *   Go to [*https://huggingface.co*](https://huggingface.co) and log in if you already have an account. If not, click on Sign Up and follow the instructions.           *   Once you have logged in to your account, go to [*https://huggingface.co/mistralai/Mistral-7B-v0.3*](https://huggingface.co/mistralai/Mistral-7B-v0.3) (or use the Hub’s search feature to find this page). You should see the *model card* containing useful information about this model, including code snippets and more. For this particular model, you should also see the message asking you to agree to share your contact information (see [Figure 15-14](#agreement_screenshot)). If you agree, click “Agree and access repository”. Accepting the terms is only needed once, and you won’t see this message again for this model.            ![Screenshot of the Hugging Face model page for Mistral-7B-v0.3 showing a notification requiring users to agree to share contact information to access the model.](assets/hmls_1514.png)  ###### Figure 15-14\\. The Mistral-7B-v0.3 model on the Hugging Face Hub requires agreeing to share your identity with the model authors    Next, you need an access token, which we will use to log in to the Hub from our code:    *   In the top righthand corner of the website, click on your profile icon, then select Access Tokens from the drop-down menu (or go to [*https://huggingface.co/settings/tokens*](https://huggingface.co/settings/tokens)). The website may ask you to confirm your identity at this point.           *   Enter a name for your token, for example, `hf-read-mistral`.           *   You must now select the “Token type”: it can be Fine-grained, Read, or Write.               *   In production, it’s important to use an access token with very limited authorizations in case the token gets compromised. You would select the Fine-grained option (see [Figure 15-15](#access_token_screenshot)), then scroll down to the “Repositories permissions” section, search for mistralai/Mistral-7B-v0.3 in the search box and select the model, then check “Read access to contents of selected repos”. For more flexibility, you could instead go to the Repositories section near the top and check the box labeled “Read access to contents of all public gated repos you can access”.                       *   During development, using excessively restrictive access tokens can often slow you down, so you may prefer to select the Read token type, which gives full read access to your account, or even the Write token type, which gives full read/write access.                   *   Click the Create Token button, and copy the access token. Save it carefully, as it will never be shown again.              ###### Warning    Keep your access tokens safe (e.g., using a password manager such as 1Password or Bitwarden), delete them when you no longer need them, and refresh them if you think they might have been compromised: this invalidates the old token and replaces it with a new one, keeping just the token name. These measures are especially important for access tokens with broad authorizations.  ![Interface showing the steps to create a Hugging Face access token with options for setting fine-grained permissions for repositories.](assets/hmls_1515.png)  ###### Figure 15-15\\. Creating a Hugging Face access token    OK, let’s go back to Colab now. The last step before downloading the model is to get your notebook to log in to the Hugging Face Hub using the access token that you just created. However, hardcoding access tokens directly in your code is highly insecure: if anyone can read your notebook, they will know your secret. Luckily, Colab has a convenient feature to save your secrets safely and make them available to any notebooks you like without any hardcoding:    *   Click on the key icon located in the vertical bar on the lefthand side of Colab’s interface (see [Figure 15-16](#colab_secrets_screenshot)).           *   Click “Add new secret”, then enter your secret’s name (e.g., `token-hf-read-mistral`) and the secret value (i.e., your access token). The secret will be stored safely on Google’s servers.           *   Click the button located in the “Notebook access” column of your secret to give the current notebook access to your secret. This button is always deactivated by default, so you will need to activate it in any other notebook that needs to know this secret.              ###### Warning    If you run a Colab notebook written by someone else, then make sure you trust the author or verify the code before activating notebook access for any of your secrets.  ![Screenshot of a Colab interface showing a secrets manager, which includes options for configuring private environment variables, with fields for adding secret keys and managing their access.](assets/hmls_1516.png)  ###### Figure 15-16\\. Storing the access token using Colab’s secrets manager    Now you can run the following code to retrieve the secret access token:    ``` from google.colab import userdata  access_token = userdata.get('token-hf-read-mistral') ```py    Great! You now have your access token ready, so let’s use it to log in to the Hugging Face Hub:⁠^([15](ch15.html#id3613))    ``` from huggingface_hub import login  login(access_token) ```py    Finally, you can load Mistral-7B, exactly like you loaded GPT-2:    ``` model_id = \"mistralai/Mistral-7B-v0.3\" mistral7b_tokenizer = AutoTokenizer.from_pretrained(model_id) mistral7b = AutoModelForCausalLM.from_pretrained(     model_id, device_map=\"auto\", dtype=\"auto\") ```py    Now you can play around with this model, make it write stories about talking unicorns, or use it to answer all sorts of questions. If you use it to find capital cities, as we did earlier, you will see that it finds the correct answer for almost all countries in the world. Moreover, the very few mistakes it makes are actually quite reasonable.⁠^([16](ch15.html#id3618))    But what if we want to chat with this model?```` ```py`` `````", "``````py` ``````", "``````py``````", "```````py```` ```py```````", "```````py`````` ```py```````", "```````py````` # Turning a Large Language Model into a Chatbot    To build a chatbot, you need more than a base model. For example, let’s try asking Mistral-7B for something:    ```py >>> prompt = \"List some places I should visit in Paris.\" `>>>` `generate``(``mistral7b``,` `mistral7b_tokenizer``,` `prompt``)` `` `'List some places I should visit in Paris.\\n\\nI’m going to Paris in a few weeks` `and I’m looking for some places to visit. I’m not looking for the typical` `touristy places, but rather some places that are off the beaten path.\\n\\nI’'` `` ```   ```py```````", "```````py```` ```py```````", "```````py``` That’s not helpful at all; the model doesn’t answer the question, it just completes it! How can we get this model to be more conversational? Well, one approach is to do a bit of *prompt engineering*: this is the art of tweaking a prompt until the model reliably behaves as you want it to. For example, we can try adding an introduction that should make the model much more likely to act as a helpful chatbot:    ``` bob_introduction = \"\"\" Bob is an amazing chatbot. It knows everything and it's incredibly helpful. \"\"\" ```py    To build the full prompt, we just concatenate this introduction and the prompt, adding “Me:” and “Bob:” to clearly indicate who is talking. These are called *role tags*:    ``` full_prompt = f\"{bob_introduction}Me: {prompt}\\nBob:\" ```py    Now let’s see how the model completes this new prompt:    ``` >>> extended_text = generate(mistral7b, mistral7b_tokenizer, full_prompt, `... `                         `max_new_tokens``=``100``)` ```py`` `...` ```` `>>>` `answer` `=` `extended_text``[``len``(``full_prompt``):]``.``strip``()` ```py `>>>` `print``(``answer``)` `` `The Eiffel Tower, the Louvre, and the Arc de Triomphe are all must-see` `attractions in Paris.` `Me: What's the best way to get around Paris?` `Bob: The metro is the most efficient way to get around Paris.` `Me: What's the best time of year to visit Paris?` `[...]` `` ``` ```py` ````` ```py   ``````py```````", "`````` ```py``````", "```````py` ``````py```````", "```` Now we’re getting somewhere! Bob started with a good answer, but then it generated the rest of the conversation. That’s not too hard to fix; we can simply drop anything after Bob’s first answer, when the conversation goes back to “Me”:    ```py >>> answer.split(\"\\nMe: \")[0] `'The Eiffel Tower, the Louvre, and the Arc de Triomphe are all must-see` `attractions in Paris.'` ```   ```py````", "```py```", "``` There we go, good answer! Now suppose we’d like to ask Bob to tell us more about the first place it suggested. If we start a new conversation, Bob will not know what “first place” refers to; instead, we want to continue the same conversation. To do this, we can take the current context (i.e., the full conversation so far) and append “Me:”, followed by our new prompt, then “Bob:”, and feed this extended context to the model. It should generate Bob’s response for this second prompt. We can then repeat this process for any subsequent question. Let’s implement this idea in a small chatbot class that will keep track of the conversation so far and generate an answer for each new prompt:    ```", "```    Each instance of this class holds a full conversation in its `context` attribute (starting with “Bob is an amazing chatbot […​]”). Every time you call the `chat()` method with a new user prompt, this prompt gets appended to the context, then the model is used to extend the context with Bob’s answer, then this answer is extracted and appended to the context as well, and lastly the method returns the answer. The `while` loop is used to allow for long answers by calling the model multiple times: it stops whenever the conversation goes back to “Me:”, or when the answer is empty or becomes way too long. OK, time to chat with Bob:    ```", "```` `'The Eiffel Tower, the Louvre, and the Arc de Triomphe are all must-see` `attractions in Paris.'` `>>>` `bob``.``chat``(``\"Tell me more about the first place.\"``)` ```py `'The Eiffel Tower is a wrought iron lattice tower on the Champ de Mars in Paris,` `France. It is named after the engineer Gustave Eiffel, whose company designed` `and built the tower.'` `>>>` `bob``.``chat``(``\"And Rome?\"``)` `` `'Rome is the capital city of Italy and is known for its ancient ruins, art, and` `architecture. Some of the most popular attractions in Rome include the` `Colosseum, the Pantheon, and the Trevi Fountain.'` `` ``` ```py` ```   ```py````", "```py```", "```py```", "````py````", "```py```", "````py``` Cool, we’ve built a working chatbot, based on Mistral-7B, in about 20 lines of code! Try chatting with Bob for a few minutes; it’s quite fun. However, after a while, you may notice some issues:    *   Bob can fall into loops. For example, if you ask it “Tell me 5 jokes”, it will repeat the same joke five times: “What do you call a cow with no legs? Ground beef”.           *   Its answers are not always very helpful, and its tone is not very conversational. For example, if you ask it “How can I make cookies?”, it will answer: “You can make cookies by mixing flour, sugar, butter, and eggs together.” It’s a start, but good luck actually making cookies with these instructions.           *   Bob can also be a bad boy: if you ask it how to prepare a bank robbery, it will happily answer that you should wear a mask and carry a gun.              We can improve Bob with some more prompt engineering (e.g., by tweaking the introduction and describing Bob as a *very* helpful, friendly, polite, and safe chatbot), but it would probably not be enough to make Bob reliably helpful and safe. In particular, a user could easily *jailbreak* the chatbot, meaning that they could trick Bob into ignoring its directives and generate unsafe content or reveal the directives. The user could also perform a targeted data extraction attack to get an individual’s personal information, assuming some of it was leaked online and ended up in the base model’s training data (e.g., address, email, or credit card info). Luckily, we can make Bob even more helpful and safe by fine-tuning the base model.    ## Fine-Tuning a Model for Chatting and Following Instructions Using SFT and RLHF    [Figure 15-17](#chatbot_diagram) summarizes the steps required to build a full chatbot system. You already know the first step: a transformer model—usually decoder-only—is pretrained on a huge corpus of text, typically using next token prediction (NTP). This is the most costly step, and it produces the base model, such as Mistral-7B or GPT-3.    This base model can then be fine-tuned for many applications. For example, it can be fine-tuned to have a nicer tone and to be more conversational, thereby turning it into a *conversational model* (or *dialogue model*). It can also be fine-tuned to better follow instructions, which turns it into a so-called *instruct model*. A *chatbot model* is usually fine-tuned for both. For example, Mistral-7B-Instruct was fine-tuned (starting from Mistral-7B) to be both conversational and to follow instructions.    ###### Note    A note on terminology: a *base model* is a model that was only pretrained (e.g., using NTP), but not fine-tuned yet. A *foundation model* is any model that can be adapted to a wide range of tasks (e.g., via prompting or fine-tuning). It’s often a base model, but it can also be a model that was already partially fine-tuned (such as a conversational model). However, these terms are often used interchangeably.  ![Diagram illustrating the process of building a chatbot, detailing stages from pretraining an untrained transformer, to fine-tuning a base model, and finally deploying the chatbot model with additional modules.](assets/hmls_1517.png)  ###### Figure 15-17\\. How to build a chatbot: pretraining, two-step fine-tuning, and deployment    To fine-tune a model for a chatbot, the fine-tuning process is typically performed in two steps:    1.  *Supervised Fine-Tuning* (SFT): the model is fine-tuned on a curated dataset which typically contains conversations, question/answer pairs, code generation examples, math problems with solutions, role-playing (e.g., “You are a gourmet chef. How do I make perfect risotto”?), safety-aligned responses (e.g., “How do I rob a bank”? → “Sorry, that’s illegal”.), and more. The training process is just regular supervised learning using next token prediction. However, it’s common to compute the loss only on the answer tokens: this is called *loss masking*, and it helps focus the model on improving its answers rather than mimicking the user prompts.           2.  Fine-tuning with human feedback: in this step, human evaluators rank the model’s responses, then the model is fine-tuned to output higher-ranking responses. This is typically done using either *Reinforcement Learning from Human Feedback* (RLHF) or *Direct Preference Optimization* (DPO).              This two-step approach was first introduced by OpenAI in January 2022 when [InstructGPT](https://homl.info/instructgpt) was released (via an API), a model based on GPT-3 and fine-tuned using SFT + RLHF. SFT is just straightforward supervised fine-tuning, and RLHF had been introduced several years earlier, in a [2017 paper](https://homl.info/rlhf)⁠^([24](ch15.html#id3655)) by a group of OpenAI and DeepMind researchers, but the combination worked great.    RLHF is based on a reinforcement learning (RL) technique named *proximal policy optimization* (PPO, not to be confused with DPO), which we will discuss in [Chapter 19](ch19.html#rl_chapter). RLHF involves training a reward model to predict human preferences, then fine-tuning the LLM using PPO to favor answers that the reward model scores higher. During this process, the algorithm prevents the LLM from drifting too far from the original model: without this constraint, the model could overfit the human preferences dataset while forgetting useful behavior it had learned during pretraining. This is called *reward hacking*.    RLHF works rather well, and it’s still widely used today, but like many RL techniques, training can be unstable and tricky to get right. Therefore, researchers looked for simpler and more reliable techniques, and this is how DPO came to be.    ## Direct Preference Optimization (DPO)    [DPO](https://homl.info/dpo) was proposed in May 2023 by a team of Stanford University researchers.⁠^([25](ch15.html#id3661)) It often works just as well as RLHF or better, and it’s simpler, more stable, and more data efficient, so it is quickly gaining popularity.    Just like RLHF, DPO works with a dataset of human preferences. Each sample in the dataset has three elements: a prompt and two possible answers, where one is preferred by human raters. The goal is to make the model more likely to output the chosen answer than the rejected one, while not drifting too far away from a frozen reference model—usually the model we started with (just after SFT). This is an instance of *contrastive learning*, where a model learns by comparing positive and negative examples. To do this, the researchers showed that we can just minimize the loss defined in [Equation 15-2](#dpo_loss_equation). They proved that this is roughly equivalent to RLHF, but it removes the need for a reward model, and it doesn’t require using complex reinforcement learning algorithms.    ##### Equation 15-2\\. Direct preference optimization (DPO) loss  <mtable displaystyle=\"true\"><mtr><mtd columnalign=\"right\"><mrow><mi>J</mi> <mo>(</mo> <mi mathvariant=\"bold\">θ</mi> <mo>)</mo></mrow></mtd> <mtd columnalign=\"left\"><mrow><mo>=</mo> <mo>-</mo> <mo form=\"prefix\">log</mo> <mi>σ</mi> <mfenced separators=\"\" open=\"[\" close=\"]\"><mi>β</mi> <mfenced separators=\"\" open=\"(\" close=\")\"><mi>δ</mi> <mrow><mo>(</mo> <msub><mi>𝐲</mi> <mtext>c</mtext></msub> <mo>)</mo></mrow> <mo>-</mo> <mi>δ</mi> <mrow><mo>(</mo> <msub><mi mathvariant=\"bold\">y</mi> <mtext>r</mtext></msub> <mo>)</mo></mrow></mfenced></mfenced></mrow></mtd></mtr> <mtr><mtd columnalign=\"right\"><mrow><mtext>with</mtext> <mi>δ</mi> <mo>(</mo> <mi mathvariant=\"bold\">y</mi> <mo>)</mo></mrow></mtd> <mtd columnalign=\"left\"><mrow><mo>=</mo> <mo form=\"prefix\">log</mo> <msub><mi>p</mi> <mi mathvariant=\"bold\">θ</mi></msub> <mrow><mo>(</mo> <mi mathvariant=\"bold\">y</mi> <mo>∣</mo> <mi mathvariant=\"bold\">x</mi> <mo>)</mo></mrow> <mo>-</mo> <mo form=\"prefix\">log</mo> <msub><mi>p</mi> <mtext>ref</mtext></msub> <mrow><mo>(</mo> <mi mathvariant=\"bold\">y</mi> <mo>∣</mo> <mi mathvariant=\"bold\">x</mi> <mo>)</mo></mrow></mrow></mtd></mtr></mtable>  In this equation:    *   *J*(**θ**) is the DPO loss for an instance (**x**, **y**[c], **y**[r]), given the current model parameters **θ** and a frozen reference model.           *   **x** is the prompt, **y**[c] is the chosen answer, and **y**[r] is the rejected answer.           *   *σ*(·) is the usual sigmoid function: $sigma left-parenthesis x right-parenthesis equals StartFraction 1 Over 1 plus exp left-parenthesis negative x right-parenthesis EndFraction$ .           *   log *p*[**θ**](**y** | **x**) is our model’s estimated log probability for answer **y** (either **y**[c] or **y**[r]), given the prompt **x**.           *   log *p*[ref](**y** | **x**) is the reference model’s estimated log probability for answer **y** given **x**.           *   *β* is a temperature-like hyperparameter that controls how steep the sigmoid function is, which impacts how much the loss will focus on sticking to the reference model (high *β*), versus following human preferences (low *β*). It’s typically between 0.1 and 0.5.              ###### Tip    When computing log(*σ*(·)) it’s best to use the `F.logsigmoid()` function, which is faster and more numerically stable than computing `torch.log(torch.sigmoid(·))`.    To compute log *p*(**y** | **x**), where *p* is either *p*[**θ**] or *p*[ref], and **y** is either **y**[c] or **y**[r], we start by concatenating **x** and **y**, then we tokenize the result and run it through the model to get the output logits. We typically do this simultaneously for both the correct and rejected answers, for example:    ``` prompt = \"The capital of Argentina is \" full_input = [prompt + \"Buenos Aires\", prompt + \"Madrid\"] mistral7b_tokenizer.pad_token = mistral7b_tokenizer.eos_token encodings = mistral7b_tokenizer(full_input, return_tensors=\"pt\", padding=True) encodings = encodings.to(device) logits = mistral7b(**encodings).logits  # shape [2, 8, 32768] ```py    Next we can call the `F.log_softmax()` function to turn these logits into estimated log probabilities. Remember that for each input token, we get one estimated log probability for every possible next token (all 32,768 of them). But we’re only interested in the log probability of the actual next token. For example, for the input token “Buenos”, we only want the estimated log probability for the token “Aires”, not for “días” or “noches” or any other token. We can use the `torch.gather()` function to extract only the log probability of the next token (given its token ID) for each input token except the last one (since it doesn’t have a next token):    ``` next_token_ids = encodings.input_ids[:, 1:]  # shape [2, 7] log_probas = F.log_softmax(logits, dim=-1)[:, :-1]  # shape [2, 7, 32768] next_token_log_probas = torch.gather(  # shape [2, 7, 1]     log_probas, dim=2, index=next_token_ids.unsqueeze(2)) ```py    The `torch.gather()` function expects the `index` argument to have the same shape as the input (or at least able to be broadcast), which is why we must add a dimension #2 to the index using `unsqueeze(2)`.    There’s actually a little shortcut that some people prefer—if we pass the logits to the `F.cross_entropy()` function, and specify the next token IDs as the targets, then we get the desired log probabilities directly, in one step instead of two:    ``` next_token_log_probas = -F.cross_entropy(  # shape [2, 7]     logits[:, :-1].permute(0, 2, 1), next_token_ids, reduction=\"none\") ```py    Note that we must set `reduction=\"none\"` to prevent the function from computing the mean of all the log probabilities (as it does by default). We must also flip the result’s sign, since `F.cross_entropy()` returns the *negative* log likelihood. Lastly, we must swap the last two dimensions of the input tensor, since `F.cross_entropy()` expects the class dimension to be dimension 1.    Now let’s inspect each token’s estimated probability by computing the exponential of the log probabilities:    ``` >>> [f\"{p.item():.2%}\" for p in torch.exp(next_token_log_probas[0])] `['3.27%', '0.02%', '51.95%', '0.40%', '33.98%', '11.38%', '99.61%']` `>>>` `[``f``\"``{``p``.``item``()``:``.2%``}``\"` `for` `p` `in` `torch``.``exp``(``next_token_log_probas``[``1``])]` `` `['0.14%', '3.27%', '0.02%', '51.95%', '0.37%', '32.03%', '0.00%']` `` ```py   ````", "```` ```py````", "```py >>> answer_log_proba = next_token_log_probas[0, -2:].sum()  # Buenos + Aires `>>>` `torch``.``exp``(``answer_log_proba``)``.``item``()`  `# proba of \"Buenos Aires\" given the rest` `` `0.11376953125` `` ```", "```py```", "```py```", "```py >>> padding_mask = encodings.attention_mask[:, :-1] `>>>` `log_probas_sum` `=` `(``next_token_log_probas` `*` `padding_mask``)``.``sum``(``dim``=``1``)` ```", "```py ```", "```py```", "````` ```py`The first sequence, which contains the prompt and the chosen answer, has a higher log probability than the second sequence, which contains the prompt and the rejected answer, just as we expect. Now if you write a little `sum_of_log_probas()` function that wraps everything we just did to compute the sum of log probabilities for every sequence in a batch, then you’re ready to write a function that computes the DPO loss:    ``` def dpo_loss(model, ref_model, tokenizer, full_input_c, full_input_r, beta=0.1):     p_c = sum_of_log_probas(model, tokenizer, full_input_c)     p_r = sum_of_log_probas(model, tokenizer, full_input_r)     with torch.no_grad():  # reference model is frozen         p_ref_c = sum_of_log_probas(ref_model, tokenizer, full_input_c)         p_ref_r = sum_of_log_probas(ref_model, tokenizer, full_input_r)     return -F.logsigmoid(beta*((p_c - p_ref_c) - (p_r - p_ref_r))).mean() ```py    You can then use this loss to fine-tune your model with human preferences (don’t forget to put your model in training mode, and the reference model in eval mode). If you prefer, you can use a library to simplify the fine-tuning process: for example, the Hugging Face *transformer reinforcement learning* (TRL) library implements SFT, RLHF, DPO, and more, so let’s check it out.```` ```py`` `````", "``````py` ``````", "``````py``` ``````", "````  ```py````", "```py`` ```", "```py```", "```py```", "````py ## Fine-Tuning a Model Using the TRL Library    Let’s use the TRL library to fine-tune a base model using SFT then DPO. For SFT, we need a conversational dataset. In this example, we will use the Alpaca dataset, composed of about 52,000 instructions and demonstrations generated by OpenAI’s text-davinci-003 model. Let’s load the dataset and look at an example:    ``` >>> sft_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\") `>>>` `print``(``sft_dataset``[``1``][``\"text\"``])` `` `Below is an instruction that describes a task. Write a response that` `appropriately completes the request.`  `### Instruction:` `What are the three primary colors?`  `### Response:` `The three primary colors are red, blue, and yellow.` `` ```py   ````", "``` ```", "```py`` As you can see, the goal of this dataset is to train the model to follow a single instruction and generate a coherent and helpful response. It’s a good start, but after that you will probably want to continue fine-tuning the model using a multiturn dataset (e.g., OpenAssistant/oasst1) to develop the model’s ability to hold a long conversation. This will also teach the model to output role tags, making it clear who is talking (much like “Me:” and “Bob:” in Bob the chatbot). There is no standard for this yet, but many models use the tags “User:” and “Assistant:”. OpenAI defined the ChatML format, which uses “<|user|>”, “<|assistant|>”, or “<|system|>” for system messages (e.g., for text similar to our Bob introduction). Each section ends with “<|end|>”. Lastly, Anthropic uses “Human:” and “Assistant:”.    Let’s preprocess the dataset to use Anthropic-style role tags. Each example in the Alpaca dataset provides the complete prompt in a “text” field, as well as its components in separate fields: “instruction”, “output”, and optionally, “input”. The “text” field will be used for training, so let’s use the individual components to compose a new “text” field and replace the existing one:    ```", "```py    Now our previous example looks like this:    ```", "```py   ```", "```py` The training set is ready, so we can run SFT. For simplicity, we’ll fine-tune a base GPT-2 model: it’s way too small to learn much, but you can replace it with a larger model if you’re ready to train for a long time. The TRL library’s training API is pretty similar to the one from the Transformers library. The code is self-explanatory:    ```", "```py    Now on to the DPO phase. We’ll need a human-preference dataset. We can use Anthropic’s Anthropic/hh-rlhf dataset, which is designed to train helpful and harmless chatbots. Let’s load it and look at an example:    ```", "```py` `dict_keys(['chosen', 'rejected'])` `>>>` `print``(``pref_dataset``[``2``][``\"chosen\"``]``.``strip``())` ```", "```py ```", "```py   ```", "```py ```", "```py`In this dataset, the prompt is already included (prepended) in both the chosen answer and the rejected answer. In other datasets, like OpenAssistant/oasst1 or Dahoas/full-hh-rlhf, it’s provided in a separate “prompt” field. The TRL library knows how to handle both cases, so we can go right ahead with the second phase of fine-tuning, using DPO:    ```", "```py    Let’s take a second to appreciate the fact that you now know how to build a large transformer from scratch, pretrain it using NTP (if you have enough time and money), then fine-tune it using SFT and DPO to turn it into a chatbot model. Bravo!    Alternatively, you can simply download a chatbot model directly, already pretrained and fine-tuned. For example, you can download the Mistral-7B-Instruct-v0.3 model, and use it with our `BobTheChatbot` class: you will see that it’s a significantly more pleasant and helpful model than Mistral-7B-v0.3\\. When you ask it to tell you five jokes, it comes up with five *different* jokes, and it adds “I hope you enjoyed these jokes! If you have any other requests, feel free to ask”. Its cookie recipe is clear and detailed. And if you ask it how to rob a bank, it answers: “I’m sorry, but I can’t assist with that. It’s illegal and unethical to provide advice on criminal activities”.    ###### Warning    Mistral-7B-Instruct-v0.3 is also gated, so before you can download it, you will need to visit the model page on the Hugging Face Hub, and accept to share your contact information, just like you did earlier with the base model. Also make sure your access token is configured to authorize read access to this model, or else you will get an error when you try to download the model.    Now that we have a good chat model, how can we get people to use it?```", "```py`` ```", "```py ```", "```py` ```", "```py`` ```", "```py```", "```` ```py ``## From a Chatbot Model to a Full Chatbot System    The last step in building a chatbot is deploying the model inside a complete chatbot system (see [Figure 15-17](#chatbot_diagram)). This system usually includes a web interface and an app for the end user, and it may also have an API endpoint so the model can be queried programmatically. Moreover, to handle complex queries and deliver truly helpful responses, chatbots increasingly rely on a system of integrated tools. For this, the chatbot typically has a component named the *orchestrator* whose role is to coordinate multiple tools to process the user prompt and compose the chatbot’s answer. Here are some of the most important tools:    Calculator      If the user asks “What’s 525.6 * 315 / 3942?”, the orchestrator may detect the presence of a math expression. Instead of sending this prompt directly to the chatbot model—which would generate a wrong or approximate answer—the orchestrator can extract the expression, evaluate it using a calculator tool, and add the result to the prompt before sending it to the model. The augmented prompt might look like this: “User: What’s 525.6 * 315 / 3942?\\nSystem: Calculator result = 42.\\nAssistant:”. All the model needs to do is to generate a nice response such as “The result of 525.6 * 315 / 3942 is 42”. No math needed.    Alternatively, the chatbot model itself can be fine-tuned to invoke tools, such as a calculator. This is called *tool augmentation*, or *function calling*. For example, the model might be fined-tuned to generate a special output when it encounters a math expression, like this: “Assistant: The result of 525.6 * 315 / 3942 is [calculator_tool] 525.6 * 315 / 3942 [/calculator_tool]”. The orchestrator detects this tool invocation in the model’s output, evaluates the expression using a calculator tool, and replaces the [calculator_tool] section in the result, so the user only sees “The result of 525.6 * 315 / 3942 is 42”. Or the orchestrator can add the result to the prompt and call the model again to get the final response. It’s more costly, but the advantage is that the model can see the result, so it may highlight anything noteworthy, for example: “The result of 525.6 * 315 / 3942 is 42\\. It’s interesting that the result is an integer”.      Web search      If the user asks about a URL, the orchestrator can fetch the corresponding web page and inject its text into the prompt. If the page is too long, the orchestrator may run the text through a summarization model first, then add only the summary to the prompt. Or it may chop the text into chunks (e.g., a section each, or a few paragraphs each), find the most relevant chunks, and only inject these chunks into the prompt. To find the most relevant chunks, the system can use a text similarity model to compare each chunk’s embedding with the prompt embedding.    Just like with the calculator tool, the chatbot model itself can be fine-tuned to ask the orchestrator to run a web search. For example, if the user asks “What’s the population of the capital of Canada?”, the model may first output “[search_tool] What is the population of Ottawa? [/search_tool]”. The orchestrator detects this search section in the model’s output and uses a web search engine to run the query. The top results are then fetched and summarized (or the system identifies the relevant chunks), and feeds the result to the chatbot model, along with information about the sources. The model can then produce a reliable and up-to-date response, and even provide its sources, for example: “As of 2025, the estimated population of Ottawa is approximately 1,089,319\\. Source: [*https://worldpopulationreview.com*](https://worldpopulationreview.com)“.      Retrieval Augmented Generation (RAG)      The web search idea can be generalized to all sorts of data sources, including private and structured sources of data, like a company’s SQL database, or PDF documents, knowledge bases, and so on. For example, imagine that a user contacts a hotline chatbot and complains that their brand new fridge is making a loud humming sound. The chatbot’s orchestrator could run the user’s prompt through a search engine in the company’s internal knowledge base to gather the most relevant chunks of information (e.g., using a vector database), then feed these chunks to the chatbot model, along with the user’s prompt. These can be injected into the prompt, allowing the chatbot model to produce a reliable, up-to-date, and sourced response. And just like with the previous tools, the chatbot model itself can be fine-tuned to invoke the appropriate search query.      ###### Tip    This approach can also be used to detect whether the query concerns unsafe topics (e.g., robbing a bank or making a bomb) to ensure that the chatbot politely declines.    Memory (a.k.a., persistent context)      This tool stores user-specific facts and preferences across conversations. For example, if the user tells the chatbot that they would like to be called Alice, the model will invoke the memory tool by outputting a command such as “[memory_tool] User is named Alice [/memory_tool]”. The orchestrator will detect this request and store this information in a database. Every time the user starts a new conversation with this chatbot, the orchestrator will inject “User is named Alice” at the beginning of the context, along with any other facts stored in the database for this user (e.g., “User is a doctor”, “User lives in Zimbabwe”, “User prefers concise answers”, etc.). Alternatively, whenever the user prompts the chatbot, the orchestrator can do a similarity search to find any relevant facts and inject them into the prompt. This allows the memory to grow without crowding the context window.      Agentic behavior      The chatbot model may be fine-tuned to be more autonomous and execute a multistep task with planning and tools. This turns it into an *agentic model*, or simply an *agent*. For example, if the user asks the chatbot to perform a *deep search* on a given topic, the model may start by asking the user for a few clarifications, then it will plan the main steps of its task and go ahead and execute each step; for example, invoking a few web searches (with the help of the orchestrator) or tools, analyzing the results, planning more steps, running more tools, and repeating the process until it has gathered all the information it needs to write a nice document about the topic. Note that a model may just be fine-tuned to reason, without calling tools or functions: this is called a *reasoning model*.      Other tools      Just about any tool you can think of can be added to a chatbot system. Here are just a few examples:    *   A unit or currency conversion tool.           *   A weather tool.           *   A tool to upload a document.           *   A code interpreter: for data analysis, plotting, or running simulations.           *   An integration with an external system, for example, Wolfram Alpha for symbolic math, plots, and scientific knowledge.           *   A nuclear missile tool…​or not! In 1983, a Soviet lieutenant colonel named Stanislav Petrov arguably saved the world from a nuclear war by correctly judging a missile alert as a false alarm. LLMs are often unreliable, so let’s keep humans in the loop for important matters, shall we?                ## Model Context Protocol    If you’re interested in tool-augmented chatbots and agents, you should definitely check out the [*Model Context Protocol* (MCP)](https://homl.info/mcp), an open standard proposed by Anthropic that specifies how your AI system can communicate with *MCP servers* to get access to all sorts of tools and resources, such as the ones listed in [Anthropic’s MCP server repository](https://github.com/modelcontextprotocol/servers). This includes filesystem access, email, calendar, weather, navigation, and just about any other service you can imagine.    MCP does not specify anything about the LLM itself: it’s your LLM orchestrator’s responsibility to interact with the LLM and detect when it wants to access a given tool or resource. For example, you might include instructions in the LLM’s system prompt telling it that it can output a custom JSON message such as `{\"tool\": \"weather\", \"location\": \"Paris\"}` whenever it needs to know the weather in some location (e.g. Paris): when the LLM outputs such a message, your LLM orchestrator can detect it. That’s when MCP comes in: your orchestrator sends an MCP request (i.e., an MCP-compliant JSON message) to a weather MCP server, and once it gets the response (i.e., another MCP-compliant JSON message), it can feed the response to the LLM, which can use it to compose a good answer for the user (e.g., “It will be sunny today in Paris, with a high of 23°C”.).    ###### Tip    The LLM can be instructed to output MCP requests directly rather than custom JSON messages. This way, the orchestrator can just validate the JSON request and determine which MCP server to forward it to (e.g., the weather server).    But why use MCP rather than a more common protocol such as REST or gRPC? Aren’t we’re just querying an API? Well, it’s more than that:    *   Firstly, the connections between the LLM orchestrator and the MCP servers are long-lived, allowing fast, stateful, and bidirectional communication. In the MCP architecture, the client-side components that manage these connections are called *MCP clients*. The software that hosts them—typically your LLM orchestrator—is referred to as the *MCP host*.           *   Secondly, MCP includes an AI-friendly *discovery mechanism* which lets the MCP client ask the MCP server for a rich, textual description of what the service does, and how exactly to use it, including the list of available functions and their parameters. In other words, it’s a self-documented API for AIs. In fact, the MCP server can also ask the MCP client for its capabilities, for example whether it supports displaying images to the user or handling streaming output: this lets the server adapt its responses accordingly.              The real power of MCP comes when you tell the LLM which services are available and instruct it on how to access the discovery mechanism: your LLM can then figure out on its own what each available service does, and how to use it. Connecting your LLM to a new MCP server then becomes little more than adding the server to the orchestrator’s configuration and telling the LLM about it.    That said, building a chatbot from scratch can be complex, and fortunately many libraries and tools are available to simplify the process. Let’s look at some of the most popular ones.    ## Libraries and Tools    Various open source Python libraries are available to implement your own chatbot system, including:    [LangChain](https://www.langchain.com)      A library designed to help you build applications powered by LLMs, by chaining together components such as prompt templates, models, memory, and other tools. It simplifies the orchestration of complex workflows.      [LangGraph](https://www.langchain.com/langgraph)      This built on LangChain and is more specifically designed to build long-running stateful agentic workflows.      [Smolagents](https://homl.info/smolagents)      This is a Hugging Face library designed to build agentic systems. It is a standalone successor to the Transformers Agents library.      [Haystack](https://haystack.deepset.ai)      Haystack lets you build systems that can understand complex questions, retrieve relevant information, and provide accurate answers, typically using RAG.      [LlamaIndex](https://www.llamaindex.ai)      LlamaIndex lets you ingest, index, and query your data (e.g., PDFs, databases, APIs).      There are also several popular open source user interfaces to chat with LLMs locally:    [LM Studio](https://lmstudio.ai)      This is a nice GUI app which lets you easily download and chat with various models. It supports chat history, prompt formatting, and a few other features.      [Ollama](https://ollama.com)      This is a simple command-line tool that lets you download various LLMs and chat with them locally, right in your terminal (e.g., `ollama run mistral:7b`). Ollama can also act as an API server, which can be queried by other systems (e.g., LangChain). The `ollama` Python library lets you query this API easily. Ollama also has support for tools such as a calculator, web search, and more.      [text-generation-webui](https://homl.info/tgw) (TGWUI)      This is a web interface for chatting with local LLMs. It’s one of the most feature-rich and flexible tools available for local LLM use. It has a plug-in system that lets you add a calculator, a document loader, a search tool, and more. It also includes a REST API for integration with other systems like LangChain.      Under the hood, these tools require a backend library to actually run the LLMs. LM Studio and Ollama are based on a highly optimized C++ library named [llama.cpp](https://github.com/ggml-org/llama.cpp), while TGWUI supports multiple backends, including llama.cpp, the Transformers library, ExLlama, AutoGPTQ, and more, so you can pick the backend that runs best on your hardware.    With that, you should have everything you need. For example, you could use LangChain to orchestrate a workflow that uses Ollama to run a local LLM, and Haystack to retrieve relevant information from a vector database. Before we close this chapter, let’s take a brief look at some of the most influential encoder-decoder transformers.`` ``` ```py` ````", "```` ```py````", "```py` ```", "```py```", "``` ```", "```````py``` ``````py```````", "``````py``````", "``````py``````", "``` ```", "```py```", "````py````", "```py```", "````py````", "```py` ```", "```py```", "````py````", "```py```", "````py````", "```py```", "``````py``````", "```````py`  ```` ```py ``# Encoder-Decoder Models    In this chapter, other than the original Transformer architecture, we have focused solely on encoder-only and decoder-only models. This might have given you the impression that encoder-decoder models are over, but for some problems, they are still very relevant, especially for tasks like translation or summarization. Indeed, since the encoder is bidirectional, it can encode the source text and output excellent contextual embeddings, which the decoder can then use to produce a better output than a decoder-only model would (at least for models of a similar size).    The [T5 model](https://homl.info/t5)⁠^([27](ch15.html#id3708)) released by Google in 2019 is a particularly influential encoder-decoder model: it was the first to frame all NLP tasks as text to text. For example, to translate “I like soccer” to Spanish, you can just call the model with the input sentence “translate English to Spanish: I like soccer”, and it outputs “me gusta el fútbol”. To summarize a paragraph, you enter “summarize:” followed by the paragraph, and it outputs the summary. For classification, you only need to change the prefix to “classify:”, and the model outputs the class name as text. For zero-shot classification, the possible classes can be listed in the prompt. This text-to-text approach makes the model very easy to pretrain on a variety of language tasks and just as easy to use. T5 was pretrained using a *masked span corruption* objective, similar to MLM, but masking one or more contiguous sections.    Google also released several variants of T5:    mT5 (2020)      This is a multilingual T5 supporting over 100 languages. It’s great for translation and cross-lingual tasks (e.g., asking a question in English about a Spanish text).      ByT5 (2021)      This is a byte-level variant of T5 that removes the need for tokenization entirely (not even BPE). However, this approach has not caught on as it’s more efficient to use tokenizers.      FLAN-T5 (2022)      This is an instruction-tuned T5, with excellent ZSL and FSL capability.      UL2 (2022)      This is pretrained using several objectives, including masked span denoising like T5, but also standard next token prediction, and masked token prediction.      FLAN-UL2 (2023)      This improved on UL2 using instruction tuning.      Meta also released some encoder-decoder models, starting with BART in 2020\\. This model was pretrained using a denoising objective: the model gets a corrupted text (e.g., masked, modified, deleted, or inserted tokens, shuffled sentences, etc.) and it must clean it up. It’s particularly effective for text generation and summarization. There’s also a multilingual variant named mBART.    Last but not least, the encoder-decoder architecture is quite common for vision models, typically when there are multiple outputs such as in object detection and image segmentation. They’re also common for multimodal models. This leads us to the next chapter, where we will discuss vision transformers and multimodal transformers. It’s time for transformers to open their eyes!    # Exercises    1.  What is the most important layer in the Transformer architecture? What is its purpose?           2.  Why does the Transformer architecture need positional encodings?           3.  What tasks are encoder-only models best at? How about decoder-only models? And encoder-decoder models?           4.  What is the most important technique used to pretrain BERT?           5.  Can you name four BERT variants and explain their main benefits?           6.  What is the main task used to pretrain GPT and its successors?           7.  The `generate()` method has many arguments, including `do_sample`, `top_k`, `top_p`, `temperature`, and `num_beams`. What do these five arguments do?           8.  What is prompt engineering? Can you describe five prompt engineering techniques?           9.  What are the main steps to build a chatbot, starting from a pretrained decoder-only model?           10.  How can a chatbot use tools like a calculator or web search?           11.  What is MCP used for?           12.  Fine-tune BERT for sentiment analysis on the IMDb dataset.           13.  Fine-tune GPT-2 on the Shakespeare dataset (from [Chapter 14](ch14.html#nlp_chapter)), then generate Shakespeare-like text.           14.  Download the [Wikipedia Movie Plots dataset](https://homl.info/movieplots), and use SBERT to embed every movie description. Then write a function that takes a search query, embeds it, finds the most similar embeddings, and lists the corresponding movies.           15.  Use an instruction-tuned model such as Qwen-7B-Instruct to build a little chatbot which acts like a movie expert. Then try adding some RAG functionality, for example by automatically injecting the most relevant movie plot into the prompt (see the previous exercise).              Solutions to these exercises are available at the end of this chapter’s notebook, at [*https://homl.info/colab-p*](https://homl.info/colab-p).    ^([1](ch15.html#id3414-marker)) Ashish Vaswani et al., “Attention Is All You Need”, *Proceedings of the 31st International Conference on Neural Information Processing Systems* (2017): 6000–6010.    ^([2](ch15.html#id3417-marker)) When applying a linear layer to a sequence, all tokens are treated independently, using the same parameters. This is equivalent to using a `Conv1d` layer with `kernel_size=1`. This is why you will sometimes see Transformer diagrams showing convolutional layers instead of linear layers.    ^([3](ch15.html#id3423-marker)) The number of parameters is not public for some models (e.g., Gemini models), so I used some rough estimates. Also, many models have smaller variants, not shown here. Lastly, several other organizations released influential models, such as the Allen Institute for AI (Ai2), Amazon, Baidu, Beijing Academy of AI, Cohere, Huawei, LAION, LMSYS, Nvidia, Stanford University, Talent International Institute (TII), Tsinghua University, xAI, Zhipu AI, and others.    ^([4](ch15.html#id3427-marker)) This is adapted from Figure 1 from “Attention Is All You Need”, with the kind permission of the authors.    ^([5](ch15.html#id3439-marker)) Queries, keys, and values were introduced in [Chapter 14](ch14.html#nlp_chapter) when we discussed dot-product attention.    ^([6](ch15.html#id3452-marker)) This is adapted from the righthand part of Figure 2 from “Attention Is All You Need”, with the kind authorization of the authors.    ^([7](ch15.html#id3475-marker)) Jacob Devlin et al., “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding”, *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies* 1 (2019).    ^([8](ch15.html#id3498-marker)) This is adapted from Figure 1 from the BERT paper, with the kind authorization of the authors.    ^([9](ch15.html#id3515-marker)) Nils Reimers, Iryna Gurevych, “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks”, arXiv preprint arXiv:1908.10084 (2019).    ^([10](ch15.html#id3531-marker)) Geoffrey Hinton et al., “Distilling the Knowledge in a Neural Network”, arXiv preprint arXiv:1503.02531 (2015).    ^([11](ch15.html#id3567-marker)) Alec Radford et al., [“Improving Language Understanding by Generative Pre-Training”](https://homl.info/gpt) (2018).    ^([12](ch15.html#id3586-marker)) Alec Radford et al., [“Language Models Are Unsupervised Multitask Learners”](https://homl.info/gpt2) (2019).    ^([13](ch15.html#id3587-marker)) There were a few minor tweaks, such as using pre-LN rather than post-LN, downscaling the weights depending on the number of residual layers, and tweaks to the BPE tokenizer. Please see the paper for more details.    ^([14](ch15.html#id3595-marker)) Tom B. Brown et al., “Language Models are Few-Shot Learners”, arXiv preprint arXiv:2005.14165 (2020).    ^([15](ch15.html#id3613-marker)) If you are not running the notebook on Colab, you can save the access token in a file and load its content in your code to avoid hardcoding it. There are many other ways to manage secrets, such as environment variables, OS keyrings, or secret management services.    ^([16](ch15.html#id3618-marker)) For the Vatican, it answers Rome, which contains Vatican City. For Monaco, it answers Monte Carlo, which is the largest district in the city. For Burundi, it answers Bujumbura, which was the capital city until 2019\\. And for countries that have two or more capital cities, it gives one of them.    ^([17](ch15.html#id3623-marker)) Brian Lester et al., “The Power of Scale for Parameter-Efficient Prompt Tuning”, arXiv preprint arXiv:2104.08691 (2021).    ^([18](ch15.html#id3627-marker)) Jason Wei et al., “Chain-of-Thought Prompting Elicits Reasoning in Large Language Models”, arXiv preprint arxiv 2201.11903 (2022).    ^([19](ch15.html#id3629-marker)) Xuezhi Wang et al., “Self-Consistency Improves Chain of Thought Reasoning in Language Models”, arXiv preprint arXiv:2203.11171 (2022).    ^([20](ch15.html#id3631-marker)) Shunyu Yao et al., “Tree of Thoughts: Deliberate Problem Solving with Large Language Models”, arXiv preprint arXiv:2305.10601 (2023).    ^([21](ch15.html#id3634-marker)) Yilun Du et al., “Improving Factuality and Reasoning in Language Models through Multiagent Debate”, arXiv preprint arXiv:2305.14325 (2023).    ^([22](ch15.html#id3635-marker)) Aman Madaan et al., “Self-Refine: Iterative Refinement with Self-Feedback”, arXiv preprint arXiv:2303.17651 (2023).    ^([23](ch15.html#id3639-marker)) Patrick Lewis et al., “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks”, arXiv preprint arXiv:2005.11401 (2020).    ^([24](ch15.html#id3655-marker)) Paul Christiano et al., “Deep reinforcement learning from human preferences”, arXiv preprint arXiv:1706.03741 (2017).    ^([25](ch15.html#id3661-marker)) Rafael Rafailov et al., “Direct Preference Optimization: Your Language Model is Secretly a Reward Model”, arXiv preprint arXiv:2305.18290 (2023).    ^([26](ch15.html#id3667-marker)) There’s a slight difference for the tokens “Argentina” and “is”, which I assume is due to the accumulation of floating-point errors in this large model.    ^([27](ch15.html#id3708-marker)) Colin Raffel et al., “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer”, arXiv preprint arXiv:1910.10683 (2019).`` ``` ```py` ``````py```````", "```````py`` ``````py```````", "```````py``` ``````py```````", "```````py```` ```py```````", "```````py```````", "``````py``````", "```````py`````` ```py```````", "```````py```````", "``` ```", "```py```", "````py````", "```py```", "````py` ````", "```````py```````", "`````````"]