- en: Chapter 15\. Transformers and transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the paper [“Attention Is All You Need” by Ashish Vaswani et al.](https://oreil.ly/R7og7)
    in 2017, the field of AI was changed forever. While the abstract of the paper
    indicates something lightweight and simple—an evolution of the architecture of
    convolutions and recurrence (see Chapters [4](ch04.html#ch04_using_data_with_pytorch_1748548966496246)
    through [9](ch09.html#ch09_understanding_sequence_and_time_series_data_1748549698134578)
    of this book)—the impact of the work was, if you’ll forgive the pun, transformative.
    It utterly revolutionized AI, beginning with NLP. Despite the authors’ claim of
    the simplicity of the approach, implementing it in code is and was inherently
    complex. At its core was a new approach to ML architecture: *Transformers* (which
    we capitalize to indicate that we’re referring to them as a concept).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we’ll explore the ideas behind Transformers at a high level,
    demonstrating the three main architectures: encoder, decoder and encoder-decoder.
    Please note that we will just be exploring at a very high level, giving an overview
    of how these architectures work. To go deep into these would require several books,
    not just a single chapter!'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll then explore *transformers*, which we lowercase to indicate that they
    are the APIs and libraries from Hugging Face that are designed to make using Transformer-based
    models easy to use. Before transformers, you had to read the papers and figure
    out how to implement the details for yourself for the most part. So, the Hugging
    Face transformers library has widened access to models created using the Transformer
    architecture and has become the de facto standard for using the many models that
    have been created using the transformer-based architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Just to clarify, for the rest of this chapter, I’ll refer to the architecture,
    models, and concepts as Transformers (with a capital *T*) and the Hugging Face
    libraries as transformers (with a lowercase *t*) to prevent confusion.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the publication of the original paper mentioned in the introduction to
    this chapter, the field of Transformers has evolved and grown, but its underlying
    basis has remained pretty much the same. In this section, we’ll explore this.
  prefs: []
  type: TYPE_NORMAL
- en: When working with LLMs anywhere (not just with Hugging Face), you’ll hear of
    the terms *encoder*, *decoder*, and *encoder-decoder*. Therefore, I think it’s
    a good idea for you to get a high-level understanding of them. Each of these architectures
    represents a different approach to text management—be it processing, classification,
    or generation. They have specific strengths for particular scenarios, and to optimize
    for your scenario, it’s good to understand them so you can choose the appropriate
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder Architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Encoder-only architectures (e.g., BERT, RoBERTa) generally excel at *understanding*
    text because of how rigorous they are in processing it. They’re bidirectional
    in nature, being able to “see” the entire input sequence at once. With that nature
    of understanding, they’re particularly effective for tasks that require a deep
    understanding and comprehension of the text and its semantics. So, they’re particularly
    suited for tasks such as classification, named-entity recognition, and extraction
    of meaning for things like question answering. Their strength is transforming
    text into rich, contextual representations, but they’re not designed to *generate*
    new text.
  prefs: []
  type: TYPE_NORMAL
- en: You can see the encoder-based architecture in [Figure 15-1](#ch15_figure_1_1748549808951859).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-1\. Encoder-based architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s explore this architecture in a little more detail. It begins with the
    tokenized inputs, which are then passed to the self-attention layer.
  prefs: []
  type: TYPE_NORMAL
- en: The self-attention layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Self-attention* is the core mechanism that allows tokens to “pay attention”
    to other tokens in the input sequence. So, for example, consider the sentence
    “I went to high school in Ireland, so I had to study how to speak Gaelic.” The
    last word in this sentence is *Gaelic*, and it’s effectively triggered by the
    word *Ireland* earlier in the sentence. If a model pays attention to the entire
    sentence, it can predict the word *Gaelic* to be the next word. On the other hand,
    if the model didn’t pay attention to the entire sentence, then it might interpret
    from the sentence something more appropriate to “how to speak,” such as *politely*
    or another adjective.'
  prefs: []
  type: TYPE_NORMAL
- en: However, the self-attention mechanism—by considering the entire sentence—can
    understand context like that more granularly. It works by having each token in
    the sentence get three vectors associated with it. These are the query (Q) vector
    (aka “What am I looking for that’s relevant to this token?”), the key (K) vector
    (aka “What tokens might reference me?”), and the value (V) vector (aka “What type
    of information do I carry?”). The representations in these vectors are learned
    over time, in much the same process as we saw in earlier chapters of this book.
    An attention score is then calculated as a function of these, and using Softmax,
    the embeddings for the words will be updated with the attention details, bending
    word embeddings closer to one another when there are similarities learned between
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Do note that self-attention is generally bidirectional, so the order of the
    words doesn’t matter.
  prefs: []
  type: TYPE_NORMAL
- en: The self-attention mechanism usually also has the context of *heads*, which
    are effectively multiple, parallel instances of the three vectors (Q, K, and V)
    that we saw previously, which can learn different representations and effectively
    specialize in different aspects of the input. A high-level representation of these
    heads is shown in [Figure 15-2](#ch15_figure_2_1748549808951907).
  prefs: []
  type: TYPE_NORMAL
- en: Thus, each head has its own set of learned weights for Q, K, and V vectors.
    The processing and learning for these vectors is done in parallel, with their
    results concatenated, and their final output projection is then a combination
    of information from each of the heads. As models have grown larger over time,
    one of the factors for this growth is the number of heads. For example, BERT-base
    has 12 heads and BERT-large has 16 heads. Architectures that also use a decoder,
    such as GPT, have grown similarly. GPT-2 had 12 attention heads, whereas GPT-3
    grew to 96!
  prefs: []
  type: TYPE_NORMAL
- en: Returning to [Figure 15-1](#ch15_figure_1_1748549808951859), the self-attention
    instance (in which [Figure 15-2](#ch15_figure_2_1748549808951907) can be encapsulated
    into the self-attention box from [Figure 15-1](#ch15_figure_1_1748549808951859))
    then outputs to a *feedforward network* (FFN), which is often more accurately
    referred to as a *position-wise feedforward network*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-2\. Multihead self-attention
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The feedforward network layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The FFN layer is vital in supporting the model’s capacity to learn complex patterns
    in the text. It does this by introducing nonlinearity into the model.
  prefs: []
  type: TYPE_NORMAL
- en: Why is that important? First of all, let’s understand the difference between
    linearity and nonlinearity. A *linear equation* is one for which the value is
    relatively easy to predict. For example, consider an equation that determines
    a house price. A linear version of this might be the cost of the land plus a particular
    dollar amount per square foot, and every house would follow the same formula.
    But as we know, house prices are far more complex than this—they don’t (unfortunately)
    follow a simple linear equation.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding sentiment can be the same. So, for example, if you were to assign
    coordinates on a graph (like we did when explaining embeddings in [Chapter 7](ch07.html#ch07_recurrent_neural_networks_for_natural_language_pro_1748549654891648))
    to the words *good* and *not*, where *good* might be +1 and *not* might be –1,
    then a linear relationship between these for *not good* would give us 0, which
    is neutral, whereas *not good* is clearly negative. So, we need equations that
    are more nuanced (i.e., nonlinear) when capturing sentiment and effectively understanding
    our text.
  prefs: []
  type: TYPE_NORMAL
- en: That’s the job of the FFN. It achieves this nonlinearity by expanding the dimensions
    of its input vector, applying a transformation to that, and using ReLU to “remove”
    the negative values (and thus remove the linearity) before restoring the vector
    back to its original dimensions. You can see this in [Figure 15-3](#ch15_figure_3_1748549808951933).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-3\. A feedforward network
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The underlying math and logic behind how it works are a little beyond the scope
    of this book, but let’s explore them with a simple example. Consider this code,
    which simulates what’s happening in [Figure 15-3](#ch15_figure_3_1748549808951933):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We start with a simple 2D tensor: `[–1.0, 2.0].`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first linear layer has the following weights and biases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'When we pass our 2D tensor through this layer to get `layer1_out`, the matrix
    multiplication gives us a 4D output that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'There are two negative values in this layer (–3 and –0.5), so when we pass
    it through the ReLU, they are set to zero and our matrix becomes this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This process is called *bending*. By taking these values out, we’re now introducing
    nonlinearity into the equation. The relationships between the values have become
    much more complex, so a process that attempts to learn the parameters of those
    relationships will have to deal with that complexity, and if it succeeds in doing
    so, it will avoid the linearity trap.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll convert back to a 2D tensor by going through another layer, with
    weights and biases like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the layer will then look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: So, the effect of the FFN is to take in a vector and output a vector of the
    same dimension, with linearity removed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can explore this with our simple code. Consider what happens when we take
    our input and apply simple, linear changes to it. So, if we take [–1.0, 2.0] and
    double it to [–2.0, 4.0], the nonlinearity introduced by the FFN will mean that
    the output won’t be a simple doubling. And similarly, if we negate it, the output
    again won’t be a simple negation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Over time, the parameters that are learned for the weights and biases should
    maintain the relationships between the tokens and allow the network to learn more
    nuanced, nonlinear equations that define the overall relationships between them.
  prefs: []
  type: TYPE_NORMAL
- en: Layer normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Referring back to [Figure 15-1](#ch15_figure_1_1748549808951859), the next step
    in the process is called *layer normalization*. At this point, the goal is to
    stabilize the data flowing through the neural network by removing outliers and
    high variance. Layer normalization does this by calculating the mean and variance
    of the input features, which it then normalizes and scales/shifts before outputting
    (see [Figure 15-4](#ch15_figure_4_1748549808951956)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-4\. Layer normalization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From a statistical perspective, the idea of removing the outliers from using
    mean and variance and then normalizing them is quite straightforward. I won’t
    go into details on the statistics here, but that’s generally the goal of doing
    these types of calculations.
  prefs: []
  type: TYPE_NORMAL
- en: The *Scale and Shift* box then becomes a mystery. Why would you want to do this?
    Well, if you dig a little bit into the logic, the idea here is that the process
    of *normalization* will drive the mean of a set of values to 0 and the standard
    deviation to 1\. The process itself can destroy distinctiveness in our input features
    by making them too alike. So, if there’s a process that we can use to return some
    level of variance to them with parameters that are learned, we can clean up the
    data without destroying it—meaning we won’t throw the baby out with the bathwater!
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, multiplying our outputs by values with offsets can change this. These
    values are typically called *gamma* and *beta* values, and they act a little like
    weights and biases. It’s probably easiest to show this in code.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, consider this example, in which we’ll take an input feature containing
    some values and then normalize them. We’ll see that the normalized values will
    have a mean of 0 and a standard deviation of 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'But when we move the values through the scale and shift by using gamma and
    beta values, we get a new set of parameters that maintain a closer relationship
    to the originals but with massive variance (aka noise) removed. The output of
    this code should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Like the FFN, this is effectively destroying and then reconstructing the features
    in a clever way. In this case, it’s designed to do it to remove variance, which
    has the effect of amplifying or dampening features as needed by shifting the overall
    distribution to better ranges for activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: I like to think of this as what you do with your TV to get a better image—sometimes,
    you adjust the contrast or the brightness. By finding the optimal values of these
    settings, you can see the important details of a particular image better. Think
    of the contrast as the scale and the brightness as the shift. If a network can
    learn these for your input features, it will improve its ability to understand
    them!
  prefs: []
  type: TYPE_NORMAL
- en: Repeated encoder layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Referring back to [Figure 15-1](#ch15_figure_1_1748549808951859), you’ll see
    that the self-attention, feedforward, and layer normalization layers can be repeated
    N times. Typically, smaller models will have 12 instances and larger ones will
    have 24\. The deeper the model, the more computational resources are required.
    More layers provide more capacity to learn complex patterns, but of course, this
    means longer training time, more GPU memory overhead, and potentially greater
    risks of overfitting. Additionally, larger models can impact the corresponding
    data requirements, leading to a possibly negative knock-on effect for complexity.
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, there’s a trade-off between the depth of the model (the number
    of layers) and the width (the size of each layer, including the number of heads).
    In some cases, models can reuse the same layer multiple times to reduce the overall
    parameter count—and ALBERT is an example of this.
  prefs: []
  type: TYPE_NORMAL
- en: The Decoder Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the encoder architecture specializes in understanding text by having attention
    across the entire context of the input sequence, the decoder architecture serves
    as the generative powerhouse. It is designed to produce sequential outputs one
    element at a time. While the encoder processes all inputs simultaneously (or as
    many of them as it can, based on the parallelization of the system), the decoder
    operates autoregressively. It generates each output token while considering both
    the encoded input representations *and* the previously generated outputs. The
    goal is to maintain coherence and contextual relevance through the process.
  prefs: []
  type: TYPE_NORMAL
- en: You can see a diagram of the encoder architecture in [Figure 15-5](#ch15_figure_5_1748549808951977).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1505.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-5\. The decoder architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s explain this from the top down. The first box is the previously generated
    tokens. In a pure decoder architecture, this is the set of tokens that have already
    been generated or provided. When they’re provided, they’re typically called the
    prompt. So, say you provide the following tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'after one is run through the decoder, the token for “clap” will be generated.
    You will now have these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: These tokens will flow into the box for token embedding + positional encoding.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding token and positional encoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This transforms each token into a vector representation called an *embedding*
    (as explained in [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)),
    which clusters words of similar semantic meaning in a similar vector space. Remember
    that these embeddings are learned over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for example, if we think about the words *it* and *cla*p at the end of
    the aforementioned token list, they may have token embeddings that look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: I have simplified the embedding to just three dimensions for readability.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to perform the *positional* encoding, which is a huge innovation
    in Transformers. In addition to an encoding in a vector space for the semantics
    and meaning of the word, an innovative method using sine and cosine waves is performed
    to encode the word’s position and the impact of this position on neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for example, given that we have 3D vectors for our encodings, we’ll create
    a 3D vector using sine waves for the odd-numbered indices and cosine waves for
    the even-numbered ones, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We then add these together to get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'While this may seem arbitrary, the positional encodings actually come from
    a specific formula. These are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: If you plot these values as a table, they’ll look like [Table 15-1](#ch15_table_1_1748549808960051).
  prefs: []
  type: TYPE_NORMAL
- en: Table 15-1\. Positional encodings
  prefs: []
  type: TYPE_NORMAL
- en: '| Position | Dimension 0 | Dimension 1 | Dimension 2 | Dimension 3 | Dimension
    4 | Dimension 5 | Dimension 6 | Dimension 7 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **0** | 0.000 | 1.000 | 0.000 | 1.000 | 0.000 | 1.000 | 0.000 | 1.000 |'
  prefs: []
  type: TYPE_TB
- en: '| **1** | 0.841 | 0.540 | 0.100 | 0.995 | 0.010 | 1.000 | 0.001 | 1.000 |'
  prefs: []
  type: TYPE_TB
- en: '| **2** | 0.909 | –0.416 | 0.199 | 0.980 | 0.020 | 1.000 | 0.002 | 1.000 |'
  prefs: []
  type: TYPE_TB
- en: '| **3** | 0.141 | –0.990 | 0.296 | 0.955 | 0.030 | 1.000 | 0.003 | 1.000 |'
  prefs: []
  type: TYPE_TB
- en: The ultimate goal here is to have a relationship between each position on the
    list and every other position that, in some dimensions, has tokens that are typically
    far apart being a little closer and those that are typically closer being a little
    further apart!
  prefs: []
  type: TYPE_NORMAL
- en: So, imagine you have an input sequence of four tokens in positions 0 through
    3, as charted in the table. The token in position 3 is as far away as possible
    from the token in position 0, so they are at extreme ends of the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: With a positional encoding like this, we are given the possibility that in some
    dimensions, they are closer together. You can see in the first column that the
    values for dimension 0 places position 3 closer to position 0 than either of the
    others, whereas in the column for dimension 2, they are further apart. By using
    these positional encodings, we’re opening up the possibility that words can be
    clustered, even if they’re far apart in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: Think back to the sentence “I went to high school in Ireland, so I had to study
    how to speak Gaelic.” In that case, the final token “Gaelic” was most accurate
    because it described a language in “Ireland,” which was earlier in the sentence.
    Without positional encoding, this would have been missed!
  prefs: []
  type: TYPE_NORMAL
- en: Also, the positional encodings are *added* to the token embeddings, so they
    provide a sort of pressure to keep together the words that might be semantically
    related in different parts of the sentence, but they don’t completely override
    the embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: This is then fed into the multihead masked attention. We’ll look at that next.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding multihead masked attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Earlier in this chapter, in the section on attention, we saw how the Q, K, and
    V vectors for each token are learned and used to update the embeddings of the
    word with attention to the other words. The idea behind *masked attention* updates
    this to ignore words that we shouldn’t be paying attention to. In other words
    (sic), the goal is that we should only pay attention to *previous* positions in
    the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, imagine you have a sequence of eight words and you want to predict the
    ninth. When you’re processing the third word in the sentence, it should only pay
    attention to the second and first word but nothing after them. You can achieve
    this with a triangular matrix like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: So, imagine this for a set of words like *the*, *cat*, and *sat* (see [Table 15-2](#ch15_table_2_1748549808960104)).
  prefs: []
  type: TYPE_NORMAL
- en: Table 15-2\. Simple masked attention
  prefs: []
  type: TYPE_NORMAL
- en: '|   | the | cat | sat |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| the | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| cat | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| sat | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: When processing *the*, using this method means we can only pay attention to
    *The* itself. When processing *cat*, we can pay attention to both *the* and *cat*.
    When processing sat, we can pay attention to *the*, *cat*, and *sat*.
  prefs: []
  type: TYPE_NORMAL
- en: So, recalling that the K, Q, and V vectors will amend the embeddings for the
    word in a way that bends them closer together for instances where the words may
    not have close syntactical meaning but are impacting one another through attention
    (like *Ireland* and *Gaelic* in the earlier example), the goal of the masked attention
    layer will only do this bending for words that we’re allowed to pay attention
    to.
  prefs: []
  type: TYPE_NORMAL
- en: When this is performed multiple times, in parallel, across multiple heads, and
    aggregated together, we get an attention adjustment of the embeddings that’s very
    similar to the one we did with the encoder—except that the masking prevents any
    amendment of the token from words *after* it in the sequence, particularly those
    that are generated.
  prefs: []
  type: TYPE_NORMAL
- en: Adding and normalizing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Next, we take the attention output from the masked attention layer and add it
    to the original input. This is called making a *residual connection*.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for example, the process might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: As we think about this, we’ll see that we don’t *replace* the original information
    with the attention mechanism but instead we *add* the new learned embeddings from
    the attention mechanism, thus preserving the original information. Over time,
    this will have the effect of helping the network learn. If some attention updates
    aren’t useful, then the network will just make them close to zero through the
    backpropagation of gradients.
  prefs: []
  type: TYPE_NORMAL
- en: This is beyond the scope of this book, and it’s generally found in the papers
    behind the creation of these models. But ultimately, the goal here is to get rid
    of a problem called the *vanishing gradient problem*—in which, if the original
    input was *not* maintained, then the gradients of the attention layer can get
    smaller and smaller with successive layers, thus limiting the number of layers
    you can use. But if you always add the gradients to the original input, then there
    will be a floor—such as the [0.5, –0.3, 0.7, 0.1] for the cat gradient previously
    mentioned—so the small changes from the attention gradients won’t push these values
    close to zero and cause the overall gradients to vanish.
  prefs: []
  type: TYPE_NORMAL
- en: This is then pushed through a layer normalization, as described in the encoder
    chapter, to remove outliers while keeping the knowledge of the sequences intact.
  prefs: []
  type: TYPE_NORMAL
- en: The feedforward layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The feedforward layer operates in exactly the same way as those layers used
    in encoders (see earlier in this chapter), with the goal of reducing any linear
    dependencies in the token sequence. The output from this is again added to the
    original data and then normalized, the logic being that the process of removing
    the outliers with the FFN should also prevent the gradients from vanishing and
    thus preserve important information. The normalization also keeps the values in
    a stable range, as repeatedly adding as we’re doing here might push some values
    far above 1.0 or below –1.0, and normalized values in these ranges tend to be
    better for matrix calculation.
  prefs: []
  type: TYPE_NORMAL
- en: We can repeat this process of masked attention -> add and normalize -> feedforward
    -> add and normalize multiple times before we get to the next layer, where we’ll
    use the learned values to predict the next token.
  prefs: []
  type: TYPE_NORMAL
- en: The linear and Softmax layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The linear and Softmax layers are responsible for turning the decoder’s representations
    into probabilities for the next token.
  prefs: []
  type: TYPE_NORMAL
- en: The linear layer will learn representations for each of the words in the dictionary
    with the transposed size of the decoder’s representations. This is a bit of a
    mouthful, so let’s explore it with an example.
  prefs: []
  type: TYPE_NORMAL
- en: Say our decoder output, having flowed through all the layers, is a 4D representation,
    like in [Table 15-3](#ch15_table_3_1748549808960134).
  prefs: []
  type: TYPE_NORMAL
- en: Table 15-3\. Simulated decoder output
  prefs: []
  type: TYPE_NORMAL
- en: '|   |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.2 | –0.5 | 0.8 | –0.3 |'
  prefs: []
  type: TYPE_TB
- en: We now have a weights matrix for each word in our vocabulary that is learned
    during training, and that matrix might look like the one in [Table 15-4](#ch15_table_4_1748549808960160).
  prefs: []
  type: TYPE_NORMAL
- en: Table 15-4\. Weights matrix for words in our vocab
  prefs: []
  type: TYPE_NORMAL
- en: '| cat | dog | sat | mat | the |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1.0 | 0.5 | 2.0 | 0.3 | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| -0.3 | 0.8 | 1.5 | 0.4 | 0.2 |'
  prefs: []
  type: TYPE_TB
- en: '| 2.0 | 0.3 | 2.1 | 0.5 | 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.4 | 0.6 | 0.9 | 0.2 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: Note that the decoder representation is 1 × 4 and that each matrix for each
    word is 4 × 1. That’s the transposition, and multiplying them out is now easy.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for *cat*, our final score will be this:'
  prefs: []
  type: TYPE_NORMAL
- en: (0.2 × 1.0) + (–.5 × –.3) + (0.8 × 2.0) + (–0.3 × 0.4) = 1.8
  prefs: []
  type: TYPE_NORMAL
- en: 'We can then get final scores for each word, as in [Table 15-5](#ch15_table_5_1748549808960183):'
  prefs: []
  type: TYPE_NORMAL
- en: Table 15-5\. Final scores for each word
  prefs: []
  type: TYPE_NORMAL
- en: '| cat | dog | sat | mat | the |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1.8 | -0.2 | 1.1 | 0.2 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: Using the Softmax function, these are then turned into probabilities, as in
    [Table 15-6](#ch15_table_6_1748549808960204).
  prefs: []
  type: TYPE_NORMAL
- en: Table 15-6\. Probabilities from Softmax function
  prefs: []
  type: TYPE_NORMAL
- en: '| cat | dog | sat | mat | the |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 47.5% | 6.4% | 23.6% | 9.6% | 12.9% |'
  prefs: []
  type: TYPE_TB
- en: And then we can take the highest-probability word as the next token, in a process
    called *greedy decoding*. Alternatively, we can take 1 from *k* possible top values,
    in a process called *top-k decoding*, in which we pick, for example, the top three
    probabilities and choose one value from there.
  prefs: []
  type: TYPE_NORMAL
- en: This is then fed back into the top box as the new token list so that the process
    can continue to predict the next token.
  prefs: []
  type: TYPE_NORMAL
- en: And that’s pretty much how decoders work, at least from a high level. In the
    next section, we’ll look at how they can be combined in the encoder-decoder architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The Encoder-Decoder Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *encoder-decoder architecture*, also known as *sequence-to-sequence*, combines
    the two aforementioned architecture types. It does this to tackle tasks that require
    transformation between input and output sequences of varying lengths. It’s proven
    to be very effective for machine translation in particular, but it also can be
    used in models for text summarization and question answering.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in [Figure 15-6](#ch15_figure_6_1748549808951997), it’s very
    similar to the decoder architecture, for the most part. The difference is the
    addition of a cross-attention layer that takes in the output from the encoder
    and injects it into the middle of the workflow.
  prefs: []
  type: TYPE_NORMAL
- en: The encoder will process the entire input sequence, creating a rich contextual
    representation that captures the full meaning of the input. The decoder layer
    can then query this, combining it with its representations to allow the decoder
    to focus additionally on relevant parts of the input when generating each new
    token.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1506.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-6\. The encoder-decoder architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, you might wonder at this point why the encoder-decoder architecture needs
    the encoder’s output, which it merges with cross-attention. Why can’t it just
    unmask in its own self-attention block? The fundamental reason boils down to the
    fact that this is more powerful for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Separation of concerns and parameter focus
  prefs: []
  type: TYPE_NORMAL
- en: If the decoder self-attention block were to be unmasked, it would have to handle
    the tasks of both understanding the input *and* generating the output simultaneously.
    That could lead to issues with learning because there’s a poor target. But if
    we separate them, each can focus on its own specialized role.
  prefs: []
  type: TYPE_NORMAL
- en: Quality
  prefs: []
  type: TYPE_NORMAL
- en: If we separate concerns, each role can build up a rich representation that’s
    suitable for its task. In particular with the encoder, we have a well-known, battle-tested
    architecture for artificial understanding that we know works for that task.
  prefs: []
  type: TYPE_NORMAL
- en: The major innovation here is the *cross-attention block*. We can demonstrate
    the intuition behind this with the analogy of a human language translator. When
    a person translates a sentence from French to English, they don’t just memorize
    the entire French corpus and then write English. Instead, while writing the English
    words, they actively look at different parts of the French sentence, focusing
    on the most relevant parts of the sentence for the word they are currently writing.
  prefs: []
  type: TYPE_NORMAL
- en: In French, the sentence “Le chat noir” translates to “The black cat,” but the
    noun and adjective are reversed. A straight translation would be “The cat black.”
    The human translator, when paying attention, would know this and would focus on
    other words in the French sentence. Cross-attention does the same thing. As the
    decoder generates each new word, it needs to refer to the source material to figure
    out what word to generate next. The words that have gone before in its output
    may not be enough.
  prefs: []
  type: TYPE_NORMAL
- en: You can see this mechanism in action in [Figure 15-7](#ch15_figure_7_1748549808952017).
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, an encoder creates a rich, contextual representation of the input
    because it artificially understands the source sentence. Cross-attention is a
    selective spotlight that highlights what the model believes to be the most relevant
    parts of this understanding for each word it generates, and this highlighting
    makes the model more effective at generating the correct words. You can see how
    this is very effective for machine translation, but it’s not much of a stretch
    to understand how it might be useful for other tasks!
  prefs: []
  type: TYPE_NORMAL
- en: The mechanism for cross-attention works with the K, Q, and V vectors as before,
    but the innovation here is that the Q vector will code from the decoder, while
    the K and V vectors will come from the encoder.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes a very high-level look at Transformers and how they work. There’s
    lots more detail—in particular, about *how* they learn things like the Q, K, and
    V vectors—that’s beyond the scope of this chapter, and I’d recommend reading the
    original paper to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s switch gears to the transformers (with a lowercase *t*) API from
    Hugging Face, which makes it really easy for you to use Transformer-based models
    in code.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_1507.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15-7\. Cross-attention
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The transformers API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At their core, transformers provide an API for working with pretrained models
    that use Python and PyTorch. The library’s success stems from three key innovations:
    a simple interface for using pretrained models, an extensive collection of pretrained
    models (as we explored in [Chapter 14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797)),
    and a vibrant community that frequently contributes improvements and new models.'
  prefs: []
  type: TYPE_NORMAL
- en: The library has evolved beyond its NLP roots to support multiple types of models,
    including those that support computer vision, audio processing, and multimodal
    tasks. Originally, the goal of the transformer-based architecture was to be effective
    in learning how a sequence of tokens is followed by another sequence of tokens.
    Then, innovative models built on this idea to allow concepts such as sound to
    be expressed as a sequence of tokens, and as a result, transformers could then
    learn sound patterns.
  prefs: []
  type: TYPE_NORMAL
- en: For developers, transformers offer multiple abstraction levels. The high-level
    pipeline API, which we explored in [Chapter 14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797),
    enables immediate use of models for common tasks, while lower-level interfaces
    provide fine-grained control for custom implementations. Also, the library’s modular
    design allows developers to mix and match components like tokenizers, models,
    and optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps most powerfully, transformers emphasize transparency and reproducibility.
    All model implementations are open source, well documented, and accompanied by
    model cards describing their capabilities, limitations, and ethical considerations.
    It’s a wonderful learning process to crack open the transformers library on GitHub
    and explore the source code for common models like GPT and Gemma.
  prefs: []
  type: TYPE_NORMAL
- en: This commitment to openness has made the transformers API an invaluable tool
    for you, and it’s something that’s well worth investing your time to learn!
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797),
    we explored how to access a model from the Hugging Face Hub, and we saw how easy
    it was to instantiate one and then use it with the various pipelines. We’ll go
    a little deeper into that in this chapter, but let’s begin by installing the requisite
    libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Many of the models in the Hugging Face Hub, which are accessible via transformers,
    need you to have an authentication token on Hugging Face, along with permission
    to use that model. In [Chapter 14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797),
    we showed you how to get an access token. After that, depending on the model you
    choose to use, you may need to ask permission on its landing page if you want
    access to it. You also learned how to use the transformers library in Google Colab,
    but there are, of course, many other ways to use transformers other than in Colab!
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have a token, you can use it in your Python code like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, if you prefer, you can set an environment variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: With that, your development environment can now support development with transformers.
    Next, we’ll look at some core concepts within the library.
  prefs: []
  type: TYPE_NORMAL
- en: Core Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of important core concepts of using transformers that you
    can take advantage of. The simplicity of the transformers design hides the core
    concepts from you until you need them, but let’s take a look at some of them here
    in a little more detail.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start with the pipelines that you learned about in [Chapter 14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797).
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `pipeline` class implements transformers’ most useful abstraction, with
    a goal of making complex transformer operations accessible with minimal code.
    You can get the core default functionality of the model by using the appropriate
    pipeline with its defaults, and you can also override the defaults to create custom
    functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines encapsulate all of ML processing—from data preprocessing to model
    inference to result formatting—in a single method.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see the power of pipelines in action, let’s look at an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: In this case, we didn’t specify a model but rather a scenario (`sentiment-analysis`),
    and the pipeline configuration chose the default model for that scenario and did
    all of the initialization for us. As you saw in Chapters [5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)
    and [6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888),
    when you’re dealing with text, you need to tokenize and sequence it. You also
    need to know which tokenizer was used by a particular model so you can ensure
    that your text is encoded correctly and then converted to tensors in the correct,
    normalized format. You also need to parse and potentially detokenize the output,
    yet none of that code is present here. Instead, you simply say that you want sentiment
    analysis and then point the pipeline toward the text you want to classify.
  prefs: []
  type: TYPE_NORMAL
- en: We’re not just limited to text sentiment analysis, of course. We can also do
    text classification, generation, summarization, and translation. Other scenarios
    include entity recognition and question answering. As more multimodal transformer-based
    models come online, there’s also image classification and segmentation, object
    detection, image generation, and audio scenarios including speech recognition
    and generation.
  prefs: []
  type: TYPE_NORMAL
- en: While there are default models for a scenario, like we saw just now, there’s
    also the ability to override the defaults and send custom parameters to a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for example, with text generation, we can use the default experience like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Or we can customize it further by passing parameters for things like the number
    of tokens to generate (`max_length`), the temperature (how creative it will be),
    and the number of tokens to evaluate when outputting a new one (`top_k`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This gives you the flexibility you need to have fine control of any particular
    model, meaning it lets you set your desired parameters to override the default
    behavior. Importantly, the pipeline’s abstraction handles the crucial steps of
    using the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'To reiterate: when you’re using a pipeline, you’re getting more than just the
    model download. Depending on your scenario, you’ll typically get the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Model loading
  prefs: []
  type: TYPE_NORMAL
- en: When you specify a model name, the pipeline API automatically downloads and
    caches the model and any associated tools, such as the tokenizer.
  prefs: []
  type: TYPE_NORMAL
- en: Preprocessing
  prefs: []
  type: TYPE_NORMAL
- en: This takes your raw inputs in their typical data format (strings, bitmaps, wave
    files, etc.) and turns them into model-compatible formats, even when multiple
    steps are needed. For example, it can tokenize a string and then turn the resulting
    tokens into embeddings or tensors.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, this helps you with the specific tokenization strategy
    that a particular model needs. One tokenization scheme does not fit all!
  prefs: []
  type: TYPE_NORMAL
- en: Batching
  prefs: []
  type: TYPE_NORMAL
- en: This abstracts away the need for you to calculate optimal batch sizes. It will
    efficiently handle batching for you while respecting memory constraints.
  prefs: []
  type: TYPE_NORMAL
- en: Post-processing
  prefs: []
  type: TYPE_NORMAL
- en: Models output tensors of probabilities, and the pipeline will turn them into
    a human-readable format that you or your code can work with.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve had a quick look at pipelines and what they are (and you’ll be
    using them a lot in this book), let’s continue our tour of the core concepts and
    switch to tokenizers.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve spoken about tokenizers a lot, and we’ve even built our own in [Chapter 5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759)
    and [Chapter 6](ch06.html#ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888).
    But prebuilt tokenizers can be very powerful and useful tools as you create your
    own apps. Hugging Face transformers give you the `AutoTokenizer` class, and they
    make your life a lot easier when you’re dealing with tokenizers by handling many
    of the complex scenarios for you. Even if you are creating your own models and
    training them from scratch, using an AutoTokenizer is probably a much smarter
    way of handling this task, rather than rolling your own.
  prefs: []
  type: TYPE_NORMAL
- en: To go a little deeper, the tokenizer is fundamental to how transformer models
    will process text. It’s the first step in converting your raw text into a format
    that the model can work with. It’s also often overlooked in discussions of building
    models, and that’s a big mistake. The tokenization strategy is vital in the design
    of any system, and a badly designed one can negatively impact your overall model
    performance. Therefore, it’s important to have a well-designed tokenizer for the
    task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, the tokenizer’s job is to break down text into smaller, numeric
    units called tokens. They can be words, parts of words (aka *subwords*), or even
    individual characters. To choose the right tokenization strategy, you’ll need
    to make trade-offs among vocabulary size, the length of the sequences of tokens
    you will use in the model architecture, and your desire and requirements to handle
    words that are uncommon.
  prefs: []
  type: TYPE_NORMAL
- en: The transformers library supports multiple approaches, with subword tokenization
    being the most common one. It’s a nice balance between character- and word-level
    tokenization that allows for less frequent words to still be in the corpus, because
    they’re made of more common subwords. For example, the word *antidisestablishmentarianism*
    isn’t a frequently used term, but it is made up of the letter combinations *anti*,
    *di*s, *est*, *ab*, *lish*, *ment*, *ari*, *an,* and *ism*, which are! It’s also
    a fun word to use with AI models that interpret your speech, to see if it can
    complete the word before you do!
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, this can give you terrific *vocabulary efficiency*, which is
    the ability to maintain a manageable vocabulary size while still being able to
    capture meaningful semantics. It can also handle *out-of-vocabulary* effectively,
    which means being effective and handling unseen words by breaking them down into
    known subwords.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s a really interesting example of this. In the very early days of transformers
    (pre-GPT), I worked on a project in which I created a transformer that I trained
    with the scripts of the TV show *Stargate,* and then I worked with the producers
    and actors on the show to do a table read of an AI-generated script. Given that
    the TV show is science fiction, with a lot of made-up words (aka technobabble),
    I used a subtoken tokenizer, following the logic that it could make up new words
    too. However, it ended up getting a little too creative! You can see the actors
    struggling with the new words in [this video of the table read](https://oreil.ly/A42Ko).
  prefs: []
  type: TYPE_NORMAL
- en: So now, let’s explore some common tokenizers in the ecosystem and see how they
    work. Note that tokenizers are associated with their model type, so when you explore
    models in the Hugging Face Hub (see [Chapter 14](ch14.html#ch14_using_third_party_models_and_hubs_1748549787242797)),
    you’ll be able to find their tokenizer. You must use the correct tokenizer with
    each model, or it won’t be able to understand your input. Depending on the licenses
    associated with the tokenizers, you could also use them to train or tune your
    own models, instead of rolling your own as we did in Chapters [4](ch04.html#ch04_using_data_with_pytorch_1748548966496246)
    and [5](ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759).
  prefs: []
  type: TYPE_NORMAL
- en: The WordPiece tokenizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The WordPiece tokenizer, which is associated with the BERT model, is a common
    tokenizer that’s highly efficient at managing subwords. It starts with a basic
    vocabulary and then iteratively adds the most frequent combinations it sees. The
    subwords are denoted by *##*. While generally created for English, it also works
    well for similar languages that have clear word boundaries denoted by spaces and
    other punctuation.
  prefs: []
  type: TYPE_NORMAL
- en: 'So now, let’s consider an example sentence that contains complex words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]`To load the tokenizer, you instantiate an instance of AutoTokenizer
    and call the `from_pretrained` method, passing the name of the tokenizer. For
    Bert’s WordPiece, you can use `bert-base-uncased` as the tokenizer name:    [PRE25]    Then,
    to tokenize the text, all you need to do is call the `tokenize` method and pass
    it your string. This will output the list of tokens:    [PRE26]    This will then
    output the list of tokens from the sentence, which looks like this:    [PRE27]    Long
    words that aren’t commonly used (like *marathoner* and *immunohistochemistry*)
    are broken into subwords, whereas others (like *conference* and *neuroscience*)
    are kept as whole words. This is based on the training corpus used in BERT, and
    the decisions about which ones are common enough and which ones are not (for example,
    I would have expected *qualified* to be quite common) were made by the original
    researcher.    If you want to see the IDs for these tokens, you can get them with
    the encode method, like this:    [PRE28]    The list of IDs is shown here:    [PRE29]    Note
    that the first and last tokens are `101` and `102`, which are special tokens for
    the start and end of the sentence that the tokenizer inserted and which are expected
    by the model.    Now, say that you decode the list of token IDs back into a string,
    like this:    [PRE30]    Then, you’ll see how the sentence has these special tokens
    inserted:    [PRE31]    I would recommend that you continue to experiment with
    the tokenizer to understand how it manages text by turning it into tokens and
    special characters. Having this knowledge is often important when you’re debugging
    model behavior or if you’re doing some kind of fine-tuning to manage how your
    vocabulary will be used.[PRE32]``  [PRE33][PRE34]py[PRE35]  [PRE36]` [PRE37] ###
    SentencePiece    SentencePiece, which is used by the T5 model, is a unique tokenizer.
    It treats all input text as a raw sequence of Unicode characters, which gives
    it strong support for non-English languages. As part of this, it treats whitespaces
    like any other characters. That makes it effective for languages like Japanese
    and Chinese that don’t always have clear word boundaries, and it also removes
    the need for language-specific preprocessing. In fact, while it was being built,
    this tokenizer learned its subword units directly from raw sentences in multiple
    languages.    Here’s how to use it:    [PRE38]py `# Tokenize the text` `tokens`
    `=` `tokenizer``.``tokenize``(``text``)` `print``(``"Tokens:"``,` `tokens``)`
    [PRE39]py   [PRE40]`py [PRE41]py`` [PRE42]  [PRE43]'
  prefs: []
  type: TYPE_NORMAL
