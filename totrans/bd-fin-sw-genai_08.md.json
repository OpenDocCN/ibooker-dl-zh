["```py`Let’s see if we can pinpoint this particular “square peg into a round hole” problem.  ![](../Images/CH07_F03_Kardell.png)  ##### Figure 7.3  Data being returned but displayed incorrectly    We can explore the data returned by the `/files` API by using the OpenAPI documentation page (or a tool such as Postman). The following listing shows the data being returned from the API.    ##### Listing 7.1  Data being returned from `/files` endpoint    ```", "```py    If we compare this code to the interface for `AchUpload` from RecentAchUploads.tsx, we can see the differences. Not only are we missing fields such as `creditTotal` and `debitTotal`, but we also need to map some of the field names.    ##### Listing 7.2  The `AchUpload` interface    ```", "```py    We should also review how the database is configured and change the amount fields from the file control record (type 9) to be `NUMERIC` instead of `VARCHAR`. This is a somewhat personal choice as arguments may be made to have a more flexible character value since the fields themselves can be formatted improperly. Making this type of change can be beneficial because the database will now enforce the data to be numeric. However, we may run into errors with existing functionality that worked before. Why? It’s because we could have unknowingly been putting spaces or other non-numeric data into that field. Of course, hopefully, our unit tests would have prevented this or, at the very least, would have quickly pointed out possible adjustments.    In our case, we have tried to create a database structure that handles both unparsed records (for those times when the records are simply not parsable) and more detailed parsed records. The following listing shows the updated constraint for the numeric field. Note that the fields are 12 characters long, with a precision of 2.    ##### Listing 7.3  ACH table for the file control record    ```", "```py    The parsing program needs to be updated as well (see listing 7.4). We use the `Decimal` datatype since it is a bit safer than using a floating point ([https://docs.python.org/3/library/decimal.xhtml](https://docs.python.org/3/library/decimal.xhtml)). Note that we pass the fields as a string to `Decimal` instead of converting it to an `int`/`float` to avoid accidental conversion. For instance, the results of `Decimal('0.1')` and `Decimal(0.1)` are `Decimal('0.1')` and `Decimal('0.1000000000000000055511151231257827021181583404541015625')`, respectively, so we still have to be careful.    ##### Listing 7.4  Converting the string to a number that includes a decimal point    ```", "```py  #1 Parses the fields using a Decimal object and f-strings   Having to update our database to convert from storing a string to numeric where appropriate adds extra work. We could have cut down some of the work had we tackled it in our previous research spikes when working on the database. In addition, we may be asking ourselves whether it is worth the effort to convert these fields to numeric for an MVP. That is a valid question. While at the very least we get the benefit of the database enforcing the formatting of our data, the tradeoff is increased development time.    Unfortunately, there is no right answer. Depending on the project, requirements, and timeframe, you may not have the extra time to convert the fields and may choose to do it later. In this case, we convert the fields since we need to update the API to return these fields anyway. We can make the changes, and based on the interface defined in the UI (listing 7.2), we know that a numeric was expected.    Now that we have the data stored correctly, we need to return the fields as part of the query. That brings us to another area that has to be refactored. We created schemas and used them in both the database and to represent the data being returned from our APIs. We would like to keep the models separate because this separation of concerns provides us with more flexibility, better security, and data integrity. We refactor our schemas directory to have folders for `api` and `database`, and we move all existing schemas to the database folder, which results in the structure presented in the following listing (note that not all files are shown).    ##### Listing 7.5  New schemas directory structure    ```", "```py    With the updated structure, we are free to add our API responses. We can now continue with updating the API as we wanted. If you remember, we wanted to return a response with the file totals. Therefore, we created ach_files_response.py that contains the `AchFilesResponse`, as shown in the following listing. We also take the time to use more of Pydantic by providing documentation to the fields.    ##### Listing 7.6  The `AchFilesResponse` class    ```", "```py  #1 By using Field from Pydantic, we can specify both constraints and documentation. #2 A description of our field shown as part of the API doc #3 A title for the field shown in our API doc #4 A constraint on the field; strings longer than max_length will cause validation errors. #5 Constrains the field to be ≥0   We have previously defined a simple query to return all the fields from the `ach_files` table, but now that we want to include the credit and debit totals in the response, we need to pull them from the parsed type 9 record. To work out the query, we can use CloudBeaver to run queries against our database and ensure the results are correct. Once we start pulling the required data, we can create a new method to return the data retrieved.    ##### Listing 7.7  Updating SQL query    ```", "```py  #1 Fields are renamed to match the AchFilesResponse. #2 When joining tables on columns that are named the same in both tables, we can employ USING. #3 Sorts our results by the created_at date in descending order #4 Although not used at the moment, we can limit the results and allow for paging through them when we have large amounts of data.   Finally, we update our route not only to call our new `get_files_response` but also to provide more documentation for our end-user, as shown in the following listing. This documentation step is important because it makes the API more accessible to nontechnical stakeholders, such as the QA department, who may be helping us with testing the APIs, or technical writers, who may be helping us with documentation. Furthermore, this change can benefit the API consumers who do not have the advantage of viewing the underlying code.    ##### Listing 7.8  Updated API route    ```", "```py  #1 We can use Pydantic again to document our fields.   With this completed, we have a usable API with documentation. Figure 7.4 shows the results of some of the documentation we obtained using Pydantic. So, in addition to being able to assist us with data validation, parsing, and type safety, Pydantic also provides these documentation benefits.  ![A screenshot of a computer  Description automatically generated](../Images/CH07_F04_Kardell.png)  ##### Figure 7.4  Our API now has documented fields and examples    We are almost there—just a few finishing touches on the UI side, and everything will be hooked together.    In the RecentAchUploads.tsx, we named our interface `AchUploads`. We would prefer to name that something closer to what the API is returning. We also have a mapping problem where we have fields defined as `creditTotal`, but they are being returned as `credit_total` from the API. While Python uses `snake_case` for variable names, TypeScript prefers `camelCase`. So, we rename `AchUploads` to `AchFiles` and leave the fields as is. Then, we create a new interface—`AchFilesResponse`.    ##### Listing 7.9  The `AchFilesResponse` interface    ```", "```py    Next, we update the call to the `/files` endpoint in the component to transform the response from `AchFilesResponse` to `AchFiles`. We do so to maintain consistent naming standards for the language we are using. However, this can also yield another benefit—providing a level of abstraction between the UI and API. This benefit can be valuable if we change the vendor of our API (not likely in this case), or something unexpectedly changes within the API (where the API did not maintain backward compatibility, or unannounced changes were made). The following listing shows the updated `axios` call.    ##### Listing 7.10  Updated `axios` call    ```", "```py  #1 A simple debug message to see the data coming back #2 For each object returned, maps the fields to their new names #3 Saves the response so we can use it   We should now have a valid response from the API and no longer see NaN for our fields when viewing the page (figure 7.5).  ![](../Images/CH07_F05_Kardell.png)  ##### Figure 7.5  Updated component for Recent ACH Uploads    When securing the ability to upload files, we worked through a few problems and did some manual testing to ensure the data was being returned as expected. We also took an opportunity to ensure our APIs were expanded with documentation and constraints where appropriate. In the next section, we take some time to explore supplementing our manual testing with some additional automated testing.    ### 7.3.1 Testing the API    Since at this point our API is working already, let’s take a look at how to incorporate some behavior-driven development (BDD) testing. Remember, this type of testing works best when the business, QA, and development all work together to create the testing scenarios. In our case, we are interested in creating a few scenarios that other groups may be able to use in their testing. We can create our features and scenarios as shown in listing 7.11\\.    The benefit of these types of tests is that other departments can plug in file names, along with credit and debit amounts, to test a variety of files. We would treat these tests as first-class citizens, meaning they take the same priority as the production code. Therefore, requests to expand the testing functionality are handled the same as production bugs and enhancement requests. For example, the tests employed here only assess a small fraction of required scenarios. We should expect to see requests to get the total number of records processed or the number of batches, check for exceptions, and so on.    ##### Listing 7.11  BDD-style scenarios    ```", "```py    For now, let’s see how to power these examples in Python code. Listing 7.12 shows some setups for these scenarios. We use the scenarios method to pull features we want to test (obviously, they should be making use of the grammar we have created). We define an instance of the `AchFileProcessor` because we are going to use it directly when parsing the file. We could have possibly gained some more functionality from using `POST` on the file, but since we are testing a single API, we did not necessarily want to muddy the water by relying on another API to work and parse the file. Note that even though we used `And` in the previous feature (e.g., `And` `that` `I` `have` `posted` `the` `file`), we use `Given` when annotating the method. This is because the `And` acts as syntactic sugar and will pick up the previous `Given/When/Then` clause. You will see that in play again when we look at the `Then` clauses.    ##### Listing 7.12  Python code for BDD    ```", "```py  #1 Imported from starlette.testclient, the TestClient is needed for making HTTP requests. #2 Loads scenarios associated with these step definitions #3 Creates the AchFileProcessor as a pytest.fixture; we can use it as needed. #4 api_response provides a way for us to store our results between steps. #5 This @given statement employs our SQL utility to clear all tables. #6 Here we take the desired filename from disk and parse it. #7 To maintain our database integrity, we need to create an ach_file_record and use it as a reference in the parse routine.   In the following listing, we execute the API and save the response. The response will be needed later when checking it against the conditions we defined with `Then`.    ##### Listing 7.13  `When` BDD code    ```", "```py  #1 No need to do anything special for hardcoded strings #2 We include the api_response fixture to store our results. #3 The client is used to retrieve the data from the endpoint. #4 We assert that we received a normal response code. #5 Prints the JSON response just for a sanity check #6 Stores the response to be used when we evaluate our @then statements   Finally, we can see our defined `Then` conditions, which validates our testing. We use a mixture of parsers depending on the situation. For the amounts, it is possible to use the `parsers.parse`. For more complicated tasks (such as parsing the filename), we can use `parsers.re`, which supports regular expressions. However, we generally recommend keeping things simple.    All of these methods use simple `assert` statements to accomplish the needed validation. The following listing shows how to confirm the filename was returned by looking at all the fields in the API response to find a field named `filename` and then ensure (`assert`) it matches the `filename` specified in the BDD test.    ##### Listing 7.14  `Then` BDD code    ```", "```py  #1 Using a regular expression to extract the filename #2 The filename we extracted and the pytest fixture are parameters. #3 Checks whether the desired filename is in the array #4 An example of using parse to get the credit_amount #5 The credit_amount and pytest fixture are parameters. #6 Gets credit_total for the first and only file in the response #7 Ensures the credit total from the response matches the expected amount   You should now have a better idea of how to incorporate this type of testing into your projects. Giving everyone the ability to contribute to testing will not necessarily make it more fun, but it will certainly help improve the robustness of our test suite. It takes little knowledge about the inner workings of testing and allows testers to focus on creating tests in a natural way.    At this point, the `/files` APIs for the MVP are completed, and we move on to building other required APIs in the next section.    ## 7.4 ACH batches APIs    When we built the UI, we also created an API to retrieve batch information for a particular file. However, that endpoint was not something we created during our initial research on FastAPI. Instead, we found a need for it when performing our UI research and building the data with WireMock. This is something that we find in projects often—we need multiple perspectives to obtain a successful result. From subject matter experts to developers completing the work, everyone has different priorities and perspectives on how to complete a project. At this point, we want to investigate how to build an API to retrieve that data.    ### 7.4.1 Adding the /files/id/batches API    With our `/files` API, we took an opportunity to revisit the BDD-type syntax for testing our API because the API has already been built. Here, we start with some tests to illustrate the TDD approach (albeit, a somewhat loose approach). First, we build a simple test class.    ##### Listing 7.15  Test class for the `/files/id/batches`    ```", "```py  #1 Defines the test client and a variable to hold the file_id #2 This setup method parses the given ACH file, and the database should be ready. #3 At this point, we are simply looking to ensure we get a valid response from the API. #4 For completeness’ sake, this method executes after the test is run.   Can you guess what the status code will be? The answer is a `404`, because we have not built the API yet. Running into `404` `errors` is typical during a development cycle where we may not have endpoints defined (as is the case here), or systems are in flux (perhaps another team changed their endpoint, or the machine is temporarily offline). The point is that during development, `404` `errors` are somewhat innocuous but are a lot more concerning in production, when something that was supposed to be there is not.    In our case, it should be simple to define the endpoint in our files.py and return a hardcoded string. The following listing shows the results of that minimal code.    ##### Listing 7.16  Defining the `/files/id/batches` endpoint    ```", "```py  #1 A simple method to return a string confirming we received the file_id   With that, we can rerun our test and should be receiving a `200`-response code. We can now venture on and build the rest of the code. Since the other endpoints make use of our `AchFileSql` class, we can add another method called `get_batches`, as shown in the following listing. Adding another method helps keep our code focused on the task at hand, which provides better maintainability and scalability going forward. Compare this to an approach where maybe we have one massive SQL query to gather all the data we could ever want about our SQL file every time, even though we only needed a tiny fragment of the data.    ##### Listing 7.17  Updating our endpoint to call `get_batches`    ```", "```py  #1 Updates the return from a string to an AchBatchesResponse (which we have not created yet) #2 Calls the method to get batches based on our file_id   Next, we create the `AchBatchesResponse`. When we create this layout, we will also need to review the database tables associated with this data. Namely, the ACH catch control record (type 5). When building this response, we can take a look at the ACH batch control record layout and see how a correct file should be formatted. This obviously affects how we build the `AchBatchesResponse` class.    ##### Listing 7.18  The `AchBatchesResponse`    ```", "```py  #1 Creates a class inheriting from the BaseModel provided by Pydantic #2 Creates fields as necessary using integers where appropriate #3 Creates decimal fields for monetary values #4 Updates our example accordingly; notice the monetary values are maintained as strings to avoid floating point errors.   As shown in the following listing, there is a need to update the record description to better align with both the `AchBatchesResponse` from listing 7.18 and with the ACH record layout in general.    ##### Listing 7.19  Updated batch control SQL table    ```", "```py  #1 Updates the table to support the numeric values   Figure 7.6 shows a trimmed-down version of the OpenAPI documentation reflecting these updates.  ![A screenshot of a computer  Description automatically generated](../Images/CH07_F06_Kardell.png)  ##### Figure 7.6  OpenAPI documentation for retrieving ACH batches    Remember, this page not only provides a great way to document our APIs but also to test and validate them. We can use this web page to `POST` an ACH file, `GET` the list of files, and then `GET` the batches. While we have already implemented unit tests to validate the APIs, this is a great way to do some more integration and manual testing of our APIs. After loading our standard sample.ach file, we should see data being returned, as in the following listing. It is worth noting again that having sample data as defined in our sample.ach file is beneficial for many reasons. Sample data can help us test edge cases and allows more robust test scenarios. Another important benefit is that it is *test* data—we are not worried about exposing nonpublic information (NPI) or personally identifiable information (PII).    ##### Listing 7.20  Sample batch data being returned    ```", "```py    It is important to note that the changes to the database allow us to rely on the database (which is our source of truth) and not necessarily the business rules we defined in Python with Pydantic. We will find this helpful as our project grows, and the carefully laid out business rules that we have in one module are bypassed when another project chooses to use its own path. Both code and technical reviews are needed to ensure that, for example, the requirement that all code must access the database through a particular module will be met.    Tools such as `pylint` allow us to build a plugin that can be used to detect these situations. While perhaps a bit beyond the scope of the book, we would like to refer you to listing 7.21, which provides the basics of a plugin that looks for imports with `psycopg`. While this is not comprehensive detection, hopefully, it will fire our imagination and give us something to dive into and research further uses of this type of technique. While code and technical reviews are extremely useful, it is nice to have guardrails that can keep us on track before we get to a final code review, only to find out we need to rework something. For more information on creating a `pylint` plugin, visit [https://mng.bz/MDjn](https://mng.bz/MDjn).    ##### Listing 7.21  A simple `pylint` plugin to detect `psycopg` use    ```", "```py  #1 The necessary imports for our plugin #2 Defines either modules that we allow psycopg to be referenced from or entire directories #3 Defines the pylint error that we will produce #4 Methods executed when “import” and “import from” statements are encountered; here, they call our _check_psycopg_usage method. #5 A method that gets the name and file we are working in and checks whether it has been whitelisted (found in ALLOWED_MODULES or ALLOWED_DIRECTORIES) #6 Adds the error message If the method is not found #7 Registers our module   While not strictly necessary for our project, a mantra of “Hey, I can automate that” can help your growth as a developer. Integrating this check with `pylint` ensures we can prevent the misuse of the module (in terms of where it is imported) throughout our code base.    With that, we are ready to switch gears and move over to the UI side to ensure we can access our endpoint.    ### 7.4.2 Supporting ACH batches in the dashboard    Now that we have an API capable of returning ACH batches, we can integrate it into our UI. Remember, when mocking out the API, we created a data structure similar to what is presented in the following listing. However, now that we actually have the data being returned, we need to update the UI to accommodate the new format. Note that if you were to start the UI, upload a file, and try to drill down into it, because of these format changes, you would receive an error message similar to `TypeError:` `Cannot` `read` `properties` `of` `undefined` `(reading` `'map')` on the `batches.map` statement. We will resolve that problem in this section.    ##### Listing 7.22  Format for batch information    ```", "```py    While we could have returned the data the same way we mocked it up, it may make our lives easier to return a simpler structure from our API that could then be manipulated with the results. This is all part of putting an MVP together. We have not invested too much into the solution, and it is not too late to adjust the game plan. A bit of rework early on can ensure we stay on the right path and prevent headaches down the road.    Our choice of SQL, Python, or TypeScript will likely not matter to the end-user. For now, we opt for the simple SQL query, and format the data on the UI side. We update our interfaces to have one dedicated to the API response, and we use the rest throughout the code.    ##### Listing 7.23  Updating interfaces for ACH batches    ```", "```py  #1 The ACH batch info interface that we will use throughout the UI #2 The ACH batch info interface that we will use throughout the UI #3 The response from the API that we will rework   We also need to add the code that will condense the response into the `AchCompanyBatchInfo`, which mimics our original structure. As shown in the following listing, we organize our records by company name, creating an array of batches for each company.    ##### Listing 7.24  Transforming the code into the desired structure    ```", "```py  #1 Defines a simple Record object to hold our condensed records by Company ID #2 Checks whether we have an entry with the key of company_id; if not, it creates one with an empty array for companyBatches. #3 Adds the batch information for the company   Our logic for the scatterplot we created remains largely the same with just an update to the names of the interfaces and a couple of minor tweaks. Preserving consistency in naming helps with code maintenance and readability, especially as other developers may make enhancements. The most important change we need to keep in mind is that now that we are using `Decimal`, we have to update the way in which we create our field `totalCreditAndDebits`. As shown in the following listing, we need to call the `add` method instead of using the standard `+` (plus) operator.    ##### Listing 7.25  Updating the computation for `totalCreditAndDebits`    ```", "```py    ### 7.4.3 Uploading files    So far, we have been using our API documentation to upload files. The final functionality that we want to look at is being able to upload from our dashboard. We have already built the UI capable of dragging a file into the browser and obtaining the file name. We also had a button that did not do anything when clicked. Now we will provide code that opens a window where the user can select a file to upload.    We know there will be at least two paths to uploading a file—dropping a file into the browser and clicking the upload button—which means we need to create a function to handle the uploads. If we can centralize the function, it will prevent duplicating code and maintenance in the future    ##### Listing 7.26  The `uploadFile` function    ```", "```py  #1 Defines a state to track whether the file is loading #2 Gets the file from the form #3 Uses our URL #4 Sets loading to true so that we can update the page to display an activity indicator #5 Sends a post request with our file to the endpoint #8 When the file is uploaded, sets loading back to false and immediately goes back to the dashboard   This feature allows us to easily update the previous code we had for `handleDrop` when a user drops in the file. As soon as the file is dropped, we call `uploadFile` with the file, and when the file is uploaded, we will route the user back to the dashboard. In this simple case, the dashboard will be automatically updated with the new file. However, we have the following limitations:    *   We only upload one file, although a user could conceivably pick more. *   We assume the file was loaded successfully.    Either way, we do not necessarily provide any feedback to our users. We won’t focus on these problems because we are working on an MVP, and we do not need to have all the bells and whistles, except in a finished production-ready product. While it may be tempting to add this functionality, we want to avoid scope creep (when our project goes beyond the original requirements), which can cause delays in rolling out the product. For now, let’s add the call to our `handleDrop` function.    ##### Listing 7.27  The `handleDrop` function    ```", "```py  #1 Updates the handleDrop code to call our routine to upload the file   We also call this code from the button click as well, which takes a little more work.    ##### Listing 7.28  Uploading files with our button    ```", "```py  #1 Defines a reference to an HTMLInputElement, which allows us to interact with the element in our code #2 Handling of the file changing #3 Executes the click event, which implements a click on the “input” HTML element, making the File Chooser open #4 Specifies our handleFileChange method when the input changes   With this code, we can now upload files through our dashboard.    ## 7.5 Putting the puzzle together    We now have the MVP up and running in our IDE environment. The final step is to move everything into the Docker environment and ensure things work.    From our previous preparations, we already know that all the pieces have been operating in Docker. This means we can use our previous Docker configuration with just a few tweaks, and we should be good to go.    Figure 7.7 shows the Docker layout, with containers and ports labeled to give us the basic idea of our layout. CloudBeaver is not necessary in our MVP—we only include it in the setup so we can work with queries and explore the database. With our configuration, we can start Docker as usual, with `docker-compose` `down;` `docker-compose` `up --build`.    ### 7.5.1 Cleaning up the configuration files    When we begin getting our code ready for a production environment (or Docker), some additional housekeeping will be required. First, we can take a look at main.py in the `app` folder. Here, we specified some CORS settings. The allowed origins are likely going to be the biggest problem, and we want to ensure that requests are coming from where we expect them (in this case, `localhost:4000`). We also limit our methods to those expected (`GET` and `POST`). We should not need to allow `OPTIONS`, which is sent by the browser as part of a Preflight CORS request, but keep in mind that different frameworks/versions may require it.    ##### Listing 7.29  Updates for CORS    ```", "```py  ![A diagram of a computer  Description automatically generated](../Images/CH07_F07_Kardell.png)  ##### Figure 7.7  Docker setup for the ACH dashboard MVP    Another part of the Python configuration we may want to clean up is the requirements.txt. We have dependencies that are specific to our development, such as `pytest` and `pytest-bdd`. In our production code, there should be no reason to include these. Attackers cannot exploit dependencies that are not present, so it is important to keep unnecessary or unused requirements out of production. We create a dev-requirements .txt, which we can use for development environments as needed, and try to leave a requirements.txt that is production ready. If your IDE (or other tooling) maintains a requirements.txt, you may find it easier to do the reverse and keep a prod-requirements .txt. This way, you can move any requirements you install over there immediately and do not necessarily need to guess. The following listing shows an example of a dev -requirements.txt.    ##### Listing 7.30  A sample dev-requirements.txt file    ```", "```py    We clean up the UI configuration files similarly by splitting tsconfig.json into tsconfig.dev.json and tsconfig.prod.json. This is because we do not need references to Jest and other development options such as `sourceMap` set in our production environment. The following listing shows how the tsconfig.prod.json has been updated to remove entries for Jest and expand to exclude any tests from the production build. We would not expect any tests to make it to the packaging steps, so excluding the *.test.ts and *.spec.ts files is mostly precautionary.    ##### Listing 7.31  Updating the tsconfig.prod.json file    ```", "```py    In addition to configuration files for the components, we also want to take a minute to ensure that the `NEXT_PUBLIC_API_URL` is pointing to the correct location. The following listing shows the configuration for the dashboard service from docker-compose.yml.    ##### Listing 7.32  Configuration for the dashboard service from docker-compose.yml    ```", "```py    With that, we should be able to bring our project up in Docker.    ### 7.5.2 Ensuring APIs are accessible    We can run the general check of our containers with `docker` `ps` `--format` `\"table {{.Names}}\\t{{.Status}}\"` (we use the format option just for clarity; `docker-compose ps` will work as well, and it is shorter) and see that our containers are up and running.    ##### Listing 7.33  Containers up and running    ```", "```py  #1 The postgres container is the only showing as healthy because it is the only container we implemented a health check for.   Now that everything is working, note that we only have a health check defined for Postgres, so we see that it is healthy, but the other containers have no health checks, so we do not see the `(healthy)` text. Since all our containers appear to be up and running, the next thing we do is navigate to http://localhost:8000/docs and verify our APIs are available (see figure 7.8).  ![A screenshot of a computer  Description automatically generated](../Images/CH07_F08_Kardell.png)  ##### Figure 7.8  Our list of defined APIs    ### 7.5.3 Programming challenges    The process itself should hopefully be clear by now—we identify desired functionality, create an API to support it, backend code to enable it, and frontend code to consume and display it (figure 7.9)  ![A diagram of a data flow  Description automatically generated](../Images/CH07_F09_Kardell.png)  ##### Figure 7.9  Process of adding functionality to our MVP    We still have several items to discuss in this chapter, but now is a good time to put out some challenges you could work on:    *   *Top files toda**y*—Provide a visualization of the largest files loaded today. *   *Today’s total**s*—Provide a component showing credit/debit totals for today. *   *Drill dow**n*—Allow the user to click a file, view batch information, and drill into the batch to see entries. *   *Transaction code breakdow**n*—When viewing the entries, provide a visualization of the breakdown of transaction codes (debits, credits, etc.) used.    ### 7.5.4 Exploring the dashboard    At this point, we should have all the components up and running in Docker, and we can navigate to the dashboard. This would be the time to click everything to ensure it functions as intended. Load some ACH files, and make sure you can view the files, batches, and the charts are working. Then take a minute to congratulate yourself—the MVP is just the start of our journey, but we put some effort into getting to this point. The pace of development and the timeline for projects is often frantic. It is important that we take the time to recognize our achievements, as well as those of our team.    ## 7.6 Gotta have more files    During development, there will come a time when we get to a point where the ACH files provided by the business are inadequate for our testing. This may be because we have specific scenarios we want to test, or we need to slim down files so we can test simple scenarios to ensure our data is working. Since ACH files are just text files, nothing is stopping us from simply copying and opening them to make the necessary updates. Fortunately (or unfortunately), we are programmers, and we cannot bother with editing files manually (except in rare cases). We can use BDD-style testing to develop syntax and create files.    Using Python and `pytest-bdd` to create our ACH files gives us not only a chance to work on our Python skills but also a chance to dive into ACH files and some of the requirements. By creating files programmatically and then turning around to load them into our dashboard, we are back to working in short development cycles. As we create and load files, we may find problems with the generation, loading/parsing, APIs, and UI. We can create an `ach_file_creation.feature` and generate a syntax for ACH file creation. The following listing shows a sample of the first test. Remember that chapters 2 and 5 reviewed some of the characteristics of ACH files. If you need additional information on required fields and other important information, refer to the ACH File Overview at [https://achdevguide.nacha.org/ach-file-overview](https://achdevguide.nacha.org/ach-file-overview).    ##### Listing 7.34  BDD-style ACH file creation    ```", "```py    This code gives us some flexibility in the number of batches and entries a file will contain, as well as amounts used. It does not necessarily reflect/enforce the correct business rules for producing ACH files. We simply took an existing file and began working backward to replace fields as we needed some flexibility in our file creation. The file exists as a “test” within our tests folder for our project, but it could very well be split out into its own project to become its own utility others will use. We’ll come back to this topic and work on it further as we continue with the project and find additional requirements.    Most of the code for supporting the grammar is centered around collecting the necessary information. In listing 7.35, we set it so that we are only expecting debits, and therefore, we can also set the appropriate service class and transaction codes. We could have also allowed users to specify transaction codes and then determined which service class code to use or vice versa. We can always approach problems differently and need to weigh the tradeoffs. Keeping in mind that nontechnical users may want to use this feature, abstracting the details of a correct service class and transaction code seems to be a good choice. Of course, later, we may want to introduce files produced with an error condition to create various scenarios and therefore need to introduce finer-grained control.    ##### Listing 7.35  Sample for creating files    ```", "```py  #1 Gets the batch count, converts it to an integer, and stores it for later #2 We are working with debits only, so we set the fields accordingly. #3 Gets the standard_entry_class and saves it for later   Similarly, the validation routine is straightforward. In this case, we know that we have a set number of records, and therefore, we want to simply count the records that are type 6\\. We perform a similar check for the number of batches. Opening, reading the entire file, and closing the file each time may not be the most efficient way to do things but is certainly sufficient for now.    ##### Listing 7.36  Validating our entries    ```", "```py  #1 Opens the file for reading #2 Counts lines that are a type 6 record #3 Asserts that the count is correct by comparing it to what was expected   Finally, we show some of the work needed to create a record in the file. In the following listing, we format amounts and pad them as necessary before finally plugging them into the returned string.    ##### Listing 7.37  Creating a file    ```", "```py  #1 Fields are right justified with a max length and padded with zeroes.   We may have invested more time in putting all this together, but we still believe these examples can improve our development skills and streamline our work later.    ## 7.7 Adding a basic login page    The dashboard is a great way to get us accustomed to loading and visualizing ACH files. However, we do not want anyone to have access to the upload page. Whether the dashboard is available as a web interface to the outside world or is for internal use only, we still want to limit users who can work with files. Later, this may facilitate functionality such as alerts when a file is successfully loaded or rejected. In addition, an audit trail of someone who loaded a file or performed some meaningful activity to the file (such as deleting batches and their associated transactions) is important in case we have disgruntled or malicious users.    Thus, this section covers adding a basic login page using NextAuth.js, which is an open source authentication solution for Next.js. While we will have hardcoded values, you can extend the solution to use other providers such as Auth0, Apple, and Google, to name just a few.    We can set up the NextAuth.js with a few files and supporting code. We create `middleware.ts`, `auth.ts`, and `auth.config.ts`, which support the authentication.    ##### Listing 7.38  Setting up our middleware    ```", "```py    We provide basic configuration that supports callbacks for checking whether a user is authorized and redirecting them as necessary.    ##### Listing 7.39  `Auth` callbacks    ```", "```py  #1 We set debug to true while in development to view additional log messages. #2 Sets a custom sign-in page to override the unbranded default page; other pages that can be set are signOut, error, verifyRequest, and newUser. #3 Checks whether the path starts with /uploads #4 Redirects to the desired page if available #5 Providers are services that can be used to sign in a user.   To power our login page, we use the credential provider in which we hardcode a couple of different users. The following listing shows how we parse the given credentials and compare them against our known values. Note that hardcoding the values for our username/passwords is never a good idea, but this example only shows the concept of adding authentication to the project.    ##### Listing 7.40  `Auth` credentials configuration    ```", "```py  #1 Defines an array of emails and passwords to use #2 An existing configuration is included. #3 Parses the email and password #4 Checks for a successful parse #5 Retrieves the parsed data #6 Finds a valid user #7 If there is no valid user, returns null #8 Returns the email for the user   The login page will be displayed with the dashboard. We also asked ChatGPT to create a logo for our company. Interestingly, we made several attempts, and while the graphics always looked pretty good, ChatGPT could never quite get the spelling right (figure 7.10)!  ![A screenshot of a login page  Description automatically generated](../Images/CH07_F10_Kardell.jpg)  ##### Figure 7.10 Login sample page    The login page is a standard form, and much of the code is in support of the layout, tooltips, and button clicks.    ##### Listing 7.41  A login form showing NextAuth.js incorporated into our login page    ```", "```py    These changes prevent access to the upload page. However, we have not done anything to prevent access to the actual API behind the UI that powers the processing. For that, we need to incorporate access tokens and ensure these are being passed and validated on the server side as well. When we are ready to take that step to ensure the APIs are secure, our UI would have to make a `POST` request and include an `Authorization` header in it.    ##### Listing 7.42  Including the `Authorization` header in a `POST` request    ```", "```py    Checking would be done similarly on the server with FastAPI.    ##### Listing 7.43  Creating a function to validate the token    ```", "```py    ##### Listing 7.44  Adding the dependency    ```", "```py    With all of that in place, when navigating to http://localhost:8000/docs, you will see a new Authorize button that will allow you to specify a token to be passed (figure 7.11). This approach provides a simple way to test our application via the OpenAPI documentation.  ![](../Images/CH07_F11_Kardell.png)  ##### Figure 7.11 Showing the Authorize button and popup    Here, we have simply used a hardcoded value, but it is possible to use a more robust authentication flow. To learn more about authentication flows such as OpenID Connect, check out their site at [https://openid.net/](https://openid.net/).    ## 7.8 Adding TLS to our dashboard    The previous section covered how to secure our dashboard’s UI and APIs. However, there is another layer of security that we want you to be aware of—the communication or network layer. As we have seen throughout the book, our URLs have been using HTTP and not HTTPS. While this is not a problem for a development environment, we explore some of the steps involved in securing the network layer. Depending on the size of the company we work for (and of course our role), we may never need to worry about this. However, understanding some of the processes in setting this up can be helpful. It can be especially beneficial when having to work through any problems we may encounter as we gain valuable troubleshooting knowledge.    We will keep this relatively simple by just adding an `nginx` instance to our Docker configuration that we can use. We will not worry about removing access to the existing HTTP ports from the outside as that is not necessary for our development environment. We simply want to see how to use Docker to understand how this process works.    The following listing shows how to add a simple service to our existing docker-compose.yml file. We define the build information and ports. We expose both the standard HTTPS `port` `443` and HTTP `port` `80` because we will be redirecting any HTTP traffic to HTTPS. If we do not expose the port, the users will receive an error when trying to connect to the web site.    ##### Listing 7.45  The `nginx` endpoint for Docker to support TLS    ```", "```py  #1 A Dockerfile to build from #2 We expose both the HTTPS 443 and HTTP 80 ports for this container.   Next, as shown in the following listing, we specify the Dockerfile that needs to be in place. We are using the latest image from Nginx, ensuring packages are up to date and then installing OpenSSL because we will be using a self-signed certificate. Another option is using certbot ([https://certbot.eff.org/](https://certbot.eff.org/)), but a self-signed certificate will be enough for our needs. In a production environment, we would have to obtain the certificate from a trusted certificate authority (CA). We use the `/etc/ssl` directories to store our certificates, but that may differ depending on the OS.    ##### Listing 7.46  Dockerfile to build the container    ```", "```py  #1 Uses the latest Nginx #2 Sets the work directory to /etc/nginx (not strictly necessary) #3 Updates and then installs OpenSSL #4 Preps directories to store certificates and keys in #5 OpenSSL command creating our certificate; we make use of the directories we just created to store certificates. #6 Copies our server configuration over to the container   The Dockerfile copies a dashboard.conf to default.conf. Let’s look at what that configuration should look like.    ##### Listing 7.47  The dashboard.conf    ```", "```py  #1 Defines the port we are listening on; 443 is a standard SSL port, but we still must include the ssl parameter to enable SSL. #2 The name of the server; we are mapping localhost and 1 27.0.0. 1 . #3 Specifies the cert and key for SSL #4 Any traffic coming to this server is redirected to the dashboard on port 3000\\. We also set additional headers to provide information to the receiving server, which helps indicate where this forwarded request came from. #5 Sets up port 80 #6 Again, we specify the localhost. #7 We redirect the browser to the SSL port.   We have now secured the dashboard using TLS. While doing so is required for production environments, it is not strictly necessary for our development. However, having a bit of knowledge about some of the basic setup can help you troubleshoot problems when environments are migrated or otherwise changed, and your application breaks. For instance, in the previous listing, headers such as `X-Real-IP` were set. If the application depends on these to be present but someone removes them from the configuration, we could end up with a troublesome bug. While uncommon, running into similar bugs is possible.    ## 7.9 Testing with Playwright    You are now familiar with a few approaches to testing our software, specifically TDD and BDD. We have also looked at building tests with `pytest`, when working in Python, and Jest, when using TypeScript. With those tools, we have focused mostly on smaller unit testing. Because our MVP is completed, we are going to use end-to-end testing now to validate our work before it gets shipped off. For that, we use a tool called Playwright. Playwright is a cross-browser, cross-platform, and cross-language testing tool. So, regardless of our environment, chances are we can use Playwright. We chose Playwright with Python because we are currently using this combination for our APIs and file parsing.    Getting started with Playwright is simply a matter of installing `pytest-playwright` with `pip` `install` `pytest-playwright` and then running `playwright` `install` to ensure browser extensions are installed as well. Once that is complete, we can start scripting. A powerful feature of Playwright is using the interactive mode from a Python console. After launching a browser with `headless=False`, we can see the results of our commands in real-time.    ##### Listing 7.48  Simple interactive Playwright    ```", "```py  #1 Specifies headless=False, which allows us to view and interact with the browser #2 We will not see the browser window until we call the new_page() method.   When we have a handle on using Playwright to navigate to URLs and select elements, we can begin writing tests. The following tests show that we rely heavily on the `get_by_role` method when selecting elements. The `get_by_role` method follows the W3C specifications for ARIA (Accessible Rich Internet Applications), which can aid in developing pages that help conform to standards used by assistive technology (e.g., screen readers).    ##### Listing 7.49  Test to populate the login form    ```", "```py  #1 Uses get_by_role to help ensure ARIA is supported #2 Uses the fill method to populate text #3 Creating screenshots is easy.   As we saw, populating fields is straightforward, and clicking buttons is just as easy. Let’s see whether we can cause a login exception by not populating any fields and clicking the button to sign us in. Notice how we use the `wait_for_selector` to ensure the alert is visible. If we do not use this, our error message may be translucent as it may not have finished rendering correctly.    ##### Listing 7.50  Capturing the exception message    ```", "```py  #1 Verifies that we were redirected to the login page when we initially go to /uploads #2 Clicking buttons is simple. #3 Gets the alert message that should appear and verifies the text #4 Uses the wait_for_selector before taking a screenshot   If `wait_for_selector` is not used, we may end up with an error message that is not completely rendered, as shown in figure 7.12 on the left. This can be a common challenge in automated UI testing—the speed at which tests execute can lead to this scenario. Often, as we step through the code to verify the test is working, we won’t encounter this situation. It is generally possible only when the test is run as part of our automation or build pipeline.  ![](../Images/CH07_F12_Kardell.png)  ##### Figure 7.12  Screen not rendered completely when we do not wait    Here, we have only touched on some basics of using Playwright to perform end-to-end testing on our dashboard by automating a browser instance. You should be able to see the benefits of running these types of tests on a variety of browsers. Our listings showed capturing a screenshot as part of the test, but that was mostly for validation purposes since we are running in a headless (no browser window) mode, and we do not actually see it running. In practice, we may only capture a screenshot of an exception coding for further troubleshooting and research. Maybe we could even combine Playwright with our BDD-style syntax to create browser automation tools for our QA department to use! Combining these into a CI/CD pipeline along with our current unit tests can further ensure our software is ready to ship.    ## 7.10 MVP wrap-up    At this point, the MVP of our dashboard is complete. We have built the necessary remaining APIs using manual, unit, and integration testing to make sure the dashboard will work flawlessly once delivered. We also worked on improving the security of our application by using TLS and user authentication.    Certainly, there are tweaks we could make to the UI and additional functionality. Make notes of those items as they are great to work on when you are capable of accepting more story points during a future sprint. As mentioned before, we must balance the functionality of the app with a delivery timeframe. Thus, it is important not to get caught up in adding all these bells and whistles.    We covered a lot of ground in this chapter. Hopefully, our previous research spikes provided us with enough background to help us stay on target. The next section addresses some of the problems we may run into when putting the MVP together.    ### 7.10.1 Troubleshooting the MVP    Inevitably, we run into problems as we work through our projects. While these can be frustrating, being able to resolve them is a necessity not limited to software development.    One of the most common problems that we run into is a CORS error that usually says something like `Access` `to` `XMLHttpRequest` `at` `some_location` `from` `origin` `some_server_name` `has` `been` `blocked` `by` `CORS` `policy:` `No` `'Access-Control-Allow-Origin' header` `is` `present` `on` `the` `requested` `resource`. It can be triggered by a number of problems, such as    *   Request went to an unexpected location. In one instance, our Docker container had not shut down as intended and was still attempting to handle requests without our CORS changes in place. *   Pydantic was throwing an error because we had specified a required field named `account_number_last_4` but had mislabeled it as `account_number` in the SQL query. This was not too difficult to track down because Pydantic provided us with a nice error that looked similar to `{'type':` `'missing',` `'loc':` `('response', 0,` `'account_number_last_4'),` `'msg':` `'Field` `required',` `'input':` `{'id': UUID('29159e96-c2b3-452e-be1b-b4258c5e77aa'),` `'transaction_code': '22',` `'amount':` `'0000000201',` `'account_number':` `'*************0034', 'addenda_count':` `1},` `'url':` `'https://errors.pydantic.dev/2.6/v/missing'}`. Pydantic has specified the type as missing and provided our object with both fields and values. Sure enough, we do not see the missing field but the `account_number` instead.    Other errors that we ran into were also Pydantic related because we failed to update the field definitions as we were going through the code. So, a message such as `1` `validation` `error` `for` `AchEntryPpdDetailsSchema` also showed `[type=string_type, input_value=Decimal('1.00'),` `input_type=Decimal]`, which was a simple fix to ensure that all our data types were matching.    Another data-type-related problem was the mix-up between strings and numerics because we were inconsistent or forgetting to update the parsing program to handle the cents when converting from a string to a numeric. So, $1.00 was showing incorrectly as $100.00\\. Interestingly, when reworking the code to go from strings to numerics, Copilot was not very helpful. It kept assuming that we wanted the next two positions in the string instead of breaking the current field up.    We also ran into problems with SQL when we specified `INNER` `JOIN` instead of `LEFT` `JOIN`. We wanted to capture the addenda counts for a particular entry. `INNER` `JOIN` worked great when there were addenda records, but without them, the row was dropped, and that was not what we wanted.    When testing with Playwright, we needed to ensure that names/IDs were included in our elements. Playwright also has great error messages. When running in strict mode, and there are multiple matches, it enumerates the possibilities and provides appropriate examples such as `get_by_role(\"banner\").get_by_role(\"button\")`, `locator(\"button\").nth(2)`, and `get_by_role(\"button\",` `name=\"Dashboard\")`.    Our final problem came from Docker, where we accidentally deleted a Dockerfile when updating our code. This resulted in the message `failed` `to` `solve:` `failed` `to read` `dockerfile:` `open` `/var/lib/docker/tmp/buildkit-mount39300809/Dockerfile:` `no` `such` `file` `or` `directory`. In another case, when updating the code for Docker, we simply copy/pasted existing code without clearing out the existing files in Docker, which triggered a problem in Next.js because we had different parameter names (slugs) for the same dynamic path. The error was clear enough: `Error:` `You` `cannot` `use` `different` `slug` `names` `for` `the` `same` `dynamic` `path` `('fileId'` `!==` `'id')`. However, it was also confusing because it was not a code problem but rather a directory structure problem (and procedural as well). Next, we had several dependency problems due to mismanagement on our part. These only appeared when we tried to build the solution in our Docker container. We ended up with errors such as `Starlette` `was` `listed` `in` `the requirements.txt`. The conflict was caused by    ```", "```py    Finally, because we did not expose `port` `443` in `docker-compose`, the connection was refused when trying to access the HTTPS endpoint. That error looked similar to `Error:` `connect` `ECONNREFUSED` `127.0.0.1:443`.    As we’ve experienced, being a full stack developer means we also get to see errors from many different areas, which makes our lives exciting!    ## Summary    *   Previous sprints explored using generative AI and technologies such as Python/FastAPI, Postgres, Docker, and Next.js to expedite software development for our ACH dashboard MVP. *   The MVP allows users to upload and parse ACH files, store them in a database, and visualize the results on a dashboard. *   Different minimum concepts include MVP, MLP, MMP, and MMF, each serving unique purposes—from testing assumptions to delivering features. *   Preparing for an MVP involves integrating components such as the ACH parser, APIs, database design, and UI to function cohesively. *   Developing the `/files` APIs ensures alignment between the backend and UI, focusing on data accuracy and seamless functionality. *   BDD and TDD approaches help integrate APIs with the UI, ensuring accurate data responses and interactions. *   Refactoring interfaces and schemas improves code maintainability, readability, and flexibility when consuming APIs. *   Supporting the API integration of ACH batches into the dashboard requires updating data structures and transforming responses to match UI needs. *   The upload functionality from the dashboard simplifies user interaction but highlights development considerations, such as code duplication and user feedback upon file upload. *   Deployment in Docker involves cleaning up configurations for production readiness, ensuring seamless communication between containers, and addressing common troubleshooting problems.```"]