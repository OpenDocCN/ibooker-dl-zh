# 第十章。深度模型的解释

到目前为止，我们已经看到了许多训练深度模型解决问题的例子。在每种情况下，我们收集一些数据，构建一个模型，并训练它，直到它在我们的训练和测试数据上产生正确的输出。然后我们为自己鼓掌，宣布问题已经解决，然后继续下一个问题。毕竟，我们有一个为输入数据产生正确预测的模型。我们还能想要什么？

但通常这只是一个开始！一旦您完成训练模型，您可能会有很多重要的问题要问。模型是如何工作的？输入样本的哪些方面导致了特定的预测？您可以信任模型的预测吗？它们有多准确？有哪些情况可能会导致失败？它到底“学到”了什么？它是否能带来关于训练数据的新见解？

所有这些问题都属于*可解释性*的范畴。它涵盖了您可能希望从模型中获得的一切，而不仅仅是机械地使用它进行预测。这是一个非常广泛的主题，它所包含的技术与它们试图回答的问题一样多样。我们无法希望在仅有一章中涵盖所有内容，但我们将尝试至少尝试一些更重要的方法。

为了做到这一点，我们将重新审视之前章节中的例子。当我们之前看到它们时，我们只是训练模型进行预测，验证其准确性，然后认为我们的工作完成了。现在我们将深入研究，看看我们还能学到什么。

# 解释预测

假设您已经训练了一个模型来识别不同种类车辆的照片。您在测试集上运行它，并发现它准确区分了汽车、船只、火车和飞机。这是否意味着它已经准备投入生产？您可以信任它在未来继续产生准确的结果吗？

也许，但如果错误的结果导致严重后果，您可能会希望进行进一步的验证。如果您知道模型为什么产生特定的预测，那将会有所帮助。它是否真的看到了车辆，还是实际上依赖于图像的无关方面？汽车的照片通常也包括道路。飞机往往是在天空的背景下剪影。火车的图片通常包括轨道，船只的图片包括大量的水。如果模型真的是在识别背景而不是车辆，它可能在测试集上表现良好，但在意外情况下会严重失败。一艘船在天空的背景下可能被分类为飞机，一辆汽车驶过水域可能被识别为船只。

另一个可能的问题是模型过于专注于细节。也许它并不真正识别*汽车*的图片，只是识别包括*车牌*的图片。或者它可能非常擅长识别救生圈，并且已经学会将它们与船只的图片联系起来。这通常会起作用，但当展示一辆汽车驶过一个游泳池，背景中有救生圈可见时，它将失败。

能够解释模型为何做出预测是解释性的重要组成部分。当模型识别出一张汽车的照片时，您希望知道它是基于实际汽车做出识别，而不是基于道路，也不是基于汽车的一个小部分。简而言之，您希望知道它给出了正确答案*出于正确的原因*。这会让您相信它也会在未来的输入上起作用。

作为一个具体的例子，让我们回到[第8章](ch08.xhtml#deep_learning_for_medicine)中的糖尿病视网膜病变模型。回想一下，这个模型以视网膜图像作为输入，并预测患者患有糖尿病视网膜病变的存在和严重程度。在输入和输出之间有数十个`Layer`对象和超过800万个训练参数。我们想要理解为什么特定的输入导致特定的输出，但我们不能希望仅通过查看模型来学习。它的复杂性远远超出人类理解。

许多技术已经被开发出来尝试回答这个问题。我们将应用其中一个最简单的技术，称为*显著性映射*。这种技术的本质是询问输入图像的哪些像素对于确定输出最重要（或“显著”）。在某种意义上，当然，*每个*像素都很重要。输出是所有输入的一个极其复杂的非线性函数。在正确的图像中，任何像素都可能包含疾病的迹象。但在特定图像中，只有其中的一小部分包含，我们想知道它们是哪些。

显著性映射使用一个简单的近似来回答这个问题：只需对所有输入取导数。如果图像的某个区域不包含疾病迹象，那么对该区域中的任何单个像素进行小的更改应该对输出产生很小的影响。导数应该很小。积极的诊断涉及许多像素之间的相关性。当这些相关性不存在时，它们不能仅通过更改一个像素来创建。但当它们存在时，对参与的任何一个像素进行更改可能会加强或削弱结果。导数应该在模型关注的“重要”区域最大。

让我们看看代码。首先，我们需要构建模型并重新加载训练参数值：

```py
import deepchem as dc
import numpy as np
from model import DRModel
from data import load_images_DR

train, valid, test = load_images_DR(split='random', seed=123)
model = DRModel(n_init_kernel=32, augment=False, model_dir='test_model')
model.restore()

```

现在我们可以使用模型对样本进行预测。例如，让我们检查前10个测试样本的预测：

```py
X = test.X
y = test.y
for i in range(10):
 prediction = np.argmax(model.predict_on_batch([X[i]]))
 print('True class: %d, Predicted class: %d' % (y[i], prediction))

```

这是输出：

```py
True class: 0, Predicted class: 0
True class: 2, Predicted class: 2
True class: 0, Predicted class: 0
True class: 0, Predicted class: 0
True class: 3, Predicted class: 0
True class: 2, Predicted class: 2
True class: 0, Predicted class: 0
True class: 0, Predicted class: 0
True class: 0, Predicted class: 0
True class: 2, Predicted class: 2

```

它对前10个样本中的9个做出了正确的预测，这还不错。但是在做出预测时它在看什么呢？显著性映射可以给我们一个答案。DeepChem使这变得容易：

```py
saliency = model.compute_saliency(X[0])

```

`compute_saliency()`接受特定样本的输入数组，并返回每个输出相对于每个输入的导数。我们可以通过查看结果的形状来更好地理解这意味着什么：

```py
print(saliency.shape)

```

这报告它是一个形状为`(5, 512, 512, 3)`的数组。`X[0]`是第0个输入图像，它是一个形状为`(512, 512, 3)`的数组，最后一个维度是三个颜色分量。此外，模型有五个输出，即样本属于五个类别中的每一个的概率。`saliency`包含每个五个输出相对于每个512×512×3个输入的导数。

这需要一些处理才能更有用。首先，我们想要取每个元素的绝对值。我们不关心一个像素是否应该变暗或变亮以增加输出，只关心它是否有影响。然后我们想要将其压缩为每个像素仅一个数字。这可以通过各种方式完成，但现在我们将简单地对第一个和最后一个维度求和。如果任何颜色分量影响任何输出预测，那么这个像素就很重要。最后，我们将归一化值为0到1之间：

```py
sal_map = np.sum(np.abs(saliency), axis=(0,3))
sal_map -= np.min(sal_map)
sal_map /= np.max(sal_map)

```

让我们看看它是什么样子。[图10-1](#saliency_map_for_an_image_with_severe_diabetic_retinopathy)展示了一个模型正确识别为严重糖尿病视网膜病变的样本。左侧是输入图像，右侧突出显示了最显著的区域为白色。

![一个显著性映射](Images/dlls_1001.png)

###### 图10-1. 严重糖尿病视网膜病变图像的显著性映射。

我们注意到的第一件事是显著性广泛分布在整个视网膜上，而不仅仅是在几个点上。然而，它并不是均匀的。显著性集中在血管沿线，特别是在血管分叉点。事实上，医生用来诊断糖尿病性视网膜病变的一些指标包括异常的血管、出血和新血管的生长。模型似乎将注意力集中在图像的正确部分，这些部分与医生最密切关注的部分相同。

# 优化输入

显著性映射和类似技术告诉您模型在进行预测时关注的信息。但它究竟是如何解释这些信息的呢？糖尿病性视网膜病变模型关注血管，但是它是通过什么来区分健康和患病的血管的呢？同样，当模型识别一张船的照片时，了解它是基于构成船的像素，而不是构成背景的像素进行识别是很重要的。但是是什么像素导致它得出看到一艘船的结论呢？是基于颜色吗？形状？小细节的组合？模型是否会同样自信地（但错误地）识别出无关的图片为一艘船？模型究竟认为一艘船是什么样子呢？

回答这些问题的一种常见方法是搜索最大化预测概率的输入。在您可以输入模型的所有可能输入中，哪些会导致最强的预测？通过检查这些输入，您可以看到模型真正“寻找”的是什么。有时结果可能与您的预期非常不同！[图10-2](#fooling)显示了经过优化以在输入到高质量图像识别模型时产生强烈预测的图像。该模型以非常高的置信度将每个图像识别为列出的类别，但对于人类来说，它们几乎没有相似之处！

![欺骗模型的图像](Images/dlls_1002.png)

###### 图10-2。欺骗高质量图像识别模型的图像。（来源：[Arxiv.org](https://arxiv.org/abs/1412.1897)。）

例如，考虑来自[第6章](ch06.xhtml#deep_learning_for_genomics)的转录因子结合模型。回想一下，该模型将DNA序列作为输入，并预测该序列是否包含转录因子JUND的结合位点。它认为结合位点是什么样子？我们想要考虑所有可能的DNA序列，并找到模型最有信心地预测存在结合位点的序列。

不幸的是，我们无法考虑所有可能的输入。长度为101的DNA序列有4^(101)种可能。如果您需要一纳秒来检查每一个，那么要处理完所有这些可能性将需要比宇宙的年龄长得多的时间。相反，我们需要一种策略来采样更少的输入。

一种可能性是只查看训练集中的序列。在这种情况下，这实际上是一个合理的策略。训练集包含来自真实染色体的数千万个碱基，因此它很可能是该模型在实践中将使用的输入的良好代表。[图10-3](#JUND-saliency)显示了模型产生最高输出的训练集中的10个序列。每个序列被预测具有大于97%概率的结合位点。其中有九个确实有结合位点，而一个是误报。对于每个序列，我们使用显著性映射来确定模型关注的内容，并根据其显著性对碱基进行着色。

![JUND显著性](Images/dlls_1003.png)

###### 图10-3。具有最高预测输出的10个训练示例。复选标记表示包含实际结合位点的样本。

看着这些输入，我们可以立即看到它所识别的核心模式：TGA ... TCA，其中...由一个或两个通常是C或G的碱基组成。显著性指示它还关注另外一个或两个在两侧的碱基。前一个碱基可以是A、C或G，后一个碱基总是C或T中的一个。这与JUND的已知结合基序一致，该基序在[图10-4](#JUND-binding-motif)中显示为一个位置权重矩阵。

![JUND结合基序](Images/dlls_1004.png)

###### 图10-4。JUND的已知结合基序，表示为一个位置权重矩阵。每个字母的高度表示该碱基出现在相应位置的概率。

唯一一个被错误预测为具有结合位点的序列并不包含这个模式。相反，它有几个重复的TGAC模式，都很接近。这看起来像一个真正的结合基序的开始，但后面从来没有跟着TCA。显然我们的模型已经学会了识别真正的结合基序，但当几个不完整的版本在附近连续出现时，它也会被误导。

训练样本并不总是能很好地代表可能输入的全部范围。如果你的训练集完全由车辆的照片组成，那么它对于模型如何响应其他输入毫无意义。也许如果展示一张雪花的照片，它会自信地将其标记为一艘船。也许甚至有一些看起来完全不像照片的输入——也许只是简单的几何图案或者随机噪音——模型会将其识别为船。为了测试这种可能性，我们不能依赖于我们已经有的输入。相反，我们需要让模型告诉我们它在寻找什么。我们从一个完全随机的输入开始，然后使用优化算法以增加模型输出的方式修改它。

让我们尝试为TF结合模型做同样的事情。我们首先生成一个完全随机的序列，并计算模型对其的预测：

```py
best_sequence = np.random.randint(4, size=101)
best_score = 
    float(model.predict_on_batch([dc.metrics.to_one_hot(best_sequence, 4)]))

```

现在来优化它。我们随机选择序列内的一个位置和一个新的碱基来设置它。如果这个改变导致输出增加，我们就保留它。否则，我们放弃这个改变并尝试其他方法：

```py
for step in range(1000):
  index = np.random.randint(101)
  base = np.random.randint(4)
  if best_sequence[index] != base:
    sequence = best_sequence.copy()
    sequence[index] = base
    score = float(model.predict_on_batch([dc.metrics.to_one_hot(sequence, 4)]))
    if score > best_score:
      best_sequence = sequence
      best_score = score

```

这很快导致了最大化预测概率的序列。在1,000步内，我们通常发现输出已经饱和并等于1.0。

[图10-5](#JUND-optimized-sequences)显示了通过这个过程生成的10个序列。所有三种最常见的结合模式（TGACTCA、TGAGTCA和TGACGTCA）的所有实例都被突出显示。每个序列至少包含其中一种模式的一个出现，通常是三个或四个。最大化模型输出的序列恰好具有我们期望的属性，这让我们对模型的工作效果感到满意。

![用于最大化模型输出的示例序列](Images/dlls_1005.png)

###### 图10-5。已经优化以最大化模型输出的示例序列。

# 预测不确定性

即使你已经确信一个模型能够产生准确的预测，这仍然留下一个重要的问题：它们究竟有多准确？在科学中，我们很少满足于仅仅一个数字；我们希望每个数字都有一个不确定性。如果模型输出1.352，我们应该将其解释为真实值在1.351和1.353之间吗？还是在0和3之间？

作为一个具体的例子，我们将使用[第4章](ch04.xhtml#machine_learning_for_molecules)中的溶解度模型。回想一下，这个模型以分子图表示的分子作为输入，并输出一个数字，指示它在水中溶解的容易程度。我们用以下代码构建和训练了这个模型。

```py
tasks, datasets, transformers = dc.molnet.load_delaney(featurizer='GraphConv')
train_dataset, valid_dataset, test_dataset = datasets
model = GraphConvModel(n_tasks=1, mode='regression', dropout=0.2)
model.fit(train_dataset, nb_epoch=100)

```

当我们首次检查这个模型时，我们评估了它在测试集上的准确性，并表示满意。现在让我们试着更好地量化它的准确性。

我们可以尝试的一个非常简单的事情就是计算模型在测试集上预测的均方根（RMS）误差：

```py
y_pred = model.predict(test_dataset)
print(np.sqrt(np.mean((test_dataset.y-y_pred)**2)))

```

这报告了一个RMS误差为0.396。因此，我们是否应该将其用作模型进行的所有预测的预期不确定性？*如果*测试集代表模型将用于的所有输入，并且*如果*所有错误都遊一个分布，那可能是一个合理的做法。不幸的是，这两者都不是安全的假设！一些预测可能比其他预测具有更大的误差，并且根据恰好在测试集中的特定分子，它们的平均误差可能比您在实践中遇到的要高或低。

我们真的希望将不同的不确定性与每个输出关联起来。我们想提前知道哪些预测更准确，哪些不太准确。为了做到这一点，我们需要更仔细地考虑导致模型预测中错误的多个因素。正如我们将看到的，必须包括两种根本不同类型的不确定性。

[图10-6](#真实与预测的溶解度)显示了训练集中分子的真实与预测的溶解度。该模型在复制训练集方面做得非常好，但并非完美。点分布在对角线周围具有有限宽度的带状区域内。即使它是在这些样本上训练的，模型在预测它们时仍然存在一些误差。鉴于此，我们必须期望它在其他未经训练的数据上至少有同样多的误差。

![真实与预测的溶解度](Images/dlls_1006.png)

###### 图10-6。训练集中分子的真实与预测的溶解度。

请注意，我们只看训练集。这种不确定性可以完全根据训练时可用的信息确定。这意味着我们可以训练一个模型来预测它！我们可以向模型添加另一组输出：对于它预测的每个值，它还将输出一个对该预测的不确定性的估计。

现在考虑[图10-7](#溶解度变化)。我们已经重复了训练过程10次，得到了10个不同的模型。我们已经使用每个模型来预测测试集中10个分子的溶解度。所有模型都是在相同的数据上训练的，并且它们在训练集上有类似的误差，但是它们对测试集分子的预测却不同！对于每个分子，根据我们使用的模型不同，我们得到了一系列不同的溶解度。

![十个分子的溶解度](Images/dlls_1007.png)

###### 图10-7。由所有在相同数据上训练的模型预测的测试集中10个分子的溶解度。

这是一种根本不同的不确定性类型，称为*认知不确定性*。这是因为许多不同的模型都能很好地拟合训练数据，而我们不知道哪一个是“最佳”的。

衡量认知不确定性的一种直接方法是训练许多模型并比较它们的结果，就像我们在[图10-7](#溶解度变化)中所做的那样。然而，通常这是代价高昂的。如果您有一个需要数周才能训练的庞大复杂模型，您不希望多次重复该过程。

一个更快的替代方法是使用dropout训练单个模型，然后使用不同的dropout掩码多次预测每个输出。通常，dropout只在训练时执行。如果在每个训练步骤中有50%的输出被随机设置为0，那么在测试时，您将改为将*每个*输出乘以0.5。但我们不这样做。我们随机将一半的输出设置为0，然后使用许多不同的随机掩码重复该过程，以获得一系列不同的预测。预测值之间的变化给出了对认知不确定性的相当好的估计。

注意，您的建模选择涉及这两种不确定性之间的权衡。如果使用具有许多参数的大型模型，您可以使其非常紧密地拟合训练数据。然而，该模型可能是欠定的，因此许多参数值的组合将同样适合训练数据。如果相反，您使用具有少量参数的小型模型，则更有可能存在一组唯一的最佳参数值，但它可能也不会像大型模型那样很好地拟合训练集。在任何情况下，在估计模型预测的准确性时，必须包括这两种类型的不确定性。

听起来很复杂。我们如何在实践中做到这一点？幸运的是，DeepChem使这变得非常容易。只需在模型的构造函数中包含一个额外的参数：

```py
model = GraphConvModel(n_tasks=1, mode='regression',
                       dropout=0.2, uncertainty=True)

```

通过包括选项`uncertainty=True`，我们告诉模型添加额外的不确定性输出，并对损失函数进行必要的更改。现在我们可以进行预测，如下所示：

```py
y_pred, y_std = model.predict_uncertainty(test_dataset)

```

这将多次使用不同的dropout掩码计算模型的输出，然后返回每个输出元素的平均值，以及每个输出元素的标准偏差的估计。

[图10-8](#solubility-uncertainty)展示了在测试集上的工作原理。对于每个样本，我们绘制了预测中的实际误差与模型的不确定性估计。数据显示了一个明显的趋势：具有较大预测不确定性的样本往往比具有较小预测不确定性的样本具有更大的误差。虚线对应于<math><mrow><mi>y</mi><mo>=</mo><mn>2</mn><mi>x</mi></mrow></math>。在这条线下的点具有预测的溶解度在真实值的两个（预测的）标准偏差内。大约90%的样本位于这个区域内。

![模型预测中的误差](Images/dlls_1008.png)

###### 图10-8。模型预测中的真实误差，与其对每个值的不确定性估计相比。

# 可解释性、可解释性和现实世界后果

错误预测的后果越严重，了解模型如何工作就越重要。对于一些模型，个别预测并不重要。在药物开发的早期阶段工作的化学家可能会使用模型来筛选数百万个潜在化合物，并选择最有前途的化合物进行合成。模型的预测准确性可能较低，但这是可以接受的。只要通过的化合物平均上比被拒绝的化合物更好，它就是有用的。

在其他情况下，每个预测都很重要。当模型用于诊断疾病或推荐治疗时，每个结果的准确性实际上可能决定患者是否生存。问题“我应该相信这个结果吗？”变得至关重要。

理想情况下，模型不仅应该产生一个诊断，还应该产生支持该诊断的证据摘要。患者的医生可以检查证据并就模型在该特定情况下是否正确运行做出知情决定。具有这种属性的模型被称为*可解释的*。

不幸的是，有太多深度学习模型是无法解释的。在这种情况下，医生面临着一个困难的选择。他们是否相信模型，即使他们不知道结果是基于什么证据？还是忽略模型，依靠自己的判断？这两种选择都不令人满意。

记住这个原则：*每个模型最终都与人类互动*。要评估模型的质量，您必须在分析中包括这些互动。它们往往与心理学或经济学一样重要，而不仅仅是机器学习。仅仅计算模型预测的相关系数或ROC AUC是不够的。您还必须考虑谁将看到这些预测，它们将如何被解释，以及它们最终将产生什么实际影响。

使模型更具解释性或可解释性可能不会影响其预测的准确性，但仍然可能对这些预测的现实世界后果产生巨大影响。这是模型设计的一个重要部分。

# 结论

深度模型被认为很难解释，但已经开发出许多有用的技术可以帮助。通过使用这些技术，您可以开始理解模型的工作原理。这有助于您决定是否信任它，并让您识别可能失败的情况。它还可能为数据提供新的见解。例如，通过分析TF结合模型，我们发现了特定转录因子的结合基序。

^([1](ch10.xhtml#idm45806164613576-marker)) Simonyan, K., A. Vedaldi, and A. Zisserman. “Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps.” [Arxiv.org](https://arxiv.org/abs/1312.6034). 2014.

^([2](ch10.xhtml#idm45806163802088-marker)) Kendall, A., and Y. Gal, “What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?” [*https://arxiv.org/abs/1703.04977*](https://arxiv.org/abs/1703.04977). 2017.
