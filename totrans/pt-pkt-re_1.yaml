- en: Chapter 1\. An Introduction to PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch is one of the most popular deep learning Python libraries, and it is
    widely used by the AI research community. Many developers and researchers use
    PyTorch to accelerate deep learning research experimentation and prototyping.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I will give you a brief introduction to what PyTorch is and
    some of the features that make it popular. I’ll also show you how to install and
    set up your PyTorch development environment on your local machine and in the cloud.
    By the end of this chapter, you will be able to verify that PyTorch is properly
    installed and run a simple PyTorch program.
  prefs: []
  type: TYPE_NORMAL
- en: What Is PyTorch?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The PyTorch library is primarily developed by Facebook’s AI Research Lab (FAIR)
    and is free and open source software with over 1,700 contributors. It allows you
    to easily run array-based calculations, build dynamic neural networks, and perform
    autodifferentiation in Python with strong graphics processing unit (GPU) acceleration—all
    important features required for deep learning research. Although some use it for
    accelerated tensor computing, most use it for deep learning development.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch’s simple and flexible interface enables fast experimentation. You can
    load data, apply transforms, and build models with a few lines of code. Then,
    you have the flexibility to write customized training, validation, and test loops
    and deploy trained models with ease.
  prefs: []
  type: TYPE_NORMAL
- en: 'It has a strong ecosystem and a large user community, including universities
    like Stanford and companies such as Uber, NVIDIA, and Salesforce. In 2019, PyTorch
    dominated machine learning and deep learning conference proceedings: 69% of the
    Conference on Computer Vision and Pattern Recognition (CVPR) proceedings used
    PyTorch, over 75% of both the Association for Computational Linguistics (ACL)
    and the North American Chapter of the ACL (NAACL) used it, and over 50% of the
    International Conference on Learning Representations (ICLR) and the International
    Conference on Machine Learning (ICML) used it as well. There are also over 60,000
    repositories on GitHub related to PyTorch.'
  prefs: []
  type: TYPE_NORMAL
- en: Many developers and researchers use PyTorch to accelerate deep learning research
    experimentation and prototyping. Its simple Python API, GPU support, and flexibility
    make it a popular choice among academic and commercial research organizations.
    Since being open sourced in 2018, PyTorch has reached a stable release and can
    be easily installed on Windows, Mac, and Linux operating systems. The framework
    continues to expand rapidly and now facilitates deployment to production environments
    in the cloud and mobile platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Why Use PyTorch?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you’re studying machine learning, conducting deep learning research, or
    building AI systems, you’ll probably need to use a deep learning framework. A
    deep learning framework makes it easy to perform common tasks such data loading,
    preprocessing, model design, training, and deployment. PyTorch has become very
    popular with the academic and research communities due to its simplicity, flexibility,
    and Python interface. Here are some reasons to learn and use PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is popular
  prefs: []
  type: TYPE_NORMAL
- en: Many companies and research organizations use PyTorch as their main deep learning
    framework. In fact, some companies have built their custom machine learning tools
    on top of PyTorch. As a result, PyTorch skills are in demand.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is supported by all major cloud platforms, such as Amazon Web Services
    (AWS), Google Cloud Platform (GCP), Microsoft Azure, and Alibaba Cloud
  prefs: []
  type: TYPE_NORMAL
- en: You can spin up a virtual machine with PyTorch preloaded for frictionless development.
    You can use prebuilt Docker images, perform large-scale training on cloud GPU
    platforms, and run models at production scale.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is supported by Google Colaboratory and Kaggle Kernels
  prefs: []
  type: TYPE_NORMAL
- en: You can run PyTorch code in a browser with no installation or configuration
    needed. You can compete in Kaggle competitions by running PyTorch directly in
    your kernel.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is mature and stable
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is regularly maintained and is now beyond release 1.8.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch supports CPU, GPU, TPU, and parallel processing
  prefs: []
  type: TYPE_NORMAL
- en: You can accelerate your training and inference using GPUs and TPUs. Tensor processing
    units (TPUs) are AI-accelerated application-specific integrated circuits (ASIC)
    chips that were developed by Google to provide an alternative to GPUs for NN hardware
    acceleration. With parallel processing, you can apply preprocessing on your CPU
    while training a model on the GPU or TPU.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch supports distributed training
  prefs: []
  type: TYPE_NORMAL
- en: You can train neural networks over multiple GPUs on multiple machines.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch supports deployment to production
  prefs: []
  type: TYPE_NORMAL
- en: With the newer TorchScript and TorchServe features, you can easily deploy models
    to production environments including cloud servers.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is beginning to support mobile deployment
  prefs: []
  type: TYPE_NORMAL
- en: Although it’s currently experimental, you can now deploy models to iOS and Android
    devices.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch has a vast ecosystem and set of open source libraries
  prefs: []
  type: TYPE_NORMAL
- en: Libraries such as Torchvision, fastai, and PyTorch Lightning extend capabilities
    and support specific fields like natural olanguage processing (NLP) and computer
    vision.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch also has a C++ frontend
  prefs: []
  type: TYPE_NORMAL
- en: Although I will focus on the Python interface in this book, PyTorch also supports
    a frontend C++ interface. If you need to build high-performance, low-latency,
    or bare-metal applications, you can write them in C++ using the same design and
    architecture as the Python API.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch supports the Open Neural Network Exchange (ONNX) format natively
  prefs: []
  type: TYPE_NORMAL
- en: You can easily export your models to ONNX format and use them with ONNX-compatible
    platforms, runtimes, or visualizers.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch has a large community of developers and user forums
  prefs: []
  type: TYPE_NORMAL
- en: There are more than 38,000 users on the PyTorch forum, and it’s easy to get
    support or post questions to the community by visiting [the PyTorch Discussion
    Forum](https://pytorch.tips/discuss).
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are familiar with PyTorch, you may already have installed it and set
    up your development environment. If not, I will show you some options to do so
    in this section. The fastest way to get started is to use Google Colaboratory
    (or *Colab*). Google Colab is a free cloud-based development environment similar
    to Jupyter Notebook and comes with PyTorch already installed. Colab comes with
    free limited GPU support and interfaces nicely with Google Drive for saving and
    sharing notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t have internet access, or you want to run the PyTorch code on your
    own hardware, then I will show you how to install PyTorch on a local machine.
    You can install PyTorch on Windows, Linux, and macOS operating systems. I recommend
    that you have an NVIDIA GPU for acceleration, but it is not required.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, you may want to develop PyTorch code using a cloud platform like AWS,
    Azure, or GCP. If you would like to use a cloud platform, I will show you the
    options to quickly get started on each platform.
  prefs: []
  type: TYPE_NORMAL
- en: Running in Google Colaboratory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With Google Colab, you can write and execute Python and PyTorch code in your
    browser. You can save files directly to your Google Drive account and easily share
    your work with others. To get started, visit [the Google Colab website](https://pytorch.tips/colab),
    as shown in [Figure 1-1](#fig_colab_welcome).
  prefs: []
  type: TYPE_NORMAL
- en: '![“Google Colaboratory welcome page”](Images/ptpr_0101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. Google Colaboratory welcome page
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you are already signed into your Google account, you will get a pop-up window.
    Click New Notebook in the bottom-right corner. If the pop-up window does not appear,
    click File and select New Notebook from the menu. You will be prompted to sign
    in or create a Google account, as shown in [Figure 1-2](#fig_google_sign_in).
  prefs: []
  type: TYPE_NORMAL
- en: '![“Google sign in”](Images/ptpr_0102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. Google sign in
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To verify your configuration, import the PyTorch library, print the installed
    version, and check if you are using a GPU, as shown in [Figure 1-3](#fig_colab_base_code).
  prefs: []
  type: TYPE_NORMAL
- en: '![“Verify PyTorch installation in Google Colaboratory”](Images/ptpr_0103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-3\. Verify PyTorch installation in Google Colaboratory
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: By default, our Colab notebook does not use a GPU. You will need to select Change
    Runtime Type from the Runtime menu, then select GPU from the “Hardware accelerator”
    drop-down menu and click Save, as shown in [Figure 1-4](#fig_colab_selct_gpu).
  prefs: []
  type: TYPE_NORMAL
- en: '![“Use a GPU in Google Colaboratory”](Images/ptpr_0104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-4\. Use a GPU in Google Colaboratory
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now run the cell again by selecting the cell and pressing Shift-Enter. You should
    see `True` as the output of `is_available()`, as shown in [Figure 1-5](#fig_colab_gpu_true).
  prefs: []
  type: TYPE_NORMAL
- en: '![“Verify GPU is active in Google Colab”](Images/ptpr_0105.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-5\. Verify GPU is active in Google Colaboratory
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Google offers a paid version called Colab Pro that provides faster GPUs, longer
    runtimes, and more memory. For the examples in this book, the free version of
    Colab should be sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: Now you have verified that PyTorch is installed, and you also know the version.
    You have also verified that you have a GPU available and that the proper drivers
    are installed and operating correctly. Next, I will show you how to verify your
    PyTorch on a local machine.
  prefs: []
  type: TYPE_NORMAL
- en: Running on a Local Computer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may want to install PyTorch on a local machine or your own server under
    certain conditions. For example, you may want to work with local storage, or use
    your own GPU or faster GPU hardware, or you may not have internet access. Running
    PyTorch does not require a GPU, but one would be needed to run GPU acceleration.
    I recommend using an NVIDIA GPU as PyTorch is closely tied to the Compute Unified
    Device Architecture (CUDA) drivers for GPU support.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Check your GPU and CUDA version first! PyTorch only supports specific GPU and
    CUDA versions, and many Mac computers use non-NVIDIA GPUs. If you are using a
    Mac, verify that you have an NVIDIA GPU by clicking the Apple icon on the menu
    bar, selecting “About This Mac,” and clicking the Displays tab. If you see an
    NVIDIA GPU on your Mac and want to use it, you’ll have to build PyTorch from scratch.
    If you do not see an NVIDIA GPU, you should use the CPU-only version of PyTorch
    or choose another computer with a different OS.
  prefs: []
  type: TYPE_NORMAL
- en: The PyTorch website offers a [convenient browser tool for installation](https://pytorch.tips/install-local),
    as shown in [Figure 1-6](#fig_pytorch_start_locally). Select the latest stable
    build, your OS, your preferred Python package manager (Conda is recommended),
    the Python language, and your CUDA version. Execute the command line and follow
    the instructions for your configuration. Note the prerequisites, installation
    instructions, and verification methods.
  prefs: []
  type: TYPE_NORMAL
- en: '![“”](Images/ptpr_0106.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-6\. PyTorch online installation configuration tool
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You should be able to run the verification code snippet in your favorite IDE
    (Jupyter Notebook, Microsoft Visual Studio Code, PyCharm, Spyder, etc.) or from
    the terminal. [Figure 1-7](#fig_verify_terminal) shows how to verify that the
    correct version of PyTorch is installed from a terminal on a Mac. The same commands
    can be used to verify this in a Windows or Linux terminal as well.
  prefs: []
  type: TYPE_NORMAL
- en: '![“”](Images/ptpr_0107.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-7\. PyTorch verification using a Mac terminal
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Running on Cloud Platforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’re familiar with cloud platforms like AWS, GCP, or Azure, you can run
    PyTorch in the cloud. Cloud platforms provide powerful hardware and infrastructure
    for training and deploying deep learning models. Remember that using cloud services,
    especially GPU instances, incurs additional costs. To get started, follow the
    instructions in [the online PyTorch cloud setup guide](https://pytorch.tips/start-cloud)
    for your platform of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up your cloud environment is beyond the scope of this book, but I’ll
    summarize the available options. Each platform offers a virtual machine instance
    as well as managed services to support PyTorch development.
  prefs: []
  type: TYPE_NORMAL
- en: Running on AWS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'AWS offers multiple options to run PyTorch in the cloud. If you prefer a fully
    managed service, you can use AWS SageMaker, or if you’d rather manage your own
    infrastructure, you can use AWS Deep Learning Amazon Machine Images (AMIs) or
    Containers:'
  prefs: []
  type: TYPE_NORMAL
- en: Amazon SageMaker
  prefs: []
  type: TYPE_NORMAL
- en: This is a fully managed service to train and deploy models. You can run Jupyter
    Notebooks from the dashboard and use the SageMaker Python SDK to train and deploy
    models in the cloud. You can run your notebooks on a dedicated GPU instance.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Deep Learning AMIs
  prefs: []
  type: TYPE_NORMAL
- en: These are preconfigured virtual machine environments. You can choose the Conda
    AMI, which has many libraries (including PyTorch) preinstalled, or you can use
    the base AMI if you’d prefer a clean environment to set up private repositories
    or custom builds.
  prefs: []
  type: TYPE_NORMAL
- en: AWS Deep Learning Containers
  prefs: []
  type: TYPE_NORMAL
- en: These are Docker images that come preinstalled with PyTorch. They enable you
    to skip the process of building and optimizing your environment from scratch and
    are mainly used for deployment.
  prefs: []
  type: TYPE_NORMAL
- en: For more detailed information on how to get started, review the [“Getting Started
    with PyTorch on AWS” instructions](https://pytorch.tips/start-aws).
  prefs: []
  type: TYPE_NORMAL
- en: Running on Microsoft Azure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Azure also offers multiple options to run PyTorch in the cloud. You can develop
    PyTorch models using a fully managed service called Azure Machine Learning, or
    you can run Data Science Virtual Machines (DSVMs) if you prefer to manage your
    own infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: Azure Machine Learning
  prefs: []
  type: TYPE_NORMAL
- en: This is an enterprise-grade machine learning service for building and deploying
    models. It includes a drag-and-drop designer and MLOps capabilities to integrate
    with existing DevOps processes.
  prefs: []
  type: TYPE_NORMAL
- en: DSVMs
  prefs: []
  type: TYPE_NORMAL
- en: These are preconfigured virtual machine environments. They come preinstalled
    with PyTorch and other deep learning frameworks as well as development tools like
    Jupyter Notebook and VS Code.
  prefs: []
  type: TYPE_NORMAL
- en: For more detailed information on how to get started, review the [Azure Machine
    Learning documentation](https://pytorch.tips/azure).
  prefs: []
  type: TYPE_NORMAL
- en: Running on Google Cloud Platform
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GCP also offers multiple options to run PyTorch in the cloud. You can develop
    PyTorch models using the managed service, called AI Platform Notebooks, or run
    Deep Learning VM images if you prefer to manage your own infrastructure:'
  prefs: []
  type: TYPE_NORMAL
- en: AI Platform Notebooks
  prefs: []
  type: TYPE_NORMAL
- en: This is a managed service whose integrated JupyterLab environment allows you
    to create preconfigured GPU instances.
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning VM images
  prefs: []
  type: TYPE_NORMAL
- en: These are preconfigured virtual machine environments. They come preinstalled
    with PyTorch and other deep learning frameworks as well as development tools.
  prefs: []
  type: TYPE_NORMAL
- en: For more detailed information on how to get started, review the instructions
    at Google Cloud [“AI and Machine Learning Products”](https://pytorch.tips/google-cloud).
  prefs: []
  type: TYPE_NORMAL
- en: Verifying Your PyTorch Environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Whether you use Colab, your local machine, or your favorite cloud platform,
    you should verify that PyTorch is properly installed and check to see if you have
    a GPU available. You’ve already seen how to do this in Colab. To verify that PyTorch
    is properly installed, use the following code snippet. The code imports the PyTorch
    library, prints the version, and checks to see if a GPU is available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You import the library using `import torch`, not `import pytorch`. PyTorch is
    originally based on the `torch` library, an open source machine learning framework
    based on the C and Lua programming languages. Keeping the library named `torch`
    allows Torch code to be reused with a more efficient PyTorch implementation.
  prefs: []
  type: TYPE_NORMAL
- en: A Fun Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have verified that your environment is configured properly, let’s
    code up a fun example to show some of the features of PyTorch and demonstrate
    best practices in machine learning. In this example, we’ll build a classic image
    classifier that will attempt to identify an image’s content based on 1,000 possible
    classes or choices.
  prefs: []
  type: TYPE_NORMAL
- en: You can access this example from [the book’s GitHub repository](https://github.com/joe-papa/pytorch-book)
    and follow along. Try running the code in Google Colab, on your local machine,
    or on a cloud platform like AWS, Azure, or GCP. Don’t worry about understanding
    all of the concepts of machine learning. We’ll cover them in more detail throughout
    the book.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In practice, you will import all the necessary libraries at the beginning of
    your code. However, in this example, we will import the libraries as they are
    used so you can see which libraries are needed for each task.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s select an image we’d like to classify. In this example, we’ll
    choose a nice fresh, hot cup of coffee. Use the following code to download the
    coffee image to your local environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the code uses the `urllib` library’s `urlretrieve()` function to
    get an image from the web. We rename the file to *coffee.jpg* by specifying `fpath`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we read our local image using the Pillow library (PIL):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure 1-8](#fig_coffee) shows what our image looks like. We can use `matplotlib`’s
    `imshow()` function to display the image on our system, as shown in the preceding
    code.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Input Image for Classifier - A fresh, hot cup of coffee](Images/ptpr_0108.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-8\. Input image for classifier
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice we haven’t used PyTorch yet. Here’s where things get exciting. Next,
    we are going to pass our image into a pretrained image classification neural network
    (NN)—but before we do so, we’ll need to *preprocess* our image. Preprocessing
    data is very common in machine learning since the NN expects the input to meet
    certain requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, the image data is an RGB 1600 × 1200-pixel JPEG-formatted image.
    We need to apply a series of preprocessing steps, called *transforms*, to convert
    the image into the proper format for the NN. We do this using Torchvision in the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We use the `Compose()` transform to define a series of transforms used to preprocess
    our image. First, we need to resize and crop the image to fit within the NN. The
    image is currently in PIL format, since that’s how we read it earlier. But our
    NN requires a tensor input, so we convert the PIL image to a tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Tensors are the fundamental data objects in PyTorch, and we’ll spend the entire
    next chapter exploring them. You can think of tensors like NumPy arrays or numerical
    arrays with a bunch of extra features. For now, we’ll just convert our image to
    a tensor array of numbers to get it ready.
  prefs: []
  type: TYPE_NORMAL
- en: We apply one more transform, called `Normalize()`, to rescale the range of pixel
    values between 0 and 1\. The values for the mean and standard deviation (std)
    were precomputed based on the data used to train the model. Normalizing the image
    improves the accuracy of the classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we call `transform(img)` to apply all the transforms to the image.
    As you can see, `img_tensor` is a 3 × 224 × 224 `torch.Tensor` representing a
    3-channel image of 224 × 224 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Efficient machine learning processes data in batches, and our model will expect
    a batch of data. However, we only have one image, so we’ll need to create a batch
    of size 1, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: We use PyTorch’s `unsqueeze()` function to add a dimension to our tensor and
    create a batch of size 1\. Now we have a tensor of size 1 × 3 × 224 × 224, which
    represents a batch size of 1 and 3 channels (RGB) of 224 × 224 pixels. PyTorch
    provides a lot of useful functions like `unsqueeze()` to manipulate tensors, and
    we’ll explore many of them in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now our image is ready for our classifier NN! We’ll use a famous image classifier
    called AlexNet. AlexNet won the ImageNet Large Scale Visual Recognition Challenge
    in 2012\. It’s easy to load this model using Torchvision, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We’re going to use a pretrained model here, so we don’t need to train it. The
    AlexNet model has been pretrained with millions of images and does a pretty good
    job at classifying images. Let’s pass in our image and see how it does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: GPU acceleration is a key benefit of PyTorch. In the first line, we use PyTorch’s
    `cuda.is_available()` function to see if our machine has a GPU. This is a very
    common line of PyTorch code, and we’ll explore GPUs further in Chapters [2](ch02.xhtml#Chapter_2)
    and [6](ch06.xhtml#pytorth_acceleration_and_optimization). We’re only classifying
    one image, so we don’t need a GPU here, but if we had a huge batch having a GPU
    might help speed things up.
  prefs: []
  type: TYPE_NORMAL
- en: The `model.eval()` function configures our AlexNet model for inference or prediction
    (as opposed to training). Certain components of the model are only used during
    training, and we don’t want to use them here. The use of `model.to(device)` and
    `batch.to(device)` sends our model and input data to the GPU if available, and
    executing `model(batch.to(device))` runs our classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output, `y`, consists of a batch of 1,000 outputs. Since our batch contains
    only one image, the first dimension is `1` while the number of classes is `1000`,
    one value for each class. The higher the value, the more likely it is that the
    image contains that class. The following code finds the winning class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Using PyTorch’s `max()` function, we see that the class with index 967 has
    the highest value, 22.3059, and thus is the winner. However, we don’t know what
    class 967 represents. Let’s load the file with class names and find out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Like we did earlier, we use `urlretrieve()` and download the text file containing
    descriptions of each class. Then, we read the file using `readlines()` and create
    a list containing class names. When we `print(classes[967])`, it shows us that
    class 967 is *espresso*!
  prefs: []
  type: TYPE_NORMAL
- en: 'Using PyTorch’s `softmax()` function, we can convert the output values to probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: To print the probability at an index, we use PyTorch’s `tensor.item()` method.
    The `item()` method is frequently used and returns the numeric value contained
    in a tensor. The results show that the model is 87.85% sure that this image is
    an image of an espresso.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use PyTorch’s `sort()` function to sort the output probabilities and
    look at the top five:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We see that the model predicts that the image is *espresso* with 87.85% probability.
    It also predicts *cup* with 7.28% and *coffee mug* with 4.3% probability, but
    it seems pretty confident that the image is an espresso.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may feel like you need an espresso right now. We covered a lot in that
    example! The core code to accomplish everything is actually much shorter. Assuming
    you have downloaded the files already, you only need to run the following code
    to classify an image using AlexNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: And that’s how you build an image classifier with PyTorch. Try running your
    own images through the model and see how it classifies them. Also, try completing
    the example on another platform. For example, if you used Colab to run the code,
    try running it locally or in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations, you’ve verified that your environment is configured properly
    and that you can execute PyTorch code! We’ll explore each topic more deeply throughout
    the remainder of the book. In the next chapter, we’ll explore the fundamentals
    of PyTorch and provide a quick reference to tensors and their operations.
  prefs: []
  type: TYPE_NORMAL
