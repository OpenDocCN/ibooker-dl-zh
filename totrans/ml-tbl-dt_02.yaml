- en: 2 Exploring tabular datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Row and column characteristics in a tabular dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possible pathologies and remedies for tabular datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding tabular data externally on the internet and internally in organizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring data to solve common problems in tabular data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tabular data may consist of practically anything—from low-level scientific research
    to consumer behavior on a website to the statistics in your fantasy sports league.
    In the end, though, the commonalities in tabular data prevail over differences,
    and you can achieve most of your data analysis job just by applying standard approaches
    and tools even without a lot of domain expertise.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll look at how to gather and prepare tabular datasets.
    We’ll also take on a practical data analysis exploration that shows the steps
    you can take to look at data from different viewpoints: by rows, by columns, under
    the light of the relationship between features, and considering their overall
    distribution in the dataset. For that example, we will use a simple toy dataset,
    the Auto MPG Data Set, a dataset freely available on the UCI Machine Learning
    website ([https://archive.ics.uci.edu/dataset/9/auto+mpg](https://archive.ics.uci.edu/dataset/9/auto+mpg)).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Row and column characteristics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Depending on the domain, it is incredible how much variety you will find in
    tabular data. That is because tabular data is the rule, not the exception, in
    the world of data, and that has been true since the very beginning. Tabular data
    has been collected for thousands of years into tables and records, from grain
    accounting in ancient Egypt to parish births, weddings, and deaths in medieval
    Europe, up to our days in modern national countries and their bureaucracies. It
    wasn’t until the 1960s that we began collecting data in computerized databases,
    which gave the term *tabular* a more electronic connotation. The widespread adoption
    of relational databases since the 1970s has popularized tabular data, making it
    ubiquitous and used for every possible application. In relational databases, data
    tables can be combined by the values of specific columns acting as joining keys.
    Such an innovation allowed computers to store more information in less disk space,
    guaranteeing the technology’s success and widespread diffusion.
  prefs: []
  type: TYPE_NORMAL
- en: You can look at open data repositories or data science competitions, such as
    Kaggle, which routinely feature tabular data competitions to give you an idea
    of the current sheer variety of tabular data. For instance, in the last two years,
    Kaggle organized the Tabular Playground Series ([https://mng.bz/pK2z](https://mng.bz/pK2z)),
    a series of competitions inspired by most common machine learning problems involving
    tabular datasets and using synthetic data devised by generative AI, as we will
    discuss more later in the chapter. Although the Tabular Playground competitions
    use generated data, the examples and the original data they took inspiration from
    are selected from real-world examples such as
  prefs: []
  type: TYPE_NORMAL
- en: Probability and amount of insurance claims
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loan defaults in the banking sector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Product testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: E-commerce sales
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environment sensor data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biological and genomic data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ecological measurements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, despite the variety of real-world applications and underlying knowledge
    domains, every tabular dataset shares the same structure of a matrix of rows and
    columns with values in numeric, time or dates, and textual forms. Such applies
    to all tabular datasets, regardless of their features. While domain knowledge
    is essential for devising optimal feature engineering for predictive algorithms,
    the basic structure of tabular data remains consistent across all domains. Such
    universality justifies the need for a book on tabular data, as examples and techniques
    can be easily transferred between domains.
  prefs: []
  type: TYPE_NORMAL
- en: Diving deeper into details, in a data table, also called a dataset, you have
    rows of values, and each row represents a unit of your analysis, which, in statistical
    terms, can be mentioned as a statistical unit or an observation. If you are analyzing
    DNA samples, for example, each row in your table represents a sample. If you are
    analyzing industrial products, each row will be a product. The principle is the
    same, and the nature of the represented units can significantly vary.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 The ideal criteria for tabular rows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The only limit to remember for rows in a tabular dataset is that examples should
    be independent unless you are working with time predictions (time series analysis)
    or other time-related problems. That’s the IID principle that you may have heard
    about before. IID is an acronym for “independent and identically distributed,”
    meaning that your samples should have been drawn independently, where each drawing
    doesn’t influence or carry information of the subsequent ones. Identically, that
    means that you always draw in the same way from the same data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s consider a simple example of IID: a coin flip. Each time we flip a coin,
    the outcome is independent of all the previous flips, and the probability of getting
    heads or tails is the same for each flip. In other words, the coin flips are identically
    distributed, according to Bernoulli distribution. If we were to generate a dataset
    by flipping the same coin repeatedly, the resulting data would be IID. The property
    of being identically distributed allows us to simplify the modeling and analysis
    of the data. For example, it allows us to randomly sample the data when creating
    cross-validation folds. It allows us to assume that predictive algorithms will
    not memorize and replicate the order of the data presented. Typical examples of
    non-IID are sales data from multiple stores, where sales from the same store tend
    to be very correlated and not necessarily with other stores’ dynamics or school
    survey data, where each class presents similar characteristics due to shared interests
    or experiences, introducing non-IID characteristics into the data. In these instances,
    the data exhibits distinctive characteristics that deviate from the assumption
    of independence and identical distribution because the samples derive from specific
    groups (stores or school classes), but similar situations also arise when the
    data is hierarchically organized, when you deal with repeated measures when you
    basically measure the same example multiple times with different but correlated
    results, or when there is any temporal dependency, typical of time series, where
    being non-IID is not a problem.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In figure 2.1, we compare how non-IID and IID situations differ when you compare
    distributions as a plotted series. You can observe an IID behavior in a feature
    on the left panel. It is based on randomly combining dice rolls with coin flips.
    On the right panel, you can examine a non-IID behavior: just notice the jump appearing
    after a certain number of samples, implying something changed in how the distribution
    was generated or in how you sampled its values.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH02_F01_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 Comparison of IID and non--IID data sequences. The left chart shows
    IID data, while the right chart illustrates non-IID data, highlighting the sequential
    correlation patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Even if we don’t know about the details of the distribution we are drawing from,
    which can be safely imagined as the result of some kind of unknown process, to
    have IID data, we should always be picking examples with no relation to each other
    and examples we presume are derived from the same situation or process. Non-IID
    data can affect your analysis in different ways. In particular, it can affect
  prefs: []
  type: TYPE_NORMAL
- en: '*Bootstrapping*, which is sampling with repetition from a sample until we obtain
    a new sample of the same size as the original, and *subsampling,* which is sampling
    without repetition from a sample until obtaining a new sample of the desired size.
    Both sampling procedures are affected because you may oversample or distort certain
    data signals, and dependency structures will be messed up. We will return to such
    sampling procedures because some learning algorithms discussed in chapter 4 use
    bootstrapping or subsampling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How your model learns,* because it can pick unwanted relationships among the
    samples that won’t be helpful at prediction time when test samples will be different
    from training one and not related to them. For instance, all learning algorithms
    based on stochasitc gradient descent (SDG) and mini-batch gradient descent, including
    deep learning, are affected by the order you present the samples to the algorithm.
    Consider how non-IID data, which has an intrinsic, hidden order, may affect the
    results from similar algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cross-validation*, which is the most efficient testing method for validating
    your machine learning models and may provide inflated estimates when data is non-IID,
    because your algorithm may learn to cluster cases based on their relationship
    specifically in the training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data that is not IID can be detected based on analysis of the data generation
    process and exploration because
  prefs: []
  type: TYPE_NORMAL
- en: 'In non-IID data, each sample is likely to exhibit some form of association
    with at least one other sample found within the same dataset. This often occurs
    because you are thinking of analyzing different units, but the units are the same.
    For instance, in medical analysis, you may be analyzing multiple medical records
    and thinking that your unit of analysis is the records themselves, which are distinct.
    In reality, you are analyzing the health data from the same patients at different
    times: the actual analysis units should be the patients, not the records. A very
    similar situation happened to the team of Prof. Andrew Ng (see [https://mng.bz/OBGE](https://mng.bz/OBGE)
    for details) when they prepared the data for a paper by using a dataset of 100,000
    x-rays from 30,000 patients, and, as they split it separating training and test
    data, they didn’t consider how the x-rays of the same patient could end up a part
    in the training set and a part in the test set, hence inflating all the results
    and distorting all the insights derived from the analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The measures in your data represent heterogeneous distributions. This typically
    happens with the passage of time and its consequent changes that reflect on the
    data at many levels. For instance, when analyzing the balance sheets of different
    companies, it is important to note that even if the companies are distinct, they
    may not represent the same distribution if they are from different years. This
    is because the macroeconomic situation is mutable and can change the characteristics
    of the companies and their sectors over time (i.e., the characteristics of the
    distribution you want to represent).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since every analysis is based on some expected distribution assumptions, data
    being IID is paramount for proper estimates in statistics and regression analysis.
    In machine learning, even if the methods are data-driven and nonparametric, IID
    data is strongly recommended, though, in practice, it is hardly found in real-world
    datasets. One fundamental limit of machine learning algorithms is that they are
    aware of the relations between features and the target but cannot figure out the
    relations between the rows. Machine learning algorithms are column-aware but not
    row-aware. Hence, it is necessary to adequately provide supplemental features
    to support the work of machine learning algorithms. Therefore, if two features
    are correlated, the algorithm will expect such correlation to derive from their
    relationship, independent of the interference of other features, not because of
    time or because of another hidden feature that affected the sampling. In other
    words, when data is not IID, the learning algorithm will learn patterns based
    on time or relationship among samples as if they were some kind of feature-based
    relationship.
  prefs: []
  type: TYPE_NORMAL
- en: 'Times series and every kind of longitudinal data representing phenomena over
    time are typically not IID. In time series, each observation is autocorrelated,
    meaning that each target value is correlated with the previous ones, as well as
    the features, called covariates in statistics, that can explain the target. Given
    your business data problem and how you collect or assemble the data, you must
    know how time affects your observations and try to control its effect using time-based
    features. Thanks to time-based features, your model can determine how time affects
    the values of the other features and your target. Usually, a time feature and
    lagged features based on time will solve the situation. Essentially, you can have
    two scenarios here:'
  prefs: []
  type: TYPE_NORMAL
- en: Each row has a date or a time interval. You can use it as a feature after appropriate
    transformation—for instance, converting dates to Unix time, a continuous numeric
    feature. In such a case, you are running a time series analysis and need to use
    proper cross-validation strategies for time series.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You pivot multiple rows related to different times, creating time-based features.
    For instance, you can have the value of a feature at different moments in time
    and create a separate feature for each of such moments (such as sales_month_1,
    sales_month_2, and so on). In this case, you just fall back to having IID data
    and can proceed to analyze without much more formality.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-sectional data, typically in tabular form, may not be naturally IID even
    when drawn from the same period. This is because interactions among the analysis
    units and their membership in specific groups can create subtle dependencies between
    observations, often called “leakage” of information. This leakage can cause unexpected
    transmission of predictive information from features to the target during training,
    and it probably won’t work the same way during prediction when the represented
    situations are different. To address this problem, it is necessary to create features
    that explicitly capture the different memberships between rows and their relationships.
    For instance, if you are working with data from different companies, failing to
    provide information about their country of origin or operations, as well as their
    sector, may create a leakage for your machine learning algorithm that could learn
    and exploit implicit orderings in the data that won’t be replicable at prediction
    time. In addition, even if you can label the examples in your training data with
    their correct group, if you cannot replicate the same in your test data, it is
    important to segregate the groups in cross-validation so that each group appears
    only in the training or validation folds. Failure to do so can cause inflated
    prediction cross-validation estimates because the machine learning algorithm can
    exploit autocorrelation within groups.
  prefs: []
  type: TYPE_NORMAL
- en: When cross-sectional data is used to compare different time periods, the data
    may not be independent even if the interactions among units and groups are absent.
    In this case, the order of observations matters, and we need to consider the time
    series dependencies between the observations. This means that the data is non-IID,
    and the usual assumptions of independence may not hold. To deal with this situation,
    we can use time series models to handle the temporal dependencies between observations.
    Time series models consider that observations taken at different points in time
    are likely to be correlated, and they can use this correlation to make better
    predictions. Using appropriate time series models and techniques, we can obtain
    accurate predictions even when dealing with non-IID data.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a final general suggestion about how to prepare your data from the point
    of view of rows, consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Verify how time affects your data. Consider using time features and modeling
    time in your analysis, using lags and moving averages, as customary in time series
    analysis, to control changes due to time alone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be aware of what you represent in rows because groups and relationships among
    them can affect your model results in training, validation, and testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If hidden groups exist in your data, explicitly represent any grouping variable
    in a feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In cross-validation, if your data contains groups, just prefer group cross-validation
    so they are never split between training and validation folds ([https://mng.bz/YDgA](https://mng.bz/YDgA)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If time is a determinant in your model, use time-based validation ([https://mng.bz/GeGO](https://mng.bz/GeGO)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we discuss the ideal criteria that should characterize
    tabular columns and what each data type implies regarding data processing.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 The ideal criteria for tabular columns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the sampling of the cases, arranged in rows, presents some challenges, remember
    it is in the columns of your data table, which are also called the features, that
    most information resides and where most of your attention should focus next. A
    column is characterized by carrying homogeneous information related to its represented
    rows. For instance, if you are building a real estate evaluation dataset and your
    rows represent houses, columns may be related to the house’s surface area (i.e.,
    how many square meters and market evaluation expressed by a recent sale price).
    By being homogeneous, you can expect that each of these columns will carry only
    the information it is designated to. For instance, you shouldn’t find sale prices
    or other information in the surface column. In addition, you should also expect
    that the column values are uniquely related to the unit that is represented by
    each row.
  prefs: []
  type: TYPE_NORMAL
- en: Before delving into data structures suitable for handling a dataset of rows
    and columns, however, you need to first recognize the five types of data that
    can populate columns and the best way to handle each of them (see table 2.1).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.1 Types of data in a tabular dataset
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Numeric features | Integers for count data, such as when enumerating sales
    by day for a product: [105, 122, 91, … 124]Floats for measurements, such as when
    enumerating sales income by day for a product: [1000.50, 1230.00, 950.80, …, 1200.00
    ] |'
  prefs: []
  type: TYPE_TB
- en: '| Ordinal features | Integers (or floats without decimals) for rankings or
    ordered levels: [0, 1, 2, 3, …, 999]Sometimes ordinal features can be verbally
    expressed by strings underling some ordered effect such as with Likert-like agreement
    or preference scales: [“strongly disagree,” “disagree,” “neither agree nor disagree,”
    “agree,” “strongly agree”] |'
  prefs: []
  type: TYPE_TB
- en: '| Low categorical features | Since they express a quality, they can be a string
    (called “labels”) or integers associated with the original labels; in that case
    you have a conversion dictionary available: {0: “red,” 1:”green,” 2: “blue”} |'
  prefs: []
  type: TYPE_TB
- en: '| High categorical features | The same as low categorical features but with
    a large number of label (e.g., US area codes or zip codes: [https://postal-codes.net/united-states](https://postal-codes.net/united-states))
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dates | They can be a string or already encoded in data formats such as “2022-03-04”,
    “Feb 15, 1957 11:45 PM” or “6/12/2022” (there are many date standards and conventions)
    |'
  prefs: []
  type: TYPE_TB
- en: Knowing each of them is the basic step in building a dataset and properly processing
    data to be fed into a machine learning algorithm. Each algorithm will require
    each type of data to be specially prepared for its best understanding and consequent
    predictive performance.
  prefs: []
  type: TYPE_NORMAL
- en: You have *numeric features* when your data is expressed by a floating number
    or an integer representing a counting of some value. Accordingly to statistics,
    features represented as floating numbers are often distinguished into ratio or
    interval scales. The distinction is that ratio scales, representing a measurement
    of something real, like unit sales or money, have an absolute real zero and are
    only positive. Interval scales are instead abstract or arbitrary measurements;
    their units can represent anything. In interval scales, the zero value is arbitrary,
    and the values can be negative. For instance, interval scales are the measure
    of temperature in Fahrenheit or Celsius degrees. You can turn a ratio scale into
    an interval one by simple transformations like subtracting the mean, which is
    a centering operation, and dividing by the standard deviation, which is a standardization
    operation. In such a fashion, you make the zero value, as well as the scale, arbitrary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Integer numbers, when the feature is a numeric one, are only ratio scales.
    However, integers can also be used for ordinal and even categorical features.
    If you need more clarification about the meaning of a numeric feature expressed
    in integers, start checking if the numbers are not continuous and if there are
    many distinct values. When such conditions are satisfied, you can be almost confident
    that you are dealing with a numeric feature. Numeric features don’t need much
    processing, and for machine learning purposes, you won’t care much if they are
    ratio or interval scale. All you must consider is that they represent a single
    kind of measurement. For instance, if you are measuring monetary values in your
    dataset, you cannot create a column representing dollars, euros, and pounds together:
    for your numeric feature to be usable, you need one measurement type for each
    column. Also, you need to ascertain that you don’t have too many missing numbers
    in your numeric features and that their values vary enough to be useful. You should
    avoid low variance or constant features in your data.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Ordinal features* are always made up of integers, usually representing rankings
    and ratings or scores. For instance, as an example of an ordinal feature, just
    think of the ratings in terms of stars for products sold on e-commerce platforms.
    Being numbers, ordinal features are pretty similar to numeric values, yet they
    require different treatment, and you cannot work with them in exactly the same
    way. First, if numeric values are a scale where each value is equidistant to the
    next and previous ones, there is no guarantee that the distance between the value
    points is always constant in an ordinal scale. For instance, as in a long-distance
    running competition, first and second place may differ by a few seconds. However,
    a second place and a third place may differ by many minutes.'
  prefs: []
  type: TYPE_NORMAL
- en: In the same way, in an ordinal scale representing some underlying numeric measure,
    some adjacent points can be numerically very near to each other, and others may
    be numerically very distant. An ordinal scale just tells you about an order, which
    means that a value comes before the next one and after the previous one. Hence,
    without making a significant approximation, you cannot process it as if it were
    purely numeric, for instance, by computing mean and standard deviation. From a
    certain point of view, you can consider ordinals as categorical features with
    meaningful ordering.
  prefs: []
  type: TYPE_NORMAL
- en: '*Categoricals* are features constituted by labels: each unique value in a categorical
    feature represents a quality referred to in the example. We can find these labels
    expressed as strings, in this case, we are unequivocally working with a categorical
    feature, or as integer numbers, and in this case, we shouldn’t mistake it for
    an ordinal. Even the missing value can be treated as a label among the others.
    Hence, missing values are more easily dealt with in categorical features. We distinguish
    between *low cardinality* and *high cardinality* categorical features based on
    the number of unique values found in the column. The distinction is helpful because
    high cardinality categorical features are challenging to handle for deep learning
    and machine learning algorithms and require a more complex treatment than low
    cardinality ones. No clear threshold exists for classifying a categorical as low
    or high cardinality. However, having more than a dozen unique values usually poses
    a challenging time for the analyst. Throughout the book, we will discuss specialized
    strategies to handle categorical features called encodings. For the moment, just
    keep in mind a special type of categorical feature—one with a single label and
    values representing the presence or absence of the label, usually 0 for the absence
    and 1 for the presence of the quality. Such features are called binary features
    or, using a more statistical term, dichotomous variables.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dates* are very commonly found in many business data and databases since time
    is essential information to be monitored for a business to work its processes
    and resources properly. Dates also vary a lot since several date formats depend
    on the country, type of business application, or adoption of particular standards.
    For this reason, some efforts have been made to standardize the dates—for instance,
    proposing the ISO 8601 standard ([https://mng.bz/zZQQ](https://mng.bz/zZQQ)),
    but we are still far from having a common standard. It is essential to know that
    dates may be reported as columns as they are already processed for better clarity
    into separated columns containing their cyclical components, which are days, months,
    hours, days of the week, and noncyclical ones—for instance, years. They can also
    be transformed into a numeric continuous value representing the flow of time;
    Unix time is the best example, representing the number of seconds that have elapsed
    since 00:00:00 UTC on January 1, 1970, excluding leap seconds. Finally, dates
    require some understanding of used conventions for treating missing information
    because, in certain applications, it is customary to avoid void or null values
    and instead prefer applying a date far in the past or future to represent that
    the time value is missing or unknown.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3 Representing rows and columns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When organizing a tabular dataset, it is important to ensure that each row is
    identifiable and uses appropriate data types for numeric, ordinal, categorical,
    and date values, which may include float, integer, string, datetime, and sometimes
    even boolean for binary features. To handle this mix of data types efficiently,
    the pandas DataFrame is the best data structure available in Python. It is an
    ordered collection of columns that provides a flexible and efficient way to manage
    and manipulate tabular data. You can learn more about the pandas DataFrame at
    [https://mng.bz/0Qw6](https://mng.bz/0Qw6).
  prefs: []
  type: TYPE_NORMAL
- en: In listing 2.1, we create a small tabular dataset from scratch with four rows,
    representing four individuals, and four columns, representing numeric and categorical
    features that characterize them, using a data dictionary. We also define a list
    containing their names to be used as a reference for accessing their information.
    We then use pandas to convert the dictionary to a DataFrame, a two-dimensional
    table-like data structure, and assign the labels.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.1 Creating a simple tabular dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a dictionary of data
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a row index
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates a pandas DataFrame from the dictionary
  prefs: []
  type: TYPE_NORMAL
- en: ④ Prints row 1
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Prints the row whose label is Alice
  prefs: []
  type: TYPE_NORMAL
- en: 'The output for the `print(df)` command should look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the rows are labeled with the example names and the columns are
    labeled with the feature names. You can access both rows and columns by their
    labels or index numbers, starting from zero. Therefore, we can access Alice’s
    information by both means of its row index, which is 1, or by its name label:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This is made possible by pandas ([https://pandas.pydata.org/](https://pandas.pydata.org/)),
    which is a Python package dedicated to data processing that allows you to quickly
    and smoothly load data from multiple sources; slice it based on columns or rows
    or on both at the same time (an operation known as *dicing*); handle missing values;
    add, rename, compute, group, and aggregate features; and pivot, reshape, and finally
    visualize your processed data. Apart from its super helpful data processing functionalities,
    it is also particularly renowned because of its data structures, Series, and DataFrames,
    the most widely used data formats for tabular data operating with Python.
  prefs: []
  type: TYPE_NORMAL
- en: In a pandas DataFrame, apart from the table of data, you also have an index
    for columns so that you can name them and for rows, helping you in the operations
    of identification and filtering. In addition, you can efficiently perform selections
    and various operations, such as combining columns or replacing missing values.
    Recently, even the popular machine learning package Scikit-learn, which has long
    accepted pandas DataFrames as input for its algorithms, has taken steps to maintain
    such data structure along all its pipelines. Now all outputs, instead of being
    transformed data into Numpy arrays, which are matrices homogeneous by type, can
    be maintained as pandas DataFrames. For more details about how this works and
    how it can affect how you use the package, see [https://mng.bz/nR54](https://mng.bz/nR54).
    A de facto standard for tabular data, pandas DataFrames will be extensively used
    throughout the book, where we will show how to apply more useful transformations
    for dealing with common tabular data characteristics and problems.
  prefs: []
  type: TYPE_NORMAL
- en: After discussing the ideal characteristics of rows and columns in a tabular
    dataset, the next section will explore what can go wrong and the implications
    and remedies.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Pathologies and remedies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a general rule, you must always strive to avoid certain conditions among
    your features, no matter what their kind, which we previously briefly mentioned
    when discussing each type of column you may have in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Previous data from data science competitions can help us see what could go wrong
    with a tabular dataset. Let’s take, for instance, the Madelon dataset ([https://archive.ics.uci.edu/ml/datasets/Madelon](https://archive.ics.uci.edu/ml/datasets/Madelon)),
    which is remembered after so many years as a very challenging data problem because
    of its specific characteristics that made prediction hard. The Madelon dataset
    is an artificial dataset generated using an ad hoc algorithm developed by Isabel
    Guyon ([https://guyon.chalearn.org/](https://guyon.chalearn.org/)), who joined
    Google Brain in 2022 as a director. The data has been presented at a contest at
    the NIPS 2003 conference, the seventh Annual Conference on Neural Information
    Processing Systems. She placed errors in data in the form of random noise and
    the target by having a part of the labels flipped. She added redundant and highly
    collinear features, clustered observations along the vertices of a five-dimension
    hypercube without providing information, and finally inserted irrelevant information.
    That made many data scientists strive to wrap their heads around the problem at
    the time. See [https://mng.bz/ga4V](https://mng.bz/ga4V) for more details about
    the generative process of the synthetic dataset if you are interested.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, Kaggle competitions such as Don’t Overfit ([https://www.kaggle.com/competitions/overfitting](https://www.kaggle.com/competitions/overfitting)),
    Don’t Overfit II ([https://www.kaggle.com/competitions/dont-overfit-ii](https://www.kaggle.com/competitions/dont-overfit-ii)),
    Categorical Feature Encoding Challenge I and II ([https://www.kaggle.com/competitions/cat-in-the-dat](https://www.kaggle.com/competitions/cat-in-the-dat)
    and [https://mng.bz/jpyP](https://mng.bz/jpyP)), extending the problem to categorical
    features and missing data, proved that when there are too many problems in data,
    even the most powerful machine learning algorithms can do very little.
  prefs: []
  type: TYPE_NORMAL
- en: Starting from such practical examples, generally speaking, the conditions you
    have to care about the most in tabular data are
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding constant or quasi-constant columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding duplicated or highly collinear columns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding irrelevant features and prioritize features that demonstrate high predictive
    power
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dealing with rare categories or with far too many labels with categoricals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spot incongruencies and misplaced, flipped, or distorted values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding too many missing cases in a column and handling existing ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Excluding leakage features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s go through each of these from a theoretical and practical point of view
    using some artificial data. More real-world examples and commands for the detection
    and resolution of such conditions will be illustrated in our example in the paragraph
    concluding the chapter. It is pretty challenging to present examples for each
    of them or find a single real-world dataset distributed to the public containing
    all such bad data examples (actually, such examples are abundant in private repositories).
    Public datasets are often curated enough to have most of such data traps already
    expunged.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Constant or quasi-constant columns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Avoid constant or quasi-constant columns. As a rule of thumb, the variance shouldn’t
    approximate zero for numeric features, and the majority class shouldn’t be over
    99.9% for categorical features. All this is paramount because machine learning
    algorithms can learn only from how empirical conditional expectations of your
    target vary with respect to your features. No change in the features implies no
    conditional change on your target from which to learn. Constant features result
    in more cumber to be handled by numerical processes behind learning algorithms,
    and quasi-constant features may even result in some overfitting because the minimal
    nonconstant part may be deterministically associated with some target output.
    The solution is to drop constant or quasi-constant columns.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.2 Dropping zero variance features
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① An ordinal encoder will transform your data from string labels to ordered
    numeric ones.
  prefs: []
  type: TYPE_NORMAL
- en: ② The VarianceThreshold class will filter all features whose variance is equal
    to or below the selected threshold.
  prefs: []
  type: TYPE_NORMAL
- en: ③ You can have the variances of all features represented by the .variances_
    attribute,
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Duplicated and highly collinear features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Avoiding duplicated or highly collinear columns is helpful because if information
    redundancy makes your learning more robust, there are some caveats. Most importantly,
    it renders learning more complicated and computationally expensive. First, duplicated
    features are useless as constant ones and should be discarded immediately because
    they just waste memory space and computation time. The discourse is different
    for highly collinear ones. No matter whether we are talking about numeric features,
    where you measure collinearity by correlations, or categorical ones, where you
    measure collinearity by association measures based on chi-square statistics when
    a set of features are very strongly associated, it is because
  prefs: []
  type: TYPE_NORMAL
- en: One is the causative feature of the other. For instance, in a dataset related
    to academic results, time spent studying and test scores are highly correlated
    because time spent is one of the determinants of good test scores, as many studies
    have determined.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They all reflect a latent feature that causes or influences them. For instance,
    in a dataset about cars, performance and emissions are partially determined by
    the type of fuel used. Even if the fuel characteristics are not recorded in the
    data and their relationship is not apparent, the performance and emissions features
    will be strongly associated with each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the first situation, the solution is simple because you just have to keep
    the causative feature and drop all the others. Nothing in the data itself will
    tell you which is the causative feature, and you basically can figure out which
    is the one only by domain knowledge, theoretical deductions, or causal analysis,
    which requires additional modeling and experimentation. The latter case is a bit
    more sophisticated because, even if you can figure out the causative feature,
    you don’t have it among your data and must decide which feature to keep. Again,
    domain knowledge can help you make decisions. Data analysis can also contribute
    with hints based on the association with the target and the data quality of the
    features at hand, using a mix of fewer errors, fewer missing data, and fewer outliers.
    Keeping only the feature most associated with the target or the one with the highest
    quality is the best choice.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.3 Finding multicollinear numeric features
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ① Sets the seed for reproducibility
  prefs: []
  type: TYPE_NORMAL
- en: '② Creates a synthetic dataset: the make_classification command will create
    a sample dataset of twenty slightly correlated features (see [https://mng.bz/eynQ](https://mng.bz/eynQ))'
  prefs: []
  type: TYPE_NORMAL
- en: '③ Adds more correlated features to the dataset: we pick the first five features,
    and we duplicate them at the end of the dataset after having added some noise,
    so they are not the same as the original ones.'
  prefs: []
  type: TYPE_NORMAL
- en: ④ Computes the variance inflation factor to spot the features that have the
    least unique contribution
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Checking for correlated features by iterating through a correlation matrix,
    we compute only the correlation coefficients of the lower triangle of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ The correlation coefficient is computed using the NumPy function np.corcoef
    ([https://mng.bz/8Olw](https://mng.bz/8Olw)).
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ The threshold to report collinearity is evaluated in absolute value (correlation
    will be negative if a feature is reversed). In addition, depending on the stage
    of the analysis, you can set this to 0.90, 0.95, or even 0.99.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous example, we deal with highly collinear numeric features in
    a synthetic dataset. We use two approaches: the variance inflation factor and
    the Pearson correlation coefficient. The variance inflation factor (VIF) is a
    typical analysis preparatory to linear models, a family of models to be discussed
    in chapter 4, which aims at quantifying how much of a feature informative continent
    is actually to be found in other features. A VIF is, therefore, more extensive
    in scope than a correlation coefficient, which is a bivariate analysis, an analysis
    between two variables at a time. Instead, the VIF tries to pounder the role of
    a feature with respect to all the others. VIFs are powerful in terms of highlighting
    potentially less contributing features. Still, they won’t reveal what feature
    is related to others or which must be removed. The higher a VIF’s value, which
    starts from 1 and goes to infinity, the more multicollinear the considered feature.
    In our example, since higher VIF values indicate a less unique contribution of
    the feature, we can quickly spot that the first and last five features of the
    dataset are more problematic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'However, you need the following one-by-one comparison by correlation analysis
    to determine which features you can narrow down the problem. By using a threshold
    set to 0.99, you will figure out only almost identical features. By setting the
    bar lower to 0.90 or 0.95, you will reveal features whose unique contribution
    is minimal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: At this point, it is up to you to evaluate what feature to keep between two
    highly collinear ones. In any case, even if you have to decide what features to
    drop based on collinearity, selecting features based on quality is something you
    always consider to later treat only with useful features. Generally speaking,
    when facing multiple errors that cannot be corrected, such as recording or measurement
    errors, a high volume of missing data, and outlying measures, a feature should
    be kept only if it shows an interesting and unique association with the target.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3 Irrelevant features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The same goes for irrelevant features, which make little sense for your problem
    and have little or no association with the target. At this stage, you just need
    to rule out improbable features that you know do not apport any information to
    your problem based on domain knowledge or simple statistical univariate tests,
    such as correlation or chi-square analysis, as we will demonstrate in the latter
    part of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting features has been widely used to reduce computing requirements during
    training, help in having a more interpretable model, and improve predictive performance
    on test data since irrelevant features create false signals and disturbances to
    machine learning algorithms, both classic and neural networks. Chapter 6 of this
    book will discuss feature selection in-depth and present tools suitable for selecting
    a working subset of your initial features.
  prefs: []
  type: TYPE_NORMAL
- en: We will focus especially on those tools based on testing the behavior of features
    being reshuffled or randomized (by random replacements of their values). The idea
    is to build a model for your problem and then shuffle each feature to check if
    the results change too much and the predictive performances decrease. Features,
    whose randomization does not affect the model performance, are likely to be irrelevant
    or carry redundant information available elsewhere among the features. Hence,
    they can be safely dropped, enhancing memory handling and training/prediction
    times.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4 Missing data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Missing data is a critical factor that must be considered, as it can affect
    any data source and pose nuisances for many learning algorithms. Deep learning
    models and numerous machine learning algorithms cannot handle missing data directly.
    However, certain specialized machine learning algorithms, such as XGBoost and
    LightGBM, can reasonably manage missing information without requiring any intervention.
    These algorithms assume the value that was previously deemed more useful in similar
    situations when confronted with a missing value. For more information, see [https://mng.bz/EaxO](https://mng.bz/EaxO).
  prefs: []
  type: TYPE_NORMAL
- en: Apart from these specialized algorithms, missing values are usually addressed
    through a process known as imputation. This involves using the information present
    in the same column (simple univariate imputation) or in all other available columns
    (multivariate imputation) to determine a reasonable replacement value or class.
    Imputing data by multivariate imputation is sometimes more effective, even for
    XGBoost and LightGBM algorithms. We will discuss multivariate imputation in more
    detail in chapter 6 of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Please take notice now that if you are dealing with missing data at the early
    stages of data preparation, missingness may be a piece of information in itself.
    For instance, if you work with a relational database query and are left or right
    joining (or full outer joining) tables, you will produce missing cases when cases
    don’t match. In such a situation, a missing case will mean no match with the conditions
    expressed in a certain database table, which can be precious information.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other cases, a missing case will mean something specific relative to how
    the data has been generated, such as in census data, where you don’t get answers
    relative to income if the answerer is too rich or poor because the necessity of
    being socially acceptable influences answers. Creating a binary feature indicating
    if a value is missing can help keep track of such patterns. See the MissingIndicator
    in the Scikit-learn package for more details on this kind of processing: [https://mng.bz/EaxO](https://mng.bz/EaxO).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.5 Rare categories
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: On the side of categoricals, dealing with too many labels or rare categories
    in a feature are two conditions that must be addressed as soon as possible, possibly
    at extraction time. We have already mentioned high cardinality categorical features
    in the chapter. Instead, you have rare categories in the context of categorical
    variables when specific categories occur infrequently or have very few instances
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Rare categories increase the possibility of overfitting at training time and
    can usually be dealt with by aggregating them, thus forming a larger class. Domain
    knowledge may guide such aggregations, suggesting similar rare categories to aggregate
    into larger ones.
  prefs: []
  type: TYPE_NORMAL
- en: When, instead, the problem is that there are way too many categories, the most
    appropriate solution is target encoding, which is mostly effective for gradient
    boosting, or using embeddings, which works the best for deep learning approaches.
    We will discuss how these approaches work and how to effectively implement them
    in chapters 6 and 7.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.6 Errors in data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Spotting incongruencies, misplaced, flipped, or distorted values is instead
    a topic of its own because fixing such a problem mostly depends on your knowledge
    of the application domain and the recording procedures for the data you are handling.
    Many errors may happen when a phenomenon is observed in the real world and recorded
    in data. Errors can range from inescapable measurement errors because of the instruments
    and sensors we use to an extensive catalog of possible mismatches, underestimates,
    and overestimates that will make the recording completely unreliable. More than
    a single error here and there, you should look for systematic errors. These errors
    almost always happen in certain situations and bias parts of the information.
    Knowing the data schema and the meaning of the data you are dealing with is the
    only reasonable remedy for such problems. It may sound very general and vague
    advice because it is, in reality, but you can only try to understand your data
    to the best of your ability and not leave anything for granted. Don’t believe
    that machine learning algorithms are robust and can fix all the errors in the
    data: systematic errors can limit the ability of your models to generalize and
    provide reliable predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.7 Leakage features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, the last point of concern you should deal with is the presence of leakage
    features. You have leakage when some predictive information that shouldn’t be
    involved in the model training temporarily inflates results, rendering poor results
    at subsequent prediction time. Leakage can happen at row and feature levels. We
    have already discussed row leakage when dealing with non-IID rows, which occurs
    when some samples are associated with others because of time mediation or the
    mediation of some other feature. Now it is time to discuss feature-level leakage,
    which is a more common problem than expected when extracting data from business
    databases.
  prefs: []
  type: TYPE_NORMAL
- en: The key principle is to ensure temporal consistency in the features used for
    modeling. Ideally, the features should align with or precede the target variable
    in time. In the best scenario, no feature should be created or generated after
    the target variable’s own time point. This temporal alignment helps to avoid potential
    data leakage and ensures that the model is making predictions based on information
    that would have been available at the time of prediction. You need a model able
    to predict the present or the future, but you cannot do that if the necessary
    features you have trained the model on are to be produced at a time point following
    the prediction itself.
  prefs: []
  type: TYPE_NORMAL
- en: When training, it is easy to break this constraint because you are taking everything
    from the past, and many information sources used for building your training data
    may not be properly documented in regard to the creation or modification time.
    Take, for instance, the example of a business offering loans using a machine learning
    algorithm. Knowing how late the payments have been may provide a tremendous predictive
    feature at training time, but reasoning how such a feature would be unavailable
    when granting the loan or not—since it is a future behavior near to the time the
    target will be defined—renders it a useless, misleading feature because of future
    leakage. As a proposed solution for this problem, we suggest closely verifying
    the generative times for features and targets, if possible, and checking if the
    target comes after the features or vice versa. Frequently, such information is
    readily available, depending on how data storage is organized in your company.
    For instance, your data could have specific meta-data showing the date and time
    when an insertion or an update has happened, or your database administrator has
    perhaps set up a particular timestamp field to snapshot the moment some changes
    occur.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, after discussing ideal conditions in tabular data and what
    could happen when they are not met, we will get nearer to real-world data by discussing
    how to find tabular data on the internet and in your organization.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Finding external and internal data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Suppose you need to find some dataset for a machine learning project. Following
    the instructions in this section, you are guaranteed to find what you need if
    it is available and accessible on the internet. In fact, not only has the number
    and quality of data repositories increased over time, but now we have new search
    and aggregation tools such as Google Dataset Search and Kaggle Datasets that allow
    us to specifically detail what we are looking for and obtain a list of results
    to choose from. But let’s start from the beginning.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the data you need always comes after defining the project and its purposes.
    Whether your objectives are business-related or academic, only after you have
    framed the goals clearly can you decide what kind of data you need to assemble.
    It is not linear because you’ll usually need to reiterate various times between
    objectives, data, and available resources. Still, data always comes after having
    a well-defined purpose and before any engineering, like building a pipeline, which
    is a sequence of data processing steps from the data repository to where the computations
    are executed, or any further modeling action. Necessarily, in the middle between
    provisioning data and processing it, there is the phase of data understanding,
    which is the concluding topic of our chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Since machine learning models need a target—that is, something to predict such
    as a number for a regression problem or a class or label for a classification
    problem—you will first be concerned with finding one or more outputs for your
    problem. Then, all machine learning models need some other information—a set of
    predictors or features called predictors—to be used to predict your target outputs.
    It is a process where you map some data (the predictors) to other data (the outputs).
    Since without any target you cannot even start your project, whereas, on the other
    end, you may begin with an incomplete set of predictors and increase them along
    the way, do always start locating your target first and then be concerned with
    your predictors.
  prefs: []
  type: TYPE_NORMAL
- en: Finding suitable data for your machine learning models and processing it correctly
    in the data preparation and data pipelining phases is one of the activities data
    scientists spend more time and effort on—and actually where they get stuck the
    most. At this time, you will need to invest efforts in terms of getting the data
    out of the repositories where it is stored after having understood how it is organized
    (the data scheme) and how to properly bring it all together into a single dataset
    in a way that is usable for your project.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Using pandas to access data stores
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Usually, you can find both the target and the predictors in the same data store,
    if not in the same table or file. Still, sometimes you may find all parts of your
    necessary data scattered around and they need to be adequately assembled before
    any usage. You may find such data inside or outside of your organization. A common
    scenario when locating data resources within an organization is to find them scattered
    among Excel files, stored in *normalized form* in a data warehouse (DWH), or left
    unmanaged in data streams. As for DWH, information is organized into multiple
    tables to minimize redundancy and improve data integrity, scalability, performance,
    and ease of maintenance. This condition is called a *normalized schema*, where
    information has been broken down into smaller, more focused tables containing
    a single subject area or entity. Typically, in such a situation, you can find
    three types of tables to be assembled:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Event tables (transaction tables or fact tables)* are designed to hold records
    of specific business events measured at a particular moment. Examples of such
    events could be an order placed in an e-commerce platform, a credit card transaction
    in banking, a doctor’s visit in healthcare, or a user’s clickstream on the internet.
    These tables can take various forms depending on the type of business and the
    specific events being tracked.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Item tables* (also known as product tables) are a type of table found in a
    DWH that provides detailed information about specific business products or events.
    For example, an item table might contain information, such as description, prices,
    and stock levels, about the individual product items purchased as part of a customer
    order or the drug prescriptions problems during patient–doctor visits. The purpose
    of an item table is to provide more in-depth data to be used for analysis and
    decision-making purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dimension tables* store additional descriptive information to add context
    to the data in other tables, such as a person’s birth date or location or a product’s
    category. When using a dimension table, caution is essential because its information
    may have been changed or updated over time. Data updating can happen in two ways:
    by maintaining a history of changes by creating a new record for each modification
    with an attached date or by simply overwriting old data with new data. In the
    latter case, you may be using data that can act as noise for your models since
    its rows are temporally misaligned, or it can leak information from the future,
    affecting your model’s capabilities to predict correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Similarly, when dealing with Excel files, you often encounter scattered and
    denormalized data. In such cases, as with DWH, you face the challenge of working
    with similar tables and the identical need to consolidate and assemble them into
    a coherent structure. For such an assembling task between event tables, item tables,
    and dimension tables, there are different tools available on the market—some of
    them even of the no-code type, where you operate without scripting but using icons
    and point-and-click actions, for instance. Historically, SQL has been the elective
    tool for programmers to gather and combine multiple data sources. Still, SQL is
    the best choice today when all your data resides in a relational database. It
    can help you structure a series of queries and temporary tables until you arrive
    at a final data table to download or directly feed to a machine learning algorithm.
    When there are multiple sources and we need to check and visualize intermediate
    results, however, pandas, our choice in the first place for handling diverse data
    types, is again the tool we suggest to master. Pandas has a lot of functionalities
    that mimic and extend those of SQL queries. Selecting, filtering, aggregating,
    ordering, and processing are as easy on pandas as on a relational database, and
    they follow the same principles, though sometimes based on different terms. For
    instance, indexes in DataFrames are assimilable to primary keys in relational
    databases. In addition, pandas DataFrames present further characteristics that
    make them more versatile and powerful than SQL for data science tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: An API that is common to other tools makes it easy to switch from pandas, which
    were created for use on a single CPU, to multiprocessing or distributed computing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can achieve processing and data exploration at the same time. In particular,
    it is straightforward to plot the data you have prepared, as we will see at the
    end of this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have more control of all the changes and the necessary manipulation steps
    that should happen on the data, which also means that it is easier to make an
    error, allowing you to store away intermediate results and preserve some data
    characteristics, such as data types and data ordering, which is instead sometimes
    hard to achieve in SQL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s imagine having three tables in a data warehouse, each containing different
    information about a company’s products. These tables need to be joined to create
    an analysis or train a machine learning model, as shown in figure 2.2.
  prefs: []
  type: TYPE_NORMAL
- en: 'The task is a quite simple example that would require a SQL query to return
    a dataset combining the three of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/CH02_F02_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 Three simple tables describing products’ features
  prefs: []
  type: TYPE_NORMAL
- en: However, the same could be easily achieved in pandas using the merge function,
    keeping control of the various stages where the data is merged and having it ready
    for further transformations to render it suitable to machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.4 Merging datasets in pandas
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: ① The first table, containing prices
  prefs: []
  type: TYPE_NORMAL
- en: ② The second table, containing descriptions
  prefs: []
  type: TYPE_NORMAL
- en: ③ The third table, containing makers and characteristics
  prefs: []
  type: TYPE_NORMAL
- en: ④ Merges the first two tables
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Merges the previous two joined tables with the third one
  prefs: []
  type: TYPE_NORMAL
- en: 'One limitation to doing this in pandas for every use case is computational
    efficiency because the package is slow. See, for instance, the following Stack
    Overflow answer: [https://mng.bz/N1x1](https://mng.bz/N1x1). There are also scalability
    problems since pandas cannot handle data larger than your computer’s available
    memory. Such limitations exist because the package has been coded with functionalities
    in mind, not performance. Consequently, most of the functions found on pandas
    are written in plain Python, and they make little access to optimized compiled
    routines—mainly routines written in Fortran and C++, as, for instance, NumPy,
    another popular matrix and array manipulation package, does. However, thanks to
    its popular API, you can learn and start with your projects using just pandas
    and then scale up to more powerful tools. As stated in the previous point, different
    products have more or less compatibility with pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: Dask ([https://www.dask.org/](https://www.dask.org/)) is an open-source Python
    library that presents both low-level and high-level interfaces for the user. It
    is designed to run on computer clusters. Dask can also work in multiprocessing
    mode on multiple CPU processors, and it handles out-of-core tasks easily. In these
    tasks, you process more data than your RAM can handle by working it by chunks
    living on the disk. This data structure copies the pandas DataFrame API but is
    much more capable of handling many rows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ray ([https://www.ray.io/](https://www.ray.io/)) is a low-level framework that
    parallelizes Python code across processors or clusters. It is ideal as a backend
    for other high-end solutions, such as Modin.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modin ([https://github.com/modin-project/modin](https://github.com/modin-project/modin))
    is perhaps the most compatible tool with Pandas API. It works just by a simple
    replacement such as `import modin.pandas` `as` `pd`, and it performs best with
    Ray as a backend.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaex ([https://vaex.io/](https://vaex.io/)) is a Python library for lazy out-of-core
    DataFrames (similar to pandas). You can also visualize and explore big tabular
    datasets on your stand-alone machine or a server. Being lazy, it can optimize
    its operations and reach a performance of up to a billion objects/rows processed
    per second.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAPIDS ([https://rapids.ai/](https://rapids.ai/)) is a collection of libraries
    for using GPUs’ computational capabilities on large matrices. It offers cuDF,
    a partial replacement for pandas. Data processing can improve efficiency (time
    to compute), not scalability, because GPUs must access your memory to determine
    what to calculate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark ([https://spark.apache.org/](https://spark.apache.org/)) is a solution
    for map-reduce, a big data processing technique, graph algorithms, streaming data,
    and SQL queries working on single-node machines or clusters. It offers various
    packages and a DataFrame data structure similar to pandas. It is the solution
    best suited to handling massive quantities of tabular data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polars ([https://www.pola.rs/](https://www.pola.rs/)) is designed as a high-performance
    DataFrame library. It has been written in Rust, which allows for faster execution,
    comparable to C/C++, and distributable computations. Polars also has better memory
    management and large dataset handling and uses a columnar storage format. Columnar
    storage formats are more efficient for storing and accessing dense data, which
    is the most typical kind of tabular data. In contrast, row-based storage formats
    are more efficient for storing and accessing mostly sparse data (pandas uses a
    row-based storage format). Polars and pandas have similar APIs. However, there
    are differences because Polars can operate both in eager mode, where commands
    are executed immediately, or lazy mode, where commands are executed upon a specific
    command. Still under development, Polars is rapidly gaining traction in the data
    science community.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on your usage of pandas and the dimensions of your tabular data, you may
    find each one of these projects interesting. As a general suggestion, we advise
    you to check if your pandas functions are available in each of the products mentioned
    previously so you won’t have to refactor your code and then evaluate the best
    solution for the size of your data.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Internet data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have created a plan for acquiring and assembling your data from
    various data storage sources such as relational databases, DWHs, and data lakes,
    we need to provide you with guidance on where to find additional sources of information
    on the internet. These sources will help you apply the deep learning and machine
    learning algorithms outlined in this book and allow you to test these algorithms
    on different datasets or even benchmark your models. In other words, we will help
    you locate online resources that can be used to increase and enhance the data
    you have already acquired.
  prefs: []
  type: TYPE_NORMAL
- en: Concerning where to look for sources, there are specialized websites where tabular
    data is collected and routinely used by researchers and practitioners. The best
    example is the UCI Machine Learning Repository ([https://archive.ics.uci.edu/](https://archive.ics.uci.edu/)).
    The machine learning community has long used this website for educational purposes
    and research on machine learning algorithms. We can also quote OpenML ([https://www.openml.org/](https://www.openml.org/)),
    the repository used by Scikit-learn, as a source for its examples. It is supported
    by the Open Machine Learning Foundation, a nonprofit organization whose mission
    is to make machine learning simple, accessible, collaborative, and open. It is
    also sponsored by private companies, such as Amazon, and universities. In both
    cases—UCI and OpenML—you can download each dataset following the instructions
    provided or by directly accessing the URL of the data itself. When we use datasets
    from these sources, we will provide the Python code snippet to download the data
    directly for you to work on.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from these open repositories, many other websites offer a shorter selection
    of open data—data provided freely for academic, scientific, or even commercial
    usage (but you have to check the usage license they came with). Governments, the
    scientific community, and sometimes even private companies grant such data to
    the public. You can get lucky and get some interesting datasets for your work
    by consulting the Harvard University Dataverse free data repository at [https://dataverse.harvard.edu/](https://dataverse.harvard.edu/)
    or by browsing the Dataset subreddit [https://www.reddit.com/r/datasets/](https://www.reddit.com/r/datasets/).
    In both cases, it is an excellent way to stumble across something interesting,
    but it may also be disappointing if you seek specific data and need help finding
    it. Suppose your search revolves around data relative to the public or macroeconomic
    sphere, such as transportation, energy, political participation, commerce, industrial
    production, consumption, and so on. In that case, the open data portals should
    provide hints on where to look for open datasets. The two best portals are Data
    Portals ([http://dataportals.org/](http://dataportals.org/)), which covers the
    world, and Open Data Monitor ([https://opendatamonitor.eu/](https://opendatamonitor.eu/)),
    which specializes in the European region. Another good source is the National
    Statistical Service. You can browse for specific countries in this comprehensive
    list provided by the United States Census Bureau ([https://mng.bz/eynQ](https://mng.bz/eynQ)).
  prefs: []
  type: TYPE_NORMAL
- en: Apart from these specific examples, the best way to come into contact with the
    type of open data you search is undoubtedly through the dataset search engine
    provided by Google, which can be found at [https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com/).
    The Google Dataset Search, comprising many possible sources and scattered repositories,
    should give you access to what you are looking for through its search results.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 shows the results from Dataset Search from Google, where we asked
    for a dataset relative to credit scoring data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH02_F03_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3 What Google Dataset Search returns for credit scoring data. Search
    input box (top of the page): where you input your search string as you would do
    in a search engine. Result display panel (left panel): where you can browse the
    results found by the search engine. Result display (center right page): where
    key information of the dataset is displayed. You are provided with links to reach
    the data.'
  prefs: []
  type: TYPE_NORMAL
- en: It is not unusual to look for outside data to start a project when you don’t
    have similar data available in your systems or want to experiment with some idea
    or machine learning model. For instance, let’s pretend you need to find external
    credit scoring data for your project to build a model from scratch based on external
    data. Credit scoring is a computed quantification of a person’s or business’s
    creditworthiness. Building a credit scoring model helps you run any business successfully,
    such as mortgages, auto loans, credit cards, and private loans, where you have
    to offer, extend, or deny credit. Credit scoring can be used for risk-based pricing—that
    is, for setting a fair price for credit, given the risks of not repaying.
  prefs: []
  type: TYPE_NORMAL
- en: Looking, for instance, for credit scoring data as you would on the Google search
    engine will open up a panel on the left side of the browser listing a series of
    datasets that could satisfy your request. You can filter the results by recency
    of the data, format, license (a point to be addressed if you want to make commercial
    use of the data), costs, and category of interest. On the right panel, a description
    of the key characteristics of the chosen dataset will appear, with a link allowing
    you to access the original repository where the data is stored and documented.
    To download the data, follow the instructions on each landing page you will be
    pointed to.
  prefs: []
  type: TYPE_NORMAL
- en: To our knowledge, Google Dataset Search is the best tool for finding the data
    you need or are looking for. In addition, we would like to point out another resource
    made available by Google through Kaggle, a company devoted to data science competitions
    that was acquired by Google in 2017\. As you will surely notice, the search engine
    often returns datasets hosted on the Kaggle platform using Google Dataset Search.
    Apart from offering a competition platform, Kaggle offers a dataset hosting service
    called Kaggle Datasets, found at [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets).
  prefs: []
  type: TYPE_NORMAL
- en: Kaggle Datasets is part of a service offered to platform users, allowing them
    to download data freely (sometimes because they open-sourced it) in exchange for
    points and rank in the gamified Kaggle system. The gaming formulation also pushes
    for data to be uploaded soon. Hence, you will find the latest data around, and
    you can expect them to be updated often. The result is an impressive collection
    of data that is not all that easy to find elsewhere, ranging from economic statistics
    to transaction collections that are helpful in building recommender systems from
    scratch. Depending on the engagement of the Kaggle user who posted the dataset
    and that of the community, you may find documentation, data analysis, machine
    learning models, and discussion associated with the data.
  prefs: []
  type: TYPE_NORMAL
- en: If you can find what you are looking for, you can download the data from the
    Kaggle Dataset directly, using the download button shown in figure 2.4, after
    registering with the website. Registering also allows you to install and use the
    `kaggle-api` command ([https://github.com/Kaggle/kaggle-api](https://github.com/Kaggle/kaggle-api))
    to download datasets from the shell. For instance, after having installed it,
    you can download the German Credit Risk data by the command
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/CH02_F04_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 Options on the German Credit Risk page at Kaggle Datasets (menu on
    the right). Easily download the entire dataset with the Kaggle API command from
    the top-down menu. You can find the command by requiring it directly on the screen
    from the menu associated with the page, as shown in the figure.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3  Synthetic data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the scenario when you have already managed to acquire some datasets but need
    to increment your tabular data available, we have to mention synthetic data generation,
    which won’t create any data from scratch but can effectively increment and improve
    the existing datasets that you have available at hand, making using machine learning
    and deep learning models that require a more significant number of examples to
    run properly feasible. Synthetic data generation is a generative AI application
    and a growing field because it can
  prefs: []
  type: TYPE_NORMAL
- en: Overcome data scarcity by inflating the number of tabular examples available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide data diversity by enhancing portions of the data, such as in the case
    of the minority class in an unbalanced classification problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate edge cases so that it can provide you with data examples that you cannot
    frequently encounter and that you can use to test your designed systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preserve privacy because it helps to generate tabular data with the same characteristics
    as the original one but with no privacy problem because all the represented data
    is fictitious
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In recent times, data generation has made great leap forwards, thanks to the
    first generative AI experiments with Generative Adversarial Networks (GANs) and
    Variational Autoencoders (VAEs), two deep learning architectures that were the
    spearhead of generative AI a few years ago.
  prefs: []
  type: TYPE_NORMAL
- en: GANs were introduced by Ian Goodfellow and his colleagues in 2014\. They consist
    of a couple of deep neural networks—namely a generator and a discriminator—trained
    simultaneously and made to interact by challenging the discriminator to guess
    the generative work of the generator against the original examples. It is an unsupervised
    process, though. Given enough time and computation, the continuous comparison
    between the generator and discriminator should lead the generator to mimic the
    real-world distribution of the original examples used for comparison by the discriminator.
    This is indeed smart and amazing, if you consider that the generator never sees
    any instance of the data it should resemble because it builds its work from purely
    random noise. This process works extremely well both for images and tabular data.
  prefs: []
  type: TYPE_NORMAL
- en: VAEs consist of an encoder and a decoder. Their function is to compress input
    data using the encoder to a state called the latent space, where the data information
    is highly condensed. Subsequently, the decoder decompresses the data with high
    fidelity and reconstructs the original data. The underlying idea is that if the
    compression-decompression process is effective, then the latent space encapsulates
    the core distributional information of the data, enabling the generation of new
    data. The architecture of VAEs is designed so that inputs are taken by the encoder
    and passed through a sequence of layers, which may consist of the same number
    of neurons or a decreasing number, until reaching a layer representing the latent
    space. This layer serves as the starting point for the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: How these deep learning approaches generate good data is proven by how a data
    science competition platform such as Kaggle uses synthetic data for its competitions.
    Due to the lack of tabular data competitions, Kaggle has recently launched a series
    of competitions based on synthetic data that don’t have anything to envy from
    traditional ones based on curated original data. The Tabular Playground Series
    has been a feature of Kaggle for the last two years and still continues to this
    day ([https://www.kaggle.com/competitions](https://www.kaggle.com/competitions)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In listing 2.5, we use the sdv package to generate 10,000 additional examples
    from the original German Credit Risk dataset, made up of 1,000 samples that we
    presented in the previous section. To install the sdv package on your system,
    use the following shell command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The sdv package, from an MIT initiative, is a set of open-source tools devised
    to help individuals and enterprises generate synthetic data starting from some
    original one. Such tools use classical statistical methods, such as the Gaussian
    copulas that can reproduce the distribution of multiple variables simultaneously,
    and deep learning. The deep learning tools are based on GANs and VAEs. In our
    example, we found that a VAE architecture can better and more easily mimic the
    data at hand. You can find more information about the sdv package on the GitHub
    page of the project ([https://github.com/sdv-dev/SDV](https://github.com/sdv-dev/SDV))
    or by reading the reference paper illustrating the methodology: Neha Patki, Roy
    Wedge, Kalyan Veeramachaneni, “The Synthetic Data Vault,” IEEE DSAA 2016 ([https://mng.bz/ga4V](https://mng.bz/ga4V)).'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.5 Generating a synthetic dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: ① Imports from sdv the function to generate metadata from your original
  prefs: []
  type: TYPE_NORMAL
- en: ② Imports from sdv the VAE to generate synthetic data
  prefs: []
  type: TYPE_NORMAL
- en: ③ Loads the German Credit Risk dataset
  prefs: []
  type: TYPE_NORMAL
- en: ④ Instantiates the metadata detector
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Detects the metadata from the original data
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Checks if the metadata has been correctly detected for each column
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Corrects the “Saving accounts” columns metadata
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Instantiates the VAE model and instructs it to train for 10,000 epochs
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Fits the VAE model with the original data
  prefs: []
  type: TYPE_NORMAL
- en: ⑩ Generates a new synthetic dataset of 10,000 examples
  prefs: []
  type: TYPE_NORMAL
- en: The script takes a while to complete since a certain number of iterations over
    the original data is necessary for the VAE to develop a good latent representation
    of the original characteristics and distributions of the data. Once completed,
    the sampled examples from the new synthetic data resemble the sample from the
    original data, but if you need more formal proof, you must resort to an adversarial
    validation evaluation, where a machine learning algorithm is challenged to distinguish
    between the original data and the generated one. A machine learning algorithm
    is more apt at detecting even subtle patterns in data; hence, if it can be confused
    when distinguishing between original and generated data, the generated data is
    assumed to be of good quality. In the following listing, we show how to proceed
    with setting up adversarial validation for the synthetic data derived from the
    German Credit Risk data.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.6 Testing a synthetic dataset using adversarial validation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ① Concatenates original and synthetic data together
  prefs: []
  type: TYPE_NORMAL
- en: ② Encodes categorical features into dummy variables
  prefs: []
  type: TYPE_NORMAL
- en: ③ Classifies if the example is original or synthetic
  prefs: []
  type: TYPE_NORMAL
- en: ④ Computes the ROC AUC score for the classification
  prefs: []
  type: TYPE_NORMAL
- en: The area under the receiver operating characteristic (ROC AUC) score, also known
    as the area under the curve (AUC), is a metric used to evaluate the performance
    of a machine learning algorithm in a binary classification problem. It measures
    the algorithm’s ability to distinguish between positive and negative instances
    based on the predicted probabilities assigned to each observation. A score of
    1 indicates a perfect classifier, while a score of 0.5 suggests a model that performs
    no better than random guessing. You can learn more about this metric by consulting
    [https://mlu-explain.github.io/roc-auc/](https://mlu-explain.github.io/roc-auc/),
    part of the Machine Learning University, an education initiative from Amazon to
    teach machine learning theory.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the ROC AUC score is approximately 0.567, which indicates that
    the synthetic data is almost indistinguishable from the original data.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will complete our overview of tabular data by exploring
    a tabular dataset retrieved from the UCI Machine Learning Repository and Kaggle
    Datasets. Many of the problems we quoted in the previous paragraphs will be spotted
    as we analyze the data, and some remedies will be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Exploratory data analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Assembling the dataset into a data matrix arranged by rows and columns is just
    the starting point of a longer process. The data is then examined, explored, transformed,
    and finally fed into a model. Studying and exploring data are crucial stages of
    the process because they give you an idea of what you may have missed in the assembling
    phase and what can be done for your specific predictive problem. *Exploratory
    data analysis* (EDA) is often associated more with the feature engineering phase—more
    in the case of using classical machine learning approaches for tabular. Usually,
    EDA is instead almost completely ignored when using deep learning. We want to
    stress that EDA shouldn’t be limited to helping create features. It is an overall
    process of exploration to discover how the data can be used at its best.
  prefs: []
  type: TYPE_NORMAL
- en: EDA certainly has a statistical flavor since it was suggested by one of the
    most prominent statisticians of the 20th century, John W. Tukey, in his 1977 masterpiece,
    *Exploratory Data Analysis*. In his work, Tukey claimed that statistical work
    doesn’t come from simply modeling and hypothesis testing based on theoretical
    assumptions. According to Tukey, data by means of EDA can also tell you what is
    possible in scientific and engineering problems and hint you at the best ways
    to curate your data. There are no predefined blueprints for an EDA. Still, smart
    usage of statistical description and tests and the graphical representation of
    the features alone and in their relationship with others can tell you how to act
    on the data. In general, first, employing descriptive statistics and plots, you
    examine every single feature, called the univariate approach in statistics. You
    then examine how features relate to each other, called the bivariate approach.
    Finally, you try to get a glimpse of all the features together using multivariate
    techniques and dimensionality reduction ones, such as t-SNE and UMAP.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this phase, just after assembling your data into a tabular matrix, EDA can
    do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Inform you about the characteristics of the data, such as numeric, ordinal,
    high/low categorical, and date features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Give you an idea of the values of each feature and how they are distributed.
    This is especially useful when working with neural networks, where value scales
    are important.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tell you if there are missing values because neural networks and some machine
    learning algorithms are deemed to fail in the presence of missing data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Locate outliers and clusters of values due to errors and mistakes in extraction
    and assembling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spot rare categories that can be eliminated or aggregated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will show you how to achieve such explorations using simple pandas commands
    since we deem an EDA process fully guided by an analyst’s reasoning more effective
    than an automated one. However, you can also use automatic data exploration tools
    if the number of features to explore is vast and you need to save time. Later,
    you can integrate the automatic EDA with specific and focused custom data explorations.
    Among the open-source packages for automatic EDA, there are a few notable ones
    that we would like to suggest you try, and all of them are valid solutions that
    are easy to learn and use:'
  prefs: []
  type: TYPE_NORMAL
- en: AutoViz ([https://github.com/AutoViML/AutoViz](https://github.com/AutoViML/AutoViz))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sweetviz ([https://github.com/fbdesignpro/sweetviz](https://github.com/fbdesignpro/sweetviz))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pandas Profiling ([https://mng.bz/DMxw](https://mng.bz/DMxw))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following chapters, we will reprise EDA to complete the panorama with
    the further explorations you may need depending on the techniques (machine learning
    or deep learning) you want to apply to your data for predictive purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1  Loading the Auto MPG example dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, as an example of how a simple EDA can hint at ideas and solutions for further
    processing your data and taking necessary remediation steps when adapting it to
    the model you want to use, we can go for the Auto MPG Data Set, a dataset freely
    available on the UCI Machine Learning repository ([https://archive.ics.uci.edu/ml/datasets/auto+mpg](https://archive.ics.uci.edu/ml/datasets/auto+mpg)).
    The dataset, assembled by Ernesto Ramos and David Donoho, originates from the
    StatLib library maintained at Carnegie Mellon University. Previously, it was featured
    in the 1983 American Statistical Association Exposition ([https://mng.bz/lYa8](https://mng.bz/lYa8))
    and the works of Ross Quinlan. Ross Quinlan is a major contributor to the development
    of the decision tree algorithm and the creator of the C4.5 ID3 algorithms. He
    quotes this dataset in his 1993 paper “Combining Instance-Based and Model-Based
    Learning” ([https://mng.bz/BXx8](https://mng.bz/BXx8)), a true milestone paper
    on regression problems in machine learning. For our purposes, it is a simple,
    manageable example because of its mixed set of features and some missing data
    to be handled in the mpg and horsepower features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a list of the available features in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'mpg: continuous'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'cylinders: multivalued discrete'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'displacement: continuous'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'horsepower: continuous'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'weight: continuous'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'acceleration: continuous'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'model year: multivalued discrete'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'origin: multivalued discrete'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'car name: string (unique for each example, it can be used as an index in a
    pandas DataFrame)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our example, the list is short, but the available features can last dozens
    of pages in most complex tabular datasets.
  prefs: []
  type: TYPE_NORMAL
- en: To upload our example dataset, we can refer to the code in the following listing
    that will connect to the UCI repository, and it will dump the data into a pandas
    DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.7 Download of Auto MPG Data Set from UCI repository
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: ① StringIO reads and writes an in-memory string buffer.
  prefs: []
  type: TYPE_NORMAL
- en: ② Requests is an HTTP library that can help you recover data from the web.
  prefs: []
  type: TYPE_NORMAL
- en: ③ Derived from the documentation on the UCI machine learning repository
  prefs: []
  type: TYPE_NORMAL
- en: ④ Fixed-width data requires providing each feature with its start and end position
    in the input.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ The dataset is read from the web by requests.get() and then turned into a
    string buffer.
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ pd.read_fwf reads a table of fixed-width formatted lines into a DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Auto MPG data set is stored in a fixed-width text file, where values are
    on the same line, separated by a certain number of spaces or a tab. The return
    carriage at the end of the row signals the end of an example. The following is
    a sample of the text file that the request library retries for us from the UCI
    Machine Learning repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This explains why we must use a fixed-width reader such as pandas `read_fwf`
    and specify the character position where we expect each value to start and end.
    More commonly, you can find other datasets organized as CSV files. If the same
    dataset had been stored in CSV format, it would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In CSV files, the values of the features are not placed in fixed positions in
    rows; they vary in position but not in order of sequence, and you find them separated
    because of a special character denominated as a separator. In our example, it
    is the comma. Please also notice that textual data, such as strings or dates,
    is often delimited between quotes to avoid confusion if the text contains the
    separator.
  prefs: []
  type: TYPE_NORMAL
- en: In such a case, you have to use the pandas `read_csv` reader, which can handle
    CSV files. Depending on the situation, your data may be stored as a JSON file
    or an XML file, but there are readers also for such formats in pandas (see table
    2.2).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2.2 Common data reading methods in pandas
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `read_csv`  | Loads delimited data from a file, URL, or file-like objectUses
    a comma as the default delimiter |'
  prefs: []
  type: TYPE_TB
- en: '| `read_json`  | Reads data from a JSON string representation |'
  prefs: []
  type: TYPE_TB
- en: '| `read_xml`  | Imports shallow XML documents as a DataFrame |'
  prefs: []
  type: TYPE_TB
- en: '| `read_html`  | Reads all tables found in the given HTML document |'
  prefs: []
  type: TYPE_TB
- en: '| `read_excel`  | Reads tabular data from an Excel XLS or XLSX file |'
  prefs: []
  type: TYPE_TB
- en: 2.4.2  Examining labels, values, distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'All you have to do is understand what kind of data you are reading from the
    internet or your local disk and use the appropriate readers among the ones offered
    by pandas: each will be automatically handled by the proper function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we already have a databook for the dataset (just read “attribute information”
    at [https://archive.ics.uci.edu/ml/datasets/Auto%2BMPG](https://archive.ics.uci.edu/ml/datasets/Auto%2BMPG)),
    we can immediately take note in specific variables about what features are numeric,
    ordinal, and categorical:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'In different cases with other datasets, you have to discover such information
    through your data exploration. The variables will have to be populated by your
    discoveries. Hence, if we do not know the characteristics of our features, we
    should discover them through first data exploration, which can be quickly accomplished
    by a few manual pandas commands followed by some deductions. For instance, we
    could ask for a sample of the first rows and immediately get an idea of the data
    we are dealing with. We get just the top five rows using the `.head(n)` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The results are shown in figure 2.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH02_F05_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 Results from the `data.head(5)` command
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, many features are floats without decimal parts, which hints
    at them being integers. Integers may also point out a numeric, ordinal, or categorical
    feature. Requiring the number of distinct values for each feature is the second
    step in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The returned results are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: With few distinct numbers, cylinders and origin features can be treated as categorical
    variables. Hence, they could be both embedded, a procedure that transforms them
    into continuous numerical features, or transformed into binaries for each value
    by one-hot encoding. Domain knowledge can help us classify cylinders as an ordinal
    feature since cylinders may have different volumes in different car models, and,
    generally, having more cylinders proportionally corresponds to more engine power.
    Similarly, the feature relative to years may also be dealt with as an ordinal
    representing time progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another quick explorative check, based on the examination of the standard deviation
    of each of your numeric features, could help you identify your data further and
    even select it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The returned results are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'By doing so, you can be aware of constant or quasi-constant features in your
    data. Excluding missing values, if a feature is set to zero or too low in variance,
    it can be safely excluded from your data because it probably won’t bring any tangible
    advantage to process it further. However, suppose missing values are present in
    good quantity. In that case, you may want to create an indicator variable to keep
    track of the missing patterns in that feature, as they could be predictive. Consider
    checking with a command the number of missing cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `.isna()` method will return a boolean telling us if any feature sample
    is missing. We can count how many missing samples there are by summing the number
    of missing samples in a feature. A True value equates 1 and a False 0\. The obtained
    results are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The `.isna` method will help you keep track of the NaN values in your features.
    As discussed, features with a large number of missing numbers may conceal some
    interesting predictive pattern and may be turned into missing binary indicator
    variables, You can use the MissingIndicator in Scikit-learn for this purpose:
    [https://mng.bz/dXoO](https://mng.bz/dXoO). Additionally, depending on your predictive
    algorithm, you always have to deal with missing values, whether there are many
    or just a few handfuls. If you use a deep learning solution, you must impute the
    missing values with a numeric value. Usually, you take the mean, median, or mode
    as a replacement for missing values. Still, more sophisticated treatments are
    also based on iterative estimations of the best value to use as a substitute.
    An example is the IterativeImputer in Scikit-learn: [https://mng.bz/rKaD](https://mng.bz/rKaD).'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is helpful to remember that imputing the missing values with the zero value
    is a good strategy for neural networks and generalized linear models when your
    numeric features are standardized when you remove the mean and divide by the standard
    deviation: in such a case, the zero value corresponds to the mean for all the
    numeric features. The same goes for most machine learning algorithms but the approach
    is differentfor the most advanced gradient boosting implementations, such as XGBoost
    and LightGBM. Such algorithms can appropriately treat missing data without any
    further intervention on your side.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes conventional numbers (such as –999) are used in tabular datasets.
    If you know that a certain value points out to a missing value, you have to modify
    your command to consider such information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code snippet, you check all the numeric features against the
    value –999, which is a marker for missing data. Basically, any number could be
    used as a marker, and you have to know in advance what is used in your data to
    set a missing value. Though a number, it won’t cause any error when training your
    model, but it will seriously mislead its learning. Hence, the missing marker should
    always be addressed as a NaN missing value to prevent potential misinterpretation
    by deep learning models and certain machine learning algorithms. Tree-based models,
    like gradient boosting or random forest models, are better equipped to handle
    such situations, especially when the marker is positioned at an extreme end of
    the feature’s data distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having checked about missing data and standard deviations, you now start checking
    the distribution of your features to spot other useful information that can guide
    your treatment of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Figure 2.6 shows the outputs of the describe method.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH02_F06_Ryan2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 Results from the `data.describe()` command
  prefs: []
  type: TYPE_NORMAL
- en: The pandas describe method allows you to represent basic descriptive statistics
    for all your numeric features. Note that missing data is ignored. After checking
    the missing values and the variance or the standard deviation, your attention
    should focus on the minimum and maximum values with respect to the mean. Too large
    or too small values should draw your attention to outliers (extreme values in
    a data distribution) or too skewed distributions. Outliers and skewed distributions
    are not uncommon in data analysis, and sometimes they have to be removed. Sometimes,
    they should be taken at face value without any corrective action, depending on
    the motivation of your analysis. In this phase, outliers and skewed distributions
    may hint at problems in collecting and assembling your data, and corrective actions
    imply removing or correcting them. For instance, an outlying value is due to errors
    in the data. Also, stacked data recorded with different methodologies may have
    generated a skewed distribution. For instance, in a table, your measures are in
    meters, and they are instead expressed in centimeters in another one.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reasons behind all such errors in data vary widely, and it is advised to
    check your data closely with descriptive statistics and charts and then figure
    out a corrective action to remove the errors. Such errors are not limited to numeric
    features but can be easily found in categorical ones. Let’s now examine the only
    string feature available, the `car_name` one, by splitting its elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The lambda function in the apply function will split the instances of `car_name`
    into single words. Since the result will be a series of lists, using the `.explore()`
    method, we unroll all the lists into a single feature. By counting the single
    values in this new feature, we get the frequency results for each word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'It looks fine at first sight, with most instances containing a brand name and
    some information on the type of car. However, among the most frequent labels,
    you notice some incongruent information, such as `"(sw)"`, which stands for sport
    wagon, and `custom`, which refers to the customized accessories in the car. At
    a closer inspection, you finally notice that, even in this curated dataset, there
    are problems since many brands are misspelled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The returned results are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In this data slice containing the words composing the categorical feature,
    we notice how some brands are misspelled (e.g., Toyota and Toyouta, Volkswagen
    and Vokswagen but also VW as a shortening). Misspells will split the examples
    related to a true category (Volkswagen and Toyota in this case) into multiple
    categories, putting your model at risk of picking up noisy evidence or discarding
    weaker signals. The same happens with other categorical and ordinal features that
    present erroneous additional categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The returned results for data origin are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s operate the same for `model_year`, ordering the results based on the
    year:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Figure 2.7 shows how the ordered value results for `model_year` will appear.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH02_F07_Ryan2b.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 Ordered value counts
  prefs: []
  type: TYPE_NORMAL
- en: 'Not all these problems with data taken singularly affect the results much.
    However, their combined presence may affect your model performances in an evident
    way when your dataset is composed of tens and tens of features with minor problems.
    Remediation is simple: just remove the cases with the erroneous category labels
    unless you can reasonably correct them or treat them as missing cases and then
    impute them. Since they require inspection and reasoning on every feature, such
    actions may take considerable time and effort. Still, you are doing EDA precisely
    for this purpose, and later, a model that is more confident and performing in
    predictions will repay your efforts in full. Tabular data has stronger requirements
    on the cleanness of data, especially when applying classical machine learning
    models than generally expected from unstructured data processed by deep learning,
    where errors are sometimes considered useful noise to help avoid overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Being on the lookout for more errors in data, it is now time, after checking
    on the categorical features, to examine closely the numeric ones too. Here, boxplots
    and histograms replace category counts, and the focus is also widened from single
    features (the so-called statistical univariate approach) to more features together.
    Each value in the features of an example may not appear erroneous per se. Still,
    if all the values are taken together, you may realize that their combination is
    highly unlikely, if not the result of some error. You start examining the distributions
    by boxplots. A boxplot, also called a box and whiskers plot, drafts the key characteristics
    of a distribution using the boundaries of a box: you can see the first quartile
    (Q1) and the third quartile (Q3) as the top and bottom sides of the box (the median,
    Q2, is represented inside the box). Its whiskers represent the farthest points
    in both directions, not exceeding Q3 + 1.5 × IQR and Q1 – 15 × IQR (as a reminder,
    IQR is the difference between Q3 and Q1). Outlying observations beyond such boundaries
    are plotted as separate points, and you can immediately spot if there are any
    present and how many there are. Since a boxplot scales to the unit measures of
    the feature you represent, if you are to compare multiple boxplots with the numeric
    features for their distributions, you need first to standardize them by subtracting
    the mean and divide by their standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Figure 2.8 shows the chart output.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH02_F08_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 Boxplots of numeric features from the Auto MPG Data Set
  prefs: []
  type: TYPE_NORMAL
- en: 'The purpose is to determine if the data is wrong because of your collection
    or aggregation procedure. Features with outliers such as mpg, horsepower, and
    acceleration require a closer inspection through a histogram to exclude the absence
    of extraneous or erroneous values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Figure 2.9 shows the resulting 64-bin histogram.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH02_F09_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 Histogram of horsepower feature
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the horsepower feature, there does not seem to be anything relevant
    except a peculiar peak at around value 150: a concentration of examples due to
    fiscal reasons. In fact, since you have to pay more taxes if you own a car exceeding
    a certain horsepower threshold, designers of many cars’ engines just stayed below
    that threshold to make their cars more marketable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the acceleration feature, everything is okay. There are just long tails
    on both sides:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Figure 2.10 shows the resulting 24-bin histogram.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH02_F10_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 Histogram of acceleration feature
  prefs: []
  type: TYPE_NORMAL
- en: 'Histograms, which are bar charts of frequencies of values bins, work for numeric
    variables. For ordinal and categorical variables, a simple bar chart on value
    counts obtains the same information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Figure 2.11 shows the resulting bar plot.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH02_F11_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 Bar plot of cylinders feature
  prefs: []
  type: TYPE_NORMAL
- en: Here, you should look for rarer classes because they can have an error or an
    outlying rare observation that you must decide whether to keep. Domain knowledge
    should help you make these decisions. In our example regarding cylinders, we actually
    should keep both the three-cylinder and five-cylinder classes, even if they are
    rare, because a quick check can reveal that we have few three-cylinder cars—cars
    installing this type of engine are smaller and tend to have much less market share
    than the more common four-cylinder vehicles. In addition, car designers underuse
    the five-cylinder layout, similar to other odd layouts, because it presents costs
    similar to the six-cylinder layouts but implies more complexity in many engineering
    aspects and many more shortcomings in terms of performance.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.3  Exploring bivariate and multivariate relationships
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Spending time examining how values are distributed using histograms and bar
    plots will provide information for catching errors that may later affect your
    work. Even if the errors on every feature may have a minimal effect, the sum of
    all errors on all the features you will be using may affect your predictive algorithm
    significantly. Once you are done spotting the problems with single features, it
    is time to check how they relate to each other. How features relate to your target
    and how you can exclude part of them, leaving your results unmodified or even
    improving them, is a topic we will discuss later when discussing feature selection.
    Your priority at this point is avoiding redundant features in your dataset. For
    redundant features, we intend duplicated features or features that are extremely
    similar:'
  prefs: []
  type: TYPE_NORMAL
- en: Duplicated features with different names.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly collinear numeric features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar categorical features, apparently different because of level aggregations
    or because of used labels that are different.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar numeric and categorical features that are both derived from the same
    source. An example is in finance when you have a probability of default and a
    corresponding default rating, which are usually expressed with alphabetic labels
    like AAA or BB.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We start figuring out how to spot duplications and high collinearity among
    numeric features. Our favored tool for this investigation is the bivariate correlation:
    the correlation between the features, one by another. When the numeric features’
    bivariate correlations are arranged in a symmetrical matrix, we have a correlation
    matrix, which can be immediately visualized as a chart for spotting collinearity
    if there are not too many features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: The resulting plot is shown in figure 2.12.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH02_F12_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.12 Correlation plot with values heatmap
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we only have a few features involved, and the readability of
    the matrix is high. When the number of features is high, it is better to list
    only the bivariate correlations that exceed a certain threshold. In both cases,
    you must pay attention to correlations that exceed the absolute value of 0.98
    to 0.99\. Also, correlations may be negative, and a negative correlation approaching
    minus one is another case of collinearity. When you have such high correlations
    between features, you must try to understand why and then decide which one to
    drop. Dropping some of the collinear features will reduce your dataset and avoid
    problems later, depending on the feature selection method used or the learning
    algorithm. In fact, with collinearity or multicollinearity, if it involves more
    than two features at once, you may experience problems in your dataset in both
    the convergence of the learning algorithm, resulting in suboptimal results and
    interpretability of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: Collinearity, though associated with correlations, also affects categorical
    features and sometimes has to be found even among categorical and numeric ones.
    In this case, you cannot use correlation. Even if correlation for this purpose
    is quite robust, and even if you are comparing ordinal and binary features, when
    working with categorical features, you have to transform them into numbers using
    a procedure called label encoding that implies assigning arbitrary numbers to
    categories. Since label encoding is based on arbitrary value assignment, you cannot
    even establish the identity of the same encoded categorical feature using correlation
    if your encoding is applied differently to the same feature.
  prefs: []
  type: TYPE_NORMAL
- en: In similar situations, we can recur to associative measures based on chi-square
    statistics or Cramer’s V measure when correlation is not applicable. Cramer’s
    V is a statistic based on the chi-square value divided by the number of labels
    in the features being compared. This operation normalizes the value recorded on
    a feature, making it comparable across all dataset features. The square root of
    the result ranges from 0 to 1, providing a measure of intensity but not directionality
    of the relationship. Regarding intensity, Cramer’s V values close to 0 indicate
    unrelated features, while those near 1 indicate a high association and collinearity
    among the features.
  prefs: []
  type: TYPE_NORMAL
- en: Cramer’s V can also be applied by comparing a categorical feature and a numeric
    one if you previously discretized the numeric one into a categorical one using
    a transformation based on deciles, for instance. Listing 2.8 is a comparison between
    two features from our example dataset. In the example, we first create a function
    to compute `cramerV` from the chi-square score from a table. We apply it to a
    comparison between a categorical and a numeric feature after having discretized
    the numeric feature using deciles.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.8 Using Cramer’s V to detect association
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: ① Imports from Scipy the function for computing the chi-square test of independence
  prefs: []
  type: TYPE_NORMAL
- en: ② Prepares a function for calculating Cramer’s V having as input the score chi-square
    test of independence and the table it has been derived from
  prefs: []
  type: TYPE_NORMAL
- en: ③ Counts all the elements in the table
  prefs: []
  type: TYPE_NORMAL
- en: ④ Figures out the minimum dimension of the table
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ The result is the root of the chi-square score divided by the number of elements
    in the table and its minimum dimension.
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Transforms the numeric feature mpg into deciles so they can fit into a table
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Creates the table between the model_year and mpg deciles to estimate their
    association
  prefs: []
  type: TYPE_NORMAL
- en: ⑧ Returns the score, the p-value, the degrees of freedom, and the expected table
    based on marginal probabilities
  prefs: []
  type: TYPE_NORMAL
- en: ⑨ Prints the resulting Cramer’s V
  prefs: []
  type: TYPE_NORMAL
- en: In the code, by providing the chi-square score and the table itself, we call
    the Cramer-V function to get a 0.855 value for Cramer’s V comparing MPGs and the
    year of the model. Cramer’s V is a reciprocal measure used to identify features
    with a similar role in prediction. Unlike directional measures, Cramer’s V is
    insensitive to feature swapping. This property allows for identifying highly correlated
    features that can be removed from the analysis. A very high positive or negative
    correlation between features may indicate redundancy, and one of the two features
    can be dropped.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a last step after your EDA has dealt with univariate and bivariate explorations,
    as we pointed out, you need to examine your dataset from a multivariate point
    of view: this will help you detect the presence of any chunk of data that doesn’t
    fit in the data. Again, you are performing such an examination not to find better
    ways to fit your model precisely but as a preliminary operation to validate your
    tabular data as fit for the problem you want to represent. Multivariate approaches
    require all features to be numeric, as previously seen for Cramer’s V; you can
    discretize numeric features for this purpose and operate projecting all your data
    to lower dimensionality, no matter how complex it is, and thus you can visualize
    it and easily spot anomalies.'
  prefs: []
  type: TYPE_NORMAL
- en: The process consists of reducing your data to a few comprehensive summary features
    of dimensions and plotting them on a chart to visually spot patterns and isolated
    clusters of points that could be anomalies. Common multivariate approaches for
    obtaining a lower dimensionality projection are
  prefs: []
  type: TYPE_NORMAL
- en: 'Principal component analysis (PCA) and singular value decomposition (SVD):
    [https://mng.bz/VVl0](https://mng.bz/VVl0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'T-distributed stochastic neighbor embedding (t-SNE): [https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Uniform manifold approximation and projection (UMAP): [https://github.com/lmcinnes/umap](https://github.com/lmcinnes/umap)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PCA and SVD have their roots in statistical analysis, and PCA has a long history
    of being applied for EDA purposes. Both are, however, approaches based on linear
    combinations of features (your reduced summary dimensions are a weighted summation
    of your data) and, thus, are only sometimes suitable to catch nonlinear patterns
    that are frequently found in real-world data. The more recent t-SNE and UMAP are
    methods that can reduce the dimensionality of data exceptionally well, allowing
    you to chart a reliable representation of your data where original data characteristics
    are maintained. There are some caveats, though, since tweaking the hyper-parameters
    of both methods may lead to entirely different projections from the same data.
    This is due to the loss of information in the data when passing from multiple
    features to a few ones that may result in different resulting plots—some more,
    some less representative of the data itself. In addition, the resulting plots
    may only sometimes be easily interpretable, especially if there are many data
    points or if the data is highly clustered. That said, t-SNE and UMAP are valuable
    tools for data exploration and visualization, as they can help identify patterns
    and clusters within complex datasets that may not be readily visible through other
    methods. Of course, a core principle of EDA does not uniquely rely solely on them
    but compares their outputs with other multivariate, bivariate, and univariate
    methods, as shown.
  prefs: []
  type: TYPE_NORMAL
- en: Before starting working with these methods, read articles such as “How to t-SNE
    Effectively” ([https://distill.pub/2016/misread-tsne/](https://distill.pub/2016/misread-tsne/))
    or “Understanding UMAP’’ ([https://mng.bz/AQxK](https://mng.bz/AQxK)), which will
    provide you with additional confidence on using the methods with the appropriate
    precautions. Another caveat is that both approaches are computationally intensive,
    and it may take quite a long time to obtain a reduction from a large and complex
    dataset. Recently, however, NVIDIA has furthermore developed its RAPIDS suite
    based on CUDA and GPU technology ([https://developer.nvidia.com/rapids](https://developer.nvidia.com/rapids)),
    which can dramatically cut the time necessary before getting results from both
    UMAP and t-SNE, making them even more effective for intensive EDA explorations.
    Listing 2.9 shows the code to analyze our example dataset using the t-SNE implementation
    in the Scikit-learn package. We will present the NVIDIA RAPIDS implementations,
    as well as their other tools for processing tabular data, later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.9 Plotting a t-SNE low-dimensional projection of a dataset
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: ① Imports the t-SNE class available in Scikit-learn
  prefs: []
  type: TYPE_NORMAL
- en: ② Imports pyplot from matplotlib for chart plotting
  prefs: []
  type: TYPE_NORMAL
- en: ③ t-SNE is set to project results in two dimensions; the other parameters are
    kept at their default settings.
  prefs: []
  type: TYPE_NORMAL
- en: ④ Only numeric and ordinal features are used; missing values are replaced with
    the mean because the t-SNE class requires a complete input data matrix.
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ By fit_transform, the projection is created based on the provided data and
    applied to the data itself.
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ The t-SNE transformed data is plotted as a bidimensional scatterplot.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.13 shows the resulting plot of the t-SNE transformed data.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH02_F13_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.13 Points distribution resulting from t-SNE two-dimensional transformation
  prefs: []
  type: TYPE_NORMAL
- en: 'The code snippet works only on the numeric features that, after having all
    the missing values replaced by the mean (neither t-SNE nor UMAP can work with
    missing data), squeeze the data into a two-dimensional representation that a scatter
    plot can plot. In this dataset, the results appear to be very regular and do not
    present much of a surprise if we think this is a selection of curated examples
    collected for a purpose: all the data points have ended up being aligned in a
    curvilinear cloud, offering an overall impression of examples chosen progressively
    and regularly based on specific criteria.'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our exploration of the Auto MPG Data Set. The next chapter will
    discuss machine learning algorithms and explore fundamental classical models routinely
    applied to tabular datasets. By proceeding with examples, we will point out each
    algorithm’s strengths and weaknesses regarding the data and features you may encounter
    in a project.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite wild differences in domains and organizations, common characteristics
    in tabular datasets make it possible to outline the best data handling and modeling
    practices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rows in a tabular dataset relate the units represented, and there are limitations
    and opportunities to be noticed when they are non-IID. Non-IID data can affect
    commonly used procedures in data science, such as bootstrapping, subsampling,
    and cross-validation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are different types of data to be found in columns: numeric (both floating
    and integer), ordinal (integer), categorical (both low and high cardinality, i.e.,
    with a low or high number of distinct labels), and dates: each one requires a
    different approach for data processing and analysis. We suggest mastering the
    pandas package (and its DataFrame data structure) to handle all such different
    kinds of features found in tabular data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Different data pathologies are connected to the type of data you have in columns:
    constant or quasi-constant features, duplicated or highly collinear features,
    irrelevant features, rare categories and other incongruencies, missing data, and
    information leakage. Also, for each one of these, there are specific remedies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding and obtaining tabular data is relatively easy if you look for open repositories
    (such as the UCI Machine Learning Repository or Kaggle Datasets) or consult the
    Google Dataset search engine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EDA plays an important role in helping you clean the data you have gathered
    internally in your organization or from the web. You can use value counts and
    descriptions, histograms, box plots, correlation matrices, and low-dimensionality
    projections such as t-SNE to reveal the structure and the problems in data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
