<html><head></head><body>
  <h1 class="tochead" id="heading_id_2">9 GPT-ing on the go</h1>

  <p class="co-summary-head">This chapter covers</p>

  <ul class="calibre5">
    <li class="co-summary-bullet">Running a large language model locally</li>

    <li class="co-summary-bullet">Comparing the results of two locally hosted large language models against those of ChatGPT</li>

    <li class="co-summary-bullet">Determining when using offline models is appropriate</li>
  </ul>

  <p class="body"><a id="marker-178"/>Imagine you are on your way to an AI conference halfway around the world. You are on a plane, cruising at 35,000 feet above the ground, and you want to prototype a new feature for your application. The airplane’s Wi-Fi is prohibitively slow and expensive. What if instead of paying all that money for a broken and borderline unusable GPT, you have one running right there on your laptop, offline? This chapter will review developers’ options to run a large language model (LLM) locally.<a id="idIndexMarker000"/></p>

  <h2 class="fm-head" id="heading_id_3">9.1 Motivating theory</h2>

  <p class="body">The introductory scenario is not too far a stretch. Although the ubiquity of high-speed internet is increasing, it has not yet achieved total coverage. You will find yourself in areas without broadband, whether at home, on the road, at school, or in the office. Hopefully, this book has successfully made the case that you should be using LLMs as a tool in your developer toolbelt. For this reason, you need to take precautions to ensure that you always have an LLM available to you in some capacity. As you use it, the more you will get from it. Like your dependency on an integrated development environment, without it, you are still a good developer; with it, however, you are much more. <a id="idIndexMarker001"/></p>

  <p class="body">But fear not. Many options are available to you. This chapter will present two, neither requiring a complex and pained installation process. You will not need to memorize the APIs from a specific vendor. These approaches are not all that different from using ChatGPT. Your prompting skills will be fully portable. Ready? Let’s get started.</p>

  <h2 class="fm-head" id="heading_id_4">9.2 Hosting your own LLM</h2>

  <p class="body"><a id="marker-179"/>When we look to run an LLM on our local machine, we immediately encounter a couple of problems: the first is that LLMs generally require significant computational resources. High-performance GPUs are typically necessary to run these models. The cost of such hardware can be prohibitive. The large size of these models means they require substantial memory to load and run. This can be a challenge even for systems with high-end GPUs, as they may not have enough VRAM to accommodate the model. The second problem we need to consider is the quality of the output of these models relative to managed LLMs like ChatGPT. <a id="idIndexMarker002"/></p>

  <p class="body">This chapter examines two instances of models explicitly selected because they do not require costly hardware. These models run on modest commodity hardware, such as the Apple MacBook Pro M2 silicon chip on which this book was written. We will start with Llama 2, an LLM developer by Meta and trained on 2 trillion tokens and offering 7 billion, 13 billion, and 70 billion parameter options. Llama 2 can present difficulties in installing and running locally; fortunately, there is a Dockerized version called Ollama, which we will use in the first section of this chapter. In the second half of the chapter, we use GPT-4All.</p>

  <p class="body">This leaves the second problem to contend with. To this end, we will use the output generated by ChatGPT as the baseline against which to measure these local models. It should not come as a surprise, but these models perform very well relative to the baseline.</p>

  <h3 class="fm-head1" id="heading_id_5">9.2.1 Baselining with ChatGPT</h3>

  <p class="body">In this chapter, we will use a novel (at least in this book) problem: calculating the standard deviation of a list of integers. The standard deviation measures the variation in a set of values. Throughout the chapter, we will use the same prompt and present it to each model: <a id="idIndexMarker003"/><a id="idIndexMarker004"/></p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">As a mathematician, you are attempting to compute the standard deviation of a list in pure Python. Please show me the code you would use and walk me through it step by step.</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">ChatGPT provides the following explanation of the steps involved and the method.</p>

  <p class="fm-code-listing-caption">Listing 9.1 ChatGPTs explanation of calculating standard deviation</p>
  <pre class="programlisting">Computing the standard deviation of a list of numbers in pure Python involves a few steps. Let's walk through them one by one with the code.
  
1.   Calculate the Mean (Average): The first step is to find the mean of the list. The mean is calculated by summing all the numbers and then dividing by the count of numbers.
  
2.   Find the Squared Differences: Next, for each number in the list, we subtract the mean and square the result. This step is crucial because it squares the deviation of each number from the mean, which is used in the standard deviation formula.
  
3.   Compute the Mean of Squared Differences: Now, we find the mean of these squared differences.
4.   Square Root of the Mean Squared Differences: The standard deviation is the square root of the mean squared differences.
  
Here's the Python code to compute the standard deviation:
  
def std_dev_chat_gpt_attempt_1(lst):
    # Step 1: Calculate the mean
    mean = sum(lst) / len(lst)
  
    # Step 2: Find squared differences
    squared_diffs = [(x - mean) ** 2 for x in lst]
  
    # Step 3: Compute the mean of squared differences
    mean_squared_diffs = sum(squared_diffs) / len(lst)
  
    # Step 4: Square root of the mean squared differences
    std_dev = mean_squared_diffs ** 0.5
 
    return std_dev</pre>

  <p class="body"><a id="marker-180"/>If we write a small <code class="fm-code-in-text">main</code> function to sum the list of integers from 1 to 4, we get the value 1.4142135623730951 or a close approximation. <a id="idIndexMarker005"/></p>

  <p class="fm-code-listing-caption">Listing 9.2 <code class="fm-code-in-text">main</code> function to drive our standard deviation calculation<a id="idIndexMarker006"/></p>
  <pre class="programlisting">if __name__ == '__main__':
    numbers = [1, 2, 3, 4, 5]
    print(std_dev_chat_gpt_attempt_1(numbers))
# <b class="fm-bold">should output 1.4142135623730951</b></pre>

  <p class="body">Feel free to use your favorite calculator, financial modeling program, or other mechanism to verify the result. You will find that this value is more or less correct. Now that we have a baseline against which to compare, we can begin our comparison.</p>

  <h3 class="fm-head1" id="heading_id_6">9.2.2 Asking Llama 2 to spit out an answer</h3>

  <p class="body">It’s time to introduce our first locally running LLM, Llama 2. As previously mentioned, this model can require considerable effort to install (at least at the time of writing). To make it easier to start evaluating this model, we will use the Dockerized version: Ollama. The following two Docker commands will run our model. <a id="idIndexMarker007"/><a id="idIndexMarker008"/><a id="marker-181"/></p>

  <p class="fm-code-listing-caption">Listing 9.3 Starting and running Ollama</p>
  <pre class="programlisting">docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
  
docker exec -it ollama ollama run llama2</pre>

  <p class="body">If this works correctly, you will (eventually) see a prompt of three greater-than symbols (<code class="fm-code-in-text">&gt;</code>). The first time you run this command, Ollama will need to download the Llama 2 model, which is several gigabytes. This will likely take a while and needs to be done with a stable internet connection. However, you will not need an internet connection once this has been completed. Therefore, ensure that you run this command before using Ollama in offline mode.</p>

  <p class="body">With Ollama installed and running, we can now ask Llama 2 to introduce itself:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">Imagine that you are the author of a book on Generative AI. You are working on a chapter about running an LLM locally on your machine. How might you describe Llama?</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="fm-code-listing-caption">Listing 9.4 Llama 2 provides some background on itself</p>
  <pre class="programlisting">As the author of a book on Generative AI, I would be delighted to introduce readers to Llama, a powerful and versatile language model that can be run locally on one's machine. Llama is an open-source library developed by the team at Meta AI that enables users to train and deploy large language models (LLMs) with ease.
  
Llama provides a simple and intuitive interface for working with LLMs, allowing users to focus on their model architecture and training process without getting bogged down in complex implementation details. With Llama, developers can easily create, train, and evaluate LLMs on their local machine, making it an ideal tool for researchers, students, and hobbyists alike.
  
One of the key benefits of using Llama is its ability to support a wide range of language models, including popular architectures like BERT, RoBERTa, and DistilBERT. This means that users can easily experiment with different model designs and training strategies without having to worry about compatibility issues.
  
Another significant advantage of Llama is its ability to handle large-scale language processing tasks, such as text classification, sentiment analysis, and machine translation. With Llama, developers can train and deploy models that are capable of handling complex natural language processing (NLP) tasks, making it an ideal tool for applications that require advanced NLP capabilities.
  
In addition to its functionality, Llama also boasts a robust and extensible architecture that allows users to easily integrate it into their own projects or workflows. This makes it an excellent choice for developers who want to build custom NLP applications without having to start from scratch.
  
Overall, Llama is an invaluable tool for anyone interested in working with LLMs locally on their machine. Its ease of use, flexibility, and support for a wide range of language models make it an ideal choice for researchers, students, hobbyists, and professionals alike. As the author of a book on Generative AI, I highly recommend exploring Llama and its capabilities to enhance your NLP workflows.</pre>

  <p class="body">With introductions out of the way, we can begin our assessment of the output from Llama 2, comparing it to the output produced by ChatGPT. We have a baseline answer that we know is correct, so we can accurately assess the correctness of the code that Llama 2 delivers. Let’s ask Llama 2 to calculate the standard deviation of a list of numbers using the same prompt as before:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">As a mathematician, you are attempting to compute the standard deviation of a list in pure Python. Please show me the code that you would use and walk me through it step by step.</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="fm-code-listing-caption">Listing 9.5 Llama 2’s first attempt to calculate standard deviation<a id="marker-182"/></p>
  <pre class="programlisting">Of course! Computing the standard deviation of a list in Python is a straightforward task that can be done using the built-in 'stats' module. <a id="idIndexMarker009"/>
  
Here's the code:
import statsmodels.api as sm
  
# Define the list of numbers to compute the standard deviation of
numbers = [1, 2, 3, 4, 5]
  
# Use the stats module to compute the standard deviation
std_dev = sm.stats.stdev(numbers)
  
# Print the result
print("Standard deviation of {0}: {1}".format(numbers, std_dev))
'''
Let's go through each line of the code:
  
1. 'import statsmodels.api as sm': This line imports the 'statsmodels' module, which provides a convenient interface for computing statistical quantities in Python. The 'api' part of the import statement tells Python <a id="idIndexMarker010"/><a id="idIndexMarker011"/>
to use this module as the primary interface for statistical computations.
  
2. 'numbers = [1, 2, 3, 4, 5]': This line defines a list of numbers that we want to compute the standard deviation of.
  
3. 'std_dev = sm.stats.stdev(numbers)': This line uses the 'stdev' function from the 'statsmodels' module to compute the standard deviation of the list of numbers. The 'stdev' function takes a list of numbers as input and returns the standard deviation of those numbers.<a id="idIndexMarker012"/><a id="idIndexMarker013"/><a id="idIndexMarker014"/>
  
4. 'print("Standard deviation of {0}: {1}".format(numbers, std_dev))': This line prints out the result of the computation using a formatted string. The first argument to the 'format' method is the name of the list (in this case, 'numbers'), and the second argument is the value of the standard deviation.<a id="idIndexMarker015"/>
  
That's it! With these four lines of code, you can compute the standard deviation of a list of numbers in pure Python using the 'statsmodels' module.<a id="idIndexMarker016"/></pre>

  <p class="body">Although this may be the way we would do it in the real world (kudos, Llama 2), it does not provide us with an apples-to-apples comparison. Let’s reframe and refine the question:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">Do not use any libraries, built in Python functions.</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">Okay, technically, it’s not a question, but hopefully, it will get the job done.</p>

  <p class="fm-code-listing-caption">Listing 9.6 Llama 2’s second attempt to calculate standard deviation</p>
  <pre class="programlisting">Of course! Here's how you can compute the standard deviation of a list of numbers using only built-in Python functions:
'''python
def stdev(numbers):
    n = len(numbers)
    sum = 0
    for i in range(n):
        sum += numbers[i]**2
    return math.sqrt(sum / (n - 1))
  
numbers = [1, 2, 3, 4, 5]
std_dev = stdev(numbers)
print("Standard deviation of {0}: {1}".format(numbers, std_dev))
'''
Let me explain how the code works:
  
1.   'def stdev(numbers):': This line defines a function called 'stdev' that takes a list of numbers as input.<a id="idIndexMarker017"/><a id="marker-183"/>
  
2.   'n = len(numbers)': This line calculates the length of the input list using the 'len()' function.<a id="idIndexMarker018"/>
  
3.   'sum = 0': This line initializes a variable 'sum' to 0.<a id="idIndexMarker019"/>
  
4.   'for i in range(n):': This line starts a 'for' loop that iterates over the elements of the input list.<a id="idIndexMarker020"/>
  
5.   'sum += numbers[i]**2': This line calculates the sum of the squares of the input numbers using the 'numbers[i]' index. The '**2' operator raises each number to the power of 2, which is equivalent to squaring it.<a id="idIndexMarker021"/>
  
6. 'return math.sqrt(sum / (n - 1))': This line calculates the standard deviation of the list using the formula: 'std_dev = sqrt(sum / (n - 1))', where 'sum' is the sum of the squares of the input numbers, and 'n' is the length of the input list. The 'math.sqrt()' function computes the square root of the result.<a id="idIndexMarker022"/>
  
7. 'print("Standard deviation of {0}: {1}".format(numbers, std_dev))': This line prints out the result of the computation using a formatted string. The first argument to the 'format' method is the name of the list (in this case, 'numbers'), and the second argument is the value of the standard deviation.<a id="idIndexMarker023"/>
  
That's it! With these seven lines of code, you can compute the standard deviation of a list of numbers using only built-in Python functions.</pre>

  <p class="body">Attentive readers (and do not worry if you missed this at first) may notice that Llama 2 subtracted one from the number of items in the list. This is curious. It does not match ChatGPT’s response. Let us see if we can figure out why. Let’s ask it:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">What is the formula for the standard deviation?</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="fm-code-listing-caption">Listing 9.7 Llama 2’s understanding of the standard deviation formula</p>
  <pre class="programlisting">The formula for the standard deviation (SD) of a set of data is:
  
SD = √(<span class="cambria">Σ</span>(xi - μ)^2 / (n - 1))
  
Where:
  
* SD is the standard deviation of the data set
* μ is the mean of the data set
* xi is each individual value in the data set
* n is the number of values in the data set
* <span class="cambria">Σ</span> is the summation symbol, indicating that the formula calculates the sum of the squared differences between each value and the mean.
  
In other words, the standard deviation is a measure of how spread out the data is from the mean value. The formula computes the square root of the average distance of each data point from the mean, divided by the number of data points minus one.</pre>

  <p class="body">This is consistent with the code that Llama 2 generated, so that’s good. However, it does not explain the apparent discrepancy in the output between this and ChatGPT. Let’s ask it directly why it subtracted the one:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">When you subtract 1 from the number of numbers in the list is that to add a degree of freedom? If so, why?</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body"><a id="marker-184"/>If you have taken statistics in the past, you may see what is coming. If not, Llama 2 will provide a useful explanation for what is meant by <i class="fm-italics">degrees of freedom</i> and why it is used. <a id="idIndexMarker024"/></p>

  <p class="fm-code-listing-caption">Listing 9.8 Llama 2’s explanation of degrees of freedom</p>
  <pre class="programlisting">Yes, when you subtract 1 from the number of numbers in the list, it is because you are adding a degree of freedom. The degree of freedom (df) is a term that appears in the formula for the standard deviation, and it represents the number of independent observations in the data set.
  
In our case, we have a list of 5 numbers, so the degree of freedom (df) is 4. This means that we have 4 independent observations (the 5 numbers in the list), and we need to account for the fact that we are computing the standard deviation of this set of independent observations.
  
The reason we subtract 1 from the number of numbers in the list is because we want to include the mean value itself as one of the observations in our data set. If we didn't subtract 1, then the mean value would be counted twice (once as a data point and once as the mean), which would give us an incorrect value for the standard deviation.
  
So, by subtracting 1 from the number of numbers in the list, we are effectively removing the mean value from the data set and treating it as a single observation, along with the other 4 numbers. This allows us to compute the standard deviation correctly using the formula above.</pre>

  <p class="body">Let’s take a quick detour to answer this question: What is the formula for the standard deviation? The answer is, which one? There are actually two: sample and population. The sample standard deviation refers to the standard deviation calculated from a subset, or <i class="fm-italics">sample</i>, of data points taken from a larger population. This formula is shown and explained in figure 9.1 This is used when working with limited information and cannot be applied to the entire dataset (population). This is contrasted to the <i class="fm-italics">population</i> standard deviation, which is how much the individual data points deviate from the average value of the population. This formula is shown in figure 9.2<a id="marker-185"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH09_F01_Crocker2.png"/></p>

    <p class="figurecaption">Figure 9.1 The formula for the sample standard deviation</p>
  </div>

  <p class="body">Both formulas involve squaring the difference between each data point and the mean, summing these squares, and then taking the square root of the sum. This provides a measure of the spread of values around the mean.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH09_F02_Crocker2.png"/></p>

    <p class="figurecaption">Figure 9.2 The formula for the population standard deviation</p>
  </div>

  <p class="body">Therefore, we can conclude that Llama 2 generated a function to calculate the sample standard deviation rather than the population standard deviation (the baseline). Given that this is in the realm of statistics specifically, we can ask,</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">How would you have changed your response if I had asked you to assume the role of a statistician rather than a mathematician?</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="fm-code-listing-caption">Listing 9.9 Llama 2’s response in the role of a statistician</p>
  <pre class="programlisting">Ah, an excellent question! As a statistician, I would use the following code to compute the standard deviation of a list of numbers using only built-in Python functions:
import math
  
# Define the list of numbers to compute the standard deviation of
numbers = [1, 2, 3, 4, 5]
  
# Compute the sum of the squares of the list of numbers
sum_squared = 0
for num in numbers:
    sum_squared += (num - math.mean(numbers)) ** 2
  
# Compute the square root of the sum of the squares to get the standard deviation
std_dev = math.sqrt(sum_squared)
  
print("Standard deviation of {0}: {1}".format(numbers, std_dev))
  
As a statistician, I would argue that the population standard deviation is the more appropriate quantity to compute in this case since we are interested in the spread of the entire population of values rather than just a random sample. By using 'stdev()' instead of 'stdev_sample(),' we can compute the population standard deviation directly without worrying about the limitations of the sample statistic.
  
Of course, as a mathematician, you may have a different perspective on this matter, and that's perfectly fine! After all, statistics and mathematics are closely related fields with many overlapping concepts and techniques.</pre>

  <p class="body">Llama 2 changes its response if we ask it to assume the statistician role. Unfortunately, it does not divide by the length of the list of numbers. Once this is fixed, we will get the correct answer for the population standard deviation. Let this be a reminder that LLMs can confidently produce incorrect answers. Always double-check the results against your knowledge or that of experts. Now, type <b class="fm-bold"><code class="fm-code-in-text">/bye</code></b> to end your session. (Typing <code class="fm-code-in-text">/bye</code> in Llama 2 signals that you wish to terminate the session.)</p>

  <p class="body"><a id="marker-186"/>Let’s focus on another LLM we can run locally: GPT-4All.<a id="idIndexMarker025"/><a id="idIndexMarker026"/></p>

  <h3 class="fm-head1" id="heading_id_7">9.2.3 Democratizing answers with GPT-4All</h3>

  <p class="body">GPT-4All is open source software developed by Anthropic that allows users to train and operate their own LLMs. It is based on GPT-3 and therefore may not operate as effectively as a GPT-4-based model; however, it can be run directly on a personal computer without the need for an internet connection. Despite the similarity in name, it is not related to GPT-4 at all. <a id="idIndexMarker027"/><a id="idIndexMarker028"/></p>

  <p class="body">Before we dive in and use it, let’s have GPT4-All introduce itself, using the following prompt:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">Imagine that you are the author of a book on Generative AI. You are working on a chapter about running an LLM locally. How might you describe GPT-4All?</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="fm-code-listing-caption">Listing 9.10 GPT-4All’s description of itself<a id="idIndexMarker029"/><a id="marker-187"/></p>
  <pre class="programlisting">GPT-4All is an open-source implementation of the Generative Pretrained Transformer (GPT) model architecture that allows users to run a Large Language Model (LLM) locally on their machines. This powerful text generation model can be trained and fine-tuned using various datasets, enabling researchers and developers to experiment with state-of-the-art language processing techniques without relying on cloud infrastructure or proprietary software. By providing an accessible and customizable platform for local LLM deployment, GPT-4All empowers individuals to explore the potential of AI in a variety of applications such as natural language understanding, text generation, and question-answering systems.</pre>

  <p class="body">Unlike Ollama, GPT-4All requires installation. Fortunately, the process is relatively quick and painless: navigate to <a class="url" href="https://gpt4all.io/">https://gpt4all.io/</a>, download the appropriate installer for your computer, and follow the installation instructions. Once you have installed the application, you will receive instructions to download a model, as shown in figure 9.3.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH09_F03_Crocker2.png"/></p>

    <p class="figurecaption">Figure 9.3 GPT-4All requires that you download models to be run.</p>
  </div>

  <p class="body">I downloaded and used Mistral OpenOrca, a high-performance parallel and distributed programming framework designed to simplify the development of large-scale, data-intensive applications on high-performance computing clusters or cloud environments. It’s particularly well suited for handling big data processing tasks, scientific simulations, machine learning algorithms, and other compute-intensive workloads that require efficient resource utilization and scalability across multiple nodes. Mistral OpenOrca provides a set of tools and libraries to manage job scheduling, communication, fault tolerance, and load balancing in distributed environments, making it an ideal choice for developers working on complex projects requiring high performance and parallelism. Both the GPT-4All introduction and the majority of this paragraph were generated by Mistral OpenOrca. <a id="idIndexMarker030"/></p>

  <p class="body"><a id="marker-188"/>If you click the Downloads button from Settings, you will see the downloaded model, as shown in figure 9.4. You will also find the complete chat history in the menu, as shown in figure 9.5.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH09_F04_Crocker2.png"/></p>

    <p class="figurecaption">Figure 9.4 The downloaded models in GPT-4All</p>
  </div>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH09_F05_Crocker2.png"/></p>

    <p class="figurecaption">Figure 9.5 The list of chats you’ve had with the selected GPT-4All model</p>
  </div>

  <p class="body">Let’s get one thing out of the way by using the following prompt:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">Is an active internet connection required to generate output?</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="fm-code-listing-caption">Listing 9.11 GPT-4All’s assurance that internet is not required</p>
  <pre class="programlisting">No, an active internet connection is not required to generate output using GPT-4All. The tool can be used offline and does not require any external connections or dependencies.</pre>

  <p class="body">Now we can move on to compare GPT4-All/Mistral OpenOrca (from here on simply referred to as GPT-4All) with the following prompt:</p>

  <table border="0" class="contenttable-0-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="calibre7">
      <tr class="contenttable-0-tr">
        <td class="contenttable-0-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-NC.png"/></p>
          </div>
        </td>

        <td class="contenttable-0-td">
          <p class="fm-table-bodyt">As a mathematician, you are attempting to compute the standard deviation of a list in pure Python. Please show me the code that you would use.<a id="marker-189"/></p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="fm-code-listing-caption">Listing 9.12 GPT-4All’s attempt to calculate standard deviation</p>
  <pre class="programlisting">To calculate the standard deviation of a list in Python, you can follow these steps:
  
1. Calculate the mean (average) of the numbers in the list.
2. Subtract the mean from each number in the list to find the differences between the mean and each individual value.
3. Square each difference obtained in step 2.
4. Find the average of these squared differences. This is the variance.
5. Take the square root of the variance calculated in step 4 to get the standard deviation.
  
Here's a Python code snippet that implements this algorithm:
  
def std_dev(numbers):
    mean = sum(numbers) / len(numbers)
    
    differences = [x - mean for x in numbers]  
# Subtract the mean from each number.
    squared_differences = [y * y for y in differences]  
# Square each difference.
    
    variance = sum(squared_differences) / len(numbers)  
# Calculate average of squared differences (variance).
    return math.sqrt(variance)  # Return the standard deviation.</pre>

  <p class="body">A couple of things to note. First, GPT-4All generates the code for the population standard deviation. Second, the text and code are generated very quickly (on my computer, four to five times more quickly than by Ollama). Third, the code is exactly right! You are encouraged to download different models, ask what each is good at, and compare the results.</p>

  <p class="body">You may wonder when you would want to use Llama 2 versus GPT-4All versus ChatGPT. Excellent question! Take a look at figure 9.6. Llama 2 is a great, general model. It excels at summarizing large bodies of text and writing contextually appropriate passages of text. GPT-4All’s use cases are as diverse as the available models. For example, Mistral OpenOrca is ideal when you need a multilingual model that can handle various languages effectively. ChatGPT is the best option if your primary goal is to have natural-language conversations with the AI model and receive the most accurate responses based on input (which really should be what you want). An obvious limitation of ChatGPT is that it requires a persistent internet connection. <a id="idIndexMarker031"/><a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="marker-190"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH09_F06_Crocker2.png"/></p>

    <p class="figurecaption">Figure 9.6 A comparison of the models that we used in this chapter</p>
  </div>

  <h2 class="fm-head" id="heading_id_8">Summary</h2>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Local LLMs require significant computational resources and costly hardware for optimal performance; however, alternatives like Llama 2 run on modest commodity hardware with varying parameter options. These models can produce output that is generally high quality, but not quite of the quality of the responses of managed LLMs like ChatGPT (at least at the time of writing).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Both population and sample standard deviations measure variability in datasets. They differ in terms of the entire population being considered versus a smaller subset or sample; this means the former provides an exact measurement for the whole group, whereas the latter is an estimate based on a portion of it.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Llama 2 excels at diverse text handling, such as generating summaries or writing coherent passages of text and code, GPT-4All offers various use cases, including multilingual support; and ChatGPT shines in natural language conversations with accurate responses (but it requires an internet connection).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">In addition to offline availability, there are various situations in which using an offline version of an LLM such as Llama 2 or GPT-4All makes sense:</p>

      <ul class="calibre6">
        <li class="fm-list-bullet">
          <p class="list"><i class="fm-italics">Privacy and security concerns—</i>Offline models eliminate the need to transmit sensitive data over the internet, reducing privacy risks and potential cybersecurity threats.</p>
        </li>

        <li class="fm-list-bullet">
          <p class="list"><i class="fm-italics">Cost savings—</i>Running a local model on your own hardware may reduce cloud computing costs associated with using an online service like ChatGPT or OpenAI API.<a id="marker-191"/></p>
        </li>
      </ul>
    </li>
  </ul>
</body></html>