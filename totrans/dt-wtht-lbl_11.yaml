- en: 9 Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Introducing autoencoders
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training of autoencoders
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of autoencoders
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python code using TensorFlow and Keras
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out of intense complexities, intense simplicities emerge.—Winston Churchill
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In the preceding chapter, we explored the concepts of deep learning. In this
    chapter, we start with unsupervised deep learning. Autoencoders are the very first
    topic. We will first cover the basics of autoencoders, what are they, and how
    we train them. We then get into the different types of autoencoders followed by
    a Python code on the implementation. Welcome to the ninth chapter, and all the
    very best!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Technical toolkit
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will continue to use the same version of Python and Jupyter Notebook as
    we have used so far. The codes and datasets used in this chapter have been checked
    in at the GitHub location. You need to install a couple of Python libraries in
    this chapter: `tensorflow` and `keras`.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Feature learning
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Predictive modeling is quite an interesting topic. Across various domains and
    business functions, predictive modeling is used for various purposes like predicting
    the sales for a business in the next year, the amount of rainfall expected, whether
    the incoming credit card transaction is fraud or not, whether the customer will
    make a purchase or not, and so on. The use cases are many, and all the aforementioned
    use cases fall under supervised learning algorithms.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  The datasets that we use have variables or attributes. They are also called
    characteristics or features.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: While we wish to create these predictive models, we are also interested in understanding
    the variables that are useful for making the prediction. Let’s consider a case
    where a bank wants to predict if an incoming transaction is fraudulent or not.
    In such a scenario, the bank will wish to know which factors are significant to
    identify an incoming transaction as fraud. Factors that might be considered include
    the amount of the transaction, the time of the transaction, the origin/source
    of the transaction, etc. The variables that are important for making a prediction
    are called *significant variables*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: To create a machine learning–based predictive model, *feature engineering* is
    used. Feature engineering, otherwise known as feature extraction, is the process
    of extracting features from the raw data to improve the overall quality of the
    model and enhance the accuracy as compared to a model where only raw data is fed
    to the machine learning model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering can be done using domain understanding, various manual methods,
    and a few automated methods too. One such method is known as feature learning.
    Feature learning is the set of techniques that help a solution automatically discover
    the representations required for feature detection. With the help of feature learning,
    manual feature engineering is not required. The effect of feature learning is
    much more relevant for datasets where images, text, audio, and video are being
    used.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Feature learning can be both supervised and unsupervised. For supervised feature
    learning, neural networks are the best example. For unsupervised feature learning,
    we have examples like matrix factorization, clustering algorithms, and autoencoders.
    We have already covered clustering and matrix factorization. In this chapter,
    we start with an introduction to autoencoders.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Introducing autoencoders
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we start with any data science problem, data plays the most significant
    role. A dataset that has a lot of noise is one of the biggest challenges in data
    science and machine learning. There are quite a few solutions available now, and
    autoencoders are one of them.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, an autoencoder is a type of artificial neural network, and it is
    used to learn the data encodings. Autoencoders are typically used for dimensionality
    reduction methods. They can also be used as generative models, which can create
    synthetic data that is like the old data. For example, if we do not have a good
    amount of data to train machine learning, we can use generated synthetic data
    to train the models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders are feed-forward neural networks, and they compress the input into
    a lower dimensional code and then try to reconstruct the output from this representation.
    The objective of an autoencoder is to learn the lower dimensional representation
    (also sometimes known as encoding) for a high-dimensional dataset. Recall from
    the previous chapters principal component analysis (PCA). Autoencoders can be
    thought of as a generalization for PCA. PCA is a linear method whereas autoencoders
    can learn nonlinear relationships as well. Hence, autoencoders are required for
    dimensionality reduction solutions wherein they capture the most significant attributes
    from the input data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Components of autoencoders
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The architecture of an autoencoder is quite simple to understand. An autoencoder
    consists of three parts: an encoder, a bottleneck or a code, and a decoder, as
    shown in figure 9.1\. In simple terms, an encoder compresses the input data, a
    bottleneck or code contains this compressed information, and the decoder decompresses
    the knowledge and hence reconstructs this data back to its original form. Once
    the decompression has been done and the data has been reconstructed to its encoded
    form, the input and output can be compared.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F01_Verdhan.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 Structure of an autoencoder with an encoder, a bottleneck, and a
    decoder
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s study these components in more detail:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '*Encoder*—The input data passes through the encoder. An encoder is nothing
    but a fully connected artificial neural network. It compresses the input data
    into an encoded representation, and in the process the output generated is reduced
    in size. An encoder compresses the input data into a compressed module known as
    a bottleneck.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Bottleneck*—The bottleneck can be considered the brain of the encoder. It
    contains the compressed information representations, and it is the job of the
    bottleneck to allow only the most important information to pass through.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Decoder*—The information received from the bottleneck is decompressed by a
    decoder. It re-creates the data back to its original or encoded form. Once the
    job of the decoder is done, the actual values are compared with the decompressed
    values created by the decoder.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are a few important points about autoencoders to consider:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: There is a loss of information in autoencoders when the decompression is done
    as compared to the original inputs. So when the compressed data is decompressed,
    there is a loss as compared to the original data.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders are specific to datasets. This means that an algorithm that is
    trained on images of flowers will not work on images of traffic signals and vice
    versa. This is because the features the autoencoder learned will be specific to
    flowers only. So we can say that autoencoders are only able to compress the data
    similar to the one used for training.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is relatively easier to train specialized instances of algorithms to perform
    well on specific types of inputs. We just need representative training datasets
    to train the autoencoder.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9.5 Training of autoencoders
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is important to note that if there is no correlation between the variables
    in the data, then it is really difficult to compress and subsequently decompress
    the input data. For us to create a meaningful solution, there should be some level
    of relationship or correlation between the variables in the input data. To create
    an autoencoder, we require an encoding method, a decoding method, and a loss function
    to compare the actual versus decompressed values.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'The process is as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The input data passes through the encoder module.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The encoder compresses the input of a model into a compact bottleneck.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The bottleneck restricts the flow of information and allows only important information
    to pass through; hence, a bottleneck is sometimes referred to as *knowledge-representation*.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The decoder decompresses the information and re-creates the data back to its
    original or encoded form. This encoder-decoder architecture is quite efficient
    in getting the most significant attributes from the input data.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The objective of the solution is to generate an output identical to the input.
    Generally, the decoder architecture is a mirror image of the coder architecture.
    This is not mandatory but is generally followed. We ensure that the dimensionality
    of the input and outputs are the same.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  If you do not know the meaning of hyperparameter, refer to the appendix.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to define four hyperparameters for training an autoencoder:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '*Code size*—This is perhaps the most significant hyperparameter. It represents
    the number of nodes in the middle layer. This decides the compression of the data
    and can also act as a regularization term. The less the value of code size, the
    more compressed the data.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*代码大小*—这可能是最重要的超参数。它表示中间层的节点数。这决定了数据的压缩程度，也可以作为正则化项。代码大小的值越小，数据就越压缩。'
- en: '*Parameter*—This denotes the depth of the autoencoder. A model that has more
    depth is obviously more complex and will have a longer processing time.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*参数*—这表示自动编码器的深度。具有更多深度的模型显然更复杂，并且处理时间会更长。'
- en: '*Number of nodes per layer*—This is the weight used per layer. It generally
    decreases with every subsequent layer as the input becomes smaller across the
    layers. It increases back in the decoder.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*每层的节点数*—这是每层使用的权重。随着输入在层间变得越来越小，它通常在后续的每一层中都会减少。在解码器中又重新增加。'
- en: '*Loss function used*—If the input values are in the [0,1] range, binary cross-entropy
    is preferred; otherwise, mean squared error is used.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用的损失函数*—如果输入值在[0,1]范围内，则首选二元交叉熵；否则，使用均方误差。'
- en: We have covered the hyperparameters used in training autoencoders. The training
    process is similar to backpropagation, which we have already covered.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了在训练自动编码器时使用的超参数。训练过程与反向传播类似，我们已经讨论过了。
- en: 9.6 Application of autoencoders
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6 自动编码器的应用
- en: Autoencoders are capable of solving a number of problems inherent to unsupervised
    learning. Major applications for autoencoders include
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器能够解决无监督学习中的一些固有问题的解决方案。自动编码器的主要应用包括
- en: '*Dimensionality reduction*—Sometimes autoencoders can learn more complex data
    projections than PCA and other techniques.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*降维*—有时自动编码器可以学习比PCA和其他技术更复杂的数据投影。'
- en: '*Anomaly detection*—The error or the reconstruction error (error between the
    actual data and the reconstructed data) can be used to detect the anomalies.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异常检测*—错误或重建误差（实际数据与重建数据之间的误差）可以用来检测异常。'
- en: '*Data compression*—It is difficult to beat the basic solutions like JPEG by
    training the algorithm. Moreover, since autoencoders are data specific, they can
    use only the types of datasets they have been trained upon. If we wish to enhance
    the capacity to include more data types and make it more general, then the amount
    of the training data required will be too high, and obviously, the time required
    will be high too.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据压缩*—通过训练算法来击败像JPEG这样的基本解决方案是困难的。此外，由于自动编码器是数据特定的，它们只能使用它们被训练过的数据集类型。如果我们希望提高包括更多数据类型的容量并使其更通用，那么所需的训练数据量将非常高，显然，所需的时间也会很高。'
- en: '*Other applications*—These include drug discovery, machine translation, image
    denoising, etc.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*其他应用*—这些包括药物发现、机器翻译、图像去噪等。'
- en: There are still not a lot of practical implementations of autoencoders in the
    real world. This is due to a multitude of reasons like the nonavailability of
    datasets, infrastructure, readiness of various systems, etc.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，自动编码器的实际应用仍然不多。这有多种原因，如数据集不可用、基础设施、各种系统的准备就绪等。
- en: 9.7 Types of autoencoders
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.7 自动编码器的类型
- en: 'There are five main types of autoencoders. A brief description of the different
    types of encoders is given next. We have kept the section mathematically light
    and skipped the math behind the scenes as it is quite complex to understand. For
    curious readers, the papers listed in section 9.10 can explain the mathematics:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器主要有五种类型。接下来将简要介绍不同类型的编码器。我们尽量使这一部分数学简单，并跳过了背后的数学，因为它相当复杂，难以理解。对于好奇的读者，第9.10节中列出的论文可以解释数学原理：
- en: '*Undercomplete autoencoders*—An undercomplete autoencoder is the simplest form
    of an autoencoder. It simply takes an input dataset and then reconstructs the
    same dataset again from the compressed bottleneck region. By penalizing the neural
    network as per the reconstruction error, the model will learn the most significant
    attributes of the data. By learning the most important attributes, the model will
    be able to reconstruct the original data from the compressed state. As we know,
    there is a loss when the compressed data is reconstructed; this loss is called
    *reconstruction* loss.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Undercomplete autoencoders are unsupervised in nature as they do not have any
    target label to train. Such types of autoencoders are used for dimensionality
    reduction. Recall in chapter 2 we discussed dimensionality reduction (PCA), and
    in chapter 6, we discussed the advanced dimensionality reduction algorithms (t-distributed
    stochastic neighbor embedding and multidimensional scaling). See figure 9.2.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F02_Verdhan.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 The performance starts to improve with more dimensions but decreases
    after some time. The curse of dimensionality is a real problem when it comes to
    creating sound data science solutions.
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Dimensionality reduction is possible using undercomplete autoencoders as the
    bottleneck is created, which is the compressed form of the input data. This compressed
    data can be decompressed back with the aid of the network. Recall in chapter 3
    we explained that PCA provides a linear combination of the input variables. For
    more details and to refresh your memory on PCA, please refer to chapter 3\. We
    know that PCA tries to get a low-dimensional hyperplane to describe the original
    dataset; undercomplete autoencoders can also learn nonlinear relationships. The
    difference is shown in figure 9.3.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F03_Verdhan.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 PCA is linear in nature while autoencoders are nonlinear. This is
    the core difference between the two algorithms.
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Interestingly, if all the nonlinear activation functions are removed from the
    undercomplete autoencoder and only linear layers are used, the autoencoder is
    equivalent to a PCA only. To make the autoencoder generalize and not memorize
    the training data, an undercomplete autoencoder is regulated and fine-tuned by
    the size of the bottleneck. It allows the solution to not memorize the training
    data and generalize very well.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  If a machine learning model works very well on the training data but does
    not work on the unseen test data, it is called overfitting.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '*Sparse autoencoders*—Sparse autoencoders are similar to undercomplete autoencoders
    except they use a different methodology to tackle overfitting. Conceptually, a
    sparse autoencoder changes the number of nodes at each of the hidden layer and
    keeps it flexible. Since it is not possible to have a neural network capable of
    a flexible number of neurons, the loss function is customized for it. In the loss
    function, a term is introduced that captures the number of activated neurons.
    The penalty term is proportional to the number of activated neurons. The higher
    the number of activated neurons, the higher the penalty. This penalty is called
    the *sparsity function*. Using the penalty, it is possible to reduce the number
    of activated neurons; hence the penalty is lower, and the network is able to tackle
    the problem of overfitting.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Contractive autoencoders*—Contractive autoencoders work on a similar concept
    as other autoencoders. They consider that the inputs that are quite similar should
    be encoded the same. Hence, they should have the same latent space representation.
    It means that there should not be much difference between the input data and the
    latent space.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Denoizing autoencoders**—*Denoizing means removing the noise, and that is
    the precise task of denoizing autoencoders. They do not take an image as an input;
    instead they take a noisy version of an image as an input as shown in figure 9.4.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F04_Verdhan.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 An original image, noisy output, and the outputs from the autoencoder
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The process of denoizing the autoencoder is depicted in figure 9.5\. The original
    image is changed by adding noise to it. This noisy image is fed to the encoder-decoder
    architecture and the output received is compared to the original image. The autoencoder
    learns the representation of the image, which is used to remove the noise; this
    is achieved by mapping the input image into a lower dimensional manifold.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH09_F05_Verdhan.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 The process of denoizing in an autoencoder. It starts with the original
    image; noise is added, which results in a noisy image, and then it is fed to the
    autoencoder.
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can use denoizing autoencoders for nonlinear dimensionality reduction.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '*Variational autoencoders*—A standard autoencoder model represents the input
    in a compressed form using the bottleneck. A variation is probabilistic generative
    models (usually Gaussian) over latent variables, which only need neural networks
    as a part of their overall structure. They are trained using expectation-maximization
    meta-algorithms. The mathematical details are beyond the scope of this book.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9.8 Python implementation of autoencoders
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create two versions of an autoencoder. The code has been taken from the
    official source at the Keras website ([https://blog.keras.io/building-autoencoders-in-keras.html](https://blog.keras.io/building-autoencoders-in-keras.html))
    and has been modified for our usage. The steps are as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary libraries:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '2\. Create our network architecture:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '3\. Add more details to the model:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '4\. Load the datasets:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '5\. Create the train and test the datasets:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '6\. Fit the model (see figure 9.6):'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![figure](../Images/CH09_F06_Verdhan.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 Fitting the model
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '7\. Test it on the test dataset:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '8\. Plot the results. You can see the original image and final output (see
    figure 9.7):'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![figure](../Images/CH09_F07_Verdhan.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 The original image (bottom) and the final outcome (top)
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 9.9 Concluding thoughts
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is a powerful tool. With a sound business problem and a quality
    dataset, we can create a lot of innovative solutions. Autoencoders are only one
    type of such solutions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we started with feature engineering, which allows us to extract
    the most significant features from a dataset. Then we moved to autoencoders. Autoencoders
    are a type of neural network only used to learn efficient coding of unlabeled
    datasets. Autoencoders can be applied to many business problems like facial recognition,
    anomaly detection, image recognition, drug discovery, machine translation, and
    so on.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 9.10 Practical next steps and suggested readings
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following provides suggestions for what to do next and offers some helpful
    reading:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Read the blog at [https://mng.bz/qxaw](https://mng.bz/qxaw).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Study the following papers:'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton, G. E., Krizhevsky, A., and Wang, S. D. (2011). Transforming Auto-encoders.
    [https://mng.bz/7p99](https://mng.bz/7p99)
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Bank, D., Koenigstein, N., and Giryes, R. (2020). Autoencoders. [https://arxiv.org/abs/2003.05991](https://arxiv.org/abs/2003.05991)
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Michelucci, U. (2020). An Introduction to Autoencoders. [https://arxiv.org/abs/2201.03898](https://arxiv.org/abs/2201.03898)
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: See the good code and dataset available on the TensorFlow official page. [https://mng.bz/mGQr](https://mng.bz/mGQr).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Predictive modeling is used in various domains to make future predictions using
    supervised learning algorithms.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key aspects of predictive modeling involve identifying significant variables
    or features for accurate predictions.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering enhances model accuracy by extracting useful features from
    raw data.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature learning automates feature detection, suitable for datasets like images,
    text, and audio.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders are a type of neural network used for data encoding, dimensionality
    reduction, and generating synthetic data.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The architecture of autoencoders includes encoder, bottleneck, and decoder components
    for data compression and reconstruction.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders face information loss, are dataset-specific, and are suitable for
    precise applications.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training autoencoders requires encoding, decoding, and defining hyperparameters
    such as code size and loss function.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Major applications include dimensionality reduction, anomaly detection, and
    data compression, among others.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of autoencoders include undercomplete, sparse, contractive, denoizing,
    and variational.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparse and contractive autoencoders address overfitting using different methodologies.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏和收缩自动编码器通过不同的方法来解决过拟合问题。
- en: A Python implementation of basic autoencoder architecture involves the Keras
    library for encoding and decoding data.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python实现基本自动编码器架构涉及使用Keras库进行编码和解码数据。
