- en: Chapter 4\. AI System Assessment and Tailoring AI Engineering for Different
    Risk Levels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the main learning objective is to understand how to practically
    classify different AI system risk levels and how to design AI engineering processes
    for each risk level (see [Figure 4-1](#chapter_4_figure_1_1748539919001611) for
    a visual of the steps to take to move toward compliance with the EU AI Act). We’ll
    explore the EU AI Act’s risk classification framework and the obligation mapping
    phase. For high-risk and limited-risk AI systems, careful planning of data governance,
    AI governance, and MLOps processes is essential to ensure compliance with the
    Act.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0401.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-1\. This chapter focuses on creating the AI system landscape in an
    organization and classifying the risk types. See [Chapter 1](ch01.html#chapter_1_understanding_the_ai_regulations_1748539916832819)
    for an explanation of the end-to-end process steps toward EU AI Act compliance.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This chapter will help you answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How many AI systems are currently in place or intended to be put into production?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What risk categories do those AI systems belong to?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can clarity be established about the role of the provider or deployer of
    each AI model?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In discussions about the EU AI Act, people frequently use terms such as “AI
    compliance,” “AI governance,” and “risk management.” While closely related and
    complementary, these are distinct concepts. Let’s begin by clarifying what each
    one means.
  prefs: []
  type: TYPE_NORMAL
- en: AI Compliance, Governance, and Risk Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We explored AI compliance and governance in the previous chapters. As a reminder,
    *AI compliance* focuses on adhering to legal, regulatory, and policy standards,
    ensuring ethical behavior, and promoting transparency through rigorous processes,
    metrics, and stakeholder engagement. *AI governance* establishes a framework for
    ethical and responsible AI development, emphasizing transparency, stakeholder
    engagement, and balancing innovation with ethical considerations. *AI risk management*
    involves identifying and addressing risks associated with AI systems, focusing
    on robust assessment practices, effective mitigation strategies, and promoting
    a culture of awareness and proactive action.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 4-1](#table-4-1) compares and contrasts AI compliance, governance, and
    risk management frameworks along a number of dimensions, including objectives,
    people and processes involved, engineering considerations, cultural practices,
    and more.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-1\. Comparison of AI compliance, governance, and risk management in
    the context of the EU AI Act
  prefs: []
  type: TYPE_NORMAL
- en: '|   | AI compliance | AI governance | AI risk management |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Objective | Ensure AI systems adhere to the requirements and obligations
    set forth in the EU AI Act, including risk management, data governance, human
    oversight, transparency, accuracy, robustness, and cybersecurity. | Establish
    a framework for overseeing AI development, deployment, and use in alignment with
    the EU AI Act, organizational values, and ethical principles. | Identify, assess,
    and mitigate risks associated with AI systems throughout their lifecycle, in compliance
    with the EU AI Act’s risk-based approach. |'
  prefs: []
  type: TYPE_TB
- en: '| People | Compliance officers, legal teams, data protection officers, AI ethics
    committees | AI ethics board, chief AI officer, AI ethics officer, cross-functional
    AI steering committee, AI project managers | Risk management teams, data scientists,
    AI engineers, cybersecurity experts |'
  prefs: []
  type: TYPE_TB
- en: '| Processes |'
  prefs: []
  type: TYPE_TB
- en: Implement risk assessment procedures to categorize AI systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish documentation practices for high-risk AI systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop incident reporting and management protocols.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create processes for conformity assessments and CE marking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Develop AI policies and guidelines aligned with the EU AI Act.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement review and approval processes for high-risk AI systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish mechanisms for ongoing monitoring and auditing of AI systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create escalation procedures for AI-related issues and decisions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Perform regular AI risk assessments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct scenario planning for AI failures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure continuous monitoring of AI system performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create incident response and recovery plans.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define processes for regular risk reporting and review.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Metrics and KPIs |'
  prefs: []
  type: TYPE_TB
- en: Number of compliance violations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time to address compliance issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Percentage of AI projects passing compliance checks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Percentage of AI systems with proper documentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequency of compliance audits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Number of AI projects reviewed and approved by the governance committee
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Percentage of AI systems adhering to established governance policies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequency and outcomes of AI system audits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of ethical reviews conducted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Number of risks identified, assessed, and mitigated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduction in AI-related incidents over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time to resolve identified risks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Percentage of high-risk AI systems with completed risk assessments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Stakeholder engagement |'
  prefs: []
  type: TYPE_TB
- en: 'Internal: Collaboration between legal, technical, and operational teams'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'External: Interaction with regulators, compliance bodies, and industry standards
    organizations'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Internal: Cross-department collaboration, inclusive decision-making processes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'External: Engagement with external advisors, industry forums, and regulatory
    bodies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Risk communication to executive leadership
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collaboration with customers to align on risk tolerances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engagement with insurers on AI-related coverage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collaboration with industry partners on risk management best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Engineering practices |'
  prefs: []
  type: TYPE_TB
- en: Implement privacy-by-design and security-by-design principles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop testing procedures for bias detection and mitigation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish version control for data, code, and AI models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conduct regular code reviews, ensuring adherence to ethical guidelines and traceability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Implement standardized AI development methodologies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish governance checkpoints at key stages of the AI lifecycle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate code review practices that specifically include governance considerations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Incorporate governance requirements into DevOps practices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Integrate risk considerations into AI architecture design.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement robust testing procedures for AI systems, including adversarial testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop failsafe mechanisms and ensure graceful degradation for high-risk AI
    systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish monitoring systems for early risk detection in deployed AI systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Technology and infrastructure |'
  prefs: []
  type: TYPE_TB
- en: Compliance management software
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated auditing tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explainable AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data protection technologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secure data storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robust logging systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compliance tracking systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: AI model registries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centralized AI governance platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data lineage tracking system
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI performance monitoring tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethical AI framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: AI risk monitoring and alerting systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulation environments for risk testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automated risk assessment tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secure sandboxing for AI testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secure data environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resilient system architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Training and education |'
  prefs: []
  type: TYPE_TB
- en: Regular compliance training for AI developers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workshops on emerging AI regulations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Certification programs for AI compliance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Governance training for all employees involved in AI projects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specialized courses on AI ethics and responsible AI development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Workshops on interpreting and applying EU AI Act requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Risk management training specific to AI technologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Educating developers on identifying and mitigating AI-specific risks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Change management |'
  prefs: []
  type: TYPE_TB
- en: Develop a roadmap for transitioning existing AI systems to comply with the Act.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish clear communication channels about compliance updates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create feedback mechanisms to continuously improve compliance processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Develop a communication strategy for rolling out new governance policies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create transition plans for adapting existing AI systems to new governance requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish feedback loops to continuously refine governance practices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Develop strategies for integrating risk management into existing AI workflows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make regular updates to risk management policies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communicate with stakeholders about risk status.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Cultural considerations |'
  prefs: []
  type: TYPE_TB
- en: Foster a culture of ethical AI development and responsible innovation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Promote transparency and accountability in AI decision-making processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facilitate cross-functional collaboration to address compliance challenges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Foster a culture of responsible AI innovation and ethical decision making.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encourage open discussions about AI risks and governance challenges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognize and reward adherence to AI governance principles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Foster a culture of proactive risk identification and mitigation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enable open reporting of potential risks without fear (create a “safe space”).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Promote a balanced approach to innovation and risk management.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: AI compliance, governance, and risk management are closely interrelated. Compliance
    is primarily reactive in nature, establishing the baseline requirements for governance,
    while effective governance is proactive, guiding the responsible development and
    use of AI. Risk management complements and informs compliance and governance efforts
    by identifying and addressing potential threats in a timely manner. When implemented
    holistically and embedded into daily operations, these three pillars provide essential
    support for compliance with the EU AI Act.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an AI System Inventory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To navigate the EU AI Act and understand its impact on your organization, you
    should first get an overview of your existing AI systems and potential AI use
    cases to assess whether they are subject to the legislation.
  prefs: []
  type: TYPE_NORMAL
- en: Note that AI use cases developed as internal research projects are generally
    excluded from compliance obligations.
  prefs: []
  type: TYPE_NORMAL
- en: Later in this chapter, I will outline the different maturity levels of AI projects
    and specify when the EU AI Act’s requirements must be fulfilled. For now, please
    note that the Act impacts AI systems based on their risk level and intended use,
    rather than their development stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'As described in [Chapter 1](ch01.html#chapter_1_understanding_the_ai_regulations_1748539916832819),
    the EU AI Act applies to:'
  prefs: []
  type: TYPE_NORMAL
- en: Providers placing AI systems on the EU market or putting them into service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users of AI systems located within the EU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providers and users outside the EU, if the AI system’s output is used within
    the EU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To help you in your inventory efforts, I recommend creating an *AI system inventory*
    by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify AI applications across all departments.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Include both custom-developed and third-party AI solutions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Document key details like department, purpose, data used, deployment status,
    and risk category.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As a reminder, Article 3 of the EU AI Act defines an AI system as “a machine-based
    system that is designed to operate with varying levels of autonomy and that may
    exhibit adaptiveness after deployment, and that, for explicit or implicit objectives,
    infers, from the input it receives, how to generate outputs such as predictions,
    content, recommendations, or decisions that can influence physical or virtual
    environments.”
  prefs: []
  type: TYPE_NORMAL
- en: You’ll want to create a comprehensive inventory template to catalog all AI systems
    used or developed within your organization, capturing key attributes for each
    system. [Figure 4-2](#chapter_4_figure_2_1748539919001650) shows an example of
    what this might look like.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0402.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-2\. An AI system catalog entry should contain basic identifying information,
    technical details, a risk assessment, and additional relevant notes
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Depending on your organization’s needs, you can create this template using tools
    like Microsoft Excel, Google Sheets (as shown in [Figure 4-3](#chapter_4_figure_3_1748539919001677)),
    Airtable, or more sophisticated database management systems. Knowledge management
    platforms such as Confluence or Notion can also be helpful for creating a catalog
    of AI systems, as can specialized AI governance tools like watsonx.ai, Dataiku,
    or Domino Data Lab.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0403.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-3\. An example of a catalog of existing AI systems for a hypothetical
    ecommerce company
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: After creating a complete and detailed inventory of the organization’s AI systems,
    the next question you’ll want to answer is whether the EU AI Act is applicable
    to those systems.
  prefs: []
  type: TYPE_NORMAL
- en: Applicability of the EU AI Act
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The EU AI Act’s applicability to a given AI system depends on several factors,
    including whether the system falls within the Act’s the scope and definition of
    AI, whether it has already been placed on the market or put into service, and
    the timing of the Act’s entry into force and application.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The relevant articles of the EU AI Act are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Article 2: Scope](https://oreil.ly/eBlrf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Article 3: Definitions](https://oreil.ly/ogId3) and [Recital 12](https://oreil.ly/5h0Su)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Article 111: AI Systems Already Placed on the Market or put into Service and
    General-Purpose AI Models Already Placed on the Market](https://oreil.ly/d8_kg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Article 113: Entry into Force and Application](https://oreil.ly/s_NYt)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Complying with the EU AI Act involves a complex process flow with many conditions,
    as pictured in [Figure 4-4](#chapter_4_figure_4_1748539919001709). After clarifying
    whether the Act is applicable to a particular system, you’ll need to establish
    your organization’s role (such as provider or deployer) and classify the system’s
    risk level. These two factors determine the specific set of obligations your organization
    must fulfill.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0404.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-4\. The process of determining the EU AI Act’s applicability to AI
    systems and their risk classification (adapted from [*https://oreil.ly/LUZWC*](https://oreil.ly/LUZWC)
    [CC BY 4.0], courtesy of the appliedAI Institute for Europe gGmbH)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We’ll look at the processes for determining applicability, risk level, and organizational
    role in more detail later in this chapter. First, let’s examine the use cases
    that are *prohibited* by the EU AI Act.
  prefs: []
  type: TYPE_NORMAL
- en: Unacceptable Risk—Prohibited AI Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Human values such as privacy, integrity, social justice, transparency, and diversity
    are at the core of EU law, and with the growing adoption of AI technologies, they
    are becoming increasingly important in software and AI development. To ensure
    the safety of AI, it’s more effective to regulate its applications rather than
    the technology itself.
  prefs: []
  type: TYPE_NORMAL
- en: Technology itself is neither inherently good nor bad, but it can be used in
    benevolent or harmful ways. To underscore this idea, [Table 4-2](#chapter_4_table_2_1748539919012629)
    gives some examples of useful and harmful applications of the same technologies.
    This is important because AI technology is general-purpose, and developers—such
    as those who release open-weight and open source foundation models—can’t fully
    control how others might use it. To give a specific example, fake reviews were
    a problem on many websites even before the widespread adoption of generative AI,
    requiring companies to devote significant resources to detecting and removing
    them. Traditional fake reviews often used similar language, making them easier
    to spot. However, the ability of GenAI tools to automatically rephrase or rewrite
    text has made detecting fake reviews increasingly challenging. In a case like
    this, the solution is not to limit the use of the technology itself; instead,
    when AI is used in a harmful way, it is that specific application that should
    be addressed and, if necessary, restricted or halted.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-2\. Useful and harmful applications of general-purpose technologies
  prefs: []
  type: TYPE_NORMAL
- en: '| General-purpose technology | Applications |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|   | Useful | Harmful |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Nuclear technology |'
  prefs: []
  type: TYPE_TB
- en: Electricity generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large-scale desalination plants to produce fresh water from seawater
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Medical imaging (X-rays, CT scans)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cancer treatments like radiation therapy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Nuclear weapons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nuclear waste (as a byproduct)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nuclear facilities (potential for reactor meltdowns and radioactive contamination)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Artificial intelligence |'
  prefs: []
  type: TYPE_TB
- en: Automating repetitive tasks and streamlining processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing vast amounts of data to provide insights for more informed business
    decisions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling personalized recommendations and experiences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous weapons systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spreading misinformation or manipulating public opinion
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Political deepfakes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Prohibited AI Use Cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The EU AI Act aims to protect fundamental rights, democracy, and the rule of
    law by banning AI applications that are incompatible with core EU values and rights.
    In general, the Act prohibits AI systems from:'
  prefs: []
  type: TYPE_NORMAL
- en: Using subliminal, manipulative, or deceptive techniques to misinterpret behavior
    and prevent informed decision making, causing significant harm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploiting vulnerabilities related to age, disability, or socioeconomic circumstances
    to distort behavior, causing significant harm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing biometric categorization systems to infer sensitive attributes such
    as race, political opinions, religious beliefs, and sexual orientation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conducting social scoring (i.e., evaluating individuals based on social behavior
    or personal traits), which can lead to discriminative treatment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating or expanding facial recognition databases through untargeted scraping
    of facial images from the internet or CCTV footage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inferring emotions in workplaces or educational institutions, except for medical
    or safety reasons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All prohibited AI systems were required to have been removed from the EU market
    by February 2025, six months after the Act entered into force (please refer to
    [Chapter 1](ch01.html#chapter_1_understanding_the_ai_regulations_1748539916832819)
    for details on the implementation timeline). It is important to correctly identify
    and classify these systems to ensure they are removed from operation, in accordance
    with the regulation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a closer look at the some of the specific types of AI systems that
    are prohibited:'
  prefs: []
  type: TYPE_NORMAL
- en: Systems that manipulate people through subliminal techniques
  prefs: []
  type: TYPE_NORMAL
- en: These are systems that use hidden tricks, like sounds or images, to influence
    people without them realizing it—for example, an AI system that plays barely audible
    music in a store to influence people to buy more things.
  prefs: []
  type: TYPE_NORMAL
- en: Systems that use facial recognition to categorize people based on sensitive
    characteristics
  prefs: []
  type: TYPE_NORMAL
- en: This means systems that use facial recognition technology to sort people into
    groups based on their race, religion, sexual orientation, or other personal characteristics.
    Such systems are prohibited to protect people from being discriminated against
    based on these characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: Social scoring systems
  prefs: []
  type: TYPE_NORMAL
- en: 'These are systems that create a score for a person based on their online behavior
    or other personal information. This score could then be used to deny people jobs,
    housing, or other opportunities. (See [“Deep Dive: Social Scoring”](#chapter_4_deep_dive_social_scoring_1748539919035416)
    for an example of social scoring.)'
  prefs: []
  type: TYPE_NORMAL
- en: Law enforcement facial recognition in public places (except for specific cases)
  prefs: []
  type: TYPE_NORMAL
- en: This means that police and other law enforcement agencies cannot normally use
    facial recognition technology to scan people in public places. There are some
    exceptions, though, such as when they are looking for missing people, trying to
    prevent terrorism, or trying to catch criminals.
  prefs: []
  type: TYPE_NORMAL
- en: AI systems that predict criminality
  prefs: []
  type: TYPE_NORMAL
- en: These are systems that use artificial intelligence to try to guess whether someone
    is likely to commit a crime in the future. This is not allowed because it could
    lead to people being punished for something they haven’t done.
  prefs: []
  type: TYPE_NORMAL
- en: Systems that build facial recognition databases without people’s permission
  prefs: []
  type: TYPE_NORMAL
- en: This means that companies and organizations cannot collect large databases of
    people’s faces without their permission. This is to protect people’s privacy.
  prefs: []
  type: TYPE_NORMAL
- en: Emotion recognition systems at work or school (except for special cases)
  prefs: []
  type: TYPE_NORMAL
- en: These are systems that use artificial intelligence to try to guess what emotions
    people are feeling. This is generally not allowed in workplaces or schools, but
    there are some exceptions. For example, an AI system might be allowed to do this
    if it is being used to help people with autism learn to understand emotions.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Chapter II, Article 5 of the EU AI Act](https://oreil.ly/I-18Z) contains 19
    recitals—numbers 3 and 28–45—that you can consult to more deeply understand the
    prohibited AI use cases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Dive: Social Scoring'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some forms of social scoring are already present in our daily lives. For example,
    financial institutions may use credit scoring systems that consider past financial
    behavior and social media data. Similarly, some insurance companies adjust rates
    based on data collected from fitness trackers. According to the EU AI Act, [Recital
    31](https://oreil.ly/cWYg4), social scoring systems are prohibited due to the
    following key concerns:'
  prefs: []
  type: TYPE_NORMAL
- en: Discriminatory outcomes
  prefs: []
  type: TYPE_NORMAL
- en: Social scoring systems may lead to discriminatory results and the exclusion
    of certain groups.
  prefs: []
  type: TYPE_NORMAL
- en: Violation of fundamental rights
  prefs: []
  type: TYPE_NORMAL
- en: These systems can violate the rights to dignity, non-discrimination, equality,
    and justice. There are also concerns about AI tools that compromise user privacy
    by collecting sensitive data without consent.
  prefs: []
  type: TYPE_NORMAL
- en: Unfair evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Social scoring systems evaluate or classify individuals based on multiple data
    points related to their social behavior across various contexts or on known, inferred,
    or predicted personal characteristics over time. The scoring mechanisms often
    lack transparency, fairness, and accountability.
  prefs: []
  type: TYPE_NORMAL
- en: Harmful treatment
  prefs: []
  type: TYPE_NORMAL
- en: The social scores obtained from these systems can result in negative treatment
    of individuals or groups in social contexts unrelated to where the data was initially
    generated or collected.
  prefs: []
  type: TYPE_NORMAL
- en: Extreme consequences
  prefs: []
  type: TYPE_NORMAL
- en: Social scoring can lead to harmful treatment that is unjustified relative to
    the gravity of a person’s social behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Broad impact
  prefs: []
  type: TYPE_NORMAL
- en: These systems can affect individuals’ access to services, employment, or other
    opportunities.
  prefs: []
  type: TYPE_NORMAL
- en: The EU AI Act aims to mitigate these risks by prohibiting social scoring AI
    systems. This ban applies to public and private actors using AI for social scoring.
  prefs: []
  type: TYPE_NORMAL
- en: Determining EU AI Act Obligations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To determine your obligations under the EU AI Act, you need to take two fundamental
    steps when assessing your AI systems:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Determine whether the Act applies to any AI systems in your inventory and clarify
    the scope of your obligations by answering the following questions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do they meet the Act’s definition of an “AI system”? (For more on this, see
    the following sidebar.)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If so, are they within the scope of the Act based on their intended use and
    risk classification? Those systems with low risk are exempt.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Are they considered high risk?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Do transparency obligations apply?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify your organization’s role with respect to each system. Is it acting
    as a provider or deployer?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you’ve identified which of your AI systems meet the Act’s definition, the
    next step is to classify their risk level. Your exact obligations will vary depending
    on this classification.
  prefs: []
  type: TYPE_NORMAL
- en: Framework for Classification of AI Systems by Risk Levels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having already discussed the unacceptable risk category, here we will consider
    the remaining three categories: high risk, limited risk, and minimal risk. Let’s
    start by reviewing the criteria that determine whether a system is classified
    as high risk.'
  prefs: []
  type: TYPE_NORMAL
- en: High risk
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The classification rules for high-risk AI systems, outlined in [Article 6 of
    the EU AI Act](https://oreil.ly/AyOj1), aim to identify systems that may pose
    significant risks to health, safety, or fundamental rights. The classification
    takes into account both the AI system’s intended purpose and the specific context
    and conditions under which it is used. Key considerations include:'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems are classified as high risk whether they are placed on the market
    or put into service as standalone products or as components of other products.
    For AI systems used as safety components of products, the classification aligns
    with the EU’s New Legislative Framework, ensuring consistency with broader product
    safety regulation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI systems intended to be used as safety components of products covered by [EU
    harmonization legislation](https://oreil.ly/d5LQd) are automatically classified
    as high risk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AI systems listed in [Annex III](https://oreil.ly/9-WO8) are classified as
    high risk. These include systems intended for use in the areas of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biometric identification and categorization of natural persons (e.g., AI systems
    for remote biometric identification in publicly accessible spaces)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Management and operation of critical infrastructure (e.g., AI systems used as
    safety components in the supply of water, gas, heating, or electricity)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Education and vocational training (e.g., AI systems used to evaluate students
    or assess educational institutions)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Employment, workers management, and access to self-employment (e.g., AI systems
    used for recruitment, promotion, and termination decisions)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to and enjoyment of key private services and public services and benefits
    (e.g., AI systems used to evaluate creditworthiness or establish credit scores)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Law enforcement (e.g., AI systems used by law enforcement to predict crimes
    or assess recidivism risk)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Migration, asylum, and border control management (e.g., AI systems used to process
    asylum, visa, and residence permit applications)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Administration of justice and democratic processes (e.g., AI systems used in
    judicial proceedings to assist judges)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Certain systems are exempted from high-risk classification even if they might
    otherwise qualify. These include:'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems used for purely accessory purposes (e.g., performing a narrowly procedural
    task, improving the result of a previously completed human activity, or performing
    a preparatory task)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI systems used exclusively for scientific research and development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI systems used for military applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EU AI Act high-risk system questionnaire
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Classifying AI applications as high risk means they are subject to stricter
    requirements under the EU AI Act related to risk management procedures, data governance
    measures, technical documentation, recordkeeping, transparency, human oversight,
    accuracy, and cybersecurity controls.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reading and interpreting the EU AI Act can be daunting. To help with determining
    whether an AI system qualifies as high risk under the Act, I have prepared a straightforward
    questionnaire, divided into four main sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Application area
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specific use cases
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Impact assessment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Technical characteristics
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The questionnaire is presented in [Table 4-3](#chapter_4_table_3_1748539919012658).
    Start with section 1 and work through each section in turn. The more affirmative
    answers you have, especially in sections 1 and 2, the more likely it is that your
    AI system is considered high risk.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Classifying an AI system with confidence can be challenging. In its white paper
    [“AI Act: Risk Classification of AI Systems from a Practical Perspective”](https://oreil.ly/a_oMu),
    the applied AI Institute examined more than 100 AI systems from different enterprise
    functions (marketing, production, purchasing, etc.) and determined that 18% were
    in the high-risk class, 42% were low risk, and for the remaining 40% it was unclear
    whether they fell into the high-risk class or not. The percentage of high-risk
    systems in this sample thus could be anywhere between 18% and 58%.'
  prefs: []
  type: TYPE_NORMAL
- en: The study identified several ambiguities in defining criteria across different
    areas as the main causes of unclear risk classifications for AI systems. For example,
    in many cases it is not clear whether a system is functioning as a “safety component.”
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-3\. EU AI Act high-risk system questionnaire
  prefs: []
  type: TYPE_NORMAL
- en: '| Question | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1\. Application area |'
  prefs: []
  type: TYPE_TB
- en: '| Is the AI system intended to be used in any of the following areas? |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Biometric identification and categorization of natural persons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Critical infrastructure (e.g., transport, energy, water supply)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Education or vocational training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Employment, worker management, or access to self-employment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Access to and enjoyment of essential private services and public services and
    benefits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Law enforcement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Migration, asylum, and border control management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Administration of justice and democratic processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '| If you checked any box in section 1, proceed to section 2\. If not, your
    system is likely not considered high risk. |'
  prefs: []
  type: TYPE_TB
- en: '| 2\. Specific use cases |'
  prefs: []
  type: TYPE_TB
- en: '| Within the selected area(s), does your AI system fall under any of these
    specific use cases? |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems used as safety components of products
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems for management or operation of critical infrastructure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems for determining access to educational institutions or assigning persons
    to such institutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems for recruitment, evaluation, promotion, or termination of work-related
    contractual relationships
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems for evaluating creditworthiness or establishing credit scores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems for dispatching emergency first response services
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems for law enforcement purposes (e.g., predicting crimes, profiling
    individuals)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems for migration, asylum, and border control management (e.g., verifying
    travel documents)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems to assist judicial authorities in researching and interpreting facts
    and the law
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '| If you checked any box in section 2, your AI system is likely considered
    high risk. If not, proceed to section 3. |'
  prefs: []
  type: TYPE_TB
- en: '| 3\. Impact assessment |'
  prefs: []
  type: TYPE_TB
- en: '| Does your AI system have the potential to: |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Cause significant harm to the health, safety, or fundamental rights of individuals?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Have a significant impact on the lives of a large number of EU residents?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Be difficult for individuals to opt out of or avoid?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '| Is the AI system intended to be used in a manner that could result in: |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Discrimination against protected groups?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Manipulation of human behavior?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Exploitation of vulnerabilities of specific groups?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '| If you checked any box in section 3, your AI system may be considered high
    risk, depending on the context and potential impact. |'
  prefs: []
  type: TYPE_TB
- en: '| 4\. Technical characteristics |'
  prefs: []
  type: TYPE_TB
- en: '| Does your AI system: | [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Use large datasets for training or operation?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Employ complex algorithms or machine learning techniques?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Have a high degree of autonomy in decision making?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Operate in a manner that is not fully transparent or explainable?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '| If you checked multiple boxes in section 4, combined with affirmative answers
    in previous sections, your system is more likely to be considered high risk. |'
  prefs: []
  type: TYPE_TB
- en: Interpretation guide
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you have affirmative answers in sections 1 and 2, or in section 1 combined
    with multiple affirmative answers in sections 3 and 4, your AI system is likely
    to be considered high risk under the EU AI Act. However, the final determination
    may depend on the specific context, implementation, and potential impact of your
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s important to note that the European Commission can adjust Annex III and
    add new high-risk AI systems as technology evolves, following the criteria outlined
    in [Article 7](https://oreil.ly/ipd1a). This allows the regulation to remain responsive
    to emerging AI applications and risks.
  prefs: []
  type: TYPE_NORMAL
- en: For a definitive assessment, it is recommended to consult legal experts and
    stay up-to-date on the latest guidance from EU authorities.
  prefs: []
  type: TYPE_NORMAL
- en: Limited risk (transparency obligations)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To be clear, the EU AI Act does not formally define a category of “limited-risk”
    AI systems. Instead, it focuses on three main categories: prohibited AI practices
    under the unacceptable risk category (Article 5), high-risk AI systems (Article
    6), and AI systems with specific transparency obligations (Article 50).'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for clarity, I use the term “limited risk” to refer to AI systems
    that are not explicitly prohibited or high risk but are still subject to the transparency
    obligations laid out in [Article 50](https://oreil.ly/cc73j). This includes:'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems that are intended to interact with natural persons
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Emotion recognition systems and biometric categorization systems
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: AI systems that generate or manipulate image, audio, text, or video content
    (including deepfakes)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As for high-risk systems, I have created a questionnaire that you can use to
    determine whether your AI system has specific transparency obligations. I’ll discuss
    what those obligations are in [Chapter 6](ch06.html#chapter_6_ai_engineering_for_limited_risk_ai_systems_1748539923606988).
  prefs: []
  type: TYPE_NORMAL
- en: EU AI Act limited-risk system questionnaire
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Answer the questions in [Table 4-4](#chapter_4_table_4_1748539919012681) to
    help determine if your AI system might be classified as a limited-risk system
    under the EU AI Act.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-4\. EU AI Act limited-risk system questionnaire
  prefs: []
  type: TYPE_NORMAL
- en: '| Question | Yes | No |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1\. Is your AI system intended to interact with humans? | [    ] | [    ]
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2\. Does your AI system fall into any of these categories? |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Emotion recognition systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Biometric categorization systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: AI systems used to generate or manipulate image, audio, or video content (“deepfakes”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '| 3\. Is your AI system specifically excluded from the limited-risk category
    for any of the following reasons? |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: It’s classified as a high-risk AI system under the EU AI Act.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: It’s used for general-purpose AI without a clearly defined intended purpose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: It’s used for a prohibited AI practice (e.g., social scoring, certain types
    of biometric identification).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '| 4\. Does your AI system have the potential to influence human behavior or
    decision making? | [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '| 5\. Is your AI system designed to be transparent about its AI nature? | [    ]
    | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '| 6\. Can users easily understand that they are interacting with an AI system?
    | [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '| 7\. Does your system provide clear disclaimers about its limitations and
    potential risks? | [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '| 8\. For systems that generate synthetic content (or manipulate image, audio,
    or video content), do you have measures in place to ensure the AI-generated content
    is clearly labeled as such? | [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: '| 9\. For biometric categorization or emotion recognition systems, do you inform
    users about the system’s purpose, functionality, and limitations? | [    ] | [    ]
    |'
  prefs: []
  type: TYPE_TB
- en: '| 10\. Do you have processes in place to handle user complaints or concerns
    about the AI system? | [    ] | [    ] |'
  prefs: []
  type: TYPE_TB
- en: Interpretation guide
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You may interpret your answers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If you answered “Yes” to question 1 and to any item in question 2, your system
    will likely be considered a limited-risk AI system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you answered “Yes” to any item in question 3, your system is likely not a
    limited-risk system (it may be high risk or prohibited).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Questions 4–10 relate to transparency and user protection requirements for limited-risk
    systems. Answering “No” to any of these may indicate areas where your system needs
    improvement to comply with the EU AI Act’s requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Please note that this questionnaire is intended as a general guide. For a definitive
    assessment, it is recommended to consult legal experts and stay up-to-date on
    the latest guidance from EU authorities.
  prefs: []
  type: TYPE_NORMAL
- en: Low risk
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even though the EU AI Act focuses more heavily on high-risk AI systems, it is
    still important address “low-risk” AI systems, as most AI currently in use across
    the EU falls into this category and some of these systems (such as chatbots and
    emotion recognition tools) may still be subject to transparency requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Act does not explicitly define a “low-risk” category. However, by process
    of elimination, low-risk AI systems can be understood to be those that:'
  prefs: []
  type: TYPE_NORMAL
- en: Do not involve practices prohibited under Article 5
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are not classified as high risk under Article 6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are not subject to transparency obligations under the terms outlined in Article
    50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These systems generally pose minimal risks to fundamental rights or public safety
    and are typically used for administrative or operational purposes rather than
    for making decisions with serious consequences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some examples of low-risk AI systems include:'
  prefs: []
  type: TYPE_NORMAL
- en: Spam filters (AI systems that help to identify and filter unwanted emails)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendation systems (AI systems that suggest products, movies, or other content
    based on user preferences)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chatbots that provide customer service (AI systems that can answer simple questions
    and provide basic assistance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI-powered games (AI systems that can generate challenges or adapt to player
    behavior)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the EU AI Act primarily focuses on regulating high-risk AI systems and
    prohibiting certain AI practices, no specific obligations are defined for low-risk
    AI systems as a distinct category.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that the classification of an AI system can depend on its specific context
    and intended use. Even a seemingly low-risk AI system could become high risk if
    used in a way that could harm individuals or society.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider facial recognition technology,which can be used for purposes
    ranging from unlocking personal devices to surveillance:'
  prefs: []
  type: TYPE_NORMAL
- en: Unlocking personal devices is considered minimal risk as it involves a consenting
    individual using the technology privately and poses little to no risk to broader
    society.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Surveillance by law enforcement, on the other hand, is classified as high risk
    due to its potential implications for fundamental rights and freedoms, such as
    privacy infringements and the risk of misidentification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you’d like more information about this complex topic, check out [“Navigating
    the EU AI Act Maze Using a Decision-Tree Approach”](https://oreil.ly/Zmit2) by
    Himly Hanif et al.
  prefs: []
  type: TYPE_NORMAL
- en: Deployer or Provider
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned earlier, an organization’s obligations under the EU AI Act depend
    on two key factors: the AI system’s risk category and whether it acts as a deployer
    or provider of the system. Let’s review the definitions of those two roles.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the terms of the EU AI Act, a *provider* is any individual, company,
    public authority, agency, or other body that:'
  prefs: []
  type: TYPE_NORMAL
- en: Develops an AI system or model, or has it developed by others
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Places the system or model on the market or puts it into service, either for
    a fee or free of charge
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does so under their own name or trademark
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Examples of providers include a tech company like Google that develops and releases
    large language models such as Gemini and a startup that develops an AI algorithm
    for predicting stock market trends and offers it as a service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The EU AI Act defines a *deployer* as any individual, company, public authority,
    agency, or other body that:'
  prefs: []
  type: TYPE_NORMAL
- en: Uses an AI system under their authority or control
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does so in a professional or business context (personal, non-professional use
    is excluded)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This definition applies regardless of whether the deployer is located within
    the EU or not, as long as the AI system’s output is used within the EU.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of deployers include a hospital using an AI system for medical diagnosis
    or treatment planning, a company using an AI-powered recruitment tool to screen
    job applicants, or a bank utilizing an AI system for credit scoring or loan approval.
  prefs: []
  type: TYPE_NORMAL
- en: An important distinction is that providers develop, place on the market, or
    put into service AI systems under their own name or trademark, whereas deployers
    use AI systems within their professional activities but do not develop or market
    the systems themselves. A company that develops an AI system for its own use can
    act as both a provider and a deployer. Additionally, a deployer may become a provider
    if they substantially modify an AI system or use it for purposes not intended
    by the original provider.
  prefs: []
  type: TYPE_NORMAL
- en: In [Table 4-5](#chapter_4_table_5_1748539919012703), I provide a practical framework
    to help determine your organization’s role and to identify all the relevant players
    who may act as deployers or providers of an AI system. The first step is to analyze
    every party involved in the machine learning lifecycle, using the CRISP-ML(Q)
    phases as a guide. For each actor, outline their specific activities, identify
    their location, and determine who sets or redefines the intended purpose of the
    AI system. Also consider the individuals or groups affected by its use. Based
    on this information, you can accurately assign the appropriate role to each identified
    organization.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-5\. Analysis of the organizations involved in the machine learning (ML)
    lifecycle
  prefs: []
  type: TYPE_NORMAL
- en: '| CRISP-ML(Q) phase | Who is implementing and executing this phase? | What
    is being done? | Where is the organization located? |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Business and data understanding |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Data preparation |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Development |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Evaluation |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Deployment |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: '| Monitoring and maintenance |   |   |   |'
  prefs: []
  type: TYPE_TB
- en: Now that you’ve determined whether the Act applies to your AI systems and clarified
    your organization’s role in relation to each system, you’re ready to identify
    the regulatory implications.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating EU AI Act Engineering Throughout the AI Development Lifecycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Beyond the requirements and obligations mentioned in the previous sections,
    the EU AI Act (along with other regulations on AI systems) introduces additional
    compliance considerations, particularly regarding transparency, risk management,
    and data governance. It is crucial to incorporate AI engineering efforts into
    every phase of the AI product lifecycle, especially when developing high-risk
    AI systems. As pictured in [Figure 4-5](#chapter_4_figure_5_1748539919001734),
    the level of engineering engagement and compliance effort tends to increase as
    a project progresses from ideation and proof of concept to the minimum viable
    product (MVP) stage, affecting development time, costs, and risk management. By
    integrating these efforts early in the development process—i.e., adopting a “shift-left”
    philosophy—teams can reduce risks and ensure that the final product is both technically
    robust and compliant with evolving AI regulations.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/taie_0405.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4-5\. The usual delivery sequence and relationship between PoC, prototype,
    pilot, and MVP
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Table 4-6](#chapter_4_table_6_1748539919012725) provides an overview of the
    different stages of AI development, illustrating the progression from initial
    idea to a market-ready product. In addition to core criteria such as scope, cost,
    and expected development time, it highlights relevant AI engineering tasks (both
    general and those specifically aimed at meeting EU AI Act compliance requirements).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4-6\. AI development stages and relevant AI engineering tasks
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Ideation | PoC | Prototype | Pilot | MVP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Purpose | Generate and explore innovative AI ideas. | Validate technical
    feasibility and potential value. | Create a working model to demonstrate core
    functionalities. | Test the AI solution in a real-world environment. | Launch
    a basic version to gather user feedback and validate market fit. |'
  prefs: []
  type: TYPE_TB
- en: '| Scope of AI product | Broad, exploring multiple possibilities | Narrow, focused
    on key technical challenges | Limited, but includes core features and user interactions
    | Broader than prototype, but still limited to specific use cases or user groups
    | Includes essential features to solve the core problem |'
  prefs: []
  type: TYPE_TB
- en: '| User involve-ment | Limited, mainly internal stakeholders | Minimal, possibly
    some expert users | Limited, typically internal testers or focus groups | Moderate,
    involving a select group of real users | Significant, engaging early adopters
    and gathering extensive feedback |'
  prefs: []
  type: TYPE_TB
- en: '| Dev. time (typical) | Days to weeks | Weeks to a couple of months | 1–3 months
    | 3–6 months | 6–12 months |'
  prefs: []
  type: TYPE_TB
- en: '| Cost | Low | Low to medium | Medium | Medium to high | High |'
  prefs: []
  type: TYPE_TB
- en: '| Risk | Very low | Low | Medium | Medium to high | High |'
  prefs: []
  type: TYPE_TB
- en: '| AI eng. tasks (general) |'
  prefs: []
  type: TYPE_TB
- en: Define initial data requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sketch basic model architectures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outline potential deployment scenarios.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Set up basic development environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform initial data collection and preprocessing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop simple model(s).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform basic model evaluation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Implement data pipelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop more complex models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up basic model versioning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement basic model serving.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Refine data pipelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement model monitoring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up CI/CD for model deployment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement basic A/B testing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Optimize data and model pipelines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement advanced monitoring and alerting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up automated retraining.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement advanced A/B testing and experimentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| AI eng. tasks for EU AI Act compliance |'
  prefs: []
  type: TYPE_TB
- en: Conduct initial risk assessment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identify potential high-risk AI systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Draft preliminary documentation of system architecture.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define an initial data governance strategy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Implement basic data quality measures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design initial logging mechanisms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Draft preliminary technical documentation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Implement more robust data governance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enhance logging and traceability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop initial risk management system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Begin human oversight implementation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Implement comprehensive data governance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Establish full logging and traceability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement a complete risk management system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finalize human oversight mechanisms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare for conformity assessment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Outcomes |'
  prefs: []
  type: TYPE_TB
- en: List of potential AI solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initial assessment of feasibility and value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Technical validation report
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Go/no-go decision for further development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Working AI model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User interface mockups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initial user feedback
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Performance metrics in real-world conditions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User adoption and satisfaction data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identified areas for improvement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Functional AI product
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initial user base
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comprehensive feedback for future iterations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Market validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'The introduction of the EU AI Act has had various implications with regard
    to AI system development:'
  prefs: []
  type: TYPE_NORMAL
- en: AI engineering and compliance tasks become more complex as the project progresses
    from ideation to MVP. As the system matures, the need for robust MLOps processes
    and governance increases. Many AI engineering tasks, such as implementing robust
    data pipelines and model monitoring, directly contribute to meeting EU AI Act
    compliance requirements by ensuring data quality and system reliability.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data governance becomes increasingly important in later stages, helping to ensure
    data quality and regulatory compliance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Human oversight mechanisms become more sophisticated in later stages, in line
    with the EU AI Act’s requirement for oversight of high-risk AI systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Risk assessment and management transition from high-level considerations in
    the early stages to comprehensive, continuous, and automated processes in the
    later stages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As the project progresses, there’s a greater emphasis on automating processes
    and ensuring scalability, particularly within MLOps tasks.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the MVP stage, the focus shifts toward preparing for audits and conformity
    assessments, reflecting the regulatory requirements for high-risk AI systems under
    the EU AI Act.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Emerging Roles in Organizations for EU AI Act Compliance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned previously, the operationalization of EU AI Act compliance happens
    on two levels: the organizational level and the AI system level. I focus on the
    former in [Appendix D](app04.html#appendix_d_emerging_roles_in_organizations_for_eu_ai_act_comp_1748539915764930),
    but I want to briefly touch on the topic here as well.'
  prefs: []
  type: TYPE_NORMAL
- en: With the rapid adoption of AI, several new roles are emerging within organizations
    to help ensure compliance with the EU AI Act. Some of these roles should be incorporated
    into machine learning teams to ensure that ethical and compliance aspects are
    fully integrated into the AI development process. Adopting a Team Topologies perspective
    provides a concrete framework for meeting the Act’s compliance requirements, ensuring
    that proper data governance is embedded throughout the ML development lifecycle.
    For further discussion of emerging roles and the implications of EU AI Act compliance
    for ML teams, see [Appendix D](app04.html#appendix_d_emerging_roles_in_organizations_for_eu_ai_act_comp_1748539915764930).
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The chapter explored the initial steps of creating an AI system landscape and
    classifying AI systems in the context of achieving compliance with the EU AI Act.
    To determine the specific requirements for your organization, you will need to
    identify the AI systems that are currently deployed and intended to be put into
    production and classify each of them based on risk level. You’ll also need to
    determine whether the organization is acting as a provider or deployer of these
    AI systems, because the obligations differ depending on the role.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, I will cover the requirements for high-risk AI systems
    as outlined by the EU AI Act and how to implement them. Key topics will include
    the need for high-quality datasets, comprehensive documentation and recordkeeping,
    transparency, robustness and accuracy, security, risk management systems, human
    oversight, addressing ethical concerns and bias mitigation, monitoring in production,
    pre-market compliance verification, and maintaining continuous compliance for
    high-risk AI systems.
  prefs: []
  type: TYPE_NORMAL
