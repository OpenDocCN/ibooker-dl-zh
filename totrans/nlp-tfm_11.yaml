- en: Chapter 10\. Training Transformers from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the opening paragraph of this book, we mentioned a sophisticated application
    called GitHub Copilot that uses GPT-like transformers to perform code autocompletion,
    a feature that is particularly useful when programming in a new language or framework
    or learning to code, or for automatically producing boilerplate code. Other products
    that use AI models for this purpose include [TabNine](https://tabnine.com) and
    [Kite](https://kite.com). Later, in [Chapter 5](ch05.xhtml#chapter_generation),
    we had a closer look at how we can use GPT models to generate high-quality text.
    In this chapter, we’ll close the circle and build our very own GPT-like model
    for generating Python source code! We call the resulting model *CodeParrot*.
  prefs: []
  type: TYPE_NORMAL
- en: So far we’ve mostly worked on data-constrained applications where the amount
    of labeled training data is limited. In these cases, transfer learning helped
    us build performant models. We took transfer learning to the limit in [Chapter 9](ch09.xhtml#chapter_fewlabels),
    where we barely used any training data at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we’ll move to the other extreme and look at what we can do
    when we are drowning in all the data we could possibly want. We’ll explore the
    pretraining step itself and learn how to train a transformer from scratch. In
    working through this problem, we’ll look at some aspects of training that we have
    not considered yet, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Gathering and processing a very large dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a custom tokenizer for our dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a model on multiple GPUs at scale
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To efficiently train large models with billions of parameters, we’ll need special
    tools for distributed training. Although the `Trainer` from ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers supports distributed training, we’ll take the opportunity to showcase
    a powerful PyTorch library called ![nlpt_pin01](Images/nlpt_pin01.png) Accelerate.
    We’ll end up touching on some of the largest NLP models in use today—but first,
    we need to find a sufficiently large dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unlike the code in the others in this book (which can be run with a Jupyter
    notebook on a single GPU), the training code in this chapter is designed to be
    run as a script with multiple GPUs. If you want to train your own version of CodeParrot,
    we recommend running the script provided in the ![nlpt_pin01](Images/nlpt_pin01.png)
    [Transformers repository](https://oreil.ly/ZyPPR).
  prefs: []
  type: TYPE_NORMAL
- en: Large Datasets and Where to Find Them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many domains where you may actually have a large amount of data at
    hand, ranging from legal documents to biomedical datasets to programming codebases.
    In most cases, these datasets are unlabeled, and their large size means that they
    can usually only be labeled through the use of heuristics, or by using accompanying
    metadata that is stored during the gathering process.
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, a very large corpus can be useful even when it is unlabeled or
    only heuristically labeled. We saw an example of this in [Chapter 9](ch09.xhtml#chapter_fewlabels),
    where we used the unlabeled part of a dataset to fine-tune a language model for
    domain adaptation. This approach typically yields a performance gain when limited
    data is available. The decision to train from scratch rather than fine-tune an
    existing model is mostly dictated by the size of your fine-tuning corpus and the
    domain differences between the available pretrained models and the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Using a pretrained model forces you to use the model’s corresponding tokenizer,
    but using a tokenizer that is trained on a corpus from another domain is typically
    suboptimal. For example, using GPT’s pretrained tokenizer on legal documents,
    other languages, or even completely different sequences such as musical notes
    or DNA sequences will result in poor tokenization (as we will see shortly).
  prefs: []
  type: TYPE_NORMAL
- en: As the amount of training data you have access to gets closer to the amount
    of data used for pretraining, it thus becomes interesting to consider training
    the model and the tokenizer from scratch, provided the necessary computational
    resources are available. Before we discuss the different pretraining objectives
    further, we first need to build a large corpus suitable for pretraining. Building
    such a corpus comes with its own set of challenges, which we’ll explore in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of Building a Large-Scale Corpus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The quality of a model after pretraining largely reflects the quality of the
    pretraining corpus. In particular, the model will inherit any defects in the pretraining
    corpus. Thus, before we attempt to create one of our own it’s good to be aware
    of some of the common issues and challenges that are associated with building
    large corpora for pretraining.
  prefs: []
  type: TYPE_NORMAL
- en: As the dataset gets larger and larger, the chances that you can fully control—or
    at least have a precise idea of—what is inside it diminish. A very large dataset
    will most likely not have been assembled by dedicated creators that craft one
    example at a time, while being aware and knowledgeable of the full pipeline and
    the task that the machine learning model will be applied to. Instead, it is much
    more likely that a very large dataset will have been created in an automatic or
    semiautomatic way by collecting data that is generated as a side effect of other
    activities. For instance, it may consist of all the documents (e.g., contracts,
    purchase orders, etc.) that a company stores, logs from user activities, or data
    gathered from the internet.
  prefs: []
  type: TYPE_NORMAL
- en: There are several important consequences that follow from the fact that large-scale
    datasets are mostly created with a high degree of automation. An important consideration
    is that there is limited control over both their content and the way they are
    created, and thus the risk of training a model on biased and lower-quality data
    increases. Recent investigations of famous large-scale datasets like BookCorpus
    and C4, which were used to train BERT and T5, respectively, have uncovered (among
    other things) that:^([1](ch10.xhtml#idm46238692182464))
  prefs: []
  type: TYPE_NORMAL
- en: A significant proportion of the C4 corpus is machine-translated rather than
    translated by humans.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disparate erasure of African-American English as a result of stopword filtering
    in C4 has resulted in an underrepresentation of such content.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is typically difficult in a large text corpus to find a middle ground between
    including (often too much) sexually or other explicit content and totally erasing
    all mention of sexuality or gender. As a surprising consequence of this, a rather
    common word like “sex” (which can have both neutral and explicit meanings) is
    completely unknown to a tokenizer that is trained on C4, since this word is fully
    absent from the corpus.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many occurrences of copyright violation in BookCorpus, and probably
    in other large-scale datasets as well.^([2](ch10.xhtml#idm46238692175664))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is genre skew toward “romance” novels in BookCorpus.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These discoveries might not be incompatible with downstream usage of the models
    trained on these corpora. For instance, the strong overrepresentation of romance
    novels in BookCorpus is probably acceptable if the model is intended to be used
    as a romance novel writing tool or for a building a game.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s illustrate the notion of a model being skewed by the data by comparing
    text generations from GPT and GPT-2\. GPT was mostly trained on BookCorpus, while
    GPT-2 was trained on web pages, blogs, and news articles linked from Reddit. We’ll
    compare similar-sized versions of both models on the same prompt, so that the
    main difference is the pretraining dataset, and we’ll use the `text-generation`
    pipeline to investigate the model outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s create a simple function to count the number of parameters in each
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The original GPT model is about the same size as the smallest GPT-2 model.
    Now we can generate three different completions from each model, each with the
    same input prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: By just sampling a handful of outputs from both models we can already see the
    distinctive “romance” skew in GPT generation, which will typically imagine a dialogue
    with a romantic interaction between a woman and a man. On the other hand, GPT-2
    was trained on webtext linked to and from Reddit articles and mostly adopts a
    neutral “they” in its generations, which contain “blog-like” or adventure-related
    elements.
  prefs: []
  type: TYPE_NORMAL
- en: In general, any model trained on a dataset will reflect the language bias and
    over- or underrepresentation of populations and events in its training data. These
    biases in the behavior of the model are important to take into consideration with
    regard to the target audience interacting with the model; for some useful guidelines,
    we refer you to a paper by Google that provides a framework for dataset development.^([3](ch10.xhtml#idm46238691930032))
  prefs: []
  type: TYPE_NORMAL
- en: This brief introduction should give you an idea of the difficult challenges
    you face when creating large text corpora. With these in mind, let’s now take
    a look at creating our own dataset!
  prefs: []
  type: TYPE_NORMAL
- en: Building a Custom Code Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To simplify the task a bit, we’ll focus on building a code generation model
    for the Python programming language only.^([4](ch10.xhtml#idm46238691921600))
    The first thing we’ll need is a large pretraining corpus consisting of Python
    source code. Fortunately, there is a natural resource that every software engineer
    knows: GitHub! The famous code-sharing website hosts terabytes of code repositories
    that are openly accessible and can be downloaded and used according to their respective
    licenses. At the time of this book’s writing, GitHub hosts more than 20 million
    code repositories. Many of them are small or test repositories created by users
    for learning, future side projects, or testing purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GitHub repositories can be accessed in two main ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Via the [GitHub REST API](https://oreil.ly/brhxw), like we saw in [Chapter 9](ch09.xhtml#chapter_fewlabels)
    when we downloaded all the GitHub issues of the ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers repository
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Via public dataset inventories like [Google BigQuery](https://oreil.ly/dYsVT)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the REST API is rate limited and we need a lot data for our pretraining
    corpus, we’ll use Google BigQuery to extract all the Python repositories. The
    `bigquery-public-data.github_repos.contents` table contains copies of all ASCII
    files that are less than 10 MB in size. Projects also need to be open source to
    be included, as determined by [GitHub’s License API](https://oreil.ly/N9zHb).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Google BigQuery dataset doesn’t contain star or downstream usage information.
    For those attributes, we can use the GitHub REST API or a service like [Libraries.io](https://libraries.io)
    that monitors open source packages. Indeed, a team from GitHub recently released
    a dataset called [CodeSearchNet](https://oreil.ly/daE43) that filtered repositories
    used in at least one downstream task using information from Libraries.io.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at what it takes to create our code dataset with Google BigQuery.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a dataset with Google BigQuery
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ll begin by extracting all the Python files in GitHub public repositories
    from the snapshot on Google BigQuery. For the sake of reproducibility and in case
    the policy around free usage of BigQuery changes in the future, we will also share
    this dataset on the Hugging Face Hub. The steps to export these files are adapted
    from the [TransCoder implementation](https://oreil.ly/vih2m) and are as follows:^([5](ch10.xhtml#idm46238691899472))
  prefs: []
  type: TYPE_NORMAL
- en: Create a Google Cloud account (a free trial should be sufficient).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a Google BigQuery project under your account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this project, create a dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this dataset, create a table where the results of the SQL request will be
    stored.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Prepare and run the following SQL query on the `github_repos` (to save the
    query results, select More > Query Options, check the “Set a destination table
    for query results” box, and specify the table name):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command processes about 2.6 TB of data to extract 26.8 million files. The
    result is a dataset of about 50 GB of compressed JSON files, each containing the
    source code of Python files. We filtered to remove empty files and small files
    such as *__init__.py* that don’t contain much useful information. We also filtered
    out files larger than 1 MB, and we downloaded the licenses for all the files so
    we can filter the training data based on licenses if we want later on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll download the results to our local machine. If you try this at home,
    make sure you have good bandwidth available and at least 50 GB of free disk space.
    The easiest way to get the resulting table to your local machine is to follow
    this two-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Export your results to Google Cloud:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a bucket and a folder in Google Cloud Storage (GCS).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Export your table to this bucket by selecting Export > Export to GCS, with an
    export format of JSON and gzip compression.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To download the bucket to your machine, use the [`gsutil` library](https://oreil.ly/JzgRk):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install `gsutil` with `pip install gsutil`.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Configure `gsutil` with your Google account: `gsutil config`.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Copy your bucket on your machine:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Alternatively, you can directly download the dataset from the Hugging Face
    Hub with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Working with a 50 GB dataset can be challenging; it requires sufficient disk
    space, and one must be careful not to run out of RAM. In the following section,
    we’ll have a look how ![nlpt_pin01](Images/nlpt_pin01.png) Datasets helps deal
    with these constraints of working with large datasets on small machines.
  prefs: []
  type: TYPE_NORMAL
- en: Working with Large Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Loading a very large dataset is often a challenging task, in particular when
    the data is larger than your machine’s RAM. For a large-scale pretraining dataset,
    this is a very common situation. In our example, we have 50 GB of compressed data
    and about 200 GB of uncompressed data, which is difficult to extract and load
    into the RAM memory of a standard-sized laptop or desktop computer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thankfully, ![nlpt_pin01](Images/nlpt_pin01.png) Datasets has been designed
    from the ground up to overcome this problem with two specific features that allow
    you to set yourself free from RAM and hard drive space limitations: memory mapping
    and streaming.'
  prefs: []
  type: TYPE_NORMAL
- en: Memory mapping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To overcome RAM limitations, ![nlpt_pin01](Images/nlpt_pin01.png) Datasets uses
    a mechanism for zero-copy and zero-overhead memory mapping that is activated by
    default. Basically, each dataset is cached on the drive in a file that is a direct
    reflection of the content in RAM memory. Instead of loading the dataset in RAM,
    ![nlpt_pin01](Images/nlpt_pin01.png) Datasets opens a read-only pointer to this
    file and uses it as a substitute for RAM, basically using the hard drive as a
    direct extension of the RAM memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to now we have mostly used ![nlpt_pin01](Images/nlpt_pin01.png) Datasets
    to access remote datasets on the Hugging Face Hub. Here, we will directly load
    our 50 GB of compressed JSON files that we have stored locally in the `codeparrot`
    repository. Since the JSON files are compressed, we first need to decompress them,
    which ![nlpt_pin01](Images/nlpt_pin01.png) Datasets takes care of for us. Be careful,
    because this requires about 180 GB of free disk space! However, it will use almost
    no RAM. By setting `delete_extracted=True` in the dataset’s downloading configuration,
    we can make sure that we delete all the files we don’t need anymore as soon as
    possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Under the hood, ![nlpt_pin01](Images/nlpt_pin01.png) Datasets extracted and
    read all the compressed JSON files by loading them in a single optimized cache
    file. Let’s see how big this dataset is once loaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the dataset is much larger than our typical RAM memory, but we
    can still load and access it, and we’re actually using a very limited amount of
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: You may wonder if this will make our training I/O-bound. In practice, NLP data
    is usually very lightweight to load in comparison to the model processing computations,
    so this is rarely an issue. In addition, the zero-copy/zero-overhead format uses
    Apache Arrow under the hood, which makes it very efficient to access any element.
    Depending on the speed of your hard drive and the batch size, iterating over the
    dataset can typically be done at a rate of a few tenths of a GB/s to several GB/s.
    This is great, but what if you can’t free enough disk space to store the full
    dataset locally? Everybody knows the feeling of helplessness when you get a full
    disk warning and need to painfully try to reclaim a few GB by looking for hidden
    files to delete. Luckily, you don’t need to store the full dataset locally if
    you use the streaming feature of ![nlpt_pin01](Images/nlpt_pin01.png) Datasets!
  prefs: []
  type: TYPE_NORMAL
- en: Streaming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some datasets (reaching up to 1 TB or more) will be difficult to fit even on
    a standard hard drive. In this case, an alternative to scaling up the server you
    are using is to *stream* the dataset. This is also possible with ![nlpt_pin01](Images/nlpt_pin01.png)
    Datasets for a number of compressed or uncompressed file formats that can be read
    line by line, like JSON Lines, CSV, or text (either raw or zip, gzip, or zstandard
    compressed). Let’s load our dataset directly from the compressed JSON files instead
    of creating a cache file from them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As you’ll see, loading the dataset is instantaneous! In streaming mode, the
    compressed JSON files will be opened and read on the fly. Our dataset is now an
    `IterableDataset` object. This means that we cannot access random elements of
    it, like `streamed_dataset[1264]`, but we need to read it in order, for instance
    with `next(iter(streamed_dataset))`. It’s still possible to use methods like `shuffle()`,
    but these will operate by fetching a buffer of examples and shuffling within this
    buffer (the size of the buffer is adjustable). When several files are provided
    as raw files (like our 184 files here), `shuffle()` will also randomize the order
    of files for the iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The samples of a streamed dataset are identical to the samples of a nonstreamed
    dataset, as we can see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The main interest of using a streaming dataset is that loading this dataset
    will not create a cache file on the drive or require any (significant) RAM memory.
    The original raw files are extracted and read on the fly when a new batch of examples
    is requested, and only that batch is loaded in memory. This reduces the memory
    footprint of our dataset from 180 GB to 50 GB. But we can take this one step further—instead
    of pointing to the local dataset we can reference the dataset on the Hub, and
    then directly download samples without downloading the raw files locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This dataset behaves exactly like the previous one, but behind the scenes downloads
    the examples on the fly. With such a setup, we can then use arbitrarily large
    datasets on an (almost) arbitrarily small server. Let’s push our dataset with
    a train and validation split to the Hugging Face Hub and access it with streaming.
  prefs: []
  type: TYPE_NORMAL
- en: Adding Datasets to the Hugging Face Hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Pushing our dataset to the Hugging Face Hub will allow us to:'
  prefs: []
  type: TYPE_NORMAL
- en: Easily access it from our training server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See how streaming datasets work seamlessly with datasets from the Hub.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Share it with the community, including you, dear reader!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To upload the dataset, we first need to log in to our Hugging Face account
    by running the following command in the terminal and providing the relevant credentials:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This is equivalent to the `notebook_login()` helper function we used in previous
    chapters. Once this is done, we can directly create a new dataset on the Hub and
    upload the compressed JSON files. To simplify things, we will create two repositories:
    one for the train split and one for the validation split. We can do this by running
    the `repo create` command of the CLI as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we’ve specified that the repository should be a dataset (in contrast to
    the model repositories used to store weights), along with the organization we’d
    like to store the repositories under. If you’re running this code under your personal
    account, you can omit the `--organization` flag. Next, we need to clone these
    empty repositories to our local machine, copy the JSON files to them, and push
    the changes to the Hub. We will take the last compressed JSON file out of the
    184 we have as the validation file (i.e., roughly 0.5 percent of our dataset).
    Execute these commands to clone the repository from the Hub to your local machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, copy all but the last GitHub file as the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then commit the files and push them to the Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, repeat the process for the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The `git add .` step can take a couple of minutes since a hash of all the files
    is computed. Uploading all the files will also take a little while. Since this
    will enable us to use streaming later in the chapter, however, this is not lost
    time, and this step will allow us to go significantly faster in the rest of our
    experiments. Note that we added a `_validation` suffix to the validation filename.
    This will enable us to load it later as a validation split.
  prefs: []
  type: TYPE_NORMAL
- en: 'And that’s it! Our two splits of the dataset as well as the full dataset are
    now live on the Hugging Face Hub at the following URLs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*https://huggingface.co/datasets/transformersbook/codeparrot*](https://huggingface.co/datasets/transformersbook/codeparrot)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*https://huggingface.co/datasets/transformersbook/codeparrot-train*](https://huggingface.co/datasets/transformersbook/codeparrot-train)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*https://huggingface.co/datasets/transformersbook/codeparrot-valid*](https://huggingface.co/datasets/transformersbook/codeparrot-valid)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s good practice to add README cards that explain how the datasets were created
    and provide as much useful information about them as possible. A well-documented
    dataset is more likely to be useful to other people, as well as your future self.
    You can read the ![nlpt_pin01](Images/nlpt_pin01.png) [Datasets README guide](https://oreil.ly/Tv9bq)
    for a detailed description of how to write good dataset documentation. You can
    also use the web editor to modify your README cards directly on the Hub later.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Tokenizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have gathered and loaded our large dataset, let’s see how we can
    efficiently process the data to feed to our model. In the previous chapters we’ve
    used tokenizers that accompanied the models we used. This made sense since these
    models were pretrained using data passed through a specific preprocessing pipeline
    defined in the tokenizer. When using a pretrained model, it’s important to stick
    with the same preprocessing design choices selected for pretraining. Otherwise
    the model may be fed out-of-distribution patterns or unknown tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, when we train a new model, using a tokenizer prepared for another
    dataset can be suboptimal. Here are a few examples of the kinds of problems we
    might run into when using an existing tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: The T5 tokenizer was trained on the [C4](https://oreil.ly/wsYIC) corpus that
    we encountered earlier, but an extensive step of stopword filtering was used to
    create it. As a result, the T5 tokenizer has never seen common English words such
    as “sex.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The CamemBERT tokenizer was also trained on a very large corpus of text, but
    only comprising French text (the French subset of the [OSCAR](https://oreil.ly/hgO5J)
    corpus). As such, it is unaware of common English words such “being.”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can easily test these features of each tokenizer in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: In many cases, splitting such short and common words into subparts will be inefficient,
    since this will increase the input sequence length of the model (which has limited
    context). Therefore, it’s important to be aware of the domain and preprocessing
    of the dataset that was used to train the tokenizer. The tokenizer and model can
    encode bias from the dataset that has an impact on the downstream behavior of
    the model. To create an optimal tokenizer for our dataset, we thus need to train
    one ourselves. Let’s see how this can be done.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Training a model involves starting from a given set of weights and using backpropagation
    from an error signal on a designed objective to minimize the loss of the model
    and find an optimal set of weights for the model to perform the task defined by
    the training objective. Training a tokenizer, on the other hand, does *not* involve
    backpropagation or weights. It is a way to create an optimal mapping from a string
    of text to a list of integers that can be ingested by the model. In today’s tokenizers,
    the optimal string-to-integer conversion involves a vocabulary consisting of a
    list of atomic strings and an associated method to convert, normalize, cut, or
    map a text string into a list of indices with this vocabulary. This list of indices
    is then the input for our neural network.
  prefs: []
  type: TYPE_NORMAL
- en: The Tokenizer Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As you saw in [Chapter 4](ch04.xhtml#chapter_ner), the tokenizer is a processing
    pipeline consisting of four steps: normalization, pretokenization, the tokenizer
    model, and postprocessing. The part of the tokenizer pipeline that can be trained
    on data is the tokenizer model. As we discussed in [Chapter 2](ch02.xhtml#chapter_classification),
    there are several subword tokenization algorithms that can be used, such as BPE,
    WordPiece, and Unigram.'
  prefs: []
  type: TYPE_NORMAL
- en: BPE starts from a list of basic units (single characters) and creates a vocabulary
    by a process of progressively creating new tokens formed by merging the most frequently
    co-occurring basic units and adding them to the vocabulary. This process is reiterated
    until a predefined vocabulary size is reached.
  prefs: []
  type: TYPE_NORMAL
- en: Unigram starts from the other end, by initializing its base vocabulary with
    all the words in the corpus, and potential subwords. Then it progressively removes
    or splits the less useful tokens to obtain a smaller and smaller vocabulary, until
    the target vocabulary size is reached. WordPiece is a predecessor of Unigram,
    and its official implementation was never open-sourced by Google.
  prefs: []
  type: TYPE_NORMAL
- en: The impact of these various algorithms on downstream performance varies depending
    on the task, and overall it’s quite difficult to identify if one algorithm is
    clearly superior to the others. Both BPE and Unigram have reasonable performance
    in most cases, but let’s have a look at some aspects to consider when evaluating.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring Tokenizer Performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The optimality and performance of a tokenizer are challenging to measure in
    practice. Some possible metrics include:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Subword fertility*, which calculates the average number of subwords produced
    per tokenized word'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Proportion of continued words*, which refers to the proportion of tokenized
    words in a corpus that are split into at least two subtokens'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Coverage metrics* like the proportion of unknown words or rarely used tokens
    in a tokenized corpus'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, robustness to misspelling or noise is often estimated, as well
    as model performance on such out-of-domain examples, as this strongly depends
    on the tokenization process.
  prefs: []
  type: TYPE_NORMAL
- en: These measures give a set of different views on the tokenizer’s performance,
    but they tend to ignore the interaction of the tokenizer with the model. For example,
    subword fertility can be minimized by including all the possible words in the
    vocabulary, but this will produce a very large vocabulary for the model.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, the performance of the various tokenization approaches is thus generally
    best estimated by using the downstream performance of the model as the ultimate
    metric. For instance, the good performance of early BPE approaches was demonstrated
    by showing improved performance on machine translation tasks by models trained
    using these tokenizers and vocabularies instead of character- or word-based tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how we can build our own tokenizer optimized for Python code.
  prefs: []
  type: TYPE_NORMAL
- en: A Tokenizer for Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need a custom tokenizer for our use case: tokenizing Python code. The question
    of pretokenization merits some discussion for programming languages. If we split
    on whitespaces and remove them, we will lose all the indentation information,
    which in Python is important for the semantics of the program (just think about
    `while` loops, or `if-then-else` statements). On the other hand, line breaks are
    not meaningful and can be added or removed without impact on the semantics. Similarly,
    splitting on punctuation, like an underscore, which is used to compose a single
    variable name from several subparts, might not make as much sense as it would
    in natural language. Using a natural language pretokenizer for tokenizing code
    thus seems potentially suboptimal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see if there are any tokenizers in the collection provided on the Hub
    that might be useful to us. We want a tokenizer that preserves spaces, so a good
    candidate could be a byte-level tokenizer like the one from GPT-2\. Let’s load
    this tokenizer and explore its tokenization properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Python has a built-in `tokenize` module that splits Python code strings into
    meaningful units (code operation, comments, indent and dedent, etc.). One issue
    with using this approach is that this pretokenizer is Python-based and as such
    is typically rather slow and limited by the Python global interpreter lock (GIL).
    On the other hand, most of the tokenizers in the ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers library are provided by the ![nlpt_pin01](Images/nlpt_pin01.png)
    Tokenizers library and are coded in Rust. The Rust tokenizers are many orders
    of magnitude faster to train and to use, and we will thus likely want to use them
    given the size of our corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is quite a strange output, so let’s try to understand what is happening
    here by running the various submodules of the tokenizer’s pipeline. First let’s
    see what normalization is applied in this tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, the GPT-2 tokenizer uses no normalization. It works directly
    on the raw Unicode inputs without any normalization steps. Let’s now take a look
    at the pretokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: What are all these `Ġ` symbols, and what are the numbers accompanying the tokens?
    Let’s explain both and see if we can understand better how this tokenizer works.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with the numbers. ![nlpt_pin01](Images/nlpt_pin01.png) Tokenizers
    has a very useful feature for switching between strings and tokens, called *offset
    tracking*. All the operations on the input string are tracked so that it’s possible
    to know exactly what part of the input string a token after tokenization corresponds
    to. These numbers simply indicate where in the original string each token comes
    from; for instance, the word `'hello'` in the first line corresponds to the characters
    8 to 13 in the original string. If some characters are removed in a normalization
    step, we are thus still able to associate each token with the respective part
    in the original string.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other curious feature of the tokenized text is the odd-looking characters,
    such as `Ċ` and `Ġ`. *Byte-level* means that this tokenizer works on bytes instead
    of Unicode characters. Each Unicode character is composed of between 1 and 4 bytes,
    depending on the character. The nice thing about bytes is that while there are
    143,859 Unicode characters in the Unicode alphabet, there are only 256 elements
    in the byte alphabet, and you can express each Unicode character as a sequence
    of these bytes. If we work on bytes we can thus express all the strings composed
    from the UTF-8 world as longer strings in this alphabet of 256 values. That is,
    we can have a model using an alphabet of only 256 words and be able to process
    any Unicode string. Let’s have a look at what the byte representations of some
    characters look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point you might wonder: why work on a byte level? Think back to our
    discussion in [Chapter 2](ch02.xhtml#chapter_classification) about the trade-offs
    between character and word tokens. We could decide to build our vocabulary from
    the 143,859 Unicode characters, but we would also like to include words—i.e.,
    combinations of Unicode characters—in our vocabulary, so this (already very large)
    size is only a lower bound for the total size of the vocabulary. This will make
    our model’s embedding layer very large because it comprises one vector for each
    vocabulary token.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other extreme, if we only use the 256 byte values as our vocabulary,
    the input sequences will be segmented in many small pieces (each byte constituting
    the Unicode characters), and as such our model will have to work on long inputs
    and spend significant compute power on reconstructing Unicode characters from
    their separate bytes, and then words from these characters. See the paper accompanying
    the ByT5 model release for a detailed study of this overhead.^([6](ch10.xhtml#idm46238691054768))
  prefs: []
  type: TYPE_NORMAL
- en: A middle-ground solution is to construct a medium-sized vocabulary by extending
    the 256-word vocabulary with the most common combinations of bytes. This is the
    approach taken by the BPE algorithm. The idea is to progressively construct a
    vocabulary of a predefined size by creating new vocabulary tokens through iteratively
    merging the most frequently co-occurring pair of tokens in the vocabulary. For
    instance, if `t` and `h` occur very frequently together, like in English, we’ll
    add a token `th` to the vocabulary to model this pair of tokens instead of keeping
    them separated. The `t` and `h` tokens are kept in the vocabulary to tokenize
    instances where they do not occur together. Starting from a basic vocabulary of
    elementary units, we can then model any string efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Be careful not to confuse the “byte” in “Byte-Pair Encoding” with the “byte”
    in “byte-level.” The name Byte-Pair Encoding comes from a data compression technique
    proposed by Philip Gage in 1994, originally operating on bytes.^([7](ch10.xhtml#idm46238691010992))
    Unlike what this name might indicate, standard BPE algorithms in NLP typically
    operate on Unicode strings rather than bytes (although there is a new type of
    BPE that specifically works on bytes, called *byte-level BPE*). If we read our
    Unicode strings in bytes we can thus reuse a simple BPE subword splitting algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: There is just one issue when using a typical BPE algorithm in NLP. These algorithms
    are designed to work with clean Unicode string as inputs, not bytes, and expect
    regular ASCII characters in the inputs, without spaces or control characters.
    But in the Unicode characters corresponding to the 256 first bytes, there are
    many control characters (newline, tab, escape, line feed, and other nonprintable
    characters). To overcome this problem, the GPT-2 tokenizer first maps all the
    256 input bytes to Unicode strings that can easily be digested by the standard
    BPE algorithms—that is, we will map our 256 elementary values to Unicode strings
    that all correspond to standard printable Unicode characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s not very important that these Unicode characters are each encoded with
    1 byte or more; what is important is that we have 256 single values at the end,
    forming our base vocabulary, and that these 256 values are correctly handled by
    our BPE algorithm. Let’s see some examples of this mapping with the GPT-2 tokenizer.
    We can access the entire mapping as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: And we can take a look at some common values of bytes and associated mapped
    Unicode characters in [Table 10-1](#unicode_mapping).
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-1\. Examples of character mappings in BPE
  prefs: []
  type: TYPE_NORMAL
- en: '| Description | Character | Bytes | Mapped bytes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Regular characters | `a` and `?` | 97 and 63 | `a` and `?` |'
  prefs: []
  type: TYPE_TB
- en: '| A nonprintable control character (carriage return) | `U+000D` | 13 | `č`
    |'
  prefs: []
  type: TYPE_TB
- en: '| A space | ` ` | 32 | `Ġ` |'
  prefs: []
  type: TYPE_TB
- en: '| A nonbreakable space | `\xa0` | 160 | `ł` |'
  prefs: []
  type: TYPE_TB
- en: '| A newline character | `\n` | 10 | `Ċ` |'
  prefs: []
  type: TYPE_TB
- en: 'We could have used a more explicit conversion, like mapping newlines to a `NEWLINE`
    string, but BPE algorithms are typically designed to work on characters. For this
    reason, keeping one Unicode character for each byte character is easier to handle
    with an out-of-the-box BPE algorithm. Now that we have been introduced to the
    dark magic of Unicode encodings, we can understand our tokenization conversion
    a bit better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can recognize the newlines, which as we now know are mapped to `Ċ`, and
    the spaces, mapped to `Ġ`. We also see that:'
  prefs: []
  type: TYPE_NORMAL
- en: Spaces, and in particular consecutive spaces, are conserved (for instance, the
    three spaces in `'ĊĠĠĠ'`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consecutive spaces are considered as a single word.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each space preceding a word is attached to and considered a part of the subsequent
    word (e.g., in `'Ġsay'`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s now experiment with the BPE model. As we’ve mentioned, it’s in charge
    of splitting the words into subunits until all subunits belong to the predefined
    vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The vocabulary of our GPT-2 tokenizer comprises 50,257 words:'
  prefs: []
  type: TYPE_NORMAL
- en: The base vocabulary with the 256 values of the bytes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 50,000 additional tokens created by repeatedly merging the most commonly co-occurring
    tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A special character added to the vocabulary to represent document boundaries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can easily check that by looking at the length attribute of the tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the full pipeline on our input code gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the BPE tokenizer keeps most of the words but will split the
    multiple spaces of our indentation into several consecutive spaces. This happens
    because this tokenizer is not specifically trained on code, but mostly on texts
    where consecutive spaces are rare. The BPE model thus doesn’t include a specific
    token in the vocabulary for indentation. This is a case where the tokenizer model
    is poorly suited for the dataset’s domain. As we discussed earlier, the solution
    is to retrain the tokenizer on the target corpus. So let’s get to it!
  prefs: []
  type: TYPE_NORMAL
- en: Training a Tokenizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s retrain our byte-level BPE tokenizer on a slice of our corpus to get
    a vocabulary better adapted to Python code. Retraining a tokenizer provided by
    ![nlpt_pin01](Images/nlpt_pin01.png) Transformers is simple. We just need to:'
  prefs: []
  type: TYPE_NORMAL
- en: Specify our target vocabulary size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare an iterator to supply lists of input strings to process to train the
    tokenizer’s model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Call the `train_new_from_iterator()` method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike deep learning models, which are often expected to memorize a lot of specific
    details from the training corpus, tokenizers are really just trained to extract
    the main statistics. In a nutshell, the tokenizer is just trained to know which
    letter combinations are the most frequent in our corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, you don’t necessarily need to train your tokenizer on a very large
    corpus; the corpus just needs to be representative of your domain and big enough
    for the tokenizer to extract statistically significant measures. But depending
    on the vocabulary size and the exact texts in the corpus, the tokenizer can end
    up storing unexpected words. We can see this, for instance, when looking at the
    longest words in the vocabulary of the GPT-2 tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'These tokens look like separator lines that are likely to be used on forums.
    This makes sense since GPT-2 was trained on a corpus centered around Reddit. Now
    let’s have a look at the last words that were added to the vocabulary, and thus
    the least frequent ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: The first token, `<|endoftext|>`, is the special token used to specify the end
    of a text sequence and was added after the BPE vocabulary was built. For each
    of these tokens our model will have to learn an associated word embedding, and
    we probably don’t want the embedding matrix to contain too many noisy words. Also
    note how some very time- and space-specific knowledge of the world (e.g., proper
    nouns like `Hitman` and `Commission`) is embedded at a very low level in our modeling
    approach by these words being granted separate tokens with associated vectors
    in the vocabulary. The creation of such specific tokens by a BPE tokenizer can
    also be an indication that the target vocabulary size is too large or that the
    corpus contains idiosyncratic tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s train a fresh tokenizer on our corpus and examine its learned vocabulary.
    Since we just need a corpus reasonably representative of our dataset statistics,
    let’s select about 1–2 GB of data, or about 100,000 documents from our corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s investigate the first and last words created by our BPE algorithm to
    see how relevant our vocabulary is. We skip the 256 byte tokens and look at the
    first tokens added thereafter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we can see various standard levels of indentation and whitespace tokens,
    as well as short common Python keywords like `self`, `or`, and `in`. This is a
    good sign that our BPE algorithm is working as intended. Now let’s check out the
    last words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Here there are still some relatively common words, like [`recv`](https://oreil.ly/tliPP),
    as well as some more noisy words probably coming from the comments.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also tokenize our simple example of Python code to see how our tokenizer
    is behaving on a simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Even though they are not code keywords, it’s a little annoying to see common
    English words like `World` or `say` being split by our tokenizer, since we’d expect
    them to occur rather frequently in the corpus. Let’s check if all the Python reserved
    keywords are in the vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'It appears that several quite frequent keywords, like `finally`, are not in
    the vocabulary either. Let’s try building a larger vocabulary using a larger sample
    of our dataset. For instance, we can build a vocabulary of 32,768 words (multiples
    of 8 are better for some efficient GPU/TPU computations) and train the tokenizer
    on a twice as large slice of our corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We don’t expect the most frequent tokens to change much when adding more documents,
    but let’s look at the last tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'A brief inspection doesn’t show any regular programming keywords here, which
    is promising. Let’s try tokenizing our sample code example with the new larger
    tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Here also the indents are conveniently kept in the vocabulary, and we see that
    common English words like `Hello`, `World`, and `say` are also included as single
    tokens. This seems more in line with our expectations of the data the model may
    see in the downstream task. Let’s investigate the common Python keywords, as we
    did before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: We are still missing the [`nonlocal` keyword](https://oreil.ly/IHAMu), but it’s
    also rarely used in practice as it makes the syntax more complex. Keeping it out
    of the vocabulary seems reasonable. After this manual inspection, our larger tokenizer
    seems well adapted for our task—but as we mentioned earlier, objectively evaluating
    the performance of a tokenizer is a challenging task without measuring the model’s
    performance. We will proceed with this one and train a model to see how well it
    works in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can easily verify that the new tokenizer is about twice as efficient than
    the standard GPT-2 tokenizer by comparing the sequence lengths of tokenized code
    examples. Our tokenizer uses approximately half as many tokens as the existing
    one to encode a text, which gives us twice the effective model context for free.
    When we train a new model with the new tokenizer on a context window of size 1,024
    it is equivalent to training the same model with the old tokenizer on a context
    window of size 2,048, with the advantage of being much faster and more memory
    efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Saving a Custom Tokenizer on the Hub
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that our tokenizer is trained, we should save it. The simplest way to save
    it and be able to access it from anywhere later is to push it to the Hugging Face
    Hub. This will be especially useful later, when we use a separate training server.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a private model repository and save our tokenizer in it as a first
    file, we can directly use the `push_to_hub()` method of the tokenizer. Since we
    already authenticated our account with `huggingface-cli login`, we can simply
    push the tokenizer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'If you don’t want to push to an organization, you can simply omit the `organization`
    argument. This will create a repository in your namespace named `codeparrot`,
    which anyone can then load by running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The tokenizer loaded from the Hub behaves exactly as we just saw. We can also
    investigate its files and saved vocabulary on the [Hub](https://oreil.ly/vcLeo).
    For reproducibility, let’s save our smaller tokenizer as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: This was a deep dive into building a tokenizer for a specific use case. Next,
    we will finally create a new model and train it from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Model from Scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here’s the part you’ve probably been waiting for: the model training. In this
    section we’ll decide which architecture works best for the task, initialize a
    fresh model without pretrained weights, set up a custom data loading class, and
    create a scalable training loop. In the grand finale we will train small and large
    GPT-2 models with 111 million and 1.5 billion parameters, respectively! But let’s
    not get ahead ourselves. First, we need to decide which architecture is best suited
    for code autocompletion.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this section we will implement a longer than usual script to train a model
    on a distributed infrastructure. Therefore, you should not run each code snippet
    independently, but instead download the script provided in the ![nlpt_pin01](Images/nlpt_pin01.png)
    [Transformers repository](https://oreil.ly/ZyPPR). Follow the accompanying instructions
    to execute the script with ![nlpt_pin01](Images/nlpt_pin01.png) Accelerate on
    your hardware.
  prefs: []
  type: TYPE_NORMAL
- en: A Tale of Pretraining Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have access to a large-scale pretraining corpus and an efficient
    tokenizer, we can start thinking about how to pretrain a transformer model. With
    such a large codebase consisting of code snippets like the one shown in [Figure 10-1](#code-snippet),
    we can tackle several tasks. Which one we choose will influence our choice of
    pretraining objectives. Let’s have a look at three common tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Code snippet](Images/nlpt_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. An example of a Python function that could be found in our dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Causal language modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A natural task with textual data is to provide a model with the beginning of
    a code sample and ask it to generate possible completions. This is a self-supervised
    training objective in which we can use the dataset without annotations. This should
    ring a bell: it’s the *causal language modeling* task we encountered in [Chapter 5](ch05.xhtml#chapter_generation).
    A directly related downstream task is code autocompletion, so we’ll definitely
    put this model on the shortlist. A decoder-only architecture such as the GPT family
    of models is usually best suited for this task, as shown in [Figure 10-2](#pretraining-clm).'
  prefs: []
  type: TYPE_NORMAL
- en: '![CLM pretraining](Images/nlpt_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. In causal language modeling, the future tokens are masked and
    the model has to predict them; typically a decoder model such as GPT is used for
    such a task
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Masked language modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A related but slightly different task is to provide a model with a noisy code
    sample, for instance with a code instruction replaced by a random or masked word,
    and ask it to reconstruct the original clean sample, as illustrated in [Figure 10-3](#pretraining-mlm).
    This is also a self-supervised training objective and is commonly called *masked
    language modeling* or the *denoising objective*. It’s harder to think about a
    downstream task directly related to denoising, but denoising is generally a good
    pretraining task to learn general representations for later downstream tasks.
    Many of the models that we have used in the previous chapters (like BERT and XLM-RoBERTa)
    are pretrained in that way. Training a masked language model on a large corpus
    can thus be combined with fine-tuning the model on a downstream task with a limited
    number of labeled examples.
  prefs: []
  type: TYPE_NORMAL
- en: '![MLM pretraining](Images/nlpt_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. In masked language modeling some of the input tokens are either
    masked or replaced, and the model’s task is to predict the original tokens; this
    is the architecture underlying the encoder branch of transformer models
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sequence-to-sequence training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An alternative task is to use a heuristic like regular expressions to separate
    comments or docstrings from code and build a large-scale dataset of (code, comments)
    pairs that can be used as an annotated dataset. The training task is then a supervised
    training objective in which one category (code or comment) is used as input for
    the model and the other category (comment or code) is used as labels. This is
    a case of *supervised learning* with (input, labels) pairs, as highlighted in
    [Figure 10-4](#pretraining-seq2seq). With a large, clean, and diverse dataset
    as well as a model with sufficient capacity, we can try to train a model that
    learns to transcript comments in code or vice versa. A downstream task directly
    related to this supervised training task is then documentation generation from
    code or code generation from documentation, depending on how we set our input/outputs.
    In this setting a sequence is translated into another sequence, which is where
    encoder-decoder architectures such as T5, BART, and PEGASUS shine.
  prefs: []
  type: TYPE_NORMAL
- en: '![Seq2seq pretraining](Images/nlpt_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-4\. Using an encoder-decoder architecture for a sequence-to-sequence
    task where the inputs are split into comment/code pairs using heuristics: the
    model gets one element as input and needs to generate the other one'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since we want to build a code autocompletion model, we’ll select the first objective
    and choose a GPT architecture for the task. So let’s initialize a fresh GPT-2
    model!
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the first time in this book that we won’t use the `from_pretrained()`
    method to load a model but initialize the new model. We will, however, load the
    configuration of `gpt2-xl` so that we use the same hyperparameters and only adapt
    the vocabulary size for the new tokenizer. We then initialize a new model with
    this configuration with the `from_config()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check how large the model actually is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'This is a 1.5B parameter model! This is a lot of capacity, but we also have
    a large dataset. In general, large language models are more efficient to train
    as long as the dataset is reasonably large. Let’s save the newly initialized model
    in a *models/* folder and push it to the Hub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Pushing the model to the Hub may take a few minutes given the size of the checkpoint
    (> 5 GB). Since this model is quite large, we’ll also create a smaller version
    that we can train to make sure everything works before scaling up. We will take
    the standard GPT-2 size as a base:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'And let’s save it to the Hub as well for easy sharing and reuse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have two models we can train, we need to make sure we can feed them
    the input data efficiently during training.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Dataloader
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To be able to train with maximal efficiency, we will want to supply our model
    with sequences filling its context. For example, if the context length of our
    model is 1,024 tokens, we always want to provide 1,024-token sequences during
    training. But some of our code examples might be shorter or longer than 1,024
    tokens. To feed batches with full sequences of `sequence_length` to our model,
    we should thus either drop the last incomplete sequence or pad it. However, this
    will render our training slightly less efficient and force us to take care of
    padding and masking padded token labels. We are much more compute- than data-constrained,
    so we’ll take the easy and efficient way here. We can use a little trick to make
    sure we don’t lose too many trailing segments: we can tokenize several examples
    and then concatenate them, separated by the special end-of-sequence token, to
    get a very long sequence. Finally, we split this sequence into equally sized chunks
    as shown in [Figure 10-5](#preprocessing-clm). With this approach, we lose at
    most a small fraction of the data at the end.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Preprocessing for CLM](Images/nlpt_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. Preparing sequences of varying length for causal language modeling
    by concatenating several tokenized examples with an EOS token before chunking
    them
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can, for instance, make sure we have roughly one hundred full sequences
    in our tokenized examples by defining our input string character length as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: '`input_characters` is the number of characters in the string input to our tokenizer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`number_of_sequences` is the number of (truncated) sequences we would like
    from our tokenizer, (e.g., 100).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequence_length` is the number of tokens per sequence returned by the tokenizer,
    (e.g., 1,024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`characters_per_token` is the average number of characters per output token
    that we first need to estimate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we input a string with `input_characters` characters we will thus get on
    average `number_of_sequences` output sequences, and we can easily calculate how
    much input data we are losing by dropping the last sequence. If `number_of_sequences=100`
    it means that we stack roughly 100 sequences and at most lose the last element,
    which might be too short or too long. This corresponds to at most losing 1% of
    our dataset. At the same time, this approach ensures that we don’t introduce a
    bias by cutting off the majority of file endings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first estimate the average character length per token in our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'With that we have all that’s needed to create our own `IterableDataset` (which
    is a helper class provided by PyTorch) for preparing constant-length inputs for
    the model. We just need to inherit from `IterableDataset` and set up the `__iter__()`
    function that yields the next element with the logic we just walked through:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'The `__iter__()` function builds up a buffer of strings until it contains enough
    characters. All the elements in the buffer are tokenized and concatenated with
    the EOS token, then the long sequence in `all_token_ids` is chunked in `seq_length`-sized
    slices. Normally, we need attention masks to stack padded sequences of varying
    length and make sure the padding is ignored during training. We have taken care
    of this by only providing sequences of the same (maximal) length, so we don’t
    need the masks here and only return the `input_ids`. Let’s test our iterable dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Nice, this works as intended and we get constant-length inputs for the model.
    Now that we have a reliable data source for the model, it’s time to build the
    actual training loop.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that we shuffled the raw dataset before creating a `ConstantLengthDataset`.
    Since this is an iterable dataset, we can’t just shuffle the whole dataset at
    the beginning. Instead, we set up a buffer with size `buffer_size` and shuffle
    the elements in this buffer before we get elements from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Training Loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now have all the elements to write our training loop. One obvious limitation
    of training our own language model is the memory limits on the GPUs we will use.
    Even on a modern graphics card you can’t train a model at GPT-2 scale in reasonable
    time. In this tutorial we will implement *data parallelism*, which will help us
    utilize several GPUs for training. Fortunately, we can use ![nlpt_pin01](Images/nlpt_pin01.png)
    Accelerate to make our code scalable. The ![nlpt_pin01](Images/nlpt_pin01.png)
    Accelerate library is designed to make distributed training—and changing the underlying
    hardware for training—easy. We can also use the `Trainer` for distributed training
    but ![nlpt_pin01](Images/nlpt_pin01.png) Accelerate gives us full control over
    the training loop, which is what we want to explore here.
  prefs: []
  type: TYPE_NORMAL
- en: '![nlpt_pin01](Images/nlpt_pin01.png) Accelerate provides an easy API to make
    training scripts run with mixed precision and in any kind of distributed setting
    (single GPU, multiple GPUs, and TPUs). The same code can then run seamlessly on
    your local machine for debugging purposes or your beefy training cluster for the
    final training run. You only need to make a handful of changes to your native
    PyTorch training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'The core part of the changes is the call to `prepare()`, which makes sure the
    model, optimizers, and dataloaders are all prepared and distributed on the infrastructure.
    These minor changes to the PyTorch training loop enable you to easily scale training
    across different infrastructures. With that in mind, let’s start building up our
    training script and define a few helper functions. First we set up the hyperparameters
    for training and wrap them in a `Namespace` for easy access:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we set up logging for training. Since we are training a model from scratch,
    the training run will take a while and require expensive infrastructure. Therefore,
    we want to make sure that all the relevant information is stored and easily accessible.
    The `setup_logging()` method sets up three levels of logging: using a standard
    Python [`Logger`](https://oreil.ly/P9Xrm), [TensorBoard](https://oreil.ly/kY5ri),
    and [Weights & Biases](https://oreil.ly/BCC3k). Depending on your preferences
    and use case, you can add or remove logging frameworks here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: Each worker gets a unique `accelerator.process_index`, which we use with the
    `FileHandler` to write the logs of each worker to an individual file. We also
    use the `accel⁠erator.is_main_process` attribute, which is only `true` for the
    main worker. We make sure we don’t initialize the TensorBoard and Weights & Biases
    loggers several times, and we decrease the logging levels for the other workers.
    We return the autogenerated, unique `wandb.run.name`, which we use later to name
    our experiment branch on the Hub.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll also define a function to log the metrics with TensorBoard and Weights
    & Biases. We again use the `accelerator.is_main_process` here to ensure that we
    only log the metrics once and not for each worker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s write a function that creates the dataloaders for the training
    and validation sets with our brand new `ConstantLengthDataset` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: At the end we wrap the dataset in a `DataLoader`, which also handles the batching.
    ​![nlpt_pin01](Images/nlpt_pin01.png)⁠ Accelerate will take care of distributing
    the batches to each worker.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another aspect we need to implement is optimization. We will set up the optimizer
    and learning rate schedule in the main loop, but we define a helper function here
    to differentiate the parameters that should receive weight decay. In general,
    biases and LayerNorm weights are not subject to weight decay:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we want to evaluate the model on the validation set from time to time,
    so let’s add an evaluation function we can call that calculates the loss and perplexity
    on the evaluation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: The perplexity measures how well the model’s output probability distributions
    predict the targeted tokens. So a lower perplexity corresponds to a better performance.
    Note that we can compute the perplexity by exponentiating the cross-entropy loss
    which we get from the model’s output. Especially at the start of training when
    the loss is still high, it is possible to get a numerical overflow when calculating
    the perplexity. We catch this error and set the perplexity to infinity in these
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: Before we put it all together in the training script, there is one more additional
    function that we’ll use. As you know by now, the Hugging Face Hub uses Git under
    the hood to store and version models and datasets. With the `Repository` class
    from the *huggingface_hub* library you can programmatically access the repository
    and pull, branch, commit, or push. We’ll use this in our script to continuously
    push model checkpoints to the Hub during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have all these helper functions in place, we are ready to write
    the heart of the training script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'This is quite a code block, but remember that this is all the code you need
    to train a fancy, large language model on a distributed infrastructure. Let’s
    deconstruct the script a little bit and highlight the most important parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Model saving*'
  prefs: []
  type: TYPE_NORMAL
- en: We run the script from within the model repository, and at the start we check
    out a new branch named after the `run_name` we get from Weights & Biases. Later,
    we commit the model at each checkpoint and push it to the Hub. With that setup
    each experiment is on a new branch and each commit represents a model checkpoint.
    Note that we need to call `wait_for_everyone()` and `unwrap_model()` to make sure
    the model is properly synchronized when we store it.
  prefs: []
  type: TYPE_NORMAL
- en: '*Optimization*'
  prefs: []
  type: TYPE_NORMAL
- en: For the model optimization we use `AdamW` with a cosine learning rate schedule
    after a linear warming-up period. For the hyperparameters, we closely follow the
    parameters described in the GPT-3 paper for similar-sized models.^([8](ch10.xhtml#idm46238687425568))
  prefs: []
  type: TYPE_NORMAL
- en: '*Evaluation*'
  prefs: []
  type: TYPE_NORMAL
- en: We evaluate the model on the evaluation set every time we save—that is, every
    `save_checkpoint_steps` and after training. Along with the validation loss we
    also log the validation perplexity.
  prefs: []
  type: TYPE_NORMAL
- en: '*Gradient accumulation and checkpointing*'
  prefs: []
  type: TYPE_NORMAL
- en: The required batch sizes don’t fit in a GPU’s memory, even when we run on the
    latest GPUs. Therefore, we implement gradient accumulation, which gathers gradients
    over several backward passes and optimizes once enough gradients are accumulated.
    In [Chapter 6](ch06.xhtml#chapter_summarization), we saw how we can do this with
    the `Trainer`. For the large model, even a single batch does not quite fit on
    a single GPU. Using a method called *gradient checkpointing* we can trade some
    of the memory footprint for an approximately 20% training slowdown.^([9](ch10.xhtml#idm46238687417424))
    This allows us to fit even the large model in a single GPU.
  prefs: []
  type: TYPE_NORMAL
- en: One aspect that might still be a bit obscure is what it means to train a model
    on multiple GPUs. There are several approaches to train models in a distributed
    fashion depending on the size of your model and volume of data. The approach utilized
    by ![nlpt_pin01](Images/nlpt_pin01.png) Accelerate is called [`DataDistributedParallelism`
    (DDP)](https://oreil.ly/m4iNm). The main advantage of this approach is that it
    allows you to train models faster with larger batch sizes that wouldn’t fit into
    any single GPU. The process is illustrated in [Figure 10-6](#ddp).
  prefs: []
  type: TYPE_NORMAL
- en: '![DDP](Images/nlpt_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. Illustration of the processing steps in DDP with four GPUs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s go through the pipeline step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: Each worker consists of a GPU. In ![nlpt_pin01](Images/nlpt_pin01.png) Accelerate,
    there is a dataloader running on the main process that prepares the batches of
    data and sends them to all the workers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each GPU receives a batch of data and calculates the loss and respective accumulated
    gradients from forward and backward passes with a local copy of the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The gradients from each node are averaged with a *reduce* pattern, and the averaged
    gradients are sent back to each worker.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The gradients are applied using the optimizer on each node individually. Although
    this might seem like redundant work, it avoids transferring copies of the large
    models between nodes. We’ll need to update the model at least once, and without
    this approach the other nodes would each need to wait until they’d received the
    updated version.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once all models are updated we start all over again, with the main worker preparing
    new batches.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This simple pattern allows us to train large models extremely fast by scaling
    up to the number of available GPUs without much additional logic. Sometimes, however,
    this is not enough. For example, if the model does not fit on a single GPU you
    might need more sophisticated [parallelism strategies](https://oreil.ly/3uhfq).
    Now that we have all the pieces needed for training, it’s time to launch a job!
    As you’ll see in the next section, this is quite simple to do.
  prefs: []
  type: TYPE_NORMAL
- en: The Training Run
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll save the training script in a file called *codeparrot_training.py* so
    that we can execute it on our training server. To make life even easier, we’ll
    add it along with a *requirements.txt* file containing all the required Python
    dependencies to the model repository on the [Hub](https://oreil.ly/ndqSB). Remember
    that the models on the Hub are essentially Git repositories so we can just clone
    the repository, add any files we want, and then push them back to the Hub. On
    the training server, we can then spin up training with the following handful of
    commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it—our model is now training! Note that `wandb login` will prompt
    you to authenticate with Weights & Biases for logging. The `accelerate config`
    command will guide you through setting up the infrastructure; you can see the
    settings used for this experiment in [Table 10-2](#codeparrot-config). We use
    an [`a2-megagpu-16g` instance](https://oreil.ly/ZJIG3) for all experiments, which
    is a workstation with 16 A100 GPUs with 40 GB of memory each.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-2\. Configuration used to train the CodeParrot models
  prefs: []
  type: TYPE_NORMAL
- en: '| Setting | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Compute environment? | multi-GPU |'
  prefs: []
  type: TYPE_TB
- en: '| How many machines? | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| DeepSpeed? | No |'
  prefs: []
  type: TYPE_TB
- en: '| How many processes? | 16 |'
  prefs: []
  type: TYPE_TB
- en: '| Use FP16? | Yes |'
  prefs: []
  type: TYPE_TB
- en: 'Running the training script with these settings on that infrastructure takes
    about 24 hours and 7 days for the small and large models, respectively. If you
    train your own custom model, make sure your code runs smoothly on smaller infrastructure
    in order to make sure that expensive long run goes smoothly as well. After the
    full training run completes successfully, you can merge the experiment branch
    on the Hub back into the main branch with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Naturally, *`RUN_NAME`* should be the name of the experiment branch on the Hub
    you would like to merge. Now that we have a trained model, let’s have a look at
    how we can investigate its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Results and Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After anxiously monitoring the logs for a week, you will probably see loss and
    perplexity curves that look like those shown in [Figure 10-7](#training_result).
    The training loss and validation perplexity go down continuously, and the loss
    curve looks almost linear on the log-log scale. We also see that the large model
    converges faster in terms of processed tokens, although the overall training takes
    longer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Training loss and validation perplexity.](Images/nlpt_1007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. Training loss and validation perplexity as a function of processed
    tokens for the small and large CodeParrot models
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So what can we do with our freshly baked language model, straight out of the
    GPU oven? Well, we can use it to write some code for us. There are two types of
    analyses we can conduct: qualitative and quantitative. In the former, we look
    at concrete examples and try to better understand in which cases the model succeeds
    and where it fails. In the latter case, we evaluate the model’s performance statistically
    on a large set of test cases. In this section we’ll explore how we can use our
    model. First we’ll have a look at a few examples, and then we’ll briefly discuss
    how we could evaluate the model systematically and more robustly. First, let’s
    wrap the small model in a pipeline and use it to continue some code inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use the generation pipeline to generate candidate completions from
    a given prompt. By default, the pipeline will generate code until a predefined
    maximum length, and the output could contain multiple functions or classes. So,
    to keep the outputs concise, we’ll implement a `first_block()` function that uses
    regular expressions to extract the first occurrence of a function or class. The
    `complete_code()` function below applies this logic to print out the completions
    generated by CodeParrot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s start with a simple example and have the model write a function for us
    that calculates the area of a rectangle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'That looks pretty good! Although not all the generations are correct, the right
    solution is in there. Now, can the model also solve a more complex task of extracting
    URLs from an HTML string? Let’s see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Although it didn’t quite get it right in the second attempt, the other three
    generations are correct. We can test the function on the Hugging Face home page:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that all the URLs starting with `https` are external pages, whereas
    the others are subpages of the main website. That’s exactly what we wanted. Finally,
    let’s load the large model and see if we can use it to translate a function from
    pure Python to NumPy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'That worked! Let’s see if we can also use the CodeParrot model to help us build
    a Scikit-learn model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: Although in the second attempt it tried to train an [extra-trees classifier](https://oreil.ly/40Uy7),
    it generated what we asked in the other cases.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.xhtml#chapter_generation) we explored a few metrics to measure
    the quality of generated text. Among these was the BLEU score, which is frequently
    used for that purpose. While this metric has limitations in general, it is particularly
    badly suited for our use case. The BLEU score measures the overlap of *n*-grams
    between the reference texts and the generated texts. When writing code we have
    a lot of freedom in terms of variables and classes, and the success of a program
    does not depend on the naming scheme as long as it is consistent. However, the
    BLEU score would punish a generation that deviates from the reference naming,
    which might in fact be almost impossible to predict (even for a human coder).
  prefs: []
  type: TYPE_NORMAL
- en: 'In software development there are much better and more reliable ways to measure
    the quality of code, such as unit tests. This is how all the OpenAI Codex models
    were evaluated: by running several code generations for coding tasks through a
    set of unit tests and calculating the fraction of generations that pass the tests.^([10](ch10.xhtml#idm46238686878640))
    For a proper performance measure we should apply the same evaluation regimen to
    our models but this is beyond the scope of this chapter. You can find details
    on how CodeParrot performs on the HumanEval benchmark in [the model’s accompanying
    blog post](https://oreil.ly/hKOP8).'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s take a step back for a moment and contemplate what we have achieved in
    this chapter. We set out to create a code autocomplete function for Python. First
    we built a custom, large-scale dataset suitable for pretraining a large language
    model. Then we created a custom tokenizer that is able to efficiently encode Python
    code with that dataset. Finally, with the help of ![nlpt_pin01](Images/nlpt_pin01.png)
    Accelerate we put everything together and wrote a training script to train small
    and large versions of a GPT-2 model from scratch on a multi-GPU infrastructure,
    in under two hundred lines of code. Investigating the model outputs, we saw that
    it can generate reasonable code continuations, and we discussed how the model
    could be systematically evaluated.
  prefs: []
  type: TYPE_NORMAL
- en: 'You now not only know how to fine-tune any of the many pretrained models on
    the Hub, but also how to pretrain a custom model from scratch when you have enough
    data and compute resources available. You are now prepared to tackle almost any
    NLP use case with transformers. So the question is: where to next? In the next
    and last chapter, we’ll have a look at where the field is currently moving and
    what new exciting applications and domains beyond NLP transformer models can tackle.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch10.xhtml#idm46238692182464-marker)) Y. Zhu et al., [“Aligning Books
    and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading
    Books”](https://arxiv.org/abs/1506.06724), (2015); J. Dodge et al., [“Documenting
    the English Colossal Clean Crawled Corpus”](https://arxiv.org/abs/2104.08758),
    (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch10.xhtml#idm46238692175664-marker)) J. Bandy and N. Vincent, [“Addressing
    *Documentation Debt* in Machine Learning Research: A Retrospective Datasheet for
    BookCorpus”](https://arxiv.org/abs/2105.05241), (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch10.xhtml#idm46238691930032-marker)) B. Hutchinson et al., [“Towards
    Accountability for Machine Learning Datasets: Practices from Software Engineering
    and Infrastructure”](https://arxiv.org/abs/2010.13561), (2020).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch10.xhtml#idm46238691921600-marker)) By comparison, GitHub Copilot supports
    over a dozen programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch10.xhtml#idm46238691899472-marker)) M.-A. Lachaux et al., [“Unsupervised
    Translation of Programming Languages”](https://arxiv.org/abs/2006.03511), (2020).
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch10.xhtml#idm46238691054768-marker)) L. Xue et al., [“⁠ByT5: Towards
    a Token-Free Future with Pre-Trained Byte-to-Byte Models”](https://arxiv.org/abs/2105.13626),
    (2021).'
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch10.xhtml#idm46238691010992-marker)) P. Gage, “A New Algorithm for Data
    Compression,” *The C Users Journal* 12, no. 2 (1994): 23–38, [*https://dx.doi.org/10.14569/IJACSA.2012.030803*](https://dx.doi.org/10.14569/IJACSA.2012.030803).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch10.xhtml#idm46238687425568-marker)) T. Brown et al., [“Language Models
    Are Few-Shot Learners”](https://arxiv.org/abs/2005.14165), (2020).
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch10.xhtml#idm46238687417424-marker)) You can read more about gradient
    checkpointing on OpenAI’s [release post](https://oreil.ly/94oj1).
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch10.xhtml#idm46238686878640-marker)) M. Chen et al., [“Evaluating Large
    Language Models Trained on Code”](https://arxiv.org/abs/2107.03374), (2021).
  prefs: []
  type: TYPE_NORMAL
