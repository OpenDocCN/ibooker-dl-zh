- en: Chapter 10\. Training Transformers from Scratch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。从头开始训练变换器
- en: In the opening paragraph of this book, we mentioned a sophisticated application
    called GitHub Copilot that uses GPT-like transformers to perform code autocompletion,
    a feature that is particularly useful when programming in a new language or framework
    or learning to code, or for automatically producing boilerplate code. Other products
    that use AI models for this purpose include [TabNine](https://tabnine.com) and
    [Kite](https://kite.com). Later, in [Chapter 5](ch05.xhtml#chapter_generation),
    we had a closer look at how we can use GPT models to generate high-quality text.
    In this chapter, we’ll close the circle and build our very own GPT-like model
    for generating Python source code! We call the resulting model *CodeParrot*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的开头段落中，我们提到了一个名为GitHub Copilot的复杂应用，它使用类似GPT的变换器来执行代码自动完成，这个功能在使用新语言或框架编程、学习编程或自动生成样板代码时特别有用。其他使用AI模型进行此类用途的产品包括[TabNine](https://tabnine.com)和[Kite](https://kite.com)。稍后，在[第5章](ch05.xhtml#chapter_generation)中，我们更仔细地研究了如何使用GPT模型生成高质量文本。在本章中，我们将闭环并构建我们自己的类似GPT的模型来生成Python源代码！我们称结果模型为*CodeParrot*。
- en: So far we’ve mostly worked on data-constrained applications where the amount
    of labeled training data is limited. In these cases, transfer learning helped
    us build performant models. We took transfer learning to the limit in [Chapter 9](ch09.xhtml#chapter_fewlabels),
    where we barely used any training data at all.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们主要在数据受限的应用程序上工作，其中有限的标记训练数据量。在这些情况下，迁移学习帮助我们构建了高性能模型。我们在[第9章](ch09.xhtml#chapter_fewlabels)中将迁移学习推向了极限，几乎没有使用任何训练数据。
- en: 'In this chapter we’ll move to the other extreme and look at what we can do
    when we are drowning in all the data we could possibly want. We’ll explore the
    pretraining step itself and learn how to train a transformer from scratch. In
    working through this problem, we’ll look at some aspects of training that we have
    not considered yet, such as the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将转向另一个极端，看看当我们淹没在可能想要的所有数据时我们可以做些什么。我们将探讨预训练步骤本身，并学习如何从头开始训练一个变换器。在解决这个问题的过程中，我们将看一些我们尚未考虑过的训练方面，比如以下内容：
- en: Gathering and processing a very large dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集和处理非常大的数据集
- en: Creating a custom tokenizer for our dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为我们的数据集创建自定义分词器
- en: Training a model on multiple GPUs at scale
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在多个GPU上规模化训练模型
- en: To efficiently train large models with billions of parameters, we’ll need special
    tools for distributed training. Although the `Trainer` from ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers supports distributed training, we’ll take the opportunity to showcase
    a powerful PyTorch library called ![nlpt_pin01](Images/nlpt_pin01.png) Accelerate.
    We’ll end up touching on some of the largest NLP models in use today—but first,
    we need to find a sufficiently large dataset.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地训练拥有数十亿参数的大型模型，我们将需要分布式训练的特殊工具。虽然![nlpt_pin01](Images/nlpt_pin01.png) Transformers的`Trainer`支持分布式训练，但我们将借此机会展示一个名为![nlpt_pin01](Images/nlpt_pin01.png)
    Accelerate的强大PyTorch库。我们最终将涉及一些当今最大的NLP模型，但首先，我们需要找到一个足够大的数据集。
- en: Warning
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Unlike the code in the others in this book (which can be run with a Jupyter
    notebook on a single GPU), the training code in this chapter is designed to be
    run as a script with multiple GPUs. If you want to train your own version of CodeParrot,
    we recommend running the script provided in the ![nlpt_pin01](Images/nlpt_pin01.png)
    [Transformers repository](https://oreil.ly/ZyPPR).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 与本书中其他章节中可以在单个GPU上的Jupyter笔记本上运行的代码不同，本章中的训练代码设计为在多个GPU上作为脚本运行。如果您想训练自己的CodeParrot版本，我们建议运行提供在![nlpt_pin01](Images/nlpt_pin01.png)
    [Transformers repository](https://oreil.ly/ZyPPR)中的脚本。
- en: Large Datasets and Where to Find Them
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大型数据集及其获取方式
- en: There are many domains where you may actually have a large amount of data at
    hand, ranging from legal documents to biomedical datasets to programming codebases.
    In most cases, these datasets are unlabeled, and their large size means that they
    can usually only be labeled through the use of heuristics, or by using accompanying
    metadata that is stored during the gathering process.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多领域，您可能实际上手头有大量数据，从法律文件到生物医学数据集再到编程代码库。在大多数情况下，这些数据集是无标签的，它们的大尺寸意味着通常只能通过使用启发式方法进行标记，或者通过在收集过程中存储的附带元数据来进行标记。
- en: Nevertheless, a very large corpus can be useful even when it is unlabeled or
    only heuristically labeled. We saw an example of this in [Chapter 9](ch09.xhtml#chapter_fewlabels),
    where we used the unlabeled part of a dataset to fine-tune a language model for
    domain adaptation. This approach typically yields a performance gain when limited
    data is available. The decision to train from scratch rather than fine-tune an
    existing model is mostly dictated by the size of your fine-tuning corpus and the
    domain differences between the available pretrained models and the corpus.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，即使是无标签或仅启发式标记的非常大语料库也可能是有用的。我们在[第9章](ch09.xhtml#chapter_fewlabels)中看到了一个例子，我们使用数据集的无标签部分来微调语言模型以进行领域适应。当有限的数据可用时，这种方法通常会带来性能提升。从头开始训练而不是微调现有模型的决定，主要取决于微调语料库的大小以及可用预训练模型与语料库之间的领域差异。
- en: Using a pretrained model forces you to use the model’s corresponding tokenizer,
    but using a tokenizer that is trained on a corpus from another domain is typically
    suboptimal. For example, using GPT’s pretrained tokenizer on legal documents,
    other languages, or even completely different sequences such as musical notes
    or DNA sequences will result in poor tokenization (as we will see shortly).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 使用预训练模型会强制您使用模型对应的分词器，但是使用在另一个领域语料库上训练的分词器通常是次优的。例如，使用GPT的预训练分词器处理法律文件、其他语言，甚至完全不同的序列，比如音乐音符或DNA序列，将导致较差的分词（我们很快就会看到）。
- en: As the amount of training data you have access to gets closer to the amount
    of data used for pretraining, it thus becomes interesting to consider training
    the model and the tokenizer from scratch, provided the necessary computational
    resources are available. Before we discuss the different pretraining objectives
    further, we first need to build a large corpus suitable for pretraining. Building
    such a corpus comes with its own set of challenges, which we’ll explore in the
    next section.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of Building a Large-Scale Corpus
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The quality of a model after pretraining largely reflects the quality of the
    pretraining corpus. In particular, the model will inherit any defects in the pretraining
    corpus. Thus, before we attempt to create one of our own it’s good to be aware
    of some of the common issues and challenges that are associated with building
    large corpora for pretraining.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: As the dataset gets larger and larger, the chances that you can fully control—or
    at least have a precise idea of—what is inside it diminish. A very large dataset
    will most likely not have been assembled by dedicated creators that craft one
    example at a time, while being aware and knowledgeable of the full pipeline and
    the task that the machine learning model will be applied to. Instead, it is much
    more likely that a very large dataset will have been created in an automatic or
    semiautomatic way by collecting data that is generated as a side effect of other
    activities. For instance, it may consist of all the documents (e.g., contracts,
    purchase orders, etc.) that a company stores, logs from user activities, or data
    gathered from the internet.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: There are several important consequences that follow from the fact that large-scale
    datasets are mostly created with a high degree of automation. An important consideration
    is that there is limited control over both their content and the way they are
    created, and thus the risk of training a model on biased and lower-quality data
    increases. Recent investigations of famous large-scale datasets like BookCorpus
    and C4, which were used to train BERT and T5, respectively, have uncovered (among
    other things) that:^([1](ch10.xhtml#idm46238692182464))
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: A significant proportion of the C4 corpus is machine-translated rather than
    translated by humans.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disparate erasure of African-American English as a result of stopword filtering
    in C4 has resulted in an underrepresentation of such content.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is typically difficult in a large text corpus to find a middle ground between
    including (often too much) sexually or other explicit content and totally erasing
    all mention of sexuality or gender. As a surprising consequence of this, a rather
    common word like “sex” (which can have both neutral and explicit meanings) is
    completely unknown to a tokenizer that is trained on C4, since this word is fully
    absent from the corpus.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many occurrences of copyright violation in BookCorpus, and probably
    in other large-scale datasets as well.^([2](ch10.xhtml#idm46238692175664))
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is genre skew toward “romance” novels in BookCorpus.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These discoveries might not be incompatible with downstream usage of the models
    trained on these corpora. For instance, the strong overrepresentation of romance
    novels in BookCorpus is probably acceptable if the model is intended to be used
    as a romance novel writing tool or for a building a game.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s illustrate the notion of a model being skewed by the data by comparing
    text generations from GPT and GPT-2\. GPT was mostly trained on BookCorpus, while
    GPT-2 was trained on web pages, blogs, and news articles linked from Reddit. We’ll
    compare similar-sized versions of both models on the same prompt, so that the
    main difference is the pretraining dataset, and we’ll use the `text-generation`
    pipeline to investigate the model outputs:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, let’s create a simple function to count the number of parameters in each
    model:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The original GPT model is about the same size as the smallest GPT-2 model.
    Now we can generate three different completions from each model, each with the
    same input prompt:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型的原始版本与最小的GPT-2模型大小相当。现在我们可以从每个模型生成三个不同的完成，每个完成都有相同的输入提示：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: By just sampling a handful of outputs from both models we can already see the
    distinctive “romance” skew in GPT generation, which will typically imagine a dialogue
    with a romantic interaction between a woman and a man. On the other hand, GPT-2
    was trained on webtext linked to and from Reddit articles and mostly adopts a
    neutral “they” in its generations, which contain “blog-like” or adventure-related
    elements.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对两个模型的少量输出进行抽样，我们已经可以看到GPT生成中独特的“浪漫”倾向，通常会想象一个女人和一个男人之间的浪漫互动对话。另一方面，GPT-2是在与Reddit文章相关的webtext上进行训练的，它在生成中大多采用中性的“they”，其中包含“类似博客”的或冒险相关的元素。
- en: In general, any model trained on a dataset will reflect the language bias and
    over- or underrepresentation of populations and events in its training data. These
    biases in the behavior of the model are important to take into consideration with
    regard to the target audience interacting with the model; for some useful guidelines,
    we refer you to a paper by Google that provides a framework for dataset development.^([3](ch10.xhtml#idm46238691930032))
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，对数据集进行训练的任何模型都会反映出语言偏见和人口和事件在其训练数据中的过度或不足代表性。模型行为中的这些偏见对于考虑与模型交互的目标受众是很重要的；对于一些有用的指南，我们建议您参考Google的一篇论文，该论文提供了数据集开发的框架。^([3](ch10.xhtml#idm46238691930032))
- en: This brief introduction should give you an idea of the difficult challenges
    you face when creating large text corpora. With these in mind, let’s now take
    a look at creating our own dataset!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简短的介绍应该让你了解到在创建大型文本语料库时所面临的困难挑战。有了这些想法，现在让我们来看看如何创建我们自己的数据集！
- en: Building a Custom Code Dataset
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建自定义代码数据集
- en: 'To simplify the task a bit, we’ll focus on building a code generation model
    for the Python programming language only.^([4](ch10.xhtml#idm46238691921600))
    The first thing we’ll need is a large pretraining corpus consisting of Python
    source code. Fortunately, there is a natural resource that every software engineer
    knows: GitHub! The famous code-sharing website hosts terabytes of code repositories
    that are openly accessible and can be downloaded and used according to their respective
    licenses. At the time of this book’s writing, GitHub hosts more than 20 million
    code repositories. Many of them are small or test repositories created by users
    for learning, future side projects, or testing purposes.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化任务，我们将专注于仅针对Python编程语言构建代码生成模型。^([4](ch10.xhtml#idm46238691921600))我们需要的第一件事是一个由Python源代码组成的大型预训练语料库。幸运的是，有一个每个软件工程师都知道的自然资源：GitHub！这个著名的代码共享网站托管着数千兆字节的代码存储库，这些存储库是公开可访问的，并且可以根据其各自的许可证进行下载和使用。在撰写本书时，GitHub托管了超过2000万个代码存储库。其中许多是由用户创建的小型或测试存储库，用于学习、未来的副业项目或测试目的。
- en: 'GitHub repositories can be accessed in two main ways:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub存储库可以通过两种主要方式访问：
- en: Via the [GitHub REST API](https://oreil.ly/brhxw), like we saw in [Chapter 9](ch09.xhtml#chapter_fewlabels)
    when we downloaded all the GitHub issues of the ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers repository
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过[GitHub REST API](https://oreil.ly/brhxw)，就像我们在[第9章](ch09.xhtml#chapter_fewlabels)中看到的那样，当我们下载了![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers存储库的所有GitHub问题时
- en: Via public dataset inventories like [Google BigQuery](https://oreil.ly/dYsVT)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过像[Google BigQuery](https://oreil.ly/dYsVT)这样的公共数据集清单
- en: Since the REST API is rate limited and we need a lot data for our pretraining
    corpus, we’ll use Google BigQuery to extract all the Python repositories. The
    `bigquery-public-data.github_repos.contents` table contains copies of all ASCII
    files that are less than 10 MB in size. Projects also need to be open source to
    be included, as determined by [GitHub’s License API](https://oreil.ly/N9zHb).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 由于REST API受速率限制，我们需要大量数据来进行预训练，因此我们将使用Google BigQuery来提取所有Python存储库。`bigquery-public-data.github_repos.contents`表包含所有小于10MB的ASCII文件的副本。项目还需要是开源的，才能被包括在内，这是由[GitHub的许可证API](https://oreil.ly/N9zHb)确定的。
- en: Tip
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The Google BigQuery dataset doesn’t contain star or downstream usage information.
    For those attributes, we can use the GitHub REST API or a service like [Libraries.io](https://libraries.io)
    that monitors open source packages. Indeed, a team from GitHub recently released
    a dataset called [CodeSearchNet](https://oreil.ly/daE43) that filtered repositories
    used in at least one downstream task using information from Libraries.io.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Google BigQuery数据集不包含星标或下游使用信息。对于这些属性，我们可以使用GitHub REST API或像[Libraries.io](https://libraries.io)这样的服务，它监视开源软件包。事实上，GitHub的一个团队最近发布了一个名为[CodeSearchNet](https://oreil.ly/daE43)的数据集，该数据集使用了来自Libraries.io的信息，过滤了至少在一个下游任务中使用的存储库。
- en: Let’s have a look at what it takes to create our code dataset with Google BigQuery.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用Google BigQuery创建我们的代码数据集。
- en: Creating a dataset with Google BigQuery
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Google BigQuery创建数据集
- en: We’ll begin by extracting all the Python files in GitHub public repositories
    from the snapshot on Google BigQuery. For the sake of reproducibility and in case
    the policy around free usage of BigQuery changes in the future, we will also share
    this dataset on the Hugging Face Hub. The steps to export these files are adapted
    from the [TransCoder implementation](https://oreil.ly/vih2m) and are as follows:^([5](ch10.xhtml#idm46238691899472))
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从Google BigQuery的快照中提取GitHub公共存储库中的所有Python文件。为了便于重现，并且以防BigQuery的免费使用政策在未来发生变化，我们还将在Hugging
    Face Hub上分享这个数据集。导出这些文件的步骤是从[TransCoder实现](https://oreil.ly/vih2m)中改编的，如下所示：^([5](ch10.xhtml#idm46238691899472))
- en: Create a Google Cloud account (a free trial should be sufficient).
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个Google Cloud账户（免费试用应该足够）。
- en: Create a Google BigQuery project under your account.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的账户下创建一个Google BigQuery项目。
- en: In this project, create a dataset.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个项目中，创建一个数据集。
- en: In this dataset, create a table where the results of the SQL request will be
    stored.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个数据集中，创建一个表，用来存储SQL请求的结果。
- en: 'Prepare and run the following SQL query on the `github_repos` (to save the
    query results, select More > Query Options, check the “Set a destination table
    for query results” box, and specify the table name):'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This command processes about 2.6 TB of data to extract 26.8 million files. The
    result is a dataset of about 50 GB of compressed JSON files, each containing the
    source code of Python files. We filtered to remove empty files and small files
    such as *__init__.py* that don’t contain much useful information. We also filtered
    out files larger than 1 MB, and we downloaded the licenses for all the files so
    we can filter the training data based on licenses if we want later on.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll download the results to our local machine. If you try this at home,
    make sure you have good bandwidth available and at least 50 GB of free disk space.
    The easiest way to get the resulting table to your local machine is to follow
    this two-step process:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'Export your results to Google Cloud:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a bucket and a folder in Google Cloud Storage (GCS).
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Export your table to this bucket by selecting Export > Export to GCS, with an
    export format of JSON and gzip compression.
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To download the bucket to your machine, use the [`gsutil` library](https://oreil.ly/JzgRk):'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install `gsutil` with `pip install gsutil`.
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Configure `gsutil` with your Google account: `gsutil config`.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Copy your bucket on your machine:'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Alternatively, you can directly download the dataset from the Hugging Face
    Hub with the following command:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Working with a 50 GB dataset can be challenging; it requires sufficient disk
    space, and one must be careful not to run out of RAM. In the following section,
    we’ll have a look how ![nlpt_pin01](Images/nlpt_pin01.png) Datasets helps deal
    with these constraints of working with large datasets on small machines.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: Working with Large Datasets
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Loading a very large dataset is often a challenging task, in particular when
    the data is larger than your machine’s RAM. For a large-scale pretraining dataset,
    this is a very common situation. In our example, we have 50 GB of compressed data
    and about 200 GB of uncompressed data, which is difficult to extract and load
    into the RAM memory of a standard-sized laptop or desktop computer.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'Thankfully, ![nlpt_pin01](Images/nlpt_pin01.png) Datasets has been designed
    from the ground up to overcome this problem with two specific features that allow
    you to set yourself free from RAM and hard drive space limitations: memory mapping
    and streaming.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Memory mapping
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To overcome RAM limitations, ![nlpt_pin01](Images/nlpt_pin01.png) Datasets uses
    a mechanism for zero-copy and zero-overhead memory mapping that is activated by
    default. Basically, each dataset is cached on the drive in a file that is a direct
    reflection of the content in RAM memory. Instead of loading the dataset in RAM,
    ![nlpt_pin01](Images/nlpt_pin01.png) Datasets opens a read-only pointer to this
    file and uses it as a substitute for RAM, basically using the hard drive as a
    direct extension of the RAM memory.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to now we have mostly used ![nlpt_pin01](Images/nlpt_pin01.png) Datasets
    to access remote datasets on the Hugging Face Hub. Here, we will directly load
    our 50 GB of compressed JSON files that we have stored locally in the `codeparrot`
    repository. Since the JSON files are compressed, we first need to decompress them,
    which ![nlpt_pin01](Images/nlpt_pin01.png) Datasets takes care of for us. Be careful,
    because this requires about 180 GB of free disk space! However, it will use almost
    no RAM. By setting `delete_extracted=True` in the dataset’s downloading configuration,
    we can make sure that we delete all the files we don’t need anymore as soon as
    possible:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Under the hood, ![nlpt_pin01](Images/nlpt_pin01.png) Datasets extracted and
    read all the compressed JSON files by loading them in a single optimized cache
    file. Let’s see how big this dataset is once loaded:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we can see, the dataset is much larger than our typical RAM memory, but we
    can still load and access it, and we’re actually using a very limited amount of
    memory.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，数据集比我们典型的RAM内存要大得多，但我们仍然可以加载和访问它，而且实际上只使用了非常有限的内存。
- en: You may wonder if this will make our training I/O-bound. In practice, NLP data
    is usually very lightweight to load in comparison to the model processing computations,
    so this is rarely an issue. In addition, the zero-copy/zero-overhead format uses
    Apache Arrow under the hood, which makes it very efficient to access any element.
    Depending on the speed of your hard drive and the batch size, iterating over the
    dataset can typically be done at a rate of a few tenths of a GB/s to several GB/s.
    This is great, but what if you can’t free enough disk space to store the full
    dataset locally? Everybody knows the feeling of helplessness when you get a full
    disk warning and need to painfully try to reclaim a few GB by looking for hidden
    files to delete. Luckily, you don’t need to store the full dataset locally if
    you use the streaming feature of ![nlpt_pin01](Images/nlpt_pin01.png) Datasets!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想知道这是否会使我们的训练受到I/O限制。实际上，与模型处理计算相比，NLP数据通常非常轻量级，因此这很少是一个问题。此外，零拷贝/零开销格式在内部使用Apache
    Arrow，这使得访问任何元素非常高效。根据硬盘驱动器的速度和批处理大小，通常可以以每秒几分之一的GB到几GB的速度迭代整个数据集。这很棒，但如果您无法释放足够的磁盘空间来本地存储完整的数据集怎么办？每个人都知道当收到磁盘已满警告并且需要痛苦地尝试删除一些隐藏文件来释放几GB时的无助感。幸运的是，如果您使用![nlpt_pin01](Images/nlpt_pin01.png)数据集的流式传输功能，您无需在本地存储完整的数据集！
- en: Streaming
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流式传输
- en: 'Some datasets (reaching up to 1 TB or more) will be difficult to fit even on
    a standard hard drive. In this case, an alternative to scaling up the server you
    are using is to *stream* the dataset. This is also possible with ![nlpt_pin01](Images/nlpt_pin01.png)
    Datasets for a number of compressed or uncompressed file formats that can be read
    line by line, like JSON Lines, CSV, or text (either raw or zip, gzip, or zstandard
    compressed). Let’s load our dataset directly from the compressed JSON files instead
    of creating a cache file from them:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据集（甚至达到1TB或更多）甚至难以适应标准硬盘。在这种情况下，除了扩大您正在使用的服务器之外，还可以*流式传输*数据集。这也适用于![nlpt_pin01](Images/nlpt_pin01.png)数据集，适用于一些可以逐行读取的压缩或未压缩文件格式，如JSON
    Lines、CSV或文本（原始或zip、gzip或zstandard压缩）。让我们直接从压缩的JSON文件中加载数据集，而不是从中创建缓存文件：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As you’ll see, loading the dataset is instantaneous! In streaming mode, the
    compressed JSON files will be opened and read on the fly. Our dataset is now an
    `IterableDataset` object. This means that we cannot access random elements of
    it, like `streamed_dataset[1264]`, but we need to read it in order, for instance
    with `next(iter(streamed_dataset))`. It’s still possible to use methods like `shuffle()`,
    but these will operate by fetching a buffer of examples and shuffling within this
    buffer (the size of the buffer is adjustable). When several files are provided
    as raw files (like our 184 files here), `shuffle()` will also randomize the order
    of files for the iteration.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将看到的，加载数据集是瞬间完成的！在流式模式下，压缩的JSON文件将被动态打开和读取。我们的数据集现在是一个`IterableDataset`对象。这意味着我们无法访问它的随机元素，比如`streamed_dataset[1264]`，但我们需要按顺序读取它，例如使用`next(iter(streamed_dataset))`。仍然可以使用`shuffle()`等方法，但这些方法将通过获取一定数量的示例并在此缓冲区内进行洗牌来操作（缓冲区的大小是可调整的）。当提供多个文件作为原始文件时（比如我们这里的184个文件），`shuffle()`还将随机化迭代的文件顺序。
- en: 'The samples of a streamed dataset are identical to the samples of a nonstreamed
    dataset, as we can see:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 流式数据集的样本与非流式数据集的样本相同，我们可以看到：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The main interest of using a streaming dataset is that loading this dataset
    will not create a cache file on the drive or require any (significant) RAM memory.
    The original raw files are extracted and read on the fly when a new batch of examples
    is requested, and only that batch is loaded in memory. This reduces the memory
    footprint of our dataset from 180 GB to 50 GB. But we can take this one step further—instead
    of pointing to the local dataset we can reference the dataset on the Hub, and
    then directly download samples without downloading the raw files locally:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用流式数据集的主要优点是，加载此数据集不会在驱动器上创建缓存文件，也不需要任何（显著的）RAM内存。当请求新的示例批次时，原始原始文件将被提取并动态读取，只有该批次被加载到内存中。这将我们数据集的内存占用从180GB减少到50GB。但我们可以再进一步——而不是指向本地数据集，我们可以引用Hub上的数据集，然后直接下载样本而不在本地下载原始文件：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This dataset behaves exactly like the previous one, but behind the scenes downloads
    the examples on the fly. With such a setup, we can then use arbitrarily large
    datasets on an (almost) arbitrarily small server. Let’s push our dataset with
    a train and validation split to the Hugging Face Hub and access it with streaming.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集的行为与之前的数据集完全相同，但在幕后动态下载示例。有了这样的设置，我们可以在（几乎）任意小的服务器上使用任意大的数据集。让我们将我们的数据集与训练和验证拆分一起推送到Hugging
    Face Hub，并使用流式访问。
- en: Adding Datasets to the Hugging Face Hub
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据集添加到Hugging Face Hub
- en: 'Pushing our dataset to the Hugging Face Hub will allow us to:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据集推送到Hugging Face Hub将允许我们：
- en: Easily access it from our training server.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轻松从我们的训练服务器访问它。
- en: See how streaming datasets work seamlessly with datasets from the Hub.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看看流式数据集如何与Hub上的数据集无缝配合。
- en: Share it with the community, including you, dear reader!
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与社区分享，包括您，亲爱的读者！
- en: 'To upload the dataset, we first need to log in to our Hugging Face account
    by running the following command in the terminal and providing the relevant credentials:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要上传数据集，我们首先需要通过在终端中运行以下命令并提供相关凭据来登录我们的Hugging Face帐户：
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This is equivalent to the `notebook_login()` helper function we used in previous
    chapters. Once this is done, we can directly create a new dataset on the Hub and
    upload the compressed JSON files. To simplify things, we will create two repositories:
    one for the train split and one for the validation split. We can do this by running
    the `repo create` command of the CLI as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于我们在之前章节中使用的`notebook_login()`辅助函数。完成后，我们可以直接在Hub上创建一个新数据集并上传压缩的JSON文件。为简化起见，我们将创建两个存储库：一个用于训练拆分，一个用于验证拆分。我们可以通过运行CLI的`repo
    create`命令来实现这一点，如下所示：
- en: '[PRE16]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Here we’ve specified that the repository should be a dataset (in contrast to
    the model repositories used to store weights), along with the organization we’d
    like to store the repositories under. If you’re running this code under your personal
    account, you can omit the `--organization` flag. Next, we need to clone these
    empty repositories to our local machine, copy the JSON files to them, and push
    the changes to the Hub. We will take the last compressed JSON file out of the
    184 we have as the validation file (i.e., roughly 0.5 percent of our dataset).
    Execute these commands to clone the repository from the Hub to your local machine:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指定了存储库应该是一个数据集（与用于存储权重的模型存储库相对），以及我们想要存储存储库的组织。如果您在个人帐户下运行此代码，可以省略`--organization`标志。接下来，我们需要将这些空存储库克隆到我们的本地计算机，将JSON文件复制到其中，并将更改推送到Hub。我们将从我们拥有的184个文件中取出最后一个压缩的JSON文件作为验证文件（即我们数据集的大约0.5％）。执行以下命令将存储库从Hub克隆到本地计算机：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, copy all but the last GitHub file as the training set:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将除了最后一个GitHub文件之外的所有文件复制为训练集：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then commit the files and push them to the Hub:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后提交文件并将其推送到Hub：
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, repeat the process for the validation set:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为验证集重复此过程：
- en: '[PRE20]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The `git add .` step can take a couple of minutes since a hash of all the files
    is computed. Uploading all the files will also take a little while. Since this
    will enable us to use streaming later in the chapter, however, this is not lost
    time, and this step will allow us to go significantly faster in the rest of our
    experiments. Note that we added a `_validation` suffix to the validation filename.
    This will enable us to load it later as a validation split.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`git add .`步骤可能需要几分钟，因为需要计算所有文件的哈希值。上传所有文件也需要一些时间。然而，由于这将使我们能够在本章后面更快地进行流式处理，因此这不是浪费时间，这一步将使我们在实验的其余部分中显着加快速度。请注意，我们在验证文件名后添加了`_validation`后缀。这将使我们能够稍后将其加载为验证拆分。'
- en: 'And that’s it! Our two splits of the dataset as well as the full dataset are
    now live on the Hugging Face Hub at the following URLs:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们的数据集的两个拆分以及完整数据集现在都在Hugging Face Hub上以以下URL上线：
- en: '[*https://huggingface.co/datasets/transformersbook/codeparrot*](https://huggingface.co/datasets/transformersbook/codeparrot)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*https://huggingface.co/datasets/transformersbook/codeparrot*](https://huggingface.co/datasets/transformersbook/codeparrot)'
- en: '[*https://huggingface.co/datasets/transformersbook/codeparrot-train*](https://huggingface.co/datasets/transformersbook/codeparrot-train)'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*https://huggingface.co/datasets/transformersbook/codeparrot-train*](https://huggingface.co/datasets/transformersbook/codeparrot-train)'
- en: '[*https://huggingface.co/datasets/transformersbook/codeparrot-valid*](https://huggingface.co/datasets/transformersbook/codeparrot-valid)'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*https://huggingface.co/datasets/transformersbook/codeparrot-valid*](https://huggingface.co/datasets/transformersbook/codeparrot-valid)'
- en: Note
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: It’s good practice to add README cards that explain how the datasets were created
    and provide as much useful information about them as possible. A well-documented
    dataset is more likely to be useful to other people, as well as your future self.
    You can read the ![nlpt_pin01](Images/nlpt_pin01.png) [Datasets README guide](https://oreil.ly/Tv9bq)
    for a detailed description of how to write good dataset documentation. You can
    also use the web editor to modify your README cards directly on the Hub later.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 添加README卡是一个好习惯，解释数据集的创建方式，并尽可能提供有关它们的有用信息。文档完善的数据集更有可能对其他人有用，也对您未来的自己有用。您可以阅读![nlpt_pin01](Images/nlpt_pin01.png)
    [数据集README指南](https://oreil.ly/Tv9bq)以详细了解如何编写良好的数据集文档。您还可以使用Web编辑器直接在Hub上修改README卡。
- en: Building a Tokenizer
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建分词器
- en: Now that we have gathered and loaded our large dataset, let’s see how we can
    efficiently process the data to feed to our model. In the previous chapters we’ve
    used tokenizers that accompanied the models we used. This made sense since these
    models were pretrained using data passed through a specific preprocessing pipeline
    defined in the tokenizer. When using a pretrained model, it’s important to stick
    with the same preprocessing design choices selected for pretraining. Otherwise
    the model may be fed out-of-distribution patterns or unknown tokens.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经收集并加载了大型数据集，让我们看看如何有效地处理数据以供模型使用。在之前的章节中，我们使用了伴随我们使用的模型的分词器。这是有道理的，因为这些模型是使用通过分词器定义的特定预处理流程传递的数据进行预训练的。在使用预训练模型时，重要的是坚持选择用于预训练的相同预处理设计选择。否则，模型可能会受到超出分布模式或未知标记的影响。
- en: 'However, when we train a new model, using a tokenizer prepared for another
    dataset can be suboptimal. Here are a few examples of the kinds of problems we
    might run into when using an existing tokenizer:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们训练一个新模型时，使用为另一个数据集准备的分词器可能是次优的。以下是使用现有分词器时可能遇到的问题的一些示例：
- en: The T5 tokenizer was trained on the [C4](https://oreil.ly/wsYIC) corpus that
    we encountered earlier, but an extensive step of stopword filtering was used to
    create it. As a result, the T5 tokenizer has never seen common English words such
    as “sex.”
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T5分词器是在我们之前遇到的[C4](https://oreil.ly/wsYIC)语料库上进行训练的，但是使用了大量的停用词过滤步骤来创建它。因此，T5分词器从未见过诸如“sex”之类的常见英语单词。
- en: The CamemBERT tokenizer was also trained on a very large corpus of text, but
    only comprising French text (the French subset of the [OSCAR](https://oreil.ly/hgO5J)
    corpus). As such, it is unaware of common English words such “being.”
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CamemBERT分词器也是在一个非常大的文本语料库上进行训练的，但只包括法语文本（[OSCAR](https://oreil.ly/hgO5J)语料库的法语子集）。因此，它不知道诸如“being”之类的常见英语单词。
- en: 'We can easily test these features of each tokenizer in practice:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在实践中轻松测试每个分词器的这些特性：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: In many cases, splitting such short and common words into subparts will be inefficient,
    since this will increase the input sequence length of the model (which has limited
    context). Therefore, it’s important to be aware of the domain and preprocessing
    of the dataset that was used to train the tokenizer. The tokenizer and model can
    encode bias from the dataset that has an impact on the downstream behavior of
    the model. To create an optimal tokenizer for our dataset, we thus need to train
    one ourselves. Let’s see how this can be done.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，将这些短而常见的单词分割成子部分将是低效的，因为这将增加模型的输入序列长度（其上下文有限）。因此，重要的是要了解用于训练分词器的数据集的领域和预处理。分词器和模型可能会从数据集中编码偏见，这会影响模型的下游行为。因此，为我们的数据集创建一个最佳的分词器，我们需要自己训练一个。让我们看看如何做到这一点。
- en: Note
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Training a model involves starting from a given set of weights and using backpropagation
    from an error signal on a designed objective to minimize the loss of the model
    and find an optimal set of weights for the model to perform the task defined by
    the training objective. Training a tokenizer, on the other hand, does *not* involve
    backpropagation or weights. It is a way to create an optimal mapping from a string
    of text to a list of integers that can be ingested by the model. In today’s tokenizers,
    the optimal string-to-integer conversion involves a vocabulary consisting of a
    list of atomic strings and an associated method to convert, normalize, cut, or
    map a text string into a list of indices with this vocabulary. This list of indices
    is then the input for our neural network.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型涉及从给定的权重集开始，并使用反向传播从设计的目标上的误差信号来最小化模型的损失，并找到模型执行训练目标定义的任务的最佳权重集。另一方面，训练分词器*不*涉及反向传播或权重。这是一种从文本字符串到可以被模型摄取的整数列表的最佳映射方式。在今天的分词器中，最佳的字符串到整数转换涉及一个由原子字符串列表组成的词汇表，以及一个将文本字符串转换、规范化、切割或映射为具有这个词汇表的索引列表的方法。然后，这个索引列表是我们神经网络的输入。
- en: The Tokenizer Model
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分词器模型
- en: 'As you saw in [Chapter 4](ch04.xhtml#chapter_ner), the tokenizer is a processing
    pipeline consisting of four steps: normalization, pretokenization, the tokenizer
    model, and postprocessing. The part of the tokenizer pipeline that can be trained
    on data is the tokenizer model. As we discussed in [Chapter 2](ch02.xhtml#chapter_classification),
    there are several subword tokenization algorithms that can be used, such as BPE,
    WordPiece, and Unigram.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[第4章](ch04.xhtml#chapter_ner)中看到的，分词器是一个由四个步骤组成的处理管道：规范化、预分词、分词器模型和后处理。可以在数据上训练的分词器管道的部分是分词器模型。正如我们在[第2章](ch02.xhtml#chapter_classification)中讨论的，有几种可以使用的子词分词算法，例如BPE、WordPiece和Unigram。
- en: BPE starts from a list of basic units (single characters) and creates a vocabulary
    by a process of progressively creating new tokens formed by merging the most frequently
    co-occurring basic units and adding them to the vocabulary. This process is reiterated
    until a predefined vocabulary size is reached.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: BPE从基本单元（单个字符）的列表开始，并通过逐渐创建由最频繁共现的基本单元合并而成的新标记，并将其添加到词汇表中的过程来创建词汇表。这个过程重复进行，直到达到预定义的词汇表大小。
- en: Unigram starts from the other end, by initializing its base vocabulary with
    all the words in the corpus, and potential subwords. Then it progressively removes
    or splits the less useful tokens to obtain a smaller and smaller vocabulary, until
    the target vocabulary size is reached. WordPiece is a predecessor of Unigram,
    and its official implementation was never open-sourced by Google.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Unigram从另一端开始，通过将其基本词汇初始化为语料库中的所有单词和潜在的子词。然后，它逐渐删除或拆分不太有用的标记，直到达到目标词汇大小为止。WordPiece是Unigram的前身，其官方实现从未被谷歌开源。
- en: The impact of these various algorithms on downstream performance varies depending
    on the task, and overall it’s quite difficult to identify if one algorithm is
    clearly superior to the others. Both BPE and Unigram have reasonable performance
    in most cases, but let’s have a look at some aspects to consider when evaluating.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同算法对下游性能的影响因任务而异，总体上很难确定哪种算法明显优于其他算法。在大多数情况下，BPE和Unigram的性能都是合理的，但让我们来看看在评估时需要考虑的一些方面。
- en: Measuring Tokenizer Performance
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 衡量分词器性能
- en: 'The optimality and performance of a tokenizer are challenging to measure in
    practice. Some possible metrics include:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器的最优性和性能在实践中很难衡量。一些可能的指标包括：
- en: '*Subword fertility*, which calculates the average number of subwords produced
    per tokenized word'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*子词多样性*，计算每个被分词单词产生的平均子词数量'
- en: '*Proportion of continued words*, which refers to the proportion of tokenized
    words in a corpus that are split into at least two subtokens'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*继续单词的比例*，指的是语料库中被分词的单词中至少被分割成两个子标记的比例'
- en: '*Coverage metrics* like the proportion of unknown words or rarely used tokens
    in a tokenized corpus'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*覆盖度指标*，如被分词语料库中未知单词或少用标记的比例'
- en: In addition, robustness to misspelling or noise is often estimated, as well
    as model performance on such out-of-domain examples, as this strongly depends
    on the tokenization process.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对拼写错误或噪声的鲁棒性通常是估计的，以及模型在这些域外示例上的性能，因为这在很大程度上取决于分词过程。
- en: These measures give a set of different views on the tokenizer’s performance,
    but they tend to ignore the interaction of the tokenizer with the model. For example,
    subword fertility can be minimized by including all the possible words in the
    vocabulary, but this will produce a very large vocabulary for the model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这些度量给出了分词器性能的一组不同视图，但它们往往忽略了分词器与模型的交互。例如，通过在词汇表中包含所有可能的单词，可以最小化子词的多样性，但这将为模型产生一个非常大的词汇表。
- en: In the end, the performance of the various tokenization approaches is thus generally
    best estimated by using the downstream performance of the model as the ultimate
    metric. For instance, the good performance of early BPE approaches was demonstrated
    by showing improved performance on machine translation tasks by models trained
    using these tokenizers and vocabularies instead of character- or word-based tokenization.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，各种标记化方法的性能通常最好通过使用模型的下游性能作为最终指标来估计。例如，早期BPE方法的良好性能是通过展示使用这些标记器和词汇表训练的模型在机器翻译任务上的性能得到改善，而不是使用基于字符或单词的标记化。
- en: Let’s see how we can build our own tokenizer optimized for Python code.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何构建一个针对Python代码进行优化的自定义标记器。
- en: A Tokenizer for Python
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python的标记器
- en: 'We need a custom tokenizer for our use case: tokenizing Python code. The question
    of pretokenization merits some discussion for programming languages. If we split
    on whitespaces and remove them, we will lose all the indentation information,
    which in Python is important for the semantics of the program (just think about
    `while` loops, or `if-then-else` statements). On the other hand, line breaks are
    not meaningful and can be added or removed without impact on the semantics. Similarly,
    splitting on punctuation, like an underscore, which is used to compose a single
    variable name from several subparts, might not make as much sense as it would
    in natural language. Using a natural language pretokenizer for tokenizing code
    thus seems potentially suboptimal.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个针对我们的用例进行定制的标记器：对Python代码进行标记。对于编程语言，预标记化的问题值得讨论。如果我们在空格上分割并删除它们，我们将丢失所有缩进信息，而在Python中，这对程序的语义很重要（想想`while`循环或`if-then-else`语句）。另一方面，换行符并不具有意义，可以添加或删除而不会影响语义。同样，像下划线这样的标点符号分割，用于将多个子部分组成单个变量名，可能在自然语言中并不那么有意义。因此，使用自然语言预标记器来标记代码似乎可能不够理想。
- en: 'Let’s see if there are any tokenizers in the collection provided on the Hub
    that might be useful to us. We want a tokenizer that preserves spaces, so a good
    candidate could be a byte-level tokenizer like the one from GPT-2\. Let’s load
    this tokenizer and explore its tokenization properties:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Hub提供的集合中是否有任何对我们有用的标记器。我们需要一个保留空格的标记器，所以一个很好的选择可能是像GPT-2这样的字节级标记器。让我们加载这个标记器并探索它的标记化属性：
- en: '[PRE24]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Note
  id: totrans-143
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Python has a built-in `tokenize` module that splits Python code strings into
    meaningful units (code operation, comments, indent and dedent, etc.). One issue
    with using this approach is that this pretokenizer is Python-based and as such
    is typically rather slow and limited by the Python global interpreter lock (GIL).
    On the other hand, most of the tokenizers in the ![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers library are provided by the ![nlpt_pin01](Images/nlpt_pin01.png)
    Tokenizers library and are coded in Rust. The Rust tokenizers are many orders
    of magnitude faster to train and to use, and we will thus likely want to use them
    given the size of our corpus.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Python有一个内置的`tokenize`模块，它将Python代码字符串分割成有意义的单元（代码操作、注释、缩进和减少等）。使用这种方法的一个问题是，这个预标记器是基于Python的，因此通常速度较慢，并受到Python全局解释器锁（GIL）的限制。另一方面，![nlpt_pin01](Images/nlpt_pin01.png)
    Transformers库中的大多数标记器都是由![nlpt_pin01](Images/nlpt_pin01.png) Tokenizers库提供的，并且是用Rust编写的。Rust标记器的训练和使用速度要快得多，因此鉴于我们的语料库规模，我们可能会想要使用它们。
- en: 'This is quite a strange output, so let’s try to understand what is happening
    here by running the various submodules of the tokenizer’s pipeline. First let’s
    see what normalization is applied in this tokenizer:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当奇怪的输出，所以让我们尝试通过运行标记器管道的各个子模块来理解这里发生了什么。首先让我们看看在这个标记器中应用了什么规范化：
- en: '[PRE26]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As we can see, the GPT-2 tokenizer uses no normalization. It works directly
    on the raw Unicode inputs without any normalization steps. Let’s now take a look
    at the pretokenization:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，GPT-2标记器不使用任何规范化。它直接在原始Unicode输入上工作，没有任何规范化步骤。现在让我们来看一下预标记化：
- en: '[PRE28]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: What are all these `Ġ` symbols, and what are the numbers accompanying the tokens?
    Let’s explain both and see if we can understand better how this tokenizer works.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些`Ġ`符号是什么，伴随标记的数字又是什么？让我们解释一下，看看我们是否能更好地理解这个标记器是如何工作的。
- en: Let’s start with the numbers. ![nlpt_pin01](Images/nlpt_pin01.png) Tokenizers
    has a very useful feature for switching between strings and tokens, called *offset
    tracking*. All the operations on the input string are tracked so that it’s possible
    to know exactly what part of the input string a token after tokenization corresponds
    to. These numbers simply indicate where in the original string each token comes
    from; for instance, the word `'hello'` in the first line corresponds to the characters
    8 to 13 in the original string. If some characters are removed in a normalization
    step, we are thus still able to associate each token with the respective part
    in the original string.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从数字开始。Tokenizers具有一个非常有用的功能，可以在字符串和标记之间进行切换，称为*偏移跟踪*。对输入字符串的所有操作都被跟踪，因此可以准确知道标记化后每个标记对应于输入字符串的哪一部分。这些数字只是指示每个标记在原始字符串中的位置；例如，第一行中的单词`'hello'`对应于原始字符串中的字符8到13。如果在规范化步骤中删除了一些字符，我们仍然能够将每个标记与原始字符串中的相应部分关联起来。
- en: 'The other curious feature of the tokenized text is the odd-looking characters,
    such as `Ċ` and `Ġ`. *Byte-level* means that this tokenizer works on bytes instead
    of Unicode characters. Each Unicode character is composed of between 1 and 4 bytes,
    depending on the character. The nice thing about bytes is that while there are
    143,859 Unicode characters in the Unicode alphabet, there are only 256 elements
    in the byte alphabet, and you can express each Unicode character as a sequence
    of these bytes. If we work on bytes we can thus express all the strings composed
    from the UTF-8 world as longer strings in this alphabet of 256 values. That is,
    we can have a model using an alphabet of only 256 words and be able to process
    any Unicode string. Let’s have a look at what the byte representations of some
    characters look like:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 标记化文本的另一个奇特特征是看起来奇怪的字符，比如`Ċ`和`Ġ`。*字节级*意味着这个分词器是基于字节而不是Unicode字符工作的。每个Unicode字符由1到4个字节组成，具体取决于字符。字节的好处在于，虽然Unicode字母表中有143,859个Unicode字符，但字节字母表中只有256个元素，你可以用这些字节的序列来表示每个Unicode字符。如果我们使用字节，我们可以将UTF-8世界中的所有字符串表示为这个256个值的字母表中的更长的字符串。也就是说，我们可以有一个模型，使用只有256个词的字母表，并能够处理任何Unicode字符串。让我们看看一些字符的字节表示是什么样的：
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'At this point you might wonder: why work on a byte level? Think back to our
    discussion in [Chapter 2](ch02.xhtml#chapter_classification) about the trade-offs
    between character and word tokens. We could decide to build our vocabulary from
    the 143,859 Unicode characters, but we would also like to include words—i.e.,
    combinations of Unicode characters—in our vocabulary, so this (already very large)
    size is only a lower bound for the total size of the vocabulary. This will make
    our model’s embedding layer very large because it comprises one vector for each
    vocabulary token.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能会想：为什么要在字节级别上工作呢？回想一下我们在[第2章](ch02.xhtml#chapter_classification)中关于字符和单词标记之间的权衡的讨论。我们可以决定从143,859个Unicode字符中构建我们的词汇表，但我们也希望在我们的词汇表中包括单词——即Unicode字符的组合，因此这个（已经非常大的）大小只是词汇表总大小的一个下限。这将使我们模型的嵌入层非常大，因为它包括每个词汇标记的一个向量。
- en: On the other extreme, if we only use the 256 byte values as our vocabulary,
    the input sequences will be segmented in many small pieces (each byte constituting
    the Unicode characters), and as such our model will have to work on long inputs
    and spend significant compute power on reconstructing Unicode characters from
    their separate bytes, and then words from these characters. See the paper accompanying
    the ByT5 model release for a detailed study of this overhead.^([6](ch10.xhtml#idm46238691054768))
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们只使用256个字节值作为我们的词汇表，输入序列将被分割成许多小片段（每个字节构成Unicode字符），因此我们的模型将不得不处理长输入，并花费大量的计算资源来从单独的字节重构Unicode字符，然后从这些字符中重构单词。有关此开销的详细研究，请参阅伴随ByT5模型发布的论文。^([6](ch10.xhtml#idm46238691054768))
- en: A middle-ground solution is to construct a medium-sized vocabulary by extending
    the 256-word vocabulary with the most common combinations of bytes. This is the
    approach taken by the BPE algorithm. The idea is to progressively construct a
    vocabulary of a predefined size by creating new vocabulary tokens through iteratively
    merging the most frequently co-occurring pair of tokens in the vocabulary. For
    instance, if `t` and `h` occur very frequently together, like in English, we’ll
    add a token `th` to the vocabulary to model this pair of tokens instead of keeping
    them separated. The `t` and `h` tokens are kept in the vocabulary to tokenize
    instances where they do not occur together. Starting from a basic vocabulary of
    elementary units, we can then model any string efficiently.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一个折中的解决方案是通过扩展256个词的词汇表来构建一个中等大小的词汇表，其中包括最常见的字节组合。这是BPE算法采用的方法。其思想是通过迭代地合并词汇表中最频繁共现的一对标记来逐步构建一个预定义大小的词汇表。例如，如果`t`和`h`经常一起出现，就像在英语中一样，我们将添加一个标记`th`到词汇表中，以模拟这一对标记，而不是将它们分开。`t`和`h`标记保留在词汇表中，以标记它们不一起出现的实例。从基本单元的词汇表开始，我们可以有效地对任何字符串进行建模。
- en: Warning
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Be careful not to confuse the “byte” in “Byte-Pair Encoding” with the “byte”
    in “byte-level.” The name Byte-Pair Encoding comes from a data compression technique
    proposed by Philip Gage in 1994, originally operating on bytes.^([7](ch10.xhtml#idm46238691010992))
    Unlike what this name might indicate, standard BPE algorithms in NLP typically
    operate on Unicode strings rather than bytes (although there is a new type of
    BPE that specifically works on bytes, called *byte-level BPE*). If we read our
    Unicode strings in bytes we can thus reuse a simple BPE subword splitting algorithm.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 小心不要混淆“字节”和“字节级”中的“字节”。Byte-Pair Encoding中的“字节”一词来自Philip Gage在1994年提出的一种数据压缩技术，最初是针对字节的。^([7](ch10.xhtml#idm46238691010992))与这个名字可能表明的不同，NLP中的标准BPE算法通常是针对Unicode字符串而不是字节操作的（尽管有一种新类型的BPE专门用于字节，称为*字节级BPE*）。如果我们将我们的Unicode字符串读取为字节，我们可以重用一个简单的BPE子词拆分算法。
- en: There is just one issue when using a typical BPE algorithm in NLP. These algorithms
    are designed to work with clean Unicode string as inputs, not bytes, and expect
    regular ASCII characters in the inputs, without spaces or control characters.
    But in the Unicode characters corresponding to the 256 first bytes, there are
    many control characters (newline, tab, escape, line feed, and other nonprintable
    characters). To overcome this problem, the GPT-2 tokenizer first maps all the
    256 input bytes to Unicode strings that can easily be digested by the standard
    BPE algorithms—that is, we will map our 256 elementary values to Unicode strings
    that all correspond to standard printable Unicode characters.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当在NLP中使用典型的BPE算法时，有一个问题。这些算法是设计用于处理干净的Unicode字符串作为输入，而不是字节，并且期望输入中有常规ASCII字符，没有空格或控制字符。但在对应于前256个字节的Unicode字符中，有许多控制字符（换行、制表符、转义、换行等其他不可打印字符）。为了克服这个问题，GPT-2分词器首先将所有256个输入字节映射到可以被标准BPE算法轻松消化的Unicode字符串上，也就是说，我们将我们的256个基本值映射到所有对应于标准可打印Unicode字符的Unicode字符串。
- en: 'It’s not very important that these Unicode characters are each encoded with
    1 byte or more; what is important is that we have 256 single values at the end,
    forming our base vocabulary, and that these 256 values are correctly handled by
    our BPE algorithm. Let’s see some examples of this mapping with the GPT-2 tokenizer.
    We can access the entire mapping as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这些Unicode字符每个是否用1个或更多字节编码并不重要；重要的是我们最终有256个单一值，形成我们的基本词汇表，并且这256个值被我们的BPE算法正确处理。让我们看一些GPT-2标记器的映射示例。我们可以通过以下方式访问整个映射：
- en: '[PRE32]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: And we can take a look at some common values of bytes and associated mapped
    Unicode characters in [Table 10-1](#unicode_mapping).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看[表10-1](#unicode_mapping)中字节和相关映射的Unicode字符的一些常见值。
- en: Table 10-1\. Examples of character mappings in BPE
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 表10-1。BPE中字符映射的示例
- en: '| Description | Character | Bytes | Mapped bytes |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 描述 | 字符 | 字节 | 映射字节 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Regular characters | `a` and `?` | 97 and 63 | `a` and `?` |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 普通字符 | `a` 和 `?` | 97 和 63 | `a` 和 `?` |'
- en: '| A nonprintable control character (carriage return) | `U+000D` | 13 | `č`
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 一个不可打印的控制字符（回车） | `U+000D` | 13 | `č` |'
- en: '| A space | ` ` | 32 | `Ġ` |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 一个空格 | ` ` | 32 | `Ġ` |'
- en: '| A nonbreakable space | `\xa0` | 160 | `ł` |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 一个不间断空格 | `\xa0` | 160 | `ł` |'
- en: '| A newline character | `\n` | 10 | `Ċ` |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 一个换行符 | `\n` | 10 | `Ċ` |'
- en: 'We could have used a more explicit conversion, like mapping newlines to a `NEWLINE`
    string, but BPE algorithms are typically designed to work on characters. For this
    reason, keeping one Unicode character for each byte character is easier to handle
    with an out-of-the-box BPE algorithm. Now that we have been introduced to the
    dark magic of Unicode encodings, we can understand our tokenization conversion
    a bit better:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以使用更明确的转换，比如将新行映射到`NEWLINE`字符串，但是BPE算法通常设计为在字符上工作。因此，保留每个字节字符的一个Unicode字符更容易处理。现在我们已经了解了Unicode编码的黑魔法，我们可以更好地理解我们的标记转换：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can recognize the newlines, which as we now know are mapped to `Ċ`, and
    the spaces, mapped to `Ġ`. We also see that:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以识别新行，现在我们知道它们被映射为`Ċ`，以及空格，映射为`Ġ`。我们还看到：
- en: Spaces, and in particular consecutive spaces, are conserved (for instance, the
    three spaces in `'ĊĠĠĠ'`).
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空格，特别是连续的空格，被保留（例如，在'ĊĠĠĠ'中的三个空格）。
- en: Consecutive spaces are considered as a single word.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续的空格被视为一个单词。
- en: Each space preceding a word is attached to and considered a part of the subsequent
    word (e.g., in `'Ġsay'`).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个单词前的空格都附加到后续单词并视为其一部分（例如，在'Ġsay'中）。
- en: Let’s now experiment with the BPE model. As we’ve mentioned, it’s in charge
    of splitting the words into subunits until all subunits belong to the predefined
    vocabulary.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试一下BPE模型。正如我们之前提到的，它负责将单词分割成子单元，直到所有子单元都属于预定义的词汇表。
- en: 'The vocabulary of our GPT-2 tokenizer comprises 50,257 words:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的GPT-2标记器的词汇表包括50,257个单词：
- en: The base vocabulary with the 256 values of the bytes
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字节的256个值的基本词汇表
- en: 50,000 additional tokens created by repeatedly merging the most commonly co-occurring
    tokens
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过重复合并最常共现的标记创建了50,000个额外的标记
- en: A special character added to the vocabulary to represent document boundaries
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个特殊字符被添加到词汇表中以表示文档边界
- en: 'We can easily check that by looking at the length attribute of the tokenizer:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过查看标记器的长度属性轻松检查：
- en: '[PRE36]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Running the full pipeline on our input code gives us the following output:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的输入代码上运行完整的流水线给我们以下输出：
- en: '[PRE38]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As we can see, the BPE tokenizer keeps most of the words but will split the
    multiple spaces of our indentation into several consecutive spaces. This happens
    because this tokenizer is not specifically trained on code, but mostly on texts
    where consecutive spaces are rare. The BPE model thus doesn’t include a specific
    token in the vocabulary for indentation. This is a case where the tokenizer model
    is poorly suited for the dataset’s domain. As we discussed earlier, the solution
    is to retrain the tokenizer on the target corpus. So let’s get to it!
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，BPE标记器保留了大部分单词，但会将缩进的多个空格分割成几个连续的空格。这是因为这个标记器并不是专门针对代码进行训练的，而是主要针对连续空格很少的文本进行训练。因此，BPE模型不包括用于缩进的特定标记。这是一个标记器模型对数据集领域不太适合的情况。正如我们之前讨论的，解决方案是在目标语料库上重新训练标记器。所以让我们开始吧！
- en: Training a Tokenizer
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练标记器
- en: 'Let’s retrain our byte-level BPE tokenizer on a slice of our corpus to get
    a vocabulary better adapted to Python code. Retraining a tokenizer provided by
    ![nlpt_pin01](Images/nlpt_pin01.png) Transformers is simple. We just need to:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在我们的语料库的一个片段上重新训练我们的字节级BPE标记器，以获得更适合Python代码的词汇表。重新训练由nlpt_pin01 Transformers提供的标记器很简单。我们只需要：
- en: Specify our target vocabulary size.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指定我们的目标词汇表大小。
- en: Prepare an iterator to supply lists of input strings to process to train the
    tokenizer’s model.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备一个迭代器，以提供要处理的输入字符串列表，以训练标记器的模型。
- en: Call the `train_new_from_iterator()` method.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调用`train_new_from_iterator()`方法。
- en: Unlike deep learning models, which are often expected to memorize a lot of specific
    details from the training corpus, tokenizers are really just trained to extract
    the main statistics. In a nutshell, the tokenizer is just trained to know which
    letter combinations are the most frequent in our corpus.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 与深度学习模型不同，后者通常期望从训练语料库中记住许多特定细节，标记器实际上只是训练以提取主要统计数据。简而言之，标记器只是训练以了解哪些字母组合在我们的语料库中最常见。
- en: 'Therefore, you don’t necessarily need to train your tokenizer on a very large
    corpus; the corpus just needs to be representative of your domain and big enough
    for the tokenizer to extract statistically significant measures. But depending
    on the vocabulary size and the exact texts in the corpus, the tokenizer can end
    up storing unexpected words. We can see this, for instance, when looking at the
    longest words in the vocabulary of the GPT-2 tokenizer:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'These tokens look like separator lines that are likely to be used on forums.
    This makes sense since GPT-2 was trained on a corpus centered around Reddit. Now
    let’s have a look at the last words that were added to the vocabulary, and thus
    the least frequent ones:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The first token, `<|endoftext|>`, is the special token used to specify the end
    of a text sequence and was added after the BPE vocabulary was built. For each
    of these tokens our model will have to learn an associated word embedding, and
    we probably don’t want the embedding matrix to contain too many noisy words. Also
    note how some very time- and space-specific knowledge of the world (e.g., proper
    nouns like `Hitman` and `Commission`) is embedded at a very low level in our modeling
    approach by these words being granted separate tokens with associated vectors
    in the vocabulary. The creation of such specific tokens by a BPE tokenizer can
    also be an indication that the target vocabulary size is too large or that the
    corpus contains idiosyncratic tokens.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s train a fresh tokenizer on our corpus and examine its learned vocabulary.
    Since we just need a corpus reasonably representative of our dataset statistics,
    let’s select about 1–2 GB of data, or about 100,000 documents from our corpus:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let’s investigate the first and last words created by our BPE algorithm to
    see how relevant our vocabulary is. We skip the 256 byte tokens and look at the
    first tokens added thereafter:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Here we can see various standard levels of indentation and whitespace tokens,
    as well as short common Python keywords like `self`, `or`, and `in`. This is a
    good sign that our BPE algorithm is working as intended. Now let’s check out the
    last words:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Here there are still some relatively common words, like [`recv`](https://oreil.ly/tliPP),
    as well as some more noisy words probably coming from the comments.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also tokenize our simple example of Python code to see how our tokenizer
    is behaving on a simple example:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Even though they are not code keywords, it’s a little annoying to see common
    English words like `World` or `say` being split by our tokenizer, since we’d expect
    them to occur rather frequently in the corpus. Let’s check if all the Python reserved
    keywords are in the vocabulary:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'It appears that several quite frequent keywords, like `finally`, are not in
    the vocabulary either. Let’s try building a larger vocabulary using a larger sample
    of our dataset. For instance, we can build a vocabulary of 32,768 words (multiples
    of 8 are better for some efficient GPU/TPU computations) and train the tokenizer
    on a twice as large slice of our corpus:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We don’t expect the most frequent tokens to change much when adding more documents,
    but let’s look at the last tokens:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'A brief inspection doesn’t show any regular programming keywords here, which
    is promising. Let’s try tokenizing our sample code example with the new larger
    tokenizer:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Here also the indents are conveniently kept in the vocabulary, and we see that
    common English words like `Hello`, `World`, and `say` are also included as single
    tokens. This seems more in line with our expectations of the data the model may
    see in the downstream task. Let’s investigate the common Python keywords, as we
    did before:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We are still missing the [`nonlocal` keyword](https://oreil.ly/IHAMu), but it’s
    also rarely used in practice as it makes the syntax more complex. Keeping it out
    of the vocabulary seems reasonable. After this manual inspection, our larger tokenizer
    seems well adapted for our task—but as we mentioned earlier, objectively evaluating
    the performance of a tokenizer is a challenging task without measuring the model’s
    performance. We will proceed with this one and train a model to see how well it
    works in practice.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然缺少[`nonlocal`关键字](https://oreil.ly/IHAMu)，但实际上很少使用，因为它使语法更加复杂。将它排除在词汇表之外似乎是合理的。经过这次手动检查，我们较大的分词器似乎很适合我们的任务——但正如我们之前提到的，客观评估分词器的性能是一项具有挑战性的任务，没有测量模型性能。我们将继续使用这个分词器并训练一个模型，看看它在实践中的表现如何。
- en: Note
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can easily verify that the new tokenizer is about twice as efficient than
    the standard GPT-2 tokenizer by comparing the sequence lengths of tokenized code
    examples. Our tokenizer uses approximately half as many tokens as the existing
    one to encode a text, which gives us twice the effective model context for free.
    When we train a new model with the new tokenizer on a context window of size 1,024
    it is equivalent to training the same model with the old tokenizer on a context
    window of size 2,048, with the advantage of being much faster and more memory
    efficient.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较分词后的代码示例的序列长度，您可以轻松验证新的分词器的效率大约是标准GPT-2分词器的两倍。我们的分词器使用的标记数量大约是现有分词器的一半，以编码文本，这为我们提供了两倍的有效模型上下文。当我们使用新的分词器在大小为1,024的上下文窗口上训练一个新模型时，相当于使用旧的分词器在大小为2,048的上下文窗口上训练相同的模型，而且速度更快，内存效率更高。
- en: Saving a Custom Tokenizer on the Hub
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Hub 上保存自定义分词器
- en: Now that our tokenizer is trained, we should save it. The simplest way to save
    it and be able to access it from anywhere later is to push it to the Hugging Face
    Hub. This will be especially useful later, when we use a separate training server.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的分词器已经训练好了，我们应该保存它。保存它并能够随时从任何地方访问的最简单方法是将它推送到 Hugging Face Hub。当我们使用一个单独的训练服务器时，这将特别有用。
- en: 'To create a private model repository and save our tokenizer in it as a first
    file, we can directly use the `push_to_hub()` method of the tokenizer. Since we
    already authenticated our account with `huggingface-cli login`, we can simply
    push the tokenizer as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个私有模型存储库并将我们的分词器作为第一个文件保存在其中，我们可以直接使用分词器的`push_to_hub()`方法。由于我们已经使用`huggingface-cli
    login`对我们的帐户进行了身份验证，我们可以简单地推送分词器如下：
- en: '[PRE60]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'If you don’t want to push to an organization, you can simply omit the `organization`
    argument. This will create a repository in your namespace named `codeparrot`,
    which anyone can then load by running:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想推送到组织，可以简单地省略`organization`参数。这将在您的命名空间中创建一个名为`codeparrot`的存储库，任何人都可以通过运行以下命令加载：
- en: '[PRE61]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The tokenizer loaded from the Hub behaves exactly as we just saw. We can also
    investigate its files and saved vocabulary on the [Hub](https://oreil.ly/vcLeo).
    For reproducibility, let’s save our smaller tokenizer as well:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Hub 加载的分词器的行为与我们刚才看到的完全一样。我们还可以在[Hub](https://oreil.ly/vcLeo)上查看它的文件和保存的词汇表。为了可重现性，让我们也保存我们的较小的分词器：
- en: '[PRE63]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: This was a deep dive into building a tokenizer for a specific use case. Next,
    we will finally create a new model and train it from scratch.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对为特定用例构建分词器的深入研究。接下来，我们将最终创建一个新模型并从头开始训练。
- en: Training a Model from Scratch
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从头开始训练模型
- en: 'Here’s the part you’ve probably been waiting for: the model training. In this
    section we’ll decide which architecture works best for the task, initialize a
    fresh model without pretrained weights, set up a custom data loading class, and
    create a scalable training loop. In the grand finale we will train small and large
    GPT-2 models with 111 million and 1.5 billion parameters, respectively! But let’s
    not get ahead ourselves. First, we need to decide which architecture is best suited
    for code autocompletion.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你可能一直在等待的部分：模型训练。在这一部分，我们将决定哪种架构最适合这项任务，初始化一个没有预训练权重的全新模型，设置一个自定义的数据加载类，并创建一个可扩展的训练循环。在最后，我们将分别训练具有1.11亿和15亿参数的小型和大型GPT-2模型！但让我们不要过于急躁。首先，我们需要决定哪种架构最适合代码自动完成。
- en: Tip
  id: totrans-247
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: In this section we will implement a longer than usual script to train a model
    on a distributed infrastructure. Therefore, you should not run each code snippet
    independently, but instead download the script provided in the ![nlpt_pin01](Images/nlpt_pin01.png)
    [Transformers repository](https://oreil.ly/ZyPPR). Follow the accompanying instructions
    to execute the script with ![nlpt_pin01](Images/nlpt_pin01.png) Accelerate on
    your hardware.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个比通常更长的脚本，以在分布式基础设施上训练模型。因此，您不应该独立运行每个代码片段，而是下载提供的脚本![nlpt_pin01](Images/nlpt_pin01.png)
    [Transformers repository](https://oreil.ly/ZyPPR)。按照附带的说明在您的硬件上使用![nlpt_pin01](Images/nlpt_pin01.png)
    Accelerate 执行脚本。
- en: A Tale of Pretraining Objectives
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练目标的故事
- en: Now that we have access to a large-scale pretraining corpus and an efficient
    tokenizer, we can start thinking about how to pretrain a transformer model. With
    such a large codebase consisting of code snippets like the one shown in [Figure 10-1](#code-snippet),
    we can tackle several tasks. Which one we choose will influence our choice of
    pretraining objectives. Let’s have a look at three common tasks.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始考虑如何预训练一个变压器模型，因为我们有了大规模的预训练语料库和一个高效的分词器。由于我们的代码库非常庞大，包含了像[图10-1](#code-snippet)中显示的代码片段，我们可以解决几个任务。我们选择哪一个将影响我们选择的预训练目标。让我们看看三个常见的任务。
- en: '![Code snippet](Images/nlpt_1001.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![代码片段](Images/nlpt_1001.png)'
- en: Figure 10-1\. An example of a Python function that could be found in our dataset
  id: totrans-252
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1。一个Python函数的示例，可以在我们的数据集中找到
- en: Causal language modeling
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 因果语言建模
- en: 'A natural task with textual data is to provide a model with the beginning of
    a code sample and ask it to generate possible completions. This is a self-supervised
    training objective in which we can use the dataset without annotations. This should
    ring a bell: it’s the *causal language modeling* task we encountered in [Chapter 5](ch05.xhtml#chapter_generation).
    A directly related downstream task is code autocompletion, so we’ll definitely
    put this model on the shortlist. A decoder-only architecture such as the GPT family
    of models is usually best suited for this task, as shown in [Figure 10-2](#pretraining-clm).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![CLM pretraining](Images/nlpt_1002.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. In causal language modeling, the future tokens are masked and
    the model has to predict them; typically a decoder model such as GPT is used for
    such a task
  id: totrans-256
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Masked language modeling
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A related but slightly different task is to provide a model with a noisy code
    sample, for instance with a code instruction replaced by a random or masked word,
    and ask it to reconstruct the original clean sample, as illustrated in [Figure 10-3](#pretraining-mlm).
    This is also a self-supervised training objective and is commonly called *masked
    language modeling* or the *denoising objective*. It’s harder to think about a
    downstream task directly related to denoising, but denoising is generally a good
    pretraining task to learn general representations for later downstream tasks.
    Many of the models that we have used in the previous chapters (like BERT and XLM-RoBERTa)
    are pretrained in that way. Training a masked language model on a large corpus
    can thus be combined with fine-tuning the model on a downstream task with a limited
    number of labeled examples.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![MLM pretraining](Images/nlpt_1003.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. In masked language modeling some of the input tokens are either
    masked or replaced, and the model’s task is to predict the original tokens; this
    is the architecture underlying the encoder branch of transformer models
  id: totrans-260
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sequence-to-sequence training
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An alternative task is to use a heuristic like regular expressions to separate
    comments or docstrings from code and build a large-scale dataset of (code, comments)
    pairs that can be used as an annotated dataset. The training task is then a supervised
    training objective in which one category (code or comment) is used as input for
    the model and the other category (comment or code) is used as labels. This is
    a case of *supervised learning* with (input, labels) pairs, as highlighted in
    [Figure 10-4](#pretraining-seq2seq). With a large, clean, and diverse dataset
    as well as a model with sufficient capacity, we can try to train a model that
    learns to transcript comments in code or vice versa. A downstream task directly
    related to this supervised training task is then documentation generation from
    code or code generation from documentation, depending on how we set our input/outputs.
    In this setting a sequence is translated into another sequence, which is where
    encoder-decoder architectures such as T5, BART, and PEGASUS shine.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![Seq2seq pretraining](Images/nlpt_1004.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-4\. Using an encoder-decoder architecture for a sequence-to-sequence
    task where the inputs are split into comment/code pairs using heuristics: the
    model gets one element as input and needs to generate the other one'
  id: totrans-264
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since we want to build a code autocompletion model, we’ll select the first objective
    and choose a GPT architecture for the task. So let’s initialize a fresh GPT-2
    model!
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the Model
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is the first time in this book that we won’t use the `from_pretrained()`
    method to load a model but initialize the new model. We will, however, load the
    configuration of `gpt2-xl` so that we use the same hyperparameters and only adapt
    the vocabulary size for the new tokenizer. We then initialize a new model with
    this configuration with the `from_config()` method:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Let’s check how large the model actually is:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'This is a 1.5B parameter model! This is a lot of capacity, but we also have
    a large dataset. In general, large language models are more efficient to train
    as long as the dataset is reasonably large. Let’s save the newly initialized model
    in a *models/* folder and push it to the Hub:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Pushing the model to the Hub may take a few minutes given the size of the checkpoint
    (> 5 GB). Since this model is quite large, we’ll also create a smaller version
    that we can train to make sure everything works before scaling up. We will take
    the standard GPT-2 size as a base:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'And let’s save it to the Hub as well for easy sharing and reuse:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: Now that we have two models we can train, we need to make sure we can feed them
    the input data efficiently during training.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Dataloader
  id: totrans-281
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To be able to train with maximal efficiency, we will want to supply our model
    with sequences filling its context. For example, if the context length of our
    model is 1,024 tokens, we always want to provide 1,024-token sequences during
    training. But some of our code examples might be shorter or longer than 1,024
    tokens. To feed batches with full sequences of `sequence_length` to our model,
    we should thus either drop the last incomplete sequence or pad it. However, this
    will render our training slightly less efficient and force us to take care of
    padding and masking padded token labels. We are much more compute- than data-constrained,
    so we’ll take the easy and efficient way here. We can use a little trick to make
    sure we don’t lose too many trailing segments: we can tokenize several examples
    and then concatenate them, separated by the special end-of-sequence token, to
    get a very long sequence. Finally, we split this sequence into equally sized chunks
    as shown in [Figure 10-5](#preprocessing-clm). With this approach, we lose at
    most a small fraction of the data at the end.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![Preprocessing for CLM](Images/nlpt_1005.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. Preparing sequences of varying length for causal language modeling
    by concatenating several tokenized examples with an EOS token before chunking
    them
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can, for instance, make sure we have roughly one hundred full sequences
    in our tokenized examples by defining our input string character length as:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'where:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '`input_characters` is the number of characters in the string input to our tokenizer.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`number_of_sequences` is the number of (truncated) sequences we would like
    from our tokenizer, (e.g., 100).'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sequence_length` is the number of tokens per sequence returned by the tokenizer,
    (e.g., 1,024).'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`characters_per_token` is the average number of characters per output token
    that we first need to estimate.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we input a string with `input_characters` characters we will thus get on
    average `number_of_sequences` output sequences, and we can easily calculate how
    much input data we are losing by dropping the last sequence. If `number_of_sequences=100`
    it means that we stack roughly 100 sequences and at most lose the last element,
    which might be too short or too long. This corresponds to at most losing 1% of
    our dataset. At the same time, this approach ensures that we don’t introduce a
    bias by cutting off the majority of file endings.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first estimate the average character length per token in our dataset:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'With that we have all that’s needed to create our own `IterableDataset` (which
    is a helper class provided by PyTorch) for preparing constant-length inputs for
    the model. We just need to inherit from `IterableDataset` and set up the `__iter__()`
    function that yields the next element with the logic we just walked through:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The `__iter__()` function builds up a buffer of strings until it contains enough
    characters. All the elements in the buffer are tokenized and concatenated with
    the EOS token, then the long sequence in `all_token_ids` is chunked in `seq_length`-sized
    slices. Normally, we need attention masks to stack padded sequences of varying
    length and make sure the padding is ignored during training. We have taken care
    of this by only providing sequences of the same (maximal) length, so we don’t
    need the masks here and only return the `input_ids`. Let’s test our iterable dataset:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Nice, this works as intended and we get constant-length inputs for the model.
    Now that we have a reliable data source for the model, it’s time to build the
    actual training loop.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-303
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice that we shuffled the raw dataset before creating a `ConstantLengthDataset`.
    Since this is an iterable dataset, we can’t just shuffle the whole dataset at
    the beginning. Instead, we set up a buffer with size `buffer_size` and shuffle
    the elements in this buffer before we get elements from the dataset.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Training Loop
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now have all the elements to write our training loop. One obvious limitation
    of training our own language model is the memory limits on the GPUs we will use.
    Even on a modern graphics card you can’t train a model at GPT-2 scale in reasonable
    time. In this tutorial we will implement *data parallelism*, which will help us
    utilize several GPUs for training. Fortunately, we can use ![nlpt_pin01](Images/nlpt_pin01.png)
    Accelerate to make our code scalable. The ![nlpt_pin01](Images/nlpt_pin01.png)
    Accelerate library is designed to make distributed training—and changing the underlying
    hardware for training—easy. We can also use the `Trainer` for distributed training
    but ![nlpt_pin01](Images/nlpt_pin01.png) Accelerate gives us full control over
    the training loop, which is what we want to explore here.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: '![nlpt_pin01](Images/nlpt_pin01.png) Accelerate provides an easy API to make
    training scripts run with mixed precision and in any kind of distributed setting
    (single GPU, multiple GPUs, and TPUs). The same code can then run seamlessly on
    your local machine for debugging purposes or your beefy training cluster for the
    final training run. You only need to make a handful of changes to your native
    PyTorch training loop:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'The core part of the changes is the call to `prepare()`, which makes sure the
    model, optimizers, and dataloaders are all prepared and distributed on the infrastructure.
    These minor changes to the PyTorch training loop enable you to easily scale training
    across different infrastructures. With that in mind, let’s start building up our
    training script and define a few helper functions. First we set up the hyperparameters
    for training and wrap them in a `Namespace` for easy access:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Next, we set up logging for training. Since we are training a model from scratch,
    the training run will take a while and require expensive infrastructure. Therefore,
    we want to make sure that all the relevant information is stored and easily accessible.
    The `setup_logging()` method sets up three levels of logging: using a standard
    Python [`Logger`](https://oreil.ly/P9Xrm), [TensorBoard](https://oreil.ly/kY5ri),
    and [Weights & Biases](https://oreil.ly/BCC3k). Depending on your preferences
    and use case, you can add or remove logging frameworks here:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Each worker gets a unique `accelerator.process_index`, which we use with the
    `FileHandler` to write the logs of each worker to an individual file. We also
    use the `accel⁠erator.is_main_process` attribute, which is only `true` for the
    main worker. We make sure we don’t initialize the TensorBoard and Weights & Biases
    loggers several times, and we decrease the logging levels for the other workers.
    We return the autogenerated, unique `wandb.run.name`, which we use later to name
    our experiment branch on the Hub.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll also define a function to log the metrics with TensorBoard and Weights
    & Biases. We again use the `accelerator.is_main_process` here to ensure that we
    only log the metrics once and not for each worker:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Next, let’s write a function that creates the dataloaders for the training
    and validation sets with our brand new `ConstantLengthDataset` class:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: At the end we wrap the dataset in a `DataLoader`, which also handles the batching.
    ​![nlpt_pin01](Images/nlpt_pin01.png)⁠ Accelerate will take care of distributing
    the batches to each worker.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Another aspect we need to implement is optimization. We will set up the optimizer
    and learning rate schedule in the main loop, but we define a helper function here
    to differentiate the parameters that should receive weight decay. In general,
    biases and LayerNorm weights are not subject to weight decay:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Finally, we want to evaluate the model on the validation set from time to time,
    so let’s add an evaluation function we can call that calculates the loss and perplexity
    on the evaluation set:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: The perplexity measures how well the model’s output probability distributions
    predict the targeted tokens. So a lower perplexity corresponds to a better performance.
    Note that we can compute the perplexity by exponentiating the cross-entropy loss
    which we get from the model’s output. Especially at the start of training when
    the loss is still high, it is possible to get a numerical overflow when calculating
    the perplexity. We catch this error and set the perplexity to infinity in these
    instances.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Before we put it all together in the training script, there is one more additional
    function that we’ll use. As you know by now, the Hugging Face Hub uses Git under
    the hood to store and version models and datasets. With the `Repository` class
    from the *huggingface_hub* library you can programmatically access the repository
    and pull, branch, commit, or push. We’ll use this in our script to continuously
    push model checkpoints to the Hub during training.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have all these helper functions in place, we are ready to write
    the heart of the training script:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'This is quite a code block, but remember that this is all the code you need
    to train a fancy, large language model on a distributed infrastructure. Let’s
    deconstruct the script a little bit and highlight the most important parts:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '*Model saving*'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: We run the script from within the model repository, and at the start we check
    out a new branch named after the `run_name` we get from Weights & Biases. Later,
    we commit the model at each checkpoint and push it to the Hub. With that setup
    each experiment is on a new branch and each commit represents a model checkpoint.
    Note that we need to call `wait_for_everyone()` and `unwrap_model()` to make sure
    the model is properly synchronized when we store it.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '*Optimization*'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: For the model optimization we use `AdamW` with a cosine learning rate schedule
    after a linear warming-up period. For the hyperparameters, we closely follow the
    parameters described in the GPT-3 paper for similar-sized models.^([8](ch10.xhtml#idm46238687425568))
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '*Evaluation*'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: We evaluate the model on the evaluation set every time we save—that is, every
    `save_checkpoint_steps` and after training. Along with the validation loss we
    also log the validation perplexity.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '*Gradient accumulation and checkpointing*'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: The required batch sizes don’t fit in a GPU’s memory, even when we run on the
    latest GPUs. Therefore, we implement gradient accumulation, which gathers gradients
    over several backward passes and optimizes once enough gradients are accumulated.
    In [Chapter 6](ch06.xhtml#chapter_summarization), we saw how we can do this with
    the `Trainer`. For the large model, even a single batch does not quite fit on
    a single GPU. Using a method called *gradient checkpointing* we can trade some
    of the memory footprint for an approximately 20% training slowdown.^([9](ch10.xhtml#idm46238687417424))
    This allows us to fit even the large model in a single GPU.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: One aspect that might still be a bit obscure is what it means to train a model
    on multiple GPUs. There are several approaches to train models in a distributed
    fashion depending on the size of your model and volume of data. The approach utilized
    by ![nlpt_pin01](Images/nlpt_pin01.png) Accelerate is called [`DataDistributedParallelism`
    (DDP)](https://oreil.ly/m4iNm). The main advantage of this approach is that it
    allows you to train models faster with larger batch sizes that wouldn’t fit into
    any single GPU. The process is illustrated in [Figure 10-6](#ddp).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '![DDP](Images/nlpt_1006.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. Illustration of the processing steps in DDP with four GPUs
  id: totrans-338
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s go through the pipeline step by step:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Each worker consists of a GPU. In ![nlpt_pin01](Images/nlpt_pin01.png) Accelerate,
    there is a dataloader running on the main process that prepares the batches of
    data and sends them to all the workers.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each GPU receives a batch of data and calculates the loss and respective accumulated
    gradients from forward and backward passes with a local copy of the model.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The gradients from each node are averaged with a *reduce* pattern, and the averaged
    gradients are sent back to each worker.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The gradients are applied using the optimizer on each node individually. Although
    this might seem like redundant work, it avoids transferring copies of the large
    models between nodes. We’ll need to update the model at least once, and without
    this approach the other nodes would each need to wait until they’d received the
    updated version.
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once all models are updated we start all over again, with the main worker preparing
    new batches.
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This simple pattern allows us to train large models extremely fast by scaling
    up to the number of available GPUs without much additional logic. Sometimes, however,
    this is not enough. For example, if the model does not fit on a single GPU you
    might need more sophisticated [parallelism strategies](https://oreil.ly/3uhfq).
    Now that we have all the pieces needed for training, it’s time to launch a job!
    As you’ll see in the next section, this is quite simple to do.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: The Training Run
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll save the training script in a file called *codeparrot_training.py* so
    that we can execute it on our training server. To make life even easier, we’ll
    add it along with a *requirements.txt* file containing all the required Python
    dependencies to the model repository on the [Hub](https://oreil.ly/ndqSB). Remember
    that the models on the Hub are essentially Git repositories so we can just clone
    the repository, add any files we want, and then push them back to the Hub. On
    the training server, we can then spin up training with the following handful of
    commands:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: And that’s it—our model is now training! Note that `wandb login` will prompt
    you to authenticate with Weights & Biases for logging. The `accelerate config`
    command will guide you through setting up the infrastructure; you can see the
    settings used for this experiment in [Table 10-2](#codeparrot-config). We use
    an [`a2-megagpu-16g` instance](https://oreil.ly/ZJIG3) for all experiments, which
    is a workstation with 16 A100 GPUs with 40 GB of memory each.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-2\. Configuration used to train the CodeParrot models
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '| Setting | Value |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
- en: '| Compute environment? | multi-GPU |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
- en: '| How many machines? | 1 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
- en: '| DeepSpeed? | No |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
- en: '| How many processes? | 16 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
- en: '| Use FP16? | Yes |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: 'Running the training script with these settings on that infrastructure takes
    about 24 hours and 7 days for the small and large models, respectively. If you
    train your own custom model, make sure your code runs smoothly on smaller infrastructure
    in order to make sure that expensive long run goes smoothly as well. After the
    full training run completes successfully, you can merge the experiment branch
    on the Hub back into the main branch with the following commands:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: Naturally, *`RUN_NAME`* should be the name of the experiment branch on the Hub
    you would like to merge. Now that we have a trained model, let’s have a look at
    how we can investigate its performance.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Results and Analysis
  id: totrans-361
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After anxiously monitoring the logs for a week, you will probably see loss and
    perplexity curves that look like those shown in [Figure 10-7](#training_result).
    The training loss and validation perplexity go down continuously, and the loss
    curve looks almost linear on the log-log scale. We also see that the large model
    converges faster in terms of processed tokens, although the overall training takes
    longer.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: '![Training loss and validation perplexity.](Images/nlpt_1007.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. Training loss and validation perplexity as a function of processed
    tokens for the small and large CodeParrot models
  id: totrans-364
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So what can we do with our freshly baked language model, straight out of the
    GPU oven? Well, we can use it to write some code for us. There are two types of
    analyses we can conduct: qualitative and quantitative. In the former, we look
    at concrete examples and try to better understand in which cases the model succeeds
    and where it fails. In the latter case, we evaluate the model’s performance statistically
    on a large set of test cases. In this section we’ll explore how we can use our
    model. First we’ll have a look at a few examples, and then we’ll briefly discuss
    how we could evaluate the model systematically and more robustly. First, let’s
    wrap the small model in a pipeline and use it to continue some code inputs:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Now we can use the generation pipeline to generate candidate completions from
    a given prompt. By default, the pipeline will generate code until a predefined
    maximum length, and the output could contain multiple functions or classes. So,
    to keep the outputs concise, we’ll implement a `first_block()` function that uses
    regular expressions to extract the first occurrence of a function or class. The
    `complete_code()` function below applies this logic to print out the completions
    generated by CodeParrot:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Let’s start with a simple example and have the model write a function for us
    that calculates the area of a rectangle:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '[PRE92]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'That looks pretty good! Although not all the generations are correct, the right
    solution is in there. Now, can the model also solve a more complex task of extracting
    URLs from an HTML string? Let’s see:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: '[PRE94]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Although it didn’t quite get it right in the second attempt, the other three
    generations are correct. We can test the function on the Hugging Face home page:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '[PRE96]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'We can see that all the URLs starting with `https` are external pages, whereas
    the others are subpages of the main website. That’s exactly what we wanted. Finally,
    let’s load the large model and see if we can use it to translate a function from
    pure Python to NumPy:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '[PRE98]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'That worked! Let’s see if we can also use the CodeParrot model to help us build
    a Scikit-learn model:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: '[PRE100]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: Although in the second attempt it tried to train an [extra-trees classifier](https://oreil.ly/40Uy7),
    it generated what we asked in the other cases.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 5](ch05.xhtml#chapter_generation) we explored a few metrics to measure
    the quality of generated text. Among these was the BLEU score, which is frequently
    used for that purpose. While this metric has limitations in general, it is particularly
    badly suited for our use case. The BLEU score measures the overlap of *n*-grams
    between the reference texts and the generated texts. When writing code we have
    a lot of freedom in terms of variables and classes, and the success of a program
    does not depend on the naming scheme as long as it is consistent. However, the
    BLEU score would punish a generation that deviates from the reference naming,
    which might in fact be almost impossible to predict (even for a human coder).
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: 'In software development there are much better and more reliable ways to measure
    the quality of code, such as unit tests. This is how all the OpenAI Codex models
    were evaluated: by running several code generations for coding tasks through a
    set of unit tests and calculating the fraction of generations that pass the tests.^([10](ch10.xhtml#idm46238686878640))
    For a proper performance measure we should apply the same evaluation regimen to
    our models but this is beyond the scope of this chapter. You can find details
    on how CodeParrot performs on the HumanEval benchmark in [the model’s accompanying
    blog post](https://oreil.ly/hKOP8).'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  id: totrans-387
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s take a step back for a moment and contemplate what we have achieved in
    this chapter. We set out to create a code autocomplete function for Python. First
    we built a custom, large-scale dataset suitable for pretraining a large language
    model. Then we created a custom tokenizer that is able to efficiently encode Python
    code with that dataset. Finally, with the help of ![nlpt_pin01](Images/nlpt_pin01.png)
    Accelerate we put everything together and wrote a training script to train small
    and large versions of a GPT-2 model from scratch on a multi-GPU infrastructure,
    in under two hundred lines of code. Investigating the model outputs, we saw that
    it can generate reasonable code continuations, and we discussed how the model
    could be systematically evaluated.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'You now not only know how to fine-tune any of the many pretrained models on
    the Hub, but also how to pretrain a custom model from scratch when you have enough
    data and compute resources available. You are now prepared to tackle almost any
    NLP use case with transformers. So the question is: where to next? In the next
    and last chapter, we’ll have a look at where the field is currently moving and
    what new exciting applications and domains beyond NLP transformer models can tackle.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch10.xhtml#idm46238692182464-marker)) Y. Zhu et al., [“Aligning Books
    and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading
    Books”](https://arxiv.org/abs/1506.06724), (2015); J. Dodge et al., [“Documenting
    the English Colossal Clean Crawled Corpus”](https://arxiv.org/abs/2104.08758),
    (2021).'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch10.xhtml#idm46238692175664-marker)) J. Bandy and N. Vincent, [“Addressing
    *Documentation Debt* in Machine Learning Research: A Retrospective Datasheet for
    BookCorpus”](https://arxiv.org/abs/2105.05241), (2021).'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch10.xhtml#idm46238691930032-marker)) B. Hutchinson et al., [“Towards
    Accountability for Machine Learning Datasets: Practices from Software Engineering
    and Infrastructure”](https://arxiv.org/abs/2010.13561), (2020).'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch10.xhtml#idm46238691921600-marker)) By comparison, GitHub Copilot supports
    over a dozen programming languages.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch10.xhtml#idm46238691899472-marker)) M.-A. Lachaux et al., [“Unsupervised
    Translation of Programming Languages”](https://arxiv.org/abs/2006.03511), (2020).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch10.xhtml#idm46238691054768-marker)) L. Xue et al., [“⁠ByT5: Towards
    a Token-Free Future with Pre-Trained Byte-to-Byte Models”](https://arxiv.org/abs/2105.13626),
    (2021).'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch10.xhtml#idm46238691010992-marker)) P. Gage, “A New Algorithm for Data
    Compression,” *The C Users Journal* 12, no. 2 (1994): 23–38, [*https://dx.doi.org/10.14569/IJACSA.2012.030803*](https://dx.doi.org/10.14569/IJACSA.2012.030803).'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch10.xhtml#idm46238687425568-marker)) T. Brown et al., [“Language Models
    Are Few-Shot Learners”](https://arxiv.org/abs/2005.14165), (2020).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch10.xhtml#idm46238687417424-marker)) You can read more about gradient
    checkpointing on OpenAI’s [release post](https://oreil.ly/94oj1).
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch10.xhtml#idm46238686878640-marker)) M. Chen et al., [“Evaluating Large
    Language Models Trained on Code”](https://arxiv.org/abs/2107.03374), (2021).
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
