- en: Chapter 8\. Diffusion Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Alongside GANs, diffusion models are one of the most influential and impactful
    generative modeling techniques for image generation to have been introduced over
    the last decade. Across many benchmarks, diffusion models now outperform previously
    state-of-the-art GANs and are quickly becoming the go-to choice for generative
    modeling practitioners, particularly for visual domains (e.g., OpenAI‚Äôs DALL.E
    2 and Google‚Äôs ImageGen for text-to-image generation). Recently, there has been
    an explosion of diffusion models being applied across wide range of tasks, reminiscent
    of the GAN proliferation that took place between 2017‚Äì2020.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many of the core ideas that underpin diffusion models share similarities with
    earlier types of generative models that we have already explored in this book
    (e.g., denoising autoencoders, energy-based models). Indeed, the name *diffusion*
    takes inspiration from the well-studied property of thermodynamic diffusion: an
    important link was made between this purely physical field and deep learning in
    2015.^([1](ch08.xhtml#idm45387010500320))'
  prefs: []
  type: TYPE_NORMAL
- en: Important progress was also being made in the field of score-based generative
    models,^([2](ch08.xhtml#idm45387010496240))^,^([3](ch08.xhtml#idm45387010494000))
    a branch of energy-based modeling that directly estimates the gradient of the
    log distribution (also known as the score function) in order to train the model,
    as an alternative to using contrastive divergence. In particular, Yang Song and
    Stefano Ermon used multiple scales of noise perturbations applied to the raw data
    to ensure the model‚Äîa *noise conditional score network* (NCSN)‚Äîperforms well on
    regions of low data density.
  prefs: []
  type: TYPE_NORMAL
- en: The breakthrough diffusion model paper came in the summer of 2020.^([4](ch08.xhtml#idm45387010490880))
    Standing on the shoulders of earlier works, the paper uncovers a deep connection
    between diffusion models and score-based generative models, and the authors use
    this fact to train a diffusion model that can rival GANs across several datasets,
    called the *Denoising Diffusion Probabilistic Model* (DDPM).
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will walk through the theoretical requirements for understanding
    how a denoising diffusion model works. You will then learn how to build your own
    denoising diffusion model using Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To help explain the key ideas that underpin diffusion models, let‚Äôs begin with
    a short story!
  prefs: []
  type: TYPE_NORMAL
- en: The DiffuseTV story describes the general idea behind a diffusion model. Now
    let‚Äôs dive into the technicalities of how we build such a model using Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Denoising Diffusion Models (DDM)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The core idea behind a denoising diffusion model is simple‚Äîwe train a deep learning
    model to denoise an image over a series of very small steps. If we start from
    pure random noise, in theory we should be able to keep applying the model until
    we obtain an image that looks as if it were drawn from the training set. What‚Äôs
    amazing is that this simple concept works so well in practice!
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs first get set up with a dataset and then walk through the forward (noising)
    and backward (denoising) diffusion processes.
  prefs: []
  type: TYPE_NORMAL
- en: Running the Code for This Example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code for this example can be found in the Jupyter notebook located at *notebooks/08_diffusion/01_ddm/ddm.ipynb*
    in the book repository.
  prefs: []
  type: TYPE_NORMAL
- en: The code is adapted from the excellent [tutorial on denoising diffusion implicit
    models](https://oreil.ly/srPCe) created by Andr√°s B√©res available on the Keras
    website.
  prefs: []
  type: TYPE_NORMAL
- en: The Flowers Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We‚Äôll be using the [Oxford 102 Flower dataset](https://oreil.ly/HfrKV) that
    is available through Kaggle. This is a set of over 8,000 color images of a variety
    of flowers.
  prefs: []
  type: TYPE_NORMAL
- en: You can download the dataset by running the Kaggle dataset downloader script
    in the book repository, as shown in [Example¬†8-1](#downloading-flower-dataset).
    This will save the flower images to the */data* folder.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-1\. Downloading the Oxford 102 Flower dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`As usual, we‚Äôll load the images in using the Keras `image_dataset_from_directory`
    function, resize the images to 64 √ó 64 pixels, and scale the pixel values to the
    range [0, 1]. We‚Äôll also repeat the dataset five times to increase the epoch length
    and batch the data into groups of 64 images, as shown in [Example¬†8-2](#flower-preprocessing-ex).'
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-2\. Loading the Oxford 102 Flower dataset
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_diffusion_models_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Load dataset (when required during training) using the Keras `image_dataset_from_directory`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_diffusion_models_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Scale the pixel values to the range [0, 1].
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_diffusion_models_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Repeat the dataset five times.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_diffusion_models_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Batch the dataset into groups of 64 images.
  prefs: []
  type: TYPE_NORMAL
- en: Example images from the dataset are shown in [Figure¬†8-2](Images/#flower_example_images).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-2\. Example images from the Oxford 102 Flower dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that we have our dataset we can explore how we should add noise to the images,
    using a forward diffusion process.`  `## The Forward Diffusion Process
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have an image <math alttext="bold x 0"><msub><mi>ùê±</mi> <mn>0</mn></msub></math>
    that we want to corrupt gradually over a large number of steps (say, <math alttext="upper
    T equals 1 comma 000"><mrow><mi>T</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>000</mn></mrow></math>
    ), so that eventually it is indistinguishable from standard Gaussian noise (i.e.,
    <math alttext="bold x Subscript upper T"><msub><mi>ùê±</mi> <mi>T</mi></msub></math>
    should have zero mean and unit variance). How should we go about doing this?
  prefs: []
  type: TYPE_NORMAL
- en: We can define a function <math alttext="q"><mi>q</mi></math> that adds a small
    amount of Gaussian noise with variance <math alttext="beta Subscript t"><msub><mi>Œ≤</mi>
    <mi>t</mi></msub></math> to an image <math alttext="bold x Subscript t minus 1"><msub><mi>ùê±</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math> to generate a new image
    <math alttext="bold x Subscript t"><msub><mi>ùê±</mi> <mi>t</mi></msub></math> .
    If we keep applying this function, we will generate a sequence of progressively
    noisier images ( <math alttext="bold x 0 comma ellipsis comma bold x Subscript
    upper T Baseline"><mrow><msub><mi>ùê±</mi> <mn>0</mn></msub> <mo>,</mo> <mo>...</mo>
    <mo>,</mo> <msub><mi>ùê±</mi> <mi>T</mi></msub></mrow></math> ), as shown in [Figure¬†8-3](#forward_diffusion_q).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-3\. The forward diffusion process <math alttext="q"><mi>q</mi></math>
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can write this update process mathematically as follows (here, <math alttext="epsilon
    Subscript t minus 1"><msub><mi>œµ</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    is a standard Gaussian with zero mean and unit variance):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="bold x Subscript t Baseline equals StartRoot 1 minus beta Subscript
    t Baseline EndRoot bold x Subscript t minus 1 Baseline plus StartRoot beta Subscript
    t Baseline EndRoot epsilon Subscript t minus 1" display="block"><mrow><msub><mi>ùê±</mi>
    <mi>t</mi></msub> <mo>=</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mi>Œ≤</mi>
    <mi>t</mi></msub></mrow></msqrt> <msub><mi>ùê±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>+</mo> <msqrt><msub><mi>Œ≤</mi> <mi>t</mi></msub></msqrt> <msub><mi>œµ</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that we also scale the input image <math alttext="bold x Subscript t minus
    1"><msub><mi>ùê±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    , to ensure that the variance of the output image <math alttext="bold x Subscript
    t"><msub><mi>ùê±</mi> <mi>t</mi></msub></math> remains constant over time. This
    way, if we normalize our original image <math alttext="bold x 0"><msub><mi>ùê±</mi>
    <mn>0</mn></msub></math> to have zero mean and unit variance, then <math alttext="bold
    x Subscript upper T"><msub><mi>ùê±</mi> <mi>T</mi></msub></math> will approximate
    a standard Gaussian distribution for large enough <math alttext="upper T"><mi>T</mi></math>
    , by induction, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: If we assume that <math alttext="bold x Subscript t minus 1"><msub><mi>ùê±</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math> has zero mean and unit
    variance then <math alttext="StartRoot 1 minus beta Subscript t Baseline EndRoot
    bold x Subscript t minus 1"><mrow><msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mi>Œ≤</mi>
    <mi>t</mi></msub></mrow></msqrt> <msub><mi>ùê±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
    will have variance <math alttext="1 minus beta Subscript t"><mrow><mn>1</mn> <mo>-</mo>
    <msub><mi>Œ≤</mi> <mi>t</mi></msub></mrow></math> and <math alttext="StartRoot
    beta Subscript t Baseline EndRoot epsilon Subscript t minus 1"><mrow><msqrt><msub><mi>Œ≤</mi>
    <mi>t</mi></msub></msqrt> <msub><mi>œµ</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></math>
    will have variance <math alttext="beta Subscript t"><msub><mi>Œ≤</mi> <mi>t</mi></msub></math>
    , using the rule that <math alttext="upper V a r left-parenthesis a upper X right-parenthesis
    equals a squared upper V a r left-parenthesis upper X right-parenthesis"><mrow><mi>V</mi>
    <mi>a</mi> <mi>r</mi> <mrow><mo>(</mo> <mi>a</mi> <mi>X</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msup><mi>a</mi> <mn>2</mn></msup> <mi>V</mi> <mi>a</mi> <mi>r</mi>
    <mrow><mo>(</mo> <mi>X</mi> <mo>)</mo></mrow></mrow></math> . Adding these together,
    we obtain a new distribution <math alttext="bold x Subscript t"><msub><mi>ùê±</mi>
    <mi>t</mi></msub></math> with zero mean and variance <math alttext="1 minus beta
    Subscript t Baseline plus beta Subscript t Baseline equals 1"><mrow><mn>1</mn>
    <mo>-</mo> <msub><mi>Œ≤</mi> <mi>t</mi></msub> <mo>+</mo> <msub><mi>Œ≤</mi> <mi>t</mi></msub>
    <mo>=</mo> <mn>1</mn></mrow></math> , using the rule that <math alttext="upper
    V a r left-parenthesis upper X plus upper Y right-parenthesis equals upper V a
    r left-parenthesis upper X right-parenthesis plus upper V a r left-parenthesis
    upper Y right-parenthesis"><mrow><mi>V</mi> <mi>a</mi> <mi>r</mi> <mo>(</mo> <mi>X</mi>
    <mo>+</mo> <mi>Y</mi> <mo>)</mo> <mo>=</mo> <mi>V</mi> <mi>a</mi> <mi>r</mi> <mo>(</mo>
    <mi>X</mi> <mo>)</mo> <mo>+</mo> <mi>V</mi> <mi>a</mi> <mi>r</mi> <mo>(</mo> <mi>Y</mi>
    <mo>)</mo></mrow></math> for independent <math alttext="upper X"><mi>X</mi></math>
    and <math alttext="upper Y"><mi>Y</mi></math> . Therefore, if <math alttext="bold
    x 0"><msub><mi>ùê±</mi> <mn>0</mn></msub></math> is normalized to a zero mean and
    unit variance, then we guarantee that this is also true for all <math alttext="bold
    x Subscript t"><msub><mi>ùê±</mi> <mi>t</mi></msub></math> , including the final
    image <math alttext="bold x Subscript upper T"><msub><mi>ùê±</mi> <mi>T</mi></msub></math>
    , which will approximate a standard Gaussian distribution. This is exactly what
    we need, as we want to be able to easily sample <math alttext="bold x Subscript
    upper T"><msub><mi>ùê±</mi> <mi>T</mi></msub></math> and then apply a reverse diffusion
    process through our trained neural network model!
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, our forward noising process <math alttext="q"><mi>q</mi></math>
    can also be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="q left-parenthesis bold x Subscript t Baseline vertical-bar bold
    x Subscript t minus 1 Baseline right-parenthesis equals script upper N left-parenthesis
    bold x Subscript t Baseline semicolon StartRoot 1 minus beta Subscript t Baseline
    EndRoot bold x Subscript t minus 1 Baseline comma beta Subscript t Baseline bold
    upper I right-parenthesis" display="block"><mrow><mi>q</mi> <mrow><mo>(</mo> <msub><mi>ùê±</mi>
    <mi>t</mi></msub> <mo>|</mo> <msub><mi>ùê±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mi>ùí©</mi> <mrow><mo>(</mo> <msub><mi>ùê±</mi> <mi>t</mi></msub>
    <mo>;</mo> <msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mi>Œ≤</mi> <mi>t</mi></msub></mrow></msqrt>
    <msub><mi>ùê±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>,</mo>
    <msub><mi>Œ≤</mi> <mi>t</mi></msub> <mi>ùêà</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The Reparameterization Trick
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It would also be useful to be able to jump straight from an image <math alttext="bold
    x 0"><msub><mi>ùê±</mi> <mn>0</mn></msub></math> to any noised version of the image
    <math alttext="bold x Subscript t"><msub><mi>ùê±</mi> <mi>t</mi></msub></math> without
    having to go through <math alttext="t"><mi>t</mi></math> applications of <math
    alttext="q"><mi>q</mi></math> . Luckily, there is a reparameterization trick that
    we can use to do this.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we define <math alttext="alpha Subscript t Baseline equals 1 minus beta
    Subscript t"><mrow><msub><mi>Œ±</mi> <mi>t</mi></msub> <mo>=</mo> <mn>1</mn> <mo>-</mo>
    <msub><mi>Œ≤</mi> <mi>t</mi></msub></mrow></math> and <math alttext="alpha overbar
    Subscript t Baseline equals product Underscript i equals 1 Overscript t Endscripts
    alpha Subscript i"><mrow><msub><mover accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover>
    <mi>t</mi></msub> <mo>=</mo> <msubsup><mo>‚àè</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>t</mi></msubsup> <msub><mi>Œ±</mi> <mi>i</mi></msub></mrow></math> , then we
    can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartLayout 1st Row 1st Column bold x Subscript t 2nd Column
    equals 3rd Column StartRoot alpha Subscript t Baseline EndRoot bold x Subscript
    t minus 1 plus StartRoot 1 minus alpha Subscript t Baseline EndRoot epsilon Subscript
    t minus 1 2nd Row 1st Column Blank 2nd Column equals 3rd Column StartRoot alpha
    Subscript t Baseline alpha Subscript t minus 1 Baseline EndRoot bold x Subscript
    t minus 2 plus StartRoot 1 minus alpha Subscript t Baseline alpha Subscript t
    minus 1 Baseline EndRoot epsilon 3rd Row 1st Column Blank 2nd Column equals 3rd
    Column  ellipsis 4th Row 1st Column Blank 2nd Column equals 3rd Column StartRoot
    alpha overbar Subscript t Baseline EndRoot bold x 0 plus StartRoot 1 minus alpha
    overbar Subscript t Baseline EndRoot epsilon EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><msub><mi>ùê±</mi> <mi>t</mi></msub></mtd>
    <mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><msqrt><msub><mi>Œ±</mi> <mi>t</mi></msub></msqrt>
    <msub><mi>ùê±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>+</mo>
    <msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mi>Œ±</mi> <mi>t</mi></msub></mrow></msqrt>
    <msub><mi>œµ</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></mtd></mtr>
    <mtr><mtd><mo>=</mo></mtd> <mtd columnalign="left"><mrow><msqrt><mrow><msub><mi>Œ±</mi>
    <mi>t</mi></msub> <msub><mi>Œ±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></msqrt>
    <msub><mi>ùê±</mi> <mrow><mi>t</mi><mo>-</mo><mn>2</mn></mrow></msub> <mo>+</mo>
    <msqrt><mrow><mn>1</mn> <mo>-</mo> <msub><mi>Œ±</mi> <mi>t</mi></msub> <msub><mi>Œ±</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></mrow></msqrt> <mi>œµ</mi></mrow></mtd></mtr>
    <mtr><mtd><mo>=</mo></mtd> <mtd columnalign="left"><mo>‚ãØ</mo></mtd></mtr> <mtr><mtd><mo>=</mo></mtd>
    <mtd columnalign="left"><mrow><msqrt><msub><mover accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover>
    <mi>t</mi></msub></msqrt> <msub><mi>ùê±</mi> <mn>0</mn></msub> <mo>+</mo> <msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mover accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover> <mi>t</mi></msub></mrow></msqrt>
    <mi>œµ</mi></mrow></mtd></mtr></mtable></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that the second line uses the fact that we can add two Gaussians to obtain
    a new Gaussian. We therefore have a way to jump from the original image <math
    alttext="bold x 0"><msub><mi>ùê±</mi> <mn>0</mn></msub></math> to any step of the
    forward diffusion process <math alttext="bold x Subscript t"><msub><mi>ùê±</mi>
    <mi>t</mi></msub></math> . Moreover, we can define the diffusion schedule using
    the <math alttext="alpha overbar Subscript t"><msub><mover accent="true"><mi>Œ±</mi>
    <mo>¬Ø</mo></mover> <mi>t</mi></msub></math> values, instead of the original <math
    alttext="beta Subscript t"><msub><mi>Œ≤</mi> <mi>t</mi></msub></math> values, with
    the interpretation that <math alttext="alpha overbar Subscript t"><msub><mover
    accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover> <mi>t</mi></msub></math> is the variance
    due to the signal (the original image, <math alttext="bold x 0"><msub><mi>ùê±</mi>
    <mn>0</mn></msub></math> ) and <math alttext="1 minus alpha overbar Subscript
    t"><mrow><mn>1</mn> <mo>-</mo> <msub><mover accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover>
    <mi>t</mi></msub></mrow></math> is the variance due to the noise ( <math alttext="epsilon"><mi>œµ</mi></math>
    ).
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward diffusion process <math alttext="q"><mi>q</mi></math> can therefore
    also be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="q left-parenthesis bold x Subscript t Baseline vertical-bar bold
    x 0 right-parenthesis equals script upper N left-parenthesis bold x Subscript
    t Baseline semicolon StartRoot alpha overbar Subscript t Baseline EndRoot bold
    x 0 comma left-parenthesis 1 minus alpha overbar Subscript t Baseline right-parenthesis
    bold upper I right-parenthesis" display="block"><mrow><mi>q</mi> <mrow><mo>(</mo>
    <msub><mi>ùê±</mi> <mi>t</mi></msub> <mo>|</mo> <msub><mi>ùê±</mi> <mn>0</mn></msub>
    <mo>)</mo></mrow> <mo>=</mo> <mi>ùí©</mi> <mrow><mo>(</mo> <msub><mi>ùê±</mi> <mi>t</mi></msub>
    <mo>;</mo> <msqrt><msub><mover accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover> <mi>t</mi></msub></msqrt>
    <msub><mi>ùê±</mi> <mn>0</mn></msub> <mo>,</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo>
    <msub><mover accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover> <mi>t</mi></msub> <mo>)</mo></mrow>
    <mi>ùêà</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion Schedules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Notice that we are also free to choose a different <math alttext="beta Subscript
    t"><msub><mi>Œ≤</mi> <mi>t</mi></msub></math> at each timestep‚Äîthey don‚Äôt all have
    be the same. How the <math alttext="beta Subscript t"><msub><mi>Œ≤</mi> <mi>t</mi></msub></math>
    (or <math alttext="alpha overbar Subscript t"><msub><mover accent="true"><mi>Œ±</mi>
    <mo>¬Ø</mo></mover> <mi>t</mi></msub></math> ) values change with <math alttext="t"><mi>t</mi></math>
    is called the *diffusion* *schedule*.
  prefs: []
  type: TYPE_NORMAL
- en: In the original paper (Ho et al., 2020), the authors chose a *linear diffusion
    schedule* for <math alttext="beta Subscript t"><msub><mi>Œ≤</mi> <mi>t</mi></msub></math>
    ‚Äîthat is, <math alttext="beta Subscript t"><msub><mi>Œ≤</mi> <mi>t</mi></msub></math>
    increases linearly with <math alttext="t"><mi>t</mi></math> , from <math alttext="beta
    1 equals"><mrow><msub><mi>Œ≤</mi> <mn>1</mn></msub> <mo>=</mo></mrow></math> 0.0001
    to <math alttext="beta Subscript upper T Baseline equals"><mrow><msub><mi>Œ≤</mi>
    <mi>T</mi></msub> <mo>=</mo></mrow></math> 0.02\. This ensures that in the early
    stages of the noising process we take smaller noising steps than in the later
    stages, when the image is already very noisy.
  prefs: []
  type: TYPE_NORMAL
- en: We can code up a linear diffusion schedule as shown in [Example¬†8-3](#linear_diffusion_schedule).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-3\. The linear diffusion schedule
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_diffusion_models_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The diffusion times are equally spaced steps between 0 and 1.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_diffusion_models_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The linear diffusion schedule is applied to the diffusion times to produce the
    noise and signal rates.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a later paper it was found that a *cosine diffusion schedule* outperformed
    the linear schedule from the original paper.^([5](ch08.xhtml#idm45387010764208))
    A cosine schedule defines the following values of <math alttext="alpha overbar
    Subscript t"><msub><mover accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover> <mi>t</mi></msub></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="alpha overbar Subscript t Baseline equals cosine squared left-parenthesis
    StartFraction t Over upper T EndFraction dot StartFraction pi Over 2 EndFraction
    right-parenthesis" display="block"><mrow><msub><mover accent="true"><mi>Œ±</mi>
    <mo>¬Ø</mo></mover> <mi>t</mi></msub> <mo>=</mo> <msup><mo form="prefix">cos</mo>
    <mn>2</mn></msup> <mrow><mo>(</mo> <mfrac><mi>t</mi> <mi>T</mi></mfrac> <mo>¬∑</mo>
    <mfrac><mi>œÄ</mi> <mn>2</mn></mfrac> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The updated equation is therefore as follows (using the trigonometric identity
    <math alttext="cosine squared left-parenthesis x right-parenthesis plus sine squared
    left-parenthesis x right-parenthesis equals 1"><mrow><msup><mo form="prefix">cos</mo>
    <mn>2</mn></msup> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>+</mo> <msup><mo
    form="prefix">sin</mo> <mn>2</mn></msup> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mn>1</mn></mrow></math> ):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="bold x Subscript t Baseline equals cosine left-parenthesis StartFraction
    t Over upper T EndFraction dot StartFraction pi Over 2 EndFraction right-parenthesis
    bold x 0 plus sine left-parenthesis StartFraction t Over upper T EndFraction dot
    StartFraction pi Over 2 EndFraction right-parenthesis epsilon" display="block"><mrow><msub><mi>ùê±</mi>
    <mi>t</mi></msub> <mo>=</mo> <mo form="prefix">cos</mo> <mrow><mo>(</mo> <mfrac><mi>t</mi>
    <mi>T</mi></mfrac> <mo>¬∑</mo> <mfrac><mi>œÄ</mi> <mn>2</mn></mfrac> <mo>)</mo></mrow>
    <msub><mi>ùê±</mi> <mn>0</mn></msub> <mo>+</mo> <mo form="prefix">sin</mo> <mrow><mo>(</mo>
    <mfrac><mi>t</mi> <mi>T</mi></mfrac> <mo>¬∑</mo> <mfrac><mi>œÄ</mi> <mn>2</mn></mfrac>
    <mo>)</mo></mrow> <mi>œµ</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: This equation is a simplified version of the actual cosine diffusion schedule
    used in the paper. The authors also add an offset term and scaling to prevent
    the noising steps from being too small at the beginning of the diffusion process.
    We can code up the cosine and offset cosine diffusion schedules as shown in [Example¬†8-4](#cosine_diffusion_schedule).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-4\. The cosine and offset cosine diffusion schedules
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_diffusion_models_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The pure cosine diffusion schedule (without offset or rescaling).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_diffusion_models_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The offset cosine diffusion schedule that we will be using, which adjusts the
    schedule to ensure the noising steps are not too small at the start of the noising
    process.
  prefs: []
  type: TYPE_NORMAL
- en: We can compute the <math alttext="alpha overbar Subscript t"><msub><mover accent="true"><mi>Œ±</mi>
    <mo>¬Ø</mo></mover> <mi>t</mi></msub></math> values for each <math alttext="t"><mi>t</mi></math>
    to show how much signal ( <math alttext="alpha overbar Subscript t"><msub><mover
    accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover> <mi>t</mi></msub></math> ) and noise
    ( <math alttext="1 minus alpha overbar Subscript t"><mrow><mn>1</mn> <mo>-</mo>
    <msub><mover accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover> <mi>t</mi></msub></mrow></math>
    ) is let through at each stage of the process for the linear, cosine, and offset
    cosine diffusion schedules, as shown in [Figure¬†8-4](#signal_and_noise_linear).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-4\. The signal and noise at each step of the noising process, for the
    linear, cosine, and offset cosine diffusion schedules
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice how the noise level ramps up more slowly in the cosine diffusion schedule.
    A cosine diffusion schedule adds noise to the image more gradually than a linear
    diffusion schedule, which improves training efficiency and generation quality.
    This can also be seen in images that have been corrupted by the linear and cosine
    schedules ([Figure¬†8-5](#diff_schedule_examples)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0805.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-5\. An image being corrupted by the linear (top) and cosine (bottom)
    diffusion schedules, at equally spaced values of t from 0 to T (source: [Ho et
    al., 2020](https://arxiv.org/abs/2006.11239))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Reverse Diffusion Process
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now let‚Äôs look at the reverse diffusion process. To recap, we are looking to
    build a neural network <math alttext="p Subscript theta Baseline left-parenthesis
    bold x Subscript t minus 1 Baseline vertical-bar bold x Subscript t Baseline right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>Œ∏</mi></msub> <mrow><mo>(</mo> <msub><mi>ùê±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>|</mo> <msub><mi>ùê±</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
    that can *undo* the noising process‚Äîthat is, approximate the reverse distribution
    <math alttext="q left-parenthesis bold x Subscript t minus 1 Baseline vertical-bar
    bold x Subscript t Baseline right-parenthesis"><mrow><mi>q</mi> <mo>(</mo> <msub><mi>ùê±</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>|</mo> <msub><mi>ùê±</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow></math> . If we can do this, we can sample
    random noise from <math alttext="script upper N left-parenthesis 0 comma bold
    upper I right-parenthesis"><mrow><mi>ùí©</mi> <mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>ùêà</mi>
    <mo>)</mo></mrow></math> and then apply the reverse diffusion process multiple
    times in order to generate a novel image. This is visualized in [Figure¬†8-6](#reverse_diff).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. The reverse diffusion process <math alttext="p Subscript theta
    Baseline period left-parenthesis bold x Subscript t minus 1 Baseline vertical-bar
    bold x Subscript t Baseline right-parenthesis"><mrow><msub><mi>p</mi> <mi>Œ∏</mi></msub>
    <mo>.</mo> <mrow><mo>(</mo> <msub><mi>ùê±</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>|</mo> <msub><mi>ùê±</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math>
    tries to *undo* the noise produced by the forward diffusion process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There are many similarities between the reverse diffusion process and the decoder
    of a variational autoencoder. In both, we aim to transform random noise into meaningful
    output using a neural network. The difference between diffusion models and VAEs
    is that in a VAE the forward process (converting images to noise) is part of the
    model (i.e., it is learned), whereas in a diffusion model it is unparameterized.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it makes sense to apply the same loss function as in a variational
    autoencoder. The original DDPM paper derives the exact form of this loss function
    and shows that it can be optimized by training a network <math alttext="epsilon
    Subscript theta"><msub><mi>œµ</mi> <mi>Œ∏</mi></msub></math> to predict the noise
    <math alttext="epsilon"><mi>œµ</mi></math> that has been added to a given image
    <math alttext="bold x bold 0"><msub><mi>ùê±</mi> <mn mathvariant="bold">0</mn></msub></math>
    at timestep <math alttext="t"><mi>t</mi></math> .
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we sample an image <math alttext="bold x bold 0"><msub><mi>ùê±</mi>
    <mn mathvariant="bold">0</mn></msub></math> and transform it by <math alttext="t"><mi>t</mi></math>
    noising steps to get the image <math alttext="bold x Subscript t Baseline equals
    StartRoot alpha overbar Subscript t Baseline EndRoot bold x 0 plus StartRoot 1
    minus alpha overbar Subscript t Baseline EndRoot epsilon"><mrow><msub><mi>ùê±</mi>
    <mi>t</mi></msub> <mo>=</mo> <msqrt><msub><mover accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover>
    <mi>t</mi></msub></msqrt> <msub><mi>ùê±</mi> <mn>0</mn></msub> <mo>+</mo> <msqrt><mrow><mn>1</mn>
    <mo>-</mo> <msub><mover accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover> <mi>t</mi></msub></mrow></msqrt>
    <mi>œµ</mi></mrow></math> . We give this new image and the noising rate <math alttext="alpha
    overbar Subscript t"><msub><mover accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover>
    <mi>t</mi></msub></math> to the neural network and ask it to predict <math alttext="epsilon"><mi>œµ</mi></math>
    , taking a gradient step against the squared error between the prediction <math
    alttext="epsilon Subscript theta Baseline left-parenthesis bold x Subscript t
    Baseline right-parenthesis"><mrow><msub><mi>œµ</mi> <mi>Œ∏</mi></msub> <mrow><mo>(</mo>
    <msub><mi>ùê±</mi> <mi>t</mi></msub> <mo>)</mo></mrow></mrow></math> and the true
    <math alttext="epsilon"><mi>œµ</mi></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'We‚Äôll take a look at the structure of the neural network in the next section.
    It is worth noting here that the diffusion model actually maintains two copies
    of the network: one that is actively trained used gradient descent and another
    (the EMA network) that is an exponential moving average (EMA) of the weights of
    the actively trained network over previous training steps. The EMA network is
    not as susceptible to short-term fluctuations and spikes in the training process,
    making it more robust for generation than the actively trained network. We therefore
    use the EMA network whenever we want to produce generated output from the network.'
  prefs: []
  type: TYPE_NORMAL
- en: The training process for the model is shown in [Figure¬†8-7](#diff_training_process).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0807.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8-7\. The training process for a denoising diffusion model (source:
    [Ho et al., 2020](https://arxiv.org/abs/2006.11239))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In Keras, we can code up this training step as illustrated in [Example¬†8-5](#diffusion_train_step).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-5\. The `train_step` function of the Keras diffusion model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_diffusion_models_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We first normalize the batch of images to have zero mean and unit variance.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_diffusion_models_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we sample noise to match the shape of the input images.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_diffusion_models_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: We also sample random diffusion times‚Ä¶‚Äã
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_diffusion_models_CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: ‚Ä¶‚Äãand use these to generate the noise and signal rates according to the cosine
    diffusion schedule.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_diffusion_models_CO4-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Then we apply the signal and noise weightings to the input images to generate
    the noisy images.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_diffusion_models_CO4-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we denoise the noisy images by asking the network to predict the noise
    and then undoing the noising operation, using the provided `noise_rates` and `signal_rates`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_diffusion_models_CO4-7)'
  prefs: []
  type: TYPE_NORMAL
- en: We can then calculate the loss (mean absolute error) between the predicted noise
    and the true noise‚Ä¶‚Äã
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](Images/8.png)](#co_diffusion_models_CO4-8)'
  prefs: []
  type: TYPE_NORMAL
- en: ‚Ä¶‚Äãand take a gradient step against this loss function.
  prefs: []
  type: TYPE_NORMAL
- en: '[![9](Images/9.png)](#co_diffusion_models_CO4-9)'
  prefs: []
  type: TYPE_NORMAL
- en: The EMA network weights are updated to a weighted average of the existing EMA
    weights and the trained network weights after the gradient step.
  prefs: []
  type: TYPE_NORMAL
- en: The U-Net Denoising Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have seen the kind of neural network that we need to build (one
    that predicts the noise added to a given image), we can look at the architecture
    that makes this possible.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of the DDPM paper used a type of architecture known as a *U-Net*.
    A diagram of this network is shown in [Figure¬†8-8](#unet_diffusion), explicitly
    showing the shape of the tensor as it passes through the network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-8\. U-Net architecture diagram
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In a similar manner to a variational autoencoder, a U-Net consists of two halves:
    the downsampling half, where input images are compressed spatially but expanded
    channel-wise, and the upsampling half, where representations are expanded spatially
    while the number of channels is reduced. However, unlike in a VAE, there are also
    *skip connections* between equivalent spatially shaped layers in the upsampling
    and downsampling parts of the network. A VAE is sequential; data flows through
    the network from input to output, one layer after another. A U-Net is different,
    because the skip connections allow information to shortcut parts of the network
    and flow through to later layers.'
  prefs: []
  type: TYPE_NORMAL
- en: A U-Net is particularly useful when we want the output to have the same shape
    as the input. In our diffusion model example, we want to predict the noise added
    to an image, which has exactly the same shape as the image itself, so a U-Net
    is the natural choice for the network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: First let‚Äôs take a look at the code that builds this U-Net in Keras, shown in
    [Example¬†8-6](#unet_keras).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-6\. A U-Net model in Keras
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_diffusion_models_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The first input to the U-Net is the image that we wish to denoise.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_diffusion_models_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This image is passed through a `Conv2D` layer to increase the number of channels.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_diffusion_models_CO5-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The second input to the U-Net is the noise variance (a scalar).
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_diffusion_models_CO5-4)'
  prefs: []
  type: TYPE_NORMAL
- en: This is encoded using a sinusoidal embedding.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_diffusion_models_CO5-5)'
  prefs: []
  type: TYPE_NORMAL
- en: This embedding is copied across spatial dimensions to match the size of the
    input image.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_diffusion_models_CO5-6)'
  prefs: []
  type: TYPE_NORMAL
- en: The two input streams are concatenated across channels.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_diffusion_models_CO5-7)'
  prefs: []
  type: TYPE_NORMAL
- en: The `skips` list will hold the output from the `DownBlock` layers that we wish
    to connect to `UpBlock` layers downstream.
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](Images/8.png)](#co_diffusion_models_CO5-8)'
  prefs: []
  type: TYPE_NORMAL
- en: The tensor is passed through a series of `DownBlock` layers that reduce the
    size of the image, while increasing the number of channels.
  prefs: []
  type: TYPE_NORMAL
- en: '[![9](Images/9.png)](#co_diffusion_models_CO5-9)'
  prefs: []
  type: TYPE_NORMAL
- en: The tensor is then passed through two `ResidualBlock` layers that hold the image
    size and number of channels constant.
  prefs: []
  type: TYPE_NORMAL
- en: '[![10](Images/10.png)](#co_diffusion_models_CO5-10)'
  prefs: []
  type: TYPE_NORMAL
- en: Next, the tensor is passed through a series of `UpBlock` layers that increase
    the size of the image, while decreasing the number of channels. The skip connections
    incorporate output from the earlier `DownBlock` layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[![11](Images/11.png)](#co_diffusion_models_CO5-11)'
  prefs: []
  type: TYPE_NORMAL
- en: The final `Conv2D` layer reduces the number of channels to three (RGB).
  prefs: []
  type: TYPE_NORMAL
- en: '[![12](Images/12.png)](#co_diffusion_models_CO5-12)'
  prefs: []
  type: TYPE_NORMAL
- en: The U-Net is a Keras `Model` that takes the noisy images and noise variances
    as input and outputs a predicted noise map.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the U-Net in detail, we need to explore four more concepts: the
    sinusoidal embedding of the noise variance, the `ResidualBlock`, the `DownBlock`,
    and the `UpBlock`.'
  prefs: []
  type: TYPE_NORMAL
- en: Sinusoidal embedding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Sinusoidal embedding* was first introduced in a paper by Vaswani et al.^([6](ch08.xhtml#idm45387008220416))
    We will be using an adaptation of that original idea as utilized in Mildenhall
    et al.‚Äôs paper titled ‚ÄúNeRF: Representing Scenes as Neural Radiance Fields for
    View Synthesis.‚Äù^([7](ch08.xhtml#idm45387008216736))'
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that we want to be able to convert a scalar value (the noise variance)
    into a distinct higher-dimensional vector that is able to provide a more complex
    representation, for use downstream in the network. The original paper used this
    idea to encode the discrete position of words in a sentence into vectors; the
    NeRF paper extends this idea to continuous values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, a scalar value *x* is encoded as shown in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="gamma left-parenthesis x right-parenthesis equals left-parenthesis
    sine left-parenthesis 2 pi e Superscript 0 f Baseline x right-parenthesis comma
    ellipsis comma sine left-parenthesis 2 pi e Superscript left-parenthesis upper
    L minus 1 right-parenthesis f right-parenthesis Baseline x right-parenthesis comma
    cosine left-parenthesis 2 pi e Superscript 0 f Baseline x right-parenthesis comma
    ellipsis comma cosine left-parenthesis 2 pi e Superscript left-parenthesis upper
    L minus 1 right-parenthesis f Baseline x right-parenthesis right-parenthesis"
    display="block"><mrow><mi>Œ≥</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mo>(</mo> <mo form="prefix">sin</mo> <mrow><mo>(</mo> <mn>2</mn> <mi>œÄ</mi>
    <msup><mi>e</mi> <mrow><mn>0</mn><mi>f</mi></mrow></msup> <mi>x</mi> <mo>)</mo></mrow>
    <mo>,</mo> <mo>‚ãØ</mo> <mo>,</mo> <mo form="prefix">sin</mo> <mrow><mo>(</mo> <mn>2</mn>
    <mi>œÄ</mi> <msup><mi>e</mi> <mrow><mo>(</mo><mi>L</mi><mo>-</mo><mn>1</mn><mo>)</mo><mi>f</mi><mo>)</mo></mrow></msup>
    <mi>x</mi> <mo>)</mo></mrow> <mo>,</mo> <mo form="prefix">cos</mo> <mrow><mo>(</mo>
    <mn>2</mn> <mi>œÄ</mi> <msup><mi>e</mi> <mrow><mn>0</mn><mi>f</mi></mrow></msup>
    <mi>x</mi> <mo>)</mo></mrow> <mo>,</mo> <mo>‚ãØ</mo> <mo>,</mo> <mo form="prefix">cos</mo>
    <mrow><mo>(</mo> <mn>2</mn> <mi>œÄ</mi> <msup><mi>e</mi> <mrow><mo>(</mo><mi>L</mi><mo>-</mo><mn>1</mn><mo>)</mo><mi>f</mi></mrow></msup>
    <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where we choose <math alttext="upper L equals 16"><mrow><mi>L</mi> <mo>=</mo>
    <mn>16</mn></mrow></math> to be half the size of our desired noise embedding length
    and <math alttext="f equals StartFraction ln left-parenthesis 1000 right-parenthesis
    Over upper L minus 1 EndFraction"><mrow><mi>f</mi> <mo>=</mo> <mfrac><mrow><mo
    form="prefix">ln</mo><mo>(</mo><mn>1000</mn><mo>)</mo></mrow> <mrow><mi>L</mi><mo>-</mo><mn>1</mn></mrow></mfrac></mrow></math>
    to be the maximum scaling factor for the frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: This produces the embedding pattern shown in [Figure¬†8-9](#sinusoidal_embedding_image).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0809.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-9\. The pattern of sinusoidal embeddings for noise variances from 0
    to 1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can code this sinusoidal embedding function as shown in [Example¬†8-7](#sinusoidal_embedding_diffusion).
    This converts a single noise variance scalar value into a vector of length 32.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-7\. The `sinusoidal_embedding` function that encodes the noise variance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: ResidualBlock
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Both the `DownBlock` and the `UpBlock` contain `ResidualBlock` layers, so let‚Äôs
    start with these. We already explored residual blocks in [Chapter¬†5](ch05.xhtml#chapter_autoregressive),
    when we built a PixelCNN, but we will recap here for completeness.
  prefs: []
  type: TYPE_NORMAL
- en: A *residual block* is a group of layers that contains a skip connection that
    adds the input to the output. Residual blocks help us to build deeper networks
    that can learn more complex patterns without suffering as greatly from vanishing
    gradient and degradation problems. The vanishing gradient problem is the assertion
    that as the network gets deeper, the gradient propagated through deeper layers
    is tiny and therefore learning is very slow. The degradation problem is the fact
    that as neural networks become deeper, they are not necessarily as accurate as
    their shallower counterparts‚Äîaccuracy seems to become saturated at a certain depth
    and then degrade rapidly.
  prefs: []
  type: TYPE_NORMAL
- en: Degradation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The degradation problem is somewhat counterintuitive, but observed in practice
    as the deeper layers must at least learn the identity mapping, which is not trivial‚Äîespecially
    considering other problems deeper networks face, such as the vanishing gradient
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: The solution, first introduced in the ResNet paper by He et al. in 2015,^([8](ch08.xhtml#idm45387008052288))
    is very simple. By including a skip connection *highway* around the main weighted
    layers, the block has the option to bypass the complex weight updates and simply
    pass through the identity mapping. This allows the network to be trained to great
    depth without sacrificing gradient size or network accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: A diagram of a `ResidualBlock` is shown in [Figure¬†8-10](#diffusion_residual).
    Note that in some residual blocks, we also include an extra `Conv2D` layer with
    kernel size 1 on the skip connection, to bring the number of channels in line
    with the rest of the block.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0810.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-10\. The `ResidualBlock` in the U-Net
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can code a `ResidualBlock` in Keras as shown in [Example¬†8-8](#diffusion_residual_code).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-8\. Code for the `ResidualBlock` in the U-Net
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_diffusion_models_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Check if the number of channels in the input matches the number of channels
    that we would like the block to output. If not, include an extra `Conv2D` layer
    on the skip connection to bring the number of channels in line with the rest of
    the block.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_diffusion_models_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Apply a `BatchNormalization` layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_diffusion_models_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Apply two `Conv2D` layers.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_diffusion_models_CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Add the original block input to the output to provide the final output from
    the block.
  prefs: []
  type: TYPE_NORMAL
- en: DownBlocks and UpBlocks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each successive `DownBlock` increases the number of channels via `block_depth`
    (=2 in our example) `ResidualBlock`s, while also applying a final `AveragePooling2D`
    layer in order to halve the size of the image. Each `ResidualBlock` is added to
    a list for use later by the `UpBlock` layers as skip connections across the U-Net.
  prefs: []
  type: TYPE_NORMAL
- en: An `UpBlock` first applies an `UpSampling2D` layer that doubles the size of
    the image, through bilinear interpolation. Each successive `UpBlock` decreases
    the number of channels via `block_depth` (=2) `ResidualBlock`s, while also concatenating
    the outputs from the `DownBlock`s through skip connections across the U-Net. A
    diagram of this process is shown in [Figure¬†8-11](#diffusion_down_up_block).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0811.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-11\. The `DownBlock` and corresponding `UpBlock` in the U-Net
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can code the `DownBlock` and `UpBlock` using Keras as illustrated in [Example¬†8-9](#diffusion_down_up_code).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-9\. Code for the `DownBlock` and `UpBlock` in the U-Net model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_diffusion_models_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `DownBlock` increases the number of channels in the image using a `ResidualBlock`
    of a given `width`‚Ä¶‚Äã
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_diffusion_models_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: ‚Ä¶‚Äãeach of which are saved to a list (`skips`) for use later by the `UpBlock`s.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_diffusion_models_CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: A final `AveragePooling2D` layer reduces the dimensionality of the image by
    half.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_diffusion_models_CO7-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The `UpBlock` begins with an `UpSampling2D` layer that doubles the size of the
    image.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_diffusion_models_CO7-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The output from a `DownBlock` layer is glued to the current output using a `Concatenate`
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_diffusion_models_CO7-6)'
  prefs: []
  type: TYPE_NORMAL
- en: A `ResidualBlock` is used to reduce the number of channels in the image as it
    passes through the `UpBlock`.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Diffusion Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We now have all the components in place to train our denoising diffusion model!
    [Example¬†8-10](#diffusion_train_code) creates, compiles, and fits the diffusion
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-10\. Code for training the `DiffusionModel`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_diffusion_models_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_diffusion_models_CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Compile the model, using the AdamW optimizer (similar to Adam but with weight
    decay, which helps stabilize the training process) and mean absolute error loss
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_diffusion_models_CO8-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the normalization statistics using the training set.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_diffusion_models_CO8-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Fit the model over 50 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: The loss curve (noise mean absolute error [MAE]) is shown in [Figure¬†8-12](#diffusion_loss).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0812.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-12\. The noise mean absolute error loss curve, by epoch
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sampling from the Denoising Diffusion Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to sample images from our trained model, we need to apply the reverse
    diffusion process‚Äîthat is, we need to start with random noise and use the model
    to gradually undo the noise, until we are left with a recognizable picture of
    a flower.
  prefs: []
  type: TYPE_NORMAL
- en: We must bear in mind that our model is trained to predict the total amount of
    noise that has been added to a given noisy image from the training set, not just
    the noise that was added at the last timestep of the noising process. However,
    we do not want to undo the noise all in one go‚Äîpredicting an image from pure random
    noise in one shot is clearly not going to work! We would rather mimic the forward
    process and undo the predicted noise gradually over many small steps, to allow
    the model to adjust to its own predictions.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, we can jump from <math alttext="x Subscript t"><msub><mi>x</mi>
    <mi>t</mi></msub></math> to <math alttext="x Subscript t minus 1"><msub><mi>x</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math> in two steps‚Äîfirst by
    using our model‚Äôs noise prediction to calculate an estimate for the original image
    <math alttext="x 0"><msub><mi>x</mi> <mn>0</mn></msub></math> and then by reapplying
    the predicted noise to this image, but only over <math alttext="t minus 1"><mrow><mi>t</mi>
    <mo>-</mo> <mn>1</mn></mrow></math> timesteps, to produce <math alttext="x Subscript
    t minus 1"><msub><mi>x</mi> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></math>
    . This idea is shown in [Figure¬†8-13](#diffusion_one_step_sample).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0813.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-13\. One step of the sampling process for our diffusion model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If we repeat this process over a number of steps, we‚Äôll eventually get back
    to an estimate for <math alttext="x 0"><msub><mi>x</mi> <mn>0</mn></msub></math>
    that has been guided gradually over many small steps. In fact, we are free to
    choose the number of steps we take, and crucially, it doesn‚Äôt have to be the same
    as the large number of steps in the training noising process (i.e., 1,000). It
    can be much smaller‚Äîin this example we choose 20.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following equation (Song et al., 2020) this process mathematically:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="bold x Subscript t minus 1 Baseline equals StartRoot alpha overbar
    Subscript t minus 1 Baseline EndRoot ModifyingBelow left-parenthesis StartFraction
    bold x Subscript t Baseline minus StartRoot 1 minus alpha overbar Subscript t
    Baseline EndRoot epsilon Subscript theta Superscript left-parenthesis t right-parenthesis
    Baseline left-parenthesis bold x Subscript t Baseline right-parenthesis Over StartRoot
    alpha overbar Subscript t Baseline EndRoot EndFraction right-parenthesis With
    bottom-brace Underscript predicted bold x 0 Endscripts plus ModifyingBelow StartRoot
    1 minus alpha overbar Subscript t minus 1 Baseline minus sigma Subscript t Superscript
    2 Baseline EndRoot dot epsilon Subscript theta Superscript left-parenthesis t
    right-parenthesis Baseline left-parenthesis bold x Subscript t Baseline right-parenthesis
    With bottom-brace Underscript direction pointing to bold x Subscript t Baseline
    Endscripts plus ModifyingBelow sigma Subscript t Baseline epsilon Subscript t
    Baseline With bottom-brace Underscript random noise Endscripts" display="block"><mrow><msub><mi>ùê±</mi>
    <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub> <mo>=</mo> <msqrt><msub><mover
    accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></msqrt>
    <munder><munder accentunder="true"><mfenced separators="" open="(" close=")"><mfrac><mrow><msub><mi>ùê±</mi>
    <mi>t</mi></msub> <mo>-</mo><msqrt><mrow><mn>1</mn><mo>-</mo><msub><mover accent="true"><mi>Œ±</mi>
    <mo>¬Ø</mo></mover> <mi>t</mi></msub></mrow></msqrt> <msubsup><mi>œµ</mi> <mi>Œ∏</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup> <mrow><mo>(</mo><msub><mi>ùê±</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow></mrow> <msqrt><msub><mover accent="true"><mi>Œ±</mi>
    <mo>¬Ø</mo></mover> <mi>t</mi></msub></msqrt></mfrac></mfenced> <mo>Ô∏∏</mo></munder>
    <mrow><mtext>predicted</mtext><msub><mi>ùê±</mi> <mn>0</mn></msub></mrow></munder>
    <mo>+</mo> <munder><munder accentunder="true"><mrow><msqrt><mrow><mn>1</mn><mo>-</mo><msub><mover
    accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>-</mo><msubsup><mi>œÉ</mi> <mi>t</mi> <mn>2</mn></msubsup></mrow></msqrt> <mo>¬∑</mo><msubsup><mi>œµ</mi>
    <mi>Œ∏</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup> <mrow><mo>(</mo><msub><mi>ùê±</mi>
    <mi>t</mi></msub> <mo>)</mo></mrow></mrow> <mo>Ô∏∏</mo></munder> <mrow><mtext>direction</mtext><mtext>pointing</mtext><mtext>to</mtext><msub><mi>ùê±</mi>
    <mi>t</mi></msub></mrow></munder> <mo>+</mo> <munder><munder accentunder="true"><mrow><msub><mi>œÉ</mi>
    <mi>t</mi></msub> <msub><mi>œµ</mi> <mi>t</mi></msub></mrow> <mo>Ô∏∏</mo></munder>
    <mrow><mtext>random</mtext><mtext>noise</mtext></mrow></munder></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs break this down. The first term inside the brackets on the righthand side
    of the equation is the estimated image <math alttext="x 0"><msub><mi>x</mi> <mn>0</mn></msub></math>
    , calculated using the noise predicted by our network <math alttext="epsilon Subscript
    theta Superscript left-parenthesis t right-parenthesis"><msubsup><mi>œµ</mi> <mi>Œ∏</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msubsup></math> . We then scale this
    by the <math alttext="t minus 1"><mrow><mi>t</mi> <mo>-</mo> <mn>1</mn></mrow></math>
    signal rate <math alttext="StartRoot alpha overbar Subscript t minus 1 Baseline
    EndRoot"><msqrt><msub><mover accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub></msqrt></math>
    and reapply the predicted noise, but this time scaled by the <math alttext="t
    minus 1"><mrow><mi>t</mi> <mo>-</mo> <mn>1</mn></mrow></math> noise rate <math
    alttext="StartRoot 1 minus alpha overbar Subscript t minus 1 Baseline minus sigma
    Subscript t Superscript 2 Baseline EndRoot"><msqrt><mrow><mn>1</mn> <mo>-</mo>
    <msub><mover accent="true"><mi>Œ±</mi> <mo>¬Ø</mo></mover> <mrow><mi>t</mi><mo>-</mo><mn>1</mn></mrow></msub>
    <mo>-</mo> <msubsup><mi>œÉ</mi> <mi>t</mi> <mn>2</mn></msubsup></mrow></msqrt></math>
    . Additional Gaussian random noise <math alttext="sigma Subscript t Baseline epsilon
    Subscript t"><mrow><msub><mi>œÉ</mi> <mi>t</mi></msub> <msub><mi>œµ</mi> <mi>t</mi></msub></mrow></math>
    is also added, with the factors <math alttext="sigma Subscript t"><msub><mi>œÉ</mi>
    <mi>t</mi></msub></math> determining how random we want our generation process
    to be.
  prefs: []
  type: TYPE_NORMAL
- en: The special case <math alttext="sigma Subscript t Baseline equals 0"><mrow><msub><mi>œÉ</mi>
    <mi>t</mi></msub> <mo>=</mo> <mn>0</mn></mrow></math> for all <math alttext="t"><mi>t</mi></math>
    corresponds to a type of model known as a *Denoising Diffusion Implicit Model*
    (DDIM), introduced by Song et al. in 2020.^([9](ch08.xhtml#idm45387007342688))
    With a DDIM, the generation process is entirely deterministic‚Äîthat is, the same
    random noise input will always give the same output. This is desirable as then
    we have a well-defined mapping between samples from the latent space and the generated
    outputs in pixel space.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we will implement a DDIM, thus making our generation process
    deterministic. The code for the DDIM sampling process (reverse diffusion) is shown
    in [Example¬†8-11](#diffusion_sampling).
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-11\. Sampling from the diffusion model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_diffusion_models_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Look over a fixed number of steps (e.g., 20).
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_diffusion_models_CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The diffusion times are all set to 1 (i.e., at the start of the reverse diffusion
    process).
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_diffusion_models_CO9-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The noise and signal rates are calculated according to the diffusion schedule.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_diffusion_models_CO9-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The U-Net is used to predict the noise, allowing us to calculate the denoised
    image estimate.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](Images/5.png)](#co_diffusion_models_CO9-5)'
  prefs: []
  type: TYPE_NORMAL
- en: The diffusion times are reduced by one step.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](Images/6.png)](#co_diffusion_models_CO9-6)'
  prefs: []
  type: TYPE_NORMAL
- en: The new noise and signal rates are calculated.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](Images/7.png)](#co_diffusion_models_CO9-7)'
  prefs: []
  type: TYPE_NORMAL
- en: The `t-1` images are calculated by reapplying the predicted noise to the predicted
    image, according to the `t-1` diffusion schedule rates.
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](Images/8.png)](#co_diffusion_models_CO9-8)'
  prefs: []
  type: TYPE_NORMAL
- en: After 20 steps, the final <math alttext="bold x 0"><msub><mi>ùê±</mi> <mn>0</mn></msub></math>
    predicted images are returned.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis of the Diffusion Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We‚Äôll now take a look at three different ways that we can use our trained model:
    for generation of new images, testing how the number of reverse diffusion steps
    affects quality, and interpolating between two images in the latent space.'
  prefs: []
  type: TYPE_NORMAL
- en: Generating images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to produce samples from our trained model, we can simply run the reverse
    diffusion process, ensuring that we denormalize the output at the end (i.e., take
    the pixel values back into the range [0, 1]). We can achieve this using the code
    in [Example¬†8-12](#diffusion_generation) inside the `DiffusionModel` class.
  prefs: []
  type: TYPE_NORMAL
- en: Example 8-12\. Generating images using the diffusion model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_diffusion_models_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Generate some initial noise maps.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_diffusion_models_CO10-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Apply the reverse diffusion process.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_diffusion_models_CO10-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The images output by the network will have mean zero and unit variance, so we
    need to denormalize by reapplying the mean and variance calculated from the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: In [Figure¬†8-14](#diffusion_samples_epoch) we can observe some samples from
    the diffusion model at different epochs of the training process.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0814.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-14\. Samples from the diffusion model at different epochs of the training
    process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Adjusting the number of diffusion steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can also test to see how adjusting the number of diffusion steps in the reverse
    process affects image quality. Intuitively, the more steps taken by the process,
    the higher the quality of the image generation.
  prefs: []
  type: TYPE_NORMAL
- en: We can see in [Figure¬†8-15](#diffusion_steps_quality) that the quality of the
    generations does indeed improve with the number of diffusion steps. With one giant
    leap from the initial sampled noise, the model can only predict a hazy blob of
    color. With more steps, the model is able to refine and sharpen its generations.
    However, the time taken to generate the images scales linearly with the number
    of diffusion steps, so there is a trade-off. There is minimal improvement between
    20 and 100 diffusion steps, so we choose 20 as a reasonable compromise between
    quality and speed in this example.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0815.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-15\. Image quality improves with the number of diffusion steps
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Interpolating between images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lastly, as we have seen previously with variational autoencoders, we can interpolate
    between points in the Gaussian latent space in order to smoothly transition between
    images in pixel space. Here we choose to use a form of spherical interpolation
    that ensures that the variance remains constant while blending the two Gaussian
    noise maps together. Specifically, the initial noise map at each step is given
    by <math alttext="a sine left-parenthesis StartFraction pi Over 2 EndFraction
    t right-parenthesis plus b cosine left-parenthesis StartFraction pi Over 2 EndFraction
    t right-parenthesis"><mrow><mi>a</mi> <mo form="prefix">sin</mo> <mrow><mo>(</mo>
    <mfrac><mi>œÄ</mi> <mn>2</mn></mfrac> <mi>t</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>b</mi>
    <mo form="prefix">cos</mo> <mrow><mo>(</mo> <mfrac><mi>œÄ</mi> <mn>2</mn></mfrac>
    <mi>t</mi> <mo>)</mo></mrow></mrow></math> , where <math alttext="t"><mi>t</mi></math>
    ranges smoothly from 0 to 1 and <math alttext="a"><mi>a</mi></math> and <math
    alttext="b"><mi>b</mi></math> are the two randomly sampled Gaussian noise tensors
    that we wish to interpolate between.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting images are shown in [Figure¬†8-16](#diffusion_interpolation).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/gdl2_0816.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8-16\. Interpolating between images using the denoising diffusion model`  `#
    Summary
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this chapter we have explored one of the most exciting and promising areas
    of generative modeling in recent times: diffusion models. In particular, we implemented
    the ideas from a key paper on generative diffusion models (Ho et al., 2020) that
    introduced the original Denoising Diffusion Probabilistic Model (DDPM). We then
    extended this with the ideas from the Denoising Diffusion Implicit Model (DDIM)
    paper to make the generation process fully deterministic.'
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how diffusion models are formed of a forward diffusion process
    and a reverse diffusion process. The forward diffusion process adds noise to the
    training data through a series of small steps, while the reverse diffusion process
    consists of a model that tries to predict the noise added.
  prefs: []
  type: TYPE_NORMAL
- en: We make use of a reparameterization trick in order to calculate the noised images
    at any step of the forward process without having to go through multiple noising
    steps. We have seen how the chosen schedule of parameters used to add noise to
    the data plays an important part in the overall success of the model.
  prefs: []
  type: TYPE_NORMAL
- en: The reverse diffusion process is parameterized by a U-Net that tries to predict
    the noise at each timestep, given the noised image and the noise rate at that
    step. A U-Net consists of `DownBlock`s that increase the number of channels while
    reducing the size of the image and `UpBlock`s that decrease the number of channels
    while increasing the size. The noise rate is encoded using sinusoidal embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling from the diffusion model is conducted over a series of steps. The U-Net
    is used to predict the noise added to a given noised image, which is then used
    to calculate an estimate for the original image. The predicted noise is then reapplied
    using a smaller noise rate. This process is repeated over a series of steps (which
    may be significantly smaller than the number of steps used during training), starting
    from a random point sampled from a standard Gaussian noise distribution, to obtain
    the final generation.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how increasing the number of diffusion steps in the reverse process improves
    the image generation quality, at the expense of speed. We also performed latent
    space arithmetic in order to interpolate between two images.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch08.xhtml#idm45387010500320-marker)) Jascha Sohl-Dickstein et al., ‚ÄúDeep
    Unsupervised Learning Using Nonequilibrium Thermodynamics,‚Äù March 12, 2015, [*https://arxiv.org/abs/1503.03585*](https://arxiv.org/abs/1503.03585)
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch08.xhtml#idm45387010496240-marker)) Yang Song and Stefano Ermon, ‚ÄúGenerative
    Modeling by Estimating Gradients of the Data Distribution,‚Äù July 12, 2019, [*https://arxiv.org/abs/1907.05600*](https://arxiv.org/abs/1907.05600).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch08.xhtml#idm45387010494000-marker)) Yang Song and Stefano Ermon, ‚ÄúImproved
    Techniques for Training Score-Based Generative Models,‚Äù June 16, 2020, [*https://arxiv.org/abs/2006.09011*](https://arxiv.org/abs/2006.09011).
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch08.xhtml#idm45387010490880-marker)) Jonathon Ho et al., ‚ÄúDenoising Diffusion
    Probabilistic Models,‚Äù June 19, 2020, [*https://arxiv.org/abs/2006.11239*](https://arxiv.org/abs/2006.11239).
  prefs: []
  type: TYPE_NORMAL
- en: ^([5](ch08.xhtml#idm45387010764208-marker)) Alex Nichol and Prafulla Dhariwal,
    ‚ÄúImproved Denoising Diffusion Probabilistic Models,‚Äù February 18, 2021, [*https://arxiv.org/abs/2102.09672*](https://arxiv.org/abs/2102.09672).
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch08.xhtml#idm45387008220416-marker)) Ashish Vaswani et al., ‚ÄúAttention
    Is All You Need,‚Äù June 12, 2017, [*https://arxiv.org/abs/1706.03762*](https://arxiv.org/abs/1706.03762).
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch08.xhtml#idm45387008216736-marker)) Ben Mildenhall et al., ‚ÄúNeRF: Representing
    Scenes as Neural Radiance Fields for View Synthesis,‚Äù March 1, 2020, [*https://arxiv.org/abs/2003.08934*](https://arxiv.org/abs/2003.08934).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch08.xhtml#idm45387008052288-marker)) Kaiming He et al., ‚ÄúDeep Residual
    Learning for Image Recognition,‚Äù December 10, 2015, [*https://arxiv.org/abs/1512.03385*](https://arxiv.org/abs/1512.03385).
  prefs: []
  type: TYPE_NORMAL
- en: ^([9](ch08.xhtml#idm45387007342688-marker)) Jiaming Song et al., ‚ÄúDenoising
    Diffusion Implicit Models,‚Äù October 6, 2020, [*https://arxiv.org/abs/2010.02502*](https://arxiv.org/abs/2010.02502)`
  prefs: []
  type: TYPE_NORMAL
