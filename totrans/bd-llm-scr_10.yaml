- en: appendix D Adding bells and whistles to the training loop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this appendix, we enhance the training function for the pretraining and fine-tuning
    processes covered in chapters 5 to 7\. In particular, it covers *learning rate
    warmup*, *cosine decay*, and *gradient clipping*. We then incorporate these techniques
    into the training function and pretrain an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make the code self-contained, we reinitialize the model we trained in chapter
    5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Vocabulary size'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Shortened context length (orig: 1024)'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Embedding dimension'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Number of attention heads'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Number of layers'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Dropout rate'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Query-key-value bias'
  prefs: []
  type: TYPE_NORMAL
- en: 'After initializing the model, we need to initialize the data loaders. First,
    we load the “The Verdict” short story:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we load the `text_data` into the data loaders:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: D.1 Learning rate warmup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Implementing a learning rate warmup can stabilize the training of complex models
    such as LLMs. This process involves gradually increasing the learning rate from
    a very low initial value (`initial_lr`) to a maximum value specified by the user
    (`peak_lr`). Starting the training with smaller weight updates decreases the risk
    of the model encountering large, destabilizing updates during its training phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we plan to train an LLM for 15 epochs, starting with an initial learning
    rate of 0.0001 and increasing it to a maximum learning rate of 0.01:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The number of warmup steps is usually set between 0.1% and 20% of the total
    number of steps, which we can calculate as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 20% warmup'
  prefs: []
  type: TYPE_NORMAL
- en: This prints `27`, meaning that we have 20 warmup steps to increase the initial
    learning rate from 0.0001 to 0.01 in the first 27 training steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we implement a simple training loop template to illustrate this warmup
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 This increment is determined by how much we increase the inital_lr in each
    of the 20 warmup steps.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Executes a typical training loop iterating over the batches in the training
    loader in each epoch'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Updates the learning rate if we are still in the warmup phase'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Applies the calculated learning rate to the optimizer'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 In a complete training loop, the loss and the model updates would be calculated,
    which are omitted here for simplicity.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the preceding code, we visualize how the learning rate was changed
    by the training loop to verify that the learning rate warmup works as intended:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The resulting plot shows that the learning rate starts with a low value and
    increases for 20 steps until it reaches the maximum value after 20 steps (figure
    D.1).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/D-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure D.1 The learning rate warmup increases the learning rate for the first
    20 training steps. After 20 steps, the learning rate reaches the peak of 0.01
    and remains constant for the rest of the training.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Next, we will modify the learning rate further so that it decreases after reaching
    the maximum learning rate, which further helps improve the model training.
  prefs: []
  type: TYPE_NORMAL
- en: D.2 Cosine decay
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another widely adopted technique for training complex deep neural networks and
    LLMs is *cosine decay*. This method modulates the learning rate throughout the
    training epochs, making it follow a cosine curve after the warmup stage.
  prefs: []
  type: TYPE_NORMAL
- en: In its popular variant, cosine decay reduces (or decays) the learning rate to
    nearly zero, mimicking the trajectory of a half-cosine cycle. The gradual learning
    decrease in cosine decay aims to decelerate the pace at which the model updates
    its weights. This is particularly important because it helps minimize the risk
    of overshooting the loss minima during the training process, which is essential
    for ensuring the stability of the training during its later phases.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can modify the training loop template by adding cosine decay:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Applies linear warmup'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Uses cosine annealing after warmup'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, to verify that the learning rate has changed as intended, we plot the
    learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The resulting learning rate plot shows that the learning rate starts with a
    linear warmup phase, which increases for 20 steps until it reaches the maximum
    value after 20 steps. After the 20 steps of linear warmup, cosine decay kicks
    in, reducing the learning rate gradually until it reaches its minimum (figure
    D.2).
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/D-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure D.2 The first 20 steps of linear learning rate warmup are followed by
    a cosine decay, which reduces the learning rate in a half-cosine cycle until it
    reaches its minimum point at the end of training.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: D.3 Gradient clipping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Gradient clipping* is another important technique for enhancing stability
    during LLM training. This method involves setting a threshold above which gradients
    are downscaled to a predetermined maximum magnitude. This process ensures that
    the updates to the model’s parameters during backpropagation stay within a manageable
    range.'
  prefs: []
  type: TYPE_NORMAL
- en: For example, applying the `max_norm=1.0` setting within PyTorch’s `clip_grad_`
    `norm_` function ensures that the norm of the gradients does not surpass 1.0\.
    Here, the term “norm” signifies the measure of the gradient vector’s length, or
    magnitude, within the model’s parameter space, specifically referring to the L2
    norm, also known as the Euclidean norm.
  prefs: []
  type: TYPE_NORMAL
- en: In mathematical terms, for a vector ***v*** composed of components ***v*** =
    [*v*[1], *v*[2], ..., *v**n*], the L2 norm is
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/Equation-eqs-D-1.png)'
  prefs: []
  type: TYPE_IMG
- en: This calculation method is also applied to matrices. For instance, consider
    a gradient matrix given by
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/Equation-eqs-D-2.png)'
  prefs: []
  type: TYPE_IMG
- en: If we want to clip these gradients to a `max_norm` of 1, we first compute the
    L2 norm of these gradients, which is
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/Equation-eqs-D-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Given that |**G**|[2] = 5 exceeds our `max_norm` of 1, we scale down the gradients
    to ensure their norm equals exactly 1\. This is achieved through a scaling factor,
    calculated as `max_norm`/|**G**|[2] = 1/5\. Consequently, the adjusted gradient
    matrix **G'** becomes
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/Equation-eqs-D-4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To illustrate this gradient clipping process, we begin by initializing a new
    model and calculating the loss for a training batch, similar to the procedure
    in a standard training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Upon calling the `.backward()` method, PyTorch calculates the loss gradients
    and stores them in a `.grad` attribute for each model weight (parameter) tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'To clarify the point, we can define the following `find_highest_gradient` utility
    function to identify the highest gradient value by scanning all the `.grad` attributes
    of the model’s weight tensors after calling `.backward()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The largest gradient value identified by the preceding code is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now apply gradient clipping and see how this affects the largest gradient
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The largest gradient value after applying the gradient clipping with the max
    norm of 1 is substantially smaller than before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: D.4 The modified training function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, we improve the `train_model_simple` training function (see chapter
    5) by adding the three concepts introduced herein: linear warmup, cosine decay,
    and gradient clipping. Together, these methods help stabilize LLM training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The code, with the changes compared to the `train_model_simple` annotated,
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Retrieves the initial learning rate from the optimizer, assuming we use
    it as the peak learning rate'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Calculates the total number of iterations in the training process'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Calculates the learning rate increment during the warmup phase'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Adjusts the learning rate based on the current phase (warmup or cosine annealing)'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 **Applies the calculated learning rate to the optimizer**'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Applies gradient clipping after the warmup phase to avoid exploding gradients'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Everything below here remains unchanged compared to the train_model_simple
    function used in chapter 5.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After defining the `train_model` function, we can use it in a similar fashion
    to train the model compared to the `train_model_simple` method we used for pretraining:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The training will take about 5 minutes to complete on a MacBook Air or similar
    laptop and prints the following outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Like pretraining, the model begins to overfit after a few epochs since it is
    a very small dataset, and we iterate over it multiple times. Nonetheless, we can
    see that the function is working since it minimizes the training set loss.
  prefs: []
  type: TYPE_NORMAL
- en: Readers are encouraged to train the model on a larger text dataset and compare
    the results obtained with this more sophisticated training function to the results
    that can be obtained with the `train_model_simple` function.
  prefs: []
  type: TYPE_NORMAL
