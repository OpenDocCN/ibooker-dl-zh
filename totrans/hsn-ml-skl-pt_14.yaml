- en: Chapter 12\. Deep Computer Vision Using Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章\. 使用卷积神经网络的深度计算机视觉
- en: 'Although IBM’s Deep Blue supercomputer beat the chess world champion Garry
    Kasparov back in 1996, it wasn’t until fairly recently that computers were able
    to reliably perform seemingly trivial tasks such as detecting a puppy in a picture
    or recognizing spoken words. Why are these tasks so effortless to us humans? The
    answer lies in the fact that perception largely takes place outside the realm
    of our consciousness, within specialized visual, auditory, and other sensory modules
    in our brains. By the time sensory information reaches our consciousness, it is
    already adorned with high-level features; for example, when you look at a picture
    of a cute puppy, you cannot choose *not* to see the puppy, *not* to notice its
    cuteness. Nor can you explain *how* you recognize a cute puppy; it’s just obvious
    to you. Thus, we cannot trust our subjective experience: perception is not trivial
    at all, and to understand it we must look at how our sensory modules work.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管IBM的Deep Blue超级计算机在1996年击败了国际象棋世界冠军加里·卡斯帕罗夫，但直到最近，计算机才能可靠地执行看似微不足道的任务，比如在图片中检测小狗或识别语音。为什么这些任务对我们人类来说如此容易？答案在于，感知在很大程度上发生在我们的意识之外，在我们大脑中专门的视觉、听觉和其他感官模块内。当感官信息到达我们的意识时，它已经带有高级特征；例如，当你看一张可爱小狗的图片时，你无法选择*不*看到小狗，*不*注意到它的可爱。你也不能解释*如何*识别一只可爱的小狗；这对你来说只是显而易见的。因此，我们不能相信我们的主观经验：感知绝非微不足道，要理解它，我们必须看看我们的感官模块是如何工作的。
- en: '*Convolutional neural networks* (CNNs) emerged from the study of the brain’s
    visual cortex, and they have been used in computer image recognition since the
    1980s. Over the last 15 years, thanks to the increase in computational power,
    the amount of available training data, and the tricks presented in [Chapter 11](ch11.html#deep_chapter)
    for training deep nets, CNNs have managed to achieve superhuman performance on
    some complex visual tasks. They power image search services, self-driving cars,
    automatic video classification systems, and more. Moreover, CNNs are not restricted
    to visual perception: they are also successful at many other tasks, such as voice
    recognition and natural language processing. However, we will focus on visual
    applications for now.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*卷积神经网络*（CNNs）起源于对大脑视觉皮层的研究，自20世纪80年代以来，它们一直被用于计算机图像识别。在过去15年里，得益于计算能力的提升、可用训练数据的增加，以及[第11章](ch11.html#deep_chapter)中提出的用于训练深度网络的技巧，CNNs在处理一些复杂的视觉任务上已经达到了超越人类的表现。它们为图像搜索服务、自动驾驶汽车、自动视频分类系统等提供了动力。此外，CNNs不仅限于视觉感知：它们在许多其他任务中也取得了成功，例如语音识别和自然语言处理。然而，我们现在将专注于视觉应用。'
- en: In this chapter we will explore where CNNs came from, what their building blocks
    look like, and how to implement them using PyTorch. Then we will discuss some
    of the best CNN architectures, as well as other visual tasks, including object
    detection (classifying multiple objects in an image and placing bounding boxes
    around them) and semantic segmentation (classifying each pixel according to the
    class of the object it belongs to).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨卷积神经网络（CNNs）的起源，它们的基本构建块是什么样的，以及如何使用PyTorch来实现它们。然后我们将讨论一些最佳的CNN架构，以及其他视觉任务，包括目标检测（在图像中分类多个对象并在它们周围放置边界框）和语义分割（根据对象所属的类别对每个像素进行分类）。
- en: The Architecture of the Visual Cortex
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视觉皮层的架构
- en: David H. Hubel and Torsten Wiesel performed a series of experiments on cats
    in [1958](https://homl.info/71)⁠^([1](ch12.html#id2734)) and [1959](https://homl.info/72)⁠^([2](ch12.html#id2735))
    (and a [few years later on monkeys](https://homl.info/73)⁠^([3](ch12.html#id2736))),
    giving crucial insights into the structure of the visual cortex (the authors received
    the Nobel Prize in Physiology or Medicine in 1981 for their work). In particular,
    they showed that many neurons in the visual cortex have a small *local receptive
    field*, meaning they react only to visual stimuli located in a limited region
    of the visual field (see [Figure 12-1](#cat_visual_cortex_diagram), in which the
    local receptive fields of five neurons are represented by dashed circles). The
    receptive fields of different neurons may overlap, and together they tile the
    whole visual field.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 大卫·H·休伯尔和托斯顿·威塞尔在[1958年](https://homl.info/71)⁠^([1](ch12.html#id2734))和[1959年](https://homl.info/72)⁠^([2](ch12.html#id2735))（以及几年后对猴子的实验](https://homl.info/73)⁠^([3](ch12.html#id2736)))对猫进行了一系列实验，为视觉皮层的结构（作者因他们的工作在1981年获得了诺贝尔生理学或医学奖）提供了关键见解。特别是，他们展示了视觉皮层中的许多神经元具有小的**局部感受野**，这意味着它们只对视觉场中有限区域的视觉刺激做出反应（参见[图12-1](#cat_visual_cortex_diagram)，其中五个神经元的局部感受野由虚线圆表示）。不同神经元的感受野可能重叠，并且共同覆盖整个视觉场。
- en: '![Diagram illustrating how biological neurons in the visual cortex respond
    to specific patterns within small receptive fields and integrate this information
    to recognize complex shapes like a house.](assets/hmls_1201.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图示说明视觉皮层中的生物神经元如何对感受野内的特定模式做出反应，并整合这些信息以识别如房屋等复杂形状。](assets/hmls_1201.png)'
- en: Figure 12-1\. Biological neurons in the visual cortex respond to specific patterns
    in small regions of the visual field called receptive fields; as the visual signal
    makes its way through consecutive brain modules, neurons respond to more complex
    patterns in larger receptive fields
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-1. 视觉皮层中的生物神经元对视觉场中称为感受野的小区域内的特定模式做出反应；随着视觉信号通过连续的脑模块，神经元对更大感受野中的更复杂模式做出反应
- en: Moreover, the authors showed that some neurons react only to images of horizontal
    lines, while others react only to lines with different orientations (two neurons
    may have the same receptive field but react to different line orientations). They
    also noticed that some neurons have larger receptive fields, and they react to
    more complex patterns that are combinations of the lower-level patterns. These
    observations led to the idea that the higher-level neurons are based on the outputs
    of neighboring lower-level neurons (in [Figure 12-1](#cat_visual_cortex_diagram),
    notice that each neuron is connected only to nearby neurons from the previous
    layer). This powerful architecture is able to detect all sorts of complex patterns
    in any area of the visual field.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，作者还表明，一些神经元只对水平线的图像做出反应，而另一些神经元只对具有不同方向的线条做出反应（两个神经元可能具有相同的感受野，但对不同的线条方向做出反应）。他们还注意到，一些神经元具有更大的感受野，并对更复杂的模式做出反应，这些模式是较低级别模式的组合。这些观察导致了一种观点，即高级神经元基于相邻的低级神经元的输出（在[图12-1](#cat_visual_cortex_diagram)中，注意每个神经元只与前一层的邻近神经元相连）。这种强大的架构能够检测视觉场任何区域的复杂模式。
- en: 'These studies of the visual cortex inspired the [neocognitron](https://homl.info/74),⁠^([4](ch12.html#id2738))
    introduced in 1980, which gradually evolved into what we now call convolutional
    neural networks. An important milestone was a [1998 paper](https://homl.info/75)⁠^([5](ch12.html#id2739))
    by Yann LeCun et al. that introduced the famous *LeNet-5* architecture, which
    became widely used by banks to recognize handwritten digits on checks. This architecture
    has some building blocks that you already know, such as fully connected layers
    and sigmoid activation functions, but it also introduces two new building blocks:
    *convolutional layers* and *pooling layers*. Let’s look at them now.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这些对视觉皮层的研究启发了1980年引入的[新认知机](https://homl.info/74)⁠^([4](ch12.html#id2738))，它逐渐演变成了我们现在所说的卷积神经网络。一个重要里程碑是Yann
    LeCun等人于1998年发表的一篇[论文](https://homl.info/75)⁠^([5](ch12.html#id2739))，介绍了著名的**LeNet-5**架构，该架构被银行广泛用于识别支票上的手写数字。这个架构有一些你已经知道的构建块，例如全连接层和sigmoid激活函数，但它还引入了两个新的构建块：**卷积层**和**池化层**。现在让我们来看看它们。
- en: Note
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Why not simply use a deep neural network with fully connected layers for image
    recognition tasks? Unfortunately, although this works fine for small images (e.g.,
    Fashion MNIST), it breaks down for larger images because of the huge number of
    parameters it requires. For example, a 100 × 100–pixel image has 10,000 pixels,
    and if the first layer has just 1,000 neurons (which already severely restricts
    the amount of information transmitted to the next layer), this means a total of
    10 million connections. And that’s just the first layer. CNNs solve this problem
    using partially connected layers and weight sharing.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不直接使用具有全连接层的深度神经网络进行图像识别任务呢？遗憾的是，尽管这对于小图像（例如，Fashion MNIST）效果很好，但由于它需要大量的参数，因此对于大图像来说，这种方法会失效。例如，一个
    100 × 100 像素的图像有 10,000 个像素，如果第一层只有 1,000 个神经元（这已经严重限制了传递给下一层的信息量），这意味着总共需要 1,000
    万个连接。而这只是第一层。CNN通过使用部分连接层和权重共享来解决这一问题。
- en: Convolutional Layers
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: 'The most important building block of a CNN is the *convolutional layer*:⁠^([6](ch12.html#id2744))
    neurons in the first convolutional layer are not connected to every single pixel
    in the input image (like they were in the layers discussed in previous chapters),
    but only to pixels in their receptive fields (see [Figure 12-2](#cnn_layers_diagram)).
    In turn, each neuron in the second convolutional layer is connected only to neurons
    located within a small rectangle in the first layer. This architecture allows
    the network to concentrate on small low-level features in the first hidden layer,
    then assemble them into larger higher-level features in the next hidden layer,
    and so on. This hierarchical structure is well-suited to deal with composite objects,
    which are common in real-world images: this is one of the reasons why CNNs work
    so well for image recognition.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: CNN最重要的构建块是*卷积层*：⁠^([6](ch12.html#id2744)) 第一卷积层中的神经元并不连接到输入图像中的每一个像素（就像之前章节中讨论的层那样），而是只连接到它们感受野中的像素（见[图12-2](#cnn_layers_diagram)）。反过来，第二卷积层中的每个神经元只连接到第一层中位于一个小矩形内的神经元。这种架构使得网络能够专注于第一隐藏层中的小低级特征，然后将它们组装成下一隐藏层中的更大高级特征，依此类推。这种层次结构非常适合处理现实世界图像中常见的复合对象：这也是CNN在图像识别中表现如此出色的原因之一。
- en: '![Diagram illustrating the structure of CNN layers, highlighting the local
    receptive fields connecting the first convolutional layer to the second through
    the input.](assets/hmls_1202.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![CNN层结构图，突出显示通过输入连接第一卷积层和第二层的局部感受野](assets/hmls_1202.png)'
- en: Figure 12-2\. CNN layers with rectangular local receptive fields
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-2\. 具有矩形局部感受野的CNN层
- en: Note
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: All the multilayer neural networks we’ve looked at so far had layers composed
    of a long line of neurons, and we had to flatten input images to 1D before feeding
    them to the neural network. In a CNN each layer is represented in 2D, which makes
    it easier to match neurons with their corresponding inputs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到的所有的多层神经网络都是由一长串神经元组成的层，我们必须将输入图像展平为1D才能将其输入到神经网络中。在CNN中，每一层都是用2D表示的，这使得匹配神经元与它们相应的输入变得更容易。
- en: A neuron located in row *i*, column *j* of a given layer is connected to the
    outputs of the neurons in the previous layer located in rows *i* to *i* + *f*[*h*]
    – 1, columns *j* to *j* + *f*[*w*] – 1, where *f*[*h*] and *f*[*w*] are the height
    and width of the receptive field (see [Figure 12-3](#slide_and_padding_diagram)).
    In order for a layer to have the same height and width as the previous layer,
    it is common to add zeros around the inputs, as shown in the diagram. This is
    called *zero* *padding*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定层的第 *i* 行、第 *j* 列的一个神经元连接到前一层中位于第 *i* 到 *i* + *f*[*h*] – 1 行、第 *j* 到 *j*
    + *f*[*w*] – 1 列的神经元输出，其中 *f*[*h*] 和 *f*[*w*] 是感受野的高度和宽度（见[图12-3](#slide_and_padding_diagram)）。为了使层具有与前一层相同的高度和宽度，通常会在输入周围添加零，如图中所示。这被称为*零填充*。
- en: It is also possible to connect a large input layer to a much smaller layer by
    spacing out the receptive fields, as shown in [Figure 12-4](#stride_diagram).
    This dramatically reduces the model’s computational complexity. The horizontal
    or vertical step size from one receptive field to the next is called the *stride*.
    In the diagram, a 5 × 7 input layer (plus zero padding) is connected to a 3 ×
    4 layer, using 3 × 3 receptive fields and a stride of 2\. In this example the
    stride is the same in both directions, which is generally the case (although there
    are exceptions). A neuron located in row *i*, column *j* in the upper layer is
    connected to the outputs of the neurons in the previous layer located in rows
    *i* × *s*[*h*] to *i* × *s*[*h*] + *f*[*h*] – 1, columns *j* × *s*[*w*] to *j*
    × *s*[*w*] + *f*[*w*] – 1, where *s*[*h*] and *s*[*w*] are the vertical and horizontal
    strides.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 通过间隔感受野，也可以将一个大型输入层连接到一个远小的层，如图[图12-4](#stride_diagram)所示。这大大降低了模型的计算复杂度。从一个感受野到下一个感受野的水平或垂直步长称为**步长**。在图中，一个5
    × 7输入层（包括零填充）通过3 × 3感受野和步长为2连接到一个3 × 4层。在这个例子中，步长在两个方向上都是相同的，这通常是情况（尽管也有例外）。上层中位于第*i*行、第*j*列的神经元连接到前一层中位于*i*
    × *s*[*h*]到*i* × *s*[*h*] + *f*[*h*] – 1行、*j* × *s*[*w*]到*j* × *s*[*w*] + *f*[*w*]
    – 1列的神经元的输出，其中*s*[*h*]和*s*[*w*]是垂直和水平步长。
- en: '![Diagram illustrating the connection between a 5 × 7 input layer and a 3 ×
    4 layer using 3 × 3 receptive fields with a stride of 2, including zero padding.](assets/hmls_1203.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![展示一个5 × 7输入层和一个3 × 4层之间的连接，使用3 × 3感受野和步长为2，包括零填充。](assets/hmls_1203.png)'
- en: Figure 12-3\. Connections between layers and zero padding
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-3\. 层之间的连接和零填充
- en: '![Diagram illustrating the concept of reducing dimensionality with a stride
    of 2 on a grid, showing overlapping operations with different colored boxes and
    lines.](assets/hmls_1204.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![展示在网格上使用步长为2降低维度概念图，显示不同颜色方框和线条的重叠操作。](assets/hmls_1204.png)'
- en: Figure 12-4\. Reducing dimensionality using a stride of 2
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-4\. 使用步长为2降低维度
- en: Filters
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滤波器
- en: A neuron’s weights can be represented as a small image the size of the receptive
    field. For example, [Figure 12-5](#filters_diagram) shows two possible sets of
    weights, called *filters* (or *convolution kernels*, or just *kernels*). The first
    one is represented as a black square with a vertical white line in the middle
    (it’s a 7 × 7 matrix full of 0s except for the central column, which is full of
    1s); neurons using these weights will ignore everything in their receptive field
    except for the central vertical line (since all inputs will be multiplied by 0,
    except for the ones in the central vertical line). The second filter is a black
    square with a horizontal white line in the middle. Neurons using these weights
    will ignore everything in their receptive field except for the central horizontal
    line.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个神经元的权重可以表示为一个与感受野大小相同的小图像。例如，[图12-5](#filters_diagram)显示了两组可能的权重，称为**滤波器**（或**卷积核**，或简称**核**）。第一个滤波器表示为一个中间有垂直白色线的黑色正方形（它是一个7
    × 7的矩阵，除了中间列，其余都是0；使用这些权重的神经元将忽略它们感受野中的所有内容，除了中央垂直线，因为所有输入都将乘以0，除了中央垂直线上的输入）。第二个滤波器是一个中间有水平白色线的黑色正方形。使用这些权重的神经元将忽略它们感受野中的所有内容，除了中央水平线。
- en: '![Diagram showing the input image processed by vertical and horizontal filters
    to produce two feature maps, each highlighting different line orientations.](assets/hmls_1205.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![展示通过垂直和水平滤波器处理输入图像以产生两个特征图，每个特征图突出显示不同的线方向。](assets/hmls_1205.png)'
- en: Figure 12-5\. Applying two different filters to get two feature maps
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-5\. 应用两个不同的滤波器以获得两个特征图
- en: Note
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In deep learning, we often build a single model that takes the raw inputs and
    produces the final outputs. This is called *end-to-end learning*. In contrast,
    classical vision systems would usually divide the system into a sequence of specialized
    modules.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，我们通常构建一个单一模型，它接受原始输入并产生最终输出。这被称为**端到端学习**。相比之下，传统的视觉系统通常会将系统划分为一系列专门的模块。
- en: 'Now if all neurons in a layer use the same vertical line filter (and the same
    bias term), and you feed the network the input image shown in [Figure 12-5](#filters_diagram)
    (the bottom image), the layer will output the top-left image. Notice that the
    vertical white lines get enhanced while the rest gets blurred. Similarly, the
    upper-right image is what you get if all neurons use the same horizontal line
    filter; notice that the horizontal white lines get enhanced while the rest is
    blurred out. Thus, a layer full of neurons using the same filter outputs a *feature
    map*, which highlights the areas in an image that activate the filter the most.
    But don’t worry, you won’t have to define the filters manually: instead, during
    training the convolutional layer will automatically learn the most useful filters
    for its task, and the layers above will learn to combine them into more complex
    patterns.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果某一层的所有神经元都使用相同的垂直线过滤器（以及相同的偏置项），并且你将[图12-5](#filters_diagram)中显示的输入图像（底部图像）输入到网络中，该层将输出左上角的图像。注意，垂直的白色线条被增强，而其余部分则被模糊。同样，右上角的图像是所有神经元都使用相同的水平线过滤器时得到的；注意，水平白色线条被增强，而其余部分则被模糊掉。因此，一个充满使用相同过滤器的神经元的层输出一个*特征图*，它突出了图像中激活过滤器最多的区域。但不用担心，你不需要手动定义过滤器：相反，在训练过程中，卷积层将自动学习其任务中最有用的过滤器，而上面的层将学会将它们组合成更复杂的模式。
- en: Stacking Multiple Feature Maps
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多个特征图的堆叠
- en: Up to now, for simplicity, I have represented each convolutional layer as a
    2D layer, but in reality a convolutional layer has multiple filters (you decide
    how many) and it outputs one feature map per filter, so the output is more accurately
    represented in 3D (see [Figure 12-6](#cnn_layers_volume_diagram)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，为了简单起见，我将每个卷积层表示为一个二维层，但在现实中，卷积层有多个过滤器（你决定多少个），并且每个过滤器输出一个特征图，因此输出更准确地用三维（参见[图12-6](#cnn_layers_volume_diagram)）表示。
- en: '![Diagram illustrating two convolutional layers with multiple filters processing
    a color image with three RGB channels, producing one feature map per filter.](assets/hmls_1206.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![展示两个具有多个过滤器的卷积层处理具有三个RGB通道的颜色图像，每个过滤器产生一个特征图的示意图。](assets/hmls_1206.png)'
- en: Figure 12-6\. Two convolutional layers with multiple filters each (kernels),
    processing a color image with three color channels; each convolutional layer outputs
    one feature map per filter
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-6\. 每个卷积层有多个过滤器（核），处理具有三个颜色通道的颜色图像；每个卷积层为每个过滤器输出一个特征图
- en: There is one neuron per pixel in each feature map, and all neurons within a
    given feature map share the same parameters (i.e., the same kernel and bias term).
    Neurons in different feature maps use different parameters. A neuron’s receptive
    field is the same as described earlier, but it extends across all the feature
    maps of the previous layer. In short, a convolutional layer simultaneously applies
    multiple trainable filters to its inputs, making it capable of detecting multiple
    features anywhere in its inputs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 每个特征图中的每个像素都有一个神经元，并且给定特征图内的所有神经元共享相同的参数（即相同的核和偏置项）。不同特征图中的神经元使用不同的参数。神经元的感受野与之前描述的相同，但它扩展到前一层的所有特征图中。简而言之，卷积层同时对其输入应用多个可训练的过滤器，使其能够在输入的任何位置检测到多个特征。
- en: Note
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The fact that all neurons in a feature map share the same parameters dramatically
    reduces the number of parameters in the model. Once the CNN has learned to recognize
    a pattern in one location, it can recognize it in any other location. In contrast,
    once a fully connected neural network has learned to recognize a pattern in one
    location, it can only recognize it in that particular location.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征图中的所有神经元共享相同的参数这一事实，显著减少了模型中的参数数量。一旦卷积神经网络学会在某个位置识别一个模式，它就可以在任何其他位置识别该模式。相比之下，一旦一个全连接神经网络学会在某个位置识别一个模式，它只能在那个特定位置识别该模式。
- en: 'Input images are also composed of multiple sublayers: one per *color channel*.
    As mentioned in [Chapter 8](ch08.html#unsupervised_learning_chapter), there are
    typically three: red, green, and blue (RGB). Grayscale images have just one channel,
    but some images may have many more—for example, satellite images that capture
    extra light frequencies (such as infrared).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像也由多个子层组成：每个颜色通道一个。如[第8章](ch08.html#unsupervised_learning_chapter)所述，通常有三个：红色、绿色和蓝色（RGB）。灰度图像只有一个通道，但有些图像可能有更多——例如，捕捉额外光频的卫星图像（如红外线）。
- en: Specifically, a neuron located in row *i*, column *j* of the feature map *k*
    in a given convolutional layer *l* is connected to the outputs of the neurons
    in the previous layer *l* – 1, located in rows *i* × *s*[*h*] to *i* × *s*[*h*]
    + *f*[*h*] – 1 and columns *j* × *s*[*w*] to *j* × *s*[*w*] + *f*[*w*] – 1, across
    all feature maps (in layer *l* – *1*). Note that, within a layer, all neurons
    located in the same row *i* and column *j* but in different feature maps are connected
    to the outputs of the exact same neurons in the previous layer.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，一个位于给定卷积层 *l* 中特征图 *k* 的第 *i* 行、第 *j* 列的神经元连接到前一层 *l* – 1 中位于行 *i* × *s*[*h*]
    到 *i* × *s*[*h*] + *f*[*h*] – 1 和列 *j* × *s*[*w*] 到 *j* × *s*[*w*] + *f*[*w*]
    – 1 的神经元输出，跨越所有特征图（在层 *l* – *1*）。注意，在一个层中，位于同一行 *i* 和列 *j* 但在不同特征图中的所有神经元都连接到前一层中完全相同的神经元输出。
- en: '[Equation 12-1](#convolutional_layer_equation) summarizes the preceding explanations
    in one big mathematical equation: it shows how to compute the output of a given
    neuron in a convolutional layer. It is a bit ugly due to all the different indices,
    but all it does is calculate the weighted sum of all the inputs, plus the bias
    term.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程12-1](#convolutional_layer_equation) 总结了前面的解释，通过一个大的数学方程式：它展示了如何计算卷积层中给定神经元的输出。由于所有不同的索引，它看起来有点丑陋，但它所做的只是计算所有输入的加权和，加上偏差项。'
- en: Equation 12-1\. Computing the output of a neuron in a convolutional layer
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程12-1\. 计算卷积层中神经元的输出
- en: <mrow><msub><mi>z</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi><mo
    lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub> <mo>=</mo> <msub><mi>b</mi>
    <mi>k</mi></msub> <mo>+</mo> <munderover><mo>∑</mo> <mrow><mi>u</mi><mo>=</mo><mn>0</mn></mrow>
    <mrow><msub><mi>f</mi> <mi>h</mi></msub> <mo>-</mo><mn>1</mn></mrow></munderover>
    <munderover><mo>∑</mo> <mrow><mi>v</mi><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi>
    <mi>w</mi></msub> <mo>-</mo><mn>1</mn></mrow></munderover> <munderover><mo>∑</mo>
    <mrow><mi>k</mi><mo>'</mo><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi> <msup><mi>n</mi>
    <mo>'</mo></msup></msub> <mo>-</mo><mn>1</mn></mrow></munderover> <msub><mi>x</mi>
    <mrow><msup><mi>i</mi> <mo>'</mo></msup> <mo lspace="0%" rspace="0%">,</mo><msup><mi>j</mi>
    <mo>'</mo></msup> <mo lspace="0%" rspace="0%">,</mo><msup><mi>k</mi> <mo>'</mo></msup></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mrow><mi>u</mi><mo lspace="0%" rspace="0%">,</mo><mi>v</mi><mo
    lspace="0%" rspace="0%">,</mo><msup><mi>k</mi> <mo>'</mo></msup> <mo lspace="0%"
    rspace="0%">,</mo><mi>k</mi></mrow></msub> <mtext>with</mtext> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mi>i</mi> <mo>'</mo>
    <mo>=</mo> <mi>i</mi> <mo>×</mo> <msub><mi>s</mi> <mi>h</mi></msub> <mo>+</mo>
    <mi>u</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>j</mi> <mo>'</mo>
    <mo>=</mo> <mi>j</mi> <mo>×</mo> <msub><mi>s</mi> <mi>w</mi></msub> <mo>+</mo>
    <mi>v</mi></mrow></mtd></mtr></mtable></mfenced></mrow>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><msub><mi>z</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi><mo
    lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub> <mo>=</mo> <msub><mi>b</mi>
    <mi>k</mi></msub> <mo>+</mo> <munderover><mo>∑</mo> <mrow><mi>u</mi><mo>=</mo><mn>0</mn></mrow>
    <mrow><msub><mi>f</mi> <mi>h</mi></msub> <mo>-</mo><mn>1</mn></mrow></munderover>
    <munderover><mo>∑</mo> <mrow><mi>v</mi><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi>
    <mi>w</mi></msub> <mo>-</mo><mn>1</mn></mrow></munderover> <munderover><mo>∑</mo>
    <mrow><mi>k</mi><mo>'</mo><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi> <msup><mi>n</mi>
    <mo>'</mo></msup></msub> <mo>-</mo><mn>1</mn></mrow></munderover> <msub><mi>x</mi>
    <mrow><msup><mi>i</mi> <mo>'</mo></msup> <mo lspace="0%" rspace="0%">,</mo><msup><mi>j</mi>
    <mo>'</mo></msup> <mo lspace="0%" rspace="0%">,</mo><msup><mi>k</mi> <mo>'</mo></msup></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mrow><mi>u</mi><mo lspace="0%" rspace="0%">,</mo><mi>v</mi><mo
    lspace="0%" rspace="0%">,</mo><msup><mi>k</mi> <mo>'</mo></msup> <mo lspace="0%"
    rspace="0%">,</mo><mi>k</mi></mrow></msub> <mtext>with</mtext> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mi>i</mi> <mo>'</mo>
    <mo>=</mo> <mi>i</mi> <mo>×</mo> <msub><mi>s</mi> <mi>h</mi></msub> <mo>+</mo>
    <mi>u</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>j</mi> <mo>'</mo>
    <mo>=</mo> <mi>j</mi> <mo>×</mo> <msub><mi>s</mi> <mi>w</mi></msub> <mo>+</mo>
    <mi>v</mi></mrow></mtd></mtr></mtable></mfenced></mrow>
- en: 'In this equation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*z*[*i*,] [*j*,] [*k*] is the output of the neuron located in row *i*, column
    *j* in feature map *k* of the convolutional layer (layer *l*).'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*z*[*i*,] [*j*,] [*k*] 是卷积层（层 *l*）中位于特征图 *k* 的第 *i* 行、第 *j* 列的神经元的输出。'
- en: As explained earlier, *s*[*h*] and *s*[*w*] are the vertical and horizontal
    strides, *f*[*h*] and *f*[*w*] are the height and width of the receptive field,
    and *f*[*n*′] is the number of feature maps in the previous layer (layer *l* –
    1).
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述，*s*[*h*] 和 *s*[*w*] 是垂直和水平步长，*f*[*h*] 和 *f*[*w*] 是感受野的高度和宽度，*f*[*n*′] 是前一层（层
    *l* – 1）中的特征图数量。
- en: '*x*[*i*′,] [*j*′,] [*k*′] is the output of the neuron located in layer *l*
    – 1, row *i*′, column *j*′, feature map *k*′ (or channel *k*′ if the previous
    layer is the input layer).'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[*i*′,] [*j*′,] [*k*′] 是位于层 *l* – 1、行 *i*′、列 *j*′、特征图 *k*′（或通道 *k*′ 如果前一层是输入层）的神经元的输出。'
- en: '*b*[*k*] is the bias term for feature map *k* (in layer *l*). You can think
    of it as a knob that tweaks the overall brightness of the feature map *k*.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b*[*k*] 是特征图 *k*（在层 *l*）的偏置项。你可以把它想象成一个可以调整特征图 *k* 整体亮度的旋钮。'
- en: '*w*[*u*,] [*v*,] [*k*′,] [*k*] is the connection weight between any neuron
    in feature map *k* of the layer *l* and its input located at row *u*, column *v*
    (relative to the neuron’s receptive field), and feature map *k*′.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[*u*,] [*v*,] [*k*′,] [*k*] 是层 *l* 中特征图 *k* 的任何神经元与其输入之间的连接权重，该输入位于行 *u*、列
    *v*（相对于神经元的感受野），以及特征图 *k*′。'
- en: Let’s see how to create and use a convolutional layer using PyTorch.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用PyTorch创建和使用卷积层。
- en: Implementing Convolutional Layers with PyTorch
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用PyTorch实现卷积层
- en: 'First, let’s load a couple of sample images using Scikit-Learn’s `load_sample_images()`
    function. The first image represents the tower of buddhist incense in China, while
    the second one represents a beautiful *Dahlia pinnata* flower. These images are
    represented as a Python list of NumPy unsigned byte arrays, so let’s stack these
    images into a single NumPy array, then convert it to a 32-bit float tensor, and
    rescale the pixel values from 0–255 to 0–1:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用Scikit-Learn的`load_sample_images()`函数加载一些样本图像。第一张图像代表中国佛教香塔，而第二张图像代表美丽的*大丽花*。这些图像以Python列表的形式表示为NumPy无符号字节数组，因此让我们将这些图像堆叠成一个单一的NumPy数组，然后将其转换为32位浮点张量，并将像素值从0–255缩放到0–1：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s look at this tensor’s shape:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个张量的形状：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We have two images, both are 427 pixels high and 640 pixels wide, and they
    have three color channels: red, green, and blue. As we saw in [Chapter 10](ch10.html#pytorch_chapter),
    PyTorch expects the channel dimension to be just *before* the height and width
    dimensions, not after, so we need to permute the dimensions using the `permute()`
    method:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两张图像，它们的高度都是427像素，宽度都是640像素，并且它们有三个颜色通道：红色、绿色和蓝色。正如我们在[第10章](ch10.html#pytorch_chapter)中看到的，PyTorch期望通道维度仅在高度和宽度维度之前，而不是之后，因此我们需要使用`permute()`方法对维度进行置换：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s also use TorchVision’s `CenterCrop` class to center-crop the images:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也使用TorchVision的`CenterCrop`类来对图像进行中心裁剪：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now let’s create a 2D convolutional layer and feed it these cropped images
    to see what comes out. For this, PyTorch provides the `nn.Conv2d` layer. Under
    the hood, this layer relies on the `torch.nn.((("torch", "F.conv2d()")))functional.conv2d()`
    function. Let’s create a convolutional layer with 32 filters, each of size 7 ×
    7 (using `kernel_size=7`, which is equivalent to using `kernel_size=(7 , 7)`),
    and apply this layer to our small batch of two images:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个二维卷积层，并将这些裁剪后的图像输入其中，看看会得到什么。为此，PyTorch提供了`nn.Conv2d`层。在底层，这个层依赖于`torch.nn.((("torch",
    "F.conv2d()")))functional.conv2d()`函数。让我们创建一个具有32个过滤器、每个大小为7 × 7的卷积层（使用`kernel_size=7`，相当于使用`kernel_size=(7
    , 7)`），并将此层应用于我们的小批量两张图像：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'When we talk about a 2D convolutional layer, “2D” refers to the number of *spatial*
    dimensions (height and width), but as you can see, the layer takes 4D inputs:
    as we saw, the two additional dimensions are the batch size (first dimension)
    and the channels (second dimension).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论一个二维卷积层时，“2D”指的是空间维度（高度和宽度），但正如你所见，该层接受4D输入：正如我们所见，这两个额外的维度是批大小（第一个维度）和通道（第二个维度）。
- en: 'Now let’s look at the output’s shape:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看输出的形状：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output shape is similar to the input shape, with two main differences.
    First, there are 32 channels instead of 3\. This is because we set `out_channels=32`,
    so we get 32 output feature maps: instead of the intensity of red, green, and
    blue at each location, we now have the intensity of each feature at each location.
    Second, the height and width have both shrunk by 6 pixels. This is due to the
    fact that the `nn.Conv2d` layer does not use any zero-padding by default, which
    means that we lose a few pixels on the sides of the output feature maps, depending
    on the size of the filters. In this case, since the kernel size is 7, we lose
    6 pixels horizontally and 6 pixels vertically (i.e., 3 pixels on each side).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输出形状与输入形状相似，但有两大主要区别。首先，通道数从3变为32。这是因为我们设置了`out_channels=32`，因此我们得到了32个输出特征图：不再是每个位置的红色、绿色和蓝色的强度，而是现在在每个位置都有每个特征的强度。其次，高度和宽度都缩小了6个像素。这是因为`nn.Conv2d`层默认不使用任何零填充，这意味着我们在输出特征图的边缘会丢失一些像素，这取决于滤波器的大小。在这种情况下，由于滤波器大小为7，我们在水平和垂直方向上各丢失了6个像素（即每边3个像素）。
- en: Warning
  id: totrans-66
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'By default, the `padding` hyperparameter is set to 0, which means that padding
    is turned off. Oddly, this is also called *valid padding* since every neuron’s
    receptive field lies strictly within *valid* positions inside the input (it does
    not go out of bounds). You can actually set `padding="valid"`, which is equivalent
    to `padding=0`. It’s not a PyTorch naming quirk: everyone uses this confusing
    nomenclature.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`padding`超参数设置为0，这意味着填充被关闭。奇怪的是，这也被称为*valid填充*，因为每个神经元的感受野严格位于输入内的*valid*位置（它不会越界）。实际上，你可以设置`padding="valid"`，这相当于`padding=0`。这不是PyTorch的命名怪癖：每个人都使用这种令人困惑的命名法。
- en: 'If instead we set `padding="same"`, then the inputs are padded with enough
    zeros on all sides to ensure that the output feature maps end up with the *same*
    size as the inputs (hence the name of this option):'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们改为设置`padding="same"`，那么输入将在所有边填充足够的零，以确保输出特征图最终的大小与输入相同（因此该选项的名称为“same”）：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: These two padding options are illustrated in [Figure 12-7](#padding_options_stride_1_diagram).
    For simplicity, only the horizontal dimension is shown here, but of course the
    same logic applies to the vertical dimension as well.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种填充选项在[图12-7](#padding_options_stride_1_diagram)中进行了说明。为了简化，这里只显示了水平维度，但当然，同样的逻辑也适用于垂直维度。
- en: '![Diagram comparing two padding options for a convolution with `stride=1` and
    `kernel_size=7`, illustrating "valid" and "same" padding concepts.](assets/hmls_1207.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![比较`stride=1`和`kernel_size=7`的卷积两种填充选项的图，说明了“valid”和“same”填充的概念。](assets/hmls_1207.png)'
- en: Figure 12-7\. Two different padding options, with `stride=1` and `kernel_size=7`
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-7\. 两种不同的填充选项，`stride=1`和`kernel_size=7`
- en: 'If the stride is greater than 1 (in any direction), then the output size will
    be much smaller than the input size. For example, assuming the input size is 70
    × 120, then if you set `stride=2` (or equivalently `stride=(2, 2)`), `padding=3`,
    and `kernel_size=7`, then the output feature maps will be 35 × 60: halved both
    vertically and horizontally. You could set a very large padding value to make
    the output size identical to the input size, but that’s almost certainly a bad
    idea since it would drown your image in a sea of zeros (for this reason, PyTorch
    raises an exception if you set `padding="same"` along with a `stride` greater
    than 1). [Figure 12-8](#padding_options_stride_2_diagram) illustrates `stride=2`,
    with `kernel_size=7` and `padding` set to 0 or 3.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果步长大于1（在任何方向上），则输出大小将远小于输入大小。例如，假设输入大小为70 × 120，那么如果你设置`stride=2`（或等价地`stride=(2,
    2)`），`padding=3`，和`kernel_size=7`，那么输出特征图将是35 × 60：垂直和水平方向都减半。你可以设置一个非常大的填充值，使输出大小与输入大小相同，但这几乎肯定是一个糟糕的主意，因为它会让你的图像淹没在零的海洋中（因此，PyTorch会在你设置`padding="same"`和步长大于1时抛出异常）。[图12-8](#padding_options_stride_2_diagram)说明了`stride=2`，`kernel_size=7`和`padding`设置为0或3。
- en: '![Diagram illustrating two padding options with `stride=2` and `kernel_size=7`:
    one with "valid" padding and ignored values, and one with "same" padding using
    zero padding.](assets/hmls_1208.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![说明使用`stride=2`和`kernel_size=7`的两种填充选项的图：一个使用“valid”填充和忽略的值，另一个使用零填充的“same”填充。](assets/hmls_1208.png)'
- en: 'Figure 12-8\. Two different padding options, with `stride=2` and `kernel_size=7`:
    the output size is much smaller'
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-8\. 两种不同的填充选项，`stride=2`和`kernel_size=7`：输出大小大大减小
- en: 'Now let’s look at the layer’s parameters (which were denoted as *w*[*u*,] [*v*,]
    [*k*′,] [*k*] and *b*[*k*] in [Equation 12-1](#convolutional_layer_equation)).
    Just like an `nn.Linear` layer, an `nn.Conv2d` layer holds all the layer’s parameters,
    including the kernels and biases, which are accessible via the `weight` and `bias`
    attributes:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看该层的参数（在[方程12-1](#convolutional_layer_equation)中表示为*w*[*u*,] [*v*,] [*k*′,]
    [*k*]和*b*[*k*]）。就像`nn.Linear`层一样，`nn.Conv2d`层包含了所有层的参数，包括核和偏置，这些参数可以通过`weight`和`bias`属性访问：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `weight` tensor is 4D, and its shape is [*output_channels*, *input_channels*,
    *kernel_height*, *kernel_width*]. The `bias` tensor is 1D, with shape [*output_channels*].
    The number of output channels is equal to the number of output feature maps, which
    is also equal to the number of filters. Most importantly, note that the height
    and width of the input images do not appear in the kernel’s shape: this is because
    all the neurons in the output feature maps share the same weights, as explained
    earlier. This means that you can feed images of any size to this layer, as long
    as they are at least as large as the kernels, and if they have the right number
    of channels (three in this case).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`weight`张量是4维的，其形状为[*output_channels*, *input_channels*, *kernel_height*, *kernel_width*]。`bias`张量是1维的，形状为[*output_channels*]。输出通道数等于输出特征图的数量，这也等于滤波器的数量。最重要的是，请注意，输入图像的高度和宽度不会出现在核的形状中：这是因为，正如前面解释的那样，输出特征图中的所有神经元共享相同的权重。这意味着只要图像的大小至少与核一样大，并且具有正确的通道数（在这个例子中是三个），您就可以将任何大小的图像输入到这个层。'
- en: 'It’s important to add an activation function after each convolutional layer.
    This is for the same reason as for `nn.Linear` layers: a convolutional layer performs
    a linear operation, so if you stacked multiple convolutional layers without any
    activation functions, they would all be equivalent to a single convolutional layer,
    and they wouldn’t be able to learn anything really complex.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个卷积层之后添加激活函数是很重要的。这与`nn.Linear`层的原因相同：卷积层执行线性操作，所以如果您堆叠多个没有激活函数的卷积层，它们都将等效于单个卷积层，并且它们将无法学习任何真正复杂的东西。
- en: Both the `weight` and `bias` parameters are initialized randomly, using a uniform
    distribution similar to the one used by the `nn.Linear` layer, between $minus
    StartFraction 1 Over StartRoot k EndRoot EndFraction$ and $plus StartFraction
    1 Over StartRoot k EndRoot EndFraction$ , where *k* is the fan[in]. In `nn.Conv2d`,
    *k* = *f*[h] × *f*[w] × *f*[n’], where *f*[h] and *f*[w] are the height and width
    of the kernel, and *f*[n’] is the number of input channels. As we saw in [Chapter 11](ch11.html#deep_chapter),
    you will generally want to reinitialize the weights depending on the activation
    function you use. For example, you should apply He initialization whenever you
    use the ReLU activation function. As for the biases, they can just be reinitialized
    to zero.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`weight`和`bias`参数都是随机初始化的，使用与`nn.Linear`层类似的均匀分布，介于$minus StartFraction 1 Over
    StartRoot k EndRoot EndFraction$和$plus StartFraction 1 Over StartRoot k EndRoot
    EndFraction$之间，其中*k*是fan[in]。在`nn.Conv2d`中，*k* = *f*[h] × *f*[w] × *f*[n’]，其中*f*[h]和*f*[w]是核的高度和宽度，*f*[n’]是输入通道数。正如我们在[第11章](ch11.html#deep_chapter)中看到的，您通常会根据所使用的激活函数重新初始化权重。例如，当您使用ReLU激活函数时，应该应用He初始化。至于偏置，它们可以简单地重新初始化为零。'
- en: 'As you can see, convolutional layers have quite a few hyperparameters: the
    number of filters (`out_channels`), the kernel size, the type of padding, the
    strides, and the activation function. As always, you can use cross-validation
    to find the right hyperparameter values, but this is very time-consuming. We will
    discuss common CNN architectures later in this chapter to give you some idea of
    which hyperparameter values work best in practice.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，卷积层有很多超参数：滤波器数量（`out_channels`）、核大小、填充类型、步长和激活函数。像往常一样，您可以使用交叉验证来找到正确的超参数值，但这非常耗时。我们将在本章后面讨论常见的CNN架构，以给您一些关于哪些超参数值在实际中效果最好的想法。
- en: 'Now, let’s look at the second common building block of CNNs: the *pooling layer*.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看CNN的第二个常见构建块：*池化层*。
- en: Pooling Layers
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 池化层
- en: Once you understand how convolutional layers work, the pooling layers are quite
    easy to grasp. Their goal is to *subsample* (i.e., shrink) the input image in
    order to reduce the computational load, the memory usage, and the number of parameters
    (thereby limiting the risk of overfitting).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了卷积层的工作原理，池化层就很容易理解了。它们的目标是*下采样*（即缩小）输入图像，以减少计算负载、内存使用量和参数数量（从而限制过拟合的风险）。
- en: Just like in convolutional layers, each neuron in a pooling layer is connected
    to the outputs of a limited number of neurons in the previous layer, located within
    a small rectangular receptive field. You must define its size, the stride, and
    the padding type, just like before. However, a pooling neuron has no weights or
    biases; all it does is aggregate the inputs using an aggregation function such
    as the max or mean. [Figure 12-9](#max_pooling_diagram) shows a *max pooling layer*,
    which is the most common type of pooling layer. In this example, we use a 2 ×
    2 *pooling kernel*,⁠^([7](ch12.html#id2783)) with a stride of 2 and no padding.
    Only the max input value in each receptive field makes it to the next layer, while
    the other inputs are dropped. For example, in the lower-left receptive field in
    [Figure 12-9](#max_pooling_diagram), the input values are 1, 5, 3, and 2, so only
    the max value, 5, is propagated to the next layer. Because of the stride of 2,
    the output image has half the height and half the width of the input image (rounded
    down since we use no padding).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在卷积层中一样，池化层中的每个神经元都连接到前一层中有限数量的神经元，这些神经元位于一个小矩形感受野内。你必须定义其大小、步长和填充类型，就像之前一样。然而，池化神经元没有权重或偏差；它所做的只是使用聚合函数（如最大值或平均值）聚合输入。[图12-9](#max_pooling_diagram)显示了*最大池化层*，这是最常见的池化层类型。在这个例子中，我们使用了一个2×2的*池化核*，步长为2，没有填充。只有每个感受野中的最大输入值才能进入下一层，而其他输入则被丢弃。例如，在[图12-9](#max_pooling_diagram)中左下角的感受野中，输入值是1、5、3和2，所以只有最大值5被传播到下一层。由于步长为2，输出图像的高度和宽度是输入图像的一半（因为我们没有使用填充，所以向下取整）。
- en: '![Diagram illustrating a max pooling layer using a 2x2 kernel with a stride
    of 2, showing how the maximum value from each receptive field is selected, reducing
    the output size.](assets/hmls_1209.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![使用2x2核和步长为2的池化层进行最大池化的示意图，展示了如何从每个感受野中选择最大值，从而减小输出大小。](assets/hmls_1209.png)'
- en: Figure 12-9\. Max pooling layer (2 × 2 pooling kernel, stride 2, no padding)
  id: totrans-87
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-9。最大池化层（2×2池化核，步长2，无填充）
- en: Note
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: A pooling layer typically works on every input channel independently, so the
    output depth (i.e., the number of channels) is the same as the input depth.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层通常独立地对每个输入通道进行操作，因此输出深度（即通道数）与输入深度相同。
- en: 'Other than reducing computations, memory usage, and the number of parameters,
    a max pooling layer also introduces some level of *invariance* to small translations,
    as shown in [Figure 12-10](#pooling_invariance_diagram). Here we assume that the
    bright pixels have a lower value than dark pixels, and we consider three images
    (A, B, C) going through a max pooling layer with a 2 × 2 kernel and stride 2\.
    Images B and C are the same as image A, but shifted by one and two pixels to the
    right. As you can see, the outputs of the max pooling layer for images A and B
    are identical. This is what translation invariance means. For image C, the output
    is different: it is shifted one pixel to the right (but there is still 50% invariance).
    By inserting a max pooling layer every few layers in a CNN, it is possible to
    get some level of translation invariance at a larger scale. Moreover, max pooling
    offers a small amount of rotational invariance and a slight scale invariance.
    Such invariance (even if it is limited) can be useful in cases where the prediction
    should not depend on these details, such as in classification tasks.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 除了减少计算量、内存使用量和参数数量外，最大池化层还引入了一定程度的小尺度平移不变性，如图[图12-10](#pooling_invariance_diagram)所示。在这里，我们假设亮像素的值低于暗像素，并考虑三个图像（A、B、C）通过一个2×2核和步长为2的最大池化层。图像B和C与图像A相同，但分别向右移动了一个和两个像素。正如你所见，最大池化层对图像A和B的输出是相同的。这就是平移不变性的意思。对于图像C，输出是不同的：它向右移动了一个像素（但仍有50%的不变性）。通过在CNN的每几层中插入一个最大池化层，可以在更大的尺度上获得一定程度的平移不变性。此外，最大池化还提供了一定程度的旋转不变性和轻微的尺度不变性。这种不变性（即使它有限）在预测不应依赖于这些细节的情况下可能是有用的，例如在分类任务中。
- en: '![Diagram illustrating how max pooling leads to invariance to small translations,
    showing reduced output size despite shifts in input grid positions.](assets/hmls_1210.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![说明最大池化如何导致对微小平移的不变性，尽管输入网格位置发生变化，但输出大小减少的示意图](assets/hmls_1210.png)'
- en: Figure 12-10\. Invariance to small translations
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-10\. 对微小平移的不变性
- en: 'However, max pooling has some downsides too. It’s obviously very destructive:
    even with a tiny 2 × 2 kernel and a stride of 2, the output will be two times
    smaller in both directions (so its area will be four times smaller), thereby dropping
    75% of the input values. And in some applications, invariance is not desirable.
    Take semantic segmentation (the task of classifying each pixel in an image according
    to the object that pixel belongs to, which we’ll explore later in this chapter):
    obviously, if the input image is translated by one pixel to the right, the output
    should also be translated by one pixel to the right. The goal in this case is
    *equivariance*, not invariance: a small change to the inputs should lead to a
    corresponding small change in the output.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，最大池化也有一些缺点。它显然是非常破坏性的：即使使用一个微小的2 × 2核和步长为2，输出在两个方向上都会缩小两倍（因此面积会缩小四倍），从而丢弃了75%的输入值。在某些应用中，不变性并不理想。以语义分割（根据像素所属的对象对图像中的每个像素进行分类的任务，我们将在本章后面探讨）为例：显然，如果输入图像向右平移一个像素，输出也应该向右平移一个像素。在这种情况下，目标是**等变性**，而不是不变性：输入的微小变化应该导致输出中相应的微小变化。
- en: Implementing Pooling Layers with PyTorch
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch实现池化层
- en: 'The following code creates an `nn.MaxPool2d` layer, using a 2 × 2 kernel. The
    strides default to the kernel size, so this layer uses a stride of 2 (horizontally
    and vertically). By default, it uses `padding=0` (i.e., “valid” padding):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码创建了一个`nn.MaxPool2d`层，使用一个2 × 2核。步长默认为核大小，因此这个层使用步长为2（水平和垂直）。默认情况下，它使用`padding=0`（即“有效”填充）：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To create an *average pooling layer*, just use `nn.AvgPool2d`, instead of `nn.MaxPool2d`.
    As you might expect, it works exactly like a max pooling layer, except it computes
    the mean rather than the max. Average pooling layers used to be very popular,
    but people mostly use max pooling layers now, as they generally perform better.
    This may seem surprising, since computing the mean generally loses less information
    than computing the max. But on the other hand, max pooling preserves only the
    strongest features, getting rid of all the meaningless ones, so the next layers
    get a cleaner signal to work with. Moreover, max pooling offers stronger translation
    invariance than average pooling, and it requires slightly less compute.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个**平均池化层**，只需使用`nn.AvgPool2d`，而不是`nn.MaxPool2d`。正如你可能预期的那样，它的工作方式与最大池化层完全一样，只不过它计算的是平均值而不是最大值。平均池化层曾经非常流行，但现在人们大多使用最大池化层，因为它们通常表现更好。这可能会让人感到惊讶，因为计算平均值通常比计算最大值损失的信息更少。但另一方面，最大池化只保留最强的特征，去除所有无意义的特征，因此下一层可以得到更干净的信号来处理。此外，最大池化比平均池化提供了更强的平移不变性，并且计算量略小。
- en: 'Note that max pooling and average pooling can also be performed along the depth
    dimension instead of the spatial dimensions, although it’s not as common. This
    can allow the CNN to learn to be invariant to various features. For example, it
    could learn multiple filters, each detecting a different rotation of the same
    pattern (such as handwritten digits; see [Figure 12-11](#depth_wise_pooling_diagram)),
    and the depthwise max pooling layer would ensure that the output is the same regardless
    of the rotation. The CNN could similarly learn to be invariant to anything: thickness,
    brightness, skew, color, and so on.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，最大池化和平均池化也可以在深度维度而不是空间维度上执行，尽管这并不常见。这可以使CNN学习对各种特征的不变性。例如，它可能学习多个滤波器，每个滤波器检测相同图案的不同旋转（例如手写数字；参见[图12-11](#depth_wise_pooling_diagram)），而深度最大池化层将确保输出不受旋转影响。CNN可以类似地学习对任何事物的不变性：厚度、亮度、倾斜、颜色等等。
- en: '![Diagram illustrating depthwise max pooling in a CNN, showcasing how it helps
    achieve rotational invariance by selecting maximum features across learned filters.](assets/hmls_1211.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![说明CNN中的深度最大池化，展示它如何通过在学习的滤波器中选择最大特征来实现旋转不变性](assets/hmls_1211.png)'
- en: Figure 12-11\. Depthwise max pooling can help the CNN learn to be invariant
    (to rotation in this case)
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-11\. 深度最大池化有助于CNN学习不变性（在这种情况下是对旋转的不变性）
- en: 'PyTorch does not include a depthwise max pooling layer, but we can implement
    a custom module based on the `torch.F.max_pool1d()` function:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 不包括深度最大池化层，但我们可以基于 `torch.F.max_pool1d()` 函数实现一个自定义模块：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'For example, suppose the input batch contains two 70 × 120 images, each with
    32 channels (i.e., the inputs have a shape of `[2, 32, 70, 120]`), and we use
    `kernel_size=4`, and the default `stride` (equal to `kernel_size`) and `padding=0`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设输入批次包含两个 70 × 120 的图像，每个图像有 32 个通道（即输入的形状为 `[2, 32, 70, 120]`），我们使用 `kernel_size=4`，默认的
    `stride`（等于 `kernel_size`）和 `padding=0`：
- en: The `forward()` method starts by merging the spatial dimensions, which gives
    us a tensor of shape `[2, 32, 8400]` (since 70 × 120 = 8,400).
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward()` 方法首先合并空间维度，这给我们一个形状为 `[2, 32, 8400]` 的张量（因为 70 × 120 = 8,400）。'
- en: It then permutes the last two dimensions, so we get a shape of `[2, 8400, 32]`.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，它再次置换最后两个维度，所以我们得到形状为 `[2, 8400, 32]`。
- en: Next, it uses the `max_pool1d()` function to compute the max pool along the
    last dimension, which corresponds to our original 32 channels. Since `kernel_size`
    and `stride` are both equal to 4, and we don’t use any padding, the size of the
    last dimension gets divided by 4, so the resulting shape is `[2, 8400, 8]`.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，它使用 `max_pool1d()` 函数来计算最后一个维度的最大池化，这对应于我们原始的 32 个通道。由于 `kernel_size` 和
    `stride` 都等于 4，我们没有使用任何填充，最后一个维度的尺寸被除以 4，因此得到的形状是 `[2, 8400, 8]`。
- en: The function then permutes the last two dimensions again, giving us a shape
    of `[2, 8, 8400]`.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数随后再次对最后两个维度进行置换，得到形状为 `[2, 8, 8400]`。
- en: Lastly, it separates the spatial dimensions to get the final shape of `[2, 8,
    50, 100]`. You can verify that the output is exactly what we were after.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，它将空间维度分开，得到最终的形状 `[2, 8, 50, 100]`。你可以验证输出正是我们想要的。
- en: 'One last type of pooling layer that you will often see in modern architectures
    is the *global average pooling layer*. It works very differently: all it does
    is compute the mean of each entire feature map. Therefore it outputs a single
    number per feature map and per instance. Although this is of course extremely
    destructive (most of the information in the feature map is lost), it can be useful
    just before the output layer, as you will see later in this chapter.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代架构中经常看到的一种最后类型的池化层是 *全局平均池化层*。它的工作方式非常不同：它所做的只是计算每个整个特征图的平均值。因此，它为每个特征图和每个实例输出一个单一的数字。虽然这当然是非常破坏性的（特征图中大部分信息都丢失了），但它可以在输出层之前非常有用，正如你将在本章后面看到的那样。
- en: 'To create such a layer, one option is to use a regular `nn.AvgPool2d` layer
    and set its kernel size to the same size as the inputs. However, this is not very
    convenient since it requires knowing the exact dimensions of the inputs ahead
    of time. A simpler solution is to use the `nn.AdaptiveAvgPool2d` layer, which
    lets you specify the desired spatial dimensions of the output: it automatically
    adapts the kernel size (with an equal stride) to get the desired result, adding
    a bit of padding if needed. If we set the output size to 1, we get a global average
    pooling layer:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建这样的层，一个选项是使用常规的 `nn.AvgPool2d` 层并将其核大小设置为与输入相同的大小。然而，这并不太方便，因为它需要提前知道输入的确切维度。一个更简单的解决方案是使用
    `nn.AdaptiveAvgPool2d` 层，它允许你指定所需的输出空间维度：它会自动调整核大小（具有相等的步长）以获得所需的结果，如果需要会添加一些填充。如果我们设置输出大小为
    1，我们得到一个全局平均池化层：
- en: '[PRE10]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Alternatively, you could just use the `torch.mean()` function to get the same
    output:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你也可以直接使用 `torch.mean()` 函数来获取相同的输出：
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now you know all the building blocks to create convolutional neural networks.
    Let’s see how to assemble them.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经知道了创建卷积神经网络的所有构建块。让我们看看如何将它们组装起来。
- en: CNN Architectures
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN 架构
- en: Typical CNN architectures stack a few convolutional layers (each one generally
    followed by a ReLU layer), then a pooling layer, then another few convolutional
    layers (+ReLU), then another pooling layer, and so on. The image gets smaller
    and smaller as it progresses through the network, but it also typically gets deeper
    and deeper (i.e., with more feature maps), thanks to the convolutional layers
    (see [Figure 12-12](#cnn_architecture_diagram)). At the top of the stack, a regular
    feedforward neural network is added, composed of a few fully connected layers
    (+ReLUs), and the final layer outputs the prediction (e.g., a softmax layer that
    outputs estimated class probabilities).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的CNN架构堆叠了几层卷积层（每一层通常后面跟着一个ReLU层），然后是一个池化层，然后是几层更多的卷积层（+ReLU），然后是另一个池化层，依此类推。随着图像在网络中前进，它的大小会越来越小，但它通常也会越来越深（即，具有更多的特征图），这要归功于卷积层（参见[图12-12](#cnn_architecture_diagram)）。在堆栈的顶部，添加了一个常规的前馈神经网络，由几个全连接层（+ReLUs）组成，并且最后一层输出预测（例如，一个输出估计类概率的softmax层）。
- en: '![Diagram illustrating a typical CNN architecture with layers for convolution,
    pooling, and fully connected operations.](assets/hmls_1212.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![说明典型CNN架构的图，包含卷积、池化和全连接操作的层](assets/hmls_1212.png)'
- en: Figure 12-12\. Typical CNN architecture
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-12. 典型的CNN架构
- en: Tip
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Instead of using a convolutional layer with a 5 × 5 kernel, it is generally
    preferable to stack two layers with 3 × 3 kernels: it will use fewer parameters
    and require fewer computations, and it will usually perform better. One exception
    is for the first convolutional layer: it can typically have a large kernel (e.g.,
    5 × 5 or 7 × 7), usually with a stride of 2 or more. This reduces the spatial
    dimension of the image without losing too much information, and since the input
    image only has three channels in general, it will not be too costly.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用5 × 5核的卷积层相比，通常更倾向于堆叠两个3 × 3核的层：它将使用更少的参数，需要更少的计算，并且通常表现更好。一个例外是第一个卷积层：它通常可以有一个大的核（例如，5
    × 5或7 × 7），通常步长为2或更多。这减少了图像的空间维度，同时不会丢失太多信息，而且由于输入图像通常只有三个通道，这不会太昂贵。
- en: 'Here is how you can implement a basic CNN to tackle the Fashion MNIST dataset
    (introduced in [Chapter 9](ch09.html#ann_chapter)):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是如何实现一个基本的CNN来解决Fashion MNIST数据集（在第9章中介绍）的方法：
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s go through this code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这段代码：
- en: 'We use the `functools.partial()` function (introduced in [Chapter 11](ch11.html#deep_chapter))
    to define `DefaultConv2d`, which acts just like `nn.Conv2d` but with different
    default arguments: a small kernel size of 3, and `"same"` padding. This avoids
    having to repeat these arguments throughout the model.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`functools.partial()`函数（在第11章中介绍）来定义`DefaultConv2d`，它就像`nn.Conv2d`一样，但具有不同的默认参数：小的核大小为3，以及`"same"`填充。这避免了在整个模型中重复这些参数。
- en: Next, we create the `nn.Sequential` model. Its first layer is a `DefaultConv2d`
    with 64 fairly large filters (7 × 7). It uses the default stride of 1 because
    the input images are not very large. It also uses `in_channels=1` because the
    Fashion MNIST images have a single color channel (i.e., grayscale). Each convolutional
    layer is followed by the ReLU activation function.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们创建`nn.Sequential`模型。它的第一层是一个具有64个相当大的过滤器（7 × 7）的`DefaultConv2d`。它使用默认步长1，因为输入图像并不很大。它还使用`in_channels=1`，因为Fashion
    MNIST图像只有一个颜色通道（即灰度）。每个卷积层后面跟着ReLU激活函数。
- en: We then add a max pooling layer with a kernel size of 2, so it divides each
    spatial dimension by a factor of 2 (rounded down if needed).
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们添加一个核大小为2的最大池化层，因此它将每个空间维度除以2的因子（如果需要则向下取整）。
- en: 'Then we repeat the same structure twice: two convolutional layers followed
    by a max pooling layer. For larger images, we could repeat this structure several
    more times. The number of repetitions is a hyperparameter you can tune.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们重复相同的结构两次：两个卷积层后面跟着一个最大池化层。对于更大的图像，我们可以重复这个结构几次。重复的次数是一个你可以调整的超参数。
- en: 'Note that the number of filters doubles as we climb up the CNN toward the output
    layer (it is initially 64, then 128, then 256). It makes sense for it to grow,
    since the number of low-level features is often fairly low (e.g., small circles,
    horizontal lines), but there are many different ways to combine them into higher-level
    features. It is a common practice to double the number of filters after each pooling
    layer: since a pooling layer divides each spatial dimension by a factor of 2,
    we can afford to double the number of feature maps in the next layer without fear
    of exploding the number of parameters, memory usage, or computational load.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意，当我们沿着卷积神经网络向上爬到输出层时，滤波器的数量会翻倍（最初是64，然后是128，接着是256）。随着层数的增加而增加是有道理的，因为低级特征的数量通常相当低（例如，小圆圈、水平线），但是将它们组合成高级特征的方法有很多种。在每次池化层之后加倍滤波器的数量是一种常见的做法：由于池化层将每个空间维度除以2，因此我们可以在不担心参数数量、内存使用或计算负载爆炸的情况下，在下一层加倍特征图的数量。
- en: Next is the fully connected network, composed of two hidden dense layers (`nn.Linear`)
    with the ReLU activation function, plus a dense output layer. Since it’s a classification
    task with 10 classes, the output layer has 10 units. As we did in [Chapter 10](ch10.html#pytorch_chapter),
    we leave out the softmax activation function, so the model will output logits
    rather than probabilities, and we must use the `nn.CrossEntropyLoss` to train
    the model. Note that we must flatten the inputs just before the first dense layer,
    since it expects a 1D array of features for each instance. We also add two dropout
    layers, with a dropout rate of 50% each, to reduce overfitting.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来是完全连接的网络，由两个具有ReLU激活函数的隐藏密集层（`nn.Linear`）和一个密集输出层组成。由于这是一个有10个类别的分类任务，输出层有10个单元。正如我们在[第10章](ch10.html#pytorch_chapter)中所做的那样，我们省略了softmax激活函数，因此模型将输出logits而不是概率，我们必须使用`nn.CrossEntropyLoss`来训练模型。请注意，我们必须在第一个密集层之前将输入展平，因为它期望每个实例都有一个特征的一维数组。我们还添加了两个dropout层，每个层的dropout率为50%，以减少过拟合。
- en: Tip
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'The first `nn.Linear` layer has 2,304 input features: where did this number
    come from? Well the Fashion MNIST images are 28 × 28 pixels, but the pooling layers
    shrink them to 14 × 14, then 7 × 7, and finally 3 × 3\. Just before the first
    `nn.Linear` layer, there are 256 feature maps, so we end up with 256 × 3 × 3 =
    2,304 input features. Figuring out the number of features can sometimes be a bit
    difficult, but one trick is to set `in_features` to some arbitrary value (say,
    999), and let training crash. The correct number of features appears in the error
    message: “RuntimeError: mat1 and mat2 shapes cannot be multiplied (32x2304 and
    999x128)”. Another option is to use `nn.LazyLinear` instead of `nn.Linear`: it’s
    just like the `nn.Linear` layer, except it only creates the weights matrix the
    first time it gets called: it can then automatically set the number of input features
    to the correct value. Other layers—such as convolutional layers and batch-norm
    layers—also have lazy variants.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个`nn.Linear`层有2,304个输入特征：这个数字是从哪里来的？嗯，时尚MNIST图像是28 × 28像素，但是池化层将它们缩小到14 ×
    14，然后是7 × 7，最后是3 × 3。在第一个`nn.Linear`层之前，有256个特征图，所以我们最终有256 × 3 × 3 = 2,304个输入特征。弄清楚特征的数量有时可能有点困难，但一个技巧是将`in_features`设置为某个任意值（比如说999），并让训练失败。正确的特征数量会出现在错误信息中：“RuntimeError：mat1和mat2的形状不能相乘（32x2304和999x128）”。另一个选项是使用`nn.LazyLinear`而不是`nn.Linear`：它就像`nn.Linear`层一样，只不过它只在第一次被调用时创建权重矩阵：然后它可以自动将输入特征的数量设置为正确的值。其他层——例如卷积层和批归一化层——也有懒惰的变体。
- en: If you train this model on the Fashion MNIST training set, it should reach close
    to 92% accuracy on the test set (you can use the `train()` and `evaluate_tm()`
    functions we defined in [Chapter 10](ch10.html#pytorch_chapter)). It’s not state
    of the art, but it is pretty good, and better than what we achieved with dense
    networks in [Chapter 9](ch09.html#ann_chapter).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这个时尚MNIST训练集上训练这个模型，它应该在测试集上达到接近92%的准确率（你可以使用我们在[第10章](ch10.html#pytorch_chapter)中定义的`train()`和`evaluate_tm()`函数）。它不是最先进的，但相当不错，比我们在[第9章](ch09.html#ann_chapter)中使用密集网络实现的要好。
- en: Over the years, variants of this fundamental architecture have been developed,
    leading to amazing advances in the field. A good measure of this progress is the
    error rate in competitions such as the ILSVRC [ImageNet challenge](https://image-net.org).
    In this competition, the error rate for image classification fell from over 26%
    to less than 2.3% in just 6 years. More precisely, this was the *top-five error
    rate*, which is the ratio of test images for which the system’s five most confident
    predictions did *not* include the correct answer. The images are fairly large
    (e.g., 256 pixels high) and there are 1,000 classes, some of which are really
    subtle (try distinguishing 120 dog breeds!). Looking at the evolution of the winning
    entries is a good way to understand how CNNs work, and how research in deep learning
    progresses.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 几年来，这种基本架构的变体得到了发展，在领域内取得了惊人的进步。衡量这种进步的一个好方法是 ILSVRC [ImageNet 挑战赛](https://image-net.org)中的错误率。在这个比赛中，图像分类的错误率在
    6 年内从超过 26% 下降到不到 2.3%。更确切地说，这是 *前五错误率*，即测试图像中系统对其五个最自信的预测没有包含正确答案的比率。图像相当大（例如，高度为
    256 像素），有 1,000 个类别，其中一些非常微妙（尝试区分 120 种狗的品种！）。查看获胜作品的演变是了解 CNN 的工作原理以及深度学习研究进展的好方法。
- en: 'We will first look at the classical LeNet-5 architecture (1998), then several
    winners of the ILSVRC challenge: AlexNet (2012), GoogLeNet (2014), ResNet (2015),
    and SENet (2017). We will also discuss a few more architectures, including VGGNet,
    Xception, ResNeXt, DenseNet, MobileNet, CSPNet, EfficientNet, and ConvNeXt (and
    we will discuss vision transformers in [Chapter 16](ch16.html#vit_chapter)).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先查看经典的 LeNet-5 架构（1998 年），然后是 ILSVRC 挑战赛的几个获胜者：AlexNet（2012 年）、GoogLeNet（2014
    年）、ResNet（2015 年）和 SENet（2017 年）。我们还将讨论一些其他架构，包括 VGGNet、Xception、ResNeXt、DenseNet、MobileNet、CSPNet、EfficientNet
    和 ConvNeXt（我们将在[第 16 章](ch16.html#vit_chapter)中讨论视觉 Transformer）。
- en: LeNet-5
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LeNet-5
- en: The [LeNet-5 architecture](https://homl.info/lenet5)⁠^([8](ch12.html#id2805))
    is perhaps the most widely known CNN architecture. As mentioned earlier, it was
    created by Yann LeCun in 1998 and has been widely used for handwritten digit recognition
    (MNIST). It is composed of the layers shown in [Table 12-1](#lenet_5_architecture).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[LeNet-5 架构](https://homl.info/lenet5)⁠^([8](ch12.html#id2805))可能是最广为人知的 CNN
    架构。如前所述，它是由 Yann LeCun 在 1998 年创建的，并被广泛用于手写数字识别（MNIST）。它由[表 12-1](#lenet_5_architecture)中显示的层组成。'
- en: Table 12-1\. LeNet-5 architecture
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表 12-1\. LeNet-5 架构
- en: '| Layer | Type | Maps | Size | Kernel size | Stride | Activation |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| Layer | 类型 | 图层 | 大小 | 核大小 | 步长 | 激活 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Out | Fully connected | – | 10 | – | – | RBF |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Out | 全连接 | – | 10 | – | – | RBF |'
- en: '| F6 | Fully connected | – | 84 | – | – | tanh |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| F6 | 全连接 | – | 84 | – | – | tanh |'
- en: '| C5 | Convolution | 120 | 1 × 1 | 5 × 5 | 1 | tanh |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| C5 | 卷积 | 120 | 1 × 1 | 5 × 5 | 1 | tanh |'
- en: '| S4 | Avg pooling | 16 | 5 × 5 | 2 × 2 | 2 | tanh |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| S4 | 平均池化 | 16 | 5 × 5 | 2 × 2 | 2 | tanh |'
- en: '| C3 | Convolution | 16 | 10 × 10 | 5 × 5 | 1 | tanh |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| C3 | 卷积 | 16 | 10 × 10 | 5 × 5 | 1 | tanh |'
- en: '| S2 | Avg pooling | 6 | 14 × 14 | 2 × 2 | 2 | tanh |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 平均池化 | 6 | 14 × 14 | 2 × 2 | 2 | tanh |'
- en: '| C1 | Convolution | 6 | 28 × 28 | 5 × 5 | 1 | tanh |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| C1 | 卷积 | 6 | 28 × 28 | 5 × 5 | 1 | tanh |'
- en: '| In | Input | 1 | 32 × 32 | – | – | – |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| In | 输入 | 1 | 32 × 32 | – | – | – |'
- en: 'As you can see, this looks pretty similar to our Fashion MNIST model: a stack
    of convolutional layers and pooling layers, followed by a dense network. Perhaps
    the main difference with more modern classification CNNs is the activation functions:
    today, we would use ReLU instead of tanh, and softmax instead of RBF (introduced
    in [Chapter 2](ch02.html#project_chapter)). There were several other minor differences
    that don’t really matter much, but in case you are interested, they are listed
    in this chapter’s notebook at [*https://homl.info/colab-p*](https://homl.info/colab-p).
    Yann LeCun’s [website](http://yann.lecun.com/exdb/lenet) also features great demos
    of LeNet-5 classifying digits.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这与我们的 Fashion MNIST 模型非常相似：一系列卷积层和池化层，之后是一个密集的网络。也许与更现代的分类 CNN 相比的主要区别在于激活函数：今天，我们会使用
    ReLU 而不是 tanh，以及 softmax 而不是 RBF（在[第 2 章](ch02.html#project_chapter)中介绍）。还有一些其他的小差异并不真正重要，但如果您感兴趣，它们在本章的笔记本中列出，见[*https://homl.info/colab-p*](https://homl.info/colab-p)。Yann
    LeCun 的[网站](http://yann.lecun.com/exdb/lenet)也展示了 LeNet-5 对数字进行分类的精彩演示。
- en: AlexNet
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AlexNet
- en: 'The [AlexNet CNN architecture](https://homl.info/80)⁠^([9](ch12.html#id2808))
    won the 2012 ILSVRC challenge by a large margin: it achieved a top-five error
    rate of 17%, while the second best competitor achieved only 26%! AlexNet was developed
    by Alex Krizhevsky (hence the name), Ilya Sutskever, and Geoffrey Hinton. It is
    similar to LeNet-5, only much larger and deeper, and it was the first to stack
    convolutional layers directly on top of one another, instead of stacking a pooling
    layer on top of each convolutional layer. [Table 12-2](#alexnet_architecture)
    presents this architecture.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[AlexNet CNN架构](https://homl.info/80)⁠^([9](ch12.html#id2808))通过大幅领先赢得了2012年ILSVRC挑战赛：它实现了17%的顶级错误率，而第二名的竞争者只有26%！AlexNet由Alex
    Krizhevsky（因此得名）、Ilya Sutskever和Geoffrey Hinton开发。它与LeNet-5相似，只是更大、更深，并且它是第一个直接堆叠卷积层而不是在每个卷积层之上堆叠池化层的架构。[表12-2](#alexnet_architecture)展示了这一架构。'
- en: Table 12-2\. AlexNet architecture
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表12-2\. AlexNet架构
- en: '| Layer | Type | Maps | Size | Kernel size | Stride | Padding | Activation
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 层 | 类型 | Maps | Size | 核大小 | 步长 | 填充 | 激活 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Out | Fully connected | – | 1,000 | – | – | – | Softmax |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| Out | 全连接 | – | 1,000 | – | – | – | Softmax |'
- en: '| F10 | Fully connected | – | 4,096 | – | – | – | ReLU |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| F10 | 全连接 | – | 4,096 | – | – | – | ReLU |'
- en: '| F9 | Fully connected | – | 4,096 | – | – | – | ReLU |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| F9 | 全连接 | – | 4,096 | – | – | – | ReLU |'
- en: '| S8 | Max pooling | 256 | 6 × 6 | 3 × 3 | 2 | `valid` | – |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| S8 | 最大池化 | 256 | 6 × 6 | 3 × 3 | 2 | `valid` | – |'
- en: '| C7 | Convolution | 256 | 13 × 13 | 3 × 3 | 1 | `same` | ReLU |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| C7 | 卷积 | 256 | 13 × 13 | 3 × 3 | 1 | `same` | ReLU |'
- en: '| C6 | Convolution | 384 | 13 × 13 | 3 × 3 | 1 | `same` | ReLU |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| C6 | 卷积 | 384 | 13 × 13 | 3 × 3 | 1 | `same` | ReLU |'
- en: '| C5 | Convolution | 384 | 13 × 13 | 3 × 3 | 1 | `same` | ReLU |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| C5 | 卷积 | 384 | 13 × 13 | 3 × 3 | 1 | `same` | ReLU |'
- en: '| S4 | Max pooling | 256 | 13 × 13 | 3 × 3 | 2 | `valid` | – |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| S4 | 最大池化 | 256 | 13 × 13 | 3 × 3 | 2 | `valid` | – |'
- en: '| C3 | Convolution | 256 | 27 × 27 | 5 × 5 | 1 | `same` | ReLU |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| C3 | 卷积 | 256 | 27 × 27 | 5 × 5 | 1 | `same` | ReLU |'
- en: '| S2 | Max pooling | 96 | 27 × 27 | 3 × 3 | 2 | `valid` | – |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| S2 | 最大池化 | 96 | 27 × 27 | 3 × 3 | 2 | `valid` | – |'
- en: '| C1 | Convolution | 96 | 55 × 55 | 11 × 11 | 4 | `valid` | ReLU |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| C1 | 卷积 | 96 | 55 × 55 | 11 × 11 | 4 | `valid` | ReLU |'
- en: '| In | Input | 3 (RGB) | 227 × 227 | – | – | – | – |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| In | 输入 | 3 (RGB) | 227 × 227 | – | – | – | – |'
- en: To reduce overfitting, the authors used two regularization techniques. First,
    they applied dropout (introduced in [Chapter 11](ch11.html#deep_chapter)) with
    a 50% dropout rate during training to the outputs of layers F9 and F10\. Second,
    they performed data augmentation by randomly shifting the training images by various
    offsets, flipping them horizontally, and changing the lighting conditions.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少过拟合，作者使用了两种正则化技术。首先，他们在训练过程中对F9和F10层的输出应用了dropout（在第11章中介绍），dropout率为50%。其次，他们通过随机平移训练图像、水平翻转和改变光照条件来执行数据增强。
- en: 'AlexNet also used a regularization technique called *local response normalization*
    (LRN): the most strongly activated neurons inhibit other neurons located at the
    same position in neighboring feature maps. Such competitive activation has been
    observed in biological neurons. This encourages different feature maps to specialize,
    pushing them apart and forcing them to explore a wider range of features, ultimately
    improving generalization. However, this technique was mostly superseded by simpler
    and more efficient regularization techniques, especially batch normalization.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: AlexNet还使用了一种名为*局部响应归一化*（LRN）的正则化技术：最强烈激活的神经元抑制位于相邻特征图中相同位置的神经元。这种竞争性激活在生物神经元中已被观察到。这鼓励不同的特征图进行专业化，将它们推开，并迫使它们探索更广泛的功能范围，从而提高泛化能力。然而，这种技术主要被更简单、更有效的正则化技术所取代，尤其是批量归一化。
- en: A variant of AlexNet called [*ZFNet*](https://homl.info/zfnet)⁠^([10](ch12.html#id2820))
    was developed by Matthew Zeiler and Rob Fergus and won the 2013 ILSVRC challenge.
    It is essentially AlexNet with a few tweaked hyperparameters (number of feature
    maps, kernel size, stride, etc.).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一种名为[*ZFNet*](https://homl.info/zfnet)⁠^([10](ch12.html#id2820))的AlexNet变体由Matthew
    Zeiler和Rob Fergus开发，并在2013年ILSVRC挑战赛中获胜。它本质上与AlexNet相似，只是对一些超参数（特征图数量、核大小、步长等）进行了微调。
- en: GoogLeNet
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GoogLeNet
- en: 'The [GoogLeNet architecture](https://homl.info/81) was developed by Christian
    Szegedy et al. from Google Research,⁠^([11](ch12.html#id2822)) and it won the
    ILSVRC 2014 challenge by pushing the top-five error rate below 7%. This great
    performance came in large part from the fact that the network was much deeper
    than previous CNNs (as you’ll see in [Figure 12-15](#googlenet_diagram)). This
    was made possible by subnetworks called *inception modules*,⁠^([12](ch12.html#id2823))
    which allow GoogLeNet to use parameters much more efficiently than previous architectures:
    GoogLeNet actually has 10 times fewer parameters than AlexNet (roughly 6 million
    instead of 60 million).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '[GoogLeNet 架构](https://homl.info/81)是由 Google 研究的 Christian Szegedy 等人开发的⁠^([11](ch12.html#id2822))，它通过将前五名的错误率推至
    7% 以下赢得了 ILSVRC 2014 挑战赛。这种出色的性能在很大程度上得益于网络比之前的 CNN 深得多（正如你将在 [图 12-15](#googlenet_diagram)
    中看到的那样）。这是通过称为 *inception 模块* 的子网络实现的⁠^([12](ch12.html#id2823))，它允许 GoogLeNet
    比之前的架构更有效地使用参数：GoogLeNet 实际上比 AlexNet 少 10 倍的参数（大约 600 万而不是 6000 万）。'
- en: '[Figure 12-14](#inception_module_diagram) shows the architecture of an inception
    module. The notation “3 × 3 + 1(S)” means that the layer uses a 3 × 3 kernel,
    stride 1, and `"same"` padding. The input signal is first fed to four different
    layers in parallel. All convolutional layers use the ReLU activation function.
    Note that the top convolutional layers use different kernel sizes (1 × 1, 3 ×
    3, and 5 × 5), allowing them to capture patterns at different scales. Also note
    that every single layer uses a stride of 1 and `"same"` padding (even the max
    pooling layer), so their outputs all have the same height and width as their inputs.
    This makes it possible to concatenate all the outputs along the depth dimension
    in the final *depth concatenation layer* (i.e., it concatenates the multiple feature
    maps output by each of the upper four convolutional layers). It can be implemented
    using the `torch.cat()` function, with `dim=1`.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 12-14](#inception_module_diagram) 展示了 inception 模块的架构。符号“3 × 3 + 1(S)”表示该层使用
    3 × 3 内核，步长 1，以及 `"same"` 填充。输入信号首先并行馈送到四个不同的层。所有卷积层都使用 ReLU 激活函数。请注意，顶部的卷积层使用不同的内核大小（1
    × 1、3 × 3 和 5 × 5），这使得它们能够捕捉不同尺度的模式。此外，请注意，每一层都使用步长 1 和 `"same"` 填充（甚至最大池化层），因此它们的输出与它们的输入具有相同的高度和宽度。这使得在最终的
    *深度拼接层*（即，它拼接了上层四个卷积层输出的多个特征图）中沿深度维度拼接所有输出成为可能。它可以使用 `torch.cat()` 函数实现，其中 `dim=1`。'
- en: '![Diagram illustrating the architecture of an inception module, showing parallel
    convolutional layers with various kernel sizes and a depth concatenation layer
    for combining outputs.](assets/hmls_1214.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![说明 inception 模块架构的图，显示具有不同内核大小的并行卷积层和用于组合输出的深度拼接层。](assets/hmls_1214.png)'
- en: Figure 12-14\. Inception module
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-14\. Inception 模块
- en: 'You may wonder why inception modules have convolutional layers with 1 × 1 kernels.
    Surely these layers cannot capture any features because they look at only one
    pixel at a time, right? In fact, these layers serve three purposes:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么 inception 模块有 1 × 1 内核的卷积层。当然，这些层无法捕捉任何特征，因为它们一次只查看一个像素，对吧？实际上，这些层有三个作用：
- en: Although they cannot capture spatial patterns, they can capture patterns along
    the depth dimension (i.e., across channels).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管它们无法捕捉空间模式，但它们可以捕捉深度维度上的模式（即，跨通道）。
- en: They are configured to output fewer feature maps than their inputs, so they
    serve as *bottleneck layers*, meaning they reduce dimensionality. This cuts the
    computational cost and the number of parameters, speeding up training and improving
    generalization.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们被配置为输出比输入更少的特征图，因此它们作为 *瓶颈层*，意味着它们降低维度。这减少了计算成本和参数数量，加快了训练速度并提高了泛化能力。
- en: Each pair of convolutional layers ([1 × 1, 3 × 3] and [1 × 1, 5 × 5]) acts like
    a single powerful convolutional layer, capable of capturing more complex patterns.
    A convolutional layer is equivalent to sweeping a dense layer across the image
    (at each location, it only looks at a small receptive field), and these pairs
    of convolutional layers are equivalent to sweeping two-layer neural networks across
    the image.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一对卷积层（[1 × 1, 3 × 3] 和 [1 × 1, 5 × 5]）就像一个单一的强大卷积层，能够捕捉更复杂的模式。卷积层相当于在图像上扫过一个密集层（在每个位置，它只查看一个小感受野），而这些成对的卷积层相当于在图像上扫过两层神经网络。
- en: In short, you can think of the whole inception module as a convolutional layer
    on steroids, able to output feature maps that capture complex patterns at various
    scales.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，你可以将整个inception模块视为一种类固醇化的卷积层，能够输出能够捕捉到各种尺度复杂模式的特征图。
- en: Now let’s look at the architecture of the GoogLeNet CNN (see [Figure 12-15](#googlenet_diagram)).
    The number of feature maps output by each convolutional layer and each pooling
    layer is shown before the kernel size. The architecture is so deep that it has
    to be represented in three columns, but GoogLeNet is actually one tall stack,
    including nine inception modules (the boxes with the spinning tops). The six numbers
    in the inception modules represent the number of feature maps output by each convolutional
    layer in the module (in the same order as in [Figure 12-14](#inception_module_diagram)).
    Note that all the convolutional layers use the ReLU activation function.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看GoogLeNet卷积神经网络（CNN）的架构（见[图12-15](#googlenet_diagram)）。每个卷积层和池化层输出的特征图数量在核大小之前显示。架构非常深，以至于需要用三列来表示，但GoogLeNet实际上是一个高高的堆叠，包括九个
    inception模块（带有旋转顶部的盒子）。inception模块中的六个数字代表模块中每个卷积层输出的特征图数量（顺序与[图12-14](#inception_module_diagram)中的相同）。请注意，所有卷积层都使用ReLU激活函数。
- en: '![Diagram of GoogLeNet architecture showing the layers and inception modules,
    detailing the number of feature maps and operations like convolution, pooling,
    and normalization.](assets/hmls_1215.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![GoogLeNet架构图，显示了层和inception模块，详细说明了特征图的数量和操作，如卷积、池化和归一化。](assets/hmls_1215.png)'
- en: Figure 12-15\. GoogLeNet architecture
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-15\. GoogLeNet架构
- en: 'Let’s go through this network:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们遍历这个网络：
- en: The first two layers divide the image’s height and width by 4 (so its area is
    divided by 16), to reduce the computational load. The first layer uses a large
    kernel size, 7 × 7, so that much of the information is preserved.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前两层将图像的高度和宽度分别除以4（因此面积除以16），以减少计算负担。第一层使用较大的核大小，7×7，这样就能保留大部分信息。
- en: Then the local response normalization layer ensures that the previous layers
    learn a wide variety of features (as discussed earlier).
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后局部响应归一化层确保前一层学习到广泛的特征（如前所述）。
- en: Two convolutional layers follow, where the first acts like a bottleneck layer.
    As mentioned, you can think of this pair as a single smarter convolutional layer.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接着是两个卷积层，其中第一个类似于瓶颈层。如前所述，你可以将这对层视为一个更智能的单个卷积层。
- en: Again, a local response normalization layer ensures that the previous layers
    capture a wide variety of patterns.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再次，一个局部响应归一化层确保了前一层能够捕捉到广泛的模式。
- en: Next, a max pooling layer reduces the image height and width by 2, again to
    speed up computations.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，一个最大池化层将图像的高度和宽度减少到原来的二分之一，再次用于加速计算。
- en: 'Then comes the CNN’s *backbone*: a tall stack of nine inception modules, interleaved
    with a couple of max pooling layers to reduce dimensionality and speed up the
    net.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后是CNN的*主干*：九个inception模块的高高堆叠，穿插着几个最大池化层以降低维度并加速网络。
- en: 'Next, the global average pooling layer outputs the mean of each feature map:
    this drops any remaining spatial information, which is fine because there is not
    much spatial information left at that point. Indeed, GoogLeNet input images are
    typically expected to be 224 × 224 pixels, so after 5 max pooling layers, each
    dividing the height and width by 2, the feature maps are down to 7 × 7\. Moreover,
    this is a classification task, not localization, so it doesn’t matter where the
    object is. Thanks to the dimensionality reduction brought by this layer, there
    is no need to have several fully connected layers at the top of the CNN (like
    in AlexNet), and this considerably reduces the number of parameters in the network
    and limits the risk of overfitting.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，全局平均池化层输出每个特征图的平均值：这去除了任何剩余的空间信息，这很好，因为在这个点上空间信息已经很少了。实际上，GoogLeNet的输入图像通常是224×224像素，经过5个最大池化层后，每个池化层将高度和宽度除以2，特征图降至7×7。此外，这是一个分类任务，而不是定位任务，所以物体在哪里并不重要。多亏了这一层带来的维度降低，就不需要在CNN的顶部有多个全连接层（如AlexNet中那样），这大大减少了网络中的参数数量，并限制了过拟合的风险。
- en: 'The last layers are self-explanatory: dropout for regularization, then a fully
    connected layer with 1,000 units (since there are 1,000 classes) and a softmax
    activation function to output estimated class probabilities.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后几层是显而易见的：用于正则化的dropout层，然后是一个包含1,000个单元的全连接层（因为共有1,000个类别），以及一个softmax激活函数来输出估计的类别概率。
- en: The original GoogLeNet architecture included two auxiliary classifiers plugged
    on top of the third and sixth inception modules. They were both composed of one
    average pooling layer, one convolutional layer, two fully connected layers, and
    a softmax activation layer. During training, their loss (scaled down by 70%) was
    added to the overall loss. The goal was to fight the vanishing gradients problem
    and regularize the network, but it was later shown that their effect was relatively
    minor.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的GoogLeNet架构在第三和第六个Inception模块的顶部插入了两个辅助分类器。它们都由一个平均池化层、一个卷积层、两个全连接层和一个softmax激活层组成。在训练过程中，它们的损失（按70%的比例缩小）被添加到总损失中。目标是解决梯度消失问题并正则化网络，但后来发现它们的效果相对较小。
- en: Several variants of the GoogLeNet architecture were later proposed by Google
    researchers, including Inception-v3 and Inception-v4, using slightly different
    inception modules to reach even better performance.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Google研究人员后来提出了GoogLeNet架构的几个变体，包括Inception-v3和Inception-v4，它们使用略微不同的Inception模块以达到更好的性能。
- en: ResNet
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ResNet
- en: 'Kaiming He et al. won the ILSVRC 2015 challenge using a [Residual Network (ResNet)](https://homl.info/82)⁠^([13](ch12.html#id2839))
    that delivered an astounding top-five error rate under 3.6%. The winning variant
    used an extremely deep CNN composed of 152 layers (other variants had 34, 50,
    and 101 layers). It confirmed the general trend: computer vision models were getting
    deeper and deeper, with fewer and fewer parameters. The key to being able to train
    such a deep network is to use *skip connections* (also called *shortcut connections*):
    the signal feeding into a layer is also added to the output of a layer located
    higher up the stack. Let’s see why this is useful.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Kaiming He等人使用一个[残差网络（ResNet）](https://homl.info/82)⁠^([13](ch12.html#id2839))赢得了2015年ILSVRC挑战赛，该网络在3.6%以下的误差率下取得了惊人的前五名。获胜的变体使用了一个由152层组成的极其深的CNN（其他变体有34、50和101层）。这证实了一般趋势：计算机视觉模型变得越来越深，参数越来越少。能够训练如此深的网络的关键是使用*跳跃连接*（也称为*快捷连接*）：输入到层的信号也被添加到堆栈中更高层的层的输出。让我们看看为什么这很有用。
- en: When training a neural network, the goal is to make it model a target function
    *h*(**x**). If you add the input **x** to the output of the network (i.e., you
    add a skip connection), then the network will be forced to model *f*(**x**) =
    *h*(**x**) – **x** rather than *h*(**x**). This is called *residual learning*
    (see [Figure 12-16](#residual_learning_diagram)).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，目标是使其模拟目标函数*h*(**x**)。如果你将输入**x**添加到网络的输出（即添加一个跳跃连接），那么网络将被迫模拟*f*(**x**)
    = *h*(**x**) – **x**而不是*h*(**x**)。这被称为*残差学习*（参见[图12-16](#residual_learning_diagram)）。
- en: '![Diagram comparing a basic neural network structure with a residual network
    that includes a skip connection, illustrating residual learning where output models
    the difference between the target function and input.](assets/hmls_1216.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![比较基本神经网络结构与包含跳跃连接的残差网络图，说明残差学习，其中输出模拟目标函数与输入之间的差异。](assets/hmls_1216.png)'
- en: Figure 12-16\. Residual learning
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-16\. 残差学习
- en: When you initialize a neural network, its weights are close to zero, so a regular
    network just outputs values close to zero when training starts. But if you add
    a skip connection, the resulting network outputs a copy of its inputs; in other
    words, it acts as the identity function at the start of training. If the target
    function is fairly close to the identity function (which is often the case), this
    will speed up training considerably.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 当你初始化一个神经网络时，其权重接近于零，因此常规网络在训练开始时只输出接近零的值。但是，如果你添加一个跳跃连接，结果网络将输出其输入的副本；换句话说，它在训练开始时充当了恒等函数。如果目标函数相当接近恒等函数（这通常是情况），这将大大加快训练速度。
- en: Moreover, if you add many skip connections, the network can start making progress
    even if several layers have not started learning yet (see [Figure 12-17](#deep_residual_network_diagram)).
    Thanks to skip connections, the signal can easily make its way across the whole
    network. The deep residual network can be seen as a stack of *residual units*
    (RUs), where each residual unit is a small neural network with a skip connection.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你添加了许多跳跃连接，即使有几层还没有开始学习，网络也能开始取得进展（参见[图12-17](#deep_residual_network_diagram)）。多亏了跳跃连接，信号可以轻松地穿越整个网络。深度残差网络可以看作是一堆*残差单元*（RUs），其中每个残差单元都是一个带有跳跃连接的小型神经网络。
- en: Now let’s look at ResNet’s architecture (see [Figure 12-18](#resnet_diagram)).
    It is surprisingly simple. It starts and ends exactly like GoogLeNet (except without
    a dropout layer), and in between is just a very deep stack of residual units.
    Each residual unit is composed of two convolutional layers (and no pooling layer!),
    with batch normalization (BN) and ReLU activation, using 3 × 3 kernels and preserving
    spatial dimensions (stride 1, `"same"` padding).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看 ResNet 的架构（参见 [图 12-18](#resnet_diagram)）。它出人意料地简单。它开始和结束都与 GoogLeNet
    完全一样（除了没有 dropout 层），中间只是一个非常深的残差单元堆叠。每个残差单元由两个卷积层（没有池化层！）组成，使用批量归一化（BN）和 ReLU
    激活，使用 3 × 3 内核并保留空间维度（步长 1，`"same"` 填充）。
- en: '![Diagram comparing a regular deep neural network with a deep residual network,
    highlighting how skip connections in residual units help bypass layers that block
    backpropagation and aid in learning.](assets/hmls_1217.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![比较常规深度神经网络和深度残差网络的图，突出显示残差单元中的跳跃连接如何帮助绕过阻碍反向传播的层并有助于学习的示意图。](assets/hmls_1217.png)'
- en: Figure 12-17\. Regular deep neural network (left) and deep residual network
    (right)
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-17\. 常规深度神经网络（左）和深度残差网络（右）
- en: '![Diagram of ResNet architecture illustrating the deep stack of residual units,
    each composed of two convolutional layers with batch normalization and ReLU activation,
    demonstrating the flow and connections between layers.](assets/hmls_1218.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![ResNet 架构的图，展示了由两个卷积层、批量归一化和 ReLU 激活组成的深度堆叠的残差单元，展示了层之间的流动和连接。](assets/hmls_1218.png)'
- en: Figure 12-18\. ResNet architecture
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-18\. ResNet 架构
- en: Note that the number of feature maps is doubled every few residual units, at
    the same time as their height and width are halved (using a convolutional layer
    with stride 2). When this happens, the inputs cannot be added directly to the
    outputs of the residual unit because they don’t have the same shape (for example,
    this problem affects the skip connection represented by the dashed arrow in [Figure 12-18](#resnet_diagram)).
    To solve this problem, the inputs are passed through a 1 × 1 convolutional layer
    with stride 2 and the right number of output feature maps (see [Figure 12-19](#resize_skip_connection_diagram)).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，特征图的数量在每几个残差单元后翻倍，同时其高度和宽度减半（使用步长为 2 的卷积层）。当这种情况发生时，输入不能直接添加到残差单元的输出中，因为它们的形状不同（例如，这个问题影响了图
    12-18 中由虚线箭头表示的跳跃连接）。为了解决这个问题，输入通过一个步长为 2 的 1 × 1 卷积层和正确的输出特征图数量传递（参见 [图 12-19](#resize_skip_connection_diagram)）。
- en: '![Diagram illustrating a skip connection in a residual network, showing how
    inputs are adjusted via a 1x1 convolution when changing feature map size and depth.](assets/hmls_1219.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![说明残差网络中跳跃连接的图，展示了在改变特征图大小和深度时，如何通过 1x1 卷积调整输入。](assets/hmls_1219.png)'
- en: Figure 12-19\. Skip connection when changing feature map size and depth
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 12-19\. 改变特征图大小和深度时的跳跃连接
- en: Tip
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: During training, for each mini-batch, you can skip a random set of residual
    units. This [*stochastic depth* technique](https://homl.info/sdepth)⁠^([14](ch12.html#id2843))
    speeds up training considerably without compromising accuracy. You can implement
    it using the `torchvision.ops.​sto⁠chastic_depth()` function.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，对于每个小批量，你可以跳过一组随机的残差单元。这种 [*随机深度* 技术](https://homl.info/sdepth)⁠^([14](ch12.html#id2843))
    可以显著加快训练速度，而不会影响准确性。你可以使用 `torchvision.ops.​sto⁠chastic_depth()` 函数来实现它。
- en: Different variations of the architecture exist, with different numbers of layers.
    ResNet-34 is a ResNet with 34 layers (only counting the convolutional layers and
    the fully connected layer)⁠^([15](ch12.html#id2847)) containing 3 RUs that output
    64 feature maps, 4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512
    maps. We will implement this architecture later in this chapter.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 架构的不同变体存在，层数数量不同。ResNet-34 是一个包含 34 层的 ResNet（仅计算卷积层和全连接层）⁠^([15](ch12.html#id2847))，包含
    3 个输出 64 个特征图的 RUs，4 个输出 128 个地图的 RUs，6 个输出 256 个地图的 RUs，以及 3 个输出 512 个地图的 RUs。我们将在本章后面实现这个架构。
- en: 'ResNets deeper than that, such as ResNet-152, use slightly different residual
    units. Instead of two 3 × 3 convolutional layers with, say, 256 feature maps,
    they use three convolutional layers: first a 1 × 1 convolutional layer with just
    64 feature maps (4 times less), which acts as a bottleneck layer (as discussed
    already), then a 3 × 3 layer with 64 feature maps, and finally another 1 × 1 convolutional
    layer with 256 feature maps (4 times 64) that restores the original depth. ResNet-152
    contains 3 such RUs that output 256 maps, then 8 RUs with 512 maps, a whopping
    36 RUs with 1,024 maps, and finally 3 RUs with 2,048 maps.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 比ResNet-152更深的ResNets，例如ResNet-152，使用略微不同的残差单元。它们不是使用两个3 × 3的卷积层，例如256个特征图，而是使用三个卷积层：首先是一个1
    × 1的卷积层，只有64个特征图（4倍少），它作为瓶颈层（如前所述），然后是一个64个特征图的3 × 3层，最后是一个256个特征图（64的四倍）的另一个1
    × 1卷积层，以恢复原始深度。ResNet-152包含3个这样的残差单元，输出256个图，然后是8个512个图的RU，接着是36个1,024个图的RU，最后是3个2,048个图的RU。
- en: Note
  id: totrans-212
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Google’s [Inception-v4 architecture](https://homl.info/84)⁠^([16](ch12.html#id2850))
    merged the ideas of GoogLeNet and ResNet and achieved a top-five error rate of
    close to 3% on ImageNet classification.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌的[Inception-v4架构](https://homl.info/84)⁠^([16](ch12.html#id2850))将GoogLeNet和ResNet的思想合并，在ImageNet分类任务上实现了接近3%的顶级错误率。
- en: Xception
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Xception
- en: 'Another variant of the GoogLeNet architecture is worth noting: [Xception](https://homl.info/xception)⁠^([17](ch12.html#id2853))
    (which stands for *Extreme Inception*) was proposed in 2016 by François Chollet
    (the author of the deep learning framework Keras), and it significantly outperformed
    Inception-v3 on a huge vision task (350 million images and 17,000 classes). Just
    like Inception-v4, it merges the ideas of GoogLeNet and ResNet, but it replaces
    the inception modules with a special type of layer called a *depthwise separable
    convolution layer* (or *separable convolution layer* for short⁠^([18](ch12.html#id2856))).
    These layers had been used before in some CNN architectures, but they were not
    as central as in the Xception architecture. While a regular convolutional layer
    uses filters that try to simultaneously capture spatial patterns (e.g., an oval)
    and cross-channel patterns (e.g., mouth + nose + eyes = face), a separable convolutional
    layer makes the strong assumption that spatial patterns and cross-channel patterns
    can be modeled separately (see [Figure 12-20](#separable_convolution_diagram)).
    Thus, it is composed of two parts: the first part applies a single spatial filter
    to each input feature map, then the second part looks exclusively for cross-channel
    patterns—it is just a regular convolutional layer with 1 × 1 filters.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: GoogLeNet架构的另一个变体值得关注：[Xception](https://homl.info/xception)⁠^([17](ch12.html#id2853))（代表*极端Inception*）由François
    Chollet（深度学习框架Keras的作者）于2016年提出，在巨大的视觉任务（3.5亿张图像和17,000个类别）上显著优于Inception-v3。就像Inception-v4一样，它将GoogLeNet和ResNet的思想合并，但它用一种称为*深度可分离卷积层*（或简称*可分离卷积层*⁠^([18](ch12.html#id2856)))的特殊层替换了inception模块。这些层在之前的一些CNN架构中已经使用过，但在Xception架构中并不像这样核心。而常规卷积层使用尝试同时捕捉空间模式（例如，椭圆形）和跨通道模式（例如，嘴巴+鼻子+眼睛=脸）的滤波器，可分离卷积层则做出了一个强烈的假设，即空间模式和跨通道模式可以分别建模（参见[图12-20](#separable_convolution_diagram)）。因此，它由两部分组成：第一部分对每个输入特征图应用单个空间滤波器，然后第二部分专门寻找跨通道模式——它只是一个具有1
    × 1滤波器的常规卷积层。
- en: '![Diagram illustrating a depthwise separable convolutional layer, showing the
    separation of spatial-only filters applied per input channel and a regular convolutional
    layer with depthwise-only filters.](assets/hmls_1220.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图示深度可分离卷积层，展示了仅应用于输入通道的空间滤波器的分离，以及仅具有深度滤波器的常规卷积层。](assets/hmls_1220.png)'
- en: Figure 12-20\. Depthwise separable convolutional layer
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-20. 深度可分离卷积层
- en: Since separable convolutional layers only have one spatial filter per input
    channel, you should avoid using them after layers that have too few channels,
    such as the input layer (granted, that’s what [Figure 12-20](#separable_convolution_diagram)
    represents, but it is just for illustration purposes). For this reason, the Xception
    architecture starts with 2 regular convolutional layers, but then the rest of
    the architecture uses only separable convolutions (34 in all), plus a few max
    pooling layers and the usual final layers (a global average pooling layer and
    a dense output layer).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 由于可分离卷积层每个输入通道只有一个空间滤波器，因此在具有太多通道的层之后，例如输入层（当然，[图12-20](#separable_convolution_diagram)
    就代表了这一点，但只是为了说明目的），你应该避免使用它们。因此，Xception架构从2个常规卷积层开始，但其余架构只使用可分离卷积（总共34个），再加上一些最大池化层和常规的最终层（一个全局平均池化层和一个密集输出层）。
- en: 'You might wonder why Xception is considered a variant of GoogLeNet, since it
    contains no inception modules at all. Well, as discussed earlier, an inception
    module contains convolutional layers with 1 × 1 filters: these look exclusively
    for cross-channel patterns. However, the convolutional layers that sit on top
    of them are regular convolutional layers that look both for spatial and cross-channel
    patterns. So you can think of an inception module as an intermediate between a
    regular convolutional layer (which considers spatial patterns and cross-channel
    patterns jointly) and a separable convolutional layer (which considers them separately).
    In practice, it seems that separable convolutional layers often perform better.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么Xception被认为是GoogLeNet的一个变体，因为它根本不包含任何Inception模块。好吧，如前所述，Inception模块包含具有1×1滤波器的卷积层：它们专门寻找跨通道模式。然而，位于其上的卷积层是常规卷积层，它们同时寻找空间和跨通道模式。因此，你可以将Inception模块视为常规卷积层（考虑空间模式和跨通道模式共同）和可分离卷积层（分别考虑它们）之间的中间层。在实践中，似乎可分离卷积层通常表现更好。
- en: 'PyTorch does not include a `SeparableConv2d` module, but it’s fairly straightforward
    to implement your own:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch没有包含 `SeparableConv2d` 模块，但实现自己的模块相当直接：
- en: '[PRE13]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Notice the `groups` argument on the seventh line: it lets you split the input
    channels into the given number of independent groups, each with its own filters
    (note that `in_channels` and `out_channels` need to be divisible by `groups`).
    By default `groups=1`, giving you a normal convolutional layer, but if you set
    both `groups=in_channels` and `out_channels=in_channels`, you get a depthwise
    convolutional layer, with one filter per input channel. That’s the first layer
    in the separable convolutional layer. The second is a regular convolutional layer,
    except we set its kernel size and stride to 1\. And that’s it!'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 注意第七行的 `groups` 参数：它允许你将输入通道分割成给定数量的独立组，每个组都有自己的滤波器（注意 `in_channels` 和 `out_channels`
    需要能被 `groups` 整除）。默认情况下 `groups=1`，这会给你一个正常的卷积层，但如果你将 `groups` 和 `out_channels`
    都设置为 `in_channels`，你将得到一个深度卷积层，每个输入通道一个滤波器。这就是可分离卷积层的第一层。第二层是一个常规卷积层，但我们将其内核大小和步长设置为1。就这样！
- en: Tip
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Separable convolutional layers use fewer parameters, less memory, and fewer
    computations than regular convolutional layers, and they often perform better.
    Consider using them by default, except after layers with few channels (such as
    the input channel).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 可分离卷积层比常规卷积层使用更少的参数、更少的内存和更少的计算，并且通常表现更好。除了在具有少量通道的层（如输入通道）之后，你可以默认使用它们。
- en: SENet
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SENet
- en: The winning architecture in the ILSVRC 2017 challenge was the [Squeeze-and-Excitation
    Network (SENet)](https://homl.info/senet).⁠^([19](ch12.html#id2863)) This architecture
    extends existing architectures such as inception networks and ResNets, and boosts
    their performance. This allowed SENet to win the competition with an astonishing
    2.25% top-five error rate! The extended versions of inception networks and ResNets
    are called *SE-Inception* and *SE-ResNet*, respectively. The boost comes from
    the fact that a SENet adds a small neural network, called an *SE block*, to every
    inception module or residual unit in the original architecture, as shown in [Figure 12-21](#senet_diagram).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年ILSVRC挑战赛中获胜的架构是[Squeeze-and-Excitation网络（SENet）](https://homl.info/senet)。⁠^([19](ch12.html#id2863))
    这个架构扩展了现有的架构，如Inception网络和ResNets，并提升了它们的性能。这使得SENet以惊人的2.25%的Top-5错误率赢得了比赛！Inception网络和ResNets的扩展版本分别称为
    *SE-Inception* 和 *SE-ResNet*。提升的原因在于SENet为原始架构中的每个Inception模块或残差单元添加了一个小的神经网络，称为
    *SE块*，如图[图12-21](#senet_diagram)所示。
- en: '![Diagram illustrating the integration of SE blocks into Inception modules
    and Residual units to enhance neural network performance.](assets/hmls_1221.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![说明SE块集成到Inception模块和残差单元以增强神经网络性能的图](assets/hmls_1221.png)'
- en: Figure 12-21\. SE-Inception module (left) and SE-ResNet unit (right)
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-21\. SE-Inception模块（左）和SE-ResNet单元（右）
- en: 'An SE block analyzes the output of the unit it is attached to, focusing exclusively
    on the depth dimension (it does not look for any spatial pattern), and it learns
    which features are usually most active together. It then uses this information
    to recalibrate the feature maps, as shown in [Figure 12-22](#recalibration_diagram).
    For example, an SE block may learn that mouths, noses, and eyes usually appear
    together in pictures: if you see a mouth and a nose, you should expect to see
    eyes as well. So, if the block sees a strong activation in the mouth and nose
    feature maps, but only mild activation in the eye feature map, it will boost the
    eye feature map (more accurately, it will reduce irrelevant feature maps). If
    the eyes were somewhat confused with something else, this feature map recalibration
    will help resolve the ambiguity.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: SE块分析其附加单元的输出，专注于深度维度（它不寻找任何空间模式），并学习哪些特征通常最活跃地一起出现。然后，它使用这些信息来重新校准特征图，如图12-22所示。例如，SE块可能学习到嘴巴、鼻子和眼睛通常在图片中一起出现：如果你看到嘴巴和鼻子，你应该期待看到眼睛。因此，如果块在嘴巴和鼻子特征图中看到强烈的激活，但在眼睛特征图中只有轻微的激活，它将增强眼睛特征图（更准确地说，它将减少无关的特征图）。如果眼睛与某些其他事物有些混淆，这种特征图重新校准将有助于解决歧义。
- en: '![Diagram illustrating how an SE block recalibrates feature maps by analyzing
    and adjusting their activation levels for improved representation.](assets/hmls_1222.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![说明SE块通过分析和调整其激活水平来重新校准特征图以改进表示的图](assets/hmls_1222.png)'
- en: Figure 12-22\. An SE block performs feature map recalibration
  id: totrans-231
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-22\. SE块执行特征图重新校准
- en: 'An SE block is composed of just three layers: a global average pooling layer,
    a hidden dense layer using the ReLU activation function, and a dense output layer
    using the sigmoid activation function (see [Figure 12-23](#seblock_diagram)).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: SE块仅由三层组成：一个全局平均池化层，一个使用ReLU激活函数的隐藏密集层，以及一个使用sigmoid激活函数的密集输出层（见[图12-23](#seblock_diagram)）。
- en: '![Diagram of an SE block architecture showing a global average pool layer followed
    by two dense layers with ReLU and sigmoid activation functions.](assets/hmls_1223.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![显示全局平均池化层后跟两个具有ReLU和sigmoid激活函数的密集层的SE块架构图](assets/hmls_1223.png)'
- en: Figure 12-23\. SE block architecture
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-23\. SE块架构
- en: 'As earlier, the global average pooling layer computes the mean activation for
    each feature map: for example, if its input contains 256 feature maps, it will
    output 256 numbers representing the overall level of response for each filter.
    The next layer is where the “squeeze” happens: this layer has significantly fewer
    than 256 neurons—typically 16 times fewer than the number of feature maps (e.g.,
    16 neurons)—so the 256 numbers get compressed into a small vector (e.g., 16 dimensions).
    This is a low-dimensional vector representation (i.e., an embedding) of the distribution
    of feature responses. This bottleneck step forces the SE block to learn a general
    representation of the feature combinations (we will see this principle in action
    again when we discuss autoencoders in [Chapter 18](ch18.html#autoencoders_chapter)).
    Finally, the output layer takes the embedding and outputs a recalibration vector
    containing one number per feature map (e.g., 256), each between 0 and 1\. The
    feature maps are then multiplied by this recalibration vector, so irrelevant features
    (with a low recalibration score) get scaled down while relevant features (with
    a recalibration score close to 1) are left alone.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，全局平均池化层计算每个特征图的平均激活值：例如，如果其输入包含256个特征图，它将输出256个数字，代表每个滤波器的整体响应水平。接下来的一层是“挤压”发生的地方：这一层比256个神经元少得多——通常是特征图数量的16倍（例如，16个神经元），因此256个数字被压缩成一个小的向量（例如，16个维度）。这是一个低维向量表示（即嵌入）特征响应的分布。这个瓶颈步骤迫使SE块学习特征组合的通用表示（我们将在讨论第18章中的自动编码器时再次看到这一原则）。最后，输出层接收嵌入并输出一个包含每个特征图一个数字的重新校准向量（例如，256个），每个数字介于0和1之间。然后，特征图乘以这个重新校准向量，因此无关的特征（具有低重新校准分数）被缩小，而相关的特征（重新校准分数接近1）则保持不变。
- en: Other Noteworthy Architectures
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 其他值得注意的架构
- en: 'There are many other CNN architectures to explore. Here’s a brief overview
    of some of the most noteworthy:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多其他 CNN 架构可以探索。以下是一些最值得注意的简要概述：
- en: '[VGGNet](https://homl.info/vggnet)⁠^([20](ch12.html#id2869))'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[VGGNet](https://homl.info/vggnet)⁠^([20](ch12.html#id2869))'
- en: VGGNet was the runner-up in the ILSVRC 2014 challenge. Karen Simonyan and Andrew
    Zisserman, from the Visual Geometry Group (VGG) research lab at Oxford University,
    developed a very simple and classical architecture; it had 2 or 3 convolutional
    layers and a pooling layer, then again 2 or 3 convolutional layers and a pooling
    layer, and so on (reaching a total of 16 or 19 convolutional layers, depending
    on the VGG variant), plus a final dense network with 2 hidden layers and the output
    layer. It used small 3 × 3 filters, but it had many of them.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: VGGNet 在 2014 年 ILSVRC 挑战赛中获得了第二名。来自牛津大学视觉几何组（VGG）研究实验室的 Karen Simonyan 和 Andrew
    Zisserman 开发了一种非常简单且经典的架构；它有 2 或 3 个卷积层和一个池化层，然后又是 2 或 3 个卷积层和一个池化层，以此类推（根据 VGG
    变体，总共有 16 或 19 个卷积层），加上一个具有 2 个隐藏层和输出层的最终密集网络。它使用了小的 3 × 3 滤波器，但数量很多。
- en: '[ResNeXt](https://homl.info/resnext)⁠^([21](ch12.html#id2872))'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '[ResNeXt](https://homl.info/resnext)⁠^([21](ch12.html#id2872))'
- en: ResNeXt improves the residual units in ResNet. Whereas the residual units in
    the best ResNet models just contain 3 convolutional layers each, the ResNeXt residual
    units are composed of many parallel stacks (e.g., 32 stacks), with 3 convolutional
    layers each. However, the first two layers in each stack only use a few filters
    (e.g., just four), so the overall number of parameters remains the same as in
    ResNet. Then the outputs of all the stacks are added together, and the result
    is passed to the next residual unit (along with the skip connection).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ResNeXt 对 ResNet 中的残差单元进行了改进。与最佳 ResNet 模型中的残差单元仅包含每个 3 个卷积层不同，ResNeXt 的残差单元由许多并行堆叠（例如，32
    个堆叠）组成，每个堆叠包含 3 个卷积层。然而，每个堆叠的前两层仅使用少量滤波器（例如，仅四个），因此整体参数数量与 ResNet 相同。然后，将所有堆叠的输出相加，并将结果传递给下一个残差单元（以及跳跃连接）。
- en: '[DenseNet](https://homl.info/densenet)⁠^([22](ch12.html#id2874))'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[DenseNet](https://homl.info/densenet)⁠^([22](ch12.html#id2874))'
- en: A DenseNet is composed of several dense blocks, each made up of a few densely
    connected convolutional layers. This architecture achieved excellent accuracy
    while using comparatively few parameters. What does “densely connected” mean?
    The output of each layer is fed as input to every layer after it within the same
    block. For example, layer four in a block takes as input the depthwise concatenation
    of the outputs of layers one, two, and three in that block. Dense blocks are separated
    by a few transition layers.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet 由几个密集块组成，每个块由几个密集连接的卷积层组成。这种架构在参数相对较少的情况下实现了出色的精度。什么是“密集连接”？每个层的输出作为输入馈送到同一块中每个后续层。例如，块中的第四层将块中第一、第二和第三层的输出深度拼接作为输入。密集块之间由几个过渡层分隔。
- en: '[MobileNet](https://homl.info/mobilenet)⁠^([23](ch12.html#id2876))'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[MobileNet](https://homl.info/mobilenet)⁠^([23](ch12.html#id2876))'
- en: MobileNets are streamlined models designed to be lightweight and fast, making
    them popular in mobile and web applications. They are based on depthwise separable
    convolutional layers, like Xception. The authors proposed several variants, trading
    a bit of accuracy for faster and smaller models. Several other CNN architectures
    are available for mobile devices, such as SqueezeNet, ShuffleNet, or MNasNet.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: MobileNets 是一种轻量级且快速的模型，旨在用于移动和 Web 应用程序。它们基于深度可分离卷积层，类似于 Xception。作者提出了几种变体，以牺牲一点精度来换取更快速和更小的模型。还有其他几种
    CNN 架构适用于移动设备，例如 SqueezeNet、ShuffleNet 或 MNasNet。
- en: '[CSPNet](https://homl.info/cspnet)⁠^([24](ch12.html#id2879))'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[CSPNet](https://homl.info/cspnet)⁠^([24](ch12.html#id2879))'
- en: A Cross Stage Partial Network (CSPNet) is similar to a DenseNet, but part of
    each dense block’s input is concatenated directly to that block’s output, without
    going through the block.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 一个跨阶段部分网络（CSPNet）与 DenseNet 类似，但每个密集块的部分输入直接连接到该块的输出，而不经过该块。
- en: '[EfficientNet](https://homl.info/efficientnet)⁠^([25](ch12.html#id2881))'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[EfficientNet](https://homl.info/efficientnet)⁠^([25](ch12.html#id2881))'
- en: EfficientNet is arguably the most important model in this list. The authors
    proposed a method to scale any CNN efficiently by jointly increasing the depth
    (number of layers), width (number of filters per layer), and resolution (size
    of the input image) in a principled way. This is called *compound scaling*. They
    used neural architecture search to find a good architecture for a scaled-down
    version of ImageNet (with smaller and fewer images), and then used compound scaling
    to create larger and larger versions of this architecture. When EfficientNet models
    came out, they vastly outperformed all existing models, across all compute budgets,
    and they remain among the best models out there today. The authors published a
    follow-up paper in 2021, introducing EfficientNetV2, which improved training time
    and parameter efficiency even further.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 无疑，EfficientNet是这个列表中最重要的模型之一。作者提出了一种方法，通过联合增加深度（层数）、宽度（每层的滤波器数量）和分辨率（输入图像的大小）来有效地缩放任何CNN。这被称为*复合缩放*。他们使用神经架构搜索来找到一个缩小版本的ImageNet（具有更小和更少的图像）的良好架构，然后使用复合缩放来创建这个架构的更大版本。当EfficientNet模型出现时，它们在所有计算预算下都大大优于所有现有模型，并且至今仍然是最好的模型之一。作者在2021年发表了一篇后续论文，介绍了EfficientNetV2，它进一步提高了训练时间和参数效率。
- en: '[ConvNeXt](https://homl.info/convnext)⁠^([26](ch12.html#id2884))'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[ConvNeXt](https://homl.info/convnext)⁠^([26](ch12.html#id2884))'
- en: ConvNeXt is quite similar to ResNet, but with a number of tweaks inspired from
    the most successful vision transformer architectures (see [Chapter 16](ch16.html#vit_chapter)),
    such as using large kernels (e.g., 7 × 7 instead of 3 × 3), using fewer activation
    functions and normalization layers in each residual unit, and more.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ConvNeXt与ResNet非常相似，但借鉴了最成功的视觉Transformer架构的一些改进（见[第16章](ch16.html#vit_chapter)），例如使用大核（例如7
    × 7而不是3 × 3），在每个残差单元中使用更少的激活函数和归一化层，等等。
- en: 'Understanding EfficientNet’s compound scaling method is helpful to gain a deeper
    understanding of CNNs, especially if you ever need to scale a CNN architecture.
    It is based on a logarithmic measure of the compute budget, denoted *ϕ*: if your
    compute budget doubles, then *ϕ* increases by 1\. In other words, the number of
    floating-point operations available for training is proportional to 2^(*ϕ*). Your
    CNN architecture’s depth, width, and resolution should scale as *α*^(*ϕ*), *β*^(*ϕ*),
    and *γ*^(*ϕ*), respectively. The factors *α*, *β*, and *γ* must be greater than
    1, and *αβ*²*γ*² should be close to 2\. The optimal values for these factors depend
    on the CNN’s architecture. To find the optimal values for the EfficientNet architecture,
    the authors started with a small baseline model (EfficientNetB0), fixed *ϕ* =
    1, and simply ran a grid search: they found α = 1.2, β = 1.1, and γ = 1.1\. They
    then used these factors to create several larger architectures, named EfficientNetB1
    to EfficientNetB7, for increasing values of *ϕ*.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 理解EfficientNet的复合缩放方法有助于更深入地理解CNN，尤其是如果您需要缩放CNN架构时。它基于计算预算的对数度量，表示为*ϕ*：如果您的计算预算加倍，那么*ϕ*增加1。换句话说，可用于训练的浮点运算次数与2^(*ϕ*)成正比。您的CNN架构的深度、宽度和分辨率应分别按*α*^(*ϕ*)、*β*^(*ϕ*)和*γ*^(*ϕ*)缩放。因子*α*、*β*和*γ*必须大于1，且*αβ*²*γ*²应接近2。这些因子的最佳值取决于CNN的架构。为了找到EfficientNet架构的最佳值，作者从一个小的基线模型（EfficientNetB0）开始，将*ϕ*固定为1，并简单地进行了网格搜索：他们找到了α
    = 1.2，β = 1.1，γ = 1.1。然后，他们使用这些因子创建了几个更大的架构，命名为EfficientNetB1到EfficientNetB7，对应于*ϕ*的增加值。
- en: I hope you enjoyed this deep dive into the main CNN architectures! But how do
    you choose the right one?
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您喜欢这次对主要CNN架构的深入探讨！但您是如何选择正确的架构的呢？
- en: Choosing the Right CNN Architecture
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择合适的CNN架构
- en: 'As you might expect, the best architecture depends on what matters most for
    your project: Accuracy? Model size (e.g., for deployment to a mobile device)?
    Inference speed? Energy consumption? [Table 12-3](#model_summary_table) lists
    some of the pretrained classification models currently available in TorchVision
    (you’ll see how to use them later in this chapter). You can find the full list
    at [*https://pytorch.org/vision/stable/models*](https://pytorch.org/vision/stable/models)
    (including models for other computer vision tasks). The table shows each model’s
    top-1 and top-5 accuracy on the ImageNet dataset, its number of parameters (in
    millions), and how much compute it requires for each image (measured in GFLOPs:
    a Giga-FLOP is one billion floating-point operations). As you can see, larger
    models are generally more accurate, but not always; for example, the small variant
    of EfficientNet v2 outperforms Inception v3 both in size and accuracy (but not
    in compute).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所预期，最佳架构取决于您项目中最重要的是什么：准确性？模型大小（例如，用于部署到移动设备）？推理速度？能耗？[表12-3](#model_summary_table)列出了TorchVision当前可用的某些预训练分类模型（您将在本章后面了解如何使用它们）。完整的列表可在[*https://pytorch.org/vision/stable/models*](https://pytorch.org/vision/stable/models)找到（包括其他计算机视觉任务的模型）。表格显示了每个模型在ImageNet数据集上的top-1和top-5准确性，其参数数量（以百万计），以及每个图像所需的计算量（以GFLOPs衡量：一个Giga-FLOP是一亿个浮点运算）。如您所见，较大的模型通常更准确，但并非总是如此；例如，EfficientNet
    v2的小型变体在大小和准确性上都优于Inception v3（但在计算量上不是）。
- en: Table 12-3\. Some of the pretrained models available in TorchVision, sorted
    by size
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 表12-3. TorchVision中可用的预训练模型，按大小排序
- en: '| Class name | Top-1 acc | Top-5 acc | Params | GFLOPs |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 类名 | Top-1 准确率 | Top-5 准确率 | 参数 | GFLOPs |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| MobileNet v3 small | 67.7% | 87.4% | 2.5M | 0.1 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| MobileNet v3 small | 67.7% | 87.4% | 2.5M | 0.1 |'
- en: '| EfficientNet B0 | 77.7% | 93.5% | 5.3M | 0.4 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNet B0 | 77.7% | 93.5% | 5.3M | 0.4 |'
- en: '| GoogLeNet | 69.8% | 89.5% | 6.6M | 1.5 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| GoogLeNet | 69.8% | 89.5% | 6.6M | 1.5 |'
- en: '| DenseNet 121 | 74.4% | 92.0% | 8.0M | 2.8 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| DenseNet 121 | 74.4% | 92.0% | 8.0M | 2.8 |'
- en: '| EfficientNet v2 small | 84.2% | 96.9% | 21.5M | 8.4 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNet v2 small | 84.2% | 96.9% | 21.5M | 8.4 |'
- en: '| ResNet 34 | 73.3% | 91.4% | 21.8M | 3.7 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| ResNet 34 | 73.3% | 91.4% | 21.8M | 3.7 |'
- en: '| Inception V3 | 77.3% | 93.5% | 27.2M | 5.7 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| Inception V3 | 77.3% | 93.5% | 27.2M | 5.7 |'
- en: '| ConvNeXt Tiny | 82.6% | 96.1% | 28.6M | 4.5 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| ConvNeXt Tiny | 82.6% | 96.1% | 28.6M | 4.5 |'
- en: '| DenseNet 161 | 77.1% | 93.6% | 28.7M | 7.7 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| DenseNet 161 | 77.1% | 93.6% | 28.7M | 7.7 |'
- en: '| ResNet 152 | 82.3% | 96.0% | 60.2M | 11.5 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| ResNet 152 | 82.3% | 96.0% | 60.2M | 11.5 |'
- en: '| AlexNet | 56.5% | 79.1% | 61.1M | 0.7 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| AlexNet | 56.5% | 79.1% | 61.1M | 0.7 |'
- en: '| EfficientNet B7 | 84.1% | 96.9% | 66.3M | 37.8 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNet B7 | 84.1% | 96.9% | 66.3M | 37.8 |'
- en: '| ResNeXt 101 32x8D | 82.8% | 96.2% | 88.8M | 16.4 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| ResNeXt 101 32x8D | 82.8% | 96.2% | 88.8M | 16.4 |'
- en: '| EfficientNet v2 large | 85.8% | 97.8% | 118.5M | 56.1 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| EfficientNet v2 large | 85.8% | 97.8% | 118.5M | 56.1 |'
- en: '| VGG 11 with BN | 70.4% | 89.8% | 132.9M | 7.6 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| VGG 11 with BN | 70.4% | 89.8% | 132.9M | 7.6 |'
- en: '| ConvNeXt Large | 84.4% | 97.0% | 197.8M | 34.4 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| ConvNeXt Large | 84.4% | 97.0% | 197.8M | 34.4 |'
- en: The smaller models will run on any GPU, but what about a large model, such as
    ConvNeXt Large? Since each parameter is represented as a 32-bit float (4 bytes),
    you might think you just need 800 MB of RAM to run a 200M parameter model, but
    you actually need *much* more, typically 5 GB per image at inference time (depending
    on the image size), and even more at training time. Let’s see why.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '较小的模型可以在任何GPU上运行，但大型模型，如ConvNeXt Large，怎么办呢？由于每个参数都表示为一个32位的浮点数（4字节），您可能会认为只需800
    MB的RAM就可以运行一个2亿参数的模型，但实际上您需要更多，通常在推理时每个图像需要5 GB的RAM（取决于图像大小），在训练时则需要更多。让我们看看原因。 '
- en: 'GPU RAM Requirements: Inference Versus Training'
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPU RAM需求：推理与训练
- en: 'CNNs need a *lot* of RAM. For example, consider a single convolutional layer
    with 200 5 × 5 filters, stride 1 and `"same"` padding, processing a 150 × 100
    RGB image (3 channels):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: CNN需要大量的RAM。例如，考虑一个具有200个5 × 5滤波器的单个卷积层，步长为1，使用`"same"`填充，处理一个150 × 100的RGB图像（3个通道）：
- en: 'The number of parameters is (5 × 5 × 3 + 1) × 200 = 15,200 (the + 1 corresponds
    to the bias terms). That’s not much: to produce the same size outputs, a fully
    connected layer would need 200 × 150 × 100 neurons, each connected to all 150
    × 100 × 3 inputs. It would have 200 × 150 × 100 × (150 × 100 × 3 + 1) ≈ 135 billion
    parameters!'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数数量是 (5 × 5 × 3 + 1) × 200 = 15,200（+ 1对应于偏置项）。这并不多：为了产生相同大小的输出，一个全连接层需要200
    × 150 × 100个神经元，每个神经元连接到所有150 × 100 × 3个输入。它将有200 × 150 × 100 × (150 × 100 × 3
    + 1) ≈ 135亿个参数！
- en: 'However, each of the 200 feature maps contains 150 × 100 neurons, and each
    of these neurons needs to compute a weighted sum of its 5 × 5 × 3 = 75 inputs:
    that’s a total of 225 million float multiplications. Not as bad as a fully connected
    layer, but still quite computationally intensive.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，每个200个特征图包含150 × 100个神经元，每个这样的神经元都需要计算其5 × 5 × 3 = 75个输入的加权总和：总共是2.25亿次的浮点数乘法。虽然没有全连接层那么糟糕，但仍然相当计算密集。
- en: Importantly, the convolutional layer’s output will occupy 200 × 150 × 100 ×
    32 = 96 million bits (12 MB) of RAM, assuming we’re using 32-bit floats.⁠^([27](ch12.html#id2889))
    And that’s just for one instance—if a training batch contains 100 instances, then
    this single convolutional layer will use up 1.2 GB of RAM!
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重要的是，卷积层的输出将占用200 × 150 × 100 × 32 = 9600万位（12 MB）的RAM，假设我们使用32位浮点数。⁠^([27](ch12.html#id2889))
    这只是对一个实例而言——如果训练批次包含100个实例，那么这个单一的卷积层将占用1.2 GB的RAM！
- en: During inference (i.e., when making a prediction for a new instance) the RAM
    occupied by one layer can be released as soon as the next layer has been computed,
    so you only need as much RAM as required by two consecutive layers. But during
    training everything computed during the forward pass needs to be preserved for
    the backward pass, so the amount of RAM needed is (at least) the total amount
    of RAM required by all layers. You can easily run out of GPU RAM.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中（即，为新的实例做出预测时），一旦计算了下一层，一层占用的RAM就可以释放，因此你只需要两个连续层所需的RAM。但在训练过程中，正向传播期间计算的所有内容都需要保留以供反向传播使用，因此所需的RAM量（至少）是所有层所需的RAM总量。你很容易耗尽GPU
    RAM。
- en: If training crashes because of an out-of-memory error, you can try reducing
    the batch size. To still get some of the benefits of large batches, you can accumulate
    the gradients after each batch, and only update the model weights every few batches.
    Alternatively, you can try reducing dimensionality using a stride, removing a
    few layers, using 16-bit floats instead of 32-bit floats, distributing the CNN
    across multiple devices, or offloading the most memory-hungry modules to the CPU
    (using `module.to("cpu")`).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练因为内存不足而崩溃，你可以尝试减小批次大小。为了仍然获得大批次的一些好处，你可以在每个批次之后累积梯度，并且只在几个批次后更新模型权重。或者，你可以尝试使用步长来降低维度，移除一些层，使用16位浮点数而不是32位浮点数，将CNN分布到多个设备上，或者将最占用内存的模块卸载到CPU（使用`module.to("cpu")`）。
- en: Yet another option is to trade more compute in exchange for a lower memory usage.
    For example, instead of saving all of the activations during the forward pass,
    you can save some of them, called *activation checkpoints*, then during the backward
    pass, you can recompute the missing activations as needed by running a partial
    forward pass starting from the previous checkpoint.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是以更高的计算量换取更低的内存使用。例如，在正向传播过程中，你不必保存所有的激活，而可以保存其中一部分，称为*激活检查点*，然后在反向传播过程中，你可以通过从上一个检查点开始运行部分正向传播来按需重新计算缺失的激活。
- en: 'To implement activation checkpointing (also called *gradient checkpointing*)
    in PyTorch, you can use the `torch.utils.checkpoint.checkpoint()` function: instead
    of calling a module `z = foo(x)`, you can call it using `z = checkpoint(foo, x)`.
    During inference, it will make no difference, but during training this module’s
    activations will no longer be saved during the forward pass, and `foo(x)` will
    be recomputed during the backward pass when needed. This approach is fairly simple
    to implement, and it doesn’t require any changes to your model architecture.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 要在PyTorch中实现激活检查点（也称为*梯度检查点*），你可以使用`torch.utils.checkpoint.checkpoint()`函数：而不是调用模块`z
    = foo(x)`，你可以使用`z = checkpoint(foo, x)`来调用它。在推理期间，这不会有任何区别，但在训练期间，这个模块的激活将不再在正向传播期间保存，并且当需要时，`foo(x)`将在反向传播期间重新计算。这种方法实现起来相当简单，并且不需要对模型架构进行任何更改。
- en: Warning
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'The forward pass needs to produce the same result if you call it twice with
    the same inputs, or else the gradients will be incorrect. This means that custom
    modules must respect a few constraints, such as avoiding in-place ops or using
    controlled states for random number generation: please see the `checkpoint()`
    function’s documentation for more details.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如果用相同的输入调用两次正向传播并产生相同的结果，否则梯度将是不正确的。这意味着自定义模块必须遵守一些约束，例如避免原地操作或使用受控状态进行随机数生成：请参阅`checkpoint()`函数的文档以获取更多详细信息。
- en: 'That said, if you’re OK with tweaking your model architecture, then there’s
    a much more efficient solution you can use to exchange compute for memory: reversible
    residual networks.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，如果你愿意调整你的模型架构，那么你可以使用一个更高效的解决方案来用内存交换计算：可逆残差网络。
- en: Reversible Residual Networks (RevNets)
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可逆残差网络（RevNets）
- en: '[RevNets](https://homl.info/revnet) were proposed by Aidan Gomez et al. in
    2017:⁠^([28](ch12.html#id2897)) they typically only increase compute by about
    33% and actually don’t require you to save any activations at all during the forward
    pass! Here’s how they work:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '[RevNets](https://homl.info/revnet)是由Aidan Gomez等人于2017年提出的：⁠^([28](ch12.html#id2897))它们通常只会增加大约33%的计算量，实际上在正向传播过程中根本不需要保存任何激活！这是它们的工作原理：'
- en: 'Each layer, called a *reversible layer*, takes two inputs of equal sizes, **x**[1]
    and **x**[2], and computes two outputs: **y**[1] = **x**[1] + f(**x**[2]) and
    **y**[2] = g(**y**[1]) + **x**[2], where f and g can be any functions, as long
    as the output size equals the input size, and as long as they always produce the
    same output for a given input. For example, f and g can be identical modules composed
    of a few convolutional layers with stride 1 and `"same"` padding (each convolutional
    layer comes with its own batch-norm and ReLU activation).'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一层，称为**可逆层**，接受两个大小相等的输入，**x**[1]和**x**[2]，并计算两个输出：**y**[1] = **x**[1] + f(**x**[2])
    和 **y**[2] = g(**y**[1]) + **x**[2]，其中f和g可以是任何函数，只要输出大小等于输入大小，并且对于给定的输入总是产生相同的输出。例如，f和g可以是包含几个步长为1和`"same"`填充（每个卷积层都带有自己的批归一化和ReLU激活）的相同模块。
- en: 'During backpropagation, the inputs of each reversible layer can be recomputed
    from the outputs whenever needed, using: **x**[2] = **y**[2] – g(**y**[1]) and
    **x**[1] = **y**[1] – f(**x**[2]) (you can easily verify that these two equalities
    follow directly from the first two). No need to store any activations during the
    forward pass: brilliant!'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在反向传播过程中，每个可逆层的输入可以在需要时从输出重新计算，使用：**x**[2] = **y**[2] – g(**y**[1]) 和 **x**[1]
    = **y**[1] – f(**x**[2])（你可以轻松验证这两个等式直接来自前两个）。在正向传播过程中不需要存储任何激活：太棒了！
- en: 'Since f and g must output the same shape as the input, reversible layers cannot
    contain convolutional layers with a stride greater than 1, or with `"valid"` padding.
    You can still use such layers in your CNN, but the RevNet trick won’t be applicable
    to them, so you will have to save their activations during the forward pass; luckily,
    a CNN usually requires only a handful of such layers. This includes the very first
    layer, which reduces the spatial dimensions and increases the number of channels:
    the result can be split in two equal parts along the channel dimension and fed
    to the first reversible layer.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 由于f和g必须输出与输入相同的形状，可逆层不能包含步长大于1的卷积层，或者带有`"valid"`填充的卷积层。你仍然可以在你的CNN中使用这样的层，但RevNet技巧将不适用于它们，因此你将不得不在正向传播过程中保存它们的激活；幸运的是，CNN通常只需要少量这样的层。这包括非常第一层，它减少了空间维度并增加了通道数：结果可以沿着通道维度分成两个相等的部分，并馈送到第一个可逆层。
- en: RevNets aren’t limited to CNNs. In fact, they are at the heart of an influential
    Transformer architecture named Reformer (see [Chapter 17](ch17.html#speedup_chapter)).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: RevNets不仅限于CNN。实际上，它们是名为Reformer的具有影响力的Transformer架构的核心（见第17章）。
- en: OK, it’s now time to get our hands dirty! Let’s implement one of the most popular
    CNN architectures from scratch using PyTorch.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在是时候让我们动手实践了！让我们使用PyTorch从头开始实现最流行的CNN架构之一。
- en: Implementing a ResNet-34 CNN Using PyTorch
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PyTorch实现ResNet-34 CNN
- en: 'Most CNN architectures described so far can be implemented pretty naturally
    using PyTorch (although generally you would load a pretrained network instead,
    as you will see). To illustrate the process, let’s implement a ResNet-34 from
    scratch with PyTorch. First, we’ll create a `ResidualUnit` layer:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止描述的大多数CNN架构都可以使用PyTorch相当自然地实现（尽管通常你会加载一个预训练的网络，正如你将看到的）。为了说明这个过程，让我们使用PyTorch从头开始实现一个ResNet-34。首先，我们将创建一个`ResidualUnit`层：
- en: '[PRE14]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As you can see, this code matches [Figure 12-19](#resize_skip_connection_diagram)
    pretty closely. In the constructor, we create all the layers we need: the main
    layers are the ones on the righthand side of the figure, and the skip connection
    corresponds to the layers on the left when the stride is greater than 1, or an
    `nn.Identity` module when the stride is 1—the `nn.Identity` module does nothing
    at all, it just returns its inputs. Then in the `forward()` method, we make the
    inputs go through both the main layers and the skip connection, then we add both
    outputs and apply the activation function.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这段代码与[图12-19](#resize_skip_connection_diagram)非常接近。在构造函数中，我们创建了所有需要的层：图的最右侧是主要层，当步长大于1时，跳过连接对应于左侧的层，或者当步长为1时对应于`nn.Identity`模块——`nn.Identity`模块什么都不做，只是返回其输入。然后在`forward()`方法中，我们让输入通过主要层和跳过连接，然后我们将两个输出相加并应用激活函数。
- en: 'Next, let’s build our `ResNet34` module! Now that we have our `ResidualUnit`
    module, the whole ResNet-34 architecture becomes one big stack of modules, so
    we can base our `ResNet34` class on a single `nn.Sequential` module. The code
    closely matches [Figure 12-18](#resnet_diagram):'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们构建我们的`ResNet34`模块！现在我们有了`ResidualUnit`模块，整个ResNet-34架构就变成了一个模块的大堆栈，因此我们可以将`ResNet34`类基于一个单一的`nn.Sequential`模块。代码与[图12-18](#resnet_diagram)非常吻合：
- en: '[PRE15]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The only tricky part in this code is the loop that adds the `ResidualUnit`
    layers to the list of layers: as explained earlier, the first 3 RUs have 64 filters,
    then the next 4 RUs have 128 filters, and so on. At each iteration, we must set
    the stride to 1 when the number of filters is the same as in the previous RU,
    or else we set it to 2; then we append the `ResidualUnit` to the list, and finally
    we update `prev_filters`.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码中唯一棘手的部分是添加`ResidualUnit`层到层列表中的循环：如前所述，前3个RU有64个滤波器，然后接下来的4个RU有128个滤波器，以此类推。在每次迭代中，当滤波器数量与上一个RU相同时要将步长设置为1，否则设置为2；然后我们将`ResidualUnit`添加到列表中，并最终更新`prev_filters`。
- en: And that’s it, you could now train this model on ImageNet or any other dataset
    of 224 × 224 images. It is amazing that in just 45 lines of code, we can build
    the model that won the ILSVRC 2015 challenge! This demonstrates both the elegance
    of the ResNet model and the expressiveness of PyTorch (and Python). Implementing
    the other CNN architectures we discussed would take more time, but it wouldn’t
    be much harder. However, TorchVision comes with several of these architectures
    built in, so why not use them instead?
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，你现在可以在ImageNet或任何其他224 × 224图像的数据集上训练这个模型了。令人惊讶的是，仅仅45行代码，我们就能构建出赢得ILSVRC
    2015挑战赛的模型！这既展示了ResNet模型的优雅性，也展示了PyTorch（以及Python）的表达能力。实现我们讨论的其他CNN架构需要更多时间，但并不会太难。然而，TorchVision内置了这些架构中的几个，所以为什么不使用它们呢？
- en: Using TorchVision’s Pretrained Models
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TorchVision的预训练模型
- en: In general, you won’t have to implement standard models like GoogLeNet, ResNet,
    or ConvNeXt manually, since pretrained networks are readily available with a couple
    lines of code using TorchVision.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，你不需要手动实现像GoogLeNet、ResNet或ConvNeXt这样的标准模型，因为使用TorchVision只需几行代码就可以轻松地获得预训练的网络。
- en: Tip
  id: totrans-305
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'TIMM is another very popular library built on PyTorch: it provides a collection
    of pretrained image classification models, as well as many related tools such
    as data loaders, data augmentation utilities, optimizers, schedulers, and more.
    Hugging Face’s Hub is also a great place to get all sorts of pretrained models
    (see [Chapter 14](ch14.html#nlp_chapter)).'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: TIMM是基于PyTorch的另一个非常流行的库：它提供了一系列预训练的图像分类模型，以及许多相关工具，如数据加载器、数据增强工具、优化器、调度器等。Hugging
    Face的Hub也是一个获取各种预训练模型的好地方（见[第14章](ch14.html#nlp_chapter)）。
- en: 'For example, you can load a ConvNeXt model pretrained on ImageNet with the
    following code. There are several variants of the ConvNeXt model—tiny, small,
    base, and large—and this code loads the base variant:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以使用以下代码加载在ImageNet上预训练的ConvNeXt模型。ConvNeXt模型有几个变体——微型、小型、基础型和大型——此代码加载的是基础型：
- en: '[PRE16]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: That’s all! This code automatically downloads the weights (338 MB) from the
    *Torch Hub*, an online repository of pretrained models. The weights are saved
    and cached for future use (e.g., in `~/.cache/torch/hub`; run `torch.hub.get_dir()`
    to find the exact path on your system). Some models have newer weights versions
    (e.g., `IMAGENET1K_V2`) or other weight variants. For the full list of available
    models, run `torchvision.models.list_models()`. To find the list of pretrained
    weights available for a given model, such as `convnext_base`, run `list(torchvision.models.get_model_weights("convnext_base"))`.
    Alternatively, visit [*https://pytorch.org/vision/main/models*](https://pytorch.org/vision/main/models).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！此代码自动从*Torch Hub*（一个在线预训练模型存储库）下载权重（338 MB）。权重被保存并缓存以供将来使用（例如，在`~/.cache/torch/hub`中；运行`torch.hub.get_dir()`以找到系统上的确切路径）。一些模型有更新的权重版本（例如，`IMAGENET1K_V2`）或其他权重变体。要获取可用模型的完整列表，请运行`torchvision.models.list_models()`。要获取给定模型（如`convnext_base`）可用的预训练权重的列表，请运行`list(torchvision.models.get_model_weights("convnext_base"))`。或者，访问[*https://pytorch.org/vision/main/models*](https://pytorch.org/vision/main/models)。
- en: 'Let’s use this model to classify the two sample images we loaded earlier. Before
    we can do this, we must first ensure that the images are preprocessed exactly
    as the model expects. In particular, they must have the right size. A ConvNeXt
    model expects 224 × 224 pixel images (other models may expect other sizes, such
    as 299 × 299). Since our sample images are 427 × 640 pixels, we need to resize
    them. We could do this using TorchVision’s `CenterCrop` and/or `Resize` transform,
    but it’s much easier and safer to use the transforms returned by `weights.transforms()`,
    as they are specifically designed for this particular pretrained model:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个模型来分类我们之前加载的两个样本图像。在我们这样做之前，我们必须首先确保图像被预处理成模型所期望的格式。特别是，它们必须具有正确的尺寸。ConvNeXt模型期望224
    × 224像素的图像（其他模型可能期望其他尺寸，例如299 × 299）。由于我们的样本图像是427 × 640像素，我们需要调整它们的大小。我们可以使用TorchVision的`CenterCrop`和/或`Resize`转换来完成此操作，但使用`weights.transforms()`返回的转换更容易、更安全，因为这些转换专门为这个特定的预训练模型设计：
- en: '[PRE17]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Importantly, these transforms also normalize the pixel intensities just like
    during training. In this case, the transforms standardize the pixel intensities
    separately for each color channel, using ImageNet’s means and standard deviations
    for each channel (we will see how to do this manually later in this chapter).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，这些转换也像训练期间一样对像素强度进行了归一化。在这种情况下，转换分别对每个颜色通道的像素强度进行标准化，使用ImageNet的每个通道的均值和标准差（我们将在本章后面看到如何手动进行此操作）。
- en: 'Next we can move the images to the GPU and pass them to the model. As always,
    remember to switch the model to evaluation mode before making predictions—the
    model is in training mode by default—and also turn off autograd:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以将图像移动到GPU，并将它们传递给模型。像往常一样，记得在做出预测之前将模型切换到评估模式——模型默认处于训练模式——并关闭autograd：
- en: '[PRE18]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The result is a 2 × 1,000 tensor containing the class logits for each image
    (recall that ImageNet has 1,000 classes). As we did in [Chapter 10](ch10.html#pytorch_chapter),
    we can use `torch.argmax()` to get the predicted class for each image (i.e., the
    class with the maximum logit):'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个包含每个图像类别logits的2 × 1,000张量（记住，ImageNet有1,000个类别）。正如我们在[第10章](ch10.html#pytorch_chapter)中所做的那样，我们可以使用`torch.argmax()`来获取每个图像的预测类别（即logits最大的类别）：
- en: '[PRE19]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'So far, so good, but what exactly do these classes represent? Well you could
    find the ImageNet class names online, but once again it’s simpler and safer to
    get the class names directly from the `weights` object. Indeed, its `meta` attribute
    is a dictionary containing metadata about the pretrained model, including the
    class names:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，一切顺利，但这些类别究竟代表什么呢？好吧，你可以在网上找到ImageNet的类别名称，但再次强调，直接从`weights`对象获取类别名称更简单、更安全。确实，它的`meta`属性是一个字典，包含有关预训练模型的元数据，包括类别名称：
- en: '[PRE20]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'There you have it: the first image is classified as a palace, and the second
    as a daisy. Since the ImageNet dataset does not have classes for Chinese towers
    or dahlia flowers, a palace and a daisy are reasonable substitutes (the tower
    is part of the Summer Palace in Beijing). Let’s look at the top-three predictions
    using `topk()`:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是结果：第一张图像被分类为宫殿，第二张为雏菊。由于ImageNet数据集没有中国塔或大丽花类别，宫殿和雏菊是合理的替代品（塔是北京颐和园的一部分）。让我们使用`topk()`查看前三个预测：
- en: '[PRE21]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s look at the estimated probabilities for each of these classes:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些类别的估计概率：
- en: '[PRE22]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As you can see, TorchVision makes it easy to download and use pretrained models,
    and it works quite well out of the box for ImageNet classes. But what if you need
    to classify images into classes that don’t belong to the ImageNet dataset, such
    as various flower species? In that case, you may still benefit from the pretrained
    models by using them to perform transfer learning.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，TorchVision使得下载和使用预训练模型变得容易，并且对于ImageNet类别来说，它开箱即用就工作得相当好。但如果你需要将图像分类到不属于ImageNet数据集的类别，比如各种花卉种类呢？在这种情况下，你仍然可以通过使用迁移学习来从预训练模型中获益。
- en: Pretrained Models for Transfer Learning
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 迁移学习的预训练模型
- en: 'If you want to build an image classifier but you do not have enough data to
    train it from scratch, then it is often a good idea to reuse the lower layers
    of a pretrained model, as we discussed in [Chapter 11](ch11.html#deep_chapter).
    In this section we will reuse the ConvNeXt model we loaded earlier—which was pretrained
    on ImageNet—and after replacing its classification head, we will fine-tune it
    on the [*102 Category Flower Dataset*](https://homl.info/flowers102)⁠^([29](ch12.html#id2920))
    (Flowers102 for short). This dataset only contains 10 images per class, and there
    are 102 classes in total (as the name indicates), so if you try to train a model
    from scratch, you will really struggle to get high accuracy. However, it’s quite
    easy to get over 90% accuracy using a good pretrained model. Let’s see how. First,
    let’s download the dataset using Torchvision:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要构建一个图像分类器，但你没有足够的数据从头开始训练它，那么重用预训练模型的底层通常是一个好主意，正如我们在[第11章](ch11.html#deep_chapter)中讨论的那样。在本节中，我们将重用之前加载的ConvNeXt模型——它在ImageNet上进行了预训练——并在替换其分类头部后，我们将对其进行微调，以[*102类别花卉数据集*](https://homl.info/flowers102)⁠^([29](ch12.html#id2920))（简称Flowers102）。此数据集每个类别只有10个图像，总共有102个类别（正如其名称所示），所以如果你尝试从头开始训练模型，你将真的很难获得高精度。然而，使用一个好的预训练模型，很容易超过90%的精度。让我们看看如何做到这一点。首先，让我们使用Torchvision下载数据集：
- en: '[PRE23]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This code uses `partial()` to avoid repeating the same arguments three times.
    We also set `transform=weights.transforms()` to preprocess the images immediately
    when they are loaded. The Flowers102 dataset comes with three predefined splits,
    for training, validation, and testing. The first two have 10 images per class,
    but surprisingly the test set has many more (it has a variable number of images
    per class, between 20 and 238). In a real project, you would normally use most
    of your data for training rather than for testing, but this dataset was designed
    for computer vision research, and the authors purposely restricted the training
    set and the validation set.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码使用`partial()`来避免三次重复相同的参数。我们还设置了`transform=weights.transforms()`，以便在加载图像时立即对图像进行预处理。Flowers102数据集包含三个预定义的分割，用于训练、验证和测试。前两个类别每个有10个图像，但令人惊讶的是测试集有更多（每个类别的图像数量在20到238之间不等）。在实际项目中，你通常会使用大部分数据用于训练而不是测试，但这个数据集是为计算机视觉研究设计的，作者故意限制了训练集和验证集。
- en: 'We then create the data loaders, as usual:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们像往常一样创建数据加载器：
- en: '[PRE24]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Many TorchVision datasets conveniently contain the class names in the `classes`
    attribute, but sadly not this dataset.^([30](ch12.html#id2922)) If you prefer
    to see lovely names like “tiger lily”, “monkshood”, or “snapdragon” rather than
    boring class IDs, then you need to manually define the list of class names:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 许多TorchVision数据集方便地在`classes`属性中包含类别名称，但遗憾的是这个数据集没有这样做.^([30](ch12.html#id2922))
    如果你更喜欢看到像“老虎莲”、“乌头”或“龙葵”这样的可爱名称，而不是无聊的类别ID，那么你需要手动定义类别名称列表：
- en: '[PRE25]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now let’s adapt our pretrained ConvNeXt-base model to this dataset. Since it
    was pretrained on ImageNet, which has 1,000 classes, the model’s head (i.e., its
    upper layers) was designed to output 1,000 logits. But we only have 102 classes,
    so we must chop the model’s head off and replace it with a smaller one. But how
    can we find it? Well let’s use the model’s `named_children()` method to find the
    name of its submodules:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将预训练的ConvNeXt-base模型适应到这个数据集上。由于它在ImageNet上进行了预训练，而ImageNet有1,000个类别，因此模型的头部（即其上层）被设计为输出1,000个logits。但我们只有102个类别，所以我们必须截断模型的头部并替换为更小的一个。但我们如何找到它呢？好吧，让我们使用模型的`named_children()`方法来找到其子模块的名称：
- en: '[PRE26]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The `features` module is the main part of the model, which includes all layers
    except for the global average pooling layer (`avgpool`) and the model’s head (`classifier`).
    Let’s look more closely at the head:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '`features`模块是模型的主要部分，它包括除了全局平均池化层（`avgpool`）和模型的头部（`classifier`）之外的所有层。让我们更仔细地看看头部：'
- en: '[PRE27]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As you can see, it’s an `nn.Sequential` module composed of a layer normalization
    layer, an `nn.Flatten` layer, and an `nn.Linear` layer with 1,024 inputs and 1,000
    outputs. This `nn.Linear` layer is the output layer, and it’s the one we need
    to replace. We must only change the number of outputs:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这是一个由一个层归一化层、一个`nn.Flatten`层和一个具有1,024个输入和1,000个输出的`nn.Linear`层组成的`nn.Sequential`模块。这个`nn.Linear`层是输出层，也是我们需要替换的那一层。我们必须只更改输出的数量：
- en: '[PRE28]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As explained in [Chapter 11](ch11.html#deep_chapter), it’s usually a good idea
    to freeze the weights of the pretrained layers, at least at the beginning of training.
    We can do this by freezing every single parameter in the model, and then unfreezing
    only the parameters of the head:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第11章](ch11.html#deep_chapter)所述，通常在训练初期冻结预训练层的权重是一个好主意。我们可以通过冻结模型中的每一个参数，然后只解冻头部的参数来实现这一点：
- en: '[PRE29]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Next, you can train this model for a few epochs, and you will already reach
    about 90% accuracy just by training the new head, without even fine-tuning the
    pretrained layers. After that, you can unfreeze the whole model, lower the learning
    rate—typically by a factor of 10—and continue training the model. Give this a
    try, and see what accuracy you can reach!
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您可以训练这个模型几个周期，仅通过训练新的头部，而不对预训练层进行微调，您就已经可以达到大约90%的准确率。之后，您可以解冻整个模型，降低学习率——通常是降低10倍——并继续训练模型。尝试一下，看看您能达到多高的准确率！
- en: 'To reach an even higher accuracy, it’s usually a good idea to perform some
    data augmentation on the training images. For this, you can try randomly flipping
    the training images horizontally, randomly rotating them by a small angle, randomly
    resizing and cropping them, and randomly tweaking their colors. This must all
    be done before running the ImageNet normalization step, which you can implement
    using a `Normalize` transform:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到更高的准确率，通常在训练图像上进行一些数据增强是一个好主意。为此，您可以尝试随机水平翻转训练图像，随机以小角度旋转它们，随机调整大小和裁剪它们，以及随机调整它们的颜色。所有这些都必须在运行ImageNet归一化步骤之前完成，您可以使用`Normalize`转换来实现这一点：
- en: '[PRE30]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Tip
  id: totrans-343
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'TorchVision comes with an `AutoAugment` transform which applies multiple augmentation
    operations optimized for ImageNet. It generalizes well to many other image datasets,
    and it also offers predefined settings for two other datasets: CIFAR10 and the
    street view house numbers (SVHN) dataset.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: TorchVision附带一个`AutoAugment`转换，它应用针对ImageNet优化的多个增强操作。它很好地推广到许多其他图像数据集，并且它还为其他两个数据集提供了预定义的设置：CIFAR10和街景房屋数字（SVHN）数据集。
- en: 'Here are some more ideas to continue to improve your model’s accuracy:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些更多想法，可以帮助进一步提高您模型的准确率：
- en: Try other pretrained models.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试其他预训练模型。
- en: 'Extend the training set: find more flower images and label them.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展训练集：找到更多花卉图像并对其进行标记。
- en: Create an ensemble of models, and combine their predictions.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个模型集合，并将它们的预测结果结合起来。
- en: Analyze failure cases, and see whether they share specific characteristics,
    such as similar texture or color. You can then try to tweak image preprocessing
    to address these issues.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析失败案例，看看它们是否具有特定的特征，例如相似的纹理或颜色。然后，您可以尝试调整图像预处理来解决这些问题。
- en: Use a learning schedule such as performance scheduling.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用性能调度等学习计划。
- en: 'Unfreeze the layers gradually, starting from the top. Alternatively, you can
    use *differential learning rates*: apply a smaller learning rate to lower layers,
    and a larger learning rate to upper layers. You can do this by using parameter
    groups (see [Chapter 11](ch11.html#deep_chapter)).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逐步解冻层，从顶部开始。或者，您可以使用*差异学习率*：对较低层应用较小的学习率，对上层应用较大的学习率。您可以通过使用参数组来实现这一点（参见[第11章](ch11.html#deep_chapter)）。
- en: Explore different optimizers and fine-tune their hyperparameters.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索不同的优化器并微调它们的超参数。
- en: Try different regularization techniques.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的正则化技术。
- en: Tip
  id: totrans-354
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: It’s worth spending time looking for models that were pretrained on similar
    images. For example, if you’re dealing with satellite images, aerial images, or
    even raster data such as digital elevation models (DEM), then models pretrained
    on ImageNet won’t help much. Instead, check out Microsoft’s *TorchGeo* library,
    which is similar to TorchVision but for geospatial data. For medical images, check
    out Project MONAI. For agricultural images, check out AgML. And so on.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 值得花时间去寻找在类似图像上预训练的模型。例如，如果您处理的是卫星图像、航空图像，甚至是像数字高程模型（DEM）这样的栅格数据，那么在ImageNet上预训练的模型帮助不大。相反，可以查看微软的*TorchGeo*库，它与TorchVision类似，但用于地理空间数据。对于医学图像，可以查看Project
    MONAI。对于农业图像，可以查看AgML。等等。
- en: With that, you can start training amazing image classifiers on your own images
    and classes! But there’s more to computer vision than just classification. For
    example, what if you also want to know *where* the flower is in a picture? Let’s
    look at this now.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，你就可以开始在自己的图像和类别上训练令人惊叹的图像分类器了！但计算机视觉不仅仅是分类。例如，如果你还想知道图片中花朵的位置呢？让我们现在来看看这个问题。
- en: Classification and Localization
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类和定位
- en: 'Localizing an object in a picture can be expressed as a regression task, as
    discussed in [Chapter 9](ch09.html#ann_chapter): to predict a bounding box around
    the object, a common approach is to predict the location of the bounding box’s
    center, as well as its width and height (alternatively, you could predict the
    horizontal and vertical coordinates of the object’s upper-left and lower-right
    corners). This means we have four numbers to predict. It does not require much
    change to the ConvNeXt model; we just need to add a second dense output layer
    with four units (e.g., on top of the global average pooling layer). Here’s a `FlowerLocator`
    model that adds a localization head to a given base model, such as our ConvNeXt
    model:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在图片中定位一个对象可以表达为一个回归任务，如[第9章](ch09.html#ann_chapter)中所述：预测围绕对象的边界框，一个常见的方法是预测边界框中心的坐标，以及其宽度和高度（或者，你也可以预测对象的左上角和右下角的水平和垂直坐标）。这意味着我们需要预测四个数字。这并不需要对ConvNeXt模型进行太多修改；我们只需要添加一个具有四个单元的第二个密集输出层（例如，在全局平均池化层之上）。这是一个`FlowerLocator`模型，它将定位头部添加到给定的基础模型中，例如我们的ConvNeXt模型：
- en: '[PRE31]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This locator model has two heads: the first outputs class logits, while the
    second outputs the bounding box. The localization head has the same number of
    inputs as the `nn.Linear` layer of the classification head, but it outputs just
    four numbers. The `forward()` method takes a batch of preprocessed images as input
    and outputs both the predicted class logits (102 per image) and the predicted
    bounding boxes (1 per image). After training this model, you can use it as follows:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定位模型有两个头部：第一个输出类别对数，而第二个输出边界框。定位头部与分类头部中的`nn.Linear`层的输入数量相同，但它只输出四个数字。`forward()`方法接收一个预处理图像的批次作为输入，并输出预测的类别对数（每张图像102个）和预测的边界框（每张图像1个）。训练好这个模型后，你可以这样使用它：
- en: '[PRE32]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'But how can we train this model? Well, we saw how to train a model with two
    or more outputs in [Chapter 10](ch10.html#pytorch_chapter), and this one is no
    different: in this case, we can use the `nn.CrossEntropyLoss` for the classification
    head, and the `nn.MSELoss` for the localization head. The final loss can just
    be a weighted sum of the two. Voilà, that’s all there is to it.'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何训练这个模型呢？嗯，我们在[第10章](ch10.html#pytorch_chapter)中看到了如何训练具有两个或更多输出的模型，而这个模型也没有什么不同：在这种情况下，我们可以为分类头部使用`nn.CrossEntropyLoss`，为定位头部使用`nn.MSELoss`。最终的损失可以简单地是这两个损失的加权总和。哇，就是这样。
- en: 'Hey, not so fast! We have a problem: the Flowers102 dataset does not include
    any bounding boxes, so we need to add them ourselves. This is often one of the
    hardest and most costly parts of a machine learning project: labeling and annotating
    the data. It’s a good idea to spend time looking for the right tools. To annotate
    images with bounding boxes, you may want to use an open source labeling tool like
    Label Studio, OpenLabeler, ImgLab, Labelme, VoTT, or VGG Image Annotator, or perhaps
    a commercial tool like LabelBox, Supervisely, Roboflow, or RectLabel. Many of
    these are now AI assisted, greatly speeding up the annotation task. You may also
    want to consider crowdsourcing platforms such as Amazon Mechanical Turk if you
    have a very large number of images to annotate. However, it is quite a lot of
    work to set up a crowdsourcing platform, prepare the form to be sent to the workers,
    supervise them, and ensure that the quality of the bounding boxes they produce
    is good, so make sure it is worth the effort. If there are just a few hundred
    or a even a couple of thousand images to label, and you don’t plan to do this
    frequently, it may be preferable to do it yourself: with the right tools, it will
    only take a few days, and you’ll also gain a better understanding of your dataset
    and task.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 嘿，别急！我们遇到了一个问题：Flowers102数据集不包含任何边界框，因此我们需要自己添加它们。这通常是机器学习项目中难度最大且成本最高的部分之一：标记和注释数据。花时间寻找合适的工具是个好主意。要使用边界框注释图像，您可能希望使用开源标记工具，如Label
    Studio、OpenLabeler、ImgLab、Labelme、VoTT或VGG Image Annotator，或者可能是一个商业工具，如LabelBox、Supervisely、Roboflow或RectLabel。其中许多现在都提供AI辅助，大大加快了注释任务。如果您有大量图像需要注释，您可能还想考虑众包平台，如Amazon
    Mechanical Turk。然而，设置众包平台、准备发送给工作人员的表格、监督他们以及确保他们产生的边界框质量良好是一项相当多的工作，所以请确保这是值得的努力。如果只有几百张甚至几千张图像需要标记，并且您不打算经常这样做，自己来做可能更合适：有了合适的工具，这只需要几天时间，您也会更好地了解您的数据集和任务。
- en: 'You can then create a custom dataset (see [Chapter 10](ch10.html#pytorch_chapter))
    where each entry contains an image, a label, and a bounding box. TorchVision conveniently
    includes a `BoundingBoxes` class that represents a list of bounding boxes. For
    example, the following code creates a bounding box for the largest flower in the
    first image of the Flowers102 training set (for now we only consider one bounding
    box per image, but we’ll discuss multiple bounding boxes per image later in this
    chapter):'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以创建一个自定义数据集（见第10章），其中每个条目包含一个图像、一个标签和一个边界框。TorchVision方便地包含一个`BoundingBoxes`类，它表示一组边界框。例如，以下代码为Flowers102训练集第一张图像中最大的花朵创建了一个边界框（目前我们只考虑每张图像一个边界框，但我们在本章后面将讨论每张图像多个边界框的情况）：
- en: '[PRE33]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Tip
  id: totrans-366
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: To visualize bounding boxes, use the `torchvi⁠sion.utils.draw_​bounding_boxes()`
    function. You will first need to convert the bounding boxes to the XYXY format
    using `torchvi⁠sion.​ops.box_convert()`.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 要可视化边界框，请使用`torchvision.utils.draw_bounding_boxes()`函数。您首先需要使用`torchvision.ops.box_convert()`将边界框转换为XYXY格式。
- en: 'The `BoundingBoxes` class is a subclass of `TVTensor`, which is a subclass
    of `torch.​Ten⁠sor`, so you can treat bounding boxes exactly like regular tensors,
    with extra features. Most importantly, you can transform bounding boxes using
    TorchVision’s transforms API v2\. For example, let’s use the transform we defined
    earlier to preprocess this bounding box:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '`BoundingBoxes`类是`TVTensor`的子类，而`TVTensor`是`torch.Tensor`的子类，因此您可以像处理常规张量一样处理边界框，并具有额外功能。最重要的是，您可以使用TorchVision的v2
    transforms API转换边界框。例如，让我们使用我们之前定义的转换来预处理这个边界框：'
- en: '[PRE34]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Warning
  id: totrans-370
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'Resizing and cropping a bounding box works as expected, but rotation is special:
    the bounding box can’t be rotated since it doesn’t have any rotation parameter,
    so instead it is resized to fit the rotated box (*not* the rotated object). As
    a result, it may end up being a bit too large for the object.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 调整和裁剪边界框的工作效果如预期，但旋转是特殊的：由于边界框没有旋转参数，因此不能旋转，所以它会被调整大小以适应旋转后的框（*而不是*旋转的对象）。因此，它可能最终会变得比对象大一些。
- en: 'You can pass a nested data structure to a transform and the output will have
    the same structure, except with all the images and bounding boxes transformed.
    For example, the following code transforms the first flower image in the training
    set and its bounding box, leaving the label unchanged. In this example, the input
    and output are both 2-tuples containing an image and a dictionary composed of
    a label and a bounding box, but you could use any other data structure:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将嵌套数据结构传递给转换，输出将具有相同的结构，但所有图像和边界框都会被转换。例如，以下代码转换训练集中的第一朵花图像及其边界框，但保持标签不变。在这个例子中，输入和输出都是包含图像和由标签和边界框组成的字典的2元组，但你也可以使用任何其他数据结构。
- en: '[PRE35]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Tip
  id: totrans-374
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: When using the MSE, a 10-pixel error for a large bounding box will be penalized
    just as much as a 10-pixel error for a small bounding box. To avoid this, you
    can use a custom loss function that computes the square root of the width and
    height—for both the target and the prediction—before computing the MSE.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用均方误差（MSE）时，对于大边界框的10像素误差和对于小边界框的10像素误差会受到相同的惩罚。为了避免这种情况，你可以使用一个自定义的损失函数，在计算MSE之前，先计算目标框和预测框的宽度和高度的平方根。
- en: 'The MSE is simple and often works fairly well to train the model, but it is
    not a great metric to evaluate how well the model can predict bounding boxes.
    The most common metric for this is the *intersection over union* (IoU, also known
    as the *Jaccard index*): it is the area of overlap between the target bounding
    box T and the predicted bounding box P, divided by the area of their union P ∪
    T (see [Figure 12-24](#iou_diagram)). In short, IoU = |P ∩ T| / |P ∪ T|, where
    |*x*| is the area of *x*. The IoU ranges from 0 (no overlap) to 1 (perfect overlap).
    It is implemented by the `torchvision.ops.box_iou()` function.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: MSE简单且通常在训练模型时效果相当好，但它并不是一个很好的度量标准来评估模型预测边界框的能力。这个最常见的度量标准是*交并比*（IoU，也称为*Jaccard指数*）：它是目标边界框T和预测边界框P之间的重叠面积除以它们的并集面积P
    ∪ T（见[图12-24](#iou_diagram)）。简而言之，IoU = |P ∩ T| / |P ∪ T|，其中|*x*|是*x*的面积。IoU的范围从0（没有重叠）到1（完全重叠）。它通过`torchvision.ops.box_iou()`函数实现。
- en: 'The IoU is not great for training because it is equal to zero whenever P and
    T have no overlap, regardless of the distance between them or their shapes: in
    this case the gradient is also equal to zero and therefore gradient descent cannot
    make any progress. Luckily, it’s possible to fix this flaw by incorporating extra
    information. For example, the *Generalized IoU* (GIoU), introduced in a [2019
    paper](https://homl.info/giou) by H. Rezatofighi et al.,⁠^([31](ch12.html#id2948))
    considers the smallest box S that contains both P and T, and it subtracts from
    the IoU the ratio of S that is not covered by P or T. In short, GIoU = IoU – |S
    – (P ∪ T)| / |S|. This means that the GIoU gets smaller as P and T get further
    apart, which gives gradient descent something to play with so it can pull P closer
    to T. Since we want to maximize the GIoU, the GIoU loss is equal to 1 – GIoU.
    This loss quickly became popular, and it is implemented by the `torchvision.ops.generalized_box_iou_loss()`
    function.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: IoU（交并比）对于训练来说并不理想，因为当P和T没有重叠时，无论它们之间的距离或形状如何，IoU都等于零：在这种情况下，梯度也等于零，因此梯度下降无法取得任何进展。幸运的是，通过结合额外信息可以修复这个缺陷。例如，H.
    Rezatofighi等人在2019年的一篇论文[2019 paper](https://homl.info/giou)中引入的*广义IoU* (GIoU)，考虑了包含P和T的最小框S，并从IoU中减去P或T未覆盖的S的比例。简而言之，GIoU
    = IoU – |S – (P ∪ T)| / |S|。这意味着随着P和T之间的距离增加，GIoU会减小，这为梯度下降提供了可以操作的空间，使其可以将P拉得更接近T。由于我们希望最大化GIoU，因此GIoU损失等于1
    – GIoU。这种损失很快变得流行，并且可以通过`torchvision.ops.generalized_box_iou_loss()`函数实现。
- en: '![Diagram illustrating the intersection over union (IoU) metric for bounding
    boxes, showing overlapping and union areas around a flower.](assets/hmls_1224.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![展示边界框交并比（IoU）度量图的示意图，显示围绕一朵花的重叠和并集区域。](assets/hmls_1224.png)'
- en: Figure 12-24\. IoU metric for bounding boxes
  id: totrans-379
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-24\. 边界框的IoU度量
- en: 'Another important variant of the IoU is the *Complete IoU* (CIoU), introduced
    in a [2020 paper](https://homl.info/ciou) by Z. Zheng et al.⁠^([32](ch12.html#id2952))
    It considers three geometric factors: the IoU (the more overlap, the better),
    the distance between the centers of P and T (the closer, the better), normalized
    by the length of the diagonal of S, and the similarity between the aspect ratios
    of P and T (the closer, the better). The loss is 1 – CIoU, and it is implemented
    by the `torchvision.ops.complete_box_iou_loss()` function. It generally performs
    better than the MSE or the GIoU, converging faster and leading to more accurate
    bounding boxes, so it is becoming the default loss for localization.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: IoU的一个重要变体是*完整IoU* (CIoU)，由Z. Zheng等人在2020年的一篇论文[2020 paper](https://homl.info/ciou)中引入。它考虑了三个几何因素：IoU（重叠越多越好）、P和T中心之间的距离（越近越好），通过S对角线的长度进行归一化，以及P和T的宽高比之间的相似性（越近越好）。损失是1
    – CIoU，它通过`torchvision.ops.complete_box_iou_loss()`函数实现。它通常比MSE或GIoU表现更好，收敛更快，并能得到更准确的边界框，因此它正在成为定位的默认损失函数。
- en: Classifying and localizing a single object is nice, but what if the images contain
    multiple objects (as is often the case in the flowers dataset)?
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 分类和定位单个对象很好，但如果图像中包含多个对象（如花卉数据集中常见的情况）怎么办呢？
- en: Object Detection
  id: totrans-382
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测
- en: 'The task of classifying and localizing multiple objects in an image is called
    *object detection*. Until a few years ago, a common approach was to take a CNN
    that was trained to classify and locate a single object roughly centered in the
    image, then slide this CNN across the image and make predictions at each step.
    The CNN was generally trained to predict not only class probabilities and a bounding
    box, but also an *objectness score*: this is the estimated probability that the
    image does indeed contain an object centered near the middle. This is a binary
    classification output; it can be produced by a dense output layer with a single
    unit, using the sigmoid activation function and trained using the binary cross-entropy
    loss.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像中分类和定位多个对象的任务被称为*目标检测*。直到几年前，一种常见的方法是使用一个CNN，该CNN被训练来分类和定位图像中大致居中的单个对象，然后将这个CNN在图像上滑动并在每一步进行预测。CNN通常被训练来预测不仅包括类概率和边界框，还包括一个*对象性分数*：这是图像确实包含一个位于中间附近的对象的估计概率。这是一个二元分类输出；可以通过具有单个单元的密集输出层，使用sigmoid激活函数，并通过二元交叉熵损失进行训练来产生。
- en: Note
  id: totrans-384
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Instead of an objectness score, a “no-object” class was sometimes added, but
    in general this did not work as well. The questions “Is an object present?” and
    “What type of object is it?” are best answered separately.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对象性分数外，有时还会添加一个“无对象”类别，但通常效果并不好。对于“是否存在对象？”和“它是什么类型的对象？”这些问题最好分别回答。
- en: This sliding-CNN approach is illustrated in [Figure 12-25](#sliding_cnn_diagram).
    In this example, the image was chopped into a 5 × 7 grid, and we see a CNN—the
    thick black rectangle—sliding across all 3 × 3 regions and making predictions
    at each step.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 这种滑动-CNN方法在[图12-25](#sliding_cnn_diagram)中进行了说明。在这个例子中，图像被切割成5 × 7的网格，我们看到一个CNN——一个粗黑矩形——在所有3
    × 3的区域滑动并在每一步进行预测。
- en: '![Diagram illustrating a sliding CNN approach on a grid over an image of pink
    roses, with colored rectangles indicating regions where predictions are made.](assets/hmls_1225.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![说明在粉色玫瑰图像上使用滑动CNN方法在网格上的图。用彩色矩形表示预测的区域。](assets/hmls_1225.png)'
- en: Figure 12-25\. Detecting multiple objects by sliding a CNN across the image
  id: totrans-388
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-25\. 通过在图像上滑动CNN检测多个对象
- en: 'In this figure, the CNN has already made predictions for three of these 3 ×
    3 regions:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，CNN已经对这三个3 × 3区域进行了预测：
- en: 'When looking at the top-left 3 × 3 region (centered on the red-shaded grid
    cell located in the second row and second column), it detected the leftmost rose.
    Notice that the predicted bounding box exceeds the boundary of this 3 × 3 region.
    That’s absolutely fine: even though the CNN could not see the bottom part of the
    rose, it was able to make a reasonable guess as to where it might be. It also
    predicted class probabilities, giving a high probability to the “rose” class.
    Lastly, it predicted a fairly high objectness score, since the center of the bounding
    box lies within the central grid cell (in this figure, the objectness score is
    represented by the thickness of the bounding box).'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当查看左上角的 3 × 3 区域（位于第二行和第二列的红色阴影网格单元格中心），它检测到了最左侧的玫瑰。注意，预测的边界框超出了这个 3 × 3 区域的边界。这是完全可以接受的：尽管
    CNN 没有看到玫瑰的底部，但它能够合理地猜测它可能的位置。它还预测了类别概率，给“玫瑰”类别一个很高的概率。最后，它预测了一个相当高的物体得分，因为边界框的中心位于中央网格单元格内（在这个图中，物体得分由边界框的厚度表示）。
- en: When looking at the next 3 × 3 region, one grid cell to the right (centered
    on the shaded blue square), it did not detect any flower centered in that region,
    so it predicted a very low objectness score; therefore, the predicted bounding
    box and class probabilities can safely be ignored. You can see that the predicted
    bounding box was no good anyway.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当查看下一个 3 × 3 区域时，向右移动一个网格单元格（中心位于阴影蓝色方块），它没有检测到该区域内居中的任何花朵，因此预测了一个非常低的物体得分；因此，预测的边界框和类别概率可以安全地忽略。你可以看到预测的边界框本身就不太合适。
- en: Finally, when looking at the next 3 × 3 region, again one grid cell to the right
    (centered on the shaded green cell), it detected the rose at the top, although
    not perfectly. This rose is not well centered within this region, so the predicted
    objectness score was not very high.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，当查看下一个 3 × 3 区域时，再次向右移动一个网格单元格（中心位于阴影绿色单元格），它检测到了顶部的玫瑰，尽管并不完美。这个玫瑰在这个区域内并没有很好地居中，所以预测的物体得分并不高。
- en: You can imagine how sliding the CNN across the whole image would give you a
    total of 15 predicted bounding boxes, organized in a 3 × 5 grid, with each bounding
    box accompanied by its estimated class probabilities and objectness score. Since
    objects can have varying sizes, you may then want to slide the CNN again across
    2 × 2 and 4 × 4 regions as well, to capture smaller and larger objects.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象将 CNN 在整个图像上滑动会产生总共 15 个预测的边界框，这些边界框以 3 × 5 的网格组织，每个边界框都伴随着其估计的类别概率和物体得分。由于物体可以有不同的尺寸，你可能还希望将
    CNN 再次滑动到 2 × 2 和 4 × 4 的区域，以捕捉更小和更大的物体。
- en: 'This technique is fairly straightforward, but as you can see it will often
    detect the same object multiple times, at slightly different positions. Some post-processing
    is needed to get rid of all the unnecessary bounding boxes. A common approach
    for this is called *non-max suppression* (NMS). Here’s how it works:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术相当直接，但正如你所看到的，它通常会多次检测到同一个物体，位置略有不同。需要一些后处理来去除所有不必要的边界框。对此的一个常见方法称为 *非极大值抑制*（NMS）。下面是如何工作的：
- en: First, get rid of all the bounding boxes for which the objectness score is below
    some threshold; since the CNN believes there’s no object at that location, the
    bounding box is useless.
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，去除所有物体得分低于某个阈值的边界框；由于 CNN 认为在那个位置没有物体，边界框就变得没有用了。
- en: Find the remaining bounding box with the highest objectness score, and get rid
    of all the other remaining bounding boxes that overlap a lot with it (e.g., with
    an IoU greater than 60%). For example, in [Figure 12-25](#sliding_cnn_diagram),
    the bounding box with the max objectness score is the thick bounding box over
    the leftmost rose. The other bounding box that touches this same rose overlaps
    a lot with the max bounding box, so we will get rid of it (although in this example
    it would already have been removed in the previous step).
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到剩余的物体得分最高的边界框，并去除所有与之重叠很大的其他剩余边界框（例如，IoU 大于 60% 的）。例如，在 [图 12-25](#sliding_cnn_diagram)
    中，物体得分最高的边界框是覆盖在最左侧玫瑰上的粗边界框。另一个接触同一玫瑰的边界框与最大边界框重叠很大，所以我们将去除它（尽管在这个例子中它已经在之前的步骤中被移除了）。
- en: Repeat step 2 until there are no more bounding boxes to get rid of.
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2，直到没有更多边界框可以去除。
- en: 'This simple approach to object detection works pretty well, but it requires
    running the CNN many times (15 times in this example), so it is quite slow. Fortunately,
    there is a much faster way to slide a CNN across an image: using a *fully convolutional
    network* (FCN).'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单的目标检测方法效果相当不错，但需要多次运行CNN（本例中为15次），因此速度相当慢。幸运的是，有一种更快的方法可以在图像上滑动CNN：使用*完全卷积网络*（FCN）。
- en: Fully Convolutional Networks
  id: totrans-399
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完全卷积网络
- en: 'The idea of FCNs was first introduced in a [2015 paper](https://homl.info/fcn)⁠^([33](ch12.html#id2962))
    by Jonathan Long et al., for semantic segmentation (the task of classifying every
    pixel in an image according to the class of the object it belongs to). The authors
    pointed out that you could replace the dense layers at the top of a CNN with convolutional
    layers. To understand this, let’s look at an example: suppose a dense layer with
    200 neurons sits on top of a convolutional layer that outputs 100 feature maps,
    each of size 7 × 7 (this is the feature map size, not the kernel size). Each neuron
    will compute a weighted sum of all 100 × 7 × 7 activations from the convolutional
    layer (plus a bias term). Now let’s see what happens if we replace the dense layer
    with a convolutional layer using 200 filters, each of size 7 × 7, and with `"valid"`
    padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel
    is exactly the size of the input feature maps and we are using `"valid"` padding).
    In other words, it will output 200 numbers, just like the dense layer did; and
    if you look closely at the computations performed by a convolutional layer, you
    will notice that these numbers will be precisely the same as those the dense layer
    produced. The only difference is that the dense layer’s output was a tensor of
    shape [*batch size*, 200], while the convolutional layer will output a tensor
    of shape [*batch size*, 200, 1, 1].'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: FCN的想法最早由Jonathan Long等人于2015年在一篇[论文](https://homl.info/fcn)⁠^([33](ch12.html#id2962))中提出，用于语义分割（根据图像中对象的类别对每个像素进行分类的任务）。作者指出，你可以用卷积层替换CNN顶部的密集层。为了理解这一点，让我们看一个例子：假设一个有200个神经元的密集层位于输出100个特征图（每个特征图大小为7
    × 7）的卷积层之上（这是特征图的大小，不是核的大小）。每个神经元将计算来自卷积层的所有100 × 7 × 7激活的加权总和（加上一个偏置项）。现在让我们看看如果我们用一个大小为7
    × 7、使用`"valid"`填充的200个滤波器的卷积层替换密集层会发生什么。这个层将输出200个特征图，每个特征图大小为1 × 1（因为核的大小正好等于输入特征图的大小，并且我们使用了`"valid"`填充）。换句话说，它将输出200个数字，就像密集层一样；如果你仔细观察卷积层执行的运算，你会注意到这些数字将与密集层产生的数字完全相同。唯一的区别是，密集层的输出是一个形状为[*批大小*,
    200]的张量，而卷积层将输出一个形状为[*批大小*, 200, 1, 1]的张量。
- en: Tip
  id: totrans-401
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: To convert a dense layer to a convolutional layer, the number of filters in
    the convolutional layer must be equal to the number of units in the dense layer,
    the filter size must be equal to the size of the input feature maps, and you must
    use `"valid"` padding. The stride may be set to 1 or more, as we will see shortly.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 将密集层转换为卷积层时，卷积层的滤波器数量必须等于密集层的单元数量，滤波器大小必须等于输入特征图的大小，并且必须使用`"valid"`填充。步长可以设置为1或更大，我们很快就会看到。
- en: Why is this important? Well, while a dense layer expects a specific input size
    (since it has one weight per input feature), a convolutional layer will happily
    process images of any size⁠^([34](ch12.html#id2967)) (however, it does expect
    its inputs to have a specific number of channels, since each kernel contains a
    different set of weights for each input channel). Since an FCN contains only convolutional
    layers (and pooling layers, which have the same property), it can be trained and
    executed on images of any size!
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这为什么很重要呢？因为密集层期望一个特定的输入大小（因为它有一个输入特征对应的权重），而卷积层将愉快地处理任何大小的图像⁠^([34](ch12.html#id2967))（然而，它确实期望其输入具有特定的通道数，因为每个核包含每个输入通道的不同权重集）。由于FCN只包含卷积层（以及具有相同属性的池化层），它可以在任何大小的图像上训练和执行！
- en: 'For example, suppose we’d already trained a CNN for flower classification and
    localization, with an extra head for objectness. It was trained on 224 × 224 images,
    and it outputs 107 values per image:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们已经在224 × 224的图像上训练了一个用于花卉分类和定位的CNN，并且有一个额外的对象性头。它在224 × 224的图像上训练，并且每张图像输出107个值：
- en: The classification head outputs 102 class logits (one per class), trained using
    the `nn.CrossEntropyLoss`.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类头输出102个类别的logits（每个类别一个），使用`nn.CrossEntropyLoss`进行训练。
- en: The objectness head outputs a single objectness logit, trained using the `nn.BCELoss`.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对象性头输出一个对象性logits，使用`nn.BCELoss`进行训练。
- en: The localization head outputs four numbers describing the bounding box, trained
    using the CIoU loss.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定位头输出四个数字来描述边界框，使用CIoU损失进行训练。
- en: We can now convert the CNN’s dense layers (`nn.Linear`) to convolutional layers
    (`nn.Conv2d`). In fact, we don’t even need to retrain the model; we can just copy
    the weights from the dense layers to the convolutional layers! Alternatively,
    we could have converted the CNN into an FCN before training.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将CNN的密集层（`nn.Linear`）转换为卷积层（`nn.Conv2d`）。实际上，我们甚至不需要重新训练模型；我们只需将密集层的权重复制到卷积层！或者，我们可以在训练之前将CNN转换为FCN。
- en: Now suppose the last convolutional layer before the output layer (also called
    the bottleneck layer) outputs 7 × 7 feature maps when the network is fed a 224
    × 224 image (see the left side of [Figure 12-26](#fcn_diagram)). For example,
    this would be the case if the network contains 5 layers with stride 2 and `"same"`
    padding, so the spatial dimensions get divided by 2⁵ = 32 overall. If we feed
    the FCN a 448 × 448 image (see the righthand side of [Figure 12-26](#fcn_diagram)),
    the bottleneck layer will now output 14 × 14 feature maps. Since the dense output
    layer was replaced by a convolutional layer using 107 filters of size 7 × 7, with
    `"valid"` padding and stride 1, the output will be composed of 107 feature maps,
    each of size 8 × 8 (since 14 – 7 + 1 = 8).
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设在输出层（也称为瓶颈层）之前的最后一个卷积层在输入224 × 224图像时输出7 × 7特征图（参见[图12-26](#fcn_diagram)的左侧）。例如，如果网络包含5层，步长为2，并且使用`"same"`填充，那么空间维度总共会被除以2⁵
    = 32。如果我们向FCN输入448 × 448的图像（参见[图12-26](#fcn_diagram)的右侧），瓶颈层现在将输出14 × 14特征图。由于密集输出层被替换为一个使用107个7
    × 7大小滤波器的卷积层，使用`"valid"`填充和步长1，输出将由107个特征图组成，每个特征图的大小为8 × 8（因为14 – 7 + 1 = 8）。
- en: 'In other words, the FCN will process the whole image only once, and it will
    output an 8 × 8 grid where each cell contains the predictions for one region of
    the image: 107 numbers representing 102 class probabilities, 1 objectness score,
    and 4 bounding box coordinates. It’s exactly like taking the original CNN and
    sliding it across the image using 8 steps per row and 8 steps per column. To visualize
    this, imagine chopping the original image into a 14 × 14 grid, then sliding a
    7 × 7 window across this grid; there will be 8 × 8 = 64 possible locations for
    the window, hence 8 × 8 predictions. However, the FCN approach is *much* more
    efficient, since the network only looks at the image once. In fact, *You Only
    Look Once* (YOLO) is the name of a very popular object detection architecture,
    which we’ll look at next.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，全卷积网络（FCN）只会处理整个图像一次，并且它会输出一个8 × 8的网格，其中每个单元格包含图像一个区域的预测：107个数字代表102个类别的概率、1个物体得分和4个边界框坐标。这就像将原始卷积神经网络（CNN）在图像上滑动，每行滑动8步，每列滑动8步。为了可视化这一点，想象将原始图像切割成一个14
    × 14的网格，然后在这个网格上滑动一个7 × 7的窗口；窗口将有8 × 8 = 64个可能的位置，因此有8 × 8个预测。然而，FCN方法要*高效得多*，因为网络只看一次图像。实际上，“你只需看一次”（YOLO）是一个非常流行的目标检测架构的名称，我们将在下一节中探讨。
- en: '![A diagram illustrating a fully convolutional network processing a small and
    a large image, showing the progression from CNN layers to feature maps and convolution
    outputs.](assets/hmls_1226.png)'
  id: totrans-411
  prefs: []
  type: TYPE_IMG
  zh: '![一个说明全卷积网络处理小图像和大图像的图表，展示了从CNN层到特征图和卷积输出的过程。](assets/hmls_1226.png)'
- en: Figure 12-26\. The same fully convolutional network processing a small image
    (left) and a large one (right)
  id: totrans-412
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-26\. 同一个全卷积网络处理小图像（左）和大图像（右）
- en: You Only Look Once
  id: totrans-413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 你只需看一次
- en: 'YOLO is a fast and accurate object detection architecture proposed by Joseph
    Redmon et al. in a [2015 paper](https://homl.info/yolo).⁠^([35](ch12.html#id2972))
    It is so fast that it can run in real time on a video, as seen in Redmon’s [demo](https://homl.info/yolodemo2).
    YOLO’s architecture is quite similar to the one we just discussed, but with a
    few important differences:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO是由Joseph Redmon等人于2015年在一篇[论文](https://homl.info/yolo)中提出的一种快速且准确的目标检测架构。⁠^([35](ch12.html#id2972))
    它如此之快，以至于可以在视频上实时运行，正如Redmon的[演示](https://homl.info/yolodemo2)所示。YOLO的架构与我们刚刚讨论的架构非常相似，但有一些重要的区别：
- en: For each grid cell, YOLO only considers objects whose bounding box center lies
    within that cell. The bounding box coordinates are relative to that cell, where
    (0, 0) means the top-left corner of the cell and (1, 1) means the bottom right.
    However, the bounding box’s height and width may extend well beyond the cell.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个网格单元，YOLO只考虑边界框中心位于该单元内的对象。边界框坐标相对于该单元，其中(0, 0)表示单元的左上角，(1, 1)表示单元的右下角。然而，边界框的高度和宽度可能远远超出单元的范围。
- en: It outputs two bounding boxes for each grid cell (instead of just one), which
    allows the model to handle cases where two objects are so close to each other
    that their bounding box centers lie within the same cell. Each bounding box also
    comes with its own objectness score.
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个网格单元，YOLO输出两个边界框（而不是只有一个），这使得模型能够处理两个对象非常接近以至于它们的边界框中心位于同一单元格内的情况。每个边界框还附带其自己的对象性得分。
- en: 'YOLO also outputs a class probability distribution for each grid cell, predicting
    20 class probabilities per grid cell since YOLO was trained on the PASCAL VOC
    dataset, which contains 20 classes. This produces a coarse *class probability
    map*. Note that the model predicts one class probability distribution per grid
    cell, not per bounding box. However, it’s possible to estimate class probabilities
    for each bounding box during post-processing by measuring how well each bounding
    box matches each class in the class probability map. For example, imagine a picture
    of a person standing in front of a car. There will be two bounding boxes: one
    large horizontal one for the car, and a smaller vertical one for the person. These
    bounding boxes may have their centers within the same grid cell. So how can we
    tell which class should be assigned to each bounding box? Well, the class probability
    map will contain a large region where the “car” class is dominant, and inside
    it there will be a smaller region where the “person” class is dominant. Hopefully,
    the car’s bounding box will roughly match the “car” region, while the person’s
    bounding box will roughly match the “person” region: this will allow the correct
    class to be assigned to each bounding box.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO还为每个网格单元输出一个类别概率分布，由于YOLO是在包含20个类别的PASCAL VOC数据集上训练的，因此每个网格单元预测20个类别的概率。这产生了一个粗略的*类别概率图*。请注意，模型为每个网格单元预测一个类别概率分布，而不是每个边界框。然而，在后期处理过程中，可以通过测量每个边界框与类别概率图中每个类别的匹配程度来估计每个边界框的类别概率。例如，想象一张一个人站在车前的图片。将有两个边界框：一个大的水平边界框用于车，一个小的垂直边界框用于人。这些边界框的中心可能位于同一个网格单元内。那么我们如何判断应该将哪个类别分配给每个边界框呢？嗯，类别概率图将包含一个“车”类别占主导地位的大区域，在其中将有一个较小的区域“人”类别占主导地位。希望车的边界框大致匹配“车”区域，而人的边界框大致匹配“人”区域：这将允许将正确的类别分配给每个边界框。
- en: YOLO was originally developed using Darknet, an open source deep learning framework
    initially developed in C by Joseph Redmon, but it was soon ported to PyTorch and
    other libraries. It has been continuously improved over the years, initially by
    Joseph Redmon et al. (YOLOv2, YOLOv3, and YOLO9000), then by various other teams
    since 2020\. Each version brought some impressive improvements in speed and accuracy,
    using a variety of techniques; for example, YOLOv3 boosted accuracy in part thanks
    to *anchor priors*, exploiting the fact that some bounding box shapes are more
    likely than others, depending on the class (e.g., people tend to have vertical
    bounding boxes, while cars usually don’t). They also increased the number of bounding
    boxes per grid cell, they trained on different datasets with many more classes
    (up to 9,000 classes organized in a hierarchy in the case of YOLO9000), they added
    skip connections to recover some of the spatial resolution that is lost in the
    CNN (we will discuss this shortly when we look at semantic segmentation), and
    much more. There are many variants of these models too, such as scaled down “tiny”
    YOLOs, optimized to be trained on less powerful machines and which can run extremely
    fast (at over 1,000 frames per second!), but with a slightly lower *mean average
    precision* (mAP).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO最初是使用Darknet开发的，这是一个由Joseph Redmon最初用C语言编写的开源深度学习框架，但很快就被移植到了PyTorch和其他库中。多年来，它一直在不断改进，最初由Joseph
    Redmon等人（YOLOv2、YOLOv3和YOLO9000）进行改进，自2020年以来，由其他多个团队进行改进。每个版本都带来了一些令人印象深刻的速度和精度提升，使用了各种技术；例如，YOLOv3通过*锚先验*提高了精度，利用了某些边界框形状比其他形状更可能的事实，这取决于类别（例如，人倾向于有垂直的边界框，而汽车通常没有）。他们还增加了每个网格单元中的边界框数量，他们在包含更多类别的不同数据集上进行了训练（在YOLO9000的情况下，有高达9,000个类别，组织在一个层次结构中），他们添加了跳过连接来恢复CNN中丢失的一些空间分辨率（我们将在查看语义分割时简要讨论这一点），还有更多。这些模型也有很多变体，例如缩小版的“tiny”YOLO，优化用于在更强大的机器上训练，并且可以运行得非常快（每秒超过1,000帧！），但*平均精度均值*（mAP）略低。
- en: 'TorchVision does not include any YOLO model, but you can use the Ultralytics
    library, which provides a simple API to download and use various pretrained YOLO
    models, based on PyTorch. These models were pretrained on the COCO dataset which
    contains over 330,000 images, including 200,000 images annotated for object detection
    with 80 different classes (person, car, truck, bicycle, ball, etc.). The Ultralytics
    library is not installed on Colab by default, so we must run `%pip install ultralytics`.
    Then we can download a YOLO model and use it. For example, here is how to use
    this library to download the YOLOv9 model (medium variant) and detect objects
    in a batch of images (the model accepts PIL images, NumPy arrays, and even URLs):'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: TorchVision不包括任何YOLO模型，但你可以使用Ultralytics库，它提供了一个简单的API来下载和使用基于PyTorch的各种预训练YOLO模型。这些模型在包含超过330,000张图片的COCO数据集上进行了预训练，其中包含200,000张图片被标注用于对象检测，有80个不同的类别（人、汽车、卡车、自行车、球等）。Ultralytics库默认不安装在Colab上，因此我们必须运行`%pip
    install ultralytics`。然后我们可以下载一个YOLO模型并使用它。例如，以下是使用此库下载YOLOv9模型（中等变体）并在一批图片中检测对象的方法（该模型接受PIL图像、NumPy数组，甚至URL）：
- en: '[PRE36]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output is a list of `Results` objects which offers a handy `summary()`
    method. For example, here is how we can see the first detected object in the first
    image:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是一个`Results`对象列表，它提供了一个方便的`summary()`方法。例如，以下是查看第一张图片中第一个检测到的对象的方法：
- en: '[PRE37]'
  id: totrans-422
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Tip
  id: totrans-423
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The Ultralytics library also provides a simple API to train a YOLO model on
    other common object detection datasets, or on your own dataset. See [*https://docs.ultralytics.com/modes/train*](https://docs.ultralytics.com/modes/train)
    for more details.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: Ultralytics库还提供了一个简单的API，可以在其他常见的对象检测数据集或你自己的数据集上训练YOLO模型。有关更多详细信息，请参阅[*https://docs.ultralytics.com/modes/train*](https://docs.ultralytics.com/modes/train)。
- en: 'Several other pretrained object detection models are available via TorchVision.
    You can use them just like the pretrained classification models (e.g., ConvNeXt),
    except that each image prediction is a represented as a dictionary containing
    two entries: `"labels"` (i.e., class IDs) and `"boxes"`. The available models
    are listed here (see the [models page](https://pytorch.org/vision/main/models)
    for the full list of variants available):'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 通过TorchVision还可以获得其他预训练的对象检测模型。你可以像使用预训练的分类模型（例如ConvNeXt）一样使用它们，只是每个图像预测都表示为一个包含两个条目的字典：`"labels"`（即类别ID）和`"boxes"`。可用的模型在此列出（查看[模型页面](https://pytorch.org/vision/main/models)以获取所有变体的完整列表）：
- en: '[Faster R-CNN](https://homl.info/fasterrcnn)⁠^([36](ch12.html#id2982))'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '[Faster R-CNN](https://homl.info/fasterrcnn)⁠^([36](ch12.html#id2982))'
- en: 'This model has two stages: the image first goes through a CNN, then the output
    is passed to a *region proposal network* (RPN) that proposes bounding boxes that
    are most likely to contain an object; a classifier is then run for each bounding
    box, based on the cropped output of the CNN.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型有两个阶段：图像首先通过CNN，然后输出传递到一个**区域提议网络**（RPN），该网络提议最有可能包含物体的边界框；然后对每个边界框运行分类器，基于CNN裁剪后的输出。
- en: '[SSD](https://homl.info/ssd)⁠^([37](ch12.html#id2987))'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '[SSD](https://homl.info/ssd)⁠^([37](ch12.html#id2987))'
- en: SSD is a single-stage detector (“look once”) similar to YOLO.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: SSD是一个单阶段检测器（“看一次”），类似于YOLO。
- en: '[SSDlite](https://homl.info/ssdlite)⁠^([38](ch12.html#id2989))'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '[SSDlite](https://homl.info/ssdlite)⁠^([38](ch12.html#id2989))'
- en: A lightweight version of SSD, well suited for mobile devices.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: SSD的轻量级版本，非常适合移动设备。
- en: '[RetinaNet](https://homl.info/retinanet)⁠^([39](ch12.html#id2991))'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '[RetinaNet](https://homl.info/retinanet)⁠^([39](ch12.html#id2991))'
- en: A single-stage detector which introduced a variant of the cross-entropy loss
    called the *focal loss* (see `torchvision.ops.sigmoid_focal_loss()`). This loss
    gives more weight to difficult samples and thereby improves performance on small
    objects and less frequent classes.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 一个单阶段检测器，它引入了一种称为**焦点损失**（见`torchvision.ops.sigmoid_focal_loss()`）的交叉熵损失变体。这种损失给困难样本更多的权重，从而提高了对小物体和较少出现的类别的性能。
- en: '[FCOS](https://homl.info/fcos)⁠^([40](ch12.html#id2996))'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '[FCOS](https://homl.info/fcos)⁠^([40](ch12.html#id2996))'
- en: A single-stage fully convolutional net which directly predicts bounding boxes
    without relying on anchor boxes.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 一个单阶段全卷积网络，它直接预测边界框而不依赖于锚框。
- en: So far, we’ve only considered detecting objects in single images. But what about
    videos? Objects must not only be detected in each frame, they must also be tracked
    over time. Let’s take a quick look at object tracking now.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只考虑了在单张图像中检测物体。但视频呢？物体必须在每一帧中被检测到，并且必须在时间上被跟踪。现在让我们快速了解一下目标跟踪。
- en: Object Tracking
  id: totrans-437
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标跟踪
- en: 'Object tracking is a challenging task: objects move, they may grow or shrink
    as they get closer or further away, their appearance may change as they turn around
    or move to different lighting conditions or backgrounds, they may be temporarily
    occluded by other objects, and so on.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 目标跟踪是一个具有挑战性的任务：物体在移动，它们可能随着接近或远离而增大或缩小，它们的外观可能随着转向或移动到不同的光照条件或背景而改变，它们可能被其他物体暂时遮挡，等等。
- en: 'One of the most popular object tracking systems is [DeepSORT](https://homl.info/deepsort).⁠^([41](ch12.html#id3003))
    It is based on a combination of classical algorithms and deep learning:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的目标跟踪系统之一是[DeepSORT](https://homl.info/deepsort)。⁠^([41](ch12.html#id3003))
    它基于经典算法和深度学习的结合：
- en: It uses *Kalman filters* to estimate the most likely current position of an
    object given prior detections, and assuming that objects tend to move at a constant
    speed.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用**卡尔曼滤波器**来估计给定先前检测的物体最可能的位置，并假设物体倾向于以恒定速度移动。
- en: It uses a deep learning model to measure the resemblance between new detections
    and existing tracked objects.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使用深度学习模型来衡量新检测与现有跟踪物体之间的相似度。
- en: Lastly, it uses the *Hungarian algorithm* to map new detections to existing
    tracked objects (or to new tracked objects). This algorithm efficiently finds
    the combination of mappings that minimizes the distance between the detections
    and the predicted positions of tracked objects, while also minimizing the appearance
    discrepancy.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，它使用**匈牙利算法**将新检测映射到现有跟踪物体（或新跟踪物体）。该算法有效地找到使检测与跟踪物体预测位置之间的距离最小化的映射组合，同时最小化外观差异。
- en: For example, imagine a red ball that just bounced off a blue ball traveling
    in the opposite direction. Based on the previous positions of the balls, the Kalman
    filter will predict that the balls will go through each other; indeed, it assumes
    that objects move at a constant speed, so it will not expect the bounce. If the
    Hungarian algorithm only considered positions, then it would happily map the new
    detections to the wrong balls, as if they had just gone through each other and
    swapped colors. But thanks to the resemblance measure, the Hungarian algorithm
    will notice the problem. Assuming the balls are not too similar, the algorithm
    will map the new detections to the correct balls.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一个红色的球刚刚从蓝色球上弹开，而蓝色球正在相反方向上移动。根据球之前的位移，卡尔曼滤波器会预测球将相互穿过；实际上，它假设物体以恒定速度移动，因此不会预期到弹跳。如果匈牙利算法只考虑位置，那么它会愉快地将新的检测映射到错误的球上，就像它们刚刚穿过彼此并交换了颜色。但是，多亏了相似度度量，匈牙利算法会注意到这个问题。假设球不太相似，算法会将新的检测映射到正确的球上。
- en: 'The Ultralytics library supports object tracking. It uses the [Bot-SORT algorithm](https://homl.info/botsort)
    by default: this algorithm is very similar to DeepSORT but it’s faster and more
    accurate thanks to improvements such as camera-motion compensation and tweaks
    to the Kalman filter.⁠^([42](ch12.html#id3009)) For example, we can track objects
    in a video using the YOLOv9 model we created earlier by executing the following
    code. In this example, we also print the ID of each tracked object at every frame,
    and we save a copy of the video with annotations (its path is displayed at the
    end):'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: Ultralytics库支持物体跟踪。它默认使用[Bot-SORT算法](https://homl.info/botsort)：这个算法与DeepSORT非常相似，但速度更快，精度更高，这得益于诸如相机运动补偿和卡尔曼滤波器调整等改进。⁠^([42](ch12.html#id3009))
    例如，我们可以使用我们之前创建的YOLOv9模型在视频中跟踪物体，通过执行以下代码。在这个例子中，我们还打印出每一帧中每个跟踪物体的ID，并将带有注释的视频副本保存（其路径显示在末尾）：
- en: '[PRE38]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: So far we have located objects using bounding boxes. This is often sufficient,
    but sometimes you need to locate objects with much more precision—for example,
    to remove the background behind a person during a videoconference call. Let’s
    see how to go down to the pixel level.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们使用边界框定位物体。这通常足够了，但有时你需要以更高的精度定位物体——例如，在视频会议通话中移除人的背景。让我们看看如何降低到像素级别。
- en: Semantic Segmentation
  id: totrans-447
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义分割
- en: In *semantic segmentation*, each pixel is classified according to the class
    of the object it belongs to (e.g., road, car, pedestrian, building, etc.), as
    shown in [Figure 12-27](#semantic_segmentation_diagram). Note that different objects
    of the same class are *not* distinguished. For example, all the bicycles on the
    righthand side of the segmented image end up as one big lump of pixels. The main
    difficulty in this task is that when images go through a regular CNN, they gradually
    lose their spatial resolution (due to the layers with strides greater than 1);
    so, a regular CNN may end up knowing that there’s a person somewhere in the bottom
    left of the image, but it might not be much more precise than that.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在*语义分割*中，每个像素根据其所属对象的类别进行分类（例如，道路、汽车、行人、建筑物等），如图[图12-27](#semantic_segmentation_diagram)所示。请注意，同一类别的不同对象*不会被区分*。例如，分割图像右侧的所有自行车最终都变成了一团像素。这项任务的主要困难在于，当图像通过常规卷积神经网络时，它们会逐渐失去空间分辨率（由于步长大于1的层）；因此，常规卷积神经网络可能只知道图像的左下角有一个人，但可能不会比这更精确。
- en: '![A street scene is displayed with semantic segmentation applied, classifying
    areas into categories like sky, buildings, people, cars, bicycles, sidewalk, and
    road.](assets/hmls_1227.png)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![应用了语义分割的街道场景，将区域分类为天空、建筑物、人物、汽车、自行车、人行道和道路等类别。](assets/hmls_1227.png)'
- en: Figure 12-27\. Semantic segmentation
  id: totrans-450
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-27\. 语义分割
- en: Just like for object detection, there are many different approaches to tackle
    this problem, some quite complex. However, a fairly simple solution was proposed
    in the 2015 paper by Jonathan Long et al., that I mentioned earlier, on fully
    convolutional networks. The authors start by taking a pretrained CNN and turning
    it into an FCN. The CNN applies an overall stride of 32 to the input image (i.e.,
    if you multiply all the strides), meaning the last layer outputs feature maps
    that are 32 times smaller than the input image. This is clearly too coarse, so
    they added a single *upsampling layer* that multiplies the resolution by 32.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 就像对象检测一样，解决这个问题有许多不同的方法，其中一些相当复杂。然而，2015年Jonathan Long等人发表的一篇论文中提出了一个相当简单的解决方案，我之前提到过，关于全卷积网络。作者首先从一个预训练的CNN开始，将其转换为FCN。CNN对输入图像应用了32的整体步长（即如果你乘以所有步长），这意味着最后一层的特征图比输入图像小32倍。这显然太粗糙了，所以他们添加了一个单独的*上采样层*，将分辨率乘以32。
- en: There are several solutions available for upsampling (increasing the size of
    an image), such as bilinear interpolation, but that only works reasonably well
    up to ×4 or ×8\. Instead, they use a *transposed convolutional layer*:⁠^([43](ch12.html#id3017))
    this is equivalent to first stretching the image by inserting empty rows and columns
    (full of zeros), then performing a regular convolution (see [Figure 12-28](#conv2d_transpose_diagram)).
    Alternatively, some people prefer to think of it as a regular convolutional layer
    that uses fractional strides (e.g., the stride is 1/2 in [Figure 12-28](#conv2d_transpose_diagram)).
    The transposed convolutional layer can be initialized to perform something close
    to linear interpolation, but since it is a trainable layer, it will learn to do
    better during training. In PyTorch, you can use the `nn.ConvTranspose2d` layer.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 对于上采样（增加图像大小）有几种解决方案，例如双线性插值，但这只适用于×4或×8。相反，他们使用一个*转置卷积层*：⁠^([43](ch12.html#id3017))
    这相当于首先通过插入空行和列（充满零）来拉伸图像，然后执行常规卷积（见[图12-28](#conv2d_transpose_diagram))。或者，有些人更喜欢将其视为使用分数步长的常规卷积层（例如，[图12-28](#conv2d_transpose_diagram)中的步长为1/2）。转置卷积层可以被初始化为执行接近线性插值的功能，但由于它是一个可训练层，它将在训练过程中学会做得更好。在PyTorch中，你可以使用`nn.ConvTranspose2d`层。
- en: Note
  id: totrans-453
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In a transposed convolutional layer, the stride defines how much the input will
    be stretched, not the size of the filter steps, so the larger the stride, the
    larger the output (unlike for convolutional layers or pooling layers).
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在转置卷积层中，步长定义了输入将被拉伸多少，而不是滤波器步长的大小，所以步长越大，输出就越大（与卷积层或池化层不同）。
- en: '![Diagram illustrating upsampling using a transposed convolutional layer, showing
    a 2x3 input being expanded to a 5x7 output with stride 2 and kernel size 3.](assets/hmls_1228.png)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![图示使用转置卷积层进行上采样，展示一个2x3的输入被扩展到5x7的输出，步长为2，卷积核大小为3。](assets/hmls_1228.png)'
- en: Figure 12-28\. Upsampling using a transposed convolutional layer
  id: totrans-456
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-28. 使用转置卷积层进行上采样
- en: 'Using transposed convolutional layers for upsampling is OK, but still too imprecise.
    To do better, Long et al. added skip connections from lower layers: for example,
    they upsampled the output image by a factor of 2 (instead of 32), and they added
    the output of a lower layer that had this double resolution. Then they upsampled
    the result by a factor of 16, leading to a total upsampling factor of 32 (see
    [Figure 12-29](#skip_plus_upsample_diagram)). This recovered some of the spatial
    resolution that was lost in earlier pooling layers. In their best architecture,
    they used a second similar skip connection to recover even finer details from
    an even lower layer. In short, the output of the original CNN goes through the
    following extra steps: upsample ×2, add the output of a lower layer (of the appropriate
    scale), upsample ×2, add the output of an even lower layer, and finally upsample
    ×8\. It is even possible to scale up beyond the size of the original image: this
    can be used to increase the resolution of an image, which is a technique called
    *super-resolution*.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 使用转置卷积层进行上采样是可以的，但仍然不够精确。为了做得更好，Long等人添加了来自较低层的跳过连接：例如，他们将输出图像上采样了2倍（而不是32倍），并且添加了具有这种双倍分辨率的较低层的输出。然后他们通过16倍因子上采样结果，导致总上采样因子为32（见[图12-29](#skip_plus_upsample_diagram)）。这恢复了一些在早期池化层中丢失的空间分辨率。在他们最好的架构中，他们使用第二个类似的跳过连接从更低层恢复更精细的细节。简而言之，原始CNN的输出经过以下额外步骤：上采样×2，添加适当尺度的较低层输出，上采样×2，添加更低层的输出，最后上采样×8。甚至可以将尺寸放大到原始图像大小之上：这可以用来提高图像的分辨率，这是一种称为*超分辨率*的技术。
- en: '![Diagram showing the use of skip connections and upsampling in convolutional
    neural networks to recover spatial resolution, with feature maps being downsampled
    and then upsampled by factors of 2 and 16.](assets/hmls_1229.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![展示在卷积神经网络中使用跳过连接和上采样以恢复空间分辨率，特征图被以2和16的因子下采样和上采样。](assets/hmls_1229.png)'
- en: Figure 12-29\. Skip layers recover some spatial resolution from lower layers
  id: totrans-459
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图12-29\. 跳过层从较低层恢复一些空间分辨率
- en: Tip
  id: totrans-460
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The FCN model is available in TorchVision, along with a couple other semantic
    segmentation models. See the notebook for a code example.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: FCN模型在TorchVision中可用，还有其他几个语义分割模型。请参阅笔记本以获取代码示例。
- en: '*Instance segmentation* is similar to semantic segmentation, but instead of
    merging all objects of the same class into one big lump, each object is distinguished
    from the others (e.g., it identifies each individual bicycle). For example the
    *Mask R-CNN* architecture, proposed in a [2017 paper](https://homl.info/maskrcnn)⁠^([44](ch12.html#id3033))
    by Kaiming He et al., extends the Faster R-CNN model by additionally producing
    a pixel mask for each bounding box. So not only do you get a bounding box around
    each object, with a set of estimated class probabilities, you also get a pixel
    mask that locates pixels in the bounding box that belong to the object. This model
    is available in TorchVision, pretrained on the COCO 2017 dataset.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '*实例分割*与语义分割类似，但不是将同一类的所有对象合并成一个大的块，而是将每个对象与其他对象区分开来（例如，它识别每个单独的自行车）。例如，由Kaiming
    He等人于2017年提出[2017年论文](https://homl.info/maskrcnn)⁠^([44](ch12.html#id3033))的*Mask
    R-CNN*架构通过为每个边界框额外生成一个像素掩码来扩展Faster R-CNN模型。因此，你不仅为每个对象得到一个带有估计类别概率的边界框，还得到一个定位边界框内属于对象的像素的像素掩码。此模型在TorchVision中可用，在COCO
    2017数据集上预训练。'
- en: Tip
  id: totrans-463
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: TorchVision’s transforms API v2 can apply to masks and videos, just like it
    applies to bounding boxes, thanks to the `Video` and `Mask` TVTensors.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`Video`和`Mask` TVTensors，TorchVision的transforms API v2可以应用于掩码和视频，就像它应用于边界框一样。
- en: 'As you can see, the field of deep computer vision is vast and fast-paced, with
    all sorts of architectures popping up every year. If you want to try the latest
    and greatest models, check out the trending papers at [*https://huggingface.co/papers*](https://huggingface.co/papers).
    Most of them used to be based on convolutional neural networks, but since 2020
    another neural net architecture has entered the computer vision space: Transformers
    (which we will discuss in [Chapter 14](ch14.html#nlp_chapter)). The progress made
    over the last 15 years has been astounding, and researchers are now focusing on
    harder and harder problems, such as *adversarial learning* (which attempts to
    make the network more resistant to images designed to fool it), *explainability*
    (understanding why the network makes a specific classification), realistic *image
    generation* (which we will come back to in [Chapter 18](ch18.html#autoencoders_chapter)),
    *single-shot learning* (a system that can recognize an object after it has seen
    it just once), predicting the next frames in a video, combining text and image
    tasks, and more.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，深度计算机视觉领域广阔且发展迅速，每年都会出现各种架构。如果你想尝试最新的最佳模型，请查看[*https://huggingface.co/papers*](https://huggingface.co/papers)上的热门论文。其中大部分曾经是基于卷积神经网络的，但自2020年以来，另一种神经网络架构已经进入计算机视觉领域：Transformer（我们将在[第14章](ch14.html#nlp_chapter)中讨论）。过去15年取得的进步令人惊叹，研究人员现在正专注于越来越难的问题，例如*对抗学习*（试图使网络对旨在欺骗它的图像更具抵抗力）、*可解释性*（理解网络为何做出特定的分类）、逼真的*图像生成*（我们将在[第18章](ch18.html#autoencoders_chapter)中再次讨论）、*单次学习*（一个系统在仅看到一次物体后就能识别该物体）、预测视频中的下一帧、结合文本和图像任务，等等。
- en: Now on to the next chapter, where we will look at how to process sequential
    data such as time series using recurrent neural networks and convolutional neural
    networks.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入下一章，我们将探讨如何使用循环神经网络和卷积神经网络处理序列数据，如时间序列。
- en: Exercises
  id: totrans-467
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: What are the advantages of a CNN over a fully connected DNN for image classification?
  id: totrans-468
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与全连接DNN相比，CNN在图像分类中有哪些优势？
- en: 'Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels,
    a stride of 2, and `"same"` padding. The lowest layer outputs 100 feature maps,
    the middle one outputs 200, and the top one outputs 400\. The input images are
    RGB images of 200 × 300 pixels:'
  id: totrans-469
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑一个由三个卷积层组成的CNN，每个层使用3 × 3核，步长为2，填充为"same"。最低层输出100个特征图，中间层输出200个，顶层输出400个。输入图像是200
    × 300像素的RGB图像：
- en: What is the total number of parameters in the CNN?
  id: totrans-470
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: CNN中的参数总数是多少？
- en: If we are using 32-bit floats, at least how much RAM will this network require
    when making a prediction for a single instance?
  id: totrans-471
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们使用32位浮点数，在为单个实例进行预测时，这个网络至少需要多少RAM？
- en: What about when training on a mini-batch of 50 images?
  id: totrans-472
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当在50张图像的小批量上进行训练时，情况如何？
- en: If your GPU runs out of memory while training a CNN, what are five things you
    could try to solve the problem?
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果在训练CNN时你的GPU内存不足，你可以尝试以下五种方法来解决问题？
- en: Why would you want to add a max pooling layer rather than a convolutional layer
    with the same stride?
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么你想要添加一个最大池化层而不是具有相同步长的卷积层？
- en: Can you name the main innovations in AlexNet, as compared to LeNet-5? What about
    the main innovations in GoogLeNet, ResNet, SENet, Xception, EfficientNet, and
    ConvNeXt?
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能说出与LeNet-5相比，AlexNet的主要创新点是什么？GoogLeNet、ResNet、SENet、Xception、EfficientNet和ConvNeXt的主要创新点又是什么？
- en: What is a fully convolutional network? How can you convert a dense layer into
    a convolutional layer?
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是全卷积网络？如何将密集层转换为卷积层？
- en: What is the main technical difficulty of semantic segmentation?
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 语义分割的主要技术难点是什么？
- en: Build your own CNN from scratch and try to achieve the highest possible accuracy
    on MNIST.
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始构建自己的CNN，并尝试在MNIST上实现尽可能高的准确率。
- en: 'Use transfer learning for large image classification, going through these steps:'
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用迁移学习进行大图像分类，按照以下步骤进行：
- en: Create a training set containing at least 100 images per class. For example,
    you could classify your own pictures based on the location (beach, mountain, city,
    etc.). Alternatively, you can use an existing dataset, such as the one used in
    PyTorch’s [transfer learning for computer vision tutorial](https://homl.info/transfertuto).
  id: totrans-480
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含每个类别至少100张图像的训练集。例如，你可以根据自己的照片根据地点（海滩、山脉、城市等）进行分类。或者，你可以使用现有的数据集，例如PyTorch的[计算机视觉迁移学习教程](https://homl.info/transfertuto)中使用的数据集。
- en: Split it into a training set, a validation set, and a test set.
  id: totrans-481
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其分为训练集、验证集和测试集。
- en: Build the input pipeline, apply the appropriate preprocessing operations, and
    optionally add data augmentation.
  id: totrans-482
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建输入管道，应用适当的预处理操作，并可选地添加数据增强。
- en: Fine-tune a pretrained model on this dataset.
  id: totrans-483
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个数据集上微调预训练模型。
- en: Go through PyTorch’s [object detection fine-tuning tutorial](https://homl.info/detectiontuto).
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历PyTorch的[目标检测微调教程](https://homl.info/detectiontuto)。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解答可在本章笔记本的末尾找到，在[*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: '^([1](ch12.html#id2734-marker)) David H. Hubel, “Single Unit Activity in Striate
    Cortex of Unrestrained Cats”, *The Journal of Physiology* 147 (1959): 226–238.'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch12.html#id2734-marker)) David H. Hubel，“未受限制的猫的条纹皮层中的单个神经元活动”，*生理学杂志*
    147 (1959): 226–238。'
- en: '^([2](ch12.html#id2735-marker)) David H. Hubel and Torsten N. Wiesel, “Receptive
    Fields of Single Neurons in the Cat’s Striate Cortex”, *The Journal of Physiology*
    148 (1959): 574–591.'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '^([2](ch12.html#id2735-marker)) David H. Hubel 和 Torsten N. Wiesel，“猫条纹皮层中单个神经元的感受野”，*生理学杂志*
    148 (1959): 574–591。'
- en: '^([3](ch12.html#id2736-marker)) David H. Hubel and Torsten N. Wiesel, “Receptive
    Fields and Functional Architecture of Monkey Striate Cortex”, *The Journal of
    Physiology* 195 (1968): 215–243.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '^([3](ch12.html#id2736-marker)) David H. Hubel 和 Torsten N. Wiesel，“猴条纹皮层的感受野和功能架构”，*生理学杂志*
    195 (1968): 215–243。'
- en: '^([4](ch12.html#id2738-marker)) Kunihiko Fukushima, “Neocognitron: A Self-Organizing
    Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift
    in Position”, *Biological Cybernetics* 36 (1980): 193–202.'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch12.html#id2738-marker)) Kunihiko Fukushima，“新认知机：一种不受位置移动影响的模式识别机制的自我组织神经网络模型”，*生物控制论*
    36 (1980): 193–202。'
- en: '^([5](ch12.html#id2739-marker)) Yann LeCun et al., “Gradient-Based Learning
    Applied to Document Recognition”, *Proceedings of the IEEE* 86, no. 11 (1998):
    2278–2324.'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '^([5](ch12.html#id2739-marker)) Yann LeCun 等人，“将基于梯度的学习方法应用于文档识别”，*IEEE汇刊*
    86, 第 11 期 (1998): 2278–2324。'
- en: ^([6](ch12.html#id2744-marker)) A convolution is a mathematical operation that
    slides one function over another and measures the integral of their pointwise
    multiplication. It has deep connections with the Fourier transform and the Laplace
    transform, and is heavily used in signal processing. Convolutional layers actually
    use cross-correlations, which are very similar to convolutions (see [*https://homl.info/76*](https://homl.info/76)
    for more details).
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch12.html#id2744-marker)) 卷积是一种数学运算，它将一个函数滑动到另一个函数上，并测量它们逐点乘积的积分。它与傅里叶变换和拉普拉斯变换有着深刻的联系，并且在信号处理中被广泛使用。卷积层实际上使用的是互相关，这与卷积非常相似（更多详情请见[*https://homl.info/76*](https://homl.info/76)）。
- en: '^([7](ch12.html#id2783-marker)) Other kernels we’ve discussed so far had weights,
    but pooling kernels do not: they are just stateless sliding windows.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch12.html#id2783-marker)) 我们之前讨论的其他核都有权重，但池化核没有：它们只是无状态的滑动窗口。
- en: '^([8](ch12.html#id2805-marker)) Yann LeCun et al., “Gradient-Based Learning
    Applied to Document Recognition”, *Proceedings of the IEEE* 86, no. 11 (1998):
    2278–2324.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '^([8](ch12.html#id2805-marker)) Yann LeCun 等人，“将基于梯度的学习方法应用于文档识别”，*IEEE汇刊*
    86, 第 11 期 (1998): 2278–2324。'
- en: '^([9](ch12.html#id2808-marker)) Alex Krizhevsky et al., “ImageNet Classification
    with Deep Convolutional Neural Networks”, *Proceedings of the 25th International
    Conference on Neural Information Processing Systems* 1 (2012): 1097–1105.'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '^([9](ch12.html#id2808-marker)) Alex Krizhevsky 等人，“使用深度卷积神经网络进行ImageNet分类”，*第25届国际神经网络信息处理系统会议论文集*
    1 (2012): 1097–1105。'
- en: '^([10](ch12.html#id2820-marker)) Matthew D. Zeiler and Rob Fergus, “Visualizing
    and Understanding Convolutional Networks”, *Proceedings of the European Conference
    on Computer Vision* (2014): 818–833.'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '^([10](ch12.html#id2820-marker)) Matthew D. Zeiler 和 Rob Fergus，“可视化并理解卷积网络”，*欧洲计算机视觉会议论文集*
    (2014): 818–833。'
- en: '^([11](ch12.html#id2822-marker)) Christian Szegedy et al., “Going Deeper with
    Convolutions”, *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition* (2015): 1–9.'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '^([11](ch12.html#id2822-marker)) Christian Szegedy 等人，“通过卷积加深”，*IEEE计算机视觉与模式识别会议论文集*
    (2015): 1–9。'
- en: ^([12](ch12.html#id2823-marker)) In the 2010 movie *Inception*, the characters
    keep going deeper and deeper into multiple layers of dreams; hence the name of
    these modules.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch12.html#id2823-marker)) 在2010年的电影《盗梦空间》中，角色们不断深入到多个梦境层中；因此这些模块的名称由此而来。
- en: ^([13](ch12.html#id2839-marker)) Kaiming He et al., “Deep Residual Learning
    for Image Recognition”, arXiv preprint arXiv:1512:03385 (2015).
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch12.html#id2839-marker)) 何凯明等， “用于图像识别的深度残差学习”， arXiv 预印本 arXiv:1512:03385
    (2015)。
- en: ^([14](ch12.html#id2843-marker)) Gao Huang, Yu Sun, et al., “Deep Networks with
    Stochastic Depth”, arXiv preprint arXiv:1603.09382 (2016).
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch12.html#id2843-marker)) 高黄，孙宇等， “具有随机深度的深度网络”， arXiv 预印本 arXiv:1603.09382
    (2016)。
- en: ^([15](ch12.html#id2847-marker)) It is a common practice when describing a neural
    network to count only layers with parameters.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch12.html#id2847-marker)) 描述神经网络时，通常只计算具有参数的层。
- en: ^([16](ch12.html#id2850-marker)) Christian Szegedy et al., “Inception–v4, Inception-ResNet
    and the Impact of Residual Connections on Learning”, arXiv preprint arXiv:1602.07261
    (2016).
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch12.html#id2850-marker)) Christian Szegedy 等， “Inception-v4，Inception-ResNet
    以及残差连接对学习的影响”， arXiv 预印本 arXiv:1602.07261 (2016)。
- en: '^([17](ch12.html#id2853-marker)) François Chollet, “Xception: Deep Learning
    with Depthwise Separable Convolutions”, arXiv preprint arXiv:1610.02357 (2016).'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch12.html#id2853-marker)) François Chollet， “Xception：使用深度可分离卷积的深度学习”，
    arXiv 预印本 arXiv:1610.02357 (2016)。
- en: ^([18](ch12.html#id2856-marker)) This name can sometimes be ambiguous, since
    spatially separable convolutions are often called “separable convolutions” as
    well.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch12.html#id2856-marker)) 这个名称有时可能有些歧义，因为空间可分离卷积通常也被称为“可分离卷积”。
- en: '^([19](ch12.html#id2863-marker)) Jie Hu et al., “Squeeze-and-Excitation Networks”,
    *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*
    (2018): 7132–7141.'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '^([19](ch12.html#id2863-marker)) 胡杰等， “挤压和激励网络”， *IEEE 计算机视觉与模式识别会议论文集* (2018):
    7132–7141。'
- en: ^([20](ch12.html#id2869-marker)) Karen Simonyan and Andrew Zisserman, “Very
    Deep Convolutional Networks for Large-Scale Image Recognition”, arXiv preprint
    arXiv:1409.1556 (2014).
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch12.html#id2869-marker)) Karen Simonyan 和 Andrew Zisserman， “用于大规模图像识别的非常深的卷积神经网络”，
    arXiv 预印本 arXiv:1409.1556 (2014)。
- en: ^([21](ch12.html#id2872-marker)) Saining Xie et al., “Aggregated Residual Transformations
    for Deep Neural Networks”, arXiv preprint arXiv:1611.05431 (2016).
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: ^([21](ch12.html#id2872-marker)) 谢赛宁等， “用于深度神经网络的聚合残差变换”， arXiv 预印本 arXiv:1611.05431
    (2016)。
- en: ^([22](ch12.html#id2874-marker)) Gao Huang et al., “Densely Connected Convolutional
    Networks”, arXiv preprint arXiv:1608.06993 (2016).
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: ^([22](ch12.html#id2874-marker)) 高黄等， “密集连接卷积网络”， arXiv 预印本 arXiv:1608.06993
    (2016)。
- en: '^([23](ch12.html#id2876-marker)) Andrew G. Howard et al., “MobileNets: Efficient
    Convolutional Neural Networks for Mobile Vision Applications”, arXiv preprint
    arXiv:1704.04861 (2017).'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: ^([23](ch12.html#id2876-marker)) Andrew G. Howard 等， “MobileNets：用于移动视觉应用的效率卷积神经网络”，
    arXiv 预印本 arXiv:1704.04861 (2017)。
- en: '^([24](ch12.html#id2879-marker)) Chien-Yao Wang et al., “CSPNet: A New Backbone
    That Can Enhance Learning Capability of CNN”, arXiv preprint arXiv:1911.11929
    (2019).'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: ^([24](ch12.html#id2879-marker)) 王建尧等， “CSPNet：一种可以增强 CNN 学习能力的新的骨干网络”， arXiv
    预印本 arXiv:1911.11929 (2019)。
- en: '^([25](ch12.html#id2881-marker)) Mingxing Tan and Quoc V. Le, “EfficientNet:
    Rethinking Model Scaling for Convolutional Neural Networks”, arXiv preprint arXiv:1905.11946
    (2019).'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: ^([25](ch12.html#id2881-marker)) 谭明兴和 Quoc V. Le， “EfficientNet：重新思考卷积神经网络的模型缩放”，
    arXiv 预印本 arXiv:1905.11946 (2019)。
- en: ^([26](ch12.html#id2884-marker)) Zhuang Liu et al, “A ConvNet for the 2020s”,
    arXiv preprint arXiv:2201.03545 (2022).
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: ^([26](ch12.html#id2884-marker)) 刘壮等， “面向 2020 年代的卷积神经网络”， arXiv 预印本 arXiv:2201.03545
    (2022)。
- en: ^([27](ch12.html#id2889-marker)) In the international system of units (SI),
    1 MB = 1,000 KB = 1,000 × 1,000 bytes = 1,000 × 1,000 × 8 bits. And 1 MiB = 1,024
    kiB = 1,024 × 1,024 bytes. So 12 MB ≈ 11.44 MiB.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: ^([27](ch12.html#id2889-marker)) 在国际单位制（SI）中，1 MB = 1,000 KB = 1,000 × 1,000
    字节 = 1,000 × 1,000 × 8 比特。而 1 MiB = 1,024 kiB = 1,024 × 1,024 字节。因此，12 MB ≈ 11.44
    MiB。
- en: '^([28](ch12.html#id2897-marker)) Aidan Gomez et al., “The Reversible Residual
    Network: Backpropagation Without Storing Activations”, arXiv preprint arXiv:1707.04585
    (2017).'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: ^([28](ch12.html#id2897-marker)) 艾丹·戈麦斯等， “在大规模类别上的自动花卉分类”， *印度计算机视觉、图形与图像处理会议论文集*
    (2008)。
- en: ^([29](ch12.html#id2920-marker)) M. Nilsback and A. Zisserman, “Automated Flower
    Classification over a Large Number of Classes”, *Proceedings of the Indian Conference
    on Computer Vision, Graphics and Image Processing* (2008).
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: ^([29](ch12.html#id2920-marker)) M. Nilsback 和 A. Zisserman， “在大量类别上的自动花卉分类”，
    *印度计算机视觉、图形与图像处理会议论文集* (2008)。
- en: '^([30](ch12.html#id2922-marker)) TorchVision PR #8838 might have fixed this
    by the time your read these lines.'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '^([30](ch12.html#id2922-marker)) TorchVision PR #8838 可能会在你阅读这些行时修复这个问题。'
- en: '^([31](ch12.html#id2948-marker)) H. Rezatofighi et al., “Generalized Intersection
    over Union: A Metric and A Loss for Bounding Box Regression”, arXiv preprint arXiv:1902.09630
    (2019).'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '^([31](ch12.html#id2948-marker)) H. Rezatofighi 等人，“Generalized Intersection
    over Union: A Metric and A Loss for Bounding Box Regression”，arXiv 预印本 arXiv:1902.09630
    (2019).'
- en: ^([32](ch12.html#id2952-marker)) Z. Zheng et al., “Enhancing Geometric Factors
    in Model Learning and Inference for Object Detection and Instance Segmentation”,
    arXiv preprint arXiv:2005.03572 (2020).
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: ^([32](ch12.html#id2952-marker)) Z. Zheng 等人，“Enhancing Geometric Factors in
    Model Learning and Inference for Object Detection and Instance Segmentation”，arXiv
    预印本 arXiv:2005.03572 (2020).
- en: '^([33](ch12.html#id2962-marker)) Jonathan Long et al., “Fully Convolutional
    Networks for Semantic Segmentation”, *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition* (2015): 3431–3440.'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '^([33](ch12.html#id2962-marker)) Jonathan Long 等人，“Fully Convolutional Networks
    for Semantic Segmentation”，*IEEE 计算机视觉与模式识别会议论文集* (2015): 3431–3440.'
- en: '^([34](ch12.html#id2967-marker)) There is one small exception: a convolutional
    layer using `"valid"` padding will complain if the input size is smaller than
    the kernel size.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: ^([34](ch12.html#id2967-marker)) 有一个小的例外：使用 `"valid"` 填充的卷积层如果输入尺寸小于核尺寸，将会报错。
- en: '^([35](ch12.html#id2972-marker)) Joseph Redmon et al., “You Only Look Once:
    Unified, Real-Time Object Detection”, *Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition* (2016): 779–788.'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '^([35](ch12.html#id2972-marker)) Joseph Redmon 等人，“You Only Look Once: Unified,
    Real-Time Object Detection”，*IEEE 计算机视觉与模式识别会议论文集* (2016): 779–788.'
- en: '^([36](ch12.html#id2982-marker)) Shaoqing Ren et al., “Faster R-CNN: Towards
    Real-Time Object Detection with Region Proposal Networks”, *Proceedings of e 28th
    International Conference on Neural Information Processing Systems* 1 (2015): 91–99.'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '^([36](ch12.html#id2982-marker)) Shaoqing Ren 等人，“Faster R-CNN: Towards Real-Time
    Object Detection with Region Proposal Networks”，*第 28 届国际神经网络信息处理系统会议论文集* 1 (2015):
    91–99.'
- en: '^([37](ch12.html#id2987-marker)) Wei Liu et al., “SSD: Single Shot Multibox
    Detector”, *Proceedings of the 14th European Conference on Computer Vision* 1
    (2016): 21–37.'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '^([37](ch12.html#id2987-marker)) Wei Liu 等人，“SSD: Single Shot Multibox Detector”，*第
    14 届欧洲计算机视觉会议论文集* 1 (2016): 21–37.'
- en: '^([38](ch12.html#id2989-marker)) Mark Sandler et al., “MobileNetV2: Inverted
    Residuals and Linear Bottlenecks”, arXiv preprint arXiv:1801.04381 (2018).'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '^([38](ch12.html#id2989-marker)) Mark Sandler 等人，“MobileNetV2: Inverted Residuals
    and Linear Bottlenecks”，arXiv 预印本 arXiv:1801.04381 (2018).'
- en: ^([39](ch12.html#id2991-marker)) Tsung-Yi Lin et al., “Focal Loss for Dense
    Object Detection”, arXiv preprint arXiv:1708.02002 (2017).
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: ^([39](ch12.html#id2991-marker)) Tsung-Yi Lin 等人，“Focal Loss for Dense Object
    Detection”，arXiv 预印本 arXiv:1708.02002 (2017).
- en: '^([40](ch12.html#id2996-marker)) Zhi Tian et al., “FCOS: Fully Convolutional
    One-Stage Object Detection”, arXiv preprint arXiv:1904.01355 (2019).'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '^([40](ch12.html#id2996-marker)) Zhi Tian 等人，“FCOS: Fully Convolutional One-Stage
    Object Detection”，arXiv 预印本 arXiv:1904.01355 (2019).'
- en: ^([41](ch12.html#id3003-marker)) Nicolai Wojke et al., “Simple Online and Realtime
    Tracking with a Deep Association Metric”, arXiv preprint arXiv:1703.07402 (2017).
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: ^([41](ch12.html#id3003-marker)) Nicolai Wojke 等人，“Simple Online and Realtime
    Tracking with a Deep Association Metric”，arXiv 预印本 arXiv:1703.07402 (2017).
- en: '^([42](ch12.html#id3009-marker)) Nir Aharon et al., “BoT-SORT: Robust Associations
    Multi-Pedestrian Tracking”, arXiv preprint arXiv:2206.14651 (2022).'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '^([42](ch12.html#id3009-marker)) Nir Aharon 等人，“BoT-SORT: Robust Associations
    Multi-Pedestrian Tracking”，arXiv 预印本 arXiv:2206.14651 (2022).'
- en: ^([43](ch12.html#id3017-marker)) This type of layer is sometimes referred to
    as a *deconvolution layer*, but it does *not* perform what mathematicians call
    a deconvolution, so this name should be avoided.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: ^([43](ch12.html#id3017-marker)) 这种类型的层有时被称为 *反卷积层*，但它并不执行数学家所说的反卷积，因此应避免使用此名称。
- en: ^([44](ch12.html#id3033-marker)) Kaiming He et al., “Mask R-CNN”, arXiv preprint
    arXiv:1703.06870 (2017).
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: ^([44](ch12.html#id3033-marker)) Kaiming He 等人，“Mask R-CNN”，arXiv 预印本 arXiv:1703.06870
    (2017).
