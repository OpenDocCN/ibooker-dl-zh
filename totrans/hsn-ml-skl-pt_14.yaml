- en: Chapter 12\. Deep Computer Vision Using Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although IBM’s Deep Blue supercomputer beat the chess world champion Garry
    Kasparov back in 1996, it wasn’t until fairly recently that computers were able
    to reliably perform seemingly trivial tasks such as detecting a puppy in a picture
    or recognizing spoken words. Why are these tasks so effortless to us humans? The
    answer lies in the fact that perception largely takes place outside the realm
    of our consciousness, within specialized visual, auditory, and other sensory modules
    in our brains. By the time sensory information reaches our consciousness, it is
    already adorned with high-level features; for example, when you look at a picture
    of a cute puppy, you cannot choose *not* to see the puppy, *not* to notice its
    cuteness. Nor can you explain *how* you recognize a cute puppy; it’s just obvious
    to you. Thus, we cannot trust our subjective experience: perception is not trivial
    at all, and to understand it we must look at how our sensory modules work.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Convolutional neural networks* (CNNs) emerged from the study of the brain’s
    visual cortex, and they have been used in computer image recognition since the
    1980s. Over the last 15 years, thanks to the increase in computational power,
    the amount of available training data, and the tricks presented in [Chapter 11](ch11.html#deep_chapter)
    for training deep nets, CNNs have managed to achieve superhuman performance on
    some complex visual tasks. They power image search services, self-driving cars,
    automatic video classification systems, and more. Moreover, CNNs are not restricted
    to visual perception: they are also successful at many other tasks, such as voice
    recognition and natural language processing. However, we will focus on visual
    applications for now.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we will explore where CNNs came from, what their building blocks
    look like, and how to implement them using PyTorch. Then we will discuss some
    of the best CNN architectures, as well as other visual tasks, including object
    detection (classifying multiple objects in an image and placing bounding boxes
    around them) and semantic segmentation (classifying each pixel according to the
    class of the object it belongs to).
  prefs: []
  type: TYPE_NORMAL
- en: The Architecture of the Visual Cortex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: David H. Hubel and Torsten Wiesel performed a series of experiments on cats
    in [1958](https://homl.info/71)⁠^([1](ch12.html#id2734)) and [1959](https://homl.info/72)⁠^([2](ch12.html#id2735))
    (and a [few years later on monkeys](https://homl.info/73)⁠^([3](ch12.html#id2736))),
    giving crucial insights into the structure of the visual cortex (the authors received
    the Nobel Prize in Physiology or Medicine in 1981 for their work). In particular,
    they showed that many neurons in the visual cortex have a small *local receptive
    field*, meaning they react only to visual stimuli located in a limited region
    of the visual field (see [Figure 12-1](#cat_visual_cortex_diagram), in which the
    local receptive fields of five neurons are represented by dashed circles). The
    receptive fields of different neurons may overlap, and together they tile the
    whole visual field.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating how biological neurons in the visual cortex respond
    to specific patterns within small receptive fields and integrate this information
    to recognize complex shapes like a house.](assets/hmls_1201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. Biological neurons in the visual cortex respond to specific patterns
    in small regions of the visual field called receptive fields; as the visual signal
    makes its way through consecutive brain modules, neurons respond to more complex
    patterns in larger receptive fields
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Moreover, the authors showed that some neurons react only to images of horizontal
    lines, while others react only to lines with different orientations (two neurons
    may have the same receptive field but react to different line orientations). They
    also noticed that some neurons have larger receptive fields, and they react to
    more complex patterns that are combinations of the lower-level patterns. These
    observations led to the idea that the higher-level neurons are based on the outputs
    of neighboring lower-level neurons (in [Figure 12-1](#cat_visual_cortex_diagram),
    notice that each neuron is connected only to nearby neurons from the previous
    layer). This powerful architecture is able to detect all sorts of complex patterns
    in any area of the visual field.
  prefs: []
  type: TYPE_NORMAL
- en: 'These studies of the visual cortex inspired the [neocognitron](https://homl.info/74),⁠^([4](ch12.html#id2738))
    introduced in 1980, which gradually evolved into what we now call convolutional
    neural networks. An important milestone was a [1998 paper](https://homl.info/75)⁠^([5](ch12.html#id2739))
    by Yann LeCun et al. that introduced the famous *LeNet-5* architecture, which
    became widely used by banks to recognize handwritten digits on checks. This architecture
    has some building blocks that you already know, such as fully connected layers
    and sigmoid activation functions, but it also introduces two new building blocks:
    *convolutional layers* and *pooling layers*. Let’s look at them now.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why not simply use a deep neural network with fully connected layers for image
    recognition tasks? Unfortunately, although this works fine for small images (e.g.,
    Fashion MNIST), it breaks down for larger images because of the huge number of
    parameters it requires. For example, a 100 × 100–pixel image has 10,000 pixels,
    and if the first layer has just 1,000 neurons (which already severely restricts
    the amount of information transmitted to the next layer), this means a total of
    10 million connections. And that’s just the first layer. CNNs solve this problem
    using partially connected layers and weight sharing.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The most important building block of a CNN is the *convolutional layer*:⁠^([6](ch12.html#id2744))
    neurons in the first convolutional layer are not connected to every single pixel
    in the input image (like they were in the layers discussed in previous chapters),
    but only to pixels in their receptive fields (see [Figure 12-2](#cnn_layers_diagram)).
    In turn, each neuron in the second convolutional layer is connected only to neurons
    located within a small rectangle in the first layer. This architecture allows
    the network to concentrate on small low-level features in the first hidden layer,
    then assemble them into larger higher-level features in the next hidden layer,
    and so on. This hierarchical structure is well-suited to deal with composite objects,
    which are common in real-world images: this is one of the reasons why CNNs work
    so well for image recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the structure of CNN layers, highlighting the local
    receptive fields connecting the first convolutional layer to the second through
    the input.](assets/hmls_1202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-2\. CNN layers with rectangular local receptive fields
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: All the multilayer neural networks we’ve looked at so far had layers composed
    of a long line of neurons, and we had to flatten input images to 1D before feeding
    them to the neural network. In a CNN each layer is represented in 2D, which makes
    it easier to match neurons with their corresponding inputs.
  prefs: []
  type: TYPE_NORMAL
- en: A neuron located in row *i*, column *j* of a given layer is connected to the
    outputs of the neurons in the previous layer located in rows *i* to *i* + *f*[*h*]
    – 1, columns *j* to *j* + *f*[*w*] – 1, where *f*[*h*] and *f*[*w*] are the height
    and width of the receptive field (see [Figure 12-3](#slide_and_padding_diagram)).
    In order for a layer to have the same height and width as the previous layer,
    it is common to add zeros around the inputs, as shown in the diagram. This is
    called *zero* *padding*.
  prefs: []
  type: TYPE_NORMAL
- en: It is also possible to connect a large input layer to a much smaller layer by
    spacing out the receptive fields, as shown in [Figure 12-4](#stride_diagram).
    This dramatically reduces the model’s computational complexity. The horizontal
    or vertical step size from one receptive field to the next is called the *stride*.
    In the diagram, a 5 × 7 input layer (plus zero padding) is connected to a 3 ×
    4 layer, using 3 × 3 receptive fields and a stride of 2\. In this example the
    stride is the same in both directions, which is generally the case (although there
    are exceptions). A neuron located in row *i*, column *j* in the upper layer is
    connected to the outputs of the neurons in the previous layer located in rows
    *i* × *s*[*h*] to *i* × *s*[*h*] + *f*[*h*] – 1, columns *j* × *s*[*w*] to *j*
    × *s*[*w*] + *f*[*w*] – 1, where *s*[*h*] and *s*[*w*] are the vertical and horizontal
    strides.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the connection between a 5 × 7 input layer and a 3 ×
    4 layer using 3 × 3 receptive fields with a stride of 2, including zero padding.](assets/hmls_1203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-3\. Connections between layers and zero padding
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Diagram illustrating the concept of reducing dimensionality with a stride
    of 2 on a grid, showing overlapping operations with different colored boxes and
    lines.](assets/hmls_1204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-4\. Reducing dimensionality using a stride of 2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Filters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A neuron’s weights can be represented as a small image the size of the receptive
    field. For example, [Figure 12-5](#filters_diagram) shows two possible sets of
    weights, called *filters* (or *convolution kernels*, or just *kernels*). The first
    one is represented as a black square with a vertical white line in the middle
    (it’s a 7 × 7 matrix full of 0s except for the central column, which is full of
    1s); neurons using these weights will ignore everything in their receptive field
    except for the central vertical line (since all inputs will be multiplied by 0,
    except for the ones in the central vertical line). The second filter is a black
    square with a horizontal white line in the middle. Neurons using these weights
    will ignore everything in their receptive field except for the central horizontal
    line.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram showing the input image processed by vertical and horizontal filters
    to produce two feature maps, each highlighting different line orientations.](assets/hmls_1205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-5\. Applying two different filters to get two feature maps
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In deep learning, we often build a single model that takes the raw inputs and
    produces the final outputs. This is called *end-to-end learning*. In contrast,
    classical vision systems would usually divide the system into a sequence of specialized
    modules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if all neurons in a layer use the same vertical line filter (and the same
    bias term), and you feed the network the input image shown in [Figure 12-5](#filters_diagram)
    (the bottom image), the layer will output the top-left image. Notice that the
    vertical white lines get enhanced while the rest gets blurred. Similarly, the
    upper-right image is what you get if all neurons use the same horizontal line
    filter; notice that the horizontal white lines get enhanced while the rest is
    blurred out. Thus, a layer full of neurons using the same filter outputs a *feature
    map*, which highlights the areas in an image that activate the filter the most.
    But don’t worry, you won’t have to define the filters manually: instead, during
    training the convolutional layer will automatically learn the most useful filters
    for its task, and the layers above will learn to combine them into more complex
    patterns.'
  prefs: []
  type: TYPE_NORMAL
- en: Stacking Multiple Feature Maps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Up to now, for simplicity, I have represented each convolutional layer as a
    2D layer, but in reality a convolutional layer has multiple filters (you decide
    how many) and it outputs one feature map per filter, so the output is more accurately
    represented in 3D (see [Figure 12-6](#cnn_layers_volume_diagram)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating two convolutional layers with multiple filters processing
    a color image with three RGB channels, producing one feature map per filter.](assets/hmls_1206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-6\. Two convolutional layers with multiple filters each (kernels),
    processing a color image with three color channels; each convolutional layer outputs
    one feature map per filter
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There is one neuron per pixel in each feature map, and all neurons within a
    given feature map share the same parameters (i.e., the same kernel and bias term).
    Neurons in different feature maps use different parameters. A neuron’s receptive
    field is the same as described earlier, but it extends across all the feature
    maps of the previous layer. In short, a convolutional layer simultaneously applies
    multiple trainable filters to its inputs, making it capable of detecting multiple
    features anywhere in its inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The fact that all neurons in a feature map share the same parameters dramatically
    reduces the number of parameters in the model. Once the CNN has learned to recognize
    a pattern in one location, it can recognize it in any other location. In contrast,
    once a fully connected neural network has learned to recognize a pattern in one
    location, it can only recognize it in that particular location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input images are also composed of multiple sublayers: one per *color channel*.
    As mentioned in [Chapter 8](ch08.html#unsupervised_learning_chapter), there are
    typically three: red, green, and blue (RGB). Grayscale images have just one channel,
    but some images may have many more—for example, satellite images that capture
    extra light frequencies (such as infrared).'
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, a neuron located in row *i*, column *j* of the feature map *k*
    in a given convolutional layer *l* is connected to the outputs of the neurons
    in the previous layer *l* – 1, located in rows *i* × *s*[*h*] to *i* × *s*[*h*]
    + *f*[*h*] – 1 and columns *j* × *s*[*w*] to *j* × *s*[*w*] + *f*[*w*] – 1, across
    all feature maps (in layer *l* – *1*). Note that, within a layer, all neurons
    located in the same row *i* and column *j* but in different feature maps are connected
    to the outputs of the exact same neurons in the previous layer.
  prefs: []
  type: TYPE_NORMAL
- en: '[Equation 12-1](#convolutional_layer_equation) summarizes the preceding explanations
    in one big mathematical equation: it shows how to compute the output of a given
    neuron in a convolutional layer. It is a bit ugly due to all the different indices,
    but all it does is calculate the weighted sum of all the inputs, plus the bias
    term.'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 12-1\. Computing the output of a neuron in a convolutional layer
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <mrow><msub><mi>z</mi> <mrow><mi>i</mi><mo lspace="0%" rspace="0%">,</mo><mi>j</mi><mo
    lspace="0%" rspace="0%">,</mo><mi>k</mi></mrow></msub> <mo>=</mo> <msub><mi>b</mi>
    <mi>k</mi></msub> <mo>+</mo> <munderover><mo>∑</mo> <mrow><mi>u</mi><mo>=</mo><mn>0</mn></mrow>
    <mrow><msub><mi>f</mi> <mi>h</mi></msub> <mo>-</mo><mn>1</mn></mrow></munderover>
    <munderover><mo>∑</mo> <mrow><mi>v</mi><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi>
    <mi>w</mi></msub> <mo>-</mo><mn>1</mn></mrow></munderover> <munderover><mo>∑</mo>
    <mrow><mi>k</mi><mo>'</mo><mo>=</mo><mn>0</mn></mrow> <mrow><msub><mi>f</mi> <msup><mi>n</mi>
    <mo>'</mo></msup></msub> <mo>-</mo><mn>1</mn></mrow></munderover> <msub><mi>x</mi>
    <mrow><msup><mi>i</mi> <mo>'</mo></msup> <mo lspace="0%" rspace="0%">,</mo><msup><mi>j</mi>
    <mo>'</mo></msup> <mo lspace="0%" rspace="0%">,</mo><msup><mi>k</mi> <mo>'</mo></msup></mrow></msub>
    <mo>×</mo> <msub><mi>w</mi> <mrow><mi>u</mi><mo lspace="0%" rspace="0%">,</mo><mi>v</mi><mo
    lspace="0%" rspace="0%">,</mo><msup><mi>k</mi> <mo>'</mo></msup> <mo lspace="0%"
    rspace="0%">,</mo><mi>k</mi></mrow></msub> <mtext>with</mtext> <mfenced separators=""
    open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><mi>i</mi> <mo>'</mo>
    <mo>=</mo> <mi>i</mi> <mo>×</mo> <msub><mi>s</mi> <mi>h</mi></msub> <mo>+</mo>
    <mi>u</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mi>j</mi> <mo>'</mo>
    <mo>=</mo> <mi>j</mi> <mo>×</mo> <msub><mi>s</mi> <mi>w</mi></msub> <mo>+</mo>
    <mi>v</mi></mrow></mtd></mtr></mtable></mfenced></mrow>
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*z*[*i*,] [*j*,] [*k*] is the output of the neuron located in row *i*, column
    *j* in feature map *k* of the convolutional layer (layer *l*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As explained earlier, *s*[*h*] and *s*[*w*] are the vertical and horizontal
    strides, *f*[*h*] and *f*[*w*] are the height and width of the receptive field,
    and *f*[*n*′] is the number of feature maps in the previous layer (layer *l* –
    1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x*[*i*′,] [*j*′,] [*k*′] is the output of the neuron located in layer *l*
    – 1, row *i*′, column *j*′, feature map *k*′ (or channel *k*′ if the previous
    layer is the input layer).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b*[*k*] is the bias term for feature map *k* (in layer *l*). You can think
    of it as a knob that tweaks the overall brightness of the feature map *k*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w*[*u*,] [*v*,] [*k*′,] [*k*] is the connection weight between any neuron
    in feature map *k* of the layer *l* and its input located at row *u*, column *v*
    (relative to the neuron’s receptive field), and feature map *k*′.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see how to create and use a convolutional layer using PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Convolutional Layers with PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let’s load a couple of sample images using Scikit-Learn’s `load_sample_images()`
    function. The first image represents the tower of buddhist incense in China, while
    the second one represents a beautiful *Dahlia pinnata* flower. These images are
    represented as a Python list of NumPy unsigned byte arrays, so let’s stack these
    images into a single NumPy array, then convert it to a 32-bit float tensor, and
    rescale the pixel values from 0–255 to 0–1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at this tensor’s shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2][PRE3]``py[PRE4]`py` Let’s also use TorchVision’s `CenterCrop` class
    to center-crop the images:    [PRE5]py` `>>>` `cropped_images` `=` `T``.``CenterCrop``((``70``,`
    `120``))(``sample_images_permuted``)` [PRE6]py [PRE7]``py[PRE8]`` [PRE9]`` Now
    let’s create a 2D convolutional layer and feed it these cropped images to see
    what comes out. For this, PyTorch provides the `nn.Conv2d` layer. Under the hood,
    this layer relies on the `torch.nn.((("torch", "F.conv2d()")))functional.conv2d()`
    function. Let’s create a convolutional layer with 32 filters, each of size 7 ×
    7 (using `kernel_size=7`, which is equivalent to using `kernel_size=(7 , 7)`),
    and apply this layer to our small batch of two images:    [PRE10]    ###### Note    When
    we talk about a 2D convolutional layer, “2D” refers to the number of *spatial*
    dimensions (height and width), but as you can see, the layer takes 4D inputs:
    as we saw, the two additional dimensions are the batch size (first dimension)
    and the channels (second dimension).    Now let’s look at the output’s shape:    [PRE11]   [PRE12]`
    The output shape is similar to the input shape, with two main differences. First,
    there are 32 channels instead of 3\. This is because we set `out_channels=32`,
    so we get 32 output feature maps: instead of the intensity of red, green, and
    blue at each location, we now have the intensity of each feature at each location.
    Second, the height and width have both shrunk by 6 pixels. This is due to the
    fact that the `nn.Conv2d` layer does not use any zero-padding by default, which
    means that we lose a few pixels on the sides of the output feature maps, depending
    on the size of the filters. In this case, since the kernel size is 7, we lose
    6 pixels horizontally and 6 pixels vertically (i.e., 3 pixels on each side).    ######
    Warning    By default, the `padding` hyperparameter is set to 0, which means that
    padding is turned off. Oddly, this is also called *valid padding* since every
    neuron’s receptive field lies strictly within *valid* positions inside the input
    (it does not go out of bounds). You can actually set `padding="valid"`, which
    is equivalent to `padding=0`. It’s not a PyTorch naming quirk: everyone uses this
    confusing nomenclature.    If instead we set `padding="same"`, then the inputs
    are padded with enough zeros on all sides to ensure that the output feature maps
    end up with the *same* size as the inputs (hence the name of this option):    [PRE13]``
    `...` [PRE14] `>>>` `fmaps``.``shape` `` `torch.Size([2, 32, 70, 120])` `` [PRE15]`
    [PRE16]   [PRE17] [PRE18]`py [PRE19]py`` [PRE20]py[PRE21][PRE22][PRE23][PRE24]py[PRE25]py
    [PRE26] [PRE27][PRE28]``py[PRE29][PRE30] >>> class_names = weights.meta["categories"]
    `>>>` `[``class_names``[``class_id``]` `for` `class_id` `in` `y_pred``]` `` `[''palace'',
    ''daisy'']` `` [PRE31][PRE32][PRE33] >>> y_top3_logits, y_top3_class_ids = y_logits.topk(k=3,
    dim=1) `>>>` `[[``class_names``[``class_id``]` `for` `class_id` `in` `top3``]`
    `for` `top3` `in` `y_top3_class_ids``]` `` `[[''palace'', ''monastery'', ''lakeside''],
    [''daisy'', ''pot'', ''ant'']]` `` [PRE34]`` [PRE35] >>> y_top3_logits.softmax(dim=1)
    `tensor([[0.8618, 0.1185, 0.0197],`  `[0.8106, 0.0964, 0.0930]], device=''cuda:0'')`
    [PRE36]` [PRE37][PRE38][PRE39][PRE40][PRE41]  [PRE42] [PRE43]`` # Pretrained Models
    for Transfer Learning    If you want to build an image classifier but you do not
    have enough data to train it from scratch, then it is often a good idea to reuse
    the lower layers of a pretrained model, as we discussed in [Chapter 11](ch11.html#deep_chapter).
    In this section we will reuse the ConvNeXt model we loaded earlier—which was pretrained
    on ImageNet—and after replacing its classification head, we will fine-tune it
    on the [*102 Category Flower Dataset*](https://homl.info/flowers102)⁠^([29](ch12.html#id2920))
    (Flowers102 for short). This dataset only contains 10 images per class, and there
    are 102 classes in total (as the name indicates), so if you try to train a model
    from scratch, you will really struggle to get high accuracy. However, it’s quite
    easy to get over 90% accuracy using a good pretrained model. Let’s see how. First,
    let’s download the dataset using Torchvision:    [PRE44]    This code uses `partial()`
    to avoid repeating the same arguments three times. We also set `transform=weights.transforms()`
    to preprocess the images immediately when they are loaded. The Flowers102 dataset
    comes with three predefined splits, for training, validation, and testing. The
    first two have 10 images per class, but surprisingly the test set has many more
    (it has a variable number of images per class, between 20 and 238). In a real
    project, you would normally use most of your data for training rather than for
    testing, but this dataset was designed for computer vision research, and the authors
    purposely restricted the training set and the validation set.    We then create
    the data loaders, as usual:    [PRE45]    Many TorchVision datasets conveniently
    contain the class names in the `classes` attribute, but sadly not this dataset.^([30](ch12.html#id2922))
    If you prefer to see lovely names like “tiger lily”, “monkshood”, or “snapdragon”
    rather than boring class IDs, then you need to manually define the list of class
    names:    [PRE46]    Now let’s adapt our pretrained ConvNeXt-base model to this
    dataset. Since it was pretrained on ImageNet, which has 1,000 classes, the model’s
    head (i.e., its upper layers) was designed to output 1,000 logits. But we only
    have 102 classes, so we must chop the model’s head off and replace it with a smaller
    one. But how can we find it? Well let’s use the model’s `named_children()` method
    to find the name of its submodules:    [PRE47]   [PRE48] >>> model.classifier
    `Sequential(`  `(0): LayerNorm2d((1024,), eps=1e-06, elementwise_affine=True)`  `(1):
    Flatten(start_dim=1, end_dim=-1)`  `(2): Linear(in_features=1024, out_features=1000,
    bias=True)` `)` [PRE49]`As you can see, it’s an `nn.Sequential` module composed
    of a layer normalization layer, an `nn.Flatten` layer, and an `nn.Linear` layer
    with 1,024 inputs and 1,000 outputs. This `nn.Linear` layer is the output layer,
    and it’s the one we need to replace. We must only change the number of outputs:    [PRE50]    As
    explained in [Chapter 11](ch11.html#deep_chapter), it’s usually a good idea to
    freeze the weights of the pretrained layers, at least at the beginning of training.
    We can do this by freezing every single parameter in the model, and then unfreezing
    only the parameters of the head:    [PRE51]    Next, you can train this model
    for a few epochs, and you will already reach about 90% accuracy just by training
    the new head, without even fine-tuning the pretrained layers. After that, you
    can unfreeze the whole model, lower the learning rate—typically by a factor of
    10—and continue training the model. Give this a try, and see what accuracy you
    can reach!    To reach an even higher accuracy, it’s usually a good idea to perform
    some data augmentation on the training images. For this, you can try randomly
    flipping the training images horizontally, randomly rotating them by a small angle,
    randomly resizing and cropping them, and randomly tweaking their colors. This
    must all be done before running the ImageNet normalization step, which you can
    implement using a `Normalize` transform:    [PRE52]    ###### Tip    TorchVision
    comes with an `AutoAugment` transform which applies multiple augmentation operations
    optimized for ImageNet. It generalizes well to many other image datasets, and
    it also offers predefined settings for two other datasets: CIFAR10 and the street
    view house numbers (SVHN) dataset.    Here are some more ideas to continue to
    improve your model’s accuracy:    *   Try other pretrained models.           *   Extend
    the training set: find more flower images and label them.           *   Create
    an ensemble of models, and combine their predictions.           *   Analyze failure
    cases, and see whether they share specific characteristics, such as similar texture
    or color. You can then try to tweak image preprocessing to address these issues.           *   Use
    a learning schedule such as performance scheduling.           *   Unfreeze the
    layers gradually, starting from the top. Alternatively, you can use *differential
    learning rates*: apply a smaller learning rate to lower layers, and a larger learning
    rate to upper layers. You can do this by using parameter groups (see [Chapter 11](ch11.html#deep_chapter)).           *   Explore
    different optimizers and fine-tune their hyperparameters.           *   Try different
    regularization techniques.              ###### Tip    It’s worth spending time
    looking for models that were pretrained on similar images. For example, if you’re
    dealing with satellite images, aerial images, or even raster data such as digital
    elevation models (DEM), then models pretrained on ImageNet won’t help much. Instead,
    check out Microsoft’s *TorchGeo* library, which is similar to TorchVision but
    for geospatial data. For medical images, check out Project MONAI. For agricultural
    images, check out AgML. And so on.    With that, you can start training amazing
    image classifiers on your own images and classes! But there’s more to computer
    vision than just classification. For example, what if you also want to know *where*
    the flower is in a picture? Let’s look at this now.[PRE53]``  [PRE54]` [PRE55]
    # Classification and Localization    Localizing an object in a picture can be
    expressed as a regression task, as discussed in [Chapter 9](ch09.html#ann_chapter):
    to predict a bounding box around the object, a common approach is to predict the
    location of the bounding box’s center, as well as its width and height (alternatively,
    you could predict the horizontal and vertical coordinates of the object’s upper-left
    and lower-right corners). This means we have four numbers to predict. It does
    not require much change to the ConvNeXt model; we just need to add a second dense
    output layer with four units (e.g., on top of the global average pooling layer).
    Here’s a `FlowerLocator` model that adds a localization head to a given base model,
    such as our ConvNeXt model:    [PRE56]py    This locator model has two heads:
    the first outputs class logits, while the second outputs the bounding box. The
    localization head has the same number of inputs as the `nn.Linear` layer of the
    classification head, but it outputs just four numbers. The `forward()` method
    takes a batch of preprocessed images as input and outputs both the predicted class
    logits (102 per image) and the predicted bounding boxes (1 per image). After training
    this model, you can use it as follows:    [PRE57]py    But how can we train this
    model? Well, we saw how to train a model with two or more outputs in [Chapter 10](ch10.html#pytorch_chapter),
    and this one is no different: in this case, we can use the `nn.CrossEntropyLoss`
    for the classification head, and the `nn.MSELoss` for the localization head. The
    final loss can just be a weighted sum of the two. Voilà, that’s all there is to
    it.    Hey, not so fast! We have a problem: the Flowers102 dataset does not include
    any bounding boxes, so we need to add them ourselves. This is often one of the
    hardest and most costly parts of a machine learning project: labeling and annotating
    the data. It’s a good idea to spend time looking for the right tools. To annotate
    images with bounding boxes, you may want to use an open source labeling tool like
    Label Studio, OpenLabeler, ImgLab, Labelme, VoTT, or VGG Image Annotator, or perhaps
    a commercial tool like LabelBox, Supervisely, Roboflow, or RectLabel. Many of
    these are now AI assisted, greatly speeding up the annotation task. You may also
    want to consider crowdsourcing platforms such as Amazon Mechanical Turk if you
    have a very large number of images to annotate. However, it is quite a lot of
    work to set up a crowdsourcing platform, prepare the form to be sent to the workers,
    supervise them, and ensure that the quality of the bounding boxes they produce
    is good, so make sure it is worth the effort. If there are just a few hundred
    or a even a couple of thousand images to label, and you don’t plan to do this
    frequently, it may be preferable to do it yourself: with the right tools, it will
    only take a few days, and you’ll also gain a better understanding of your dataset
    and task.    You can then create a custom dataset (see [Chapter 10](ch10.html#pytorch_chapter))
    where each entry contains an image, a label, and a bounding box. TorchVision conveniently
    includes a `BoundingBoxes` class that represents a list of bounding boxes. For
    example, the following code creates a bounding box for the largest flower in the
    first image of the Flowers102 training set (for now we only consider one bounding
    box per image, but we’ll discuss multiple bounding boxes per image later in this
    chapter):    [PRE58]py    ###### Tip    To visualize bounding boxes, use the `torchvi⁠sion.utils.draw_​bounding_boxes()`
    function. You will first need to convert the bounding boxes to the XYXY format
    using `torchvi⁠sion.​ops.box_convert()`.    The `BoundingBoxes` class is a subclass
    of `TVTensor`, which is a subclass of `torch.​Ten⁠sor`, so you can treat bounding
    boxes exactly like regular tensors, with extra features. Most importantly, you
    can transform bounding boxes using TorchVision’s transforms API v2\. For example,
    let’s use the transform we defined earlier to preprocess this bounding box:    [PRE59]py   [PRE60]`
    # Object Detection    The task of classifying and localizing multiple objects
    in an image is called *object detection*. Until a few years ago, a common approach
    was to take a CNN that was trained to classify and locate a single object roughly
    centered in the image, then slide this CNN across the image and make predictions
    at each step. The CNN was generally trained to predict not only class probabilities
    and a bounding box, but also an *objectness score*: this is the estimated probability
    that the image does indeed contain an object centered near the middle. This is
    a binary classification output; it can be produced by a dense output layer with
    a single unit, using the sigmoid activation function and trained using the binary
    cross-entropy loss.    ###### Note    Instead of an objectness score, a “no-object”
    class was sometimes added, but in general this did not work as well. The questions
    “Is an object present?” and “What type of object is it?” are best answered separately.    This
    sliding-CNN approach is illustrated in [Figure 12-25](#sliding_cnn_diagram). In
    this example, the image was chopped into a 5 × 7 grid, and we see a CNN—the thick
    black rectangle—sliding across all 3 × 3 regions and making predictions at each
    step.  ![Diagram illustrating a sliding CNN approach on a grid over an image of
    pink roses, with colored rectangles indicating regions where predictions are made.](assets/hmls_1225.png)  ######
    Figure 12-25\. Detecting multiple objects by sliding a CNN across the image    In
    this figure, the CNN has already made predictions for three of these 3 × 3 regions:    *   When
    looking at the top-left 3 × 3 region (centered on the red-shaded grid cell located
    in the second row and second column), it detected the leftmost rose. Notice that
    the predicted bounding box exceeds the boundary of this 3 × 3 region. That’s absolutely
    fine: even though the CNN could not see the bottom part of the rose, it was able
    to make a reasonable guess as to where it might be. It also predicted class probabilities,
    giving a high probability to the “rose” class. Lastly, it predicted a fairly high
    objectness score, since the center of the bounding box lies within the central
    grid cell (in this figure, the objectness score is represented by the thickness
    of the bounding box).           *   When looking at the next 3 × 3 region, one
    grid cell to the right (centered on the shaded blue square), it did not detect
    any flower centered in that region, so it predicted a very low objectness score;
    therefore, the predicted bounding box and class probabilities can safely be ignored.
    You can see that the predicted bounding box was no good anyway.           *   Finally,
    when looking at the next 3 × 3 region, again one grid cell to the right (centered
    on the shaded green cell), it detected the rose at the top, although not perfectly.
    This rose is not well centered within this region, so the predicted objectness
    score was not very high.              You can imagine how sliding the CNN across
    the whole image would give you a total of 15 predicted bounding boxes, organized
    in a 3 × 5 grid, with each bounding box accompanied by its estimated class probabilities
    and objectness score. Since objects can have varying sizes, you may then want
    to slide the CNN again across 2 × 2 and 4 × 4 regions as well, to capture smaller
    and larger objects.    This technique is fairly straightforward, but as you can
    see it will often detect the same object multiple times, at slightly different
    positions. Some post-processing is needed to get rid of all the unnecessary bounding
    boxes. A common approach for this is called *non-max suppression* (NMS). Here’s
    how it works:    1.  First, get rid of all the bounding boxes for which the objectness
    score is below some threshold; since the CNN believes there’s no object at that
    location, the bounding box is useless.           2.  Find the remaining bounding
    box with the highest objectness score, and get rid of all the other remaining
    bounding boxes that overlap a lot with it (e.g., with an IoU greater than 60%).
    For example, in [Figure 12-25](#sliding_cnn_diagram), the bounding box with the
    max objectness score is the thick bounding box over the leftmost rose. The other
    bounding box that touches this same rose overlaps a lot with the max bounding
    box, so we will get rid of it (although in this example it would already have
    been removed in the previous step).           3.  Repeat step 2 until there are
    no more bounding boxes to get rid of.              This simple approach to object
    detection works pretty well, but it requires running the CNN many times (15 times
    in this example), so it is quite slow. Fortunately, there is a much faster way
    to slide a CNN across an image: using a *fully convolutional network* (FCN).    ##
    Fully Convolutional Networks    The idea of FCNs was first introduced in a [2015
    paper](https://homl.info/fcn)⁠^([33](ch12.html#id2962)) by Jonathan Long et al.,
    for semantic segmentation (the task of classifying every pixel in an image according
    to the class of the object it belongs to). The authors pointed out that you could
    replace the dense layers at the top of a CNN with convolutional layers. To understand
    this, let’s look at an example: suppose a dense layer with 200 neurons sits on
    top of a convolutional layer that outputs 100 feature maps, each of size 7 × 7
    (this is the feature map size, not the kernel size). Each neuron will compute
    a weighted sum of all 100 × 7 × 7 activations from the convolutional layer (plus
    a bias term). Now let’s see what happens if we replace the dense layer with a
    convolutional layer using 200 filters, each of size 7 × 7, and with `"valid"`
    padding. This layer will output 200 feature maps, each 1 × 1 (since the kernel
    is exactly the size of the input feature maps and we are using `"valid"` padding).
    In other words, it will output 200 numbers, just like the dense layer did; and
    if you look closely at the computations performed by a convolutional layer, you
    will notice that these numbers will be precisely the same as those the dense layer
    produced. The only difference is that the dense layer’s output was a tensor of
    shape [*batch size*, 200], while the convolutional layer will output a tensor
    of shape [*batch size*, 200, 1, 1].    ###### Tip    To convert a dense layer
    to a convolutional layer, the number of filters in the convolutional layer must
    be equal to the number of units in the dense layer, the filter size must be equal
    to the size of the input feature maps, and you must use `"valid"` padding. The
    stride may be set to 1 or more, as we will see shortly.    Why is this important?
    Well, while a dense layer expects a specific input size (since it has one weight
    per input feature), a convolutional layer will happily process images of any size⁠^([34](ch12.html#id2967))
    (however, it does expect its inputs to have a specific number of channels, since
    each kernel contains a different set of weights for each input channel). Since
    an FCN contains only convolutional layers (and pooling layers, which have the
    same property), it can be trained and executed on images of any size!    For example,
    suppose we’d already trained a CNN for flower classification and localization,
    with an extra head for objectness. It was trained on 224 × 224 images, and it
    outputs 107 values per image:    *   The classification head outputs 102 class
    logits (one per class), trained using the `nn.CrossEntropyLoss`.           *   The
    objectness head outputs a single objectness logit, trained using the `nn.BCELoss`.           *   The
    localization head outputs four numbers describing the bounding box, trained using
    the CIoU loss.              We can now convert the CNN’s dense layers (`nn.Linear`)
    to convolutional layers (`nn.Conv2d`). In fact, we don’t even need to retrain
    the model; we can just copy the weights from the dense layers to the convolutional
    layers! Alternatively, we could have converted the CNN into an FCN before training.    Now
    suppose the last convolutional layer before the output layer (also called the
    bottleneck layer) outputs 7 × 7 feature maps when the network is fed a 224 × 224
    image (see the left side of [Figure 12-26](#fcn_diagram)). For example, this would
    be the case if the network contains 5 layers with stride 2 and `"same"` padding,
    so the spatial dimensions get divided by 2⁵ = 32 overall. If we feed the FCN a
    448 × 448 image (see the righthand side of [Figure 12-26](#fcn_diagram)), the
    bottleneck layer will now output 14 × 14 feature maps. Since the dense output
    layer was replaced by a convolutional layer using 107 filters of size 7 × 7, with
    `"valid"` padding and stride 1, the output will be composed of 107 feature maps,
    each of size 8 × 8 (since 14 – 7 + 1 = 8).    In other words, the FCN will process
    the whole image only once, and it will output an 8 × 8 grid where each cell contains
    the predictions for one region of the image: 107 numbers representing 102 class
    probabilities, 1 objectness score, and 4 bounding box coordinates. It’s exactly
    like taking the original CNN and sliding it across the image using 8 steps per
    row and 8 steps per column. To visualize this, imagine chopping the original image
    into a 14 × 14 grid, then sliding a 7 × 7 window across this grid; there will
    be 8 × 8 = 64 possible locations for the window, hence 8 × 8 predictions. However,
    the FCN approach is *much* more efficient, since the network only looks at the
    image once. In fact, *You Only Look Once* (YOLO) is the name of a very popular
    object detection architecture, which we’ll look at next.  ![A diagram illustrating
    a fully convolutional network processing a small and a large image, showing the
    progression from CNN layers to feature maps and convolution outputs.](assets/hmls_1226.png)  ######
    Figure 12-26\. The same fully convolutional network processing a small image (left)
    and a large one (right)    ## You Only Look Once    YOLO is a fast and accurate
    object detection architecture proposed by Joseph Redmon et al. in a [2015 paper](https://homl.info/yolo).⁠^([35](ch12.html#id2972))
    It is so fast that it can run in real time on a video, as seen in Redmon’s [demo](https://homl.info/yolodemo2).
    YOLO’s architecture is quite similar to the one we just discussed, but with a
    few important differences:    *   For each grid cell, YOLO only considers objects
    whose bounding box center lies within that cell. The bounding box coordinates
    are relative to that cell, where (0, 0) means the top-left corner of the cell
    and (1, 1) means the bottom right. However, the bounding box’s height and width
    may extend well beyond the cell.              *   It outputs two bounding boxes
    for each grid cell (instead of just one), which allows the model to handle cases
    where two objects are so close to each other that their bounding box centers lie
    within the same cell. Each bounding box also comes with its own objectness score.           *   YOLO
    also outputs a class probability distribution for each grid cell, predicting 20
    class probabilities per grid cell since YOLO was trained on the PASCAL VOC dataset,
    which contains 20 classes. This produces a coarse *class probability map*. Note
    that the model predicts one class probability distribution per grid cell, not
    per bounding box. However, it’s possible to estimate class probabilities for each
    bounding box during post-processing by measuring how well each bounding box matches
    each class in the class probability map. For example, imagine a picture of a person
    standing in front of a car. There will be two bounding boxes: one large horizontal
    one for the car, and a smaller vertical one for the person. These bounding boxes
    may have their centers within the same grid cell. So how can we tell which class
    should be assigned to each bounding box? Well, the class probability map will
    contain a large region where the “car” class is dominant, and inside it there
    will be a smaller region where the “person” class is dominant. Hopefully, the
    car’s bounding box will roughly match the “car” region, while the person’s bounding
    box will roughly match the “person” region: this will allow the correct class
    to be assigned to each bounding box.              YOLO was originally developed
    using Darknet, an open source deep learning framework initially developed in C
    by Joseph Redmon, but it was soon ported to PyTorch and other libraries. It has
    been continuously improved over the years, initially by Joseph Redmon et al. (YOLOv2,
    YOLOv3, and YOLO9000), then by various other teams since 2020\. Each version brought
    some impressive improvements in speed and accuracy, using a variety of techniques;
    for example, YOLOv3 boosted accuracy in part thanks to *anchor priors*, exploiting
    the fact that some bounding box shapes are more likely than others, depending
    on the class (e.g., people tend to have vertical bounding boxes, while cars usually
    don’t). They also increased the number of bounding boxes per grid cell, they trained
    on different datasets with many more classes (up to 9,000 classes organized in
    a hierarchy in the case of YOLO9000), they added skip connections to recover some
    of the spatial resolution that is lost in the CNN (we will discuss this shortly
    when we look at semantic segmentation), and much more. There are many variants
    of these models too, such as scaled down “tiny” YOLOs, optimized to be trained
    on less powerful machines and which can run extremely fast (at over 1,000 frames
    per second!), but with a slightly lower *mean average precision* (mAP).    TorchVision
    does not include any YOLO model, but you can use the Ultralytics library, which
    provides a simple API to download and use various pretrained YOLO models, based
    on PyTorch. These models were pretrained on the COCO dataset which contains over
    330,000 images, including 200,000 images annotated for object detection with 80
    different classes (person, car, truck, bicycle, ball, etc.). The Ultralytics library
    is not installed on Colab by default, so we must run `%pip install ultralytics`.
    Then we can download a YOLO model and use it. For example, here is how to use
    this library to download the YOLOv9 model (medium variant) and detect objects
    in a batch of images (the model accepts PIL images, NumPy arrays, and even URLs):    [PRE61]    The
    output is a list of `Results` objects which offers a handy `summary()` method.
    For example, here is how we can see the first detected object in the first image:    [PRE62]   ``######
    Tip    The Ultralytics library also provides a simple API to train a YOLO model
    on other common object detection datasets, or on your own dataset. See [*https://docs.ultralytics.com/modes/train*](https://docs.ultralytics.com/modes/train)
    for more details.    Several other pretrained object detection models are available
    via TorchVision. You can use them just like the pretrained classification models
    (e.g., ConvNeXt), except that each image prediction is a represented as a dictionary
    containing two entries: `"labels"` (i.e., class IDs) and `"boxes"`. The available
    models are listed here (see the [models page](https://pytorch.org/vision/main/models)
    for the full list of variants available):    [Faster R-CNN](https://homl.info/fasterrcnn)⁠^([36](ch12.html#id2982))      This
    model has two stages: the image first goes through a CNN, then the output is passed
    to a *region proposal network* (RPN) that proposes bounding boxes that are most
    likely to contain an object; a classifier is then run for each bounding box, based
    on the cropped output of the CNN.      [SSD](https://homl.info/ssd)⁠^([37](ch12.html#id2987))      SSD
    is a single-stage detector (“look once”) similar to YOLO.      [SSDlite](https://homl.info/ssdlite)⁠^([38](ch12.html#id2989))      A
    lightweight version of SSD, well suited for mobile devices.      [RetinaNet](https://homl.info/retinanet)⁠^([39](ch12.html#id2991))      A
    single-stage detector which introduced a variant of the cross-entropy loss called
    the *focal loss* (see `torchvision.ops.sigmoid_focal_loss()`). This loss gives
    more weight to difficult samples and thereby improves performance on small objects
    and less frequent classes.      [FCOS](https://homl.info/fcos)⁠^([40](ch12.html#id2996))      A
    single-stage fully convolutional net which directly predicts bounding boxes without
    relying on anchor boxes.      So far, we’ve only considered detecting objects
    in single images. But what about videos? Objects must not only be detected in
    each frame, they must also be tracked over time. Let’s take a quick look at object
    tracking now.``  [PRE63] my_video = "https://homl.info/cars.mp4" results = model.track(source=my_video,
    stream=True, save=True) for frame_results in results:     summary = frame_results.summary()  #
    similar summary as earlier + track id     track_ids = [obj["track_id"] for obj
    in summary]     print("Track ids:", track_ids) [PRE64]` [PRE65][PRE66][PRE67][PRE68][PRE69]
    [PRE70]`py` [PRE71] [PRE72][PRE73]'
  prefs: []
  type: TYPE_NORMAL
