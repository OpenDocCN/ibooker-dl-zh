["```py\nfrom langchain_community.tools import DuckDuckGoSearchRun\n\nquery = \"What's the weather today in Toronto?\"\n\nsearch_engine = DuckDuckGoSearchRun()\noutput = search_engine.run(query)\n```", "```py\n!pip install wikipedia\n\nfrom langchain.tools import WikipediaQueryRun\nfrom langchain_community.utilities import WikipediaAPIWrapper\nwikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n\noutput = wikipedia.load(\"Winter Olympics\")\n```", "```py\nfrom langchain_experimental.utilities import PythonREPL\n\npython = PythonREPL()\npython.run(\"456 * 345\")\n```", "```py\nimport sqlalchemy as sa\nfrom langchain_community.utilities import SQLDatabase\n\nDATABASE_URI = <database_uri>\n\ndb = SQLDatabase.from_uri(DATABASE_URI)\n\noutput = db.run(\n    \"SELECT * FROM COMPANIES WHERE Name LIKE :comp;\",\n    parameters={\"comp\": \"Apple%\"},\n    fetch=\"all\")\n```", "```py\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.agents import AgentType\nfrom langchain_community.tools import DuckDuckGoSearchRun\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.messages import HumanMessage\n\nsearch_engine = DuckDuckGoSearchRun()\nmodel = ChatOpenAI(model=\"gpt-4o\")\n\ntools = [\n       Tool(\n           name=\"Search\",\n           func=search_engine.run,\n           description=\"search engine for answer factual queries\"\n       )\n   ]\nagent = initialize_agent(tools, model, verbose=True)\nagent.run(\"What are some tourist destinations in North Germany?\")\n```", "```py\n<|begin_of_text|><|start_header_id|>system<|end_header_id|>\nEnvironment: ipython\nTools: brave_search, wolfram_alpha\n\nGive responses to answers in a concise fashion. <|eot_id|>\n```", "```py\n<|start_header_id|>user<|end_header_id|>\n\nHow many medals did Azerbaijan win in the 2024 Summer Olympics?\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n```", "```py\n<|python_tag|>brave_search.call(query=\"How many medals did Azerbaijan win in `the` `2024` `Summer` `Olympics``?``\")<|eom_id|>`\n```", "```py`` ```", "```py <|start_header_id|>user<|end_header_id|>  Here is a list of tools available. While invoking a tool, respond in JSON. The format is as follows:  {\"tool_name\": tool name, \"arguments\": dictionary with keys representing  argument names and values representing argument values}.  {     \"type\": \"local_function\",     \"function\": {     \"name\": \"find_citations\",     \"description\": \"Find the citations for any claims made\",     \"parameters\": {         \"type\": \"object\",         \"properties\": {         \"claim_sentence\": {             \"type\": \"string\",             \"description\": \"A sentence in the input representing a claim\"         },         \"model\": {             \"type\": \"string\",             \"enum\": [\"weak\", \"strong\"],             \"description\": \"The type of citation model to use. A weak model is `preferred` `if` `the` `claim` `sentence` `contains` `entities` `and` `numbers``.` `\"` ```", "```py ```", "```py` ```", "```py```", "````` ```py` ## Data Stores    A typical agent may need to interact with several types of data sources to accomplish its tasks. Commonly used data sources include prompt repositories, session memory, and tools data.    ### Prompt repository    A prompt repository is a collection of detailed prompts instructing the language model how to perform a specific task. If you can anticipate the types of tasks that an agent will be asked to perform while in production, you can construct prompts providing detailed instructions on how to solve them. The prompts can even include directions on how to advance a specific workflow. Let’s look at an example.    Many language models struggle with basic arithmetic operations, even simple questions like:    ``` Is 9.11 greater than 9.9? ```py    Until recently, even state-of-the-art language models claimed that 9.11 is greater than 9.9\\. (They were recently updated with a fix after this limitation went viral on [social media](https://oreil.ly/ztWGW).)    If you are aware of such limitations that are relevant to your use case, then you can mitigate a proportion of them using detailed prompts. For the number comparison issue, for example:    > *Prompt:* If you are asked to compare two numbers using the greater than/lesser than operation, then perform the following: >  > Take the two numbers and ensure they have the same number of decimal places. After that, subtract one from the other. If the result is a positive number, then the first number is greater. If the result is a negative number, then the second number is greater. If the result is zero, the two numbers are equal.    Now, if the agent needs to perform a task that includes number comparison, it first retrieves this prompt from the prompt repository. This enables it to overcome its inherent limitation, as it will follow the detailed step-by-step instructions in the prompt.    ###### Note    Why don’t we just add all these prompts to the context window, thus eschewing retrieval? For one, the prompts may be too numerous and may not fit within the context window. Secondly, tokens are expensive, and it is inefficient to include prompts that may not be relevant to the current task. Finally, language models can adhere to only a limited set of concurrent instructions, so it is more efficient to retrieve them on demand.    Prompts can also include input-output examples, known as few-shot learning, as introduced in [Chapter 1](ch01.html#chapter_llm-introduction). Agents can retrieve them on demand to help accomplish their tasks. We will discuss effective ways of retrieving relevant examples in [Chapter 12](ch12.html#ch12).    Next, let’s explore how agents use session memory to advance their goals.    ### Session memory    We would like to store logs of the steps undertaken by the agent during current and past sessions. These logs can also be augmented with feedback from verification processes, both automated and human. The agent can then retrieve session logs for better context understanding and as a guide to determine next actions in the current workflow. Let’s explore this with an example:    > *Prompt:* Is it going to be warmer tomorrow than today in Paris? Give me details in Celsius.    The agent goes through a sequence of steps to solve this query, which is all logged in the session history. The session history for this query might look like this:    > *User prompt*: Is it going to be warmer tomorrow than today in Paris? Give me details in Celsius. >  > *LLM observation*: The user has a preference for Celsius units for temperature data. >  > *LLM Output*: Let’s perform this step-by-step. >  > 1.  First, let’s retrieve today’s temperature. >      >      > 2.  Next, let’s retrieve tomorrow’s temperature. >      >      > 3.  We calculate the difference and generate the answer. >      >      >  > *External verifier*: Task decomposition is correct >  > *Tool selection:* <WeatherAPI> selected >  > *Tool info retrieval*: Retrieve information relevant to the query *Find temperature data from <WeatherAPI>* >  > *Tool invocation*: get_temperature_curweek(*Paris*) >  > *Tool output* : Weather data - Paris…​ >  > *LLM output*: Today’s temperature is 27 degrees Celsius. Tomorrow’s forecast is 23 degrees Celsius. Therefore, tomorrow is not going to be warmer in Paris than today. >  > *External Verifier*: Arithmetic operation is correct. >  > *Agent*: LLM output is dispatched to the user >  > *User feedback*: User marked this as correct    As we can see, session history can contain very rich information that can provide valuable personalized context to the LLM about the current user as well as guide the model toward the correct agentic workflow.    In more advanced implementations, multiple levels of logging can be defined, so that during retrieval, one can retrieve all the logs of a session or only the important steps, based on the logging level specified.    ###### Tip    Along with session history, the agent could also be provided with access to gold-truth training examples representing correct workflows, which can be used by the agent to guide its trajectory during test time.    Session memory can also include records of interaction between the human and the agentic system. These can be used to personalize models. We will discuss this further in [Chapter 12](ch12.html#ch12).    Next, let’s explore how the agent can interact with tools data.    ### Tools data    Tools data comprise detailed information necessary to invoke a tool, such as database schemas, API documentation, sample API calls, and more. When the agent decides to invoke a tool, the model retrieves the pertinent tool information from the tools data store.    For example, consider a SQL tool for retrieving data from a database. To generate the right SQL query, the model could retrieve the database schema from the tools data store. The tools data contains information about the tables and columns, the descriptions of each column and their data types, and optionally information about indices and primary/secondary keys.    ###### Note    You can also fine-tune the LLM on a dataset representing valid SQL queries to your database, which can potentially remove the need to consult the schema before generating a query.    To sum it up, agents can use data stores in several ways. They can access prompts and few-shot examples from a prompt repository, they can access agentic workflow history and intermediate outputs by models in previous sessions for better personalized context understanding and workflow guidance, and they can access tool documentation to invoke tools correctly.    Agents can also access external knowledge from the web, databases, knowledge graphs, etc. Retrieving the right information from these sources is an entire sub-system unto itself. We will discuss the mechanics of retrieval in Chapters [11](ch11.html#chapter_llm_interfaces) and [12](ch12.html#ch12).    We will now discuss the agent loop prompt, which is responsible for driving the LLM’s behavior during an agentic session.    ## Agent Loop Prompt    Recall that LLMs do not have session memory. But a typical agentic workflow relies on several LLM calls! We need a mechanism to provide information about session state and the expected role of the LLM at any given time in the session. This agent loop is driven by a system prompt.    An example of a simple agent loop system prompt is:    > *Prompt:* You are an AI model currently answering questions. You have access to the following tools: {tool_description}. For each question, you can invoke one or more tools where necessary to access information or execute actions. You can invoke a tool in this format: <TOOLNAME> <Tool Arguments>. The results of these tool calls are not provided to the user. When you are ready with the final answer, output the answer using the <Answer> tag.    I find that a prompt like this is sufficient for most use cases. However, if you feel like the model is not reasoning correctly, you can try ReAct prompting.    ### ReAct    At the time of this writing, ReAct (Reasoning + Acting) prompting is the most popular prompt for the agent loop. A typical ReAct prompt looks like this:    > *Prompt:* You are an AI assistant capable of reasoning and acting. For each question, follow this process: >  > 1.  Thought: Reflect on the current state and plan your next steps. >      >      > 2.  Action: Execute the steps to gather information or call tools. >      >      > 3.  Observation: Record the results of your actions. >      >      > 4.  Final Answer: If you have an answer, provide a final response. Else continue the Thought → Action → Observation → loop until you have an answer.    Despite its popularity, ReAct prompting has been shown to be [brittle](https://oreil.ly/RRZO9).    ### Reflection    The agent loop may include self-verification or correction steps. This was pioneered by [Shinn et al.](https://oreil.ly/xFVt0) with the Reflexion paradigm.    Here is the system prompt for [Reflection-Llama-3.1](https://oreil.ly/foB-P) that uses reflection techniques:    > *Prompt:* You are a world-class AI system, capable of complex reasoning and reflection. Reason through the query inside <thinking> tags, and then provide your final response inside <output> tags. If you detect that you made a mistake in your reasoning at any point, correct yourself inside <reflection> tags.    The <reflection> tags are meant for the model to self-introspect and self-correct. We can also specify conditions when <reflection> tags should be activated, for example, when the agent performs the same action consecutively more than three times (which might mean it is stuck in a loop).    ###### Warning    The effectiveness of reflection-based methods are overstated. They might do more harm than good if they are invoked too often, causing the model to second-guess solutions.    Next, let’s discuss guardrails and verifiers, components that ensure that an agentic system can thrive in production.    ## Guardrails and Verifiers    In production environments, mistakes can be catastrophic. Depending on the use case, the agent might need to adhere to strict standards in factuality, safety, accuracy, and many other criteria.    Safety is ensured by using guardrails, components that ensure models do not overstep their bounds during the course of their workflows. Some examples of guardrails include toxic language detectors, personally identifiable information (PII) detectors, input filters that restrict the type of queries users are permitted to make, and more.    Verifiers ensure that quality standards of the agentic system are so that the agent is able to recover and self-correct from mistakes. As agentic systems are still in their infancy, the importance of good and well-placed verifiers is paramount. Verifiers can be as simple as token-matching tools but can also be fine-tuned models, symbolic verifiers, and so on.    Let’s learn more about guardrails and verifiers.    ### Safety Guardrails    Recall from [Chapter 2](ch02.html#ch02) that LLMs are trained largely on human-generated web text. Unfortunately a significant proportion of human-generated text contains toxic, abusive, violent, or pornographic content. We do not want our LLM applications to generate content that violates the safety of the user, nor do we want users to misuse the model to generate unsafe content. While we can certainly use techniques like alignment training to make the model less likely to emit harmful content, we cannot guarantee 100% success and therefore need to institute inference-time guardrails to ensure safe usage. Libraries like [Guardrails](https://oreil.ly/F7yax) and NVIDIA’s [NeMo-Guardrails](https://oreil.ly/p7Dqz), and models like [Llama Guard](https://oreil.ly/8S08P) facilitate setting up these guardrails.    The Guardrails library provides a large (and growing) number of data validators to ensure safety and validity of LLM inputs and outputs. Here are some important ones:    Detect PII      This validator can be used to detect personally identifiable information in both the input and output text. [Microsoft Presidio](https://oreil.ly/eG8T1) is employed under the hood to perform the PII identification.      Prompt injection      This validator can detect certain types of adversarial prompting and thus can be used to prevent users from misusing the LLM. The [Rebuff](https://oreil.ly/nIyE5) library is used under the hood to detect prompt injection.      Not safe for work (NSFW) text      This validator detects NSFW text in the LLM output. This includes text with profanity, violence, and sexual content. The *Profanity free* validator also exists for detecting only profanity in text.      Politeness check      This validator checks if the LLM output text is sufficiently polite. A related validator is *Toxic language*.      Web sanitization      This validator checks the LLM output for any security vulnerabilities, including if it contains code that can be executed in a browser. The [Bleach](https://oreil.ly/r3Xrl) library is used under the hood to find potential vulnerabilities and sanitize the output.      What happens if the validation checks fail and there is indeed harmful content in the input or output? Guardrails provides a few options:    Re-ask      In this method, the LLM is asked to regenerate the output, with the prompt containing instructions to specifically abide by the criteria on which the output previously failed validation.      Fix      In this method, the library fixes the output by itself without asking the LLM for a regeneration. Fixes can involve deletion or replacement of certain parts of the input or output.      Filter      If structured data generation is used, this option enables filtering out only the attribute for which the validation failed. The rest of the output will be fed back to the user.      Refrain      In this setting, the output is simply not returned to the user, and the user receives a refusal.      Noop      No action is taken, but the validation failure is logged for further inspection.      Exception      This raises a software exception when the validation fails. Exception handlers can be written to activate custom behavior.      fix_reask      In this method, the library tries to fix the output by itself and then runs validation on the new output. If the validation still fails, then the LLM is asked to regenerate the output.      Let’s look at the PII guardrail as an example:    ``` from guardrails import Guard from guardrails.hub import DetectPII  guard = Guard().use(     DetectPII, [\"EMAIL_ADDRESS\", \"PHONE_NUMBER\"], \"reask\")  guard.validate(\"The Nobel prize this year was won by Geoff Hinton, `who` `can` `be` `reached` `at` `+``1` `234` `567` `8900``\")` ```py   `` `Next, let’s look at how verification modules work.` ``  `` `### Verification modules    As we have seen throughout the book, current LLMs suffer from problems like reasoning limitations and hallucinations that severely limit their robustness. However, production-ready applications need to demonstrate a certain level of reliability to be accepted by users. One way to extend the reliability of LLM-based systems is to use a human-in-the-loop who can manually verify the output and provide feedback. However, in the real world a human-in-the-loop is not always desired or feasible. The most popular alternative is to use external verification modules as part of the LLM system. These modules can range from rule-based programs to smaller fine-tuned LLMs to symbolic solvers. There are also efforts to use LLMs as verifiers, called “LLM-as-a-judge.”    Related components include fallback modules. These modules are activated when the verification process fails and retrying/fixing doesn’t work. Fallback modules can be as simple as messages like, “I am sorry I cannot entertain your request” to more complex workflows.    Let’s discuss an example. Consider an abstractive summarization application that operates on financial documents. To ensure quality and reliability of the generated summaries, we need to embed verification and self-fixing into the system architecture.    How do we verify the quality of an abstractive summary? While single-number metrics are available to automatically quantify the quality of a summary, a more holistic approach would be to define a list of criteria that a good summary should satisfy and verify whether each criterion is fulfilled.    ###### Note    Several single-number quantitative metrics exist for evaluating summaries. These include metrics like [BLEU, ROUGE](https://oreil.ly/LPlFJ), and [BERTScore](https://oreil.ly/gsOGl). BLEU and ROUGE rely on token overlap heuristics and have been shown to be [woefully inadequate](https://oreil.ly/rSzbR). Techniques like BERTScore that apply semantic similarity have been shown to be more promising, but in the end, the reality is that summaries have subjective notions of quality and need a more holistic approach for verification.    For the summarization of financial documents application, here is a list of important criteria:    Factuality      The summary is factually correct and does not make incorrect assumptions or conclusions from the source text.      Specificity      The summary doesn’t *oversummarize*; it avoids being generic and provides specific details, whether numbers or named entities.      Relevance      Also called precision, this is calculated as the percentage of sentences in the summary that are deemed relevant and thus merit inclusion in the summary.      Completeness      Also called recall, this is calculated as the percentage of relevant items in the source document that are included in the summary.      Repetitiveness      The summary should not be repetitive, even if there is repetition in the source document.      Coherence      When read in full, the summary should provide a clear picture of the content in the source document, while minimizing ambiguity. This is one of the list’s more subjective criteria.      Structure      While defining the summarization task, we might specify a structure for the summaries. For example, the summary could be expected to contain some predefined sections and subsections. The generated summary should follow the specified structure.      Formatting      The generated summary should follow proper formatting. For example, if the summary is to be generated as a bulleted list, then all the items in the summary should be represented by bullets.      Ordering      The ordering of the items in the summary should not impede the understanding of the summary content. We also might want to specify an order for the summaries, for example, chronological.      Error handling      In case of errors or omissions in the source document, there should be appropriate error handling.      How do we automatically verify whether a given summary meets all these criteria? We can use a combination of rule-based methods and fine-tuned models. Ultimately, the rigor of the methods used for verification depends on the degree of reliability needed for your application. However, we notice that once we reduce the scope of the verification process to verify fitness of individual criteria rather than the application as a whole, it becomes easier to verify accurately using inexpensive techniques. Let’s look at how we can build verifiers for each criteria of the abstractive summarization task:    Factuality      Verifying whether an LLM-generated statement is factual is extremely difficult if we do not have access to ground truth. But for summarization applications, we do have access to the ground truth. Therefore, we can verify factuality by taking each sentence in the summary and checking whether, given the source text, one can logically conclude the statement in the summary. This can be framed as a natural language inference (NLI) problem, which is a standard NLP task.    In the NLI task, we have a hypothesis and a premise, and the goal is to check if the hypothesis is logically entailed by the premise. In our example, the hypothesis is a sentence in the summary and the premise is the source text.    Training an NLI model specific to your domain might be a cumbersome task. If you do not have access to an NLI model, you can use token overlap and similar statistics to approximate factuality verification.    For numbers and named entities, factuality verification can be performed by using string matches. You can verify if all the numbers and named entities in the summary are indeed present in the source text.      Specificity      One way for a summary to be specific is to include numbers and named entities where relevant. For each sentence in the summary, we can check whether the content in the source document related to the topic of the sentence contains any numbers and named entities, and if these are reflected in the summary. Numbers and named entities can be tagged and detected using regular expressions or libraries like [spaCy](https://oreil.ly/zatAW).      Relevance/precision      We can train a classification model that detects whether a sentence in the summary is relevant. Note that there are limits to this approach. If this classification model was good enough, we could have directly used it to select relevant sentences from the source text to build the summary! In practice, this classification model can be used to remove irrelevant content that is more obvious.      Recall/completeness      What content merits inclusion in the summary is a difficult question, especially if there is a hard limit on the summary length. You can train a ranking model that ranks sentences in the source document by importance, and then verify if the top-ranked sentences are represented in the summary. You can also specify beforehand the type of content that you need represented in the summary and build a classification model for determining which parts of the source document contain pertinent information. Using similarity metrics like embedding similarity, you can then find if the content has been adequately represented in the summary.      Repetitiveness      This can be discovered by using string difference algorithms like the [Jaccard distance](https://oreil.ly/Ny_Ku) or by calculating the embedding similarity between pairs of summary sentences.      Coherence      This is perhaps one of the most difficult criteria to verify. One way to solve this, albeit a more expensive solution, is to build a prerequisite detection model. For each sentence in the summary, we detect if all the sentences that come before it are sufficient prerequisites for understanding the correct sentence. For more information on prerequisite detection techniques, see [Thareja et al.](https://oreil.ly/6JnRs)      Structure      If we specify a predetermined structure (sections and subsections) for the summary, we can easily identify if the structure is adhered to by checking if the desired section and subsection titles are present in the summary. We can also verify using embedding similarity techniques if the content within the sections and subsections is faithful to the title of the section/subsection.      Formatting      This involves checking whether the content is in the appropriate formatting, for example, whether it is a bulleted list or a valid JSON object.      Ordering      The desired order can be chronological, alphabetical, a domain, or task-specific ordering. If it is supposed to be chronological, you can verify by extracting dates in the summary and checking if the summary contains dates in a chronological order. If the ordering requirements are more complex, then verifying adherence to order may become an extremely difficult task.      ###### Tip    Do not expect your verification process to be strictly better than your summary model. If that was the case, you could have used the verification process to generate the summary!    We can also deploy symbolic verifiers like [SAT](https://oreil.ly/lOsg_) (Boolean satisfiability) solvers and logic planners. This type of verification is beyond the scope of this book.    Once verification modules are part of our system architecture, we will also need to decide what action to perform when the verification fails. One option is to just resample from the language model again. Regeneration can be performed for the full output or only for the output that failed verification. We can also develop antifragile architectures that have fallbacks in case of failure, which we will discuss in [Chapter 13](ch13.html#ch13).    ###### Warning    Adding more verifiers can drastically increase system latency. Thus, their inclusion has to be balanced with accuracy and system latency needs.    Finally, let’s discuss agent orchestration software that connects all these components.` ``  `` `## Agent Orchestration Software    For agentic workflows to proceed smoothly, we need software that connects all the components. Orchestration software manages state; invokes tools; initiates retrieval; pipes buffers; and logs intermediate and final outputs. Many agentic frameworks, both open source and proprietary, perform this function, including [LangChain](https://oreil.ly/7vmlY), [LlamaIndex](https://oreil.ly/uxejK), [CrewAI](https://oreil.ly/Ntxii), [AutoGen](https://oreil.ly/tx3qy), [MetaGPT](https://oreil.ly/HI-Jn), [XAgent](https://oreil.ly/sA_DR), [llama-stack-apps](https://oreil.ly/SBGC_), and so on.    ###### Tip    Agents are a relatively new paradigm, so all these agentic frameworks are expected to change a lot in the coming months and years. These frameworks are implemented in an opinionated fashion and hence are less flexible. For prototyping, I suggest picking LangChain or LlamaIndex for ease of use. For production use, you might want to build a framework internally from scratch or by extending the open source ones. This book’s [GitHub repo](https://oreil.ly/llm-playbooks) contains a rudimentary agentic framework as well.    Now that we have learned all the different agentic system components, it is time to get building! The book’s [GitHub repository](https://oreil.ly/llm-playbooks) contains sample implementations of various types of agents. Try modifying them for your use case to understand the tradeoffs being made.    ###### Tip    The keep it simple, stupid (KISS) principle applies to agents perhaps more than any other recent paradigm. Don’t complicate your agentic architecture unless there is a compelling reason to do so. We will discuss this more in [Chapter 13](ch13.html#ch13).` `` ```` ```py`` `````", "``` `` `# Summary    In this chapter, we discussed the different ways in which LLMs can interface with external tools. We introduced the agentic paradigm and provided a formal definition of agents. We identified the components of an agentic system in detail, exploring models, tools, data stores, guardrails and verifiers, and agentic orchestration software. We learned how to define and implement our own tools.    In the next chapter, we will explore data representation and retrieval, crucial elements of interfacing LLMs with external data.` `` ```"]