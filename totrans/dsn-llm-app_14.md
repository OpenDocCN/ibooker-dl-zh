# Chapter 11\. Representation Learning and Embeddings

In the previous chapter, we learned how we can interface language models with external tools, including data stores. External data can be present in the form of text files, database tables, and knowledge graphs. Data can span a wide variety of content types, from proprietary domain-specific knowledge bases to intermediate results and outputs generated by LLMs.

If the data are structured, for example residing in a relational database, the language model can issue a SQL query to retrieve the data it needs. But what if the data are present in unstructured form?

One way to retrieve data from unstructured text datasets is to search by keywords or use regular expressions. For the Apple CFO example in the previous chapter, we can retrieve text containing CFO mentions from a corpus containing financial disclosures, hoping that it will contain the join date or tenure information. For instance, you can use the regex:

```py
pattern = r"(?i)\b(?:C\.?F\.?O|Chief\s+Financial\s+Officer)\b"
```

Keyword search is limited in its effectiveness. There are a very large number of ways to express CFO join date or tenure in a corpus, if it is present at all. Trying to use a catch-all regex like the above could result in a large proportion of false positives.

Therefore, we need to move beyond keyword search. Over the last few decades, the field of information retrieval has developed several methods like BM25 that have shaped search systems. We will learn more about these methods in [Chapter 12](ch12.html#ch12). In the LLM era, embedding-based search systems are fast becoming the standard way of implementing search.

In this chapter, we will learn how embeddings work. We will explore the concept of semantic similarity and examine various similarity measures. We will learn how to use popular embedding models and evaluate their performance. We will also show how to fine-tune embedding models to suit specific use cases and domains. We will show how to interpret these embeddings using sparse autoencoders (SAEs). Finally, we will discuss techniques for optimizing embeddings to reduce storage requirements and computational overhead.

# Introduction to Embeddings

Representation learning is a subfield of machine learning that deals with learning to represent data in a way that captures its meaningful features, often in a low dimensional space. In the context of NLP, this involves transforming textual units like words, sentences, or paragraphs into vector form, called embeddings. Embeddings capture semantic (meaning-related) and pragmatic (social context-related) features of the input.

Embeddings can be generated using both open source libraries and paywalled APIs. [Sentence Transformers](https://oreil.ly/4OSVd) is a very well-known open source library for generating embeddings, and it provides access to embedding models that performs competitively with respect to proprietary ones.

Let’s generate embeddings using the `Sentence Transformers` library:

```py
from sentence_transformers import SentenceTransformer, util
sbert_model = SentenceTransformer('msmarco-distilbert-base-tas-b')
embedding = sbert_model.encode("American pizza is one of the nation's greatest
cultural exports", show_progress_bar=True, device='cuda',

convert_to_tensor=True)
print("Embedding size:", embedding.shape[0])
print(embedding)
```

Output:

```py
Embedding size: 768

tensor([-3.9256e-01,  1.0734e-01,  1.3579e-01,  7.6147e-02,  5.2521e-02,
-6.5887e-03,  1.9225e-01,  3.5374e-01,  2.5725e-01,  5.6408e-02,...])
```

For this model, the embedding size is 768, which means each vector has 768 dimensions. The sequence length of this particular model is 512, which means the input text is restricted to 512 tokens, beyond which it will be truncated. The embedding vector is made up of floating-point numbers, which by themselves are not interpretable. We will discuss techniques for interpreting embeddings later in this chapter.

Most embedding models used today are based on encoder-only language models, which we introduced in [Chapter 4](ch04.html#chapter_transformer-architecture). The underlying models are BERT, RoBERTa, MPNet, etc., and are typically fine-tuned on paraphrasing/question-answering/natural language inference datasets. Let’s see how to derive embeddings from these types of models (which is what the `sentence_transformers.encode()` function does under the hood):

```py
from transformers import AutoTokenizer, AutoModel
import torch

tokenizer=
AutoTokenizer.from_pretrained(
  "sentence-transformers/msmarco-distilbert-base-tas-b")
model =
AutoModel.from_pretrained("sentence-transformers/msmarco-distilbert-base-tas-b")

input = tokenizer(
  'American pizza is one of the nation's greatest cultural exports',
padding=True, truncation=True, return_tensors='pt')

with torch.no_grad():
        output = model(**input, return_dict=True)
       embedding = output.last_hidden_state[:, 0]
print(embedding)
```

In this example, the embedding is drawn from the [CLS] token of the last layer of the DistilBERT model. Other ways of extracting embeddings from models include:

*   Mean pooling, where the average is taken across all token outputs in the sequence

*   Max pooling, where the maximum value in each dimension across all tokens is taken

*   Weighted mean, where more weight is given to the last few tokens

*   Last token, where the embedding is just the encoder output of the last token

###### Tip

Whether the last token (or the first token) contains good representations of the entire sequence depends a lot on the pre-training and the fine-tuning objective. BERT’s pre-training objective (next-sentence prediction) ensures that the [CLS] token is much richer in representation than, say, RoBERTa, which doesn’t use the next-sentence prediction objective and thus its <s> start sequence token isn’t as informative.

Recently, decoder-based embedding models have started gaining prominence, like the [SGPT family of models](https://oreil.ly/AztT9). OpenAI exposes a single embedding endpoint for both search and similarity. OpenAI embeddings have a much larger maximum sequence length (8,192 tokens), and a much larger dimension size (1,536–3,072). Cohere and Jina are examples of other embedding providers.

Choosing the right model for your task depends on cost, latency, storage limitations, performance, and the data domain of your use case. I suggest starting off with the small but effective all-mpnet-base-v2 model available through the Sentence Transformers library, which I consider the workhorse of the field of NLP. As always, experimenting with different models never hurts. More tips on selecting the right models will be provided throughout the rest of the chapter. Later in the chapter, we will also show how to evaluate embedding models and introduce popular benchmarks.

###### Warning

There is no such thing as infinite compression! Embedding sizes are fixed, so the longer your input, the less information can be encoded in its embedding. Managing this tradeoff differs by use case.

# Semantic Search

The true value of embeddings can be appreciated when we use them for representing a large text corpus. The vectors representing the data occupy what we call an embedding space. Similar texts are located closer to each other in the embedding space. This property allows us to use similarity measures to accomplish meaningful tasks like clustering or semantic search. Semantic search refers to techniques that take into account the meaning and context of queries and documents to identify documents that are most relevant to a given query.

We can visualize the embedding space by using dimensionality reduction techniques like [PCA](https://oreil.ly/Rk1M9) or [t-SNE](https://oreil.ly/0xNrB).

[Figure 11-1](#embedding-visualization) depicts the visualization of embeddings of posts on X (formerly Twitter) by members of the US Congress created by [Nomic AI](https://oreil.ly/XsXls) using its Atlas tool. You can view a detailed version of the visualization at [Nomic’s blog](https://oreil.ly/AORpk).

Let’s explore how we can use embeddings for semantic search. For a given user query, we can generate an embedding of the query and then identify document embeddings closest to it in the vector space. The texts corresponding to the top-k (k can be as small as 1 but can vary according to application needs) closest vectors are provided as a response to the search query. This process is called *retrieval*. The texts are then fed into the LLM prompt along with the user query, and the LLM uses the information provided in the context to answer the user query. This two-step process has traditionally been called the *retriever-reader* framework, with the LLM playing the role of the reader in this example.

![embedding-visualization](assets/dllm_1101.png)

###### Figure 11-1\. Embedding space visualization

As a simple illustrative example, consider two sentences that make up our corpus:

```py
chunks = ['The President of the U.S is Joe Biden',
'Ramen consumption has increased in the last 5 months']
```

Given the query “president of usa,” we can encode the query and the chunks using Sentence Transformers:

```py
from sentence_transformers import SentenceTransformer, util
sbert_model = SentenceTransformer('msmarco-distilbert-base-tas-b')
chunk_embeddings = sbert_model.encode(chunks, show_progress_bar=True,
device='cuda', normalize_embeddings=True, convert_to_tensor=True)
query_embedding = sbert_model.encode(query, device='cuda',
normalize_embeddings=True, convert_to_tensor=True)
matches = util.semantic_search(query_embedding, chunk_embeddings,
score_function=util.dot_score)
```

The output is:

```py
[[{'corpus_id': 0, 'score': 0.8643729090690613},
  {'corpus_id': 1, 'score': 0.6223753690719604}]]
```

As you can see, the similarity score is much higher for the first sentence, and thus we return the first sentence as the query response.

###### Note

There is a distinction between symmetric semantic search and asymmetric semantic search. In symmetric search, the query text is of similar size as the document text. In asymmetric search, the query text is much shorter than the document text, as with search engine and question-answering assistant queries. There are models available that are specialized for only symmetric or asymmetric search. In some models, the query and chunk texts are encoded using separate models.

# Similarity Measures

Commonly used similarity measures include dot product, cosine similarity, and Euclidean distance. Refer to the [Pinecone](https://oreil.ly/X_qcD) tutorial on similarity measures if you need a backgrounder. While using embedding models, use the similarity measure that was used to train the model. You will find this information in the model card or Hugging Face model hub page.

###### Tip

If you set `normalize_embeddings` to `True` as an argument in the `encode()` function, it will normalize the embeddings to unit length. This will ensure that both dot product and cosine similarity will have the same values. Note that dot product is a faster operation than cosine similarity. Sentence Transformers provides [separate models](https://oreil.ly/LOu75) trained on dot product and cosine similarity and mentions that models trained on dot product tend to prefer longer chunks during retrieval.

While the notion of semantic similarity is powerful, it is not a panacea for all applications. The semantic similarity task is underspecified. To start with, there are several notions of similarity. Similarity refers to the sameness or alikeness of the entities being compared. But for the same two entities, some dimensions are similar and some are different.

For example, consider the three sentences:

> After his 25th anniversary at the company, Mr. Pomorenko confirmed that he is not retiring.
> 
> Mr. Pomorenko announced his retirement yesterday.
> 
> Mr. Pomorenko did not announce his retirement yesterday.

Now let’s use the Sentence Transformers all-mpnet-base-v2 embedding model to encode these sentences and calculate their similarity:

```py
!pip install sentence-transformers

from sentence_transformers import SentenceTransformer, util
model = SentenceTransformer('all-mpnet-base-v2')

sentences = ['After his 25th anniversary at the company, Mr. Pomorenko
confirmed that he is not retiring',  'Mr. Pomorenko announced his retirement
yesterday']
embeddings = model.encode(sentences)
cosine_scores = util.cos_sim(embeddings[0], embeddings[1])
print("Cosine Similarity:", cosine_scores.item())
```

Output:

```py
Cosine Similarity: 0.7870
```

If you replace the second sentence with “Mr. Pomorenko did not announce his retirement yesterday,” the output is:

```py
Cosine Similarity: 0.7677!
```

As you can see, both these sentences are perceived as equally similar to the first sentence. In some aspects, this is true. They are similar because they both talk about Mr. Pomorenko. They are also similar because both deal with the subject of retirement. On the other hand, one sentence conveys the opposite meaning to the other, by suggesting a retirement is happening versus not happening.

###### Tip

One way to handle the false positives arising due to the model using undesirable similarity dimensions (like negation) is to just increase the k value in the top-k results that are returned as a response to the query. Then, the LLM can distinguish between false positives and use the correct information for answering the query. However, increasing the top-k also increases the context length of the prompt, increasing latency and cost.

Our application requirements determine which similarity dimensions are important to us. If negation is an important relation for our application to distinguish, it might be a good idea to reflect that in our embedding space. This is where fine-tuning embedding models can come in handy. Fine-tuning embedding models allows you to “edit” your embedding space to your own liking. The process is relatively simple and can be potentially quite beneficial.

Fine-tuning embeddings can also be very useful when you are working with specialized data domains whose token distribution deviates from general-purpose data. Let’s now discuss how to fine-tune embedding models.

# Fine-Tuning Embedding Models

The Sentence Transformers library facilitates fine-tuning embedding models using the [`SentenceTransformerTrainer` class](https://oreil.ly/Jahep). To fine-tune an embedding model, we need a base model to fine-tune on, a training dataset, and a learning objective.

## Base Models

You can fine-tune a fine-tuned model like all-mpnet-base-v2, or you can fine-tune a base model like MPNet, from which all-mpnet-base-v2 is defined. You will need more training data to fine-tune a base model than to further fine-tune an already fine-tuned model. Other candidates’ models for fine-tuning include [BGE-M3](https://oreil.ly/Sh8pZ) and [jina-embeddings-v3](https://oreil.ly/lFiWX). A full list of models available through Sentence Transformers can be accessed [online](https://oreil.ly/Onyuv). Remember to check the licenses for a given model before using it for commercial purposes.

Some of the factors to keep in mind while choosing a base model include the performance of the base model, the size of the embedding models (which determines how fast the model can encode text), the number of dimensions of the model (which determines the amount of storage taken up by the embeddings), and the licensing implications. The MPNet or all-mpnet-base-v2 is a solid first choice that has served me well on many projects.

###### Tip

If a model has been fine-tuned for a particular task like semantic search, it is not optimal to further fine-tune it on a different task.

## Training Dataset

There are many different ways to structure your dataset. The most common way is in the form of triplets consisting of (anchor, positive, negative) examples. For a given anchor sentence, the positive sentence is a sentence we would like to be closer to the anchor sentence in embedding space, and the negative sentence is a sentence we would like to be farther apart from the anchor in embedding space. For example, to fine-tune the model to help it distinguish negation sentences, our training set can be composed of triplets where the negative sentence contradicts the anchor and the positive sentences.

[Figure 11-2](#embed-dataset) shows an embedding dataset composed of triplets for helping the model distinguish negation.

![embed-dataset](assets/dllm_1102.png)

###### Figure 11-2\. Fine-tuning dataset for negation

Datasets can also be composed of sentence pairs, where the sentences could represent a (query, response) pair, or a (passage, summary) pair, or a pair of paraphrases. The downstream use cases determine the type of dataset needed. The [Sentence Transformers website](https://oreil.ly/geI1M) shows all the different ways a dataset can be formatted.

Training datasets can be as small as a few thousand examples, to [billions of tokens](https://oreil.ly/oNI4n) when used for domain adaptation.

Note that certain loss functions require your dataset to be in a specific format. We will discuss loss functions in detail next.

## Loss Functions

Recall our discussion on loss functions for training LLMs in [Chapter 4](ch04.html#chapter_transformer-architecture). The [Sentence Transformers library](https://oreil.ly/9Qaop) supports a wide range of loss functions for training embedding models. Let’s explore a few commonly used ones.

For a triplet dataset, you can compute a [triplet loss](https://oreil.ly/yXHNU). For a training dataset consisting of an (anchor, positive, negative) triplet, the triplet loss minimizes the distance between the anchor sentence and the positive sentence, and maximizes the distance between the anchor sentence and the negative sentence.

Mathematically, the loss is calculated as:

Loss = max(d(a, p) – d(a, n) + margin, 0)

where d is a distance measure, typically Euclidean distance. The margin is a hyperparameter that represents the distance by which the negative example should be farther away from the anchor than the positive example. When using Euclidean distance as the distance measure, I suggest a margin of 5, but make sure to tune it if you are not getting sufficient results.

If you are using a dataset composed of pairs like (query, response), (passage, summary), etc., you can use the [Multiple Negatives Ranking Loss](https://oreil.ly/oNcsQ).

In a batch containing (query, response) pairs (q1, r1), (q2, r2)…​(qn, rn), for each query, there will be a positive pair, e.g., (q1, r1) and n – 1 negative pairs, e.g., (q1, r2), (q1, r3)…​etc. The loss function minimizes the negative log likelihood.

###### Tip

Use [`CachedMultipleNegativesRankingLoss`](https://oreil.ly/QwBlI), available in Sentence Transformers, which allows you to use larger batch sizes, leading to better performance.

Now that we have discussed all the ingredients needed for fine-tuning, let’s put it all together with the `SentenceTransformerTrainer` class:

```py
from datasets import load_dataset
from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer
from sentence_transformers.losses import TripletLoss

model = SentenceTransformer( "'all-mpnet-base-v2'")

dataset = load_dataset("csv", data_files="negatives_dataset.csv")

loss = TripletLoss(model)

trainer = SentenceTransformerTrainer(
    model=model,
    train_dataset=dataset
    loss=loss
   )
trainer.train()
model.save_pretrained("mpnet_finetuned_negatives")
```

The full code is available in the book’s [GitHub repo](https://oreil.ly/llm-playbooks).

###### Tip

Watch out for overfitting! You can reduce your learning rate if you notice the model overfitting.

###### Note

[Zhou et al.](https://oreil.ly/BPdRD) show that in the context of embeddings, cosine similarity tends to underestimate the similarity between high-frequency words. This is because high-frequency words occupy distinct regions in the embedding space, leading to larger distances from other words. On the other hand, low-frequency words tend to be more concentrated geometrically.

# Instruction Embeddings

So far we have seen that embedding models are specialized for solving a specific task, like semantic search or paraphrasing. A recent development ties together embedding models and the concept of instruction-tuning, which we discussed in [Chapter 6](ch06.html#llm-fine-tuning). Imagine if you could use the same embedding model to generate different embeddings for the same document, based on the task it is going to be used for. One such model is called Instructor. [Instructor embeddings](https://oreil.ly/mSIhG) allow you to optionally specify the domain, text type (whether it is a sentence, paragraph, etc.), and task, along with the text during encoding.

Here is an example:

```py
!pip install InstructorEmbedding

from InstructorEmbedding import INSTRUCTOR
model = INSTRUCTOR('hkunlp/instructor-large')

customized_embeddings = model.encode(
[['Represent the question for retrieving supporting documents:',
  'Who is the CEO of Apple'],
 ['Represent the sentence for retrieval:',
  'Tim Cook is the CEO of Apple'],
 ['Represent the sentence for retrieval:',
  'He is a musically gifted CEO'],
)
```

The creators of Instructor recommend using this instruction template:

```py
‘Represent the {domain} {text_type} for {task_objective}:’
```

where `{domain}` represents the domain of the text like law, finance, etc. The optional `{text_type}` represents the unit of text being encoded, like a question, sentence, paragraph, etc. `{task_objective}` represents the task for which we are using the embeddings, like semantic search, paraphrase detection, etc.

In the context of semantic search, they recommend the instruction “Represent the question for retrieving supporting documents” for queries, and “Represent the sentence for retrieval” for documents.

Another way the principle of instruction-tuning can be applied to retrieval is with *description-based retrieval*, where the query can be the description of the text that needs to be retrieved, rather than an instantiation (example) of the text that needs to be retrieved. [Ravfogel et al.](https://oreil.ly/rp8Q-) have published description-based retrieval models that in my experience are very effective. Note that these models have a dual-encoder setup: separate models are used to encode the query and documents.

# Optimizing Embedding Size

Many applications involve generating billions of embeddings. As we have seen, modern embeddings sometimes have as many as thousands of dimensions. If each dimension is represented in float32, then it needs four bytes of memory per dimension. Therefore, storing 100 million vectors generated from the all-mpnet-base-v2 model, which has 768 dimensions, needs close to 300 GB of memory!

It is not uncommon to represent a single sentence, almost always no longer than 40 tokens, with a 768-dimension vector. Do we really need 768 dimensions to represent 40 tokens? The reality is that embedding training is very inefficient, and a large number of dimensions are not really useful.

Therefore, several embedding truncation and quantization approaches have been developed to optimize embedding size and reduce storage and compute requirements. If you are operating in an environment with more than a few million vectors, these techniques are likely to be useful to you. Let’s look at some of these approaches.

## Matryoshka Embeddings

Matryoshka embeddings are named after [Matryoshka dolls](https://oreil.ly/OC6Yj), which refer to a set of wooden dolls that are placed inside each other in decreasing order of size, originating from Russia. Matryoshka embeddings are trained such that the earlier dimensions of the vector contain more important information than the later dimensions. This allows us to truncate vectors depending on the requirements of the application with respect to cost, latency, and performance.

The technique used to train these embeddings is called Matryoshka Representation Learning (MRL). In MRL, we first choose a set of truncation dimensions. For example a 1,024-dimension vector can have truncation dimensions 128, 256, 512, and 768\. During the training process, we calculate the loss over each of the truncation dimensions as well as the full dimension. The losses are then added and weighted. In our example, the first 128 dimensions learn from the loss calculated over the first 128, 256, 512, 768, and 1,024 dimensions of the vector. The end result is that the initial dimensions of the vector will encode more important information because they learn from richer losses.

Training using MRL is supported by the Sentence Transformers library. Let’s see how it works in practice:

```py
from sentence_transformers import SentenceTransformer
from sentence_transformers import SentenceTransformerTrainer, losses
from datasets import load_dataset

model = SentenceTransformer("all-mpnet-base-v2")
train_dataset = load_dataset("csv", data_files="finetune_dataset.csv")
loss = losses.MultipleNegativesRankingLoss(model)
loss = losses.MatryoshkaLoss(model, loss, [768, 512, 256, 128]])

trainer = SentenceTransformerTrainer(
    model=model,
    train_dataset=train_dataset,
    loss=loss,
)
trainer.train()
```

[Tom Aarsen](https://oreil.ly/sA5fo) observed in his experiments that even at 8.3% of the original embedding size, the Matryoshka model preserves 98.37% of the original performance. This makes it a very effective technique that will come in handy when you are working with large datasets.

Similar to how we can reduce the effective dimension of our embeddings using MRL, we can also reduce the effective number of layers of the embedding model, leading to faster inference. This is done by extracting embeddings from the lower layers of the model. To facilitate the lower layers of the model aligning high-quality embeddings with the embeddings of the last layer of the model, a K-L divergence loss is employed between the final layer and each of the lower layers. This technique was first introduced by [Li et al.’s](https://oreil.ly/fzIPD) Espresso Sentence Embeddings.

[Tom Aarsen](https://oreil.ly/DIoTe) observed in his experiments that removing half the layers leads to a 2x improvement in speed with 85% of the original performance preserved.

The Sentence Transformers library allows you to combine Matryoshka representations with layer reduction using the [Matryoshka2dLoss](https://oreil.ly/xzG-a).

## Binary and Integer Embeddings

An alternative to truncation is quantization. With binary and integer quantization, the number of vector dimensions remains the same, but each dimension is represented by fewer bits. Recall that typically embedding vectors are represented in float32, thus taking four bytes of memory per dimension.

At the extreme level, the four bytes can be represented with just one bit, resulting in a 32x reduction in storage requirements. This type of compression is generally done by sacrificing the precision of the vector values.

A simple way to convert a four-byte vector to a one-bit vector is to assign a value of 1 if the original value is positive, and 0 if it is negative. Note that you might need to perform some scaling to achieve best results. After packing these bits into bytes, a 512-dimension vector can be represented in just 512 / 8 = 64 bytes, instead of 512 × 4 = 2,048 bytes.

Another advantage with using binary embeddings is that computing similarity only needs simple bitwise operations, thus vastly speeding up retrieval. However, quantization negatively affects performance.

You can use the `Sentence Transformers` library to quantize embeddings:

```py
from sentence_transformers.quantization import quantize_embeddings

model = SentenceTransformer("all-mpnet-base-v2")
embeddings = model.encode(["I heard the horses are excited for Halloween.",
"Dalmatians are the most patriotic of dogs.", "This restaurant is making me
nostalgic."])
binary_embeddings = quantize_embeddings(embeddings, precision="binary")
```

`quantize_embeddings` also supports int8 quantization. In this scheme, the four bytes representing each dimension are converted into an integer value, represented in one byte. The integer can be either signed or unsigned, thus representing values between –127 and 127 or between 0 and 255, respectively. The conversion process is guided using a calibration dataset of embeddings, from which we calculate the minimum and maximum value of each dimension. These values are then used in the normalization formula to convert the numbers from one range to another.

###### Tip

It has been shown that for some [embedding models](https://oreil.ly/Mp3pu), binary embeddings perform better than int8 embeddings despite the reduced precision! This is largely because of the calibration dataset used and the challenge involved in mapping float values to buckets of int8 values.

## Product Quantization

Another promising quantization method is called [*product quantization*](https://oreil.ly/aJq2C). In this technique, a vector is divided into chunks of equal size. The chunks are then clustered. The number of clusters is set to the number of values that can be represented by the quantized embedding. For example, if we aim to quantize to int8, then the number of values that can be represented is 256, and thus the number of clusters is 256\. Each cluster is associated with an identifier, which is a unique value between 0 and 255\. Each chunk belongs to the cluster whose centroid the chunk is closest to.

Thus, the original float32 vector can now be represented by a list of cluster identifiers corresponding to the clusters the chunks belong to. The larger the chunk size, the more the compression. Thus if the vector is divided into five chunks, the resulting embedding will have only five dimensions. Unlike int8 and binary quantization, product quantization also reduces the number of dimensions needed to represent a vector. However, the performance drop is higher.

Choose your quantization technique by determining your relative product priorities for criteria like cost, performance, and speed.

###### Tip

Optimizing embeddings for storage come with a performance hit. However, if there is plenty of redundancy in the document corpus, answers to typical user queries might be found in several documents, and hence the user may not feel this performance drop.

Now that we have seen various techniques to practically implement embedding-based retrieval, let’s next figure out the textual units we need to embed into distinct vectors.

# Chunking

As noted in [“Introduction to Embeddings”](#introduction-to-embeddings), embedding models support very limited context lengths, and the effectiveness of embedding similarity matching decreases as the text length increases. Therefore, it is natural to split documents into manageable units called chunks and embed each chunk into one or more vectors.

A chunk can be defined as a semantically coherent and not necessarily contiguous part of a document. The average chunk length depends on the context length supported by the language model, and the number of chunks returned to the model (the top-k) in response to a user query. As models become increasingly affordable to operate and support ever-larger context lengths, the permissible chunk size grows.

Each chunk can either be represented by a single vector or can be further broken down into units, with each unit being represented by a separate vector. A unit could be a sentence, a paragraph, or even a section. Typically, the smaller the unit, the better. For your application, test your expected user queries against different granularities and see what works best.

Consider a scenario where a document corpus has been broken down into units represented by embeddings. For a given user query, we can calculate the cosine similarity between the user query vector and each of the document vectors. The chunks corresponding to the most similar vectors are then retrieved. This ensures that the embedding matching happens at a lower granularity, like a sentence, but the model receives the entirety of the chunk the sentence belongs to, thus providing sufficient background context to the model.

A question I am frequently asked by ML practitioners is, “What is the ideal chunk size and what are some effective chunking strategies?” Determining the right chunk size and boundaries are key challenges practitioners face when using embedding-based retrieval. In this section, we will discuss a few chunking strategies, introduced in order of increasing complexity.

In the basic implementation of embedding-based retrieval, each vector is a distinct island, disconnected from all other islands. The text represented by Vector A is not able to influence text represented by Vector B in any way. Therefore, we need to connect these islands in some way or make these islands as self-contained as possible. With these objectives in mind, let’s look at some chunking strategies that go beyond naive paragraph or section splitting.

## Sliding Window Chunking

Consider a situation where the embedding similarity function returns a unit in Chunk 45 as the most similar vector to your query vector. However, text in Chunk 44, which immediately precedes Chunk 45 in the document, contains relevant information contextualizing Chunk 45\. The vectors in Chunk 44 have a very low similarity score with the query, and as a result, Chunk 44 is not selected for retrieval. One way to fix this is by using sliding window chunking, where each text can be present in multiple chunks, thus allowing neighboring context to be effectively represented in a coherent block.

## Metadata-Aware Chunking

Any metadata that you have about the document can be leveraged to determine chunking boundaries. Useful metadata information includes paragraph boundaries, section and subsection boundaries, etc. If the metadata isn’t already available, you might need to use document parsing techniques to extract this information. Several libraries can facilitate this, including [Unstructured](https://oreil.ly/CoX46).

## Layout-Aware Chunking

A more involved form of metadata-aware chunking is layout-aware chunking. In this approach we use computer vision techniques to extract layout information about the document, including the placement and scope of textual elements, the titles, subtitles, font size of text, etc.; use this metadata to inform the chunking process. Both open source and proprietary tools can facilitate layout extraction. They include tools like [Amazon Textractor](https://oreil.ly/fvkiT), [Unstructured](https://oreil.ly/CoX46), and layout-aware language models like [LayoutLMv3](https://oreil.ly/Od5fA).

For example, using this approach we can know the scope of a subsection, and thus insert the subsection title at the beginning of each chunk comprising text from that subsection.

You can also use techniques like ColPali that employ vision models to directly embed a page or section of the document and perform retrieval over it. This may remove the need for chunking entirely but might be more expensive overall.

## Semantic Chunking

The principle behind semantic chunking is that similar information should be grouped into coherent chunks. Paragraph boundaries provide a weak signal for semantic chunking, but more advanced methods can be employed. One approach is to cluster the document based on topics, with each chunk containing information pertaining to the same topic. The chunks need not necessarily be built from contiguous text if it makes sense for the application. A more advanced approach is to use [Bollinger bands-based chunking](https://oreil.ly/1MwK1). The book’s [GitHub repository](https://oreil.ly/llm-playbooks) contains an experimental implementation of this form of chunking.

Semantic chunking can also be employed to connect different chunks with each other. Once the chunks have been assigned, similar chunks can be grouped based on embedding similarity, allowing them to be retrieved along with the chunk having the highest similarity score. Each chunk does not necessarily need to consist of content from the same document, as long as the metadata associated with each sub-chunk is retained.

A basic implementation of semantic chunking is available in [LangChain](https://oreil.ly/tm8tk).

###### Note

Highly performant semantic chunking can be performed through LLMs. But it will be a huge cost overhead if the size of your data corpus is very large. Sometimes good old regex can be enough. Jina AI created a complex 50-line [regex-based chunker](https://oreil.ly/x5UO8) that you can try as an initial option.

Despite using all these techniques, effective chunking still remains a problem. Consider the following real-world example from a financial document:

> Page 5: *All numbers in the document are in millions*
> 
> Page 84: *The related party transaction amounts to $213.45*

In this case the related party transaction actually amounts to $213M dollars but the LLM would never know this because the text from page 5 is not likely to be part of the same chunk.

A related problem is the difficulty in understanding scope boundaries. When does a subsection end and a new subsection begins? What is the scope of the rule in page 5 in the given example? What if it is overridden in the middle of a document? Not all documents have perfect visual cues or structure. Not all documents are well structured into sections, subsections, and paragraphs. These are unsolved problems and are the cause of a sizable proportion of RAG failure modes.

## Late Chunking

One way of supporting long-range dependencies in text is to use [late chunking](https://oreil.ly/IxTQx), a method introduced by Jina AI. Recall from earlier in the chapter that embeddings are generated by typically pooling the vectors from the last layer of the underlying language model.

Given that we have access to long-context language models that can accept an entire long document in a single input, we can use such a long-context model as our underlying model for generating embeddings. We feed an entire document (or as large a part as the model can handle) to the long-context model, so that vectors are generated for each of the input tokens. As explained in [Chapter 4](ch04.html#chapter_transformer-architecture), each token vector encapsulates its meaning based on its relationship with all other tokens in the sequence. This enables long-context dependencies to be captured.

The pooling operation to extract the embeddings is performed on smaller segments of the input, where the segment boundaries can be determined by any of the chunking algorithms. Thus, we can have several embeddings representing the same document but each of them representing distinct parts of the input.

# Vector Databases

Depending on your application, you may have to deal with millions or billions of vectors, with the need to generate and store new vectors and their associated metadata tags every day. Vector databases facilitate this. Both self-hosted and cloud-based, open source, and proprietary options are available. Weviate, Milvus, Pinecone, Chroma, Qdrant, and LanceDB are some of the popular vector databases. More established players like ElasticSearch, Redis, and Postgres also provide vector database support.

These days, the features provided by vector databases are converging, given the prevalence of a small set of very popular retrieval use cases.

Let’s now look at how vector databases work. Probably the simplest one to get started with is Chroma, which is open source and can run locally on your machine or can be deployed on AWS:

```py
!pip install chromadb

import chromadb
chroma_client = chromadb.Client()

collection = chroma_client.create_collection(name="mango_science")
chunks = ['353 varieties of mangoes are now extinct',
'Mangoes are grown in the tropics']
metadata = [{"topic": "extinction", "chapter": "2"}, {"topic": "regions",
  "chapter": "5"}]
unique_ids = [str(i) for i in range(len(chunks))]

collection.add(
   documents=chunks,
   metadatas=metadata,
   ids=unique_ids
  )
results = collection.query(
   query_texts=["Where are mangoes grown?"],
   n_results=2,
   where={"chapter": { "$ne": "2"}},
   where_document={"$contains":"grown"}
)
```

Most vector databases offer:

*   Approximate nearest neighbor search in addition to exact search, to reduce latency

*   Ability to filter using metadata, like the *where* clause in SQL

*   Ability to integrate keyword search or algorithms like BM25

*   Support Boolean search operations, so that multiple search clauses can be combined with AND or OR operations

*   Ability to update or delete entries in the database in real time

# Interpreting Embeddings

What features of text do embeddings learn? Why are two sentences sometimes closer to/farther from each other in the embedding space than we expect? Can we know what each dimension of an embedding vector represents?

A key limitation in embedding-based retrieval compared to traditional techniques is the lack of interpretability in ranking decisions. There is a whole body of research dedicated to improving interpretability of neural networks, LLMs, and embeddings. In [Chapter 5](ch05.html#chapter_utilizing_llms), we introduced some interpretability techniques for understanding LLMs. In this section, we will focus on embedding interpretability in particular. One benefit of understanding the features represented in embedding space is that we could leverage that knowledge to steer embeddings for our own purposes.

One promising technique for imparting interpretability is to use SAEs. Let’s understand what they mean and how they are trained and used to enhance interpretability.

A language model may learn millions of features, but for any given input, only a few of those features are relevant or activated. This is what we mean by sparsity. Even as they learn lots of features, there are only a limited number of dimensions in an embedding vector. Therefore, each dimension contributes to many features that can interfere with each other. If you train a [sparse autoencoder](https://oreil.ly/oiXb7) over these embeddings, you can derive independent interpretable features.

In his [Prism project](https://oreil.ly/efzz1), Linus Lee uses SAEs to explore the features of a T5-based embedding model.

Some of the identified features include:

*   Presence of negation

*   Expression of possibility or speculation

*   Employment and labor concepts

*   Possessive syntax at sentence start

For a longer list of identified features, refer to [Linus Lee’s blog post](https://oreil.ly/efzz1).

# Summary

In this chapter, we introduced the concept of embeddings, examined their internals, and showed various techniques for generating them. We also discussed techniques for fine-tuning embeddings on our own data. We learned how to determine the data granularities at which we construct embeddings, discussing several chunking techniques in the process. Finally, we explored techniques to visualize and interpret embeddings.

In the next chapter, we will explore RAG, an application paradigm that is by far the most popular use case for embeddings today. We will present the steps involved in a typical RAG workflow and review each of these steps in detail. We will also discuss the technical decisions involved in building a RAG application and provide pointers on how to think through various tradeoffs.