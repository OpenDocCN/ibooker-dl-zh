- en: 2 Harnessing the power of large language models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Understanding the basics of LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Connecting to and consuming the OpenAI API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring and using open source LLMs with LM Studio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompting LLMs with prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the optimal LLM for your specific needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term *large language models* (LLMs) has now become a ubiquitous descriptor
    of a form of AI. These LLMs have been developed using generative pretrained transformers
    (GPTs). While other architectures also power LLMs, the GPT form is currently the
    most successful.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs and GPTs are *generative* models, which means they are trained to *generate*
    rather than predict or classify content. To illustrate this further, consider
    figure 2.1, which shows the difference between generative and predictive/classification
    models. Generative models create something from the input, whereas predictive
    and classifying models classify it.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 The difference between generative and predictive models
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We can further define an LLM by its constituent parts, as shown in figure 2.2\.
    In this diagram, *data* represents the content used to train the model, and *architecture*
    is an attribute of the model itself, such as the number of parameters or size
    of the model. Models are further trained specifically to the desired use case,
    including chat, completions, or instruction. Finally, *fine-tuning* is a feature
    added to models that refines the input data and model training to better match
    a particular use case or domain.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 The main elements that describe an LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The transformer architecture of GPTs, which is a specific architecture of LLMs,
    allows the models to be scaled to billions of parameters in size. This requires
    these large models to be trained on terabytes of documents to build a foundation.
    From there, these models will be successively trained using various methods for
    the desired use case of the model.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT, for example, is trained effectively on the public internet and then
    fine-tuned using several training strategies. The final fine-tuning training is
    completed using an advanced form called *reinforcement learning with human feedback*
    (RLHF). This produces a model use case called chat completions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Chat completions* LLMs are designed to improve through iteration and refinement—in
    other words, chatting. These models have also been benchmarked to be the best
    in task completion, reasoning, and planning, which makes them ideal for building
    agents and assistants. Completion models are trained/designed only to provide
    generated content on input text, so they don’t benefit from iteration.'
  prefs: []
  type: TYPE_NORMAL
- en: For our journey to build powerful agents in this book, we focus on the class
    of LLMs called chat completions models. That, of course, doesn’t preclude you
    from trying other model forms for your agents. However, you may have to significantly
    alter the code samples provided to support other model forms.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll uncover more details about LLMs and GPTs later in this chapter when we
    look at running an open source LLM locally. In the next section, we look at how
    to connect to an LLM using a growing standard from OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Mastering the OpenAI API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Numerous AI agents and assistant projects use the OpenAI API SDK to connect
    to an LLM. While not standard, the basic concepts describing a connection now
    follow the OpenAI pattern. Therefore, we must understand the core concepts of
    an LLM connection using the OpenAI SDK.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will look at connecting to an LLM model using the OpenAI Python
    SDK/package. We’ll discuss connecting to a GPT-4 model, the model response, counting
    tokens, and how to define consistent messages. Starting in the following subsection,
    we’ll examine how to use OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 Connecting to the chat completions model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To complete the exercises in this section and subsequent ones, you must set
    up a Python developer environment and get access to an LLM. Appendix A walks you
    through setting up an OpenAI account and accessing GPT-4 or other models. Appendix
    B demonstrates setting up a Python development environment with Visual Studio
    Code (VS Code), including installing needed extensions. Review these sections
    if you want to follow along with the scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Start by opening the source code `chapter_2` folder in VS Code and creating
    a new Python virtual environment. Again, refer to appendix B if you need assistance.
  prefs: []
  type: TYPE_NORMAL
- en: Then, install the OpenAI and Python dot environment packages using the command
    in the following listing. This will install the required packages into the virtual
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.1 `pip` installs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, open the `connecting.py` file in VS Code, and inspect the code shown in
    listing 2.2\. Be sure to set the model’s name to an appropriate name—for example,
    gpt-4\. At the time of writing, the `gpt-4-1106-preview` was used to represent
    GPT-4 Turbo.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.2 `connecting.py`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Loads the secrets stored in the .env file'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Checks to see whether the key is set'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Creates a client with the key'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Uses the create function to generate a response'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Returns just the content of the response'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Executes the request and returns the response'
  prefs: []
  type: TYPE_NORMAL
- en: A lot is happening here, so let’s break it down by section, starting with the
    beginning and loading the environment variables. In the `chapter_2` folder is
    another file called `.env`, which holds environment variables. These variables
    are set automatically by calling the `load_dotenv` function.
  prefs: []
  type: TYPE_NORMAL
- en: You must set your OpenAI API key in the `.env` file, as shown in the next listing.
    Again, refer to appendix A to find out how to get a key and find a model name.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.3 `.env`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After setting the key, you can debug the file by pressing the F5 key or selecting
    Run > Start Debugging from the VS Code menu. This will run the code, and you should
    see something like “The capital of France is Paris.”
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the response from a generative model depends on the probability.
    The model will probably give us a correct and consistent answer in this case.
  prefs: []
  type: TYPE_NORMAL
- en: You can play with these probabilities by adjusting the temperature of the request.
    If you want a model to be more consistent, turn the temperature down to 0, but
    if you want the model to produce more variation, turn the temperature up. We’ll
    explore setting the temperature further in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Understanding the request and response
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Digging into the chat completions request and response features can be helpful.
    We’ll focus on the request first, as shown next. The request encapsulates the
    intended model, the messages, and the temperature.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.4 The chat completions request
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The model or deployment used to respond to the request'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The system role message'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The user role message'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 The temperature or variability of the request'
  prefs: []
  type: TYPE_NORMAL
- en: 'Within the request, the `messages` block describes a set of messages and roles
    used in a request. Messages for a chat completions model can be defined in three
    roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '*System role* —A message that describes the request’s rules and guidelines.
    It can often be used to describe the role of the LLM in making the request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*User role* —Represents and contains the message from the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Assistant role* —Can be used to capture the message history of previous responses
    from the LLM. It can also inject a message history when perhaps none existed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The message sent in a single request can encapsulate an entire conversation,
    as shown in the JSON in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.5 Messages with history
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can see how this can be applied by opening `message_history.py` in VS Code
    and debugging it by pressing F5\. After the file runs, be sure to check the output.
    Then, try to run the sample a few more times to see how the results change.
  prefs: []
  type: TYPE_NORMAL
- en: The results will change from each run to the next due to the high temperature
    of `.7`. Go ahead and reduce the temperature to `.0`, and run the `message_history.py`
    sample a few more times. Keeping the temperature at `0` will show the same or
    similar results each time.
  prefs: []
  type: TYPE_NORMAL
- en: Setting a request’s temperature will often depend on your particular use case.
    Sometimes, you may want to limit the responses’ stochastic nature (randomness).
    Reducing the temperature to `0` will give consistent results. Likewise, a value
    of `1.0` will give the most variability in the responses.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we also want to know what information is being returned for each request.
    The next listing shows the output format for the response. You can see this output
    by running the `message_history.py` file in VS Code.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.6 Chat completions response
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 A model may return more than one response.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Responses returned in the assistant role'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Indicates the model used'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Counts the number of input (prompt) and output (completion) tokens used'
  prefs: []
  type: TYPE_NORMAL
- en: It can be helpful to track the number of *input tokens* (those used in prompts)
    and the *output tokens* (the number returned through completions). Sometimes,
    minimizing and reducing the number of tokens can be essential. Having fewer tokens
    typically means LLM interactions will be cheaper, respond faster, and produce
    better and more consistent results.
  prefs: []
  type: TYPE_NORMAL
- en: That covers the basics of connecting to an LLM and returning responses. Throughout
    this book, we’ll review and expand on how to interact with LLMs. Until then, we’ll
    explore in the next section how to load and use open source LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Exploring open source LLMs with LM Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Commercial LLMs, such as GPT-4 from OpenAI, are an excellent place to start
    to learn how to use modern AI and build agents. However, commercial agents are
    an external resource that comes at a cost, reduces data privacy and security,
    and introduces dependencies. Other external influences can further complicate
    these factors.
  prefs: []
  type: TYPE_NORMAL
- en: It’s unsurprising that the race to build comparable open source LLMs is growing
    more competitive every day. As a result, there are now open source LLMs that may
    be adequate for numerous tasks and agent systems. There have even been so many
    advances in tooling in just a year that hosting LLMs locally is now very easy,
    as we’ll see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Installing and running LM Studio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LM Studio is a free download that supports downloading and hosting LLMs and
    other models locally for Windows, Mac, and Linux. The software is easy to use
    and offers several helpful features to get you started quickly. Here is a quick
    summary of steps to download and set up LM Studio:'
  prefs: []
  type: TYPE_NORMAL
- en: Download LM Studio from [https://lmstudio.ai/](https://lmstudio.ai/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After downloading, install the software per your operating system. Be aware
    that some versions of LM Studio may be in beta and require installation of additional
    tools or libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch the software.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 2.3 shows the LM Studio window running. From there, you can review the
    current list of hot models, search for others, and even download. The home page
    content can be handy for understanding the details and specifications of the top
    models.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 LM Studio software showing the main home page
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An appealing feature of LM Studio is its ability to analyze your hardware and
    align it with the requirements of a given model. The software will let you know
    how well you can run a given model. This can be a great time saver in guiding
    what models you experiment with.
  prefs: []
  type: TYPE_NORMAL
- en: Enter some text to search for a model, and click Go. You’ll be taken to the
    search page interface, as shown in figure 2.4\. From this page, you can see all
    the model variations and other specifications, such as context token size. After
    you click the Compatibility Guess button, the software will even tell you if the
    model will run on your system.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 The LM Studio search page
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Click to download any model that will run on your system. You may want to stick
    with models designed for chat completions, but if your system is limited, work
    with what you have. In addition, if you’re unsure of which model to use, go ahead
    and download to try them. LM Studio is a great way to explore and experiment with
    many models.
  prefs: []
  type: TYPE_NORMAL
- en: After the model is downloaded, you can then load and run the model on the chat
    page or as a server on the server page. Figure 2.5 shows loading and running a
    model on the chat page. It also shows the option for enabling and using a GPU
    if you have one.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 The LM Studio chat page with a loaded, locally running LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To load and run a model, open the drop-down menu at the top middle of the page,
    and select a downloaded model. A progress bar will appear showing the model loading,
    and when it’s ready, you can start typing into the UI.
  prefs: []
  type: TYPE_NORMAL
- en: The software even allows you to use some or all of your GPU, if detected, for
    the model inference. A GPU will generally speed up the model response times in
    some capacities. You can see how adding a GPU can affect the model’s performance
    by looking at the performance status at the bottom of the page, as shown in figure
    2.5.
  prefs: []
  type: TYPE_NORMAL
- en: Chatting with a model and using or playing with various prompts can help you
    determine how well a model will work for your given use case. A more systematic
    approach is using the prompt flow tool for evaluating prompts and LLMs. We’ll
    describe how to use prompt flow in chapter 9.
  prefs: []
  type: TYPE_NORMAL
- en: LM Studio also allows a model to be run on a server and made accessible using
    the OpenAI package. We’ll see how to use the server feature and serve a model
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Serving an LLM locally with LM Studio
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Running an LLM locally as a server is easy with LM Studio. Just open the server
    page, load a model, and then click the Start Server button, as shown in figure
    2.6\. From there, you can copy and paste any of the examples to connect with your
    model.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 The LM Studio server page and a server running an LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You can review an example of the Python code by opening `chapter_2/lmstudio_
    server.py` in VS Code. The code is also shown here in listing 2.7\. Then, run
    the code in the VS Code debugger (press F5).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.7 `lmstudio_server.py`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Currently not used; can be anything'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Feel free to change the message as you like.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Default code outputs the whole message.'
  prefs: []
  type: TYPE_NORMAL
- en: If you encounter problems connecting to the server or experience any other problems,
    be sure your configuration for the Server Model Settings matches the model type.
    For example, in figure 2.6, shown earlier, the loaded model differs from the server
    settings. The corrected settings are shown in figure 2.7.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 Choosing the correct Server Model Settings for the loaded model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now, you can use a locally hosted LLM or a commercial model to build, test,
    and potentially even run your agents. The following section will examine how to
    build prompts using prompt engineering more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Prompting LLMs with prompt engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A prompt defined for LLMs is the message content used in the request for better
    response output. *Prompt engineering* is a new and emerging field that attempts
    to structure a methodology for building prompts. Unfortunately, prompt building
    isn’t a well-established science, and there is a growing and diverse set of methods
    defined as prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, organizations such as OpenAI have begun documenting a universal
    set of strategies, as shown in figure 2.8\. These strategies cover various tactics,
    some requiring additional infrastructure and considerations. As such, the prompt
    engineering strategies relating to more advanced concepts will be covered in the
    indicated chapters.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 OpenAI prompt engineering strategies reviewed in this book, by chapter
    location
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Each strategy in figure 2.8 unfolds into tactics that can further refine the
    specific method of prompt engineering. This chapter will examine the fundamental
    Write Clear Instructions strategy. Figure 2.9 shows the tactics for this strategy
    in more detail, along with examples for each tactic. We’ll look at running these
    examples using a code demo in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 The tactics for the Write Clear Instructions strategy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Write Clear Instructions strategy is about being careful and specific about
    what you ask for. Asking an LLM to perform a task is no different from asking
    a person to complete the same task. Generally, the more information and context
    relevant to a task you can specify in a request, the better the response.
  prefs: []
  type: TYPE_NORMAL
- en: This strategy has been broken down into specific tactics you can apply to prompts.
    To understand how to use those, a code demo (`prompt_engineering.py`) with various
    prompt examples is in the `chapter 2` source code folder.
  prefs: []
  type: TYPE_NORMAL
- en: Open the `prompt_engineering.py` file in VS Code, as shown in listing 2.8\.
    This code starts by loading all the JSON Lines files in the `prompts` folder.
    Then, it displays the list of files as choices and allows the user to select a
    prompt option. After selecting the option, the prompts are submitted to an LLM,
    and the response is printed.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.8 `prompt_engineering.py` `(main())`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Collects all the files for the given folder'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Prints the list of files as choices'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Inputs the user’s choice'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Loads the prompt and parses it into messages'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Submits the prompt to an OpenAI LLM'
  prefs: []
  type: TYPE_NORMAL
- en: A commented-out section from the listing demonstrates how to connect to a local
    LLM. This will allow you to explore the same prompt engineering tactics applied
    to open source LLMs running locally. By default, this example uses the OpenAI
    model we configured previously in section 2.1.1\. If you didn’t complete that
    earlier, please go back and do it before running this one.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.10 shows the output of running the prompt engineering tactics tester,
    the `prompt_engineering.py` file in VS Code. When you run the tester, you can
    enter a value for the tactic you want to test and watch it run.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 The output of the prompt engineering tactics tester
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the following sections, we’ll explore each prompt tactic in more detail.
    We’ll also examine the various examples.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Creating detailed queries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The basic premise of this tactic is to provide as much detail as possible but
    also to be careful not to give irrelevant details. The following listing shows
    the JSON Lines file examples for exploring this tactic.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.9 `detailed_queries.jsonl`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The first example doesn’t use detailed queries.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 First ask the LLM a very general question.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Ask a more specific question, and ask for examples.'
  prefs: []
  type: TYPE_NORMAL
- en: This example demonstrates the difference between using detailed queries and
    not. It also goes a step further by asking for examples. Remember, the more relevance
    and context you can provide in your prompt, the better the overall response. Asking
    for examples is another way of enforcing the relationship between the question
    and the expected output.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Adopting personas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adopting personas grants the ability to define an overarching context or set
    of rules to the LLM. The LLM can then use that context and/or rules to frame all
    later output responses. This is a compelling tactic and one that we’ll make heavy
    use of throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.10 shows an example of employing two personas to answer the same question.
    This can be an enjoyable technique for exploring a wide range of novel applications,
    from getting demographic feedback to specializing in a specific task or even rubber
    ducking.
  prefs: []
  type: TYPE_NORMAL
- en: GPT rubber ducking
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '*Rubber ducking* is a problem-solving technique in which a person explains
    a problem to an inanimate object, like a rubber duck, to understand or find a
    solution. This method is prevalent in programming and debugging, as articulating
    the problem aloud often helps clarify the problem and can lead to new insights
    or solutions.'
  prefs: []
  type: TYPE_NORMAL
- en: GPT rubber ducking uses the same technique, but instead of an inanimate object,
    we use an LLM. This strategy can be expanded further by giving the LLM a persona
    specific to the desired solution domain.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.10 `adopting_personas.jsonl`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 First persona'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Second persona'
  prefs: []
  type: TYPE_NORMAL
- en: A core element of agent profiles is the persona. We’ll employ various personas
    to assist agents in completing their tasks. When you run this tactic, pay particular
    attention to the way the LLM outputs the response.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3 Using delimiters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Delimiters are a useful way of isolating and getting the LLM to focus on some
    part of a message. This tactic is often combined with other tactics but can work
    well independently. The following listing demonstrates two examples, but there
    are several other ways of describing delimiters, from XML tags to using markdown.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.11 `using_delimiters.jsonl`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The delimiter is defined by character type and repetition.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The delimiter is defined by XML standards.'
  prefs: []
  type: TYPE_NORMAL
- en: When you run this tactic, pay attention to the parts of the text the LLM focuses
    on when it outputs the response. This tactic can be beneficial for describing
    information in a hierarchy or other relationship patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.4 Specifying steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Specifying steps is another powerful tactic that can have many uses, including
    in agents, as shown in listing 2.12\. It’s especially powerful when developing
    prompts or agent profiles for complex multistep tasks. You can specify steps to
    break down these complex prompts into a step-by-step process that the LLM can
    follow. In turn, these steps can guide the LLM through multiple interactions over
    a more extended conversation and many iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.12 `specifying_steps.jsonl`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Notice the tactic of using delimiters.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Steps can be completely different operations.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.5 Providing examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Providing examples is an excellent way to guide the desired output of an LLM.
    There are numerous ways to demonstrate examples to an LLM. The system message/prompt
    can be a helpful way to emphasize general output. In the following listing, the
    example is added as the last LLM assistant reply, given the prompt “Teach me about
    Python.”
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.13 `providing_examples.jsonl`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Injects the sample output as the “previous” assistant reply'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Adds a limit output tactic to restrict the size of the output and match
    the example'
  prefs: []
  type: TYPE_NORMAL
- en: Providing examples can also be used to request a particular output format from
    a complex series of tasks that derive the output. For example, asking an LLM to
    produce code that matches a sample output is an excellent use of examples. We’ll
    employ this tactic throughout the book, but other methods exist for guiding output.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.6 Specifying output length
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The tactic of specifying output length can be helpful in not just limiting tokens
    but also in guiding the output to a desired format. Listing 2.14 shows an example
    of using two different techniques for this tactic. The first limits the output
    to fewer than 10 words. This can have the added benefit of making the response
    more concise and directed, which can be desirable for some use cases. The second
    example demonstrates limiting output to a concise set of bullet points. This method
    can help narrow down the output and keep answers short. More concise answers generally
    mean the output is more focused and contains less filler.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.14 `specifying_output_length.jsonl`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Restricting the output makes the answer more concise.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Restricts the answer to a short set of bullets'
  prefs: []
  type: TYPE_NORMAL
- en: Keeping answers brief can have additional benefits when developing multi-agent
    systems. Any agent system that converses with other agents can benefit from more
    concise and focused replies. It tends to keep the LLM more focused and reduces
    noisy communication.
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to run through all the examples of the prompt tactics for this strategy.
    As mentioned, we’ll cover other prompt engineering strategies and tactics in future
    chapters. We’ll finish this chapter by looking at how to pick the best LLM for
    your use case.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Choosing the optimal LLM for your specific needs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While being a successful crafter of AI agents doesn’t require an in-depth understanding
    of LLMs, it’s helpful to be able to evaluate the specifications. Like a computer
    user, you don’t need to know how to build a processor to understand the differences
    in processor models. This analogy holds well for LLMs, and while the criteria
    may be different, it still depends on some primary considerations.
  prefs: []
  type: TYPE_NORMAL
- en: From our previous discussion and look at LM Studio, we can extract some fundamental
    criteria that will be important to us when considering LLMs. Figure 2.11 explains
    the essential criteria to define what makes an LLM worth considering for creating
    a GPT agent or any LLM task.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 The important criteria to consider when consuming an LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For our purposes of building AI agents, we need to look at each of these criteria
    in terms related to the task. Model context size and speed could be considered
    the sixth and seventh criteria, but they are usually considered variations of
    a model deployment architecture and infrastructure. An eighth criterion to consider
    for an LLM is cost, but this depends on many other factors. Here is a summary
    of how these criteria relate to building AI agents:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Model performance* —You’ll generally want to understand the LLM’s performance
    for a given set of tasks. For example, if you’re building an agent specific to
    coding, then an LLM that performs well on code will be essential.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model parameters (size)* —The size of a model is often an excellent indication
    of inference performance and how well the model responds. However, the size of
    a model will also dictate your hardware requirements. If you plan to use your
    own locally hosted model, the model size will also primarily dictate the computer
    and GPU you need. Fortunately, we’re seeing small, very capable open source models
    being released regularly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case (model type)* —The type of model has several variations. Chat completions
    models such as ChatGPT are effective for iterating and reasoning through a problem,
    whereas models such as completion, question/answer, and instruct are more related
    to specific tasks. A chat completions model is essential for agent applications,
    especially those that iterate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training input* —Understanding the content used to train a model will often
    dictate the domain of a model. While general models can be effective across tasks,
    more specific or fine-tuned models can be more relevant to a domain. This may
    be a consideration for a domain-specific agent where a smaller, more fine-tuned
    model may perform as well as or better than a larger model such as GPT-4\.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training method* —It’s perhaps less of a concern, but it can be helpful to
    understand what method was used to train a model. How a model is trained can affect
    its ability to generalize, reason, and plan. This can be essential for planning
    agents but perhaps less significant for agents than for a more task-specific assistant.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Context token size* —The context size of a model is more specific to the model
    architecture and type. It dictates the size of context or memory the model may
    hold. A smaller context window of less than 4,000 tokens is typically more than
    enough for simple tasks. However, a large context window can be essential when
    using multiple agents—all conversing over a task. The models will typically be
    deployed with variations on the context window size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model speed (model deployment)* —The speed of a model is dictated by its *inference
    speed* (or how fast a model replies to a request), which in turn is dictated by
    the infrastructure it runs on. If your agent isn’t directly interacting with users,
    raw real-time speed may not be necessary. On the other hand, an LLM agent interacting
    in real time needs to be as quick as possible. For commercial models, speed will
    be determined and supported by the provider. Your infrastructure will determine
    the speed for those wanting to run their LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model cost (project budget)* —The cost is often dictated by the project. Whether
    learning to build an agent or implementing enterprise software, cost is always
    a consideration. A significant tradeoff exists between running your LLMs versus
    using a commercial API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a lot to consider when choosing which model you want to build a production
    agent system on. However, picking and working with a single model is usually best
    for research and learning purposes. If you’re new to LLMs and agents, you’ll likely
    want to choose a commercial option such as GPT-4 Turbo. Unless otherwise stated,
    the work in this book will depend on GPT-4 Turbo.
  prefs: []
  type: TYPE_NORMAL
- en: Over time, models will undoubtedly be replaced by better models. So you may
    need to upgrade or swap out models. To do this, though, you must understand the
    performance metrics of your LLMs and agents. Fortunately, in chapter 9, we’ll
    explore evaluating LLMs, prompts, and agent profiles with prompt flow.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the following exercises to help you engage with the material in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Exercise 1*—Consuming Different LLMs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Use the `connecting.py` code example to consume a different LLM
    from OpenAI or another provider.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks*:'
  prefs: []
  type: TYPE_NORMAL
- en: Modify `connecting.py` to connect to a different LLM.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose an LLM from OpenAI or another provider.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the API keys and endpoints in the code.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Execute the modified code and validate the response.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exercise 2*—Exploring Prompt Engineering Tactics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Explore various prompt engineering tactics, and create variations
    for each.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks:*'
  prefs: []
  type: TYPE_NORMAL
- en: Review the prompt engineering tactics covered in the chapter.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Write variations for each tactic, experimenting with different phrasing and
    structures.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Test the variations with an LLM to observe different outcomes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Document the results, and analyze the effectiveness of each variation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exercise 3*—Downloading and Running an LLM with LM Studio'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Download an LLM using LM Studio, and connect it to prompt engineering
    tactics.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks:*'
  prefs: []
  type: TYPE_NORMAL
- en: Install LM Studio on your machine.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Download an LLM using LM Studio.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Serve the model using LM Studio.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Write Python code to connect to the served model.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate the prompt engineering tactics example with the served model.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exercise 4*—Comparing Commercial and Open source LLMs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Compare the performance of a commercial LLM such as GPT-4 Turbo
    with an open source model using prompt engineering examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks:*'
  prefs: []
  type: TYPE_NORMAL
- en: Implement the prompt engineering examples using GPT-4 Turbo.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the implementation using an open source LLM.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the models based on criteria such as response accuracy, coherence,
    and speed.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Document the evaluation process, and summarize the findings.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exercise 5*—Hosting Alternatives for LLMs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Contrast and compare alternatives for hosting an LLM versus using
    a commercial model.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks:*'
  prefs: []
  type: TYPE_NORMAL
- en: Research different hosting options for LLMs (e.g., local servers, cloud services).
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the benefits and drawbacks of each hosting option.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare these options to using a commercial model in terms of cost, performance,
    and ease of use.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a report summarizing the comparison and recommending the best approach
    based on specific use cases.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs use a type of architecture called generative pretrained transformers (GPTs).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative models (e.g., LLMs and GPTs) differ from predictive/classification
    models by learning how to represent data and not simply classify it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are a collection of data, architecture, and training for specific use cases,
    called *fine-tuning.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenAI API SDK can be used to connect to an LLM from models, such as GPT-4,
    and also used to consume open source LLMs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can quickly set up Python environments and install the necessary packages
    for LLM integration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can handle various requests and generate unique responses that can be used
    to enhance programming skills related to LLM integration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source LLMs are an alternative to commercial models and can be hosted locally
    using tools such as LM Studio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering is a collection of techniques that help craft more effective
    prompts to improve LLM responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can be used to power agents and assistants, from simple chatbots to fully
    capable autonomous workers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the most suitable LLM for specific needs depends on the performance,
    parameters, use case, training input, and other criteria.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running LLMs locally requires a variety of skills, from setting up GPUs to understanding
    various configuration options.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
