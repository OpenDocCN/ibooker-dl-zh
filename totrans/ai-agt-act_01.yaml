- en: 2 Harnessing the power of large language models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 利用大型语言模型的力量
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding the basics of LLMs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解LLMs的基本知识
- en: Connecting to and consuming the OpenAI API
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到并使用OpenAI API
- en: Exploring and using open source LLMs with LM Studio
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LM Studio探索和使用开源LLMs
- en: Prompting LLMs with prompt engineering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用提示工程提示LLMs
- en: Choosing the optimal LLM for your specific needs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择适合您特定需求的最佳LLM
- en: The term *large language models* (LLMs) has now become a ubiquitous descriptor
    of a form of AI. These LLMs have been developed using generative pretrained transformers
    (GPTs). While other architectures also power LLMs, the GPT form is currently the
    most successful.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: “大型语言模型”（LLMs）这一术语现在已成为一种AI形式的普遍描述。这些LLMs是使用生成预训练的transformers（GPTs）开发的。虽然其他架构也能驱动LLMs，但GPT形式目前是最成功的。
- en: LLMs and GPTs are *generative* models, which means they are trained to *generate*
    rather than predict or classify content. To illustrate this further, consider
    figure 2.1, which shows the difference between generative and predictive/classification
    models. Generative models create something from the input, whereas predictive
    and classifying models classify it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs和GPTs是**生成**模型，这意味着它们被训练来**生成**内容，而不是预测或分类。为了进一步说明这一点，请考虑图2.1，它展示了生成模型与预测/分类模型之间的区别。生成模型从输入中创建内容，而预测和分类模型则对其进行分类。
- en: '![figure](../Images/2-1.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-1.png)'
- en: Figure 2.1 The difference between generative and predictive models
  id: totrans-10
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.1 生成模型与预测模型的区别
- en: We can further define an LLM by its constituent parts, as shown in figure 2.2\.
    In this diagram, *data* represents the content used to train the model, and *architecture*
    is an attribute of the model itself, such as the number of parameters or size
    of the model. Models are further trained specifically to the desired use case,
    including chat, completions, or instruction. Finally, *fine-tuning* is a feature
    added to models that refines the input data and model training to better match
    a particular use case or domain.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过其组成部分进一步定义LLM，如图2.2所示。在这个图中，“数据”代表用于训练模型的内容，“架构”是模型本身的属性，例如参数数量或模型大小。模型进一步被特定地训练以适应期望的应用场景，包括聊天、完成或指令。最后，“微调”是添加到模型中的功能，它通过优化输入数据和模型训练以更好地匹配特定用例或领域。
- en: '![figure](../Images/2-2.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-2.png)'
- en: Figure 2.2 The main elements that describe an LLM
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.2 描述LLM的主要元素
- en: The transformer architecture of GPTs, which is a specific architecture of LLMs,
    allows the models to be scaled to billions of parameters in size. This requires
    these large models to be trained on terabytes of documents to build a foundation.
    From there, these models will be successively trained using various methods for
    the desired use case of the model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: GPTs的transformer架构，作为LLMs的一种特定架构，使得模型可以扩展到数十亿个参数的大小。这要求这些大型模型在数TB的文档上进行训练以建立基础。从那里，这些模型将依次使用各种方法进行训练，以适应模型期望的应用场景。
- en: ChatGPT, for example, is trained effectively on the public internet and then
    fine-tuned using several training strategies. The final fine-tuning training is
    completed using an advanced form called *reinforcement learning with human feedback*
    (RLHF). This produces a model use case called chat completions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 以ChatGPT为例，它在公共互联网上进行了有效的训练，然后使用几种训练策略进行微调。最终的微调训练使用一种高级形式完成，称为**强化学习与人类反馈**（RLHF）。这产生了一个名为聊天完成的模型用例。
- en: '*Chat completions* LLMs are designed to improve through iteration and refinement—in
    other words, chatting. These models have also been benchmarked to be the best
    in task completion, reasoning, and planning, which makes them ideal for building
    agents and assistants. Completion models are trained/designed only to provide
    generated content on input text, so they don’t benefit from iteration.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**聊天完成**型LLMs旨在通过迭代和改进来提高性能——换句话说，就是聊天。这些模型也已被基准测试为在任务完成、推理和规划方面表现最佳，这使得它们非常适合构建代理人和助手。完成模型仅被训练/设计来在输入文本上提供生成内容，因此它们不受益于迭代。'
- en: For our journey to build powerful agents in this book, we focus on the class
    of LLMs called chat completions models. That, of course, doesn’t preclude you
    from trying other model forms for your agents. However, you may have to significantly
    alter the code samples provided to support other model forms.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们专注于构建强大代理人的旅程，我们关注的是被称为聊天完成模型的LLMs类别。当然，这并不妨碍你尝试为你的代理人使用其他模型形式。然而，你可能需要显著修改提供的代码示例以支持其他模型形式。
- en: We’ll uncover more details about LLMs and GPTs later in this chapter when we
    look at running an open source LLM locally. In the next section, we look at how
    to connect to an LLM using a growing standard from OpenAI.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面，当我们查看在本地运行开源 LLM 时，我们将揭示更多关于 LLM 和 GPT 的细节。在下一节中，我们将探讨如何使用 OpenAI 的一个日益增长的标准连接到
    LLM。
- en: 2.1 Mastering the OpenAI API
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 掌握 OpenAI API
- en: Numerous AI agents and assistant projects use the OpenAI API SDK to connect
    to an LLM. While not standard, the basic concepts describing a connection now
    follow the OpenAI pattern. Therefore, we must understand the core concepts of
    an LLM connection using the OpenAI SDK.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 AI 代理和助手项目使用 OpenAI API SDK 连接到 LLM。虽然不是标准，但现在描述连接的基本概念遵循 OpenAI 模式。因此，我们必须了解使用
    OpenAI SDK 进行 LLM 连接的核心概念。
- en: This chapter will look at connecting to an LLM model using the OpenAI Python
    SDK/package. We’ll discuss connecting to a GPT-4 model, the model response, counting
    tokens, and how to define consistent messages. Starting in the following subsection,
    we’ll examine how to use OpenAI.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨使用 OpenAI Python SDK/包连接到 LLM 模型。我们将讨论连接到 GPT-4 模型、模型响应、计数字符数以及如何定义一致的消息。从以下子节开始，我们将检查如何使用
    OpenAI。
- en: 2.1.1 Connecting to the chat completions model
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 连接到聊天完成模型
- en: To complete the exercises in this section and subsequent ones, you must set
    up a Python developer environment and get access to an LLM. Appendix A walks you
    through setting up an OpenAI account and accessing GPT-4 or other models. Appendix
    B demonstrates setting up a Python development environment with Visual Studio
    Code (VS Code), including installing needed extensions. Review these sections
    if you want to follow along with the scenarios.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本节及后续章节的练习，你必须设置一个 Python 开发环境并获取访问 LLM 的权限。附录 A 会指导你如何设置 OpenAI 账户并访问 GPT-4
    或其他模型。附录 B 展示了如何使用 Visual Studio Code (VS Code) 设置 Python 开发环境，包括安装所需的扩展。如果你想跟随场景进行，请回顾这些部分。
- en: Start by opening the source code `chapter_2` folder in VS Code and creating
    a new Python virtual environment. Again, refer to appendix B if you need assistance.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 首先在 VS Code 中打开源代码 `chapter_2` 文件夹，并创建一个新的 Python 虚拟环境。再次，如果需要帮助，请参考附录 B。
- en: Then, install the OpenAI and Python dot environment packages using the command
    in the following listing. This will install the required packages into the virtual
    environment.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用以下列表中的命令安装 OpenAI 和 Python dot 环境包。这将把所需的包安装到虚拟环境中。
- en: Listing 2.1 `pip` installs
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.1 `pip` 安装
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next, open the `connecting.py` file in VS Code, and inspect the code shown in
    listing 2.2\. Be sure to set the model’s name to an appropriate name—for example,
    gpt-4\. At the time of writing, the `gpt-4-1106-preview` was used to represent
    GPT-4 Turbo.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在 VS Code 中打开 `connecting.py` 文件，检查列表 2.2 中显示的代码。确保将模型的名称设置为合适的名称——例如，gpt-4。在撰写本文时，使用
    `gpt-4-1106-preview` 来表示 GPT-4 Turbo。
- en: Listing 2.2 `connecting.py`
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.2 `connecting.py`
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Loads the secrets stored in the .env file'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 加载 .env 文件中存储的秘密'
- en: '#2 Checks to see whether the key is set'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 检查是否设置了密钥'
- en: '#3 Creates a client with the key'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用密钥创建客户端'
- en: '#4 Uses the create function to generate a response'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 使用 create 函数生成响应'
- en: '#5 Returns just the content of the response'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 仅返回响应的内容'
- en: '#6 Executes the request and returns the response'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 执行请求并返回响应'
- en: A lot is happening here, so let’s break it down by section, starting with the
    beginning and loading the environment variables. In the `chapter_2` folder is
    another file called `.env`, which holds environment variables. These variables
    are set automatically by calling the `load_dotenv` function.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里发生了很多事情，所以让我们按部分分解，从开始加载环境变量开始。在 `chapter_2` 文件夹中还有一个名为 `.env` 的文件，它包含环境变量。这些变量通过调用
    `load_dotenv` 函数自动设置。
- en: You must set your OpenAI API key in the `.env` file, as shown in the next listing.
    Again, refer to appendix A to find out how to get a key and find a model name.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须在 `.env` 文件中设置你的 OpenAI API 密钥，如下一个列表所示。再次，请参考附录 A 了解如何获取密钥和找到模型名称。
- en: Listing 2.3 `.env`
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.3 `.env`
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After setting the key, you can debug the file by pressing the F5 key or selecting
    Run > Start Debugging from the VS Code menu. This will run the code, and you should
    see something like “The capital of France is Paris.”
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 设置密钥后，你可以通过按 F5 键或从 VS Code 菜单中选择“运行”>“开始调试”来调试文件。这将运行代码，你应该会看到类似“法国的首都是巴黎”的内容。
- en: Remember that the response from a generative model depends on the probability.
    The model will probably give us a correct and consistent answer in this case.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，生成模型的响应取决于概率。在这种情况下，模型很可能会给出正确且一致的答案。
- en: You can play with these probabilities by adjusting the temperature of the request.
    If you want a model to be more consistent, turn the temperature down to 0, but
    if you want the model to produce more variation, turn the temperature up. We’ll
    explore setting the temperature further in the next section.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过调整请求的温度来玩这些概率。如果您想让模型更一致，请将温度降低到 0，但如果您想让模型产生更多变化，请提高温度。我们将在下一节进一步探讨设置温度。
- en: 2.1.2 Understanding the request and response
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 理解请求和响应
- en: Digging into the chat completions request and response features can be helpful.
    We’ll focus on the request first, as shown next. The request encapsulates the
    intended model, the messages, and the temperature.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 深入研究聊天完成请求和响应功能可能会有所帮助。我们将首先关注请求，如下所示。请求封装了预期的模型、消息和温度。
- en: Listing 2.4 The chat completions request
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.4 聊天完成请求
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 The model or deployment used to respond to the request'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 用于响应请求的模型或部署'
- en: '#2 The system role message'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 系统角色消息'
- en: '#3 The user role message'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 用户角色消息'
- en: '#4 The temperature or variability of the request'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 请求的温度或变异性'
- en: 'Within the request, the `messages` block describes a set of messages and roles
    used in a request. Messages for a chat completions model can be defined in three
    roles:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在请求中，`messages` 块描述了一组用于请求的消息和角色。聊天完成模型的消息可以定义在三个角色中：
- en: '*System role* —A message that describes the request’s rules and guidelines.
    It can often be used to describe the role of the LLM in making the request.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*系统角色* — 描述请求的规则和指南的消息。它通常用于描述 LLM 在请求中的角色。'
- en: '*User role* —Represents and contains the message from the user.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用户角色* — 代表并包含用户的消息。'
- en: '*Assistant role* —Can be used to capture the message history of previous responses
    from the LLM. It can also inject a message history when perhaps none existed.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*助手角色* — 可以用来捕获 LLM 之前响应的消息历史。它还可以在可能没有消息历史的情况下注入消息历史。'
- en: The message sent in a single request can encapsulate an entire conversation,
    as shown in the JSON in the following listing.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 单个请求中发送的消息可以封装整个对话，如下一个列表中的 JSON 所示。
- en: Listing 2.5 Messages with history
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.5 带有历史记录的消息
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You can see how this can be applied by opening `message_history.py` in VS Code
    and debugging it by pressing F5\. After the file runs, be sure to check the output.
    Then, try to run the sample a few more times to see how the results change.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在 VS Code 中打开 `message_history.py` 并按 F5 调试来查看如何应用此功能。文件运行后，请务必检查输出。然后，尝试再次运行示例，以查看结果如何变化。
- en: The results will change from each run to the next due to the high temperature
    of `.7`. Go ahead and reduce the temperature to `.0`, and run the `message_history.py`
    sample a few more times. Keeping the temperature at `0` will show the same or
    similar results each time.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `.7` 的高温度，结果会从每次运行到下一次运行而变化。请继续将温度降低到 `.0`，并多次运行 `message_history.py` 示例。将温度保持在
    `0` 将每次都显示相同或类似的结果。
- en: Setting a request’s temperature will often depend on your particular use case.
    Sometimes, you may want to limit the responses’ stochastic nature (randomness).
    Reducing the temperature to `0` will give consistent results. Likewise, a value
    of `1.0` will give the most variability in the responses.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 设置请求的温度通常会取决于您的特定用例。有时，您可能想限制响应的随机性（随机性）。将温度降低到 `0` 将给出一致的结果。同样，`1.0` 的值将给出响应中最多的变异性。
- en: Next, we also want to know what information is being returned for each request.
    The next listing shows the output format for the response. You can see this output
    by running the `message_history.py` file in VS Code.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们还想了解每次请求返回的信息。下一个列表显示了响应的输出格式。您可以通过在 VS Code 中运行 `message_history.py`
    文件来查看此输出。
- en: Listing 2.6 Chat completions response
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表2.6 聊天完成响应
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 A model may return more than one response.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 模型可能返回多个响应。'
- en: '#2 Responses returned in the assistant role'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 在助手角色中返回的响应'
- en: '#3 Indicates the model used'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 指示使用的模型'
- en: '#4 Counts the number of input (prompt) and output (completion) tokens used'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 计算输入（提示）和输出（完成）标记的数量'
- en: It can be helpful to track the number of *input tokens* (those used in prompts)
    and the *output tokens* (the number returned through completions). Sometimes,
    minimizing and reducing the number of tokens can be essential. Having fewer tokens
    typically means LLM interactions will be cheaper, respond faster, and produce
    better and more consistent results.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪输入标记（用于提示中的标记）和输出标记（通过完成返回的标记数量）的数量可能会有所帮助。有时，最小化和减少标记数量可能是至关重要的。通常，标记数量较少意味着LLM交互将更便宜，响应更快，并产生更好、更一致的结果。
- en: That covers the basics of connecting to an LLM and returning responses. Throughout
    this book, we’ll review and expand on how to interact with LLMs. Until then, we’ll
    explore in the next section how to load and use open source LLMs.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这涵盖了连接到LLM并返回响应的基本知识。在整个书中，我们将回顾和扩展如何与LLM交互。在此之前，我们将在下一节中探讨如何加载和使用开源LLM。
- en: 2.2 Exploring open source LLMs with LM Studio
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 使用LM Studio探索开源LLM
- en: Commercial LLMs, such as GPT-4 from OpenAI, are an excellent place to start
    to learn how to use modern AI and build agents. However, commercial agents are
    an external resource that comes at a cost, reduces data privacy and security,
    and introduces dependencies. Other external influences can further complicate
    these factors.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 商业LLM，如OpenAI的GPT-4，是学习如何使用现代AI和构建代理的绝佳起点。然而，商业代理是一个外部资源，需要付费，会降低数据隐私和安全，并引入依赖。其他外部影响将进一步复杂化这些因素。
- en: It’s unsurprising that the race to build comparable open source LLMs is growing
    more competitive every day. As a result, there are now open source LLMs that may
    be adequate for numerous tasks and agent systems. There have even been so many
    advances in tooling in just a year that hosting LLMs locally is now very easy,
    as we’ll see in the next section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 建立与开源大型语言模型（LLM）相媲美的竞争日益激烈，这并不令人惊讶。因此，现在已经有了一些开源LLM，它们可能足够用于众多任务和代理系统。仅在一年内，工具的发展就取得了许多进步，以至于现在在本地托管LLM变得非常容易，正如我们将在下一节中看到的。
- en: 2.2.1 Installing and running LM Studio
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 安装和运行LM Studio
- en: 'LM Studio is a free download that supports downloading and hosting LLMs and
    other models locally for Windows, Mac, and Linux. The software is easy to use
    and offers several helpful features to get you started quickly. Here is a quick
    summary of steps to download and set up LM Studio:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: LM Studio是一个免费下载的软件，支持在Windows、Mac和Linux上本地下载和托管LLM和其他模型。该软件易于使用，并提供了一些有助于快速入门的有用功能。以下是下载和设置LM
    Studio的步骤快速总结：
- en: Download LM Studio from [https://lmstudio.ai/](https://lmstudio.ai/).
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://lmstudio.ai/](https://lmstudio.ai/)下载LM Studio。
- en: After downloading, install the software per your operating system. Be aware
    that some versions of LM Studio may be in beta and require installation of additional
    tools or libraries.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载后，根据您的操作系统安装软件。请注意，LM Studio的一些版本可能处于测试版，需要安装额外的工具或库。
- en: Launch the software.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动软件。
- en: Figure 2.3 shows the LM Studio window running. From there, you can review the
    current list of hot models, search for others, and even download. The home page
    content can be handy for understanding the details and specifications of the top
    models.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3显示了正在运行的LM Studio窗口。从那里，您可以查看当前的热门模型列表，搜索其他模型，甚至下载。主页内容对于了解顶级模型的详细信息和规格非常有用。
- en: '![figure](../Images/2-3.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-3.png)'
- en: Figure 2.3 LM Studio software showing the main home page
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.3 LM Studio软件显示主主页
- en: An appealing feature of LM Studio is its ability to analyze your hardware and
    align it with the requirements of a given model. The software will let you know
    how well you can run a given model. This can be a great time saver in guiding
    what models you experiment with.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: LM Studio的一个吸引人的特点是它能够分析您的硬件，并将其与给定模型的 requirements 对齐。软件将告诉您您能多好地运行给定模型。这可以在指导您尝试哪些模型时节省大量时间。
- en: Enter some text to search for a model, and click Go. You’ll be taken to the
    search page interface, as shown in figure 2.4\. From this page, you can see all
    the model variations and other specifications, such as context token size. After
    you click the Compatibility Guess button, the software will even tell you if the
    model will run on your system.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 输入一些文本以搜索模型，然后点击“Go”。您将被带到如图2.4所示的搜索页面界面。从该页面，您可以查看所有模型变体和其他规格，例如上下文标记大小。在您点击兼容性猜测按钮后，软件甚至会告诉您该模型是否能在您的系统上运行。
- en: '![figure](../Images/2-4.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/2-4.png)'
- en: Figure 2.4 The LM Studio search page
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图2.4 LM Studio搜索页面
- en: Click to download any model that will run on your system. You may want to stick
    with models designed for chat completions, but if your system is limited, work
    with what you have. In addition, if you’re unsure of which model to use, go ahead
    and download to try them. LM Studio is a great way to explore and experiment with
    many models.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: After the model is downloaded, you can then load and run the model on the chat
    page or as a server on the server page. Figure 2.5 shows loading and running a
    model on the chat page. It also shows the option for enabling and using a GPU
    if you have one.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-5.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 The LM Studio chat page with a loaded, locally running LLM
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To load and run a model, open the drop-down menu at the top middle of the page,
    and select a downloaded model. A progress bar will appear showing the model loading,
    and when it’s ready, you can start typing into the UI.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: The software even allows you to use some or all of your GPU, if detected, for
    the model inference. A GPU will generally speed up the model response times in
    some capacities. You can see how adding a GPU can affect the model’s performance
    by looking at the performance status at the bottom of the page, as shown in figure
    2.5.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Chatting with a model and using or playing with various prompts can help you
    determine how well a model will work for your given use case. A more systematic
    approach is using the prompt flow tool for evaluating prompts and LLMs. We’ll
    describe how to use prompt flow in chapter 9.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: LM Studio also allows a model to be run on a server and made accessible using
    the OpenAI package. We’ll see how to use the server feature and serve a model
    in the next section.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Serving an LLM locally with LM Studio
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Running an LLM locally as a server is easy with LM Studio. Just open the server
    page, load a model, and then click the Start Server button, as shown in figure
    2.6\. From there, you can copy and paste any of the examples to connect with your
    model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-6.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 The LM Studio server page and a server running an LLM
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You can review an example of the Python code by opening `chapter_2/lmstudio_
    server.py` in VS Code. The code is also shown here in listing 2.7\. Then, run
    the code in the VS Code debugger (press F5).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.7 `lmstudio_server.py`
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Currently not used; can be anything'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Feel free to change the message as you like.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Default code outputs the whole message.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: If you encounter problems connecting to the server or experience any other problems,
    be sure your configuration for the Server Model Settings matches the model type.
    For example, in figure 2.6, shown earlier, the loaded model differs from the server
    settings. The corrected settings are shown in figure 2.7.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-7.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 Choosing the correct Server Model Settings for the loaded model
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now, you can use a locally hosted LLM or a commercial model to build, test,
    and potentially even run your agents. The following section will examine how to
    build prompts using prompt engineering more effectively.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Prompting LLMs with prompt engineering
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A prompt defined for LLMs is the message content used in the request for better
    response output. *Prompt engineering* is a new and emerging field that attempts
    to structure a methodology for building prompts. Unfortunately, prompt building
    isn’t a well-established science, and there is a growing and diverse set of methods
    defined as prompt engineering.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, organizations such as OpenAI have begun documenting a universal
    set of strategies, as shown in figure 2.8\. These strategies cover various tactics,
    some requiring additional infrastructure and considerations. As such, the prompt
    engineering strategies relating to more advanced concepts will be covered in the
    indicated chapters.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-8.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: Figure 2.8 OpenAI prompt engineering strategies reviewed in this book, by chapter
    location
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Each strategy in figure 2.8 unfolds into tactics that can further refine the
    specific method of prompt engineering. This chapter will examine the fundamental
    Write Clear Instructions strategy. Figure 2.9 shows the tactics for this strategy
    in more detail, along with examples for each tactic. We’ll look at running these
    examples using a code demo in the following sections.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-9.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 The tactics for the Write Clear Instructions strategy
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The Write Clear Instructions strategy is about being careful and specific about
    what you ask for. Asking an LLM to perform a task is no different from asking
    a person to complete the same task. Generally, the more information and context
    relevant to a task you can specify in a request, the better the response.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: This strategy has been broken down into specific tactics you can apply to prompts.
    To understand how to use those, a code demo (`prompt_engineering.py`) with various
    prompt examples is in the `chapter 2` source code folder.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Open the `prompt_engineering.py` file in VS Code, as shown in listing 2.8\.
    This code starts by loading all the JSON Lines files in the `prompts` folder.
    Then, it displays the list of files as choices and allows the user to select a
    prompt option. After selecting the option, the prompts are submitted to an LLM,
    and the response is printed.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.8 `prompt_engineering.py` `(main())`
  id: totrans-119
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '#1 Collects all the files for the given folder'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Prints the list of files as choices'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Inputs the user’s choice'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Loads the prompt and parses it into messages'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Submits the prompt to an OpenAI LLM'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: A commented-out section from the listing demonstrates how to connect to a local
    LLM. This will allow you to explore the same prompt engineering tactics applied
    to open source LLMs running locally. By default, this example uses the OpenAI
    model we configured previously in section 2.1.1\. If you didn’t complete that
    earlier, please go back and do it before running this one.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的注释部分演示了如何连接到本地 LLM。这将允许你探索应用于本地运行的开源 LLM 的相同提示工程策略。默认情况下，此示例使用我们在 2.1.1
    节中配置的 OpenAI 模型。如果你没有完成之前的操作，请返回并完成它，然后再运行此示例。
- en: Figure 2.10 shows the output of running the prompt engineering tactics tester,
    the `prompt_engineering.py` file in VS Code. When you run the tester, you can
    enter a value for the tactic you want to test and watch it run.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10 显示了运行提示工程策略测试器的输出，即 VS Code 中的 `prompt_engineering.py` 文件。当你运行测试器时，你可以为要测试的策略输入一个值，并观察其运行。
- en: '![figure](../Images/2-10.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/2-10.png)'
- en: Figure 2.10 The output of the prompt engineering tactics tester
  id: totrans-129
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 2.10 提示工程策略测试器的输出
- en: In the following sections, we’ll explore each prompt tactic in more detail.
    We’ll also examine the various examples.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将更详细地探讨每个提示策略。我们还将检查各种示例。
- en: 2.3.1 Creating detailed queries
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 创建详细查询
- en: The basic premise of this tactic is to provide as much detail as possible but
    also to be careful not to give irrelevant details. The following listing shows
    the JSON Lines file examples for exploring this tactic.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 此策略的基本前提是尽可能提供详细的信息，但也要小心不要提供无关紧要的细节。以下列表显示了用于探索此策略的 JSON Lines 文件示例。
- en: Listing 2.9 `detailed_queries.jsonl`
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.9 `detailed_queries.jsonl`
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 The first example doesn’t use detailed queries.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 第一个示例没有使用详细的查询。'
- en: '#2 First ask the LLM a very general question.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 首先向 LLM 提出一个非常一般的问题。'
- en: '#3 Ask a more specific question, and ask for examples.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 提出一个更具体的问题，并请求示例。'
- en: This example demonstrates the difference between using detailed queries and
    not. It also goes a step further by asking for examples. Remember, the more relevance
    and context you can provide in your prompt, the better the overall response. Asking
    for examples is another way of enforcing the relationship between the question
    and the expected output.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例演示了使用详细查询和不使用查询之间的差异。它还进一步通过请求示例。记住，你能在提示中提供越多相关性和上下文，整体响应就越好。请求示例是加强问题与预期输出之间关系的一种方式。
- en: 2.3.2 Adopting personas
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 采用角色
- en: Adopting personas grants the ability to define an overarching context or set
    of rules to the LLM. The LLM can then use that context and/or rules to frame all
    later output responses. This is a compelling tactic and one that we’ll make heavy
    use of throughout this book.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 采用角色赋予 LLM 定义一个总体上下文或一组规则的能力。LLM 可以然后使用该上下文和/或规则来构建所有后续的输出响应。这是一个有吸引力的策略，我们将在整本书中大量使用它。
- en: Listing 2.10 shows an example of employing two personas to answer the same question.
    This can be an enjoyable technique for exploring a wide range of novel applications,
    from getting demographic feedback to specializing in a specific task or even rubber
    ducking.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.10 显示了使用两个角色回答相同问题的示例。这可以是一种探索广泛新颖应用的愉快技术，从获取人口统计反馈到专门从事特定任务，甚至橡皮鸭技术。
- en: GPT rubber ducking
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: GPT 橡皮鸭
- en: '*Rubber ducking* is a problem-solving technique in which a person explains
    a problem to an inanimate object, like a rubber duck, to understand or find a
    solution. This method is prevalent in programming and debugging, as articulating
    the problem aloud often helps clarify the problem and can lead to new insights
    or solutions.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*橡皮鸭* 是一种问题解决技术，其中一个人向一个无生命物体（如橡皮鸭）解释一个问题，以理解或找到解决方案。这种方法在编程和调试中很常见，因为大声阐述问题往往有助于澄清问题，并可能导致新的见解或解决方案。'
- en: GPT rubber ducking uses the same technique, but instead of an inanimate object,
    we use an LLM. This strategy can be expanded further by giving the LLM a persona
    specific to the desired solution domain.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 橡皮鸭技术使用相同的技巧，但使用的是 LLM 而不是无生命物体。通过给 LLM 赋予特定于所需解决方案领域的角色，这种策略可以进一步扩展。
- en: Listing 2.10 `adopting_personas.jsonl`
  id: totrans-145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 2.10 `adopting_personas.jsonl`
- en: '[PRE9]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 First persona'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 第一个角色'
- en: '#2 Second persona'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 第二个角色'
- en: A core element of agent profiles is the persona. We’ll employ various personas
    to assist agents in completing their tasks. When you run this tactic, pay particular
    attention to the way the LLM outputs the response.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 代理配置文件的核心元素是角色。我们将使用各种角色来帮助代理完成他们的任务。当你运行此策略时，请特别注意 LLM 输出响应的方式。
- en: 2.3.3 Using delimiters
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Delimiters are a useful way of isolating and getting the LLM to focus on some
    part of a message. This tactic is often combined with other tactics but can work
    well independently. The following listing demonstrates two examples, but there
    are several other ways of describing delimiters, from XML tags to using markdown.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.11 `using_delimiters.jsonl`
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 The delimiter is defined by character type and repetition.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The delimiter is defined by XML standards.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: When you run this tactic, pay attention to the parts of the text the LLM focuses
    on when it outputs the response. This tactic can be beneficial for describing
    information in a hierarchy or other relationship patterns.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.4 Specifying steps
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Specifying steps is another powerful tactic that can have many uses, including
    in agents, as shown in listing 2.12\. It’s especially powerful when developing
    prompts or agent profiles for complex multistep tasks. You can specify steps to
    break down these complex prompts into a step-by-step process that the LLM can
    follow. In turn, these steps can guide the LLM through multiple interactions over
    a more extended conversation and many iterations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.12 `specifying_steps.jsonl`
  id: totrans-159
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 Notice the tactic of using delimiters.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Steps can be completely different operations.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.5 Providing examples
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Providing examples is an excellent way to guide the desired output of an LLM.
    There are numerous ways to demonstrate examples to an LLM. The system message/prompt
    can be a helpful way to emphasize general output. In the following listing, the
    example is added as the last LLM assistant reply, given the prompt “Teach me about
    Python.”
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.13 `providing_examples.jsonl`
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 Injects the sample output as the “previous” assistant reply'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Adds a limit output tactic to restrict the size of the output and match
    the example'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Providing examples can also be used to request a particular output format from
    a complex series of tasks that derive the output. For example, asking an LLM to
    produce code that matches a sample output is an excellent use of examples. We’ll
    employ this tactic throughout the book, but other methods exist for guiding output.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.6 Specifying output length
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The tactic of specifying output length can be helpful in not just limiting tokens
    but also in guiding the output to a desired format. Listing 2.14 shows an example
    of using two different techniques for this tactic. The first limits the output
    to fewer than 10 words. This can have the added benefit of making the response
    more concise and directed, which can be desirable for some use cases. The second
    example demonstrates limiting output to a concise set of bullet points. This method
    can help narrow down the output and keep answers short. More concise answers generally
    mean the output is more focused and contains less filler.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Listing 2.14 `specifying_output_length.jsonl`
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Restricting the output makes the answer more concise.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Restricts the answer to a short set of bullets'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Keeping answers brief can have additional benefits when developing multi-agent
    systems. Any agent system that converses with other agents can benefit from more
    concise and focused replies. It tends to keep the LLM more focused and reduces
    noisy communication.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to run through all the examples of the prompt tactics for this strategy.
    As mentioned, we’ll cover other prompt engineering strategies and tactics in future
    chapters. We’ll finish this chapter by looking at how to pick the best LLM for
    your use case.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Choosing the optimal LLM for your specific needs
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While being a successful crafter of AI agents doesn’t require an in-depth understanding
    of LLMs, it’s helpful to be able to evaluate the specifications. Like a computer
    user, you don’t need to know how to build a processor to understand the differences
    in processor models. This analogy holds well for LLMs, and while the criteria
    may be different, it still depends on some primary considerations.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: From our previous discussion and look at LM Studio, we can extract some fundamental
    criteria that will be important to us when considering LLMs. Figure 2.11 explains
    the essential criteria to define what makes an LLM worth considering for creating
    a GPT agent or any LLM task.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/2-11.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: Figure 2.11 The important criteria to consider when consuming an LLM
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'For our purposes of building AI agents, we need to look at each of these criteria
    in terms related to the task. Model context size and speed could be considered
    the sixth and seventh criteria, but they are usually considered variations of
    a model deployment architecture and infrastructure. An eighth criterion to consider
    for an LLM is cost, but this depends on many other factors. Here is a summary
    of how these criteria relate to building AI agents:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '*Model performance* —You’ll generally want to understand the LLM’s performance
    for a given set of tasks. For example, if you’re building an agent specific to
    coding, then an LLM that performs well on code will be essential.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model parameters (size)* —The size of a model is often an excellent indication
    of inference performance and how well the model responds. However, the size of
    a model will also dictate your hardware requirements. If you plan to use your
    own locally hosted model, the model size will also primarily dictate the computer
    and GPU you need. Fortunately, we’re seeing small, very capable open source models
    being released regularly.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Use case (model type)* —The type of model has several variations. Chat completions
    models such as ChatGPT are effective for iterating and reasoning through a problem,
    whereas models such as completion, question/answer, and instruct are more related
    to specific tasks. A chat completions model is essential for agent applications,
    especially those that iterate.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training input* —Understanding the content used to train a model will often
    dictate the domain of a model. While general models can be effective across tasks,
    more specific or fine-tuned models can be more relevant to a domain. This may
    be a consideration for a domain-specific agent where a smaller, more fine-tuned
    model may perform as well as or better than a larger model such as GPT-4\.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training method* —It’s perhaps less of a concern, but it can be helpful to
    understand what method was used to train a model. How a model is trained can affect
    its ability to generalize, reason, and plan. This can be essential for planning
    agents but perhaps less significant for agents than for a more task-specific assistant.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Context token size* —The context size of a model is more specific to the model
    architecture and type. It dictates the size of context or memory the model may
    hold. A smaller context window of less than 4,000 tokens is typically more than
    enough for simple tasks. However, a large context window can be essential when
    using multiple agents—all conversing over a task. The models will typically be
    deployed with variations on the context window size.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model speed (model deployment)* —The speed of a model is dictated by its *inference
    speed* (or how fast a model replies to a request), which in turn is dictated by
    the infrastructure it runs on. If your agent isn’t directly interacting with users,
    raw real-time speed may not be necessary. On the other hand, an LLM agent interacting
    in real time needs to be as quick as possible. For commercial models, speed will
    be determined and supported by the provider. Your infrastructure will determine
    the speed for those wanting to run their LLMs.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model cost (project budget)* —The cost is often dictated by the project. Whether
    learning to build an agent or implementing enterprise software, cost is always
    a consideration. A significant tradeoff exists between running your LLMs versus
    using a commercial API.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a lot to consider when choosing which model you want to build a production
    agent system on. However, picking and working with a single model is usually best
    for research and learning purposes. If you’re new to LLMs and agents, you’ll likely
    want to choose a commercial option such as GPT-4 Turbo. Unless otherwise stated,
    the work in this book will depend on GPT-4 Turbo.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Over time, models will undoubtedly be replaced by better models. So you may
    need to upgrade or swap out models. To do this, though, you must understand the
    performance metrics of your LLMs and agents. Fortunately, in chapter 9, we’ll
    explore evaluating LLMs, prompts, and agent profiles with prompt flow.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Exercises
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the following exercises to help you engage with the material in this chapter:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '*Exercise 1*—Consuming Different LLMs'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Use the `connecting.py` code example to consume a different LLM
    from OpenAI or another provider.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks*:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Modify `connecting.py` to connect to a different LLM.
  id: totrans-199
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose an LLM from OpenAI or another provider.
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the API keys and endpoints in the code.
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Execute the modified code and validate the response.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exercise 2*—Exploring Prompt Engineering Tactics'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Explore various prompt engineering tactics, and create variations
    for each.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks:*'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Review the prompt engineering tactics covered in the chapter.
  id: totrans-206
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Write variations for each tactic, experimenting with different phrasing and
    structures.
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Test the variations with an LLM to observe different outcomes.
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Document the results, and analyze the effectiveness of each variation.
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exercise 3*—Downloading and Running an LLM with LM Studio'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Download an LLM using LM Studio, and connect it to prompt engineering
    tactics.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks:*'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Install LM Studio on your machine.
  id: totrans-213
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Download an LLM using LM Studio.
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Serve the model using LM Studio.
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Write Python code to connect to the served model.
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrate the prompt engineering tactics example with the served model.
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exercise 4*—Comparing Commercial and Open source LLMs'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Compare the performance of a commercial LLM such as GPT-4 Turbo
    with an open source model using prompt engineering examples.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks:*'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Implement the prompt engineering examples using GPT-4 Turbo.
  id: totrans-221
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Repeat the implementation using an open source LLM.
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the models based on criteria such as response accuracy, coherence,
    and speed.
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Document the evaluation process, and summarize the findings.
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exercise 5*—Hosting Alternatives for LLMs'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Contrast and compare alternatives for hosting an LLM versus using
    a commercial model.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks:*'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Research different hosting options for LLMs (e.g., local servers, cloud services).
  id: totrans-228
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate the benefits and drawbacks of each hosting option.
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare these options to using a commercial model in terms of cost, performance,
    and ease of use.
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Write a report summarizing the comparison and recommending the best approach
    based on specific use cases.
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs use a type of architecture called generative pretrained transformers (GPTs).
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative models (e.g., LLMs and GPTs) differ from predictive/classification
    models by learning how to represent data and not simply classify it.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs are a collection of data, architecture, and training for specific use cases,
    called *fine-tuning.*
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenAI API SDK can be used to connect to an LLM from models, such as GPT-4,
    and also used to consume open source LLMs.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can quickly set up Python environments and install the necessary packages
    for LLM integration.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can handle various requests and generate unique responses that can be used
    to enhance programming skills related to LLM integration.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source LLMs are an alternative to commercial models and can be hosted locally
    using tools such as LM Studio.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering is a collection of techniques that help craft more effective
    prompts to improve LLM responses.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs can be used to power agents and assistants, from simple chatbots to fully
    capable autonomous workers.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the most suitable LLM for specific needs depends on the performance,
    parameters, use case, training input, and other criteria.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running LLMs locally requires a variety of skills, from setting up GPUs to understanding
    various configuration options.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
