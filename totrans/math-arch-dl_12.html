<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="ch-fully-bayes-model-parameter-estimation">13 Fully Bayes model parameter estimation</h1>
<p class="co-summary-head">This chapter covers</p>
<ul class="calibre6">
<li class="co-summary-bullet">Fully Bayes parameter estimation for unsupervised modeling</li>
<li class="co-summary-bullet">Injecting prior belief into parameter estimation</li>
<li class="co-summary-bullet">Estimating Gaussian likelihood parameters with known or unknown mean and precision</li>
<li class="co-summary-bullet">Normal-gamma and Wishart distributions</li>
</ul>
<p class="body"><a id="marker-447"/>Suppose we have a data set of interest: say, all images containing a cat. If we represent images as points in a high-dimensional feature space, our data set of interest forms a subspace of that feature space. Now we want to create an <i class="fm-italics">unsupervised</i> model for our data set of interest. This means we want to identify a probability density function <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> whose sample cloud (the set of points obtained by repeatedly sampling the probability distribution many times) largely overlaps our subspace of interest. Of course, we do not know the exact subspace of interest, but we have collected a set of samples <i class="timesitalic">X</i> from the data set of interest: that is, the training data. We can use the point cloud for <i class="timesitalic">X</i> as a surrogate for the unknown subspace of interest. Thus, we are essentially trying to identify a probability density function <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span> whose sample cloud, by and large, overlaps <i class="timesitalic">X</i>.</p>
<p class="body">Once we have the model <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>)</span>, we can use it to generate more data samples; these will be computer-generated cat images. This is generative modeling. Also, given a new image <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>, we can estimate the probability of it being an image of a cat by evaluating <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_a.png" width="14"/></span>)</span>.</p>
<h2 class="fm-head" id="fully-bayes-estimation-an-informal-introduction">13.1 Fully Bayes estimation: An informal introduction</h2>
<p class="body"><a id="marker-448"/>Let‚Äôs recap Bayes‚Äô theorem:</p><!--<p class="Body"><span class="times">$$\overbrace{p\left(  \theta  \middle\vert  X
\right)}^{\text{posterior  probability}}  = \frac{ p\left( X, \theta
\right) }{ p\left(X \right) }
= \frac{ \overbrace{p\left( X \middle\vert \theta
\right)}^{\text{likelihood}}  \;  \overbrace{p\left( \theta
\right)}^{\text{prior  probability}} }{ \underbrace{p\left(X
\right)}_{evidence} }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="122" src="../../OEBPS/Images/eq_13-01.png" width="400"/></p>
</div>
<p class="fm-equation-caption">Equation 13.1 <span class="calibre" id="eq-posterior-prior-likelihood-evidence-ch12"/></p>
<p class="body">Here, <span class="math"><i class="fm-italics">X</i> = {<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">1</sub>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sub class="fm-subscript">2</sub>,‚ãØ}</span> denotes the training data set. Our ultimate goal is to identify the likelihood function <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<i class="fm-italics">Œ∏</i>)</span>. Estimating the likelihood function has two aspects: selecting the function family and estimating the parameters. We usually preselect the family from our knowledge of the problem and then estimate the model parameters. For instance, the family for our model likelihood function might be Gaussian: <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<i class="fm-italics">Œ∏</i>) = <span class="cambria">ùí©</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>,¬†<b class="fm-bold">Œ£</b>)</span> as before, the semicolon separates the model variables from model parameters). Then <span class="math"><i class="fm-italics">Œ∏</i> = {<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>,¬†<b class="fm-bold">Œ£</b>}</span> are the model parameters to estimate. We estimate <i class="timesitalic">Œ∏</i> such that the overall likelihood <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i>|<i class="fm-italics">Œ∏</i>) = ‚àè<i class="fm-italics"><sub class="fm-subscript">i¬†</sub>p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sub class="fm-subscript">i</sub></i>|<i class="fm-italics">Œ∏</i>)</span> best fits the training data <i class="timesitalic">X</i>.</p>
<p class="body">We want to re-emphasize the mental picture that <i class="fm-italics">best fit</i> implies that the sample cloud of the likelihood function (repeated samples from <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<i class="fm-italics">Œ∏</i>)</span>) largely overlaps the training data set <i class="timesitalic">X</i>. For the Gaussian case, this implies that the mean <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span> should fall at a place where there is a very high concentration of training data points, and the covariance matrix <span class="timesbold">Œ£</span> should be such that the elliptical base of the likelihood function tightly contains as many training data points as possible.</p>
<h3 class="fm-head1" id="parameter-estimation-and-belief-injection">13.1.1 Parameter estimation and belief injection</h3>
<p class="body">There are various possible approaches to parameter estimation. The simplest approach is <i class="fm-italics">maximum likelihood parameter estimation</i> MLE), introduced in section <a class="url" href="../Text/06.xhtml#sec-max_likelihood_estimation">6.6.2</a>. In MLE, we choose the parameter values that maximize <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i>|<i class="fm-italics">Œ∏</i>)</span>, the likelihood of observing the training data set. This makes some sense. After all, the only thing we know to be true is that the input data set <i class="timesitalic">X</i> <i class="fm-italics">has been observed</i>‚Äîthis being unsupervised data, we do not know anything else. It is reasonable to choose the parameters that maximize the probability of that known truth. If the training data set is large, MLE estimation works well.</p>
<p class="body">However, in the absence of a sufficiently large amount of training data, it often helps to inject our prior knowledge about the system into the estimation‚Äîprior knowledge can cover for the lack of data. This injection of guess/belief into the system is done via the prior probability density. To do this, we can no longer maximize the likelihood, as likelihood ignores the prior. We have to do maximum a posteriori (MAP) estimation, which maximizes the posterior probability. The posterior probability is the product of likelihood (which depends on the data) and the prior (which does not depend on data; we will make it reflect our prior belief).</p>
<p class="body">There are two possible MAP paradigms. We saw one of them in section <a class="url" href="../Text/06.xhtml#sec-MAP_estimation">6.6.3</a>, where we injected our belief that the unknown parameters must be <i class="fm-italics">small</i> in magnitude and set <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œ∏</i>) <span class="cambria">‚àù</span> <i class="fm-italics">e</i><sup class="fm-superscript">‚àí||<i class="fm-italics1">Œ∏</i>||<sup class="fm-superscript">2</sup></sup></span> as a <i class="fm-italics">regularizer</i>. The system was incentivized to select parameter values that are relatively smaller in magnitude. In this chapter, we study a different paradigm; let‚Äôs illustrate it with an example.</p>
<p class="body">Suppose we model the likelihood as a Gaussian: <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<i class="fm-italics">Œ∏</i>) = <span class="cambria">ùí©</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>,¬†<b class="fm-bold">Œ£</b>)</span>. We have to estimate the parameters <span class="math"><i class="fm-italics">Œ∏</i> = {<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>,¬†<b class="fm-bold">Œ£</b>}</span> from the training data <i class="timesitalic">X</i>, for which we must maximize the posterior probability. To compute the posterior probability, we need the prior probability. In addition, we must somehow inject constant values as our belief (lacking observed data) about the parameter values. How about modeling the prior probability as a Gaussian probability density function in the parameter space? Ignoring the covariance matrix parameter for the sake of simplicity, we can model the probability density of the mean parameter as <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>) = <span class="cambria">ùí©</span>(<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>; <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">0</sub>, <b class="fm-bold">Œ£</b><sub class="fm-subscript">0</sub>)</span>. We are essentially saying that we believe the parameter <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span> is likely to have a value near <span class="math"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">0</sub></span> with a confidence <span class="math"><b class="fm-bold">Œ£</b><sub class="fm-subscript">0</sub></span>. In other words, we are injecting a constant value as our belief in the parameter <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>. We can treat the covariance similarly. Later, we prove that in this paradigm, with a low volume of training data, the prior dominates. Once sufficient training data is digested, the effect of the prior fades, and the solution gets closer and closer to the MLE. This is the fully Bayes parameter estimation technique in a nutshell.<a id="marker-449"/></p>
<p class="body">In this chapter, we discuss Bayes estimation of parameters for a Gaussian likelihood function for a series of increasingly complex scenarios. In section <a class="url" href="#sec-bayesinf-muonly">13.3</a>, we deal with the case where the variance of the parameters to be estimated is known (constant) but the mean is unknown, so the mean is expressed as a (Gaussian) random variable. Then, in section <a class="url" href="#sec-bayesinf-sigmaonly">13.6</a>, we examine the case where the mean is known (constant) but the variance is unknown. Finally, in section <a class="url" href="#sec-bayesinf-musigma">13.7</a>, we examine the case where both are unknown. Both the univariate and multivariate cases are dealt with for each scenario.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for this chapter, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/woYW">http://mng.bz/woYW</a>.</p>
<h2 class="fm-head" id="sec-mle-recap">13.2 MLE for Gaussian parameter values recap)</h2>
<p class="body">We have discussed the details of this in section <a class="url" href="../Text/06.xhtml#sec-gauss_max_likelihood_estimation">6.8</a>. Here we recap the main results. Suppose we have a data set <span class="math"><i class="fm-italics">X</i> = {<i class="fm-italics">x</i><sup class="fm-superscript">(1)</sup>, <i class="fm-italics">x</i><sup class="fm-superscript">(2)</sup>,‚ãØ, <i class="fm-italics">x</i><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>}</span>. We have decided to model the data distribution as a Gaussian <span class="math"><span class="cambria">ùí©</span>(<i class="fm-italics">x</i>;<i class="fm-italics">Œº</i>, <i class="fm-italics">œÉ</i>)</span>‚Äîwe want to estimate the parameters <i class="timesitalic">Œº</i>, <i class="timesitalic">œÉ</i> that best ‚Äúexplain" or ‚Äúfit" the observed data set <i class="timesitalic">X</i>. MLE is one of the simplest approaches to solving this problem. Here we estimate the parameters such that <i class="fm-italics">the likelihood of the data observed during training is maximized</i>. This can be loosely visualized as estimating a probability density function whose peak coincides with the region in the input space with the densest population of training data. We looked at MLE in section <a class="url" href="../Text/06.xhtml#sec-gauss_max_likelihood_estimation">6.8</a>. Here we simply restate the expressions.</p>
<p class="body">Let‚Äôs denote the (as yet unknown) mean and variance of the data distribution as <i class="timesitalic">Œº</i> and <i class="timesitalic">œÉ</i>. Then from equation <a class="url" href="../Text/05.xhtml#eq-univar-normal">5.22</a>, we get</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;p\left(x^{ \left( i \right) } \middle\vert \mu, \sigma \right)
=  \frac{1}{{\sqrt {2\pi } \sigma}}e^{{\frac{ - \left( {x^{ \left( i
\right) } - \mu } \right)^2 } {2\sigma ^2 }}}\\
&amp;p\left( X \middle\vert \mu, \sigma \right) = p\left(x^{ \left( 1
\right) }, \cdots, x^{ \left( n \right) } \middle\vert \mu, \sigma
\right)
= \prod_{i=1}^{n} p\left(x^{ \left( i \right) } \middle\vert \mu, \sigma
\right)
= \frac{1}{\left(\sqrt {2\pi } \sigma\right)^{n}} e^{{\frac{
-\sum\displaylimits_{i=1}^{n} \left( {x^{ \left( i \right) } - \mu }
\right)^2 } {2\sigma ^2 }}}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="142" src="../../OEBPS/Images/eq_13-01-a.png" width="615"/></p>
</div>
<p class="body">Maximizing the log-likelihood <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i>|<i class="fm-italics">Œº</i>, <i class="fm-italics">œÉ</i>)</span> has a closed-form solution:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\mu = \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x^{ \left( i \right) }
\nonumber \\
&amp;\sigma^{2} = s = \frac{1}{n}\sum_{i=1}^{n} \left( {x^{ \left( i
\right) } - \bar{x} } \right)^2\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="119" src="../../OEBPS/Images/eq_13-02.png" width="196"/></p>
</div>
<p class="fm-equation-caption">Equation 13.2 <span class="calibre" id="eq-gauss-MLE-univar-ch-bayesinf"/></p>
<p class="body">Thus the MLE mean and variance are essentially the mean and variance of the training data samples (see section <a class="url" href="../Text/06.xhtml#sec-model_param_estimation">6.6</a> for the derivation of these expressions).</p>
<p class="body">The corresponding expressions for multivariate Gaussian MLE are</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\vec{\mu} =  \bar{\vec{x}} = \frac{1}{n} \sum_{i=1}^{n} \vec{x}^{
\left( i \right) } \nonumber \\
&amp;\boldsymbol{\Sigma} =  S = \frac{1}{n} \sum_{i=1}^{n} \left(
\vec{x}^{ \left( i \right) } - \bar{\vec{x}} \right) \left( \vec{x}^{
\left( i \right) } - \bar{\vec{x}} \right) ^{T}
\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="121" src="../../OEBPS/Images/eq_13-03.png" width="270"/></p>
</div>
<p class="fm-equation-caption">Equation 13.3 <span class="calibre" id="eq-gauss-MLE-multivar-ch-bayesinf"/></p>
<p class="body">These MLE parameter values are to be used to evaluate <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>) = <span class="cambria">ùí©</span>(<i class="fm-italics">x</i>;<i class="fm-italics">Œº</i>, <i class="fm-italics">œÉ</i>)</span>‚Äîthe probability of an unknown data point <i class="timesitalic">x</i> coming from the distribution represented by the training data set <i class="timesitalic">X</i>.<a id="marker-450"/></p>
<h2 class="fm-head" id="sec-bayesinf-muonly">13.3 Fully Bayes parameter estimation: Gaussian, unknown mean, known precision</h2>
<p class="body">MLE may not be that accurate when the available data set is small that is, <i class="timesitalic">n</i>, size of the data set <i class="timesitalic">X</i>, is small). In many problems, we have a prior idea of the mean and sigma of the data set. Unfortunately, MLE provides no way to bake such a prior belief into the estimation. Fully Bayes parameter estimation techniques try to fix this drawback: here we are not simply maximizing the likelihood of the observed data. Instead, we maximize the posterior probability of the estimated parameter(s). This posterior probability involves the product of the likelihood and a prior probability (see equation <a class="url" href="#eq-posterior-prior-likelihood-evidence-ch12">13.1</a>). The likelihood term captures the effect of the training data‚Äîmaximizing it alone is MLE‚Äîbut does not capture the effect of a prior belief. On the other hand, the prior term does not depend on the data. This is where we bake in our belief or guess or prior knowledge about the data distribution. Thus, our estimate for the data distribution parameters will consider the data and the prior guess. We will soon see that the estimation is such that as the size of the data set (<i class="timesitalic">n</i>, length of <i class="timesitalic">X</i>) increases, the effect of the prior term decreases, and the effect of the likelihood term increases. In the limit, at infinite data availability, the Bayesian inference ields the MLE. At the other extreme, when no data is available (<span class="math"><i class="fm-italics">n</i> = 0</span>), the Fully Bayes estimates for the parameters are the same as the prior estimates.</p>
<p class="body">Let‚Äôs we examine Bayesian parameter estimation. For starters, we deal with a relatively simple case where we have a Gaussian data distribution with a known (constant) variance but unknown and modeled mean. The data distribution is Gaussian (as usual, the semicolon in <span class="math"><span class="cambria">ùí©</span>(<i class="fm-italics">x</i>;<i class="fm-italics">Œº<sub class="fm-subscript">n</sub></i>, <i class="fm-italics">œÉ</i>)</span> separates the variables from parameters):</p><!--<p class="Body"><span class="times">$$p\left( x \middle\vert \mu, \sigma \right) = p\left( x \middle\vert \mu \right) = \mathcal{N} \left( x; \mu, \sigma
\right) \propto e^{ \frac{ - \left( x - \mu  \right)^2 } {2\sigma ^2 }
}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="45" src="../../OEBPS/Images/eq_13-03-a.png" width="334"/></p>
</div>
<p class="body">The training data set is denoted <span class="math"><i class="fm-italics">X</i> = {<i class="fm-italics">x</i><sup class="fm-superscript">(1)</sup>, <i class="fm-italics">x</i><sup class="fm-superscript">(2)</sup>,‚ãØ, <i class="fm-italics">x</i><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>}</span>, and its overall likelihood is</p><!--<p class="Body"><span class="times">$$p\left( X
\middle\vert \mu \right) = p\left(x^{ \left( 1 \right) }, \cdots, x^{
\left( n \right) } \middle\vert \mu \right)
= \prod_{i=1}^{n}  p\left(x^{ \left( i \right) } \middle\vert \mu
\right)
\propto e^{{\frac{ -\sum\displaylimits_{i=1}^{n} \left( {x^{ \left( i
\right) } - \mu } \right)^2 } {2\sigma ^2 }}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="65" src="../../OEBPS/Images/eq_13-03-b.png" width="465"/></p>
</div>
<p class="body"><a id="marker-451"/>The variance is known by assumption‚Äîhence it is treated as a constant instead of a random variable. The mean <i class="timesitalic">Œº</i> is unknown and is treated as a Gaussian random variable, with mean <span class="math"><i class="fm-italics">Œº</i><sub class="fm-subscript">0</sub></span> and variance <span class="math"><i class="fm-italics">œÉ</i><sub class="fm-subscript">0</sub></span> (not to be confused with <i class="timesitalic">Œº</i> and <i class="timesitalic">œÉ</i>, the mean and variance of the data itself ). So, the prior is</p><!--<p class="Body"><span class="times">$$p\left( \mu
\right) =  \mathcal{N} \left( \mu; \mu_{0}, \sigma_{0} \right)
\propto  e^{{\frac{ - \left( { \mu - \mu_{0} } \right)^2 }
{2\sigma_{0}^2 }}}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="54" src="../../OEBPS/Images/eq_13-03-c.png" width="250"/></p>
</div>
<p class="body">The posterior probability of the unknown <i class="timesitalic">Œº</i> parameter is a product of two Gaussians, which is a Gaussian itself. Let‚Äôs denote the as-yet-unknown) mean and variance of this product Gaussian as <i class="timesitalic">Œº<sub class="fm-subscript">n</sub></i> and <i class="timesitalic">œÉ<sub class="fm-subscript">n</sub></i>. Here the subscript <i class="timesitalic">n</i> is to remind us that the posterior has been obtained by digesting <i class="timesitalic">n</i> data instances from <span class="math"><i class="fm-italics">X</i> = {<i class="fm-italics">x</i><sup class="fm-superscript">(1)</sup>, <i class="fm-italics">x</i><sup class="fm-superscript">(2)</sup>,‚ãØ, <i class="fm-italics">x</i><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>}</span>. Thus, the Gaussian posterior can be denoted as</p><!--<p class="Body"><span class="times">$$p\left( \mu \middle\vert X \right) = \mathcal{N}
\left( \mu; \mu_{n}, \sigma_{n} \right) \propto e^{ { \frac{ - \left( {
\mu - \mu_{n} } \right)^2 } { 2\sigma_{n}^2 } } }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="48" src="../../OEBPS/Images/eq_13-03-d.png" width="269"/></p>
</div>
<p class="body">Using Bayes‚Äô theorem,</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;p\left( \mu \middle\vert X \right) \propto p\left( X \middle\vert
\mu \right)  p\left( \mu \right) \\
&amp;\text{  or }\;\;\;
\overbrace{ e^{ { \frac{ - \left( { \mu - \mu_{n} } \right)^2 } { 2\sigma_{n}^2 } }
}
}^{posterior}
\propto
\overbrace{ e^{  -\frac{ \sum\displaylimits_{i=1}^{n} \left( {x^{ \left( i \right) }
- \mu } \right)^2 } {2\sigma ^2 } }
}^{likelihood}
   \;\;\;
\overbrace{ e^{ -\frac{ \left( { \mu - \mu_{0} } \right)^2 } { 2\sigma_{0}^2 } }
}^{ prior
}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="119" src="../../OEBPS/Images/eq_13-03-e.png" width="316"/></p>
</div>
<p class="body">By comparing the coefficients of <span class="math"><i class="fm-italics">Œº</i><sup class="fm-superscript">2</sup></span> and <i class="timesitalic">Œº</i> on the exponents of the left and right sides, we determine the unknown parameters of the posterior distribution:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\frac{1}{ \sigma_{n}^{2} } &amp;= \frac{1}{ \sigma_{0}^{2} } + \frac{ n
}{ \sigma^{2} } \;\;\; \text{ or } \sigma_{n}^{2}  = \frac{
\sigma_{0}^{2} \sigma^{2} }{ n \sigma_{0}^{2} + \sigma^{2} } \nonumber
\\[4pt]
\mu_{ n } &amp;= \sigma_{ n }^{ 2 } \left( \frac{ \sum\displaylimits_{ i=1 }^{ n }  x^{ \left( i \right) }  }{ \sigma^{2} } + \frac{ \mu_{0} }{
\sigma_{0}^{2} } \right) =
\frac{ \sigma_{0}^{2} \sigma^{2} }{ n \sigma_{0}^{2} + \sigma^{2}
}  \left( \frac{ n \bar{x} }{ \sigma^{2} } + \frac{ \mu_{0} }{
\sigma_{0}^{2} } \right) \nonumber \\[4pt]
&amp; =\frac{ \bar{x} } { 1 + \frac{\sigma^{2} }{ n \sigma_{0}^{2} } } +
\frac{ \mu_{0} } { 1 + \frac{ n \sigma_{0}^{2} }{ \sigma^{2} }
}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="197" src="../../OEBPS/Images/eq_13-04.png" width="384"/></p>
</div>
<p class="fm-equation-caption">Equation 13.4 <span class="calibre" id="eq-bayesinf-muonlyunivar"/></p>
<p class="body">The significance of various closely named variables should be clearly understood:<a id="marker-452"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="timesitalic">Œº</i>, <i class="timesitalic">œÉ</i> are the mean and variance of the <i class="fm-italics">data</i> distribution <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>)</span>‚Äîassumed to be Gaussian. The final goal is to estimate <i class="timesitalic">Œº</i>, <i class="timesitalic">œÉ</i> that best fits the data set <i class="timesitalic">X</i>. On the other hand, <span class="math"><i class="fm-italics">Œº</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">œÉ</i><sub class="fm-subscript">0</sub></span> are the mean and variance of the <i class="fm-italics">parameter</i> distribution <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œº</i>)</span>, which captures our prior belief about the value of the data mean <i class="timesitalic">Œº</i> (remember, by assumption, the data mean is also a Gaussian random variable). <i class="timesitalic">Œº<sub class="fm-subscript">n</sub></i>, <i class="timesitalic">œÉ<sub class="fm-subscript">n</sub></i> are the mean and variance of the posterior distribution <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œº</i>|<i class="fm-italics">X</i>)</span> for the data mean <i class="timesitalic">Œº</i> as computed from <i class="timesitalic">n</i> data point samples. This is a Gaussian random variable because it is a product of two Gaussians.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The posterior distribution of the unknown mean parameter, <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œº</i>|<i class="fm-italics">X</i>)</span>, is a Gaussian with mean <i class="timesitalic">Œº<sub class="fm-subscript">n</sub></i>. So, it will attain a maximum when <span class="math"><i class="fm-italics">Œº</i> = <i class="fm-italics">Œº<sub class="fm-subscript">n</sub></i></span>. In other words, the MAP estimate for the unknown mean <i class="timesitalic">Œº</i> is <span class="math"><i class="fm-italics">Œº</i><sub class="fm-subscript">MAP</sub> = <i class="fm-italics">Œº<sub class="fm-subscript">n</sub></i></span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Even though <i class="timesitalic">Œº<sub class="fm-subscript">n</sub></i> is the best estimate of <i class="timesitalic">Œº</i>, <i class="timesitalic">œÉ<sub class="fm-subscript">n</sub></i> is not approximating the <i class="timesitalic">œÉ</i> of the data, <i class="timesitalic">œÉ</i> is known in this case by assumption. Here, <i class="timesitalic">œÉ<sub class="fm-subscript">n</sub></i> is the variance of the posterior distribution, reflecting our <i class="fm-italics">uncertainty</i> about the estimate of <i class="timesitalic">Œº</i>. That is why, as the number of data instances becomes very large, <i class="timesitalic">œÉ<sub class="fm-subscript">n</sub></i> approaches <span class="math">0</span> (indicating we have zero uncertainty or full confidence in the estimate of the mean.)</p>
</li>
</ul>
<p class="body">The estimate for our data distribution is <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>) = <span class="cambria">ùí©</span>(<i class="fm-italics">x</i>;<i class="fm-italics">Œº<sub class="fm-subscript">n</sub></i>, <i class="fm-italics">œÉ</i>)</span>, where <i class="timesitalic">Œº<sub class="fm-subscript">n</sub></i> is given by equation <a class="url" href="#eq-bayesinf-muonlyunivar">13.4</a>. Note that it is a combination of the MLE <i class="timesitalic">xÃÑ</i> and prior guess <span class="math"><i class="fm-italics">Œº</i><sub class="fm-subscript">0</sub></span>. Using this, given any arbitrary data instance <i class="timesitalic">x</i>, we can infer the probability of <i class="timesitalic">x</i> belonging to the class of the training data set <i class="timesitalic">X</i>.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for Bayesian estimation with unknown mean and known variance, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/ZA75">http://mng.bz</a> <a class="url" href="http://mng.bz/ZA75">/ZA75</a>.</p>
<p class="fm-code-listing-caption" id="listing-13.1-pytorch-bayesian-estimation-with-unknown-mean-known-variance">Listing 13.1 PyTorch- Bayesian estimation with unknown mean, known variance</p>
<pre class="programlisting">import torch

def inference_unknown_mean(X, prior_dist, sigma_known):
    mu_mle = X.mean()
    n = X.shape[0]

    mu_0 = prior_dist.mean
    sigma_0 = prior_dist.scale                             <span class="fm-combinumeral">‚ë†</span>


    mu_n = mu_mle / (1 + sigma_known**2 / (n * sigma_0**2)) +
             mu_0 / (1 + n * sigma_0**2 / sigma_known**2)  <span class="fm-combinumeral">‚ë°</span>

    sigma_n = math.sqrt(
        (sigma_0**2 * sigma_known**2) /
        (n*sigma_0**2+sigma_known**2))                     <span class="fm-combinumeral">‚ë¢</span>

    posterior_dist = torch.Normal(mu_n, sigma_n)
    return posterior_dist</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Parameters of the prior</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Mean of the posterior, following equation <a class="url" href="#eq-bayesinf-muonlyunivar">13.4</a></p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¢</span> Standard deviation of the posterior, following equation <a class="url" href="#eq-bayesinf-muonlyunivar">13.4</a></p>
<h2 class="fm-head" id="small-and-large-volumes-of-training-data-and-strong-and-weak-priors">13.4 Small and large volumes of training data, and strong and weak priors</h2>
<p class="body"><a id="marker-453"/>Let‚Äôs examine the behavior of equation <a class="url" href="#eq-bayesinf-muonlyunivar">13.4</a> when <span class="math"><i class="fm-italics">n</i> = 0</span> (no data) and when <span class="math"><i class="fm-italics">n</i> ‚Üí ‚àû</span> (lots of data):</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\lim_{n \longrightarrow 0}
\begin{cases}
&amp;\mu_{n} = \mu_{0}\\
&amp;\sigma_{n} = \sigma_{0}
\end{cases} \\[5pt]
&amp;\lim_{ n \longrightarrow \infty }
\begin{cases}
&amp;\mu_{n} = \bar{x} = \mu_{MLE} \\
&amp;\sigma_{n} = 0
\end{cases}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="140" src="../../OEBPS/Images/eq_13-04-a.png" width="180"/></p>
</div>
<p class="body">This agrees with our notion that with little data, the posterior is dominated by the prior, while with lots of data, the posterior is dominated by the likelihood. With lots of data, the variance of the parameter is zero (we are saying with <i class="fm-italics">full certainty</i> that the best value for the mean is the sample mean for the data, aka the MLE estimate for the mean). In general, with more training data (that is, larger values of <i class="timesitalic">n</i>), the posterior shifts closer to the likelihood. This can be seen by analyzing equation <a class="url" href="#eq-bayesinf-muonlyunivar">13.4</a>. It agrees with our intuition that with little data, we try to compensate with our pre-existing (prior) belief as to the value of the parameters. As the number of training data instances increases, the effect of the prior is reduced, and the likelihood (which is a function of the data) begins to dominate.</p>
<p class="body">A low variance for the prior (that is, small <span class="math"><i class="fm-italics">œÉ</i><sub class="fm-subscript">0</sub></span>) essentially means we have low uncertainty in our prior belief (remember, the entropy/uncertainty of a Gaussian is proportional to its variance). Such high-confidence priors resist being overwhelmed by the data and are called <i class="fm-italics">strong priors</i>. On the other hand, a large <span class="math"><i class="fm-italics">œÉ</i><sub class="fm-subscript">0</sub></span> implies low /confidence in the prior mean value. This is a <i class="fm-italics">weak prior</i> that is easily overwhelmed by the data. We can see this in the final expression for mean in equation <a class="url" href="#eq-bayesinf-muonlyunivar">13.4</a>: we have <span class="math"><i class="fm-italics">nœÉ</i><sub class="fm-subscript">0</sub><sup class="fm-superscript">2</sup>/<i class="fm-italics">œÉ</i><sup class="fm-superscript">2</sup></span> in the denominator of the second term. In general, the second term vanishes with larger <i class="timesitalic">n</i>, thereby removing the effect of the prior <span class="math"><i class="fm-italics">Œº</i><sub class="fm-subscript">0</sub></span> and making the posterior mean coincide with the MLE mean. But the smaller the <span class="math"><i class="fm-italics">œÉ</i><sub class="fm-subscript">0</sub></span>, the larger the <i class="timesitalic">n</i> required to achieve this, and vice versa.</p>
<h2 class="fm-head" id="sec-conjugate-prior">13.5 Conjugate priors</h2>
<p class="body">In section <a class="url" href="#sec-bayesinf-muonly">13.3</a>, given a Gaussian likelihood, choosing the Gaussian family for the prior made the posterior also belong to the Gaussian family. This simplified things considerably. If the prior was chosen from another family, the posterior‚Äîwhich is the product of the likelihood and prior‚Äîmay not belong to a simple or even known distribution family.</p>
<p class="body">Thus, a Gaussian likelihood with a Gaussian prior results in a Gaussian posterior for the mean. Such priors are said to be <i class="fm-italics">conjugate</i>. Formally, for a specific family of likelihood, the choice of the prior that results in the posterior belonging to the same family as the prior is called a conjugate prior. For instance, Gaussians for the mean (with known variance) are conjugate to a Gaussian likelihood. Soon we will see that for a Gaussian likelihood, a gamma distribution for the precision (inverse of the variance) results in a gamma posterior. In other words, a gamma prior to the precision is conjugate to a Gaussian likelihood. In the multivariate case, instead of gamma, we have the Wishart distribution as a conjugate prior.<a id="marker-454"/></p>
<h2 class="fm-head" id="sec-bayesinf-sigmaonly">13.6 Fully Bayes parameter estimation: Gaussian, unknown precision, known mean</h2>
<p class="body">In section <a class="url" href="#sec-bayesinf-muonly">13.3</a>, we discussed fully Bayes parameter estimation with the assumption that we somehow know the variance <i class="timesitalic">œÉ</i> and only want to estimate the mean <i class="timesitalic">Œº</i>. Now we examine the case where the mean is known but the variance is unknown and expressed as a random variable. The computations become simpler if we use precision <i class="timesitalic">Œª</i> instead of variance <i class="timesitalic">œÉ</i>. They are related by the expression <span class="math">1/<i class="fm-italics">œÉ</i><sup class="fm-superscript">2</sup></span>. Thus we have a data set <i class="timesitalic">X</i>, which is assumed to be sampled from a Gaussian distribution with a constant mean <i class="timesitalic">Œº</i>, while the precision <i class="timesitalic">Œª</i> is a random variable with a gamma distribution. The probability density function for the data is thus <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>|<i class="fm-italics">Œº</i>, <i class="fm-italics">Œª</i>) = <span class="cambria">ùí©</span>(<i class="fm-italics">x</i>; <i class="fm-italics">Œº</i>, 1/‚àö<i class="fm-italics">Œª</i>)</span>.</p>
<p class="body">We model the prior random variable for precision with a gamma distribution. The likelihood is Gaussian, and since the product of a Gaussian and gamma is another gamma (due to the conjugate prior property of gamma), the resulting posterior is a gamma. The gamma function parameters for the posterior can be derived via coefficient comparison. The maximum of the posterior is our estimate for the parameter.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Gamma distribution</p>
<p class="fm-sidebar-text">The gamma distribution is introduced in the appendix; if necessary, please read that first. Here we state the relevant properties. The probability density function for a random variable <i class="timesitalic">Œª</i> having a gamma distribution is a function with two parameters <i class="timesitalic">Œ±</i>, <i class="timesitalic">Œ≤</i>:</p><!--<p class="FM-SideBar-Text"><span class="times">$$p\left( \lambda \middle\vert \alpha, \beta
\right) = \gamma \left(\lambda; \alpha, \beta \right) = \frac{ \beta^{
\alpha } }{ \Gamma\left( \alpha \right) } \lambda^{ \left( \alpha - 1
\right)} e^{ - \beta \lambda }\;\;\;\;\;\text{    where $\alpha, \beta
&gt; 0$, $\lambda \geq 0$ }$$</span></p>-->
<p class="sidebarafigures"><img alt="" class="calibre2" height="53" src="../../OEBPS/Images/eq_13-05.png" width="509"/></p>
<p class="sidebaracaptions">Equation 13.5</p>
</div>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Maximum of a gamma distribution</p>
<p class="fm-sidebar-text">To maximize the gamma probability density function <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œª</i>|<i class="fm-italics">X</i>) = <i class="fm-italics">Œª</i><sup class="fm-superscript">(<i class="fm-italics1">Œ±<sub class="fm-subscript">n</sub></i> ‚àí 1)</sup><i class="fm-italics">e</i><sup class="fm-superscript">‚àí<i class="fm-italics1">Œ≤<sub class="fm-subscript">n</sub>Œª</i></sup></span> for a random variable <i class="timesitalic">Œª</i>, we take the derivative and equate to zero:</p><!--<p class="FM-SideBar-Text"><span class="times">$$\begin{aligned}
&amp;\frac{d}{ d \lambda } \left( \lambda^{ \alpha - 1 } e^{ -\beta
\lambda} \right) = 0 \implies \left( \alpha - 1 \right) \lambda^{ \alpha
- 2 } e^{ -\beta \lambda} + \lambda^{ \alpha - 1 } \left( -\beta \right) e^{ -\beta \lambda} = 0 \\
&amp;\lambda = \frac{ \alpha - 1 }{ \beta }\end{aligned}$$</span></p>-->
<p class="sidebarafigures"><img alt="" class="calibre2" height="98" src="../../OEBPS/Images/eq_13-05-a.png" width="474"/></p>
</div>
<h3 class="fm-head1" id="estimating-the-precision-parameter">13.6.1 Estimating the precision parameter</h3>
<p class="body"><a id="marker-455"/>Let‚Äôs return to the fully Bayes estimation of the precision parameter when the mean is known. We model the data distribution with a Gaussian: <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>|<i class="fm-italics">Œº</i>, <i class="fm-italics">Œª</i>) = <span class="cambria">ùí©</span>(<i class="fm-italics">x</i>; <i class="fm-italics">Œº</i>, 1/‚àö<i class="fm-italics">Œª</i>)</span> (we have expressed this Gaussian in terms of the precision, <i class="timesitalic">Œª</i>, which is related to the variance <i class="timesitalic">œÉ</i> as <span class="math"><i class="fm-italics">Œª</i> = 1/<i class="fm-italics">œÉ</i> <sup class="fm-superscript">2</sup></span>). The training data set is <span class="math"><i class="fm-italics">X</i> = {<i class="fm-italics">x</i><sup class="fm-superscript">(1)</sup>, <i class="fm-italics">x</i><sup class="fm-superscript">(2)</sup>,‚ãØ, <i class="fm-italics">x</i><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>}</span>, and its overall likelihood is</p><!--<p class="Body"><span class="times">$$p\left( X
\middle\vert \mu, \lambda\right) = p\left( X \middle\vert \lambda
\right)
= \prod_{i=1}^{n} \left( \mathcal{N} \left( x^{ \left( i \right) }; \mu
,  \frac{1}{\sqrt{\lambda}} \right) \right)
\propto \lambda^{ \frac{ n }{ 2 } }  e^{ -\frac{ \lambda } { 2 }
\sum\displaylimits_{i=1}^{n} \left( {x^{ \left( i \right) } - \mu }
\right)^2 }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="64" src="../../OEBPS/Images/eq_13-05-b.png" width="510"/></p>
</div>
<p class="body">We model the prior for the precision with a gamma distribution with parameters <span class="math"><i class="fm-italics">Œ±</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">Œ≤</i><sub class="fm-subscript">0</sub></span>:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œª</i>) = <i class="fm-italics">Œ≥</i>(<i class="fm-italics">Œª</i>;<i class="fm-italics">Œ±</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">Œ≤</i><sub class="fm-subscript">0</sub>) <span class="cambria">‚àù</span> <i class="fm-italics">Œª</i><sup class="fm-superscript">(<i class="fm-italics1">Œ±</i><sub class="fm-subscript">0</sub>‚àí1)</sup><i class="fm-italics">e</i><sup class="fm-superscript">‚àí<i class="fm-italics1">Œ≤</i><sub class="fm-subscript">0</sub><i class="fm-italics1">Œª</i></sup></span></p>
<p class="body">We know the corresponding posterior‚Äîa product of a Gaussian and a gamma‚Äîis gamma distribution (due to the conjugate prior property of gamma distribution). Let‚Äôs denote the posterior as</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œª</i>|<i class="fm-italics">X</i>) = <i class="fm-italics">Œ≥</i>(<i class="fm-italics">Œª</i>;<i class="fm-italics">Œ±<sub class="fm-subscript">n</sub></i>, <i class="fm-italics">Œ≤<sub class="fm-subscript">n</sub></i>)</span></p>
<p class="body">From Bayes‚Äô theorem,</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;p\left( \lambda \middle\vert X \right) \propto p\left( X
\middle\vert \lambda\right) \; p\left( \lambda \right)  \\
&amp; \text{ or }\;\;\;
\overbrace{
\lambda^{ \left( \alpha_{n} - 1 \right)} e^{ - \beta_{n} \lambda }
}^{posterior}
\propto
\overbrace{
\frac{ \lambda^{ \frac{ n }{ 2 } } } { { \sqrt {2\pi }  } } e^{ -\frac{
\lambda }{ 2 } \sum\displaylimits_{i=1}^{n} \left( x^{ \left( i \right)
} - \mu  \right)^2 }
}^{likelihood}
\;\;\;
\overbrace{
\lambda^{ \left( \alpha_{0} - 1 \right)} e^{ - \beta_{0} \lambda }
}^{prior}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="86" src="../../OEBPS/Images/eq_13-05-c.png" width="610"/></p>
</div>
<p class="body">Substituting</p><!--<p class="Body"><span class="times">$$s = \frac{1}{n } \sum\displaylimits_{i=0}^{n}
\left( x^{ \left( i \right) } - \mu \right)^{2}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="56" src="../../OEBPS/Images/eq_13-05-d.png" width="159"/></p>
</div>
<p class="body">and comparing the powers of <i class="timesitalic">Œª</i> and <i class="timesitalic">e</i>, we get</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp; \alpha_{n}  = \frac{ n }{ 2 } + \alpha_{0} \nonumber \\
&amp; \beta_{n} = \frac{ 1 }{ 2 } \sum\displaylimits_{i=1}^{n} \left(
{x^{ \left( i \right) } - \mu } \right)^2 + \beta_{0} = \frac{ n }{ 2 } s + \beta_{0}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="109" src="../../OEBPS/Images/eq_13-06.png" width="287"/></p>
</div>
<p class="fm-equation-caption">Equation 13.6 <span class="calibre" id="eq-bayesinf-sigmaonlyunivar"/></p>
<p class="body">Notice that as before, at low values of <i class="timesitalic">n</i>, the posterior is dominated by the prior but gets closer and closer to the likelihood estimate as <i class="timesitalic">n</i> increases. In other words, in the absence of sufficient data, we let our belief take over the estimation; but if and when data is available, the estimation is dominated by the data-based entity likelihood.</p>
<p class="body">The MAP point estimate for the parameter <i class="timesitalic">Œª</i> given data set <i class="timesitalic">X</i> is obtained by maximizing this posterior distribution <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œª</i>|<i class="fm-italics">X</i>) = <i class="fm-italics">Œ≥</i>(<i class="fm-italics">Œª</i>;<i class="fm-italics">Œ±<sub class="fm-subscript">n</sub></i>, <i class="fm-italics">Œ≤<sub class="fm-subscript">n</sub></i>)</span>, which yields <span class="math"><i class="fm-italics">Œª<sub class="fm-subscript">MAP</sub></i> = 1/<i class="fm-italics">œÉ<sub class="fm-subscript">MAP</sub></i><sup class="fm-superscript">2</sup> = (<i class="fm-italics">Œ±<sub class="fm-subscript">n</sub></i>‚Äì1/<i class="fm-italics">Œ≤<sub class="fm-subscript">n</sub></i>)</span>. (Section A.5 in the appendix shows how to obtain the maximum of a gamma distribution.) Thus our estimate for the training data distribution is <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>) = <span class="cambria">ùí©</span>(<i class="fm-italics">x</i>; <i class="fm-italics">Œº</i>, <i class="fm-italics">œÉ</i><sub class="fm-subscript">MAP</sub>)</span>, where <span class="math">1/<i class="fm-italics">œÉ<sub class="fm-subscript">MAP</sub></i><sup class="fm-superscript">2</sup> = (<i class="fm-italics">Œ±<sub class="fm-subscript">n</sub></i>‚Äì1/<i class="fm-italics">Œ≤<sub class="fm-subscript">n</sub></i>)</span>.</p>
<p class="body">Given a large volume of data, the MAP estimate for the unknown precision/variance becomes identical to the MLE estimate (proof outline shown):</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\lim_{n \rightarrow \infty} \frac{1}{ \sigma_{\text{MAP}}^{2} } =
\lim_{n \rightarrow \infty} \left( \frac{ \alpha_{n} - 1 }{ \beta_{n}  }
\right) = \lim_{n \rightarrow \infty} \frac{\frac{ n }{ 2 } + \alpha_{0}
- 1}{\frac{ n }{ 2 } s + \beta_{0} }
= \lim_{n \rightarrow \infty} \frac{1 + \frac{\alpha_{0} - 1}{\frac{ n
}{ 2 }} }{  s + \frac{\beta_{0}}{\frac{ n }{ 2 }} } = \frac{1}{s}\\
&amp;\text{ or }\;\; \lim_{n \rightarrow \infty} \sigma_{\text{MAP}}^{2}
= s = \frac{1}{n } \sum\displaylimits_{i=0}^{n} \left( x^{ \left( i
\right) } - \mu \right)^{2} =
\sigma_{\text{MLE}}^{2}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="179" src="../../OEBPS/Images/eq_13-06-a.png" width="484"/></p>
</div>
<p class="body">On the other hand, given no data, the MAP estimate for the unknown precision/variance is completely determined by the prior (proof outline shown):</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\lim_{n \rightarrow 0} \frac{1}{ \sigma_{\text{MAP}}^{2} } =
\lim_{n \rightarrow 0} \left( \frac{ \alpha_{n} - 1 }{ \beta_{n}  }
\right) = \lim_{n \rightarrow 0} \frac{\frac{ n }{ 2 } + \alpha_{0} - 1}{\frac{ n }{ 2 } s + \beta_{0} }
=\frac{ \alpha_{0} - 1 }{  \beta_{0} }\\
&amp;\text{ or }\;\; \lim_{n \rightarrow 0} \sigma_{\text{MAP}}^{2} =
\frac{ \beta_{0} }{ \alpha_{0} - 1 }\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="138" src="../../OEBPS/Images/eq_13-06-b.png" width="399"/></p>
</div>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for Bayesian estimation with a known mean and unknown variance, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/2nZ9">http://mng.bz</a> <a class="url" href="http://mng.bz/2nZ9">/2nZ9</a>.<a id="marker-456"/></p>
<p class="fm-code-listing-caption" id="listing-13.2-pytorch-bayesian-estimation-with-unknown-variance-known-mean">Listing 13.2 PyTorch- Bayesian estimation with unknown variance, known mean</p>
<pre class="programlisting">import torch

def inference_unknown_variance(X, prior_dist):
    sigma_mle = torch.std(X)
    n = X.shape[0]

    alpha_0 = prior_dist.concentration
    beta_0 = prior_dist.rate                 <span class="fm-combinumeral">‚ë†</span>

    alpha_n = n / 2 + alpha_0
    beta_n = n / 2 * sigma_mle ** 2 + beta_0 <span class="fm-combinumeral">‚ë°</span>

    posterior_dist = torch.Gamma(alpha_n, beta_n)
    return posterior_dist</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Parameters of the prior</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Parameters of the posterior</p>
<h2 class="fm-head" id="sec-bayesinf-musigma">13.7 Fully Bayes parameter estimation: Gaussian, unknown mean, unknown precision</h2>
<p class="body">In section <a class="url" href="#sec-bayesinf-muonly">13.3</a>, we saw that if the variance is known, the conjugate prior to the mean is a Gaussian (aka normal) distribution. Likewise, when the mean is known, the conjugate prior to the precision is a gamma distribution. If both are unknown, we end up with a normal-gamma distribution.</p>
<h3 class="fm-head1" id="normal-gamma-distribution">13.7.1 Normal-gamma distribution</h3>
<p class="body">Normal-gamma is a probability distribution of two random variables, say, <i class="timesitalic">Œº</i> and <i class="timesitalic">Œª</i>, whose density is defined in terms of four parameters <span class="math"><i class="fm-italics">Œº</i><sup class="fm-superscript">‚Ä≤</sup>, <i class="fm-italics">Œª</i><sup class="fm-superscript">‚Ä≤</sup>, <i class="fm-italics">Œ±</i><sup class="fm-superscript">‚Ä≤</sup>, and <i class="fm-italics">Œ≤</i><sup class="fm-superscript">‚Ä≤</sup></span>, as follows:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;p\left( \mu, \lambda; \; \mu^{'}, \lambda^{'}, \alpha^{'},
\beta^{'} \right) = \mathcal{N}\gamma \left( \mu, \lambda ; \; \mu^{'},
\lambda^{'}, \alpha^{'}, \beta^{'} \right) \\
&amp;= \frac{ \beta^{ '\alpha^{'} } \sqrt{ \lambda^{'} } } {
\Gamma\left( \alpha^{'} \right) \sqrt{2\pi} } e^{ -\frac{\lambda}{2}
\lambda^{'}  \left( \mu - \mu^{'} \right)^{2} } \lambda^{ \alpha^{'} -
\frac{1}{2} } e^{ -\beta^{'} \lambda }\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure1"><img alt="" class="calibre5" height="59" src="../../OEBPS/Images/eq_13-06-c.png" width="675"/></p>
</div>
<p class="body">Although it looks complicated, a simple way to remember it is a product of a normal and a gamma distribution.<a id="marker-457"/></p>
<p class="body">The normal-gamma distribution attains a maximum at</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\mu &amp;= \mu^{'} \\
\lambda &amp;= \frac{ \alpha^{'} - \frac{1}{2} } { \beta^{'}
}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="85" src="../../OEBPS/Images/eq_13-06-d.png" width="90"/></p>
</div>
<h3 class="fm-head1" id="estimating-the-mean-and-precision-parameters">13.7.2 Estimating the mean and precision parameters</h3>
<p class="body">As before, we model the data distribution with a Gaussian: <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">x</i>|<i class="fm-italics">Œº</i> , <i class="fm-italics">Œª</i>) = <span class="cambria">ùí©</span>(<i class="fm-italics">x</i>; <i class="fm-italics">Œº</i>, 1/‚àö<i class="fm-italics">Œª</i>)</span> we have expressed this Gaussian in terms of the precision, <i class="timesitalic">Œª</i>, which is related to the variance <i class="timesitalic">œÉ</i> as <span class="math"><i class="fm-italics">Œª</i> = 1/<i class="fm-italics">œÉ</i><sup class="fm-superscript">2</sup></span>). The training data set is <span class="math"><i class="fm-italics">X</i> = {<i class="fm-italics">x</i><sup class="fm-superscript">(1)</sup>, <i class="fm-italics">x</i><sup class="fm-superscript">(2)</sup>,‚ãØ, <i class="fm-italics">x</i><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>}</span>, and its overall likelihood is</p><!--<p class="Body"><span class="times">$$p\left( X
\middle\vert \mu, \lambda\right)
= \prod_{i=1}^{n} \left( \mathcal{N} \left( x^{ \left( i \right) }; \mu
,  \frac{1}{\sqrt{\lambda}} \right) \right)
\propto \lambda^{ \frac{ n }{ 2 } }  e^{ -\frac{ \lambda } { 2 }
\sum\displaylimits_{i=1}^{n} \left( {x^{ \left( i \right) } - \mu }
\right)^2 }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="61" src="../../OEBPS/Images/eq_13-06-e.png" width="428"/></p>
</div>
<p class="body">We model the prior for the mean as a Gaussian with mean <span class="math"><i class="fm-italics">Œº</i><sub class="fm-subscript">0</sub></span> and precision <span class="math"><i class="fm-italics">Œª</i><sub class="fm-subscript">0</sub><i class="fm-italics">Œª</i></span>:</p><!--<p class="Body"><span class="times">$$p\left( \mu \middle\vert \lambda \right) =
\mathcal{N}\left( \mu ;  \mu_{0}, \lambda_{0} \lambda \right)
\propto   \lambda^{ \frac{1}{2} }  e^{ -\frac{ \lambda_{0} \lambda } { 2
}  \left(  \mu - \mu_{0}  \right)^{2} }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="43" src="../../OEBPS/Images/eq_13-06-f.png" width="330"/></p>
</div>
<p class="body">We model the prior for the precision as a gamma distribution with parameters <span class="math"><i class="fm-italics">Œ±</i><sub class="fm-subscript">0</sub></span>, <span class="math"><i class="fm-italics">Œ≤</i><sub class="fm-subscript">0</sub></span>:</p><!--<p class="Body"><span class="times"><i class="fm-italics">p</em>(<i class="fm-italics">Œª</em>) = <i class="fm-italics">Œ≥</em>(<i class="fm-italics">Œª</em>;<i class="fm-italics">Œ±</em><sub class="FM-Subscript">0</sub>, <i class="fm-italics">Œ≤</em><sub class="FM-Subscript">0</sub>) <span class="cambria">‚àù</span> <i class="fm-italics">Œª</em><sup class="FM-Superscript">(<i class="fm-italics">Œ±</em><sub class="FM-Subscript">0</sub>‚àí1)</sup><i class="fm-italics">e</em><sup class="FM-Superscript">‚àí<i class="fm-italics">Œ≤</em><sub class="FM-Subscript">0</sub><i class="fm-italics">Œª</em></sup></span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="31" src="../../OEBPS/Images/eq_13-06-g.png" width="278"/></p>
</div>
<p class="body">The overall prior probability for the mean and precision parameters is the product of the two, a normal-gamma distribution with parameters <span class="math"><i class="fm-italics">Œº</i><sup class="fm-superscript">0</sup>, <i class="fm-italics">Œª</i><sup class="fm-superscript">0</sup>, <i class="fm-italics">Œ±</i><sup class="fm-superscript">0</sup>, <i class="fm-italics">Œ≤</i><sup class="fm-superscript">0</sup></span>:</p><!--<p class="Body"><span class="times">$$p\left( \mu, \lambda \right)  =
\mathcal{N}\gamma \left( \mu, \lambda \middle\vert \mu^{0}, \lambda^{0},
\alpha^{0}, \beta^{0} \right)
\propto
\lambda^{ \frac{1}{2} }  e^{ -\frac{ \lambda_{0} \lambda } { 2
}  \left(  \mu - \mu_{0}  \right)^{2} }
\;\;\;
\lambda ^{ \left( \alpha_{0} - 1 \right)} e^{ - \beta_{0}
\lambda  }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="39" src="../../OEBPS/Images/eq_13-06-h.png" width="525"/></p>
</div>
<p class="body">The posterior probability for the mean and precision parameters is the joint (that is, product) of the likelihood and the prior. As such, we know it is another normal-gamma distribution (due to the conjugate prior property of normal-gamma):</p><!--<p class="Body"><span class="times">$$p\left( \mu, \lambda \middle\vert X \right) =
\mathcal{N}\gamma \left( \mu, \lambda \middle\vert \mu^{n}, \lambda^{n},
\alpha^{n}, \beta^{n} \right)
\propto e^{ -\frac{\lambda}{2} \lambda_{n}  \left( \mu - \mu_{n} \right)^{2} }
\lambda^{ \alpha_{n} - \frac{1}{2} } e^{ -\beta_{n} \lambda }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="37" src="../../OEBPS/Images/eq_13-06-i.png" width="513"/></p>
</div>
<p class="body">Using Bayes‚Äô theorem,</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;p\left( \mu, \lambda \middle\vert X \right) \propto p\left( X
\middle\vert \mu, \lambda\right) p\left( \mu, \lambda \right) \\
&amp;\text{  or }\;\;\;\\
&amp;\overbrace{ e^{ -\frac{\lambda}{2} \lambda_{n}  \left( \mu - \mu_{n} \right)^{2} }
\lambda^{ \alpha_{n} - \frac{1}{2} } e^{ -\beta_{n} \lambda }
}^{ \text{posterior} }
\propto
\overbrace{
\lambda^{ \frac{ n }{ 2 } }  e^{ -\frac{ \lambda } { 2 }
\sum\displaylimits_{i=1}^{n} \left( {x^{ \left( i \right) } - \mu }
\right)^2 }
}^{ \text{likelihood} }
\;\;\;
\overbrace{
\lambda^{ \frac{1}{2} }  e^{ -\frac{ \lambda_{0} \lambda } { 2
}  \left(  \mu - \mu_{0}  \right)^{2} }
}^{ \text{prior mean} }
\;\;\;
\overbrace{
\lambda ^{ \left( \alpha_{0} - 1 \right)} e^{ - \beta_{0} \lambda  }
}^{ \text{prior precision} }\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="131" src="../../OEBPS/Images/eq_13-06-j.png" width="605"/></p>
</div>
<p class="body">Substituting</p><!--<p class="Body"><span class="times">$$s = \frac{1}{n } \sum\displaylimits_{i=0}^{n}
\left( x^{ \left( i \right) } - \mu \right)^{2}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="64" src="../../OEBPS/Images/eq_13-06-k.png" width="158"/></p>
</div>
<p class="body">and comparing coefficients, the unknown parameters of the posterior distribution can be determined:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\mu_{n} = \frac{ \left( n \bar{x} + \mu_{0} \lambda_{0} \right) }{ n + \lambda_{0} }
&amp;\lambda_{n} = n + \lambda_{0} \nonumber \\
&amp;\alpha_{n} = \frac{n}{2} + \alpha_{0}
&amp;\beta_{n} = \frac{ ns }{ 2 } + \beta_{ 0 } + \frac{ n \lambda_{0} }
{ 2 \left( n + \lambda_{0} \right) } \left( \bar{x} - \mu_{0} \right)^{ 2 }\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="102" src="../../OEBPS/Images/eq_13-07.png" width="482"/></p>
</div>
<p class="fm-equation-caption">Equation 13.7 <span class="calibre" id="eq-normal-gamma"/></p>
<p class="body">To obtain the fully Bayes parameter estimate, we take the maximum of the normal-gamma posterior probability density function:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\mu &amp;= \mu_{n} \\
\lambda &amp;= \frac{ \alpha_{n} - \frac{1}{2} } { \beta_{n}
}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="84" src="../../OEBPS/Images/eq_13-07-a.png" width="92"/></p>
</div>
<p class="body">Thus the final probability density function for the data is</p><!--<p class="Body"><span class="times">$p\left( x \right) = \mathcal{N} \left( x; \mu_{n}
,  \sqrt{ \frac{ \beta_{n} } { \alpha_{n} - \frac{1}{2}
}  }\right)$</span>.</p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="59" src="../../OEBPS/Images/eq_13-07-b.png" width="199"/></p>
</div>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for Bayesian estimation with an unknown mean and unknown variance, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/1oQy">http://mng.bz/1oQy</a>.<a id="marker-458"/></p>
<p class="fm-code-listing-caption" id="listing-13.3-pytorch-code-for-a-normal-gamma-distribution">Listing 13.3 PyTorch code for a normal-gamma distribution</p>
<pre class="programlisting">import torch

class NormalGamma():                            <span class="fm-combinumeral">‚ë†</span>


    def __init__(self, mu_, lambda_, alpha_, beta_):
        self.mu_ = mu_
        self.lambda_ = lambda_
        self.alpha_ = alpha_
        self.beta_ = beta_

    @property
    def mean(self):
        return self.mu_, self.alpha_/ self.beta_

    @property
    def mode(self):
        return self.mu_, (self.alpha_-0.5)/ self.beta_</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Since PyTorch doesn‚Äôt implement normal-gamma distribution, we implement a bare-bones version.</p>
<p class="fm-code-listing-caption" id="heading_id_2">Listing 13.4 PyTorch: Bayesian estimation with unknown mean, unknown variance</p>
<pre class="programlisting">import torch

def inference_unknown_mean_variance(X, prior_dist):
    mu_mle = X.mean()
    sigma_mle = X.std()
    n = X.shape[0]

    mu_0 = prior_dist.mu_
    lambda_0 = prior_dist.lambda_
    alpha_0 = prior_dist.alpha_
    beta_0 = prior_dist.beta_                   <span class="fm-combinumeral">‚ë†</span>

    mu_n = (n * mu_mle + mu_0 * lambda_0) / (lambda_0 + n)
    lambda_n = n + lambda_0
    alpha_n = n / 2 + alpha_0
    beta_n = n / 2 * sigma_mle ** 2 + beta_0 +
             0.5* n * lambda_0/ (n + lambda_0) *
             (mu_mle - mu_0) ** 2               <span class="fm-combinumeral">‚ë°</span>

    posterior_dist = NormalGamma(mu_n, lambda_n, alpha_n, beta_n)

    return posterior_dist</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Parameters of the prior</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Parameters of the posterior</p>
<h2 class="fm-head" id="sec-bayesinf-statsville">13.8 Example: Fully Bayesian inferencing</h2>
<p class="body"><a id="marker-459"/>Let‚Äôs revisit the problem discussed in section <a class="url" href="../Text/06.xhtml#sec-gauss_max_likelihood_estimation">6.8</a> of predicting whether a resident of Statsville is female based on height. For this purpose, we have collected height samples from adult female residents of Statsville. Unfortunately, due to unforeseen circumstances, we collected a very small sample. Armed with our knowledge of Bayesian inference, we do not want to let this deter us from trying to build a model. Based on physical considerations, we can assume that the distribution of heights is Gaussian. Our goal is to estimate the parameters (<i class="timesitalic">Œº</i>, <i class="timesitalic">œÉ</i>) of this Gaussian.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for this example, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/Pn4g">http://mng.bz/Pn4g</a>.</p>
<p class="body">Let‚Äôs first create the data set by sampling five points from a Gaussian distribution with <span class="math"><i class="fm-italics">Œº</i> = 152</span> and <span class="math"><i class="fm-italics">œÉ</i> = 8</span>. In real-life scenarios, we do not know the mean and standard deviation of the true distribution. But for the sake of this example, let‚Äôs assume that the mean height is <span class="math">152</span> cm and the standard deviation is <span class="math">8</span> cm. Our data matrix, <i class="timesitalic">X</i>, is as follows:</p><!--<p class="Body"><span class="times">$$X = \begin{bmatrix} 164.32 \\ 149.65 \\ 134.56 \\ 156.54 \\ 143.32
\end{bmatrix}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="175" src="../../OEBPS/Images/eq_13-07-c.png" width="142"/></p>
</div>
<h3 class="fm-head1" id="maximum-likelihood-estimation">13.8.1 Maximum likelihood estimation</h3>
<p class="body">If we relied on MLE, our approach would be to compute the mean and standard deviation of the data set and use this normal distribution as our model. We use the following equations to compute the mean and standard deviation of our normal distribution:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
\mu_{MLE} = \frac{1}{N} \sum_{i=1}^nx_{i}\end{aligned}$$</span> <span class="times">$$\begin{aligned}
\sigma_{MLE} = \frac{1}{N} \sum_{i=1}^n(x_{i}  -
\mu)^2\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="135" src="../../OEBPS/Images/eq_13-07-d.png" width="183"/></p>
</div>
<p class="body">The mean, <i class="timesitalic">Œº</i>, comes out to be 149.68, and the standard deviation, <i class="timesitalic">œÉ</i>, is 11.52. This differs significantly from the true mean (152) and standard deviation (8) because the number of data points is low. In such low-data scenarios, the maximum likelihood estimates are not very reliable.</p>
<h3 class="fm-head1" id="bayesian-inference">13.8.2 Bayesian inference</h3>
<p class="body"><a id="marker-460"/>Can we do better than MLE? One potential method is to use Bayesian inference with a good prior. How do we select a good prior? Well, let‚Äôs say that we know from an old survey that the average and standard deviation of the height of adult female residents of Neighborville, the neighboring town, are 150 cm and 9 cm, respectively. Additionally, we have no reason to believe that the distribution of heights at Statsville is significantly different. So we can use this information to ‚Äúinitialize" our prior. The prior distribution encodes our beliefs about the parameter values.</p>
<p class="body">Given that we are dealing with an unknown mean and unknown variance, we model the prior as a normal-gamma distribution:</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œ∏</i>) = <span class="cambria">ùí©</span><i class="fm-italics">Œ≥</i>(<i class="fm-italics">Œº</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">Œª</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">Œ±</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">Œ≤</i><sub class="fm-subscript">0</sub>)</span></p>
<p class="body">We choose <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œ∏</i>)</span> such that <span class="math"><i class="fm-italics">Œº</i><sub class="fm-subscript">0</sub></span> = 150, <span class="math"><i class="fm-italics">Œª</i><sub class="fm-subscript">0</sub></span> = 100, <span class="math"><i class="fm-italics">Œ±</i><sub class="fm-subscript">0</sub></span> = 10.5, and <span class="math"><i class="fm-italics">Œ≤</i><sub class="fm-subscript">0</sub></span> = 810. This implies that</p>
<p class="fm-equation"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œ∏</i>) = <span class="cambria">ùí©</span><i class="fm-italics">Œ≥</i>(150, 100, 10.5, 810)</span></p>
<p class="body"><span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œ∏</i>|<i class="fm-italics">X</i>)</span> is a normal-gamma distribution whose parameters can be computed using equations described in section <a class="url" href="#eq-normal-gamma">13.7</a>. The PyTorch code for computing the posterior is shown next.</p>
<p class="fm-code-listing-caption" id="listing-13.5-pytorch-computing-posterior-probability-using-bayesian-inference">Listing 13.5 PyTorch- Computing posterior probability using Bayesian inference</p>
<pre class="programlisting">prior_dist = NormalGamma(150, 100, 10.5, 810)                   <span class="fm-combinumeral">‚ë†</span>

posterior_dist = inference_unknown_mean_variance(X, prior_dist) <span class="fm-combinumeral">‚ë°</span>

map_mu, map_precision =  posterior_dist.mode                    <span class="fm-combinumeral">‚ë¢</span>

map_std = math.sqrt(1 / map_precision)                          <span class="fm-combinumeral">‚ë£</span>

map_dist = Normal(map_mu, map_std)                              <span class="fm-combinumeral">‚ë§</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Initializes the normal-gamma distribution</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Computes the posterior</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë¢</span> The mode of the distribution refers to parameter values with the highest probability density.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë£</span> Computes the standard deviation using precision</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë§</span> map_mu and map_std refer to the parameter values that maximize the posterior distribution.</p>
<p class="body">The MAP estimates for <i class="timesitalic">Œº</i> and <i class="timesitalic">œÉ</i> obtained using Bayesian inference are 149.98 and 9.56, respectively, which are better than the MLE estimates of 149.68 and 11.52 (the true <i class="timesitalic">Œº</i> and <i class="timesitalic">œÉ</i> are 152 and 9, respectively).</p>
<p class="body">Now that we‚Äôve estimated the parameters, we can find out the probability that a sample lies in the range using the formula</p><!--<p class="Body"><span class="times">$$\begin{aligned} p(a &lt; X &lt;= b) = \int_{a}^b p(X) dX\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="59" src="../../OEBPS/Images/eq_13-07-e.png" width="224"/></p>
</div>
<p class="body">The details of this can be found in section <a class="url" href="../Text/06.xhtml#sec-gauss_max_likelihood_estimation">6.8</a>.</p>
<h2 class="fm-head" id="sec-bayesinf-multivarmuonly">13.9 Fully Bayes parameter estimation: Multivariate Gaussian, unknown mean, known precision</h2>
<p class="body"><a id="marker-461"/>This is the multivariate case; the univariate version is discussed in section <a class="url" href="#sec-bayesinf-muonly">13.3</a>. The computations follow along the same lines as the univariate ones.</p>
<p class="body">We model the data distribution as a Gaussian <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>, <b class="fm-bold">Œõ</b>) = <span class="cambria">ùí©</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>, <b class="fm-bold">Œõ</b><sup class="fm-superscript">‚àí1</sup>)</span>, where we have expressed the Gaussian in terms of the <i class="fm-italics">precision matrix</i> <span class="timesbold">Œõ</span> instead of the covariance matrix <span class="timesbold">Œ£</span>, where <span class="math"><b class="fm-bold">Œõ</b> = <b class="fm-bold">Œ£</b><sup class="fm-superscript">‚àí1</sup></span>. The training data set is <span class="math"><i class="fm-italics">X</i> ‚â° {<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(1)</sup>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(2)</sup>, ‚ãØ, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>, ‚ãØ, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>}</span>, and its overall likelihood is</p><!--<p class="Body"><span class="times">$$p\left( X
\middle\vert \vec{\mu} \right) = \propto e^{ -\frac{1}{2}
\sum\displaylimits_{i=1}^{n} \left( \vec{x}^{ \left( i \right) } - \vec{
\mu } \right)^{T} \boldsymbol{\Lambda} \left( \vec{x}^{ \left( i \right)
} - \vec{ \mu } \right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="46" src="../../OEBPS/Images/eq_13-07-f.png" width="280"/></p>
</div>
<p class="body">We model the prior for the mean as a Gaussian:</p><!--<p class="Body"><span class="times">$$p\left( \vec{\mu } \right) = \mathcal{N}\left(
\vec{\mu } ; \;\; \vec{\mu}_{0}, \boldsymbol{\Lambda}_{0}^{-1} \right)
\propto e^{ -\frac{1}{2} \left( \vec{\mu }- \vec{ \mu_{0} }\right)^{T}
\boldsymbol{\Lambda}_{0} \left( \vec{\mu }- \vec{ \mu_{0} }\right)
}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="42" src="../../OEBPS/Images/eq_13-07-g.png" width="352"/></p>
</div>
<p class="body">The posterior probability density is a Gaussian (because it is the product of two Gaussians). Let‚Äôs denote it as</p><!--<p class="Body"><span class="times">$$p\left( \vec{\mu} \middle\vert X \right) =
\mathcal{N}\left( \vec{\mu } ; \;\; \vec{\mu}_{n},
\boldsymbol{\Lambda}_{n}^{-1} \right)
\propto e^{ -\frac{1}{2}
\left( \vec{\mu} - \vec{\mu_{n}} \right)^{T} \boldsymbol{\Lambda}_{n}
\left( \vec{\mu} - \vec{\mu_{n}} \right)
}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="38" src="../../OEBPS/Images/eq_13-07-h.png" width="369"/></p>
</div>
<p class="body">Using Bayes‚Äô theorem,</p><!--<p class="Body"><span class="times">$$\begin{aligned} p\left( \vec{\mu} \middle\vert X \right) &amp;\propto p\left( X
\middle\vert \vec{\mu} \right) p\left( \vec{ \mu } \right) \\
\text{  or  }\;\;\;
\overbrace{ e^{ -\frac{1}{2} \left( \vec{\mu} - \vec{\mu_{n}} \right)^{T}
\boldsymbol{\Lambda}_{n} \left( \vec{\mu} - \vec{\mu_{n}} \right) }
}^{posterior}
&amp;\propto
\overbrace{ e^{ -\frac{1}{2} \sum\displaylimits_{i=1}^{n} \left( \vec{x}^{ \left( i
\right) } - \vec{ \mu } \right)^{T} \boldsymbol{\Lambda} \left(
\vec{x}^{ \left( i \right) } - \vec{ \mu } \right) }
}^{likelihood}
\overbrace{ e^{ -\frac{1}{2} \left( \vec{\mu }- \vec{ \mu_{0} }\right)^{T}
\boldsymbol{\Lambda}_{0} \left( \vec{\mu }- \vec{ \mu_{0} } \right) }
}^{prior}  \\
&amp;\propto e^{
-\frac{1}{2} \left(
\sum\displaylimits_{i=1}^{n} \left( \vec{x}^{ \left( i \right) } - \vec{
\mu } \right)^{T} \boldsymbol{\Lambda} \left( \vec{x}^{ \left( i \right)
} - \vec{ \mu } \right)
+
\left( \vec{ \mu } - \vec{ \mu_{0} }\right)^{T} \boldsymbol{\Lambda}_{0}
\left( \vec{ \mu } - \vec{ \mu_{0} }\right)
\right)
}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="136" src="../../OEBPS/Images/eq_13-07-i.png" width="508"/></p>
</div>
<p class="body">Let‚Äôs examine the exponent of the rightmost expression.</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\sum\displaylimits_{i=1}^{n} \left( \vec{x}^{ \left( i \right) } -
\vec{ \mu } \right)^{T} \boldsymbol{\Lambda} \left( \vec{x}^{ \left( i
\right) } - \vec{ \mu } \right)
\;\;\; +\;\;\;
\left( \vec{ \mu } - \vec{ \mu_{0} }\right)^{T} \boldsymbol{ \Lambda_{0}
} \left( \vec{ \mu } - \vec{ \mu_{0} }\right)\\
&amp;= \sum\displaylimits_{i=1}^{n} \vec{x}^{ \left( i \right)^{T}
}  \boldsymbol{ \Lambda } \vec{x}^{ \left( i \right) } +
\sum\displaylimits_{i=1}^{n}  \vec{\mu}^{T} \boldsymbol{ \Lambda }
\vec{\mu} - 2 \vec{\mu}^{T} \boldsymbol{ \Lambda }
\overbrace{
\sum\displaylimits_{i=1}^{n}  \vec{x}^{ \left( i \right) }
}^{ n \bar{ \vec{x}} }
+  \vec{\mu}^{T} \boldsymbol{ \Lambda_{0} } \vec{\mu} - 2\vec{\mu}^{T}
\boldsymbol{ \Lambda_{0} } \vec{\mu}_{0} + \vec{\mu}_{0}^{T}
\boldsymbol{ \Lambda_{0} } \vec{\mu}_{0} \\
&amp;= \sum\displaylimits_{i=1}^{n} \vec{x}^{ \left( i \right)^{T}
}  \boldsymbol{ \Lambda } \vec{x}^{ \left( i \right) } + n \vec{\mu}^{T}
\boldsymbol{ \Lambda } \vec{\mu} - 2 n \vec{\mu}^{T} \boldsymbol{
\Lambda } \bar{ \vec{x}}
+  \vec{\mu}^{T} \boldsymbol{ \Lambda_{0} } \vec{\mu} - 2\vec{\mu}^{T}
\boldsymbol{ \Lambda_{0} }  \vec{\mu}_{0} + \vec{\mu}_{0}^{T}
\boldsymbol{ \Lambda_{0} }  \vec{\mu}_{0} \\
&amp;= \vec{\mu}^{T} \left( n\boldsymbol{ \Lambda } + \boldsymbol{
\Lambda_{0} } \right) \vec{\mu} - 2 \vec{\mu}^{T} \left( n \boldsymbol{
\Lambda }  \bar{ \vec{x}} + \boldsymbol{ \Lambda_{0} }  \vec{\mu}_{0}
\right) + \text{    constant terms without }
\vec{\mu}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="259" src="../../OEBPS/Images/eq_13-07-j.png" width="720"/></p>
</div>
<p class="body"><a id="marker-462"/>We ignored the last constant terms because they will be rolled into the overall constant of proportionality). Thus</p><!--<p class="Body"><span class="times">$$\begin{aligned} p\left( \vec{\mu} \middle\vert X \right) &amp;\propto e^{ -\frac{1}{2} \left( \vec{\mu} - \vec{\mu_{n}} \right)^{T}
\boldsymbol{\Lambda}_{n} \left( \vec{\mu} - \vec{\mu_{n}} \right) } \\
&amp;\propto e^{ -\frac{1}{2} \left(
\vec{\mu}^{T} \left( n \boldsymbol{ \Lambda } + \boldsymbol{ \Lambda_{0}
} \right) \vec{\mu} - 2 \vec{\mu}^{T} \left( n \boldsymbol{ \Lambda
}  \bar{ \vec{x}} + \boldsymbol{ \Lambda_{0} }  \vec{\mu}_{0} \right) +
\text{    constant terms without } \vec{\mu}
\right)}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="77" src="../../OEBPS/Images/eq_13-07-k.png" width="471"/></p>
</div>
<p class="body">Comparing coefficients:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\boldsymbol{\Lambda}_{n} = n \boldsymbol{\Lambda} +
\boldsymbol{\Lambda}_{0}
&amp; \vec{\mu_{n}} = \boldsymbol{\Lambda}_{n}^{-1} \left( n
\boldsymbol{\Lambda} \bar{\vec{x}} + \boldsymbol{\Lambda}_{0}
\vec{\mu}_{0} \right) \text{    (remember $\boldsymbol{\Lambda}$ is known)}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="47" src="../../OEBPS/Images/eq_13-07-l.png" width="543"/></p>
</div>
<p class="body">The posterior probability maximizes at <i class="timesitalic"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">n</sub></i>. Thus <span class="math"><i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">MAP</sub></i> = <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">n</sub></i></span></p>
<p class="body">is the MAP estimate for the mean parameter of the multivariate Gaussian data distribution: <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>) = <span class="cambria">ùí©</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">n</sub></i>, <b class="fm-bold">Œõ</b><sup class="fm-superscript">-1</sup>)</span>.</p>
<p class="body">Note the following:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\begin{rcases}
&amp; \lim_{ n \rightarrow \infty }  n
\boldsymbol{\Lambda}_{n}^{-1}  =  \lim_{ n \rightarrow \infty }  n
\left( n \boldsymbol{\Lambda} + \boldsymbol{\Lambda}_{0}  \right)
^{-1}  \\
&amp; \;\; =  \lim_{ n \rightarrow \infty }  \left( n^{-1} \right)^{-1}
\left( n \boldsymbol{\Lambda} + \boldsymbol{\Lambda}_{0}  \right) ^{-1}
=  \lim_{ n \rightarrow \infty } \left( n^{-1} n \boldsymbol{\Lambda} + n^{-1} \boldsymbol{\Lambda}_{0}  \right) ^{-1}
= \boldsymbol{\Lambda} ^{-1}  \\
&amp; \lim_{ n \rightarrow \infty } \boldsymbol{\Lambda}_{n}^{-1}   =
\lim_{  \rightarrow \infty } n^{-1} n \boldsymbol{\Lambda}_{n}^{-1}  =
\lim_{ n \rightarrow \infty } n^{-1} \boldsymbol{\Lambda} ^{-1}  = 0
\end{rcases}
\lim_{ n \rightarrow \infty } \vec{ \mu_{n} }  = \bar{ \vec{x} } \\
&amp;  \lim_{n \rightarrow 0} \vec{ \mu_{n} }  =
\vec{\mu}_{0}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="146" src="../../OEBPS/Images/eq_13-07-m.png" width="650"/></p>
</div>
<p class="body">With a large volume of data, the estimated mean parameter <span class="math"><i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">MAP</sub></i> = <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">n</sub></i></span> approaches the MLE <span class="math"><i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">MLE</sub></i> = <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x2.png" width="13"/></span></span>.</p>
<p class="body">With a low volume of data, the estimated posterior mean parameter <span class="math"><i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">MAP</sub></i> = <i class="fm-italics"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">n</sub></i></span> approaches the prior <span class="math"><span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span><sub class="fm-subscript">0</sub></span>.</p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> Fully functional code for multivariate Bayesian inferencing of the mean of a Gaussian likelihood with known precision, executable via Jupyter Notebook, can be found at <a class="url" href="http://mng.bz/J2AP">http://mng.bz/J2AP</a>.</p>
<p class="fm-code-listing-caption" id="listing-13.5-pytorch-multivariate-bayesian-inferencing-unknown-mean">Listing 13.6 PyTorch- Multivariate Bayesian inferencing, unknown mean</p>
<pre class="programlisting">def inference_known_precision(X, prior_dist, precision_known):
    mu_mle = X.mean(dim=0)
    n = X.shape[0]


    mu_0 = prior_dist.mean
    precision_0 = prior_dist.precision_matrix       <span class="fm-combinumeral">‚ë†</span>


    precision_n = n * precision_known + precision_0 <span class="fm-combinumeral">‚ë°</span>
    mu_n = torch.matmul(
        n * torch.matmul(
            mu_mle.unsqueeze(0), precision_known) + torch.matmul(
                mu_0.unsqueeze(0), precision_0),
        torch.inverse(precision_n)
     )

    posterior_dist = MultivariateNormal(
        mu_n, precision_matrix=precision_n)
    return posterior_dist</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë†</span> Parameters of the prior</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">‚ë°</span> Parameters of the posterior</p>
<h2 class="fm-head" id="sec-bayesinf-multivarpreconly">13.10 Fully Bayes parameter estimation: Multivariate, unknown precision, known mean</h2>
<p class="body">In section <a class="url" href="#sec-bayesinf-sigmaonly">13.6</a>, we discussed the univariate case, and now we examine the multivariate case. For the univariate case, we had to look at the gamma distribution. For the multivariate case, we have to look at the Wishart distribution.<a id="marker-463"/></p>
<h3 class="fm-head1" id="wishart-distribution">13.10.1 Wishart distribution</h3>
<p class="body">Suppose we have a Gaussian random data vector <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> with probability density function <span class="math"><span class="cambria">ùí©</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>, <b class="fm-bold">Œ£</b>)</span>. Once again, we use <i class="fm-italics">precision matrix</i> <span class="timesbold">Œõ</span> instead of the covariance matrix <span class="timesbold">Œ£</span>, where <span class="math"><b class="fm-bold">Œõ</b> = <b class="fm-bold">Œ£</b><sup class="fm-superscript">‚àí1</sup></span>. Consider the case where we know the mean <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span> but want to estimate the precision <span class="timesbold">Œõ</span>. How do we express the prior? Note that <span class="math"><i class="fm-italics">p</i>(<b class="fm-bold">Œõ</b>)</span> is the probability density function of a <i class="fm-italics">matrix</i>. So far, we have encountered probability distributions of scalars and vectors, not a matrix. Also, this is not an arbitrary matrix. We are talking about a <i class="fm-italics">symmetric, non-negative definite</i> matrix (all covariance and precision matrices belong to this category). Consequently, the distribution we are looking for is not a joint distribution of all the <span class="math"><i class="fm-italics">d</i><sup class="fm-superscript">2</sup></span> matrix elements <i class="timesitalic">d</i> denotes the dimensionality of the data: that is, all <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> and <span class="times"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span> vectors are <span class="math"><i class="fm-italics">d</i> √ó 1</span>). Rather, it is a joint distribution of <span class="math">(<i class="fm-italics">d</i>(<i class="fm-italics">d</i> + 1))/2</span> elements in the matrix‚Äîthe diagonal and those above or below (diagonal elements above and below are identical because the matrix is symmetric).</p>
<p class="body">The space of such matrices is called a <i class="fm-italics">Wishart ensemble</i>. The probability of a random-precision matrix <span class="timesbold">Œõ</span> of size <span class="math"><i class="fm-italics">d</i> √ó <i class="fm-italics">d</i></span> can be expressed as a Wishart distribution. This distribution has two parameters:</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><i class="timesitalic">ŒΩ</i>, a scalar, satisfying <span class="math"><i class="fm-italics">ŒΩ</i> &gt; <i class="fm-italics">d</i> ‚àí 1</span></p>
</li>
<li class="fm-list-bullet">
<p class="list"><span class="timesbold">W</span>, a <span class="math"><i class="fm-italics">d</i> √ó <i class="fm-italics">d</i></span> symmetric non-negative definite matrix</p>
</li>
</ul>
<p class="body">The probability density function is</p><!--<p class="Body"><span class="times">$$p\left( \boldsymbol{\Lambda}; \nu, \boldsymbol{ W
} \right) = \mathcal{W}\left( \boldsymbol{\Lambda}; \nu, \boldsymbol{ W
} \right) =
\frac{
| \boldsymbol{\Lambda} |^{ \frac{\nu - d-1}{2} } e^{ -\frac{1}{2} Tr\left( \boldsymbol{ W }^{-1} \boldsymbol{ \Lambda } \right) }
}{ 2^{ \frac{ \nu d }{2} } | \boldsymbol{ W } |^{ \frac{ \nu }{2} }
\Gamma_{d} \left( \frac{ \nu }{2} \right)
}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="75" src="../../OEBPS/Images/eq_13-07-n1.png" width="398"/></p>
</div>
<p class="body">where</p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list"><span class="cambria">ùí≤</span> denotes Wishart.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><span class="math">|<b class="fm-bold">W</b>|</span>, <span class="math">|<b class="fm-bold">Œõ</b>|</span> denote the determinants of the matrices <span class="timesbold">W</span> and <span class="timesbold">Œõ</span>, respectively.</p>
</li>
<li class="fm-list-bullet">
<p class="list"><span class="math"><i class="fm-italics">Tr</i>(<i class="fm-italics">A</i>)</span> denotes the trace of a matrix <i class="timesitalic">A</i> (sum of the diagonal elements).</p>
</li>
<li class="fm-list-bullet">
<p class="list"><i class="timesitalic">Œì</i> denotes the multivariate gamma function</p>
</li>
</ul><!--<p class="Body"><span class="times">$$\Gamma_{d}
\left( \frac{n}{2} \right)  =\pi^{ \frac{ d \left( d - 1 \right) }{ 4 }
} \prod_{j=1}^{d} \Gamma\left( \frac{ n-j+1 }{ 2 } \right)$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="55" src="../../OEBPS/Images/eq_13-07-n2.png" width="254"/></p>
</div>
<p class="body">The Wishart is the multivariate version of the gamma distribution. Its expected value is</p>
<p class="fm-equation"><span class="math"><span class="segoe">ùîº</span>(<b class="fm-bold">Œõ</b>) = <i class="fm-italics">ŒΩ</i><b class="fm-bold">W</b></span></p>
<p class="body">Its maxima occur at</p>
<p class="fm-equation"><span class="math"><b class="fm-bold">Œõ</b> = (<i class="fm-italics">ŒΩ</i> ‚àí <i class="fm-italics">d</i> ‚àí 1)<b class="fm-bold">W</b> for <i class="fm-italics">ŒΩ</i> ‚â• <i class="fm-italics">d</i> + 1</span><a id="marker-464"/></p>
<h3 class="fm-head1" id="estimating-precision">13.10.2 Estimating precision</h3>
<p class="body">As before, we model the data distribution as a Gaussian <span class="math"><i class="fm-italics">p</i>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>|<span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>, <b class="fm-bold">Œõ</b>) = <span class="cambria">ùí©</span>(<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span>; <span class="infigure"><img alt="" class="calibre9" height="29" src="../../OEBPS/Images/AR_micro.png" width="15"/></span>, <b class="fm-bold">Œõ</b><sup class="fm-superscript">‚àí1</sup>)</span>, where we have expressed the Gaussian in terms of the <i class="fm-italics">precision matrix</i> <span class="timesbold">Œõ</span> instead of the covariance matrix <span class="timesbold">Œ£</span>, where <span class="math"><b class="fm-bold">Œõ</b> = <b class="fm-bold">Œ£</b><sup class="fm-superscript">‚àí1</sup></span>.</p>
<p class="body">The training data set is <span class="math"><i class="fm-italics">X</i> ‚â° {<span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(1)</sup>, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(2)</sup>,‚ãØ, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">i</i>)</sup>,‚ãØ, <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><sup class="fm-superscript">(<i class="fm-italics1">n</i>)</sup>}</span>, and its overall likelihood is</p><!--<p class="Body"><span class="times">$$p\left( X
\middle\vert\boldsymbol{ \Lambda }\right) = \propto |
\boldsymbol{\Lambda} |^{ \frac{n}{2} } e^{ -\frac{1}{2}
\sum\displaylimits_{i=1}^{n} \left( \vec{x}^{ \left( i \right) } - \vec{
\mu } \right)^{T} \boldsymbol{\Lambda} \left( \vec{x}^{ \left( i \right)
} - \vec{ \mu } \right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="37" src="../../OEBPS/Images/eq_13-07-n3.png" width="315"/></p>
</div>
<p class="body">We model the prior probability of the precision matrix as a Wishart distribution. Hence,</p><!--<p class="Body"><span class="times">$$p \left( \boldsymbol{ \Lambda } \right) =
\mathcal{W}\left( \boldsymbol{\Lambda}; \nu_{0}, \boldsymbol{ W_{0} }
\right) \propto  | \boldsymbol{\Lambda} |^{ \frac{ \nu_{0} - d - 1 }{ 2
} } e^{ -\frac{1}{2} Tr\left( \boldsymbol{ W_{0} }^{-1} \boldsymbol{
\Lambda } \right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="40" src="../../OEBPS/Images/eq_13-07-n4.png" width="367"/></p>
</div>
<p class="body">The posterior is another Wishart (owing to the Wishart conjugate prior property):</p><!--<p class="Body"><span class="times">$$p\left( \boldsymbol{\Lambda} \middle\vert X
\right) = \mathcal{W}\left( \boldsymbol{\Lambda}; \nu_{n}, \boldsymbol{ W_{n} } \right)
\propto
| \boldsymbol{\Lambda} |^{ \frac{ \nu_{n} - d - 1 }{ 2 } } e^{
-\frac{1}{2} Tr\left( \boldsymbol{ W_{n} }^{-1} \boldsymbol{ \Lambda }
\right) }$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="37" src="../../OEBPS/Images/eq_13-07-n5.png" width="391"/></p>
</div>
<p class="body">Using Bayes‚Äô theorem for the training data set <i class="timesitalic">X</i>,</p><!--<p class="Body"><span class="times">$$\begin{aligned} p\left( \boldsymbol{\Lambda} \middle\vert X \right) &amp;\propto p\left(
X \middle\vert \boldsymbol{\Lambda} \right) \;\;\; p\left(  \boldsymbol{\Lambda} \right)  \\
\overbrace{
| \boldsymbol{\Lambda} |^{ \frac{ \nu_{n} - d - 1 }{ 2 } } e^{
-\frac{1}{2} Tr\left( \boldsymbol{ W_{n} }^{-1} \boldsymbol{ \Lambda }
\right) }
}^{posterior}
&amp;\propto
\overbrace{
| \boldsymbol{\Lambda} |^{ \frac{n}{2} } e^{
-\frac{1}{2} \sum\displaylimits_{i=1}^{n} \left( \vec{x}^{ \left( i
\right) } - \vec{ \mu } \right)^{T} \boldsymbol{\Lambda} \left(
\vec{x}^{ \left( i \right) } - \vec{ \mu } \right)
}
}^{likelihood}
\;\;\;
\overbrace{
| \boldsymbol{\Lambda} |^{ \frac{\nu_{0}-d-1}{2} } e^{ -\frac{1}{2} Tr\left( \boldsymbol{ W_{0} }^{-1} \boldsymbol{ \Lambda } \right) }
}^{prior}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="106" src="../../OEBPS/Images/eq_13-07-o1.png" width="590"/></p>
</div>
<p class="body">Let‚Äôs study a pair of simple lemmas that will come in handy.</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\vec{x}^{T} A \vec{x} = Tr\left( \vec{x}^{T} A \vec{x} \right)\\
&amp;\vec{x}^{T} A \vec{x} = Tr\left( A \vec{x} \vec{x}^{T}
\right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="80" src="../../OEBPS/Images/eq_13-07-o2.png" width="160"/></p>
</div>
<p class="body">where <i class="timesitalic">Tr</i> refers to Trace of a matrix (sum of diagonal elements).<a id="marker-465"/></p>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">The first lemma is almost trivial‚Äîthe quadratic form <span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span><i class="fm-italics"><sup class="fm-superscript">T</sup>A</i><span class="infigure"><img alt="" class="calibre9" height="24" src="../../OEBPS/Images/AR_x.png" width="14"/></span> is a scalar, so of course it is the same as its trace.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The second lemma follows directly from the matrix property of a trace: <span class="math"><i class="fm-italics">Tr</i>(<i class="fm-italics">BC</i>) = <i class="fm-italics">Tr</i>(<i class="fm-italics">CB</i>)</span>.</p>
</li>
</ul>
<p class="body">Using the lemmas, the exponent of the likelihood term is</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\sum\displaylimits_{i=1}^{n} \left( \vec{x}^{ \left( i \right) } -
\vec{ \mu } \right)^{T} \boldsymbol{\Lambda} \left( \vec{x}^{ \left( i
\right) } - \vec{ \mu } \right) =
\sum\displaylimits_{i=1}^{n} Tr\left(
  \left(  \vec{x}^{ \left( i \right) } - \vec{ \mu } \right)
  \left( \vec{x}^{ \left( i \right) } - \vec{ \mu } \right)^{T}
\boldsymbol{\Lambda}
\right)  \\
&amp;= Tr\left(
  \overbrace{
    \sum\displaylimits_{i=1}^{n} \left( \vec{x}^{ \left( i \right) } -
\vec{ \mu } \right) \left( \vec{x}^{ \left( i \right) } - \vec{ \mu }
\right)^{T}
   }^{nS}
  \boldsymbol{\Lambda}
\right)
=  Tr\left( n  S \boldsymbol{\Lambda} \right)\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="125" src="../../OEBPS/Images/eq_13-07-o3.png" width="836"/></p>
</div>
<p class="body">where</p><!--<p class="Body"><span class="times">$$S = \frac{1}{n}
\sum\displaylimits_{i=1}^{n} \left( \vec{x}^{ \left( i \right) } - \vec{
\mu } \right)  \left( \vec{x}^{ \left( i \right) } - \vec{ \mu }
\right)^{T} \text{    (remember, $\vec{\mu}$ is known)}.$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="60" src="../../OEBPS/Images/eq_13-07-o4.png" width="440"/></p>
</div>
<p class="body">Thus, the posterior density is</p><!--<p class="Body"><span class="times">$$\begin{aligned} p\left( \boldsymbol{\Lambda} \middle\vert X \right)
\;\;p\left(  \boldsymbol{\Lambda} \right)
&amp;\propto
| \boldsymbol{\Lambda} |^{ \frac{n}{2} } e^{ -\frac{1}{2} Tr\left( n S \boldsymbol{\Lambda} \right) }
| \boldsymbol{\Lambda} |^{ \frac{\nu_{0}-d-1}{2} } e^{ -\frac{1}{2} Tr\left( \boldsymbol{ W_{0} }^{-1} \boldsymbol{ \Lambda } \right) } \\
&amp;\propto
| \boldsymbol{\Lambda} |^{ \frac{n+\nu_{0}-d-1}{2} } e^{
-\frac{1}{2}
\left( Tr\left(  n  S \boldsymbol{ \Lambda }  \right)
+ Tr\left( \boldsymbol{ W_{0} }^{-1} \boldsymbol{ \Lambda } \right)
\right)
}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="74" src="../../OEBPS/Images/eq_13-07-o5.png" width="416"/></p>
</div>
<p class="body">Since <span class="math"><i class="fm-italics">Tr</i>(<i class="fm-italics">A</i>) + <i class="fm-italics">Tr</i>(<i class="fm-italics">B</i>) = <i class="fm-italics">Tr</i>(<i class="fm-italics">A</i> + <i class="fm-italics">B</i>)</span>,</p><!--<p class="Body"><span class="times">$$\begin{aligned} p\left( X \middle\vert \boldsymbol{\Lambda} \right) \; p\left(  \boldsymbol{\Lambda} \right)
&amp;\propto
| \boldsymbol{\Lambda} |^{
                              \frac{
                                  \overbrace{ n+\nu_{0} }^{ \nu_{n} }
                                   -d-1}{2}
                             } e^{
-\frac{1}{2} Tr\left(
   \left(
      \overbrace{ n S  + \boldsymbol{ W_{0} }^{-1} }
   \right)
   \boldsymbol{ \Lambda }
\right)
}
&amp;\propto
| \boldsymbol{\Lambda} |^{ \frac{\nu_{n}-d-1}{2} } e^{
-\frac{1}{2} Tr\left(
   \boldsymbol{ W_{n} }^{-1}
   \boldsymbol{ \Lambda }
\right)
}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="86" src="../../OEBPS/Images/eq_13-07-o6.png" width="606"/></p>
</div>
<p class="body">Comparing coefficients, we determine the unknown parameters of the posterior distribution:</p><!--<p class="Body"><span class="times">$$\begin{aligned}
&amp;\boldsymbol{ W_{n} }^{-1} = \left( n S  + \boldsymbol{ W_{0} }^{-1}
\right)\\
&amp;\nu_{n} = n + \nu_{0}\end{aligned}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="88" src="../../OEBPS/Images/eq_13-07-o7.png" width="194"/></p>
</div>
<p class="body">where</p><!--<p class="Body"><span class="times">$$S = \frac{1}{n} \sum\displaylimits_{i=1}^{n}
\left( \vec{x}^{ \left( i \right) } - \vec{ \mu } \right)  \left(
\vec{x}^{ \left( i \right) } - \vec{ \mu } \right)^{T}$$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="62" src="../../OEBPS/Images/eq_13-07-o8.png" width="242"/></p>
</div>
<p class="body">The maximum of the posterior density function, <span class="math"><b class="fm-bold"><span class="cambria">ùí≤</span></b>(<b class="fm-bold">Œõ</b>;<i class="fm-italics">ŒΩ<sub class="fm-subscript">n</sub></i>, <b class="fm-bold">W</b><sub class="subscript-bold">n</sub>)</span>, ields an estimate for the precision parameter of the data distribution: <span class="math"><b class="fm-bold">Œõ</b> = (<i class="fm-italics">ŒΩ<sub class="fm-subscript">n</sub></i> ‚àí <i class="fm-italics">d</i> ‚àí 1)<b class="fm-bold">W</b><sub class="subscript-bold">n</sub> for <i class="fm-italics">ŒΩ<sub class="fm-subscript">n</sub></i> ‚â• <i class="fm-italics">d</i> + 1</span> i.e.,</p><!--<p class="Body"><span class="times">$p\left( \vec{x} \right) = \mathcal{N}\left( \vec{x}
; \;\; \vec{ \mu }, \frac{ \left( n S  + \boldsymbol{ W_{0} }^{-1}
\right) }{ \left( \nu_{n} - d - 1 \right) } \right).$</span></p>-->
<div class="figure">
<p class="figure2"><img alt="" class="calibre5" height="94" src="../../OEBPS/Images/eq_13-07-o9.png" width="299"/></p>
</div>
<h2 class="fm-head" id="summary-12">Summary<a id="marker-466"/></h2>
<ul class="calibre6">
<li class="fm-list-bullet">
<p class="list">A generative model that models the underlying data distribution can be more powerful than a black box discriminative model. Once we choose a model family, we need to estimate the model parameters, <i class="timesitalic">Œ∏</i>. We can estimate the best values of <i class="timesitalic">Œ∏</i> from the training data <i class="timesitalic">X</i> using Bayes‚Äô theorem.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The posterior distribution <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œ∏</i>|<i class="fm-italics">X</i>)</span> is a function of the product of likelihood <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i>|<i class="fm-italics">Œ∏</i>)</span> and the prior <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œ∏</i>)</span>. The prior expresses our belief in the value of the parameters. The posterior is dominated by the prior for small data sets and the likelihood for large data sets. Injecting belief via a good prior distribution can be helpful in settings with very little training data.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Maximum likelihood estimation only relies on the data, in contrast to maximum a posteriori (MAP) estimation, which relies on the data as well as the prior information.</p>
</li>
<li class="fm-list-bullet">
<p class="list">We can use Bayesian estimation for the mean of a Gaussian likelihood when the variance is known. When the likelihood is Gaussian <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">X</i>) ‚àº <i class="fm-italics">N</i>(<i class="fm-italics">Œº</i>, <i class="fm-italics">œÉ</i>)</span>, we model the prior as a normal distribution <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œº</i>) ‚àº <i class="fm-italics">N</i>(<i class="fm-italics">Œº</i><sub class="fm-subscript">0</sub>, <i class="fm-italics">œÉ</i><sub class="fm-subscript">0</sub>)</span>. The posterior distribution is also a normal distribution <span class="math"><i class="fm-italics">p</i>(<i class="fm-italics">Œº</i>|<i class="fm-italics">X</i>) ‚àº <i class="fm-italics">N</i>(<i class="fm-italics">Œº<sub class="fm-subscript">n</sub></i>, <i class="fm-italics">œÉ<sub class="fm-subscript">n</sub></i>)</span>, where <!--<span class="times">$\sigma_{n}^{2}  = \frac{ \sigma_{0}^{2}\sigma^{2} }{ n \sigma_{0}^{2} + \sigma^{2}}$</span>--><span class="infigure"><img alt="" class="calibre5" height="53" src="../../OEBPS/Images/eq_13-07-p1a.png" width="116"/></span> and <!--<span class="times">$\mu_{n}= \frac{ \bar{x} } { 1 + \frac{\sigma^{2} }{ n \sigma_{0}^{2} } } + \frac{ \mu_{0} } { 1 + \frac{ n \sigma_{0}^{2} }{\sigma^{2} } }$</span>--><span class="infigure"><img alt="" class="calibre5" height="56" src="../../OEBPS/Images/eq_13-07-p2a.png" width="179"/></span>. We can also use the estimated parameter to make predictions about new instances of data.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Weak priors imply a high degree of uncertainty/lower confidence in our prior belief and can easily be overwhelmed by the data. In contrast, strong priors imply a lower degree of uncertainty/higher confidence in our prior belief and will resist data overload.</p>
</li>
<li class="fm-list-bullet">
<p class="list">For a specific family of likelihood, the choice of the prior that results in the posterior belonging to the same family as the prior is called a conjugate prior.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The gamma function is <!--(<span class="times"><i class="fm-italics">Œì</em>(<i class="fm-italics">Œ±</em>) = ‚à´<sub class="FM-Subscript"><i class="fm-italics">x</em>=0</sub><sup class="FM-Superscript">‚àû</sup><i class="fm-italics">x</em><sup class="FM-Superscript">(<i class="fm-italics">Œ±</em> ‚àí 1)</sup><i class="fm-italics">e</em><sup class="FM-Superscript">‚àí<i class="fm-italics">x</em></sup><i class="fm-italics">dx</em></span>)--><span class="infigure"><img alt="" class="calibre5" height="42" src="../../OEBPS/Images/eq_13-07-p3a.png" width="251"/></span>, and the gamma distribution is <!--<span class="times">$p\left( \lambda\middle\vert \alpha, \beta \right) = \gamma \left(\lambda; \alpha, \beta\right) = \frac{ \beta^{ \alpha } }{ \Gamma\left( \alpha \right) }\lambda^{ \left( \alpha - 1 \right)} e^{ - \beta \lambda}$</span>--><span class="infigure"><img alt="" class="calibre5" height="42" src="../../OEBPS/Images/eq_13-07-p4a.png" width="411"/></span>. The gamma distribution varies with different values of <i class="timesitalic">Œ±</i> and <i class="timesitalic">Œ≤</i>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">In the case of Bayesian estimation of the precision of the Gaussian likelihood for a known mean, the precision <i class="timesitalic">Œª</i> is the inverse of the variance. We can model the prior as a gamma distribution <!--<span class="times">$p\left( \lambda \right)  = \frac{ \beta_{0}^{\alpha_{0} } }{ \Gamma\left( \alpha_{0} \right) } \lambda ^{ \left(\alpha_{0} - 1 \right)} e^{ - \beta_{0} \lambda }$</span>--><span class="infigure"><img alt="" class="calibre5" height="51" src="../../OEBPS/Images/eq_13-07-p5a.png" width="254"/></span>. The posterior distribution is also a gamma distribution, <!--<span class="times">$p\left( \lambda \middle\vert X \right) = \frac{
\beta_{n}^{ \alpha_{n} } }{ \Gamma\left( \alpha_{n} \right) } \lambda ^{\left( \alpha_{n} - 1 \right)} e^{ - \beta_{n} \lambda}$</span>--><span class="infigure"><img alt="" class="calibre5" height="44" src="../../OEBPS/Images/eq_13-07-p6a.png" width="274"/></span>, where <!--<span class="times">$\alpha_{n}  = \frac{ n }{ 2 } +\alpha_{0}$</span>--><span class="infigure"><img alt="" class="calibre5" height="37" src="../../OEBPS/Images/eq_13-07-p7a.png" width="115"/></span> and <!--<span class="times">$\beta_{n} = \frac{ 1}{ 2 } \sum_{i=1}^{n} \left( {x^{ \left( i \right) } - \mu } \right)^2 +\beta_{0} = \frac{ n }{ 2 } s + \beta_{0}$</span>--><span class="infigure"><img alt="" class="calibre5" height="49" src="../../OEBPS/Images/eq_13-07-p8a.png" width="358"/></span>.</p>
</li>
<li class="fm-list-bullet">
<p class="list">In Bayesian estimation of both the mean and precision of a Gaussian likelihood, we model the prior as a normal-gamma distribution. The posterior is another normal-gamma distribution. The posterior distribution can be used to predict new data instances.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The multivariate setting of Bayesian inferencing of the mean of a Gaussian likelihood is known as precision. We can model the prior as a multivariate normal distribution; the posterior is also a multivariate normal distribution.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The Wishart distribution is the multivariate version of the gamma distribution. With multivariate Bayesian inferencing of the precision of a Gaussian likelihood with a known mean, we can model the prior as a Wishart distribution. The corresponding posterior is also a Wishart distribution.<a id="marker-467"/></p>
</li>
</ul>
</div></body></html>