["```py\nimport argparse\n\nfrom fastapi import FastAPI, Request\nfrom fastapi.responses import Response\nimport torch\nimport uvicorn\n\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    StoppingCriteria,\n    StoppingCriteriaList,\n)\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)    #1\ntorch.backends.cuda.enable_flash_sdp(False)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nstop_tokens = [\"def\", \"class\", \"Instruction\", \"Output\"]     #2\nstop_token_ids = [589, 823, 9597, 2301]\n\nclass StopOnTokens(StoppingCriteria):\n    def __call__(\n        self,\n        input_ids: torch.LongTensor,\n        scores: torch.FloatTensor,\n        **kwargs,\n    ) -> bool:\n        stop_ids = stop_token_ids\n        for stop_id in stop_ids:\n            if input_ids[0][-1] == stop_id:\n                return True\n        return False\n\ntokenizer = AutoTokenizer.from_pretrained(\"Deci/DeciCoder-1b\")    #3\ntokenizer.add_special_tokens( #3\n    {\"additional_special_tokens\": stop_tokens}, #3\n    replace_additional_special_tokens=False, #3\n) #3\nmodel = AutoModelForCausalLM.from_pretrained( #3\n    \"Deci/DeciCoder-1b\", torch_dtype=torch.bfloat16, trust_remote_code=True #3\n) #3\nmodel = model.to(device) #3\n\napp = FastAPI()     #4\n\n@app.post(\"/generate\")\nasync def generate(request: Request) -> Response:\n    \"\"\"Generate LLM Response\n\n    The request should be a JSON object with the following fields:\n    - prompt: the prompt to use for the generation.\n    \"\"\"\n    request_dict = await request.json()\n    prompt = request_dict.pop(\"prompt\")\n\n    # ...      #5\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)    #6\n    response_tokens = model.generate(\n        inputs[\"input_ids\"],\n        max_new_tokens=1024,\n        stopping_criteria=StoppingCriteriaList([StopOnTokens()]),\n    )\n    input_length = inputs[\"input_ids\"].shape[1]\n    response = tokenizer.decode(\n        response_tokens[0][input_length:], skip_special_tokens=True\n    )\n\n    return response\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()     #7\n    parser.add_argument(\"--host\", type=str, default=None)\n    parser.add_argument(\"--port\", type=int, default=8000)\n    args = parser.parse_args()\n\n    uvicorn.run(app, host=args.host, port=args.port, log_level=\"debug\")\n```", "```py\n$ curl --request POST --header \"Content-Type: application/json\" --data \n↪ '{\"prompt\":\"def hello_world(name):\"}' http://localhost:8000/generate\n```", "```py\n$ wget https://raw.githubusercontent.com/milvus-io/milvus/master/scripts/\n↪ standalone_embed.sh\n$ bash standalone_embed.sh start\n```", "```py\n$ bash standalone_embed.sh stop\n```", "```py\n$ bash standalone_embed.sh delete\n```", "```py\nfrom pymilvus import (\n    connections,\n    utility,\n    FieldSchema,\n    CollectionSchema,\n    DataType,\n    Collection,\n)\n\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom sentence_transformers import SentenceTransformer\n\nfrom tqdm.auto import tqdm\nfrom uuid import uuid4\n\nconnections.connect(\"default\", host=\"localhost\", port=\"19530\")    #1\n\nclass PythonCodeIngestion:\n    def __init__(\n        self,\n        collection,\n        python_code=None,\n        embedder=None,\n        tokenizer=None,\n        text_splitter=None,\n        batch_limit=100,\n    ):\n        self.collection = collection\n        self.python_code = python_code or load_dataset(\n            \"iamtarun/python_code_instructions_18k_alpaca\",\n            split=\"train\",\n        )\n        self.embedder = embedder or SentenceTransformer(\n            \"krlvi/sentence-t5-base-nlpl-code_search_net\"\n        )\n        self.tokenizer = tokenizer or AutoTokenizer.from_pretrained(\n            \"Deci/DeciCoder-1b\"\n        )\n        self.text_splitter = (\n            text_splitter\n            or RecursiveCharacterTextSplitter(\n                chunk_size=400,\n                chunk_overlap=20,\n                length_function=self.token_length,\n                separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n            )\n        )\n        self.batch_limit = batch_limit\n\n    def token_length(self, text):\n        tokens = self.tokenizer.encode(text)\n        return len(tokens)\n\n    def get_metadata(self, page):\n        return {\n            \"instruction\": page[\"instruction\"],\n            \"input\": page[\"input\"],\n            \"output\": page[\"output\"],\n        }\n\n    def split_texts_and_metadatas(self, page):\n        basic_metadata = self.get_metadata(page)\n        prompts = self.text_splitter.split_text(page[\"prompt\"])\n        metadatas = [\n            {\"chunk\": j, \"prompt\": prompt, **basic_metadata}\n            for j, prompt in enumerate(prompts)\n        ]\n        return prompts, metadatas\n\n    def upload_batch(self, texts, metadatas):\n        ids = [str(uuid4()) for _ in range(len(texts))]\n        embeddings = self.embedder.encode(texts)\n        self.collection.insert([ids, embeddings, metadatas])\n\n    def batch_upload(self):\n        batch_texts = []\n        batch_metadatas = []\n\n        for page in tqdm(self.python_code):\n            texts, metadatas = self.split_texts_and_metadatas(page)\n\n            batch_texts.extend(texts)\n            batch_metadatas.extend(metadatas)\n\n            if len(batch_texts) >= self.batch_limit:\n                self.upload_batch(batch_texts, batch_metadatas)\n                batch_texts = []\n                batch_metadatas = []\n\n        if len(batch_texts) > 0:\n            self.upload_batch(batch_texts, batch_metadatas)\n\n        self.collection.flush()\n```", "```py\nif __name__ == \"__main__\":\n    collection_name = \"milvus_llm_example\"\n    dim = 768\n\n    if utility.has_collection(collection_name):     #1\n        utility.drop_collection(collection_name)\n\n    fields = [\n        FieldSchema(\n            name=\"ids\",\n            dtype=DataType.VARCHAR,\n            is_primary=True,\n            auto_id=False,\n            max_length=36,\n        ),\n        FieldSchema(\n            name=\"embeddings\", dtype=DataType.FLOAT_VECTOR, dim=dim\n        ),\n        FieldSchema(name=\"metadata\", dtype=DataType.JSON),\n    ]\n\n    schema = CollectionSchema(\n        fields, f\"{collection_name} is collection of python code prompts\"\n    )\n\n    print(f\"Create collection {collection_name}\")\n    collection = Collection(collection_name, schema)\n\n    collection = Collection(collection_name)    #2\n    print(collection.num_entities)\n\n    python_code_ingestion = PythonCodeIngestion(collection)    #3\n    python_code_ingestion.batch_upload()\n    print(collection.num_entities)\n\n    search_index = {                #4\n        \"index_type\": \"IVF_FLAT\",\n        \"metric_type\": \"L2\",\n        \"params\": {\"nlist\": 128},      #5\n    }\n    collection.create_index(\"embeddings\", search_index)\n```", "```py\n    collection.load()      #1\n\n    query = (             #2\n        \"Construct a neural network model in Python to classify \"\n        \"the MNIST data set correctly.\"\n    )\n    search_embedding = python_code_ingestion.embedder.encode(query)\n    search_params = {\n        \"metric_type\": \"L2\",\n        \"params\": {\"nprobe\": 10},     #3\n    }\n    results = collection.search(\n        [search_embedding],\n        \"embeddings\",\n        search_params,\n        limit=3,\n        output_fields=[\"metadata\"],\n    )\n    for hits in results:\n        for hit in hits:\n            print(hit.distance)\n            print(hit.entity.metadata[\"instruction\"])\n```", "```py\n    # 0.7066953182220459\n    # Create a neural network in Python to identify\n    # hand-written digits from the MNIST dataset.\n    # 0.7366453409194946\n    # Create a question-answering system using Python\n    # and Natural Language Processing.\n    # 0.7389795184135437\n    # Write a Python program to create a neural network model that can\n    # classify handwritten digits (0-9) with at least 95% accuracy.\n```", "```py\nfrom contextlib import asynccontextmanager\n\nfrom pymilvus import (\n    connections,\n    Collection,\n)\nfrom sentence_transformers import SentenceTransformer\nconnections.connect(\"default\", host=\"localhost\", port=\"19530\")    #1\n\ncollection_name = \"milvus_llm_example\"\ncollection = Collection(collection_name)\n\nembedder = SentenceTransformer(                  #2\n    \"krlvi/sentence-t5-base-nlpl-code_search_net\"\n)\nembedder = embedder.to(device)\n```", "```py\ndef token_length(text):\n    tokens = tokenizer([text], return_tensors=\"pt\")\n    return tokens[\"input_ids\"].shape[1]\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    collection.load()                #1\n    yield\n    collection.release()            #2\n\napp = FastAPI(lifespan=lifespan)      #3\n```", "```py\n    request_dict = await request.json()      #1\n    prompt = request_dict.pop(\"prompt\")\n\n    search_embedding = embedder.encode(prompt)      #2\n    search_params = {\n        \"metric_type\": \"L2\",\n        \"params\": {\"nprobe\": 10},\n    }\n    results = collection.search(\n        [search_embedding],\n        \"embeddings\",\n        search_params,\n        limit=5,\n        output_fields=[\"metadata\"],\n    )\n\n    examples = []\n    for hits in results:\n        for hit in hits:\n            metadata = hit.entity.metadata\n            examples.append(\n                f\"Instruction: {metadata['instruction']}\\n\"\n                f\"Output: {metadata['output']}\\n\\n\"\n            )\n\n    prompt_instruction = (\n        \"You are an expert software engineer who specializes in Python. \"\n        \"Write python code to fulfill the request from the user.\\n\\n\"\n    )\n    prompt_user = f\"Instruction: {prompt}\\nOutput: \"\n\n    max_tokens = 2048\n    token_count = token_length(prompt_instruction+prompt_user)\n\n    prompt_examples = \"\"\n    for example in examples:\n        token_count += token_length(example)\n        if token_count < max_tokens:\n            prompt_examples += example\n        else:\n            break\n\n    full_prompt = f\"{prompt_instruction}{prompt_examples}{prompt_user}\"\n\n    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n```", "```py\n$ curl --request POST --header \"Content-Type: application/json\" --data \n↪ '{\"prompt\":\"def hello_world(name):\"}' http://localhost:8000/generate\n```", "```py\n$ npm install -g yo generator-code\n```", "```py\n$ yo code\n```", "```py\n{\n  \"name\": \"llm-coding-copilot\",\n  \"displayName\": \"llm_coding_copilot\",\n  \"description\": \"VSCode extension to add LLM code suggestions inline.\",\n  \"version\": \"0.0.1\",\n  \"engines\": {\n    \"vscode\": \"^1.86.0\"\n  },\n  \"categories\": [\n    \"Other\"\n  ],\n  \"activationEvents\": [\n    \"onCommand:editor.action.inlineSuggest.trigger\"\n  ],\n  \"main\": \"./extension.js\",\n  \"contributes\": {\n    \"commands\": [{\n      \"command\": \"llm-coding-copilot.helloWorld\",\n      \"title\": \"Hello World\"\n    }],\n    \"keybindings\": [{\n      \"key\": \"Alt+s\",\n      \"command\": \"editor.action.inlineSuggest.trigger\",\n      \"mac\": \"Alt+s\"\n    }]\n  },\n  \"scripts\": {\n    \"lint\": \"eslint .\",\n    \"pretest\": \"npm run lint\",\n    \"test\": \"vscode-test\"\n  },\n  \"devDependencies\": {\n    \"@types/vscode\": \"^1.86.0\",\n    \"@types/mocha\": \"^10.0.6\",\n    \"@types/node\": \"18.x\",\n    \"eslint\": \"^8.56.0\",\n    \"typescript\": \"^5.3.3\",\n    \"@vscode/test-cli\": \"^0.0.4\",\n    \"@vscode/test-electron\": \"^2.3.8\"\n  }\n}\n```", "```py\n// Import VSCode API library\nconst vscode = require('vscode');\n\n// This method is called when your extension is activated\nfunction activate(context) {\n  console.log('Congratulations, your extension \"llm-coding-copilot\" is now\n↪ active!');\n\n  // This creates and registers a new command, matching package.json\n  // But we won’t use it!\n  let disposable = vscode.commands.registerCommand('llm-coding-\n↪ copilot.helloWorld', function () {\n    // The code you place here will be executed every time your command is\n↪ executed\n\n    // Display a message box to the user\n    vscode.window.showInformationMessage('Hello World from llm_coding_\n↪ copilot!');\n  });\n\n  context.subscriptions.push(disposable);\n}\n\n// This method is called when your extension is deactivated\nfunction deactivate() {}\n\nmodule.exports = {\n  activate,\n  deactivate\n}\n```", "```py\n// Create inline completion provider, this makes suggestions inline\nconst provider = {\n    provideInlineCompletionItems: async (\n            document, position, context, token\n        ) => {\n        // Inline suggestion code goes here\n\n    }\n};\n\n// Add provider to Python files\nvscode.languages.registerInlineCompletionItemProvider(\n    { scheme: 'file', language: 'python' },\n    provider\n);\n// Example of adding provider to all languages\nvscode.languages.registerInlineCompletionItemProvider(\n    { pattern: '**' },\n    provider\n);\n```", "```py\n// Create inline completion provider, this makes suggestions inline\nconst provider = {\n    provideInlineCompletionItems: async (\n            document, position, context, token\n        ) => {\n        // Grab VSCode editor and selection\n        const editor = vscode.window.activeTextEditor;\n        const selection = editor.selection;\n        const triggerKindManual = 0\n        const manuallyTriggered = context.triggerKind == triggerKindManual\n\n        // If highlighted back to front, put cursor at the end and rerun\n        if (manuallyTriggered && position.isEqual(selection.start)) {\n            editor.selection = new vscode.Selection(\n                selection.start, selection.end\n            )\n            vscode.commands.executeCommand(\n                \"editor.action.inlineSuggest.trigger\"\n            )\n            return []\n        }\n\n        // On activation send highlighted text to LLM for suggestions\n        if (manuallyTriggered && selection && !selection.isEmpty) {\n            // Grab highlighted text\n            const selectionRange = new vscode.Range(\n                selection.start, selection.end\n            );\n            const highlighted = editor.document.getText(selectionRange);\n\n            // Send highlighted code to LLM\n        }\n    }\n};\n```", "```py\n// On activation send highlighted text to LLM for suggestions\nif (manuallyTriggered && selection && !selection.isEmpty) {\n    // Grab highlighted text\n    const selectionRange = new vscode.Range(\n        selection.start, selection.end\n    );\n    const highlighted = editor.document.getText(\n        selectionRange\n    );\n\n    // Send highlighted text to LLM API\n    var payload = {\n        prompt: highlighted\n    };\n\n    const response = await fetch(\n        'http://localhost:8000/generate', {\n        method: 'POST',\n        headers: {\n            'Content-Type': 'application/json',\n        },\n        body: JSON.stringify(payload),\n    });\n\n    // Return response as suggestion to VSCode editor\n    var responseText = await response.text();\n\n    range = new vscode.Range(selection.end, selection.end)\n    return new Promise(resolve => {\n        resolve([{ insertText: responseText, range }])\n    })\n}\n```", "```py\nclass ListNode:\n    def __init__(self, val=0, next=None):\n        self.val = val\n        self.next = next\n\ndef reverseList(head): \n    prev = None\n    current = head\n\n    while current is not None:\n        nxt = current.next\n        current.next = prev \n        prev = current\n        current = nxt\n\n    head = prev \n    return head\n```", "```py\ndef reverse(head):\n    prev = None\n    current = head\n    while current:\n        next = current.next\n        current.next = prev\n        prev = current\n        current = next\n    return prev\n```"]