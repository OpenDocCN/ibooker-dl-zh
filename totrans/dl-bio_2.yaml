- en: Chapter 2\. Learning the Language of Proteins
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章\. 学习蛋白质的语言
- en: 'Life as we know it operates on *proteins*. The human genome holds about 20,000
    *genes*, each made of DNA, that serve as blueprints for building different proteins.
    Some proteins have simple, well-understood functions—like collagen, which provides
    structural support and elasticity to tissues, or hemoglobin, which transports
    oxygen and carbon dioxide between the lungs and the rest of the body. Others have
    slightly more abstract roles: they act as messengers, modulators, or signal carriers,
    transmitting information within and between cells. For example, insulin is a protein
    hormone that signals cells to absorb sugar from the bloodstream.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所知的生活依赖于*蛋白质*。人类基因组大约有20,000个*基因*，每个基因由DNA组成，作为构建不同蛋白质的蓝图。一些蛋白质具有简单、易于理解的函数——如胶原蛋白，它为组织提供结构支持和弹性，或血红蛋白，它在肺部和身体其他部位之间运输氧气和二氧化碳。其他蛋白质则具有稍微抽象的角色：它们作为信使、调节剂或信号载体，在细胞内和细胞间传递信息。例如，胰岛素是一种蛋白质激素，它向细胞发出吸收血液中糖分的信号。
- en: We’ll dive into how DNA and proteins work in more detail soon. But for now,
    imagine a protein as a blobby molecular machine bumping around in the crowded
    cell environment, occasionally making productive collisions. Its shape and movement
    may seem chaotic, but both have been fine-tuned by millions of years of evolution
    to carry out very specific molecular functions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快将更详细地探讨DNA和蛋白质的工作原理。但到目前为止，想象一下蛋白质作为一个在拥挤的细胞环境中四处碰撞的粘稠分子机器，偶尔进行有成效的碰撞。其形状和运动可能看起来很混乱，但经过数百万年的进化，它们都已经被精细调整以执行非常具体的分子功能。
- en: 'One key detail for this chapter: a protein can be represented as a sequence
    of its constituent building blocks, called *amino acids*. Just as English uses
    26 letters to form words, proteins use an alphabet of 20 amino acids to form long
    chains with specific shapes and jobs. With that in mind, the goal of this chapter
    is simple: we’ll train a model to predict a protein’s function given its amino
    acid sequence. For example:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的一个关键细节：蛋白质可以被表示为其构成单元的序列，这些单元被称为*氨基酸*。正如英语使用26个字母来形成单词一样，蛋白质使用20种氨基酸的字母表来形成具有特定形状和功能的长期链。考虑到这一点，本章的目标很简单：我们将训练一个模型，根据蛋白质的氨基酸序列来预测其功能。例如：
- en: Given the sequence of the COL1A1 collagen protein (`MFSFVDLR...`), we might
    predict its function is likely `structural` with probability 0.7, `enzymatic`
    with probability 0.01, and so on.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定COL1A1胶原蛋白蛋白的序列（`MFSFVDLR...`），我们可能会预测其功能很可能是`结构性`的，概率为0.7，`酶促`的，概率为0.01，等等。
- en: Given the sequence of the INS insulin protein (`MALWMRLL...`), we might predict
    its function is likely *metabolic* with probability 0.6, *signaling* with probability
    0.3, and so on.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给定INS胰岛素蛋白的序列（`MALWMRLL...`），我们可能会预测其功能很可能是*代谢性*的，概率为0.6，*信号传导*的，概率为0.3，等等。
- en: Tip
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: To get hands-on with the material right away, open the companion Colab notebook
    and try running the code as you read the chapter. Exploring the examples interactively
    is one of the best ways to build intuition and make the ideas stick.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了立即动手实践，打开配套的Colab笔记本，在阅读章节的同时尝试运行代码。交互式探索示例是建立直觉和使想法牢固的最佳方式之一。
- en: Biology Primer
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物入门
- en: We already highlighted that proteins are essential units of function within
    the cell, fulfilling a vast range of biological roles. A protein’s function is
    very closely tied to its 3D structure, which in turn is determined by its primary
    amino acid sequence.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经强调过，蛋白质是细胞内功能的基本单位，承担着广泛的生物角色。蛋白质的功能与其3D结构密切相关，而3D结构又由其一级氨基酸序列决定。
- en: 'To recap the flow of information: a gene encodes the primary amino acid sequence
    of a protein. That sequence determines the protein’s structure, and the structure
    governs its function.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾信息流：一个基因编码蛋白质的一级氨基酸序列。这个序列决定了蛋白质的结构，而结构又决定了其功能。
- en: Protein Structure
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蛋白质结构
- en: 'Protein structure is typically described in four hierarchical levels:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质结构通常用四个层次来描述：
- en: Primary structure
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一级结构
- en: The linear sequence of amino acids
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 氨基酸的线性序列
- en: Secondary structure
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 二级结构
- en: Local folding into structural elements such as alpha helices and beta sheets
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 局部折叠成如α螺旋和β片层等结构元素
- en: Tertiary structure
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 三级结构
- en: The overall 3D shape formed by the complete amino acid chain
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 完整氨基酸链形成的整体3D形状
- en: Quaternary structure
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 四级结构
- en: The assembly of multiple protein subunits into a functional complex (not all
    proteins have this)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 将多个蛋白质亚基组装成功能复合体（并非所有蛋白质都有这种结构）
- en: As an example, [Figure 2-1](#id1) shows the structural organization levels of
    hemoglobin.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[图2-1](#id1)展示了血红蛋白的结构组织层次。
- en: The human genetic code specifies 20 main amino acids. Each has a unique chemical
    structure, but they can be grouped by shared biochemical properties—such as hydrophobicity
    (how they interact with water), charge (positive, negative, or neutral), and polarity
    (how evenly electrical charge is distributed over the molecule).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 人类遗传密码指定了20种主要氨基酸。每种氨基酸都有独特的化学结构，但它们可以根据共享的生化特性进行分组——例如疏水性（它们与水的相互作用）、电荷（正、负或中性）和极性（电荷在分子上的分布均匀性）。
- en: Although biochemistry students are often expected to memorize all 20 amino acids,
    complete with names, structures, and single-letter codes (don’t ask us how we
    know), it’s more practical here to focus on their functional roles (summarized
    in [Figure 2-2](#amino-acids)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管生物化学学生通常被期望记住所有20种氨基酸，包括名称、结构和单字母代码（不要问我们如何知道），但在这里更实际的做法是关注它们的机能作用（总结在[图2-2](#amino-acids)中）。
- en: '![](assets/dlfb_0201.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0201.png)'
- en: 'Figure 2-1\. The four levels of protein structure, as illustrated by the hemoglobin
    protein. Source: [Wikipedia](https://oreil.ly/BD2Qa).'
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1。蛋白质结构的四个层次，如图中血红蛋白蛋白所示。来源：[维基百科](https://oreil.ly/BD2Qa)。
- en: For instance, `D` (aspartic acid) and `E` (glutamic acid) are both negatively
    charged and often interchangeable without drastically altering a protein’s function.
    But other amino acids play much more specific roles, and even a single substitution
    can dramatically alter how a protein folds or functions—sometimes with serious
    effects. In fact, many genetic diseases are caused by such point mutations. One
    famous example is sickle cell anemia, which is caused by a single-letter change
    in the gene for hemoglobin that replaces a hydrophilic amino acid (`E`) with a
    hydrophobic one (`V`), which ultimately leads to misshapen red blood cells.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，`D`（天冬氨酸）和`E`（谷氨酸）都是带负电荷的，并且通常可以互换，而不会极大地改变蛋白质的功能。但其他氨基酸扮演着更加具体的角色，甚至一个氨基酸的替换也可能极大地改变蛋白质的折叠或功能——有时会产生严重影响。事实上，许多遗传疾病都是由这种点突变引起的。一个著名的例子是镰状细胞性贫血，这是由于血红蛋白基因中的一个字母变化，将亲水性氨基酸（`E`）替换为疏水性氨基酸（`V`），最终导致红细胞变形。
- en: '![](assets/dlfb_0202.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0202.png)'
- en: Figure 2-2\. Chart showing the chemical structures of the 20 standard amino
    acids found in living organisms, grouped by biochemical similarity, color-coded
    by side-chain properties (e.g., acidic, basic, polar, nonpolar), and annotated
    with their names, one- and three-letter codes, and example DNA codons (the triplet
    DNA bases that code for that amino acid). Adapted from an infographic by [Compound
    Interest](https://oreil.ly/o7Lyq).
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-2。显示生物体中发现的20种标准氨基酸的化学结构图，按生化相似性分组，按侧链特性（例如，酸性、碱性、极性、非极性）着色编码，并标注了它们的名称、一字母和三字母代码以及示例DNA密码子（编码该氨基酸的三联DNA碱基）。改编自[Compound
    Interest](https://oreil.ly/o7Lyq)的信息图。
- en: With that introduction to protein structure, let’s now look at function—what
    proteins actually do in the cell.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对蛋白质结构的介绍，我们现在来看看功能——蛋白质在细胞中实际上做什么。
- en: Protein Function
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蛋白质功能
- en: 'Proteins carry out nearly every task required for life: they catalyze chemical
    reactions, transmit signals, transport molecules, provide structural support,
    and regulate gene expression. Because of this diversity, systematically cataloging
    protein functions is a massive undertaking—and one of the most widely used frameworks
    for doing so is the *Gene Ontology* (GO) project.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质执行生命所需的几乎所有任务：它们催化化学反应、传递信号、运输分子、提供结构支持和调节基因表达。由于这种多样性，系统地编制蛋白质功能目录是一项庞大的工作——而最广泛使用的框架之一是*基因本体*（GO）项目。
- en: 'The GO system organizes protein function into three broad categories, each
    capturing a different aspect of how proteins behave in the cell:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: GO系统将蛋白质功能组织成三个广泛的类别，每个类别都捕捉到蛋白质在细胞中行为的不同方面：
- en: Biological process
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 生物过程
- en: This contributes to—like cell division, response to stress, carbohydrate metabolism,
    or immune signaling.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这有助于——如细胞分裂、对压力的反应、碳水化合物代谢或免疫信号。
- en: Molecular function
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 分子功能
- en: This describes the specific biochemical activity of the protein itself—such
    as binding to DNA or ATP (a molecule that stores and transfers energy in cells),
    acting as a kinase (an enzyme that attaches a small chemical tag called a phosphate
    group to other molecules to change their activity), or transporting ions across
    membranes.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这描述了蛋白质本身的特定生化活性——例如结合DNA或ATP（一种在细胞中储存和转移能量的分子），作为激酶（一种将称为磷酸基团的小化学标签附着到其他分子上以改变其活性的酶），或跨膜运输离子。
- en: Cellular component
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 细胞组分
- en: This indicates where in the cell the protein usually resides—such as the nucleus,
    mitochondria, or extracellular space. Although it’s technically a location label
    and not a function *per se*, it often provides important clues about the protein’s
    role (e.g., proteins in the mitochondria are probably involved in energy production).
    We’ll return to this theme in [Chapter 6](ch06.html#learning-spatial-organization-patterns-within-cells).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这表示蛋白质通常驻留在细胞中的位置——例如细胞核、线粒体或细胞外空间。尽管这从技术上讲是一个位置标签，而不是一个功能本身，但它通常提供了关于蛋白质作用的重要线索（例如，线粒体内的蛋白质可能参与能量产生）。我们将在[第6章](ch06.html#learning-spatial-organization-patterns-within-cells)中回到这个主题。
- en: Each protein can have multiple GO annotations across these categories. For example,
    a single protein might bind ATP (molecular function), drive muscle contraction
    (biological process), and localize to muscle fibers (cellular component). Some
    annotations are derived from direct experimental assays, while others are inferred
    computationally through similarity to known proteins. In this chapter, we’ll work
    with a curated subset of high-confidence, experimentally validated GO annotations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每个蛋白质可以在这几个类别中具有多个GO注释。例如，一个单一的蛋白质可能结合ATP（分子功能），驱动肌肉收缩（生物过程），并定位到肌纤维（细胞组分）。一些注释来自直接的实验检测，而其他注释则是通过与其他已知蛋白质的相似性通过计算推断出来的。在本章中，我们将使用一组经过精心挑选的高置信度、实验验证的GO注释。
- en: Predicting Protein Function
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测蛋白质功能
- en: 'Why predict a protein’s function from its sequence? This is actually a fundamental
    challenge in modern biology. Here are a few of the most common and impactful applications:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要从序列中预测蛋白质的功能？这实际上是现代生物学中的一个基本挑战。以下是一些最常见和最有影响的应用：
- en: Biotechnology and protein engineering
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 生物技术和蛋白质工程
- en: If we can reliably predict function from sequence, we can begin to design new
    proteins with desired properties. This could be useful for designing enzymes for
    industrial chemistry, therapeutic proteins for medicine, or synthetic biology
    components.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以从序列中可靠地预测功能，我们就可以开始设计具有所需特性的新蛋白质。这可以用于设计工业化学中的酶、医学中的治疗蛋白质或合成生物学组件。
- en: Understanding disease mechanisms
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 理解疾病机制
- en: Many diseases are caused by specific sequence changes (variants, or mutations)
    that disrupt protein function. A good predictive model can help identify how specific
    mutations alter function, offering insights into disease mechanisms and potential
    therapeutic targets.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 许多疾病是由特定的序列变化（变异或突变）引起的，这些变化破坏了蛋白质的功能。一个好的预测模型可以帮助识别特定突变如何改变功能，从而为疾病机制和潜在的治疗靶点提供见解。
- en: Genome annotation
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 基因组注释
- en: As we continue sequencing the genomes of new species, we’re uncovering vast
    numbers of proteins whose functions remain unknown. For newly identified proteins—especially
    those that are distantly evolutionarily related to any known ones—computational
    prediction is essential for assigning functional hypotheses.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们继续测序新物种的基因组，我们正在发现大量功能未知的蛋白质。对于新发现的蛋白质——尤其是那些与已知蛋白质在进化上关系较远的蛋白质——计算预测对于分配功能假设是必不可少的。
- en: Metagenomics and microbiome analysis
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 原核生物学和微生物组分析
- en: When sequencing entire microbial communities, such as gut bacteria or ocean
    microbiota, many protein-coding genes have no close matches in existing databases.
    Predicting function from sequence helps uncover the roles of these unknown proteins,
    advancing our understanding of microbial ecosystems and their effects on hosts
    or the environment.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当测序整个微生物群落，如肠道细菌或海洋微生物群时，许多蛋白质编码基因在现有数据库中没有接近的匹配。从序列中预测功能有助于揭示这些未知蛋白质的作用，推进我们对微生物生态系统及其对宿主或环境的影响的理解。
- en: Note
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注
- en: 'Although the task may sound somewhat straightforward—input a sequence, output
    a function—accurate protein function prediction is an extremely challenging problem.
    To succeed, a model must implicitly understand a range of highly complex biological
    principles: how amino acid sequence determines 3D structure (a Nobel Prize–winning
    machine learning problem in its own right), how structure enables function, and
    how these functions operate in the dynamic, crowded environment of the cell.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这项任务听起来可能有些简单——输入一个序列，输出一个功能——准确的蛋白质功能预测是一个极其具有挑战性的问题。为了成功，模型必须隐式地理解一系列高度复杂的生物学原理：氨基酸序列如何决定3D结构（一个本身也是诺贝尔奖获奖的机器学习问题），结构如何使功能得以实现，以及这些功能如何在细胞动态、拥挤的环境中运作。
- en: In this chapter, we won’t aim for state-of-the-art performance. Instead, our
    goal is to build a simple working model and develop intuition for how protein
    sequences can be mapped to functional annotations. Along the way, we’ll introduce
    several useful machine learning techniques—including using pretrained models to
    extract embeddings, visualizing those embeddings, and training lightweight classifiers
    on top of them—that will become recurring tools in later chapters.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们不会追求最先进的性能。相反，我们的目标是构建一个简单的可工作模型，并培养对蛋白质序列如何映射到功能注释的直觉。在这个过程中，我们将介绍几种有用的机器学习技术——包括使用预训练模型提取嵌入、可视化这些嵌入以及在它们之上训练轻量级分类器——这些技术将成为后续章节中反复出现的工具。
- en: Machine Learning Primer
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习入门
- en: We’ve briefly reviewed the biological background of proteins and how their function
    is encoded. Now, we’ll turn to the machine learning techniques that allow us to
    learn from protein sequences in practice.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要回顾了蛋白质的生物背景以及它们的函数是如何编码的。现在，我们将转向那些允许我们从蛋白质序列中学习的机器学习技术。
- en: Large Language Models
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型
- en: 'It’s hard these days to go anywhere without bumping into *large language models*
    (LLMs). Many recent breakthrough models in AI—such as ChatGPT, Gemini, Claude,
    and Llama—fall under this category. While these models involve immense engineering,
    the fundamental idea behind them is surprisingly simple: they’re trained to predict
    the next token (e.g., a word or character) given the preceding context. There
    are slight variations—such as masked language models, which hide random tokens
    during training to encourage contextual reasoning—but the core principle remains
    the same.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这些天，无论走到哪里都难以避开*大型语言模型*（LLMs）。许多最近在人工智能领域的突破性模型，如ChatGPT、Gemini、Claude和Llama，都属于这一类别。虽然这些模型涉及大量的工程，但它们背后的基本思想却出奇地简单：它们被训练来根据先前的上下文预测下一个标记（例如，一个单词或字符）。虽然有一些细微的差别——例如，掩码语言模型在训练过程中隐藏随机标记以鼓励上下文推理——但核心原则保持不变。
- en: One of the most surprising discoveries in modern AI has been that if you train
    a large enough model (in terms of the number of parameters) on enough data (in
    terms of total tokens), remarkable capabilities emerge without explicit supervision.
    These models can suddenly summarize text, translate between languages, and even
    generate creative writing like poems and stories—despite never being trained directly
    to do so.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现代人工智能中最令人惊讶的发现之一是，如果你在足够多的数据（以总标记数衡量）上训练足够大的模型（以参数数量衡量），那么在无需明确监督的情况下，会突然出现非凡的能力。这些模型可以突然总结文本，在不同语言之间进行翻译，甚至生成像诗歌和故事这样的创造性写作——尽管它们从未被直接训练来做这些事情。
- en: 'This holds promise for biology. In many ways, biology is language-like: DNA
    and proteins are sequences built from discrete alphabets, with complex patterns
    and context-dependent “grammar.” By training language models on massive corpora
    of biological sequences—using the same next-token prediction objective—we should
    be able to learn rich representations of biological information.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这对生物学来说很有希望。在许多方面，生物学就像是一种语言：DNA和蛋白质是由离散的字母表构建的序列，具有复杂的模式和上下文相关的“语法”。通过在大量的生物序列语料库上训练语言模型——使用相同的下一个标记预测目标——我们应该能够学习到丰富的生物信息表示。
- en: These learned representations can then be used for a wide range of downstream
    tasks, such as predicting a protein’s function, inferring the effects of mutations,
    or identifying structural properties—all without needing to retrain a new model
    from scratch.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些学习到的表示可以用于广泛的下游任务，例如预测蛋白质的功能、推断突变的影响或识别结构特性——而无需从头开始重新训练一个新模型。
- en: 'Later in this chapter, we will explore one of the most successful protein language
    models to date: ESM2.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我们将探讨迄今为止最成功的蛋白质语言模型之一：ESM2。
- en: Embeddings
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入
- en: One of the most powerful and versatile outputs of language models is their ability
    to generate *embeddings*. An embedding is a numerical vector—a list of floating-point
    numbers—that encodes the meaning or structure of an entity like a word, sentence,
    or protein sequence. For example, a protein might be represented by an embedding
    such as `[0.1, -0.3, 1.3, 0.9, 0.2]`, which could capture aspects of its biochemical
    or structural properties in a compact numerical form.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型最强大和最通用的输出之一是它们生成*嵌入*的能力。嵌入是一个数值向量——一系列浮点数，它编码了一个实体（如单词、句子或蛋白质序列）的意义或结构。例如，一个蛋白质可能由一个嵌入表示，如`[0.1,
    -0.3, 1.3, 0.9, 0.2]`，这可以以紧凑的数值形式捕捉其生化或结构特性的某些方面。
- en: Embeddings from language models are not just arbitrary numbers—they are structured
    so that similar inputs result in similar embeddings. Related words like `lion`,
    `tiger`, and `panther` cluster together in a linguistic “semantic space.” Likewise,
    protein sequences with similar structure or function—such as collagen I and collagen
    II—will tend to have embeddings that are close together in what we might call
    a “protein space.”
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型的嵌入不仅仅是任意数字——它们是有结构的，使得相似的输入产生相似的嵌入。像“狮子”、“老虎”和“豹”这样的相关词汇在语言“语义空间”中聚集在一起。同样，具有相似结构或功能的蛋白质序列——例如胶原蛋白I和胶原蛋白II——往往会在我们可能称之为“蛋白质空间”的地方具有接近的嵌入。
- en: This idea generalizes to the concept of a *latent space*—a continuous, abstract
    space where similar entities are positioned close together based on learned patterns.
    In such spaces, we can perform powerful operations, such as interpolation, clustering,
    and generative design. For proteins, latent spaces can capture functional relationships
    that aren’t apparent from sequence alone—for example, two proteins with very different
    sequences and evolutionary histories may have converged on similar functions and
    therefore appear close together in the latent space. These representations can
    also help predict new functions for uncharacterized proteins by comparing them
    to annotated neighbors in the space.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法推广到*潜在空间*的概念——一个连续的、抽象的空间，其中根据学习到的模式，相似的实体被放置在一起。在这样的空间中，我们可以执行强大的操作，如插值、聚类和生成设计。对于蛋白质，潜在空间可以捕捉仅从序列中不明显的功能关系——例如，两个序列和进化历史非常不同的蛋白质可能已经收敛到相似的功能，因此在潜在空间中看起来很接近。这些表示还可以通过将它们与空间中注释的邻近蛋白质进行比较，帮助预测未表征蛋白质的新功能。
- en: Note
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: To identify proteins with similar structure or function, you can compare their
    embeddings using *cosine similarity*—a measure of how aligned two vectors are,
    regardless of their magnitude. This works even when sequences differ significantly
    at the amino acid level. By computing cosine similarities between a query protein
    and a set of known proteins, you can rank the closest matches in embedding space.
    These top hits often share functional roles, structural features, or evolutionary
    history.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要识别具有相似结构或功能的蛋白质，你可以使用*余弦相似度*来比较它们的嵌入——这是一个衡量两个向量如何对齐的度量，无论它们的幅度如何。即使序列在氨基酸水平上有显著差异，这也适用。通过计算查询蛋白质和一组已知蛋白质之间的余弦相似度，你可以在嵌入空间中对最接近的匹配进行排序。这些顶级命中通常具有功能角色、结构特征或进化历史。
- en: Pretraining and Fine-tuning
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练和微调
- en: Many machine learning tasks share underlying structure. Whether your goal is
    detecting hate speech, answering law school entrance questions, or writing poems
    about capybaras, your model first needs a strong foundation in how language works.
    Rather than training from scratch for every task, we typically start from a general-purpose
    model that’s been pretrained on a huge, diverse dataset.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习任务具有共同的底层结构。无论你的目标是检测仇恨言论、回答法学院入学问题，还是写关于水豚的诗，你的模型首先需要在语言工作原理方面有一个坚实的基础。我们通常不是为每个任务从头开始训练，而是从一个在大量、多样化的数据集上预训练的通用模型开始。
- en: '*Pretraining* gives a model broad knowledge and general capabilities. For a
    specific application, we often follow it with a smaller, focused training step
    called *fine-tuning*, where the model is trained further on a domain-specific
    dataset. This two-stage process is now standard in many areas of machine learning,
    especially as pretrained language models have become increasingly powerful.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*预训练*给模型提供了广泛的知识和一般能力。对于特定的应用，我们通常随后进行一个较小的、专注的训练步骤，称为*微调*，其中模型在特定领域的数据集上进一步训练。这个两阶段过程现在在许多机器学习领域都是标准的，尤其是在预训练语言模型变得越来越强大的情况下。'
- en: 'In this first technical chapter of the book, we’ll take a slightly different
    approach. Rather than fine-tuning the entire pretrained model, we’ll treat it
    as a frozen feature extractor: we’ll use its embeddings as input to a smaller
    classifier that we’ll train from scratch. This strategy is efficient, requires
    little data, and still leverages the rich representations learned by the pretrained
    model. We’ll explore full transfer learning with fine-tuning in later chapters.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第一章技术章节中，我们将采取一种略有不同的方法。我们不会微调整个预训练模型，而是将其视为一个冻结的特征提取器：我们将使用其嵌入作为输入，到一个我们从零开始训练的小型分类器。这种策略效率高，所需数据少，并且仍然利用了预训练模型学习到的丰富表示。我们将在后面的章节中探讨带有微调的全迁移学习。
- en: Representations of Proteins and Protein LMs
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蛋白质和蛋白质LM的表示
- en: 'Previously, we discussed what proteins are and how their structure is organized
    hierarchically—from a linear chain of amino acids, to local folding, to the final
    3D form that enables their function. To make this less abstract, let’s load up
    and visualize an example protein structure using the `py3Dmol` library:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们讨论了蛋白质是什么以及它们的结构是如何分层组织的——从氨基酸的线性链到局部折叠，再到最终的功能性3D形态。为了使这一点不那么抽象，让我们使用`py3Dmol`库加载并可视化一个蛋白质结构示例：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Running this code in our companion Colab notebook will display an interactive
    3D rendering of your chosen protein. A screenshot of the visualization of collagen
    is shown in [Figure 2-3](#collagen-structure).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的配套Colab笔记本中运行此代码将显示您选择的蛋白质的交互式3D渲染。图2-3展示了胶原蛋白的可视化截图。[图2-3](#collagen-structure)。
- en: '![](assets/dlfb_0203.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0203.png)'
- en: Figure 2-3\. A 3D structure of the collagen protein rendered with `py3Dmol`.
    Collagen is a structural protein that forms triple-helical fibers, visible here
    as intertwined ribbonlike strands.
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3. 使用`py3Dmol`渲染的胶原蛋白的3D结构。胶原蛋白是一种结构蛋白，形成三螺旋纤维，在此处可见为交织的带状结构。
- en: Try viewing the other examples, such as `insulin` and `proteasome`, to appreciate
    the incredible structural diversity of proteins. Their shapes often reflect their
    specialized roles. For example, the long, springy structure of collagen relates
    to its function as a flexible, supportive scaffold found throughout many tissues
    in the body.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试查看其他示例，例如`insulin`和`proteasome`，以欣赏蛋白质的惊人结构多样性。它们的形状通常反映了它们的专门角色。例如，胶原蛋白的长弹簧状结构与其作为灵活、支持性支架的功能有关，这种支架遍布身体许多组织。
- en: Numerical Representation of a Protein
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蛋白质的数值表示
- en: While 3D visualizations are useful for exploration, machine learning models
    require numerical input. To analyze or model proteins with machine learning techniques,
    we typically start from their 1D amino acid sequence.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然3D可视化对于探索很有用，但机器学习模型需要数值输入。要使用机器学习技术分析或建模蛋白质，我们通常从它们的1D氨基酸序列开始。
- en: 'Protein sequences for most known organisms can be retrieved from public databases
    such as [Uniprot](https://oreil.ly/9OqAK). For example, here’s the amino acid
    sequence of human insulin:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数已知生物的蛋白质序列可以从公共数据库中检索，例如[Uniprot](https://oreil.ly/9OqAK)。例如，以下是人体胰岛素的氨基酸序列：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Output:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE2]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This sequence representation is easy to store and manipulate, but it still needs
    to be converted to a numerical format before it can be used by machine learning
    models.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这种序列表示法易于存储和处理，但在使用机器学习模型之前，仍需将其转换为数值格式。
- en: One-Hot Encoding of a Protein Sequence
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蛋白质序列的一热编码
- en: 'The simplest way to convert a protein sequence into numerical form is with
    *one-hot encoding*. Here is how it works:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 将蛋白质序列转换为数值形式的最简单方法是使用*一热编码*。以下是它是如何工作的：
- en: There are 20 standard amino acids.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有20种标准氨基酸。
- en: Each amino acid is represented by a binary vector of length 20, where only one
    position is 1 (indicating the identity of that amino acid) and all other positions
    are `0`.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每种氨基酸都由一个长度为20的二进制向量表示，其中只有一个位置是1（表示该氨基酸的身份），其他所有位置都是`0`。
- en: A protein sequence is then converted into a sequence of these one-hot vectors—one
    for each amino acid.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，将蛋白质序列转换为这些一热向量的序列——每个氨基酸一个。
- en: 'Let’s walk through a toy example: encoding the short protein `MALWN` (the first
    five amino acids of the insulin precursor protein).'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个玩具示例来了解：编码短的蛋白质`MALWN`（胰岛素前体蛋白的前五个氨基酸）。
- en: 'First, let’s define the mapping between an amino acid letter code to an integer
    index:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义氨基酸字母代码到整数索引之间的映射：
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Output:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Given a protein sequence, we can convert it to a sequence of integers:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个蛋白质序列，我们可以将其转换为整数序列：
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Output:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: And given a sequence of integers, we can convert it into a one-hot encoding
    (see [Figure 2-4](#protein-one-hot-encoding)).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个整数序列，我们可以将其转换为独热编码（见[图2-4](#protein-one-hot-encoding)）。
- en: '![](assets/dlfb_0204.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0204.png)'
- en: Figure 2-4\. One-hot encoding converts a protein’s amino acid sequence into
    a binary matrix where each row corresponds to one amino acid and each column to
    a possible residue. Most values are zero, with a single “1” indicating the presence
    of a specific amino acid at each position.
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4。独热编码将蛋白质的氨基酸序列转换为二进制矩阵，其中每一行对应一个氨基酸，每一列对应一个可能的残基。大多数值都是零，只有一个“1”表示在每个位置上存在特定的氨基酸。
- en: 'In [Figure 2-4](#protein-one-hot-encoding), we see that:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2-4](#protein-one-hot-encoding)中，我们看到：
- en: The resulting matrix has the shape `[5, 20]`, where each of the five rows corresponds
    to one amino acid in the sequence, and each column represents one of the 20 standard
    amino acids.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果矩阵的形状为 `[5, 20]`，其中五行中的每一行对应序列中的一个氨基酸，每一列代表20种标准氨基酸之一。
- en: Each row contains all zeros except for a single 1 in the position corresponding
    to that amino acid’s identity, preserving its categorical nature without implying
    any numerical ordering or similarity.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每一行除了对应氨基酸身份的位置有一个单独的1之外，其余都是零，这保留了其分类性质，而没有暗示任何数值排序或相似性。
- en: Note
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Why not just skip the one-hot encoding step and use amino acid indices directly?
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不直接跳过独热编码步骤，而使用氨基酸索引呢？
- en: The issue is that numeric indices (like 3 versus 17) imply an artificial order
    and relative similarity, even though amino acids are categorical entities without
    meaningful numerical relationships.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于，数值索引（如3与17）暗示了一种人为的顺序和相对相似性，尽管氨基酸是分类实体，没有有意义的数值关系。
- en: One-hot encoding avoids this by assigning each amino acid a distinct binary
    vector—ensuring that the model treats them as equally separate and avoids inferring
    nonexistent patterns from arbitrary index values.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 独热编码通过为每个氨基酸分配一个独特的二进制向量来避免这一点——确保模型将它们视为完全分开的，并避免从任意索引值中推断不存在的模式。
- en: 'In code, we can use the handy `jax.nn.one_hot` utility from the JAX library
    to get this embedding:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码中，我们可以使用JAX库中的便捷的`jax.nn.one_hot`实用工具来获取这个嵌入：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can visualize the resulting one-hot encoding matrix as a heatmap as in [Figure 2-5](#one-hot-matrix-visualized)
    (essentially re-creating the earlier [Figure 2-4](#protein-one-hot-encoding)):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将生成的独热编码矩阵可视化为热图，如[图2-5](#one-hot-matrix-visualized)所示（本质上是在重新创建早期的[图2-4](#protein-one-hot-encoding)）：
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '![](assets/dlfb_0205.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0205.png)'
- en: Figure 2-5\. One-hot encoded representation of a toy protein sequence (`MALWM`),
    visualized with a heatmap. This binary matrix encodes the identity of each residue
    without implying any similarity between them.
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5。使用热图可视化的玩具蛋白质序列（`MALWM`）的独热编码表示。这个二进制矩阵编码了每个残基的身份，而没有暗示它们之间的任何相似性。
- en: Now that we’ve constructed a basic numerical representation of a protein, we’re
    ready to move beyond this simplistic format and explore *learned embeddings*—dense
    vector representations that encode much more biological meaning about each amino
    acid.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了一个蛋白质的基本数值表示，我们准备超越这种简单格式，探索*学习嵌入*——密集向量表示，它编码了关于每个氨基酸的更多生物学意义。
- en: Learned Embeddings of Amino Acids
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 氨基酸的嵌入学习
- en: In the rest of this chapter, we’ll use a pretrained protein language model called
    [ESM2](https://oreil.ly/iZmXA), released by Meta in 2023 (ESM stands for *evolutionary
    scale modeling*). These models are hosted on the [Hugging Face platform](https://huggingface.co).
    If you haven’t encountered it yet, Hugging Face is a fantastic resource with [thousands
    of pretrained models](https://huggingface.co/models) ready for you to use and
    explore.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将使用Meta在2023年发布的预训练蛋白质语言模型[ESM2](https://oreil.ly/iZmXA)（ESM代表*进化尺度建模*）。这些模型托管在[Hugging
    Face平台](https://huggingface.co)。如果你还没有遇到它，Hugging Face是一个极好的资源，拥有[数千个预训练模型](https://huggingface.co/models)供你使用和探索。
- en: We’ll explore how the ESM2 model works in more detail shortly, but first, let’s
    examine how it represents individual amino acids. We’ll access the model using
    the Hugging Face transformers library. ESM2 is based on the *transformer* neural
    network architecture [introduced in 2017](https://oreil.ly/XPvFW), which has become
    the standard for modeling sequences like text and proteins.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Ideally, we’d load the ESM2 model using JAX/Flax, but it’s only officially available
    in PyTorch at the moment. In practice, being comfortable with multiple deep learning
    frameworks is useful—so here we’ll use PyTorch to load the model and extract embeddings,
    which we’ll then process and build on top of using JAX.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the book will use JAX/Flax exclusively, but this brief mixing of
    frameworks is a good example of how flexible real-world workflows can be.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can check the model’s token-to-index mapping:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Output:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This is similar to the manual amino acid indexing we did earlier, but it includes
    special tokens like `<unk>` for unknown residues, `<eos>` for end-of-sequence,
    and rare amino acids like `U` (selenocysteine) and `O` (pyrrolysine).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the ESM2 tokenizer to encode our tiny protein sequence:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Output:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If desired, we can drop the special start (`<cls>`) and end (`<eos>`) tokens:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Output:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now we’ll extract the learned token embeddings from the model using `model.get_input_embeddings()`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Output:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Each of the 33 possible tokens is embedded into a 1,280-dimensional space.
    While humans can’t visualize such high-dimensional spaces directly, we can apply
    dimensionality reduction techniques like t-SNE or UMAP to project the embeddings
    down to two dimensions. This allows us to inspect how the model organizes different
    tokens in a more interpretable form:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Output:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can see that the t-SNE–transformed array has shape `(33, 2)`, meaning that
    each of the 33 tokens has been projected into a 2D space. [Figure 2-6](#tsne-no-chemical-properties)
    shows a scatterplot of these points, giving us a visual sense of how the model
    organizes token embeddings:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](assets/dlfb_0206.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. A 2D t-SNE projection of the learned token embeddings from the
    ESM2 model. Even without labels, clusters begin to emerge—hinting that the model
    has learned to organize tokens in a meaningful way.
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To sanity-check whether similar types of tokens cluster in the 2D embedding
    space, we can label each token using known amino acid properties (like those shown
    earlier in the chapter) and replot the t-SNE projection in [Figure 2-7](#tsne-with-chemical-properties):'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](assets/dlfb_0207.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: Figure 2-7\. Coloring the t-SNE projection by amino acid properties reveals
    clear clusters of amino acids with similar biochemical roles that tend to group
    together in embedding space, reflecting the model’s ability to capture meaningful
    biological structure. Technical non–amino acid tokens also group together in this
    latent space.
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tokens with similar biochemical properties tend to cluster together. For instance,
    hydrophobic amino acids like `F`, `Y`, and `W` group in the upper right, while
    special-purpose tokens such as `<cls>` and `<eos>` appear together on the left
    side of the plot. This structure suggests that the model has learned meaningful
    distinctions among amino acids based on the roles they play within protein sequences.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 具有相似生化性质的标记词往往会聚集在一起。例如，疏水性氨基酸如`F`、`Y`和`W`聚集在右上角，而特殊用途的标记词如`<cls>`和`<eos>`则出现在图的左侧。这种结构表明，模型已经根据氨基酸在蛋白质序列中扮演的角色，学会了氨基酸之间有意义的区分。
- en: Now that we’ve explored what these token embeddings look like, let’s dive into
    how the ESM2 model actually works—and how it learns such representations in the
    first place.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探索了这些标记嵌入看起来像什么，让我们深入了解ESM2模型实际上是如何工作的——以及它是如何首先学习这些表示的。
- en: The ESM2 Protein Language Model
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ESM2蛋白质语言模型
- en: 'Now that you’re more familiar with token embeddings, let’s discuss how the
    ESM2 model actually works. ESM2 is a *masked language model* (MLM), which means
    it was trained by repeatedly masking a random subset of amino acids in each protein
    sequence and asking the model to predict them. In the case of ESM2, a randomly
    selected 15% of the amino acids in each sequence were masked during training.
    [Figure 2-8](#language-model-training) illustrates this visually, comparing it
    to masked language modeling in natural language tasks:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对标记嵌入更熟悉了，让我们讨论ESM2模型实际上是如何工作的。ESM2是一个*掩码语言模型*（MLM），这意味着它是通过重复在每个蛋白质序列中随机掩码氨基酸的子集来训练的，并要求模型预测它们。在ESM2的情况下，每个序列中随机选择的15%的氨基酸在训练期间被掩码。[图2-8](#language-model-training)直观地说明了这一点，并将其与自然语言任务中的掩码语言建模进行了比较：
- en: '![](assets/dlfb_0208.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0208.png)'
- en: 'Figure 2-8\. A comparison between masked language modeling in natural and protein
    language models. In natural language, models are trained to predict missing words
    (or sometimes subwords) from surrounding context. Protein language models use
    the same principle: randomly masking amino acids in a sequence and training the
    model to predict them from the surrounding context.'
  id: totrans-159
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-8。自然语言和蛋白质语言模型中掩码语言建模的比较。在自然语言中，模型被训练来预测缺失的单词（或有时是子词）从周围上下文中。蛋白质语言模型使用相同的原理：随机掩码序列中的氨基酸，并训练模型从周围上下文中预测它们。
- en: 'Let’s try masking one amino acid in the insulin protein sequence and see whether
    the model can predict it:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试在胰岛素蛋白质序列中掩码一个氨基酸，看看模型是否能预测它：
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `<mask>` token tells the model to predict the amino acid at that position.
    To do this, we load the full language model, `EsmForMaskedLM`, which includes
    the language prediction head.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`<mask>`标记词告诉模型预测该位置的氨基酸。为此，我们加载了完整的语言模型`EsmForMaskedLM`，它包括语言预测头。'
- en: Note
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: To accelerate inference, we’ll use a smaller ESM2 model variant (150M parameters
    with 640-dimensional embeddings) rather than the large, 650M model with 1,280-dimensional
    embeddings used earlier. This is a good reminder that many models on Hugging Face
    come in different sizes, and swapping between them is often as simple as changing
    a model checkpoint.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加速推理，我们将使用较小的ESM2模型变体（150M参数，640维嵌入）而不是之前使用的较大、650M参数、1,280维嵌入的模型。这是一个很好的提醒，许多Hugging
    Face上的模型有不同的尺寸，并且在这些模型之间切换通常只需更改模型检查点。
- en: Of course, there’s a trade-off—smaller models may capture less information and
    typically perform worse on complex tasks. Still, they’re great for rapid prototyping
    and exploring model behavior.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这里有一个权衡——较小的模型可能捕捉到的信息较少，通常在复杂任务上的表现较差。然而，它们非常适合快速原型设计和探索模型行为。
- en: 'We load up the model:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载了模型：
- en: '[PRE24]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: And we’ll run it to get predictions for the masked token. We see that the model
    correctly predicts the token `L` (leucine) with very high probability in [Figure 2-9](#predict-missing-aa).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行它以获取对掩码标记词的预测。我们看到模型以非常高的概率正确预测了标记词`L`（亮氨酸），如[图2-9](#predict-missing-aa)所示。
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](assets/dlfb_0209.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0209.png)'
- en: Figure 2-9\. Model prediction for a masked leucine (`L`) in the insulin sequence.
    The model confidently predicts the correct amino acid (`L`) with high probability,
    showing that it has learned common sequence patterns in proteins.
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-9。胰岛素序列中掩码亮氨酸（`L`）的模型预测。模型自信地以高概率预测了正确的氨基酸（`L`），这表明它已经学会了蛋白质中的常见序列模式。
- en: 'Let’s rewrite this code as a more general form as `MaskPredictor`, with methods
    that mask a sequence, make a prediction, and plot the predictions:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这段代码重写为一个更通用的形式，称为`MaskPredictor`，其中包含掩码序列、进行预测和绘制预测的方法：
- en: '[PRE26]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Let’s try it on a different position—index 26, where the correct amino acid
    is `N` (asparagine). The result is shown in [Figure 2-10](#predict-missing-aa-less-clear):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在不同的位置尝试一下——索引26，正确的氨基酸是`N`（天冬酰胺）。结果如图[图2-10](#predict-missing-aa-less-clear)所示：
- en: '[PRE27]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](assets/dlfb_0210.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0210.png)'
- en: Figure 2-10\. Model prediction for a masked asparagine (`N`) in the insulin
    sequence. Here, the model is more uncertain—it assigns moderate probability to
    several possible amino acids, indicating that this position is harder to predict
    based on surrounding context.
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-10\. 对胰岛素序列中掩码天冬酰胺（`N`）的模型预测。在这里，模型更加不确定——它对几个可能的氨基酸分配了适中的概率，这表明基于周围上下文，这个位置更难以预测。
- en: In this case, the model doesn’t strongly prefer any one amino acid. It assigns
    moderate probability to several, including `A`, `T`, and `S` (with the model assigning
    the true amino acid `N` a fairly low probability). This uncertainty could reflect
    the biochemical flexibility of that position—some regions of proteins can tolerate
    different residues due to redundancy, structural flexibility, or lack of strict
    functional constraints. These are often called “permissive” positions and are
    common in disordered (unstructured) or surface regions of proteins.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，模型并不强烈偏好任何一种氨基酸。它对包括`A`、`T`和`S`在内的几个氨基酸分配了适中的概率（模型将真正的氨基酸`N`分配了相当低的概率）。这种不确定性可能反映了该位置的生化灵活性——由于冗余、结构灵活性或缺乏严格的功能约束，蛋白质的一些区域可以容忍不同的残基。这些通常被称为“宽容”位置，在蛋白质的无序（非结构化）或表面区域中很常见。
- en: 'This example illustrates that the model has learned and understands the probabilistic
    grammar of proteins. The next question is: how can we leverage this understanding
    to represent an entire protein, and not just one amino acid at a time?'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子说明模型已经学习和理解了蛋白质的概率语法。下一个问题是：我们如何利用这种理解来表示整个蛋白质，而不仅仅是单个氨基酸？
- en: Strategies for Extracting an Embedding for an Entire Protein
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取整个蛋白质嵌入的策略
- en: So far, we’ve explored how the ESM2 model represents individual amino acids.
    But many downstream tasks—like predicting protein function—require a fixed-length
    representation for the entire protein sequence. How can we convert a variable-length
    sequence of amino acids into a single embedding vector that captures the protein’s
    overall structure and meaning?
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了ESM2模型如何表示单个氨基酸。但许多下游任务——如预测蛋白质功能——需要整个蛋白质序列的固定长度表示。我们如何将氨基酸的可变长度序列转换成一个单一的嵌入向量，该向量能够捕捉蛋白质的整体结构和意义？
- en: 'Several strategies are commonly used:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的几种策略是：
- en: Concatenation of amino acid embeddings
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 氨基酸嵌入的连接
- en: 'One simple approach is to loop through each amino acid in a sequence, extract
    its embedding, and concatenate them into one long vector. For example, if a protein
    has length 10 and each amino acid has a 640-dimensional embedding, this yields
    a protein embedding of length `10 × 640 = 6400`. While this preserves fine-grained
    information of each amino acid, it has several drawbacks:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是遍历序列中的每个氨基酸，提取其嵌入，并将它们连接成一个长向量。例如，如果一个蛋白质长度为10，每个氨基酸有一个640维的嵌入，这将产生一个长度为`10
    × 640 = 6400`的蛋白质嵌入。虽然这种方法保留了每个氨基酸的细粒度信息，但它有几个缺点：
- en: Variable length
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 可变长度
- en: Different proteins will yield different-length embeddings, which complicates
    model input formatting.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的蛋白质会产生不同长度的嵌入，这使模型输入格式化变得复杂。
- en: Scalability
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性
- en: Long proteins produce huge embeddings. For example, titin—the longest known
    human protein at ~34,000 amino acids—would produce an embedding with over *43
    million* values. That’s unwieldy for most models.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 长蛋白质会产生巨大的嵌入。例如，肌球蛋白——已知最长的已知人类蛋白质，约34,000个氨基酸——会产生一个包含超过*4300万*个值的嵌入。这对大多数模型来说难以处理。
- en: Limited modeling
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 有限的建模
- en: This approach treats amino acids independently, ignoring the contextual relationships
    that are central to protein function.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法独立地处理氨基酸，忽略了蛋白质功能中至关重要的上下文关系。
- en: Averaging of amino acid embeddings
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 氨基酸嵌入的平均值
- en: A more compact approach is to average the token embeddings across the sequence.
    Using the same example of a length-10 protein with 640-dim embeddings, we take
    the mean across all 10 embeddings to produce a final 640-dimensional vector.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更紧凑的方法是在序列中平均标记嵌入。使用相同的长度为10的蛋白质和640维嵌入的例子，我们取所有10个嵌入的平均值，以产生一个最终的640维向量。
- en: This has the advantage of producing fixed-size vectors, regardless of protein
    length.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这具有产生固定大小向量的优势，无论蛋白质长度如何。
- en: 'It’s efficient and sometimes used, but also crude—averaging discards ordering
    and interaction information. It’s like summarizing a novel by averaging all its
    word vectors: some meaning survives, but the nuance is lost.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它既高效又有时被使用，但也很粗糙——平均会丢弃顺序和交互信息。这就像通过平均所有单词向量来总结一部小说：一些意义幸存，但细微差别丢失了。
- en: Using the model’s contextual sequence embeddings
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型的上下文序列嵌入
- en: A more principled approach is to extract the hidden representations for the
    entire sequence directly from the language model. Since ESM2 is trained to predict
    masked tokens based on their surrounding context, its internal layers encode rich,
    contextualized embeddings for every amino acid in the sequence.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更原则的方法是直接从语言模型中提取整个序列的隐藏表示。由于ESM2被训练根据周围上下文预测掩码标记，其内部层为序列中的每个氨基酸编码了丰富的、上下文化的嵌入。
- en: Concretely, we can pass a protein sequence through ESM2 and extract the final
    hidden layer activations, resulting in a tensor of shape (`L', D`), where `L'`
    is the number of output tokens (which may differ from the input length `L`), and
    `D` is the model’s hidden size (e.g., 640).
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具体来说，我们可以将蛋白质序列通过ESM2传递，并提取最终的隐藏层激活，结果是一个形状为(`L', D`)的张量，其中`L'`是输出标记的数量（可能不同于输入长度`L`），`D`是模型的隐藏大小（例如，640）。
- en: We then apply mean pooling across the sequence length to produce a fixed-length
    embedding of shape (`D,`). While averaging may seem simplistic, it often works
    surprisingly well—because the model has already integrated contextual information
    into each token’s representation using self-attention, the pooled vector still
    captures meaningful dependencies across the sequence.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们在序列长度上应用平均池化以产生形状为(`D,`)的固定长度嵌入，其中`D`是嵌入的维度。虽然平均可能看起来很简单，但它通常出奇地好——因为模型已经使用自注意力将上下文信息整合到每个标记的表示中，因此池化向量仍然可以捕捉到序列中的有意义依赖关系。
- en: This final approach is the most common and powerful in practice—and it’s the
    one we’ll explore in the next section.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这种最终的方法在实践中最常见且最强大——我们将在下一节中探讨它。
- en: Extracellular Versus Membrane Protein Embeddings
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 细胞外与膜蛋白嵌入
- en: 'We’ll introduce the GO dataset properly in the next section on protein function
    prediction. For now, let’s use it to associate each UniProt protein accession
    and sequence with its known cellular location:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节关于蛋白质功能预测中正确介绍GO数据集。现在，让我们用它来关联每个UniProt蛋白质访问号和序列与其已知的细胞位置：
- en: '[PRE28]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Output:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE29]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: For each protein sequence identified by an `EntryID`, the `term` column provides
    its GO annotation for cellular localization.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个由`EntryID`识别的蛋白质序列，`term`列提供了其细胞定位的GO注释。
- en: 'Let’s focus on two specific locations:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们关注两个特定的位置：
- en: '`extracellular` (GO:0005576)'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`细胞外` (GO:0005576)'
- en: Proteins secreted outside the cell, often involved in signaling, immune response,
    or structural roles
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 分泌到细胞外的蛋白质，通常涉及信号传导、免疫反应或结构作用
- en: '`membrane` (GO:0016020)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`膜` (GO:0016020)'
- en: Proteins embedded in or associated with cell membranes, frequently functioning
    in transport, signaling, or cell–cell interaction
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入或与细胞膜相关的蛋白质，通常在运输、信号传导或细胞间相互作用中发挥作用
- en: 'We’ll filter the dataset to proteins annotated with only one of these two locations:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将过滤数据集，只包含以下两种位置之一注释的蛋白质：
- en: '[PRE30]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Output:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE31]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We’ll now extract embeddings from these sequences. The function `get_mean_embeddings`
    computes the mean hidden state across each sequence, summarizing the model’s representation
    of protein sequences:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将从这些序列中提取嵌入。函数`get_mean_embeddings`计算每个序列的平均隐藏状态，总结模型对蛋白质序列的表示：
- en: '[PRE32]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now we’ll extract embeddings using a smaller ESM2 model, which produces 320-dimensional
    representations and requires significantly less memory than larger variants:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用较小的ESM2模型提取嵌入，该模型产生320维度的表示，并且比更大的变体需要显著更少的内存：
- en: '[PRE33]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We then calculate the embeddings:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来计算嵌入：
- en: '[PRE34]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Output:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE35]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Each set of 20 sampled proteins is now represented as a `(20, 320)` embedding
    matrix. This means that for each sequence—regardless of its original length—we
    obtain a fixed-size vector of 320 dimensions. These vectors correspond to the
    mean of the final hidden layer activations across all tokens in the sequence,
    and should capture some information about the overall protein structure.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在每一组20个采样的蛋白质现在表示为一个`(20, 320)`嵌入矩阵。这意味着对于每个序列——无论其原始长度如何——我们都获得一个固定大小的320维向量。这些向量对应于序列中所有标记的最终隐藏层激活的平均值，并且应该包含关于蛋白质整体结构的某些信息。
- en: To visualize how these embeddings might relate to protein localization, we project
    them into two dimensions using t-SNE, a common method for visualizing high-dimensional
    data. [Figure 2-11](#membrane-protein-embeddings) shows that the extracellular
    and membrane proteins tend to form distinct clusters in this space.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化这些嵌入如何与蛋白质定位相关，我们使用t-SNE（一种用于可视化高维数据的常用方法）将它们投影到二维空间。图2-11[图2-11](#membrane-protein-embeddings)显示，细胞外和膜蛋白在这个空间中倾向于形成不同的簇。
- en: '[PRE36]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '![](assets/dlfb_0211.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0211.png)'
- en: Figure 2-11\. Two-dimensional t-SNE projection of the 320-dimensional embeddings
    from a small ESM2 model. Even with this lightweight model, we observe a tendency
    for extracellular and membrane proteins to form separate clusters, suggesting
    that the embeddings contain information relevant to cellular localization.
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-11. 来自小型ESM2模型的320维嵌入的二维t-SNE投影。即使在这个轻量级模型中，我们也观察到细胞外和膜蛋白倾向于形成单独的簇，这表明嵌入包含与细胞定位相关的信息。
- en: 'While the separation isn’t perfect, there’s a clear trend: extracellular proteins
    tend to cluster in a different region of embedding space than membrane proteins.
    It’s quite striking that the model picks up on this purely from sequence. This
    suggests that the learned embeddings reflect biologically meaningful patterns—even
    without any explicit supervision for cellular location.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然分离并不完美，但有一个明显的趋势：细胞外蛋白倾向于在嵌入空间的一个不同区域聚集，与膜蛋白不同。模型仅从序列中捕捉到这一点是非常引人注目的。这表明学习到的嵌入反映了生物上有意义的模式——即使没有对细胞位置的任何明确监督。
- en: 'With this initial exploration complete, we now turn to the central machine
    learning task of this chapter: predicting protein function. Let’s begin by preparing
    the dataset.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成初步探索之后，我们现在转向本章的核心机器学习任务：预测蛋白质功能。让我们首先准备数据集。
- en: Preparing the Data
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: Many machine learning books and blog posts jump straight into the exciting parts—training
    and evaluating models—as soon as possible. But in practice, training is often
    a small fraction of the overall workflow. A significant portion of time is spent
    understanding, cleaning, and structuring the data. And when things go wrong with
    a model, the root cause is often found in the data. So, rather than handing you
    a polished CSV from the ether, we’ll walk through the data preparation process
    step-by-step—starting from real-world resources and working through the steps
    needed to turn them into something a model can use.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习书籍和博客文章都尽可能快地直接跳到令人兴奋的部分——训练和评估模型。但在实践中，训练通常只是整个工作流程的一小部分。大量时间花在理解、清理和结构化数据上。当模型出现问题的时候，根本原因通常在数据中。因此，我们不会从虚无中给你一个完美的CSV文件，而是会一步一步地展示数据准备过程——从现实世界的资源开始，逐步完成将其转化为模型可以使用的过程。
- en: Our goal is to fine-tune a model to predict protein function from sequence,
    which means assembling a dataset of `(protein_sequence, protein_function)` pairs.
    Fortunately, biologists have developed systematic frameworks for defining protein
    functions, and curated datasets already exist. One of the most widely used resources
    is the [CAFA (Critical Assessment of Functional Annotation)](https://oreil.ly/87EN_)
    challenge, a community-driven competition where teams build models to predict
    protein function. We’ll use CAFA data as our raw material, but we’ll still need
    to process and structure it ourselves.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是微调一个模型来从序列中预测蛋白质功能，这意味着组装一个`(蛋白质序列，蛋白质功能)`对的数据库。幸运的是，生物学家已经开发了定义蛋白质功能的系统框架，并且已经存在经过整理的数据集。最广泛使用的资源之一是[CAFA（功能注释关键评估）](https://oreil.ly/87EN_)挑战，这是一个由社区驱动的比赛，团队构建模型来预测蛋白质功能。我们将使用CAFA数据作为我们的原材料，但我们仍然需要自己处理和结构化它。
- en: Note
  id: totrans-233
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you’re familiar with AlphaFold and protein structure prediction, you may
    have heard of the similarly named CASP (Critical Assessment of Structure Prediction),
    which plays a similar role in the protein structure community. Public benchmarks
    like these have been instrumental in driving progress across a wide range of computational
    biology problems.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉AlphaFold和蛋白质结构预测，你可能听说过同名的CASP（结构预测关键评估），它在蛋白质结构社区中扮演着类似的角色。这类公开基准在推动计算生物学广泛问题上的进步中起到了关键作用。
- en: Let’s now explore the CAFA dataset.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索CAFA数据集。
- en: Loading the CAFA3 Data
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载CAFA3数据
- en: 'There have been several rounds of CAFA, but the CAFA3 dataset is the most recent
    publicly available one. We first downloaded the “CAFA3 Targets” and “CAFA3 Training
    Data” files from the [CAFA website](https://oreil.ly/87EN_). Let’s start by loading
    the label file, which tells us the functional annotations for each protein:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: CAFA已经进行了几轮，但CAFA3数据集是最新的公开可用数据集。我们首先从[CAFA网站](https://oreil.ly/87EN_)下载了“CAFA3
    Targets”和“CAFA3 Training Data”文件。让我们先加载标签文件，它告诉我们每个蛋白质的功能注释：
- en: '[PRE37]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Output:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE38]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This dataframe contains three columns:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据框包含三列：
- en: '`EntryID`'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`EntryID`'
- en: The UniProt ID of the protein
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质的UniProt ID
- en: '`term`'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '`term`'
- en: A GO accession code describing a specific protein function
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 描述特定蛋白质功能的GO访问号
- en: '`aspect`'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '`aspect`'
- en: 'The GO category the function belongs to; one of three types of function described
    in the introduction: biological process (BPO), molecular function (MFO), and cellular
    component (CCO)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 函数所属的GO类别；介绍中描述的三种功能类型之一：生物过程（BPO）、分子功能（MFO）和细胞组分（CCO）
- en: 'The `term` column contains only GO accession codes. To make these more interpretable,
    we’d ideally like to know their corresponding human-readable descriptions. This
    information isn’t included directly in the CAFA files, but it is available via
    the [Gene Ontology downloads page](https://oreil.ly/uNhm2). The ontology is stored
    in graph format as a `.obo` file, and we can use the `obonet` Python library to
    parse it. Here’s how we retrieve the term descriptions:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`term`列只包含GO访问号。为了使这些更易于理解，我们最好知道它们对应的人类可读描述。这些信息没有直接包含在CAFA文件中，但可以通过[基因本体下载页面](https://oreil.ly/uNhm2)获得。本体以`.obo`文件格式存储为图形格式，我们可以使用`obonet`
    Python库来解析它。以下是检索术语描述的方法：'
- en: '[PRE39]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The function will load the annotations from a local file if it already exists,
    or download and cache them if not:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果该功能已存在于本地文件中，则该函数将加载注释，否则将下载并缓存它们：
- en: '[PRE40]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Output:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE41]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can then merge the human-readable term descriptions back onto the labels
    dataframe:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以将可读的术语描述合并回标签数据框：
- en: '[PRE42]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Output:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE43]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In this chapter, we’ll focus specifically on molecular functions (`MFO`)—that
    is, what a protein does at the biochemical level. Later, you may want to extend
    this chapter’s approach to include the other two GO categories.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将特别关注分子功能（`MFO`）——也就是说，蛋白质在生化层面的作用。稍后，你可能希望将本章的方法扩展到包括其他两个GO类别。
- en: 'Let’s take a look at which molecular functions are most commonly annotated
    in the dataset:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在数据集中哪些分子功能被标注得最频繁：
- en: '[PRE44]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Output:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE45]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We can already see that the distribution of function annotations is highly skewed.
    Some terms—like `molecular_function`, `binding`, and `protein binding`—appear
    tens of thousands of times, while others occur only once. Labels like `molecular_function`
    are arguably overly generic and provide little meaningful information, making
    them unhelpful for machine learning. We’ll filter these out in a later step.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经可以看到功能注释的分布非常倾斜。一些术语——如`molecular_function`、`binding`和`protein binding`——出现了数万次，而其他术语只出现一次。像`molecular_function`这样的标签可能过于通用，提供的有意义信息很少，对机器学习没有帮助。我们将在后续步骤中过滤掉这些标签。
- en: Next, let’s load the protein sequences associated with each protein ID. This
    information is stored in the file *train_sequences.fasta*, a standard format for
    representing biological sequences such as proteins and DNA. We can use BioPython’s
    `SeqIO` module to parse the *.fasta* file into a format we can work with.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们加载与每个蛋白质ID相关的蛋白质序列。这些信息存储在文件*train_sequences.fasta*中，这是表示蛋白质和DNA等生物序列的标准格式。我们可以使用BioPython的`SeqIO`模块将*.fasta*文件解析为我们能处理的形式。
- en: Note
  id: totrans-265
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 备注
- en: 'A quick aside: no one starts out knowing what BioPython’s `SeqIO` module is,
    or how *.fasta* files work, or what GO annotations mean—and that’s completely
    normal. Working at the intersection of biology and machine learning means constantly
    encountering new tools and terminology. Frequent looking up of new terms and tools
    is not just OK, it’s expected.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地说：没有人一开始就知道BioPython的`SeqIO`模块是什么，或者*.fasta*文件是如何工作的，或者GO注释意味着什么——这是完全正常的。在生物学和机器学习的交叉领域工作意味着不断遇到新的工具和术语。频繁查阅新术语和工具不仅是可以接受的，而且是预期的。
- en: 'We’ll convert the *.fasta* sequences into a pandas dataframe to make them easier
    to manipulate:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把*.fasta*序列转换为pandas数据框，以便更容易地操作：
- en: '[PRE46]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Output:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE47]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We’ve also computed the length of each sequence, since protein lengths can vary
    widely and this information will be useful later when filtering data.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还计算了每个序列的长度，因为蛋白质长度可能差异很大，这些信息将在稍后过滤数据时很有用。
- en: 'One important detail: the CAFA dataset includes proteins from many different
    organisms. To isolate human proteins, we’ll use the associated taxonomy file provided
    in the download:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的细节：CAFA数据集包括来自许多不同生物体的蛋白质。为了隔离人类蛋白质，我们将使用下载中提供的关联分类文件：
- en: '[PRE48]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Output:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE49]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This file contains a taxonomy ID (`taxonomyID`) for each protein, based on
    NCBI’s organism classification system. We’ll merge this onto our sequence dataframe
    and keep only proteins with `taxonomyID == 9606`, which corresponds to *Homo sapiens*:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件包含每个蛋白质的物种分类ID (`taxonomyID`)，基于NCBI的物种分类系统。我们将将其合并到我们的序列数据框中，并仅保留`taxonomyID
    == 9606`的蛋白质，这对应于*Homo sapiens*：
- en: '[PRE50]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now let’s get an overview of the number of unique proteins and molecular function
    terms in our filtered dataset:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们概述一下我们过滤后的数据集中独特蛋白质和分子功能术语的数量：
- en: '[PRE51]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Output:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE52]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Let’s also take a look at the resulting `sequence_df` after merging in the
    function labels:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也看看合并功能标签后的结果`sequence_df`：
- en: '[PRE53]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Output:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE54]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'From this table, we can already see that many proteins are associated with
    multiple molecular functions. To quantify this, we examine the distribution of
    the number of functions per protein in [Figure 2-12](#functions-per-protein):'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张表中，我们已能看出许多蛋白质与多个分子功能相关。为了量化这一点，我们检查了每个蛋白质的功能数量的分布[图2-12](#functions-per-protein)：
- en: '[PRE55]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![](assets/dlfb_0212.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0212.png)'
- en: Figure 2-12\. Distribution of the number of molecular functions annotated per
    protein. The y-axis is shown on a logarithmic scale to make rare cases more visible.
    While most proteins have fewer than 20 annotated functions, a small number of
    proteins are associated with more than 50 distinct molecular roles.
  id: totrans-289
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-12\. 每个蛋白质注释的分子功能数量的分布。y轴以对数尺度显示，以便使罕见情况更明显。虽然大多数蛋白质的注释功能少于20个，但少数蛋白质与超过50个不同的分子角色相关。
- en: 'This pattern reflects a complex biological reality: while many proteins carry
    out a single, well-defined function, others are involved in a wide variety of
    molecular roles. For example, some proteins act as enzymes, bind to other molecules,
    and participate in multiple pathways. From a machine learning perspective, this
    means our model must be able to assign multiple function labels to a single protein
    and also cope with the fact that some labels are much rarer than others.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模式反映了复杂的生物现实：虽然许多蛋白质执行单一、明确的功能，但其他蛋白质参与广泛的分子角色。例如，一些蛋白质作为酶，结合其他分子，并参与多个途径。从机器学习的角度来看，这意味着我们的模型必须能够为单个蛋白质分配多个功能标签，并且还要应对某些标签比其他标签更罕见的事实。
- en: 'Let’s now take a closer look at the most frequent molecular function labels.
    Some terms are so broad and universally assigned that they offer little meaningful
    insight. For example, `molecular function` applies to nearly all proteins, `binding`
    covers 93%, and `protein binding` appears in 89% of cases. These labels will tend
    to dominate the loss during training and can cause the model to fixate on predicting
    them at the expense of more meaningful functions. As a dataset preprocessing step,
    we’ll explicitly remove these overly generic terms:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来仔细看看最频繁的分子功能标签。一些术语非常宽泛且普遍分配，因此它们提供的意义不大。例如，`分子功能`适用于几乎所有蛋白质，`结合`覆盖了93%，而`蛋白质结合`出现在89%的情况下。这些标签往往会主导训练过程中的损失，并可能导致模型专注于预测这些标签，而牺牲更有意义的函数。作为数据集预处理步骤，我们将明确删除这些过于通用的术语：
- en: '[PRE56]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Output:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE57]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'On the opposite end of the spectrum, some molecular functions are extremely
    rare—for example, `GO:0099609` (microtubule lateral binding) appears only once.
    To learn meaningful associations, our model needs enough training examples per
    function. So we’ll filter out the rarest labels and keep only those that appear
    in at least 50 proteins:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在光谱的另一端，一些分子功能非常罕见——例如，`GO:0099609`（微管侧向结合）只出现一次。为了学习有意义的关联，我们的模型需要每个功能足够的训练示例。因此，我们将过滤掉最罕见的标签，只保留那些至少出现在50个蛋白质中的标签：
- en: '[PRE58]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Output:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE59]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: This gives us a cleaner set of function labels that are more amenable to learning.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一个更干净的功能标签集，更适合学习。
- en: Note
  id: totrans-300
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Thresholds used during data processing—like how many times a label must appear
    to be included—are somewhat arbitrary, but they can significantly affect model
    performance. These decisions are effectively hyperparameters and should be tuned
    based on the specific task, dataset size, and model capacity.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据处理过程中使用的阈值——例如，一个标签必须出现多少次才能被包含——是相当任意的，但它们可以显著影响模型性能。这些决策实际上是超参数，应根据具体任务、数据集大小和模型容量进行调整。
- en: 'Now we’ll reshape the dataframe so that each row corresponds to one protein,
    and each column corresponds to a molecular function label. We’ll use the `pivot`
    function in pandas to create this multilabel format:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将重新塑形数据框，使得每一行对应一个蛋白质，每一列对应一个分子功能标签。我们将使用pandas中的`pivot`函数创建这种多标签格式：
- en: '[PRE60]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Output:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE61]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Great—this dataset is now in a format that’s almost ready for machine learning.
    Before we move on, let’s run a few final sanity checks.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了——这个数据集现在几乎已经准备好用于机器学习了。在我们继续之前，让我们运行一些最后的合理性检查。
- en: First, how many unique proteins do we have?
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有多少种独特的蛋白质？
- en: '[PRE62]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Output:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE63]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: This number is in the right ballpark. There are roughly 21,000 protein-coding
    genes in the human genome, and since we applied several filtering steps, we expect
    a somewhat smaller number. It’s always worth keeping rough order-of-magnitude
    expectations in mind—if we saw 1,000 or 1,000,000 here, we’d suspect something
    was off.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数字在合理的范围内。人类基因组中大约有21,000个蛋白质编码基因，由于我们应用了几个过滤步骤，我们预计这个数字会小一些。始终值得记住粗略的数量级预期——如果我们在这里看到1,000或1,000,000，我们会怀疑有什么问题。
- en: 'Next, let’s check whether any protein sequences are duplicated:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们检查是否有任何蛋白质序列是重复的：
- en: '[PRE64]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Output:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE65]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'It seems that a few protein sequences are repeated. For example, the entries
    `P0DP23`, `P0DP24`, and `P0DP25` all share the same sequence:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来有一些蛋白质序列是重复的。例如，条目`P0DP23`、`P0DP24`和`P0DP25`都共享相同的序列：
- en: '[PRE66]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Output:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE67]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: These seem to be legitimate biological duplicates—proteins with different Uniprot
    identifiers but identical sequences—so we’ll keep them in the dataset.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 这些似乎是合法的生物重复——具有不同Uniprot标识符但序列相同的蛋白质——因此我们将它们保留在数据集中。
- en: At this point, we have a final dataset linking 10,709 human proteins to one
    or more of 303 molecular functions.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们有一个最终数据集，将10,709个人类蛋白质与303个分子功能中的一个或多个联系起来。
- en: 'Since our simple mean embedding approach can be quite memory intensive, we’ll
    filter the dataset to include only proteins with a maximum length of 500 amino
    acids. This helps avoid out-of-memory errors during model inference and training:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的简单平均嵌入方法可能非常占用内存，我们将过滤数据集，只包含最大长度为500个氨基酸的蛋白质。这有助于避免在模型推理和训练过程中出现内存不足错误：
- en: '[PRE68]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Output:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE69]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: This roughly halves the dataset, which is perfectly fine for initial prototyping.
    You can always remove this constraint later if time and memory allow.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这大约将数据集减半，这对于初始原型设计来说完全没问题。如果时间和内存允许，你总是可以稍后移除这个限制。
- en: Now that we have a clean and compact dataset, let’s process it further for compatibility
    with machine learning.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个干净且紧凑的数据集，让我们进一步处理它以适应机器学习。
- en: Splitting the Dataset into Subsets
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据集分割成子集
- en: 'We will split our dataset into three distinct subsets:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把数据集分成三个不同的子集：
- en: Training set
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集
- en: Used to fit the model. The model sees this data during training and uses it
    to learn patterns.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 用于拟合模型。模型在训练期间看到这些数据，并使用它来学习模式。
- en: Validation set
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集
- en: Used to evaluate the model’s performance during development. We use this to
    tune hyperparameters and compare model variants.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 用于在开发期间评估模型的性能。我们使用它来调整超参数并比较模型变体。
- en: Test set
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集
- en: Used only once, for final evaluation. Crucially, we avoid using this data to
    guide model design decisions. It serves as our best estimate of how well the model
    would generalize to completely unseen data.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 仅用于最终评估。关键的是，我们避免使用这些数据来指导模型设计决策。它作为我们对模型如何泛化到完全未见过的数据的最佳估计。
- en: 'We’ll split the proteins by their `EntryID`, ensuring that each protein appears
    in only one subset:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将蛋白质按其`EntryID`分割，确保每个蛋白质只出现在一个子集中：
- en: '[PRE70]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Now we’ll extract the rows for each split from our dataframe `sequence_df`:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将从我们的`sequence_df`数据框中提取每个分割的行：
- en: '[PRE71]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Output:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE72]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: This gives us clean, nonoverlapping training, validation, and test sets—each
    containing a subset of proteins we’ll use throughout model development and evaluation.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了干净、不重叠的训练、验证和测试集——每个集都包含我们将用于整个模型开发和评估的蛋白质子集。
- en: Converting Protein Sequences into Their Mean Embeddings
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将蛋白质序列转换为它们的平均嵌入
- en: We will now convert the sequences from each dataset split into their corresponding
    mean embeddings, just as we did earlier. Since this step can be time-consuming—especially
    with larger models—it’s worth thinking about how to do it efficiently. Using a
    GPU can significantly speed up computation, but we can also avoid repeating work
    by computing the embeddings only once, storing them to disk, and loading them
    later.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将每个数据集分割的序列转换为相应的平均嵌入，就像我们之前做的那样。由于这一步可能很耗时——特别是对于更大的模型——值得考虑如何高效地完成它。使用GPU可以显著加快计算速度，但我们也可以通过只计算一次嵌入、将其存储到磁盘并在以后加载来避免重复工作。
- en: 'To make this process more convenient, we’ll use a pair of helper functions
    to store and load sequence embeddings:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这个过程更加方便，我们将使用一对辅助函数来存储和加载序列嵌入：
- en: '[PRE73]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Let’s use the more powerful (but computationally expensive) ESM2 model with
    640-dimensional embeddings and store the embeddings for each split using the `store_sequence_embeddings`
    function:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用更强大（但计算成本更高）的ESM2模型，具有640维嵌入，并使用`store_sequence_embeddings`函数存储每个分割的嵌入：
- en: '[PRE74]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Once the embeddings are stored, we can load them back into memory whenever
    needed. Here’s a glimpse of the resulting training dataset that the model will
    learn from:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入存储后，我们可以在需要时将其加载回内存。以下是从模型学习的训练数据集的预览：
- en: '[PRE75]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Output:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE76]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: You’ll notice a series of columns labeled `ME:1` through `ME:640`. These represent
    the mean-pooled hidden states from the final layer of the ESM2 model—effectively
    a fixed-length numerical summary of each protein sequence. These embeddings capture
    biochemical and structural information learned during pretraining and will serve
    as the input features for our classifier.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到一系列标记为`ME:1`到`ME:640`的列。这些代表ESM2模型最终层的平均池化隐藏状态——实际上是每个蛋白质序列的固定长度数值摘要。这些嵌入捕获了预训练期间学习的生化结构和信息，并将作为我们分类器的输入特征。
- en: 'This dataframe becomes the input to a `convert_to_tfds` function, which we’ve
    defined to make it easier to prepare the datasets for each split:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据框成为`convert_to_tfds`函数的输入，我们定义了这个函数，以便更容易为每个分割准备数据集：
- en: '[PRE77]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Let’s now use our `convert_to_tfds` function to build a TensorFlow-compatible
    dataset from the training DataFrame:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用我们的`convert_to_tfds`函数从训练数据框构建一个TensorFlow兼容的数据集：
- en: '[PRE78]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Fetching a batch of data from these datasets is straightforward. We just batch
    the dataset, convert it to a NumPy iterator, and retrieve a batch by calling `next`:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些数据集中获取一批数据很简单。我们只需将数据集分批，将其转换为NumPy迭代器，然后通过调用`next`来检索一批：
- en: '[PRE79]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Output:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE80]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: These shapes confirm that each input is a 640-dimensional embedding vector (from
    the ESM2 model), and each target is a 303-dimensional binary vector representing
    the presence or absence of each molecular function label.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 这些形状确认了每个输入都是一个640维的嵌入向量（来自ESM2模型），每个目标是一个303维的二进制向量，表示每个分子功能标签的存在或不存在。
- en: Tip
  id: totrans-363
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Because the training dataset includes `.repeat()`, it yields batches indefinitely
    by looping over the data. This is useful for training, where we want to cycle
    through the dataset multiple times. In contrast, the validation and test datasets
    are not repeated—so their batches will eventually be exhausted, which is exactly
    what we want during evaluation, where each example should be seen only once.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 因为训练数据集包括`.repeat()`，它通过循环数据无限期地产生批次。这在训练中很有用，因为我们希望多次循环数据集。相比之下，验证和测试数据集不会被重复——因此它们的批次最终会耗尽，这正是我们在评估期间想要的，因为在评估期间每个示例只应被看到一次。
- en: 'To streamline the dataset setup, we’ve wrapped the entire pipeline into a single
    helper function, `build_dataset`:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'This function loads the saved mean embeddings from disk for all three splits
    and constructs `tf.data.Dataset` objects that are ready for training:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: With this, we now have our data fully preprocessed and ready to use in training
    a model.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Training the Model
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now train a simple [Flax](https://oreil.ly/MjH5C) linear model on top
    of the mean protein embeddings. Recall that each protein sequence has a variable
    length, but we’ve already transformed them into fixed-size embeddings. Our goal
    is to predict which of the 303 possible molecular functions each protein performs.
    This is a *multilabel classification* problem, meaning each protein may be associated
    with several function labels simultaneously.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: 'In this setup, we’ll train a lightweight MLP (multilayer perceptron)—a stack
    of dense layers with nonlinearities. Importantly, we are not fine-tuning the original
    ESM2 model: it remains frozen, and our model simply learns on top of its embeddings.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the model code:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Some notes on this very lightweight model:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: It uses `nn.Sequential` to stack layers, which keeps the definition clean and
    readable for this simple model.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use a GELU (Gaussian Error Linear Unit) activation function, which is a smooth,
    nonlinear alternative to ReLU.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final layer is an `nn.Dense` layer projecting to the number of function
    labels (`num_targets`). It returns logits, not probabilities—so we’ll apply a
    suitable activation (like sigmoid) inside the loss function to convert these logits
    into predicted probabilities.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model is frozen on top of the ESM2 embeddings—meaning it does not update
    the transformer weights. It learns only to map fixed embeddings to functional
    labels. This is efficient and interpretable, and it reduces memory usage during
    training.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may also have noticed that we attached a convenience function, `create_train_state`,
    to the model class for creating a training state. This encapsulates model initialization,
    parameter registration, and optimizer setup into a single `TrainState` object.
    It’s particularly useful because it allows us to construct the training state
    right when everything needed—the model, dummy input for shape inference, and optimizer
    config—is readily available.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s instantiate the model with the correct number of output targets, based
    on how many GO term columns we have in the training dataframe:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: This model is now ready to be trained to predict which molecular functions a
    protein is involved in, using the precomputed embeddings as input.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Training Loop
  id: totrans-384
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the model and dataset ready, we can now define a function to perform a
    single training step. This step includes:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: A forward pass through the model
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the loss
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating gradients
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating the model parameters using those gradients
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s how we implement it:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'In this setup:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: We use a sigmoid activation and binary cross-entropy loss, appropriate for multilabel
    classification. The logits go through a sigmoid activation, not softmax—because
    we want independent yes/no predictions for each possible protein function. Remember
    that each protein could have many functions at once.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用sigmoid激活和二元交叉熵损失，这适用于多标签分类。logits通过sigmoid激活，而不是softmax——因为我们希望对每个可能的蛋白质功能进行独立的是/否预测。记住，每个蛋白质可能同时具有许多功能。
- en: '`@jax.jit` compiles the training step for better performance.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`@jax.jit`编译训练步骤以获得更好的性能。'
- en: 'Next, let’s implement some metrics to evaluate how well the model is doing
    beyond the loss alone, using tools from `sklearn`:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们实现一些指标来评估模型除了损失之外的表现，使用`sklearn`中的工具：
- en: '[PRE86]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'We’ll track the following evaluation metrics for each function label:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将跟踪以下评估指标，针对每个功能标签：
- en: Accuracy
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 准确度
- en: The fraction of correct predictions across all labels. In multilabel classification
    with imbalanced data (like this), accuracy can be misleading—most labels are zero,
    so a model that always predicts “no function” would appear accurate. Still, it’s
    an intuitive metric and we’ll include it for now.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 所有标签中正确预测的比例。在多标签分类和不平衡数据（如本例）中，准确度可能会误导——大多数标签为零，因此总是预测“无功能”的模型看起来很准确。尽管如此，它是一个直观的指标，我们目前将其包括在内。
- en: Recall
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率
- en: The proportion of actual function labels the model correctly predicted (i.e.,
    true positives/all actual positives). High recall means the model doesn’t miss
    many true functions.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 模型正确预测的实际功能标签的比例（即，真阳性/所有实际阳性）。高召回率意味着模型不会错过许多真功能。
- en: Precision
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度
- en: The proportion of predicted function labels that are correct (i.e., true positives/all
    predicted positives). High precision means the model avoids false alarms.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 预测功能标签中正确的比例（即，真阳性/所有预测阳性）。高精确度意味着模型避免了误报。
- en: Area under the precision-recall curve (auPRC)
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度-召回率曲线下的面积（auPRC）
- en: Summarizes the tradeoff between precision and recall at different thresholds.
    Particularly useful in highly imbalanced settings like this one.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 总结了在不同阈值下精确度和召回率之间的权衡。在高度不平衡的设置中，如本例中，特别有用。
- en: Area under the receiver operating characteristic curve (auROC)
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 接收器操作特征曲线下的面积（auROC）
- en: Measures the model’s ability to distinguish positive from negative examples
    across all thresholds. While it’s a standard metric of discrimination ability,
    it can sometimes be misleading in highly imbalanced datasets, as it gives equal
    weight to both classes.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量模型在所有阈值下区分正例和负例的能力。虽然它是区分能力的标准指标，但在高度不平衡的数据集中，它有时可能会误导，因为它对两个类别给予相同的权重。
- en: In a multilabel setting, we calculate these metrics for each protein function
    (i.e., per target/label), then average them to get a holistic view of model performance.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在多标签设置中，我们针对每个蛋白质功能（即，每个目标/标签）计算这些指标，然后取平均值以获得模型性能的整体视图。
- en: 'We apply these metrics calculations during the evaluation step `eval_step`:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在评估步骤`eval_step`中应用这些指标计算：
- en: '[PRE87]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'The evaluation computes metrics per protein in the batch. For each protein,
    we:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 评估计算批次中每个蛋白质的指标。对于每个蛋白质，我们：
- en: Apply sigmoid to its 303 logits to get function probabilities.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将sigmoid应用于其303个logits以获得功能概率。
- en: Threshold those probabilities (e.g., at 0.5) to get binary predictions.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些概率（例如，在0.5处）阈值化以获得二元预测。
- en: Compare these to the true function labels to compute metrics like accuracy,
    precision, recall, auPRC, and auROC.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将这些与真实的函数标签进行比较，以计算准确度、精确度、召回率、auPRC和auROC等指标。
- en: We repeat this for every protein in the batch and then average the resulting
    metrics across proteins. This tells us how well the model predicts sets of functions
    per protein. It does not report performance per GO term. If we wanted per-function
    metrics (e.g., how well the model predicts `GO:0003677`), we’d need to compute
    metrics column-wise instead.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对批次中的每个蛋白质重复此操作，然后平均蛋白质间的结果指标。这告诉我们模型如何预测每个蛋白质的功能集。它不报告每个GO术语的性能。如果我们想要每个功能的指标（例如，模型如何预测`GO:0003677`），我们需要按列计算指标。
- en: 'In the next chunk of code, everything comes together into a `train` function,
    and variations of this basic setup will be repeated in every chapter. We have
    the training loop where we first initialize our model training state and then
    loop over the dataset in batches to train the model and evaluate it every so often:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分代码中，所有内容都整合到一个`train`函数中，并且这种基本设置将在每一章中重复。我们有训练循环，首先初始化我们的模型训练状态，然后以批处理的方式遍历数据集以训练模型，并时不时地评估它：
- en: '[PRE88]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'A few notes on this training loop:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个训练循环的一些注意事项：
- en: Efficient batch sampling
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的批量采样
- en: Training data is streamed via `.as_numpy_iterator()`, and the `.repeat()` in
    the dataset ensures infinite looping over the data.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据通过 `.as_numpy_iterator()` 流式传输，数据集中的 `.repeat()` 确保数据无限循环。
- en: Regular evaluation
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 定期评估
- en: Every `eval_every` step, the model is evaluated on the full validation set to
    monitor progress using metrics we defined previously, like auPRC and auROC.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 每 `eval_every` 步，模型将在整个验证集上评估，以使用我们之前定义的指标（如 auPRC 和 auROC）来监控进度。
- en: Metric aggregation
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 指标聚合
- en: Validation metrics are computed batch-wise and then averaged across all batches
    using `pd.DataFrame(...).mean(axis=0)`. This gives a stable estimate of performance
    across the entire validation set.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 验证指标是按批计算，然后使用 `pd.DataFrame(...).mean(axis=0)` 在所有批次上平均，这给出了整个验证集性能的稳定估计。
- en: 'Let’s now train the model. But first, a quick trick: to avoid unnecessarily
    repeating training from scratch every time you rerun your code cell, we use the
    `@restorable` decorator. This lightweight utility checks whether a trained model
    already exists at a specified path. If it does, it:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们训练模型。但首先，一个快速技巧：为了避免每次重新运行代码单元格时都从零开始不必要地重复训练，我们使用 `@restorable` 装饰器。这个轻量级实用工具检查指定路径上是否已存在已训练的模型。如果存在，它：
- en: Skips retraining
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跳过重新训练
- en: Restores the model into a valid `TrainState`
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型恢复到有效的 `TrainState`
- en: Returns the model along with any saved metrics
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 返回模型以及任何保存的指标
- en: 'This makes your workflow much faster and more reproducible, especially during
    iterative development and debugging. Let’s take a look at how this is used:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得你的工作流程更快、更可重复，尤其是在迭代开发和调试期间。让我们看看它是如何使用的：
- en: '[PRE89]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: Some additional parameters worth mentioning are the optimizer (here, `optax.adam`)
    and the total number of training steps (`num_steps`). Given that we have 2,100
    training examples and a batch size of 32, it will take about 66 steps for the
    model to see the entire training set once. Setting `num_steps=300` means the model
    will see each training data point several times.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 一些值得注意的额外参数包括优化器（这里，`optax.adam`）和总训练步数（`num_steps`）。鉴于我们有 2,100 个训练示例和批大小为
    32，模型需要大约 66 步才能看到整个训练集一次。设置 `num_steps=300` 意味着模型将多次看到每个训练数据点。
- en: 'Having trained the model with the previous `train` call, we can now evaluate
    its training dynamics and performance on the validation set, as shown in [Figure 2-13](#mlp-model-eval):'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前的 `train` 调训练模型后，我们现在可以评估其在验证集上的训练动态和性能，如图 [图 2-13](#mlp-model-eval) 所示：
- en: '[PRE90]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: '![](assets/dlfb_0213.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0213.png)'
- en: Figure 2-13\. Training and evaluation of the MLP model over 300 steps. On the
    left, loss curves for the training and validation splits show rapid convergence,
    with stability reached after ~30 steps. On the right, auPRC, precision, and recall
    improve gradually. Accuracy and auROC metrics are very high due to class imbalance
    and are not very informative for this problem.
  id: totrans-435
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-13\. 在 300 步中训练和评估 MLP 模型。在左侧，训练和验证分割的损失曲线显示快速收敛，在 ~30 步后达到稳定性。在右侧，auPRC、精确度和召回率逐渐提高。由于类别不平衡，准确率和
    auROC 指标非常高，但这些指标对于这个问题并不很有信息量。
- en: In the left panel, we observe that both training and validation loss drop sharply
    within the first ~30 steps and then stabilize. This is a typical learning curve,
    indicating rapid convergence without substantial instability (e.g., no major spikes
    or divergence). It suggests that the model—a shallow MLP operating on top of frozen
    pretrained embeddings—quickly captures the low-hanging signal in the data.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧面板中，我们观察到训练和验证损失在最初的 ~30 步骤内急剧下降，然后趋于稳定。这是一个典型的学习曲线，表明模型快速收敛且没有实质性的不稳定性（例如，没有大的峰值或发散）。这表明该模型——一个在冻结预训练嵌入之上运行的浅层
    MLP 模型——能够快速捕捉数据中的低垂信号。
- en: 'In the right panel, we track several evaluation metrics over time:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧面板中，我们随着时间的推移跟踪几个评估指标：
- en: Accuracy and auROC start high and remain flat, but these can be misleading in
    imbalanced, multilabel settings like this one. Since most function labels are
    negative (i.e., a protein lacks the majority of all possible functions), a model
    that mostly predicts zeros can still achieve a high score on these metrics. For
    that reason, we don’t put much weight on these metrics in this context.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率和 auROC 开始很高并保持平稳，但在这种不平衡、多标签设置中可能会误导。由于大多数功能标签是负的（即，蛋白质缺乏所有可能功能的大多数），一个主要预测零的模型仍然可以在这些指标上获得高分。因此，我们在这个背景下不太重视这些指标。
- en: auPRC steadily improves and does not fully plateau, suggesting the model continues
    to learn subtle distinctions and could potentially benefit from further training
    (i.e., by increasing `num_steps`).
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: auPRC稳步提高且没有完全达到平台期，表明模型继续学习细微的区别，并可能从进一步训练中受益（例如，通过增加`num_steps`）。
- en: Precision improves more quickly than recall, indicating the model becomes increasingly
    confident in its predictions but still fails to capture some true positives.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精度比召回率提高得更快，表明模型对其预测越来越有信心，但仍未能捕捉到一些真实正例。
- en: Together, these trends indicate that while most of the learning happens early
    on, there may still be headroom—particularly in recall and auPRC—if training were
    extended further or if a more powerful architecture were used.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 这些趋势共同表明，尽管大部分学习发生在早期，但如果进一步延长训练时间或使用更强大的架构，召回率和auPRC可能仍有提升空间。
- en: Tip
  id: totrans-442
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: It can be slightly tedious to manually log metrics inside every training loop
    and then hook up custom plotting code to visualize them. To streamline this, later
    chapters introduce a MetricsLogger (for capturing values) and MetricsPlotter (for
    rendering them).
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个训练循环中手动记录指标并连接自定义绘图代码以可视化它们可能会有些繁琐。为了简化这个过程，后面的章节将介绍MetricsLogger（用于捕获值）和MetricsPlotter（用于渲染它们）。
- en: 'Beyond that, many modern machine learning workflows use hosted (or self-hosted)
    dashboards to automatically collect, store, and display metrics in real time.
    These tools help monitor experiments, compare training runs, and share results
    across teams. We encourage you to check them out. Popular options include:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，许多现代机器学习工作流程使用托管（或自托管）仪表板来自动收集、存储和实时显示指标。这些工具有助于监控实验、比较训练运行并在团队间共享结果。我们鼓励您去了解一下。流行的选项包括：
- en: '[TensorBoard](https://oreil.ly/tSPIP)'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[TensorBoard](https://oreil.ly/tSPIP)'
- en: '[Weights & Biases (W&B)](https://oreil.ly/Loybs)'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Weights & Biases (W&B)](https://oreil.ly/Loybs)'
- en: '[MLflow](https://oreil.ly/A1faJ)'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[MLflow](https://oreil.ly/A1faJ)'
- en: It’s great to see the model training successfully and loss and metrics curves
    trending in the right direction—but that’s just the beginning. The real insight
    comes from analyzing the model’s predictions, understanding where it performs
    well, and identifying its limitations.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 看到模型训练成功，损失和指标曲线趋势正确——但这只是开始。真正的洞察来自于分析模型的预测，了解其表现良好的地方，并确定其局限性。
- en: Examining the Model Predictions
  id: totrans-449
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查模型预测
- en: 'With a trained model in hand, it’s time to explore its strengths and weaknesses.
    We’ll start by generating predictions for the entire validation set and storing
    them in a dataframe for easier inspection:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个训练好的模型后，是时候探索其优势和劣势了。我们将首先为整个验证集生成预测，并将它们存储在数据框中以方便检查：
- en: '[PRE91]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'To get a high-level sense of how the model is performing, we can visualize
    the full prediction matrix as a heatmap. In [Figure 2-14](#predicted-functional-annotation),
    we plot two side-by-side heatmaps: one showing the true protein-function annotations
    (left) and the other showing the model’s predicted probabilities (right). Each
    column corresponds to a protein function, and each row to a protein:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得模型性能的高层次感知，我们可以将完整的预测矩阵可视化为热图。在[图2-14](#predicted-functional-annotation)中，我们并列展示了两个热图：一个显示真实的蛋白质-功能注释（左侧），另一个显示模型的预测概率（右侧）。每一列对应一个蛋白质功能，每一行对应一个蛋白质：
- en: '[PRE92]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: '![](assets/dlfb_0214.png)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0214.png)'
- en: Figure 2-14\. Heatmap overview of protein function prediction. The left panel
    shows the ground truth functional annotations for each protein in the validation
    set, while the right panel shows the model’s predicted probabilities. Both matrices
    are sparse, with vertical bands reflecting common function labels.
  id: totrans-455
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-14. 蛋白质功能预测的热图概述。左侧面板显示了验证集中每个蛋白质的真实功能注释，而右侧面板显示了模型预测的概率。这两个矩阵都是稀疏的，垂直带反映了常见功能标签。
- en: 'This visualization is quite zoomed out and high level, but it helps build intuition
    about overall model behavior:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 这种可视化相当放大且高层次，但它有助于建立对整体模型行为的直觉：
- en: Some protein functions appear frequently in the dataset (visible as vertical
    stripes), and the model tends to predict these relatively well.
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些蛋白质功能在数据集中出现频率较高（表现为垂直条纹），模型倾向于对这些功能进行相对较好的预测。
- en: Rare functions are harder to capture—the model often misses them entirely, leading
    to sparse or empty columns in the predicted heatmap.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀有功能更难捕捉——模型经常完全错过它们，导致预测热图中出现稀疏或空列。
- en: A few functions are over-predicted, visible as faint vertical lines across many
    proteins, suggesting the model is overly confident for those categories.
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些函数被过度预测，表现为许多蛋白质中的微弱垂直线，这表明模型对这些类别过于自信。
- en: Many cells in the predicted matrix show intermediate color tones, which reflect
    more uncertain probabilities (not a confident near-0 or near-1).
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测矩阵中的许多单元格显示中间色调，这反映了更不确定的概率（不是自信的接近0或接近1）。
- en: 'We’ll now shift from this qualitative view to a quantitative one by evaluating
    model performance on each protein function individually:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将从这个定性观点转向定量观点，通过评估每个蛋白质功能的模型性能来进行：
- en: '[PRE93]'
  id: totrans-462
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Output:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE94]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'This analysis reveals substantial variation in model performance across protein
    functions. For instance, the model performs well on functions like `GO:0004930`
    (G protein–coupled receptor activity), but it struggles with others, such as `GO:0003774`
    (cytoskeletal motor activity). However, interpreting these results requires caution:
    some metrics may be based on very few validation examples, and performance is
    naturally limited for functions that are underrepresented during training. A high
    score on a frequent function may simply reflect ample training data, while low
    scores on rare functions may be expected.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 这项分析揭示了模型在蛋白质功能上的性能存在很大差异。例如，模型在`GO:0004930`（G蛋白偶联受体活性）等函数上表现良好，但在其他函数上，如`GO:0003774`（细胞骨架马达活性）上则表现不佳。然而，解释这些结果需要谨慎：一些指标可能基于非常少的验证示例，并且对于在训练过程中代表性不足的函数，性能自然会有限。频繁函数的高分可能仅仅反映了充足的训练数据，而罕见函数的低分可能是预期的。
- en: 'Let’s take a closer look at whether there’s a relationship between how often
    a protein function appears in the training data and how well the model learns
    to predict it in the validation set:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看蛋白质功能在训练数据中出现的频率与模型在验证集中预测它的好坏之间是否存在关系：
- en: '[PRE95]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Output:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE96]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'At a glance, it seems that functions with higher predictive performance (e.g.,
    higher auPRC) also tend to have more training examples. In [Figure 2-15](#auprc-over-train-n),
    we visualize this relationship more clearly with a scatterplot:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，似乎预测性能更高的函数（例如，更高的auPRC）也往往有更多的训练示例。在[图2-15](#auprc-over-train-n)中，我们通过散点图更清楚地可视化这种关系：
- en: '[PRE97]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: '![](assets/dlfb_0215.png)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/dlfb_0215.png)'
- en: Figure 2-15\. Relationship between training frequency and predictive performance
    (auPRC) across protein functions. Commonly observed functions in the training
    set tend to be predicted more accurately by the model.
  id: totrans-473
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-15. 蛋白质功能训练频率与预测性能（auPRC）之间的关系。训练集中常见的功能往往被模型更准确地预测。
- en: 'This plot shows a clear trend: protein functions that occur more frequently
    in the training set tend to be predicted more accurately by the model on the validation
    set (as measured by auPRC). This aligns with expectations—machine learning models
    usually perform better on well-represented classes. It also highlights the challenge
    of class imbalance: rare functions are often poorly predicted, not necessarily
    due to biological complexity but because the model has limited data to learn from.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图显示了明显的趋势：在训练集中出现频率更高的蛋白质功能，在验证集中被模型更准确地预测（以auPRC衡量）。这与预期相符——机器学习模型通常在代表性良好的类别上表现更好。这也突出了类别不平衡的挑战：罕见的功能往往预测不佳，这不一定是因为生物复杂性，而是因为模型学习的数据有限。
- en: But how do we know whether a specific auPRC score is actually good? An auPRC
    value of, say, 0.8 for a certain protein function might sound promising—but is
    that better than chance? Is it meaningful? To interpret these scores, we need
    something to compare them against.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何知道特定的auPRC分数是否真的很好？例如，对于某种蛋白质功能，auPRC值为0.8可能听起来很有希望——但这是否比随机更好？它是否有意义？为了解释这些分数，我们需要一些东西来比较它们。
- en: Evaluating Model Usefulness
  id: totrans-476
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估模型的有用性
- en: 'To ground our evaluation, we’ll compare our model against two simple baselines:'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的评估有据可依，我们将把我们的模型与两个简单的基线进行比较：
- en: Coin flip
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 抛硬币
- en: For each protein function, randomly predict 0 or 1 with equal probability. This
    gives us a baseline for total ignorance.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个蛋白质功能，以相等的概率随机预测0或1。这为我们提供了一个完全无知的基础。
- en: Proportional guessing
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 比例猜测
- en: Predict 1 for each function with probability equal to its frequency in the training
    set. This reflects prior class distribution knowledge, but without any learning.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个函数，预测其出现概率等于其在训练集中的频率。这反映了先验的类别分布知识，但没有任何学习。
- en: These baselines help contextualize the model’s performance. If our trained model
    doesn’t outperform these simple heuristics, it’s a sign that it may not have learned
    meaningful structure from the data.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基线有助于将模型的性能置于上下文中。如果我们的训练模型没有优于这些简单的启发式方法，这可能是一个迹象，表明它可能没有从数据中学习到有意义的结构。
- en: 'Here are implementations for the baselines:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是基线的实现：
- en: '[PRE98]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'These baselines should give us simple but informative reference points. Let’s
    now apply these prediction methods, alongside our trained model:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基线应该给我们提供简单但信息丰富的参考点。现在，让我们将这些预测方法应用于我们的训练模型：
- en: '[PRE99]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Now let’s evaluate the baselines in exactly the same way as our model—by computing
    per-protein metrics and averaging them:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们以与我们的模型完全相同的方式评估基线——通过计算每个蛋白质的指标并取平均值：
- en: '[PRE100]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Output:'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE101]'
  id: totrans-490
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: Our model clearly outperforms both baselines across all metrics—especially in
    precision, auPRC, and auROC. This is expected, as the trained model leverages
    actual sequence features to make more informed predictions. As noted earlier,
    accuracy is not a reliable metric in this setting, and even simple proportional
    guessing achieves a deceptively high accuracy due to class imbalance.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在所有指标上都明显优于两个基线——特别是在精确度、auPRC和auROC方面。这是预期的，因为训练模型利用实际的序列特征做出更明智的预测。如前所述，准确性在这个设置中不是一个可靠的指标，即使是简单的比例猜测也由于类别不平衡而达到欺骗性的高准确性。
- en: Most of the model’s performance gains come from a large increase in precision,
    while the improvement in recall is more modest. This means the model is good at
    correctly identifying positive cases when it makes a prediction, but it tends
    to miss many true positives—it’s cautious and biased toward predicting “no function.”
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的性能提升主要来自精确度的显著提高，而召回率的提升则更为适度。这意味着当模型做出预测时，它擅长正确识别正例，但它往往错过许多真实正例——它谨慎且倾向于预测“无功能”。
- en: 'This highlights a key trade-off: the model is conservative but accurate. Depending
    on your application, you may want to tune this behavior—for example, by lowering
    the decision threshold to improve recall, as discussed earlier.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 这突出了一个关键权衡：模型保守但准确。根据您的应用，您可能希望调整这种行为——例如，通过降低决策阈值以提高召回率，如前所述。
- en: 'Next, we’ll break down the model’s strengths and weaknesses by individual protein
    function and compare performance against both baselines. This allows us to see
    which specific functions the model predicts well—and where it struggles:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将通过单个蛋白质功能分解模型的优缺点，并与两个基线进行比较。这使我们能够看到模型预测得好的具体功能，以及它遇到困难的地方：
- en: '[PRE102]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'In [Figure 2-16](#best-predicted-functions), we visualize the function-level
    auPRC scores as a bar plot to highlight which functional categories the model
    handles best:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图2-16](#best-predicted-functions)中，我们将函数级别的auPRC分数可视化为条形图，以突出模型处理最好的功能类别：
- en: '[PRE103]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: '![](assets/dlfb_0216.png)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dlfb_0216.png)'
- en: Figure 2-16\. Top 20 protein functions ranked by model auPRC on the validation
    set. Bars show the auPRC achieved by the model, compared against two simple baselines
    (i.e., coin flips and proportional guessing).
  id: totrans-499
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-16\. 模型在验证集上按auPRC排名的前20个蛋白质功能。条形图显示了模型实现的auPRC，与两个简单基线（即抛硬币和比例猜测）进行比较。
- en: Many of the top-performing protein functions in the plot are related to membrane
    or signaling roles (e.g., GPCR activity, kinase activity, transmembrane receptor
    activity). One possible reason is that these functions often involve well-conserved
    sequence features—such as transmembrane helices or catalytic domains—that may
    be easier for models to learn. While speculative, this aligns with the idea that
    functions tied to strong structural or biochemical motifs may produce clearer
    sequence-level signals than more context-dependent roles.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 图表中表现最好的蛋白质功能大多与膜或信号传导作用相关（例如，GPCR活性、激酶活性、跨膜受体活性）。一个可能的原因是，这些功能通常涉及高度保守的序列特征，如跨膜螺旋或催化结构域，这可能更容易让模型学习。虽然这是推测性的，但这与这样一个观点相一致：与强大的结构或生化基序相关的功能可能比更依赖上下文的作用产生更清晰的序列水平信号。
- en: Together, these results suggest that the model is capable of detecting meaningful
    biological signal for certain classes of protein function—and that it significantly
    outperforms simple baselines.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果共同表明，该模型能够检测到某些蛋白质功能类别的有意义的生物信号，并且它显著优于简单基线。
- en: Conducting a Final Check on the Test Set
  id: totrans-502
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在测试集上进行最终检查
- en: 'Take a look at the next section for ideas on how to extend and improve this
    model. Once you are satisfied with your exploration, we can move on to the final
    step of this project: making the final predictions on the test set. Remember not
    to touch the test set until the last stage of your project.'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 查看下一节，了解如何扩展和改进这个模型。一旦你对探索结果满意，我们就可以继续进行这个项目的最后一步：在测试集上进行最终预测。记住，在项目最后阶段之前不要触碰测试集。
- en: Warning
  id: totrans-504
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Be sure not to touch the test set until you’ve fully finalized your model—including
    all hyperparameters, architectures, and training choices. Evaluating on the test
    set repeatedly can lead to overly optimistic results and undermine the validity
    of your findings.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在完全最终确定你的模型（包括所有超参数、架构和训练选择）之前不要触碰测试集。在测试集上重复评估可能导致过于乐观的结果，并损害你发现的有效性。
- en: 'We’ll make predictions on the test set of proteins in the same way we did for
    the validation set:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以与验证集相同的方式对蛋白质的测试集进行预测：
- en: '[PRE104]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Output:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE105]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: The test set metrics closely mirror those observed on the validation set, which
    is good. In many workflows, test performance is slightly lower due to repeated
    use of the validation set during development—potentially leading to mild overfitting.
    However, in this case, we haven’t done extensive tuning, so the gap is minimal.
    Because the test set was held out throughout, its results provide a more reliable
    estimate of how the model will generalize to truly unseen data. These are the
    metrics we would report externally.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集的指标与验证集上观察到的指标非常相似，这是好的。在许多工作流程中，由于在开发过程中重复使用验证集，测试性能略低，可能导致轻微的过拟合。然而，在这种情况下，我们没有进行广泛的调整，因此差距很小。因为测试集在整个过程中都被保留出来，其结果提供了对模型如何泛化到真正未见数据的更可靠估计。这些是我们将对外报告的指标。
- en: Improvements and Extensions
  id: totrans-511
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 改进和扩展
- en: 'The model we’ve built demonstrates that protein function can be predicted from
    sequence using pretrained embeddings and a lightweight classifier. However, many
    directions remain to improve, interpret, and extend this work. We split these
    ideas into two broad categories: analysis-driven insights and machine learning
    improvements.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建的模型表明，可以使用预训练嵌入和轻量级分类器从序列中预测蛋白质功能。然而，还有很多方向需要改进、解释和扩展这项工作。我们将这些想法分为两大类：分析驱动的见解和机器学习改进。
- en: 'But before diving into technical upgrades, it’s worth stepping back to revisit
    the bigger picture:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 但在深入技术升级之前，值得退一步重新审视更大的图景：
- en: Why are you doing this?
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 你为什么要这样做？
- en: Who will use the model, and what do they actually need? Can you share this prototype
    with users now to gather early feedback?
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 谁将使用这个模型，他们实际上需要什么？你现在可以与用户分享这个原型以收集早期反馈吗？
- en: When are you done?
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 你什么时候才算完成？
- en: Is the current model already good enough? What specific improvements would meaningfully
    increase its utility? What benchmarks exist for this or similar tasks?
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 当前模型已经足够好吗？哪些具体的改进可以有意义地增加其效用？这个或类似任务有哪些基准？
- en: What matters most?
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 什么最重要？
- en: Is performance across all functions equally important, or do you care about
    a specific class (e.g., enzymes versus nonenzymes)? Focusing your optimization
    accordingly can save time.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 所有功能的表现是否同等重要，或者你是否关心特定的类别（例如，酶与非酶）？相应地专注于优化可以节省时间。
- en: Do you need interpretability?
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要可解释性吗？
- en: For some applications, understanding why a model makes a prediction may matter
    more than maximizing performance.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些应用来说，理解模型为何做出预测可能比最大化性能更重要。
- en: Ideally, you’ll have thought about some of these questions before starting the
    modeling—but revisiting them now can help guide your next steps.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你在开始建模之前就已经考虑过一些这些问题——但现在重新审视这些问题可以帮助指导你的下一步。
- en: Biological and Analytical Exploration
  id: totrans-523
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生物和解析探索
- en: 'Even with a fixed model, we can learn a lot more by probing its behavior and
    comparing it to biological expectations:'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是固定的模型，通过探究其行为并将其与生物预期进行比较，我们也能学到更多：
- en: Threshold tuning
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值调整
- en: Our results showed that the model has high auPRC but low recall at a default
    probability threshold of 0.5\. You could optimize this threshold (e.g., per protein
    function or globally) using a metric like F1 score to find a better trade-off
    between precision and recall.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的结果显示，模型在默认概率阈值0.5下具有高auPRC但召回率低。你可以使用F1分数等指标优化这个阈值（例如，按蛋白质功能或全局优化），以找到精度和召回率之间更好的权衡。
- en: Species generalization
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 物种泛化
- en: The current dataset is human only, but this might be unnecessarily limited.
    Try including protein-function pairs from other species to see if performance
    improves.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 当前数据集仅包含人类数据，但这可能是不必要的限制。尝试包括来自其他物种的蛋白质-功能对，看看性能是否有所提升。
- en: Function-specific performance drivers
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 功能特定的性能驱动因素
- en: Why does the model do well on some functions (e.g., GPCR activity) but poorly
    on others (e.g., growth factor activity)? You could investigate whether function
    prevalence, sequence length, or other properties correlate with performance.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么模型在某些功能（例如，GPCR 活性）上表现良好，而在其他功能（例如，生长因子活性）上表现不佳？你可以调查功能普遍性、序列长度或其他属性是否与性能相关。
- en: Examine protein multifunctionality
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 检查蛋白质的多功能性
- en: Does the model struggle more with proteins that have many functions? Group proteins
    by number of annotated functions and plot performance (e.g., auPRC) to see if
    there’s a trend.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是否在具有许多功能的蛋白质上遇到更多困难？按注释的功能数量对蛋白质进行分组，并绘制性能（例如，auPRC）图，以查看是否存在趋势。
- en: False positives that might be real
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是真实的假阳性
- en: Find proteins where the model confidently predicts a function that isn’t labeled.
    Could the model be correct and the annotation missing? How might you follow this
    up?
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 找到模型自信地预测了未标记功能的蛋白质。模型可能是正确的，而注释缺失了吗？你将如何跟进？
- en: Machine Learning Improvements
  id: totrans-535
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习改进
- en: 'From a machine learning perspective, here are a few directions you could explore:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 从机器学习的角度来看，这里有几个你可以探索的方向：
- en: Tune the MLP
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 调整 MLP
- en: Our model is a small MLP on top of frozen embeddings. Try adding more layers,
    dropout, or batch normalization to increase capacity while controlling overfitting.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型是在冻结嵌入之上的小型 MLP。尝试添加更多层、dropout 或批量归一化，以增加容量同时控制过拟合。
- en: Alternative input encodings
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 替代输入编码
- en: We used the mean-pooled embedding, which loses sequence order information. Try
    attention pooling or a small 1D CNN or transformer on top of the token-level embeddings.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了平均池化嵌入，这会丢失序列顺序信息。尝试使用注意力池化或在小型 1D CNN 或 transformer 上使用标记级嵌入。
- en: Feature engineering
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程
- en: You could augment the input to include protein length, species (if you extend
    beyond human), or even simple statistics like embedding norms. These additional
    features might help the model distinguish protein types more effectively.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以增强输入，包括蛋白质长度、物种（如果你扩展到人类之外），或者甚至简单的统计数据，如嵌入范数。这些附加特征可能有助于模型更有效地区分蛋白质类型。
- en: 'Train a per-function head:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 训练每个功能的头部：
- en: Instead of predicting all functions jointly, try training separate models (or
    heads) for each function. This can help when tasks are highly imbalanced or unrelated.
    Alternatively, you could cluster GO functions into a few categories and train
    one model per cluster.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是联合预测所有功能，尝试为每个功能训练单独的模型（或头部）。当任务高度不平衡或不相关时，这可能会很有帮助。或者，你也可以将 GO 功能聚类到几个类别中，并为每个聚类训练一个模型。
- en: Predict function hierarchically
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 层次预测功能
- en: Rather than treating each function independently, you could use the GO hierarchy
    to add structure to predictions—for example, predicting broad function categories
    first and then refining to more specific ones.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是独立处理每个功能，你可以使用 GO 层次结构为预测添加结构——例如，首先预测广泛的功能类别，然后细化到更具体的功能。
- en: Try alternative base models
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试替代基础模型
- en: You could plug in other protein language models from Hugging Face or explore
    combining embeddings from multiple models by concatenating them.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以插入 Hugging Face 上的其他蛋白质语言模型，或者探索通过连接它们来组合多个模型的嵌入。
- en: Unfreeze the language model
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 解冻语言模型
- en: The ESM2 embeddings are pretrained on a generic task. Fine-tuning the language
    model directly for protein function classification may boost performance, though
    it requires more compute and a more involved training setup.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: ESM2 嵌入是在通用任务上预训练的。直接微调语言模型以进行蛋白质功能分类可能会提高性能，尽管这需要更多的计算和更复杂的训练设置。
- en: Note
  id: totrans-551
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: While it’s tempting to chase performance gains through increasingly complex
    models, always align your efforts with the actual goals of your project. Improving
    interpretability or expanding biological coverage may be more valuable than inching
    up another point on a leaderboard.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通过越来越复杂的模型追求性能提升很有吸引力，但始终将你的努力与项目的实际目标保持一致。提高可解释性或扩展生物学覆盖范围可能比在排行榜上提高一点更重要。
- en: Summary
  id: totrans-553
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we took our first hands-on step into the world of deep learning
    for biology. Starting with a dataset of human proteins, we explored how to extract
    meaningful representations using a pretrained protein language model, trained
    a simple classifier to predict protein function, and evaluated its performance
    using quantitative metrics.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们迈出了在生物学深度学习领域的第一个实际操作步骤。从人类蛋白质数据集开始，我们探讨了如何使用预训练的蛋白质语言模型提取有意义的表示，训练了一个简单的分类器来预测蛋白质功能，并使用定量指标评估其性能。
- en: 'Along the way, we encountered practical challenges typical of biological modeling:
    getting comfortable with a new modeling setup, dealing with imbalanced label distributions,
    and carefully interpreting evaluation metrics.'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们遇到了典型的生物学建模的实践挑战：适应新的建模设置，处理不平衡的标签分布，以及仔细解释评估指标。
- en: In the next chapter, we’ll build on these foundations by shifting our focus
    from proteins to DNA. You’ll define convolutional neural networks from scratch
    in Flax and train them end to end to model regulatory sequences, predict functional
    elements, and discover motif patterns directly from genomic data.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将在此基础上构建，将焦点从蛋白质转移到DNA。你将从头开始定义卷积神经网络，并在Flax中训练它们，以建模调控序列，预测功能元素，并直接从基因组数据中发现基序模式。
