- en: Chapter 2\. Learning the Language of Proteins
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Life as we know it operates on *proteins*. The human genome holds about 20,000
    *genes*, each made of DNA, that serve as blueprints for building different proteins.
    Some proteins have simple, well-understood functions—like collagen, which provides
    structural support and elasticity to tissues, or hemoglobin, which transports
    oxygen and carbon dioxide between the lungs and the rest of the body. Others have
    slightly more abstract roles: they act as messengers, modulators, or signal carriers,
    transmitting information within and between cells. For example, insulin is a protein
    hormone that signals cells to absorb sugar from the bloodstream.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll dive into how DNA and proteins work in more detail soon. But for now,
    imagine a protein as a blobby molecular machine bumping around in the crowded
    cell environment, occasionally making productive collisions. Its shape and movement
    may seem chaotic, but both have been fine-tuned by millions of years of evolution
    to carry out very specific molecular functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'One key detail for this chapter: a protein can be represented as a sequence
    of its constituent building blocks, called *amino acids*. Just as English uses
    26 letters to form words, proteins use an alphabet of 20 amino acids to form long
    chains with specific shapes and jobs. With that in mind, the goal of this chapter
    is simple: we’ll train a model to predict a protein’s function given its amino
    acid sequence. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: Given the sequence of the COL1A1 collagen protein (`MFSFVDLR...`), we might
    predict its function is likely `structural` with probability 0.7, `enzymatic`
    with probability 0.01, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the sequence of the INS insulin protein (`MALWMRLL...`), we might predict
    its function is likely *metabolic* with probability 0.6, *signaling* with probability
    0.3, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To get hands-on with the material right away, open the companion Colab notebook
    and try running the code as you read the chapter. Exploring the examples interactively
    is one of the best ways to build intuition and make the ideas stick.
  prefs: []
  type: TYPE_NORMAL
- en: Biology Primer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We already highlighted that proteins are essential units of function within
    the cell, fulfilling a vast range of biological roles. A protein’s function is
    very closely tied to its 3D structure, which in turn is determined by its primary
    amino acid sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap the flow of information: a gene encodes the primary amino acid sequence
    of a protein. That sequence determines the protein’s structure, and the structure
    governs its function.'
  prefs: []
  type: TYPE_NORMAL
- en: Protein Structure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Protein structure is typically described in four hierarchical levels:'
  prefs: []
  type: TYPE_NORMAL
- en: Primary structure
  prefs: []
  type: TYPE_NORMAL
- en: The linear sequence of amino acids
  prefs: []
  type: TYPE_NORMAL
- en: Secondary structure
  prefs: []
  type: TYPE_NORMAL
- en: Local folding into structural elements such as alpha helices and beta sheets
  prefs: []
  type: TYPE_NORMAL
- en: Tertiary structure
  prefs: []
  type: TYPE_NORMAL
- en: The overall 3D shape formed by the complete amino acid chain
  prefs: []
  type: TYPE_NORMAL
- en: Quaternary structure
  prefs: []
  type: TYPE_NORMAL
- en: The assembly of multiple protein subunits into a functional complex (not all
    proteins have this)
  prefs: []
  type: TYPE_NORMAL
- en: As an example, [Figure 2-1](#id1) shows the structural organization levels of
    hemoglobin.
  prefs: []
  type: TYPE_NORMAL
- en: The human genetic code specifies 20 main amino acids. Each has a unique chemical
    structure, but they can be grouped by shared biochemical properties—such as hydrophobicity
    (how they interact with water), charge (positive, negative, or neutral), and polarity
    (how evenly electrical charge is distributed over the molecule).
  prefs: []
  type: TYPE_NORMAL
- en: Although biochemistry students are often expected to memorize all 20 amino acids,
    complete with names, structures, and single-letter codes (don’t ask us how we
    know), it’s more practical here to focus on their functional roles (summarized
    in [Figure 2-2](#amino-acids)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-1\. The four levels of protein structure, as illustrated by the hemoglobin
    protein. Source: [Wikipedia](https://oreil.ly/BD2Qa).'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For instance, `D` (aspartic acid) and `E` (glutamic acid) are both negatively
    charged and often interchangeable without drastically altering a protein’s function.
    But other amino acids play much more specific roles, and even a single substitution
    can dramatically alter how a protein folds or functions—sometimes with serious
    effects. In fact, many genetic diseases are caused by such point mutations. One
    famous example is sickle cell anemia, which is caused by a single-letter change
    in the gene for hemoglobin that replaces a hydrophilic amino acid (`E`) with a
    hydrophobic one (`V`), which ultimately leads to misshapen red blood cells.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. Chart showing the chemical structures of the 20 standard amino
    acids found in living organisms, grouped by biochemical similarity, color-coded
    by side-chain properties (e.g., acidic, basic, polar, nonpolar), and annotated
    with their names, one- and three-letter codes, and example DNA codons (the triplet
    DNA bases that code for that amino acid). Adapted from an infographic by [Compound
    Interest](https://oreil.ly/o7Lyq).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With that introduction to protein structure, let’s now look at function—what
    proteins actually do in the cell.
  prefs: []
  type: TYPE_NORMAL
- en: Protein Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Proteins carry out nearly every task required for life: they catalyze chemical
    reactions, transmit signals, transport molecules, provide structural support,
    and regulate gene expression. Because of this diversity, systematically cataloging
    protein functions is a massive undertaking—and one of the most widely used frameworks
    for doing so is the *Gene Ontology* (GO) project.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The GO system organizes protein function into three broad categories, each
    capturing a different aspect of how proteins behave in the cell:'
  prefs: []
  type: TYPE_NORMAL
- en: Biological process
  prefs: []
  type: TYPE_NORMAL
- en: This contributes to—like cell division, response to stress, carbohydrate metabolism,
    or immune signaling.
  prefs: []
  type: TYPE_NORMAL
- en: Molecular function
  prefs: []
  type: TYPE_NORMAL
- en: This describes the specific biochemical activity of the protein itself—such
    as binding to DNA or ATP (a molecule that stores and transfers energy in cells),
    acting as a kinase (an enzyme that attaches a small chemical tag called a phosphate
    group to other molecules to change their activity), or transporting ions across
    membranes.
  prefs: []
  type: TYPE_NORMAL
- en: Cellular component
  prefs: []
  type: TYPE_NORMAL
- en: This indicates where in the cell the protein usually resides—such as the nucleus,
    mitochondria, or extracellular space. Although it’s technically a location label
    and not a function *per se*, it often provides important clues about the protein’s
    role (e.g., proteins in the mitochondria are probably involved in energy production).
    We’ll return to this theme in [Chapter 6](ch06.html#learning-spatial-organization-patterns-within-cells).
  prefs: []
  type: TYPE_NORMAL
- en: Each protein can have multiple GO annotations across these categories. For example,
    a single protein might bind ATP (molecular function), drive muscle contraction
    (biological process), and localize to muscle fibers (cellular component). Some
    annotations are derived from direct experimental assays, while others are inferred
    computationally through similarity to known proteins. In this chapter, we’ll work
    with a curated subset of high-confidence, experimentally validated GO annotations.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting Protein Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Why predict a protein’s function from its sequence? This is actually a fundamental
    challenge in modern biology. Here are a few of the most common and impactful applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Biotechnology and protein engineering
  prefs: []
  type: TYPE_NORMAL
- en: If we can reliably predict function from sequence, we can begin to design new
    proteins with desired properties. This could be useful for designing enzymes for
    industrial chemistry, therapeutic proteins for medicine, or synthetic biology
    components.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding disease mechanisms
  prefs: []
  type: TYPE_NORMAL
- en: Many diseases are caused by specific sequence changes (variants, or mutations)
    that disrupt protein function. A good predictive model can help identify how specific
    mutations alter function, offering insights into disease mechanisms and potential
    therapeutic targets.
  prefs: []
  type: TYPE_NORMAL
- en: Genome annotation
  prefs: []
  type: TYPE_NORMAL
- en: As we continue sequencing the genomes of new species, we’re uncovering vast
    numbers of proteins whose functions remain unknown. For newly identified proteins—especially
    those that are distantly evolutionarily related to any known ones—computational
    prediction is essential for assigning functional hypotheses.
  prefs: []
  type: TYPE_NORMAL
- en: Metagenomics and microbiome analysis
  prefs: []
  type: TYPE_NORMAL
- en: When sequencing entire microbial communities, such as gut bacteria or ocean
    microbiota, many protein-coding genes have no close matches in existing databases.
    Predicting function from sequence helps uncover the roles of these unknown proteins,
    advancing our understanding of microbial ecosystems and their effects on hosts
    or the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Although the task may sound somewhat straightforward—input a sequence, output
    a function—accurate protein function prediction is an extremely challenging problem.
    To succeed, a model must implicitly understand a range of highly complex biological
    principles: how amino acid sequence determines 3D structure (a Nobel Prize–winning
    machine learning problem in its own right), how structure enables function, and
    how these functions operate in the dynamic, crowded environment of the cell.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we won’t aim for state-of-the-art performance. Instead, our
    goal is to build a simple working model and develop intuition for how protein
    sequences can be mapped to functional annotations. Along the way, we’ll introduce
    several useful machine learning techniques—including using pretrained models to
    extract embeddings, visualizing those embeddings, and training lightweight classifiers
    on top of them—that will become recurring tools in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning Primer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve briefly reviewed the biological background of proteins and how their function
    is encoded. Now, we’ll turn to the machine learning techniques that allow us to
    learn from protein sequences in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s hard these days to go anywhere without bumping into *large language models*
    (LLMs). Many recent breakthrough models in AI—such as ChatGPT, Gemini, Claude,
    and Llama—fall under this category. While these models involve immense engineering,
    the fundamental idea behind them is surprisingly simple: they’re trained to predict
    the next token (e.g., a word or character) given the preceding context. There
    are slight variations—such as masked language models, which hide random tokens
    during training to encourage contextual reasoning—but the core principle remains
    the same.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the most surprising discoveries in modern AI has been that if you train
    a large enough model (in terms of the number of parameters) on enough data (in
    terms of total tokens), remarkable capabilities emerge without explicit supervision.
    These models can suddenly summarize text, translate between languages, and even
    generate creative writing like poems and stories—despite never being trained directly
    to do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'This holds promise for biology. In many ways, biology is language-like: DNA
    and proteins are sequences built from discrete alphabets, with complex patterns
    and context-dependent “grammar.” By training language models on massive corpora
    of biological sequences—using the same next-token prediction objective—we should
    be able to learn rich representations of biological information.'
  prefs: []
  type: TYPE_NORMAL
- en: These learned representations can then be used for a wide range of downstream
    tasks, such as predicting a protein’s function, inferring the effects of mutations,
    or identifying structural properties—all without needing to retrain a new model
    from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Later in this chapter, we will explore one of the most successful protein language
    models to date: ESM2.'
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most powerful and versatile outputs of language models is their ability
    to generate *embeddings*. An embedding is a numerical vector—a list of floating-point
    numbers—that encodes the meaning or structure of an entity like a word, sentence,
    or protein sequence. For example, a protein might be represented by an embedding
    such as `[0.1, -0.3, 1.3, 0.9, 0.2]`, which could capture aspects of its biochemical
    or structural properties in a compact numerical form.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings from language models are not just arbitrary numbers—they are structured
    so that similar inputs result in similar embeddings. Related words like `lion`,
    `tiger`, and `panther` cluster together in a linguistic “semantic space.” Likewise,
    protein sequences with similar structure or function—such as collagen I and collagen
    II—will tend to have embeddings that are close together in what we might call
    a “protein space.”
  prefs: []
  type: TYPE_NORMAL
- en: This idea generalizes to the concept of a *latent space*—a continuous, abstract
    space where similar entities are positioned close together based on learned patterns.
    In such spaces, we can perform powerful operations, such as interpolation, clustering,
    and generative design. For proteins, latent spaces can capture functional relationships
    that aren’t apparent from sequence alone—for example, two proteins with very different
    sequences and evolutionary histories may have converged on similar functions and
    therefore appear close together in the latent space. These representations can
    also help predict new functions for uncharacterized proteins by comparing them
    to annotated neighbors in the space.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To identify proteins with similar structure or function, you can compare their
    embeddings using *cosine similarity*—a measure of how aligned two vectors are,
    regardless of their magnitude. This works even when sequences differ significantly
    at the amino acid level. By computing cosine similarities between a query protein
    and a set of known proteins, you can rank the closest matches in embedding space.
    These top hits often share functional roles, structural features, or evolutionary
    history.
  prefs: []
  type: TYPE_NORMAL
- en: Pretraining and Fine-tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many machine learning tasks share underlying structure. Whether your goal is
    detecting hate speech, answering law school entrance questions, or writing poems
    about capybaras, your model first needs a strong foundation in how language works.
    Rather than training from scratch for every task, we typically start from a general-purpose
    model that’s been pretrained on a huge, diverse dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '*Pretraining* gives a model broad knowledge and general capabilities. For a
    specific application, we often follow it with a smaller, focused training step
    called *fine-tuning*, where the model is trained further on a domain-specific
    dataset. This two-stage process is now standard in many areas of machine learning,
    especially as pretrained language models have become increasingly powerful.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this first technical chapter of the book, we’ll take a slightly different
    approach. Rather than fine-tuning the entire pretrained model, we’ll treat it
    as a frozen feature extractor: we’ll use its embeddings as input to a smaller
    classifier that we’ll train from scratch. This strategy is efficient, requires
    little data, and still leverages the rich representations learned by the pretrained
    model. We’ll explore full transfer learning with fine-tuning in later chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: Representations of Proteins and Protein LMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Previously, we discussed what proteins are and how their structure is organized
    hierarchically—from a linear chain of amino acids, to local folding, to the final
    3D form that enables their function. To make this less abstract, let’s load up
    and visualize an example protein structure using the `py3Dmol` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Running this code in our companion Colab notebook will display an interactive
    3D rendering of your chosen protein. A screenshot of the visualization of collagen
    is shown in [Figure 2-3](#collagen-structure).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. A 3D structure of the collagen protein rendered with `py3Dmol`.
    Collagen is a structural protein that forms triple-helical fibers, visible here
    as intertwined ribbonlike strands.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Try viewing the other examples, such as `insulin` and `proteasome`, to appreciate
    the incredible structural diversity of proteins. Their shapes often reflect their
    specialized roles. For example, the long, springy structure of collagen relates
    to its function as a flexible, supportive scaffold found throughout many tissues
    in the body.
  prefs: []
  type: TYPE_NORMAL
- en: Numerical Representation of a Protein
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While 3D visualizations are useful for exploration, machine learning models
    require numerical input. To analyze or model proteins with machine learning techniques,
    we typically start from their 1D amino acid sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Protein sequences for most known organisms can be retrieved from public databases
    such as [Uniprot](https://oreil.ly/9OqAK). For example, here’s the amino acid
    sequence of human insulin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This sequence representation is easy to store and manipulate, but it still needs
    to be converted to a numerical format before it can be used by machine learning
    models.
  prefs: []
  type: TYPE_NORMAL
- en: One-Hot Encoding of a Protein Sequence
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The simplest way to convert a protein sequence into numerical form is with
    *one-hot encoding*. Here is how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: There are 20 standard amino acids.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each amino acid is represented by a binary vector of length 20, where only one
    position is 1 (indicating the identity of that amino acid) and all other positions
    are `0`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A protein sequence is then converted into a sequence of these one-hot vectors—one
    for each amino acid.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s walk through a toy example: encoding the short protein `MALWN` (the first
    five amino acids of the insulin precursor protein).'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s define the mapping between an amino acid letter code to an integer
    index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Given a protein sequence, we can convert it to a sequence of integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: And given a sequence of integers, we can convert it into a one-hot encoding
    (see [Figure 2-4](#protein-one-hot-encoding)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. One-hot encoding converts a protein’s amino acid sequence into
    a binary matrix where each row corresponds to one amino acid and each column to
    a possible residue. Most values are zero, with a single “1” indicating the presence
    of a specific amino acid at each position.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In [Figure 2-4](#protein-one-hot-encoding), we see that:'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting matrix has the shape `[5, 20]`, where each of the five rows corresponds
    to one amino acid in the sequence, and each column represents one of the 20 standard
    amino acids.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each row contains all zeros except for a single 1 in the position corresponding
    to that amino acid’s identity, preserving its categorical nature without implying
    any numerical ordering or similarity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Why not just skip the one-hot encoding step and use amino acid indices directly?
  prefs: []
  type: TYPE_NORMAL
- en: The issue is that numeric indices (like 3 versus 17) imply an artificial order
    and relative similarity, even though amino acids are categorical entities without
    meaningful numerical relationships.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding avoids this by assigning each amino acid a distinct binary
    vector—ensuring that the model treats them as equally separate and avoids inferring
    nonexistent patterns from arbitrary index values.
  prefs: []
  type: TYPE_NORMAL
- en: 'In code, we can use the handy `jax.nn.one_hot` utility from the JAX library
    to get this embedding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize the resulting one-hot encoding matrix as a heatmap as in [Figure 2-5](#one-hot-matrix-visualized)
    (essentially re-creating the earlier [Figure 2-4](#protein-one-hot-encoding)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. One-hot encoded representation of a toy protein sequence (`MALWM`),
    visualized with a heatmap. This binary matrix encodes the identity of each residue
    without implying any similarity between them.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that we’ve constructed a basic numerical representation of a protein, we’re
    ready to move beyond this simplistic format and explore *learned embeddings*—dense
    vector representations that encode much more biological meaning about each amino
    acid.
  prefs: []
  type: TYPE_NORMAL
- en: Learned Embeddings of Amino Acids
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the rest of this chapter, we’ll use a pretrained protein language model called
    [ESM2](https://oreil.ly/iZmXA), released by Meta in 2023 (ESM stands for *evolutionary
    scale modeling*). These models are hosted on the [Hugging Face platform](https://huggingface.co).
    If you haven’t encountered it yet, Hugging Face is a fantastic resource with [thousands
    of pretrained models](https://huggingface.co/models) ready for you to use and
    explore.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll explore how the ESM2 model works in more detail shortly, but first, let’s
    examine how it represents individual amino acids. We’ll access the model using
    the Hugging Face transformers library. ESM2 is based on the *transformer* neural
    network architecture [introduced in 2017](https://oreil.ly/XPvFW), which has become
    the standard for modeling sequences like text and proteins.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Ideally, we’d load the ESM2 model using JAX/Flax, but it’s only officially available
    in PyTorch at the moment. In practice, being comfortable with multiple deep learning
    frameworks is useful—so here we’ll use PyTorch to load the model and extract embeddings,
    which we’ll then process and build on top of using JAX.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the book will use JAX/Flax exclusively, but this brief mixing of
    frameworks is a good example of how flexible real-world workflows can be.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check the model’s token-to-index mapping:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This is similar to the manual amino acid indexing we did earlier, but it includes
    special tokens like `<unk>` for unknown residues, `<eos>` for end-of-sequence,
    and rare amino acids like `U` (selenocysteine) and `O` (pyrrolysine).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the ESM2 tokenizer to encode our tiny protein sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If desired, we can drop the special start (`<cls>`) and end (`<eos>`) tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’ll extract the learned token embeddings from the model using `model.get_input_embeddings()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Each of the 33 possible tokens is embedded into a 1,280-dimensional space.
    While humans can’t visualize such high-dimensional spaces directly, we can apply
    dimensionality reduction techniques like t-SNE or UMAP to project the embeddings
    down to two dimensions. This allows us to inspect how the model organizes different
    tokens in a more interpretable form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the t-SNE–transformed array has shape `(33, 2)`, meaning that
    each of the 33 tokens has been projected into a 2D space. [Figure 2-6](#tsne-no-chemical-properties)
    shows a scatterplot of these points, giving us a visual sense of how the model
    organizes token embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. A 2D t-SNE projection of the learned token embeddings from the
    ESM2 model. Even without labels, clusters begin to emerge—hinting that the model
    has learned to organize tokens in a meaningful way.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To sanity-check whether similar types of tokens cluster in the 2D embedding
    space, we can label each token using known amino acid properties (like those shown
    earlier in the chapter) and replot the t-SNE projection in [Figure 2-7](#tsne-with-chemical-properties):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0207.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-7\. Coloring the t-SNE projection by amino acid properties reveals
    clear clusters of amino acids with similar biochemical roles that tend to group
    together in embedding space, reflecting the model’s ability to capture meaningful
    biological structure. Technical non–amino acid tokens also group together in this
    latent space.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tokens with similar biochemical properties tend to cluster together. For instance,
    hydrophobic amino acids like `F`, `Y`, and `W` group in the upper right, while
    special-purpose tokens such as `<cls>` and `<eos>` appear together on the left
    side of the plot. This structure suggests that the model has learned meaningful
    distinctions among amino acids based on the roles they play within protein sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve explored what these token embeddings look like, let’s dive into
    how the ESM2 model actually works—and how it learns such representations in the
    first place.
  prefs: []
  type: TYPE_NORMAL
- en: The ESM2 Protein Language Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that you’re more familiar with token embeddings, let’s discuss how the
    ESM2 model actually works. ESM2 is a *masked language model* (MLM), which means
    it was trained by repeatedly masking a random subset of amino acids in each protein
    sequence and asking the model to predict them. In the case of ESM2, a randomly
    selected 15% of the amino acids in each sequence were masked during training.
    [Figure 2-8](#language-model-training) illustrates this visually, comparing it
    to masked language modeling in natural language tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/dlfb_0208.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2-8\. A comparison between masked language modeling in natural and protein
    language models. In natural language, models are trained to predict missing words
    (or sometimes subwords) from surrounding context. Protein language models use
    the same principle: randomly masking amino acids in a sequence and training the
    model to predict them from the surrounding context.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s try masking one amino acid in the insulin protein sequence and see whether
    the model can predict it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The `<mask>` token tells the model to predict the amino acid at that position.
    To do this, we load the full language model, `EsmForMaskedLM`, which includes
    the language prediction head.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To accelerate inference, we’ll use a smaller ESM2 model variant (150M parameters
    with 640-dimensional embeddings) rather than the large, 650M model with 1,280-dimensional
    embeddings used earlier. This is a good reminder that many models on Hugging Face
    come in different sizes, and swapping between them is often as simple as changing
    a model checkpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, there’s a trade-off—smaller models may capture less information and
    typically perform worse on complex tasks. Still, they’re great for rapid prototyping
    and exploring model behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'We load up the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: And we’ll run it to get predictions for the masked token. We see that the model
    correctly predicts the token `L` (leucine) with very high probability in [Figure 2-9](#predict-missing-aa).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0209.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-9\. Model prediction for a masked leucine (`L`) in the insulin sequence.
    The model confidently predicts the correct amino acid (`L`) with high probability,
    showing that it has learned common sequence patterns in proteins.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s rewrite this code as a more general form as `MaskPredictor`, with methods
    that mask a sequence, make a prediction, and plot the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s try it on a different position—index 26, where the correct amino acid
    is `N` (asparagine). The result is shown in [Figure 2-10](#predict-missing-aa-less-clear):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0210.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-10\. Model prediction for a masked asparagine (`N`) in the insulin
    sequence. Here, the model is more uncertain—it assigns moderate probability to
    several possible amino acids, indicating that this position is harder to predict
    based on surrounding context.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this case, the model doesn’t strongly prefer any one amino acid. It assigns
    moderate probability to several, including `A`, `T`, and `S` (with the model assigning
    the true amino acid `N` a fairly low probability). This uncertainty could reflect
    the biochemical flexibility of that position—some regions of proteins can tolerate
    different residues due to redundancy, structural flexibility, or lack of strict
    functional constraints. These are often called “permissive” positions and are
    common in disordered (unstructured) or surface regions of proteins.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example illustrates that the model has learned and understands the probabilistic
    grammar of proteins. The next question is: how can we leverage this understanding
    to represent an entire protein, and not just one amino acid at a time?'
  prefs: []
  type: TYPE_NORMAL
- en: Strategies for Extracting an Embedding for an Entire Protein
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve explored how the ESM2 model represents individual amino acids.
    But many downstream tasks—like predicting protein function—require a fixed-length
    representation for the entire protein sequence. How can we convert a variable-length
    sequence of amino acids into a single embedding vector that captures the protein’s
    overall structure and meaning?
  prefs: []
  type: TYPE_NORMAL
- en: 'Several strategies are commonly used:'
  prefs: []
  type: TYPE_NORMAL
- en: Concatenation of amino acid embeddings
  prefs: []
  type: TYPE_NORMAL
- en: 'One simple approach is to loop through each amino acid in a sequence, extract
    its embedding, and concatenate them into one long vector. For example, if a protein
    has length 10 and each amino acid has a 640-dimensional embedding, this yields
    a protein embedding of length `10 × 640 = 6400`. While this preserves fine-grained
    information of each amino acid, it has several drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: Variable length
  prefs: []
  type: TYPE_NORMAL
- en: Different proteins will yield different-length embeddings, which complicates
    model input formatting.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs: []
  type: TYPE_NORMAL
- en: Long proteins produce huge embeddings. For example, titin—the longest known
    human protein at ~34,000 amino acids—would produce an embedding with over *43
    million* values. That’s unwieldy for most models.
  prefs: []
  type: TYPE_NORMAL
- en: Limited modeling
  prefs: []
  type: TYPE_NORMAL
- en: This approach treats amino acids independently, ignoring the contextual relationships
    that are central to protein function.
  prefs: []
  type: TYPE_NORMAL
- en: Averaging of amino acid embeddings
  prefs: []
  type: TYPE_NORMAL
- en: A more compact approach is to average the token embeddings across the sequence.
    Using the same example of a length-10 protein with 640-dim embeddings, we take
    the mean across all 10 embeddings to produce a final 640-dimensional vector.
  prefs: []
  type: TYPE_NORMAL
- en: This has the advantage of producing fixed-size vectors, regardless of protein
    length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It’s efficient and sometimes used, but also crude—averaging discards ordering
    and interaction information. It’s like summarizing a novel by averaging all its
    word vectors: some meaning survives, but the nuance is lost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the model’s contextual sequence embeddings
  prefs: []
  type: TYPE_NORMAL
- en: A more principled approach is to extract the hidden representations for the
    entire sequence directly from the language model. Since ESM2 is trained to predict
    masked tokens based on their surrounding context, its internal layers encode rich,
    contextualized embeddings for every amino acid in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Concretely, we can pass a protein sequence through ESM2 and extract the final
    hidden layer activations, resulting in a tensor of shape (`L', D`), where `L'`
    is the number of output tokens (which may differ from the input length `L`), and
    `D` is the model’s hidden size (e.g., 640).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We then apply mean pooling across the sequence length to produce a fixed-length
    embedding of shape (`D,`). While averaging may seem simplistic, it often works
    surprisingly well—because the model has already integrated contextual information
    into each token’s representation using self-attention, the pooled vector still
    captures meaningful dependencies across the sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This final approach is the most common and powerful in practice—and it’s the
    one we’ll explore in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Extracellular Versus Membrane Protein Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ll introduce the GO dataset properly in the next section on protein function
    prediction. For now, let’s use it to associate each UniProt protein accession
    and sequence with its known cellular location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: For each protein sequence identified by an `EntryID`, the `term` column provides
    its GO annotation for cellular localization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s focus on two specific locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`extracellular` (GO:0005576)'
  prefs: []
  type: TYPE_NORMAL
- en: Proteins secreted outside the cell, often involved in signaling, immune response,
    or structural roles
  prefs: []
  type: TYPE_NORMAL
- en: '`membrane` (GO:0016020)'
  prefs: []
  type: TYPE_NORMAL
- en: Proteins embedded in or associated with cell membranes, frequently functioning
    in transport, signaling, or cell–cell interaction
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll filter the dataset to proteins annotated with only one of these two locations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll now extract embeddings from these sequences. The function `get_mean_embeddings`
    computes the mean hidden state across each sequence, summarizing the model’s representation
    of protein sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’ll extract embeddings using a smaller ESM2 model, which produces 320-dimensional
    representations and requires significantly less memory than larger variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We then calculate the embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Each set of 20 sampled proteins is now represented as a `(20, 320)` embedding
    matrix. This means that for each sequence—regardless of its original length—we
    obtain a fixed-size vector of 320 dimensions. These vectors correspond to the
    mean of the final hidden layer activations across all tokens in the sequence,
    and should capture some information about the overall protein structure.
  prefs: []
  type: TYPE_NORMAL
- en: To visualize how these embeddings might relate to protein localization, we project
    them into two dimensions using t-SNE, a common method for visualizing high-dimensional
    data. [Figure 2-11](#membrane-protein-embeddings) shows that the extracellular
    and membrane proteins tend to form distinct clusters in this space.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0211.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-11\. Two-dimensional t-SNE projection of the 320-dimensional embeddings
    from a small ESM2 model. Even with this lightweight model, we observe a tendency
    for extracellular and membrane proteins to form separate clusters, suggesting
    that the embeddings contain information relevant to cellular localization.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'While the separation isn’t perfect, there’s a clear trend: extracellular proteins
    tend to cluster in a different region of embedding space than membrane proteins.
    It’s quite striking that the model picks up on this purely from sequence. This
    suggests that the learned embeddings reflect biologically meaningful patterns—even
    without any explicit supervision for cellular location.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this initial exploration complete, we now turn to the central machine
    learning task of this chapter: predicting protein function. Let’s begin by preparing
    the dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many machine learning books and blog posts jump straight into the exciting parts—training
    and evaluating models—as soon as possible. But in practice, training is often
    a small fraction of the overall workflow. A significant portion of time is spent
    understanding, cleaning, and structuring the data. And when things go wrong with
    a model, the root cause is often found in the data. So, rather than handing you
    a polished CSV from the ether, we’ll walk through the data preparation process
    step-by-step—starting from real-world resources and working through the steps
    needed to turn them into something a model can use.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal is to fine-tune a model to predict protein function from sequence,
    which means assembling a dataset of `(protein_sequence, protein_function)` pairs.
    Fortunately, biologists have developed systematic frameworks for defining protein
    functions, and curated datasets already exist. One of the most widely used resources
    is the [CAFA (Critical Assessment of Functional Annotation)](https://oreil.ly/87EN_)
    challenge, a community-driven competition where teams build models to predict
    protein function. We’ll use CAFA data as our raw material, but we’ll still need
    to process and structure it ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you’re familiar with AlphaFold and protein structure prediction, you may
    have heard of the similarly named CASP (Critical Assessment of Structure Prediction),
    which plays a similar role in the protein structure community. Public benchmarks
    like these have been instrumental in driving progress across a wide range of computational
    biology problems.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now explore the CAFA dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the CAFA3 Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There have been several rounds of CAFA, but the CAFA3 dataset is the most recent
    publicly available one. We first downloaded the “CAFA3 Targets” and “CAFA3 Training
    Data” files from the [CAFA website](https://oreil.ly/87EN_). Let’s start by loading
    the label file, which tells us the functional annotations for each protein:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This dataframe contains three columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`EntryID`'
  prefs: []
  type: TYPE_NORMAL
- en: The UniProt ID of the protein
  prefs: []
  type: TYPE_NORMAL
- en: '`term`'
  prefs: []
  type: TYPE_NORMAL
- en: A GO accession code describing a specific protein function
  prefs: []
  type: TYPE_NORMAL
- en: '`aspect`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The GO category the function belongs to; one of three types of function described
    in the introduction: biological process (BPO), molecular function (MFO), and cellular
    component (CCO)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `term` column contains only GO accession codes. To make these more interpretable,
    we’d ideally like to know their corresponding human-readable descriptions. This
    information isn’t included directly in the CAFA files, but it is available via
    the [Gene Ontology downloads page](https://oreil.ly/uNhm2). The ontology is stored
    in graph format as a `.obo` file, and we can use the `obonet` Python library to
    parse it. Here’s how we retrieve the term descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The function will load the annotations from a local file if it already exists,
    or download and cache them if not:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then merge the human-readable term descriptions back onto the labels
    dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: In this chapter, we’ll focus specifically on molecular functions (`MFO`)—that
    is, what a protein does at the biochemical level. Later, you may want to extend
    this chapter’s approach to include the other two GO categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at which molecular functions are most commonly annotated
    in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: We can already see that the distribution of function annotations is highly skewed.
    Some terms—like `molecular_function`, `binding`, and `protein binding`—appear
    tens of thousands of times, while others occur only once. Labels like `molecular_function`
    are arguably overly generic and provide little meaningful information, making
    them unhelpful for machine learning. We’ll filter these out in a later step.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s load the protein sequences associated with each protein ID. This
    information is stored in the file *train_sequences.fasta*, a standard format for
    representing biological sequences such as proteins and DNA. We can use BioPython’s
    `SeqIO` module to parse the *.fasta* file into a format we can work with.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A quick aside: no one starts out knowing what BioPython’s `SeqIO` module is,
    or how *.fasta* files work, or what GO annotations mean—and that’s completely
    normal. Working at the intersection of biology and machine learning means constantly
    encountering new tools and terminology. Frequent looking up of new terms and tools
    is not just OK, it’s expected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll convert the *.fasta* sequences into a pandas dataframe to make them easier
    to manipulate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We’ve also computed the length of each sequence, since protein lengths can vary
    widely and this information will be useful later when filtering data.
  prefs: []
  type: TYPE_NORMAL
- en: 'One important detail: the CAFA dataset includes proteins from many different
    organisms. To isolate human proteins, we’ll use the associated taxonomy file provided
    in the download:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This file contains a taxonomy ID (`taxonomyID`) for each protein, based on
    NCBI’s organism classification system. We’ll merge this onto our sequence dataframe
    and keep only proteins with `taxonomyID == 9606`, which corresponds to *Homo sapiens*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s get an overview of the number of unique proteins and molecular function
    terms in our filtered dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s also take a look at the resulting `sequence_df` after merging in the
    function labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'From this table, we can already see that many proteins are associated with
    multiple molecular functions. To quantify this, we examine the distribution of
    the number of functions per protein in [Figure 2-12](#functions-per-protein):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0212.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-12\. Distribution of the number of molecular functions annotated per
    protein. The y-axis is shown on a logarithmic scale to make rare cases more visible.
    While most proteins have fewer than 20 annotated functions, a small number of
    proteins are associated with more than 50 distinct molecular roles.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This pattern reflects a complex biological reality: while many proteins carry
    out a single, well-defined function, others are involved in a wide variety of
    molecular roles. For example, some proteins act as enzymes, bind to other molecules,
    and participate in multiple pathways. From a machine learning perspective, this
    means our model must be able to assign multiple function labels to a single protein
    and also cope with the fact that some labels are much rarer than others.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now take a closer look at the most frequent molecular function labels.
    Some terms are so broad and universally assigned that they offer little meaningful
    insight. For example, `molecular function` applies to nearly all proteins, `binding`
    covers 93%, and `protein binding` appears in 89% of cases. These labels will tend
    to dominate the loss during training and can cause the model to fixate on predicting
    them at the expense of more meaningful functions. As a dataset preprocessing step,
    we’ll explicitly remove these overly generic terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'On the opposite end of the spectrum, some molecular functions are extremely
    rare—for example, `GO:0099609` (microtubule lateral binding) appears only once.
    To learn meaningful associations, our model needs enough training examples per
    function. So we’ll filter out the rarest labels and keep only those that appear
    in at least 50 proteins:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: This gives us a cleaner set of function labels that are more amenable to learning.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Thresholds used during data processing—like how many times a label must appear
    to be included—are somewhat arbitrary, but they can significantly affect model
    performance. These decisions are effectively hyperparameters and should be tuned
    based on the specific task, dataset size, and model capacity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we’ll reshape the dataframe so that each row corresponds to one protein,
    and each column corresponds to a molecular function label. We’ll use the `pivot`
    function in pandas to create this multilabel format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Great—this dataset is now in a format that’s almost ready for machine learning.
    Before we move on, let’s run a few final sanity checks.
  prefs: []
  type: TYPE_NORMAL
- en: First, how many unique proteins do we have?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: This number is in the right ballpark. There are roughly 21,000 protein-coding
    genes in the human genome, and since we applied several filtering steps, we expect
    a somewhat smaller number. It’s always worth keeping rough order-of-magnitude
    expectations in mind—if we saw 1,000 or 1,000,000 here, we’d suspect something
    was off.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s check whether any protein sequences are duplicated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'It seems that a few protein sequences are repeated. For example, the entries
    `P0DP23`, `P0DP24`, and `P0DP25` all share the same sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: These seem to be legitimate biological duplicates—proteins with different Uniprot
    identifiers but identical sequences—so we’ll keep them in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have a final dataset linking 10,709 human proteins to one
    or more of 303 molecular functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since our simple mean embedding approach can be quite memory intensive, we’ll
    filter the dataset to include only proteins with a maximum length of 500 amino
    acids. This helps avoid out-of-memory errors during model inference and training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: This roughly halves the dataset, which is perfectly fine for initial prototyping.
    You can always remove this constraint later if time and memory allow.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a clean and compact dataset, let’s process it further for compatibility
    with machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting the Dataset into Subsets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will split our dataset into three distinct subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: Training set
  prefs: []
  type: TYPE_NORMAL
- en: Used to fit the model. The model sees this data during training and uses it
    to learn patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Validation set
  prefs: []
  type: TYPE_NORMAL
- en: Used to evaluate the model’s performance during development. We use this to
    tune hyperparameters and compare model variants.
  prefs: []
  type: TYPE_NORMAL
- en: Test set
  prefs: []
  type: TYPE_NORMAL
- en: Used only once, for final evaluation. Crucially, we avoid using this data to
    guide model design decisions. It serves as our best estimate of how well the model
    would generalize to completely unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll split the proteins by their `EntryID`, ensuring that each protein appears
    in only one subset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we’ll extract the rows for each split from our dataframe `sequence_df`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: This gives us clean, nonoverlapping training, validation, and test sets—each
    containing a subset of proteins we’ll use throughout model development and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Converting Protein Sequences into Their Mean Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will now convert the sequences from each dataset split into their corresponding
    mean embeddings, just as we did earlier. Since this step can be time-consuming—especially
    with larger models—it’s worth thinking about how to do it efficiently. Using a
    GPU can significantly speed up computation, but we can also avoid repeating work
    by computing the embeddings only once, storing them to disk, and loading them
    later.
  prefs: []
  type: TYPE_NORMAL
- en: 'To make this process more convenient, we’ll use a pair of helper functions
    to store and load sequence embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s use the more powerful (but computationally expensive) ESM2 model with
    640-dimensional embeddings and store the embeddings for each split using the `store_sequence_embeddings`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the embeddings are stored, we can load them back into memory whenever
    needed. Here’s a glimpse of the resulting training dataset that the model will
    learn from:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: You’ll notice a series of columns labeled `ME:1` through `ME:640`. These represent
    the mean-pooled hidden states from the final layer of the ESM2 model—effectively
    a fixed-length numerical summary of each protein sequence. These embeddings capture
    biochemical and structural information learned during pretraining and will serve
    as the input features for our classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataframe becomes the input to a `convert_to_tfds` function, which we’ve
    defined to make it easier to prepare the datasets for each split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now use our `convert_to_tfds` function to build a TensorFlow-compatible
    dataset from the training DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Fetching a batch of data from these datasets is straightforward. We just batch
    the dataset, convert it to a NumPy iterator, and retrieve a batch by calling `next`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: These shapes confirm that each input is a 640-dimensional embedding vector (from
    the ESM2 model), and each target is a 303-dimensional binary vector representing
    the presence or absence of each molecular function label.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Because the training dataset includes `.repeat()`, it yields batches indefinitely
    by looping over the data. This is useful for training, where we want to cycle
    through the dataset multiple times. In contrast, the validation and test datasets
    are not repeated—so their batches will eventually be exhausted, which is exactly
    what we want during evaluation, where each example should be seen only once.
  prefs: []
  type: TYPE_NORMAL
- en: 'To streamline the dataset setup, we’ve wrapped the entire pipeline into a single
    helper function, `build_dataset`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'This function loads the saved mean embeddings from disk for all three splits
    and constructs `tf.data.Dataset` objects that are ready for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: With this, we now have our data fully preprocessed and ready to use in training
    a model.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now train a simple [Flax](https://oreil.ly/MjH5C) linear model on top
    of the mean protein embeddings. Recall that each protein sequence has a variable
    length, but we’ve already transformed them into fixed-size embeddings. Our goal
    is to predict which of the 303 possible molecular functions each protein performs.
    This is a *multilabel classification* problem, meaning each protein may be associated
    with several function labels simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this setup, we’ll train a lightweight MLP (multilayer perceptron)—a stack
    of dense layers with nonlinearities. Importantly, we are not fine-tuning the original
    ESM2 model: it remains frozen, and our model simply learns on top of its embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the model code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Some notes on this very lightweight model:'
  prefs: []
  type: TYPE_NORMAL
- en: It uses `nn.Sequential` to stack layers, which keeps the definition clean and
    readable for this simple model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use a GELU (Gaussian Error Linear Unit) activation function, which is a smooth,
    nonlinear alternative to ReLU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final layer is an `nn.Dense` layer projecting to the number of function
    labels (`num_targets`). It returns logits, not probabilities—so we’ll apply a
    suitable activation (like sigmoid) inside the loss function to convert these logits
    into predicted probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This model is frozen on top of the ESM2 embeddings—meaning it does not update
    the transformer weights. It learns only to map fixed embeddings to functional
    labels. This is efficient and interpretable, and it reduces memory usage during
    training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may also have noticed that we attached a convenience function, `create_train_state`,
    to the model class for creating a training state. This encapsulates model initialization,
    parameter registration, and optimizer setup into a single `TrainState` object.
    It’s particularly useful because it allows us to construct the training state
    right when everything needed—the model, dummy input for shape inference, and optimizer
    config—is readily available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s instantiate the model with the correct number of output targets, based
    on how many GO term columns we have in the training dataframe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: This model is now ready to be trained to predict which molecular functions a
    protein is involved in, using the precomputed embeddings as input.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Training Loop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the model and dataset ready, we can now define a function to perform a
    single training step. This step includes:'
  prefs: []
  type: TYPE_NORMAL
- en: A forward pass through the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computing the loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating the model parameters using those gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s how we implement it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'In this setup:'
  prefs: []
  type: TYPE_NORMAL
- en: We use a sigmoid activation and binary cross-entropy loss, appropriate for multilabel
    classification. The logits go through a sigmoid activation, not softmax—because
    we want independent yes/no predictions for each possible protein function. Remember
    that each protein could have many functions at once.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`@jax.jit` compiles the training step for better performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, let’s implement some metrics to evaluate how well the model is doing
    beyond the loss alone, using tools from `sklearn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll track the following evaluation metrics for each function label:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs: []
  type: TYPE_NORMAL
- en: The fraction of correct predictions across all labels. In multilabel classification
    with imbalanced data (like this), accuracy can be misleading—most labels are zero,
    so a model that always predicts “no function” would appear accurate. Still, it’s
    an intuitive metric and we’ll include it for now.
  prefs: []
  type: TYPE_NORMAL
- en: Recall
  prefs: []
  type: TYPE_NORMAL
- en: The proportion of actual function labels the model correctly predicted (i.e.,
    true positives/all actual positives). High recall means the model doesn’t miss
    many true functions.
  prefs: []
  type: TYPE_NORMAL
- en: Precision
  prefs: []
  type: TYPE_NORMAL
- en: The proportion of predicted function labels that are correct (i.e., true positives/all
    predicted positives). High precision means the model avoids false alarms.
  prefs: []
  type: TYPE_NORMAL
- en: Area under the precision-recall curve (auPRC)
  prefs: []
  type: TYPE_NORMAL
- en: Summarizes the tradeoff between precision and recall at different thresholds.
    Particularly useful in highly imbalanced settings like this one.
  prefs: []
  type: TYPE_NORMAL
- en: Area under the receiver operating characteristic curve (auROC)
  prefs: []
  type: TYPE_NORMAL
- en: Measures the model’s ability to distinguish positive from negative examples
    across all thresholds. While it’s a standard metric of discrimination ability,
    it can sometimes be misleading in highly imbalanced datasets, as it gives equal
    weight to both classes.
  prefs: []
  type: TYPE_NORMAL
- en: In a multilabel setting, we calculate these metrics for each protein function
    (i.e., per target/label), then average them to get a holistic view of model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply these metrics calculations during the evaluation step `eval_step`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The evaluation computes metrics per protein in the batch. For each protein,
    we:'
  prefs: []
  type: TYPE_NORMAL
- en: Apply sigmoid to its 303 logits to get function probabilities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threshold those probabilities (e.g., at 0.5) to get binary predictions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare these to the true function labels to compute metrics like accuracy,
    precision, recall, auPRC, and auROC.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We repeat this for every protein in the batch and then average the resulting
    metrics across proteins. This tells us how well the model predicts sets of functions
    per protein. It does not report performance per GO term. If we wanted per-function
    metrics (e.g., how well the model predicts `GO:0003677`), we’d need to compute
    metrics column-wise instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chunk of code, everything comes together into a `train` function,
    and variations of this basic setup will be repeated in every chapter. We have
    the training loop where we first initialize our model training state and then
    loop over the dataset in batches to train the model and evaluate it every so often:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'A few notes on this training loop:'
  prefs: []
  type: TYPE_NORMAL
- en: Efficient batch sampling
  prefs: []
  type: TYPE_NORMAL
- en: Training data is streamed via `.as_numpy_iterator()`, and the `.repeat()` in
    the dataset ensures infinite looping over the data.
  prefs: []
  type: TYPE_NORMAL
- en: Regular evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Every `eval_every` step, the model is evaluated on the full validation set to
    monitor progress using metrics we defined previously, like auPRC and auROC.
  prefs: []
  type: TYPE_NORMAL
- en: Metric aggregation
  prefs: []
  type: TYPE_NORMAL
- en: Validation metrics are computed batch-wise and then averaged across all batches
    using `pd.DataFrame(...).mean(axis=0)`. This gives a stable estimate of performance
    across the entire validation set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now train the model. But first, a quick trick: to avoid unnecessarily
    repeating training from scratch every time you rerun your code cell, we use the
    `@restorable` decorator. This lightweight utility checks whether a trained model
    already exists at a specified path. If it does, it:'
  prefs: []
  type: TYPE_NORMAL
- en: Skips retraining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restores the model into a valid `TrainState`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Returns the model along with any saved metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This makes your workflow much faster and more reproducible, especially during
    iterative development and debugging. Let’s take a look at how this is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: Some additional parameters worth mentioning are the optimizer (here, `optax.adam`)
    and the total number of training steps (`num_steps`). Given that we have 2,100
    training examples and a batch size of 32, it will take about 66 steps for the
    model to see the entire training set once. Setting `num_steps=300` means the model
    will see each training data point several times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having trained the model with the previous `train` call, we can now evaluate
    its training dynamics and performance on the validation set, as shown in [Figure 2-13](#mlp-model-eval):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0213.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-13\. Training and evaluation of the MLP model over 300 steps. On the
    left, loss curves for the training and validation splits show rapid convergence,
    with stability reached after ~30 steps. On the right, auPRC, precision, and recall
    improve gradually. Accuracy and auROC metrics are very high due to class imbalance
    and are not very informative for this problem.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the left panel, we observe that both training and validation loss drop sharply
    within the first ~30 steps and then stabilize. This is a typical learning curve,
    indicating rapid convergence without substantial instability (e.g., no major spikes
    or divergence). It suggests that the model—a shallow MLP operating on top of frozen
    pretrained embeddings—quickly captures the low-hanging signal in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the right panel, we track several evaluation metrics over time:'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy and auROC start high and remain flat, but these can be misleading in
    imbalanced, multilabel settings like this one. Since most function labels are
    negative (i.e., a protein lacks the majority of all possible functions), a model
    that mostly predicts zeros can still achieve a high score on these metrics. For
    that reason, we don’t put much weight on these metrics in this context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: auPRC steadily improves and does not fully plateau, suggesting the model continues
    to learn subtle distinctions and could potentially benefit from further training
    (i.e., by increasing `num_steps`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision improves more quickly than recall, indicating the model becomes increasingly
    confident in its predictions but still fails to capture some true positives.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Together, these trends indicate that while most of the learning happens early
    on, there may still be headroom—particularly in recall and auPRC—if training were
    extended further or if a more powerful architecture were used.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It can be slightly tedious to manually log metrics inside every training loop
    and then hook up custom plotting code to visualize them. To streamline this, later
    chapters introduce a MetricsLogger (for capturing values) and MetricsPlotter (for
    rendering them).
  prefs: []
  type: TYPE_NORMAL
- en: 'Beyond that, many modern machine learning workflows use hosted (or self-hosted)
    dashboards to automatically collect, store, and display metrics in real time.
    These tools help monitor experiments, compare training runs, and share results
    across teams. We encourage you to check them out. Popular options include:'
  prefs: []
  type: TYPE_NORMAL
- en: '[TensorBoard](https://oreil.ly/tSPIP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Weights & Biases (W&B)](https://oreil.ly/Loybs)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[MLflow](https://oreil.ly/A1faJ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s great to see the model training successfully and loss and metrics curves
    trending in the right direction—but that’s just the beginning. The real insight
    comes from analyzing the model’s predictions, understanding where it performs
    well, and identifying its limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Examining the Model Predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With a trained model in hand, it’s time to explore its strengths and weaknesses.
    We’ll start by generating predictions for the entire validation set and storing
    them in a dataframe for easier inspection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'To get a high-level sense of how the model is performing, we can visualize
    the full prediction matrix as a heatmap. In [Figure 2-14](#predicted-functional-annotation),
    we plot two side-by-side heatmaps: one showing the true protein-function annotations
    (left) and the other showing the model’s predicted probabilities (right). Each
    column corresponds to a protein function, and each row to a protein:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0214.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-14\. Heatmap overview of protein function prediction. The left panel
    shows the ground truth functional annotations for each protein in the validation
    set, while the right panel shows the model’s predicted probabilities. Both matrices
    are sparse, with vertical bands reflecting common function labels.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This visualization is quite zoomed out and high level, but it helps build intuition
    about overall model behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: Some protein functions appear frequently in the dataset (visible as vertical
    stripes), and the model tends to predict these relatively well.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rare functions are harder to capture—the model often misses them entirely, leading
    to sparse or empty columns in the predicted heatmap.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A few functions are over-predicted, visible as faint vertical lines across many
    proteins, suggesting the model is overly confident for those categories.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many cells in the predicted matrix show intermediate color tones, which reflect
    more uncertain probabilities (not a confident near-0 or near-1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We’ll now shift from this qualitative view to a quantitative one by evaluating
    model performance on each protein function individually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'This analysis reveals substantial variation in model performance across protein
    functions. For instance, the model performs well on functions like `GO:0004930`
    (G protein–coupled receptor activity), but it struggles with others, such as `GO:0003774`
    (cytoskeletal motor activity). However, interpreting these results requires caution:
    some metrics may be based on very few validation examples, and performance is
    naturally limited for functions that are underrepresented during training. A high
    score on a frequent function may simply reflect ample training data, while low
    scores on rare functions may be expected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a closer look at whether there’s a relationship between how often
    a protein function appears in the training data and how well the model learns
    to predict it in the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'At a glance, it seems that functions with higher predictive performance (e.g.,
    higher auPRC) also tend to have more training examples. In [Figure 2-15](#auprc-over-train-n),
    we visualize this relationship more clearly with a scatterplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0215.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-15\. Relationship between training frequency and predictive performance
    (auPRC) across protein functions. Commonly observed functions in the training
    set tend to be predicted more accurately by the model.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This plot shows a clear trend: protein functions that occur more frequently
    in the training set tend to be predicted more accurately by the model on the validation
    set (as measured by auPRC). This aligns with expectations—machine learning models
    usually perform better on well-represented classes. It also highlights the challenge
    of class imbalance: rare functions are often poorly predicted, not necessarily
    due to biological complexity but because the model has limited data to learn from.'
  prefs: []
  type: TYPE_NORMAL
- en: But how do we know whether a specific auPRC score is actually good? An auPRC
    value of, say, 0.8 for a certain protein function might sound promising—but is
    that better than chance? Is it meaningful? To interpret these scores, we need
    something to compare them against.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating Model Usefulness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To ground our evaluation, we’ll compare our model against two simple baselines:'
  prefs: []
  type: TYPE_NORMAL
- en: Coin flip
  prefs: []
  type: TYPE_NORMAL
- en: For each protein function, randomly predict 0 or 1 with equal probability. This
    gives us a baseline for total ignorance.
  prefs: []
  type: TYPE_NORMAL
- en: Proportional guessing
  prefs: []
  type: TYPE_NORMAL
- en: Predict 1 for each function with probability equal to its frequency in the training
    set. This reflects prior class distribution knowledge, but without any learning.
  prefs: []
  type: TYPE_NORMAL
- en: These baselines help contextualize the model’s performance. If our trained model
    doesn’t outperform these simple heuristics, it’s a sign that it may not have learned
    meaningful structure from the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are implementations for the baselines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'These baselines should give us simple but informative reference points. Let’s
    now apply these prediction methods, alongside our trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s evaluate the baselines in exactly the same way as our model—by computing
    per-protein metrics and averaging them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: Our model clearly outperforms both baselines across all metrics—especially in
    precision, auPRC, and auROC. This is expected, as the trained model leverages
    actual sequence features to make more informed predictions. As noted earlier,
    accuracy is not a reliable metric in this setting, and even simple proportional
    guessing achieves a deceptively high accuracy due to class imbalance.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the model’s performance gains come from a large increase in precision,
    while the improvement in recall is more modest. This means the model is good at
    correctly identifying positive cases when it makes a prediction, but it tends
    to miss many true positives—it’s cautious and biased toward predicting “no function.”
  prefs: []
  type: TYPE_NORMAL
- en: 'This highlights a key trade-off: the model is conservative but accurate. Depending
    on your application, you may want to tune this behavior—for example, by lowering
    the decision threshold to improve recall, as discussed earlier.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll break down the model’s strengths and weaknesses by individual protein
    function and compare performance against both baselines. This allows us to see
    which specific functions the model predicts well—and where it struggles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'In [Figure 2-16](#best-predicted-functions), we visualize the function-level
    auPRC scores as a bar plot to highlight which functional categories the model
    handles best:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/dlfb_0216.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-16\. Top 20 protein functions ranked by model auPRC on the validation
    set. Bars show the auPRC achieved by the model, compared against two simple baselines
    (i.e., coin flips and proportional guessing).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Many of the top-performing protein functions in the plot are related to membrane
    or signaling roles (e.g., GPCR activity, kinase activity, transmembrane receptor
    activity). One possible reason is that these functions often involve well-conserved
    sequence features—such as transmembrane helices or catalytic domains—that may
    be easier for models to learn. While speculative, this aligns with the idea that
    functions tied to strong structural or biochemical motifs may produce clearer
    sequence-level signals than more context-dependent roles.
  prefs: []
  type: TYPE_NORMAL
- en: Together, these results suggest that the model is capable of detecting meaningful
    biological signal for certain classes of protein function—and that it significantly
    outperforms simple baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Conducting a Final Check on the Test Set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Take a look at the next section for ideas on how to extend and improve this
    model. Once you are satisfied with your exploration, we can move on to the final
    step of this project: making the final predictions on the test set. Remember not
    to touch the test set until the last stage of your project.'
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Be sure not to touch the test set until you’ve fully finalized your model—including
    all hyperparameters, architectures, and training choices. Evaluating on the test
    set repeatedly can lead to overly optimistic results and undermine the validity
    of your findings.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll make predictions on the test set of proteins in the same way we did for
    the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: The test set metrics closely mirror those observed on the validation set, which
    is good. In many workflows, test performance is slightly lower due to repeated
    use of the validation set during development—potentially leading to mild overfitting.
    However, in this case, we haven’t done extensive tuning, so the gap is minimal.
    Because the test set was held out throughout, its results provide a more reliable
    estimate of how the model will generalize to truly unseen data. These are the
    metrics we would report externally.
  prefs: []
  type: TYPE_NORMAL
- en: Improvements and Extensions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The model we’ve built demonstrates that protein function can be predicted from
    sequence using pretrained embeddings and a lightweight classifier. However, many
    directions remain to improve, interpret, and extend this work. We split these
    ideas into two broad categories: analysis-driven insights and machine learning
    improvements.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But before diving into technical upgrades, it’s worth stepping back to revisit
    the bigger picture:'
  prefs: []
  type: TYPE_NORMAL
- en: Why are you doing this?
  prefs: []
  type: TYPE_NORMAL
- en: Who will use the model, and what do they actually need? Can you share this prototype
    with users now to gather early feedback?
  prefs: []
  type: TYPE_NORMAL
- en: When are you done?
  prefs: []
  type: TYPE_NORMAL
- en: Is the current model already good enough? What specific improvements would meaningfully
    increase its utility? What benchmarks exist for this or similar tasks?
  prefs: []
  type: TYPE_NORMAL
- en: What matters most?
  prefs: []
  type: TYPE_NORMAL
- en: Is performance across all functions equally important, or do you care about
    a specific class (e.g., enzymes versus nonenzymes)? Focusing your optimization
    accordingly can save time.
  prefs: []
  type: TYPE_NORMAL
- en: Do you need interpretability?
  prefs: []
  type: TYPE_NORMAL
- en: For some applications, understanding why a model makes a prediction may matter
    more than maximizing performance.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, you’ll have thought about some of these questions before starting the
    modeling—but revisiting them now can help guide your next steps.
  prefs: []
  type: TYPE_NORMAL
- en: Biological and Analytical Exploration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even with a fixed model, we can learn a lot more by probing its behavior and
    comparing it to biological expectations:'
  prefs: []
  type: TYPE_NORMAL
- en: Threshold tuning
  prefs: []
  type: TYPE_NORMAL
- en: Our results showed that the model has high auPRC but low recall at a default
    probability threshold of 0.5\. You could optimize this threshold (e.g., per protein
    function or globally) using a metric like F1 score to find a better trade-off
    between precision and recall.
  prefs: []
  type: TYPE_NORMAL
- en: Species generalization
  prefs: []
  type: TYPE_NORMAL
- en: The current dataset is human only, but this might be unnecessarily limited.
    Try including protein-function pairs from other species to see if performance
    improves.
  prefs: []
  type: TYPE_NORMAL
- en: Function-specific performance drivers
  prefs: []
  type: TYPE_NORMAL
- en: Why does the model do well on some functions (e.g., GPCR activity) but poorly
    on others (e.g., growth factor activity)? You could investigate whether function
    prevalence, sequence length, or other properties correlate with performance.
  prefs: []
  type: TYPE_NORMAL
- en: Examine protein multifunctionality
  prefs: []
  type: TYPE_NORMAL
- en: Does the model struggle more with proteins that have many functions? Group proteins
    by number of annotated functions and plot performance (e.g., auPRC) to see if
    there’s a trend.
  prefs: []
  type: TYPE_NORMAL
- en: False positives that might be real
  prefs: []
  type: TYPE_NORMAL
- en: Find proteins where the model confidently predicts a function that isn’t labeled.
    Could the model be correct and the annotation missing? How might you follow this
    up?
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning Improvements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'From a machine learning perspective, here are a few directions you could explore:'
  prefs: []
  type: TYPE_NORMAL
- en: Tune the MLP
  prefs: []
  type: TYPE_NORMAL
- en: Our model is a small MLP on top of frozen embeddings. Try adding more layers,
    dropout, or batch normalization to increase capacity while controlling overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative input encodings
  prefs: []
  type: TYPE_NORMAL
- en: We used the mean-pooled embedding, which loses sequence order information. Try
    attention pooling or a small 1D CNN or transformer on top of the token-level embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs: []
  type: TYPE_NORMAL
- en: You could augment the input to include protein length, species (if you extend
    beyond human), or even simple statistics like embedding norms. These additional
    features might help the model distinguish protein types more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Train a per-function head:'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of predicting all functions jointly, try training separate models (or
    heads) for each function. This can help when tasks are highly imbalanced or unrelated.
    Alternatively, you could cluster GO functions into a few categories and train
    one model per cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Predict function hierarchically
  prefs: []
  type: TYPE_NORMAL
- en: Rather than treating each function independently, you could use the GO hierarchy
    to add structure to predictions—for example, predicting broad function categories
    first and then refining to more specific ones.
  prefs: []
  type: TYPE_NORMAL
- en: Try alternative base models
  prefs: []
  type: TYPE_NORMAL
- en: You could plug in other protein language models from Hugging Face or explore
    combining embeddings from multiple models by concatenating them.
  prefs: []
  type: TYPE_NORMAL
- en: Unfreeze the language model
  prefs: []
  type: TYPE_NORMAL
- en: The ESM2 embeddings are pretrained on a generic task. Fine-tuning the language
    model directly for protein function classification may boost performance, though
    it requires more compute and a more involved training setup.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: While it’s tempting to chase performance gains through increasingly complex
    models, always align your efforts with the actual goals of your project. Improving
    interpretability or expanding biological coverage may be more valuable than inching
    up another point on a leaderboard.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took our first hands-on step into the world of deep learning
    for biology. Starting with a dataset of human proteins, we explored how to extract
    meaningful representations using a pretrained protein language model, trained
    a simple classifier to predict protein function, and evaluated its performance
    using quantitative metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Along the way, we encountered practical challenges typical of biological modeling:
    getting comfortable with a new modeling setup, dealing with imbalanced label distributions,
    and carefully interpreting evaluation metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll build on these foundations by shifting our focus
    from proteins to DNA. You’ll define convolutional neural networks from scratch
    in Flax and train them end to end to model regulatory sequences, predict functional
    elements, and discover motif patterns directly from genomic data.
  prefs: []
  type: TYPE_NORMAL
