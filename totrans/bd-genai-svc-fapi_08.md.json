["```py\nimport time\n\ndef task():\n    print(\"Start of sync task\")\n    time.sleep(5) ![1](assets/1.png)\n    print(\"After 5 seconds of sleep\")\n\nstart = time.time()\nfor _ in range(3): ![2](assets/2.png)\n    task()\nduration = time.time() - start\nprint(f\"\\nProcess completed in: {duration} seconds\")\n\"\"\"\nStart of sync task\nAfter 5 seconds of sleep\nStart of sync task\nAfter 5 seconds of sleep\nStart of sync task\nAfter 5 seconds of sleep\n\nProcess completed in: 15.014271020889282 seconds\n\"\"\"\n```", "```py\nimport time\nimport asyncio\n\nasync def task(): ![1](assets/1.png)\n    print(\"Start of async task\")\n    await asyncio.sleep(5) ![2](assets/2.png)\n    print(\"Task resumed after 5 seconds\")\n\nasync def spawn_tasks():\n    await asyncio.gather(task(), task(), task()) ![3](assets/3.png)\n\nstart = time.time()\nasyncio.run(spawn_tasks()) ![4](assets/4.png)\nduration = time.time() - start\n\nprint(f\"\\nProcess completed in: {duration} seconds\")\n\"\"\"\nStart of async task\nStart of async task\nStart of async task\nTask resumed after 5 seconds\nTask resumed after 5 seconds\nTask resumed after 5 seconds\n\nProcess completed in: 5.0057971477508545 seconds ![5](assets/5.png) \"\"\"\n```", "```py\nimport asyncio\n\nasync def main():\n    print(\"Before sleeping\")\n    await asyncio.sleep(3) ![1](assets/1.png)\n    print(\"After sleeping for 3 seconds\")\n\nasyncio.run(main()) ![2](assets/2.png)\n\n\"\"\"\nBefore sleeping\nAfter sleeping for 3 seconds ![3](assets/3.png) \"\"\"\n```", "```py\n$ pip install openai\n```", "```py`##### Esempio 5-4\\. Confronto tra client OpenAI sincrono e asincrono    ```", "```py    La differenza tra i client sync e async è che con la versione async,FastAPI può iniziare a elaborare gli input dell'utente in parallelo senza attendere la risposta dell'API OpenAI per l'input precedente.    Sfruttando il codice asincrono, puoi ottenere un enorme aumento del throughput e scalare un volume maggiore di richieste simultanee. Tuttavia, devi fare attenzione quando scrivi codice asincrono (async).    Ecco alcune insidie e problemi comuni che potresti incontrare con il codice async:    *   La comprensione e il debug degli errori possono essere più complessi a causa del flusso di esecuzione non lineare dei task concorrenti.           *   Alcune librerie, come `aiohttp`, richiedono gestori di contesto asincroni nidificati per una corretta implementazione. Questo può creare confusione molto velocemente.           *   Mischiare codice asincrono e sincrono può annullare i vantaggi in termini di prestazioni, ad esempio se dimentichi di contrassegnare le funzioni con le parole chiave `async` e `await`.           *   Il mancato utilizzo di strumenti e librerie compatibili con l'asincronia può anche annullare i vantaggi in termini di prestazioni; ad esempio, utilizzare il pacchetto `requests` invece di `aiohttp` per effettuare chiamate API asincrone.           *   Dimenticare di attendere le coroutine all'interno di una funzione asincrona o attendere le non-coroutine può portare a un comportamento inaspettato. Tutte le parole chiave `async` devono essere seguite da un `await`.           *   Una gestione non corretta delle risorse (ad esempio, connessioni API/database aperte o buffer di file) può causare perdite di memoria che bloccano il computer. Puoi anche perdere memoria se non limiti il numero di operazioni concorrenti nel codice async.           *   Potresti anche imbatterti in problemi di concorrenza e di condizioni di gara in cui il principio di thread-safety viene violato, causando deadlock sulle risorse con conseguentecorruzione dei dati.              Questo elenco non è esaustivo e, come puoi vedere, ci sono diverse insidie nell'utilizzo della programmazione asincrona. Per questo motivo, ti consiglio di iniziare a scrivere programmi sincroni per capire il flusso e la logica di base del tuo codice, prima di affrontare le complessità della migrazione a un'implementazione asincrona.```", "```py```", "```py import os from fastapi import FastAPI from openai import AsyncOpenAI, OpenAI  app = FastAPI()  @app.get(\"/block\") async def block_server_controller():     completion = sync_client.chat.completions.create(...) ![1](assets/1.png)     return completion.choices[0].message.content  @app.get(\"/slow\") def slow_text_generator():     completion = sync_client.chat.completions.create(...) ![2](assets/2.png)     return completion.choices[0].message.content  @app.get(\"/fast\") async def fast_text_generator():     completion = await async_client.chat.completions.create(...) ![3](assets/3.png)     return completion.choices[0].message.content ```", "```py $ pip install beautifulsoup lxml aiohttp ```", "```py`##### Esempio 5-6\\. Creazione di uno scraper web asincrono    ```", "```py    [![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO5-1)      Un semplice schema regex che raccoglie gli URL in un gruppo denominato `url`e che corrisponde ai protocolli `http` e `https`. Per semplicità, questo schema corrisponde a URL definiti in modo più generico e non convalida la struttura di un nome di dominio o di un percorso, né tiene conto delle stringhe di query o delle ancore in un URL.      [![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO5-2)      Trova tutte le corrispondenze non sovrapposte del modello regex nel testo.      [![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO5-3)      Utilizza il pacchetto `bs4` Beautiful Soup per analizzare la stringa HTML. Nelle pagine di Wikipedia, il contenuto dell'articolo è annidato all'interno di un contenitore `div` con `id=\"bodyContent\"`, quindi la logica di parsing presuppone che vengano passati solo gli URL di Wikipedia. Puoi cambiare questa logica per altri URL o semplicemente utilizzare `soup.getText()`per prendere qualsiasi contenuto testuale annidato all'interno dell'HTML. Tuttavia, tieni presente che se analizzi l'HTML grezzo in questo modo, ci sarà molto rumore nel contenuto analizzato, il che può confondere LLM.      [![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO5-4)      Data una sessione `aiohttp` e un URL, esegui una richiesta asincrona `get`. Crea un gestore di contesto asincrono `response` e `await` la risposta all'interno di questo gestore di contesto.      [![5](assets/5.png)](#co_achieving_concurrency_in_ai_workloads_CO5-5)      Dato un elenco di URL, crea un gestore di contesto asincrono della sessione client per eseguire in modo asincrono più chiamate di recupero. Poiché `fetch()` è una funzione coroutine (cioè utilizza la parola chiave `await` ),`fetch_all()` dovrà eseguire più coroutine `fetch()` all'interno di `asyncio.gather()` da programmare per l'esecuzione asincrona sul ciclo degli eventi.      [![6](assets/6.png)](#co_achieving_concurrency_in_ai_workloads_CO5-6)      Controlla che tutti gli URL siano stati recuperati con successo e, in caso contrario, emette un avviso.      Ora hai le funzioni di scraper di cui hai bisogno per implementare la funzione di web scraping nel tuo endpoint `/generate/text`.    Successivamente, aggiorna il gestore del testo per utilizzare le funzioni di scraper attraverso una dipendenza in modo asincrono, come mostrato nell'[Esempio 5-7](#web_scraper_fastapi).    ##### Esempio 5-7\\. Iniettare la funzionalità di web scraper come dipendenza nel gestore FastAPI LLM    ```", "```py    [![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO6-1)      Implementa una dipendenza `get_urls_content` FastAPI che ottiene un prompt dell'utente dal corpo della richiesta e trova tutti gli URL. Restituisce quindi il contenuto di tutti gli URL come una lunga stringa. La dipendenza ha una gestione delle eccezioni integrata per gestire eventuali errori di I/O restituendo una stringa vuota e registrando un avviso sul server.      [![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO6-2)      Quando utilizzi `aiohttp` all'interno di FastAPI, non devi gestire tu stesso il ciclo degli eventi perché FastAPI, in quanto framework asincrono, gestisce il ciclo degli eventi. Puoi definire il tuo endpoint come una funzione asincrona e utilizzare `aiohttp` per effettuare richieste HTTP asincrone all'interno del gestore o tramite una dipendenza come in questo esempio.      [![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO6-3)      Inietta i risultati della chiamata alla dipendenza `get_urls_content` nel gestore tramite la classe `Depends` di FastAPI. L'utilizzo di una dipendenza FastAPI ha permesso di mantenere la logica del controllore piccola, pulita e leggibile.      Ora esegui il client Streamlit nel browser e prova la tua nuova funzione. La[Figura 5-6](#llm_summary) mostra il mio esperimento.  ![bgai 0506](assets/bgai_0506.png)  ###### Figura 5-6\\. Richiesta al modello TinyLlama self-hosted di riassumere un articolo di Wikipedia    Congratulazioni! Hai imparato a costruire un semplice scraper web non bloccante per lavorare con il tuo LLM. In questo mini-progetto, hai sfruttato il pacchetto `re` per abbinare i modelli di URL nel prompt dell'utente e poi hai usato la libreria `aiohttp` per recuperare in modo asincrono più pagine contemporaneamente. Hai poi usato il pacchetto `BeautifulSoup` per analizzare il contenuto degli articoli di Wikipedia prendendo il contenuto testuale del contenitore `div` con l'ID di `bodyContent` all'interno della stringa HTML recuperata. Per altri siti web o per le pagine interne dell'azienda, puoi sempre modificare la logica di parsing per ottenere un parsing appropriato. Infine, hai avvolto l'intera logica di scraping all'interno di una dipendenza FastAPI con gestione delle eccezioni integrata per sfruttare la dependency injection mentre aggiornavi il gestore del modello di testo.    Tieni presente che il tuo scraper non può gestire pagine complesse con layout dinamici renderizzati dal server. In questi casi, puoi aggiungere un browser headless al tuo web scraper per navigare nelle pagine dinamiche.    Inoltre, recuperare i contenuti di siti esterni sarà una sfida, poiché la maggior parte dei siti può implementare protezioni anti-scraping come il *blocco dell'IP* o i *CAPTCHA* come deterrenti comuni. Mantenere la *qualità* e la *coerenza* *dei dati* con i siti esterni è anche una sfida continua, poiché potrebbe essere necessario aggiornare regolarmente gli script di scraping per garantire un'estrazione accurata e affidabile.    Ora dovresti sentirti più a tuo agio nella costruzione di servizi alimentati da GenAI che devono interagire con il web facendo richieste di rete asincrone.    Successivamente, esamineremo altre interazioni asincrone di I/O, come quelle con i database e il filesystem, costruendo una funzione *talk to your documents*.    Questa funzionalità permette agli utenti di caricare documenti attraverso l'interfaccia Streamlit del tuo servizio. Il contenuto dei documenti caricati viene poi estratto, elaborato e salvato in un database. Successivamente, durante le interazioni dell'utente con il LLM, un sistema di recupero asincrono recupera contenuti semanticamente rilevanti dal database, che vengono poi utilizzati per aumentare il contesto fornito al LLM.    Questo processo è denominato RAG, che verrà sviluppato come modulo per il tuo LLM.```", "```py```", "``` $ pip install aiofiles python-multipart ```", "````` ##### Esempio 5-8\\. Implementazione di un endpoint per il caricamento asincrono di file    ```py # upload.py  import os import aiofiles from aiofiles.os import makedirs from fastapi import UploadFile  DEFAULT_CHUNK_SIZE = 1024 * 1024 * 50  # 50 megabytes  async def save_file(file: UploadFile) -> str:     await makedirs(\"uploads\", exist_ok=True)     filepath = os.path.join(\"uploads\", file.filename)     async with aiofiles.open(filepath, \"wb\") as f:         while chunk := await file.read(DEFAULT_CHUNK_SIZE):             await f.write(chunk)     return filepath  # main.py  from fastapi import FastAPI, HTTPException, status, File from typing import Annotated from upload import save_file  @app.post(\"/upload\") async def file_upload_controller(     file: Annotated[UploadFile, File(description=\"Uploaded PDF documents\")] ):     if file.content_type != \"application/pdf\":         raise HTTPException(             detail=f\"Only uploading PDF documents are supported\",             status_code=status.HTTP_400_BAD_REQUEST,         )     try:         await save_file(file)     except Exception as e:         raise HTTPException(             detail=f\"An error occurred while saving file - Error: {e}\",             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,         )     return {\"filename\": file.filename, \"message\": \"File uploaded successfully\"}  # client.py  import requests import streamlit as st  st.write(\"Upload a file to FastAPI\") file = st.file_uploader(\"Choose a file\", type=[\"pdf\"])  if st.button(\"Submit\"):     if file is not None:         files = {\"file\": (file.name, file, file.type)}         response = requests.post(\"http://localhost:8000/upload\", files=files)         st.write(response.text)     else:         st.write(\"No file uploaded.\") ```    A questo punto dovresti essere in grado di caricare i file tramite l'interfaccia utente di Streamlit, come puoi vedere nella [Figura 5-10](#streamlit_upload).  ![bgai 0510](assets/bgai_0510.png)  ###### Figura 5-10\\. Caricamento di file tramite Streamlit al servizio FastAPI    Con l'implementazione della funzionalità di upload, puoi ora dedicarti alla costruzione del modulo RAG. La[Figura 5-11](#rag_module_detailed) mostra la pipeline dettagliata, che apre il componente di trasformazione dei dati nella [Figura 5-9](#rag_module).  ![bgai 0511](assets/bgai_0511.png)  ###### Figura 5-11\\. Pipeline dettagliata di elaborazione dei dati RAG    Come puoi vedere nella [Figura 5-11](#rag_module_detailed), devi recuperare in modo asincrono i file memorizzati dal disco rigido e farli passare attraverso una pipeline di trasformazione dei dati prima della memorizzazione tramite un client di database asincrono.    La pipeline di trasformazione dei dati è composta dalle seguenti parti:    Estrattore      Estrarre il contenuto dei PDF e memorizzarlo in file di testo sul disco rigido.      Caricatore      Carica in modo asincrono un file di testo in memoria a pezzi.      Pulitore      Rimuove gli spazi bianchi o i caratteri di formattazione ridondanti dai pezzi di testo.      Incorporatore      Utilizza un modello di incorporamento pre-addestrato e auto-ospitato per convertire il testo in vettori di incorporamento.      Una volta che gli utenti caricano i loro file PDF sul filesystem del tuo server tramite il processo mostrato nell'[Esempio 5-8](#upload_file), puoi immediatamente convertirli in file di testo tramite la libreria `pypdf`. Poiché non esiste una libreria asincrona per caricare i file PDF binari, dovrai prima convertirli in file di testo.    L['esempio 5-9](#rag_extract) mostra come caricare i PDF, estrarre ed elaborare il loro contenuto e poi archiviarli come file di testo.    ###### Nota    Dovrai installare diversi pacchetti per eseguire i prossimi esempi:    ```py $ pip install qdrant_client aiofiles pypdf loguru ```   ```py`##### Esempio 5-9\\. Estrattore PDF-testo RAG    ``` # rag/extractor.py  from pypdf import PdfReader  def pdf_text_extractor(filepath: str) -> None:     content = \"\"     pdf_reader = PdfReader(filepath, strict=True) ![1](assets/1.png)     for page in pdf_reader.pages:         page_text = page.extract_text()         if page_text:             content += f\"{page_text}\\n\\n\" ![2](assets/2.png)     with open(filepath.replace(\"pdf\", \"txt\"), \"w\", encoding=\"utf-8\") as file: ![3](assets/3.png)         file.write(content) ```py    [![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO7-1)      Utilizza la libreria `pypdf` per aprire un puntatore di flusso a un file PDF con `strict=True`in modo che qualsiasi errore di lettura venga registrato sul terminale. Nota che non esiste un'implementazione asincrona della libreria `pypdf`, quindi la funzione viene dichiarata con la normale parola chiave `def`. È importante evitare di utilizzare questa funzione all'interno di una funzione asincrona per evitare di bloccare il ciclo di eventi che esegue il thread principale del server. Vedrai come le attività in background di FastAPI possono aiutare a risolvere questo problema.      [![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO7-2)      Esegue un loop su ogni pagina del documento PDF ed estrae e aggiunge tutti i contenuti testuali in una lunga stringa.      [![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO7-3)      Scrivi il contenuto del documento PDF in un file di testo per l'elaborazione a valle. Specifica `encoding=\"utf-8\"` per evitare problemi su piattaforme come Windows.      L'estrattore di testo convertirà i file PDF in semplici file di testo che potremo riversare in memoria in pezzi utilizzando un caricatore di file asincrono.Ogni pezzo può poi essere ripulito e incorporato in un vettore di incorporamento utilizzando un modello di incorporamento open source come `jinaai/jina-embeddings-v2-base-en`, disponibile per il download dall'[hub](https://oreil.ly/gI74r) del [modello Hugging Face](https://oreil.ly/gI74r).    ###### Nota    Ho scelto l'embedder di base Jina perché corrisponde alle prestazioni del modello proprietario di OpenAI `text-embedding-ada-002`.    L['esempio 5-10](#rag_transform) mostra l'implementazione della pipeline di trasformazione dei dati RAG che include le funzioni async di caricamento del testo, pulizia e incorporamento.    ##### Esempio 5-10\\. Funzioni di trasformazione dei dati RAG    ``` # rag/transform.py  import re from typing import Any, AsyncGenerator  import aiofiles from transformers import AutoModel  DEFAULT_CHUNK_SIZE = 1024 * 1024 * 50  # 50 megabytes  embedder = AutoModel.from_pretrained(     \"jinaai/jina-embeddings-v2-base-en\", trust_remote_code=True ![1](assets/1.png) )  async def load(filepath: str) -> AsyncGenerator[str, Any]:     async with aiofiles.open(filepath, \"r\", encoding=\"utf-8\") as f: ![2](assets/2.png)         while chunk := await f.read(DEFAULT_CHUNK_SIZE): ![3](assets/3.png)             yield chunk ![4](assets/4.png)  def clean(text: str) -> str:     t = text.replace(\"\\n\", \" \")     t = re.sub(r\"\\s+\", \" \", t)     t = re.sub(r\"\\. ,\", \"\", t)     t = t.replace(\"..\", \".\")     t = t.replace(\". .\", \".\")     cleaned_text = t.replace(\"\\n\", \" \").strip()     return cleaned_text ![5](assets/5.png)  def embed(text: str) -> list[float]:     return embedder.encode(text).tolist() ![6](assets/6.png) ```py    [![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO8-1)      Scarica e utilizza il modello open source `jina-embeddings-v2-base-en` per incorporare stringhe di testo in vettori di incorporamento. Imposta `trust_remote_code=True` per scaricare i pesi del modello e le configurazioni del tokenizer. Senza questo parametro impostato su `True`, i pesi del modello scaricato saranno inizializzati con valori casuali invece che con valori addestrati.      [![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO8-2)      Utilizza la libreria `aiofiles` per aprire una connessione asincrona a un file del filesystem.      [![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO8-3)      Carica il contenuto dei documenti di testo in pezzi per un'operazione di I/O efficiente dal punto di vista della memoria.      [![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO8-4)      Invece di restituire un `chunk`, lo rende in modo che la funzione `load()` diventi un *generatore asincrono*. I generatori asincroni possono essere iterati con `async for loop`s in modo che le operazioni di blocco al loro interno possano essere `await`ed essere avviate/riprese dal ciclo di eventi. Sia i cicli asincroni `for` che i normali cicli `for` iterano in modo sequenziale sull'iterabile, ma i cicli asincroni `for` permettono di iterare su un iteratore asincrono.      [![5](assets/5.png)](#co_achieving_concurrency_in_ai_workloads_CO8-5)      Pulisci il testo eliminando gli spazi, le virgole, i punti e le interruzioni di riga.      [![6](assets/6.png)](#co_achieving_concurrency_in_ai_workloads_CO8-6)      Utilizza il modello di incorporazione Jina per convertire un brano di testo in un vettore di incorporazione.      Una volta che i dati sono stati elaborati in vettori di incorporamento, puoi memorizzarli nel *database dei vettori*.    A differenza delle alternative convenzionali come i database relazionali, un database vettoriale è progettato specificamente per gestire operazioni di archiviazione e recupero dei dati ottimizzate per la *ricerca semantica*, che produce risultati migliori rispetto alle ricerche per parole chiave che possono restituire risultati non ottimali o incompleti.    I seguenti esempi di codice richiedono l'esecuzione di un'istanza locale del database vettoriale `qdrant` sul tuo computer locale per il modulo RAG. L'impostazione di un database locale ti permetterà di fare esperienza pratica di lavoro asincrono con database vettoriali di livello di produzione. Per eseguire il database in un container, devi avere Docker installato sul tuo computer e poi estrarre ed eseguire il container del database vettoriale `qdrant`.^([7](ch05.html#id843)) Se non hai familiarità con Docker, non preoccuparti: imparerai di più su Docker e la containerizzazione nel[Capitolo 12](ch12.html#ch12).    ``` $ docker pull qdrant/qdrant ![1](assets/1.png) $ docker run -p 6333:6333 -p 6334:6334 \\  ![2](assets/2.png)     -v $(pwd)/qdrant_storage:/qdrant/storage:z \\ ![3](assets/3.png)     qdrant/qdrant ```py    [![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO9-1)      Scarica l'immagine del database vettoriale `qdrant` dal repository `qdrant` nel registro Docker.      [![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO9-2)      Esegui l'immagine `qdrant/qdrant`, quindi esponi e mappa le porte del container `6333` e `6334` sulle stesse porte del computer host.      [![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO9-3)      Monta il database `qdrant` sul filesystem del computer host nella directory principale del tuo progetto.      Poiché l'archiviazione e il recupero del database sono operazioni di I/O, dovresti utilizzare un client di database asincrono. Fortunatamente, `qdrant` fornisce un client di database asincrono con cui lavorare.    ###### Suggerimento    Puoi utilizzare altri fornitori di database vettoriali come Weaviate, Elastic, Milvus, Pinecone, Chroma o altri in sostituzione di Qdrant. Ognuno di essi ha una serie di caratteristiche e limitazioni da considerare per il tuo caso d'uso.    Se scegli un altro provider di database, assicurati che sia disponibile un client di database asincrono che puoi utilizzare.    Invece di scrivere diverse funzioni per memorizzare e recuperare i dati dal database, puoi usare il modello di repository menzionato nel [Capitolo 2](ch02.html#ch02). Con il modello di repository, puoi astrarre le operazioni di basso livello di creazione, lettura, aggiornamento e cancellazione del database con dei valori predefiniti che corrispondono al tuo caso d'uso.    L['esempio 5-11](#rag_repository) mostra l'implementazione del modello di repository per il database vettoriale Qdrant.    ##### Esempio 5-11\\. Configurazione del client del database vettoriale utilizzando il modello del repository    ``` # rag/repository.py  from loguru import logger from qdrant_client import AsyncQdrantClient from qdrant_client.http import models from qdrant_client.http.models import ScoredPoint   class VectorRepository: ![1](assets/1.png)     def __init__(self, host: str = \"localhost\", port: int = 6333) -> None:         self.db_client = AsyncQdrantClient(host=host, port=port)      async def create_collection(self, collection_name: str, size: int) -> bool: ![2](assets/2.png)         vectors_config = models.VectorParams(             size=size, distance=models.Distance.COSINE ![3](assets/3.png)         )         response = await self.db_client.get_collections()          collection_exists = any(             collection.name == collection_name             for collection in response.collections         )         if collection_exists: ![4](assets/4.png)             logger.debug(                 f\"Collection {collection_name} already exists - recreating it\"             )             await self.db_client.delete_collection(collection_name)             return await self.db_client.create_collection(                 collection_name,                 vectors_config=vectors_config,             )          logger.debug(f\"Creating collection {collection_name}\")         return await self.db_client.create_collection(             collection_name=collection_name,             vectors_config=models.VectorParams(                 size=size, distance=models.Distance.COSINE             ),         )      async def delete_collection(self, name: str) -> bool:         logger.debug(f\"Deleting collection {name}\")         return await self.db_client.delete_collection(name)      async def create(         self,         collection_name: str,         embedding_vector: list[float],         original_text: str,         source: str,     ) -> None:         response = await self.db_client.count(collection_name=collection_name)         logger.debug(             f\"Creating a new vector with ID {response.count} \"             f\"inside the {collection_name}\"         )         await self.db_client.upsert(             collection_name=collection_name,             points=[                 models.PointStruct(                     id=response.count,                     vector=embedding_vector,                     payload={                         \"source\": source,                         \"original_text\": original_text,                     },                 )             ],         )      async def search(         self,         collection_name: str,         query_vector: list[float],         retrieval_limit: int,         score_threshold: float, ![5](assets/5.png)     ) -> list[ScoredPoint]:         logger.debug(             f\"Searching for relevant items in the {collection_name} collection\"         )         response = await self.db_client.query_points(             collection_name=collection_name,             query_vector=query_vector,             limit=retrieval_limit,             score_threshold=score_threshold,         )         return response.points ```py    [![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO10-1)      Utilizza il pattern repository per interagire con il database vettoriale tramite un client asincrono. Normalmente, nel pattern repository implementerai i metodi `create`, `get`, `update` e `delete`. Ma per ora implementiamo i metodi `create_​col⁠lection`, `delete_collection`, `create` e `search`.      [![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO10-2)      I vettori devono essere memorizzati in una collezione. Una collezione è un insieme di punti nominati che puoi utilizzare durante una ricerca. Le collezioni sono simili alle tabelle di un database relazionale.      [![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO10-3)      Comunica al database che tutti i vettori di questa collezione devono essere confrontati tramite il calcolo della somiglianza del coseno che calcola le distanze tra i vettori.      [![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO10-4)      Controlla se una collezione esiste prima di crearne una nuova, altrimenti crea nuovamente la collezione.      [![5](assets/5.png)](#co_achieving_concurrency_in_ai_workloads_CO10-5)      Imposta `retrieval_limit` e `score_threshold` per limitare il numero di elementi nei risultati della ricerca.      La classe `VectorRepository` dovrebbe ora rendere più semplice l'interazione con il database.    Quando si memorizzano le incorporazioni vettoriali, si memorizzano anche alcuni *metadati*, tra cui il nome del documento di origine, la posizione del testo all'interno della fonte e il testo originale estratto. I sistemi RAG si basano su questi metadati per aumentare i prompt di LLM e per mostrare le informazioni sulla fonte agli utenti.    ###### Suggerimento    Attualmente, la conversione del testo in vettori di incorporamento è un processo irreversibile. Pertanto, dovrai memorizzare il testo che ha creato l'incorporamento insieme al vettore di incorporamento come metadati.    Ora puoi estendere il sito `VectorRepository` e creare il sito `VectorService` che ti permette di concatenare la pipeline di elaborazione e archiviazione dei dati, come mostrato nell'[esempio 5-12](#rag_db_service).    ##### Esempio 5-12\\. Servizio di database vettoriale    ``` # rag/service.py  import os  from loguru import logger from .repository import VectorRepository from .transform import clean, embed, load   class VectorService(VectorRepository): ![1](assets/1.png)     def __init__(self):         super().__init__()      async def store_file_content_in_db( ![2](assets/2.png)         self,         filepath: str,         chunk_size: int = 512,         collection_name: str = \"knowledgebase\",         collection_size: int = 768,     ) -> None:         await self.create_collection(collection_name, collection_size)         logger.debug(f\"Inserting {filepath} content into database\")         async for chunk in load(filepath, chunk_size): ![3](assets/3.png)             logger.debug(f\"Inserting '{chunk[0:20]}...' into database\")              embedding_vector = embed(clean(chunk))             filename = os.path.basename(filepath)             await self.create(                 collection_name, embedding_vector, chunk, filename             )   vector_service = VectorService() ![4](assets/4.png) ```py    [![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO11-1)      Crea la classe `VectorService` ereditando la classe `VectorRepository` in mododa poter utilizzare ed estendere i metodi comuni di funzionamento del database dell'[Esempio 5-11](#rag_repository).      [![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO11-2)      Utilizza il metodo del servizio `store_file_content_in_db` per caricare, trasformare e archiviare in modo asincrono i documenti di testo grezzi nel database.      [![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO11-3)      Utilizza un generatore asincrono `load()` per caricare pezzi di testo da un filein modo asincrono.      [![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO11-4)      Crea un'istanza di `VectorService` da importare e utilizzare nell'applicazione.      Il passo finale della pipeline di elaborazione e archiviazione dei dati RAG consiste nell'eseguire la logica di estrazione e archiviazione del testo all'interno di `file_upload_controller`come attività in background. L'implementazione è mostrata nell'[Esempio 5-13](#rag_data_processor) in modo che il gestore possa attivare entrambe le operazioni in background dopo aver risposto all'utente.    ##### Esempio 5-13\\. Aggiornare il gestore di upload per elaborare e memorizzare il contenuto del file PDF nel database vettoriale    ``` # main.py  from fastapi import (     BackgroundTasks,     FastAPI,     File,     UploadFile,     status,     HTTPException, ) from typing import Annotated from rag import pdf_text_extractor, vector_service  @app.post(\"/upload\") async def file_upload_controller(     file: Annotated[UploadFile, File(description=\"A file read as UploadFile\")],     bg_text_processor: BackgroundTasks, ![1](assets/1.png) ):     ... # Raise an HTTPException if data upload is not a PDF file     try:         filepath = await save_file(file)         bg_text_processor.add_task(pdf_text_extractor, filepath) ![2](assets/2.png)         bg_text_processor.add_task( ![3](assets/3.png)             vector_service.store_file_content_in_db,             filepath.replace(\"pdf\", \"txt\"),             512,             \"knowledgebase\",             768,         )      except Exception as e:         raise HTTPException(             detail=f\"An error occurred while saving file - Error: {e}\",             status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,         )     return {\"filename\": file.filename, \"message\": \"File uploaded successfully\"} ```py    [![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO12-1)      Inietta la funzione dei compiti in background di FastAPI nel gestore per elaborare i caricamenti di file in background. I compiti in background di FastAPI saranno eseguiti in ordine subito dopo che il gestore avrà inviato una risposta al client.      [![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO12-2)      Esegue la funzione di estrazione del testo PDF in background dopo aver ricevuto una risposta dal client. Poiché `pdf_text_extractor` è una funzione sincrona, FastAPI eseguirà questa funzione su un thread separato all'interno del pool di thread per evitare di bloccare il ciclo di eventi.      [![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO12-3)      Esegui la funzione asincrona `vector_service.store_file_content_in_db` in background sul ciclo di eventi gestito da FastAPI non appena `pdf_text_extractor` ha terminato l'elaborazione. Imposta la funzione per caricare il contenuto del documento di testo in pezzi da 512 caratteri e memorizzarli nella collezione di vettori di `knowledgebase`, che accetta vettori di dimensioni 768.      Dopo aver costruito la pipeline di archiviazione dei dati RAG, ora puoi concentrarti sul sistema di ricerca e recupero, che ti permetterà di aumentare i prompt dell'utente all'LLM con le conoscenze del database. L'[Esempio 5-14](#rag_generation) integra le operazioni di ricerca e recupero RAG con il gestore LLM per aumentare i prompt LLM con uncontesto aggiuntivo.    ##### Esempio 5-14\\. Integrazione di RAG con l'endpoint che serve LLM    ``` # dependencies.py  from rag import vector_service from rag.transform import embed from schemas import TextModelRequest, TextModelResponse  async def get_rag_content(body: TextModelRequest = Body(...)) -> str: ![1](assets/1.png)     rag_content = await vector_service.search( ![2](assets/2.png)         \"knowledgebase\", embed(body.prompt), 3, 0.7     )     rag_content_str = \"\\n\".join( ![3](assets/3.png)         [c.payload[\"original_text\"] for c in rag_content]     )      return rag_content_str   # main.py  ... # other imports from dependencies import get_rag_content, get_urls_content  @app.post(\"/generate/text\", response_model_exclude_defaults=True) async def serve_text_to_text_controller(     request: Request,     body: TextModelRequest = Body(...),     urls_content: str = Depends(get_urls_content),     rag_content: str = Depends(get_rag_content), ![4](assets/4.png) ) -> TextModelResponse:     ... # Raise HTTPException for invalid models     prompt = body.prompt + \" \" + urls_content + rag_content     output = generate_text(models[\"text\"], prompt, body.temperature)     return TextModelResponse(content=output, ip=request.client.host) ```py    [![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO13-1)      Crea la funzione di dipendenza `get_rag_content` da iniettare nel gestore del servizio LLM. Questa dipendenza ha accesso alla richiesta `body` e successivamente all'utente `prompt`.      [![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO13-2)      Utilizza `vector_service` per cercare nel database i contenuti rilevanti per l'utente `prompt`. Converti l'utente `prompt` in un embedding utilizzando la funzione `embed` quando passa alla funzione `vector_service.search`. Recupera i tre elementi più rilevanti solo se il loro punteggio di somiglianza coseno è superiore a `0.7` (o al 70%).      [![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO13-3)      Unisce il carico di testo dei tre elementi più rilevanti recuperati come `rag_​con⁠tent_str` e lo restituisce.      [![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO13-4)      Iniettare i risultati della funzione di dipendenza `get_rag_content` nel gestore LLM per aumentare il prompt finale del LLM con il contenuto del database vettoriale `knowledgebase`. Il gestore LLM può ora recuperare il contenuto delle pagine web e del database vettoriale RAG.      Se ora visiti il tuo browser e carichi un documento PDF, dovresti essere in grado di fare domande su di esso al tuo LLM. La[Figura 5-12](#rag_results) mostra il mio esperimento con il servizio caricando un campione di questo libro nella sua forma grezza e chiedendo al LLM di descrivere chisono.    ###### Nota    A seconda del modello e delle dimensioni degli input, potresti osservare un calo delle prestazioni o eccezioni come problemi di lunghezza dei token.  ![bgai 0512](assets/bgai_0512.png)  ###### Figura 5-12\\. Sfruttare il RAG per fornire risposte alle query degli utenti    Congratulazioni, ora hai un sistema RAG perfettamente funzionante, grazie a modelli open source e a un database vettoriale.    Questo progetto più lungo è servito come esercitazione pratica per imparare i concetti relativi alla programmazione asincrona e alle operazioni di I/O con il filesystem e un database vettoriale costruendo un modulo RAG per il tuo sistema LLM. Nota che il sistema RAG che abbiamo appena costruito insieme ha ancora molte limitazioni:    *   La suddivisione del testo può dividere le parole a metà con conseguente scarso recupero econfusione in LLM.           *   Il LLM può continuare a produrre allucinazioni e risultati incoerenti anche con i prompt aumentati.           *   Il sistema di ricerca e recupero può avere prestazioni scarse in alcuni casi.           *   I prompt aumentati possono superare la finestra contestuale di LLM.           *   Le informazioni recuperate dal database possono essere prive di fatti rilevanti a causa di una base di conoscenza obsoleta o incompleta, di query ambigue o di un algoritmo di recupero inadeguato.           *   Il contesto recuperato potrebbe non essere ordinato in base alla rilevanza della query dell'utente.              Puoi lavorare per migliorare ulteriormente il modulo RAG implementando varie altre tecniche, che non tratterò in questo libro:    *   Ottimizza le operazioni di divisione del testo, dimensionamento dei pezzi, pulizia e incorporazione.           *   Eseguire trasformazioni delle query utilizzando il LLM per aiutare il sistema di reperimento e di incremento tramite tecniche come la compressione del prompt, il concatenamento, la raffinazione, l'aggregazione e così via, per ridurre le allucinazioni e migliorare le prestazioni del LLM.           *   Riassumere o scomporre prompt aumentati di grandi dimensioni per alimentare il contesto nei modelli utilizzando un approccio a finestra scorrevole.           *   Migliorare gli algoritmi di recupero per gestire query ambigue e implementare meccanismi di fallback per i dati incompleti.           *   Migliorare le prestazioni di recupero con metodi come la *massima rilevanza marginale* (MMR) per arricchire il processo di incremento con documenti più diversi.           *   Implementare altre tecniche RAG avanzate come il reranking e il filtraggio del recupero, gli indici gerarchici del database, la fusione RAG, i pensieri aumentati dal recupero (RAT) e così via, per migliorare le prestazioni generali della generazione.              Ti lascio approfondire queste tecniche e metterle in pratica da solo come esercizi aggiuntivi.    Nella prossima sezione, esamineremo altre tecniche per ottimizzare i tuoi servizi GenAI per evitare di bloccare il server con operazioni delimitate dal calcolo, come l'inferenza del modello.```` ```py`` `````", "``````py`  ``````", "``````py ````` # Ottimizzare il servizio di modelli percompiti di inferenza AIcon limiti di memoria e dicalcolo    Finora abbiamo analizzato l'ottimizzazione delle operazioni del nostro servizio che sono delimitate dall'I/O. Hai imparato a sfruttare la programmazione asincrona per interagire con il web, i database e i file costruendo un web scraper e un modulo RAG.    Utilizzando strumenti e tecniche async, il tuo servizio è rimasto reattivo quando interagisce con il web, il filesystem e i database. Tuttavia, se il modello è in self-hosting, il passaggio a tecniche di programmazione async non eliminerà completamente i lunghi tempi di attesa, perché il collo di bottiglia saranno le operazioni di inferenza del modello.    ## Operazioni delimitate dal calcolo    È possibile accelerare l'inferenza eseguendo i modelli su GPU per parallelizzare in modo massiccio i calcoli. Le moderne GPU hanno una potenza di calcolo sbalorditiva, misurata dal numero di operazioni*in virgola mobile* al secondo (FLOPS), e raggiungono i teraflop (NVIDIA A100) o i petaflop (NVIDIA H100) di calcolo. Tuttavia, nonostante la potenza e le capacità di parallelizzazione significative, i core delle moderne GPU sono spesso sottoutilizzati nei carichi di lavoro simultanei con modelli più grandi.    Quando i modelli sono auto-ospitati sulle GPU, i parametri del modello vengono caricati dal disco alla RAM (I/O bound) e poi spostati dalla RAM alla memoria ad alta larghezza di banda della GPU dalla CPU (memory bound). Una volta che i parametri del modello sono caricati sulla memoria della GPU, viene eseguita l'inferenza (compute bound).    Controintuitivamente, l'inferenza del modello per i modelli GenAI più grandi, come SDXL e LLMs, non è delimitata dall'I/O o dal calcolo, ma piuttosto dalla memoria. Ciò significa che ci vuole più tempo per caricare 1 MB di dati nei core di calcolo della GPU di quanto ne serva per elaborare 1 MB di dati. Inevitabilmente, per massimizzare la concorrenza del tuo servizio, dovrai *raggruppare* le richieste di inferenza e inserire il batch più grande possibile nella memoria a banda larga della GPU.    Pertanto, anche quando si utilizzano tecniche asincrone e GPU di ultima generazione, il server può essere bloccato in attesa che miliardi di parametri del modello vengano caricati nella memoria ad alta larghezza di banda della GPU durante ogni richiesta. Per evitare di bloccare il server, puoi disaccoppiare le operazioni di model-serving legate alla memoria dal server FastAPI esternalizzando il model-serving, come abbiamo accennato nel [Capitolo 3](ch03.html#ch03).    Vediamo come delegare il servizio del modello a un altro processo.    ## Modello esternalizzante al servizio    Quando esternalizzi i tuoi carichi di lavoro che prevedono l'uso di modelli, hai a disposizione diverse opzioni: puoi ospitare i modelli su un altro server FastAPI o utilizzare server specializzati nell'inferenza dei modelli.    I server di inferenza specializzati supportano solo una serie limitata di architetture di modelli GenAI. Tuttavia, se l'architettura del tuo modello è supportata, risparmierai molto tempo senza dover implementare da solo le ottimizzazioni per l'inferenza. Ad esempio, se hai bisogno di auto-ospitare LLMs, i framework LLM-serving possono eseguire per te diverse ottimizzazioni per l'inferenza, come l'elaborazione batch, il parallelismo tensoriale, la quantizzazione, la cache, lo streaming delle uscite, la gestione della memoria GPU, ecc.    Dato che in questo capitolo abbiamo lavorato principalmente con LLMs, ti mostrerò come integrare vLLM, un server LLM open source che può avviare per te un server FastAPI conforme alle specifiche API OpenAI. vLLM si integra perfettamente anche con le più diffuse architetture open source del modello Hugging Face, come GPT, Llama, Gemma, Mistral, Falcon, ecc.    ###### Nota    Al momento in cui scriviamo, altri server di hosting LLM che puoi utilizzare sono NVIDIA Triton Inference Server, Ray Serve, Hugging Face Inference e OpenLLM, tra gli altri.    Ci sono caratteristiche, vantaggi e svantaggi nell'utilizzo di ciascuno di essi, comprese le architetture dei modelli supportati. Ti consiglio di fare una ricerca su questi server prima di adottarli nei tuoi casi d'uso.    Puoi avviare il tuo server vLLM FastAPI con un solo comando, come mostrato nell'[Esempio 5-15](#vllm). Per eseguire il codice dell'[Esempio 5-15](#vllm), devi installare `vllm` utilizzando:    ```py $ pip install vllm ```   ```py`###### Avvertenze    Al momento in cui scriviamo, vLLM supporta solo le piattaforme Linux (inclusa WSL) con GPU compatibili con NVIDIA per eseguire le dipendenze del toolkit CUDA. Purtroppo, non è possibile installare vLLM su macchine Mac o Windows per effettuare test locali.    vLLM è stato progettato per i carichi di lavoro di inferenza di produzione su GPU NVIDIA in ambienti Linux, dove il server può delegare le richieste a più core di GPU tramite il *parallelismo tensoriale*. Supporta anche il calcolo distribuito quando si tratta di scalare i servizi al di là di una singola macchina grazie alla dipendenza da Ray Serve.    Consulta la documentazione di vLLM per maggiori dettagli sull'inferenza distribuita e sul servizio.    ##### Esempio 5-15\\. Avvio del server vLLM FastAPI OpenAI API per TinyLama su una macchina Linux con 4 GPU NVIDIA T4 da 16 GB    ``` $ python -m vllm.entrypoints.openai.api_server \\ ![1](assets/1.png) --model \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \\ --dtype float16 \\ ![2](assets/2.png) --tensor-parallel-size 4 \\ ![3](assets/3.png) --api-key \"your_secret_token\" ![4](assets/4.png) ```py    [![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO14-1)      Avvia un server API compatibile con OpenAI con FastAPI per servire il modello TinyLlama.      [![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO14-2)      Usa il tipo di dati di media precisione `float16`.`float16` è compatibile con l'hardware della GPU, mentre `bfloat16` è generalmente compatibile con l'hardware della CPU.      [![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO14-3)      Sfrutta la funzione di parallelismo tensoriale di vLLM per eseguire il server API su quattro GPU.      [![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO14-4)      Imposta un token segreto per l'autenticazione di base per proteggere il server LLM. Questo è utile per la comunicazione sicura da macchina a macchina, ad esempio per comunicare direttamente con il tuo servizio FastAPI attuale.      Con il server vLLM FastAPI attivo e funzionante, ora puoi sostituire la logica di servizio del modello nel tuo servizio attuale con chiamate di rete al server vLLM. Fai riferimento all'[Esempio 5-16](#vllm_fastapi_text_generation) per i dettagli dell'implementazione.    ##### Esempio 5-16\\. Sostituzione del model serving con chiamate API asincrone al nuovo server vLLM    ``` # models.py  import os import aiohttp from loguru import logger  async def generate_text(prompt: str, temperature: float = 0.7) -> str:     system_prompt = \"You are an AI assistant\"     messages = [         {\"role\": \"system\", \"content\": system_prompt},         {\"role\": \"user\", \"content\": prompt},     ]     data = {\"temperature\": temperature, \"messages\": messages}     headers = {\"Authorization\": f\"Bearer {os.environ.get('VLLM_API_KEY')}\"} try:    async with aiohttp.ClientSession() as session: ![1](assets/1.png)         response = await session.post(             \"http://localhost:8000/v1/chat\", json=data, headers=headers         )         predictions = await response.json() except Exception as e:     logger.error(f\"Failed to obtain predictions from vLLM - Error: {e}\")     return (         \"Failed to obtain predictions from vLLM - \"         \"See server logs for more details\"     ) try:     output = predictions[\"choices\"][0][\"message\"][\"content\"] ![2](assets/2.png)     logger.debug(f\"Generated text: {output}\")     return output except KeyError as e:     logger.error(f\"Failed to parse predictions from vLLM - Error: {e}\")     return (         \"Failed to parse predictions from vLLM - \"         \"See server logs for more details\"     ) ```py    [![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO15-1)      Usa `aiohttp` per creare una sessione asincrona per l'invio di richieste `POST` al server FastAPI di vLLM. Questa logica sostituisce la logica di inferenza della pipeline del modello Hugging Face sull'attuale server FastAPI.      [![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO15-2)      Poiché il server vLLM è compatibile con OpenAI, puoi accedere ai contenuti in uscita seguendo le specifiche API OpenAI.      Successivamente, rimuovi il codice relativo alla vita FastAPI in modo che il tuo servizio attuale non carichi il modello TinyLlama. Puoi ottenere questo risultato seguendo il codice riportato nell'[Esempio 5-17](#vllm_fastapi_handler).    ##### Esempio 5-17\\. Rimuovi la durata di vita di FastAPI e aggiorna il gestore della generazione del testo in modo che sia asincrono    ``` # main.py  from fastapi import FastAPI, Request from schemas import TextModelRequest, TextModelResponse from models import generate_text  # Remove the asynccontextmanager to remove TinyLlama from FastAPI ![1](assets/1.png) # @asynccontextmanager # async def lifespan(app: FastAPI): #     models[\"text\"] = load_text_model() #     yield #     models.clear()  # Remove the `lifespan` argument from `FastAPI()` app = FastAPI()  @app.post(\"/generate/text\") async def serve_text_to_text_controller(     request: Request, body: TextModelRequest ) -> TextModelResponse: ![2](assets/2.png)     ...  # controller logic     output = await generate_text(body.prompt, body.temperature)     return TextModelResponse(content=output, ip=request.client.host) ```py    [![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO16-1)      Non è più necessario utilizzare FastAPI `lifespan` poiché il modello è ora servito da un server FastAPI vLLM esterno.      [![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO16-2)      Rendi `serve_text_to_text_controller` un gestore di rotte asincrone, in quanto ora esegue operazioni di I/O verso il server vLLM e non esegue più operazioni di inferenza del modello sincrone e delimitate da calcoli, in quanto la loro gestione è delegata al server vLLM.      Congratulazioni, ora hai ottenuto la concurrency con i tuoi carichi di lavoro di inferenza AI. Hai implementato una forma di multiprocessing su una singola macchina spostando i tuoi carichi di lavoro di inferenza LLM su un altro server. Entrambi i server sono ora in esecuzione su core separati con il tuo server LLM che delega il lavoro a più core GPU, sfruttando il parallelismo. Questo significa che il tuo server principale è ora in grado di elaborare più richieste in entrata e di svolgere altre attività oltre all'elaborazione di un'operazione di inferenza LLM alla volta.    ###### Suggerimento    Tieni presente che tutte le concomitanze che hai ottenuto finora sono state limitate a una sola macchina.    Per supportare un numero maggiore di utenti contemporanei, potresti aver bisogno di più macchine con core CPU e GPU. A quel punto, framework di calcolo distribuito come Ray Serve e Kubernetes possono aiutarti a scalare e orchestrare i tuoi servizi oltre una singola macchina worker utilizzando il parallelismo.    Prima di integrare vLLM, si verificavano lunghi tempi di attesa tra una richiesta e l'altra perché il server principale era troppo impegnato nell'esecuzione delle operazioni di inferenza. Con vLLM, ora si ottiene una massiccia riduzione della latenza e un aumento del throughput del servizio LLM.    Oltre ai meccanismi di compressione del modello come la quantizzazione, vLLM utilizza altre tecniche di ottimizzazione come il batching continuo delle richieste, il partizionamento della cache (paged attention), la riduzione dell'ingombro della memoria della GPU tramite la condivisione della memoria e lo streaming delle uscite per ottenere una latenza ridotta e un throughput elevato.    Analizziamo più in dettaglio i meccanismi di batching delle richieste e di attenzione paginata per capire come ottimizzare ulteriormente l'inferenza LLM.    ### Richiesta di dosaggio e dosaggio continuo    Come abbiamo discusso nel [Capitolo 3](ch03.html#ch03), le LLMs producono la previsione del token successivo in modo autoregressivo, come si può vedere nella [Figura 5-13](#autoregressive_prediction5).  ![bgai 0513](assets/bgai_0513.png)  ###### Figura 5-13\\. Previsione autoregressiva    Ciò significa che le LLM devono eseguire diverse iterazioni di inferenza in un ciclo per produrre una risposta, e ogni iterazione produce un singolo token di uscita. La sequenza di input cresce man mano che il token di uscita di ogni iterazione viene aggiunto alla fine, e la nuova sequenza viene inoltrata al modello nella fase di iterazione successiva. Una volta che il modello genera un token di fine sequenza, il ciclo di generazione si interrompe. In sostanza, le LLM producono una sequenza di token di completamento, fermandosi solo dopo aver prodotto un token di stop o aver raggiunto la lunghezza massima della sequenza.    Il LLM deve calcolare diverse mappe di attenzione per ogni token della sequenza in modo da poter fare iterativamente le previsioni del token successivo.    Fortunatamente, le GPU possono parallelizzare i calcoli delle mappe di attenzione per ogni iterazione. Come hai imparato, queste mappe di attenzione catturano il significato e il contesto di ogni token all'interno della sequenza di input e sono costose da calcolare.Per questo motivo, per ottimizzare l'inferenza, gli LLMs utilizzano il *caching* *chiave-valore* (KV) per memorizzare le mappe calcolate nella memoria della GPU.    ###### Suggerimento    La formula della mappa di attenzione calcola un *valore (V)* in base a una *query (Q)*e a una *chiave (K)*.    > Q = KV    Questo calcolo deve essere eseguito per ogni token della sequenza, ma fortunatamente può essere vettorializzato utilizzando operazioni di moltiplicazione matriciale di grandi dimensioni su una GPU.    Tuttavia, immagazzinare i parametri nella memoria della GPU per riutilizzarli tra un'iterazione e l'altra può consumare enormi quantità di memoria della GPU. Ad esempio, un modello con 13B parametri consuma quasi 1 MB di stato per ogni token della sequenza, oltre a tutti i 13B parametri del modello. Questo significa che c'è un numero limitato di token che puoi immagazzinare in memoria per riutilizzarli.    Se utilizzi una GPU di fascia alta, come la A100 con 40 GB di RAM, puoi tenere in memoria solo 14 K token contemporaneamente, mentre il resto della memoria viene utilizzato per memorizzare 26 GB di parametri del modello. In breve, la memoria consumata dalla GPU scala con le dimensioni del modello di base più la lunghezza della sequenza di token.    A peggiorare le cose, se devi servire più utenti in contemporanea con richieste in batch, la memoria della tua GPU deve essere condivisa tra più inferenze LLM. Di conseguenza, hai meno memoria per memorizzare sequenze più lunghe e il tuo LLM è vincolato a una finestra di contesto più breve. D'altra parte, se vuoi mantenere una finestra di contesto ampia, non puoi gestire più utenti in contemporanea. Ad esempio, una lunghezza di sequenza di 2048 significa che la dimensione del batch sarà limitata a 7 richieste simultanee (o 7 sequenze prompt). Realisticamente, questo è un limite delimitato e non lascia spazio alla memorizzazione di calcoli intermedi, che ridurranno ulteriormente i numeri sopra citati.    Ciò significa che gli LLMs non riescono a saturare completamente le risorse disponibili della GPU. Il motivo principale è che una parte significativa della banda di memoria della GPU viene consumata per caricare i parametri del modello invece di elaborare gli input.    Il primo passo per ridurre il carico dei tuoi servizi è quello di integrare i modelli più efficienti. Spesso i modelli più piccoli e compressi possono svolgere il lavoro che gli chiedi, con prestazioni simili alle loro controparti più grandi.    Un'altra soluzione adeguata al problema del sottoutilizzo della GPU è l'implementazione del*batching delle richieste*, in cui il modello elabora più input in gruppi, riducendo l'overhead del caricamento dei parametri del modello per ogni richiesta. In questo modo si utilizza in modo più efficiente la larghezza di banda della memoria del chip, ottenendo un maggiore utilizzo dei calcoli, un throughput più elevato e un'inferenza LLM meno costosa. I server di inferenza LLM come vLLM sfruttano il batching più l'attenzione veloce, la cache KV e i meccanismi di attenzione paginata per massimizzare il throughput.    Puoi vedere la differenza di latenza di risposta e di throughput con e senza il batching nella [Figura 5-14](#with_without_batching).  ![bgai 0514](assets/bgai_0514.png)  ###### Figura 5-14\\. Latenza di risposta e throughput del server LLM con e senza batching    Esistono due modi per implementare il batching:    Dosaggio statico      La dimensione del lotto rimane costante.      Dosaggio dinamico o continuo      La dimensione del lotto viene determinata in base alla domanda.      Nel *batching statico*, aspettiamo che arrivi un numero predeterminato di richieste in entrata prima di raggrupparle ed elaborarle attraverso il modello. Tuttavia, poiché le richieste possono terminare in qualsiasi momento all'interno di un batch, stiamo effettivamente ritardando le risposte a ogni richiesta - aumentando la latenza - fino a quando l'intero batch non viene elaborato.    Rilasciare la risorsa GPU può essere complicato anche quando si elabora un batch e si aggiungono nuove richieste al batch che possono trovarsi in stati di completamento diversi. Di conseguenza, la GPU rimane sottoutilizzata perché le sequenze generate all'interno di un batch variano e non corrispondono alla lunghezza della sequenza più lunga di quel batch.    La[Figura 5-15](#static_batching) illustra il batching statico nel contesto dell'inferenza LLM.  ![bgai 0515](assets/bgai_0515.png)  ###### Figura 5-15\\. Batching statico con dimensione fissa del lotto    Nella [Figura 5-15](#static_batching) si notano i blocchi bianchi che rappresentano il tempo di calcolo della GPU sottoutilizzato. Solo una sequenza di input nel batch ha saturato la GPU per tutta la durata dell'elaborazione del batch.    Oltre ad aggiungere inutili tempi di attesa e a non saturare l'utilizzo della GPU, ciò che rende problematico il batching statico è che gli utenti di un servizio di chatbot alimentato da LLM non forniranno prompt di lunghezza fissa o non si aspetteranno output di lunghezza fissa. La variazione degli output generati potrebbe causare un massiccio sottoutilizzo delle GPU.    Una soluzione consiste nell'evitare di assumere sequenze di input o output fisse e nell'impostare invece dimensioni dinamiche del batch durante l'elaborazione di un batch.Nel *batch* *dinamico* o *continuo*, la dimensione del batch può essere impostata in base alla lunghezza della sequenza di richieste in arrivo e alla risorsa GPU disponibile. Con questo approccio, le richieste di nuova generazione possono essere inserite in un batch sostituendo le richieste completate per ottenere un utilizzo della GPU più elevato rispetto al batch statico.    La[Figura 5-16](#dynamic_batching) mostra come il batching dinamico o continuo possa saturare completamente la risorsa GPU.  ![bgai 0516](assets/bgai_0516.png)  ###### Figura 5-16\\. Dosaggio dinamico/continuo con dimensione variabile del lotto    Mentre i parametri del modello vengono caricati, le richieste possono continuare ad arrivare e il server di inferenza LLM le pianifica e le inserisce nel batch per massimizzare l'utilizzo della GPU. Questo approccio porta a un throughput più elevato e a una latenza ridotta.    Se stai costruendo un server di inferenza LLM, probabilmente vorrai inserire il meccanismo di batching continuo nel tuo server. Tuttavia, la buona notizia è che il server vLLM fornisce già il batching continuo con il suo server FastAPI, quindi non dovrai implementare tutto questo da solo. Inoltre, viene fornito con un'altra importante funzionalità di ottimizzazione della GPU, che lo distingue da altri framework di inferenza LLM alternativi: l'attenzione paginata.    ### Attenzione al pager    L'uso efficiente della memoria è una sfida cruciale per i sistemi che gestiscono un servizio ad alto rendimento, in particolare per gli LLMs. Per un'inferenza più veloce, i modelli odierni si affidano alle *cache KV* per memorizzare e riutilizzare le mappe di attenzione, che crescono esponenzialmente con l'aumentare della lunghezza delle sequenze di input.    L*'attenzione a pagine* è una soluzione innovativa progettata per ridurre al minimo la richiesta di memoria di queste cache KV, migliorando così l'efficienza della memoria degli LLMs e rendendoli più praticabili per l'uso su dispositivi con risorse limitate. Negli LLMs basati su trasformatori, i tensori delle chiavi e dei valori di attenzione vengono generati per ogni token di ingresso per catturare il contesto essenziale. Invece di ricalcolare questi tensori a ogni passo, vengono salvati nella memoria della GPU sotto forma di cache KV, che funge da memoria del modello. Tuttavia, la cache KV può raggiungere dimensioni enormi, come 40 GB per un modello con 13B parametri, rappresentando una sfida significativa per l'archiviazione e l'accesso efficienti, in particolare su hardware con risorse limitate.    L'attenzione a pagine introduce un metodo che suddivide la cache KV in segmenti più piccoli e gestibili, chiamati *pagine*, ognuno dei quali contiene un vettore KV per un determinato numero di token. Grazie a questa segmentazione, l'attenzione a pagine può caricare e accedere in modo efficiente alla cache KV durante i calcoli dell'attenzione. Si può paragonare questa tecnica al modo in cui la memoria virtuale viene gestita dai sistemi operativi, in cui la disposizione logica dei dati è separata dalla loro memorizzazione fisica. In sostanza, una tabella di blocchi mappa i blocchi logici in quelli fisici, consentendo l'allocazione dinamica della memoria quando vengono elaborati nuovi token. L'idea di base è quella di evitare la frammentazione della memoria sfruttando i blocchi logici (invece di quelli fisici) e di utilizzare una tabella di mappatura per accedere rapidamente ai dati archiviati in una memoria fisica paginata.    Puoi suddividere il meccanismo dell'attenzione paginata in diverse fasi:    Partizionare la cache KV      La cache è suddivisa in pagine di dimensioni fisse, ognuna delle quali contiene una parte delle coppie chiave-valore.      Creare la tabella di ricerca      Viene creata una tabella per mappare le chiavi di query alle pagine corrispondenti, facilitando l'assegnazione e il recupero rapido.      Caricamento selettivo      Durante l'inferenza vengono caricate solo le pagine necessarie per la sequenza di input corrente, riducendo l'ingombro della memoria.      Calcolo dell'attenzione      Il modello calcola l'attenzione utilizzando le coppie chiave-valore delle pagine caricate. Questo approccio mira a rendere gli LLMs più accessibili affrontando il collo di bottiglia della memoria, consentendo potenzialmente la loro distribuzione su una gamma più ampia di dispositivi.      Le fasi sopra descritte permettono al server vLLM di massimizzare l'efficienza dell'uso della memoria attraverso la mappatura dei blocchi di memoria fisica e logica in modo che la cache KV sia memorizzata e recuperata in modo efficiente durante la generazione.    In un [post pubblicato sul blog di Anyscale.com](https://oreil.ly/WgRfJ), gli autori hanno studiato e confrontato le prestazioni di vari framework di LLM durante l'inferenza, concludendo che i meccanismi di attenzione paginata e di batching continuo sono così potenti nell'ottimizzare l'uso della memoria della GPU che il server vLLM è stato in grado di ridurre le latenze di 4 volte e il throughput fino a 23 volte.    Nella prossima sezione, rivolgeremo la nostra attenzione ai carichi di lavoro GenAI che possono richiedere molto tempo per essere elaborati e che sono delimitati dal calcolo. Questo è principalmente il caso di modelli non LLM di grandi dimensioni come SDXL, dove l'esecuzione di inferenze in batch (come la generazione di immagini in batch) per più utenti può rivelarsi .````  ```py`# Gestione dei compiti di inferenza dell'intelligenza artificiale a lungo termine    Grazie alla possibilità di ospitare i modelli in un processo separato al di fuori del ciclo di eventi FastAPI, puoi rivolgere la tua attenzione alle operazioni bloccanti che richiedono molto tempo per essere completate.    Nella sezione precedente, hai sfruttato framework specializzati come vLLM per ospitare esternamente e ottimizzare i carichi di lavoro di inferenza dei tuoi LLMs. Tuttavia, potresti ancora imbatterti in modelli che possono richiedere molto tempo per generare risultati. Per evitare che i tuoi utenti attendano, dovresti gestire le attività che generano modelli e che richiedono molto tempo per essere completate.    Diversi modelli GenAI, come Stable Diffusion XL, possono richiedere diversi minuti, anche su una GPU, per produrre risultati. Nella maggior parte dei casi, puoi chiedere ai tuoi utenti di aspettare fino al completamento del processo di generazione. Ma se gli utenti utilizzano un singolo modello contemporaneamente, il server dovrà mettere in coda queste richieste. Quando i tuoi utenti lavorano con i modelli generativi, devono interagire con essi più volte per guidare il modello verso i risultati desiderati. Questo modello di utilizzo crea un grande arretrato di richieste e gli utenti alla fine della coda dovranno aspettare a lungo prima di vedere i risultati.    Se ci fosse un modo per gestire le attività di lunga durata senza far aspettare gli utenti, sarebbe perfetto. Fortunatamente, FastAPI fornisce un meccanismo per risolvere questo tipo di problemi.    I *compiti in background* di FastAPI sono un meccanismo che puoi sfruttare per rispondere agli utenti mentre i tuoi modelli sono impegnati nell'elaborazione delle richieste. Questa funzione ti è stata presentata brevemente durante la costruzione del modulo RAG, dove un compito in background popolava un database vettoriale con il contenuto dei documenti PDF caricati.    Utilizzando i task in background, i tuoi utenti possono continuare a inviare richieste o a svolgere la loro giornata senza dover aspettare. Puoi salvare i risultati su disco o su un database per poterli recuperare in un secondo momento, oppure fornire un sistema di polling in modo che il client possa fare un ping per ricevere gli aggiornamenti mentre il modello elabora le richieste. Un'altra opzione è quella di creare una connessione live tra il client e il server in modo che l'interfaccia utente venga aggiornata con i risultati non appena questi sono disponibili. Tutte queste soluzioni sono realizzabili con i task in background di FastAPI.    L['esempio 5-18](#fastapi_background_tasks) mostra come implementare i task in background per gestire le inferenze del modello a lungo termine.    ##### Esempio 5-18\\. Utilizzo di task in background per gestire l'inferenza del modello a lungo termine (ad esempio, la generazione di immagini in batch)    ``` # main.py  from fastapi import BackgroundTasks import aiofiles  ...  async def batch_generate_image(prompt: str, count: int) -> None:     images = generate_images(prompt, count) ![1](assets/1.png)     for i, image in enumerate(images):         async with aiofiles.open(f\"output_{i}.png\", mode='wb') as f:             await f.write(image) ![2](assets/2.png)  @app.get(\"/generate/image/background\") def serve_image_model_background_controller(     background_tasks: BackgroundTasks, prompt: str, count: int ![3](assets/3.png) ):     background_tasks.add_task(batch_generate_image, prompt, count) ![4](assets/4.png)     return {\"message\": \"Task is being processed in the background\"} ![5](assets/5.png) ```py    [![1](assets/1.png)](#co_achieving_concurrency_in_ai_workloads_CO17-1)      Genera più immagini in un lotto utilizzando un'API esterna di model-serving come [Ray Serve](https://oreil.ly/NjlV4).      [![2](assets/2.png)](#co_achieving_concurrency_in_ai_workloads_CO17-2)      Esegui il loop delle immagini generate e salva ogni immagine su disco in modo asincrono utilizzando la libreria `aiofiles`. In produzione, puoi anche salvare le immagini di output su soluzioni di cloud storage da cui i clienti possono attingere direttamente.      [![3](assets/3.png)](#co_achieving_concurrency_in_ai_workloads_CO17-3)      Abilita il controller ad eseguire attività in background.      [![4](assets/4.png)](#co_achieving_concurrency_in_ai_workloads_CO17-4)      Passa la definizione della funzione `batch_generate_image` a un gestore di attività in background FastAPI con gli argomenti richiesti.      [![5](assets/5.png)](#co_achieving_concurrency_in_ai_workloads_CO17-5)      Restituisce un generico messaggio di successo al client prima di elaborare l'attività in background, in modo che l'utente non rimanga in attesa.      Nell'[Esempio 5-18](#fastapi_background_tasks), stai permettendo a FastAPI di eseguire operazioni di inferenza in background (tramite un'API esterna del server del modello) in modo che il ciclo degli eventi rimanga sbloccato per elaborare altre richieste in arrivo. Puoi anche eseguire più attività in background, come la generazione di immagini in batch (in processi separati) e l'invio di e-mail di notifica. Queste attività vengono aggiunte a una coda ed elaborate in modo sequenziale senza bloccare l'utente. Puoi quindi memorizzare le immagini generate ed esporre un endpoint aggiuntivo che i clienti possono utilizzare per richiedere aggiornamenti sullo stato e recuperare i risultati dell'inferenza.    ###### Avvertenze    I task in background vengono eseguiti nello stesso ciclo di eventi, ma non forniscono un vero parallelismo, bensì solo una concomitanza.    Se esegui operazioni pesanti per la CPU, come l'inferenza dell'intelligenza artificiale, nei taskin background, bloccherai il ciclo di eventi principale fino a quando tutti i task in background non saranno stati completati. Allo stesso modo, fai attenzione ai taskin background asincroni: se non attendi le operazioni di I/O bloccanti, il task bloccherà il server principale dalla risposta ad altre richieste, anche se viene eseguito in background. FastAPI esegue i task in background non asincroni in un pool di thread interno.    Sebbene i task in background di FastAPI siano un ottimo strumento per gestire semplici lavori in batch, non sono scalabili e non sono in grado di gestire le eccezioni o i tentativi di risposta come gli strumenti specializzati. Altri framework per il servizio di ML come Ray Serve, BentoML e vLLM possono gestire meglio il servizio di modelli in scala fornendo funzioni come il batching delle richieste. Anche strumenti più sofisticati come Celery (un gestore di code), Redis (un database per la cache) e RabbitMQ (un broker di messaggi) possono essere utilizzati in combinazione per implementare una pipeline di inferenza più robusta e affidabile.    # Sommario    Questo capitolo ha esplorato gli aspetti complessi dell'applicazione della concomitanza nei sistemi di intelligenza artificiale.    Sei stato introdotto ai concetti di concorrenza e parallelismo, compresi i vari tipi di operazioni bloccanti che impediscono di servire simultaneamente gli utenti. Hai scoperto le tecniche di concorrenza come il multithreading, il multiprocessing e la programmazione asincrona, oltre alle loro differenze, somiglianze, vantaggi e svantaggi in vari casi d'uso.    Poi hai imparato a conoscere i pool di thread e i loop di eventi, in particolare in un ambiente server FastAPI, e hai compreso il loro ruolo nell'elaborazione delle richieste in modo concorrente. Questo ha comportato la comprensione di come e perché il server si può bloccare se non stai attento a come dichiari i gestori delle rotte.    In seguito, hai scoperto come implementare la programmazione asincrona per gestire le operazioni di blocco I/O. Attraverso esempi pratici, hai sviluppato una comprensione più profonda delle interazioni asincrone con i database e i contenuti web, costruendo sia un web scraper che un modulo RAG.    Inoltre, hai visto perché i modelli GenAI più grandi possono essere affamati di memoria e creare operazioni di blocco delimitate dalla memoria. In questo contesto, ti sono state presentate tecniche di ottimizzazione della memoria come il batching continuo e l'attenzione paginata nel servire LLMs per ridurre al minimo i colli di bottiglia legati alla memoria.    Infine, hai imparato a conoscere gli approcci per gestire i processi di inferenza dell'intelligenza artificiale di lunga durata, assicurandoti che il tuo servizio rimanga reattivo anche per operazioni prolungate.    Grazie alle conoscenze acquisite in questo capitolo, ora sei pronto ad applicare i principi della concorrenza ai tuoi servizi, creando applicazioni AI resilienti, scalabili e ad alte prestazioni.    La capacità di gestire più utenti contemporaneamente è un traguardo importante, ma ci sono ulteriori ottimizzazioni che puoi effettuare per migliorare ulteriormente l'esperienza utente dei tuoi servizi GenAI. Puoi fornire aggiornamenti in tempo reale tramite tecnologie di streaming per mostrare progressivamente agli utenti risultati quasi in tempo reale durante la generazione. Questo è particolarmente utile per gli LLMs che possono avere tempi di generazione più lunghi in scenari di conversazione.    Il prossimo capitolo esplorerà i carichi di lavoro in streaming dell'intelligenza artificiale, illustrando l'uso di tecnologie di comunicazione in tempo reale come gli eventi inviati dal server (SSE) e i WebSocket (WS). Imparerai la differenza tra queste tecnologie e come implementare lo streaming di modelli costruendo endpoint per interazioni in tempo reale da testo a testo, da testo a voce e da voce a testo.    # Riferimenti aggiuntivi    *   Kwon, W., et al. (2023).[\"Efficient Memory Management for Large Language Model Serving with PagedAttention\".](https://oreil.ly/PtCqL) arXiv preprint arXiv:2309.06180.           *   Lewis, P., et al. (2022).[\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\".](https://oreil.ly/r5yVL) arXiv preprint arXiv:2005.11401.              ^([1](ch05.html#id795-marker)) Un core è una singola unità di elaborazione all'interno di una CPU o di una GPU che esegue le istruzioni. Le CPU e le GPU moderne dispongono di più core per eseguire i compiti simultaneamente.    ^([2](ch05.html#id802-marker)) Il multithreading nella maggior parte dei linguaggi è parallelo (eseguito su più core) e non concorrente. Python sta cambiando nelle prossime versioni per fare lo stesso (free-threading Python).    ^([3](ch05.html#id817-marker)) Puoi trovare un'implementazione personalizzata in [OpenAI Cookbook su GitHub](https://oreil.ly/8E7GQ).    ^([4](ch05.html#id822-marker)) Il costo dell'impostazione dei fili viene comunque sostenuto, ma viene fatto in anticipo per evitare di farlo al volo in seguito.    ^([5](ch05.html#id830-marker)) P. Lewis et al. (2022), [\"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\",](https://oreil.ly/GCk08) arXiv preprint arXiv:2005.11401.    ^([6](ch05.html#id841-marker)) L'operazione del prodotto di punti moltiplica i componenti di due vettori e poi somma i risultati. Può essere utilizzata per calcolare il coseno dell'angolo tra i vettori per quantificare la loro somiglianza di direzione (cioè l'allineamento). I database vettoriali la utilizzano per eseguire ricerche semantiche sugli embedding dei documenti.    ^([7](ch05.html#id843-marker)) Consulta la [documentazione](https://oreil.ly/V4itQ) di [Docker](https://oreil.ly/V4itQ) per le istruzioni di installazione.```` ```py`` ``````", "```````"]