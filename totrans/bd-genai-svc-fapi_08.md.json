["```py\nimport time\n\ndef task():\n    print(\"Start of sync task\")\n    time.sleep(5) ![1](assets/1.png)\n    print(\"After 5 seconds of sleep\")\n\nstart = time.time()\nfor _ in range(3): ![2](assets/2.png)\n    task()\nduration = time.time() - start\nprint(f\"\\nProcess completed in: {duration} seconds\")\n\"\"\"\nStart of sync task\nAfter 5 seconds of sleep\nStart of sync task\nAfter 5 seconds of sleep\nStart of sync task\nAfter 5 seconds of sleep\n\nProcess completed in: 15.014271020889282 seconds\n\"\"\"\n```", "```py\nimport time\nimport asyncio\n\nasync def task(): ![1](assets/1.png)\n    print(\"Start of async task\")\n    await asyncio.sleep(5) ![2](assets/2.png)\n    print(\"Task resumed after 5 seconds\")\n\nasync def spawn_tasks():\n    await asyncio.gather(task(), task(), task()) ![3](assets/3.png)\n\nstart = time.time()\nasyncio.run(spawn_tasks()) ![4](assets/4.png)\nduration = time.time() - start\n\nprint(f\"\\nProcess completed in: {duration} seconds\")\n\"\"\"\nStart of async task\nStart of async task\nStart of async task\nTask resumed after 5 seconds\nTask resumed after 5 seconds\nTask resumed after 5 seconds\n\nProcess completed in: 5.0057971477508545 seconds ![5](assets/5.png) \"\"\"\n```", "```py\nimport asyncio\n\nasync def main():\n    print(\"Before sleeping\")\n    await asyncio.sleep(3) ![1](assets/1.png)\n    print(\"After sleeping for 3 seconds\")\n\nasyncio.run(main()) ![2](assets/2.png)\n\n\"\"\"\nBefore sleeping\nAfter sleeping for 3 seconds ![3](assets/3.png) \"\"\"\n```", "```py\n$ pip install openai\n```", "```py\nimport os\nfrom fastapi import FastAPI, Body\nfrom openai import OpenAI, AsyncOpenAI\n\napp = FastAPI()\n\nsync_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\nasync_client = AsyncOpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n\n@app.post(\"/sync\")\ndef sync_generate_text(prompt: str = Body(...)):\n    completion = sync_client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": prompt,\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n    return completion.choices[0].message.content\n\n@app.post(\"/async\")\nasync def async_generate_text(prompt: str = Body(...)):\n    completion = await async_client.chat.completions.create(\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": prompt,\n            }\n        ],\n        model=\"gpt-3.5-turbo\",\n    )\n    return completion.choices[0].message.content\n```", "```py\nimport os\nfrom fastapi import FastAPI\nfrom openai import AsyncOpenAI, OpenAI\n\napp = FastAPI()\n\n@app.get(\"/block\")\nasync def block_server_controller():\n    completion = sync_client.chat.completions.create(...) ![1](assets/1.png)\n    return completion.choices[0].message.content\n\n@app.get(\"/slow\")\ndef slow_text_generator():\n    completion = sync_client.chat.completions.create(...) ![2](assets/2.png)\n    return completion.choices[0].message.content\n\n@app.get(\"/fast\")\nasync def fast_text_generator():\n    completion = await async_client.chat.completions.create(...) ![3](assets/3.png)\n    return completion.choices[0].message.content\n```", "```py\n$ pip install beautifulsoup lxml aiohttp\n```", "```py\n# scraper.py\n\nimport asyncio\nimport re\n\nimport aiohttp\nfrom bs4 import BeautifulSoup\nfrom loguru import logger\n\ndef extract_urls(text: str) -> list[str]:\n    url_pattern = r\"(?P<url>https?:\\/\\/[^\\s]+)\" ![1](assets/1.png)\n    urls = re.findall(url_pattern, text) ![2](assets/2.png)\n    return urls\n\ndef parse_inner_text(html_string: str) -> str:\n    soup = BeautifulSoup(html_string, \"lxml\")\n    if content := soup.find(\"div\", id=\"bodyContent\"): ![3](assets/3.png)\n        return content.get_text()\n    logger.warning(\"Could not parse the HTML content\")\n    return \"\"\n\nasync def fetch(session: aiohttp.ClientSession, url: str) -> str:\n    async with session.get(url) as response: ![4](assets/4.png)\n        html_string = await response.text()\n        return parse_inner_text(html_string)\n\nasync def fetch_all(urls: list[str]) -> str:\n    async with aiohttp.ClientSession() as session: ![5](assets/5.png)\n        results = await asyncio.gather(\n            *[fetch(session, url) for url in urls], return_exceptions=True\n        )\n    success_results = [result for result in results if isinstance(result, str)]\n    if len(results) != len(success_results): ![6](assets/6.png)\n        logger.warning(\"Some URLs could not be fetched\")\n    return \" \".join(success_results)\n```", "```py\n# dependencies.py\n\nfrom fastapi import Body\nfrom loguru import logger\n\nfrom schemas import TextModelRequest\nfrom scraper import extract_urls, fetch_all\n\nasync def get_urls_content(body: TextModelRequest = Body(...)) -> str: ![1](assets/1.png)\n    urls = extract_urls(body.prompt)\n    if urls:\n        try:\n            urls_content = await fetch_all(urls)\n            return urls_content\n        except Exception as e:\n            logger.warning(f\"Failed to fetch one or several URls - Error: {e}\")\n    return \"\"\n\n# main.py\n\nfrom fastapi import Body, Depends, Request\nfrom dependencies import construct_prompt\nfrom schemas import TextModelResponse\n\n@app.post(\"/generate/text\", response_model_exclude_defaults=True) ![2](assets/2.png)\nasync def serve_text_to_text_controller(\n    request: Request,\n    body: TextModelRequest = Body(...),\n    urls_content: str = Depends(get_urls_content) ![3](assets/3.png)\n) -> TextModelResponse:\n    ... # rest of controller logic\n    prompt = body.prompt + \" \" + urls_content\n    output = generate_text(models[\"text\"], prompt, body.temperature)\n    return TextModelResponse(content=output, ip=request.client.host)\n```", "```py\n$ pip install aiofiles python-multipart\n```", "```py\n# upload.py\n\nimport os\nimport aiofiles\nfrom aiofiles.os import makedirs\nfrom fastapi import UploadFile\n\nDEFAULT_CHUNK_SIZE = 1024 * 1024 * 50  # 50 megabytes\n\nasync def save_file(file: UploadFile) -> str:\n    await makedirs(\"uploads\", exist_ok=True)\n    filepath = os.path.join(\"uploads\", file.filename)\n    async with aiofiles.open(filepath, \"wb\") as f:\n        while chunk := await file.read(DEFAULT_CHUNK_SIZE):\n            await f.write(chunk)\n    return filepath\n\n# main.py\n\nfrom fastapi import FastAPI, HTTPException, status, File\nfrom typing import Annotated\nfrom upload import save_file\n\n@app.post(\"/upload\")\nasync def file_upload_controller(\n    file: Annotated[UploadFile, File(description=\"Uploaded PDF documents\")]\n):\n    if file.content_type != \"application/pdf\":\n        raise HTTPException(\n            detail=f\"Only uploading PDF documents are supported\",\n            status_code=status.HTTP_400_BAD_REQUEST,\n        )\n    try:\n        await save_file(file)\n    except Exception as e:\n        raise HTTPException(\n            detail=f\"An error occurred while saving file - Error: {e}\",\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n        )\n    return {\"filename\": file.filename, \"message\": \"File uploaded successfully\"}\n\n# client.py\n\nimport requests\nimport streamlit as st\n\nst.write(\"Upload a file to FastAPI\")\nfile = st.file_uploader(\"Choose a file\", type=[\"pdf\"])\n\nif st.button(\"Submit\"):\n    if file is not None:\n        files = {\"file\": (file.name, file, file.type)}\n        response = requests.post(\"http://localhost:8000/upload\", files=files)\n        st.write(response.text)\n    else:\n        st.write(\"No file uploaded.\")\n```", "```py\n$ pip install qdrant_client aiofiles pypdf loguru\n```", "```py\n# rag/extractor.py\n\nfrom pypdf import PdfReader\n\ndef pdf_text_extractor(filepath: str) -> None:\n    content = \"\"\n    pdf_reader = PdfReader(filepath, strict=True) ![1](assets/1.png)\n    for page in pdf_reader.pages:\n        page_text = page.extract_text()\n        if page_text:\n            content += f\"{page_text}\\n\\n\" ![2](assets/2.png)\n    with open(filepath.replace(\"pdf\", \"txt\"), \"w\", encoding=\"utf-8\") as file: ![3](assets/3.png)\n        file.write(content)\n```", "```py\n# rag/transform.py\n\nimport re\nfrom typing import Any, AsyncGenerator\n\nimport aiofiles\nfrom transformers import AutoModel\n\nDEFAULT_CHUNK_SIZE = 1024 * 1024 * 50  # 50 megabytes\n\nembedder = AutoModel.from_pretrained(\n    \"jinaai/jina-embeddings-v2-base-en\", trust_remote_code=True ![1](assets/1.png)\n)\n\nasync def load(filepath: str) -> AsyncGenerator[str, Any]:\n    async with aiofiles.open(filepath, \"r\", encoding=\"utf-8\") as f: ![2](assets/2.png)\n        while chunk := await f.read(DEFAULT_CHUNK_SIZE): ![3](assets/3.png)\n            yield chunk ![4](assets/4.png)\n\ndef clean(text: str) -> str:\n    t = text.replace(\"\\n\", \" \")\n    t = re.sub(r\"\\s+\", \" \", t)\n    t = re.sub(r\"\\. ,\", \"\", t)\n    t = t.replace(\"..\", \".\")\n    t = t.replace(\". .\", \".\")\n    cleaned_text = t.replace(\"\\n\", \" \").strip()\n    return cleaned_text ![5](assets/5.png)\n\ndef embed(text: str) -> list[float]:\n    return embedder.encode(text).tolist() ![6](assets/6.png)\n```", "```py\n$ docker pull qdrant/qdrant ![1](assets/1.png)\n$ docker run -p 6333:6333 -p 6334:6334 \\  ![2](assets/2.png)\n    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\ ![3](assets/3.png)\n    qdrant/qdrant\n```", "```py\n# rag/repository.py\n\nfrom loguru import logger\nfrom qdrant_client import AsyncQdrantClient\nfrom qdrant_client.http import models\nfrom qdrant_client.http.models import ScoredPoint\n\nclass VectorRepository: ![1](assets/1.png)\n    def __init__(self, host: str = \"localhost\", port: int = 6333) -> None:\n        self.db_client = AsyncQdrantClient(host=host, port=port)\n\n    async def create_collection(self, collection_name: str, size: int) -> bool: ![2](assets/2.png)\n        vectors_config = models.VectorParams(\n            size=size, distance=models.Distance.COSINE ![3](assets/3.png)\n        )\n        response = await self.db_client.get_collections()\n\n        collection_exists = any(\n            collection.name == collection_name\n            for collection in response.collections\n        )\n        if collection_exists: ![4](assets/4.png)\n            logger.debug(\n                f\"Collection {collection_name} already exists - recreating it\"\n            )\n            await self.db_client.delete_collection(collection_name)\n            return await self.db_client.create_collection(\n                collection_name,\n                vectors_config=vectors_config,\n            )\n\n        logger.debug(f\"Creating collection {collection_name}\")\n        return await self.db_client.create_collection(\n            collection_name=collection_name,\n            vectors_config=models.VectorParams(\n                size=size, distance=models.Distance.COSINE\n            ),\n        )\n\n    async def delete_collection(self, name: str) -> bool:\n        logger.debug(f\"Deleting collection {name}\")\n        return await self.db_client.delete_collection(name)\n\n    async def create(\n        self,\n        collection_name: str,\n        embedding_vector: list[float],\n        original_text: str,\n        source: str,\n    ) -> None:\n        response = await self.db_client.count(collection_name=collection_name)\n        logger.debug(\n            f\"Creating a new vector with ID {response.count} \"\n            f\"inside the {collection_name}\"\n        )\n        await self.db_client.upsert(\n            collection_name=collection_name,\n            points=[\n                models.PointStruct(\n                    id=response.count,\n                    vector=embedding_vector,\n                    payload={\n                        \"source\": source,\n                        \"original_text\": original_text,\n                    },\n                )\n            ],\n        )\n\n    async def search(\n        self,\n        collection_name: str,\n        query_vector: list[float],\n        retrieval_limit: int,\n        score_threshold: float, ![5](assets/5.png)\n    ) -> list[ScoredPoint]:\n        logger.debug(\n            f\"Searching for relevant items in the {collection_name} collection\"\n        )\n        response = await self.db_client.query_points(\n            collection_name=collection_name,\n            query_vector=query_vector,\n            limit=retrieval_limit,\n            score_threshold=score_threshold,\n        )\n        return response.points\n```", "```py\n# rag/service.py\n\nimport os\n\nfrom loguru import logger\nfrom .repository import VectorRepository\nfrom .transform import clean, embed, load\n\nclass VectorService(VectorRepository): ![1](assets/1.png)\n    def __init__(self):\n        super().__init__()\n\n    async def store_file_content_in_db( ![2](assets/2.png)\n        self,\n        filepath: str,\n        chunk_size: int = 512,\n        collection_name: str = \"knowledgebase\",\n        collection_size: int = 768,\n    ) -> None:\n        await self.create_collection(collection_name, collection_size)\n        logger.debug(f\"Inserting {filepath} content into database\")\n        async for chunk in load(filepath, chunk_size): ![3](assets/3.png)\n            logger.debug(f\"Inserting '{chunk[0:20]}...' into database\")\n\n            embedding_vector = embed(clean(chunk))\n            filename = os.path.basename(filepath)\n            await self.create(\n                collection_name, embedding_vector, chunk, filename\n            )\n\nvector_service = VectorService() ![4](assets/4.png)\n```", "```py\n# main.py\n\nfrom fastapi import (\n    BackgroundTasks,\n    FastAPI,\n    File,\n    UploadFile,\n    status,\n    HTTPException,\n)\nfrom typing import Annotated\nfrom rag import pdf_text_extractor, vector_service\n\n@app.post(\"/upload\")\nasync def file_upload_controller(\n    file: Annotated[UploadFile, File(description=\"A file read as UploadFile\")],\n    bg_text_processor: BackgroundTasks, ![1](assets/1.png)\n):\n    ... # Raise an HTTPException if data upload is not a PDF file\n    try:\n        filepath = await save_file(file)\n        bg_text_processor.add_task(pdf_text_extractor, filepath) ![2](assets/2.png)\n        bg_text_processor.add_task( ![3](assets/3.png)\n            vector_service.store_file_content_in_db,\n            filepath.replace(\"pdf\", \"txt\"),\n            512,\n            \"knowledgebase\",\n            768,\n        )\n\n    except Exception as e:\n        raise HTTPException(\n            detail=f\"An error occurred while saving file - Error: {e}\",\n            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n        )\n    return {\"filename\": file.filename, \"message\": \"File uploaded successfully\"}\n```", "```py\n# dependencies.py\n\nfrom rag import vector_service\nfrom rag.transform import embed\nfrom schemas import TextModelRequest, TextModelResponse\n\nasync def get_rag_content(body: TextModelRequest = Body(...)) -> str: ![1](assets/1.png)\n    rag_content = await vector_service.search( ![2](assets/2.png)\n        \"knowledgebase\", embed(body.prompt), 3, 0.7\n    )\n    rag_content_str = \"\\n\".join( ![3](assets/3.png)\n        [c.payload[\"original_text\"] for c in rag_content]\n    )\n\n    return rag_content_str\n\n# main.py\n\n... # other imports\nfrom dependencies import get_rag_content, get_urls_content\n\n@app.post(\"/generate/text\", response_model_exclude_defaults=True)\nasync def serve_text_to_text_controller(\n    request: Request,\n    body: TextModelRequest = Body(...),\n    urls_content: str = Depends(get_urls_content),\n    rag_content: str = Depends(get_rag_content), ![4](assets/4.png)\n) -> TextModelResponse:\n    ... # Raise HTTPException for invalid models\n    prompt = body.prompt + \" \" + urls_content + rag_content\n    output = generate_text(models[\"text\"], prompt, body.temperature)\n    return TextModelResponse(content=output, ip=request.client.host)\n```", "```py\n$ pip install vllm\n```", "```py\n$ python -m vllm.entrypoints.openai.api_server \\ ![1](assets/1.png)\n--model \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \\ --dtype float16 \\ ![2](assets/2.png)\n--tensor-parallel-size 4 \\ ![3](assets/3.png)\n--api-key \"your_secret_token\" ![4](assets/4.png)\n```", "```py\n# models.py\n\nimport os\nimport aiohttp\nfrom loguru import logger\n\nasync def generate_text(prompt: str, temperature: float = 0.7) -> str:\n    system_prompt = \"You are an AI assistant\"\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    data = {\"temperature\": temperature, \"messages\": messages}\n    headers = {\"Authorization\": f\"Bearer {os.environ.get('VLLM_API_KEY')}\"}\ntry:\n   async with aiohttp.ClientSession() as session: ![1](assets/1.png)\n        response = await session.post(\n            \"http://localhost:8000/v1/chat\", json=data, headers=headers\n        )\n        predictions = await response.json()\nexcept Exception as e:\n    logger.error(f\"Failed to obtain predictions from vLLM - Error: {e}\")\n    return (\n        \"Failed to obtain predictions from vLLM - \"\n        \"See server logs for more details\"\n    )\ntry:\n    output = predictions[\"choices\"][0][\"message\"][\"content\"] ![2](assets/2.png)\n    logger.debug(f\"Generated text: {output}\")\n    return output\nexcept KeyError as e:\n    logger.error(f\"Failed to parse predictions from vLLM - Error: {e}\")\n    return (\n        \"Failed to parse predictions from vLLM - \"\n        \"See server logs for more details\"\n    )\n```", "```py\n# main.py\n\nfrom fastapi import FastAPI, Request\nfrom schemas import TextModelRequest, TextModelResponse\nfrom models import generate_text\n\n# Remove the asynccontextmanager to remove TinyLlama from FastAPI ![1](assets/1.png)\n# @asynccontextmanager\n# async def lifespan(app: FastAPI):\n#     models[\"text\"] = load_text_model()\n#     yield\n#     models.clear()\n\n# Remove the `lifespan` argument from `FastAPI()`\napp = FastAPI()\n\n@app.post(\"/generate/text\")\nasync def serve_text_to_text_controller(\n    request: Request, body: TextModelRequest\n) -> TextModelResponse: ![2](assets/2.png)\n    ...  # controller logic\n    output = await generate_text(body.prompt, body.temperature)\n    return TextModelResponse(content=output, ip=request.client.host)\n```", "```py\n# main.py\n\nfrom fastapi import BackgroundTasks\nimport aiofiles\n\n...\n\nasync def batch_generate_image(prompt: str, count: int) -> None:\n    images = generate_images(prompt, count) ![1](assets/1.png)\n    for i, image in enumerate(images):\n        async with aiofiles.open(f\"output_{i}.png\", mode='wb') as f:\n            await f.write(image) ![2](assets/2.png)\n\n@app.get(\"/generate/image/background\")\ndef serve_image_model_background_controller(\n    background_tasks: BackgroundTasks, prompt: str, count: int ![3](assets/3.png)\n):\n    background_tasks.add_task(batch_generate_image, prompt, count) ![4](assets/4.png)\n    return {\"message\": \"Task is being processed in the background\"} ![5](assets/5.png)\n```"]