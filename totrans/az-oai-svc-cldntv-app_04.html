<html><head></head><body>
<div id="book-content" class="calibre2">
<div id="sbo-rt-content" class="calibre3"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 3. Implementing Cloud Native Generative AI with Azure OpenAI Service" class="calibre5"><div class="preface" id="implementing_cloud_native_generative_ai_with_azure">
      <h1 class="calibre4"><span class="keep-together">Chapter 3. </span>Implementing Cloud Native Generative AI with Azure OpenAI Service</h1>
      <p class="subtitle">This chapter will focus on the implementation of generative AI architectures<a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" id="xi_implementationofgenerativeAI3386" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> with Microsoft Azure and Azure OpenAI models, always aiming to present all available options, and minimize the required development, integration, and usage cost, while accelerating the operationalization. For that purpose, I’ve included a series of best practices and typical architectures that will allow you to choose the best building blocks for your specific scenarios. </p>
      <p class="subtitle">We will include the most relevant Azure OpenAI implementation approaches, based on existing features and repositories that will continue evolving, improving, and including new functionalities. I’ve included URLs to the original documentation because they are continuously updated with new features, so these links will allow you to explore any details you need. Most of them rely on official accelerators from GitHub repositories, and projects that you will be able to follow and/or fork. But before getting into the details, let’s explore some fundamental topics that will help you understand the full extent of what a generative AI with Azure OpenAI Service means.</p>
      <section data-type="sect1" class="calibre5" data-pdf-bookmark="Defining the Knowledge Scope of Azure OpenAI Service–Enabled Apps"><div class="preface" id="defining_the_knowledge_scope_of_azure_openai_enabl">
        <h1 class="calibre4">Defining the Knowledge Scope of Azure OpenAI Service–Enabled Apps</h1>
        <p class="subtitle">Generative AI applications<a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="knowledge scope of Azure OpenAI-enabled apps" id="xi_implementationofgenerativeAIknowledgescopeofAzureOpenAIenabledapps3738" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Azure OpenAI Service" data-secondary="knowledge scope of enabled apps" id="xi_AzureOpenAIServiceknowledgescopeofenabledapps3738" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="knowledge scope of Azure OpenAI-enabled apps" id="xi_knowledgescopeofAzureOpenAIenabledapps3738" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> on Microsoft Azure are not only for regular ChatGPT-type applications. They are advanced architectures that rely on diverse technology pieces, including the core infrastructure (servers, <a href="https://oreil.ly/y5mXm" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">GPUs</a>, etc.) required to run generative AI models, and that allow adopters to create conversational applications and search engines, develop and integrate new AI copilots into their applications, customize customer attention, etc. </p>
        <p class="subtitle">From an Azure OpenAI point of view, we are talking about a managed service that includes advanced functionalities that will allow you to implement <em class="hyperlink">different levels of knowledge</em>, depending on the desired scope of your applications, and based on default capabilities and specific adjustment and customization techniques. By levels of knowledge, we mean something that goes beyond the initial scope of the LLM and its massive dataset (e.g., adding new information for an internal company application, based on its own data). Some of the options to adjust that knowledge include the <span class="keep-together">following:</span></p>
        <dl class="stafflist">
          <dt class="calibre10">Baseline LLM</dt>
          <dd class="calibre11">
            <p class="subtitle">Azure OpenAI’s language models<a contenteditable="false" data-type="indexterm" data-primary="baseline LLM, knowledge base option" id="id467" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="baseline" id="id468" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> are trained on enormous datasets containing billions of words. These datasets are carefully curated to include a wide range of topics, genres, and writing styles. The size and diversity of the training data helps the models develop a broad understanding of human language. The specific details of the training data have not been disclosed, but it includes text data from a variety of sources, including books, articles, websites, and other publicly available written material. Additionally, the training process (RLHF)<a contenteditable="false" data-type="indexterm" data-primary="reinforcement learning from human feedback (RLHF)" id="id469" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="machine learning (ML)" data-secondary="RLHF" id="id470" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="RLHF (reinforcement learning from human feedback)" id="id471" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> includes human reviewers who help annotate and curate the data, flagging and addressing potential biases or problematic content. Feedback loops with reviewers are established to continuously improve and refine the models. One of the key advantages of the enterprise-grade Azure OpenAI service is that <a href="https://oreil.ly/qA5Ok" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">your data is only yours</a> and is not used by anyone to retrain models<a contenteditable="false" data-type="indexterm" data-primary="ChatGPT" data-secondary="training process" id="id472" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>. The end-to-end process is explained in <a href="https://oreil.ly/2uFNS" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">OpenAI’s public paper</a> titled “Training language models to follow instructions with human feedback,” and their official GPT model card, shown in <a data-type="xref" href="#fig_1_the_chatgpt_training_process_source" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-1</a>.</p>
            <figure class="calibre18"><div id="fig_1_the_chatgpt_training_process_source" class="figure">
            <img src="assets/aoas_0301.png" alt="" class="calibre19"/>
            <h6 class="calibre20"><span class="keep-together">Figure 3-1. </span>The ChatGPT training process (source: adapted from an image by <a href="https://oreil.ly/9Lt-2" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">OpenAI</a>; <a href="https://oreil.ly/YGKJ5" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Creative Commons 4.0 license</a>)</h6>
          </div></figure>
          </dd>
          <dt class="calibre10">Additional knowledge (grounding)</dt>
          <dd class="calibre11">
            <p class="subtitle">You can provide the LLMs with some additional context or knowledge<a contenteditable="false" data-type="indexterm" data-primary="grounding techniques" id="xi_groundingtechniques32082" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="grounding techniques" id="xi_LLMsLargeLanguageModelsgroundingtechniques32082" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, making them specific to the activity scope of the developed system. This could go from setting the topic of discussion for a chatbot to specifying URLs that are related to the topics we want to include. There are different ways to implement this grounding:</p>
            <dl class="stafflist">
              <dt class="calibre10">Fine tuning</dt>
              <dd class="calibre11"><p class="subtitle">Using small knowledge bases or private data to retrain the LLM<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning GPT models" id="id473" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="fine-tuning GPT models" id="id474" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> with new additional information. Available via Azure OpenAI Service, it’s a good option to adjust the knowledge scope of the LLM, but a less cost-efficient option (as we will explore in <a data-type="xref" href="ch05.html#operationalizing_generative_ai_implementations" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Chapter 5</a> when we calculate the cost of the Azure OpenAI–enabled implementations). In reality, there are very few use cases that require fine-tuning, because it updates the weights of the models but does not necessarily make the model more factual with respect to the data it was fine-tuned for. Most use cases can be achieved through retrieval-augmented generation (RAG)<a contenteditable="false" data-type="indexterm" data-primary="retrieval-augmented generation (RAG)" id="id475" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="RAG (retrieval-augmented generation)" id="id476" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p></dd>
              <dt class="calibre10">RAG, embeddings-based retrieval</dt>
              <dd class="calibre11"><p class="subtitle">Based on <a href="https://oreil.ly/QlN0L" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Microsoft’s definition</a>, embeddings<a contenteditable="false" data-type="indexterm" data-primary="embedding-based grounding" id="id477" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> are: </p>
              <blockquote class="calibre36 pcalibre5 pcalibre6">
                <p class="calibre37">representations or encodings of tokens, such as sentences, paragraphs, or documents, in a high-dimensional vector space, where each dimension corresponds to a learned feature or attribute of the language. Embeddings are the way that the model captures and stores the meaning and the relationships of the language, and the way that the model compares and contrasts different tokens or units of language. Embeddings are the bridge between the discrete and the continuous, and between the symbolic and the numeric, aspects of language for the model.</p>
              </blockquote>
              </dd>
              <dt class="calibre10">RAG, index-based retrieval</dt>
              <dd class="calibre11"><p class="subtitle">The ability to index<a contenteditable="false" data-type="indexterm" data-primary="index-based retrieval, RAG" id="id478" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> existing files so we can locate them when interacting with the LLM engine. Microsoft <a href="https://oreil.ly/iJSKP" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">defines indexes</a> as: </p>       
              <blockquote class="calibre36 pcalibre5 pcalibre6">
                <p class="calibre37">crawlers that extract searchable content from data sources and populate a search index using field-to-field mappings between source data and a search index. This approach is sometimes referred to as a “pull model” because the search service pulls data in without you having to write any code that adds data to an index.</p>
              </blockquote>
              </dd>
            <dt class="calibre10">RAG, hybrid search</dt>
            <dd class="calibre11"><p class="subtitle">As the result of combining grounding<a contenteditable="false" data-type="indexterm" data-primary="hybrid search-based grounding" id="id479" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="search use case" data-secondary="hybrid search-based grounding" id="id480" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> techniques, <a href="https://oreil.ly/c2W8A" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">hybrid search</a> leverages both embedding-based retrieval in combination with index-based retrieval to unlock some of the most powerful techniques.</p></dd>
            <dt class="calibre10">Other grounding techniques</dt>
            <dd class="calibre11"><p class="subtitle">Other techniques include contextualization<a contenteditable="false" data-type="indexterm" data-primary="contextualization grounding technique" id="id481" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> (providing information about topics and/or specific URLs to define a reduced knowledge scope) and live internet results<a contenteditable="false" data-type="indexterm" data-primary="live internet results, in grounding" id="id482" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> to complement the LLM information and include external sources.</p></dd>
          </dl>
        </dd>
        </dl>
        <p class="subtitle">As you can see in <a data-type="xref" href="#fig_2_knowledge_scope_for_generative_ai" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-2</a>, all these elements contribute to the creation of an extended knowledge domain from regular LLMs, based on internet and private data. The rest of this chapter will focus on different techniques to implement them with Azure OpenAI and other Microsoft services.</p>
        <p class="subtitle">Summarizing, any generative AI architecture or approach will depend on the knowledge domains and levels we require for the end solutions. If our application can rely on (just) the LLM, which already contains a massive amount of information, then we can implement the model with no additional building blocks. On the other hand, if we need to add specific information from other sources (including PDFs, text documents, websites, databases, etc.), then we will leverage the so-called fine-tuning and grounding techniques.</p>
        <p class="subtitle">Let’s now explore the available interfaces and tools for you to create new applications with Azure OpenAI Service. You will understand the key building blocks before moving into a step-by-step guide of the most relevant implementation approaches<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_implementationofgenerativeAIknowledgescopeofAzureOpenAIenabledapps3738" id="id483" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_AzureOpenAIServiceknowledgescopeofenabledapps3738" id="id484" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_knowledgescopeofAzureOpenAIenabledapps3738" id="id485" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_groundingtechniques32082" id="id486" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_LLMsLargeLanguageModelsgroundingtechniques32082" id="id487" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
        <figure class="calibre18"><div id="fig_2_knowledge_scope_for_generative_ai" class="figure">
          <img src="assets/aoas_0302.png" alt="" class="calibre19"/>
          <h6 class="calibre20"><span class="keep-together">Figure 3-2. </span>Knowledge scope for generative AI</h6>
        </div></figure>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Generative AI Modeling with Azure OpenAI Service" class="calibre5"><div class="preface" id="generative_ai_modeling_with_azure_openai">
        <h1 class="calibre4">Generative AI Modeling with Azure OpenAI Service</h1>
        <p class="subtitle">One of the key adoption factors that encourages people to use Azure OpenAI is the availability of different visual and code-based interfaces that you can leverage while using the service. In this section we will explore them as well as how to use these interfaces depending on your generative AI implementation approach.</p>
        <div data-type="warning" epub:type="warning" class="calibre15"><h6 class="calibre17">Warning</h6>
          <p class="subtitle">Initially, Microsoft released Azure OpenAI Service with “Gated General Availability,” meaning that any organization willing to use the service had to <em class="hyperlink">complete a detailed application form</em> to explain the potential use cases and guarantee good usage of the platform. Microsoft’s goal was to validate that any application enabled by Azure OpenAI was always aligned<a contenteditable="false" data-type="indexterm" data-primary="responsible AI (RAI)" id="id488" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="RAI (responsible AI)" id="id489" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> with their <a href="https://oreil.ly/QsuYY" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">responsible AI approach</a> and the <a href="https://oreil.ly/7ojQP" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">intended use</a> of the platform. If you are getting started with Azure OpenAI Service, <a href="https://oreil.ly/MDBhf" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">check first if you still need to apply for access</a> and prepare the required information for the <a href="https://oreil.ly/dp14y" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">application form</a>. </p>
        </div>
        <section data-type="sect2" data-pdf-bookmark="Azure OpenAI Service Building Blocks" class="calibre5"><div class="preface" id="azure_openai_service_building_blocks">
          <h2 class="calibre21">Azure OpenAI Service Building Blocks</h2>
          <p class="subtitle">Before diving into the “how to,” let’s explore the available building blocks<a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="building blocks" id="xi_implementationofgenerativeAIbuildingblocks35990" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> for any Azure OpenAI practitioner to prepare and deploy new solutions. Essentially, there are two primary components for the Azure OpenAI Service<a contenteditable="false" data-type="indexterm" data-primary="development interfaces" id="id490" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="interfaces, generative AI" data-secondary="development interfaces" id="id491" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="building blocks" data-tertiary="development interfaces" id="id492" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>: the <em class="hyperlink">visual interfaces</em> that allow users to test, customize, and deploy their generative AI models and the <em class="hyperlink">development interfaces</em> that enable the exploitation and integration of those advanced capabilities with any application.</p>
          <p class="subtitle">Both elements are complementary and great assets for any kind of adopter, as they require a relatively low level of AI knowledge to make them work<a contenteditable="false" data-type="indexterm" data-primary="citizen users" id="id493" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>. For example, <em class="hyperlink">citizen users</em> (hybrid technical-business profiles that are not very technical, but that understand the principles of generative AI and have some knowledge of prompt engineering, can use the visual playground, or leverage Microsoft Copilot Studio) and <em class="hyperlink">regular developers</em> are great candidates for the development platforms.</p>
          <section data-type="sect3" data-pdf-bookmark="Visual interfaces: Azure OpenAI Studio and Playground" class="calibre5"><div class="preface" id="visual_interfaces_azure_openai_studio_and_playgro">
            <h3 class="calibre38">Visual interfaces: Azure OpenAI Studio and Playground</h3>
            <p class="subtitle">As with any other <a href="https://oreil.ly/TDoRH" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure AI service</a>, Azure OpenAI<a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="building blocks" data-tertiary="visual interfaces" id="xi_implementationofgenerativeAIbuildingblocksvisualinterfaces363101" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="interfaces, generative AI" id="xi_interfacesgenerativeAI363101" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="visual interfaces, generative AI" id="xi_visualinterfacesgenerativeAI363101" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="interfaces, generative AI" data-secondary="visual interfaces" id="xi_interfacesgenerativeAIvisualinterfaces363101" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="user interface (UI)" id="xi_userinterfaceUI363101" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> includes the notion<a contenteditable="false" data-type="indexterm" data-primary="Azure OpenAI Studio" id="xi_AzureOpenAIStudio363121" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> of a “Studio” (i.e., <a href="https://oreil.ly/LWQO1" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure OpenAI Studio)</a> that makes the interaction with generative AI models very simple, by providing an intuitive UI that facilitates service deployments and leverages existing Azure OpenAI APIs without any code required from the user <span class="keep-together">perspective.</span> </p>
            <p class="subtitle">Azure OpenAI Studio includes access to <a href="https://oreil.ly/BI5Ue" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">all available models</a> (by type and geographic region), predefined prompting scenarios and examples, and several applications<a contenteditable="false" data-type="indexterm" data-primary="Azure OpenAI Playgrounds" id="xi_AzureOpenAIPlaygrounds364215" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> called <em class="hyperlink">playgrounds</em>. The Azure OpenAI Playgrounds are different apps within Azure OpenAI Service<a contenteditable="false" data-type="indexterm" data-primary="Assistants playground" id="id494" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Completions playground" id="id495" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="DALL·E playground" id="id496" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="bring your own data playground" id="id497" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, which include (as you can see in <a data-type="xref" href="#fig_3_azure_openai_studio" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-3</a>) a customizable ChatGPT type of instance (<em class="hyperlink">Chat</em>), other GPT language models for nonchat scenarios (<em class="hyperlink">Completion</em>), a playground to connect AI models with your data (<em class="hyperlink">Bring your own data</em>), and one for image generation applications with OpenAI’s DALL·E models. </p>
            <figure class="calibre18"><div id="fig_3_azure_openai_studio" class="figure">
              <img src="assets/aoas_0303.png" alt="" class="calibre19"/>
              <h6 class="calibre20"><span class="keep-together">Figure 3-3. </span>Azure OpenAI Studio</h6>
            </div></figure>
            <p class="subtitle">You can access each playground (and their related management features) from the left panel of the studio, or visit them directly by following the URLs included here:</p>
            <dl class="stafflist">
              <dt class="calibre10"><a href="https://oreil.ly/FRU9K" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Chat playground</a></dt>
              <dd class="calibre11">
                <p class="subtitle">This includes both the conversational Chat playground<a contenteditable="false" data-type="indexterm" data-primary="Chat playground" id="xi_Chatplayground37373" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> with the <a href="https://oreil.ly/K_fjL" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">features and settings</a> required to create a private ChatGPT implementation, and the bring your own data (represented as one of the playgrounds in Azure OpenAI Studio) functionality that I will explain later in this section<a contenteditable="false" data-type="indexterm" data-primary="Chat Completion API" id="id498" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>. The Chat playground (shown in <a data-type="xref" href="#fig_4_azure_openai_studio_chat_playground" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-4</a>) leverages the <a href="https://oreil.ly/ZJOLp" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Chat Completion API</a>.</p>
            <figure class="calibre18"><div id="fig_4_azure_openai_studio_chat_playground" class="figure">
              <img src="assets/aoas_0304.png" alt="" class="calibre19"/>
              <h6 class="calibre20"><span class="keep-together">Figure 3-4. </span>Azure OpenAI Studio: Chat playground</h6>
            </div></figure>
            <p class="subtitle">As indicated in <a data-type="xref" href="#fig_4_azure_openai_studio_chat_playground" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-4</a>, the main tiles and features of the Chat playground comprise the following:</p>
            <dl class="stafflist">
              <dt class="calibre10">1. Assistant setup</dt>
              <dd class="calibre11">
                <p class="subtitle">This area is located on the left side of the screen and allows users to configure the chatbot’s behavior. Users can choose from templates or create their own custom system messages. This section helps users define how the chatbot should act and respond to user queries:</p>           
                <dl class="stafflist">
                <dt class="calibre10">System message</dt>
                <dd class="calibre11"><p class="subtitle">A type<a contenteditable="false" data-type="indexterm" data-primary="meta-prompts" id="id499" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="meta-prompts" id="id500" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> of <a href="https://oreil.ly/OmKQO" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">meta-prompt</a> (i.e., a prompt that sets the by-default context of the discussion) to guide the AI system’s behavior. It can be used to introduce the system, set expectations, provide feedback, or handle errors. One important thing to remember is that even if there is no token limit for this message, it will be included with every API call, so it counts against the overall <a href="https://oreil.ly/BI5Ue" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">token limit/context length</a> of the model.</p></dd>
                <dt class="calibre10">Examples</dt>
                <dd class="calibre11"><p class="subtitle">This area is located at the bottom-left corner of the screen. You can add examples to the bot intelligence, so it learns the proper way to answer specific questions. It’s a good option when we don’t need to fully retrain a model, for example, when you need to add a couple of topics from your company’s knowledge base and you want to define the best way to answer. From the official description: “Add examples to show the chat what responses you want. It will try to mimic any responses you add here so make sure they match the rules you laid out in the system <span class="keep-together">message.”</span></p></dd>
              </dl>
            </dd>
              <dt class="calibre10">2. Chat session</dt>
              <dd class="calibre11">
                <p class="subtitle">This area is located in the middle of the screen and serves as the main interaction point between you and the chatbot. You can type your queries here and the chatbot will respond accordingly. The chat session allows you to test the chatbot’s performance and make adjustments to the assistant setup as needed, as well as import and export bot configurations<a contenteditable="false" data-type="indexterm" data-primary="JavaScript Object Notation (JSON)" id="id501" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, or get the result as a <a href="https://oreil.ly/LZJH4" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">JavaScript Object Notation (JSON) file</a>.</p>
              </dd>
              <dt class="calibre10">3. Deploy to</dt>
              <dd class="calibre11">
                <p class="subtitle">This option allows you to deploy your chatbot to a specific platform or environment. Azure OpenAI Studio allows direct deployments to both <a href="https://oreil.ly/TtlXr" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure Web Apps</a> and <a href="https://oreil.ly/YV0SN" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Microsoft Copilot Studio</a>. We will explore these deployment options later in this <span class="keep-together">chapter.</span></p>
              </dd>
              <dt class="calibre10">4. Configuration</dt>
              <dd class="calibre11">
                <p class="subtitle">This area is located in the top-right corner of the screen. It provides options for you to access deployment and session settings. Users can also clear the chat history and manage parameters related to the chatbot’s deployment:</p>
                <dl class="stafflist">
                  <dt class="calibre10">Deployment</dt>
                  <dd class="calibre11">
                    <p class="subtitle">To handle session-level configurations, such as the Azure OpenAI deployment resource you want to use (e.g., you may have several for different geographic regions), as well as the memory of the session, which will impact how many interactions the system can remember when getting new questions:</p>
                    <dl class="stafflist">
                      <dt class="calibre10">Deployment instance</dt>
                      <dd class="calibre11"><p class="subtitle">You will select one option, from the resources you have <a href="https://oreil.ly/-4D4f" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">previously deployed</a> (if you haven’t, you will need to create one before using Azure OpenAI Studio), based on the geography and model needs you may have.</p></dd>
                      <dt class="calibre10">Past messages included and current token count</dt>
                      <dd class="calibre11"><p class="subtitle">Session-level parameters you may want to adjust for the specific test you do via the Chat playground. These parameters will be gone when you finish the playground session, except if you deploy an application (we will see the deployment options in a couple of <span class="keep-together">sections).</span></p></dd>
                    </dl>
                  </dd>
                  <dt class="calibre10">Parameters</dt>
                  <dd class="calibre11">
                    <p class="subtitle">This right panel includes all technical settings that will allow you to configure the expected output message, including the level of creativity versus determinism of the answer:</p>
                    <dl class="stafflist">
                      <dt class="calibre10">Max response</dt>
                      <dd class="calibre11"><p class="subtitle">This parameter helps you set a limit on the number of tokens per model response. The max response is measured in the number of tokens, and it is shared between the question (including system message, examples, message history, and prompt/user query) and the model’s response.</p></dd>
                      <dt class="calibre10">Temperature</dt>
                      <dd class="calibre11"><p class="subtitle">This parameter and the Top-p parameter are direct alternatives to control the AI model’s randomness. Lowering the temperature means that the model will produce more repetitive and deterministic responses. Increasing the temperature will result in more unexpected or creative responses. Try adjusting temperature or Top-p, but not both<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_Chatplayground37373" id="id502" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p></dd>
                    </dl>
                  </dd>
                </dl>
              </dd>
            </dl>
          </dd>
              <dt class="calibre10">
                <a href="https://oreil.ly/S4KFy" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
                  Assistants playground
                </a>

              </dt>
              <dd class="calibre11">
                <p class="subtitle"><a href="https://oreil.ly/MdxvG" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Released in 2024</a>, the Assistants playground<a contenteditable="false" data-type="indexterm" data-primary="Assistants playground" id="id503" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> is visually similar to the Chat playground, but it includes:</p>
             
              <ul class="calibre22">
                <li class="calibre23">
                  <p class="calibre24">The ability to handle conversation threads, by using the “thread ID” parameter that converts the chat discussion into a stateful application that keeps context and memory. You can see the details in Azure OpenAI’s <a href="https://oreil.ly/ErRd6" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Assistants API specification</a>.</p>
                </li>
                <li class="calibre8">
                  <p class="calibre24">Other functionalities such as the API call log, the <a href="https://oreil.ly/3jSFV" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Code Interpreter</a>, and <a href="https://oreil.ly/2R7Pz" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">function calling</a>. </p>
                </li>
              </ul>
            </dd>
            
              <dd class="calibre11">
                <p class="subtitle">Keep in mind that this is a relatively new option, but the <a href="https://oreil.ly/HH4hH" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">official documentation</a> includes the detailed steps for creation and management of assistant files. Keep an eye on and bookmark this URL to follow any news and technical resources.</p>
              </dd>
              <dt class="calibre10">
                <a href="https://oreil.ly/zJYtL" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
                  Completions playground
                </a>
                 
              </dt>
              <dd class="calibre11">
                <p class="subtitle">As we reviewed in <a data-type="xref" href="ch01.html#introduction_to_generative_ai_and_azure_openai_ser" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Chapter 1</a>, the completion<a contenteditable="false" data-type="indexterm" data-primary="Completions playground" id="xi_Completionsplayground3155188" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> skill is (along with chat and embeddings models) one of the core concepts for NLP and modern LLMs. Completion focuses on unitary interactions for all kinds of text-based requests (with no need for memory between interactions, as you may need for chat-based applications in which the model keeps the discussion context). It leverages the <a href="https://oreil.ly/Uczv9" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Completions API</a>. As shown in <a data-type="xref" href="#fig_5_azure_openai_studio_completions_playground" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-5</a>, the Completions playground allows you to type a prompt, or choose from a series of examples. It also includes the same kind of setting parameters that we reviewed in the Chat playground.</p>
           
            <figure class="calibre18"><div id="fig_5_azure_openai_studio_completions_playground" class="figure">
              <img src="assets/aoas_0305.png" alt="" class="calibre19"/>
              <h6 class="calibre20"><span class="keep-together">Figure 3-5. </span>Azure OpenAI Studio: Completions playground</h6>
            </div></figure>

              <p class="subtitle">You can generate an answer (completion), and even regenerate it to obtain a totally new output. If you choose one of the examples from the drop-down menu, you will see an automatic prompt appear and the corresponding completion, highlighted as in <a data-type="xref" href="#fig_6_azure_openai_studio_completions_playground_examp" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-6</a>.</p>

            <figure class="calibre18"><div id="fig_6_azure_openai_studio_completions_playground_examp" class="figure">
              <img src="assets/aoas_0306.png" alt="" class="calibre19"/>
              <h6 class="calibre20"><span class="keep-together">Figure 3-6. </span>Azure OpenAI Studio: Completions playground (example)</h6>
            </div></figure>

              <p class="subtitle">Summarizing, you may use chat for multistep scenarios where you need to maintain a sequence of interactions with the AI model, while completions can be used for specific unitary cases. As you will see later, these two playgrounds are just visual interfaces that consume existing Azure OpenAI <a href="https://oreil.ly/Uczv9" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">completion</a> and <a href="https://oreil.ly/ZJOLp" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">chat</a> APIs<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_Completionsplayground3155188" id="id504" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
            </dd>
              <dt class="calibre10">Bring your own data playground</dt>
              <dd class="calibre11">
                <p class="subtitle">Even if Azure OpenAI Studio shows this feature as a separate playground<a contenteditable="false" data-type="indexterm" data-primary="bring your own data playground" id="xi_bringyourowndataplayground317691" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, it is technically part of the Chat playground. To access this functionally, you can either use the Chat playground<a contenteditable="false" data-type="indexterm" data-primary="Chat playground" data-secondary="and bring your own data playground" data-secondary-sortas="bring your own data playground" id="id505" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>’s Assistant setup and select the “Add your data” tab or go directly to the “Bring your own data” tile of the Studio (<a data-type="xref" href="#fig_7_azure_openai_studio_bring_your_own_data" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-7</a>). For both cases, the result will be the same.</p>
           <p class="subtitle">Once you reach this point, the sequence of steps is pretty simple. As you can see in <a data-type="xref" href="#fig_8_azure_openai_studio_bring_your_own_data_source_de" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-8</a>, the system will allow you to select your own sources of data, to combine their knowledge with the baseline LLM. That knowledge can come from PDF files, text-based documents, slides, web files, etc. In this case, besides the Azure OpenAI resource previously deployed, the bring your own data functionality will leverage other resources such as Azure Data Lake Gen2/Azure Storage, to save the files, and Azure Cognitive Search<a contenteditable="false" data-type="indexterm" data-primary="Azure Cognitive Search" id="id506" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, to index the files. Azure Cognitive Search offers a vector search functionality based<a contenteditable="false" data-type="indexterm" data-primary="Embeddings API" id="id507" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> on the <a href="https://oreil.ly/imKOS" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Embeddings API</a>) that I will explain by the end of the chapter. Finally, you can always check the <a href="https://oreil.ly/z_iRM" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">official documentation</a> to follow the latest updates for this Azure OpenAI feature, as it is a quickly evolving one due to the continuous incorporation of new <span class="keep-together">functionalities.</span></p>
            <figure class="calibre18"><div id="fig_7_azure_openai_studio_bring_your_own_data" class="figure">
              <img src="assets/aoas_0307.png" alt="" class="calibre19"/>
              <h6 class="calibre20"><span class="keep-together">Figure 3-7. </span>Azure OpenAI Studio: Bring your own data</h6>
            </div></figure>
           
            <figure class="calibre18"><div id="fig_8_azure_openai_studio_bring_your_own_data_source_de" class="figure">
              <img src="assets/aoas_0308.png" alt="" class="calibre19"/>
              <h6 class="calibre20"><span class="keep-together">Figure 3-8. </span>Azure OpenAI Studio: Bring your own data source details</h6>
            </div></figure>
          </dd>
            
              <dt class="calibre10"><a href="https://oreil.ly/r4h7Y" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">DALL·E playground</a></dt>
              <dd class="calibre11">
                <p class="subtitle">The last playground tile provides direct access to the generative AI DALL·E<a contenteditable="false" data-type="indexterm" data-primary="DALL·E playground" id="xi_DALLEplayground319495" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> models (versions 2 and 3) from OpenAI. This is a text-to-image model that allows you to create new images from just text-based descriptions. Imagine describing a place or a scene and getting a visual representation in the form of images that are freshly created on demand. This means they didn’t exist previously and that you can integrate this capability into your solutions and combine it with the rest of the language. The DALL·E playground (shown in <a data-type="xref" href="#fig_9_azure_openai_studio_dall_e_playground" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-9</a>) leverages<a contenteditable="false" data-type="indexterm" data-primary="Image Generation API" id="id508" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> the <a href="https://oreil.ly/bm-7a" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Image Generation API</a>.</p>
            <figure class="calibre18"><div id="fig_9_azure_openai_studio_dall_e_playground" class="figure">
              <img src="assets/aoas_0309.png" alt="" class="calibre19"/>
              <h6 class="calibre20"><span class="keep-together">Figure 3-9. </span>Azure OpenAI Studio: DALL·E playground</h6>
            </div></figure>
            <p class="subtitle">As shown in <a data-type="xref" href="#fig_9_azure_openai_studio_dall_e_playground" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-9</a>, relevant aspects of the playground include the following:</p>
            <dl class="stafflist">
            <dt class="calibre10">1. Playground</dt>
            <dd class="calibre11"><p class="subtitle">The DALL·E playground is visually simple—a prompt field and the results (image) below. It’s similar to the structure of the <a href="https://oreil.ly/YwDy-" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Bing Create application</a>, but with the option to deploy the DALL·E model for your own development.</p></dd>
            <dt class="calibre10">2. Settings</dt>
            <dd class="calibre11"><p class="subtitle"><em class="hyperlink"><strong class="calibre13"> </strong></em>The settings panel offers you the option to choose the number of images you want to generate and the image size.</p></dd>
            <dt class="calibre10">3. Album</dt>
            <dd class="calibre11"><p class="subtitle">The album section showcases all past image experiments, offering you the option to review previously created images, generate new ones, etc<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_DALLEplayground319495" id="id509" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p></dd>
          </dl>
          </dd>
        </dl>
            <p class="subtitle">Besides the different playgrounds, you can also explore the left-side <em class="hyperlink">Management </em>panel shown in <a data-type="xref" href="#fig_10_azure_openai_studio_management_panels" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-10</a>, which include options such as deployments, models, data files, quotas, and content filters. </p>
            <figure class="calibre18"><div id="fig_10_azure_openai_studio_management_panels" class="figure">
              <img src="assets/aoas_0310.png" alt="" class="calibre19"/>
              <h6 class="calibre20"><span class="keep-together">Figure 3-10. </span>Azure OpenAI Studio: Management panels</h6>
            </div></figure>
            <p class="subtitle">Let’s explore the most important features:</p>
            <dl class="stafflist">
              <dt class="calibre10">
                <a href="https://oreil.ly/PGocU" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
                  Deployments
                </a>
              </dt>
              <dd class="calibre11">
                <p class="subtitle">Allows you to deploy<a contenteditable="false" data-type="indexterm" data-primary="Management panel, Azure OpenAI Studio" id="xi_ManagementpanelAzureOpenAIStudio322440" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Deployments, Azure OpenAI Studio" id="id510" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> any specific model instance <a href="https://oreil.ly/XZnCX" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">available in the geographic region</a> of your Azure OpenAI resource and to visualize those that you previously deployed (<a data-type="xref" href="#fig_11_azure_openai_studio_deployments" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-11</a>).</p>
                <figure class="calibre18"><div id="fig_11_azure_openai_studio_deployments" class="figure">
                  <img src="assets/aoas_0311.png" alt="" class="calibre19"/>
                  <h6 class="calibre20"><span class="keep-together">Figure 3-11. </span>Azure OpenAI Studio: deployments</h6>
                </div></figure>
              </dd>
              <dt class="calibre10">
                <a href="https://oreil.ly/Bpsud" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
                  Content filters
                </a>
              </dt>
              <dd class="calibre11">
                <p class="subtitle">For responsible AI moderation. Each filter<a contenteditable="false" data-type="indexterm" data-primary="content filters" id="id511" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> from those in <a data-type="xref" href="#fig_12_azure_openai_studio_content_filters" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-12</a> (e.g., hate, sexual, self-harm, and violence topics for both prompts and completions, with different levels of filtering) can be applied to the deployments, and those deployments will include the content filter for each chat or completion implementation. We will explore this feature in <a data-type="xref" href="ch04.html#additional_cloud_and_ai_capabilities" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Chapter 4</a>, as part of the responsible AI measures for generative AI implementations.</p>
                <figure class="calibre18"><div id="fig_12_azure_openai_studio_content_filters" class="figure">
                  <img src="assets/aoas_0312.png" alt="" class="calibre19"/>
                  <h6 class="calibre20"><span class="keep-together">Figure 3-12. </span>Azure OpenAI Studio: content filters</h6>
                </div></figure>
              </dd>
              <dt class="calibre10">
                <a href="https://oreil.ly/oC3Hj" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
                  Models
                </a>
              </dt>
              <dd class="calibre11">
                <p class="subtitle">This option<a contenteditable="false" data-type="indexterm" data-primary="models" data-secondary="Azure OpenAI Studio" id="id512" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> shows the <a href="https://oreil.ly/XZnCX" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">available Azure OpenAI models</a>, related to the specific geographic region of the chosen deployment. </p>
              </dd>
              <dt class="calibre10">
                <a href="https://oreil.ly/2TZwW" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
                  Data files
                </a>
              </dt>
              <dd class="calibre11">
                <p class="subtitle">This file management feature<a contenteditable="false" data-type="indexterm" data-primary="file management feature, Azure OpenAI Studio" id="id513" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="data file management, Azure OpenAI Studio" id="id514" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> allows you to <a href="https://oreil.ly/FDMr1" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">prepare the dataset for fine-tuned implementations</a>. We will explore more about fine-tuning later in this chapter.</p>
              </dd>
              <dt class="calibre10">
                <a href="https://oreil.ly/ONn5Q" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
                  Quotas
                </a>
              </dt>
              <dd class="calibre11">
                <p class="subtitle">The quota<a contenteditable="false" data-type="indexterm" data-primary="quotas panel, Azure OpenAI Studio" id="id515" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="usage quotas, Azure OpenAI Studio" id="id516" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> panel shows the <a href="https://oreil.ly/bEN4D" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">usage quotas</a> related to different models and geographic regions. It also helps you <a href="https://oreil.ly/iiysu" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">request a quota increase</a> if you need more. Alternatively, and I will explain this in <a data-type="xref" href="ch06.html#elaborating_generative_ai_business_cases" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Chapter 6</a> as part of the pricing and estimation exercise, you have an option to hire dedicated capacity, by leveraging the so-called <a href="https://oreil.ly/KCC6K" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">provisioned throughput units (PTU) for Azure OpenAI</a>, which are reserved instances with performance and service availability benefits.</p>
              </dd>
            </dl>
            <p class="subtitle">We will explore some of these functionalities later in this chapter and in <a data-type="xref" href="ch04.html#additional_cloud_and_ai_capabilities" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Chapter 4</a>, as they will all be relevant, depending on the type of Azure OpenAI implementation you plan to utilize. Now, let’s see what you can do to deploy these models via Azure OpenAI Studio<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_implementationofgenerativeAIbuildingblocksvisualinterfaces363101" id="id517" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_visualinterfacesgenerativeAI363101" id="id518" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_interfacesgenerativeAIvisualinterfaces363101" id="id519" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_userinterfaceUI363101" id="id520" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_AzureOpenAIStudio363121" id="id521" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_AzureOpenAIPlaygrounds364215" id="id522" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_ManagementpanelAzureOpenAIStudio322440" id="id523" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Deployment interfaces: Web apps and Microsoft Copilot agents" class="calibre5"><div class="preface" id="deployment_interfaces_web_apps_and_microsoft_copi">
            <h3 class="calibre38">Deployment interfaces: Web apps and Microsoft Copilot agents</h3>
            <p class="subtitle">As mentioned in this chapter, the Chat playground includes some easy-to-use deployment<a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="building blocks" data-tertiary="deployment interfaces" id="xi_implementationofgenerativeAIbuildingblocksdeploymentinterfaces3271102" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="deployment of generative AI" data-secondary="interfaces" id="xi_deploymentofgenerativeAIinterfaces3271102" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="interfaces, generative AI" data-secondary="deployment interfaces" id="xi_interfacesgenerativeAIdeploymentinterfaces3271102" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Chat playground" data-secondary="deployment interfaces" id="xi_Chatplaygrounddeploymentinterfaces3271102" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> options. They are not available for the rest of the playgrounds, but they can simplify the preliminary deployment of Azure OpenAI models for internal testing and use purposes, without any coding required. These no-code deployments can incorporate the specific knowledge from the bring your own data functionality. There are two possibilities:</p>
            <dl class="stafflist">
              <dt class="calibre10">Web apps with <a href="https://oreil.ly/moBFz" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure App Service</a></dt>
              <dd class="calibre11">
                <p class="subtitle">The first available deployment<a contenteditable="false" data-type="indexterm" data-primary="web apps" id="xi_Webapps327550" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Azure App Service" id="id524" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> option, which you can use with or without the “bring your own data” feature activated. As we discussed in <a data-type="xref" href="ch02.html#designing_cloud_native_architectures_for_generativ" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Chapter 2</a>, App Service is the Azure option to deploy native web apps; it allows integrations with both external and internal systems and web development with a variety of programming languages. From Azure OpenAI Studio and its Chat playground, you can simply “Deploy to” and then configure your deployment (see <a data-type="xref" href="#fig_13_azure_openai_studio_web_app_deployment" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-13</a>).</p>
                <figure class="calibre18"><div id="fig_13_azure_openai_studio_web_app_deployment" class="figure">
                  <img src="assets/aoas_0313.png" alt="" class="calibre19"/>
                  <h6 class="calibre20"><span class="keep-together">Figure 3-13. </span>Azure OpenAI Studio: web app deployment</h6>
                </div></figure>
              <p class="subtitle">As shown in <a data-type="xref" href="#fig_13_azure_openai_studio_web_app_deployment" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-13</a>,  configuration options include the following:</p>
              <dl class="stafflist">
                <dt class="calibre10">Choosing the web app</dt>
                <dd class="calibre11"><p class="subtitle">You can create a new App Service resource directly from this feature (in that case, you will need to define the “app name” that will be part of your web app URL), or choose an existing one if you have previously deployed via <a href="https://oreil.ly/dPLy2" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure portal’s App Service panel</a>.</p></dd>
                <dt class="calibre10">Pricing plan</dt>
                <dd class="calibre11"><p class="subtitle">To select the preferred <a href="https://oreil.ly/IdDXQ" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">pricing tier</a> for the web app.</p></dd>
                <dt class="calibre10">Chat history</dt>
                <dd class="calibre11"><p class="subtitle">A functionality that allows the web app users to recover their <a href="https://oreil.ly/-yyQg" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">previous interactions</a> with chat. It relies on <a href="https://oreil.ly/-yyQg" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Cosmos DB (Azure’s NoSQL database)</a>, which obviously adds cost to the existing Azure OpenAI and App Service resources.</p></dd>
              </dl>
           </dd>
           <dd class="calibre11">
              <p class="subtitle">Once you have selected all these options, you can click on Deploy. You will need to wait around 10 minutes for all the resources to be deployed, then you will be able to launch your web app from the studio, or by typing the URL <em class="hyperlink">https://&lt;appname&gt;.azurewebsites.net</em><em class="hyperlink">.</em><strong class="calibre13"> </strong>The look and feel will be something like the interface you see in <a data-type="xref" href="#fig_14_azure_openai_studio_web_app_interface" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-14</a>.</p>
           
            <figure class="calibre18"><div id="fig_14_azure_openai_studio_web_app_interface" class="figure">
              <img src="assets/aoas_0314.png" alt="" class="calibre19"/>
              <h6 class="calibre20"><span class="keep-together">Figure 3-14. </span>Azure OpenAI Studio: web app interface</h6>
            </div></figure>

              <p class="subtitle">The UI of the new app will contain a regular chatbot setup, with options to share and check previous discussions on the top-right side of the window. You can also <a href="https://oreil.ly/BVUkG" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">customize the visual aspect of the application</a> by using the <a href="https://oreil.ly/MeBin" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">official source code</a>, and deploy it programmatically, with Azure App Service and using your preferred programming language, instead of leveraging Azure OpenAI Studio<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_Webapps327550" id="id525" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>. </p>
            </dd>
              <dt class="calibre10">Bots<a contenteditable="false" data-type="indexterm" data-primary="bring your own data playground" id="id526" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Microsoft Copilot Studio" id="id527" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Power Virtual Agents (PVAs)" id="id528" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="PVAs (Power Virtual Agents)" id="id529" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> with <a href="https://oreil.ly/YV0SN" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Microsoft Copilot Studio (formerly Power Virtual Agents [PVAs])</a></dt>
              <dd class="calibre11">
                <p class="subtitle">This option is available for Chat playground implementations that include the “bring your own data” feature. That means that if you don’t add extended knowledge from PDFs or other documents, the Chat playground won’t include Microsoft Copilot Studio/PVA as a deployment option in the top-right corner in <a data-type="xref" href="#fig_15_azure_openai_studio_copilot_deployment" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-15</a>.</p>
              
           
            <figure class="calibre18"><div id="fig_15_azure_openai_studio_copilot_deployment" class="figure">
              <img src="assets/aoas_0315.png" alt="" class="calibre19"/>
              <h6 class="calibre20"><span class="keep-together">Figure 3-15. </span>Azure OpenAI Studio: Copilot deployment</h6>
            </div></figure>
       
              <p class="subtitle">How to handle PVAs is outside of the scope of this book, but you can explore the <a href="https://oreil.ly/Qi9J3" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">detailed instructions from the official documentation</a> that show how to use PVAs with Azure OpenAI for the <em class="hyperlink">generative answers</em> feature. This option is available for only certain geographic regions, so you will need to validate if your deployments with Azure OpenAI models show the PVA deployment option in the Chat playground. If this is not the case, you may want to deploy new models in other regions. </p>
            </dd>
            </dl>
            <p class="subtitle">Summarizing, these visual interfaces can help you leverage Azure OpenAI models in a simple manner. They provide an intuitive way to launch the Azure OpenAI APIs in just a few clicks. However, you will need code-based tools to implement the other advanced architectures you will see later in this chapter. Let’s now explore those APIs and other development kits so you can leverage everything that Azure OpenAI Service has to offer<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_implementationofgenerativeAIbuildingblocksdeploymentinterfaces3271102" id="id530" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_deploymentofgenerativeAIinterfaces3271102" id="id531" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_interfacesgenerativeAIdeploymentinterfaces3271102" id="id532" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_Chatplaygrounddeploymentinterfaces3271102" id="id533" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
          </div></section>
          <section data-type="sect3" class="calibre5" data-pdf-bookmark="Development interfaces: APIs and SDKs"><div class="preface" id="development_interfaces_apis_and_sdks">
            <h3 class="calibre38">Development interfaces: APIs and SDKs</h3>
            <p class="subtitle">In addition to all the previously explored interfaces<a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="building blocks" data-tertiary="development interfaces" id="xi_implementationofgenerativeAIbuildingblocksdevelopmentinterfaces331769" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="development interfaces" id="xi_developmentinterfaces331769" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="interfaces, generative AI" data-secondary="development interfaces" id="xi_interfacesgenerativeAIdevelopmentinterfaces331769" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, one of the key enablers for integrating Azure OpenAI with existing or new applications is the ability to consume the preconfigured models as regular endpoints. From a development point of view, we can call those models by using the APIs and related software development kits (SDKs) and pass any input and configuration parameters within the code. This section covers the main pieces you need to know—the <em class="hyperlink">Azure OpenAI Service REST APIs</em>, including the <a href="https://oreil.ly/qH3FL" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">official API reference documentation</a>, with specific details for chat, completions, embeddings, and other deployments. There is also an <a href="https://oreil.ly/mbA1v" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">official repo</a> with the full specifications. There are general APIs that will help you with the configuration and deployment of Azure OpenAI services, while the service APIs help you consume the models to bring the AI capabilities to your generative AI applications. </p>
            <p class="subtitle">The main APIs<a contenteditable="false" data-type="indexterm" data-primary="APIs (application programming interfaces)" id="xi_APIsapplicationprogramminginterfaces331829" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> you need to know and their high-level call details are as follows:</p>
            <dl class="stafflist">
              <dt class="calibre10">
                <a href="https://oreil.ly/xkqqk" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
                  General management APIs
                </a>
              </dt>
              <dd class="calibre11">
                <p class="subtitle">For Azure AI service account management<a contenteditable="false" data-type="indexterm" data-primary="account management, Azure AI APIs" id="id534" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> (including Azure OpenAI), with tasks such as account creation, deletion, listing, etc.</p>
              </dd>
              <dt class="calibre10">
                <a href="https://oreil.ly/Y7VMR" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
                  APIs for model-related information
                </a>
              </dt>
              <dd class="calibre11">
                <p class="subtitle">To obtain the list of available Azure OpenAI models<a contenteditable="false" data-type="indexterm" data-primary="models" data-secondary="APIs for model-related information" id="id535" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> and information about their specific capabilities and the model lifecycle (including potential deprecation details).</p>
              </dd>
              <dt class="calibre10">
                <a href="https://oreil.ly/Uczv9" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
                  Completions
                </a>
              </dt>
              <dd class="calibre11">
                <p class="subtitle">The required API for nonchat language scenarios<a contenteditable="false" data-type="indexterm" data-primary="Completions API" id="xi_CompletionsAPI334267" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>. This and other APIs are versioned by using the “YYYY-MM-DD” date structure for <code class="calibre12">api-version</code>, and you will need to copy the resource name and deployment-ID from the Azure OpenAI model you previously deployed (remember the step-by-step process from the Azure portal, in <a data-type="xref" href="ch02.html#designing_cloud_native_architectures_for_generativ" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Chapter 2</a>). To create a completion resource, the POST operation is:</p>
           
            <pre data-type="programlisting" class="calibre35">POST https://{your-resource-name}.openai.azure.com/openai/deployments/
  {deployment-id}/<em class="calibre39"><strong class="calibre40">completions</strong></em>?api-version={api-version}</pre>
          
              <p class="subtitle">The request and response dynamic follows this structure, with the prompt parameter the input for the model to generate a specific completion, and a series of <a href="https://oreil.ly/Uczv9" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">optional parameters</a> such as <code class="calibre12">max_tokens</code> (the limit of tokens for the expected answer) or the number <code class="calibre12">n</code> of expected completions/answers.</p>
             
                <p class="subtitle"><em class="hyperlink">Request</em>:</p>
          
            <pre data-type="programlisting" class="calibre35">curl https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/\
  YOUR_DEPLOYMENT_NAME/completions?api-version=YYYY-MM-DD\
  -H "Content-Type: application/json" \
  -H "api-key: YOUR_API_KEY" \
  -d "{
        \"prompt\": \"The best thing in life\",
        \"max_tokens\": 5,  
        \"n\": 1
      }"</pre>
            
                <p class="subtitle"><em class="hyperlink">Response</em>:</p>
           
            <pre data-type="programlisting" data-code-language="json" class="calibre35"><code class="p">{</code><code class="w"/>
<code class="w">    </code><code class="nt">"id"</code><code class="p">:</code><code class="w"> </code><code class="s">"cmpl-4kGh7iXtjW4lc9eGhff6Hp8C7btdQ"</code><code class="p">,</code><code class="w"/>
<code class="w">    </code><code class="nt">"object"</code><code class="p">:</code><code class="w"> </code><code class="s">"text_completion"</code><code class="p">,</code><code class="w"/>
<code class="w">    </code><code class="nt">"created"</code><code class="p">:</code><code class="w"> </code><code class="mi">1646932609</code><code class="p">,</code><code class="w"/>
<code class="w">    </code><code class="nt">"model"</code><code class="p">:</code><code class="w"> </code><code class="s">"gpt-35-turbo-instruct"</code><code class="p">,</code><code class="w"/>
<code class="w">    </code><code class="nt">"choices"</code><code class="p">:</code><code class="w"> </code><code class="p">[</code><code class="w"/>
<code class="w">        </code><code class="p">{</code><code class="w"/>
<code class="w">            </code><code class="nt">"text"</code><code class="p">:</code><code class="w"> </code><code class="s">", is eating burgers with a milkshake"</code><code class="p">,</code><code class="w"/>
<code class="w">            </code><code class="nt">"index"</code><code class="p">:</code><code class="w"> </code><code class="mi">0</code><code class="p">,</code><code class="w"/>
<code class="w">            </code><code class="nt">"logprobs"</code><code class="p">:</code><code class="w"> </code><code class="kn">null</code><code class="p">,</code><code class="w"/>
<code class="w">            </code><code class="nt">"finish_reason"</code><code class="p">:</code><code class="w"> </code><code class="s">"length"</code><code class="w"/>
<code class="w">        </code><code class="p">}</code><code class="w"/>
<code class="w">    </code><code class="p">]</code><code class="w"/>
<code class="p">}</code><code class="w"/></pre>
           
              <p class="subtitle">The answers (completions) contain the <code class="calibre12">finish_reason</code> parameter. <code class="calibre12">finish_reason</code> defines why the model stopped generating more information; for most cases this will be due to <code class="calibre12">max_tokens</code>, which stops the model once it reaches the limit. However, there is another option that we will explore in <a data-type="xref" href="ch04.html#additional_cloud_and_ai_capabilities" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Chapter 4</a> that stops the model due to what we call <em class="hyperlink">content filters</em>.</p>
       </dd>
              <dt class="calibre10">
                <a href="https://oreil.ly/ZJOLp" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
                  Chat completions
                </a>
              </dt>
              <dd class="calibre11">
                <p class="subtitle">Dedicated API<a contenteditable="false" data-type="indexterm" data-primary="Chat Completion API" id="xi_ChatCompletionAPI338633" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> for chat scenarios (and the only supported one for future model versions), including the configuration parameters we previously reviewed with the Chat playground. This includes input parameters we discussed for the Azure OpenAI Playground, such as <code class="calibre12">temperature</code> and <code class="calibre12">max_tokens</code>. There is one important parameter for chat messages, known as the <a href="https://oreil.ly/WLv1g" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">ChatRole</a>. This allows you to split the interactions based on different roles:</p>
                  <dl class="stafflist">
                    <dt class="calibre10">System</dt>
                    <dd class="calibre11"><p class="subtitle">Helps you set the behavior of the assistant.</p></dd>
                    <dt class="calibre10">User</dt>
                    <dd class="calibre11"><p class="subtitle">Provides input for chat completions.</p></dd>
                    <dt class="calibre10">Assistant</dt>
                    <dd class="calibre11"><p class="subtitle">Provides responses to system-instructed, user-prompted input.</p></dd>
                    <dt class="calibre10">Function</dt>
                    <dd class="calibre11"><p class="subtitle">Provides function results for chat completions. We will explore this concept later in this chapter, after we cover the different Azure OpenAI APIs.</p></dd>
                  </dl>
              <p class="subtitle">The sequence for a typical chat scenario follows these steps:</p>
              <dl class="stafflist">
                <dt class="calibre10">1. Resource creation</dt>
                <dd class="calibre11">
                  <p class="subtitle">Using a similar structure to what you have seen in a regular completion API call (including the date as the API version). The regular POST operation for chat completion is:</p>
                  <pre data-type="programlisting" class="calibre35">POST https://{your-resource-name}.openai.azure.com/openai/deployments/
  {deployment-id}/<strong class="calibre41">chat/completions</strong>?api-version={api-version}</pre>
                </dd>
                <dt class="calibre10">2. System message</dt>
                <dd class="calibre11">
                  <p class="subtitle">This is how you set the context of the chat engine, by defining the scope of the discussion, allowed or forbidden topics, etc. The system message<a contenteditable="false" data-type="indexterm" data-primary="system message" id="id536" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> is also called the context prompt<a contenteditable="false" data-type="indexterm" data-primary="context prompt" id="id537" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="prompt engineering" data-secondary="context prompt" id="id538" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> or <em class="hyperlink">meta-prompt</em>. The <a href="https://oreil.ly/vFEYS" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"><code class="calibre12">messages</code> parameter</a>, along with the <a href="https://oreil.ly/y5HFq" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"><code class="calibre12">role</code> subparameter</a>, is the place<a contenteditable="false" data-type="indexterm" data-primary="parameter customization" id="id539" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> where you will define your system message, using:</p>
                  <pre data-type="programlisting" data-code-language="json" class="calibre35"><code class="p">{</code><code class="w">
  </code><code class="nt">"messages"</code><code class="p">:</code><code class="w"> </code><code class="p">[</code><code class="w">
    </code><code class="p">{</code><code class="w">
      </code><code class="nt">"role"</code><code class="p">:</code><code class="w"> </code><strong class="calibre41"><code class="s1">"system"</code></strong><code class="p">,</code><code class="w">
      </code><code class="nt">"content"</code><code class="p">:</code><code class="w"> </code><strong class="calibre41"><code class="s1">"the context and system message to add to your chat"</code></strong><code class="w">
    </code><code class="p">}</code><code class="w">
  </code><code class="p">]</code><code class="w">
</code><code class="p">}</code></pre>
                </dd>
                <dt class="calibre10">3. User-assistant interaction</dt>
                <dd class="calibre11">
                  <p class="subtitle">This leverages the same <code class="calibre12">messages</code> parameter, with the <em class="hyperlink">user</em> and <em class="hyperlink">assistant</em> roles. The structure for both roles is similar to what we have discussed for the system message, and the response includes the same <code class="calibre12">finish-reason</code> parameter that will give you a hint about the result (i.e., if the completion has finished due to the <code class="calibre12">max_tokens</code> assigned to the answer, or if there is a filtering reason due to negative topic detection)<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_CompletionsAPI334267" id="id540" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_ChatCompletionAPI338633" id="id541" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
                </dd>
              </dl>
            </dd>
           
              <dt class="calibre10">
                <a href="https://oreil.ly/bm-7a" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
                  Image generation
                </a>
              </dt>
              <dd class="calibre11">
                <p class="subtitle">The API call to generate images<a contenteditable="false" data-type="indexterm" data-primary="Image Generation API" id="id542" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> based on text-to-image DALL·E models. As with the visual playground, the input parameters include the text-based prompt, and two optional inputs such as the number <code class="calibre12">n</code> of desired images (if you don’t include it, the system will generate only one image), and the size (by default 1024×1024, with alternative 256×256 and 512×512 options). The POST operation to create an image generation resource is:</p>
            
              <pre data-type="programlisting" class="calibre35">POST https://{your-resource-name}.openai.azure.com/openai/\
  <strong class="calibre41">images/generations</strong>:submit?api-version={api-version}</pre>
            
              <p class="subtitle">Here is an example<a contenteditable="false" data-type="indexterm" data-primary="curl command-line tool" id="id543" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> of a <a href="https://oreil.ly/xApmg" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">curl (command-line tool for downloading and uploading files from various protocols and servers)</a> request:</p>
           
              <pre data-type="programlisting" class="calibre35">curl -X POST \
  https://{your-resource-name}.openai.azure.com/openai/deployments/\
  {deployment-id}/images/generations?api-version=2023-12-01-preview \
  -H "Content-Type: application/json" \
  -H "api-key: YOUR_API_KEY" \
  -d '{
        "prompt": "An avocado chair",
        "size": "1024x1024",
        "n": 3,
        "quality": "hd",
        "style": "vivid"
      }'</pre>
       
              <p class="subtitle">The end-to-end process includes three different steps:</p>
              <ol class="calibre42">
                <li class="calibre43">
                  <p class="calibre24"><em class="hyperlink">Request</em><em class="hyperlink"><strong class="calibre13"> </strong></em>the image generation (<a href="https://oreil.ly/kPf-m" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">via POST operation</a>), which helps you pre-generate the images based on the text-based input prompt. It returns an operation ID that you will leverage for the next step.</p>
                </li>
                <li class="calibre43">
                  <p class="calibre24"><em class="hyperlink">Get</em><em class="hyperlink"><strong class="calibre13"> </strong></em>the result of the image generation (<a href="https://oreil.ly/lxX0B" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">GET operation</a>), which allows you to recover the pre-generated images for the specific operation ID.</p>
                </li>
                <li class="calibre34">
                  <p class="calibre24"><em class="hyperlink">Delete</em><em class="hyperlink"><strong class="calibre13"> </strong></em>the previously loaded images (<a href="https://oreil.ly/5UfTB" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">DELETE operation</a>) from the server, for the specific Azure OpenAI resource, and the existing operation ID. If you don’t use this option, the images will be automatically deleted after 24 hours.</p>
                </li>
              </ol>
         </dd>
              <dt class="calibre10">
                <a href="https://oreil.ly/hKakE" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
                  Speech to text
                </a>
              </dt>
              <dd class="calibre11">
                <p class="subtitle">Based<a contenteditable="false" data-type="indexterm" data-primary="speech-to-text (S2T) APIs" id="id544" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Whisper (speech-to-text model)" id="id545" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="speech-to-text model (Whisper)" id="id546" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> on the <a href="https://oreil.ly/SJNcT" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure OpenAI Whisper model</a>, these APIs allow you create transcriptions from audio pieces, for a variety of languages and accents, with great performance and the possibility to combine it with other Azure OpenAI models. You can specify the input audio file, language, discussion style, output format (by default a JSON file), etc. This Azure OpenAI speech-to-text (S2T) feature has a limitation of 25 MB for the input audio file, but you can leverage<a contenteditable="false" data-type="indexterm" data-primary="Azure AI Speech" id="id547" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="Azure AI Speech services" id="id548" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> the <a href="https://oreil.ly/NnMTz" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">batch transcription mode of Azure AI Speech</a> (not Azure OpenAI, but the <a href="https://oreil.ly/-HLPL" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure AI Speech services for voice ↔ text features</a>) to transcribe bigger files. The POST operation looks similar to the previous APIs:</p>
              </dd>
            <dd class="calibre11">
              <pre data-type="programlisting" class="calibre35">POST https://{your-resource-name}.openai.azure.com/openai/deployments/
  {deployment-id}/<strong class="calibre41">audio/transcriptions</strong>?api-version={api-version}</pre>
           
              <p class="subtitle">The corresponding curl request (illustrative example):</p>
            
              <pre data-type="programlisting" class="calibre35">curl $AZURE_OPENAI_ENDPOINT/openai/deployments/MyDeploymentName/\
  audio/transcriptions?api-version=2023-09-01-preview \
  -H "api-key: $AZURE_OPENAI_KEY" \
  -H "Content-Type: multipart/form-data" \
  -F file="@./wikipediaOcelot.wav"</pre>
            </dd>
              <dt class="calibre10">
                <a href="https://oreil.ly/imKOS" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
                  Embeddings
                </a>
              </dt>
              <dd class="calibre11">
                <p class="subtitle">This API<a contenteditable="false" data-type="indexterm" data-primary="Embeddings API" id="id549" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> call allows you to generate embeddings from specific text inputs, from some of the architectures you will see in this chapter. The model and its specific input length will depend on <a href="https://oreil.ly/gvAHr" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">model availability</a> at the time of your implementation. The POST operation is similar to the previous ones, and the dynamic is as simple as <a href="https://oreil.ly/xFJTh" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">requesting the embeddings</a> for a text input and <a href="https://oreil.ly/yYCuU" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">obtaining a JSON response</a> with the generated embeddings, for you to store (we will see several vector store/database options by the end of the chapter) and leverage them later:</p>
          
              <pre data-type="programlisting" class="calibre35">POST https://{your-resource-name}.openai.azure.com/openai/deployments/
  {deployment-id}/<strong class="calibre41">embeddings</strong>?api-version={api-version}</pre>
        
              <p class="subtitle">And the corresponding curl example:</p>
            
              <pre data-type="programlisting" class="calibre35">curl https://YOUR_RESOURCE_NAME.openai.azure.com/openai/deployments/\
  YOUR_DEPLOYMENT_NAME/embeddings?api-version=2023-05-15\
  -H 'Content-Type: application/json' \
  -H 'api-key: YOUR_API_KEY' \
  -d '{"input": "Sample Document goes here"}'</pre>
          </dd>
              <dt class="calibre10">
                <a href="https://oreil.ly/1pqcT" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">
                  Fine-tuning
                </a>
              </dt>
              <dd class="calibre11">
                <p class="subtitle">As we reviewed at the beginning of this chapter, one of the implementation options includes the ability to fine-tune<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning GPT models" data-secondary="development interfaces" id="id550" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="fine-tuning GPT models" id="id551" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> pre-built models with your specific, available information. We will see more details later in this chapter, but for now keep in mind that if you choose this option, there is a specific set of APIs that you can leverage to create, manage, explore, and delete new fine-tuning “jobs.” Also, you will handle your own input files for the fine-tuned models. </p>
              </dd>
              <dt class="calibre10">Other relevant APIs</dt>
              <dd class="calibre11">
                <p class="subtitle">Other relevant APIs include the following:</p>
              
              <dl class="stafflist">
                <dt class="calibre10"><a href="https://oreil.ly/2yZuu" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Bing Search</a></dt>
                <dd class="calibre11">
                  <p class="subtitle">The Bing Search API<a contenteditable="false" data-type="indexterm" data-primary="Bing Search API" id="id552" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> allows you to leverage Microsoft Bing’s search engine for your own development. You can extend the capabilities of your Azure OpenAI–enabled implementations with live search functionalities.</p>
                </dd>
                <dt class="calibre10"><a href="https://oreil.ly/vxtJA" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Form Recognizer (currently known as Azure AI Document Intelligence)</a></dt>
                <dd class="calibre11">
                  <p class="subtitle">This helps you transform<a contenteditable="false" data-type="indexterm" data-primary="Form Recognizer API" id="id553" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Azure AI Document Intelligence (Form Recognizer)" id="id554" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> information from forms and images into structured data. It includes advanced optical character recognition (OCR) functionalities that will support your Azure OpenAI development with specific data sources such as PDF or DOC files.</p>
                </dd>
                <dt class="calibre10"><a href="https://oreil.ly/wp6r8" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure AI Search (previously known as Azure Cognitive Search)</a></dt>
                <dd class="calibre11">
                  <p class="subtitle">One of the most important elements for RAG architectures<a contenteditable="false" data-type="indexterm" data-primary="Azure AI Search" id="id555" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, for both vectors and index approaches.</p>
                </dd>
              </dl>
          
              <p class="subtitle">In addition<a contenteditable="false" data-type="indexterm" data-primary=".NET developers, Azure OpenAI library for" data-primary-sortas="NET developers, Azure OpenAI library for" id="id556" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Python" id="id557" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> to these APIs, there is an Azure <a href="https://oreil.ly/9XMBN" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">OpenAI library for .NET developers</a> and the <a href="https://oreil.ly/-cwGH" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">OpenAI library for Python</a>, which essentially replicates the features of the official API for a .NET development environment. It provides an interface with the rest of the Azure SDK ecosystem, and it facilitates the connection to Azure OpenAI resources or to non–Azure OpenAI endpoints.</p>
            </dd>
            </dl>
            <p class="subtitle">This set of visual and development interfaces are your toolkit for most of the Azure OpenAI implementations out there. They are rapidly evolving, but the links to the official documentation will help you access updated information any time. Now, before moving on to the implementation approaches, let’s take a look at a powerful feature that will enable your generative AI systems to interact with other external APIs: function calling<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_implementationofgenerativeAIbuildingblocksdevelopmentinterfaces331769" id="id558" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_developmentinterfaces331769" id="id559" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_interfacesgenerativeAI363101" id="id560" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_interfacesgenerativeAIdevelopmentinterfaces331769" id="id561" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_APIsapplicationprogramminginterfaces331829" id="id562" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Interoperability features: Function calling and “JSONization”" class="calibre5"><div class="preface" id="interoperability_features_function_calling_and_j">
            <h3 class="calibre38">Interoperability features: Function calling and “JSONization”</h3>
            <p class="subtitle">The <a href="https://oreil.ly/bQdsv" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure OpenAI function calling</a> option<a contenteditable="false" data-type="indexterm" data-primary="Azure OpenAI Service" data-secondary="function calling option" id="id563" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> is a way to leverage language<a contenteditable="false" data-type="indexterm" data-primary="function calling with Azure OpenAI" id="id564" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> models<a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="building blocks" data-tertiary="interoperability features" id="id565" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="interoperability features, generative AI" id="id566" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> to generate API calls and structure data outputs based on a specific target format. Technically, it is one of the options within the Chat Completion API—the <a href="https://oreil.ly/WLv1g" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">function</a> chat role. You can see <a href="https://oreil.ly/0nhYM" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">several samples</a> on how to use this functionality, but it essentially relies on the following steps:</p>
            <ol class="stafflist">
              <li class="calibre34">
                <p class="calibre24">Calling the Chat Completions API, including the functions (based on the official <a href="https://oreil.ly/5Q-8c" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">FunctionDefinition format</a>) and the user’s input</p>
              </li>
              <li class="calibre34">
                <p class="calibre24">Using the model’s chat response to call your API or function</p>
              </li>
              <li class="calibre34">
                <p class="calibre24">Calling the Chat Completions API again, including the response from your function, to get a final response</p>
              </li>
            </ol>
            <p class="subtitle">This is a relatively new functionality, so you can expect some feature improvements over time. You can always check the <a href="https://oreil.ly/UAYNH" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">official documentation</a> to get the latest details and advice. Additionally, you can also explore<a contenteditable="false" data-type="indexterm" data-primary="JavaScript Object Notation (JSON)" id="id567" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> the <a href="https://oreil.ly/Fi3-l" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">JSON mode</a> for Azure OpenAI, as it allows you to get a JSON object from the Chat Completions API answer, a powerful feature for interoperability purposes.</p>
            <p class="subtitle">This completes the first part of this section. You have learned about the knowledge domains, how to leverage different building blocks to improve and increase the level of knowledge of your generative AI solutions, and the availability tools you will use for implementation. Now, we will move to the next part of this chapter, in which we will explore some of the most relevant development approaches, based on the industry’s best practices. Let’s get started<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_implementationofgenerativeAIbuildingblocks35990" id="id568" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
          </div></section>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Potential Implementation Approaches" class="calibre5"><div class="preface" id="potential_implementation_approaches">
          <h2 class="calibre21">Potential Implementation Approaches</h2>
          <p class="subtitle">There are several ways to implement generative AI applications with Azure OpenAI Service. The type of implementations you use will mostly depend on your specific use case, as well as the technical and financial context for adoption. This means there are situations where the most expensive option is not always the best, or other options may have limitations, such as when we don’t have specific data besides our website, etc. Let’s explore the primary implementation types, based on the customization levels of <a data-type="xref" href="#fig_16_implementation_approaches_with_azure_openai" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-16</a>.</p>
          <figure class="calibre18"><div id="fig_16_implementation_approaches_with_azure_openai" class="figure">
            <img src="assets/aoas_0316.png" alt="" class="calibre19"/>
            <h6 class="calibre20"><span class="keep-together">Figure 3-16. </span>Implementation approaches with Azure OpenAI Service</h6>
          </div></figure>
          <p class="subtitle">As you can see from the figure, you can customize a model by preparing a good meta-prompt, adjusting technical parameters, providing one or a few “shots” as examples to guide the model, and implementing fine-tuning and/or grounding techniques. The next sections will go into the details of how to do all of this.</p>
          <section data-type="sect3" data-pdf-bookmark="Basic Azure ChatGPT instance" class="calibre5"><div class="preface" id="basic_azure_chatgpt_instance">
            <h3 class="calibre38">Basic Azure ChatGPT instance</h3>
            <p class="subtitle">A basic, private GPT type of instance<a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="Azure ChatGPT instance" id="xi_implementationofgenerativeAIAzureChatGPTinstance356453" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Azure ChatGPT instance" id="xi_AzureChatGPTinstance356453" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="ChatGPT" data-secondary="Azure ChatGPT instance" id="xi_ChatGPTAzureChatGPTinstance356453" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> is the simplest kind of implementation, and one of the most popular Azure OpenAI cases nowadays. When companies want to have a private “ChatGPT” for their employees, this is the answer. It keeps your own data safe and private and deploys the instance within your own cloud infrastructure. It’s one of the favorite options for internal use with employees.</p>
            <p class="subtitle">The deployment process is relatively simple:</p>
            <ol class="stafflist">
              <li class="calibre34">
                <p class="calibre24">Within your Azure OpenAI Studio, deploy a GPT-3.5 Turbo, GPT-4, GPT-4 Turbo, or GPT-4o model instance. This type of model is technically similar to what ChatGPT is, and it will deliver that level of performance. Remember to choose the specific geographic region that is closest to you.</p>
              </li>
              <li class="calibre34">
                <p class="calibre24">Once you have created the resource, go to the visual playground. There, you will see a left menu with the option “Chat.”</p>
              </li>
              <li class="calibre34">
                <p class="calibre24">Once there, you can prepare the <a href="https://oreil.ly/OmKQO" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">system message</a> / meta-prompt to contextualize the chatbot by telling it something like “You are an AI assistant for company X, to answer questions from the employees” (internal use) or “You are an AI assistant for company X with website Y. If anyone asks something that is not related to this topic, say you cannot answer” (for clients).</p>
              </li>
              <li class="calibre34">
                <p class="calibre24">You can also customize parameters such as the max length of the answers or the temperature of the messages, which is a metric between 0 and 1 to define the level of creativity of the model.</p>
              </li>
              <li class="calibre34">
                <p class="calibre24">Once you have tested performance and you are ready to deploy the model, you can come back to the resource page (Azure portal) and find both the endpoint and the keys for that specific resource. That page contains examples of code to for calling the APIs.</p>
              </li>
            </ol>
            <p class="subtitle">The end-to-end architecture (<a data-type="xref" href="#fig_17_simplified_azure_chatgpt_architecture" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-17</a>) is pretty simple—a pre-deployed model that we can directly consume from our applications, based on the existing endpoints and APIs.</p>
            <figure class="calibre18"><div id="fig_17_simplified_azure_chatgpt_architecture" class="figure">
              <img src="assets/aoas_0317.png" alt="" class="calibre19"/>
              <h6 class="calibre20"><span class="keep-together">Figure 3-17. </span>Simplified Azure ChatGPT architecture</h6>
            </div></figure>
            <p class="subtitle">This type of implementation is good enough for internal company cases where you don’t require any customization based on private data, for example, internal chatbots for employee productivity based on general internet information, or search engines for intranet sites. For the rest of the cases where there is some custom data involved, we will explore other options. Let’s dig into the first of them next<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_implementationofgenerativeAIAzureChatGPTinstance356453" id="id569" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_AzureChatGPTinstance356453" id="id570" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_ChatGPTAzureChatGPTinstance356453" id="id571" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Minimal customization with one- or few-shot learning" class="calibre5"><div class="preface" id="minimal_customization_with_one_or_few_shot_learni">
            <h3 class="calibre38">Minimal customization with one- or few-shot learning</h3>
            <p class="subtitle">Besides the baseline model, and system message/meta-prompt and parameters customization<a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="minimal customization with one- or few-shot learning" id="xi_implementationofgenerativeAIminimalcustomizationwithoneorfewshotlearning3592105" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="one- or few-shot learning" id="xi_oneorfewshotlearning3592105" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="machine learning (ML)" data-secondary="one- or few-shot learning" id="xi_machinelearningMLoneorfewshotlearning3592105" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, there is an option to perform <em class="hyperlink">one- or few-shot learning</em>, which means providing the LLM with examples of discussions based on the expected output for a specific topic. This is a useful and simple option for small adjustments, and it relies on a very similar architecture to the previous one, with relatively light changes. The main difference when compared to the previous approach is the inclusion of one or few examples to guide the LLM before starting to use it (<a data-type="xref" href="#fig_18_one_few_shot_learning_architecture" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-18</a>).</p>
            <figure class="calibre18"><div id="fig_18_one_few_shot_learning_architecture" class="figure">
              <img src="assets/aoas_0318.png" alt="" class="calibre19"/>
              <h6 class="calibre20"><span class="keep-together">Figure 3-18. </span>One/few-shot learning architecture</h6>
            </div></figure>
            <p class="subtitle">The one-shot/few-shot learning process can be achieved in several ways:</p>
            <ul class="stafflist">
              <li class="calibre8">
                <p class="calibre24">Via APIs (code)</p>
                <ul class="calibre33">
                  <li class="calibre8">
                    <p class="calibre24">Use the Chat Completions API<a contenteditable="false" data-type="indexterm" data-primary="Chat Completion API" id="id572" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> with GPT-4 and other models that are designed to take input formatted in a chat-like transcript. You can provide conversational examples that are used by the model for in-context learning.</p>
                  </li>
                  <li class="calibre8">
                    <p class="calibre24">Use the Completions API<a contenteditable="false" data-type="indexterm" data-primary="Completions API" id="id573" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> with the GPT-3 models, which can take a string of text with no specific format rules. You can provide a set of training examples as part of the prompt to give additional context to the model.</p>
                  </li>
                </ul>
              </li>
              <li class="calibre8">
                <p class="calibre24">Via playground (visual)</p>
                <ul class="calibre33">
                  <li class="calibre8">
                    <p class="calibre24">Use the Chat playground <a contenteditable="false" data-type="indexterm" data-primary="Chat playground" data-secondary="one- or few-shot learning" id="id574" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>to interact with GPT-4, GPT-4o, etc. You can add few-shot examples in the chat transcript and see how the model responds.</p>
                  </li>
                  <li class="calibre8">
                    <p class="calibre24">Use the Completions playground<a contenteditable="false" data-type="indexterm" data-primary="Completions playground" id="id575" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> to interact with the GPT-x models. You can write your prompt with few-shot examples and see how the model completes it.</p>
                  </li>
                </ul>
              </li> 
            </ul>

              
              
            <p class="subtitle">Overall, all these customizations are intended to improve the performance of the model versus a regular vanilla “ChatGPT” implementation like the one we previously explored, but there are ways to retrain the model in a deeper way, like the one we will explore next.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Fine-tuned GPT models" class="calibre5"><div class="preface" id="fine_tuned_gpt_models">
            <h3 class="calibre38">Fine-tuned GPT models</h3>
            <p class="subtitle">As mentioned earlier in the chapter, there are different ways to customize<a contenteditable="false" data-type="indexterm" data-primary="fine-tuning GPT models" data-secondary="training process" id="xi_finetuningGPTmodelstrainingprocess362990" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="generative pre-trained transformer (GPT) architecture" id="xi_generativepretrainedtransformerGPTarchitecture362990" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="fine-tuning GPT models" id="xi_implementationofgenerativeAIfinetuningGPTmodels362990" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="GPT (generative pre-trained transformer) architecture" id="xi_GPTgenerativepretrainedtransformerarchitecture362990" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> an LLM to adjust its knowledge scope. Most of them rely on the orchestration/combination of the LLM with other knowledge pieces, without really combining the data sources (i.e., grounding). In this case, we will focus on the only way to “retrain” an Azure OpenAI model with custom company data: the <a href="https://oreil.ly/T0GP8" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure OpenAI Service fine-tuning feature</a>.</p>
            <p class="subtitle">This approach may have some advantages for companies with very specific and valuable data intellectual property, but its cost (you will need to add hosting cost to the regular API calls for the fine-tuning process) and technical complexity will probably lead you (and most of the adopters out there) to other kinds of grounding approaches with better performance/cost balance. </p>
            <p class="subtitle">Also, the fine-tuning feature relies on a very special kind of training<a contenteditable="false" data-type="indexterm" data-primary="supervised learning" id="id576" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="machine learning (ML)" data-secondary="supervised learning" id="id577" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> process. It is not the regular label-based training process you can do, for example, in classification tasks with traditional AI models. We are talking about a new kind of supervised process that leverages Azure OpenAI’s prompting system to inject information based on the <a href="https://oreil.ly/SdBph" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">JSON Lines (JSONL) file format</a>.</p>
            <p class="subtitle">For example, with GPT-3.5 Turbo, you will leverage the system and user roles to reeducate the model:</p>
              <pre data-type="programlisting" data-code-language="json" class="calibre35"><code class="p">{</code><code class="w"/>
<code class="w">  </code><code class="nt">"messages"</code><code class="p">:</code><code class="w"> </code><code class="p">[</code><code class="w"/>
<code class="w">    </code><code class="p">{</code><code class="w"/>
<code class="w">      </code><code class="nt">"role"</code><code class="p">:</code><code class="w"> </code><code class="s">"system"</code><code class="p">,</code><code class="w"/>
<code class="w">      </code><code class="nt">"content"</code><code class="p">:</code><code class="w"> </code><code class="s">"Marv is a factual chatbot that is also sarcastic."</code><code class="w"/>
<code class="w">    </code><code class="p">},</code><code class="w"/>
<code class="w">    </code><code class="p">{</code><code class="w"/>
<code class="w">      </code><code class="nt">"role"</code><code class="p">:</code><code class="w"> </code><code class="s">"user"</code><code class="p">,</code><code class="w"/>
<code class="w">      </code><code class="nt">"content"</code><code class="p">:</code><code class="w"> </code><code class="s">"Who wrote 'Romeo and Juliet'?"</code><code class="w"/>
<code class="w">    </code><code class="p">},</code><code class="w"/>
<code class="w">    </code><code class="p">{</code><code class="w"/>
<code class="w">      </code><code class="nt">"role"</code><code class="p">:</code><code class="w"> </code><code class="s">"assistant"</code><code class="p">,</code><code class="w"/>
<code class="w">      </code><code class="nt">"content"</code><code class="p">:</code><code class="w"> </code><code class="s">"Oh, just some guy named William Shakespeare. Heard of him?"</code><code class="w"/>
<code class="w">    </code><code class="p">}</code><code class="w"/>
<code class="w">  </code><code class="p">]</code><code class="w"/>
<code class="p">}</code><code class="w"/></pre>
            <p class="subtitle">Other legacy models such as DaVinci require a prompt/completion format based on a question-answer logic:</p>
              <pre data-type="programlisting" data-code-language="json" class="calibre35"><code class="p">{</code><code class="nt">"prompt"</code><code class="p">:</code><code class="w"> </code><code class="s">"&lt;prompt text&gt;"</code><code class="p">,</code><code class="w"> </code><code class="nt">"completion"</code><code class="p">:</code><code class="w"> </code><code class="s">"&lt;ideal generated text&gt;"</code><code class="p">}</code><code class="w"/>
<code class="p">{</code><code class="nt">"prompt"</code><code class="p">:</code><code class="w"> </code><code class="s">"&lt;prompt text&gt;"</code><code class="p">,</code><code class="w"> </code><code class="nt">"completion"</code><code class="p">:</code><code class="w"> </code><code class="s">"&lt;ideal generated text&gt;"</code><code class="p">}</code><code class="w"/></pre>
            <p class="subtitle">This new way to inject data and knowledge<a contenteditable="false" data-type="indexterm" data-primary="knowledge scope of Azure OpenAI-enabled apps" id="id578" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> allows us to reeducate the model in a very granular manner, but it is a complex way to do so. You can see the overall architecture in <a data-type="xref" href="#fig_19_azure_openai_fine_tuning_architecture" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-19</a>, in which you will basically customize the model, based on a fine-tuning process that relies on specific organizational data.</p>
            <figure class="calibre18"><div id="fig_19_azure_openai_fine_tuning_architecture" class="figure">
              <img src="assets/aoas_0319.png" alt="" class="calibre19"/>
              <h6 class="calibre20"><span class="keep-together">Figure 3-19. </span>Azure OpenAI fine-tuning architecture</h6>
            </div></figure>
            <p class="subtitle">The steps to perform <em class="hyperlink">fine-tuning</em> with Azure OpenAI Service are:</p>
            <ol class="stafflist">
              <li class="calibre34">
                <p class="calibre24"><em class="hyperlink">Prepare your dataset</em> in JSONL format. For recent models such as GPT-3.5 Turbo, GPT-4, and GPT-4o, you will leverage the Chat Completions API structure for system and user messages.</p>
              </li>
              <li class="calibre34">
                <p class="calibre24">Launch the <em class="hyperlink">custom model wizard</em> from Azure OpenAI Studio, as shown in <a data-type="xref" href="#fig_20_azure_openai_custom_model_wizard" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-20</a>, to train your new customized model.</p>
                <figure class="calibre18"><div id="fig_20_azure_openai_custom_model_wizard" class="figure">
                  <img src="assets/aoas_0320.png" alt="" class="calibre19"/>
                  <h6 class="calibre20"><span class="keep-together">Figure 3-20. </span>Azure OpenAI: custom model wizard</h6>
                </div></figure>
              </li>
              <li class="calibre34">
                <p class="calibre24"><em class="hyperlink">Select a base model</em> (e.g., GPT-3.5 Turbo), choosing your training data and, optionally, your validation data to evaluate model performance. Those datasets are the JSON files you previously prepared.</p>
              </li>
              <li class="calibre34">
                <p class="calibre24">Review your choices and <em class="hyperlink">launch the training</em> of the new customized model. Check the status of your customized model and wait for the training to finish.</p>
              </li>
              <li class="calibre34">
                <p class="calibre24"><em class="hyperlink">Deploy your customized model</em> for use in an application or service, via APIs.</p>
              </li>
            </ol>
            <p class="subtitle">All these options<a contenteditable="false" data-type="indexterm" data-primary="retrieval-augmented generation (RAG)" id="xi_retrievalaugmentedgenerationRAG367933" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="grounding techniques" id="xi_groundingtechniques367933" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="RAG (retrieval-augmented generation)" id="xi_RAGretrievalaugmentedgeneration367933" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> can work <a href="https://oreil.ly/cK_7b" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">depending on the type of application</a> and the intended scope of the model customization. However, there are ways to combine the LLM with internal data sources, from which you can extract knowledge, and then refer to that information from the Azure OpenAI completion and chat completion models. This is what we call <a href="https://oreil.ly/26QYs" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">RAG</a> or grounding, and there are different ways to implement it. The next sections contain different grounding alternatives.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Embedding-based grounding" class="calibre5"><div class="preface" id="embedding_based_grounding">
            <h3 class="calibre38">Embedding-based grounding</h3>
            <p class="subtitle">As you now know from earlier chapters, embeddings<a contenteditable="false" data-type="indexterm" data-primary="embedding-based grounding" id="xi_embeddingbasedgrounding368365" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> are mathematical representations of text-based information as vectors in a vector space, and an alternative and/or complement to the traditional index-based approach. These embeddings are stored and managed as mathematical vectors that represent distances between topics. This means if we are looking for information about animals and we have a vectorized knowledge base that includes animal-related topics, we can recover the Top-k answers (i.e., the most relevant “K” number of pieces of information).</p>
            <p class="subtitle">You can use the Azure OpenAI embeddings API<a contenteditable="false" data-type="indexterm" data-primary="Embeddings API" id="id579" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> to generate vector representations of text that capture the semantic meaning and similarity of the text. Some possible use cases for embeddings are document search, text classification, clustering, or text <span class="keep-together">similarity.</span></p>
            <p class="subtitle">The end-to-end process to create and use an embedding-based system is aligned with what you have seen thus far in this chapter. From an Azure OpenAI perspective, the steps are as follows:</p>
            <ol class="stafflist">
              <li class="calibre34">
                <p class="calibre24"><em class="hyperlink">Select the knowledge base</em> that contains the information that will complement the baseline LLM knowledge domain. This may include PDF, DOC, PPT, TXT, and other file formats. In Azure, you may store that information via Azure Blob Storage or Azure Data Lake Gen2. Keep in mind that if your files are similar to any general information that may be available on the internet (for example, public descriptions of industry concepts), you probably don’t need to ground them. However, if you have very specific files with information on how to answer questions, or perform internal tasks, those may be good candidates for embeddings generation.</p>
              </li>
              <li class="calibre34">
                <p class="calibre24"><em class="hyperlink">Choose and deploy your database/vector store</em>. By the end of this chapter, you will see all available options for implementation in Azure with Azure OpenAI–generated embeddings.</p>
              </li>
              <li class="calibre34">
                <p class="calibre24"><em class="hyperlink">Prepare the input dataset</em>. This includes two different steps:</p>
                <ol class="calibre44">
                  <li class="calibre45">
                    <p class="calibre24"><em class="hyperlink">Extract the information</em> from your documents. For example, you can use Azure Document Intelligence/Form Recognizer to extract text from your PDFs with the OCR feature. You can also use other non-Azure tools.</p>
                  </li>
                  <li class="calibre45">
                    <p class="calibre24"><em class="hyperlink">Split the information</em>. For this to work, it is important to keep in mind<a contenteditable="false" data-type="indexterm" data-primary="token limit" id="id580" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> the <a href="https://oreil.ly/SQSGw" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">embeddings model token limit</a> (e.g., 8K for Ada model version 2) to prepare the input without exceeding the limit<a contenteditable="false" data-type="indexterm" data-primary="tokenizer, OpenAI" id="id581" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="OpenAI" id="id582" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> (you can use <a href="https://oreil.ly/DDQHG" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">OpenAI’s tokenizer tool</a> to understand the extent of what 8K means in terms of document length). This means you will need to make one API call for each of the limited-size blocks you have prepared before, or leverage <a href="https://oreil.ly/3DHfa" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">chunking techniques</a> to split and handle larger documents.</p>
                  </li>
                </ol>
              </li>
              <li class="calibre34">
                <p class="calibre24"><em class="hyperlink">Leverage the Azure OpenAI <a href="https://oreil.ly/gvAHr" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">embeddings models</a></em>. Use the API operations you saw earlier in this chapter and get the mathematical vectors from the API response. <em class="hyperlink">Store the vectors</em> within the chosen vector store.</p>
              </li>
              <li class="calibre34">
                <p class="calibre24">Any time you want to find information from your knowledge base, or if you want to leverage it from any chat or search application, you will need to <em class="hyperlink">generate the embeddings of the question itself</em>, then perform the search against the vector search. Keep in mind that you will need to leverage the same model (e.g., Ada version 2) for both your knowledge base and the question. You can send the result of the search, with the Top-k results, to the chat or search application, directly or by including it as content for the answer of the completion.</p>
              </li>
            </ol>
            <p class="subtitle">This process is similar for other embeddings and conversation models (for example, those that are available via Azure AI Studio’s model catalog and Hugging Face), and the high-level architecture includes the elements you can see in <a data-type="xref" href="#fig_21_embedding_based_grounding_architecture" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-21</a>: basically, the baseline Azure OpenAI model gets complemented with the internal knowledge base that contains PDFs, Word docs, etc. Instead of retraining/fine-tuning the model, we just combine it with that knowledge base so it can find similarities between the users’ questions and the information contained within the data sources.</p>
            <figure class="calibre18"><div id="fig_21_embedding_based_grounding_architecture" class="figure">
              <img src="assets/aoas_0321.png" alt="" class="calibre19"/>
              <h6 class="calibre20"><span class="keep-together">Figure 3-21. </span>Embedding-based grounding architecture</h6>
            </div></figure>
            <p class="subtitle">You can find more information and code examples on <a href="https://oreil.ly/8Duc8" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">how to create embeddings</a> from the official Microsoft documentation (in addition to the API definitions we covered earlier in this chapter). </p>
            <p class="subtitle">Additionally, there is <a href="https://oreil.ly/iG5UU" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">one official Microsoft accelerator</a> for this type of implementation that you can leverage during the development phase. There are several deployment and storage options. Feel free to explore the code to see the API call details<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_embeddingbasedgrounding368365" id="id583" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Document indexing/retrieval-based grounding" class="calibre5"><div class="preface" id="document_indexing_retrieval_based_grounding">
            <h3 class="calibre38">Document indexing/retrieval-based grounding</h3>
            <p class="subtitle">The document indexing/retrieval-based grounding<a contenteditable="false" data-type="indexterm" data-primary="document indexing/retrieval-based grounding" id="xi_documentindexingretrievalbasedgrounding372163" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="retrieval-based grounding/document indexing" id="xi_retrievalbasedgroundingdocumentindexing372163" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> approach is an alternative to the embedding-based approach. In this case, we will not generate mathematical vectors. Instead, we will generate indexes of specific documents, so Azure OpenAI Service can find the information from those sources and include it as part of its answers. For that purpose, we will also use Azure Cognitive Search<a contenteditable="false" data-type="indexterm" data-primary="Azure Cognitive Search" id="id584" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, which is a service that allows you to index, understand, and retrieve relevant data from a knowledge base or a collection of documents.</p>
            <p class="subtitle">The combination of both services enables powerful chatbot applications that can communicate with users in natural language and provide intuitive and personalized interactions, based on specific data from the organization. Much like the embedding-based approach, there is an official <a href="https://oreil.ly/JNWAz" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Microsoft accelerator</a> available for you to deploy your first proof of concept, in addition<a contenteditable="false" data-type="indexterm" data-primary="GPT-RAG" id="id585" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> to a second one called <a href="https://oreil.ly/Q5NK9" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">GPT-RAG</a> from the Microsoft Argentina team, with some additional functionalities for bigger implementations. You can explore both to see updated details and implementation approaches with Azure OpenAI and Azure Cognitive Search. You can also see the high-level architecture of the key building blocks in <a data-type="xref" href="#fig_22_retrieval_based_grounding_architecture" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Figure 3-22</a>.</p>
            <figure class="calibre18"><div id="fig_22_retrieval_based_grounding_architecture" class="figure">
              <img src="assets/aoas_0322.png" alt="" class="calibre19"/>
              <h6 class="calibre20"><span class="keep-together">Figure 3-22. </span>Retrieval-based grounding architecture</h6>
            </div></figure>
            <p class="subtitle">The main difference when compared to the embedding-based approach is that instead of generating embeddings for both the knowledge base and the user question, you will just perform a search against the Azure AI Search engine (or any equivalent, as we will explore in <a data-type="xref" href="ch04.html#additional_cloud_and_ai_capabilities" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Chapter 4</a> for vector databases).</p>
            <p class="subtitle">You may see this option as something a bit simpler than the embeddings approach, and a better fit for applications where you need to find the source of information (and even provide a link to the original document as part of the answer); embeddings can potentially handle bigger datasets and deliver better performance. However, it really depends on the specific dataset and its knowledge scope and file format as well as the envisioned use case, so my recommendation is for you to try both options and evaluate the one that delivers best results from a user perspective<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_documentindexingretrievalbasedgrounding372163" id="id586" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_retrievalbasedgroundingdocumentindexing372163" id="id587" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Hybrid search–based grounding" class="calibre5"><div class="preface" id="hybrid_search_based_grounding">
            <h3 class="calibre38">Hybrid search–based grounding</h3>
            <p class="subtitle">There are newer implementation approaches based on <a href="https://oreil.ly/mwZPy" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">hybrid search techniques</a>. Concretely, hybrid search<a contenteditable="false" data-type="indexterm" data-primary="hybrid search-based grounding" id="id588" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="search use case" data-secondary="hybrid search-based grounding" id="id589" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> combines vector embeddings and doc retrieval capabilities. The <a href="https://oreil.ly/c2W8A" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">hybrid search feature</a> from Azure AI Search<a contenteditable="false" data-type="indexterm" data-primary="Azure AI Search" id="id590" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> offers that combination, plus<a contenteditable="false" data-type="indexterm" data-primary="reranking technique for hybrid search" id="id591" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> a <a href="https://oreil.ly/S7b8p" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">reranking technique</a> that produces the final result, with better performance than the previously mentioned grounding techniques. Now, let’s explore some additional grounding options that can add more knowledge scope to your generative AI <span class="keep-together">applications.</span></p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Other grounding techniques" class="calibre5"><div class="preface" id="other_grounding_techniques">
            <h3 class="calibre38">Other grounding techniques</h3>
            <p class="subtitle">We have explored several fine-tuning and grounding techniques, mainly based on text information from different sources. But what happens if you want to leverage other kinds of data? Or if the required information can be found only via live internet results? Here are some other grounding techniques you may want to explore:</p>
            <dl class="stafflist">
              <dt class="calibre10">LLM + web results</dt>
              <dd class="calibre11">
                <p class="subtitle">This approach<a contenteditable="false" data-type="indexterm" data-primary="LLM + web results grounding technique" id="id592" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Bing Web Search API" id="id593" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> relies on the <a href="https://oreil.ly/qud-9" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Bing Web Search API</a> to extend the knowledge scope of Azure OpenAI Service models. As you may know, all LLMs are based on training datasets that go up to a specific date (e.g., initial Azure OpenAI models were updated with data up to 2021). If you need updated information, you can use the Bing Web Search API to find web pages, images, videos, news, etc., or use it to create a custom search instance that filters web results based on the criteria. The result from the API can then be used by Azure OpenAI to return an answer based on that information.</p>
              </dd>
              <dt class="calibre10">LLM + tabular data and/or databases</dt>
              <dd class="calibre11">
                <p class="subtitle">Similar to other sources, tabular data<a contenteditable="false" data-type="indexterm" data-primary="LLM + tabular data and/or databases grounding technique" id="id594" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="tabular data with LLM, grounding technique" id="id595" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="databases with LLM, grounding technique" id="id596" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> (e.g., Excel and CSV files) and regular SQL-type databases (e.g., SQL Server, Azure SQL, PostgreSQL) can be good grounding sources. You can develop what the industry calls Database Copilots to allow end users to query information without any complex SQL syntax, just natural language–based prompts. Or you can leverage it for <a href="https://oreil.ly/s3snz" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">other data exploration</a> topics, such as exploratory data analysis or root-case analysis.</p>
              </dd>
            </dl>
            <p class="subtitle">Just as with the other previous grounding options, there is an <a href="https://oreil.ly/eFneC" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">official Microsoft accelerator</a> that combines these grounding techniques, with specific code samples and updated implementations.</p>
            <p class="subtitle">At the end of the day, each implementation approach (baseline, fine-tuned, or grounding based) serves a different purpose, but the next section is a summarized guide for you to understand the pros and cons of each one, so you can make the most informed decision and create your generative AI applications with Azure OpenAI with the best balance of performance, cost, and technical complexity<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_retrievalaugmentedgenerationRAG367933" id="id597" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_groundingtechniques367933" id="id598" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_finetuningGPTmodelstrainingprocess362990" id="id599" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_generativepretrainedtransformerGPTarchitecture362990" id="id600" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_implementationofgenerativeAIfinetuningGPTmodels362990" id="id601" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_GPTgenerativepretrainedtransformerarchitecture362990" id="id602" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_RAGretrievalaugmentedgeneration367933" id="id603" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
          </div></section>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Approach Comparison and Final Recommendation" class="calibre5"><div class="preface" id="approach_comparison_and_final_recommendation">
          <h2 class="calibre21">Approach Comparison and Final Recommendation</h2>
          <p class="subtitle">There is not a single right answer<a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="comparison of approaches" id="xi_implementationofgenerativeAIcomparisonofapproaches375348" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> to the question, “Which approach should I use for my generative AI implementation?” It really depends on the use case, type and volume of available data, existing IT architectures, available budget and resources, etc. Again, there is no right answer, and the choice relies for now on experimentation and performance testing.</p>
          <p class="subtitle"><a data-type="xref" href="#table-3-1" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Table 3-1</a> shows the general pros and cons of the implementation approaches<a contenteditable="false" data-type="indexterm" data-primary="ChatGPT" data-secondary="basic implementation approach" id="id604" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="one- or few-shot learning" id="id605" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="fine-tuning GPT models" data-secondary="versus other implementation approaches" data-secondary-sortas="other implementation approaches" id="id606" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="embedding-based grounding" id="id607" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="retrieval-based grounding/document indexing" id="id608" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="hybrid search-based grounding" id="id609" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Bing Search API" id="id610" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="large language models (LLMs)" data-secondary="grounding techniques" id="id611" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="machine learning (ML)" data-secondary="one- or few-shot learning" id="id612" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="search use case" data-secondary="hybrid search-based grounding" id="id613" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
          <table id="table-3-1" class="calibre25">
            <caption class="calibre26"><span class="keep-together">Table 3-1. </span>Comparison of implementation approaches with Azure OpenAI Service</caption>
            <thead class="calibre27">
            <tr class="calibre28">
               <th class="calibre29"/>
              <th class="calibre29">Approach</th>
              <th class="calibre29">Pros</th>
              <th class="calibre29">Cons</th>
            </tr>
          </thead>
          <tbody class="calibre30">
            <tr class="calibre28">
              <td class="calibre31">1</td>
              <td class="calibre31">Basic ChatGPT-type instance (vanilla, private)</td>
              <td class="calibre31">
                <ul class="stafflist">
                  <li class="calibre8">Relatively simple and quick to deploy</li>
                  <li class="calibre8">Good option for internal (employee) use cases</li>
                  <li class="calibre8">Available via Azure OpenAI’s visual playground</li>
                  <li class="calibre8">Option to define the topic scope based on URLs, by leveraging the system message</li>
                </ul>
              </td>
              <td class="calibre31">
                <ul class="stafflist">
                  <li class="calibre8">Lack of updated data</li>
                  <li class="calibre8">Very limited for client-side applications</li>
                  <li class="calibre8">Higher risk of model hallucination</li>
                </ul>
              </td>
            </tr>
            <tr class="calibre32">
              <td class="calibre31">2</td>
              <td class="calibre31">Examples with one-shot/few-shot learning</td>
              <td class="calibre31">
                <ul class="stafflist">
                  <li class="calibre8">Easy to implement</li>
                  <li class="calibre8">Good option to adapt system behavior based on specific pieces of knowledge from your company</li>
                  <li class="calibre8">Available via Azure OpenAI’s visual playground</li>
                </ul>
              </td>
              <td class="calibre31">
                <ul class="stafflist">
                  <li class="calibre8">Lack of updated data</li>
                  <li class="calibre8">Very limited for client-side applications</li>
                  <li class="calibre8">Higher risk of model hallucination</li>
                </ul>
              </td>
            </tr>
            <tr class="calibre28">
              <td class="calibre31">3</td>
              <td class="calibre31">Fine-tuning</td>
              <td class="calibre31">
                <ul class="stafflist">
                  <li class="calibre8">Good to fine-tune an existing model with specific company data</li>
                  <li class="calibre8">Leverages mature product features</li>
                </ul>
              </td>
              <td class="calibre31">
                <ul class="stafflist">
                  <li class="calibre8">Complex to prepare input data for both fine-tuning and few-shot learning</li>
                  <li class="calibre8">Increased cost for fine-tuned models</li>
                </ul>
              </td>
            </tr>
            <tr class="calibre32">
              <td class="calibre31">4</td>
              <td class="calibre31">Embedding-based grounding<br class="calibre3"/> (vectors with Azure AI Search)</td>
              <td class="calibre31">
                <ul class="stafflist">
                  <li class="calibre8">Great for customization without requiring fine-tuning </li>
                  <li class="calibre8">Good fit for large amounts of data</li>
                  <li class="calibre8">Easy use of embeddings APIs</li>
                </ul>
              </td>
              <td class="calibre31">
                <ul class="stafflist">
                  <li class="calibre8">Requires preparation of the input data based on token limits</li>
                  <li class="calibre8">Need to scan files via OCR to extract content first</li>
                  <li class="calibre8">Initial embeddings generation cost for custom data (depending on the data scope)</li>
                </ul>
              </td>
            </tr>
            <tr class="calibre28">
              <td class="calibre31">5</td>
              <td class="calibre31">Retrieval-based grounding (indexing with Azure AI Search, no embeddings)</td>
              <td class="calibre31">
                <ul class="stafflist">
                  <li class="calibre8">Good option for information retrieval from existing files</li>
                  <li class="calibre8">Indexing allows for citing sources (good for explainability)</li>
                  <li class="calibre8">Option to use the “add your own data” option from the Playground, for small implementations</li>
                </ul>
              </td>
              <td class="calibre31">
                <ul class="stafflist">
                  <li class="calibre8">Potentially less performant than embeddings for large amounts of private data (to be confirmed during your preliminary experimentation)</li>
                </ul>
              </td>
            </tr>
            <tr class="calibre32">
              <td class="calibre31">6</td>
              <td class="calibre31">Hybrid search</td>
              <td class="calibre31">
                <ul class="stafflist">
                  <li class="calibre8">More performant thanks to the combination of indexing, embeddings, and reranking of model results</li>
                  <li class="calibre8">Relatively feasible via Azure OpenAI Playground</li>
                </ul>
              </td>
              <td class="calibre31">
                <ul class="stafflist">
                  <li class="calibre8">Complex, but for Azure OpenAI, no more than the regular embedding-based RAG</li>
                </ul>
              </td>
            </tr>
            <tr class="calibre28">
              <td class="calibre31">7</td>
              <td class="calibre31">Other grounding techniques (Bing Search, databases, etc.)</td>
              <td class="calibre31">
                <ul class="stafflist">
                  <li class="calibre8">Great to add live results to the LLM, and to explore internal sources such as databases and tabular files</li>
                  <li class="calibre8">Updated results with no need to retrain or adjust the model</li>
                </ul>
              </td>
              <td class="calibre31">
                <ul class="stafflist">
                  <li class="calibre8">A bit more complex (requires orchestration engines such as LangChain or Semantic Kernel)</li>
                  <li class="calibre8">Less documentation available for this kind of implementation</li>
                </ul>
              </td>
            </tr>
          </tbody>
          </table>
          <p class="subtitle">These implementation approaches have different advantages and levels of complexity. One of the key aspects is the ability to evaluate how well they perform, and how good these Azure OpenAI models are for specific questions and tasks. Let’s explore all of this in the next section<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_implementationofgenerativeAIcomparisonofapproaches375348" id="id614" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="AI Performance Evaluation Methods" class="calibre5"><div class="preface" id="ai_performance_evaluation_methods">
          <h2 class="calibre21">AI Performance Evaluation Methods</h2>
          <p class="subtitle">One of the key stages of any generative AI project is model performance evaluation<a contenteditable="false" data-type="indexterm" data-primary="implementation of generative AI" data-secondary="performance evaluation methods" id="xi_implementationofgenerativeAIperformanceevaluationmethods389096" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="performance evaluation methods" id="xi_performanceevaluationmethods389096" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="metrics for performance evaluation" id="xi_metricsforperformanceevaluation389096" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>. However, it is not a simple task to evaluate the performance of LLM-enabled systems, and it is not fully standardized yet. That said, you can start evaluating metrics with Azure OpenAI and Azure AI Studio, as you will see in <a data-type="xref" href="ch05.html#operationalizing_generative_ai_implementations" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Chapter 5</a> with LLMOps and prompt flow for evals.</p>
          <p class="subtitle">Here is a selection of the most important metrics for generative AI evaluation:</p>
          <dl class="stafflist">
            <dt class="calibre10">Groundedness</dt>
            <dd class="calibre11">
              <p class="subtitle">Groundedness<a contenteditable="false" data-type="indexterm" data-primary="groundedness" id="id615" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> refers to how well a generative AI’s responses are based on the information given or available in the input. This is a good metric to analyze how AI sticks to the facts, in order to avoid hallucinations. You can explore the new <a href="https://oreil.ly/Lk4ZI" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Groundedness Detection feature</a> from the AI Content Safety Studio.</p>
            </dd>
            <dt class="calibre10">Similarity</dt>
            <dd class="calibre11">
              <p class="subtitle">This metric<a contenteditable="false" data-type="indexterm" data-primary="similarity metric, generative AI performance" id="id616" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> measures how much a GPT output resembles that of a human one. This is useful for human validation of the results from Azure OpenAI models.</p>
            </dd>
            <dt class="calibre10">Relevance</dt>
            <dd class="calibre11">
              <p class="subtitle">It measures<a contenteditable="false" data-type="indexterm" data-primary="relevance metric, generative AI performance" id="id617" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> how connected an AI’s output is to the input given. It’s like checking if someone’s answer in a conversation is related to the question you asked.</p>
            </dd>
            <dt class="calibre10">Classification accuracy</dt>
            <dd class="calibre11">
              <p class="subtitle">A metric for classification<a contenteditable="false" data-type="indexterm" data-primary="classification accuracy metric, generative AI performance" id="id618" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> tasks, between 0 and 1, that measures the output of the AI model compared to a ground truth.</p>
            </dd>
            <dt class="calibre10">Levenshtein distance</dt>
            <dd class="calibre11">
              <p class="subtitle">This measures<a contenteditable="false" data-type="indexterm" data-primary="Levenshtein distance metric, generative AI performance" id="id619" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> how many changes, such as adding, deleting, or changing pieces, you would need to make to get from the AI’s output to the expected output.</p>
            </dd>
            <dt class="calibre10">Coherence</dt>
            <dd class="calibre11">
              <p class="subtitle">This checks if the AI’s output makes sense and follows a logical order<a contenteditable="false" data-type="indexterm" data-primary="coherence metric, generative AI performance" id="id620" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, like checking if a story has a beginning, middle, and end, and doesn’t jump around <span class="keep-together">randomly.</span></p>
            </dd>
            <dt class="calibre10">Fluency</dt>
            <dd class="calibre11">
              <p class="subtitle">This measures how smoothly the AI’s output reads<a contenteditable="false" data-type="indexterm" data-primary="fluency metric, generative AI performance" id="id621" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>, by checking if a written paragraph is easy to read and understand, from a linguistics and grammar point of view.</p>
            </dd>
            <dt class="calibre10">F1 score</dt>
            <dd class="calibre11">
              <p class="subtitle">This is a balance<a contenteditable="false" data-type="indexterm" data-primary="F1 score metric, generative AI performance" id="id622" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> between the words in the model answer and the ground truth.</p>
            </dd>
            <dt class="calibre10">
              Other metrics
            </dt>
            <dd class="calibre11">
              <p class="subtitle">Other metrics from traditional NLP.</p>
            </dd>
          </dl>
          <p class="subtitle">From an Azure perspective, you can explore the available metrics for evaluation<a contenteditable="false" data-type="indexterm" data-primary="Azure AI Studio" data-secondary="metrics for evaluation" id="id623" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="Azure Databricks with MLflow" id="id624" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/> via <a href="https://oreil.ly/q2S7r" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure AI Studio</a> and <a href="https://oreil.ly/3kONX" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure Databricks with MLFlow</a>. Here are several ongoing initiatives from some of the main industry actors (including Microsoft and OpenAI), but you can expect more news and tools in the upcoming months and years:</p>
          <ul class="stafflist">
            <li class="calibre8">
              <p class="calibre24">Microsoft’s <a href="https://oreil.ly/H6gB8" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">LLM evaluation framework</a></p>
            </li>
            <li class="calibre8">
              <p class="calibre24">Microsoft’s <a href="https://oreil.ly/4NrIz" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">evaluation flows (Azure AI Studio)</a></p>
            </li>
            <li class="calibre8">
              <p class="calibre24">Microsoft’s documentation for <a href="https://oreil.ly/VtjxD" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">LLM metrics monitoring</a></p>
            </li>
            <li class="calibre8">
              <p class="calibre24">OpenAI’s <a href="https://oreil.ly/NgdLZ" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Evals project</a> </p>
            </li>
          </ul>
          <p class="subtitle">Additionally, there are other families of metrics that you can use to measure and analyze performance:</p>
          <dl class="stafflist">
            <dt class="calibre10">Positive/negative review of answers</dt>
            <dd class="calibre11">
              <p class="subtitle">A manual way to both track performance and potentially reeducate the model with weighted reconfigurations (e.g., few-shot learning with the good answers). You could enable this by using a positive/negative sign in the UI and by adding a binary numeric value at the database level if you decide to store the questions and answers for review purposes (e.g., ID, question, answer, review) in a JSON file stored via Cosmos DB. For this purpose, my recommendation is to create a set of test questions, and to involve subject-matter experts during the creation of that set and during the evaluation of the system.</p>
            </dd>
            <dt class="calibre10">Traditional product analytics metrics</dt>
            <dd class="calibre11">
              <p class="subtitle">For example, session time, amount of re-questioning to get the best answer, overall product rating, etc. This would require tools such as <a href="https://oreil.ly/2RHm1" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Microsoft Clarity</a>, Pendo, Amplitude, Mixpanel, etc., connected to the cloud native app (e.g., iOS, Android, web, etc.). Alternatively, there are cloud native features such as <a href="https://oreil.ly/HXkzL" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Azure App Insights</a> that can be deployed as part of the generative AI app monitoring system. Additionally, these tools can be leveraged to track performance for A/B testing experiments (for example, if we launch two different versions of the AI model with different user sets)<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_implementationofgenerativeAIperformanceevaluationmethods389096" id="id625" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_performanceevaluationmethods389096" id="id626" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_metricsforperformanceevaluation389096" id="id627" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>.</p>
            </dd>
          </dl>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Conclusion" class="calibre5"><div class="preface" id="conclusion_2">
        <h1 class="calibre4">Conclusion</h1>
        <p class="subtitle">This chapter includes not only the available visual and code-based tools for your Azure OpenAI implementations, but also the recommended implementation approaches, to help you understand the differences between regular, fine-tuned, and grounded LLMs. Once again, there is not a perfect or a single way to do it<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_implementationofgenerativeAI3386" id="id628" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2"/>. All of these approaches try to leverage the existing power of the Azure OpenAI models, and the ability to increase the knowledge scope of your applications with examples, internal data sources, live internet search, etc. In <a data-type="xref" href="ch04.html#additional_cloud_and_ai_capabilities" class="pcalibre1 pcalibre3 calibre6 pcalibre4 pcalibre2">Chapter 4</a> we’ll take a look at additional building blocks for your generative AI development. </p>
      </div></section>
    </div></section></div>
</div>
</body></html>