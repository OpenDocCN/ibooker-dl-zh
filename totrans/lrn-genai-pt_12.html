<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">10 Training a Transformer to translate English to French</h1>
<p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="idIndexMarker001"/><a id="marker-217"/><a id="idIndexMarker002"/></p>
<ul class="calibre5">
<li class="co-summary-bullet">Tokenizing English and French phrases to subwords</li>
<li class="co-summary-bullet">Understanding word embedding and positional encoding</li>
<li class="co-summary-bullet">Training a Transformer from scratch to translate English to French</li>
<li class="co-summary-bullet">Using the trained Transformer to translate an English phrase into French</li>
</ul>
<p class="body">In the last chapter, we built a Transformer from scratch that can translate between any two languages, based on the paper “Attention Is All You Need.”<sup class="footnotenumber" id="footnote-000-backlink"><a class="url1" href="#footnote-000">1</a></sup> Specifically, we implemented the self-attention mechanism, using query, key, and value vectors to calculate scaled dot product attention (SDPA). <a id="idIndexMarker003"/></p>
<p class="body">To have a deeper understanding of self-attention and Transformers, we’ll use English-to-French translation as our case study in this chapter. By exploring the process of training a model for converting English sentences into French, you will gain a deep understanding of the Transformer’s architecture and the functioning of the attention mechanism.</p>
<p class="body">Picture yourself having amassed a collection of more than 47,000 English-to-French translation pairs. Your objective is to train the encoder-decoder Transformer from the last chapter using this dataset. This chapter will walk you through all phases of the project. You’ll first use subword tokenization to break English and French phrases into tokens. You’ll then build your English and French vocabularies, which contain all unique tokens in each language. The vocabularies allow you to represent English and French phrases as sequences of indexes. After that, you’ll use word embedding to transform these indexes (essentially one-hot vectors) into compact vector representations. We’ll add positional encodings to the word embeddings to form input embeddings. Positional encodings allow the Transformer to know the ordering of tokens in the sequence.</p>
<p class="body">Finally, you’ll train the encoder-decoder Transformer from chapter 9 to translate English to French by using the collection of English-to-French translations as the training dataset. After training, you’ll learn to translate common English phrases to French with the trained Transformer. Specifically, you’ll use the encoder to capture the meaning of the English phrase. You’ll then use the decoder in the trained Transformer to generate the French translation in an autoregressive manner, starting with the beginning token <code class="fm-code-in-text">"BOS"</code>. In each time step, the decoder generates the most likely next token based on previously generated tokens and the encoder’s output, until the predicted token is <code class="fm-code-in-text">"EOS"</code>, which signals the end of the sentence. The trained model can translate common English phrases accurately as if you were using Google Translate for the task. <a id="idIndexMarker004"/><a id="idIndexMarker005"/><a id="marker-218"/></p>
<h2 class="fm-head" id="heading_id_3">10.1 Subword tokenization</h2>
<p class="body">As we discussed in chapter 8, there are three tokenization methods: character-level tokenization, word-level tokenization, and subword tokenization. In this chapter, we’ll use subword tokenization, which strikes a balance between the other two methods. It keeps frequently used words whole in the vocabulary and splits less common or more complex words into subcomponents. <a id="idIndexMarker006"/><a id="idIndexMarker007"/><a id="idIndexMarker008"/></p>
<p class="body">In this section, you’ll learn to tokenize both English and French phrases into subwords. You’ll then create dictionaries to map tokens to indexes. The training data are then converted to sequences of indexes and placed in batches for training purposes.</p>
<h3 class="fm-head1" id="heading_id_4">10.1.1 Tokenizing English and French phrases</h3>
<p class="body">Go to <a class="url" href="https://mng.bz/WVAw">https://mng.bz/WVAw</a> to download the zip file that contains the English-to-French translations I collected from various sources. Unzip the file and place en2fr.csv in the folder /files/ on your computer. <a id="idIndexMarker009"/><a id="idIndexMarker010"/></p>
<p class="body">We’ll load the data and print out an English phrase, along with its French translation, as follows:</p>
<pre class="programlisting">import pandas as pd
  
df=pd.read_csv("files/en2fr.csv")                                <span class="fm-combinumeral">①</span>
num_examples=len(df)                                             <span class="fm-combinumeral">②</span>
print(f"there are {num_examples} examples in the training data")
print(df.iloc[30856]["en"])                                      <span class="fm-combinumeral">③</span>
print(df.iloc[30856]["fr"])                                      <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Loads the CSV file</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Counts how many pairs of phrases are in the data</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Prints out an example of an English phrase</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Prints out the corresponding French translation</p>
<p class="body">The output from the preceding code snippet is</p>
<pre class="programlisting">there are 47173 examples in the training data
How are you?
Comment êtes-vous?</pre>
<p class="body">There are 47,173 pairs of English-to-French translations in the training data. We have printed out the English phrase “How are you?” and the corresponding French translation “Comment êtes-vous?” as an example.</p>
<p class="body">Run the following line of code in a new cell in this Jupyter Notebook to install the <code class="fm-code-in-text">transformers</code> library on your computer:<a id="idIndexMarker011"/><a id="marker-219"/></p>
<pre class="programlisting">!pip install transformers</pre>
<p class="body">Next, we’ll tokenize both the English and the French phrases in the dataset. We’ll use the pretrained XLM model from Hugging Face as the tokenizer because it excels at handling multiple languages, including English and French phrases.</p>
<p class="fm-code-listing-caption">Listing 10.1 A pretrained tokenizer</p>
<pre class="programlisting">from transformers import XLMTokenizer                           <span class="fm-combinumeral">①</span>
  
tokenizer = XLMTokenizer.from_pretrained("xlm-clm-enfr-1024")
  
tokenized_en=tokenizer.tokenize("I don't speak French.")        <span class="fm-combinumeral">②</span>
print(tokenized_en)
tokenized_fr=tokenizer.tokenize("Je ne parle pas français.")    <span class="fm-combinumeral">③</span>
print(tokenized_fr)
print(tokenizer.tokenize("How are you?"))
print(tokenizer.tokenize("Comment êtes-vous?"))</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Imports the pretrained tokenizer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Uses the tokenizer to tokenize an English sentence</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Tokenizes a French sentence</p>
<p class="body">The output from code listing 10.1 is</p>
<pre class="programlisting">['i&lt;/w&gt;', 'don&lt;/w&gt;', "'t&lt;/w&gt;", 'speak&lt;/w&gt;', 'fr', 'ench&lt;/w&gt;', '.&lt;/w&gt;']
['je&lt;/w&gt;', 'ne&lt;/w&gt;', 'parle&lt;/w&gt;', 'pas&lt;/w&gt;', 'franc', 'ais&lt;/w&gt;', '.&lt;/w&gt;']
['how&lt;/w&gt;', 'are&lt;/w&gt;', 'you&lt;/w&gt;', '?&lt;/w&gt;']
['comment&lt;/w&gt;', 'et', 'es-vous&lt;/w&gt;', '?&lt;/w&gt;']</pre>
<p class="body">In the preceding code block, we use a pretrained tokenizer from the XLM model to divide the English sentence “I don’t speak French.” into a group of tokens. In chapter 8, you developed a custom word-level tokenizer. However, this chapter introduces the use of a more efficient pretrained subword tokenizer, surpassing the word-level tokenizer in effectiveness. The sentence “I don’t speak French.” is thus tokenized into <code class="fm-code-in-text">['i', 'don', "'t", 'speak', 'fr', 'ench', '.']</code>. Similarly, the French sentence “Je ne parle pas français.” is split into six tokens: <code class="fm-code-in-text">['je', 'ne', 'parle', 'pas', 'franc', 'ais', '.']</code>. We have also tokenized the English phrase “How are you?” and its French translation. The results are shown in the last two lines of the preceding output. <a id="idIndexMarker012"/></p>
<p class="fm-callout"><span class="fm-callout-head">NOTE</span> You may have noticed that the XLM model uses '<code class="fm-code-in-text2">&lt;/w&gt;</code>' as a token separator, except in cases where two tokens are part of the same word. Subword tokenization typically results in each token being either a complete word or a punctuation mark, but there are occasions when a word is divided into syllables. For example, the word “French” is divided into “fr” and “ench.” It’s noteworthy that the model doesn’t insert <code class="fm-code-in-text2">&lt;/w&gt;</code> between “fr” and “ench,” as these syllables jointly constitute the word “French.”<a id="idIndexMarker013"/><a id="marker-220"/></p>
<p class="body">Deep-learning models such as Transformers cannot process raw text directly; hence we need to convert text into numerical representations before feeding them to the models. For that purpose, we create a dictionary to map all English tokens to integers.</p>
<p class="fm-code-listing-caption">Listing 10.2 Mapping English tokens to indexes</p>
<pre class="programlisting">from collections import Counter
  
en=df["en"].tolist()                                           <span class="fm-combinumeral">①</span>
  
en_tokens=[["BOS"]+tokenizer.tokenize(x)+["EOS"] for x in en]  <span class="fm-combinumeral">②</span>
PAD=0
UNK=1
word_count=Counter()
for sentence in en_tokens:
    for word in sentence:
        word_count[word]+=1
frequency=word_count.most_common(50000)                        <span class="fm-combinumeral">③</span>
total_en_words=len(frequency)+2
en_word_dict={w[0]:idx+2 for idx,w in enumerate(frequency)}    <span class="fm-combinumeral">④</span>
en_word_dict["PAD"]=PAD
en_word_dict["UNK"]=UNK
en_idx_dict={v:k for k,v in en_word_dict.items()}              <span class="fm-combinumeral">⑤</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Obtains all English sentences from the training dataset</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Tokenizes all English sentences</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Counts the frequency of tokens</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Creates a dictionary to map tokens to indexes</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Creates a dictionary to map indexes to tokens</p>
<p class="body">We insert the tokens <code class="fm-code-in-text">"BOS"</code> (beginning of the sentence) and <code class="fm-code-in-text">"EOS"</code> (end of the sentence) at the start and end of each phrase, respectively. The dictionary <code class="fm-code-in-text">en_word_dict</code> assigns each token a unique integer value. Further, the <code class="fm-code-in-text">"PAD"</code> token, used for padding, is allocated the integer 0, while the <code class="fm-code-in-text">"UNK"</code> token, representing unknown tokens, is given the integer 1. A reverse dictionary, <code class="fm-code-in-text">en_idx_dict</code>, maps integers (indexes) back to their corresponding tokens. This reverse mapping is essential for converting a sequence of integers back into a sequence of tokens, enabling us to reconstruct the original English phrase.<a id="idIndexMarker014"/><a id="idIndexMarker015"/><a id="idIndexMarker016"/><a id="idIndexMarker017"/></p>
<p class="body">Using the dictionary <code class="fm-code-in-text">en_word_dict</code>, we can transform the English sentence “I don’t speak French.” into its numerical representation. This process involves looking up each token in the dictionary to find its corresponding integer value. For instance:<a id="idIndexMarker018"/><a id="marker-221"/></p>
<pre class="programlisting">enidx=[en_word_dict.get(i,UNK) for i in tokenized_en]   
print(enidx)</pre>
<p class="body">The preceding lines of code produce the following output:</p>
<pre class="programlisting">[15, 100, 38, 377, 476, 574, 5]</pre>
<p class="body">This means that the English sentence “I don’t speak French.” is now represented by a sequence of integers <span class="times">[15, 100, 38, 377, 476, 574, 5]</span>.</p>
<p class="body">We can also revert the numerical representations into tokens using the dictionary <code class="fm-code-in-text">en_idx_dict</code>. This process involves mapping each integer in the numerical sequence back to its corresponding token as defined in the dictionary. Here’s how it is done:<a id="idIndexMarker019"/></p>
<pre class="programlisting">entokens=[en_idx_dict.get(i,"UNK") for i in enidx]          <span class="fm-combinumeral">①</span>
print(entokens)
en_phrase="".join(entokens)                                 <span class="fm-combinumeral">②</span>
en_phrase=en_phrase.replace("&lt;/w&gt;"," ")                     <span class="fm-combinumeral">③</span>
for x in '''?:;.,'("-!&amp;)%''':
    en_phrase=en_phrase.replace(f" {x}",f"{x}")             <span class="fm-combinumeral">④</span>
print(en_phrase)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Converts indexes to tokens</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Joins tokens into a string</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Replaces the separator with a space</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Removes the space before punctuations</p>
<p class="body">The output of the preceding code snippet is</p>
<pre class="programlisting">['i&lt;/w&gt;', 'don&lt;/w&gt;', "'t&lt;/w&gt;", 'speak&lt;/w&gt;', 'fr', 'ench&lt;/w&gt;', '.&lt;/w&gt;']
i don't speak french. </pre>
<p class="body">The dictionary <code class="fm-code-in-text">en_idx_dict</code> is used to translate numbers back into their original tokens. Following this, these tokens are transformed into the complete English phrase. This is done by first joining the tokens into a single string and then substituting the separator <code class="fm-code-in-text">''&lt;/w&gt;''</code> with a space. We also remove the space before punctuation marks. Notice that the restored English phrase has all lowercase letters because the pretrained tokenizer automatically converts uppercase letters into lowercase to reduce the number of unique tokens. As you’ll see in the next chapter, some models, such as GPT2 and ChatGPT, don’t do this; hence, they have a larger vocabulary.<a id="idIndexMarker020"/></p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 10.1</p>
<p class="fm-sidebar-text">In listing 10.1, we have split the sentence “How are you?” into tokens <code class="fm-code-in-text1">['how&lt;/w&gt;', 'are&lt;/w&gt;', 'you&lt;/w&gt;', '?&lt;/w&gt;']</code>. Follow the steps in this subsection to (i) convert the tokens into indexes using the dictionary <code class="fm-code-in-text1">en_word_dict</code>; (ii) convert the indexes back to tokens using the dictionary <code class="fm-code-in-text1">en_idx_dict</code>; (iii) restore the English sentence by joining the tokens into a string, changing the separator <code class="fm-code-in-text1">'&lt;/w&gt;'</code> to a space, and removing the space before punctuation marks.<a id="marker-222"/><a id="idIndexMarker021"/><a id="idIndexMarker022"/></p>
</div>
<p class="body">We can apply the same steps to French phrases to map tokens to indexes and vice versa.</p>
<p class="fm-code-listing-caption">Listing 10.3 Mapping French tokens to indexes</p>
<pre class="programlisting">fr=df["fr"].tolist()       
fr_tokens=[["BOS"]+tokenizer.tokenize(x)+["EOS"] for x in fr]  <span class="fm-combinumeral">①</span>
word_count=Counter()
for sentence in fr_tokens:
    for word in sentence:
        word_count[word]+=1
frequency=word_count.most_common(50000)                        <span class="fm-combinumeral">②</span>
total_fr_words=len(frequency)+2
fr_word_dict={w[0]:idx+2 for idx,w in enumerate(frequency)}    <span class="fm-combinumeral">③</span>
fr_word_dict["PAD"]=PAD
fr_word_dict["UNK"]=UNK
fr_idx_dict={v:k for k,v in fr_word_dict.items()}              <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Tokenizes all French sentences</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Counts the frequency of French tokens</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates a dictionary to map French tokens to indexes</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Creates a dictionary to map indexes to French tokens</p>
<p class="body">The dictionary <code class="fm-code-in-text">fr_word_dict</code> assigns an integer to each French token, while <code class="fm-code-in-text">fr_idx_dict</code> maps these integers back to their corresponding French tokens. Next, I’ll demonstrate how to transform the French phrase “Je ne parle pas français.” into its numerical representation:<a id="idIndexMarker023"/><a id="idIndexMarker024"/></p>
<pre class="programlisting">fridx=[fr_word_dict.get(i,UNK) for i in tokenized_fr]   
print(fridx)</pre>
<p class="body">The output from the preceding code snippet is</p>
<pre class="programlisting">[28, 40, 231, 32, 726, 370, 4]</pre>
<p class="body">The tokens for the French phrase “Je ne parle pas français.” are converted into a sequence of integers, as shown.</p>
<p class="body">We can transform the numerical representations back into French tokens using the dictionary <code class="fm-code-in-text">fr_idx_dict</code>. This involves translating each number in the sequence back to its respective French token in the dictionary. Once the tokens are retrieved, they can be joined to reconstruct the original French phrase. Here’s how it’s done:<a id="idIndexMarker025"/></p>
<pre class="programlisting">frtokens=[fr_idx_dict.get(i,"UNK") for i in fridx] 
print(frtokens)
fr_phrase="".join(frtokens)
fr_phrase=fr_phrase.replace("&lt;/w&gt;"," ")
for x in '''?:;.,'("-!&amp;)%''':
    fr_phrase=fr_phrase.replace(f" {x}",f"{x}")  
print(fr_phrase)</pre>
<p class="body">The output from the preceding code block is</p>
<pre class="programlisting">['je&lt;/w&gt;', 'ne&lt;/w&gt;', 'parle&lt;/w&gt;', 'pas&lt;/w&gt;', 'franc', 'ais&lt;/w&gt;', '.&lt;/w&gt;']
je ne parle pas francais. </pre>
<p class="body">It’s important to recognize that the restored French phrase doesn’t exactly match its original form. This discrepancy is due to the tokenization process, which transforms all uppercase letters into lowercase and eliminates accent marks in French.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 10.2</p>
<p class="fm-sidebar-text">In listing 10.1, we have split the sentence “Comment êtes-vous?” into tokens <code class="fm-code-in-text1">['comment&lt;/w&gt;', 'et', 'es-vous&lt;/w&gt;', '?&lt;/w&gt;']</code>. Follow the steps in this subsection to (i) convert the tokens into indexes using the dictionary <code class="fm-code-in-text1">fr_word_dict</code>; (ii) convert the indexes back to tokens using the dictionary <code class="fm-code-in-text1">fr_idx_dict</code>; (iii) restore the French phrase by joining the tokens into a string, changing the separator <code class="fm-code-in-text1">'&lt;/w&gt;'</code> to a space, and removing the space before punctuation marks.</p>
</div>
<p class="body">Save the four dictionaries in the folder /files/ on your computer so that you can load them up and start translating later without worrying about first mapping tokens to indexes and vice versa:</p>
<pre class="programlisting">import pickle
  
with open("files/dict.p","wb") as fb:
    pickle.dump((en_word_dict,en_idx_dict,
                 fr_word_dict,fr_idx_dict),fb)</pre>
<p class="body"><a id="marker-223"/>The four dictionaries are now saved in a single pickle file <code class="fm-code-in-text">dict.p</code>. Alternatively, you can download the file from the book’s GitHub repository.<a id="idIndexMarker026"/></p>
<h3 class="fm-head1" id="heading_id_5">10.1.2 Sequence padding and batch creation</h3>
<p class="body">We’ll divide the training data into batches during training for computational efficiency and accelerated convergence, as we have done in previous chapters. <a id="idIndexMarker027"/><a id="idIndexMarker028"/><a id="idIndexMarker029"/></p>
<p class="body">Creating batches for other data formats such as images is straightforward: simply group a specific number of inputs to form a batch since they all have the same size. However, in natural language processing, batching can be more complex due to the varying lengths of sentences. To standardize the length within a batch, we pad the shorter sequences. This uniformity is crucial since the numerical representations fed into the Transformer need to have the same length. For instance, English phrases in a batch may vary in length (this can also happen to French phrases in a batch). To address this, we append zeros to the end of the numerical representations of shorter phrases in a batch, ensuring that all inputs to the Transformer model are of equal length.</p>
<p class="fm-callout"><span class="fm-callout-head">Note</span> Incorporating <code class="fm-code-in-text2">BOS</code> and <code class="fm-code-in-text2">EOS</code> tokens at the beginning and end of each sentence, as well as padding shorter sequences within a batch, is a distinctive feature in machine language translation. This distinction arises from the fact that the input consists of entire sentences or phrases. In contrast, as you will see in the next two chapters, training a text generation model does not entail these processes; the model’s input contains a predetermined number of tokens.</p>
<p class="body">We start by converting all English phrases into their numerical representations and then apply the same process to the French phrases:</p>
<pre class="programlisting">out_en_ids=[[en_word_dict.get(w,UNK) for w in s] for s in en_tokens]
out_fr_ids=[[fr_word_dict.get(w,UNK) for w in s] for s in fr_tokens]
sorted_ids=sorted(range(len(out_en_ids)),
                  key=lambda x:len(out_en_ids[x]))
out_en_ids=[out_en_ids[x] for x in sorted_ids]
out_fr_ids=[out_fr_ids[x] for x in sorted_ids]</pre>
<p class="body">Next, we put the numerical representations into batches for training:</p>
<pre class="programlisting">import numpy as np
  
batch_size=128
idx_list=np.arange(0,len(en_tokens),batch_size)
np.random.shuffle(idx_list)
  
batch_indexs=[]
for idx in idx_list:
    batch_indexs.append(np.arange(idx,min(len(en_tokens),
                                          idx+batch_size)))</pre>
<p class="body"><a id="marker-224"/>Note that we have sorted observations in the training dataset by the length of the English phrases before placing them into batches. This method ensures that the observations within each batch are of a comparable length, consequently decreasing the need for padding. As a result, this approach not only reduces the overall size of the training data but also accelerates the training process.</p>
<p class="body">To pad sequences in a batch to the same length, we define the following function:</p>
<pre class="programlisting">def seq_padding(X, padding=0):
    L = [len(x) for x in X]
    ML = max(L)                                                   <span class="fm-combinumeral">①</span>
    padded_seq = np.array([np.concatenate([x, [padding] * (ML - len(x))])
        if len(x) &lt; ML else x for x in X])                        <span class="fm-combinumeral">②</span>
    return padded_seq</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Find out the length of the longest sequence in the batch.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> If a batch is shorter than the longest sequence, add 0s to the sequence at the end.</p>
<p class="body">The function <code class="fm-code-in-text">seq_padding()</code> first identifies the longest sequence within the batch. Then it appends zeros to the end of shorter sequences to ensure that every sequence in the batch matches this maximum length.<a id="idIndexMarker030"/></p>
<p class="body">To conserve space, we have created a <code class="fm-code-in-text">Batch()</code> class within the local module ch09util.py that you downloaded in the last chapter (see figure 10.1).<a id="idIndexMarker031"/></p>
<p class="fm-code-listing-caption">Listing 10.4 Creating a <code class="fm-code-in-text">B</code>atc<code class="fm-code-in-text">h()</code> class in the local module</p>
<pre class="programlisting">import torch
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
class Batch:
    def __init__(self, src, trg=None, pad=0):
        src = torch.from_numpy(src).to(DEVICE).long()
        self.src = src
        self.src_mask = (src != pad).unsqueeze(-2)              <span class="fm-combinumeral">①</span>
        if trg is not None:
            trg = torch.from_numpy(trg).to(DEVICE).long()
            self.trg = trg[:, :-1]                              <span class="fm-combinumeral">②</span>
            self.trg_y = trg[:, 1:]                             <span class="fm-combinumeral">③</span>
            self.trg_mask = make_std_mask(self.trg, pad)        <span class="fm-combinumeral">④</span>
            self.ntokens = (self.trg_y != pad).data.sum()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a source mask to hide padding at the end of the sentence</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates input to the decoder</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Shifts the input one token to the right and uses it as output</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Creates a target mask</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="220" src="../../OEBPS/Images/CH10_F01_Liu.png" width="752"/></p>
<p class="figurecaption">Figure 10.1 What does the <code class="fm-code-in-text">Batch()</code> class do? The <code class="fm-code-in-text">Batch()</code> class takes two inputs: <code class="fm-code-in-text">src</code> and <code class="fm-code-in-text">trg</code>, sequences of indexes for the source language and the target language, respectively. It adds several attributes to the training data: <code class="fm-code-in-text">src_mask</code>, the source mask to conceal padding; <code class="fm-code-in-text">modified trg</code>, the input to the decoder; <code class="fm-code-in-text">trg_y</code>, the output to the decoder; <code class="fm-code-in-text">trg_mask</code>, the target mask to hide padding and future tokens. <a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="idIndexMarker034"/><a id="idIndexMarker035"/><a id="idIndexMarker036"/><a id="idIndexMarker037"/><a id="idIndexMarker038"/><a id="marker-225"/></p>
</div>
<p class="body">The <code class="fm-code-in-text">Batch()</code> class processes a batch of English and French phrases, converting them into a format suitable for training. To make this explanation more tangible, consider the English phrase “How are you?” and its French equivalent “Comment êtes-vous?” as our example. The <code class="fm-code-in-text">Batch()</code> class receives two inputs: <code class="fm-code-in-text">src</code>, which is the sequence of indexes representing the tokens in “How are you?”, and <code class="fm-code-in-text">trg</code>, the sequence of indexes for the tokens in “Comment êtes-vous?”. This class generates a tensor, <code class="fm-code-in-text">src_mask</code>, to conceal the padding at the sentence’s end. For instance, the sentence “How are you?” is broken down into six tokens: <code class="fm-code-in-text">['BOS', 'how', 'are', 'you', '?', 'EOS']</code>. If this sequence is part of a batch with a maximum length of eight tokens, two zeros are added to the end. The <code class="fm-code-in-text">src_mask</code> tensor instructs the model to disregard the final two tokens in such scenarios.<a id="idIndexMarker039"/><a id="idIndexMarker040"/><a id="idIndexMarker041"/><a id="idIndexMarker042"/></p>
<p class="body">The <code class="fm-code-in-text">Batch()</code> class additionally prepares the input and output for the Transformer’s decoder. Consider the French phrase “Comment êtes-vous?”, which is transformed into six tokens: <code class="fm-code-in-text">['BOS', 'comment', 'et', 'es-vous', '?', 'EOS']</code>. The indexes of these first five tokens serve as the input to the decoder, named <code class="fm-code-in-text">trg</code>. Next, we shift this input one token to the right to form the decoder’s output, <code class="fm-code-in-text">trg_y</code>. Hence, the input comprises indexes for <code class="fm-code-in-text">['BOS', 'comment', 'et', 'es-vous', '?']</code>, while the output consists of indexes for <code class="fm-code-in-text">['comment', 'et', 'es-vous', '?', 'EOS']</code>. This approach mirrors what we discussed in chapter 8 and is designed to force the model to predict the next token based on the previous ones.<a id="idIndexMarker043"/><a id="idIndexMarker044"/><a id="marker-226"/><a id="idIndexMarker045"/></p>
<p class="body">The <code class="fm-code-in-text">Batch()</code> class also generates a mask, <code class="fm-code-in-text">trg_mask</code>, for the decoder’s input. The aim of this mask is to conceal the subsequent tokens in the input, ensuring that the model relies solely on previous tokens for making predictions. This mask is produced by the <code class="fm-code-in-text">make_std_mask()</code> function, which is defined within the local module ch09util:<a id="idIndexMarker046"/><a id="idIndexMarker047"/><a id="idIndexMarker048"/></p>
<pre class="programlisting">import numpy as np
def subsequent_mask(size):
    attn_shape = (1, size, size)
    subsequent_mask = np.triu(np.ones(attn_shape),k=1).astype('uint8')
    output = torch.from_numpy(subsequent_mask) == 0
    return output
def make_std_mask(tgt, pad):
    tgt_mask=(tgt != pad).unsqueeze(-2)
    output=tgt_mask &amp; subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)
    return output </pre>
<p class="body">The <code class="fm-code-in-text">subsequent_mask()</code> function generates a mask specifically for a sequence, instructing the model to focus solely on the actual sequence and disregard the padded zeros at the end, which are used only to standardize sequence lengths. The <code class="fm-code-in-text">make_std_mask()</code> function, on the other hand, constructs a standard mask for the target sequence. This standard mask has the dual role of concealing both the padded zeros and the future tokens in the target sequence. <a id="idIndexMarker049"/><a id="idIndexMarker050"/></p>
<p class="body">Next, we import the <code class="fm-code-in-text">Batch()</code> class from the local module and use it to create batches of training data:<a id="idIndexMarker051"/></p>
<pre class="programlisting">from utils.ch09util import Batch
  
class BatchLoader():
    def __init__(self):
        self.idx=0
    def __iter__(self):
        return self
    def __next__(self):
        self.idx += 1
        if self.idx&lt;=len(batch_indexs):
            b=batch_indexs[self.idx-1]
            batch_en=[out_en_ids[x] for x in b]
            batch_fr=[out_fr_ids[x] for x in b]
            batch_en=seq_padding(batch_en)
            batch_fr=seq_padding(batch_fr)
            return Batch(batch_en,batch_fr)
        raise StopIteration</pre>
<p class="body">The <code class="fm-code-in-text">BatchLoader()</code> class creates data batches intended for training. Each batch in this list contains 128 pairs, where each pair contains numerical representations of an English phrase and its corresponding French translation.<a id="idIndexMarker052"/></p>
<h2 class="fm-head" id="heading_id_6">10.2 Word embedding and positional encoding</h2>
<p class="body">After tokenization in the last section, English and French phrases are represented by sequences of indexes. In this section, you’ll use word embedding to transform these indexes (essentially one-hot vectors) into compact vector representations. Doing so captures the semantic information and interrelationship of tokens in a phrase. Word embedding also improves training efficiency: instead of bulky one-hot vectors, word embedding uses continuous, lower-dimensional vectors to reduce the model’s complexity and dimensionality.<a id="idIndexMarker053"/><a id="idIndexMarker054"/><a id="idIndexMarker055"/><a id="idIndexMarker056"/><a id="idIndexMarker057"/><a id="idIndexMarker058"/><a id="idIndexMarker059"/><a id="idIndexMarker060"/><a id="marker-227"/><a id="idIndexMarker061"/></p>
<p class="body">The attention mechanism processes all tokens in a phrase at the same time instead of sequentially. This enhances its efficiency but doesn’t inherently allow it to recognize the sequence order of the tokens. Therefore, we’ll add positional encodings to the input embeddings by using sine and cosine functions of varying frequencies.</p>
<h3 class="fm-head1" id="heading_id_7">10.2.1 Word embedding</h3>
<p class="body">The numerical representations of the English and French phrases involve a large number of indexes. To determine the exact number of distinct indexes required for each language, we can count the number of unique elements in the <code class="fm-code-in-text">en_word_dict</code> and <code class="fm-code-in-text">fr_word_dict</code> dictionaries. Doing so generates the total number of unique tokens in each language’s vocabulary (we’ll use them as inputs to the Transformer later):<a id="idIndexMarker062"/><a id="idIndexMarker063"/></p>
<pre class="programlisting">src_vocab = len(en_word_dict)
tgt_vocab = len(fr_word_dict)
print(f"there are {src_vocab} distinct English tokens")
print(f"there are {tgt_vocab} distinct French tokens")</pre>
<p class="body">The output is</p>
<pre class="programlisting">there are 11055 distinct English tokens
there are 11239 distinct French tokens</pre>
<p class="body">In our dataset, there are 11,055 unique English tokens and 11,239 unique French tokens. Utilizing one-hot encoding for these would result in an excessively high number of parameters to train. To address this, we will employ word embeddings, which compress the numerical representations into continuous vectors, each with a length of <code class="fm-code-in-text">d_model = 256</code>.</p>
<p class="body">This is achieved through the use of the <code class="fm-code-in-text">Embeddings()</code> class, which is defined in the local module ch09util:<a id="idIndexMarker064"/></p>
<pre class="programlisting">import math
  
class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        super().__init__()
        self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model
  
    def forward(self, x):
        out = self.lut(x) * math.sqrt(self.d_model)
        return out</pre>
<p class="body">The <code class="fm-code-in-text">Embeddings()</code> class defined previously utilizes PyTorch’s <code class="fm-code-in-text">Embedding()</code> class. It also multiplies the output by the square root of <code class="fm-code-in-text">d_model</code>, which is 256. This multiplication is intended to counterbalance the division by the square root of <code class="fm-code-in-text">d_model</code> that occurs later during the computation of attention scores. The <code class="fm-code-in-text">Embeddings()</code> class decreases the dimensionality of the numerical representations of English and French phrases. We discussed in detail how PyTorch’s <code class="fm-code-in-text">Embedding()</code> class works in chapter 8. <a id="idIndexMarker065"/><a id="idIndexMarker066"/><a id="idIndexMarker067"/></p>
<h3 class="fm-head1" id="heading_id_8">10.2.2 Positional encoding</h3>
<p class="body"><a id="marker-228"/>To accurately represent the sequence order of elements in both input and output, we introduce the <code class="fm-code-in-text">PositionalEncoding()</code> class in the local module.<a id="idIndexMarker068"/></p>
<p class="fm-code-listing-caption">Listing 10.5 A class to calculate positional encoding</p>
<pre class="programlisting">class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout, max_len=5000):       <span class="fm-combinumeral">①</span>
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)
        pe = torch.zeros(max_len, d_model, device=DEVICE)
        position = torch.arange(0., max_len, 
                                device=DEVICE).unsqueeze(1)
        div_term = torch.exp(torch.arange(
            0., d_model, 2, device=DEVICE)
            * -(math.log(10000.0) / d_model))
        pe_pos = torch.mul(position, div_term)
        pe[:, 0::2] = torch.sin(pe_pos)                        <span class="fm-combinumeral">②</span>
        pe[:, 1::2] = torch.cos(pe_pos)                        <span class="fm-combinumeral">③</span>
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)  
   
    def forward(self, x):
        x=x+self.pe[:,:x.size(1)].requires_grad_(False)        <span class="fm-combinumeral">④</span>
        out=self.dropout(x)
        return out</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Initiates the class, allowing a maximum of 5,000 positions</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Applies sine function to even indexes in the vector</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Applies cosine function to odd indexes in the vector</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Adds positional encoding to word embedding</p>
<p class="body">The <code class="fm-code-in-text">PositionalEncoding()</code> class generates vectors for sequence positions using sine functions for even indexes and cosine functions for odd indexes. It’s important to note that in the <code class="fm-code-in-text">PositionalEncoding()</code> class, the <code class="fm-code-in-text">requires_grad_(False)</code> argument is included because there is no need to train these values. They remain constant across all inputs, and they don’t change during the training process. <a id="idIndexMarker069"/><a id="idIndexMarker070"/><a id="idIndexMarker071"/></p>
<p class="body">For example, the indexes for the six tokens <code class="fm-code-in-text">['BOS', 'how', 'are', 'you', '?', 'EOS']</code> from the English phrase are first processed through a word embedding layer. This step transforms these indexes into a tensor with the dimensions of (1, 6, 256): 1 means there is only 1 sequence in the batch; 6 means there are 6 tokens in the sequence; 256 means each token is represented by a 256-value vector. After this word embedding process, the <code class="fm-code-in-text">PositionalEncoding()</code> class is employed to calculate the positional encodings for the indexes corresponding to the tokens <code class="fm-code-in-text">['BOS', 'how', 'are', 'you', '?', 'EOS']</code>. This is done to provide the model with information about the position of each token in the sequence. Better yet, we can tell you the exact values of the positional encodings for the previous six tokens by using the following code block:<a id="idIndexMarker072"/><a id="marker-229"/></p>
<pre class="programlisting">from utils.ch09util import PositionalEncoding
import torch
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
  
pe = PositionalEncoding(256, 0.1)                              <span class="fm-combinumeral">①</span>
x = torch.zeros(1, 8, 256).to(DEVICE)                          <span class="fm-combinumeral">②</span>
y = pe.forward(x)                                              <span class="fm-combinumeral">③</span>
print(f"the shape of positional encoding is {y.shape}")
print(y)                                                       <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Instantiates the PositionalEncoding() class and set the model dimension to 256</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates a word embedding and fills it with zeros</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Calculates the input embedding by adding positional encoding to the word embedding</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Prints out the input embedding, which is the same as positional encoding since word embedding is set to zero</p>
<p class="body">We first create an instance, <code class="fm-code-in-text">pe</code>, of the <code class="fm-code-in-text">PositionalEncoding()</code> class by setting the model dimension to 256 and the dropout rate to 0.1. Since the output from this class is the sum of word embedding and positional encoding, we create a word embedding filled with zeros and feed it to <code class="fm-code-in-text">pe</code>: this way, the output is the same as the positional encoding.<a id="idIndexMarker073"/></p>
<p class="body">After running the preceding code block, you’ll see the following output:</p>
<pre class="programlisting">the shape of positional encoding is torch.Size([1, 8, 256])
tensor([[[ 0.0000e+00,  1.1111e+00,  0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00,  1.1111e+00],
         [ 9.3497e-01,  6.0034e-01,  8.9107e-01,  ...,  1.1111e+00,
           1.1940e-04,  1.1111e+00],
         [ 0.0000e+00, -4.6239e-01,  1.0646e+00,  ...,  1.1111e+00,
           2.3880e-04,  1.1111e+00],
         ...,
         [-1.0655e+00,  3.1518e-01, -1.1091e+00,  ...,  1.1111e+00,
           5.9700e-04,  1.1111e+00],
         [-3.1046e-01,  1.0669e+00, -0.0000e+00,  ...,  0.0000e+00,
           7.1640e-04,  1.1111e+00],
         [ 7.2999e-01,  8.3767e-01,  2.5419e-01,  ...,  1.1111e+00,
           8.3581e-04,  1.1111e+00]]], device='cuda:0')</pre>
<p class="body">The preceding tensor represents the positional encoding for the English phrase “How are you?” It’s important to note that this positional encoding also has the dimensions of (1, 6, 256), which matches the size of the word embedding for “How are you?”. The next step involves combining the word embedding and positional encoding into a single tensor.</p>
<p class="body">An essential characteristic of positional encodings is that their values are the same no matter what the input sequences are. This means that regardless of the specific input sequence, the positional encoding for the first token will always be the same 256-value vector, <code class="fm-code-in-text">[0.0000e+00, 1.1111e+00, ..., 1.1111e+00]</code>, as shown in the above output. Similarly, the positional encoding for the second token will always be <code class="fm-code-in-text">[9.3497e-01, 6.0034e-01, ..., 1.1111e+00]</code>, and so on. Their values don’t change during the training process either. <a id="idIndexMarker074"/><a id="idIndexMarker075"/><a id="idIndexMarker076"/><a id="idIndexMarker077"/><a id="idIndexMarker078"/><a id="idIndexMarker079"/><a id="idIndexMarker080"/><a id="idIndexMarker081"/><a id="idIndexMarker082"/></p>
<h2 class="fm-head" id="heading_id_9">10.3 Training the Transformer for English-to-French translation</h2>
<p class="body"><a id="marker-230"/>Our constructed English-to-French translation model can be viewed as a multicategory classifier. The core objective is to predict the next token in the French vocabulary when translating an English sentence. This is somewhat similar to the image classification project we discussed in chapter 2, though this model is significantly more complex. This complexity necessitates careful selection of the loss function, optimizer, and training loop parameters.<a id="idIndexMarker083"/><a id="idIndexMarker084"/></p>
<p class="body">In this section, we will detail the process of selecting an appropriate loss function and optimizer. We will train the Transformer using batches of English-to-French translations as our training dataset. After the model is trained, you’ll learn how to translate common English phrases into French.</p>
<h3 class="fm-head1" id="heading_id_10">10.3.1 Loss function and the optimizer</h3>
<p class="body">First, we import the <code class="fm-code-in-text">create_model()</code> function from the local module ch09util.py and construct a Transformer so that we can train it to translate English to French:<a id="idIndexMarker085"/><a id="idIndexMarker086"/><a id="idIndexMarker087"/><a id="idIndexMarker088"/><a id="idIndexMarker089"/></p>
<pre class="programlisting">from utils.ch09util import create_model
  
model = create_model(src_vocab, tgt_vocab, N=6,
    d_model=256, d_ff=1024, h=8, dropout=0.1)</pre>
<p class="body">The paper “Attention Is All You Need” uses various combinations of hyperparameters when constructing the model. Here we choose a model dimension of 256 with 8 heads because we find this combination does a good job translating English to French in our setting. Interested readers could potentially use a validation set to tune hyperparameters to select the best model in their own projects.</p>
<p class="body">We’ll follow the original paper “Attention Is All You Need” and use label smoothing during training. Label smoothing is commonly used in training deep neural networks to improve the generalization of the model. It is used to address overconfidence problems (the predicted probability is greater than the true probability) and overfitting in classifications. Specifically, it modifies the way the model learns by adjusting the target labels, aiming to reduce the model’s confidence in the training data, which can lead to better performance on unseen data.</p>
<p class="body">In a typical classification task, target labels are represented in a one-hot encoding format. This representation implies absolute certainty about the correctness of the label for each training sample. Training with absolute certainty can lead to two main problems. The first is overfitting: the model becomes overly confident in its predictions, fitting too closely to the training data, which can harm its performance on new, unseen data. The second problem is poor calibration: models trained this way often output overconfident probabilities. For instance, they might output a probability of 99% for a correct class when, realistically, the confidence should be lower.</p>
<p class="body">Label smoothing adjusts the target labels to be less confident. Instead of having a target label of <code class="fm-code-in-text">[1, 0, 0]</code> for a three-class problem, you might have something like <code class="fm-code-in-text">[0.9, 0.05, 0.05]</code>. This approach encourages the model not to be too confident about its predictions by penalizing overconfident outputs. The smoothed labels are a mixture of the original label and some distribution over the other labels (usually the uniform distribution).</p>
<p class="body">We define the following <code class="fm-code-in-text">LabelSmoothing()</code> class in the local module ch09util.<a id="idIndexMarker090"/><a id="marker-231"/></p>
<p class="fm-code-listing-caption">Listing 10.6 A class to conduct label smoothing</p>
<pre class="programlisting">class LabelSmoothing(nn.Module):
    def __init__(self, size, padding_idx, smoothing=0.1):
        super().__init__()
        self.criterion = nn.KLDivLoss(reduction='sum')  
        self.padding_idx = padding_idx
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.size = size
        self.true_dist = None
    def forward(self, x, target):
        assert x.size(1) == self.size
        true_dist = x.data.clone()                              <span class="fm-combinumeral">①</span>
        true_dist.fill_(self.smoothing / (self.size - 2))
        true_dist.scatter_(1, 
               target.data.unsqueeze(1), self.confidence)       <span class="fm-combinumeral">②</span>
        true_dist[:, self.padding_idx] = 0
        mask = torch.nonzero(target.data == self.padding_idx)
        if mask.dim() &gt; 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
        self.true_dist = true_dist
        output = self.criterion(x, true_dist.clone().detach())  <span class="fm-combinumeral">③</span>
        return output</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Extracts predictions from the model</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Extracts actual labels from the training data and adds noise to them</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Uses the smoothed labels as targets when calculating loss</p>
<p class="body">The <code class="fm-code-in-text">LabelSmoothing()</code> class first extracts the predictions from the model. It then smoothes the actual labels in the training dataset by adding noise to it. The parameter <code class="fm-code-in-text">smoothing</code> controls how much noise we inject into the actual label. The label <code class="fm-code-in-text">[1, 0, 0]</code> is smoothed to <code class="fm-code-in-text">[0.9, 0.05, 0.05]</code> if you set <code class="fm-code-in-text">smoothing=0.1</code>, and it is smoothed to <code class="fm-code-in-text">[0.95, 0.025, 0.025]</code> if you set <code class="fm-code-in-text">smoothing=0.05</code>, for example. The class then calculates the loss by comparing the predictions with the smoothed labels.<a id="idIndexMarker091"/><a id="idIndexMarker092"/></p>
<p class="body">As in previous chapters, the optimizer we use is the Adam optimizer. However, instead of using a constant learning rate throughout training, we define the <code class="fm-code-in-text">NoamOpt()</code> class in the local module to change the learning rate during the training process:<a id="idIndexMarker093"/></p>
<pre class="programlisting">class NoamOpt:
    def __init__(self, model_size, factor, warmup, optimizer):
        self.optimizer = optimizer
        self._step = 0
        self.warmup = warmup                                  <span class="fm-combinumeral">①</span>
        self.factor = factor
        self.model_size = model_size
        self._rate = 0
    def step(self):                                           <span class="fm-combinumeral">②</span>
        self._step += 1
        rate = self.rate()
        for p in self.optimizer.param_groups:
            p['lr'] = rate
        self._rate = rate
        self.optimizer.step()
    def rate(self, step=None):
        if step is None:
            step = self._step
        output = self.factor * (self.model_size ** (-0.5) *
        min(step ** (-0.5), step * self.warmup ** (-1.5)))    <span class="fm-combinumeral">③</span>
        return output</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines warm-up steps</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> A step() method to apply the optimizer to adjust model parameters</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Calculates the learning rate based on steps</p>
<p class="body"><a id="marker-232"/>The <code class="fm-code-in-text">NoamOpt()</code> class, as defined previously, implements a warm-up learning rate strategy. First, it increases the learning rate linearly during the initial warmup steps of training. Following this warm-up period, the class then decreases the learning rate, adjusting it in proportion to the inverse square root of the training step number.<a id="idIndexMarker094"/></p>
<p class="body">Next, we create the optimizer for training:</p>
<pre class="programlisting">from utils.ch09util import NoamOpt
  
optimizer = NoamOpt(256, 1, 2000, torch.optim.Adam(
    model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))</pre>
<p class="body">To define the loss function for training, we first create the following <code class="fm-code-in-text">SimpleLossCompute()</code> class in the local module.<a id="idIndexMarker095"/></p>
<p class="fm-code-listing-caption">Listing 10.7 A class to compute loss</p>
<pre class="programlisting">class SimpleLossCompute:
    def __init__(self, generator, criterion, opt=None):
        self.generator = generator
        self.criterion = criterion
        self.opt = opt
    def __call__(self, x, y, norm):
        x = self.generator(x)                                    <span class="fm-combinumeral">①</span>
        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),
                              y.contiguous().view(-1)) / norm    <span class="fm-combinumeral">②</span>
        loss.backward()                                          <span class="fm-combinumeral">③</span>
        if self.opt is not None:
            self.opt.step()                                      <span class="fm-combinumeral">④</span>
            self.opt.optimizer.zero_grad()
        return loss.data.item() * norm.float()</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Uses the model to make predictions</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Compares the predictions with labels to calculate loss, utilizing label smoothing</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Calculates gradients with respect to model parameters</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Adjusts model parameters (backpropagate)</p>
<p class="body">The <code class="fm-code-in-text">SimpleLossCompute()</code> class is designed with three key elements: <code class="fm-code-in-text">generator</code>, serving as the prediction model; <code class="fm-code-in-text">criterion</code>, which is a function to calculate loss; and <code class="fm-code-in-text">opt</code>, the optimizer. This class processes a batch of training data, denoted as <span class="times">(x, y)</span>, by utilizing the generator for predictions. It subsequently evaluates the loss by comparing these predictions with the actual labels y (which is handled by the <code class="fm-code-in-text">LabelSmoothing()</code> class defined earlier; the actual labels y will be smoothed in the process). The class computes gradients relative to the model parameters and utilizes the optimizer to update these parameters accordingly.<a id="idIndexMarker096"/><a id="idIndexMarker097"/><a id="idIndexMarker098"/><a id="idIndexMarker099"/><a id="marker-233"/><a id="idIndexMarker100"/></p>
<p class="body">We are now ready to define the loss function:</p>
<pre class="programlisting">from utils.ch09util import (LabelSmoothing,
       SimpleLossCompute)
  
criterion = LabelSmoothing(tgt_vocab, 
                           padding_idx=0, smoothing=0.1)
loss_func = SimpleLossCompute(
            model.generator, criterion, optimizer)</pre>
<p class="body">Next, we’ll train the Transformer by using the data we prepared earlier in the chapter.<a id="idIndexMarker101"/><a id="idIndexMarker102"/><a id="idIndexMarker103"/><a id="idIndexMarker104"/></p>
<h3 class="fm-head1" id="heading_id_11">10.3.2 The training loop</h3>
<p class="body">We could potentially divide the training data into a train set and a validation set and train the model until the performance of the model doesn’t improve on the validation set, similar to what we have done in chapter 2. However, to save space, we’ll train the model for 100 epochs. We’ll calculate the loss and the number of tokens from each batch. After each epoch, we calculate the average loss in the epoch as the ratio between the total loss and the total number of tokens.<a id="idIndexMarker105"/><a id="idIndexMarker106"/></p>
<p class="fm-code-listing-caption">Listing 10.8 Training a Transformer to translate English to French</p>
<pre class="programlisting">for epoch in range(100):
    model.train()
    tloss=0
    tokens=0
    for batch in BatchLoader():
        out = model(batch.src, batch.trg, 
                    batch.src_mask, batch.trg_mask)            <span class="fm-combinumeral">①</span>
        loss = loss_func(out, batch.trg_y, batch.ntokens)      <span class="fm-combinumeral">②</span>
        tloss += loss
        tokens += batch.ntokens                                <span class="fm-combinumeral">③</span>
    print(f"Epoch {epoch}, average loss: {tloss/tokens}")
torch.save(model.state_dict(),"files/en2fr.pth")               <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Makes predictions using the Transformer</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Calculates loss and adjusts model parameters</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Counts the number of tokens in the batch</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Saves the weights in the trained model after training</p>
<p class="body">This training process takes a couple of hours if you are using a CUDA-enabled GPU. It may take a full day if you are using CPU training. Once the training is done, the model weights are saved as en2fr.pth on your computer. Alternatively, you can download the trained weights from my website (<a class="url" href="https://gattonweb.uky.edu/faculty/lium/gai/ch9.zip">https://gattonweb.uky.edu/faculty/lium/gai/ch9.zip</a>).<a id="idIndexMarker107"/><a id="idIndexMarker108"/><a id="marker-234"/></p>
<h2 class="fm-head" id="heading_id_12">10.4 Translating English to French with the trained model</h2>
<p class="body">Now that you have trained the Transformer, you can use it to translate any English sentence to French. We define a function <code class="fm-code-in-text">translate()</code> as shown in the following listing.</p>
<p class="fm-code-listing-caption">Listing 10.9 Defining a <code class="fm-code-in-text">translate()</code> function to translate English to French</p>
<pre class="programlisting">def translate(eng):
    tokenized_en=tokenizer.tokenize(eng)
    tokenized_en=["BOS"]+tokenized_en+["EOS"]
    enidx=[en_word_dict.get(i,UNK) for i in tokenized_en]  
    src=torch.tensor(enidx).long().to(DEVICE).unsqueeze(0)    
    src_mask=(src!=0).unsqueeze(-2)
    memory=model.encode(src,src_mask)                           <span class="fm-combinumeral">①</span>
    start_symbol=fr_word_dict["BOS"]
    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)
    translation=[]
    for i in range(100):
        out = model.decode(memory,src_mask,ys,
        subsequent_mask(ys.size(1)).type_as(src.data))          <span class="fm-combinumeral">②</span>
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.data[0]    
        ys = torch.cat([ys, torch.ones(1, 1).type_as(
            src.data).fill_(next_word)], dim=1)
        sym = fr_idx_dict[ys[0, -1].item()]
        if sym != 'EOS':
            translation.append(sym)
        else:
            break                                               <span class="fm-combinumeral">③</span>
    trans="".join(translation)
    trans=trans.replace("&lt;/w&gt;"," ") 
    for x in '''?:;.,'("-!&amp;)%''':
        trans=trans.replace(f" {x}",f"{x}")                     <span class="fm-combinumeral">④</span>
    print(trans) 
    return trans</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Uses encoder to convert the English phrase to a vector representation</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Predicts the next token using the decoder</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Stops translating when the next token is “EOS”</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Joins the predicted tokens to form a French sentence</p>
<p class="body">To translate an English phrase to French, we first use the tokenizer to convert the English sentence to tokens. We then add <code class="fm-code-in-text">"BOS"</code> and <code class="fm-code-in-text">"EOS"</code> at the beginning and the end of the phrase. We use the dictionary <code class="fm-code-in-text">en_word_dict</code> we created earlier in the chapter to convert tokens to indexes. We feed the sequence of indexes to the encoder in the trained model. The encoder produces an abstract vector representation and passes it on to the decoder.</p>
<p class="body">Based on the abstract vector representation of the English sentence produced by the encoder, the decoder in the trained model starts translating in an autoregressive manner, starting with the beginning token <code class="fm-code-in-text">"BOS"</code>. In each time step, the decoder generates the most likely next token based on previously generated tokens, until the predicted token is <code class="fm-code-in-text">"EOS"</code>, which signals the end of the sentence. Note this is slightly different from the text generation approach discussed in chapter 8, where the next token is chosen randomly in accordance with its predicted probabilities. Here, the method for selecting the next token is deterministic, meaning the token with the highest probability is chosen with certainty because we mainly care about accuracy. However, you can switch to stochastic prediction as we did in chapter 8 and use <code class="fm-code-in-text">top-K</code> sampling and temperature if you want your translation to be creative.</p>
<p class="body"><a id="marker-235"/>Finally, we change the token separator to a space and remove the space before the punctuation marks. The output is the French translation in a clean format.</p>
<p class="body">Let’s try the <code class="fm-code-in-text">translate()</code> function with the English phrase “Today is a beautiful day!”:</p>
<pre class="programlisting">from utils.ch09util import subsequent_mask
  
with open("files/dict.p","rb") as fb:
    en_word_dict,en_idx_dict,\
    fr_word_dict,fr_idx_dict=pickle.load(fb)
trained_weights=torch.load("files/en2fr.pth",
                           map_location=DEVICE)
model.load_state_dict(trained_weights)
model.eval()
eng = "Today is a beautiful day!"
translated_fr = translate(eng)</pre>
<p class="body">The output is</p>
<pre class="programlisting">aujourd'hui est une belle journee!</pre>
<p class="body">You can verify that the French translation indeed means “Today is a beautiful day!” by using, say, Google Translate.</p>
<p class="body">Let’s try a longer sentence and see if the trained model can successfully translate:</p>
<pre class="programlisting">eng = "A little boy in jeans climbs a small tree while another child looks on."
translated_fr = translate(eng)</pre>
<p class="body">The output is</p>
<pre class="programlisting">un petit garcon en jeans grimpe un petit arbre tandis qu'un autre enfant regarde. </pre>
<p class="body">When I translate the preceding output back to Engl<a id="idTextAnchor000"/>ish using Google Translate, it says, “a little boy in jeans climbs a small tree while another child watches”—not exactly the same as the original English sentence, but the meaning is the same.</p>
<p class="body">Next, we’ll test if the trained model generates the same translation for the two English sentences “I don’t speak French.” and “I do not speak French.” First, let’s try the sentence “I don’t speak French.”:</p>
<pre class="programlisting">eng = "I don't speak French."
translated_fr = translate(eng)</pre>
<p class="body">The output is</p>
<pre class="programlisting">je ne parle pas francais. </pre>
<p class="body">Now let’s try the sentence “I do not speak French.”:</p>
<pre class="programlisting">eng = "I do not speak French."
translated_fr = translate(eng)</pre>
<p class="body">The output this time is</p>
<pre class="programlisting">je ne parle pas francais. </pre>
<p class="body">The results indicate that French translations of the two sentences are exactly the same. This suggests that the encoder component of the Transformer successfully grasps the semantic essence of the two phrases. It then represents them as similar abstract continuous vector forms, which are subsequently passed on to the decoder. The decoder then generates translations based on these vectors and produces identical results.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 10.3</p>
<p class="fm-sidebar-text">Use the <code class="fm-code-in-text1">translate()</code> function to translate the following two English sentences to French. Compare the results with those from Google Translate and see if they are the same: (i) I love skiing in the winter! (ii) How are you?</p>
</div>
<p class="body"><a id="marker-236"/>In this chapter, you trained an encoder-decoder Transformer to translate English to French by using more than 47,000 pairs of English-to-French translations. The trained model works well, translating common English phrases correctly!</p>
<p class="body">In the following chapters, you’ll explore decoder-only Transformers. You’ll learn to build them from scratch and use them to generate coherent text, better than the text you generated in chapter 8 using long short-term memory.</p>
<h2 class="fm-head" id="heading_id_13">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">Transformers process input data such as sentences in parallel, unlike recurrent neural networks, which handle data sequentially. This parallelism enhances their efficiency but doesn’t inherently allow them to recognize the sequence order of the input. To address this, Transformers add positional encodings to the input embeddings. These positional encodings are unique vectors assigned to each position in the input sequence and align in dimension with the input embeddings.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Label smoothing is commonly used in training deep neural networks to improve the generalization of the model. It is used to address overconfidence problems (the predicted probability is greater than the true probability) and overfitting in classifications. Specifically, it modifies the way the model learns by adjusting the target labels, aiming to reduce the model’s confidence in the training data, which can lead to better performance on unseen data.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Based on the encoder’s output that captures the meaning of the English phrase, the decoder in the trained Transformer starts translating in an autoregressive manner, starting with the beginning token <code class="fm-code-in-text">"BOS"</code>. In each time step, the decoder generates the most likely next token based on previously generated tokens, until the predicted token is <code class="fm-code-in-text">"EOS"</code>, which signals the end of the sentence.<a id="marker-237"/></p>
</li>
</ul>
<hr class="calibre6"/>
<p class="fm-footnote"><a id="footnote-000"/><sup class="footnotenumber1"><a class="url1" href="#footnote-000-backlink">1</a></sup>  Vaswani et al, 2017, “Attention Is All You Need.” <a class="url" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>.</p>
</div></body></html>