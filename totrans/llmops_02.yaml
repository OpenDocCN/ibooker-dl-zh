- en: Chapter 2\. Introduction to LLMOps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：LLMOps 简介
- en: The size and complexity of LLMs’ architecture can make productionizing these
    models incredibly hard. *Productionizing* means not just deploying a model but
    also monitoring it, evaluating it, and optimizing its performance.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: LLM架构的规模和复杂性使得将这些模型投入生产变得极其困难。“投入生产”不仅意味着部署一个模型，还包括监控它、评估它和优化其性能。
- en: There are constantly new challenges. Depending on your application, these may
    include how to process data, how to store and dynamically adapt prompts, how to
    monitor user interaction, and—most pressing—how to prevent the model from spreading
    misinformation or memorizing training data (which can lead it to release personal
    information). That’s why operationalizing LLMs, which means managing them day-to-day
    in production, requires a new framework.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 持续出现新的挑战。根据您的应用程序，这些挑战可能包括如何处理数据、如何存储和动态调整提示、如何监控用户交互，以及——最紧迫的是——如何防止模型传播错误信息或记住训练数据（这可能导致其泄露个人信息）。这就是为什么需要将LLM操作化，即在生产中日常管理它们，需要一个新框架。
- en: '*LLMOps*, as it’s called, is an operational framework for putting LLM applications
    in production. Although its name and principles are inspired by its older siblings,
    MLOps and DevOps, LLMOps is significantly more nuanced. The LLMOps framework can
    help companies reduce technical debt, maintain compliance, deal with LLMs’ dynamic
    and experimental nature, and minimize operational and reputational risk by avoiding
    common pitfalls.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称和原则所暗示的，*LLMOps*是一个将LLM应用程序投入生产的操作框架。尽管它的名称和原则受到了其前辈MLOps和DevOps的启发，但LLMOps要复杂得多。LLMOps框架可以帮助公司减少技术债务，保持合规性，处理LLM的动态和实验性，并通过避免常见陷阱来最小化运营和声誉风险。
- en: This chapter starts by discussing what LLMOps is and how and where it departs
    from MLOps. We’ll then introduce you to the LLMOps engineer role and where it
    fits into existing ML teams. From there, we’ll look at how to measure LLMOps readiness
    within teams, assess your organization’s LLMOps maturity, and identify crucial
    KPIs for measuring success. Toward the end of this chapter, we will outline some
    challenges that are specific to productionizing LLM applications.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章首先讨论LLMOps是什么，以及它如何以及在哪里与MLOps不同。然后，我们将向您介绍LLMOps工程师的角色以及它在现有ML团队中的位置。从那里，我们将探讨如何在团队中衡量LLMOps的准备工作，评估您组织的LLMOps成熟度，并确定衡量成功的关键KPI。在本章的末尾，我们将概述一些特定于将LLM应用程序投入生产的挑战。
- en: What Are Operational Frameworks?
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是操作框架？
- en: '*Operational frameworks* provide a structured approach to managing complex
    workflows and pipelines within an organization. These frameworks integrate tools
    and practices to automate and streamline organizational processes and ensure consistency
    and quality across the project lifecycle.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*操作框架*提供了一种结构化的方法来管理组织内部复杂的工作流程和管道。这些框架整合工具和实践，以自动化和简化组织流程，并确保在整个项目生命周期中保持一致性和质量。'
- en: Some of the earliest operational frameworks can be traced back to military strategy
    and the Industrial Revolution. Two of the most popular ones, both introduced in
    1986, are [Toyota’s Lean Production System](https://oreil.ly/oNheh), which put
    Toyota ahead of most of its contemporaries, and [Six Sigma](https://oreil.ly/GHXWt),
    Motorola’s data-driven approach to improving processes and reducing defects.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最早的操作框架可以追溯到军事战略和工业革命。其中两个最受欢迎的，都是在1986年推出的，分别是[丰田精益生产系统](https://oreil.ly/oNheh)，它使丰田领先于大多数同行，以及[六西格玛](https://oreil.ly/GHXWt)，摩托罗拉的数据驱动方法，用于改进流程和减少缺陷。
- en: 'In 2008, the tech industry began to adopt what is now one of the most popular
    operational frameworks in software: DevOps. (The term combines *software development*
    and *operations*.) In 2018, MLOps, an operational framework for non-generative
    machine learning (ML) models, became the talk of the town; since then we’ve seen
    SecOps (Security Operations), DataSecOps, and many more.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2008年，科技行业开始采用现在软件中最受欢迎的操作框架之一：DevOps。（这个术语结合了*软件开发*和*运维*。）2018年，MLOps，一个用于非生成式机器学习（ML）模型的操作框架，成为了热门话题；从那时起，我们看到了SecOps（安全运维）、DataSecOps以及更多。
- en: 'With the massive adoption of LLMs in 2023, a new operational framework started
    floating around within companies that were building LLM applications: LLMOps.
    LLMOps is still in its infancy, but as generative models become integral to software
    products, its popularity is likely to boom.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM在2023年的大规模采用，一个新的操作框架开始在构建LLM应用程序的公司中流传：LLMOps。LLMOps仍处于起步阶段，但随着生成模型成为软件产品的核心，其受欢迎程度可能会激增。
- en: '[Figure 2-1](#ch02_figure_1_1748895480187715) shows the slow rise of Ops frameworks
    over the years. Use of LLMOps started to pick up with the mass adoption of LLMs
    in early 2023\. As of this writing, more and more enterprises are realizing that
    they can add value and profit with LLM-based offerings, leading to an upward trend
    for LLMOps. In fact, 2025 may be the best year yet for LLMOps frameworks.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-1](#ch02_figure_1_1748895480187715) 展示了Ops框架多年来的缓慢增长。LLMOps的使用随着2023年初LLMs的广泛应用而开始增加。截至本文撰写时，越来越多的企业意识到他们可以通过基于LLM的提供来增加价值和利润，导致LLMOps呈上升趋势。事实上，2025年可能是LLMOps框架最好的年份。'
- en: 'Deploying LLMs can be as simple as integrating a chatbot into your website
    via an API or as complicated as building your own LLM and frontend from scratch.
    But maintaining them to be performant (productionizing them)—that is, keeping
    them reliable, scalable, secure, and robust—is a massive challenge. This book,
    like LLMOps, is focused on exactly this question: what happens *after* you deploy
    your LLM in production?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 部署LLM可能像通过API将聊天机器人集成到您的网站上一样简单，也可能像从头开始构建自己的LLM和前端一样复杂。但是，保持它们的表现（将它们投入生产）——也就是说，保持它们可靠、可扩展、安全、健壮——是一个巨大的挑战。本书，就像LLMOps一样，专注于这个问题：在生产中部署您的LLM后会发生什么？
- en: '![](assets/llmo_0201.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/llmo_0201.png)'
- en: Figure 2-1\. Google Ngram showing the rise in popularity of the term LLMOps
    from 2019 to 2024
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. Google Ngram显示从2019年到2024年LLMOps术语流行度的增长
- en: 'From MLOps to LLMOps: Why Do We Need a New Framework?'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从MLOps到LLMOps：为什么我们需要一个新的框架？
- en: There is some overlap between MLOps and LLMOps; both deal with the operational
    lifecycles of ML models, after all. They also share common principles in terms
    of managing ML workflows. However, the two frameworks diverge in their primary
    focuses and objectives. While MLOps handles non-generative models (both language
    and computer vision), LLMOps deals with generative language models—and thus with
    mammoth levels of complexity. The complexity of these models owes not only to
    their scale and architecture but also to the unique processes involved in data
    engineering, domain adaptation, evaluation, and monitoring for them. The key distinctions
    are apparent in LLMs’ prediction transparency, latency, and memory and computational
    requirements.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps和LLMOps之间有一些重叠；毕竟，它们都处理ML模型的运营生命周期。它们在管理ML工作流程方面也共享一些共同原则。然而，这两个框架在主要重点和目标上有所不同。MLOps处理非生成式模型（包括语言和计算机视觉），而LLMOps处理生成式语言模型——因此具有巨大的复杂性。这些模型的复杂性不仅源于它们的规模和架构，还源于数据工程、领域适应性、评估和监控它们所涉及的独特流程。这些关键区别在LLMs的预测透明度、延迟、内存和计算需求中显而易见。
- en: Perhaps the biggest difference is the shift in how end users consume these models.
    Non-generative ML models are predictive tools used for passive consumption, such
    as in dashboarding, recommendations, and analytics. By contrast, LLM applications
    are deployed as [Software 3.0](https://oreil.ly/g9dn6) in consumer-facing applications
    for active user interaction. This brings several challenges from Software 1.0
    (DevOps) back to the surface. In fact, it wouldn’t be wrong to say that LLMOps
    shares more similarities with DevOps than with MLOps.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最大的不同是终端用户消费这些模型的方式的转变。非生成式ML模型是用于被动消费的预测工具，例如在仪表板、推荐和数据分析中。相比之下，LLM应用作为[软件3.0](https://oreil.ly/g9dn6)在面向消费者的应用中部署，用于主动用户交互。这带来了从软件1.0（DevOps）回归的几个挑战。事实上，说LLMOps与DevOps比与MLOps有更多相似之处并不过分。
- en: Building your own generative AI application requires tools, frameworks, and
    expectations to match the scale and complexity of these models, and this is far
    beyond the scope of the existing MLOps solutions. To help you understand the differences
    between the various kinds of Ops frameworks, let’s do a thought experiment.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 构建自己的生成式AI应用需要工具、框架和与这些模型规模和复杂性相匹配的期望，而这远远超出了现有MLOps解决方案的范围。为了帮助您理解不同类型的Ops框架之间的差异，让我们进行一个思想实验。
- en: 'Imagine MLOps as being something like building a small home from the ground
    up. In comparison, DevOps, which deals with the entire product lifecycle, would
    be like developing a large shopping complex. And LLMOps? It’s more like building
    the Burj Khalifa. For all three frameworks, you are working with the same construction
    materials: wood, steel, concrete, bricks, hammers, and so on. Much of the basic
    process is the same, too: you lay down the foundation, lay down the plumbing,
    and finally build the walls. But you wouldn’t contract your local construction
    workers to engineer the Burj Khalifa, would you?'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 想象MLOps就像是从头开始建造一个小房子。相比之下，DevOps处理整个产品生命周期，就像开发一个大型的购物中心。而LLMOps呢？它更像是建造迪拜哈利法塔。对于这三个框架，你都在使用相同的建筑材料：木材、钢材、混凝土、砖块、锤子等等。大部分的基本流程也是相同的：你打下地基，铺设管道，最后建造墙壁。但你不会雇佣当地的建筑工人来设计迪拜哈利法塔，对吧？
- en: Most of MLOps for natural language modeling is based on building smaller, discriminative
    models for tasks like sentiment analysis, topic modeling, and summarization. For
    these tasks, you first hypothesize ideal features, model your data as a function
    of those features, and then optimize the model for that specific task.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数自然语言建模的MLOps都是基于构建较小的、判别性的模型来完成诸如情感分析、主题建模和摘要等任务。对于这些任务，你首先假设理想特征，将你的数据作为这些特征的函数进行建模，然后针对特定任务优化模型。
- en: LLMs, by contrast, are generative, domain agnostic, and task agnostic. That
    fundamental difference means you can use the same model for summarizing or for
    answering questions, without having to fine-tune it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与之相反，LLMs（大型语言模型）是生成性的、领域无关的、任务无关的。这种基本差异意味着你可以使用同一个模型来进行总结或回答问题，而无需对其进行微调。
- en: The best use cases for LLMs are when you don’t know what features to optimize
    for (or, even better, if the features are too abstract) and when you need to model
    multimodal data within a single pipeline. Unlike smaller discriminative models,
    LLMs don’t rely on predefined features or task-specific architectures. Instead,
    as you learned in [Chapter 1](ch01.html#ch01_introduction_to_large_language_models_1748895465615150),
    they are trained on vast amounts of text data to learn the patterns and structures
    in language itself. For LLMs, the training process involves a *loss function*,
    which measures how well the model’s generated outputs match the expected outputs
    across various tasks. For example, during training, the model might generate a
    sequence of text; the loss function calculates the difference between this generated
    sequence and the target sequence.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的最佳用例是在你不知道要优化哪些特征（或者，更好的是，如果特征过于抽象）时，以及当你需要在单个管道内对多模态数据进行建模时。与较小的判别性模型不同，LLMs不依赖于预定义的特征或特定任务的架构。相反，正如你在[第1章](ch01.html#ch01_introduction_to_large_language_models_1748895465615150)中学到的，它们是在大量的文本数据上训练的，以学习语言本身的模式和结构。对于LLMs，训练过程涉及一个*损失函数*，该函数衡量模型生成的输出与各种任务中预期的输出之间的匹配程度。例如，在训练过程中，模型可能会生成一系列文本；损失函数计算这个生成序列与目标序列之间的差异。
- en: Additionally, for a standard comparison, the hyperparameter space for a 175-billion
    parameter GPT-4 ​model would likely be approximately 1,500 times larger than that
    for a standard discriminative BERT model (which has 110 million parameters). This
    makes it incredibly costly to fine-tune and do the kind of iterative training
    that is typical in MLOps. [Table 2-1](#ch02_table_1_1748895480195288) outlines
    some of the key differences between the MLOps and LLMOps model lifecycles.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了进行标准比较，一个1750亿参数的GPT-4模型的超参数空间可能比标准判别性BERT模型（具有1.1亿参数）的超参数空间大约大1500倍。这使得微调和在MLOps中进行典型的迭代训练变得极其昂贵。[表2-1](#ch02_table_1_1748895480195288)概述了MLOps和LLMOps模型生命周期之间的一些关键差异。
- en: Table 2-1\. Comparing the MLOps and LLMOps model lifecycles
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-1\. 比较MLOps和LLMOps模型生命周期
- en: '| Lifecycle step | Context | MLOps | LLMOps |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 生命周期步骤 | 环境 | MLOps | LLMOps |'
- en: '| --- | --- | --- | --- |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Data collection | Ease | Always and straightforward. | Sometimes—data composition
    is a very hard problem. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 数据收集 | 简易程度 | 总是直接。 | 有时——数据组成是一个非常困难的问题。 |'
- en: '| Data preprocessing | Ease | Fix missing data and outliers—very easy! | Hard.
    Requires:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '| 数据预处理 | 简易程度 | 修复缺失数据和异常值——非常容易！ | 困难。需要：'
- en: Deduplication
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 去重
- en: Toxicity filtering
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 毒性过滤
- en: Diversity management
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多样性管理
- en: Quantity control
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数量控制
- en: '|'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Model learning | Model size | ML models, such as classifiers or regressors,
    typically have at max around 200 million parameters and are generally less computationally
    intensive for experimentation than are LLMs. | LLMs typically have 100 billion
    or even trillions of parameters, making them significantly larger and more complex
    than many non-generative models. Their massive scale impacts resource requirements,
    storage, and computational efficiency. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 模型学习 | 模型大小 | 机器学习模型，如分类器或回归器，通常最大有约2亿个参数，并且与LLM相比，在实验中通常计算量较小。 | LLM通常有1000亿或甚至万亿个参数，这使得它们比许多非生成模型大得多且复杂得多。它们的巨大规模影响了资源需求、存储和计算效率。
    |'
- en: '| Hyperparameters | Accuracy | Limited hyperparameter space makes search easy,
    making them ideal for predictive problems. | Massive hyperparameter space leads
    to real-time search latency. This makes them ideal for generative and evolutionary
    tasks that require creativity. Accuracy can be a computational bottleneck. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 准确性 | 有限的超参数空间使得搜索变得容易，使它们非常适合预测问题。 | 巨大的超参数空间会导致实时搜索延迟。这使得它们非常适合需要创造力的生成和进化任务。准确性可能成为计算瓶颈。
    |'
- en: '| Model training duration | Training scale and duration | Less resource intensive
    and generally faster. Can be easily deployed as notebooks on the cloud or as containerized
    solutions across a single node. | Involves processing massive datasets with distributed
    training across large clusters of GPUs or TPUs and optimizing for parallel processing.
    Can take days or weeks. Operationalizing may require dynamic resource scaling
    and involves designing scalable infrastructure and orchestration systems to handle
    varying workloads efficiently. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 模型训练时长 | 训练规模和时长 | 资源消耗较少，通常更快。可以轻松部署为云上的笔记本或跨单个节点的容器化解决方案。 | 涉及使用分布式训练在大型GPU或TPU集群上处理大量数据集，并针对并行处理进行优化。可能需要几天或几周。实施可能需要动态资源扩展，并涉及设计可扩展的基础设施和编排系统以高效处理不同的工作负载。
    |'
- en: '| Domain adaptation | Cost | Full fine-tuning is essential and affordable.
    | Full fine-tuning is too expensive and pretty rare. Instead, popular techniques
    are:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '| 领域适应性 | 成本 | 完整微调是必需且负担得起的。 | 完整微调成本过高且相当罕见。相反，流行的技术包括：'
- en: Prompt engineering
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程
- en: RAGs
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAGs
- en: Knowledge graphs
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识图谱
- en: Parameter-efficient fine-tuning
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数高效微调
- en: '|'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Evaluation | Ease |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 评估 | 简便 |'
- en: Easy
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单
- en: Discriminative
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区分性
- en: Well-defined probability space
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明确定义的概率空间
- en: '|'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Extremely hard problem
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极其困难的问题
- en: Generative and thus hard to detect
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成性且因此难以检测
- en: Unbounded probability space
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无界概率空间
- en: '|'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Robustness | Static versus dynamic | Model behavior stays the same in production.
    | Model behavior changes in production based on interactions and requires constant
    monitoring for alignment. |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 坚韧性 | 静态与动态 | 模型在生产中的行为保持不变。 | 模型在生产中的行为基于交互而变化，需要持续监控以保持一致性。 |'
- en: '| Security | Secure | Highly secure. |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 安全性 | 安全 | 高度安全。 |'
- en: Highly vulnerable
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极度脆弱
- en: Needs a DataSecOps framework
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要一个DataSecOps框架
- en: '|'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Four Goals for LLMOps
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMOps的四个目标
- en: 'LLMOps operates on an LLM-specific set of design pattern principles to ensure
    that your LLM applications achieve four key goals: security, scalability, robustness,
    and reliability. Let’s take a closer look at these goals:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps基于一套针对LLM特定的设计模式原则来确保您的LLM应用实现四个关键目标：安全性、可扩展性、坚韧性和可靠性。让我们更详细地看看这些目标：
- en: Security
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性
- en: Minimizing the regulatory, reputational, and operational risks associated with
    deploying LLM applications in production
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最小化与在生产中部署LLM应用相关的监管、声誉和运营风险
- en: Scalability
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性
- en: Building, storing, and maintaining LLM applications that can generalize across
    data and scale efficiently on demand while optimizing latency, throughput, and
    costs
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 构建、存储和维护能够跨数据泛化且按需高效扩展的LLM应用，同时优化延迟、吞吐量和成本
- en: Robustness
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 坚韧性
- en: Maintaining high performance of LLM applications over time in the face of data
    drift, model drift, third-party updates, and other challenges
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在面对数据漂移、模型漂移、第三方更新和其他挑战的情况下，在长时间内保持LLM应用的高性能
- en: Reliability
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 可靠性
- en: Implementing rigorous inference monitoring, error handling, and redundancy mechanisms
    to prevent downtime and failures
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 实施严格的推理监控、错误处理和冗余机制，以防止停机和服务中断
- en: LLMOps teams automate repetitive processes to more quickly optimize these applications
    at scale and avoid those “LLM-oops!” moments. Another key aspect of the LLMOps
    framework is fostering consistency, transparency, and collaboration between diverse
    interdisciplinary teams. These teams often include data engineers, data scientists,
    research scientists, software engineers, and business operations teams.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps 团队自动化重复性流程，以更快地大规模优化这些应用程序，并避免那些“LLM-oops！”时刻。LLMOps 框架的另一个关键方面是促进不同跨学科团队之间的连贯性、透明度和协作。这些团队通常包括数据工程师、数据科学家、研究科学家、软件工程师和业务运营团队。
- en: LLMOps is an entirely new field, so, as of this writing in 2025, there are very
    few mature tools and resources available. The LLMOps teams at various organizations
    are thus developing their tools and processes internally, based on prototypical
    open source libraries and toolkits.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps 是一个全新的领域，因此，截至 2025 年撰写本文时，可用的成熟工具和资源非常有限。因此，各个组织的 LLMOps 团队正在内部开发他们的工具和流程，基于典型的开源库和工具包。
- en: LLMOps Teams and Roles
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMOps 团队和角色
- en: 'Today, there are two kinds of companies out there building with LLMs: the newer
    startups, which focus primarily on LLM applications, and companies with existing
    ML models that are now building their own GenAI teams.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，有两种类型的公司在使用 LLM 构建产品：专注于 LLM 应用程序的新兴初创公司，以及现在正在构建自己的 GenAI 团队的拥有现有 ML 模型的公司。
- en: In the latter category, there are so few skilled Ops professionals that most
    companies recruit ML engineer candidates internally and then upskill them to LLMOps,
    instead of hiring externally. A major reason for this is a general lack of clarity
    around use cases as well as the expected job responsibilities. So the current
    norm is to hire 8 to 10 people internally from different departments, which could
    include product managers, full-stack engineers, system architects, data engineers,
    data scientists, ML engineers, platform engineers, cybersecurity professionals,
    and developer advocates. Many companies want someone who already understands the
    business inside and out to test the feasibility of several potential use cases
    and projects before committing to one.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在后一类中，由于缺乏熟练的运营专业人员，大多数公司选择从内部招聘 ML 工程师候选人，然后提升他们的 LLMOps 技能，而不是从外部招聘。主要原因是对用例以及预期的工作职责缺乏清晰的了解。因此，目前的规范是从不同部门内部招聘
    8 到 10 人，这些部门可能包括产品经理、全栈工程师、系统架构师、数据工程师、数据科学家、ML 工程师、平台工程师、网络安全专业人士和开发者倡导者。许多公司希望找到已经深入了解业务的人，在做出承诺之前测试几个潜在用例和项目的可行性。
- en: Newer startups, however, have no choice but to build a team from the ground
    up. These teams can look very different, depending on whether they are working
    on LLMOps infrastructure (LLMOps SaaS companies) or LLM use cases such as copywriting,
    education, or process optimization. [Figure 2-2](#ch02_figure_2_1748895480187751)
    provides a very basic model of an LLMOps team, but these teams come in all shapes
    and sizes, with different levels of business and technical maturity. (Later in
    the chapter, we’ll look at how to assess your company’s LLMOps maturity.)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，较新的初创公司别无选择，只能从头开始组建团队。这些团队可能看起来非常不同，这取决于他们是在开发 LLMOps 基础设施（LLMOps SaaS 公司）还是
    LLM 用例，如文案写作、教育或流程优化。[图 2-2](#ch02_figure_2_1748895480187751) 提供了一个非常基本的 LLMOps
    团队模型，但这些团队在形状和规模上各不相同，具有不同的业务和技术成熟度。（在本章的后面部分，我们将探讨如何评估您公司的 LLMOps 成熟度。）
- en: '![](assets/llmo_0202.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/llmo_0202.png)'
- en: Figure 2-2\. Structure of an LLMOps team
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. LLMOps 团队结构
- en: 'Most companies cannot afford the cost of building and training an LLM for use
    in their application. Instead, most choose to use a foundational model. This is
    where AI engineers come in: to build a quick proof of concept using LLMOps tools.
    Most come from a software-engineering background; they may know how to build a
    full-stack application quickly, but don’t necessarily have a deep understanding
    of ML/LLM models’ inner workings or how to optimize them.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数公司承担不起为应用构建和训练 LLM 的成本。相反，大多数公司选择使用基础模型。这就是 AI 工程师发挥作用的地方：使用 LLMOps 工具快速构建一个概念验证。大多数来自软件工程背景；他们可能知道如何快速构建全栈应用程序，但并不一定深入了解
    ML/LLM 模型的内部工作原理或如何优化它们。
- en: Once the proof of concept is completed, building, deploying, and optimizing
    ML models for the business falls to LLM engineers, ​ML scientists (interchangeable
    titles, depending on the organization), and LLMOps ​engineers tasked as reliability
    engineers.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦概念验证完成，构建、部署和优化用于商业的机器学习（ML）模型的工作就落在了LLM工程师、ML科学家（根据组织名称可互换）和被指定为可靠性工程师的LLMOps工程师身上。
- en: If a team chooses to deploy an LLM application using API integration, the initial
    deployment will be pretty straightforward. The LLMOps engineers work alongside
    AI engineers to scale the model and improve its performance. Ideally, each AI
    engineer is paired with an LLMOps engineer; the AI engineer can focus on adapting
    and fine-tuning the model to meet the business needs, while the LLMOps engineer
    manages deployment and optimization. For open source models, managing deployment
    automatically falls to the LLMOps engineer.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个团队选择使用API集成部署LLM应用，初始部署将会相当直接。LLMOps工程师与AI工程师并肩工作，以扩展模型并提高其性能。理想情况下，每位AI工程师都配有一名LLMOps工程师；AI工程师可以专注于调整和微调模型以满足业务需求，而LLMOps工程师则负责部署和优化。对于开源模型，部署管理自动由LLMOps工程师负责。
- en: As the tech industry moves from non-generative models to generative models,
    it is shifting away from feature engineering, or creating features to model the
    data and experimenting with different hyperparameters to optimize performance.
    Generative models, and specifically LLMs, do not require feature engineering.
    Today, the core requirements are usually prompt engineering or building a RAG
    pipeline—skills that lie within the domain of AI engineers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 随着技术行业从非生成模型转向生成模型，它正逐渐远离特征工程，即创建特征以建模数据和实验不同的超参数以优化性能。生成模型，特别是LLM，不需要特征工程。今天，核心要求通常是提示工程或构建一个RAG管道——这些技能属于AI工程师的领域。
- en: Another big shift is that the data engineering pipeline and the monitoring and
    evaluation pipeline have become far more complex. Evaluating LLMs is much more
    than straightforward quantitative scoring. Some industry benchmarks exist—like
    [BLEU](https://oreil.ly/OxqU2), a benchmark used to evaluate machine translation,
    and [ROUGE](https://oreil.ly/1y3e4), a benchmark used to evaluate summarization,
    but these are only loosely correlated with application performance. Moving to
    a model that has a better score is no guarantee of better user satisfaction. Standardized
    scores may be good for comparing LLMs in general, but ultimately users care about
    whether the LLM application is solving their problem.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重大转变是数据工程管道和监控评估管道变得更加复杂。评估大型语言模型（LLM）远不止于简单的量化评分。一些行业基准存在——例如[BLEU](https://oreil.ly/OxqU2)，这是一个用于评估机器翻译的基准，以及[ROUGE](https://oreil.ly/1y3e4)，这是一个用于评估摘要的基准，但这些与实际应用性能只有松散的相关性。转向得分更高的模型并不能保证更好的用户满意度。标准化分数可能适用于比较LLM的一般情况，但最终用户关心的是LLM应用是否解决了他们的问题。
- en: 'In addition, since LLMs are deployed in consumer-facing applications, metrics
    like perceived latency and throughput become make-or-break factors in market competition.
    The model is not the only deciding factor: the deployment, evaluation, and monitoring
    pipelines are also at the front and center of performance assessment.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于LLM被部署在面向消费者的应用中，感知延迟和吞吐量等指标成为市场竞争中的决定性因素。模型不是唯一的决定因素：部署、评估和监控管道也是性能评估的前沿和中心。
- en: 'Let’s look at some of the roles involved in these pipelines:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看这些管道中涉及的某些角色：
- en: Data engineer
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师
- en: Data engineers are professionals responsible for designing, building, and maintaining
    systems and pipelines that enable the efficient collection, storage, and transformation
    of data as well as access to it. Their work ensures that data is available, reliable,
    and organized in a way that allows data scientists and other engineers to create
    and evaluate models and data-driven applications. Data retrieval and movement
    are the most fundamental skills for data engineers, but as they progress in their
    career, developing data architectures becomes more important.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程师是负责设计、构建和维护系统管道的专业人员，这些系统管道能够高效地收集、存储、转换数据以及提供对数据的访问。他们的工作确保数据可用、可靠且组织有序，以便数据科学家和其他工程师能够创建和评估模型以及数据驱动应用。数据检索和移动是数据工程师最基本的能力，但随着他们职业生涯的发展，开发数据架构变得越来越重要。
- en: 'Data engineering for LLMs requires specialized understanding: how to chunk
    the data, what tokenization model to use, and so on. Thus, it’s best to pair each
    data engineer with an ML scientist (with an LLM engineering background) along
    with the LLMOps engineer for automating and streamlining LLM systems at scale.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为大型语言模型进行的数据工程需要专业的理解：如何分块数据，使用什么分词模型等。因此，最好将每个数据工程师与一个机器学习科学家（具有LLM工程背景）以及LLMOps工程师配对，以在规模上自动化和简化LLM系统。
- en: AI engineer
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能工程师
- en: The core skill set of an AI engineer is full-stack engineering and development
    (React, Node.js, Django) plus familiarity with the common LLMOps tools and frameworks
    for deploying applications, like LangChain and Llama Index. They need a foundational
    understanding of end-to-end AI application development, including prompt engineering
    and RAG systems. Companies building their teams from scratch usually look to hire
    AI engineers who also know a lot about using cloud services to deploy and manage
    AI applications and have experience working with external APIs and vector databases.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能工程师的核心技能集包括全栈工程和开发（React、Node.js、Django）以及熟悉用于部署应用程序的常见LLMOps工具和框架，如LangChain和Llama
    Index。他们需要对端到端AI应用程序开发有基础理解，包括提示工程和RAG系统。从头开始建立团队的公司通常寻求那些对使用云服务部署和管理AI应用程序有深入了解，并且有与外部API和向量数据库一起工作的经验的AI工程师。
- en: ML scientist
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习科学家
- en: The day-to-day work of an ML scientist, NLP scientist, or LLM engineer involves
    researching, designing, and optimizing LLMs using frameworks like PyTorch, TensorFlow,
    and JAX. This role requires a deep understanding of NLP algorithms and tasks,
    such as tokenization, named-entity recognition (NER), sentiment analysis, and
    machine translation. Candidates should know model architectures and training and
    fine-tuning processes.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习科学家、自然语言处理科学家或大型语言模型工程师的日常工作涉及使用PyTorch、TensorFlow和JAX等框架研究、设计和优化大型语言模型。这个角色需要深入理解自然语言处理算法和任务，例如分词、命名实体识别（NER）、情感分析和机器翻译。候选人应了解模型架构以及训练和微调过程。
- en: LLMOps engineer
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps工程师
- en: The goal  of an LLMOps engineer is to ensure that LLM applications remain reliable,
    robust, secure, and scalable. The day-to-day work involves building and maintaining
    operational LLM pipelines as a project owner.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps工程师的目标是确保大型语言模型的应用保持可靠、健壮、安全和可扩展。日常工作中涉及作为项目负责人构建和维护操作性的LLM管道。
- en: Let’s now look at the LLMOps engineer role in more detail.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们更详细地看看LLMOps工程师的角色。
- en: The LLMOps Engineer Role
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLMOps工程师角色
- en: This role requires extensive expertise in deploying, monitoring, fine-tuning,
    training, scaling, and optimizing LLM models in production environments; infrastructure
    and platform engineering; data engineering; and system reliability.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这个角色需要广泛的专业知识，包括在生产环境中部署、监控、微调、训练、扩展和优化LLM模型；基础设施和平台工程；数据工程；以及系统可靠性。
- en: Companies building LLM teams usually seek LLMOps engineers who understand the
    unique challenges associated with LLMs and are experienced in making build-versus-buy
    trade-off, including by weighing cost efficiency against system performance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 建立LLM团队的公司通常寻求理解与LLM相关的独特挑战，并在权衡成本效率与系统性能方面有经验的LLMOps工程师。
- en: To stand out as a candidate for this role, you’ll need proficiency in a unique
    blend of specialized skills and a deep technical understanding of the entire LLM
    lifecycle, from data management to model deployment and monitoring. You’ll also
    need to be a problem solver, a strong team player, and an effective communicator
    as well as meticulously detail oriented.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要成为这个角色的候选人，你需要精通一系列专业技能的独特组合，并对整个LLM生命周期（从数据管理到模型部署和监控）有深入的技术理解。你还需要是一个问题解决者，一个强大的团队玩家，一个有效的沟通者，以及一个细致入微的人。
- en: To illustrate what this means in practice, let’s look at a typical day in the
    work life of a fictional LLMOps engineer.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这在实际中的含义，让我们看看一个虚构的LLMOps工程师典型的工作日。
- en: A Day in the Life
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一天的工作
- en: 'While the specific responsibilities for this role will vary across organizations,
    the core tasks typically encompass a blend of infrastructure management, collaboration,
    optimization, and compliance. To give you a quick taste of what this looks like
    in practice, here’s what a typical LLM engineer’s workday might look like:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个角色的具体职责在不同组织之间会有所不同，但核心任务通常包括基础设施管理、协作、优化和合规性的结合。为了快速了解这看起来是什么样子，以下是一个典型的LLM工程师工作日可能的样子：
- en: '7:30 AM–8:30 AM: Morning check-in'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 早上7:30至8:30：晨间检查
- en: Review monitoring dashboards for any overnight alerts or performance issues
    in deployed LLMs; address any urgent issues or escalate them to the appropriate
    teams.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审查监控仪表板，查看任何夜间警报或已部署 LLM 的性能问题；解决任何紧急问题或将它们升级到适当的团队。
- en: Lead the team’s daily stand-up meeting to discuss ongoing projects, blockers,
    and priorities for the day.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 领导团队每日站立会议，讨论正在进行的项目、阻塞点和当天的优先事项。
- en: '8:30 AM–10:00 AM: Infrastructure management and optimization'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '8:30 AM–10:00 AM: 基础设施管理和优化'
- en: Modularize code for reusability​, creating separate modules for provisioning
    GPUs, managing storage, and networking.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将代码模块化以提高可重用性，为提供 GPU、管理存储和网络创建单独的模块。
- en: Implement batching mechanisms to process multiple inference requests, reducing
    the per-request overhead.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施批处理机制以处理多个推理请求，减少每个请求的额外开销。
- en: Implement latency optimization techniques like kernel fusion, quantization,
    and dynamic batching to enhance model performance.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施延迟优化技术，如内核融合、量化动态批处理，以增强模型性能。
- en: '10:00 AM–11:30 AM: Collaboration and project planning meeting'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '10:00 AM–11:30 AM: 协作和项目规划会议'
- en: Meet with data scientists, ML engineers, and red teaming engineers to discuss
    usage requirements, timelines, monitoring errors, and scaling challenges.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与数据科学家、机器学习工程师和红队工程师会面，讨论使用要求、时间表、监控错误和扩展挑战。
- en: '11:30 AM–12:30 PM: API development and model deployment'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '11:30 AM–12:30 PM: API 开发和模型部署'
- en: Design inference endpoints and cache for different models in production, ensuring
    compatibility with the rest of the application.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计生产中不同模型的推理端点和缓存，确保与整个应用程序的兼容性。
- en: '12:30 PM–1:30 PM: Lunch break'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '12:30 PM–1:30 PM: 午餐休息'
- en: '1:30 PM–3:00 PM: Monitoring and troubleshooting'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '1:30 PM–3:00 PM: 监控和故障排除'
- en: Troubleshoot any issues that arise. For example, let’s say the users are experiencing
    long delays when making requests to the inference API. The engineer would identify
    the source of latency by examining hardware utilization and network latency and
    reviewing usage logs. They would then implement a solution; e.g., using a Pod
    Autoscaler or caching the frequent requests.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决出现的任何问题。例如，假设用户在向推理 API 发出请求时遇到长时间延迟。工程师将通过检查硬件利用率和网络延迟以及审查使用日志来识别延迟的原因。然后，他们会实施解决方案；例如，使用
    Pod 自动扩展或缓存频繁请求。
- en: '3:00 PM–4:30 PM: Research and continuous learning'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '3:00 PM–4:30 PM: 研究和持续学习'
- en: Experiment with new tools, libraries, or frameworks that could be integrated
    into the existing tech stack to improve efficiency and performance.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试将新工具、库或框架集成到现有技术堆栈中，以提高效率和性能。
- en: '4:30 PM–5:30 PM: End-of-day wrap-up, review, and on-call prep'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '4:30 PM–5:30 PM: 结束一天的工作，回顾和待命准备'
- en: Review the day’s tasks and update the tickets with completed tasks and next
    steps.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审查当天的任务，并更新工单，包括已完成的任务和下一步行动。
- en: Prepare for any on-call duties, ensuring that all monitoring systems are correctly
    configured and that you’re ready to respond to any incidents.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备任何待命任务，确保所有监控系统配置正确，并准备好应对任何事件。
- en: Attend any final meetings or syncs with cross-functional teams to ensure alignment
    on upcoming priorities.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参加任何与跨职能团队的最终会议或同步，以确保对即将到来的优先事项达成一致。
- en: Wrap up any remaining tasks and make sure that all systems are running smoothly
    before logging off for the day.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完成任何剩余的任务，确保所有系统在当天下班前运行顺畅。
- en: After regular working hours, if you are on call, you’ll need to remain available
    to address any critical issues that may arise.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在正常工作时间之外，如果你在待命，你需要保持可用状态，以应对可能出现的任何关键问题。
- en: Hiring an LLMOps Engineer Externally
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部招聘 LLMOps 工程师
- en: 'There are two ways to fill an LLMOps engineer role: you can hire externally,
    or you can hire internally and upskill your people, training ML engineers to become
    LLMOps engineers. This section will look at external hiring first and then discuss
    how to upskill current employees.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 填充 LLMOps 工程师角色的两种方式：你可以外部招聘，或者内部招聘并提升员工技能，培训机器学习工程师成为 LLMOps 工程师。本节将首先探讨外部招聘，然后讨论如何提升现有员工的技能。
- en: 'If you’re hiring for this role, other skills to look for in candidates for
    LLMOps engineering roles include experience or proficiency in:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在招聘这个职位，其他在 LLMOps 工程师职位候选人中要寻找的技能包括在以下方面的经验或熟练度：
- en: Converting models to and from libraries like PyTorch or JAX
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型转换为 PyTorch 或 JAX 等库，或从这些库转换回来
- en: Understanding ML metrics like accuracy, precision, recall, and PR-AUC
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解机器学习指标，如准确率、精确度、召回率和PR-AUC
- en: Understanding data drift and concept drift
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据漂移和概念漂移
- en: Running and benchmarking models to understand the impact of computational graph
    representation on performance across the neural engine, GPU, and CPU
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行和基准测试模型以了解计算图表示对神经网络、GPU和CPU性能的影响
- en: Deploying and scaling ML models in cloud environments like AWS, GCP, and Azure
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在AWS、GCP和Azure等云环境中部署和扩展机器学习模型
- en: Using LLM inference latency optimization techniques, including kernel fusion,
    quantization, and dynamic batching
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LLM推理延迟优化技术，包括内核融合、量化动态批处理
- en: Building Ops pipelines for data engineering, deployment, and infrastructure
    as code (IaC) using tools like Terraform, managing vector databases, and ETL processes
    for large-scale training datasets
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Terraform等工具构建数据工程、部署和基础设施即代码（IaC）的Ops流水线，管理矢量数据库，以及为大规模训练数据集执行ETL过程
- en: Understanding red-teaming strategies, interfaces, and guidelines
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解红队策略、接口和指南
- en: Using Docker for containerization and Kubernetes for orchestration to ensure
    scalable and consistent deployments
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Docker进行容器化，使用Kubernetes进行编排，以确保可扩展和一致的部署
- en: Collaborating on and managing projects with teams that include LLM engineers,
    data scientists, and ML/NLP engineers
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与包括LLM工程师、数据科学家和ML/NLP工程师在内的团队协作和管理项目
- en: Every company, of course, has its own interviewing process. Some conduct many
    rounds of interviews; others combine several of the interviews into a single on-site
    meeting. This section describes a fairly standard four-round interview process,
    as pictured in [Figure 2-3](#ch02_figure_3_1748895480187773).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，每家公司都有自己的面试流程。有些公司进行多轮面试；而有些公司则将几轮面试合并为一次现场会议。本节描述了一个相当标准的四轮面试流程，如图2-3所示。[图2-3](#ch02_figure_3_1748895480187773)。
- en: '![](assets/llmo_0203.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![LLMOps面试组件](assets/llmo_0203.png)'
- en: Figure 2-3\. Components of an LLMOps interview
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3\. LLMOps面试的组件
- en: 'Let’s look at each round in more detail:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看每一轮：
- en: 'Round 1: Initial screening'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 第一轮：初步筛选
- en: The goal during initial screening is to determine that the applicant has the
    fundamental skills and experience required for the role. This can be assessed
    via a resume assessment sheet. The questions you’re asking here are very high
    level. Do they have experience with deploying LLMs in production environments?
    Do they mention specific frameworks and tools for managing LLM pipelines, and
    are these the same tools your company uses? If not, will they be able to learn
    and adopt your stack? Can they show or talk about any past projects?
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在初步筛选阶段的目标是确定申请人具备该职位所需的基本技能和经验。这可以通过简历评估表来评估。您在这里提出的问题非常高级。他们是否有在生产环境中部署LLM的经验？他们是否提到了用于管理LLM流水线的特定框架和工具，并且这些工具与贵公司使用的是否相同？如果不是，他们能否学习和采用您的技术栈？他们能否展示或讨论任何过去的项目？
- en: 'Round 2: Technical assessment'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 第二轮：技术评估
- en: 'The goal in this round is to assess the candidate’s technical proficiency in
    core areas such as LLM deployment, data engineering, and infrastructure management.
    Some questions you might ask include:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本轮的目标是评估候选人在核心领域（如LLM部署、数据工程和基础设施管理）的技术熟练度。您可能会问的问题包括：
- en: Describe the steps you take to fine-tune a pretrained large language model.
    How do you ensure that the model is optimized for the specific use case?
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述您调整预训练大型语言模型的步骤。您如何确保模型针对特定用例进行了优化？
- en: Walk me through the deployment process of an LLM you’ve worked on. What challenges
    did you face? How did you overcome them?
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带我了解您参与的一个LLM部署过程。您遇到了哪些挑战？您是如何克服它们的？
- en: How would you set up a CI/CD pipeline for LLM training, fine-tuning, and deployment?
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您会如何设置LLM训练、微调和部署的CI/CD流水线？
- en: How do you design and manage data pipelines for large-scale ML projects? What
    tools do you use, and how do you ensure data quality?
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您如何设计和管理大规模机器学习项目的数据流水线？您使用哪些工具，以及您如何确保数据质量？
- en: How do you monitor and troubleshoot latency issues in production?
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您如何监控和解决生产中的延迟问题？
- en: How do you handle versioning and tracking datasets used in training or fine-tuning
    LLMs?
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您如何处理用于训练或微调LLM的版本控制和数据集跟踪？
- en: How do you ensure high availability and cost-efficiency in your cloud infrastructure?
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您如何确保云基础设施的高可用性和成本效益？
- en: 'Round 3: System design interview'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 第三轮：系统设计面试
- en: 'The goal in this round is to assess the candidate’s ability to design scalable,
    reliable, and maintainable systems for deploying and managing LLMs. Sample questions
    for this round could include:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一轮中，目标是评估候选人设计可扩展、可靠和可维护的系统以部署和管理LLM的能力。这一轮的示例问题可能包括：
- en: Tell me about a time when you had to make a build-versus-buy decision for a
    component of your ML infrastructure. What factors did you consider?
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请告诉我一次你必须为你的ML基础设施组件做出构建或购买决策的情况。你考虑了哪些因素？
- en: How would you design an API for serving LLM inferences at scale? Discuss considerations
    for load balancing, fault tolerance, and latency reduction.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你会如何设计一个API来大规模提供LLM推理服务？讨论负载均衡、容错和降低延迟的考虑因素。
- en: How would you use an IaC tool to manage the cloud infrastructure for an LLM
    deployment? What strategies would you employ to optimize resource usage and cost?
    What potential problems do you think you’d encounter while scaling it?
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你会如何使用IaC工具来管理LLM部署的云基础设施？你会采用哪些策略来优化资源使用和成本？你认为在扩展过程中可能会遇到哪些潜在问题？
- en: Describe your approach to dynamic batching in inference service. How do techniques
    like quantizing and mixed precision training affect the performance and efficiency
    of LLMs?
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述你在推理服务中动态批处理的方法。量化混合精度训练等技术如何影响LLM的性能和效率？
- en: How do you manage memory in CUDA when training LLMs? What strategies do you
    use to prevent issues like out-of-memory errors?
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何在CUDA中管理LLM训练时的内存？你使用哪些策略来防止内存不足错误等问题？
- en: How do you benchmark model performance before and after optimization? What metrics
    do you consider, and what tools do you use?
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何在优化前后基准测试模型性能？你考虑哪些指标，以及你使用哪些工具？
- en: How would you diagnose and address performance degradation after an optimization?
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你会如何诊断和解决优化后的性能下降问题？
- en: 'Final round: Behavioral interview'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一轮：行为面试
- en: 'In the fourth round, you’ve narrowed the pool to the most qualified candidates.
    Now you need to assess their personalities: Are they self-driven? Can they work
    in a team, handle challenges with equanimity, and contribute to a collaborative
    environment? Sample questions:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在第四轮中，你已经将候选人缩小到最合格的候选人。现在你需要评估他们的个性：他们是自我驱动的吗？他们能否团队合作，以平和的心态应对挑战，并为协作环境做出贡献？示例问题：
- en: How do you stay up-to-date on the latest advancements in LLMs and machine learning
    operations?
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何保持对LLM和机器学习操作最新进展的了解？
- en: How do you approach integrating data scientists’ feedback into the deployment
    process?
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何将数据科学家对部署过程的反馈整合到部署过程中？
- en: How would you collaborate with a red-teaming engineer to address potential security
    vulnerabilities in an LLM deployment?
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你会如何与红队工程师合作，以解决LLM部署中的潜在安全漏洞？
- en: What experience do you have with on-call rotations? How do you handle critical
    incidents during off hours?
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在值班轮换方面有什么经验？你如何在非工作时间处理关键事件？
- en: 'You’ll also want to ensure the candidate aligns with your organization’s values,
    particularly in areas like innovation, continuous learning, and collaboration.
    You can ask questions like:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你还希望确保候选人符合你组织的价值观，特别是在创新、持续学习和协作等领域。你可以问一些问题，比如：
- en: What are your favorite technology blogs or podcasts?
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你最喜欢的技术博客或播客是什么？
- en: How do you keep up with new advances in the field?
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何跟上该领域的最新进展？
- en: 'Hiring Internally: Upskilling an MLOps Engineer into an LLMOps Engineer'
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 内部招聘：将MLOps工程师提升为LLMOps工程师
- en: The gap between MLOps engineers and LLMOps engineers is significant in terms
    of the scale, complexity, and the technical challenges involved in their roles.
    Thus, upskilling an existing employee requires a focused effort to build their
    understanding.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: MLOps工程师和LLMOps工程师之间的差距在规模、复杂性和他们角色中涉及的技术挑战方面是显著的。因此，提升现有员工的技能需要集中精力来建立他们的理解。
- en: That said, the foundational skills of MLOps—such as model deployment, automation,
    and cloud management—provide a solid base from which to grow. With dedicated learning
    and hands-on experience, an MLOps engineer can transition into the LLMOps domain
    effectively. The core upside of hiring internally is that candidates are already
    aligned with the organization’s values and culture and have a keen understanding
    of its KPIs.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，MLOps的基础技能——如模型部署、自动化和云管理——提供了一个坚实的基石，可以在此基础上发展。通过专注的学习和实际经验，MLOps工程师可以有效地过渡到LLMOps领域。内部招聘的核心优势在于，候选人已经与组织的价值观和文化保持一致，并且对KPIs有深刻的理解。
- en: To excel, new LLMOps engineers need resources and training to deepen their understanding
    of large-scale model architectures and transformer architectures, attention mechanisms,
    infrastructure management, and LLM-specific optimization techniques. They also
    need to understand how LLMs differ from non-generative ML models. We recommend
    pairing them up with LLM engineers to experiment with and evaluate different models.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了脱颖而出，新的LLMOps工程师需要资源和培训来深化他们对大规模模型架构和Transformer架构、注意力机制、基础设施管理和LLM特定优化技术的理解。他们还需要了解LLMs与非生成性ML模型的不同之处。我们建议将他们与LLM工程师配对，以实验和评估不同的模型。
- en: This role isn’t just about building an app that uses LLMs. LLMOps engineers
    also manage the balance among cost, cloud resources, and user experience and handle
    huge unstructured datasets. Therefore, pair them with data engineers to help build
    a data-processing pipeline so they can familiarize themselves with the data sources
    at their disposal, how the data is structured in the databases, how different
    databases retrieve information, what the company’s latency expectations are, and
    how to handle data filtering. Allow them to introduce multi-node setups and distributed
    systems for different models while focusing on cost optimization and errors. Get
    them to benchmark different LLM models and debug their performance optimization.
    Finally, allow them to present their logging practices and the guardrails they
    have set up for maintaining reliability and performance at scale.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这个角色不仅仅是构建一个使用LLMs的应用程序。LLMOps工程师还管理成本、云资源和用户体验之间的平衡，并处理大量的非结构化数据集。因此，将他们与数据工程师配对，以帮助他们构建数据处理管道，使他们熟悉可用的数据源，数据在数据库中的结构，不同数据库如何检索信息，公司的延迟期望，以及如何处理数据过滤。允许他们为不同的模型引入多节点设置和分布式系统，同时专注于成本优化和错误处理。让他们基准测试不同的LLM模型并调试其性能优化。最后，允许他们展示他们的日志实践和为维护可扩展性和性能而设置的护栏。
- en: Most MLOps engineers already have skills in model versioning, data versioning,
    managing rollbacks, and GitHub actions, so upskilling these professionals can
    be an effective strategy for building a strong LLMOps team.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数MLOps工程师已经具备模型版本控制、数据版本控制、管理回滚和GitHub操作等技能，因此提升这些专业人士的技能可以成为构建强大LLMOps团队的有效策略。
- en: Next, let’s look at how to make sure the goals of your LLMOps engineers are
    aligned with your ​organizational goals.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何确保你的LLMOps工程师的目标与你的组织目标保持一致。
- en: LLMs and Your Organization
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs和你的组织
- en: You learned at the beginning of this chapter that the four key goals of the
    LLMOps framework are security, scalability, robustness, and reliability. For LLMOps
    teams, then, the next big question is how to measure the application’s performance
    against those goals. How will you know you’re succeeding? The company’s expectations
    should be clearly defined and remain quantitatively, as well as qualitatively,
    measurable at all times.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你在本章的开头了解到，LLMOps框架的四个关键目标是安全性、可扩展性、鲁棒性和可靠性。因此，对于LLMOps团队来说，下一个重大问题是如何衡量应用程序的性能是否符合这些目标。你将如何知道你正在取得成功？公司的期望应该明确界定，并且始终以定性和定量可衡量的方式存在。
- en: 'Three kinds of metrics will allow you to measure your team’s performance toward
    its goals: SLOs, SLAs, and KPIs. These terms are in common usage among site reliability
    engineers, and now LLMOps teams are rapidly adopting them as well:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 三种类型的指标将允许你衡量你团队的目标绩效：SLOs、SLAs和KPIs。这些术语在站点可靠性工程师中普遍使用，现在LLMOps团队也在迅速采用它们：
- en: Service-level objectives (SLOs)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 服务级别目标（SLOs）
- en: Service-level objectives are specific, measurable targets set by an organization
    to gauge the quality of its services internally. They define what level of service
    the organization aims to achieve. For example, an SLO for a cloud-hosting company
    might be to ensure that server uptime is at least 99.9% per month.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 服务级别目标是由组织设定的具体、可衡量的目标，用于衡量其服务的内部质量。它们定义了组织希望达到的服务水平。例如，对于一家云托管公司，其服务级别目标可能是确保服务器每月至少99.9%的时间在线。
- en: Service-level agreements (SLAs)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 服务级别协议（SLAs）
- en: Service-level agreements are formal contracts between a service provider and
    a customer that define the level of service the provider commits to deliver. They
    typically include specific performance goals and stipulate remedies if those metrics
    are not met. For example, if the uptime for an internet provider falls below 99.9%
    annually, the SLA might stipulate that the customer will receive a 10% discount
    on its next billing cycle.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 服务级别协议（SLA）是服务提供商和客户之间正式的合同，定义了提供商承诺提供的服务水平。它们通常包括具体的绩效目标，并规定如果未达到这些指标，将采取的补救措施。例如，如果互联网提供商的年正常运行时间低于99.9%，SLA可能会规定客户在下一次账单周期中将获得10%的折扣。
- en: Key performance indicators (KPIs)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 关键绩效指标（KPIs）
- en: Key performance indicators measure the overall success and performance of specific
    business activities. They provide insight into how well the organization is achieving
    its strategic objectives. For example, an important KPI for an app might be its
    churn rate or the percentage of customers who stop using the app over a certain
    period.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 关键绩效指标衡量特定商业活动的整体成功和绩效。它们提供了洞察力，了解组织如何实现其战略目标。例如，一个应用程序的重要KPI可能是其流失率或一定时期内停止使用应用程序的客户百分比。
- en: In August 2024, a [Gartner Research study predicted](https://oreil.ly/dIBVc)
    that 30% of existing GenAI projects would fail by 2025\. (In fact, Gartner [published
    similar findings](https://oreil.ly/Rq3K3) in 2018, predicting that 85% of ML projects
    would fail in production by 2022.) The key failure points outlined in the 2024
    study are notable because they are the operational aspects of LLM development
    and deployment—including data quality issues, the lack of a strong evaluation
    framework, and the high costs of scaling these models in production.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在2024年8月，[Gartner Research的研究报告预测](https://oreil.ly/dIBVc)到2025年将有30%的现有生成式人工智能（GenAI）项目失败。（实际上，Gartner在2018年[发布了类似的研究结果](https://oreil.ly/Rq3K3)，预测到2022年将有85%的机器学习（ML）项目在生产中失败。）2024年研究中概述的关键失败点值得关注，因为这些是LLM开发和部署的操作方面——包括数据质量问题、缺乏强大的评估框架以及在生产中扩展这些模型的高成本。
- en: That said, one of the most obvious issues is mismatched expectations between
    management and engineering teams. For the last 10 years, one of data scientists’
    biggest skill gaps has been in translating ML model metrics into organizational
    and product success metrics. In other words, when you’re measuring abstract goals
    like model security, scalability, robustness, and reliability, how do you communicate
    what this means for the business? That’s what SLOs, SLAs, and KPIs are for.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，最明显的问题之一是管理层和工程团队之间的期望不匹配。在过去10年中，数据科学家最大的技能差距之一就是将机器学习模型指标转化为组织和产品成功指标。换句话说，当你在衡量抽象目标，如模型安全性、可扩展性、鲁棒性和可靠性时，你如何传达这对业务意味着什么？这正是SLOs、SLAs和KPIs的作用所在。
- en: Using an SLO-SLA-KPI framework allows LLMOps teams to automate, streamline,
    and manage expectations across multiple stakeholders. SLOs make it evident to
    all stakeholders what level of service is being aimed for. SLAs ensure accountability
    so that everyone involved is aware of their roles and the agreed-upon service
    levels. This can also help you track performance and address any deviations from
    the expected standards. And KPIs provide visibility into real-time data to help
    detect potential issues early, facilitating informed decision-making.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SLO-SLA-KPI框架允许LLMOps团队自动化、简化和管理多个利益相关者的期望。SLO使所有利益相关者都能清楚地了解所追求的服务水平。SLA确保问责制，使所有参与人员都了解他们的角色和约定的服务级别。这也有助于您跟踪绩效并解决任何与预期标准不符的情况。KPIs则提供了实时数据的可见性，有助于早期发现潜在问题，从而促进明智的决策。
- en: The Four Goals of LLMOps
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMOps的四个目标
- en: Let’s look more closely at the LLMOps goals to see how these metrics translate.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看LLMOps的目标，看看这些指标是如何转换的。
- en: Reliability
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可靠性
- en: As you know well by now, LLMs are extremely complex, with billions of parameters.
    Their behavior can be unpredictable, and they sometimes exhibit unexpected responses
    or errors due to their scale and the intricacies of their training data. Additionally,
    if the training data is biased, outdated, or unrepresentative of certain domains,
    the model’s performance can be unreliable in those areas.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，LLMs极其复杂，拥有数十亿个参数。它们的行为可能不可预测，并且由于规模和训练数据的复杂性，有时会表现出意外的响应或错误。此外，如果训练数据存在偏差、过时或无法代表某些领域，模型在这些领域的性能可能不可靠。
- en: LLMs also struggle at times with understanding the context, nuance, and intent
    behind user queries. This can lead to incorrect, irrelevant, or misleading responses.
    What’s more, LLMs usually aren’t updated in real time. As language evolves, if
    new information is not integrated into the training data via regular retraining,
    models can become outdated, leading to decreased reliability.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs有时也难以理解用户查询背后的上下文、细微差别和意图。这可能导致不正确、不相关或误导性的响应。更重要的是，LLMs通常不会实时更新。随着语言的发展，如果新信息没有通过定期重新训练整合到训练数据中，模型可能会过时，导致可靠性下降。
- en: All of these issues come down to reliability. The reliability of LLM-based applications
    can be measured in terms of system availability, error rates, and customer satisfaction.
    [Table 2-2](#ch02_table_2_1748895480195314) shows how these metrics would look
    as SLOs, SLAs, and KPIs.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题都归结为可靠性。基于LLM的应用程序的可靠性可以通过系统可用性、错误率和客户满意度来衡量。[表2-2](#ch02_table_2_1748895480195314)显示了这些指标作为SLOs、SLAs和KPIs时的样子。
- en: Table 2-2\. Example reliability comparison of SLOs, SLAs, and KPIs
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-2\. SLOs、SLAs和KPIs的示例可靠性比较
- en: '|   | SLO | SLA | KPI |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|   | SLO | SLA | KPI |'
- en: '| --- | --- | --- | --- |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Availability | Maintain 99.9% uptime per month | Ensure that the system is
    available for at least 99.95% of requests over a rolling 30-day period | Customer
    satisfaction score (CSAT) related to system availability |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 可用性 | 每月保持99.9%的在线时间 | 确保系统在30天的滚动期间至少对99.95%的请求可用 | 与系统可用性相关的客户满意度评分（CSAT）
    |'
- en: '| Error rate | Keep the error rate below 0.1% for all API requests | Ensure
    that less than 1% of user interactions result in errors | Track error rate trends
    over time and analyze root causes of major errors |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 错误率 | 确保所有API请求的错误率低于0.1% | 确保用户交互中少于1%的结果是错误 | 跟踪错误率趋势并分析主要错误的根本原因 |'
- en: '| Customer satisfaction | CSAT score of at least 90% | Ensure that the net
    promoter score (NPS) remains above 8 | Post-interaction surveys or feedback forms;
    CSAT score |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 客户满意度 | CSAT评分至少为90% | 确保净推荐值（NPS）保持在8以上 | 交互后调查或反馈表；CSAT评分 |'
- en: Scalability
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可扩展性
- en: LLMs tend to have a large memory footprint, often exceeding the capacity of
    a single machine. Distributing a model across multiple GPUs or nodes while maintaining
    performance is technically challenging. Handling large volumes of data efficiently
    is critical.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs往往具有很大的内存占用，通常超过单个机器的容量。在保持性能的同时，将模型分布到多个GPU或节点在技术上具有挑战性。高效处理大量数据至关重要。
- en: Another significant challenge is scaling the data pipeline to feed data into
    the model at the required speed without causing bottlenecks. This can be especially
    hard for applications like chatbots or interactive services, where low latency
    is crucial. Scaling while keeping response times low can be challenging, since
    scaling up increases resource contention and network overhead. Therefore, balancing
    performance with cost efficiency is a constant concern.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重大挑战是在不造成瓶颈的情况下扩展数据管道，以便以所需的速度将数据输入模型。这对于像聊天机器人或交互式服务这样的应用尤其困难，在这些应用中，低延迟至关重要。在保持低响应时间的同时进行扩展可能具有挑战性，因为扩展会增加资源竞争和网络开销。因此，平衡性能与成本效益是一个持续关注的问题。
- en: LLMOps scalability can be measured via metrics like latency, throughput, response
    time, resource scaling, capacity planning, and recovery time objective (RTO),
    as shown in [Table 2-3](#ch02_table_3_1748895480195328).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps的可扩展性可以通过延迟、吞吐量、响应时间、资源扩展、容量规划和恢复时间目标（RTO）等指标来衡量，如[表2-3](#ch02_table_3_1748895480195328)所示。
- en: Table 2-3\. Example scalability comparison of SLOs, SLAs, and KPIs
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-3\. SLOs、SLAs和KPIs的示例可扩展性比较
- en: '|   | SLO | SLA | KPI |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '|   | SLO | SLA | KPI |'
- en: '| --- | --- | --- | --- |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Latency | Serve 95% of requests within 200 milliseconds | Keep the average
    response time for API calls under 100 milliseconds | Average response time for
    user interactions |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 延迟 | 在200毫秒内服务95%的请求 | 保持API调用的平均响应时间低于100毫秒 | 用户交互的平均响应时间 |'
- en: '| Throughput | Process a minimum of 1,000 requests per second during peak traffic
    hours | Handle at least 1 million concurrent connections without degradation |
    Peak throughput capacity during load testing |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 吞吐量 | 在高峰时段每秒至少处理1,000个请求 | 在不降级的情况下处理至少1百万个并发连接 | 负载测试期间的峰值吞吐量 |'
- en: '| Response time | Maintain a web page load time of under 3 seconds for 95%
    of users | Ensure that the login process completes within 500 milliseconds for
    99% of users | User experience metrics related to response times |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 响应时间 | 保持95%用户的网页加载时间低于3秒 | 确保登录过程在99%的用户中在500毫秒内完成 | 与响应时间相关的用户体验指标 |'
- en: '| Resource scaling | Automatically scale up resources to handle a 50% increase
    in traffic within 5 minutes | Ensure that adding servers linearly increases throughput
    without impacting latency | Scalability test results and cost-effectiveness of
    scaling solutions |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 资源扩展 | 在5分钟内自动扩展资源以处理流量增加50% | 确保添加服务器线性增加吞吐量而不影响延迟 | 可扩展性测试结果和扩展解决方案的成本效益
    |'
- en: '| Capacity planning | Maintain CPU utilization below 80% during peak hours
    | Ensure that enough database connections are available to handle double the anticipated
    peak load | Resource utilization trends and forecasting accuracy |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 容量规划 | 在高峰时段保持CPU利用率低于80% | 确保有足够的数据库连接来处理预期峰值负载的两倍 | 资源利用趋势和预测准确性 |'
- en: '| RTO | Achieve a recovery time objective of under 30 minutes for critical
    system failures | Ensure that the system can recover from a database failure and
    restore service within 15 minutes | Historical RTO metrics and improvement initiatives
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| RTO | 实现关键系统故障的恢复时间目标低于30分钟 | 确保系统可以从数据库故障中恢复并在15分钟内恢复服务 | 历史RTO指标和改进措施 |'
- en: Robustness
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 坚韧性
- en: Over time, the statistical properties of the model’s training data can change,
    leading to a drift in the model’s performance. This is particularly problematic
    for models that interact with real-time or rapidly changing data. This can lead
    to performance degradation in the form of outdated or irrelevant responses.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，模型训练数据的统计特性可能会发生变化，导致模型性能的漂移。这对于与实时或快速变化的数据交互的模型尤其成问题。这可能导致以过时或不相关响应的形式的性能下降。
- en: Continuous training and fine-tuning are necessary to maintain robustness, but
    they require significant computational resources and careful management to avoid
    introducing new biases or errors. You can measure robustness via metrics like
    data freshness, model evaluation, and consistency, as shown in [Table 2-4](#ch02_table_4_1748895480195338).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 持续的训练和微调对于保持坚韧性是必要的，但它们需要大量的计算资源以及谨慎的管理以避免引入新的偏差或错误。您可以通过数据新鲜度、模型评估和一致性等指标来衡量坚韧性，如[表2-4](#ch02_table_4_1748895480195338)所示。
- en: Table 2-4\. Example robustness comparison of SLOs, SLAs, and KPIs
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-4. SLO、SLA和KPI的坚韧性比较示例
- en: '|   | SLO | SLA | KPI |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|   | SLO | SLA | KPI |'
- en: '| --- | --- | --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Data freshness | Ensure that dashboard data is refreshed every 5 minutes
    | Guarantee that data is updated in real time | Data refresh latency and accuracy
    of real-time data updates |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 数据新鲜度 | 确保仪表板数据每5分钟刷新一次 | 保证数据实时更新 | 数据刷新延迟和实时数据更新的准确性 |'
- en: '| Model evaluation | Maintain a performance degradation rate of less than 5%
    over 6 months | Guarantee regular updates and reviews of model evaluation metrics
    | Accuracy, relevance, and update frequency of evaluation metrics |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 模型评估 | 在6个月内保持性能下降率低于5% | 保证模型评估指标的定期更新和审查 | 评估指标的准确性、相关性和更新频率 |'
- en: '| Consistency | Guarantee strong consistency for data reads and writes across
    all regions | Maintain eventual consistency with a maximum propagation delay of
    1 second | Consistency model adherence and replication latency |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 一致性 | 保证跨所有区域的数据读取和写入的强一致性 | 保持最大传播延迟为1秒的最终一致性 | 一致性模型遵守和复制延迟 |'
- en: Security
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全性
- en: 'Maintaining LLM application security is challenging: these are complex models
    handling sensitive data in the face of constantly evolving security threats. LLMs
    are especially vulnerable to adversarial attacks, data poisoning, and other forms
    of exploitation that can compromise their integrity and security.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 维护LLM应用的安全性具有挑战性：这些模型在面临不断演变的威胁时处理敏感数据。LLM尤其容易受到对抗性攻击、数据中毒和其他形式的利用，这可能会损害其完整性和安全性。
- en: Managing and controlling access to the LLM and its data is complicated, especially
    in a multi-access or multi-tenant environment, but it’s critical for preventing
    unauthorized access and misuse. [Table 2-5](#ch02_table_5_1748895480195348) shows
    some ways to measure it.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 管理和控制对LLM及其数据的访问是复杂的，尤其是在多访问或多租户环境中，但对于防止未经授权的访问和滥用至关重要。[表2-5](#ch02_table_5_1748895480195348)展示了测量的一些方法。
- en: Table 2-5\. Example security comparison of SLOs, SLAs, and KPIs
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-5. SLOs、SLAs 和 KPIs 的示例安全比较
- en: '|   | SLO | SLA | KPI |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '|   | SLO | SLA | KPI |'
- en: '| --- | --- | --- | --- |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Data privacy | Ensure data encryption for all in-transit and at-rest data
    | Ensure no breaches occur | Encryption compliance status |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 数据隐私 | 确保所有传输中和静止数据的数据加密 | 确保没有发生违规行为 | 加密合规状态 |'
- en: '| Model integrity | Detect and address any model tampering within 24 hours
    | Guarantee prompt detection of and response to unauthorized modifications | Number
    of unauthorized modifications detected |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 模型完整性 | 在 24 小时内检测并解决任何模型篡改 | 保证及时检测和响应未经授权的修改 | 检测到的未经授权修改的数量 |'
- en: '| Access control | Achieve a user authentication success rate of 99.9% | Ensure
    robust user authentication and authorization mechanisms | Rate of unauthorized
    access attempts |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 访问控制 | 实现用户身份验证成功率 99.9% | 确保强大的用户身份验证和授权机制 | 未授权访问尝试的比率 |'
- en: '| Red teaming | Ensure detection of 99.9% of attempted adversarial attacks
    | Ensure regular security assessments and updates | Frequency of security assessments
    and the number of critical vulnerabilities identified |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 红队行动 | 确保检测到 99.9% 的尝试性对抗攻击 | 确保定期进行安全评估和更新 | 安全评估的频率和识别出的关键漏洞数量 |'
- en: When all teams—whether they are involved in development, operations, or management—understand
    the agreed-upon service levels and performance indicators, they can work together
    more effectively toward common goals. This alignment fosters a unified approach
    to managing and improving project performance. It also helps in making data-backed
    decisions about resource allocation, process changes, and strategic adjustments.
    Most importantly, it helps in building trust and ensuring that everyone is on
    the same page regarding expectations and outcomes.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有团队——无论是参与开发、运营还是管理——都理解了约定的服务级别和性能指标时，他们可以更有效地共同朝着共同目标工作。这种一致性促进了管理项目性能的统一方法。它还有助于做出基于数据的关于资源配置、流程变更和战略调整的决策。最重要的是，它有助于建立信任，确保每个人都对期望和结果保持一致。
- en: Overall, implementing an SLO-SLA-KPI framework​ not only enhances transparency
    and fosters collaboration, but it also serves as a foundational element in evaluating
    and advancing the maturity of your LLMOps practices, which is the topic of this
    chapter’s final section.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，实施 SLO-SLA-KPI 框架不仅增强了透明度和促进了协作，而且还作为评估和提升您 LLMOps 实践成熟度的基础要素，这也是本章最后部分的主题。
- en: The LLMOps Maturity Model
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMOps 成熟度模型
- en: LLMOps maturity is a way of determining how well an organization’s LLM operations
    align with industry best practices and standards. Assessing LLMOps maturity helps
    organizations identify their strengths, areas for improvement, and opportunities
    for scaling and enhancing the robustness of their LLM systems.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: LLMOps 成熟度是一种确定组织 LLM 运营与行业最佳实践和标准一致性的方法。评估 LLMOps 成熟度有助于组织识别其优势、改进领域和扩展及增强其
    LLM 系统鲁棒性的机会。
- en: A few years ago, Microsoft published a [machine learning operations maturity
    model](https://oreil.ly/1Tjgw) detailing a progressive set of requirements and
    stages to measure the maturity of MLOps production environments and processes.
    The LLMOps maturity model we present here, inspired by Microsoft’s MLOps model,
    is meant to do the same for LLMOps teams. Although this is by no means a comprehensive
    audit, we hope to see several variations put into practice.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，微软发布了一个 [机器学习运营成熟度模型](https://oreil.ly/1Tjgw)，详细说明了衡量 MLOps 生产环境和流程成熟度的逐步要求和阶段。我们在这里提出的
    LLMOps 成熟度模型，受微软 MLOps 模型的启发，旨在为 LLMOps 团队做同样的事情。虽然这绝对不是一项全面的审计，但我们希望看到几种变体得到实践。
- en: 'The three LLMOps maturity levels are as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 三种 LLMOps 成熟度级别如下：
- en: Level 0
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Level 0
- en: No LLMOps practices are implemented. The organization’s lack of formal structures
    and processes for managing and deploying its LLM systems hinders its effectiveness.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 没有实施 LLMOps 实践。组织缺乏管理和部署其 LLM 系统的正式结构和流程，这阻碍了其有效性。
- en: Level 1
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Level 1
- en: The organization applies MLOps practices but without LLM-specific adaptations.
    This is an improvement over Level 0 in terms of formalization and processes but
    still lacks the sophistication needed for full LLM operations.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 组织应用了 MLOps 实践，但没有针对 LLM 特定的调整。这在形式化和流程方面比 Level 0 有所改进，但仍然缺乏全面 LLM 运营所需的复杂性。
- en: Level 2
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Level 2
- en: Achieving Level 2 represents a mature LLMOps state, characterized by advanced
    documentation, robust monitoring and compliance measures, and the integration
    of sophisticated orchestration and human review strategies. Usually, this can
    be assessed by asking some questions about whether decision strategies and model
    performance measures and metrics are well documented within the team.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 达到级别 2 代表成熟的 LLMOps 状态，其特征是先进的文档、稳健的监控和合规措施，以及复杂的编排和人工审查策略的整合。通常，这可以通过询问关于决策策略、模型性能指标和度量是否在团队中得到良好记录的问题来评估。
- en: Various measures of LLMOps maturity levels are outlined in [Table 2-6](#ch02_table_6_1748895480195357).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [表 2-6](#ch02_table_6_1748895480195357) 中概述了 LLMOps 成熟度的各种措施。
- en: Table 2-6\. Documentation and strategy measures of LLMOps maturity
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-6\. LLMOps 成熟度的文档和策略措施
- en: '|   | Level 0: No LLMOps | Level 1: MLOps, no LLMOps | Level 2: Full LLMOps
    |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|   | 级别 0：无 LLMOps | 级别 1：MLOps，无 LLMOps | 级别 2：完整 LLMOps |'
- en: '| --- | --- | --- | --- |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Are the business goals and KPIs of the LLM project documented and kept up
    to date? | Not documented | There is documentation, but it’s often outdated |
    Full documentation with regular updates; KPIs include model performance metrics,
    operational efficiency, and cost-effectiveness |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| LLM 项目的业务目标和 KPI 是否已记录并保持最新？ | 未记录 | 有记录，但通常过时 | 完整记录并定期更新；KPI 包括模型性能指标、运营效率和成本效益
    |'
- en: '| Are LLM model risk evaluation metrics documented? | No formal risk assessment
    | Basic risk evaluation for model accuracy and data security | Comprehensive risk
    evaluation including bias, fairness, data drift, and performance degradation with
    mitigation strategies in place |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| LLM 模型风险评估指标是否已记录？ | 没有正式的风险评估 | 对模型准确性和数据安全进行基本风险评估 | 包括偏差、公平性、数据漂移和性能退化在内的全面风险评估，并实施缓解策略
    |'
- en: '| Is there a documented and regularly updated overview of all team members
    involved in the project, along with their responsibilities? | No documentation
    | High-level roles are documented, but responsibilities may be unclear for the
    newer roles | Detailed team structure with roles, responsibilities, and contact
    details that is regularly reviewed and updated |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 是否有关于所有参与项目的团队成员及其职责的记录和定期更新？ | 没有记录 | 高级角色有记录，但新角色的职责可能不明确 | 详细团队结构，包括角色、职责和联系信息，定期审查和更新
    |'
- en: '| Is the choice of LLM well documented and cost-compared against other open
    source/proprietary offerings? | No documentation or cost analysis | Basic documentation
    of LLM choice, minimal cost comparison | Detailed documentation including rationale
    for choice, performance benchmarks, and cost comparison against alternative models
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| LLM 的选择是否记录良好，并与其他开源/专有产品进行了成本比较？ | 没有记录或成本分析 | LLM 选择有基本记录，成本比较最少 | 包括选择理由、性能基准和与替代模型成本比较的详细记录
    |'
- en: '| Is the API for the model vendor well documented, including request and response
    structure, data types, and other relevant details? | No API documentation | Model
    developed in-house | Comprehensive API documentation, including request/response
    examples, data types, error codes, and versioning details |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 模型供应商的 API 是否有良好的记录，包括请求和响应结构、数据类型和其他相关细节？ | 没有 API 文档 | 模型内部开发 | 包括请求/响应示例、数据类型、错误代码和版本详情的全面
    API 文档 |'
- en: '| Is the software architecture well documented and kept up to date? | No documentation
    | There is a high-level architecture overview, but it may be outdated | Detailed
    architecture diagrams including data flow, system components, and integration
    points that are updated regularly |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 软件架构是否记录良好并保持最新？ | 没有记录 | 有高级架构概述，但可能过时 | 包括数据流、系统组件和集成点的详细架构图，定期更新 |'
- en: Documenting the factors shown in [Table 2-6](#ch02_table_6_1748895480195357)
    can be incredibly helpful when choosing and deploying any new model. For example,
    given the significant costs that are associated with deploying an LLM application,
    cost-benchmark analysis documentation allows the company to decide which model
    to roll into production and estimate the project timeline.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 记录 [表 2-6](#ch02_table_6_1748895480195357) 中显示的因素在选择和部署任何新模型时可能非常有帮助。例如，考虑到部署
    LLM 应用程序相关的显著成本，成本基准分析文档允许公司决定哪个模型投入生产，并估计项目时间表。
- en: After deployment, the company also needs to assess how well the team has documented
    the model’s performance measures and metrics. This is to make sure that everyone
    on the team understands the expectations and that they are comprehensively monitoring
    the model performance in production.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 部署后，公司还需要评估团队如何记录模型性能指标和度量。这是为了确保团队中的每个人都理解期望，并且全面监控生产中的模型性能。
- en: '[Table 2-7](#ch02_table_7_1748895480195367) outlines three levels of LLMOps
    maturity with regard to model performance and evaluation. Keeping these different
    levels in mind, organizations and the LLMOps team will be better prepared to deal
    with contingencies and can better align projects with business goals, mitigate
    risks, and enhance operational efficiency.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[表2-7](#ch02_table_7_1748895480195367)概述了关于模型性能和评估的LLMOps成熟度的三个级别。考虑到这些不同的级别，组织和LLMOps团队将更好地准备应对突发事件，并能更好地将项目与业务目标对齐，降低风险，并提高运营效率。'
- en: Table 2-7\. Model performance and evaluation measures of LLMOps maturity
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-7\. LLMOps成熟度的模型性能和评估措施
- en: '|   | Level 0: No LLMOps | Level 1: MLOps, no LLMOps | Level 2: Full LLMOps
    |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|   | 级别0：无LLMOps | 级别1：MLOps，无LLMOps | 级别2：完整的LLMOps |'
- en: '| --- | --- | --- | --- |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Does the LLM system operate within its knowledge limits, and recognize when
    it is operating outside those limits? | No mechanisms to detect limits | Basic
    detection of operational limits | Advanced guardrails, limit detection mechanisms,
    and documentation of context-aware warnings using techniques like confidence scoring
    and thresholding |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| LLM系统是否在其知识限制内运行，并能够识别其是否超出这些限制？ | 无检测机制 | 基本检测操作限制 | 高级防护栏、限制检测机制，以及使用置信度评分和阈值等技术记录上下文感知警告的文档
    |'
- en: '| Are the LLM’s inputs and outputs automatically stored? | No automatic storage
    | Basic storage of inputs and outputs | Automated storage of all inputs and outputs
    with indexing for easy retrieval and analysis |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| LLM的输入和输出是否自动存储？ | 无自动存储 | 基本存储输入和输出 | 所有输入和输出的自动存储，并带有索引以便于检索和分析 |'
- en: '| Is A/B testing performed regularly? | No A/B testing | Occasional A/B testing
    with limited coverage | Regular A/B testing with comprehensive test coverage and
    analysis, using tools like Optimizely or custom frameworks |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 是否定期进行A/B测试？ | 无A/B测试 | 有时进行有限的A/B测试 | 定期进行全面的A/B测试，包括使用Optimizely或自定义框架等工具的测试覆盖和分析
    |'
- en: '| Are all API requests and responses logged, and are API response time and
    health status monitored? | No logging or monitoring | Basic logging and response
    time monitoring | Comprehensive logging with detailed request/response analysis;
    real-time health monitoring using tools like ELK stack |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 是否记录了所有API请求和响应，并监控API响应时间和健康状态？ | 无记录或监控 | 基本记录和响应时间监控 | 综合记录，包括详细的请求/响应分析；使用ELK堆栈等工具进行实时健康监控
    |'
- en: '| Is the LLM monitored for toxicity and bias? | No outlier detection or bias
    monitoring | Basic outlier detection with manual review | Advanced automated toxicity
    and bias detection pipelines using statistical methods and regular bias audits,
    with automated alerting for low-confidence predictions |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 是否对LLM进行毒性偏差监控？ | 无异常检测或偏差监控 | 基本异常检测，并手动审查 | 使用统计方法进行高级自动化毒性和偏差检测管道，定期进行偏差审计，并对低置信度预测进行自动警报
    |'
- en: '| Are processes in place to ensure that LLM operations comply with regulations
    such as GDPR, HIPAA, and other relevant data protection laws? | No process exists
    | Process to ensure that LLM operations comply with regulations such as GDPR,
    HIPAA, or other relevant data protection laws | Process to ensure that LLM operations
    comply with regulations such as GDPR, HIPAA, or other relevant data collection
    and protection laws and copyright laws |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 是否有流程确保LLM操作符合GDPR、HIPAA和其他相关数据保护法规？ | 没有流程 | 确保LLM操作符合GDPR、HIPAA或其他相关数据保护法规的流程
    | 确保LLM操作符合GDPR、HIPAA或其他相关数据收集和保护法规以及版权法律的流程 |'
- en: '| Does the LLM-based app use anonymization to protect users’ identities while
    maintaining the data’s utility for LLMs? | No anonymization | Basic anonymization
    techniques applied | Advanced automated anonymization methods, including data
    masking and aggregations |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 基于LLM的应用程序是否使用匿名化来保护用户身份，同时保持数据对LLM的效用？ | 无匿名化 | 应用基本匿名化技术 | 高级自动化匿名化方法，包括数据掩码和聚合
    |'
- en: '| Does the organization perform regular security reviews and audits of LLM
    infrastructure and code? | No regular reviews | Periodic security reviews and
    audits | Regular, comprehensive security reviews and audits, including third-party
    assessments and vulnerability scans |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 组织是否对LLM基础设施和代码进行定期的安全审查和审计？ | 没有定期审查 | 定期安全审查和审计 | 定期、全面的安全审查和审计，包括第三方评估和漏洞扫描
    |'
- en: 'Let’s look at these levels in more detail:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看这些级别：
- en: 'Level 0: No LLMOps'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 第0级：无LLMOps
- en: Machine learning efforts are often isolated and experimental, and they lack
    any systematic deployment and monitoring framework. The models may be developed
    in silos, often resulting in unreliability and inefficiency. Chevrolet’s [chatbot
    blunder](https://oreil.ly/CtHrG) is an excellent example; due to a lack of monitoring
    and guardrails, the app was abused by the community for algebra homework. It also
    offered Chevrolet cars in no-take-backsies deals and promoted Tesla cars instead.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作通常孤立且实验性，缺乏任何系统性的部署和监控框架。模型可能在孤岛中开发，通常导致不可靠和低效。雪佛兰的[聊天机器人错误](https://oreil.ly/CtHrG)是一个很好的例子；由于缺乏监控和护栏，该应用程序被社区用于代数作业。它还提供了无退货的雪佛兰汽车交易，并推广了特斯拉汽车。
- en: 'Level 1: MLOps, no LLMOps'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 第1级：MLOps，无LLMOps
- en: The organization is likely to have a robust pipeline for model training, testing,
    and deployment, with automated monitoring and retraining workflows. However, this
    setup is designed to build for small models and is not fully optimized for the
    specific challenges of LLMs.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 该组织可能拥有一个强大的模型训练、测试和部署流程，包括自动监控和再训练工作流程。然而，这个设置是为了构建小型模型而设计的，并不完全针对LLM的具体挑战进行优化。
- en: 'Level 2: Full LLMOps'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 第2级：全面LLMOps
- en: At the highest level of maturity, the organization has adopted LLMOps practices
    and is fully optimized for LLM applications. Its infrastructure is capable of
    handling large-scale LLM deployments, fine-tuning, real-time inference, auto-scaling,
    and resource management. Mature LLMOps teams have failover and rollback mechanisms
    in place and can act quickly if the updated model underperforms after deployment.
    The organization can deliver more reliable responses, get a good ROI, and reduce
    operational risks.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高成熟度级别，组织已采用LLMOps实践，并完全针对LLM应用进行优化。其基础设施能够处理大规模LLM部署、微调、实时推理、自动扩展和资源管理。成熟的LLMOps团队已建立故障转移和回滚机制，如果部署后的更新模型表现不佳，可以迅速采取行动。组织可以提供更可靠的响应，获得良好的投资回报率，并降低运营风险。
- en: Conclusion
  id: totrans-272
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In this chapter, we discussed the team structure for organizations building
    LLM applications. We discussed various roles and how to build a highly effective
    team. Finally, we discussed a framework for typing the LLM performance metrics
    with the business KPIs. In the next chapter, we will talk about how LLMs have
    changed the data engineering landscape, and we’ll show you how to build performant
    data pipelines for ​LLMs.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了构建LLM应用的组织团队结构。我们讨论了各种角色以及如何构建一个高效团队。最后，我们讨论了一个框架，用于将LLM性能指标与业务KPI相结合。在下一章中，我们将讨论LLM如何改变数据工程领域，并展示如何为LLM构建高性能数据管道。
- en: References
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Azure Machine Learning. [“Machine Learning Operations Maturity Model”](https://oreil.ly/EebdS),
    Learn Azure, accessed May 21, 2025.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Azure机器学习。[“机器学习运营成熟度模型”](https://oreil.ly/EebdS)，学习Azure，访问日期：2025年5月21日。
- en: Friedman, Itamar. [“Software 3.0—The Era of Intelligent Software Development”](https://oreil.ly/AtCSq),
    *Medium*, May 3, 2022.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Friedman, Itamar。[“软件3.0——智能软件开发时代”](https://oreil.ly/AtCSq)，*Medium*，2022年5月3日。
- en: 'Lin, Chin-Yew. [“ROUGE: A Package for Automatic Evaluation of Summaries”](https://oreil.ly/IvSev),
    *Text Summarization Branches Out*, (Association for Computational Linguistics,
    2024).'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: Lin, Chin-Yew。[“ROUGE：自动评估摘要的包”](https://oreil.ly/IvSev)，*文本摘要分支*，（计算语言学协会，2024年）。
- en: 'Kadambi, Sreedher. [“Shingo Principles: Bridging Lean and Toyota Production
    System Success”](https://oreil.ly/wCmAE). Skil Global, May 28, 2021.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Kadambi, Sreedher。[“Shingo原则：连接精益和丰田生产系统成功”](https://oreil.ly/wCmAE)，Skil Global，2021年5月28日。
- en: 'Mcintyre, Branden. [“Chevy Chatbot Misfire: A Case Study in LLM Guardrails
    and Best Practices”](https://oreil.ly/VQHov), *Medium*, December 22, 2023.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Mcintyre, Branden。[“雪佛兰聊天机器人失误：LLM护栏和最佳实践的案例研究”](https://oreil.ly/VQHov)，*Medium*，2023年12月22日。
- en: 'Papineni, Kishore, et al. [“BLEU: A Method for Automatic Evaluation of Machine
    Translation”](https://oreil.ly/zyIEO), *ACL ’02: Proceedings of the 40th Annual
    Meeting of the Association for Computational Linguistics*, edited by Pierre Isabelle,
    Eugene Charniak, and Dekang Lin (Association for Computational Linguistics, 2002).'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 'Papineni, Kishore, 等人. [“BLEU: 一种用于机器翻译自动评估的方法”](https://oreil.ly/zyIEO), *ACL
    ''02: 计算语言学协会第40届年度会议论文集*，由Pierre Isabelle，Eugene Charniak和Dekang Lin编辑（计算语言学协会，2002年）。'
- en: Further Reading
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Shingo, Shigeo. *Zero Quality Control: Source Inspection and the Poka-Yoke
    System*, (Routledge, 2021).'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: Shingo, Shigeo. *零缺陷控制：源头检验和误操作预防系统*，（Routledge，2021年）。
- en: 'Tennant, Geoff. *Six Sigma: SPC and TQM in Manufacturing and Services*, (Routledge,
    2001).'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: Tennant, Geoff. *六西格玛：制造业和服务业的统计过程控制（SPC）和全面质量管理（TQM）*，（Routledge，2001年）。
