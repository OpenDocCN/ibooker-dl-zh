- en: Chapter 2\. Introduction to LLMOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The size and complexity of LLMs’ architecture can make productionizing these
    models incredibly hard. *Productionizing* means not just deploying a model but
    also monitoring it, evaluating it, and optimizing its performance.
  prefs: []
  type: TYPE_NORMAL
- en: There are constantly new challenges. Depending on your application, these may
    include how to process data, how to store and dynamically adapt prompts, how to
    monitor user interaction, and—most pressing—how to prevent the model from spreading
    misinformation or memorizing training data (which can lead it to release personal
    information). That’s why operationalizing LLMs, which means managing them day-to-day
    in production, requires a new framework.
  prefs: []
  type: TYPE_NORMAL
- en: '*LLMOps*, as it’s called, is an operational framework for putting LLM applications
    in production. Although its name and principles are inspired by its older siblings,
    MLOps and DevOps, LLMOps is significantly more nuanced. The LLMOps framework can
    help companies reduce technical debt, maintain compliance, deal with LLMs’ dynamic
    and experimental nature, and minimize operational and reputational risk by avoiding
    common pitfalls.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter starts by discussing what LLMOps is and how and where it departs
    from MLOps. We’ll then introduce you to the LLMOps engineer role and where it
    fits into existing ML teams. From there, we’ll look at how to measure LLMOps readiness
    within teams, assess your organization’s LLMOps maturity, and identify crucial
    KPIs for measuring success. Toward the end of this chapter, we will outline some
    challenges that are specific to productionizing LLM applications.
  prefs: []
  type: TYPE_NORMAL
- en: What Are Operational Frameworks?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Operational frameworks* provide a structured approach to managing complex
    workflows and pipelines within an organization. These frameworks integrate tools
    and practices to automate and streamline organizational processes and ensure consistency
    and quality across the project lifecycle.'
  prefs: []
  type: TYPE_NORMAL
- en: Some of the earliest operational frameworks can be traced back to military strategy
    and the Industrial Revolution. Two of the most popular ones, both introduced in
    1986, are [Toyota’s Lean Production System](https://oreil.ly/oNheh), which put
    Toyota ahead of most of its contemporaries, and [Six Sigma](https://oreil.ly/GHXWt),
    Motorola’s data-driven approach to improving processes and reducing defects.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2008, the tech industry began to adopt what is now one of the most popular
    operational frameworks in software: DevOps. (The term combines *software development*
    and *operations*.) In 2018, MLOps, an operational framework for non-generative
    machine learning (ML) models, became the talk of the town; since then we’ve seen
    SecOps (Security Operations), DataSecOps, and many more.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the massive adoption of LLMs in 2023, a new operational framework started
    floating around within companies that were building LLM applications: LLMOps.
    LLMOps is still in its infancy, but as generative models become integral to software
    products, its popularity is likely to boom.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 2-1](#ch02_figure_1_1748895480187715) shows the slow rise of Ops frameworks
    over the years. Use of LLMOps started to pick up with the mass adoption of LLMs
    in early 2023\. As of this writing, more and more enterprises are realizing that
    they can add value and profit with LLM-based offerings, leading to an upward trend
    for LLMOps. In fact, 2025 may be the best year yet for LLMOps frameworks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploying LLMs can be as simple as integrating a chatbot into your website
    via an API or as complicated as building your own LLM and frontend from scratch.
    But maintaining them to be performant (productionizing them)—that is, keeping
    them reliable, scalable, secure, and robust—is a massive challenge. This book,
    like LLMOps, is focused on exactly this question: what happens *after* you deploy
    your LLM in production?'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/llmo_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. Google Ngram showing the rise in popularity of the term LLMOps
    from 2019 to 2024
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'From MLOps to LLMOps: Why Do We Need a New Framework?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is some overlap between MLOps and LLMOps; both deal with the operational
    lifecycles of ML models, after all. They also share common principles in terms
    of managing ML workflows. However, the two frameworks diverge in their primary
    focuses and objectives. While MLOps handles non-generative models (both language
    and computer vision), LLMOps deals with generative language models—and thus with
    mammoth levels of complexity. The complexity of these models owes not only to
    their scale and architecture but also to the unique processes involved in data
    engineering, domain adaptation, evaluation, and monitoring for them. The key distinctions
    are apparent in LLMs’ prediction transparency, latency, and memory and computational
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the biggest difference is the shift in how end users consume these models.
    Non-generative ML models are predictive tools used for passive consumption, such
    as in dashboarding, recommendations, and analytics. By contrast, LLM applications
    are deployed as [Software 3.0](https://oreil.ly/g9dn6) in consumer-facing applications
    for active user interaction. This brings several challenges from Software 1.0
    (DevOps) back to the surface. In fact, it wouldn’t be wrong to say that LLMOps
    shares more similarities with DevOps than with MLOps.
  prefs: []
  type: TYPE_NORMAL
- en: Building your own generative AI application requires tools, frameworks, and
    expectations to match the scale and complexity of these models, and this is far
    beyond the scope of the existing MLOps solutions. To help you understand the differences
    between the various kinds of Ops frameworks, let’s do a thought experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine MLOps as being something like building a small home from the ground
    up. In comparison, DevOps, which deals with the entire product lifecycle, would
    be like developing a large shopping complex. And LLMOps? It’s more like building
    the Burj Khalifa. For all three frameworks, you are working with the same construction
    materials: wood, steel, concrete, bricks, hammers, and so on. Much of the basic
    process is the same, too: you lay down the foundation, lay down the plumbing,
    and finally build the walls. But you wouldn’t contract your local construction
    workers to engineer the Burj Khalifa, would you?'
  prefs: []
  type: TYPE_NORMAL
- en: Most of MLOps for natural language modeling is based on building smaller, discriminative
    models for tasks like sentiment analysis, topic modeling, and summarization. For
    these tasks, you first hypothesize ideal features, model your data as a function
    of those features, and then optimize the model for that specific task.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs, by contrast, are generative, domain agnostic, and task agnostic. That
    fundamental difference means you can use the same model for summarizing or for
    answering questions, without having to fine-tune it.
  prefs: []
  type: TYPE_NORMAL
- en: The best use cases for LLMs are when you don’t know what features to optimize
    for (or, even better, if the features are too abstract) and when you need to model
    multimodal data within a single pipeline. Unlike smaller discriminative models,
    LLMs don’t rely on predefined features or task-specific architectures. Instead,
    as you learned in [Chapter 1](ch01.html#ch01_introduction_to_large_language_models_1748895465615150),
    they are trained on vast amounts of text data to learn the patterns and structures
    in language itself. For LLMs, the training process involves a *loss function*,
    which measures how well the model’s generated outputs match the expected outputs
    across various tasks. For example, during training, the model might generate a
    sequence of text; the loss function calculates the difference between this generated
    sequence and the target sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, for a standard comparison, the hyperparameter space for a 175-billion
    parameter GPT-4 ​model would likely be approximately 1,500 times larger than that
    for a standard discriminative BERT model (which has 110 million parameters). This
    makes it incredibly costly to fine-tune and do the kind of iterative training
    that is typical in MLOps. [Table 2-1](#ch02_table_1_1748895480195288) outlines
    some of the key differences between the MLOps and LLMOps model lifecycles.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-1\. Comparing the MLOps and LLMOps model lifecycles
  prefs: []
  type: TYPE_NORMAL
- en: '| Lifecycle step | Context | MLOps | LLMOps |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data collection | Ease | Always and straightforward. | Sometimes—data composition
    is a very hard problem. |'
  prefs: []
  type: TYPE_TB
- en: '| Data preprocessing | Ease | Fix missing data and outliers—very easy! | Hard.
    Requires:'
  prefs: []
  type: TYPE_NORMAL
- en: Deduplication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Toxicity filtering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diversity management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantity control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model learning | Model size | ML models, such as classifiers or regressors,
    typically have at max around 200 million parameters and are generally less computationally
    intensive for experimentation than are LLMs. | LLMs typically have 100 billion
    or even trillions of parameters, making them significantly larger and more complex
    than many non-generative models. Their massive scale impacts resource requirements,
    storage, and computational efficiency. |'
  prefs: []
  type: TYPE_TB
- en: '| Hyperparameters | Accuracy | Limited hyperparameter space makes search easy,
    making them ideal for predictive problems. | Massive hyperparameter space leads
    to real-time search latency. This makes them ideal for generative and evolutionary
    tasks that require creativity. Accuracy can be a computational bottleneck. |'
  prefs: []
  type: TYPE_TB
- en: '| Model training duration | Training scale and duration | Less resource intensive
    and generally faster. Can be easily deployed as notebooks on the cloud or as containerized
    solutions across a single node. | Involves processing massive datasets with distributed
    training across large clusters of GPUs or TPUs and optimizing for parallel processing.
    Can take days or weeks. Operationalizing may require dynamic resource scaling
    and involves designing scalable infrastructure and orchestration systems to handle
    varying workloads efficiently. |'
  prefs: []
  type: TYPE_TB
- en: '| Domain adaptation | Cost | Full fine-tuning is essential and affordable.
    | Full fine-tuning is too expensive and pretty rare. Instead, popular techniques
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAGs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameter-efficient fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Evaluation | Ease |'
  prefs: []
  type: TYPE_TB
- en: Easy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discriminative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well-defined probability space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Extremely hard problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generative and thus hard to detect
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unbounded probability space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Robustness | Static versus dynamic | Model behavior stays the same in production.
    | Model behavior changes in production based on interactions and requires constant
    monitoring for alignment. |'
  prefs: []
  type: TYPE_TB
- en: '| Security | Secure | Highly secure. |'
  prefs: []
  type: TYPE_TB
- en: Highly vulnerable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Needs a DataSecOps framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Four Goals for LLMOps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LLMOps operates on an LLM-specific set of design pattern principles to ensure
    that your LLM applications achieve four key goals: security, scalability, robustness,
    and reliability. Let’s take a closer look at these goals:'
  prefs: []
  type: TYPE_NORMAL
- en: Security
  prefs: []
  type: TYPE_NORMAL
- en: Minimizing the regulatory, reputational, and operational risks associated with
    deploying LLM applications in production
  prefs: []
  type: TYPE_NORMAL
- en: Scalability
  prefs: []
  type: TYPE_NORMAL
- en: Building, storing, and maintaining LLM applications that can generalize across
    data and scale efficiently on demand while optimizing latency, throughput, and
    costs
  prefs: []
  type: TYPE_NORMAL
- en: Robustness
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining high performance of LLM applications over time in the face of data
    drift, model drift, third-party updates, and other challenges
  prefs: []
  type: TYPE_NORMAL
- en: Reliability
  prefs: []
  type: TYPE_NORMAL
- en: Implementing rigorous inference monitoring, error handling, and redundancy mechanisms
    to prevent downtime and failures
  prefs: []
  type: TYPE_NORMAL
- en: LLMOps teams automate repetitive processes to more quickly optimize these applications
    at scale and avoid those “LLM-oops!” moments. Another key aspect of the LLMOps
    framework is fostering consistency, transparency, and collaboration between diverse
    interdisciplinary teams. These teams often include data engineers, data scientists,
    research scientists, software engineers, and business operations teams.
  prefs: []
  type: TYPE_NORMAL
- en: LLMOps is an entirely new field, so, as of this writing in 2025, there are very
    few mature tools and resources available. The LLMOps teams at various organizations
    are thus developing their tools and processes internally, based on prototypical
    open source libraries and toolkits.
  prefs: []
  type: TYPE_NORMAL
- en: LLMOps Teams and Roles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Today, there are two kinds of companies out there building with LLMs: the newer
    startups, which focus primarily on LLM applications, and companies with existing
    ML models that are now building their own GenAI teams.'
  prefs: []
  type: TYPE_NORMAL
- en: In the latter category, there are so few skilled Ops professionals that most
    companies recruit ML engineer candidates internally and then upskill them to LLMOps,
    instead of hiring externally. A major reason for this is a general lack of clarity
    around use cases as well as the expected job responsibilities. So the current
    norm is to hire 8 to 10 people internally from different departments, which could
    include product managers, full-stack engineers, system architects, data engineers,
    data scientists, ML engineers, platform engineers, cybersecurity professionals,
    and developer advocates. Many companies want someone who already understands the
    business inside and out to test the feasibility of several potential use cases
    and projects before committing to one.
  prefs: []
  type: TYPE_NORMAL
- en: Newer startups, however, have no choice but to build a team from the ground
    up. These teams can look very different, depending on whether they are working
    on LLMOps infrastructure (LLMOps SaaS companies) or LLM use cases such as copywriting,
    education, or process optimization. [Figure 2-2](#ch02_figure_2_1748895480187751)
    provides a very basic model of an LLMOps team, but these teams come in all shapes
    and sizes, with different levels of business and technical maturity. (Later in
    the chapter, we’ll look at how to assess your company’s LLMOps maturity.)
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/llmo_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. Structure of an LLMOps team
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Most companies cannot afford the cost of building and training an LLM for use
    in their application. Instead, most choose to use a foundational model. This is
    where AI engineers come in: to build a quick proof of concept using LLMOps tools.
    Most come from a software-engineering background; they may know how to build a
    full-stack application quickly, but don’t necessarily have a deep understanding
    of ML/LLM models’ inner workings or how to optimize them.'
  prefs: []
  type: TYPE_NORMAL
- en: Once the proof of concept is completed, building, deploying, and optimizing
    ML models for the business falls to LLM engineers, ​ML scientists (interchangeable
    titles, depending on the organization), and LLMOps ​engineers tasked as reliability
    engineers.
  prefs: []
  type: TYPE_NORMAL
- en: If a team chooses to deploy an LLM application using API integration, the initial
    deployment will be pretty straightforward. The LLMOps engineers work alongside
    AI engineers to scale the model and improve its performance. Ideally, each AI
    engineer is paired with an LLMOps engineer; the AI engineer can focus on adapting
    and fine-tuning the model to meet the business needs, while the LLMOps engineer
    manages deployment and optimization. For open source models, managing deployment
    automatically falls to the LLMOps engineer.
  prefs: []
  type: TYPE_NORMAL
- en: As the tech industry moves from non-generative models to generative models,
    it is shifting away from feature engineering, or creating features to model the
    data and experimenting with different hyperparameters to optimize performance.
    Generative models, and specifically LLMs, do not require feature engineering.
    Today, the core requirements are usually prompt engineering or building a RAG
    pipeline—skills that lie within the domain of AI engineers.
  prefs: []
  type: TYPE_NORMAL
- en: Another big shift is that the data engineering pipeline and the monitoring and
    evaluation pipeline have become far more complex. Evaluating LLMs is much more
    than straightforward quantitative scoring. Some industry benchmarks exist—like
    [BLEU](https://oreil.ly/OxqU2), a benchmark used to evaluate machine translation,
    and [ROUGE](https://oreil.ly/1y3e4), a benchmark used to evaluate summarization,
    but these are only loosely correlated with application performance. Moving to
    a model that has a better score is no guarantee of better user satisfaction. Standardized
    scores may be good for comparing LLMs in general, but ultimately users care about
    whether the LLM application is solving their problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, since LLMs are deployed in consumer-facing applications, metrics
    like perceived latency and throughput become make-or-break factors in market competition.
    The model is not the only deciding factor: the deployment, evaluation, and monitoring
    pipelines are also at the front and center of performance assessment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some of the roles involved in these pipelines:'
  prefs: []
  type: TYPE_NORMAL
- en: Data engineer
  prefs: []
  type: TYPE_NORMAL
- en: Data engineers are professionals responsible for designing, building, and maintaining
    systems and pipelines that enable the efficient collection, storage, and transformation
    of data as well as access to it. Their work ensures that data is available, reliable,
    and organized in a way that allows data scientists and other engineers to create
    and evaluate models and data-driven applications. Data retrieval and movement
    are the most fundamental skills for data engineers, but as they progress in their
    career, developing data architectures becomes more important.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data engineering for LLMs requires specialized understanding: how to chunk
    the data, what tokenization model to use, and so on. Thus, it’s best to pair each
    data engineer with an ML scientist (with an LLM engineering background) along
    with the LLMOps engineer for automating and streamlining LLM systems at scale.'
  prefs: []
  type: TYPE_NORMAL
- en: AI engineer
  prefs: []
  type: TYPE_NORMAL
- en: The core skill set of an AI engineer is full-stack engineering and development
    (React, Node.js, Django) plus familiarity with the common LLMOps tools and frameworks
    for deploying applications, like LangChain and Llama Index. They need a foundational
    understanding of end-to-end AI application development, including prompt engineering
    and RAG systems. Companies building their teams from scratch usually look to hire
    AI engineers who also know a lot about using cloud services to deploy and manage
    AI applications and have experience working with external APIs and vector databases.
  prefs: []
  type: TYPE_NORMAL
- en: ML scientist
  prefs: []
  type: TYPE_NORMAL
- en: The day-to-day work of an ML scientist, NLP scientist, or LLM engineer involves
    researching, designing, and optimizing LLMs using frameworks like PyTorch, TensorFlow,
    and JAX. This role requires a deep understanding of NLP algorithms and tasks,
    such as tokenization, named-entity recognition (NER), sentiment analysis, and
    machine translation. Candidates should know model architectures and training and
    fine-tuning processes.
  prefs: []
  type: TYPE_NORMAL
- en: LLMOps engineer
  prefs: []
  type: TYPE_NORMAL
- en: The goal  of an LLMOps engineer is to ensure that LLM applications remain reliable,
    robust, secure, and scalable. The day-to-day work involves building and maintaining
    operational LLM pipelines as a project owner.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now look at the LLMOps engineer role in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: The LLMOps Engineer Role
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This role requires extensive expertise in deploying, monitoring, fine-tuning,
    training, scaling, and optimizing LLM models in production environments; infrastructure
    and platform engineering; data engineering; and system reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Companies building LLM teams usually seek LLMOps engineers who understand the
    unique challenges associated with LLMs and are experienced in making build-versus-buy
    trade-off, including by weighing cost efficiency against system performance.
  prefs: []
  type: TYPE_NORMAL
- en: To stand out as a candidate for this role, you’ll need proficiency in a unique
    blend of specialized skills and a deep technical understanding of the entire LLM
    lifecycle, from data management to model deployment and monitoring. You’ll also
    need to be a problem solver, a strong team player, and an effective communicator
    as well as meticulously detail oriented.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate what this means in practice, let’s look at a typical day in the
    work life of a fictional LLMOps engineer.
  prefs: []
  type: TYPE_NORMAL
- en: A Day in the Life
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While the specific responsibilities for this role will vary across organizations,
    the core tasks typically encompass a blend of infrastructure management, collaboration,
    optimization, and compliance. To give you a quick taste of what this looks like
    in practice, here’s what a typical LLM engineer’s workday might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '7:30 AM–8:30 AM: Morning check-in'
  prefs: []
  type: TYPE_NORMAL
- en: Review monitoring dashboards for any overnight alerts or performance issues
    in deployed LLMs; address any urgent issues or escalate them to the appropriate
    teams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lead the team’s daily stand-up meeting to discuss ongoing projects, blockers,
    and priorities for the day.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '8:30 AM–10:00 AM: Infrastructure management and optimization'
  prefs: []
  type: TYPE_NORMAL
- en: Modularize code for reusability​, creating separate modules for provisioning
    GPUs, managing storage, and networking.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement batching mechanisms to process multiple inference requests, reducing
    the per-request overhead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement latency optimization techniques like kernel fusion, quantization,
    and dynamic batching to enhance model performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '10:00 AM–11:30 AM: Collaboration and project planning meeting'
  prefs: []
  type: TYPE_NORMAL
- en: Meet with data scientists, ML engineers, and red teaming engineers to discuss
    usage requirements, timelines, monitoring errors, and scaling challenges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '11:30 AM–12:30 PM: API development and model deployment'
  prefs: []
  type: TYPE_NORMAL
- en: Design inference endpoints and cache for different models in production, ensuring
    compatibility with the rest of the application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '12:30 PM–1:30 PM: Lunch break'
  prefs: []
  type: TYPE_NORMAL
- en: '1:30 PM–3:00 PM: Monitoring and troubleshooting'
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshoot any issues that arise. For example, let’s say the users are experiencing
    long delays when making requests to the inference API. The engineer would identify
    the source of latency by examining hardware utilization and network latency and
    reviewing usage logs. They would then implement a solution; e.g., using a Pod
    Autoscaler or caching the frequent requests.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3:00 PM–4:30 PM: Research and continuous learning'
  prefs: []
  type: TYPE_NORMAL
- en: Experiment with new tools, libraries, or frameworks that could be integrated
    into the existing tech stack to improve efficiency and performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '4:30 PM–5:30 PM: End-of-day wrap-up, review, and on-call prep'
  prefs: []
  type: TYPE_NORMAL
- en: Review the day’s tasks and update the tickets with completed tasks and next
    steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepare for any on-call duties, ensuring that all monitoring systems are correctly
    configured and that you’re ready to respond to any incidents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attend any final meetings or syncs with cross-functional teams to ensure alignment
    on upcoming priorities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wrap up any remaining tasks and make sure that all systems are running smoothly
    before logging off for the day.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After regular working hours, if you are on call, you’ll need to remain available
    to address any critical issues that may arise.
  prefs: []
  type: TYPE_NORMAL
- en: Hiring an LLMOps Engineer Externally
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two ways to fill an LLMOps engineer role: you can hire externally,
    or you can hire internally and upskill your people, training ML engineers to become
    LLMOps engineers. This section will look at external hiring first and then discuss
    how to upskill current employees.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re hiring for this role, other skills to look for in candidates for
    LLMOps engineering roles include experience or proficiency in:'
  prefs: []
  type: TYPE_NORMAL
- en: Converting models to and from libraries like PyTorch or JAX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding ML metrics like accuracy, precision, recall, and PR-AUC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding data drift and concept drift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running and benchmarking models to understand the impact of computational graph
    representation on performance across the neural engine, GPU, and CPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying and scaling ML models in cloud environments like AWS, GCP, and Azure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using LLM inference latency optimization techniques, including kernel fusion,
    quantization, and dynamic batching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building Ops pipelines for data engineering, deployment, and infrastructure
    as code (IaC) using tools like Terraform, managing vector databases, and ETL processes
    for large-scale training datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding red-teaming strategies, interfaces, and guidelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Docker for containerization and Kubernetes for orchestration to ensure
    scalable and consistent deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collaborating on and managing projects with teams that include LLM engineers,
    data scientists, and ML/NLP engineers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every company, of course, has its own interviewing process. Some conduct many
    rounds of interviews; others combine several of the interviews into a single on-site
    meeting. This section describes a fairly standard four-round interview process,
    as pictured in [Figure 2-3](#ch02_figure_3_1748895480187773).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/llmo_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Components of an LLMOps interview
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s look at each round in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Round 1: Initial screening'
  prefs: []
  type: TYPE_NORMAL
- en: The goal during initial screening is to determine that the applicant has the
    fundamental skills and experience required for the role. This can be assessed
    via a resume assessment sheet. The questions you’re asking here are very high
    level. Do they have experience with deploying LLMs in production environments?
    Do they mention specific frameworks and tools for managing LLM pipelines, and
    are these the same tools your company uses? If not, will they be able to learn
    and adopt your stack? Can they show or talk about any past projects?
  prefs: []
  type: TYPE_NORMAL
- en: 'Round 2: Technical assessment'
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal in this round is to assess the candidate’s technical proficiency in
    core areas such as LLM deployment, data engineering, and infrastructure management.
    Some questions you might ask include:'
  prefs: []
  type: TYPE_NORMAL
- en: Describe the steps you take to fine-tune a pretrained large language model.
    How do you ensure that the model is optimized for the specific use case?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walk me through the deployment process of an LLM you’ve worked on. What challenges
    did you face? How did you overcome them?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How would you set up a CI/CD pipeline for LLM training, fine-tuning, and deployment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you design and manage data pipelines for large-scale ML projects? What
    tools do you use, and how do you ensure data quality?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you monitor and troubleshoot latency issues in production?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you handle versioning and tracking datasets used in training or fine-tuning
    LLMs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you ensure high availability and cost-efficiency in your cloud infrastructure?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Round 3: System design interview'
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal in this round is to assess the candidate’s ability to design scalable,
    reliable, and maintainable systems for deploying and managing LLMs. Sample questions
    for this round could include:'
  prefs: []
  type: TYPE_NORMAL
- en: Tell me about a time when you had to make a build-versus-buy decision for a
    component of your ML infrastructure. What factors did you consider?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How would you design an API for serving LLM inferences at scale? Discuss considerations
    for load balancing, fault tolerance, and latency reduction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How would you use an IaC tool to manage the cloud infrastructure for an LLM
    deployment? What strategies would you employ to optimize resource usage and cost?
    What potential problems do you think you’d encounter while scaling it?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe your approach to dynamic batching in inference service. How do techniques
    like quantizing and mixed precision training affect the performance and efficiency
    of LLMs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you manage memory in CUDA when training LLMs? What strategies do you
    use to prevent issues like out-of-memory errors?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you benchmark model performance before and after optimization? What metrics
    do you consider, and what tools do you use?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How would you diagnose and address performance degradation after an optimization?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Final round: Behavioral interview'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the fourth round, you’ve narrowed the pool to the most qualified candidates.
    Now you need to assess their personalities: Are they self-driven? Can they work
    in a team, handle challenges with equanimity, and contribute to a collaborative
    environment? Sample questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How do you stay up-to-date on the latest advancements in LLMs and machine learning
    operations?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you approach integrating data scientists’ feedback into the deployment
    process?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How would you collaborate with a red-teaming engineer to address potential security
    vulnerabilities in an LLM deployment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What experience do you have with on-call rotations? How do you handle critical
    incidents during off hours?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You’ll also want to ensure the candidate aligns with your organization’s values,
    particularly in areas like innovation, continuous learning, and collaboration.
    You can ask questions like:'
  prefs: []
  type: TYPE_NORMAL
- en: What are your favorite technology blogs or podcasts?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do you keep up with new advances in the field?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hiring Internally: Upskilling an MLOps Engineer into an LLMOps Engineer'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The gap between MLOps engineers and LLMOps engineers is significant in terms
    of the scale, complexity, and the technical challenges involved in their roles.
    Thus, upskilling an existing employee requires a focused effort to build their
    understanding.
  prefs: []
  type: TYPE_NORMAL
- en: That said, the foundational skills of MLOps—such as model deployment, automation,
    and cloud management—provide a solid base from which to grow. With dedicated learning
    and hands-on experience, an MLOps engineer can transition into the LLMOps domain
    effectively. The core upside of hiring internally is that candidates are already
    aligned with the organization’s values and culture and have a keen understanding
    of its KPIs.
  prefs: []
  type: TYPE_NORMAL
- en: To excel, new LLMOps engineers need resources and training to deepen their understanding
    of large-scale model architectures and transformer architectures, attention mechanisms,
    infrastructure management, and LLM-specific optimization techniques. They also
    need to understand how LLMs differ from non-generative ML models. We recommend
    pairing them up with LLM engineers to experiment with and evaluate different models.
  prefs: []
  type: TYPE_NORMAL
- en: This role isn’t just about building an app that uses LLMs. LLMOps engineers
    also manage the balance among cost, cloud resources, and user experience and handle
    huge unstructured datasets. Therefore, pair them with data engineers to help build
    a data-processing pipeline so they can familiarize themselves with the data sources
    at their disposal, how the data is structured in the databases, how different
    databases retrieve information, what the company’s latency expectations are, and
    how to handle data filtering. Allow them to introduce multi-node setups and distributed
    systems for different models while focusing on cost optimization and errors. Get
    them to benchmark different LLM models and debug their performance optimization.
    Finally, allow them to present their logging practices and the guardrails they
    have set up for maintaining reliability and performance at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Most MLOps engineers already have skills in model versioning, data versioning,
    managing rollbacks, and GitHub actions, so upskilling these professionals can
    be an effective strategy for building a strong LLMOps team.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s look at how to make sure the goals of your LLMOps engineers are
    aligned with your ​organizational goals.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs and Your Organization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You learned at the beginning of this chapter that the four key goals of the
    LLMOps framework are security, scalability, robustness, and reliability. For LLMOps
    teams, then, the next big question is how to measure the application’s performance
    against those goals. How will you know you’re succeeding? The company’s expectations
    should be clearly defined and remain quantitatively, as well as qualitatively,
    measurable at all times.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three kinds of metrics will allow you to measure your team’s performance toward
    its goals: SLOs, SLAs, and KPIs. These terms are in common usage among site reliability
    engineers, and now LLMOps teams are rapidly adopting them as well:'
  prefs: []
  type: TYPE_NORMAL
- en: Service-level objectives (SLOs)
  prefs: []
  type: TYPE_NORMAL
- en: Service-level objectives are specific, measurable targets set by an organization
    to gauge the quality of its services internally. They define what level of service
    the organization aims to achieve. For example, an SLO for a cloud-hosting company
    might be to ensure that server uptime is at least 99.9% per month.
  prefs: []
  type: TYPE_NORMAL
- en: Service-level agreements (SLAs)
  prefs: []
  type: TYPE_NORMAL
- en: Service-level agreements are formal contracts between a service provider and
    a customer that define the level of service the provider commits to deliver. They
    typically include specific performance goals and stipulate remedies if those metrics
    are not met. For example, if the uptime for an internet provider falls below 99.9%
    annually, the SLA might stipulate that the customer will receive a 10% discount
    on its next billing cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Key performance indicators (KPIs)
  prefs: []
  type: TYPE_NORMAL
- en: Key performance indicators measure the overall success and performance of specific
    business activities. They provide insight into how well the organization is achieving
    its strategic objectives. For example, an important KPI for an app might be its
    churn rate or the percentage of customers who stop using the app over a certain
    period.
  prefs: []
  type: TYPE_NORMAL
- en: In August 2024, a [Gartner Research study predicted](https://oreil.ly/dIBVc)
    that 30% of existing GenAI projects would fail by 2025\. (In fact, Gartner [published
    similar findings](https://oreil.ly/Rq3K3) in 2018, predicting that 85% of ML projects
    would fail in production by 2022.) The key failure points outlined in the 2024
    study are notable because they are the operational aspects of LLM development
    and deployment—including data quality issues, the lack of a strong evaluation
    framework, and the high costs of scaling these models in production.
  prefs: []
  type: TYPE_NORMAL
- en: That said, one of the most obvious issues is mismatched expectations between
    management and engineering teams. For the last 10 years, one of data scientists’
    biggest skill gaps has been in translating ML model metrics into organizational
    and product success metrics. In other words, when you’re measuring abstract goals
    like model security, scalability, robustness, and reliability, how do you communicate
    what this means for the business? That’s what SLOs, SLAs, and KPIs are for.
  prefs: []
  type: TYPE_NORMAL
- en: Using an SLO-SLA-KPI framework allows LLMOps teams to automate, streamline,
    and manage expectations across multiple stakeholders. SLOs make it evident to
    all stakeholders what level of service is being aimed for. SLAs ensure accountability
    so that everyone involved is aware of their roles and the agreed-upon service
    levels. This can also help you track performance and address any deviations from
    the expected standards. And KPIs provide visibility into real-time data to help
    detect potential issues early, facilitating informed decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: The Four Goals of LLMOps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s look more closely at the LLMOps goals to see how these metrics translate.
  prefs: []
  type: TYPE_NORMAL
- en: Reliability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you know well by now, LLMs are extremely complex, with billions of parameters.
    Their behavior can be unpredictable, and they sometimes exhibit unexpected responses
    or errors due to their scale and the intricacies of their training data. Additionally,
    if the training data is biased, outdated, or unrepresentative of certain domains,
    the model’s performance can be unreliable in those areas.
  prefs: []
  type: TYPE_NORMAL
- en: LLMs also struggle at times with understanding the context, nuance, and intent
    behind user queries. This can lead to incorrect, irrelevant, or misleading responses.
    What’s more, LLMs usually aren’t updated in real time. As language evolves, if
    new information is not integrated into the training data via regular retraining,
    models can become outdated, leading to decreased reliability.
  prefs: []
  type: TYPE_NORMAL
- en: All of these issues come down to reliability. The reliability of LLM-based applications
    can be measured in terms of system availability, error rates, and customer satisfaction.
    [Table 2-2](#ch02_table_2_1748895480195314) shows how these metrics would look
    as SLOs, SLAs, and KPIs.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-2\. Example reliability comparison of SLOs, SLAs, and KPIs
  prefs: []
  type: TYPE_NORMAL
- en: '|   | SLO | SLA | KPI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Availability | Maintain 99.9% uptime per month | Ensure that the system is
    available for at least 99.95% of requests over a rolling 30-day period | Customer
    satisfaction score (CSAT) related to system availability |'
  prefs: []
  type: TYPE_TB
- en: '| Error rate | Keep the error rate below 0.1% for all API requests | Ensure
    that less than 1% of user interactions result in errors | Track error rate trends
    over time and analyze root causes of major errors |'
  prefs: []
  type: TYPE_TB
- en: '| Customer satisfaction | CSAT score of at least 90% | Ensure that the net
    promoter score (NPS) remains above 8 | Post-interaction surveys or feedback forms;
    CSAT score |'
  prefs: []
  type: TYPE_TB
- en: Scalability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs tend to have a large memory footprint, often exceeding the capacity of
    a single machine. Distributing a model across multiple GPUs or nodes while maintaining
    performance is technically challenging. Handling large volumes of data efficiently
    is critical.
  prefs: []
  type: TYPE_NORMAL
- en: Another significant challenge is scaling the data pipeline to feed data into
    the model at the required speed without causing bottlenecks. This can be especially
    hard for applications like chatbots or interactive services, where low latency
    is crucial. Scaling while keeping response times low can be challenging, since
    scaling up increases resource contention and network overhead. Therefore, balancing
    performance with cost efficiency is a constant concern.
  prefs: []
  type: TYPE_NORMAL
- en: LLMOps scalability can be measured via metrics like latency, throughput, response
    time, resource scaling, capacity planning, and recovery time objective (RTO),
    as shown in [Table 2-3](#ch02_table_3_1748895480195328).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-3\. Example scalability comparison of SLOs, SLAs, and KPIs
  prefs: []
  type: TYPE_NORMAL
- en: '|   | SLO | SLA | KPI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Latency | Serve 95% of requests within 200 milliseconds | Keep the average
    response time for API calls under 100 milliseconds | Average response time for
    user interactions |'
  prefs: []
  type: TYPE_TB
- en: '| Throughput | Process a minimum of 1,000 requests per second during peak traffic
    hours | Handle at least 1 million concurrent connections without degradation |
    Peak throughput capacity during load testing |'
  prefs: []
  type: TYPE_TB
- en: '| Response time | Maintain a web page load time of under 3 seconds for 95%
    of users | Ensure that the login process completes within 500 milliseconds for
    99% of users | User experience metrics related to response times |'
  prefs: []
  type: TYPE_TB
- en: '| Resource scaling | Automatically scale up resources to handle a 50% increase
    in traffic within 5 minutes | Ensure that adding servers linearly increases throughput
    without impacting latency | Scalability test results and cost-effectiveness of
    scaling solutions |'
  prefs: []
  type: TYPE_TB
- en: '| Capacity planning | Maintain CPU utilization below 80% during peak hours
    | Ensure that enough database connections are available to handle double the anticipated
    peak load | Resource utilization trends and forecasting accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| RTO | Achieve a recovery time objective of under 30 minutes for critical
    system failures | Ensure that the system can recover from a database failure and
    restore service within 15 minutes | Historical RTO metrics and improvement initiatives
    |'
  prefs: []
  type: TYPE_TB
- en: Robustness
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Over time, the statistical properties of the model’s training data can change,
    leading to a drift in the model’s performance. This is particularly problematic
    for models that interact with real-time or rapidly changing data. This can lead
    to performance degradation in the form of outdated or irrelevant responses.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous training and fine-tuning are necessary to maintain robustness, but
    they require significant computational resources and careful management to avoid
    introducing new biases or errors. You can measure robustness via metrics like
    data freshness, model evaluation, and consistency, as shown in [Table 2-4](#ch02_table_4_1748895480195338).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-4\. Example robustness comparison of SLOs, SLAs, and KPIs
  prefs: []
  type: TYPE_NORMAL
- en: '|   | SLO | SLA | KPI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data freshness | Ensure that dashboard data is refreshed every 5 minutes
    | Guarantee that data is updated in real time | Data refresh latency and accuracy
    of real-time data updates |'
  prefs: []
  type: TYPE_TB
- en: '| Model evaluation | Maintain a performance degradation rate of less than 5%
    over 6 months | Guarantee regular updates and reviews of model evaluation metrics
    | Accuracy, relevance, and update frequency of evaluation metrics |'
  prefs: []
  type: TYPE_TB
- en: '| Consistency | Guarantee strong consistency for data reads and writes across
    all regions | Maintain eventual consistency with a maximum propagation delay of
    1 second | Consistency model adherence and replication latency |'
  prefs: []
  type: TYPE_TB
- en: Security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Maintaining LLM application security is challenging: these are complex models
    handling sensitive data in the face of constantly evolving security threats. LLMs
    are especially vulnerable to adversarial attacks, data poisoning, and other forms
    of exploitation that can compromise their integrity and security.'
  prefs: []
  type: TYPE_NORMAL
- en: Managing and controlling access to the LLM and its data is complicated, especially
    in a multi-access or multi-tenant environment, but it’s critical for preventing
    unauthorized access and misuse. [Table 2-5](#ch02_table_5_1748895480195348) shows
    some ways to measure it.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-5\. Example security comparison of SLOs, SLAs, and KPIs
  prefs: []
  type: TYPE_NORMAL
- en: '|   | SLO | SLA | KPI |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Data privacy | Ensure data encryption for all in-transit and at-rest data
    | Ensure no breaches occur | Encryption compliance status |'
  prefs: []
  type: TYPE_TB
- en: '| Model integrity | Detect and address any model tampering within 24 hours
    | Guarantee prompt detection of and response to unauthorized modifications | Number
    of unauthorized modifications detected |'
  prefs: []
  type: TYPE_TB
- en: '| Access control | Achieve a user authentication success rate of 99.9% | Ensure
    robust user authentication and authorization mechanisms | Rate of unauthorized
    access attempts |'
  prefs: []
  type: TYPE_TB
- en: '| Red teaming | Ensure detection of 99.9% of attempted adversarial attacks
    | Ensure regular security assessments and updates | Frequency of security assessments
    and the number of critical vulnerabilities identified |'
  prefs: []
  type: TYPE_TB
- en: When all teams—whether they are involved in development, operations, or management—understand
    the agreed-upon service levels and performance indicators, they can work together
    more effectively toward common goals. This alignment fosters a unified approach
    to managing and improving project performance. It also helps in making data-backed
    decisions about resource allocation, process changes, and strategic adjustments.
    Most importantly, it helps in building trust and ensuring that everyone is on
    the same page regarding expectations and outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, implementing an SLO-SLA-KPI framework​ not only enhances transparency
    and fosters collaboration, but it also serves as a foundational element in evaluating
    and advancing the maturity of your LLMOps practices, which is the topic of this
    chapter’s final section.
  prefs: []
  type: TYPE_NORMAL
- en: The LLMOps Maturity Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMOps maturity is a way of determining how well an organization’s LLM operations
    align with industry best practices and standards. Assessing LLMOps maturity helps
    organizations identify their strengths, areas for improvement, and opportunities
    for scaling and enhancing the robustness of their LLM systems.
  prefs: []
  type: TYPE_NORMAL
- en: A few years ago, Microsoft published a [machine learning operations maturity
    model](https://oreil.ly/1Tjgw) detailing a progressive set of requirements and
    stages to measure the maturity of MLOps production environments and processes.
    The LLMOps maturity model we present here, inspired by Microsoft’s MLOps model,
    is meant to do the same for LLMOps teams. Although this is by no means a comprehensive
    audit, we hope to see several variations put into practice.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three LLMOps maturity levels are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Level 0
  prefs: []
  type: TYPE_NORMAL
- en: No LLMOps practices are implemented. The organization’s lack of formal structures
    and processes for managing and deploying its LLM systems hinders its effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Level 1
  prefs: []
  type: TYPE_NORMAL
- en: The organization applies MLOps practices but without LLM-specific adaptations.
    This is an improvement over Level 0 in terms of formalization and processes but
    still lacks the sophistication needed for full LLM operations.
  prefs: []
  type: TYPE_NORMAL
- en: Level 2
  prefs: []
  type: TYPE_NORMAL
- en: Achieving Level 2 represents a mature LLMOps state, characterized by advanced
    documentation, robust monitoring and compliance measures, and the integration
    of sophisticated orchestration and human review strategies. Usually, this can
    be assessed by asking some questions about whether decision strategies and model
    performance measures and metrics are well documented within the team.
  prefs: []
  type: TYPE_NORMAL
- en: Various measures of LLMOps maturity levels are outlined in [Table 2-6](#ch02_table_6_1748895480195357).
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-6\. Documentation and strategy measures of LLMOps maturity
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Level 0: No LLMOps | Level 1: MLOps, no LLMOps | Level 2: Full LLMOps
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Are the business goals and KPIs of the LLM project documented and kept up
    to date? | Not documented | There is documentation, but it’s often outdated |
    Full documentation with regular updates; KPIs include model performance metrics,
    operational efficiency, and cost-effectiveness |'
  prefs: []
  type: TYPE_TB
- en: '| Are LLM model risk evaluation metrics documented? | No formal risk assessment
    | Basic risk evaluation for model accuracy and data security | Comprehensive risk
    evaluation including bias, fairness, data drift, and performance degradation with
    mitigation strategies in place |'
  prefs: []
  type: TYPE_TB
- en: '| Is there a documented and regularly updated overview of all team members
    involved in the project, along with their responsibilities? | No documentation
    | High-level roles are documented, but responsibilities may be unclear for the
    newer roles | Detailed team structure with roles, responsibilities, and contact
    details that is regularly reviewed and updated |'
  prefs: []
  type: TYPE_TB
- en: '| Is the choice of LLM well documented and cost-compared against other open
    source/proprietary offerings? | No documentation or cost analysis | Basic documentation
    of LLM choice, minimal cost comparison | Detailed documentation including rationale
    for choice, performance benchmarks, and cost comparison against alternative models
    |'
  prefs: []
  type: TYPE_TB
- en: '| Is the API for the model vendor well documented, including request and response
    structure, data types, and other relevant details? | No API documentation | Model
    developed in-house | Comprehensive API documentation, including request/response
    examples, data types, error codes, and versioning details |'
  prefs: []
  type: TYPE_TB
- en: '| Is the software architecture well documented and kept up to date? | No documentation
    | There is a high-level architecture overview, but it may be outdated | Detailed
    architecture diagrams including data flow, system components, and integration
    points that are updated regularly |'
  prefs: []
  type: TYPE_TB
- en: Documenting the factors shown in [Table 2-6](#ch02_table_6_1748895480195357)
    can be incredibly helpful when choosing and deploying any new model. For example,
    given the significant costs that are associated with deploying an LLM application,
    cost-benchmark analysis documentation allows the company to decide which model
    to roll into production and estimate the project timeline.
  prefs: []
  type: TYPE_NORMAL
- en: After deployment, the company also needs to assess how well the team has documented
    the model’s performance measures and metrics. This is to make sure that everyone
    on the team understands the expectations and that they are comprehensively monitoring
    the model performance in production.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 2-7](#ch02_table_7_1748895480195367) outlines three levels of LLMOps
    maturity with regard to model performance and evaluation. Keeping these different
    levels in mind, organizations and the LLMOps team will be better prepared to deal
    with contingencies and can better align projects with business goals, mitigate
    risks, and enhance operational efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2-7\. Model performance and evaluation measures of LLMOps maturity
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Level 0: No LLMOps | Level 1: MLOps, no LLMOps | Level 2: Full LLMOps
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Does the LLM system operate within its knowledge limits, and recognize when
    it is operating outside those limits? | No mechanisms to detect limits | Basic
    detection of operational limits | Advanced guardrails, limit detection mechanisms,
    and documentation of context-aware warnings using techniques like confidence scoring
    and thresholding |'
  prefs: []
  type: TYPE_TB
- en: '| Are the LLM’s inputs and outputs automatically stored? | No automatic storage
    | Basic storage of inputs and outputs | Automated storage of all inputs and outputs
    with indexing for easy retrieval and analysis |'
  prefs: []
  type: TYPE_TB
- en: '| Is A/B testing performed regularly? | No A/B testing | Occasional A/B testing
    with limited coverage | Regular A/B testing with comprehensive test coverage and
    analysis, using tools like Optimizely or custom frameworks |'
  prefs: []
  type: TYPE_TB
- en: '| Are all API requests and responses logged, and are API response time and
    health status monitored? | No logging or monitoring | Basic logging and response
    time monitoring | Comprehensive logging with detailed request/response analysis;
    real-time health monitoring using tools like ELK stack |'
  prefs: []
  type: TYPE_TB
- en: '| Is the LLM monitored for toxicity and bias? | No outlier detection or bias
    monitoring | Basic outlier detection with manual review | Advanced automated toxicity
    and bias detection pipelines using statistical methods and regular bias audits,
    with automated alerting for low-confidence predictions |'
  prefs: []
  type: TYPE_TB
- en: '| Are processes in place to ensure that LLM operations comply with regulations
    such as GDPR, HIPAA, and other relevant data protection laws? | No process exists
    | Process to ensure that LLM operations comply with regulations such as GDPR,
    HIPAA, or other relevant data protection laws | Process to ensure that LLM operations
    comply with regulations such as GDPR, HIPAA, or other relevant data collection
    and protection laws and copyright laws |'
  prefs: []
  type: TYPE_TB
- en: '| Does the LLM-based app use anonymization to protect users’ identities while
    maintaining the data’s utility for LLMs? | No anonymization | Basic anonymization
    techniques applied | Advanced automated anonymization methods, including data
    masking and aggregations |'
  prefs: []
  type: TYPE_TB
- en: '| Does the organization perform regular security reviews and audits of LLM
    infrastructure and code? | No regular reviews | Periodic security reviews and
    audits | Regular, comprehensive security reviews and audits, including third-party
    assessments and vulnerability scans |'
  prefs: []
  type: TYPE_TB
- en: 'Let’s look at these levels in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Level 0: No LLMOps'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning efforts are often isolated and experimental, and they lack
    any systematic deployment and monitoring framework. The models may be developed
    in silos, often resulting in unreliability and inefficiency. Chevrolet’s [chatbot
    blunder](https://oreil.ly/CtHrG) is an excellent example; due to a lack of monitoring
    and guardrails, the app was abused by the community for algebra homework. It also
    offered Chevrolet cars in no-take-backsies deals and promoted Tesla cars instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Level 1: MLOps, no LLMOps'
  prefs: []
  type: TYPE_NORMAL
- en: The organization is likely to have a robust pipeline for model training, testing,
    and deployment, with automated monitoring and retraining workflows. However, this
    setup is designed to build for small models and is not fully optimized for the
    specific challenges of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Level 2: Full LLMOps'
  prefs: []
  type: TYPE_NORMAL
- en: At the highest level of maturity, the organization has adopted LLMOps practices
    and is fully optimized for LLM applications. Its infrastructure is capable of
    handling large-scale LLM deployments, fine-tuning, real-time inference, auto-scaling,
    and resource management. Mature LLMOps teams have failover and rollback mechanisms
    in place and can act quickly if the updated model underperforms after deployment.
    The organization can deliver more reliable responses, get a good ROI, and reduce
    operational risks.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed the team structure for organizations building
    LLM applications. We discussed various roles and how to build a highly effective
    team. Finally, we discussed a framework for typing the LLM performance metrics
    with the business KPIs. In the next chapter, we will talk about how LLMs have
    changed the data engineering landscape, and we’ll show you how to build performant
    data pipelines for ​LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Azure Machine Learning. [“Machine Learning Operations Maturity Model”](https://oreil.ly/EebdS),
    Learn Azure, accessed May 21, 2025.
  prefs: []
  type: TYPE_NORMAL
- en: Friedman, Itamar. [“Software 3.0—The Era of Intelligent Software Development”](https://oreil.ly/AtCSq),
    *Medium*, May 3, 2022.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lin, Chin-Yew. [“ROUGE: A Package for Automatic Evaluation of Summaries”](https://oreil.ly/IvSev),
    *Text Summarization Branches Out*, (Association for Computational Linguistics,
    2024).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kadambi, Sreedher. [“Shingo Principles: Bridging Lean and Toyota Production
    System Success”](https://oreil.ly/wCmAE). Skil Global, May 28, 2021.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mcintyre, Branden. [“Chevy Chatbot Misfire: A Case Study in LLM Guardrails
    and Best Practices”](https://oreil.ly/VQHov), *Medium*, December 22, 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Papineni, Kishore, et al. [“BLEU: A Method for Automatic Evaluation of Machine
    Translation”](https://oreil.ly/zyIEO), *ACL ’02: Proceedings of the 40th Annual
    Meeting of the Association for Computational Linguistics*, edited by Pierre Isabelle,
    Eugene Charniak, and Dekang Lin (Association for Computational Linguistics, 2002).'
  prefs: []
  type: TYPE_NORMAL
- en: Further Reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Shingo, Shigeo. *Zero Quality Control: Source Inspection and the Poka-Yoke
    System*, (Routledge, 2021).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tennant, Geoff. *Six Sigma: SPC and TQM in Manufacturing and Services*, (Routledge,
    2001).'
  prefs: []
  type: TYPE_NORMAL
