<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">2</span></span> <span class="chapter-title-text">Working with text data</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Preparing text for large language model training</li> 
    <li class="readable-text" id="p3">Splitting text into word and subword tokens</li> 
    <li class="readable-text" id="p4">Byte pair encoding as a more advanced way of tokenizing text</li> 
    <li class="readable-text" id="p5">Sampling training examples with a sliding window approach</li> 
    <li class="readable-text" id="p6">Converting tokens into vectors that feed into a large language model</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>So far, we’ve covered the general structure of large language models (LLMs) and learned that they are pretrained on vast amounts of text. Specifically, our focus was on decoder-only LLMs based on the transformer architecture, which underlies the models used in ChatGPT and other popular GPT-like LLMs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>During the pretraining stage, LLMs process text, one word at a time. Training LLMs with millions to billions of parameters using a next-word prediction task yields models with impressive capabilities. These models can then be further finetuned to follow general instructions or perform specific target tasks. But before we can implement and train LLMs, we need to prepare the training dataset, as illustrated in figure 2.1.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p9">  
   <img alt="figure" src="../Images/2-1.png" width="1012" height="524"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.1</span> The three main stages of coding an LLM. This chapter focuses on step 1 of stage 1: implementing the data sample pipeline.</h5>
  </div> 
  <div class="readable-text" id="p10"> 
   <p>You’ll learn how to prepare input text for training LLMs. This involves splitting text into individual word and subword tokens, which can then be encoded into vector representations for the LLM. You’ll also learn about advanced tokenization schemes like byte pair encoding, which is utilized in popular LLMs like GPT. Lastly, we’ll implement a sampling and data-loading strategy to produce the input-output pairs necessary for training LLMs.</p> 
  </div> 
  <div class="readable-text" id="p11"> 
   <h2 class=" readable-text-h2"><span class="num-string">2.1</span> Understanding word embeddings</h2> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>Deep neural network models, including LLMs, cannot process raw text directly. Since text is categorical, it isn’t compatible with the mathematical operations used to implement and train neural networks. Therefore, we need a way to represent words as continuous-valued vectors. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p13"> 
   <p><span class="print-book-callout-head">Note</span>  Readers unfamiliar with vectors and tensors in a computational context can learn more in appendix A, section A.2.2.</p> 
  </div> 
  <div class="readable-text" id="p14"> 
   <p>The concept of converting data into a vector format is often referred to as <em>embedding</em>. Using a specific neural network layer or another pretrained neural network model, we can embed different data types—for example, video, audio, and text, as illustrated in figure 2.2. However, it’s important to note that different data formats require distinct embedding models. For example, an embedding model designed for text would not be suitable for embedding audio or video data.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p15">  
   <img alt="figure" src="../Images/2-2.png" width="1100" height="566"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.2</span> Deep learning models cannot process data formats like video, audio, and text in their raw form. Thus, we use an embedding model to transform this raw data into a dense vector representation that deep learning architectures can easily understand and process. Specifically, this figure illustrates the process of converting raw data into a three-dimensional numerical vector.</h5>
  </div> 
  <div class="readable-text" id="p16"> 
   <p>At its core, an embedding is a mapping from discrete objects, such as words, images, or even entire documents, to points in a continuous vector space—the primary purpose of embeddings is to convert nonnumeric data into a format that neural networks can process.</p> 
  </div> 
  <div class="readable-text intended-text" id="p17"> 
   <p>While word embeddings are the most common form of text embedding, there are also embeddings for sentences, paragraphs, or whole documents. Sentence or paragraph embeddings are popular choices for<em> retrieval-augmented generation.</em> Retrieval-augmented generation combines generation (like producing text) with retrieval (like searching an external knowledge base) to pull relevant information when generating text, which is a technique that is beyond the scope of this book. Since our goal is to train GPT-like LLMs, which learn to generate text one word at a time, we will focus on word embeddings.</p> 
  </div> 
  <div class="readable-text intended-text" id="p18"> 
   <p>Several algorithms and frameworks have been developed to generate word embeddings. One of the earlier and most popular examples is the <em>Word2Vec</em> approach. Word2Vec trained neural network architecture to generate word embeddings by predicting the context of a word given the target word or vice versa. The main idea behind Word2Vec is that words that appear in similar contexts tend to have similar meanings. Consequently, when projected into two-dimensional word embeddings for visualization purposes, similar terms are clustered together, as shown in figure 2.3.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p19">  
   <img alt="figure" src="../Images/2-3.png" width="983" height="692"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.3</span> If word embeddings are two-dimensional, we can plot them in a two-dimensional scatterplot for visualization purposes as shown here. When using word embedding techniques, such as Word2Vec, words corresponding to similar concepts often appear close to each other in the embedding space. For instance, different types of birds appear closer to each other in the embedding space than in countries and cities.</h5>
  </div> 
  <div class="readable-text intended-text" id="p20"> 
   <p>Word embeddings can have varying dimensions, from one to thousands. A higher dimensionality might capture more nuanced relationships but at the cost of computational efficiency.</p> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>While we can use pretrained models such as Word2Vec to generate embeddings for machine learning models, LLMs commonly produce their own embeddings that are part of the input layer and are updated during training. The advantage of optimizing the embeddings as part of the LLM training instead of using Word2Vec is that the embeddings are optimized to the specific task and data at hand. We will implement such embedding layers later in this chapter. (LLMs can also create contextualized output embeddings, as we discuss in chapter 3.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p22"> 
   <p>Unfortunately, high-dimensional embeddings present a challenge for visualization because our sensory perception and common graphical representations are inherently limited to three dimensions or fewer, which is why figure 2.3 shows two-dimensional embeddings in a two-dimensional scatterplot. However, when working with LLMs, we typically use embeddings with a much higher dimensionality. For both GPT-2 and GPT-3, the embedding size (often referred to as the dimensionality of the model’s hidden states) varies based on the specific model variant and size. It is a tradeoff between performance and efficiency. The smallest GPT-2 models (117M and 125M parameters) use an embedding size of 768 dimensions to provide concrete examples. The largest GPT-3 model (175B parameters) uses an embedding size of 12,288 dimensions. </p> 
  </div> 
  <div class="readable-text intended-text" id="p23"> 
   <p>Next, we will walk through the required steps for preparing the embeddings used by an LLM, which include splitting text into words, converting words into tokens, and turning tokens into embedding vectors.</p> 
  </div> 
  <div class="readable-text" id="p24"> 
   <h2 class=" readable-text-h2"><span class="num-string">2.2</span> Tokenizing text</h2> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>Let’s discuss how we split input text into individual tokens, a required preprocessing step for creating embeddings for an LLM. These tokens are either individual words or special characters, including punctuation characters, as shown in figure 2.4.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p26">  
   <img alt="figure" src="../Images/2-4.png" width="827" height="664"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.4</span> A view of the text processing steps in the context of an LLM. Here, we split an input text into individual tokens, which are either words or special characters, such as punctuation characters.</h5>
  </div> 
  <div class="readable-text" id="p27"> 
   <p>The text we will tokenize for LLM training is “The Verdict,” a short story by Edith Wharton, which has been released into the public domain and is thus permitted to be used for LLM training tasks. The text is available on Wikisource at <a href="https://en.wikisource.org/wiki/The_Verdict">https://en.wikisource.org/wiki/The_Verdict</a>, and you can copy and paste it into a text file, which I copied into a text file <code>"the-verdict.txt".</code></p> 
  </div> 
  <div class="readable-text intended-text" id="p28"> 
   <p>Alternatively, you can find this <code>"the-verdict.txt"</code> file in this book’s GitHub repository at <a href="https://mng.bz/Adng">https://mng.bz/Adng</a>. You can download the file with the following Python code:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p29"> 
   <div class="code-area-container"> 
    <pre class="code-area">import urllib.request
url = ("https://raw.githubusercontent.com/rasbt/"
       "LLMs-from-scratch/main/ch02/01_main-chapter-code/"
       "the-verdict.txt")
file_path = "the-verdict.txt"
urllib.request.urlretrieve(url, file_path)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p30"> 
   <p>Next, we can load the <code>the-verdict.txt</code> file using Python’s standard file reading utilities. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p31"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.1</span> Reading in a short story as text sample into Python</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()
print("Total number of character:", len(raw_text))
print(raw_text[:99])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p32"> 
   <p>The print command prints the total number of characters followed by the first 99 characters of this file for illustration purposes:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p33"> 
   <div class="code-area-container"> 
    <pre class="code-area">Total number of character: 20479
I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p34"> 
   <p>Our goal is to tokenize this 20,479-character short story into individual words and special characters that we can then turn into embeddings for LLM training.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p35"> 
   <p><span class="print-book-callout-head">Note</span>  It’s common to process millions of articles and hundreds of thousands of books—many gigabytes of text—when working with LLMs. However, for educational purposes, it’s sufficient to work with smaller text samples like a single book to illustrate the main ideas behind the text processing steps and to make it possible to run it in a reasonable time on consumer hardware.</p> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>How can we best split this text to obtain a list of tokens? For this, we go on a small excursion and use Python’s regular expression library <code>re</code> for illustration purposes. (You don’t have to learn or memorize any regular expression syntax since we will later transition to a prebuilt tokenizer.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p37"> 
   <p>Using some simple example text, we can use the <code>re.split</code> command with the following syntax to split a text on whitespace characters:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p38"> 
   <div class="code-area-container"> 
    <pre class="code-area">import re
text = "Hello, world. This, is a test."
result = re.split(r'(\s)', text)
print(result)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>The result is a list of individual words, whitespaces, and punctuation characters:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p40"> 
   <div class="code-area-container"> 
    <pre class="code-area">['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>This simple tokenization scheme mostly works for separating the example text into individual words; however, some words are still connected to punctuation characters that we want to have as separate list entries. We also refrain from making all text lowercase because capitalization helps LLMs distinguish between proper nouns and common nouns, understand sentence structure, and learn to generate text with proper capitalization.</p> 
  </div> 
  <div class="readable-text intended-text" id="p42"> 
   <p>Let’s modify the regular expression splits on whitespaces (<code>\s</code>), commas, and periods (<code>[,.]</code>):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p43"> 
   <div class="code-area-container"> 
    <pre class="code-area">result = re.split(r'([,.]|\s)', text)
print(result)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p44"> 
   <p>We can see that the words and punctuation characters are now separate list entries just as we wanted:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p45"> 
   <div class="code-area-container"> 
    <pre class="code-area">['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is',
' ', 'a', ' ', 'test', '.', '']</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p46"> 
   <p>A small remaining problem is that the list still includes whitespace characters. Optionally, we can remove these redundant characters safely as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p47"> 
   <div class="code-area-container"> 
    <pre class="code-area">result = [item for item in result if item.strip()]
print(result)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>The resulting whitespace-free output looks like as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p49"> 
   <div class="code-area-container"> 
    <pre class="code-area">['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']</pre>  
   </div> 
  </div> 
  <div class="readable-text print-book-callout" id="p50"> 
   <p><span class="print-book-callout-head">Note</span>  When developing a simple tokenizer, whether we should encode whitespaces as separate characters or just remove them depends on our application and its requirements. Removing whitespaces reduces the memory and computing requirements. However, keeping whitespaces can be useful if we train models that are sensitive to the exact structure of the text (for example, Python code, which is sensitive to indentation and spacing). Here, we remove whitespaces for simplicity and brevity of the tokenized outputs. Later, we will switch to a tokenization scheme that includes whitespaces.</p> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>The tokenization scheme we devised here works well on the simple sample text. Let’s modify it a bit further so that it can also handle other types of punctuation, such as question marks, quotation marks, and the double-dashes we have seen earlier in the first 100 characters of Edith Wharton’s short story, along with additional special characters:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p52"> 
   <div class="code-area-container"> 
    <pre class="code-area">text = "Hello, world. Is this-- a test?"
result = re.split(r'([,.:;?_!"()\']|--|\s)', text)
result = [item.strip() for item in result if item.strip()]
print(result)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p53"> 
   <p>The resulting output is:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p54"> 
   <div class="code-area-container"> 
    <pre class="code-area">['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p55"> 
   <p>As we can see based on the results summarized in figure 2.5, our tokenization scheme can now handle the various special characters in the text successfully.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p56">  
   <img alt="figure" src="../Images/2-5.png" width="577" height="124"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.5</span> The tokenization scheme we implemented so far splits text into individual words and punctuation characters. In this specific example, the sample text gets split into 10 individual tokens.</h5>
  </div> 
  <div class="readable-text" id="p57"> 
   <p>Now that we have a basic tokenizer working, let’s apply it to Edith Wharton’s entire short story:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p58"> 
   <div class="code-area-container"> 
    <pre class="code-area">preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', raw_text)
preprocessed = [item.strip() for item in preprocessed if item.strip()]
print(len(preprocessed))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p59"> 
   <p>This print statement outputs <code>4690</code>, which is the number of tokens in this text (without whitespaces). Let’s print the first 30 tokens for a quick visual check:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p60"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(preprocessed[:30])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p61"> 
   <p>The resulting output shows that our tokenizer appears to be handling the text well since all words and special characters are neatly separated:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p62"> 
   <div class="code-area-container"> 
    <pre class="code-area">['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a',
'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough',
'--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to',
'hear', 'that', ',', 'in']</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p63"> 
   <h2 class=" readable-text-h2"><span class="num-string">2.3</span> Converting tokens into token IDs</h2> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>Next, let’s convert these tokens from a Python string to an integer representation to produce the token IDs. This conversion is an intermediate step before converting the token IDs into embedding vectors.</p> 
  </div> 
  <div class="readable-text intended-text" id="p65"> 
   <p>To map the previously generated tokens into token IDs, we have to build a vocabulary first. This vocabulary defines how we map each unique word and special character to a unique integer, as shown in figure 2.6.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p66">  
   <img alt="figure" src="../Images/2-6.png" width="1100" height="802"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.6</span> We build a vocabulary by tokenizing the entire text in a training dataset into individual tokens. These individual tokens are then sorted alphabetically, and duplicate tokens are removed. The unique tokens are then aggregated into a vocabulary that defines a mapping from each unique token to a unique integer value. The depicted vocabulary is purposely small and contains no punctuation or special characters for simplicity.</h5>
  </div> 
  <div class="readable-text" id="p67"> 
   <p>Now that we have tokenized Edith Wharton’s short story and assigned it to a Python variable called <code>preprocessed</code>, let’s create a list of all unique tokens and sort them alphabetically to determine the vocabulary size:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p68"> 
   <div class="code-area-container"> 
    <pre class="code-area">all_words = sorted(set(preprocessed))
vocab_size = len(all_words)
print(vocab_size)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p69"> 
   <p>After determining that the vocabulary size is 1,130 via this code, we create the vocabulary and print its first 51 entries for illustration purposes.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p70"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.2</span> Creating a vocabulary</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">vocab = {token:integer for integer,token in enumerate(all_words)}
for i, item in enumerate(vocab.items()):
    print(item)
    if i &gt;= 50:
        break</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p72"> 
   <div class="code-area-container"> 
    <pre class="code-area">('!', 0)
('"', 1)
("'", 2)
...
('Her', 49)
('Hermia', 50)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>As we can see, the dictionary contains individual tokens associated with unique integer labels. Our next goal is to apply this vocabulary to convert new text into token IDs (figure 2.7).<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p74">  
   <img alt="figure" src="../Images/2-7.png" width="897" height="657"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.7</span> Starting with a new text sample, we tokenize the text and use the vocabulary to convert the text tokens into token IDs. The vocabulary is built from the entire training set and can be applied to the training set itself and any new text samples. The depicted vocabulary contains no punctuation or special characters for simplicity.</h5>
  </div> 
  <div class="readable-text" id="p75"> 
   <p>When we want to convert the outputs of an LLM from numbers back into text, we need a way to turn token IDs into text. For this, we can create an inverse version of the vocabulary that maps token IDs back to the corresponding text tokens. </p> 
  </div> 
  <div class="readable-text intended-text" id="p76"> 
   <p>Let’s implement a complete tokenizer class in Python with an <code>encode</code> method that splits text into tokens and carries out the string-to-integer mapping to produce token IDs via the vocabulary. In addition, we’ll implement a <code>decode</code> method that carries out the reverse integer-to-string mapping to convert the token IDs back into text. The following listing shows the code for this tokenizer implementation.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p77"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.3</span> Implementing a simple text tokenizer</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">class SimpleTokenizerV1:
    def __init__(self, vocab):
        self.str_to_int = vocab           <span class="aframe-location"/> #1
        self.int_to_str = {i:s for s,i in vocab.items()}       <span class="aframe-location"/> #2

    def encode(self, text):        <span class="aframe-location"/> #3
        preprocessed = re.split(r'([,.?_!"()\']|--|\s)', text)
        preprocessed = [
            item.strip() for item in preprocessed if item.strip()
        ]
        ids = [self.str_to_int[s] for s in preprocessed]
        return ids

    def decode(self, ids):        <span class="aframe-location"/> #4
        text = " ".join([self.int_to_str[i] for i in ids]) 

        text = re.sub(r'\s+([,.?!"()\'])', r'\1', text)   <span class="aframe-location"/> #5
        return text</pre> 
    <div class="code-annotations-overlay-container">
     #1 Stores the vocabulary as a class attribute for access in the encode and decode methods
     <br/>#2 Creates an inverse vocabulary that maps token IDs back to the original text tokens
     <br/>#3 Processes input text into token IDs
     <br/>#4 Converts token IDs back into text
     <br/>#5 Removes spaces before the specified punctuation
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p78"> 
   <p>Using the <code>SimpleTokenizerV1</code> Python class, we can now instantiate new tokenizer objects via an existing vocabulary, which we can then use to encode and decode text, as illustrated in figure 2.8.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p79">  
   <img alt="figure" src="../Images/2-8.png" width="1100" height="685"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.8</span> Tokenizer implementations share two common methods: an encode method and a decode method. The encode method takes in the sample text, splits it into individual tokens, and converts the tokens into token IDs via the vocabulary. The decode method takes in token IDs, converts them back into text tokens, and concatenates the text tokens into natural text.</h5>
  </div> 
  <div class="readable-text intended-text" id="p80"> 
   <p>Let’s instantiate a new tokenizer object from the <code>SimpleTokenizerV1</code> class and tokenize a passage from Edith Wharton’s short story to try it out in practice:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p81"> 
   <div class="code-area-container"> 
    <pre class="code-area">tokenizer = SimpleTokenizerV1(vocab)
text = """"It's the last he painted, you know," 
       Mrs. Gisburn said with pardonable pride."""
ids = tokenizer.encode(text)
print(ids)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>The preceding code prints the following token IDs:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p83"> 
   <div class="code-area-container"> 
    <pre class="code-area">[1, 56, 2, 850, 988, 602, 533, 746, 5, 1126, 596, 5, 1, 67, 7, 38, 851, 1108, 754, 793, 7]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>Next, let’s see whether we can turn these token IDs back into text using the decode method:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p85"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(tokenizer.decode(ids))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>This outputs:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p87"> 
   <div class="code-area-container"> 
    <pre class="code-area">'" It\' s the last he painted, you know," Mrs. Gisburn said with 
pardonable pride.'</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>Based on this output, we can see that the decode method successfully converted the token IDs back into the original text.</p> 
  </div> 
  <div class="readable-text intended-text" id="p89"> 
   <p>So far, so good. We implemented a tokenizer capable of tokenizing and detokenizing text based on a snippet from the training set. Let’s now apply it to a new text sample not contained in the training set:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p90"> 
   <div class="code-area-container"> 
    <pre class="code-area">text = "Hello, do you like tea?"
print(tokenizer.encode(text))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>Executing this code will result in the following error:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p92"> 
   <div class="code-area-container"> 
    <pre class="code-area">KeyError: 'Hello'</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>The problem is that the word “Hello” was not used in the “The Verdict” short story. Hence, it is not contained in the vocabulary. This highlights the need to consider large and diverse training sets to extend the vocabulary when working on LLMs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p94"> 
   <p>Next, we will test the tokenizer further on text that contains unknown words and discuss additional special tokens that can be used to provide further context for an LLM during training.</p> 
  </div> 
  <div class="readable-text" id="p95"> 
   <h2 class=" readable-text-h2"><span class="num-string">2.4</span> Adding special context tokens</h2> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>We need to modify the tokenizer to handle unknown words. We also need to address the usage and addition of special context tokens that can enhance a model’s understanding of context or other relevant information in the text. These special tokens can include markers for unknown words and document boundaries, for example. In particular, we will modify the vocabulary and tokenizer, <code>SimpleTokenizerV2</code>, to support two new tokens, <code>&lt;|unk|&gt;</code> and <code>&lt;|endoftext|&gt;</code>, as illustrated in figure 2.9.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p97">  
   <img alt="figure" src="../Images/2-9.png" width="859" height="553"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.9</span> We add special tokens to a vocabulary to deal with certain contexts. For instance, we add an <code>&lt;|unk|&gt;</code> token to represent new and unknown words that were not part of the training data and thus not part of the existing vocabulary. Furthermore, we add an <code>&lt;|endoftext|&gt;</code> token that we can use to separate two unrelated text sources. </h5>
  </div> 
  <div class="readable-text" id="p98"> 
   <p>We can modify the tokenizer to use an <code>&lt;|unk|&gt;</code> token if it encounters a word that is not part of the vocabulary. Furthermore, we add a token between unrelated texts. For example, when training GPT-like LLMs on multiple independent documents or books, it is common to insert a token before each document or book that follows a previous text source, as illustrated in figure 2.10. This helps the LLM understand that although these text sources are concatenated for training, they are, in fact, unrelated.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p99">  
   <img alt="figure" src="../Images/2-10.png" width="862" height="529"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.10</span> When working with multiple independent text source, we add <code>&lt;|endoftext|&gt;</code> tokens between these texts. These <code>&lt;|endoftext|&gt;</code> tokens act as markers, signaling the start or end of a particular segment, allowing for more effective processing and understanding by the LLM.</h5>
  </div> 
  <div class="readable-text" id="p100"> 
   <p>Let’s now modify the vocabulary to include these two special tokens, <code>&lt;unk&gt;</code> and <code>&lt;|endoftext|&gt;</code>, by adding them to our list of all unique words:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p101"> 
   <div class="code-area-container"> 
    <pre class="code-area">all_tokens = sorted(list(set(preprocessed)))
all_tokens.extend(["&lt;|endoftext|&gt;", "&lt;|unk|&gt;"])
vocab = {token:integer for integer,token in enumerate(all_tokens)}

print(len(vocab.items()))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>Based on the output of this print statement, the new vocabulary size is 1,132 (the previous vocabulary size was 1,130).</p> 
  </div> 
  <div class="readable-text intended-text" id="p103"> 
   <p>As an additional quick check, let’s print the last five entries of the updated vocabulary:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p104"> 
   <div class="code-area-container"> 
    <pre class="code-area">for i, item in enumerate(list(vocab.items())[-5:]):
    print(item)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p105"> 
   <p>The code prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p106"> 
   <div class="code-area-container"> 
    <pre class="code-area">('younger', 1127)
('your', 1128)
('yourself', 1129)
('&lt;|endoftext|&gt;', 1130)
('&lt;|unk|&gt;', 1131)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p107"> 
   <p>Based on the code output, we can confirm that the two new special tokens were indeed successfully incorporated into the vocabulary. Next, we adjust the tokenizer from code listing 2.3 accordingly as shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p108"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.4</span> A simple text tokenizer that handles unknown words</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">class SimpleTokenizerV2:
    def __init__(self, vocab):
        self.str_to_int = vocab
        self.int_to_str = { i:s for s,i in vocab.items()}

    def encode(self, text):
        preprocessed = re.split(r'([,.:;?_!"()\']|--|\s)', text)
        preprocessed = [
            item.strip() for item in preprocessed if item.strip()
        ]
        preprocessed = [item if item in self.str_to_int           <span class="aframe-location"/> #1
                        else "&lt;|unk|&gt;" for item in preprocessed]

        ids = [self.str_to_int[s] for s in preprocessed]
        return ids

    def decode(self, ids):
        text = " ".join([self.int_to_str[i] for i in ids])

        text = re.sub(r'\s+([,.:;?!"()\'])', r'\1', text)   <span class="aframe-location"/> #2
        return text</pre> 
    <div class="code-annotations-overlay-container">
     #1 Replaces unknown words by &lt;|unk|&gt; tokens
     <br/>#2 Replaces spaces before the specified punctuations
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p109"> 
   <p>Compared to the <code>SimpleTokenizerV1</code> we implemented in listing 2.3, the new <code>SimpleTokenizerV2</code> replaces unknown words with <code>&lt;|unk|&gt;</code> tokens. </p> 
  </div> 
  <div class="readable-text intended-text" id="p110"> 
   <p>Let’s now try this new tokenizer out in practice. For this, we will use a simple text sample that we concatenate from two independent and unrelated sentences:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p111"> 
   <div class="code-area-container"> 
    <pre class="code-area">text1 = "Hello, do you like tea?"
text2 = "In the sunlit terraces of the palace."
text = " &lt;|endoftext|&gt; ".join((text1, text2))
print(text)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p112"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p113"> 
   <div class="code-area-container"> 
    <pre class="code-area">Hello, do you like tea? &lt;|endoftext|&gt; In the sunlit terraces of 
the palace.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p114"> 
   <p>Next, let’s tokenize the sample text using the <code>SimpleTokenizerV2</code> on the vocab we previously created in listing 2.2:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p115"> 
   <div class="code-area-container"> 
    <pre class="code-area">tokenizer = SimpleTokenizerV2(vocab)
print(tokenizer.encode(text))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p116"> 
   <p>This prints the following token IDs:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p117"> 
   <div class="code-area-container"> 
    <pre class="code-area">[1131, 5, 355, 1126, 628, 975, 10, 1130, 55, 988, 956, 984, 722, 988, 1131, 7]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p118"> 
   <p>We can see that the list of token IDs contains <code>1130</code> for the <code>&lt;|endoftext|&gt;</code> separator token as well as two <code>1131</code> tokens, which are used for unknown words. </p> 
  </div> 
  <div class="readable-text intended-text" id="p119"> 
   <p>Let’s detokenize the text for a quick sanity check:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p120"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(tokenizer.decode(tokenizer.encode(text)))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p121"> 
   <p>The output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p122"> 
   <div class="code-area-container"> 
    <pre class="code-area">&lt;|unk|&gt;, do you like tea? &lt;|endoftext|&gt; In the sunlit terraces of 
the &lt;|unk|&gt;.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p123"> 
   <p>Based on comparing this detokenized text with the original input text, we know that the training dataset, Edith Wharton’s short story “The Verdict,” does not contain the words “Hello” and “palace.”</p> 
  </div> 
  <div class="readable-text intended-text" id="p124"> 
   <p>Depending on the LLM, some researchers also consider additional special tokens such as the following:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p125"> <code>[BOS]</code> <em>(beginning of sequence) </em>—This token marks the start of a text. It signifies to the LLM where a piece of content begins. </li> 
   <li class="readable-text" id="p126"> <code>[EOS]</code> <em>(end of sequence) </em>—This token is positioned at the end of a text and is especially useful when concatenating multiple unrelated texts, similar to <code>&lt;|endoftext|&gt;</code>. For instance, when combining two different Wikipedia articles or books, the <code>[EOS]</code> token indicates where one ends and the next begins. </li> 
   <li class="readable-text" id="p127"> <code>[PAD]</code> <em>(padding) </em>—When training LLMs with batch sizes larger than one, the batch might contain texts of varying lengths. To ensure all texts have the same length, the shorter texts are extended or “padded” using the <code>[PAD]</code> token, up to the length of the longest text in the batch. </li> 
  </ul> 
  <div class="readable-text" id="p128"> 
   <p>The tokenizer used for GPT models does not need any of these tokens; it only uses an <code>&lt;|endoftext|&gt;</code> token for simplicity. <code>&lt;|endoftext|&gt;</code> is analogous to the <code>[EOS]</code> token. <code>&lt;|endoftext|&gt;</code> is also used for padding. However, as we’ll explore in subsequent chapters, when training on batched inputs, we typically use a mask, meaning we don’t attend to padded tokens. Thus, the specific token chosen for padding becomes inconsequential.</p> 
  </div> 
  <div class="readable-text intended-text" id="p129"> 
   <p>Moreover, the tokenizer used for GPT models also doesn’t use an <code>&lt;|unk|&gt;</code> token for out-of-vocabulary words. Instead, GPT models use a <em>byte pair encoding</em> tokenizer, which breaks words down into subword units, which we will discuss next.</p> 
  </div> 
  <div class="readable-text" id="p130"> 
   <h2 class=" readable-text-h2"><span class="num-string">2.5</span> Byte pair encoding</h2> 
  </div> 
  <div class="readable-text" id="p131"> 
   <p>Let’s look at a more sophisticated tokenization scheme based on a concept called byte pair encoding (BPE). The BPE tokenizer was used to train LLMs such as GPT-2, GPT-3, and the original model used in ChatGPT.</p> 
  </div> 
  <div class="readable-text intended-text" id="p132"> 
   <p>Since implementing BPE can be relatively complicated, we will use an existing Python open source library called <em>tiktoken</em> (<a href="https://github.com/openai/tiktoken">https://github.com/openai/tiktoken</a>), which implements the BPE algorithm very efficiently based on source code in Rust. Similar to other Python libraries, we can install the tiktoken library via Python’s <code>pip</code> installer from the terminal:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p133"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install tiktoken</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p134"> 
   <p>The code we will use is based on tiktoken 0.7.0. You can use the following code to check the version you currently have installed:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p135"> 
   <div class="code-area-container"> 
    <pre class="code-area">from importlib.metadata import version
import tiktoken
print("tiktoken version:", version("tiktoken"))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p136"> 
   <p>Once installed, we can instantiate the BPE tokenizer from tiktoken as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p137"> 
   <div class="code-area-container"> 
    <pre class="code-area">tokenizer = tiktoken.get_encoding("gpt2")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p138"> 
   <p>The usage of this tokenizer is similar to the <code>SimpleTokenizerV2</code> we implemented previously via an <code>encode</code> method:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p139"> 
   <div class="code-area-container"> 
    <pre class="code-area">text = (
    "Hello, do you like tea? &lt;|endoftext|&gt; In the sunlit terraces"
     "of someunknownPlace."
)
integers = tokenizer.encode(text, allowed_special={"&lt;|endoftext|&gt;"})
print(integers)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p140"> 
   <p>The code prints the following token IDs: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p141"> 
   <div class="code-area-container"> 
    <pre class="code-area">[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250,
 8812, 2114, 286, 617, 34680, 27271, 13]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p142"> 
   <p>We can then convert the token IDs back into text using the decode method, similar to our <code>SimpleTokenizerV2</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p143"> 
   <div class="code-area-container"> 
    <pre class="code-area">strings = tokenizer.decode(integers)
print(strings)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p144"> 
   <p>The code prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p145"> 
   <div class="code-area-container"> 
    <pre class="code-area">Hello, do you like tea? &lt;|endoftext|&gt; In the sunlit terraces of
 someunknownPlace.</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p146"> 
   <p>We can make two noteworthy observations based on the token IDs and decoded text. First, the <code>&lt;|endoftext|&gt;</code> token is assigned a relatively large token ID, namely, <code>50256</code>. In fact, the BPE tokenizer, which was used to train models such as GPT-2, GPT-3, and the original model used in ChatGPT, has a total vocabulary size of 50,257, with <code>&lt;|endoftext|&gt;</code> being assigned the largest token ID.</p> 
  </div> 
  <div class="readable-text intended-text" id="p147"> 
   <p>Second, the BPE tokenizer encodes and decodes unknown words, such as <code>someunknownPlace</code>, correctly. The BPE tokenizer can handle any unknown word. How does it achieve this without using <code>&lt;|unk|&gt;</code> tokens?</p> 
  </div> 
  <div class="readable-text intended-text" id="p148"> 
   <p>The algorithm underlying BPE breaks down words that aren’t in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words. So, thanks to the BPE algorithm, if the tokenizer encounters an unfamiliar word during tokenization, it can represent it as a sequence of subword tokens or characters, as illustrated in figure 2.11.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p149">  
   <img alt="figure" src="../Images/2-11.png" width="602" height="279"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.11</span> BPE tokenizers break down unknown words into subwords and individual characters. This way, a BPE tokenizer can parse any word and doesn’t need to replace unknown words with special tokens, such as <code>&lt;|unk|&gt;</code>.</h5>
  </div> 
  <div class="readable-text" id="p150"> 
   <p>The ability to break down unknown words into individual characters ensures that the tokenizer and, consequently, the LLM that is trained with it can process any text, even if it contains words that were not present in its training data.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p151"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 2.1 Byte pair encoding of unknown words </h5> 
   </div> 
   <div class="readable-text" id="p152"> 
    <p>Try the BPE tokenizer from the tiktoken library on the unknown words “Akwirw ier” and print the individual token IDs. Then, call the <code>decode</code> function on each of the resulting integers in this list to reproduce the mapping shown in figure 2.11. Lastly, call the decode method on the token IDs to check whether it can reconstruct the original input, “Akwirw ier.”</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p153"> 
   <p>A detailed discussion and implementation of BPE is out of the scope of this book, but in short, it builds its vocabulary by iteratively merging frequent characters into subwords and frequent subwords into words. For example, BPE starts with adding all individual single characters to its vocabulary (“a,” “b,” etc.). In the next stage, it merges character combinations that frequently occur together into subwords. For example, “d” and “e” may be merged into the subword “de,” which is common in many English words like “define,” “depend,” “made,” and “hidden.” The merges are determined by a frequency cutoff.</p> 
  </div> 
  <div class="readable-text" id="p154"> 
   <h2 class=" readable-text-h2"><span class="num-string">2.6</span> Data sampling with a sliding window</h2> 
  </div> 
  <div class="readable-text" id="p155"> 
   <p>The next step in creating the embeddings for the LLM is to generate the input–target pairs required for training an LLM. What do these input–target pairs look like? As we already learned, LLMs are pretrained by predicting the next word in a text, as depicted in figure 2.12.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p156">  
   <img alt="figure" src="../Images/2-12.png" width="752" height="354"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.12</span> Given a text sample, extract input blocks as subsamples that serve as input to the LLM, and the LLM’s prediction task during training is to predict the next word that follows the input block. During training, we mask out all words that are past the target. Note that the text shown in this figure must undergo tokenization before the LLM can process it; however, this figure omits the tokenization step for clarity.</h5>
  </div> 
  <div class="readable-text" id="p157"> 
   <p>Let’s implement a data loader that fetches the input–target pairs in figure 2.12 from the training dataset using a sliding window approach. To get started, we will tokenize the whole “The Verdict” short story using the BPE tokenizer:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p158"> 
   <div class="code-area-container"> 
    <pre class="code-area">with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()

enc_text = tokenizer.encode(raw_text)
print(len(enc_text))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p159"> 
   <p>Executing this code will return <code>5145</code>, the total number of tokens in the training set, after applying the BPE tokenizer.</p> 
  </div> 
  <div class="readable-text intended-text" id="p160"> 
   <p>Next, we remove the first 50 tokens from the dataset for demonstration purposes, as it results in a slightly more interesting text passage in the next steps:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p161"> 
   <div class="code-area-container"> 
    <pre class="code-area">enc_sample = enc_text[50:]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p162"> 
   <p>One of the easiest and most intuitive ways to create the input–target pairs for the next-word prediction task is to create two variables, <code>x</code> and <code>y</code>, where <code>x</code> contains the input tokens and <code>y</code> contains the targets, which are the inputs shifted by 1:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p163"> 
   <div class="code-area-container"> 
    <pre class="code-area">context_size = 4        <span class="aframe-location"/> #1
x = enc_sample[:context_size]
y = enc_sample[1:context_size+1]
print(f"x: {x}")
print(f"y:      {y}")</pre> 
    <div class="code-annotations-overlay-container">
     #1 The context size determines how many tokens are included in the input.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p164"> 
   <p>Running the previous code prints the following output:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p165"> 
   <div class="code-area-container"> 
    <pre class="code-area">x: [290, 4920, 2241, 287]
y:      [4920, 2241, 287, 257]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p166"> 
   <p>By processing the inputs along with the targets, which are the inputs shifted by one position, we can create the next-word prediction tasks (see figure 2.12), as follows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p167"> 
   <div class="code-area-container"> 
    <pre class="code-area">for i in range(1, context_size+1):
    context = enc_sample[:i]
    desired = enc_sample[i]
    print(context, "----&gt;", desired)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p168"> 
   <p>The code prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p169"> 
   <div class="code-area-container"> 
    <pre class="code-area">[290] ----&gt; 4920
[290, 4920] ----&gt; 2241
[290, 4920, 2241] ----&gt; 287
[290, 4920, 2241, 287] ----&gt; 257</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p170"> 
   <p>Everything left of the arrow (<code>----&gt;</code>) refers to the input an LLM would receive, and the token ID on the right side of the arrow represents the target token ID that the LLM is supposed to predict. Let’s repeat the previous code but convert the token IDs into text:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p171"> 
   <div class="code-area-container"> 
    <pre class="code-area">for i in range(1, context_size+1):
    context = enc_sample[:i]
    desired = enc_sample[i]
    print(tokenizer.decode(context), "----&gt;", tokenizer.decode([desired]))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p172"> 
   <p>The following outputs show how the input and outputs look in text format:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p173"> 
   <div class="code-area-container"> 
    <pre class="code-area"> and ----&gt;  established
 and established ----&gt;  himself
 and established himself ----&gt;  in
 and established himself in ----&gt;  a</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p174"> 
   <p>We’ve now created the input–target pairs that we can use for LLM training.</p> 
  </div> 
  <div class="readable-text intended-text" id="p175"> 
   <p>There’s only one more task before we can turn the tokens into embeddings: implementing an efficient data loader that iterates over the input dataset and returns the inputs and targets as PyTorch tensors, which can be thought of as multidimensional arrays. In particular, we are interested in returning two tensors: an input tensor containing the text that the LLM sees and a target tensor that includes the targets for the LLM to predict, as depicted in figure 2.13. While the figure shows the tokens in string format for illustration purposes, the code implementation will operate on token IDs directly since the <code>encode</code> method of the BPE tokenizer performs both tokenization and conversion into token IDs as a single step.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p176">  
   <img alt="figure" src="../Images/2-13.png" width="922" height="439"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.13</span> To implement efficient data loaders, we collect the inputs in a tensor, <code>x</code>, where each row represents one input context. A second tensor, <code>y</code>, contains the corresponding prediction targets (next words), which are created by shifting the input by one position.</h5>
  </div> 
  <div class="readable-text print-book-callout" id="p177"> 
   <p><span class="print-book-callout-head">Note</span>  For the efficient data loader implementation, we will use PyTorch’s built-in <code>Dataset</code> and <code>DataLoader</code> classes. For additional information and guidance on installing PyTorch, please see section A.2.1.3 in appendix A.</p> 
  </div> 
  <div class="readable-text" id="p178"> 
   <p>The code for the dataset class is shown in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p179"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.5</span> A dataset for batched inputs and targets</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import torch
from torch.utils.data import Dataset, DataLoader
class GPTDatasetV1(Dataset):
    def __init__(self, txt, tokenizer, max_length, stride):
        self.input_ids = []
        self.target_ids = []

        token_ids = tokenizer.encode(txt)   <span class="aframe-location"/> #1

        for i in range(0, len(token_ids) - max_length, stride):    <span class="aframe-location"/> #2
            input_chunk = token_ids[i:i + max_length]
            target_chunk = token_ids[i + 1: i + max_length + 1]
            self.input_ids.append(torch.tensor(input_chunk))
            self.target_ids.append(torch.tensor(target_chunk))

    def __len__(self):   <span class="aframe-location"/> #3
        return len(self.input_ids)

    def __getitem__(self, idx):        <span class="aframe-location"/> #4
        return self.input_ids[idx], self.target_ids[idx]</pre> 
    <div class="code-annotations-overlay-container">
     #1 Tokenizes the entire text
     <br/>#2 Uses a sliding window to chunk the book into overlapping sequences of max_length
     <br/>#3 Returns the total number of rows in the dataset
     <br/>#4 Returns a single row from the dataset
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p180"> 
   <p>The <code>GPTDatasetV1</code> class is based on the PyTorch <code>Dataset</code> class and defines how individual rows are fetched from the dataset, where each row consists of a number of token IDs (based on a <code>max_length</code>) assigned to an <code>input_chunk</code> tensor. The <code>target_ chunk</code> tensor contains the corresponding targets. I recommend reading on to see what the data returned from this dataset looks like when we combine the dataset with a PyTorch <code>DataLoader</code>—this will bring additional intuition and clarity.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p181"> 
   <p><span class="print-book-callout-head">Note</span>  If you are new to the structure of PyTorch <code>Dataset</code> classes, such as shown in listing 2.5, refer to section A.6 in appendix A, which explains the general structure and usage of PyTorch <code>Dataset</code> and <code>DataLoader</code> classes.</p> 
  </div> 
  <div class="readable-text" id="p182"> 
   <p>The following code uses the <code>GPTDatasetV1</code> to load the inputs in batches via a PyTorch <code>DataLoader</code>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p183"> 
   <h5 class=" listing-container-h5 browsable-container-h5"><span class="num-string">Listing 2.6</span> A data loader to generate batches with input-with pairs</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def create_dataloader_v1(txt, batch_size=4, max_length=256,
                         stride=128, shuffle=True, drop_last=True,
                         num_workers=0):
    tokenizer = tiktoken.get_encoding("gpt2")                        <span class="aframe-location"/> #1
    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)  <span class="aframe-location"/> #2
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        drop_last=drop_last,    <span class="aframe-location"/> #3
        num_workers=num_workers    <span class="aframe-location"/> #4
    )

    return dataloader</pre> 
    <div class="code-annotations-overlay-container">
     #1 Initializes the tokenizer
     <br/>#2 Creates dataset
     <br/>#3 drop_last=True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training.
     <br/>#4 The number of CPU processes to use for preprocessing
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p184"> 
   <p>Let’s test the <code>dataloader</code> with a batch size of 1 for an LLM with a context size of 4 to develop an intuition of how the <code>GPTDatasetV1</code> class from listing 2.5 and the <code>create_ dataloader_v1</code> function from listing 2.6 work together:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p185"> 
   <div class="code-area-container"> 
    <pre class="code-area">with open("the-verdict.txt", "r", encoding="utf-8") as f:
    raw_text = f.read()

dataloader = create_dataloader_v1(
    raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)
data_iter = iter(dataloader)     <span class="aframe-location"/> #1
first_batch = next(data_iter)
print(first_batch)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Converts dataloader into a Python iterator to fetch the next entry via Python’s built-in next() function
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p186"> 
   <p>Executing the preceding code prints the following:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p187"> 
   <div class="code-area-container"> 
    <pre class="code-area">[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p188"> 
   <p>The <code>first_batch</code> variable contains two tensors: the first tensor stores the input token IDs, and the second tensor stores the target token IDs. Since the <code>max_length</code> is set to 4, each of the two tensors contains four token IDs. Note that an input size of 4 is quite small and only chosen for simplicity. It is common to train LLMs with input sizes of at least 256.</p> 
  </div> 
  <div class="readable-text intended-text" id="p189"> 
   <p>To understand the meaning of <code>stride=1</code>, let’s fetch another batch from this dataset:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p190"> 
   <div class="code-area-container"> 
    <pre class="code-area">second_batch = next(data_iter)
print(second_batch)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p191"> 
   <p>The second batch has the following contents:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p192"> 
   <div class="code-area-container"> 
    <pre class="code-area">[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p193"> 
   <p>If we compare the first and second batches, we can see that the second batch’s token IDs are shifted by one position (for example, the second ID in the first batch’s input is 367, which is the first ID of the second batch’s input). The <code>stride</code> setting dictates the number of positions the inputs shift across batches, emulating a sliding window approach, as demonstrated in figure 2.14.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p194">  
   <img alt="figure" src="../Images/2-14.png" width="1100" height="783"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.14</span> When creating multiple batches from the input dataset, we slide an input window across the text. If the stride is set to 1, we shift the input window by one position when creating the next batch. If we set the stride equal to the input window size, we can prevent overlaps between the batches.</h5>
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p195"> 
    <h5 class=" callout-container-h5 readable-text-h5">Exercise 2.2 Data loaders with different strides and context sizes</h5> 
   </div> 
   <div class="readable-text" id="p196"> 
    <p>To develop more intuition for how the data loader works, try to run it with different settings such as <code>max_length=2</code> and <code>stride=2,</code> and <code>max_length=8</code> and <code>stride=2</code>.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p197"> 
   <p>Batch sizes of 1, such as we have sampled from the data loader so far, are useful for illustration purposes. If you have previous experience with deep learning, you may know that small batch sizes require less memory during training but lead to more noisy model updates. Just like in regular deep learning, the batch size is a tradeoff and a hyperparameter to experiment with when training LLMs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p198"> 
   <p>Let’s look briefly at how we can use the data loader to sample with a batch size greater than 1:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p199"> 
   <div class="code-area-container"> 
    <pre class="code-area">dataloader = create_dataloader_v1(
    raw_text, batch_size=8, max_length=4, stride=4,
    shuffle=False
)

data_iter = iter(dataloader)
inputs, targets = next(data_iter)
print("Inputs:\n", inputs)
print("\nTargets:\n", targets)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p200"> 
   <p>This prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p201"> 
   <div class="code-area-container"> 
    <pre class="code-area">Inputs:
 tensor([[   40,   367,  2885,  1464],
        [ 1807,  3619,   402,   271],
        [10899,  2138,   257,  7026],
        [15632,   438,  2016,   257],
        [  922,  5891,  1576,   438],
        [  568,   340,   373,   645],
        [ 1049,  5975,   284,   502],
        [  284,  3285,   326,    11]])

Targets:
 tensor([[  367,  2885,  1464,  1807],
        [ 3619,   402,   271, 10899],
        [ 2138,   257,  7026, 15632],
        [  438,  2016,   257,   922],
        [ 5891,  1576,   438,   568],
        [  340,   373,   645,  1049],
        [ 5975,   284,   502,   284],
        [ 3285,   326,    11,   287]])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p202"> 
   <p>Note that we increase the stride to 4 to utilize the data set fully (we don’t skip a single word). This avoids any overlap between the batches since more overlap could lead to increased overfitting.</p> 
  </div> 
  <div class="readable-text" id="p203"> 
   <h2 class=" readable-text-h2"><span class="num-string">2.7</span> Creating token embeddings</h2> 
  </div> 
  <div class="readable-text" id="p204"> 
   <p>The last step in preparing the input text for LLM training is to convert the token IDs into embedding vectors, as shown in figure 2.15.<span class="aframe-location"/> As a preliminary step, we must initialize these embedding weights with random values. This initialization serves as the starting point for the LLM’s learning process. In chapter 5, we will optimize the embedding weights as part of the LLM training.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p205">  
   <img alt="figure" src="../Images/2-15.png" width="827" height="649"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.15</span> Preparation involves tokenizing text, converting text tokens to token IDs, and converting token IDs into embedding vectors. Here, we consider the previously created token IDs to create the token embedding vectors.</h5>
  </div> 
  <div class="readable-text intended-text" id="p206"> 
   <p>A continuous vector representation, or embedding, is necessary since GPT-like LLMs are deep neural networks trained with the backpropagation algorithm. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p207"> 
   <p><span class="print-book-callout-head">Note</span>  If you are unfamiliar with how neural networks are trained with backpropagation, please read section A.4 in appendix A.</p> 
  </div> 
  <div class="readable-text" id="p208"> 
   <p>Let’s see how the token ID to embedding vector conversion works with a hands-on example. Suppose we have the following four input tokens with IDs 2, 3, 5, and 1:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p209"> 
   <div class="code-area-container"> 
    <pre class="code-area">input_ids = torch.tensor([2, 3, 5, 1])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p210"> 
   <p>For the sake of simplicity, suppose we have a small vocabulary of only 6 words (instead of the 50,257 words in the BPE tokenizer vocabulary), and we want to create embeddings of size 3 (in GPT-3, the embedding size is 12,288 dimensions):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p211"> 
   <div class="code-area-container"> 
    <pre class="code-area">vocab_size = 6
output_dim = 3</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p212"> 
   <p>Using the <code>vocab_size</code> and <code>output_dim</code>, we can instantiate an embedding layer in PyTorch, setting the random seed to <code>123</code> for reproducibility purposes:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p213"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.manual_seed(123)
embedding_layer = torch.nn.Embedding(vocab_size, output_dim)
print(embedding_layer.weight)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p214"> 
   <p>The print statement prints the embedding layer’s underlying weight matrix:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p215"> 
   <div class="code-area-container"> 
    <pre class="code-area">Parameter containing:
tensor([[ 0.3374, -0.1778, -0.1690],
        [ 0.9178,  1.5810,  1.3010],
        [ 1.2753, -0.2010, -0.1606],
        [-0.4015,  0.9666, -1.1481],
        [-1.1589,  0.3255, -0.6315],
        [-2.8400, -0.7849, -1.4096]], requires_grad=True)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p216"> 
   <p>The weight matrix of the embedding layer contains small, random values. These values are optimized during LLM training as part of the LLM optimization itself. Moreover, we can see that the weight matrix has six rows and three columns. There is one row for each of the six possible tokens in the vocabulary, and there is one column for each of the three embedding dimensions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p217"> 
   <p>Now, let’s apply it to a token ID to obtain the embedding vector: </p> 
  </div> 
  <div class="browsable-container listing-container" id="p218"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(embedding_layer(torch.tensor([3])))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p219"> 
   <p>The returned embedding vector is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p220"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=&lt;EmbeddingBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p221"> 
   <p>If we compare the embedding vector for token ID 3 to the previous embedding matrix, we see that it is identical to the fourth row (Python starts with a zero index, so it’s the row corresponding to index 3). In other words, the embedding layer is essentially a lookup operation that retrieves rows from the embedding layer’s weight matrix via a token ID.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p222"> 
   <p><span class="print-book-callout-head">Note</span>  For those who are familiar with one-hot encoding, the embedding layer approach described here is essentially just a more efficient way of implementing one-hot encoding followed by matrix multiplication in a fully connected layer, which is illustrated in the supplementary code on GitHub at <a href="https://mng.bz/ZEB5">https://mng.bz/ZEB5</a>. Because the embedding layer is just a more efficient implementation equivalent to the one-hot encoding and matrix-multiplication approach, it can be seen as a neural network layer that can be optimized via backpropagation.</p> 
  </div> 
  <div class="readable-text" id="p223"> 
   <p>We’ve seen how to convert a single token ID into a three-dimensional embedding vector. Let’s now apply that to all four input IDs (<code>torch.tensor([2,</code> <code>3,</code> <code>5,</code> <code>1])</code>):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p224"> 
   <div class="code-area-container"> 
    <pre class="code-area">print(embedding_layer(input_ids))</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p225"> 
   <p>The print output reveals that this results in a 4 × 3 matrix:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p226"> 
   <div class="code-area-container"> 
    <pre class="code-area">tensor([[ 1.2753, -0.2010, -0.1606],
        [-0.4015,  0.9666, -1.1481],
        [-2.8400, -0.7849, -1.4096],
        [ 0.9178,  1.5810,  1.3010]], grad_fn=&lt;EmbeddingBackward0&gt;)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p227"> 
   <p>Each row in this output matrix is obtained via a lookup operation from the embedding weight matrix, as illustrated in figure 2.16.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p228">  
   <img alt="figure" src="../Images/2-16.png" width="1100" height="656"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.16</span> Embedding layers perform a lookup operation, retrieving the embedding vector corresponding to the token ID from the embedding layer’s weight matrix. For instance, the embedding vector of the token ID 5 is the sixth row of the embedding layer weight matrix (it is the sixth instead of the fifth row because Python starts counting at 0). We assume that the token IDs were produced by the small vocabulary from section 2.3.</h5>
  </div> 
  <div class="readable-text intended-text" id="p229"> 
   <p>Having now created embedding vectors from token IDs, next we’ll add a small modification to these embedding vectors to encode positional information about a token within a text.</p> 
  </div> 
  <div class="readable-text" id="p230"> 
   <h2 class=" readable-text-h2"><span class="num-string">2.8</span> Encoding word positions</h2> 
  </div> 
  <div class="readable-text" id="p231"> 
   <p>In principle, token embeddings are a suitable input for an LLM. However, a minor shortcoming of LLMs is that their self-attention mechanism (see chapter 3) doesn’t have a notion of position or order for the tokens within a sequence. The way the previously introduced embedding layer works is that the same token ID always gets mapped to the same vector representation, regardless of where the token ID is positioned in the input sequence, as shown in figure 2.17.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p232">  
   <img alt="figure" src="../Images/2-17.png" width="677" height="374"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.17</span> The embedding layer converts a token ID into the same vector representation regardless of where it is located in the input sequence. For example, the token ID 5, whether it’s in the first or fourth position in the token ID input vector, will result in the same embedding vector.</h5>
  </div> 
  <div class="readable-text" id="p233"> 
   <p>In principle, the deterministic, position-independent embedding of the token ID is good for reproducibility purposes. However, since the self-attention mechanism of LLMs itself is also position-agnostic, it is helpful to inject additional position information into the LLM.</p> 
  </div> 
  <div class="readable-text intended-text" id="p234"> 
   <p>To achieve this, we can use two broad categories of position-aware embeddings: relative positional embeddings and absolute positional embeddings. Absolute positional embeddings are directly associated with specific positions in a sequence. For each position in the input sequence, a unique embedding is added to the token’s embedding to convey its exact location. For instance, the first token will have a specific positional embedding, the second token another distinct embedding, and so on, as illustrated in figure 2.18.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p235">  
   <img alt="figure" src="../Images/2-18.png" width="829" height="294"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.18</span> Positional embeddings are added to the token embedding vector to create the input embeddings for an LLM. The positional vectors have the same dimension as the original token embeddings. The token embeddings are shown with value 1 for simplicity.</h5>
  </div> 
  <div class="readable-text" id="p236"> 
   <p>Instead of focusing on the absolute position of a token, the emphasis of relative positional embeddings is on the relative position or distance between tokens. This means the model learns the relationships in terms of “how far apart” rather than “at which exact position.” The advantage here is that the model can generalize better to sequences of varying lengths, even if it hasn’t seen such lengths during training.</p> 
  </div> 
  <div class="readable-text intended-text" id="p237"> 
   <p>Both types of positional embeddings aim to augment the capacity of LLMs to understand the order and relationships between tokens, ensuring more accurate and context-aware predictions. The choice between them often depends on the specific application and the nature of the data being processed.</p> 
  </div> 
  <div class="readable-text intended-text" id="p238"> 
   <p>OpenAI’s GPT models use absolute positional embeddings that are optimized during the training process rather than being fixed or predefined like the positional encodings in the original transformer model. This optimization process is part of the model training itself. For now, let’s create the initial positional embeddings to create the LLM inputs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p239"> 
   <p>Previously, we focused on very small embedding sizes for simplicity. Now, let’s consider more realistic and useful embedding sizes and encode the input tokens into a 256-dimensional vector representation, which is smaller than what the original GPT-3 model used (in GPT-3, the embedding size is 12,288 dimensions) but still reasonable for experimentation. Furthermore, we assume that the token IDs were created by the BPE tokenizer we implemented earlier, which has a vocabulary size of 50,257:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p240"> 
   <div class="code-area-container"> 
    <pre class="code-area">vocab_size = 50257
output_dim = 256
token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p241"> 
   <p>Using the previous <code>token_embedding_layer</code>, if we sample data from the data loader, we embed each token in each batch into a 256-dimensional vector. If we have a batch size of 8 with four tokens each, the result will be an 8 × 4 × 256 tensor.</p> 
  </div> 
  <div class="readable-text intended-text" id="p242"> 
   <p>Let’s instantiate the data loader (see section 2.6) first:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p243"> 
   <div class="code-area-container"> 
    <pre class="code-area">max_length = 4
dataloader = create_dataloader_v1(
    raw_text, batch_size=8, max_length=max_length,
   stride=max_length, shuffle=False
)
data_iter = iter(dataloader)
inputs, targets = next(data_iter)
print("Token IDs:\n", inputs)
print("\nInputs shape:\n", inputs.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p244"> 
   <p>This code prints</p> 
  </div> 
  <div class="browsable-container listing-container" id="p245"> 
   <div class="code-area-container"> 
    <pre class="code-area">Token IDs:
 tensor([[   40,   367,  2885,  1464],
        [ 1807,  3619,   402,   271],
        [10899,  2138,   257,  7026],
        [15632,   438,  2016,   257],
        [  922,  5891,  1576,   438],
        [  568,   340,   373,   645],
        [ 1049,  5975,   284,   502],
        [  284,  3285,   326,    11]])

Inputs shape:
 torch.Size([8, 4])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p246"> 
   <p>As we can see, the token ID tensor is 8 × 4 dimensional, meaning that the data batch consists of eight text samples with four tokens each.</p> 
  </div> 
  <div class="readable-text intended-text" id="p247"> 
   <p>Let’s now use the embedding layer to embed these token IDs into 256-dimensional vectors:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p248"> 
   <div class="code-area-container"> 
    <pre class="code-area">token_embeddings = token_embedding_layer(inputs)
print(token_embeddings.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p249"> 
   <p>The print function call returns</p> 
  </div> 
  <div class="browsable-container listing-container" id="p250"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.Size([8, 4, 256])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p251"> 
   <p>The 8 × 4 × 256–dimensional tensor output shows that each token ID is now embedded as a 256-dimensional vector.</p> 
  </div> 
  <div class="readable-text intended-text" id="p252"> 
   <p>For a GPT model’s absolute embedding approach, we just need to create another embedding layer that has the same embedding dimension as the <code>token_embedding_ layer</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p253"> 
   <div class="code-area-container"> 
    <pre class="code-area">context_length = max_length
pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)
pos_embeddings = pos_embedding_layer(torch.arange(context_length))
print(pos_embeddings.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p254"> 
   <p>The input to the <code>pos_embeddings</code> is usually a placeholder vector <code>torch.arange(context_length)</code>, which contains a sequence of numbers 0, 1, ..., up to the maximum input length –1. The <code>context_length</code> is a variable that represents the supported input size of the LLM. Here, we choose it similar to the maximum length of the input text. In practice, input text can be longer than the supported context length, in which case we have to truncate the text.</p> 
  </div> 
  <div class="readable-text intended-text" id="p255"> 
   <p>The output of the print statement is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p256"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.Size([4, 256])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p257"> 
   <p>As we can see, the positional embedding tensor consists of four 256-dimensional vectors. We can now add these directly to the token embeddings, where PyTorch will add the 4 × 256–dimensional <code>pos_embeddings</code> tensor to each 4 × 256–dimensional token embedding tensor in each of the eight batches:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p258"> 
   <div class="code-area-container"> 
    <pre class="code-area">input_embeddings = token_embeddings + pos_embeddings
print(input_embeddings.shape)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p259"> 
   <p>The print output is</p> 
  </div> 
  <div class="browsable-container listing-container" id="p260"> 
   <div class="code-area-container"> 
    <pre class="code-area">torch.Size([8, 4, 256])</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p261"> 
   <p>The <code>input_embeddings</code> we created, as summarized in figure 2.19, are the embedded input examples that can now be processed by the main LLM modules, which we will begin implementing in the next chapter.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p262">  
   <img alt="figure" src="../Images/2-19.png" width="1100" height="1231"/> 
   <h5 class=" figure-container-h5"><span class="num-string">Figure 2.19</span> As part of the input processing pipeline, input text is first broken up into individual tokens. These tokens are then converted into token IDs using a vocabulary. The token IDs are converted into embedding vectors to which positional embeddings of a similar size are added, resulting in input embeddings that are used as input for the main LLM layers.</h5>
  </div> 
  <div class="readable-text" id="p263"> 
   <h2 class=" readable-text-h2">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p264"> LLMs require textual data to be converted into numerical vectors, known as embeddings, since they can’t process raw text. Embeddings transform discrete data (like words or images) into continuous vector spaces, making them compatible with neural network operations. </li> 
   <li class="readable-text" id="p265"> As the first step, raw text is broken into tokens, which can be words or characters. Then, the tokens are converted into integer representations, termed token IDs. </li> 
   <li class="readable-text" id="p266"> Special tokens, such as <code>&lt;|unk|&gt;</code> and <code>&lt;|endoftext|&gt;</code>, can be added to enhance the model’s understanding and handle various contexts, such as unknown words or marking the boundary between unrelated texts. </li> 
   <li class="readable-text" id="p267"> The byte pair encoding (BPE) tokenizer used for LLMs like GPT-2 and GPT-3 can efficiently handle unknown words by breaking them down into subword units or individual characters. </li> 
   <li class="readable-text" id="p268"> We use a sliding window approach on tokenized data to generate input–target pairs for LLM training. </li> 
   <li class="readable-text" id="p269"> Embedding layers in PyTorch function as a lookup operation, retrieving vectors corresponding to token IDs. The resulting embedding vectors provide continuous representations of tokens, which is crucial for training deep learning models like LLMs. </li> 
   <li class="readable-text" id="p270"> While token embeddings provide consistent vector representations for each token, they lack a sense of the token’s position in a sequence. To rectify this, two main types of positional embeddings exist: absolute and relative. OpenAI’s GPT models utilize absolute positional embeddings, which are added to the token embedding vectors and are optimized during the model training. </li> 
  </ul>
 </div></div></body></html>