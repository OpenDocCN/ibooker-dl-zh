- en: 6 Using context to learn domain-specific language
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 使用上下文学习特定领域的语言
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Classifying query intent
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类查询意图
- en: Query-sense disambiguation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询意义消歧
- en: Identifying key terminology from user signals
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从用户信号中识别关键术语
- en: Learning related phrases from user signals
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从用户信号中学习相关短语
- en: Learning misspellings and alternate term variations from user signals
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从用户信号中学习拼写错误和替代术语变体
- en: In chapter 5, we demonstrated both how to generate and use a semantic knowledge
    graph (SKG) and how to extract entities, facts, and relationships explicitly into
    a knowledge graph. Both techniques rely on navigating either the linguistic connections
    between terms in a single document or the statistical co-occurrences of terms
    across multiple documents and contexts. You learned to use knowledge graphs to
    find related terms, and how those related terms can integrate into various query-rewriting
    strategies to increase recall or precision.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在第五章中，我们展示了如何生成和使用语义知识图谱（SKG），以及如何将实体、事实和关系明确地提取到知识图谱中。这两种技术都依赖于在单个文档中术语之间的语言联系，或者多个文档和上下文中术语的统计共现。你学习了如何使用知识图谱来查找相关术语，以及这些相关术语如何整合到各种查询重写策略中，以提高召回率或精确度。
- en: In this chapter, we’ll dive deeper into understanding query intent and the nuances
    of using different contexts to interpret domain-specific terminology in queries.
    We’ll start by exploring query classification and then show how those classifications
    can be used to disambiguate queries with multiple potential meanings. Both approaches
    will extend our use of SKGs from the last chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更深入地了解查询意图以及使用不同上下文来解释查询中特定领域术语的细微差别。我们将从探索查询分类开始，然后展示如何使用这些分类来消除具有多种潜在意义的查询的歧义。这两种方法都将扩展我们在上一章中使用的SKG。
- en: While those SKG-based approaches are more effective at contextualizing and interpreting
    queries, they continue to rely on having high-quality documents that accurately
    represent your domain. As a result, their efficacy for interpreting user queries
    depends on how well the queries overlap with the content being searched.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于SKG的方法在语境化和解释查询方面更有效，但它们仍然依赖于拥有高质量文档，这些文档能够准确代表你的领域。因此，它们在解释用户查询方面的有效性取决于查询与搜索内容重叠的程度。
- en: For example, if 75% of your users are searching for clothing, but most of your
    inventory is films and digital media, then when they search for the query `shorts`
    and all the results are videos with short run times (known as “digital shorts”),
    most of your users will be confused by the results. Given the data in your query
    logs, it would be better if “shorts” could map to other related terms more commonly
    found in your query signals, like “pants”, “clothing”, and “shirts”.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你的75%用户正在搜索服装，但你的库存大多是电影和数字媒体，那么当用户搜索查询“短裤”时，所有结果都是运行时间短的视频（称为“数字短片”），大多数用户会对结果感到困惑。根据你的查询日志中的数据，如果“短裤”能够映射到你在查询信号中更常见的其他相关术语，如“裤子”、“服装”和“衬衫”，那就更好了。
- en: It can be very beneficial to not only rely on the content of your documents
    to learn relationships between terms and phrases, but to also use your user-generated
    signals. In this chapter, we’ll demonstrate techniques to extract key phrases,
    learn related phrases, and identify common misspellings or alternative spellings
    based on user signals. By using both content-based context and behavioral context
    from real user interactions, your search engine will better understand domain-specific
    terminology and actual user intent.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅依赖于文档内容来学习术语和短语之间的关系，而且使用用户生成的信号也是非常有益的。在本章中，我们将展示从用户信号中提取关键短语、学习相关短语以及识别常见拼写错误或替代拼写的技巧。通过结合基于内容的上下文和来自真实用户交互的行为上下文，你的搜索引擎将更好地理解特定领域的术语和实际用户意图。
- en: 6.1 Classifying query intent
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 分类查询意图
- en: The goal or intent of a query usually matters more than the keywords. A search
    for `driver crashed` can mean two *very* different things in the context of news
    or travel content versus a computer technology context. Similarly, someone searching
    in e-commerce for a specific product name or product ID is probably searching
    for a very specific item with a high likelihood of wanting to purchase it. A general
    search like `kitchen appliances` could indicate the user just intends to browse
    available products to see what’s available.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 查询的目标或意图通常比关键词更重要。在新闻或旅行内容与计算机技术背景下，搜索`driver crashed`可能意味着两种 *非常* 不同的含义。同样，在电子商务中搜索特定产品名称或产品ID的人可能正在寻找一个非常具体的商品，并且有很大可能性想要购买它。像`kitchen
    appliances`这样的通用搜索可能表明用户只是打算浏览可用的产品，看看有什么可用。
- en: In both contexts, a query classifier can be effective at determining the general
    kind of query being issued. Depending on the domain, a query’s context could be
    automatically applied (e.g., filtering the category of documents), used to modify
    the relevance algorithm (automatically boosting specific products), or even used
    to drive a different user experience (skipping the results page and going directly
    to a specific product’s page). In this section, we’ll show how to use the SKG
    from chapter 5 as a classifier for incoming queries to build a query classifier.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，查询分类器可以有效地确定所发出查询的一般类型。根据领域，查询的上下文可以自动应用（例如，过滤文档的类别），用于修改相关性算法（自动提升特定产品），甚至可以用来驱动不同的用户体验（跳过结果页面，直接进入特定产品的页面）。在本节中，我们将展示如何使用第5章中的SKG作为分类器，为传入的查询构建查询分类器。
- en: An SKG traversal does a *k*-nearest neighbor search at each level of the graph
    traversal. *K*-nearest neighbor is a type of classification that takes a data
    point (such as a query or term) and tries to find the top *k* most similar other
    data points in a vector space. If we have a field like `category` or `classification`
    on our documents, we can ask the SKG to “find the category with the highest relatedness
    to my starting node”. Since the starting node is typically a user’s query, an
    SKG can classify that query.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: SKG遍历在每个图遍历级别执行 *k* 个最近邻搜索。*K* 个最近邻是一种分类，它接受一个数据点（如查询或术语）并尝试在向量空间中找到最接近的 *k*
    个其他数据点。如果我们文档上有`category`或`classification`这样的字段，我们可以要求SKG“找到与我的起始节点最相关的类别”。由于起始节点通常是用户的查询，SKG可以对该查询进行分类。
- en: We’ll continue to use the indexed Stack Exchange datasets as an SKG to be extended
    for query classification (in this section) and query-sense disambiguation (in
    section 6.2).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用索引的Stack Exchange数据集作为SKG，以扩展查询分类（在本节中）和查询意义消歧（在第6.2节中）。
- en: Listing 6.1 shows a function that takes a user query and traverses the SKG to
    find semantically related categories to classify the query. Since we’ve indexed
    multiple different Stack Exchange categories (scifi, health, cooking, devops,
    etc.), we’ll use those categories as our classifications.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.1显示了一个函数，它接受用户查询并遍历SKG以找到与查询语义相关的类别进行分类。由于我们已经索引了多个不同的Stack Exchange类别（科幻、健康、烹饪、devops等），我们将使用这些类别作为我们的分类。
- en: Listing 6.1 Query classification using the SKG
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.1 使用SKG进行查询分类
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 The initial node of the graph based on a query matching against a field'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 基于查询匹配字段而确定的图初始节点'
- en: '#2 The field from which we’ll find related classifications. In this case, we
    traverse to the category field.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 我们将从中找到相关分类的字段。在这种情况下，我们遍历到类别字段。'
- en: '#3 Only classifications occurring in at least this number of documents will
    be returned.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 只有至少出现在这么多文档中的分类才会被返回。'
- en: '#4 Sets the number of classifications to return'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 设置要返回的分类数量'
- en: '#5 Traverses the SKG to classify the query'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 遍历SKG以分类查询'
- en: '#6 Prints a query and its classifications'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 打印查询及其分类'
- en: 'Example query classifications:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 示例查询分类：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This request uses the SKG to find the top *k* nearest neighbors based on a comparison
    of the semantic similarity between the query and each available classification
    (within the `category` field).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个请求使用SKG根据查询与每个可用分类（在`category`字段内）之间的语义相似度比较，找到最接近的 *k* 个邻居。
- en: We see classification scores for each potential category for each query, with
    `airplane` and `passport` classified to “travel”, `vitamins` classified to “health”
    and “cooking”, and `alien` classified to “scifi”. When we refine the `airplane`
    query to a more specific query like `airplane AND crash`, however, we see that
    the classification changes from “travel” to “scifi”, because documents about airplane
    crashes are more likely to occur within “scifi” documents than “travel” documents.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到每个查询的每个潜在类别的分类得分，例如，“飞机”和“护照”被分类为“旅行”，“维生素”被分类为“健康”和“烹饪”，“外星人”被分类为“科幻”。然而，当我们将“飞机”查询细化为一个更具体的查询，如“飞机
    AND 碰撞”时，我们看到分类从“旅行”变为“科幻”，因为关于飞机坠毁的文档更有可能出现在“科幻”文档中，而不是“旅行”文档中。
- en: As another example, `driver` could have multiple meanings. It returns two potential
    classifications (“travel” or “devops”), with the “travel” category being the clear
    choice when no other context is provided. When additional context *is* provided,
    however, we can see that the query `driver AND taxi` gets appropriately classified
    to the “travel” category, while `driver AND install` gets appropriately classified
    to the “devops” category.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，“司机”可能有多种含义。它返回两个潜在分类（“旅行”或“devops”），在没有其他上下文提供的情况下，“旅行”类别是明显的选择。然而，当提供额外的上下文时，我们可以看到查询“司机
    AND 出租车”被适当地分类到“旅行”类别，而“司机 AND 安装”则被适当地分类到“devops”类别。
- en: This ability for the SKG to find semantic relationships between arbitrary combinations
    of terms makes it useful for on-the-fly classification of incoming queries. You
    can auto-apply the classifications as query filters or boosts, route queries to
    a context-specific algorithm or landing page, or automatically disambiguate query
    terms. We’ll explore using a two-level graph traversal in the next section to
    implement query-sense disambiguation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: SKG能够找到任意组合的术语之间的语义关系，这使得它在实时分类传入查询时非常有用。您可以自动应用分类作为查询过滤器或提升，将查询路由到特定上下文的算法或着陆页，或自动消歧查询术语。我们将在下一节中探讨使用两级图遍历来实现查询意义消歧。
- en: 6.2 Query-sense disambiguation
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 查询意义消歧
- en: When interpreting users’ intent from their queries, understanding exactly what
    they meant by each word is challenging. The problem of polysemy, or ambiguous
    terms, can significantly affect your search results.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当从用户的查询中解读用户意图时，理解他们每个词的确切含义是具有挑战性的。多义性问题或模糊术语可能会显著影响您的搜索结果。
- en: If someone searches for `server`, this could refer to someone who takes orders
    and waits on tables at a restaurant, or it could mean a computer that runs software
    on a network. Ideally, we want our search engine to be able to disambiguate each
    of these word senses and generate a unique list of related terms within each disambiguated
    context. Figure 6.1 demonstrates these two potential contexts for the word “server”
    and the kinds of related terms one might find within each context.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有人搜索“服务器”，这可能指的是在餐厅接受订单并服务等候的员工，或者它可能意味着运行网络软件的计算机。理想情况下，我们希望我们的搜索引擎能够区分这些词义，并在每个区分的上下文中生成一个独特的相关术语列表。图6.1展示了“服务器”这个词的两个潜在上下文以及在每个上下文中可能找到的相关术语类型。
- en: '![figure](../Images/CH06_F01_Grainger.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH06_F01_Grainger.png)'
- en: Figure 6.1 Differentiating multiple senses of the ambiguous term “server”
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1 区分模糊术语“服务器”的多个意义
- en: In section 6.1, we demonstrated how to use an SKG to automatically classify
    queries into a set of known categories. Given that we already know how to classify
    our queries, adding a second-level traversal can provide a contextualized list
    of related terms for each query classification.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6.1节中，我们展示了如何使用SKG（语义知识图谱）自动将查询分类到一组已知类别中。鉴于我们已经知道如何对查询进行分类，添加二级遍历可以为每个查询分类提供一个相关术语的上下文列表。
- en: In other words, by traversing from query to classification and then to terms,
    we can generate a list of terms that describe a contextualized interpretation
    of the original query within each of the top classifications. The following listing
    shows a function that disambiguates a query this way utilizing an SKG.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，通过从查询遍历到分类，然后到术语，我们可以在每个顶级分类中生成一个术语列表，这些术语描述了原始查询在上下文中的语境化解释。以下列表显示了一个使用SKG以这种方式消歧查询的功能。
- en: Listing 6.2 Disambiguating query intent across different contexts
  id: totrans-39
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.2 在不同上下文中消歧查询意图
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 The starting node of the graph traversal (the user’s query)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 图形遍历的起始节点（用户的查询）'
- en: '#2 The first traversal returns the contexts for disambiguating the query.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 第一次遍历返回用于区分查询的上下文。'
- en: '#3 The second traversal is from keywords related to both the query AND each
    related context.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 第二次遍历是从与查询和每个相关上下文相关的关键词开始的。'
- en: You can see from this listing that a context field (the `category` field by
    default) and a keywords field (the `body` field by default) are used as part of
    a two-level traversal. For any query that is passed in, we first find the most
    semantically related category and then the terms most semantically related to
    the original query within that category.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从这个列表中看到，上下文字段（默认为`category`字段）和关键词字段（默认为`body`字段）被用作两级遍历的一部分。对于任何传入的查询，我们首先找到最相关的语义类别，然后在该类别中找到与原始查询最相关的术语。
- en: The following listing demonstrates how to call this function, passing in three
    different queries containing ambiguous terms for which we want to find differentiated
    meanings.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表展示了如何调用此函数，传递包含模糊术语的三个不同查询，我们希望找到这些术语的不同含义。
- en: Listing 6.3 Running query-sense disambiguation for several queries
  id: totrans-46
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.3 运行多个查询的查询意义区分
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The results of the queries in listing 6.3 can be found in tables 6.1–6.3, followed
    by the search-engine-specific SKG request used to disambiguate `chef` in listing
    6.4\. Each disambiguation context (`category` field) is scored relative to the
    query, and each discovered keyword (`body` field) is scored relative to both the
    query and the disambiguation context.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.3中查询的结果可以在表6.1–6.3中找到，随后是用于在列表6.4中区分`chef`的搜索引擎特定SKG请求。每个区分上下文（`category`字段）相对于查询进行评分，每个发现的关键词（`body`字段）相对于查询和区分上下文进行评分。
- en: Table 6.1 Related terms lists contextualized by category for the query `server`
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.1 相关术语列表根据类别对查询`服务器`进行上下文化
- en: '| Query: server |  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 查询：服务器 |  |'
- en: '| --- | --- |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `Context: devops 0.83796` `Keywords:`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '| `上下文：devops 0.83796` `关键词：`'
- en: '`server 0.93698`'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`服务器 0.93698`'
- en: '`servers 0.76818`'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`服务器 0.76818`'
- en: '`docker 0.75955`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker 0.75955`'
- en: '`code 0.72832`'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`代码 0.72832`'
- en: '`configuration 0.70686`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`配置 0.70686`'
- en: '`deploy 0.70634`'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`部署 0.70634`'
- en: '`nginx 0.70366`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`nginx 0.70366`'
- en: '`jenkins 0.69934`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`jenkins 0.69934`'
- en: '`git 0.68932`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`git 0.68932`'
- en: '`ssh 0.6836`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`ssh 0.6836`'
- en: '| `Context: cooking -0.1574` `Keywords:`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '| `上下文：烹饪 -0.1574` `关键词：`'
- en: '`server 0.66363`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`服务器 0.66363`'
- en: '`restaurant 0.16482`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`餐厅 0.16482`'
- en: '`pie 0.12882`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`派 0.12882`'
- en: '`served 0.12098`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`已提供 0.12098`'
- en: '`restaurants 0.11679`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`餐厅 0.11679`'
- en: '`knife 0.10788`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`刀片 0.10788`'
- en: '`pieces 0.10135`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`部件 0.10135`'
- en: '`serve 0.08934`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`服务 0.08934`'
- en: '`staff 0.0886`'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`员工 0.0886`'
- en: '`dish 0.08553`'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`菜肴 0.08553`'
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| `Context: travel -0.15959` `Keywords:`'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '| `上下文：旅行 -0.15959` `关键词：`'
- en: '`server 0.81226`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`服务器 0.81226`'
- en: '`tipping 0.54391`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`小费 0.54391`'
- en: '`vpn 0.45352`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`VPN 0.45352`'
- en: '`tip 0.41117`'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`小费 0.41117`'
- en: '| `Context: scifi -0.28208` `Keywords:`'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '| `上下文：科幻 -0.28208` `关键词：`'
- en: '`server 0.78173`'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`服务器 0.78173`'
- en: '`flynn''s 0.53341`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`弗莱尼的 0.53341`'
- en: '`computer 0.28075`'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`计算机 0.28075`'
- en: '`computers 0.2593`'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`计算机 0.2593`'
- en: '|'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| `servers 0.39053` `firewall 0.33092`'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '| `服务器 0.39053` `防火墙 0.33092`'
- en: '`restaurant 0.21698`'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`餐厅 0.21698`'
- en: '`tips 0.19524`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`小贴士 0.19524`'
- en: '`bill 0.18951`'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`账单 0.18951`'
- en: '`cash 0.18485`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`现金 0.18485`'
- en: '| `flynn 0.24963` `servers 0.24778`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '| `弗莱尼 0.24963` `服务器 0.24778`'
- en: '`grid 0.23889`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`网格 0.23889`'
- en: '`networking 0.2178`'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`网络 0.2178`'
- en: '`shutdown 0.21121`'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`关闭 0.21121`'
- en: '`hacker 0.19444`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`黑客 0.19444`'
- en: '|'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Table 6.1 shows the top most semantically related categories for the query `server`,
    followed by the most semantically related keywords from the `body` field within
    each of those category contexts. Based on the data, we see that the category of
    “devops” is the most semantically related (positive score of `0.83796`), whereas
    the next three categories all contain negative scores (`-0.1574` for “cooking”,
    `-0.15959` for “travel”, and `-0.28208` for “scifi”). For the query `server`,
    the “devops” category is thus overwhelmingly the most likely category to be relevant.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1显示了查询`服务器`的最相关语义类别，随后是每个类别上下文中的`body`字段中最相关的关键词。根据数据，我们看到“devops”类别的语义相关性最高（得分为`0.83796`），而接下来的三个类别都包含负分数（“烹饪”为`-0.1574`，“旅行”为`-0.15959`，“科幻”为`-0.28208`）。对于查询`服务器`，因此“devops”类别是最可能相关的类别。
- en: If we look at the different term lists that come back for each of the categories,
    we also see several distinct meanings arise. In the “devops” category, the meaning
    of the term “server” is focused on tools related to managing, building, and deploying
    code to a computer server. In the “scifi” category, the meaning revolves around
    computer grids being hacked and having their networks shut down. In the “travel”
    category, on the other hand, the overwhelming sense of the word “server” is related
    to some-one working in a restaurant, with terms like “tipping”, “restaurant”,
    and “bill” showing up.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看每个类别返回的不同术语列表，我们也会看到出现几个不同的含义。在“devops”类别中，术语“server”的含义集中在与管理和部署代码到计算机服务器相关的工具上。在“scifi”类别中，含义则围绕计算机网格被黑客攻击和网络被关闭。在“travel”类别中，另一方面，“server”一词的压倒性含义与在餐厅工作的人相关，如“小费”、“餐厅”和“账单”等术语的出现。
- en: When implementing an intelligent search application using this data, if you
    know the user’s context is related to travel, it makes sense to use the specific
    meaning within the “travel” category. If the context is unknown, the best choice
    is usually either the most semantically related category or the most popular category
    among your users.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用此数据实现智能搜索应用时，如果你知道用户的上下文与旅行相关，那么在“旅行”类别中使用特定的含义是有意义的。如果上下文未知，通常最好的选择是使用与语义最相关的类别或用户中最受欢迎的类别。
- en: Table 6.2 Contextualized related terms lists by category for the query `driver`
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 6.2 查询“driver”的按类别划分的上下文相关术语列表
- en: '| Query: driver |  |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 查询：driver |  |'
- en: '| --- | --- |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| --- | ---'
- en: '| `Context: travel 0.38996` `Keywords:`'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '| `上下文：旅行 0.38996` `关键词：`'
- en: '`driver 0.93417`'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`driver 0.93417`'
- en: '`drivers 0.76932`'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`drivers 0.76932`'
- en: '`taxi 0.71977`'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`taxi 0.71977`'
- en: '`car 0.65572`'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`car 0.65572`'
- en: '`license 0.61319`'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`license 0.61319`'
- en: '`driving 0.60849`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`driving 0.60849`'
- en: '`taxis 0.57708`'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`taxis 0.57708`'
- en: '`traffic 0.52823`'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`traffic 0.52823`'
- en: '`bus 0.52306`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`bus 0.52306`'
- en: '`driver''s 0.51043`'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`driver''s 0.51043`'
- en: '| `Context: devops 0.08917` `Keywords:`'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '| `上下文：devops 0.08917` `关键词：`'
- en: '`ipam 0.78219`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`ipam 0.78219`'
- en: '`driver 0.77583`'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`driver 0.77583`'
- en: '`aufs 0.73758`'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '`aufs 0.73758`'
- en: '`overlayfs 0.73758`'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`overlayfs 0.73758`'
- en: '`container_name 0.73483`'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`container_name 0.73483`'
- en: '`overlay2 0.69079`'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`overlay2 0.69079`'
- en: '`cgroup 0.68438`'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`cgroup 0.68438`'
- en: '`docker 0.67529`'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker 0.67529`'
- en: '`compose.yml 0.65012`'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`compose.yml 0.65012`'
- en: '`compose 0.55631`'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`compose 0.55631`'
- en: '|'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Table 6.2 demonstrates a query-sense disambiguation for the query `driver`.
    In this case, there are two related categories, with “travel” being the most semantically
    related (`0.38996`) versus “devops” (`0.08917`). We can see two very distinct
    meanings of “driver” appear within each of these contexts, with “driver” in the
    “travel” category being related to “taxi”, “car”, “license”, “driving”, and “bus”,
    whereas within the “devops” category “driver” is related to “ipam”, “aufs”, and
    “overlayfs”, which are all different kinds of computer-related drivers.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.2 展示了查询“driver”的查询意义消歧。在这种情况下，有两个相关类别，“travel”与语义最相关（`0.38996`）相对于“devops”（`0.08917`）。我们可以看到在每个上下文中“driver”有两个非常不同的含义，其中“travel”类别中的“driver”与“出租车”、“汽车”、“驾照”、“驾驶”和“公交车”相关，而在“devops”类别中，“driver”与“ipam”、“aufs”和“overlayfs”相关，这些都是不同类型的计算机驱动程序。
- en: If someone searches for `driver`, they usually do not intend to find documents
    about both meanings of the word in the search results. There are several ways
    to deal with multiple potential meanings for queried keywords, such as grouping
    results by meaning to highlight the differences, choosing only the most likely
    meaning, carefully interspersing different meanings within the search results
    to provide diversity, or providing alternative query suggestions for different
    contexts. An intentional choice here is usually much better than lazily lumping
    multiple different meanings together.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有人搜索“driver”，他们通常不希望搜索结果中出现关于该词两种含义的文档。处理查询关键词的多个潜在含义有几种方法，例如按含义分组结果以突出差异，仅选择最可能的含义，在搜索结果中仔细穿插不同的含义以提供多样性，或者为不同的上下文提供替代查询建议。在这里进行有意的选择通常比随意将多个不同的含义混合在一起要好得多。
- en: Table 6.3 Contextualized related terms lists by category for the query `chef`
  id: totrans-128
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 6.3 查询“chef”的按类别划分的上下文相关术语列表
- en: '| Query: chef |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 查询：chef |  |'
- en: '| --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | ---'
- en: '| `Context: cooking 0.37731` `Keywords:`'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '| `上下文：cooking 0.37731` `关键词：`'
- en: '`chef 0.93239`'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`chef 0.93239`'
- en: '`chefs 0.5151`'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`chefs 0.5151`'
- en: '`www.pamperedchef.com 0.41292`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`www.pamperedchef.com 0.41292`'
- en: '`kitchen 0.39127`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`kitchen 0.39127`'
- en: '`restaurant 0.38975`'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`restaurant 0.38975`'
- en: '`cooking 0.38332`'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`cooking 0.38332`'
- en: '`chef''s 0.37392`'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`chef''s 0.37392`'
- en: '`professional 0.36688`'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '`professional 0.36688`'
- en: '`nakiri 0.36599`'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`nakiri 0.36599`'
- en: '`pampered 0.34736`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '`pampered 0.34736`'
- en: '| `Context: devops 0.34959` `Keywords:`'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '| `上下文：devops 0.34959` `关键词：`'
- en: '`chef 0.87653`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`chef 0.87653`'
- en: '`puppet 0.79142`'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`puppet 0.79142`'
- en: '`docs.chef.io 0.7865`'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`docs.chef.io 0.7865`'
- en: '`ansible 0.73888`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`ansible 0.73888`'
- en: '`www.chef.io 0.72073`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '`www.chef.io 0.72073`'
- en: '`learn.chef.io 0.71902`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`learn.chef.io 0.71902`'
- en: '`default.rb 0.70194`'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`default.rb 0.70194`'
- en: '`configuration 0.68296`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`configuration 0.68296`'
- en: '`inspec 0.65237`'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`inspec 0.65237`'
- en: '`cookbooks 0.61503`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`cookbooks 0.61503`'
- en: '|'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: As a final example, table 6.3 demonstrates the query disambiguation for the
    query `chef`. The top two contexts both show reasonably positive relatedness scores,
    indicating that both meanings are likely interpretations. While the “cooking”
    context has a slightly higher score (`0.37731`) than the “devops” context (`0.34959`),
    it would still be important to consider the user’s context as far as possible
    when choosing between these two meanings. The meaning of `chef` within the “devops”
    context is related to the Chef configuration management software used to build
    and deploy servers (related terms include “puppet” and “ansible”), whereas within
    the “cooking” context it refers to a person who prepares food (“cooking”, “taste”,
    “restaurant”, “ingredients”). The Chef software borrows inspiration from the cooking
    domain as a metaphor for how to prepare and serve software, so it’s not surprising
    to see a term like “cookbooks” appear in the “devops” category.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的例子，表6.3展示了查询`chef`的消歧。前两个上下文都显示了合理的正相关度分数，表明这两种含义都可能是解释。虽然“烹饪”上下文的分数（`0.37731`）略高于“devops”上下文（`0.34959`），但在选择这两种含义之间时，尽可能考虑用户上下文仍然很重要。在“devops”上下文中，“chef”的含义与用于构建和部署服务器的Chef配置管理软件相关（相关术语包括“puppet”和“ansible”），而在“烹饪”上下文中，它指的是准备食物的人（“烹饪”、“口味”、“餐厅”、“食材”）。Chef软件从烹饪领域汲取灵感，作为准备和提供软件的隐喻，因此看到“cookbooks”这样的术语出现在“devops”类别中并不令人惊讶。
- en: The search-engine-specific SKG request used to disambiguate a query can be seen
    by invoking the `print_disambigutaion_request` function. This can be useful for
    understanding and running the internal SKG request directly against your configured
    search engine or vector database. The Solr-specific SKG request syntax printed
    for this `chef` query-sense disambiguation function call is shown in the following
    listing.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 用于消歧查询的特定搜索引擎SKG请求可以通过调用`print_disambigutaion_request`函数来查看。这有助于理解和直接在配置的搜索引擎或向量数据库上运行内部SKG请求。为这个`chef`查询含义消歧函数调用打印的Solr特定SKG请求语法如下所示。
- en: Listing 6.4 Solr SKG disambiguation request for the query `chef`
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.4为查询`chef`的Solr SKG消歧请求
- en: '[PRE4]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Result:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 结果：
- en: '[PRE5]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 The starting node is a query for chef.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 起始节点是对chef的查询。'
- en: '#2 The first SKG traversal finds terms from the category field most related
    to the starting node. These categories are the disambiguation contexts.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 第一次SKG遍历从类别字段中找到与起始节点最相关的术语。这些类别是消歧上下文。'
- en: '#3 The final SKG traversal finds the terms from the body field related to the
    disambiguation context.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 最后一次SKG遍历找到与消歧上下文相关的正文字段中的术语。'
- en: This is the internal Solr SKG request used for disambiguating the query `chef`
    with a `context_limit` of `2`. The request will be specific to whichever search
    engine or vector database is configured, or it will fall back on Solr if the engine
    does not have SKG capabilities. See appendix B for instructions on changing your
    configured search engine.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这是用于消歧具有`context_limit`为`2`的查询`chef`的内部Solr SKG请求。请求将特定于配置的任何搜索引擎或向量数据库，如果没有SKG功能，将回退到Solr。有关更改配置的搜索引擎的说明，请参阅附录B。
- en: By combining query classification, term disambiguation, and query expansion,
    an SKG can power enhanced domain-specific and highly contextualized semantic search
    capabilities within your AI-powered search engine. We’ll dive into using these
    techniques further in chapter 7 when we apply them in a live semantic search application.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合查询分类、术语消歧和查询扩展，SKG可以为您的AI搜索引擎提供增强的特定领域和高度上下文化的语义搜索能力。我们将在第7章中进一步探讨这些技术的应用，当时我们将它们应用于一个实际的语义搜索应用中。
- en: 6.3 Learning related phrases from query signals
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 从查询信号中学习相关短语
- en: Thus far, you’ve seen how to use your content as a knowledge graph to discover
    related terms, classify queries, and disambiguate terms. While these techniques
    are powerful, they are also entirely dependent upon the quality of your documents.
    Throughout the rest of this chapter, we’ll explore the other major source of knowledge
    about your domain—user signals (queries, clicks, and subsequent actions). Often,
    user signals can lead to similar, if not even more useful, insights than document
    content for interpreting queries.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到了如何使用你的内容作为知识图谱来发现相关术语、分类查询和消歧术语。虽然这些技术很强大，但它们完全依赖于你文档的质量。在本章的其余部分，我们将探讨关于你的领域的另一个主要知识来源——用户信号（查询、点击和后续操作）。通常，用户信号可以导致与文档内容相似，甚至更有用的见解，用于解释查询。
- en: As a starting point for learning domain-specific terminology from real user
    behavior, let’s consider what your query logs represent. For every query to your
    search engine, a query log contains an identifier for the person running the search,
    the query that was run, and the timestamp of the query. This means that if a single
    user searches for multiple terms, you can group those searches together and also
    know in which order the terms were entered.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 作为从真实用户行为中学习特定领域术语的起点，让我们考虑一下你的查询日志代表什么。对于你搜索引擎的每个查询，查询日志包含运行搜索的人的标识符、运行的查询以及查询的时间戳。这意味着如果单个用户搜索多个术语，你可以将这些搜索分组，并且也知道术语输入的顺序。
- en: While it’s not always true, one reasonable assumption is that if someone enters
    two different queries within a very short timespan, the second query is likely
    to be either a refinement of the first query or about a related topic. Figure
    6.2 demonstrates a realistic sequence of searches you might find for a single
    user in your query logs.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这并不总是正确的，但一个合理的假设是，如果有人在很短的时间内输入两个不同的查询，那么第二个查询很可能是第一个查询的细化或关于相关主题的。图6.2演示了你在查询日志中可能找到的某个用户的真实搜索序列。
- en: '![figure](../Images/CH06_F02_Grainger.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH06_F02_Grainger.png)'
- en: Figure 6.2 A typical sequence of searches from query logs for a particular user
  id: totrans-170
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2 某特定用户的查询日志中典型的搜索序列
- en: When looking at these queries, we intuitively understand that `iphond` is a
    misspelling of `iphone`, that `iphone accesories` is a misspelling of `iphone
    accessories`, and that `iphone`, `pink phone case`, and `pink iphone case` are
    all related queries. We’ll deal with the misspellings in a later section, but
    we can consider those to also be related terms for now.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看这些查询时，我们直观地理解到`iphond`是`iphone`的拼写错误，`iphone accesories`是`iphone accessories`的拼写错误，以及`iphone`、`pink
    phone case`和`pink iphone case`都是相关查询。我们将在后面的章节中处理这些拼写错误，但现在我们可以考虑这些也是相关术语。
- en: While it’s not wise to depend on a single user’s signals to deduce that two
    queries are related, similar query patterns across many users indicate likely
    relationships. As we demonstrated in section 5.4.5, queries can be expanded to
    include related terms to improve recall. In this section, we’ll explore techniques
    for learning related queries, first through mining query logs and then through
    cross-referencing product interaction logs.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然依赖单个用户的信号来推断两个查询相关并不明智，但许多用户之间的相似查询模式表明可能存在关联关系。正如我们在5.4.5节中展示的那样，查询可以扩展以包含相关术语来提高召回率。在本节中，我们将探讨学习相关查询的技术，首先通过挖掘查询日志，然后通过交叉引用产品交互日志。
- en: 6.3.1 Mining query logs for related queries
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 从查询日志中挖掘相关查询
- en: Before we start mining user signals for related queries, let’s first convert
    our signals into a simpler format for processing. The following listing provides
    a transformation from our generic signal structure to a simple structure that
    maps each occurrence of a query term to the user who searched for that term.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始挖掘用户信号以获取相关查询之前，让我们首先将我们的信号转换为更简单的格式以便处理。以下列表提供了从我们的通用信号结构到简单结构的转换，该结构将每个查询术语的出现映射到搜索该术语的用户。
- en: Listing 6.5 Mapping signals into keyword, user pairs
  id: totrans-175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.5 将信号映射到关键词、用户对
- en: '[PRE6]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Loads all documents from the signals corpus into a Spark view'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将信号语料库中的所有文档加载到Spark视图中'
- en: '#2 Selects keyword and user data from query signals'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 从查询信号中选择关键词和用户数据'
- en: 'Output:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE7]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can see from this listing that over 725,000 queries are represented. Our
    goal is to find pairs of related queries based on how many users entered both
    queries. The more frequently two queries co-occur across different users’ query
    logs, the more related those queries are presumed to be.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从这个列表中看到，有超过725,000个查询被表示出来。我们的目标是根据有多少用户输入了这两个查询来找到相关查询对。两个查询在不同用户的查询日志中共同出现的频率越高，这些查询被认为越相关。
- en: The next listing shows each query pair where both queries were searched by the
    same user, along with the number of users that searched for both queries (`users_cooc`).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表显示了每个查询对，其中两个查询都是由同一用户搜索的，以及搜索了这两个查询的用户数量（`users_cooc`）。
- en: Listing 6.6 Total occurrences and co-occurrences of queries
  id: totrans-183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.6 查询的总出现次数和共同出现次数
- en: '[PRE8]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Counts the number of users that searched for both k1 and k2'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 统计同时搜索了k1和k2的用户数量'
- en: '#2 Limits keyword pairs to only one permutation to avoid duplicate pairs'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 限制关键词对仅有一个排列，以避免重复的查询对'
- en: '#3 Joins the user_searches view with itself on the user field to find all keyword
    pairs searched by the same user'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 在用户字段上将user_searches视图与自身连接，以找到所有由同一用户搜索的关键词对'
- en: 'Output:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE9]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In listing 6.6, the first query produces the most searched-for keywords, as
    seen in the results. While these may be the most popular queries, they aren’t
    necessarily the queries that co-occur the most often with other queries. The second
    query produces the total number of query pairs (244,876) where both queries were
    searched by the same user at least once. The final query ranks these query pairs
    by popularity. These top query pairs are highly related.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表6.6中，第一个查询产生了最常搜索的关键词，如结果所示。虽然这些可能是最受欢迎的查询，但它们并不一定是与其他查询共同出现频率最高的查询。第二个查询产生了至少由同一用户搜索过一次的两个查询对的总数（244,876）。最后一个查询按流行度对这些查询对进行排名。这些顶级查询对高度相关。
- en: Notice, however, that the top result only has `23` co-occurring users, which
    means the number of data points is sparse and will likely include more noise further
    down the list. In the next section, we’ll explore a technique to combine signals
    along a different axis (product interactions), which can help with this sparsity
    problem.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，然而，最上面的结果只有`23`个共同出现的用户，这意味着数据点的数量稀疏，并且列表下方的噪声可能会更多。在下一节中，我们将探讨一种沿不同轴（产品交互）组合信号的技术，这有助于解决稀疏性问题。
- en: While directly aggregating the number of searches into co-occurrences by users
    helps find the most popular query pairs, the popularity of searches isn’t the
    only metric useful for finding relatedness. The keywords “and” and “of” are highly
    co-occurring, as are “phones”, “movies”, “computers”, and “electronics”, because
    they are all general words that many people search. To additionally focus on the
    strength of the relationship between terms independent of their individual popularity,
    we can use a technique called *pointwise mutual information*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然直接将搜索数量聚合为用户共同出现的情况有助于找到最受欢迎的查询对，但搜索的流行度并不是唯一有用的度量标准，用于寻找相关性。关键词“和”和“的”高度共同出现，同样，“手机”、“电影”、“电脑”和“电子产品”也是高度共同出现的，因为它们都是许多人搜索的通用词。为了进一步关注术语之间关系强度，而独立于它们的个人流行度，我们可以使用一种称为*点互信息*的技术。
- en: '*Pointwise mutual information* (PMI) is a measure of association between any
    two events. In the context of natural language processing, PMI predicts the likelihood
    of two words occurring together because they are related versus the likelihood
    of them occurring together by chance. Many formulas can be used to calculate and
    normalize PMI, but we’ll use a variation called PMI^k, where `k = 2`, which does
    a better job than PMI at keeping scores consistent regardless of word frequencies.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '*点互信息*（PMI）是衡量任何两个事件之间关联度的指标。在自然语言处理的情况下，PMI预测两个词共同出现的可能性，因为它们是相关的，而不是因为它们偶然共同出现的可能性。可以用于计算和归一化PMI的公式有很多，但我们将使用一种称为PMI^k的变体，其中`k
    = 2`，这种变体比PMI在保持分数一致方面做得更好，无论词频如何。'
- en: The formula for calculating PMI² is shown in figure 6.3.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 计算PMI²的公式显示在图6.3中。
- en: '![figure](../Images/grainger-ch6-eqs-0x.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/grainger-ch6-eqs-0x.png)'
- en: Figure 6.3 PMI² score
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3 PMI²分数
- en: In our implementation, `k1` and `k2` represent two different keywords that we
    want to compare. `P(k1,k2)` represents how often the same user searches for both
    keywords, whereas `P(k1)` and `P(k2)` represent how often a user only searches
    for the first keyword or second keyword, respectively. Intuitively, if the keywords
    appear together more often than they would be expected to, based on their likelihood
    of randomly appearing together, then they will have a higher PMI² score. The higher
    the score, the more likely the terms are to be semantically related.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实现中，`k1` 和 `k2` 代表我们想要比较的两个不同的关键词。`P(k1,k2)` 表示相同用户同时搜索这两个关键词的频率，而 `P(k1)`
    和 `P(k2)` 分别表示用户只搜索第一个关键词或第二个关键词的频率。直观上，如果关键词出现的频率比它们随机出现的频率更高，那么它们的 PMI² 分数将更高。分数越高，这些术语在语义上相关的可能性就越大。
- en: The following listing demonstrates the PMI² calculation on our co-occurring
    query pairs dataset.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表展示了在我们共现查询对数据集上进行的 PMI² 计算。
- en: Listing 6.7 PMI² calculation on user searches
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.7 用户搜索的 PMI² 计算
- en: '[PRE10]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '#1 The PMI calculation'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 PMI 计算'
- en: 'Output:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE11]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The results from listing 6.7 are sorted by PMI² score, and we set a minimum
    occurrences threshold at `>5` to help remove noise. “hp laptops”, “dell laptops”,
    and “sony laptops” show up as related, as well as brands like “kenwood” and “alpine”.
    Notably, there is also noise in the pairs, like “wireless mouse” with “godfather”
    and “quicken” with “portable dvd players”. One caveat of using PMI is that a small
    number of occurrences together across a few users can lead to noise more easily
    than when using co-occurrence, which is based upon the assumption of terms commonly
    co-occurring.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.7 的结果按 PMI² 分数排序，我们设置了一个最小出现次数阈值 `>5` 以帮助去除噪声。“hp 笔记本”，“dell 笔记本”和“sony
    笔记本”显示为相关，以及像“kenwood”和“alpine”这样的品牌。值得注意的是，在这些成对中出现了一些噪声，如“无线鼠标”与“教父”以及“quicken”与“便携式
    DVD 播放器”。使用 PMI 的一个缺点是，在少数用户中同时出现的小数量可以比使用基于术语通常共现的共现更容易产生噪声。
- en: One way to blend the benefits of both the co-occurrence model and the PMI² models
    is to create a composite score. This will provide a blend of popularity and likelihood
    of occurrence, which should move query pairs that match on both scores to the
    top of the list. Listing 6.8 demonstrates one way to blend these two measures
    together. Specifically, we take a ranked list of all co-occurrence scores (`r1`)
    along with a ranked list of all PMI² scores (`r2`) and blend them together to
    generate a composite ranking score as shown in figure 6.4.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 一种结合共现模型和 PMI² 模型优点的方法是创建一个综合分数。这将提供流行度和发生可能性的混合，这应该会将同时在两个分数上匹配的查询对移动到列表的顶部。列表
    6.8 展示了将这两个度量结合起来的方法。具体来说，我们取所有共现分数的排名列表（`r1`）以及所有 PMI² 分数的排名列表（`r2`），并将它们混合在一起，生成如图
    6.4 所示的综合排名分数。
- en: '![figure](../Images/grainger-ch6-eqs-1x.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/grainger-ch6-eqs-1x.png)'
- en: Figure 6.4 Composite ranking score combining co-occurrence and PMI² ranking
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.4 结合共现和 PMI² 排名的综合排名分数
- en: The `comp_score`, or composite rank score, shown in figure 6.4 assigns a high
    score to query pairs (query `q1` and query `q2`) where their rank in the co-occurrence
    list (`r1`) and their rank in the PMI² list (`r2`) is high, and it assigns a lower
    rank as the terms move further down in the rank lists. The result is a blended
    ranking that considers both the popularity (co-occurrence) and the likelihood
    of the relatedness of queries regardless of their popularity (PMI²). The following
    listing shows how to calculate the `comp_score` based on the already-calculated
    co-occurrence and PMI² scores.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 中显示的 `comp_score`，或综合排名分数，为那些在共现列表（`r1`）和 PMI² 列表（`r2`）中排名高的查询对（查询 `q1`
    和查询 `q2`）分配高分，而当术语在排名列表中进一步下降时，它分配的排名较低。结果是考虑了流行度（共现）和查询相关性的可能性（无论其流行度如何）的混合排名。下面的列表展示了如何根据已计算的共现和
    PMI² 分数计算 `comp_score`。
- en: Listing 6.8 Calculating a composite score from co-occurrence and PMI
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.8 从共现和 PMI 计算综合分数
- en: '[PRE12]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 The composite score calculation combines the sorted ranks of the PMI2 score
    and the co-occurrences.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 综合分数计算结合了 PMI² 分数的排序排名和共现。'
- en: '#2 Ranks the co-occurrence scores from best (highest co-occurrence) to worst
    (lowest co-occurrence)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 从最佳（最高共现）到最差（最低共现）对共现分数进行排名'
- en: '#3 Ranks the PMI2 scores from best (highest PMI2) to worst (lowest PMI2 score)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 从最佳（最高 PMI²）到最差（最低 PMI² 分数）对 PMI² 分数进行排名'
- en: 'Output:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE13]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Overall, the composite rank score does a reasonable job of blending our co-occurrence
    and PMI² metrics to overcome the limitations of each. The top results shown in
    listing 6.8 all look reasonable. One problem we already noted in this section,
    however, is that the co-occurrence numbers are very sparse. Specifically, the
    highest co-occurrence of any query pairs, out of over 700,000 query signals, was
    `23` overlapping users for “green lantern” and “captain america”, as shown in
    listing 6.6.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，复合排名分数合理地融合了我们的共现和PMI²指标，克服了各自的局限性。列表6.8中显示的前几项结果看起来都很合理。然而，在本节中我们已经指出的问题之一是，共现数字非常稀疏。具体来说，在超过70万个查询信号中，任何查询对之间的最高共现是列表6.6中显示的“绿灯侠”和“美国队长”的`23`个重叠用户。
- en: In the next section, we’ll show a way we can overcome this sparse data problem,
    where there is a lack of overlap between users for specific query pairs. We’ll
    accomplish this by aggregating many users together into a larger group with similar
    behaviors. Specifically, we’ll switch our focus to the products where user queries
    overlap, as opposed to focusing on the individual users issuing the overlapping
    queries.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将展示一种克服这种稀疏数据问题的方法，即对于特定的查询对，用户之间缺乏重叠。我们将通过将许多用户聚集到一个具有相似行为的较大群体中来实现这一点。具体来说，我们将把我们的重点从关注发出重叠查询的个别用户转移到关注用户查询重叠的产品上。
- en: 6.3.2 Finding related queries through product interactions
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 通过产品交互查找相关查询
- en: The technique used to find related terms in section 6.3.1 depends on many users
    searching for overlapping queries. As we saw, with over 700,000 query signals,
    the highest overlap of any query pair was `23` users. Because the data can be
    so sparse, it can often make sense to aggregate on something other than users.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在6.3.1节中用于查找相关术语的技术依赖于许多用户搜索重叠查询。正如我们所看到的，在超过70万个查询信号中，任何查询对之间的最高重叠是`23`个用户。由于数据可能非常稀疏，通常有理由在除了用户之外的其他事物上进行汇总。
- en: In this section, we’ll demonstrate how we can use the same technique (using
    co-occurrence and PMI²) but rolling up based on product click signals instead
    of users. Since you’ll hopefully have many more users than products, and since
    particular products are likely to be clicked in response to similar keywords,
    this technique helps overcome the data sparsity problem and generates higher overlaps
    between queries.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将演示如何使用相同的技巧（使用共现和PMI²）但基于产品点击信号而不是用户进行汇总。由于你可能会拥有比产品多得多的用户，并且由于特定产品可能会因类似关键词而被点击，这种技巧有助于克服数据稀疏性问题，并在查询之间产生更高的重叠。
- en: 'The transformation in listing 6.9 combines separate query and click signals
    into single rows with three key columns: `keyword`, `user`, and `product`.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.9中的转换将单独的查询和点击信号合并为具有三个关键列的单行：`keyword`（关键词），`user`（用户）和`product`（产品）。
- en: Listing 6.9 Mapping raw signals into keyword, user, product groupings
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.9 将原始信号映射到关键词、用户、产品分组
- en: '[PRE14]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Utilizes click signals to produce keyword, user, and product groupings'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 利用点击信号生成关键词、用户和产品分组'
- en: 'Output:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE15]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Using this data, we’ll now be able to determine the strength of the relationship
    between any two keywords based on their use across independent users searching
    for the same products. Listing 6.10 generates pairs of keywords to determine their
    potential relationship for all keyword pairs where both keywords were used in
    a query for the same document. The idea behind looking for overlapping queries
    for each user in section 6.3.1 was that each user is likely to search for related
    items. Each product is also likely to be searched for by related queries, though,
    so we can shift our mental model from “find how many users searched for both queries”
    to “find how many documents were found by both queries across all users”.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些数据，我们现在将能够根据独立用户在搜索相同产品时对它们的使用情况来确定任何两个关键词之间的关系强度。列表6.10生成关键词对，以确定所有关键词对中两个关键词的潜在关系，其中两个关键词都用于同一文档的查询。在6.3.1节中寻找重叠查询的背后的想法是，每个用户都可能搜索相关项目。然而，每个产品也可能被相关查询搜索，因此我们可以将我们的心理模型从“找出有多少用户搜索了两个查询”转变为“找出所有用户通过两个查询找到的文档数量”。
- en: 'The result of this transformation in listing 6.10 now includes the following
    columns:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 列表6.10中这种转换的结果现在包括以下列：
- en: '`k1`, `k2`—The two keywords that are potentially related because they both
    resulted in a click on the same product.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`k1`，`k2`——这两个关键词可能相关，因为它们都导致点击了同一产品。'
- en: '`n_users1`—The number of users who searched for `k1` that clicked on a product
    that was also clicked on after a search by some user for `k2`.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_users1`—搜索 `k1` 并点击了在某个用户搜索 `k2` 后也点击的产品的人数。'
- en: '`n_users2`—The number of users who searched for `k2` that clicked on a product
    that was also clicked on after a search by some user for `k1`.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_users2`—搜索 `k2` 并点击了在某个用户搜索 `k1` 后也点击的产品的人数。'
- en: '`users_cooc`—Represents the total number of users who searched for either `k1`
    or `k2` and visited a product visited by other searchers for `k1` or `k2`. Calculated
    as `n_users1` + `n_users2`.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`users_cooc`—表示搜索过 `k1` 或 `k2` 并访问了其他搜索者搜索 `k1` 或 `k2` 后访问的产品的人数。计算为 `n_users1`
    + `n_users2`。'
- en: '`n_products`—The number of products that were clicked on by searchers for both
    `k1` and `k2`.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_products`—被同时搜索 `k1` 和 `k2` 的搜索者点击的产品数量。'
- en: Listing 6.10 Keyword pairs leading to the same product being clicked
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.10 导致点击相同产品关键词对
- en: '[PRE16]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Output:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE17]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `users_cooc` and `n_products` calculations are two different ways to look
    at overall signal quality for how confident we are that any two terms `k1` and
    `k2` are related. The results are currently sorted by `n_products`, and you can
    see that the top of the list of relationships is quite clean. These keyword pairs
    represent multiple kinds of meaningful semantic relationships, including the following:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '`users_cooc` 和 `n_products` 的计算是两种不同的方式来观察整体信号质量，以确定我们对于任意两个术语 `k1` 和 `k2`
    是否相关的信心程度。当前结果按 `n_products` 排序，你可以看到关系列表的顶部非常清晰。这些关键词对代表了多种有意义的语义关系，包括以下内容：'
- en: '*Spelling variations*—“laptops” ⇒ “laptop” ; “headphones” ⇒ “head phones” ;
    etc.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*拼写变体*—“laptops” ⇒ “laptop” ; “headphones” ⇒ “head phones” ; 等。'
- en: '*Brand associations*—“tablet” ⇒ “ipad” ; “laptop” ⇒ “hp” ; “mac” ⇒ “apple”
    ; etc.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*品牌关联*—“tablet” ⇒ “ipad” ; “laptop” ⇒ “hp” ; “mac” ⇒ “apple” ; 等。'
- en: '*Synonyms/alternate names*—“netbook” ⇒ “laptop” ; “tablet pc” ⇒ “tablet”'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*同义词/别名*—“netbook” ⇒ “laptop” ; “tablet pc” ⇒ “tablet”'
- en: '*Category expansion*—“ipad” ⇒ “tablet” ; “iphone 4” ⇒ “iphone” ; “tablet” ⇒
    “computers” ; “laptops” ⇒ “computers”'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*类别扩展*—“ipad” ⇒ “tablet” ; “iphone 4” ⇒ “iphone” ; “tablet” ⇒ “computers” ;
    “laptops” ⇒ “computers”'
- en: You can write custom, domain-specific algorithms to identify some of these specific
    types of relationships, as we’ll do for spelling variations in section 6.5.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以编写自定义的、特定领域的算法来识别这些特定类型的关系，就像我们在第 6.5 节中为拼写变体所做的那样。
- en: It’s also possible to use `n_users1` and `n_users2` to identify which of the
    two queries is more popular. In the case of spelling variations, we see that `headphones`
    is used more commonly than `head phones` (1,829 versus 492 users) and is also
    more common than `headphone` (1,617 versus 367 users). Likewise, we see that `tablet`
    is much more common in usage than `tablet` `pc` (1,408 versus 296 users).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用 `n_users1` 和 `n_users2` 来确定两个查询中哪一个更受欢迎。在拼写变体的情况下，我们看到 `headphones` 比
    `head phones`（1,829 比 492 用户）更常用，也比 `headphone`（1,617 比 367 用户）更常用。同样，我们看到 `tablet`
    在使用上比 `tablet pc`（1,408 比 296 用户）更常见。
- en: While our current list of keyword pairs looks clean, it only represents the
    keyword pairs that both occurred together in searches that led to the same products.
    Determining the popularity of each keyword overall will provide a better sense
    of which specific keywords are the most important for our knowledge graph. The
    following listing calculates the most popular keywords from our query signals
    that resulted in at least one product click.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们当前的列表看起来很干净，但它只代表了在导致相同产品点击的搜索中同时出现的所有关键词对。确定每个关键词的整体流行度将更好地了解哪些具体关键词对我们知识图谱来说最重要。以下列表计算了至少导致一个产品点击的查询信号中最受欢迎的关键词。
- en: Listing 6.11 Computing keyword searches that resulted in clicks
  id: totrans-246
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.11 计算导致点击的关键词搜索
- en: '[PRE18]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Output:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE19]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This list is identical to the list from listing 6.6, but instead of showing
    the number of users who searched for a keyword, this list shows the number of
    users who searched for a keyword and also clicked on a product. We’ll use this
    as our master list of queries for the PMI² calculation.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表与列表 6.6 中的列表相同，但这个列表显示的是搜索过关键词并点击了产品的人数，而不是搜索关键词的人数。我们将使用这个列表作为我们的 PMI²
    计算的主查询列表。
- en: With our query pairs and query popularity now based on queries and product interactions,
    the rest of our calculations (PMI² and composite score) are the same as in section
    6.3.1, so we’ll omit them here (they are included in the notebooks for you to
    run). After calculating the PMI² and composite scores, the following listing shows
    the final results of our product-interaction-based related terms calculations.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的查询对和查询流行度现在基于查询和产品交互，我们其余的计算（PMI²和综合评分）与第 6.3.1 节中的相同，因此我们在这里省略它们（它们包含在笔记本中供你运行）。在计算了
    PMI² 和综合评分之后，以下列表显示了基于产品交互的相关术语计算的最终结果。
- en: Listing 6.12 Related terms scoring based on product interactions
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.12 基于产品交互的相关术语评分
- en: '[PRE20]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Output:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE21]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The results of listings 6.11 and 6.12 show the benefit of aggregating at a less
    granular level. By looking at all queries that led to a particular product being
    clicked on, the list of query pairs is now much larger than in section 6.3.1,
    where query pairs were aggregated by individual users. You can see that there
    are now 1,579,710 query pairs under consideration versus 244,876 (per listing
    6.6) when aggregating by user.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 6.11 和 6.12 的结果显示了在较粗粒度级别聚合的好处。通过查看导致特定产品被点击的所有查询，查询对列表现在比第 6.3.1 节中按单个用户聚合的查询对列表大得多。你可以看到，现在有
    1,579,710 个查询对在考虑之中，而按用户聚合时为 244,876（参见列表 6.6）。
- en: Further, you can see that the related queries include more fine-grained variations
    for top queries (`ipad`, `ipad 2`, `ipad2`, `i pad`, `ipads`, `i pad 2`). Having
    more granular variations like this will come in handy if you are combining this
    related term discovery with other algorithms, like misspelling detection, which
    we’ll cover in section 6.5.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，你可以看到相关查询包括更多针对顶级查询（`ipad`、`ipad 2`、`ipad2`、`i pad`、`ipads`、`i pad 2`）的细粒度变体。如果你将这种相关术语发现与其他算法（如拼写错误检测）结合使用，这些细粒度变体将非常有用，我们将在第
    6.5 节中介绍拼写错误检测。 '
- en: Between the SKG approach in the last chapter and query log mining in this chapter,
    you’ve now seen multiple techniques for discovering related phrases. Before we
    can apply the related phrases, however, we first need to be able to identify such
    known phrases in incoming queries. In the next section, we’ll cover how we can
    generate a list of known phrases from our query signals.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中介绍的 SKG 方法与本章中的查询日志挖掘之间，你已经看到了多种发现相关短语的技巧。然而，在我们能够应用这些相关短语之前，我们首先需要能够识别这些已知短语在进入的查询中。在下一节中，我们将介绍如何从我们的查询信号中生成已知短语列表。
- en: 6.4 Phrase detection from user signals
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 从用户信号中检测短语
- en: 'In section 5.3, we discussed several techniques for extracting arbitrary phrases
    and relationships from documents. While this can go a long way toward discovering
    all the relevant domain-specific phrases within your content, this approach suffers
    from two different problems:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 5.3 节中，我们讨论了几种从文档中提取任意短语和关系的技术。虽然这种方法可以大大有助于发现你内容中的所有相关特定领域短语，但这种方法存在两个不同的问题：
- en: '*It generates a lot of noise*—Not every noun phrase across your potentially
    massive set of documents is important, and the odds of identifying incorrect phrases
    (false positives) increase as your number of documents increases.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它会产生很多噪声*——在你可能的大量文档集中，并非每个名词短语都是重要的，随着你文档数量的增加，识别错误短语（假阳性）的概率也会增加。'
- en: '*It ignores what your users care about*—The real measure of user interest is
    communicated by what they search for. They may only be interested in a subset
    of your content or may be looking for things that aren’t even represented well
    within your content.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*它忽略了用户所关心的内容*——用户兴趣的真实度量是通过他们所搜索的内容来传达的。他们可能只对你内容的一个子集感兴趣，或者他们可能正在寻找的内容在你的内容中甚至没有得到很好的体现。'
- en: In this section, we’ll focus on how to identify important domain-specific phrases
    from your user signals.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将关注如何从你的用户信号中识别重要的特定领域短语。
- en: 6.4.1 Treating queries as entities
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 将查询视为实体
- en: The easiest way to extract entities from query logs is to treat the entire query
    as one entity. In use cases like our RetroTech e-commerce site, this works very
    well, as many of the queries are product names, categories, brand names, company
    names, or people’s names (actors, musicians, etc.). Given that context, most of
    the high-popularity queries end up being entities that can be used directly as
    phrases without needing any special parsing.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 从查询日志中提取实体的最简单方法是将整个查询视为一个实体。在我们的RetroTech电子商务网站等用例中，这种方法非常有效，因为许多查询都是产品名称、类别、品牌名称、公司名称或人们的名字（演员、音乐家等）。考虑到这个背景，大多数高流行度查询最终都成为可以直接用作短语而无需任何特殊解析的实体。
- en: 'Looking back at the output of listing 6.11, you’ll find the following most
    popular queries:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾列表6.11的输出，你会发现以下最受欢迎的查询：
- en: '[PRE22]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: These are entities that belong in a known-entities list, with many of them being
    multiword phrases. In this case, the simplest method for extracting entities is
    also the most powerful—just use the queries as your entities list. The higher
    the frequency of each query across users, the more confident you can be about
    adding it to your entities list.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是应属于已知实体列表中的实体，其中许多是多词短语。在这种情况下，提取实体的最简单方法也是最强大的方法——只需将查询作为你的实体列表即可。每个查询在用户中的频率越高，你将其添加到实体列表中的信心就越大。
- en: One way to reduce potential false positives from noisy queries is to find phrases
    that overlap in both your documents and queries. Additionally, if you have different
    fields in your documents, like a product name or company, you can cross-reference
    your queries with those fields to assign a type to the entities found within your
    queries.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 减少噪声查询中潜在假阳性的一个方法是在你的文档和查询中找到重叠的短语。此外，如果你的文档中有不同的字段，如产品名称或公司，你可以将你的查询与这些字段交叉引用，为查询中找到的实体分配一个类型。
- en: Depending on the complexity of your queries, using the most common searches
    as your key entities may be the most straightforward way to achieve a high-quality
    entities list.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 根据查询的复杂性，使用最常见的搜索作为你的关键实体可能是实现高质量实体列表的最直接方法。
- en: 6.4.2 Extracting entities from more complex queries
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 从更复杂的查询中提取实体
- en: In some use cases, the queries may contain more noise (Boolean structure, advanced
    query operators, etc.) and therefore may not be directly usable as entities. In
    those cases, the best approach to extracting entities may be to reapply the entity
    extraction strategies from chapter 5, but on your query signals.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些用例中，查询可能包含更多噪声（布尔结构、高级查询运算符等），因此可能不能直接用作实体。在这些情况下，提取实体的最佳方法可能是重新应用第5章中提到的实体提取策略，但针对你的查询信号。
- en: Out of the box, a lexical search engine parses queries as individual keywords
    and looks them up in the inverted index. For example, a query for `new york city`
    will be automatically interpreted as the Boolean query `new AND york AND city`
    (or if you set the default operator to `OR`, then `new OR york OR city`). The
    relevance ranking algorithms will then score each keyword individually instead
    of understanding that certain words combine to make phrases that then take on
    a different meaning.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，词汇搜索引擎将查询解析为单个关键词并在倒排索引中查找它们。例如，对`new york city`的查询将被自动解释为布尔查询`new AND
    york AND city`（或者如果你将默认运算符设置为`OR`，那么就是`new OR york OR city`）。然后，相关性排名算法将单独对每个关键词进行评分，而不是理解某些单词组合成短语后会产生不同的含义。
- en: Being able to identify and extract domain-specific phrases from queries can
    enable more accurate query interpretation and relevance. We already demonstrated
    one way to extract domain-specific phrases from documents in section 5.3, using
    the spaCy NLP library to do a dependency parse and extract out noun phrases. While
    queries are often too short to perform a true dependency parse, it’s still possible
    to apply some part of speech filtering on any discovered phrases in queries to
    omit non-noun phrases. If you need to split sections of queries apart, you can
    also tokenize the queries and remove query syntax (`AND`, `OR`, etc.) before looking
    for phrases to extract. Handling the specific query patterns for your application
    may require some domain-specific query parsing logic, but if your queries are
    largely single phrases or easily tokenizable into multiple phrases, your queries
    likely represent the best source of domain-specific phrases to extract and add
    to your knowledge graph. We’ll walk through code examples of phrase identification
    when parsing queries in section 7.4\.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 能够从查询中识别和提取特定领域短语可以启用更准确的查询解释和相关性。我们已经在第5.3节中演示了一种从文档中提取特定领域短语的方法，使用spaCy NLP库进行依存句法分析并提取出名词短语。虽然查询通常太短而无法进行真正的依存句法分析，但仍然可以在查询中发现的任何短语上应用一些词性过滤，以排除非名词短语。如果您需要将查询的部分分开，您还可以在查找短语之前对查询进行分词并移除查询语法（`AND`、`OR`等）。处理您应用程序的特定查询模式可能需要一些特定领域的查询解析逻辑，但如果您的查询主要是单个短语或可以轻松分词成多个短语，那么您的查询很可能是提取和添加到您的知识图谱中特定领域短语的最佳来源。我们将在第7.4节中通过代码示例介绍在解析查询时识别短语的方法。
- en: 6.5 Misspellings and alternative representations
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 拼写错误和替代表示
- en: 'We’ve covered detecting domain-specific phrases and finding related phrases,
    but there are two very important subcategories of related phrases that typically
    require special handling: misspellings and alternative spellings (also known as
    *alternative labels*). When entering queries, users will commonly misspell their
    keywords, and the general expectation is that an AI-powered search system will
    be able to understand and properly handle those misspellings.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经介绍了检测特定领域短语和查找相关短语，但有两个非常重要的相关短语子类别通常需要特殊处理：拼写错误和替代拼写（也称为*替代标签*）。当输入查询时，用户通常会拼写他们的关键词，普遍的预期是AI驱动的搜索系统能够理解和正确处理这些拼写错误。
- en: While general related phrases for “laptop” might be “computer”, “netbook”, or
    “tablet”, misspellings would look more like “latop”, “laptok”, or “lapptop”. *Alternative
    labels* are functionally no different than misspellings but occur when multiple
    valid variations for a phrase exist (such as “specialized” versus “specialised”
    or “cybersecurity” versus “cyber security”). In the case of both misspellings
    and alternative labels, the end goal is usually to normalize the less common variant
    into the more common, canonical form and then search for the canonical version.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“笔记本电脑”的一般相关短语可能是“计算机”、“上网本”或“平板电脑”，而拼写错误可能看起来更像是“latop”、“laptok”或“lapptop”。*替代标签*在功能上与拼写错误没有区别，但发生在存在一个短语的多个有效变体时（例如，“specialized”与“specialised”或“cybersecurity”与“cyber
    security”）。在拼写错误和替代标签的情况下，最终目标通常是将不太常见的变体归一化到更常见的、规范的形式，然后搜索规范版本。
- en: Spell-checking can be implemented in multiple ways. In this section, we’ll cover
    the out-of-the-box document-based spell-checking that is found in most search
    engines, and we’ll also show how user signals can be mined to fine-tune spelling
    corrections based upon real user interactions with your search engine.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 拼写检查可以以多种方式实现。在本节中，我们将介绍大多数搜索引擎中发现的现成文档基础拼写检查，同时也会展示如何挖掘用户信号以根据用户与您的搜索引擎的实际交互来微调拼写修正。
- en: 6.5.1 Learning spelling corrections from documents
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 从文档中学习拼写修正
- en: Most search engines contain some spell-checking capabilities out of the box,
    based on the terms found within a collection’s documents. Apache Solr, for example,
    provides file-based, dictionary-based, and index-based spell-checking components.
    The file-based spell-checker requires assembling a list of terms that can be spell-corrected.
    The dictionary-based spell-checker can build a list of terms to be spell-corrected
    from fields in an index. The index-based spell-checker can use a field on the
    main index to spell-check directly without having to build a separate spell-checking
    index. Additionally, if someone has built a list of spelling corrections offline,
    you can use a synonym list to directly replace or expand any misspellings to their
    canonical form.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数搜索引擎都包含一些即插即用的拼写检查功能，这些功能基于集合文档中的术语。例如，Apache Solr提供了基于文件、基于字典和基于索引的拼写检查组件。基于文件的拼写检查器需要组装一个可以拼写修正的术语列表。基于字典的拼写检查器可以从索引的字段中构建一个要拼写修正的术语列表。基于索引的拼写检查器可以使用主索引上的字段直接进行拼写检查，而无需构建单独的拼写检查索引。此外，如果有人已经离线构建了一个拼写修正的列表，你可以使用同义词列表直接替换或扩展任何误拼为它们的规范形式。
- en: Elasticsearch and OpenSearch have similar spellchecking capabilities, even allowing
    specific contexts to refine the scope of the spelling suggestions to a particular
    category or geographical location.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch和OpenSearch具有类似的拼写检查功能，甚至允许特定上下文将拼写建议的范围缩小到特定类别或地理位置。
- en: 'While we encourage you to test out these out-of-the-box spell-checking algorithms,
    they all unfortunately suffer from a major problem: lack of user context. Specifically,
    anytime a keyword is searched that doesn’t appear a minimum number of times in
    the index, the spell-checking component begins looking at all terms in the index
    that are *off by the minimum number of characters*, and they then return the most
    prevalent keywords in the index that match the criteria. The following listing
    shows an example of where out-of-the-box index-based spell-checking configuration
    falls short.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们鼓励你测试这些即插即用的拼写检查算法，但不幸的是，它们都存在一个主要问题：缺乏用户上下文。具体来说，任何搜索的单词在索引中出现的次数少于最小次数时，拼写检查组件就会查看索引中所有偏离最小字符数的术语，然后返回与标准匹配的索引中最常见的关键词。以下列表显示了即插即用的基于索引的拼写检查配置的不足之处。
- en: Listing 6.13 Using out-of-the-box spelling corrections on documents
  id: totrans-283
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.13 在文档上使用即插即用的拼写修正
- en: '[PRE23]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Output:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE24]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In listing 6.13, you can see a user query for `moden`. The spell-checker returns
    the suggested spelling corrections of “modes”, “model”, “modern”, and “modem”,
    plus one suggestion that only appears in a few documents, which we’ll ignore.
    Since our collection is tech products, it may be obvious which of these is likely
    the best spelling correction: it’s “modem”. In fact, it is unlikely that a user
    would intentionally search for “modes” or “model” as standalone queries, as those
    are both generic terms that would usually only make sense within a context containing
    other words.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表6.13中，你可以看到一个用户查询`moden`。拼写检查器返回了“modes”、“model”、“modern”和“modem”这些建议的拼写修正，以及一个只出现在少数文档中的建议，我们将忽略它。由于我们的集合是技术产品，可能很明显哪个是可能的最佳拼写修正：它是“modem”。实际上，用户不太可能故意搜索“modes”或“model”作为独立的查询，因为这些词都是通用术语，通常只有在包含其他单词的上下文中才有意义。
- en: The content-based index has no way to distinguish easily that end users would
    be unlikely to search for “modern” or “model”. Thus, while content-based spell-checkers
    can work well in many cases, it is often more accurate to learn spelling corrections
    from users’ query behavior.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 基于内容索引没有简单的方法来区分最终用户不太可能搜索“现代”或“型号”。因此，尽管基于内容的拼写检查器在很多情况下可以很好地工作，但通常从用户的查询行为中学习拼写修正会更准确。
- en: 6.5.2 Learning spelling corrections from user signals
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.2 从用户信号中学习拼写修正
- en: Returning to our core thesis from section 6.3 that users tend to search for
    related queries until they find the expected results, it follows that a user who
    misspelled a particular query and received bad results would then try to correct
    their query.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 回到第6.3节的核心论点，即用户倾向于搜索相关查询，直到找到预期的结果，因此，如果一个用户拼写了一个特定的查询并收到了不良的结果，那么他们就会尝试纠正他们的查询。
- en: 'We already know how to find related phrases (discussed in section 6.3), but
    in this section we’ll cover how to specifically distinguish a misspelling based
    on user signals. This task largely comes down to two goals:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道如何找到相关短语（在第6.3节中讨论），但在这个章节中，我们将介绍如何根据用户信号特别区分拼写错误。这项任务主要归结为两个目标：
- en: Find terms with similar spellings.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到拼写相似的术语。
- en: Figure out which term is the correct spelling versus the misspelled variant.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定哪个术语是正确的拼写与拼写错误的变体。
- en: For this task, we’ll rely solely on query signals. We’ll perform some up-front
    normalization to make the query analysis case-insensitive and filter duplicate
    queries to avoid signal spam. (We’ll discuss signal normalization in sections
    8.2–8.3.) The following listing shows a query that grabs our normalized query
    signals.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我们将仅依靠查询信号。我们将进行一些前置规范化，使查询分析不区分大小写，并过滤重复的查询以避免信号垃圾邮件。（我们将在第8.2-8.3节中讨论信号规范化。）以下列表显示了一个获取我们的规范化查询信号的查询。
- en: Listing 6.14 Getting all queries searched by users
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.14 获取用户搜索的所有查询
- en: '[PRE25]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '#1 Lowercasing the queries makes the query analysis ignore uppercase vs. lowercased
    variants.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将查询转换为小写使得查询分析忽略了大小写变体。'
- en: '#2 Grouping by user prevents spam from a single user entering the same query
    many times.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 通过用户分组防止单个用户多次输入相同的查询造成的垃圾邮件。'
- en: For the purposes of this section, we’re going to assume that the queries can
    contain multiple different keywords and that we want to treat each of these keywords
    as a potential spelling variant. This will allow individual terms to be found
    and substituted within a future query, as opposed to treating the entire query
    as a single phrase. It will also allow us to throw out certain terms that are
    likely to be noise, such as stop words or standalone numbers.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节的目的，我们将假设查询可以包含多个不同的关键词，并且我们希望将这些关键词视为潜在的拼写变体。这将允许在未来的查询中找到并替换单个术语，而不是将整个查询视为一个短语。这还将允许我们排除某些可能是噪音的术语，例如停用词或独立数字。
- en: The following listing demonstrates the process of tokenizing each query to generate
    a word list upon which we can do further analysis.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表演示了将每个查询分词以生成单词列表的过程，我们可以在此基础上进行进一步分析。
- en: Listing 6.15 Finding words by tokenizing and filtering query terms
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.15 通过分词和过滤查询术语查找单词
- en: '[PRE26]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '#1 Defines stop words that shouldn’t be considered as misspellings or corrections
    utilizing the Natural Language Toolkit (nltk)'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 定义不应被视为拼写错误或更正的停用词，使用自然语言工具包（nltk）。'
- en: '#2 Removes noisy terms including stop words, very short terms, and numbers'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 移除噪音术语，包括停用词、非常短的术语和数字'
- en: '#3 Splits the query on whitespace into individual terms if tokenizing'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 如果分词，则在空白处拆分查询以形成单个术语'
- en: '#4 Aggregates the occurrences of valid keywords'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 聚合有效关键词的出现次数'
- en: Once the list of tokens has been cleaned up, the next step is to distinguish
    high-occurrence tokens from infrequently occurring tokens. Since misspellings
    will occur relatively infrequently and correct spellings will occur more frequently,
    we will use the relative number of occurrences to determine which version is most
    likely the canonical spelling and which variations are the misspellings.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦清理了标记列表，下一步就是区分高出现频率的标记和低出现频率的标记。由于拼写错误相对较少，而正确的拼写将更频繁地出现，我们将使用出现次数的相对数量来确定哪个版本最可能是标准拼写，哪些变体是拼写错误。
- en: To ensure our spell correction list is as clean as possible, we’ll set some
    thresholds for popular terms and some for low-occurrence terms that are more likely
    misspellings. Because some collections may contain hundreds of documents and other
    collections could contain millions, we can’t just look at an absolute number for
    these thresholds, so we’ll use quantiles instead. The following listing shows
    the calculations for each of the quantiles between `0.1` and `0.9`.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们的拼写纠正列表尽可能干净，我们将为流行术语和一些可能是拼写错误的低出现频率术语设置一些阈值。由于某些集合可能包含数百个文档，而其他集合可能包含数百万个文档，我们不能仅仅查看这些阈值的绝对数字，因此我们将使用分位数。以下列表显示了`0.1`到`0.9`之间每个分位数的计算。
- en: Listing 6.16 Calculating quantiles to identify spelling candidates
  id: totrans-309
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.16 计算分位数以识别拼写候选词
- en: '[PRE27]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Output:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE28]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Here we see that 80% of the terms are searched for `142.2` times or less. Likewise,
    only 20% of terms are searched for `6.0` times or less. Using the Pareto principle,
    let’s assume that most of our misspellings fall within the bottom-searched 20%
    of our terms and that the majority of our most important terms fall within the
    top 20% of queries searched. If you want higher precision (only generate spelling
    corrections for high-value terms and only if there’s a low probability of false
    positives), you can push these to the `0.1` quantile for misspellings and the
    `0.9` quantile for correctly spelled terms. You can also go the other direction
    to attempt to generate a larger misspelling list with a higher chance of false
    positives.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到80%的术语搜索次数为`142.2`次或更少。同样，只有20%的术语搜索次数为`6.0`次或更少。使用帕累托原则，让我们假设大多数拼写错误都位于搜索量最低的底部20%的术语中，而大多数最重要的术语都位于搜索量最高的顶部20%的查询中。如果你想要更高的精度（只为高价值术语生成拼写更正，并且只有在错误率为低的情况下），你可以将这些术语推到`0.1`分位数的`拼写错误`和`0.9`分位数的正确拼写术语。你也可以采取相反的方向，尝试生成一个更大的拼写错误列表，并提高错误率的可能性。
- en: In Listing 6.17, we’ll divide the terms into buckets, assigning low-frequency
    terms to the `misspellings` bucket and high-frequency terms to the `corrections`
    bucket. These buckets will be a starting point for finding high-quality spelling
    corrections when enough users search for both the misspelling candidate and the
    correction candidate.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表6.17中，我们将术语分成桶，将低频术语分配到`拼写错误`桶，将高频术语分配到`更正`桶。这些桶将是当足够多的用户搜索拼写错误候选词和更正候选词时，寻找高质量拼写更正的起点。
- en: Listing 6.17 Identifying spelling correction candidates
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.17 识别拼写更正候选词
- en: '[PRE29]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '#1 Terms at or below the 0.2 quantile are added to the misspellings list.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 位于或低于0.2分位数的术语被添加到`拼写错误`列表中。'
- en: '#2 The number of searches is retained to keep track of popularity.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 保留搜索次数以跟踪流行度。'
- en: '#3 The length of the term will be used later to set thresholds for edit distance
    calculations.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 术语的长度将用于稍后设置编辑距离计算的阈值。'
- en: '#4 The first letter of the term is stored to limit the scope of the misspellings
    checked.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 存储术语的第一个字母以限制检查拼写错误的范围。'
- en: '#5 The top 20% of terms have the same data stored but in the corrections list.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 顶部20%的术语在`更正`列表中有相同的数据存储。'
- en: To efficiently compare all of the `misspellings` and the `corrections` values,
    we first load them into dataframes in listing 6.17\. You can imagine that `corrections`
    is a pristine list of the most popular searched terms, while the `misspellings`
    list should provide a good candidate list for less commonly searched terms that
    are more likely to be misspellings.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地比较所有`拼写错误`和`更正`值，我们首先将它们加载到列表6.17中的数据框中。你可以想象，`更正`是一个原始的、最受欢迎的搜索词列表，而`拼写错误`列表应该提供一份不太常见的搜索词候选列表，这些搜索词更有可能是拼写错误。
- en: When we compare misspelled candidates with correctly spelled candidates and
    decide how many character differences (or *edit distances*) are allowed, we need
    to consider the term length. The following listing shows a simple `good_match`
    function, which defines a general heuristic for how many edit distances a term
    match can be off by while still considering the misspelling a likely permutation
    of the correction candidate.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们比较拼写错误的候选词与正确拼写的候选词，并决定允许多少字符差异（或*编辑距离*）时，我们需要考虑术语长度。以下列表显示了一个简单的`good_match`函数，它定义了一个通用的启发式方法，即一个术语匹配可以偏离多少编辑距离，同时仍然认为拼写错误是更正候选词的可能排列。
- en: Listing 6.18 Finding proper spellings by lengths and edit distance
  id: totrans-324
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.18 通过长度和编辑距离查找正确的拼写
- en: '[PRE30]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: With our `misspellings` and `corrections` candidates loaded into dataframes
    and the `good_match` function defined, it’s time to generate our spelling correction
    list. Just like in section 6.5.1, where spelling corrections were generated from
    edit distances and counts of term occurrences within our collection of documents,
    listing 6.19 generates spelling corrections based on edit distances and term occurrences
    within our query logs.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将`拼写错误`和`更正`候选词加载到数据框中，并定义了`good_match`函数后，是时候生成我们的拼写更正列表了。就像在第6.5.1节中，拼写更正是从编辑距离和术语在我们文档集合中的出现次数生成的，列表6.19基于编辑距离和术语在我们查询日志中的出现次数生成拼写更正。
- en: Listing 6.19 Mapping misspellings to their correct spellings
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.19 将拼写错误映射到它们的正确拼写
- en: '[PRE31]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '#1 Groups the misspelling and correction candidates on the first letter of
    the word'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 根据单词的第一个字母将拼写错误和更正候选词分组。'
- en: '#2 Calculates the edit distance between each misspelling and correction candidate'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 计算每个拼写错误和纠正候选之间的编辑距离'
- en: '#3 Applies the good_match function using the lengths of the terms and the edit
    distance'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用术语长度和编辑距离应用good_match函数'
- en: '#4 Aggregates all the misspellings by name'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 按名称聚合所有拼写错误'
- en: '#5 Gets the 20 most misspelled words'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 获取最常见的20个拼写错误'
- en: 'Output:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE32]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: As you can see, we now have a relatively clean list of spelling corrections
    based on user signals. Our query of `moden` maps correctly to “modem”, as opposed
    to unlikely search terms like “model” and “modern”, which we saw in the document-based
    spelling correction in listing 6.13.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们现在基于用户信号有一个相对干净的拼写纠正列表。我们的查询`moden`正确映射到“modem”，而不是像在列表6.13中看到的基于文档的拼写纠正中那样的不可能的搜索词，如“model”和“modern”。
- en: There are numerous other ways that you could go about creating a spelling correction
    model. If you wanted to generate multiterm spelling corrections from documents,
    you could generate bigrams and trigrams to perform chained Bayesian analysis on
    probabilities of consecutive terms occurring. Likewise, to generate multiterm
    spelling corrections from query signals, you could remove the tokenization of
    queries by setting `tokenize` to `False` when calling `valid_keyword_occurrences`.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以以多种方式创建拼写纠正模型。如果您想从文档中生成多词拼写纠正，您可以从文档中生成二元组和三元组以执行链式贝叶斯分析，分析连续术语出现的概率。同样，要从查询信号中生成多词拼写纠正，您可以在调用`valid_keyword_occurrences`时将`tokenize`设置为`False`以移除查询的分词。
- en: Listing 6.20 Finding multiterm spell corrections from full queries
  id: totrans-338
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.20 从完整查询中查找多词拼写纠正
- en: '[PRE33]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Output:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE34]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: You can see some of the common multiword misspellings and their corrections
    in listing 6.20, now that the queries are no longer being tokenized. Note that
    the single-term words are largely the same, but multiword queries have also been
    spell-checked. This is a great way to normalize product names, so that “iphone4
    s”, “iphones 4s”, and “iphone s4” are all correctly mapped to the canonical “iphone
    4s”. Note that in some cases this can be a lossy process, as “hp touchpad 32”
    maps to “hp touchpad”, and “iphone3” maps to “iphone”. Depending on your use case,
    you may find it beneficial to only spell-correct individual terms, or to include
    special handling in your `good_match` function for brand variations to ensure
    the spell-check code doesn’t mistakenly delete relevant query context.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 现在查询不再被分词，您可以在列表6.20中看到一些常见的多词拼写错误及其纠正。请注意，单词项在很大程度上是相同的，但多词查询也已进行了拼写检查。这是一种规范产品名称的绝佳方式，使“iphone4
    s”、“iphones 4s”和“iphone s4”都正确映射到规范名称“iphone 4s”。请注意，在某些情况下，这可能是一个有损的过程，例如“hp
    touchpad 32”映射到“hp touchpad”，“iphone3”映射到“iphone”。根据您的用例，您可能会发现仅对单个术语进行拼写纠正是有益的，或者您可以在`good_match`函数中包含特殊处理以处理品牌变体，以确保拼写检查代码不会错误地删除相关的查询上下文。
- en: 6.6 Pulling it all together
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 汇总
- en: In this chapter, we dove deeper into understanding the context and meaning of
    domain-specific language. We showed how to use SKGs to classify queries and disambiguate
    terms that have different or nuanced meanings based on their context. We also
    explored how to mine relationships from user signals, which usually provides a
    better context for understanding your users than looking at your documents alone.
    We also showed how to extract phrases, misspellings, and alternative labels from
    query signals, enabling domain-specific terminology to be learned directly from
    users as opposed to only from documents.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们更深入地了解了特定领域语言的上下文和意义。我们展示了如何使用SKGs来分类查询和区分基于上下文具有不同或细微意义的术语。我们还探讨了如何从用户信号中挖掘关系，这通常比仅查看您的文档更能提供理解用户的好上下文。我们还展示了如何从查询信号中提取短语、拼写错误和替代标签，使特定领域的术语可以直接从用户那里学习，而不仅仅是来自文档。
- en: At this point, you should feel confident about learning domain-specific phrases
    and related phrases from documents or user signals, classifying queries to your
    available content, and disambiguating the meaning of terminology based on the
    query classification. These techniques are critical tools in your toolbox for
    interpreting query intent.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您应该对从文档或用户信号中学习特定领域短语和相关短语、对可用内容进行查询分类以及根据查询分类区分术语的意义感到自信。这些技术是您工具箱中用于解释查询意图的关键工具。
- en: Our goal isn’t just to assemble a large toolbox, though. Our goal is to use
    each of these tools where appropriate to build an end-to-end semantic search layer.
    This means we need to model known phrases into our knowledge graph, extract those
    phrases from incoming queries, handle misspellings, classify queries, disambiguate
    incoming terms, and ultimately generate a rewritten query for the search engine
    that uses each of our AI-powered search techniques. In the next chapter, we’ll
    show you how to assemble each of these techniques into a working semantic search
    system designed to best interpret and model query intent.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标不仅仅是组装一个大型工具箱。我们的目标是根据需要使用这些工具中的每一个来构建一个端到端的语义搜索层。这意味着我们需要将已知短语建模到我们的知识图谱中，从传入的查询中提取这些短语，处理拼写错误，对查询进行分类，消除传入术语的歧义，并最终为使用我们每个AI驱动的搜索技术的搜索引擎生成一个重写的查询。在下一章中，我们将向您展示如何将这些技术组装成一个工作语义搜索系统，该系统旨在最佳解释和建模查询意图。
- en: Summary
  id: totrans-347
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Classifying queries using a semantic knowledge graph (SKG) can help interpret
    query intent and improve query routing and filtering.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用语义知识图谱（SKG）对查询进行分类可以帮助解释查询意图并改进查询路由和过滤。
- en: Query-sense disambiguation can deliver a more contextual understanding of a
    user’s query, particularly for terms with significantly divergent meanings across
    different contexts.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询意义消歧可以提供对用户查询的更语境化的理解，尤其是对于在不同语境中有显著不同含义的术语。
- en: In addition to learning from documents, domain-specific phrases and related
    phrases can also be learned from user signals.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了从文档中学习外，特定领域的短语和相关短语也可以从用户信号中学习。
- en: Misspellings and spelling variations can be learned from both documents and
    user signals, with document-based approaches being more robust and user-signal-based
    approaches better representing user intent.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拼写错误和拼写变体可以从文档和用户信号中学习，基于文档的方法更稳健，而基于用户信号的方法更好地代表用户意图。
