- en: 6 Using context to learn domain-specific language
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Classifying query intent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query-sense disambiguation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying key terminology from user signals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning related phrases from user signals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning misspellings and alternate term variations from user signals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In chapter 5, we demonstrated both how to generate and use a semantic knowledge
    graph (SKG) and how to extract entities, facts, and relationships explicitly into
    a knowledge graph. Both techniques rely on navigating either the linguistic connections
    between terms in a single document or the statistical co-occurrences of terms
    across multiple documents and contexts. You learned to use knowledge graphs to
    find related terms, and how those related terms can integrate into various query-rewriting
    strategies to increase recall or precision.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll dive deeper into understanding query intent and the nuances
    of using different contexts to interpret domain-specific terminology in queries.
    We’ll start by exploring query classification and then show how those classifications
    can be used to disambiguate queries with multiple potential meanings. Both approaches
    will extend our use of SKGs from the last chapter.
  prefs: []
  type: TYPE_NORMAL
- en: While those SKG-based approaches are more effective at contextualizing and interpreting
    queries, they continue to rely on having high-quality documents that accurately
    represent your domain. As a result, their efficacy for interpreting user queries
    depends on how well the queries overlap with the content being searched.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if 75% of your users are searching for clothing, but most of your
    inventory is films and digital media, then when they search for the query `shorts`
    and all the results are videos with short run times (known as “digital shorts”),
    most of your users will be confused by the results. Given the data in your query
    logs, it would be better if “shorts” could map to other related terms more commonly
    found in your query signals, like “pants”, “clothing”, and “shirts”.
  prefs: []
  type: TYPE_NORMAL
- en: It can be very beneficial to not only rely on the content of your documents
    to learn relationships between terms and phrases, but to also use your user-generated
    signals. In this chapter, we’ll demonstrate techniques to extract key phrases,
    learn related phrases, and identify common misspellings or alternative spellings
    based on user signals. By using both content-based context and behavioral context
    from real user interactions, your search engine will better understand domain-specific
    terminology and actual user intent.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Classifying query intent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal or intent of a query usually matters more than the keywords. A search
    for `driver crashed` can mean two *very* different things in the context of news
    or travel content versus a computer technology context. Similarly, someone searching
    in e-commerce for a specific product name or product ID is probably searching
    for a very specific item with a high likelihood of wanting to purchase it. A general
    search like `kitchen appliances` could indicate the user just intends to browse
    available products to see what’s available.
  prefs: []
  type: TYPE_NORMAL
- en: In both contexts, a query classifier can be effective at determining the general
    kind of query being issued. Depending on the domain, a query’s context could be
    automatically applied (e.g., filtering the category of documents), used to modify
    the relevance algorithm (automatically boosting specific products), or even used
    to drive a different user experience (skipping the results page and going directly
    to a specific product’s page). In this section, we’ll show how to use the SKG
    from chapter 5 as a classifier for incoming queries to build a query classifier.
  prefs: []
  type: TYPE_NORMAL
- en: An SKG traversal does a *k*-nearest neighbor search at each level of the graph
    traversal. *K*-nearest neighbor is a type of classification that takes a data
    point (such as a query or term) and tries to find the top *k* most similar other
    data points in a vector space. If we have a field like `category` or `classification`
    on our documents, we can ask the SKG to “find the category with the highest relatedness
    to my starting node”. Since the starting node is typically a user’s query, an
    SKG can classify that query.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll continue to use the indexed Stack Exchange datasets as an SKG to be extended
    for query classification (in this section) and query-sense disambiguation (in
    section 6.2).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 shows a function that takes a user query and traverses the SKG to
    find semantically related categories to classify the query. Since we’ve indexed
    multiple different Stack Exchange categories (scifi, health, cooking, devops,
    etc.), we’ll use those categories as our classifications.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.1 Query classification using the SKG
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The initial node of the graph based on a query matching against a field'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The field from which we’ll find related classifications. In this case, we
    traverse to the category field.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Only classifications occurring in at least this number of documents will
    be returned.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Sets the number of classifications to return'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Traverses the SKG to classify the query'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Prints a query and its classifications'
  prefs: []
  type: TYPE_NORMAL
- en: 'Example query classifications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This request uses the SKG to find the top *k* nearest neighbors based on a comparison
    of the semantic similarity between the query and each available classification
    (within the `category` field).
  prefs: []
  type: TYPE_NORMAL
- en: We see classification scores for each potential category for each query, with
    `airplane` and `passport` classified to “travel”, `vitamins` classified to “health”
    and “cooking”, and `alien` classified to “scifi”. When we refine the `airplane`
    query to a more specific query like `airplane AND crash`, however, we see that
    the classification changes from “travel” to “scifi”, because documents about airplane
    crashes are more likely to occur within “scifi” documents than “travel” documents.
  prefs: []
  type: TYPE_NORMAL
- en: As another example, `driver` could have multiple meanings. It returns two potential
    classifications (“travel” or “devops”), with the “travel” category being the clear
    choice when no other context is provided. When additional context *is* provided,
    however, we can see that the query `driver AND taxi` gets appropriately classified
    to the “travel” category, while `driver AND install` gets appropriately classified
    to the “devops” category.
  prefs: []
  type: TYPE_NORMAL
- en: This ability for the SKG to find semantic relationships between arbitrary combinations
    of terms makes it useful for on-the-fly classification of incoming queries. You
    can auto-apply the classifications as query filters or boosts, route queries to
    a context-specific algorithm or landing page, or automatically disambiguate query
    terms. We’ll explore using a two-level graph traversal in the next section to
    implement query-sense disambiguation.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Query-sense disambiguation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When interpreting users’ intent from their queries, understanding exactly what
    they meant by each word is challenging. The problem of polysemy, or ambiguous
    terms, can significantly affect your search results.
  prefs: []
  type: TYPE_NORMAL
- en: If someone searches for `server`, this could refer to someone who takes orders
    and waits on tables at a restaurant, or it could mean a computer that runs software
    on a network. Ideally, we want our search engine to be able to disambiguate each
    of these word senses and generate a unique list of related terms within each disambiguated
    context. Figure 6.1 demonstrates these two potential contexts for the word “server”
    and the kinds of related terms one might find within each context.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F01_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 Differentiating multiple senses of the ambiguous term “server”
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In section 6.1, we demonstrated how to use an SKG to automatically classify
    queries into a set of known categories. Given that we already know how to classify
    our queries, adding a second-level traversal can provide a contextualized list
    of related terms for each query classification.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, by traversing from query to classification and then to terms,
    we can generate a list of terms that describe a contextualized interpretation
    of the original query within each of the top classifications. The following listing
    shows a function that disambiguates a query this way utilizing an SKG.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.2 Disambiguating query intent across different contexts
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The starting node of the graph traversal (the user’s query)'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The first traversal returns the contexts for disambiguating the query.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The second traversal is from keywords related to both the query AND each
    related context.'
  prefs: []
  type: TYPE_NORMAL
- en: You can see from this listing that a context field (the `category` field by
    default) and a keywords field (the `body` field by default) are used as part of
    a two-level traversal. For any query that is passed in, we first find the most
    semantically related category and then the terms most semantically related to
    the original query within that category.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing demonstrates how to call this function, passing in three
    different queries containing ambiguous terms for which we want to find differentiated
    meanings.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.3 Running query-sense disambiguation for several queries
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The results of the queries in listing 6.3 can be found in tables 6.1–6.3, followed
    by the search-engine-specific SKG request used to disambiguate `chef` in listing
    6.4\. Each disambiguation context (`category` field) is scored relative to the
    query, and each discovered keyword (`body` field) is scored relative to both the
    query and the disambiguation context.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.1 Related terms lists contextualized by category for the query `server`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Query: server |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Context: devops 0.83796` `Keywords:`'
  prefs: []
  type: TYPE_NORMAL
- en: '`server 0.93698`'
  prefs: []
  type: TYPE_NORMAL
- en: '`servers 0.76818`'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker 0.75955`'
  prefs: []
  type: TYPE_NORMAL
- en: '`code 0.72832`'
  prefs: []
  type: TYPE_NORMAL
- en: '`configuration 0.70686`'
  prefs: []
  type: TYPE_NORMAL
- en: '`deploy 0.70634`'
  prefs: []
  type: TYPE_NORMAL
- en: '`nginx 0.70366`'
  prefs: []
  type: TYPE_NORMAL
- en: '`jenkins 0.69934`'
  prefs: []
  type: TYPE_NORMAL
- en: '`git 0.68932`'
  prefs: []
  type: TYPE_NORMAL
- en: '`ssh 0.6836`'
  prefs: []
  type: TYPE_NORMAL
- en: '| `Context: cooking -0.1574` `Keywords:`'
  prefs: []
  type: TYPE_NORMAL
- en: '`server 0.66363`'
  prefs: []
  type: TYPE_NORMAL
- en: '`restaurant 0.16482`'
  prefs: []
  type: TYPE_NORMAL
- en: '`pie 0.12882`'
  prefs: []
  type: TYPE_NORMAL
- en: '`served 0.12098`'
  prefs: []
  type: TYPE_NORMAL
- en: '`restaurants 0.11679`'
  prefs: []
  type: TYPE_NORMAL
- en: '`knife 0.10788`'
  prefs: []
  type: TYPE_NORMAL
- en: '`pieces 0.10135`'
  prefs: []
  type: TYPE_NORMAL
- en: '`serve 0.08934`'
  prefs: []
  type: TYPE_NORMAL
- en: '`staff 0.0886`'
  prefs: []
  type: TYPE_NORMAL
- en: '`dish 0.08553`'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `Context: travel -0.15959` `Keywords:`'
  prefs: []
  type: TYPE_NORMAL
- en: '`server 0.81226`'
  prefs: []
  type: TYPE_NORMAL
- en: '`tipping 0.54391`'
  prefs: []
  type: TYPE_NORMAL
- en: '`vpn 0.45352`'
  prefs: []
  type: TYPE_NORMAL
- en: '`tip 0.41117`'
  prefs: []
  type: TYPE_NORMAL
- en: '| `Context: scifi -0.28208` `Keywords:`'
  prefs: []
  type: TYPE_NORMAL
- en: '`server 0.78173`'
  prefs: []
  type: TYPE_NORMAL
- en: '`flynn''s 0.53341`'
  prefs: []
  type: TYPE_NORMAL
- en: '`computer 0.28075`'
  prefs: []
  type: TYPE_NORMAL
- en: '`computers 0.2593`'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| `servers 0.39053` `firewall 0.33092`'
  prefs: []
  type: TYPE_NORMAL
- en: '`restaurant 0.21698`'
  prefs: []
  type: TYPE_NORMAL
- en: '`tips 0.19524`'
  prefs: []
  type: TYPE_NORMAL
- en: '`bill 0.18951`'
  prefs: []
  type: TYPE_NORMAL
- en: '`cash 0.18485`'
  prefs: []
  type: TYPE_NORMAL
- en: '| `flynn 0.24963` `servers 0.24778`'
  prefs: []
  type: TYPE_NORMAL
- en: '`grid 0.23889`'
  prefs: []
  type: TYPE_NORMAL
- en: '`networking 0.2178`'
  prefs: []
  type: TYPE_NORMAL
- en: '`shutdown 0.21121`'
  prefs: []
  type: TYPE_NORMAL
- en: '`hacker 0.19444`'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.1 shows the top most semantically related categories for the query `server`,
    followed by the most semantically related keywords from the `body` field within
    each of those category contexts. Based on the data, we see that the category of
    “devops” is the most semantically related (positive score of `0.83796`), whereas
    the next three categories all contain negative scores (`-0.1574` for “cooking”,
    `-0.15959` for “travel”, and `-0.28208` for “scifi”). For the query `server`,
    the “devops” category is thus overwhelmingly the most likely category to be relevant.
  prefs: []
  type: TYPE_NORMAL
- en: If we look at the different term lists that come back for each of the categories,
    we also see several distinct meanings arise. In the “devops” category, the meaning
    of the term “server” is focused on tools related to managing, building, and deploying
    code to a computer server. In the “scifi” category, the meaning revolves around
    computer grids being hacked and having their networks shut down. In the “travel”
    category, on the other hand, the overwhelming sense of the word “server” is related
    to some-one working in a restaurant, with terms like “tipping”, “restaurant”,
    and “bill” showing up.
  prefs: []
  type: TYPE_NORMAL
- en: When implementing an intelligent search application using this data, if you
    know the user’s context is related to travel, it makes sense to use the specific
    meaning within the “travel” category. If the context is unknown, the best choice
    is usually either the most semantically related category or the most popular category
    among your users.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.2 Contextualized related terms lists by category for the query `driver`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Query: driver |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Context: travel 0.38996` `Keywords:`'
  prefs: []
  type: TYPE_NORMAL
- en: '`driver 0.93417`'
  prefs: []
  type: TYPE_NORMAL
- en: '`drivers 0.76932`'
  prefs: []
  type: TYPE_NORMAL
- en: '`taxi 0.71977`'
  prefs: []
  type: TYPE_NORMAL
- en: '`car 0.65572`'
  prefs: []
  type: TYPE_NORMAL
- en: '`license 0.61319`'
  prefs: []
  type: TYPE_NORMAL
- en: '`driving 0.60849`'
  prefs: []
  type: TYPE_NORMAL
- en: '`taxis 0.57708`'
  prefs: []
  type: TYPE_NORMAL
- en: '`traffic 0.52823`'
  prefs: []
  type: TYPE_NORMAL
- en: '`bus 0.52306`'
  prefs: []
  type: TYPE_NORMAL
- en: '`driver''s 0.51043`'
  prefs: []
  type: TYPE_NORMAL
- en: '| `Context: devops 0.08917` `Keywords:`'
  prefs: []
  type: TYPE_NORMAL
- en: '`ipam 0.78219`'
  prefs: []
  type: TYPE_NORMAL
- en: '`driver 0.77583`'
  prefs: []
  type: TYPE_NORMAL
- en: '`aufs 0.73758`'
  prefs: []
  type: TYPE_NORMAL
- en: '`overlayfs 0.73758`'
  prefs: []
  type: TYPE_NORMAL
- en: '`container_name 0.73483`'
  prefs: []
  type: TYPE_NORMAL
- en: '`overlay2 0.69079`'
  prefs: []
  type: TYPE_NORMAL
- en: '`cgroup 0.68438`'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker 0.67529`'
  prefs: []
  type: TYPE_NORMAL
- en: '`compose.yml 0.65012`'
  prefs: []
  type: TYPE_NORMAL
- en: '`compose 0.55631`'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.2 demonstrates a query-sense disambiguation for the query `driver`.
    In this case, there are two related categories, with “travel” being the most semantically
    related (`0.38996`) versus “devops” (`0.08917`). We can see two very distinct
    meanings of “driver” appear within each of these contexts, with “driver” in the
    “travel” category being related to “taxi”, “car”, “license”, “driving”, and “bus”,
    whereas within the “devops” category “driver” is related to “ipam”, “aufs”, and
    “overlayfs”, which are all different kinds of computer-related drivers.
  prefs: []
  type: TYPE_NORMAL
- en: If someone searches for `driver`, they usually do not intend to find documents
    about both meanings of the word in the search results. There are several ways
    to deal with multiple potential meanings for queried keywords, such as grouping
    results by meaning to highlight the differences, choosing only the most likely
    meaning, carefully interspersing different meanings within the search results
    to provide diversity, or providing alternative query suggestions for different
    contexts. An intentional choice here is usually much better than lazily lumping
    multiple different meanings together.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6.3 Contextualized related terms lists by category for the query `chef`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Query: chef |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `Context: cooking 0.37731` `Keywords:`'
  prefs: []
  type: TYPE_NORMAL
- en: '`chef 0.93239`'
  prefs: []
  type: TYPE_NORMAL
- en: '`chefs 0.5151`'
  prefs: []
  type: TYPE_NORMAL
- en: '`www.pamperedchef.com 0.41292`'
  prefs: []
  type: TYPE_NORMAL
- en: '`kitchen 0.39127`'
  prefs: []
  type: TYPE_NORMAL
- en: '`restaurant 0.38975`'
  prefs: []
  type: TYPE_NORMAL
- en: '`cooking 0.38332`'
  prefs: []
  type: TYPE_NORMAL
- en: '`chef''s 0.37392`'
  prefs: []
  type: TYPE_NORMAL
- en: '`professional 0.36688`'
  prefs: []
  type: TYPE_NORMAL
- en: '`nakiri 0.36599`'
  prefs: []
  type: TYPE_NORMAL
- en: '`pampered 0.34736`'
  prefs: []
  type: TYPE_NORMAL
- en: '| `Context: devops 0.34959` `Keywords:`'
  prefs: []
  type: TYPE_NORMAL
- en: '`chef 0.87653`'
  prefs: []
  type: TYPE_NORMAL
- en: '`puppet 0.79142`'
  prefs: []
  type: TYPE_NORMAL
- en: '`docs.chef.io 0.7865`'
  prefs: []
  type: TYPE_NORMAL
- en: '`ansible 0.73888`'
  prefs: []
  type: TYPE_NORMAL
- en: '`www.chef.io 0.72073`'
  prefs: []
  type: TYPE_NORMAL
- en: '`learn.chef.io 0.71902`'
  prefs: []
  type: TYPE_NORMAL
- en: '`default.rb 0.70194`'
  prefs: []
  type: TYPE_NORMAL
- en: '`configuration 0.68296`'
  prefs: []
  type: TYPE_NORMAL
- en: '`inspec 0.65237`'
  prefs: []
  type: TYPE_NORMAL
- en: '`cookbooks 0.61503`'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: As a final example, table 6.3 demonstrates the query disambiguation for the
    query `chef`. The top two contexts both show reasonably positive relatedness scores,
    indicating that both meanings are likely interpretations. While the “cooking”
    context has a slightly higher score (`0.37731`) than the “devops” context (`0.34959`),
    it would still be important to consider the user’s context as far as possible
    when choosing between these two meanings. The meaning of `chef` within the “devops”
    context is related to the Chef configuration management software used to build
    and deploy servers (related terms include “puppet” and “ansible”), whereas within
    the “cooking” context it refers to a person who prepares food (“cooking”, “taste”,
    “restaurant”, “ingredients”). The Chef software borrows inspiration from the cooking
    domain as a metaphor for how to prepare and serve software, so it’s not surprising
    to see a term like “cookbooks” appear in the “devops” category.
  prefs: []
  type: TYPE_NORMAL
- en: The search-engine-specific SKG request used to disambiguate a query can be seen
    by invoking the `print_disambigutaion_request` function. This can be useful for
    understanding and running the internal SKG request directly against your configured
    search engine or vector database. The Solr-specific SKG request syntax printed
    for this `chef` query-sense disambiguation function call is shown in the following
    listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.4 Solr SKG disambiguation request for the query `chef`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The starting node is a query for chef.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The first SKG traversal finds terms from the category field most related
    to the starting node. These categories are the disambiguation contexts.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The final SKG traversal finds the terms from the body field related to the
    disambiguation context.'
  prefs: []
  type: TYPE_NORMAL
- en: This is the internal Solr SKG request used for disambiguating the query `chef`
    with a `context_limit` of `2`. The request will be specific to whichever search
    engine or vector database is configured, or it will fall back on Solr if the engine
    does not have SKG capabilities. See appendix B for instructions on changing your
    configured search engine.
  prefs: []
  type: TYPE_NORMAL
- en: By combining query classification, term disambiguation, and query expansion,
    an SKG can power enhanced domain-specific and highly contextualized semantic search
    capabilities within your AI-powered search engine. We’ll dive into using these
    techniques further in chapter 7 when we apply them in a live semantic search application.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Learning related phrases from query signals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Thus far, you’ve seen how to use your content as a knowledge graph to discover
    related terms, classify queries, and disambiguate terms. While these techniques
    are powerful, they are also entirely dependent upon the quality of your documents.
    Throughout the rest of this chapter, we’ll explore the other major source of knowledge
    about your domain—user signals (queries, clicks, and subsequent actions). Often,
    user signals can lead to similar, if not even more useful, insights than document
    content for interpreting queries.
  prefs: []
  type: TYPE_NORMAL
- en: As a starting point for learning domain-specific terminology from real user
    behavior, let’s consider what your query logs represent. For every query to your
    search engine, a query log contains an identifier for the person running the search,
    the query that was run, and the timestamp of the query. This means that if a single
    user searches for multiple terms, you can group those searches together and also
    know in which order the terms were entered.
  prefs: []
  type: TYPE_NORMAL
- en: While it’s not always true, one reasonable assumption is that if someone enters
    two different queries within a very short timespan, the second query is likely
    to be either a refinement of the first query or about a related topic. Figure
    6.2 demonstrates a realistic sequence of searches you might find for a single
    user in your query logs.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/CH06_F02_Grainger.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 A typical sequence of searches from query logs for a particular user
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When looking at these queries, we intuitively understand that `iphond` is a
    misspelling of `iphone`, that `iphone accesories` is a misspelling of `iphone
    accessories`, and that `iphone`, `pink phone case`, and `pink iphone case` are
    all related queries. We’ll deal with the misspellings in a later section, but
    we can consider those to also be related terms for now.
  prefs: []
  type: TYPE_NORMAL
- en: While it’s not wise to depend on a single user’s signals to deduce that two
    queries are related, similar query patterns across many users indicate likely
    relationships. As we demonstrated in section 5.4.5, queries can be expanded to
    include related terms to improve recall. In this section, we’ll explore techniques
    for learning related queries, first through mining query logs and then through
    cross-referencing product interaction logs.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.1 Mining query logs for related queries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we start mining user signals for related queries, let’s first convert
    our signals into a simpler format for processing. The following listing provides
    a transformation from our generic signal structure to a simple structure that
    maps each occurrence of a query term to the user who searched for that term.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.5 Mapping signals into keyword, user pairs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Loads all documents from the signals corpus into a Spark view'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Selects keyword and user data from query signals'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You can see from this listing that over 725,000 queries are represented. Our
    goal is to find pairs of related queries based on how many users entered both
    queries. The more frequently two queries co-occur across different users’ query
    logs, the more related those queries are presumed to be.
  prefs: []
  type: TYPE_NORMAL
- en: The next listing shows each query pair where both queries were searched by the
    same user, along with the number of users that searched for both queries (`users_cooc`).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.6 Total occurrences and co-occurrences of queries
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Counts the number of users that searched for both k1 and k2'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Limits keyword pairs to only one permutation to avoid duplicate pairs'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Joins the user_searches view with itself on the user field to find all keyword
    pairs searched by the same user'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In listing 6.6, the first query produces the most searched-for keywords, as
    seen in the results. While these may be the most popular queries, they aren’t
    necessarily the queries that co-occur the most often with other queries. The second
    query produces the total number of query pairs (244,876) where both queries were
    searched by the same user at least once. The final query ranks these query pairs
    by popularity. These top query pairs are highly related.
  prefs: []
  type: TYPE_NORMAL
- en: Notice, however, that the top result only has `23` co-occurring users, which
    means the number of data points is sparse and will likely include more noise further
    down the list. In the next section, we’ll explore a technique to combine signals
    along a different axis (product interactions), which can help with this sparsity
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: While directly aggregating the number of searches into co-occurrences by users
    helps find the most popular query pairs, the popularity of searches isn’t the
    only metric useful for finding relatedness. The keywords “and” and “of” are highly
    co-occurring, as are “phones”, “movies”, “computers”, and “electronics”, because
    they are all general words that many people search. To additionally focus on the
    strength of the relationship between terms independent of their individual popularity,
    we can use a technique called *pointwise mutual information*.
  prefs: []
  type: TYPE_NORMAL
- en: '*Pointwise mutual information* (PMI) is a measure of association between any
    two events. In the context of natural language processing, PMI predicts the likelihood
    of two words occurring together because they are related versus the likelihood
    of them occurring together by chance. Many formulas can be used to calculate and
    normalize PMI, but we’ll use a variation called PMI^k, where `k = 2`, which does
    a better job than PMI at keeping scores consistent regardless of word frequencies.'
  prefs: []
  type: TYPE_NORMAL
- en: The formula for calculating PMI² is shown in figure 6.3.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/grainger-ch6-eqs-0x.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 PMI² score
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In our implementation, `k1` and `k2` represent two different keywords that we
    want to compare. `P(k1,k2)` represents how often the same user searches for both
    keywords, whereas `P(k1)` and `P(k2)` represent how often a user only searches
    for the first keyword or second keyword, respectively. Intuitively, if the keywords
    appear together more often than they would be expected to, based on their likelihood
    of randomly appearing together, then they will have a higher PMI² score. The higher
    the score, the more likely the terms are to be semantically related.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing demonstrates the PMI² calculation on our co-occurring
    query pairs dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.7 PMI² calculation on user searches
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The PMI calculation'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The results from listing 6.7 are sorted by PMI² score, and we set a minimum
    occurrences threshold at `>5` to help remove noise. “hp laptops”, “dell laptops”,
    and “sony laptops” show up as related, as well as brands like “kenwood” and “alpine”.
    Notably, there is also noise in the pairs, like “wireless mouse” with “godfather”
    and “quicken” with “portable dvd players”. One caveat of using PMI is that a small
    number of occurrences together across a few users can lead to noise more easily
    than when using co-occurrence, which is based upon the assumption of terms commonly
    co-occurring.
  prefs: []
  type: TYPE_NORMAL
- en: One way to blend the benefits of both the co-occurrence model and the PMI² models
    is to create a composite score. This will provide a blend of popularity and likelihood
    of occurrence, which should move query pairs that match on both scores to the
    top of the list. Listing 6.8 demonstrates one way to blend these two measures
    together. Specifically, we take a ranked list of all co-occurrence scores (`r1`)
    along with a ranked list of all PMI² scores (`r2`) and blend them together to
    generate a composite ranking score as shown in figure 6.4.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/grainger-ch6-eqs-1x.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 Composite ranking score combining co-occurrence and PMI² ranking
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The `comp_score`, or composite rank score, shown in figure 6.4 assigns a high
    score to query pairs (query `q1` and query `q2`) where their rank in the co-occurrence
    list (`r1`) and their rank in the PMI² list (`r2`) is high, and it assigns a lower
    rank as the terms move further down in the rank lists. The result is a blended
    ranking that considers both the popularity (co-occurrence) and the likelihood
    of the relatedness of queries regardless of their popularity (PMI²). The following
    listing shows how to calculate the `comp_score` based on the already-calculated
    co-occurrence and PMI² scores.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.8 Calculating a composite score from co-occurrence and PMI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The composite score calculation combines the sorted ranks of the PMI2 score
    and the co-occurrences.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Ranks the co-occurrence scores from best (highest co-occurrence) to worst
    (lowest co-occurrence)'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Ranks the PMI2 scores from best (highest PMI2) to worst (lowest PMI2 score)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Overall, the composite rank score does a reasonable job of blending our co-occurrence
    and PMI² metrics to overcome the limitations of each. The top results shown in
    listing 6.8 all look reasonable. One problem we already noted in this section,
    however, is that the co-occurrence numbers are very sparse. Specifically, the
    highest co-occurrence of any query pairs, out of over 700,000 query signals, was
    `23` overlapping users for “green lantern” and “captain america”, as shown in
    listing 6.6.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll show a way we can overcome this sparse data problem,
    where there is a lack of overlap between users for specific query pairs. We’ll
    accomplish this by aggregating many users together into a larger group with similar
    behaviors. Specifically, we’ll switch our focus to the products where user queries
    overlap, as opposed to focusing on the individual users issuing the overlapping
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 Finding related queries through product interactions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The technique used to find related terms in section 6.3.1 depends on many users
    searching for overlapping queries. As we saw, with over 700,000 query signals,
    the highest overlap of any query pair was `23` users. Because the data can be
    so sparse, it can often make sense to aggregate on something other than users.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll demonstrate how we can use the same technique (using
    co-occurrence and PMI²) but rolling up based on product click signals instead
    of users. Since you’ll hopefully have many more users than products, and since
    particular products are likely to be clicked in response to similar keywords,
    this technique helps overcome the data sparsity problem and generates higher overlaps
    between queries.
  prefs: []
  type: TYPE_NORMAL
- en: 'The transformation in listing 6.9 combines separate query and click signals
    into single rows with three key columns: `keyword`, `user`, and `product`.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.9 Mapping raw signals into keyword, user, product groupings
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Utilizes click signals to produce keyword, user, and product groupings'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Using this data, we’ll now be able to determine the strength of the relationship
    between any two keywords based on their use across independent users searching
    for the same products. Listing 6.10 generates pairs of keywords to determine their
    potential relationship for all keyword pairs where both keywords were used in
    a query for the same document. The idea behind looking for overlapping queries
    for each user in section 6.3.1 was that each user is likely to search for related
    items. Each product is also likely to be searched for by related queries, though,
    so we can shift our mental model from “find how many users searched for both queries”
    to “find how many documents were found by both queries across all users”.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of this transformation in listing 6.10 now includes the following
    columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`k1`, `k2`—The two keywords that are potentially related because they both
    resulted in a click on the same product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_users1`—The number of users who searched for `k1` that clicked on a product
    that was also clicked on after a search by some user for `k2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_users2`—The number of users who searched for `k2` that clicked on a product
    that was also clicked on after a search by some user for `k1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`users_cooc`—Represents the total number of users who searched for either `k1`
    or `k2` and visited a product visited by other searchers for `k1` or `k2`. Calculated
    as `n_users1` + `n_users2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_products`—The number of products that were clicked on by searchers for both
    `k1` and `k2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Listing 6.10 Keyword pairs leading to the same product being clicked
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `users_cooc` and `n_products` calculations are two different ways to look
    at overall signal quality for how confident we are that any two terms `k1` and
    `k2` are related. The results are currently sorted by `n_products`, and you can
    see that the top of the list of relationships is quite clean. These keyword pairs
    represent multiple kinds of meaningful semantic relationships, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Spelling variations*—“laptops” ⇒ “laptop” ; “headphones” ⇒ “head phones” ;
    etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Brand associations*—“tablet” ⇒ “ipad” ; “laptop” ⇒ “hp” ; “mac” ⇒ “apple”
    ; etc.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Synonyms/alternate names*—“netbook” ⇒ “laptop” ; “tablet pc” ⇒ “tablet”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Category expansion*—“ipad” ⇒ “tablet” ; “iphone 4” ⇒ “iphone” ; “tablet” ⇒
    “computers” ; “laptops” ⇒ “computers”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can write custom, domain-specific algorithms to identify some of these specific
    types of relationships, as we’ll do for spelling variations in section 6.5.
  prefs: []
  type: TYPE_NORMAL
- en: It’s also possible to use `n_users1` and `n_users2` to identify which of the
    two queries is more popular. In the case of spelling variations, we see that `headphones`
    is used more commonly than `head phones` (1,829 versus 492 users) and is also
    more common than `headphone` (1,617 versus 367 users). Likewise, we see that `tablet`
    is much more common in usage than `tablet` `pc` (1,408 versus 296 users).
  prefs: []
  type: TYPE_NORMAL
- en: While our current list of keyword pairs looks clean, it only represents the
    keyword pairs that both occurred together in searches that led to the same products.
    Determining the popularity of each keyword overall will provide a better sense
    of which specific keywords are the most important for our knowledge graph. The
    following listing calculates the most popular keywords from our query signals
    that resulted in at least one product click.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.11 Computing keyword searches that resulted in clicks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This list is identical to the list from listing 6.6, but instead of showing
    the number of users who searched for a keyword, this list shows the number of
    users who searched for a keyword and also clicked on a product. We’ll use this
    as our master list of queries for the PMI² calculation.
  prefs: []
  type: TYPE_NORMAL
- en: With our query pairs and query popularity now based on queries and product interactions,
    the rest of our calculations (PMI² and composite score) are the same as in section
    6.3.1, so we’ll omit them here (they are included in the notebooks for you to
    run). After calculating the PMI² and composite scores, the following listing shows
    the final results of our product-interaction-based related terms calculations.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.12 Related terms scoring based on product interactions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The results of listings 6.11 and 6.12 show the benefit of aggregating at a less
    granular level. By looking at all queries that led to a particular product being
    clicked on, the list of query pairs is now much larger than in section 6.3.1,
    where query pairs were aggregated by individual users. You can see that there
    are now 1,579,710 query pairs under consideration versus 244,876 (per listing
    6.6) when aggregating by user.
  prefs: []
  type: TYPE_NORMAL
- en: Further, you can see that the related queries include more fine-grained variations
    for top queries (`ipad`, `ipad 2`, `ipad2`, `i pad`, `ipads`, `i pad 2`). Having
    more granular variations like this will come in handy if you are combining this
    related term discovery with other algorithms, like misspelling detection, which
    we’ll cover in section 6.5.
  prefs: []
  type: TYPE_NORMAL
- en: Between the SKG approach in the last chapter and query log mining in this chapter,
    you’ve now seen multiple techniques for discovering related phrases. Before we
    can apply the related phrases, however, we first need to be able to identify such
    known phrases in incoming queries. In the next section, we’ll cover how we can
    generate a list of known phrases from our query signals.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Phrase detection from user signals
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In section 5.3, we discussed several techniques for extracting arbitrary phrases
    and relationships from documents. While this can go a long way toward discovering
    all the relevant domain-specific phrases within your content, this approach suffers
    from two different problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '*It generates a lot of noise*—Not every noun phrase across your potentially
    massive set of documents is important, and the odds of identifying incorrect phrases
    (false positives) increase as your number of documents increases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*It ignores what your users care about*—The real measure of user interest is
    communicated by what they search for. They may only be interested in a subset
    of your content or may be looking for things that aren’t even represented well
    within your content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we’ll focus on how to identify important domain-specific phrases
    from your user signals.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.1 Treating queries as entities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The easiest way to extract entities from query logs is to treat the entire query
    as one entity. In use cases like our RetroTech e-commerce site, this works very
    well, as many of the queries are product names, categories, brand names, company
    names, or people’s names (actors, musicians, etc.). Given that context, most of
    the high-popularity queries end up being entities that can be used directly as
    phrases without needing any special parsing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking back at the output of listing 6.11, you’ll find the following most
    popular queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: These are entities that belong in a known-entities list, with many of them being
    multiword phrases. In this case, the simplest method for extracting entities is
    also the most powerful—just use the queries as your entities list. The higher
    the frequency of each query across users, the more confident you can be about
    adding it to your entities list.
  prefs: []
  type: TYPE_NORMAL
- en: One way to reduce potential false positives from noisy queries is to find phrases
    that overlap in both your documents and queries. Additionally, if you have different
    fields in your documents, like a product name or company, you can cross-reference
    your queries with those fields to assign a type to the entities found within your
    queries.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the complexity of your queries, using the most common searches
    as your key entities may be the most straightforward way to achieve a high-quality
    entities list.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.2 Extracting entities from more complex queries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In some use cases, the queries may contain more noise (Boolean structure, advanced
    query operators, etc.) and therefore may not be directly usable as entities. In
    those cases, the best approach to extracting entities may be to reapply the entity
    extraction strategies from chapter 5, but on your query signals.
  prefs: []
  type: TYPE_NORMAL
- en: Out of the box, a lexical search engine parses queries as individual keywords
    and looks them up in the inverted index. For example, a query for `new york city`
    will be automatically interpreted as the Boolean query `new AND york AND city`
    (or if you set the default operator to `OR`, then `new OR york OR city`). The
    relevance ranking algorithms will then score each keyword individually instead
    of understanding that certain words combine to make phrases that then take on
    a different meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to identify and extract domain-specific phrases from queries can
    enable more accurate query interpretation and relevance. We already demonstrated
    one way to extract domain-specific phrases from documents in section 5.3, using
    the spaCy NLP library to do a dependency parse and extract out noun phrases. While
    queries are often too short to perform a true dependency parse, it’s still possible
    to apply some part of speech filtering on any discovered phrases in queries to
    omit non-noun phrases. If you need to split sections of queries apart, you can
    also tokenize the queries and remove query syntax (`AND`, `OR`, etc.) before looking
    for phrases to extract. Handling the specific query patterns for your application
    may require some domain-specific query parsing logic, but if your queries are
    largely single phrases or easily tokenizable into multiple phrases, your queries
    likely represent the best source of domain-specific phrases to extract and add
    to your knowledge graph. We’ll walk through code examples of phrase identification
    when parsing queries in section 7.4\.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Misspellings and alternative representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve covered detecting domain-specific phrases and finding related phrases,
    but there are two very important subcategories of related phrases that typically
    require special handling: misspellings and alternative spellings (also known as
    *alternative labels*). When entering queries, users will commonly misspell their
    keywords, and the general expectation is that an AI-powered search system will
    be able to understand and properly handle those misspellings.'
  prefs: []
  type: TYPE_NORMAL
- en: While general related phrases for “laptop” might be “computer”, “netbook”, or
    “tablet”, misspellings would look more like “latop”, “laptok”, or “lapptop”. *Alternative
    labels* are functionally no different than misspellings but occur when multiple
    valid variations for a phrase exist (such as “specialized” versus “specialised”
    or “cybersecurity” versus “cyber security”). In the case of both misspellings
    and alternative labels, the end goal is usually to normalize the less common variant
    into the more common, canonical form and then search for the canonical version.
  prefs: []
  type: TYPE_NORMAL
- en: Spell-checking can be implemented in multiple ways. In this section, we’ll cover
    the out-of-the-box document-based spell-checking that is found in most search
    engines, and we’ll also show how user signals can be mined to fine-tune spelling
    corrections based upon real user interactions with your search engine.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.1 Learning spelling corrections from documents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most search engines contain some spell-checking capabilities out of the box,
    based on the terms found within a collection’s documents. Apache Solr, for example,
    provides file-based, dictionary-based, and index-based spell-checking components.
    The file-based spell-checker requires assembling a list of terms that can be spell-corrected.
    The dictionary-based spell-checker can build a list of terms to be spell-corrected
    from fields in an index. The index-based spell-checker can use a field on the
    main index to spell-check directly without having to build a separate spell-checking
    index. Additionally, if someone has built a list of spelling corrections offline,
    you can use a synonym list to directly replace or expand any misspellings to their
    canonical form.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch and OpenSearch have similar spellchecking capabilities, even allowing
    specific contexts to refine the scope of the spelling suggestions to a particular
    category or geographical location.
  prefs: []
  type: TYPE_NORMAL
- en: 'While we encourage you to test out these out-of-the-box spell-checking algorithms,
    they all unfortunately suffer from a major problem: lack of user context. Specifically,
    anytime a keyword is searched that doesn’t appear a minimum number of times in
    the index, the spell-checking component begins looking at all terms in the index
    that are *off by the minimum number of characters*, and they then return the most
    prevalent keywords in the index that match the criteria. The following listing
    shows an example of where out-of-the-box index-based spell-checking configuration
    falls short.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.13 Using out-of-the-box spelling corrections on documents
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In listing 6.13, you can see a user query for `moden`. The spell-checker returns
    the suggested spelling corrections of “modes”, “model”, “modern”, and “modem”,
    plus one suggestion that only appears in a few documents, which we’ll ignore.
    Since our collection is tech products, it may be obvious which of these is likely
    the best spelling correction: it’s “modem”. In fact, it is unlikely that a user
    would intentionally search for “modes” or “model” as standalone queries, as those
    are both generic terms that would usually only make sense within a context containing
    other words.'
  prefs: []
  type: TYPE_NORMAL
- en: The content-based index has no way to distinguish easily that end users would
    be unlikely to search for “modern” or “model”. Thus, while content-based spell-checkers
    can work well in many cases, it is often more accurate to learn spelling corrections
    from users’ query behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5.2 Learning spelling corrections from user signals
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Returning to our core thesis from section 6.3 that users tend to search for
    related queries until they find the expected results, it follows that a user who
    misspelled a particular query and received bad results would then try to correct
    their query.
  prefs: []
  type: TYPE_NORMAL
- en: 'We already know how to find related phrases (discussed in section 6.3), but
    in this section we’ll cover how to specifically distinguish a misspelling based
    on user signals. This task largely comes down to two goals:'
  prefs: []
  type: TYPE_NORMAL
- en: Find terms with similar spellings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure out which term is the correct spelling versus the misspelled variant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For this task, we’ll rely solely on query signals. We’ll perform some up-front
    normalization to make the query analysis case-insensitive and filter duplicate
    queries to avoid signal spam. (We’ll discuss signal normalization in sections
    8.2–8.3.) The following listing shows a query that grabs our normalized query
    signals.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.14 Getting all queries searched by users
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Lowercasing the queries makes the query analysis ignore uppercase vs. lowercased
    variants.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Grouping by user prevents spam from a single user entering the same query
    many times.'
  prefs: []
  type: TYPE_NORMAL
- en: For the purposes of this section, we’re going to assume that the queries can
    contain multiple different keywords and that we want to treat each of these keywords
    as a potential spelling variant. This will allow individual terms to be found
    and substituted within a future query, as opposed to treating the entire query
    as a single phrase. It will also allow us to throw out certain terms that are
    likely to be noise, such as stop words or standalone numbers.
  prefs: []
  type: TYPE_NORMAL
- en: The following listing demonstrates the process of tokenizing each query to generate
    a word list upon which we can do further analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.15 Finding words by tokenizing and filtering query terms
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Defines stop words that shouldn’t be considered as misspellings or corrections
    utilizing the Natural Language Toolkit (nltk)'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Removes noisy terms including stop words, very short terms, and numbers'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Splits the query on whitespace into individual terms if tokenizing'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Aggregates the occurrences of valid keywords'
  prefs: []
  type: TYPE_NORMAL
- en: Once the list of tokens has been cleaned up, the next step is to distinguish
    high-occurrence tokens from infrequently occurring tokens. Since misspellings
    will occur relatively infrequently and correct spellings will occur more frequently,
    we will use the relative number of occurrences to determine which version is most
    likely the canonical spelling and which variations are the misspellings.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure our spell correction list is as clean as possible, we’ll set some
    thresholds for popular terms and some for low-occurrence terms that are more likely
    misspellings. Because some collections may contain hundreds of documents and other
    collections could contain millions, we can’t just look at an absolute number for
    these thresholds, so we’ll use quantiles instead. The following listing shows
    the calculations for each of the quantiles between `0.1` and `0.9`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.16 Calculating quantiles to identify spelling candidates
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Here we see that 80% of the terms are searched for `142.2` times or less. Likewise,
    only 20% of terms are searched for `6.0` times or less. Using the Pareto principle,
    let’s assume that most of our misspellings fall within the bottom-searched 20%
    of our terms and that the majority of our most important terms fall within the
    top 20% of queries searched. If you want higher precision (only generate spelling
    corrections for high-value terms and only if there’s a low probability of false
    positives), you can push these to the `0.1` quantile for misspellings and the
    `0.9` quantile for correctly spelled terms. You can also go the other direction
    to attempt to generate a larger misspelling list with a higher chance of false
    positives.
  prefs: []
  type: TYPE_NORMAL
- en: In Listing 6.17, we’ll divide the terms into buckets, assigning low-frequency
    terms to the `misspellings` bucket and high-frequency terms to the `corrections`
    bucket. These buckets will be a starting point for finding high-quality spelling
    corrections when enough users search for both the misspelling candidate and the
    correction candidate.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.17 Identifying spelling correction candidates
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Terms at or below the 0.2 quantile are added to the misspellings list.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The number of searches is retained to keep track of popularity.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The length of the term will be used later to set thresholds for edit distance
    calculations.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 The first letter of the term is stored to limit the scope of the misspellings
    checked.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 The top 20% of terms have the same data stored but in the corrections list.'
  prefs: []
  type: TYPE_NORMAL
- en: To efficiently compare all of the `misspellings` and the `corrections` values,
    we first load them into dataframes in listing 6.17\. You can imagine that `corrections`
    is a pristine list of the most popular searched terms, while the `misspellings`
    list should provide a good candidate list for less commonly searched terms that
    are more likely to be misspellings.
  prefs: []
  type: TYPE_NORMAL
- en: When we compare misspelled candidates with correctly spelled candidates and
    decide how many character differences (or *edit distances*) are allowed, we need
    to consider the term length. The following listing shows a simple `good_match`
    function, which defines a general heuristic for how many edit distances a term
    match can be off by while still considering the misspelling a likely permutation
    of the correction candidate.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.18 Finding proper spellings by lengths and edit distance
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: With our `misspellings` and `corrections` candidates loaded into dataframes
    and the `good_match` function defined, it’s time to generate our spelling correction
    list. Just like in section 6.5.1, where spelling corrections were generated from
    edit distances and counts of term occurrences within our collection of documents,
    listing 6.19 generates spelling corrections based on edit distances and term occurrences
    within our query logs.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.19 Mapping misspellings to their correct spellings
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Groups the misspelling and correction candidates on the first letter of
    the word'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Calculates the edit distance between each misspelling and correction candidate'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Applies the good_match function using the lengths of the terms and the edit
    distance'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Aggregates all the misspellings by name'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Gets the 20 most misspelled words'
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we now have a relatively clean list of spelling corrections
    based on user signals. Our query of `moden` maps correctly to “modem”, as opposed
    to unlikely search terms like “model” and “modern”, which we saw in the document-based
    spelling correction in listing 6.13.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous other ways that you could go about creating a spelling correction
    model. If you wanted to generate multiterm spelling corrections from documents,
    you could generate bigrams and trigrams to perform chained Bayesian analysis on
    probabilities of consecutive terms occurring. Likewise, to generate multiterm
    spelling corrections from query signals, you could remove the tokenization of
    queries by setting `tokenize` to `False` when calling `valid_keyword_occurrences`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 6.20 Finding multiterm spell corrections from full queries
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: You can see some of the common multiword misspellings and their corrections
    in listing 6.20, now that the queries are no longer being tokenized. Note that
    the single-term words are largely the same, but multiword queries have also been
    spell-checked. This is a great way to normalize product names, so that “iphone4
    s”, “iphones 4s”, and “iphone s4” are all correctly mapped to the canonical “iphone
    4s”. Note that in some cases this can be a lossy process, as “hp touchpad 32”
    maps to “hp touchpad”, and “iphone3” maps to “iphone”. Depending on your use case,
    you may find it beneficial to only spell-correct individual terms, or to include
    special handling in your `good_match` function for brand variations to ensure
    the spell-check code doesn’t mistakenly delete relevant query context.
  prefs: []
  type: TYPE_NORMAL
- en: 6.6 Pulling it all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we dove deeper into understanding the context and meaning of
    domain-specific language. We showed how to use SKGs to classify queries and disambiguate
    terms that have different or nuanced meanings based on their context. We also
    explored how to mine relationships from user signals, which usually provides a
    better context for understanding your users than looking at your documents alone.
    We also showed how to extract phrases, misspellings, and alternative labels from
    query signals, enabling domain-specific terminology to be learned directly from
    users as opposed to only from documents.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should feel confident about learning domain-specific phrases
    and related phrases from documents or user signals, classifying queries to your
    available content, and disambiguating the meaning of terminology based on the
    query classification. These techniques are critical tools in your toolbox for
    interpreting query intent.
  prefs: []
  type: TYPE_NORMAL
- en: Our goal isn’t just to assemble a large toolbox, though. Our goal is to use
    each of these tools where appropriate to build an end-to-end semantic search layer.
    This means we need to model known phrases into our knowledge graph, extract those
    phrases from incoming queries, handle misspellings, classify queries, disambiguate
    incoming terms, and ultimately generate a rewritten query for the search engine
    that uses each of our AI-powered search techniques. In the next chapter, we’ll
    show you how to assemble each of these techniques into a working semantic search
    system designed to best interpret and model query intent.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classifying queries using a semantic knowledge graph (SKG) can help interpret
    query intent and improve query routing and filtering.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query-sense disambiguation can deliver a more contextual understanding of a
    user’s query, particularly for terms with significantly divergent meanings across
    different contexts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to learning from documents, domain-specific phrases and related
    phrases can also be learned from user signals.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Misspellings and spelling variations can be learned from both documents and
    user signals, with document-based approaches being more robust and user-signal-based
    approaches better representing user intent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
