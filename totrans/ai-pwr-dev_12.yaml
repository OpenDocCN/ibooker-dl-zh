- en: 9 GPT-ing on the go
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Running a large language model locally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing the results of two locally hosted large language models against those
    of ChatGPT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determining when using offline models is appropriate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imagine you are on your way to an AI conference halfway around the world. You
    are on a plane, cruising at 35,000 feet above the ground, and you want to prototype
    a new feature for your application. The airplane’s Wi-Fi is prohibitively slow
    and expensive. What if instead of paying all that money for a broken and borderline
    unusable GPT, you have one running right there on your laptop, offline? This chapter
    will review developers’ options to run a large language model (LLM) locally.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Motivating theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The introductory scenario is not too far a stretch. Although the ubiquity of
    high-speed internet is increasing, it has not yet achieved total coverage. You
    will find yourself in areas without broadband, whether at home, on the road, at
    school, or in the office. Hopefully, this book has successfully made the case
    that you should be using LLMs as a tool in your developer toolbelt. For this reason,
    you need to take precautions to ensure that you always have an LLM available to
    you in some capacity. As you use it, the more you will get from it. Like your
    dependency on an integrated development environment, without it, you are still
    a good developer; with it, however, you are much more.
  prefs: []
  type: TYPE_NORMAL
- en: But fear not. Many options are available to you. This chapter will present two,
    neither requiring a complex and pained installation process. You will not need
    to memorize the APIs from a specific vendor. These approaches are not all that
    different from using ChatGPT. Your prompting skills will be fully portable. Ready?
    Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Hosting your own LLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we look to run an LLM on our local machine, we immediately encounter a
    couple of problems: the first is that LLMs generally require significant computational
    resources. High-performance GPUs are typically necessary to run these models.
    The cost of such hardware can be prohibitive. The large size of these models means
    they require substantial memory to load and run. This can be a challenge even
    for systems with high-end GPUs, as they may not have enough VRAM to accommodate
    the model. The second problem we need to consider is the quality of the output
    of these models relative to managed LLMs like ChatGPT.'
  prefs: []
  type: TYPE_NORMAL
- en: This chapter examines two instances of models explicitly selected because they
    do not require costly hardware. These models run on modest commodity hardware,
    such as the Apple MacBook Pro M2 silicon chip on which this book was written.
    We will start with Llama 2, an LLM developer by Meta and trained on 2 trillion
    tokens and offering 7 billion, 13 billion, and 70 billion parameter options. Llama
    2 can present difficulties in installing and running locally; fortunately, there
    is a Dockerized version called Ollama, which we will use in the first section
    of this chapter. In the second half of the chapter, we use GPT-4All.
  prefs: []
  type: TYPE_NORMAL
- en: This leaves the second problem to contend with. To this end, we will use the
    output generated by ChatGPT as the baseline against which to measure these local
    models. It should not come as a surprise, but these models perform very well relative
    to the baseline.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.1 Baselining with ChatGPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this chapter, we will use a novel (at least in this book) problem: calculating
    the standard deviation of a list of integers. The standard deviation measures
    the variation in a set of values. Throughout the chapter, we will use the same
    prompt and present it to each model:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| As a mathematician, you are attempting to compute the standard deviation
    of a list in pure Python. Please show me the code you would use and walk me through
    it step by step. |'
  prefs: []
  type: TYPE_TB
- en: ChatGPT provides the following explanation of the steps involved and the method.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.1 ChatGPTs explanation of calculating standard deviation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If we write a small `main` function to sum the list of integers from 1 to 4,
    we get the value 1.4142135623730951 or a close approximation.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.2 `main` function to drive our standard deviation calculation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Feel free to use your favorite calculator, financial modeling program, or other
    mechanism to verify the result. You will find that this value is more or less
    correct. Now that we have a baseline against which to compare, we can begin our
    comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.2 Asking Llama 2 to spit out an answer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It’s time to introduce our first locally running LLM, Llama 2\. As previously
    mentioned, this model can require considerable effort to install (at least at
    the time of writing). To make it easier to start evaluating this model, we will
    use the Dockerized version: Ollama. The following two Docker commands will run
    our model.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.3 Starting and running Ollama
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If this works correctly, you will (eventually) see a prompt of three greater-than
    symbols (`>`). The first time you run this command, Ollama will need to download
    the Llama 2 model, which is several gigabytes. This will likely take a while and
    needs to be done with a stable internet connection. However, you will not need
    an internet connection once this has been completed. Therefore, ensure that you
    run this command before using Ollama in offline mode.
  prefs: []
  type: TYPE_NORMAL
- en: 'With Ollama installed and running, we can now ask Llama 2 to introduce itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Imagine that you are the author of a book on Generative AI. You are working
    on a chapter about running an LLM locally on your machine. How might you describe
    Llama? |'
  prefs: []
  type: TYPE_TB
- en: Listing 9.4 Llama 2 provides some background on itself
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'With introductions out of the way, we can begin our assessment of the output
    from Llama 2, comparing it to the output produced by ChatGPT. We have a baseline
    answer that we know is correct, so we can accurately assess the correctness of
    the code that Llama 2 delivers. Let’s ask Llama 2 to calculate the standard deviation
    of a list of numbers using the same prompt as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| As a mathematician, you are attempting to compute the standard deviation
    of a list in pure Python. Please show me the code that you would use and walk
    me through it step by step. |'
  prefs: []
  type: TYPE_TB
- en: Listing 9.5 Llama 2’s first attempt to calculate standard deviation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Although this may be the way we would do it in the real world (kudos, Llama
    2), it does not provide us with an apples-to-apples comparison. Let’s reframe
    and refine the question:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Do not use any libraries, built in Python functions. |'
  prefs: []
  type: TYPE_TB
- en: Okay, technically, it’s not a question, but hopefully, it will get the job done.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.6 Llama 2’s second attempt to calculate standard deviation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Attentive readers (and do not worry if you missed this at first) may notice
    that Llama 2 subtracted one from the number of items in the list. This is curious.
    It does not match ChatGPT’s response. Let us see if we can figure out why. Let’s
    ask it:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| What is the formula for the standard deviation? |'
  prefs: []
  type: TYPE_TB
- en: Listing 9.7 Llama 2’s understanding of the standard deviation formula
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This is consistent with the code that Llama 2 generated, so that’s good. However,
    it does not explain the apparent discrepancy in the output between this and ChatGPT.
    Let’s ask it directly why it subtracted the one:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| When you subtract 1 from the number of numbers in the list is that to add
    a degree of freedom? If so, why? |'
  prefs: []
  type: TYPE_TB
- en: If you have taken statistics in the past, you may see what is coming. If not,
    Llama 2 will provide a useful explanation for what is meant by *degrees of freedom*
    and why it is used.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.8 Llama 2’s explanation of degrees of freedom
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a quick detour to answer this question: What is the formula for
    the standard deviation? The answer is, which one? There are actually two: sample
    and population. The sample standard deviation refers to the standard deviation
    calculated from a subset, or *sample*, of data points taken from a larger population.
    This formula is shown and explained in figure 9.1 This is used when working with
    limited information and cannot be applied to the entire dataset (population).
    This is contrasted to the *population* standard deviation, which is how much the
    individual data points deviate from the average value of the population. This
    formula is shown in figure 9.2'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F01_Crocker2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 The formula for the sample standard deviation
  prefs: []
  type: TYPE_NORMAL
- en: Both formulas involve squaring the difference between each data point and the
    mean, summing these squares, and then taking the square root of the sum. This
    provides a measure of the spread of values around the mean.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F02_Crocker2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 The formula for the population standard deviation
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we can conclude that Llama 2 generated a function to calculate the
    sample standard deviation rather than the population standard deviation (the baseline).
    Given that this is in the realm of statistics specifically, we can ask,
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| How would you have changed your response if I had asked you to assume the
    role of a statistician rather than a mathematician? |'
  prefs: []
  type: TYPE_TB
- en: Listing 9.9 Llama 2’s response in the role of a statistician
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Llama 2 changes its response if we ask it to assume the statistician role. Unfortunately,
    it does not divide by the length of the list of numbers. Once this is fixed, we
    will get the correct answer for the population standard deviation. Let this be
    a reminder that LLMs can confidently produce incorrect answers. Always double-check
    the results against your knowledge or that of experts. Now, type **`/bye`** to
    end your session. (Typing `/bye` in Llama 2 signals that you wish to terminate
    the session.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s focus on another LLM we can run locally: GPT-4All.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.2.3 Democratizing answers with GPT-4All
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GPT-4All is open source software developed by Anthropic that allows users to
    train and operate their own LLMs. It is based on GPT-3 and therefore may not operate
    as effectively as a GPT-4-based model; however, it can be run directly on a personal
    computer without the need for an internet connection. Despite the similarity in
    name, it is not related to GPT-4 at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we dive in and use it, let’s have GPT4-All introduce itself, using the
    following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Imagine that you are the author of a book on Generative AI. You are working
    on a chapter about running an LLM locally. How might you describe GPT-4All? |'
  prefs: []
  type: TYPE_TB
- en: Listing 9.10 GPT-4All’s description of itself
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Unlike Ollama, GPT-4All requires installation. Fortunately, the process is
    relatively quick and painless: navigate to [https://gpt4all.io/](https://gpt4all.io/),
    download the appropriate installer for your computer, and follow the installation
    instructions. Once you have installed the application, you will receive instructions
    to download a model, as shown in figure 9.3.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F03_Crocker2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 GPT-4All requires that you download models to be run.
  prefs: []
  type: TYPE_NORMAL
- en: I downloaded and used Mistral OpenOrca, a high-performance parallel and distributed
    programming framework designed to simplify the development of large-scale, data-intensive
    applications on high-performance computing clusters or cloud environments. It’s
    particularly well suited for handling big data processing tasks, scientific simulations,
    machine learning algorithms, and other compute-intensive workloads that require
    efficient resource utilization and scalability across multiple nodes. Mistral
    OpenOrca provides a set of tools and libraries to manage job scheduling, communication,
    fault tolerance, and load balancing in distributed environments, making it an
    ideal choice for developers working on complex projects requiring high performance
    and parallelism. Both the GPT-4All introduction and the majority of this paragraph
    were generated by Mistral OpenOrca.
  prefs: []
  type: TYPE_NORMAL
- en: If you click the Downloads button from Settings, you will see the downloaded
    model, as shown in figure 9.4\. You will also find the complete chat history in
    the menu, as shown in figure 9.5.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F04_Crocker2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 The downloaded models in GPT-4All
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F05_Crocker2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 The list of chats you’ve had with the selected GPT-4All model
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get one thing out of the way by using the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Is an active internet connection required to generate output? |'
  prefs: []
  type: TYPE_TB
- en: Listing 9.11 GPT-4All’s assurance that internet is not required
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can move on to compare GPT4-All/Mistral OpenOrca (from here on simply
    referred to as GPT-4All) with the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/logo-NC.png)'
  prefs: []
  type: TYPE_IMG
- en: '| As a mathematician, you are attempting to compute the standard deviation
    of a list in pure Python. Please show me the code that you would use. |'
  prefs: []
  type: TYPE_TB
- en: Listing 9.12 GPT-4All’s attempt to calculate standard deviation
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: A couple of things to note. First, GPT-4All generates the code for the population
    standard deviation. Second, the text and code are generated very quickly (on my
    computer, four to five times more quickly than by Ollama). Third, the code is
    exactly right! You are encouraged to download different models, ask what each
    is good at, and compare the results.
  prefs: []
  type: TYPE_NORMAL
- en: You may wonder when you would want to use Llama 2 versus GPT-4All versus ChatGPT.
    Excellent question! Take a look at figure 9.6\. Llama 2 is a great, general model.
    It excels at summarizing large bodies of text and writing contextually appropriate
    passages of text. GPT-4All’s use cases are as diverse as the available models.
    For example, Mistral OpenOrca is ideal when you need a multilingual model that
    can handle various languages effectively. ChatGPT is the best option if your primary
    goal is to have natural-language conversations with the AI model and receive the
    most accurate responses based on input (which really should be what you want).
    An obvious limitation of ChatGPT is that it requires a persistent internet connection.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/CH09_F06_Crocker2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 A comparison of the models that we used in this chapter
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Local LLMs require significant computational resources and costly hardware for
    optimal performance; however, alternatives like Llama 2 run on modest commodity
    hardware with varying parameter options. These models can produce output that
    is generally high quality, but not quite of the quality of the responses of managed
    LLMs like ChatGPT (at least at the time of writing).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both population and sample standard deviations measure variability in datasets.
    They differ in terms of the entire population being considered versus a smaller
    subset or sample; this means the former provides an exact measurement for the
    whole group, whereas the latter is an estimate based on a portion of it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Llama 2 excels at diverse text handling, such as generating summaries or writing
    coherent passages of text and code, GPT-4All offers various use cases, including
    multilingual support; and ChatGPT shines in natural language conversations with
    accurate responses (but it requires an internet connection).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition to offline availability, there are various situations in which
    using an offline version of an LLM such as Llama 2 or GPT-4All makes sense:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Privacy and security concerns—*Offline models eliminate the need to transmit
    sensitive data over the internet, reducing privacy risks and potential cybersecurity
    threats.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Cost savings—*Running a local model on your own hardware may reduce cloud
    computing costs associated with using an online service like ChatGPT or OpenAI
    API.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
