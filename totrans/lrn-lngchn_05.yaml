- en: Chapter 5\. Cognitive Architectures with LangGraph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we’ve looked at the most common features of LLM applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompting techniques in the [Preface](preface01.html#pr01_preface_1736545679069216)
    and [Chapter 1](ch01.html#ch01_llm_fundamentals_with_langchain_1736545659776004)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG in Chapters [2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)
    and [3](ch03.html#ch03_rag_part_ii_chatting_with_your_data_1736545666793580)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory in [Chapter 4](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next question should be: How do we assemble these pieces into a coherent
    application that achieves the goal we set out to solve? To draw a parallel with
    the world of bricks and mortar, a swimming pool and a one-story house are built
    of the same materials, but obviously serve very different purposes. What makes
    them uniquely suited to their different purposes is the plan for how those materials
    are combined—that is, their architecture. The same is true when building LLM applications.
    The most important decisions you have to make are how to assemble the different
    components you have at your disposal (such as RAG, prompting techniques, memory)
    into something that achieves your purpose.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we look at specific architectures, let’s walk through an example. Any
    LLM application you might build will start from a purpose: what the app is designed
    to do. Let’s say you want to build an email assistant—an LLM application that
    reads your emails before you do and aims to reduce the amount of emails you need
    to look at. The application might do this by archiving a few uninteresting ones,
    directly replying to some, and marking others as deserving of your attention later.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You probably also would want the app to be bound by some constraints in its
    action. Listing those constraints helps tremendously, as they will help inform
    the search for the right architecture. [Chapter 8](ch08.html#ch08_patterns_to_make_the_most_of_llms_1736545674143600)
    covers these constraints in more detail and how to work with them. For this hypothetical
    email assistant, let’s say we’d like it to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Minimize the number of times it interrupts you (after all, the whole point is
    to save time).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid having your email correspondents receive a reply that you’d never have
    sent yourself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This hints at the key trade-off often faced when building LLM apps: the trade-off
    between *agency* (or the capacity to act autonomously) and *reliability* (or the
    degree to which you can trust its outputs). Intuitively, the email assistant will
    be more useful if it takes more actions without your involvement, but if you take
    it too far, it will inevitably send emails you wish it hadn’t.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to describe the degree of autonomy of an LLM application is to evaluate
    how much of the behavior of the application is determined by an LLM (versus code):'
  prefs: []
  type: TYPE_NORMAL
- en: Have an LLM decide the output of a step (for instance, write a draft reply to
    an email).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Have an LLM decide the next step to take (for instance, for a new email, decide
    between the three actions it can take on an email: archive, reply, or mark for
    review).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have an LLM decide what steps are available to take (for instance, have the
    LLM write code that executes a dynamic action you didn’t preprogram into the application).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can classify a number of popular *recipes* for building LLM applications
    based on where they fall in this spectrum of autonomy, that is, which of the three
    tasks just mentioned are handled by an LLM and which remain in the hands of the
    developer or user. These recipes can be called *cognitive architectures*. In the
    artificial intelligence field, the term *cognitive architecture* has long been
    used to denote models of human reasoning (and their implementations in computers).
    An LLM cognitive architecture (the term was first applied to LLMs, to our knowledge,
    in a paper^([1](ch05.html#id699))) can be defined as a recipe for the steps to
    be taken by an LLM application (see [Figure 5-1](#ch05_figure_1_1736545670023944)).
    A *step* is, for instance, retrieval of relevant documents (RAG), or calling an
    LLM with a chain-of-thought prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer application  Description automatically generated](assets/lelc_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Cognitive architectures for LLM applications
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now let’s look at each of the major architectures, or recipes, that you can
    use when building your application (as shown in [Figure 5-1](#ch05_figure_1_1736545670023944)):'
  prefs: []
  type: TYPE_NORMAL
- en: '0: Code'
  prefs: []
  type: TYPE_NORMAL
- en: This is not an LLM cognitive architecture (hence we numbered it **0**), as it
    doesn’t use LLMs at all. You can think of this as regular software you’re used
    to writing. The first interesting architecture (for this book, at any rate) is
    actually the next one.
  prefs: []
  type: TYPE_NORMAL
- en: '1: LLM call'
  prefs: []
  type: TYPE_NORMAL
- en: This is the majority of the examples we’ve seen in the book so far, with one
    LLM call only. This is useful mostly when it’s part of a larger application that
    makes use of an LLM for achieving a specific task, such as translating or summarizing
    a piece of text.
  prefs: []
  type: TYPE_NORMAL
- en: '2: Chain'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next level up, so to speak, comes with the use of multiple LLM calls in
    a predefined sequence. For instance, a text-to-SQL application (which receives
    as input from the user a natural language description of some calculation to make
    over a database) could make use of two LLM calls in sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: One LLM call to generate a SQL query, from the natural language query, provided
    by the user, and a description of the database contents, provided by the developer.
  prefs: []
  type: TYPE_NORMAL
- en: And another LLM call to write an explanation of the query appropriate for a
    nontechnical user, given the query generated in the previous call. This one could
    then be used to enable the user to check if the generated query matches his request.
  prefs: []
  type: TYPE_NORMAL
- en: '3: Router'
  prefs: []
  type: TYPE_NORMAL
- en: 'This next step comes from using the LLM to define the sequence of steps to
    take. That is, whereas the chain architecture always executes a static sequence
    of steps (however many) determined by the developer, the router architecture is
    characterized by using an LLM to choose between certain predefined steps. An example
    would be a RAG application with multiple indexes of documents from different domains,
    with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: An LLM call to pick which of the available indexes to use, given the user-supplied
    query and the developer-supplied description of the indexes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A retrieval step that queries the chosen index for the most relevant documents
    for the user query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another LLM call to generate an answer, given the user-supplied query and the
    list of relevant documents fetched from the index.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That’s as far as we’ll go in this chapter. We will talk about each of these
    architectures in turn. The next chapters discuss the agentic architectures, which
    make even more use of LLMs. But first let’s talk about some better tooling to
    help us on this journey.
  prefs: []
  type: TYPE_NORMAL
- en: 'Architecture #1: LLM Call'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an example of the LLM call architecture, we’ll return to the chatbot we created
    in [Chapter 4](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431).
    This chatbot will respond directly to user messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by creating a `StateGraph`, to which we’ll add a node to represent the
    LLM call:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also draw a visual representation of the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The graph we just made looks like [Figure 5-2](#ch05_figure_2_1736545670023979).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a chatbot  Description automatically generated](assets/lelc_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. The LLM call architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can run it with the familiar `stream()` method you’ve seen in earlier chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5] const input = {messages: [new HumanMessage(''hi!)]} for await (const
    chunk of await graph.stream(input)) {   console.log(chunk) } [PRE6] { "chatbot":
    { "messages": [AIMessage("How can I help you?")] } } [PRE7]`  [PRE8] from typing
    import Annotated, TypedDict  from langchain_core.messages import HumanMessage,
    SystemMessage from langchain_openai import ChatOpenAI  from langgraph.graph import
    END, START, StateGraph from langgraph.graph.message import add_messages  # useful
    to generate SQL query model_low_temp = ChatOpenAI(temperature=0.1) # useful to
    generate natural language outputs model_high_temp = ChatOpenAI(temperature=0.7)  class
    State(TypedDict):     # to track conversation history     messages: Annotated[list,
    add_messages]     # input     user_query: str     # output     sql_query: str     sql_explanation:
    str  class Input(TypedDict):     user_query: str  class Output(TypedDict):     sql_query:
    str     sql_explanation: str  generate_prompt = SystemMessage(     """You are
    a helpful data analyst who generates SQL queries for users based   on their questions."""
    )  def generate_sql(state: State) -> State:     user_message = HumanMessage(state["user_query"])     messages
    = [generate_prompt, *state["messages"], user_message]     res = model_low_temp.invoke(messages)     return
    {         "sql_query": res.content,         # update conversation history         "messages":
    [user_message, res],     }  explain_prompt = SystemMessage(     "You are a helpful
    data analyst who explains SQL queries to users." )  def explain_sql(state: State)
    -> State:     messages = [         explain_prompt,         # contains user''s
    query and SQL query from prev step         *state["messages"],     ]     res =
    model_high_temp.invoke(messages)     return {         "sql_explanation": res.content,         #
    update conversation history         "messages": res,     }  builder = StateGraph(State,
    input=Input, output=Output) builder.add_node("generate_sql", generate_sql) builder.add_node("explain_sql",
    explain_sql) builder.add_edge(START, "generate_sql") builder.add_edge("generate_sql",
    "explain_sql") builder.add_edge("explain_sql", END)  graph = builder.compile()
    [PRE9] import {   HumanMessage,   SystemMessage } from "@langchain/core/messages";
    import { ChatOpenAI } from "@langchain/openai"; import {   StateGraph,   Annotation,   messagesStateReducer,   START,   END,
    } from "@langchain/langgraph";  // useful to generate SQL query const modelLowTemp
    = new ChatOpenAI({ temperature: 0.1 }); // useful to generate natural language
    outputs const modelHighTemp = new ChatOpenAI({ temperature: 0.7 });  const annotation
    = Annotation.Root({   messages: Annotation({ reducer: messagesStateReducer, default:
    () => [] }),   user_query: Annotation(),   sql_query: Annotation(),   sql_explanation:
    Annotation(), });  const generatePrompt = new SystemMessage(   `You are a helpful
    data analyst who generates SQL queries for users based on   their questions.`
    );  async function generateSql(state) {   const userMessage = new HumanMessage(state.user_query);   const
    messages = [generatePrompt, ...state.messages, userMessage];   const res = await
    modelLowTemp.invoke(messages);   return {     sql_query: res.content as string,     //
    update conversation history     messages: [userMessage, res],   }; }  const explainPrompt
    = new SystemMessage(   "You are a helpful data analyst who explains SQL queries
    to users." );  async function explainSql(state) {   const messages = [explainPrompt,
    ...state.messages];   const res = await modelHighTemp.invoke(messages);   return
    {     sql_explanation: res.content as string,     // update conversation history     messages:
    res,   }; }  const builder = new StateGraph(annotation)   .addNode("generate_sql",
    generateSql)   .addNode("explain_sql", explainSql)   .addEdge(START, "generate_sql")   .addEdge("generate_sql",
    "explain_sql")   .addEdge("explain_sql", END);  const graph = builder.compile();
    [PRE10] graph.invoke({   "user_query": "What is the total sales for each product?"
    }) [PRE11] await graph.invoke({   user_query: "What is the total sales for each
    product?" }) [PRE12] {   "sql_query": "SELECT product_name, SUM(sales_amount)
    AS total_sales\nFROM        sales\nGROUP BY product_name;",   "sql_explanation":
    "This query will retrieve the total sales for each product        by summing up
    the sales_amount column for each product and grouping the       results by product_name.",
    } [PRE13] from typing import Annotated, Literal, TypedDict  from langchain_core.documents
    import Document from langchain_core.messages import HumanMessage, SystemMessage
    from langchain_core.vectorstores.in_memory import InMemoryVectorStore from langchain_openai
    import ChatOpenAI, OpenAIEmbeddings  from langgraph.graph import END, START, StateGraph
    from langgraph.graph.message import add_messages  embeddings = OpenAIEmbeddings()
    # useful to generate SQL query model_low_temp = ChatOpenAI(temperature=0.1) #
    useful to generate natural language outputs model_high_temp = ChatOpenAI(temperature=0.7)  class
    State(TypedDict):     # to track conversation history     messages: Annotated[list,
    add_messages]     # input     user_query: str     # output     domain: Literal["records",
    "insurance"]     documents: list[Document]     answer: str  class Input(TypedDict):     user_query:
    str  class Output(TypedDict):     documents: list[Document]     answer: str  #
    refer to Chapter 2 on how to fill a vector store with documents medical_records_store
    = InMemoryVectorStore.from_documents([], embeddings) medical_records_retriever
    = medical_records_store.as_retriever()  insurance_faqs_store = InMemoryVectorStore.from_documents([],
    embeddings) insurance_faqs_retriever = insurance_faqs_store.as_retriever()  router_prompt
    = SystemMessage(     """You need to decide which domain to route the user query
    to. You have two   domains to choose from:  - records: contains medical records
    of the patient, such as   diagnosis, treatment, and prescriptions.  - insurance:
    contains frequently asked questions about insurance   policies, claims, and coverage.  Output
    only the domain name.""" )  def router_node(state: State) -> State:     user_message
    = HumanMessage(state["user_query"])     messages = [router_prompt, *state["messages"],
    user_message]     res = model_low_temp.invoke(messages)     return {         "domain":
    res.content,         # update conversation history         "messages": [user_message,
    res],     }  def pick_retriever(     state: State, ) -> Literal["retrieve_medical_records",
    "retrieve_insurance_faqs"]:     if state["domain"] == "records":         return
    "retrieve_medical_records"     else:         return "retrieve_insurance_faqs"  def
    retrieve_medical_records(state: State) -> State:     documents = medical_records_retriever.invoke(state["user_query"])     return
    {         "documents": documents,     }  def retrieve_insurance_faqs(state: State)
    -> State:     documents = insurance_faqs_retriever.invoke(state["user_query"])     return
    {         "documents": documents,     }  medical_records_prompt = SystemMessage(     """You
    are a helpful medical chatbot who answers questions based on the   patient''s
    medical records, such as diagnosis, treatment, and   prescriptions.""" )  insurance_faqs_prompt
    = SystemMessage(     """You are a helpful medical insurance chatbot who answers
    frequently asked   questions about insurance policies, claims, and coverage."""
    )  def generate_answer(state: State) -> State:     if state["domain"] == "records":         prompt
    = medical_records_prompt     else:         prompt = insurance_faqs_prompt     messages
    = [         prompt,         *state["messages"],         HumanMessage(f"Documents:
    {state["documents"]}"),     ]     res = model_high_temp.invoke(messages)     return
    {         "answer": res.content,         # update conversation history         "messages":
    res,     }  builder = StateGraph(State, input=Input, output=Output) builder.add_node("router",
    router_node) builder.add_node("retrieve_medical_records", retrieve_medical_records)
    builder.add_node("retrieve_insurance_faqs", retrieve_insurance_faqs) builder.add_node("generate_answer",
    generate_answer) builder.add_edge(START, "router") builder.add_conditional_edges("router",
    pick_retriever) builder.add_edge("retrieve_medical_records", "generate_answer")
    builder.add_edge("retrieve_insurance_faqs", "generate_answer") builder.add_edge("generate_answer",
    END)  graph = builder.compile() [PRE14] import {   HumanMessage,   SystemMessage
    } from "@langchain/core/messages"; import {   ChatOpenAI,   OpenAIEmbeddings }
    from "@langchain/openai"; import {   MemoryVectorStore } from "langchain/vectorstores/memory";
    import {   DocumentInterface } from "@langchain/core/documents"; import {   StateGraph,   Annotation,   messagesStateReducer,   START,   END,
    } from "@langchain/langgraph";  const embeddings = new OpenAIEmbeddings(); //
    useful to generate SQL query const modelLowTemp = new ChatOpenAI({ temperature:
    0.1 }); // useful to generate natural language outputs const modelHighTemp = new
    ChatOpenAI({ temperature: 0.7 });  const annotation = Annotation.Root({   messages:
    Annotation({ reducer: messagesStateReducer, default: () => [] }),   user_query:
    Annotation(),   domain: Annotation(),   documents: Annotation(),   answer: Annotation(),
    });  // refer to Chapter 2 on how to fill a vector store with documents const
    medicalRecordsStore = await MemoryVectorStore.fromDocuments(   [],   embeddings
    ); const medicalRecordsRetriever = medicalRecordsStore.asRetriever();  const insuranceFaqsStore
    = await MemoryVectorStore.fromDocuments(   [],   embeddings ); const insuranceFaqsRetriever
    = insuranceFaqsStore.asRetriever();  const routerPrompt = new SystemMessage(   `You
    need to decide which domain to route the user query to. You have two   domains
    to choose from:  - records: contains medical records of the patient, such as diagnosis,   treatment,
    and prescriptions.  - insurance: contains frequently asked questions about insurance   policies,
    claims, and coverage.  Output only the domain name.` );  async function routerNode(state)
    {   const userMessage = new HumanMessage(state.user_query);   const messages =
    [routerPrompt, ...state.messages, userMessage];   const res = await modelLowTemp.invoke(messages);   return
    {     domain: res.content as "records" | "insurance",     // update conversation
    history     messages: [userMessage, res],   }; }  function pickRetriever(state)
    {   if (state.domain === "records") {     return "retrieve_medical_records";   }
    else {     return "retrieve_insurance_faqs";   } }  async function retrieveMedicalRecords(state)
    {   const documents = await medicalRecordsRetriever.invoke(state.user_query);   return
    {     documents,   }; }  async function retrieveInsuranceFaqs(state) {   const
    documents = await insuranceFaqsRetriever.invoke(state.user_query);   return {     documents,   };
    }  const medicalRecordsPrompt = new SystemMessage(   `You are a helpful medical
    chatbot who answers questions based on the   patient''s medical records, such
    as diagnosis, treatment, and   prescriptions.` );  const insuranceFaqsPrompt =
    new SystemMessage(   `You are a helpful medical insurance chatbot who answers
    frequently asked   questions about insurance policies, claims, and coverage.`
    );  async function generateAnswer(state) {   const prompt =     state.domain ===
    "records" ? medicalRecordsPrompt : insuranceFaqsPrompt;   const messages = [     prompt,     ...state.messages,     new
    HumanMessage(`Documents: ${state.documents}`),   ];   const res = await modelHighTemp.invoke(messages);   return
    {     answer: res.content as string,     // update conversation history     messages:
    res,   }; }  const builder = new StateGraph(annotation)   .addNode("router", routerNode)   .addNode("retrieve_medical_records",
    retrieveMedicalRecords)   .addNode("retrieve_insurance_faqs", retrieveInsuranceFaqs)   .addNode("generate_answer",
    generateAnswer)   .addEdge(START, "router")   .addConditionalEdges("router", pickRetriever)   .addEdge("retrieve_medical_records",
    "generate_answer")   .addEdge("retrieve_insurance_faqs", "generate_answer")   .addEdge("generate_answer",
    END);  const graph = builder.compile(); [PRE15] input = {     "user_query": "Am
    I covered for COVID-19 treatment?" } for c in graph.stream(input):     print(c)
    [PRE16] const input = {   user_query: "Am I covered for COVID-19 treatment?" }
    for await (const chunk of await graph.stream(input)) { console.log(chunk) } [PRE17]
    {     "router": {         "messages": [             HumanMessage(content="Am I
    covered for COVID-19 treatment?"),             AIMessage(content="insurance"),         ],         "domain":
    "insurance",     } } {     "retrieve_insurance_faqs": {         "documents": [...]     }
    } {     "generate_answer": {         "messages": AIMessage(             content="...",         ),         "answer":
    "...",     } } [PRE18]`'
  prefs: []
  type: TYPE_NORMAL
