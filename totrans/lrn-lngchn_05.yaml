- en: Chapter 5\. Cognitive Architectures with LangGraph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we’ve looked at the most common features of LLM applications:'
  prefs: []
  type: TYPE_NORMAL
- en: Prompting techniques in the [Preface](preface01.html#pr01_preface_1736545679069216)
    and [Chapter 1](ch01.html#ch01_llm_fundamentals_with_langchain_1736545659776004)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG in Chapters [2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)
    and [3](ch03.html#ch03_rag_part_ii_chatting_with_your_data_1736545666793580)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory in [Chapter 4](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next question should be: How do we assemble these pieces into a coherent
    application that achieves the goal we set out to solve? To draw a parallel with
    the world of bricks and mortar, a swimming pool and a one-story house are built
    of the same materials, but obviously serve very different purposes. What makes
    them uniquely suited to their different purposes is the plan for how those materials
    are combined—that is, their architecture. The same is true when building LLM applications.
    The most important decisions you have to make are how to assemble the different
    components you have at your disposal (such as RAG, prompting techniques, memory)
    into something that achieves your purpose.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we look at specific architectures, let’s walk through an example. Any
    LLM application you might build will start from a purpose: what the app is designed
    to do. Let’s say you want to build an email assistant—an LLM application that
    reads your emails before you do and aims to reduce the amount of emails you need
    to look at. The application might do this by archiving a few uninteresting ones,
    directly replying to some, and marking others as deserving of your attention later.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You probably also would want the app to be bound by some constraints in its
    action. Listing those constraints helps tremendously, as they will help inform
    the search for the right architecture. [Chapter 8](ch08.html#ch08_patterns_to_make_the_most_of_llms_1736545674143600)
    covers these constraints in more detail and how to work with them. For this hypothetical
    email assistant, let’s say we’d like it to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Minimize the number of times it interrupts you (after all, the whole point is
    to save time).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid having your email correspondents receive a reply that you’d never have
    sent yourself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This hints at the key trade-off often faced when building LLM apps: the trade-off
    between *agency* (or the capacity to act autonomously) and *reliability* (or the
    degree to which you can trust its outputs). Intuitively, the email assistant will
    be more useful if it takes more actions without your involvement, but if you take
    it too far, it will inevitably send emails you wish it hadn’t.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to describe the degree of autonomy of an LLM application is to evaluate
    how much of the behavior of the application is determined by an LLM (versus code):'
  prefs: []
  type: TYPE_NORMAL
- en: Have an LLM decide the output of a step (for instance, write a draft reply to
    an email).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Have an LLM decide the next step to take (for instance, for a new email, decide
    between the three actions it can take on an email: archive, reply, or mark for
    review).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have an LLM decide what steps are available to take (for instance, have the
    LLM write code that executes a dynamic action you didn’t preprogram into the application).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can classify a number of popular *recipes* for building LLM applications
    based on where they fall in this spectrum of autonomy, that is, which of the three
    tasks just mentioned are handled by an LLM and which remain in the hands of the
    developer or user. These recipes can be called *cognitive architectures*. In the
    artificial intelligence field, the term *cognitive architecture* has long been
    used to denote models of human reasoning (and their implementations in computers).
    An LLM cognitive architecture (the term was first applied to LLMs, to our knowledge,
    in a paper^([1](ch05.html#id699))) can be defined as a recipe for the steps to
    be taken by an LLM application (see [Figure 5-1](#ch05_figure_1_1736545670023944)).
    A *step* is, for instance, retrieval of relevant documents (RAG), or calling an
    LLM with a chain-of-thought prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer application  Description automatically generated](assets/lelc_0501.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-1\. Cognitive architectures for LLM applications
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now let’s look at each of the major architectures, or recipes, that you can
    use when building your application (as shown in [Figure 5-1](#ch05_figure_1_1736545670023944)):'
  prefs: []
  type: TYPE_NORMAL
- en: '0: Code'
  prefs: []
  type: TYPE_NORMAL
- en: This is not an LLM cognitive architecture (hence we numbered it **0**), as it
    doesn’t use LLMs at all. You can think of this as regular software you’re used
    to writing. The first interesting architecture (for this book, at any rate) is
    actually the next one.
  prefs: []
  type: TYPE_NORMAL
- en: '1: LLM call'
  prefs: []
  type: TYPE_NORMAL
- en: This is the majority of the examples we’ve seen in the book so far, with one
    LLM call only. This is useful mostly when it’s part of a larger application that
    makes use of an LLM for achieving a specific task, such as translating or summarizing
    a piece of text.
  prefs: []
  type: TYPE_NORMAL
- en: '2: Chain'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next level up, so to speak, comes with the use of multiple LLM calls in
    a predefined sequence. For instance, a text-to-SQL application (which receives
    as input from the user a natural language description of some calculation to make
    over a database) could make use of two LLM calls in sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: One LLM call to generate a SQL query, from the natural language query, provided
    by the user, and a description of the database contents, provided by the developer.
  prefs: []
  type: TYPE_NORMAL
- en: And another LLM call to write an explanation of the query appropriate for a
    nontechnical user, given the query generated in the previous call. This one could
    then be used to enable the user to check if the generated query matches his request.
  prefs: []
  type: TYPE_NORMAL
- en: '3: Router'
  prefs: []
  type: TYPE_NORMAL
- en: 'This next step comes from using the LLM to define the sequence of steps to
    take. That is, whereas the chain architecture always executes a static sequence
    of steps (however many) determined by the developer, the router architecture is
    characterized by using an LLM to choose between certain predefined steps. An example
    would be a RAG application with multiple indexes of documents from different domains,
    with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: An LLM call to pick which of the available indexes to use, given the user-supplied
    query and the developer-supplied description of the indexes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A retrieval step that queries the chosen index for the most relevant documents
    for the user query.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another LLM call to generate an answer, given the user-supplied query and the
    list of relevant documents fetched from the index.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That’s as far as we’ll go in this chapter. We will talk about each of these
    architectures in turn. The next chapters discuss the agentic architectures, which
    make even more use of LLMs. But first let’s talk about some better tooling to
    help us on this journey.
  prefs: []
  type: TYPE_NORMAL
- en: 'Architecture #1: LLM Call'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an example of the LLM call architecture, we’ll return to the chatbot we created
    in [Chapter 4](ch04.html#ch04_using_langgraph_to_add_memory_to_your_chatbot_1736545668266431).
    This chatbot will respond directly to user messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by creating a `StateGraph`, to which we’ll add a node to represent the
    LLM call:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also draw a visual representation of the graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The graph we just made looks like [Figure 5-2](#ch05_figure_2_1736545670023979).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a chatbot  Description automatically generated](assets/lelc_0502.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-2\. The LLM call architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'You can run it with the familiar `stream()` method you’ve seen in earlier chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Notice how the input to the graph was in the same shape as the `State` object
    we defined earlier; that is, we sent in a list of messages in the `messages` key
    of a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the simplest possible architecture for using an LLM, which is not to
    say that it should never be used. Here are some examples of where you might see
    it in action in popular products, among many others:'
  prefs: []
  type: TYPE_NORMAL
- en: AI-powered features such as summarize and translate (such as you can find in
    Notion, a popular writing software) can be powered by a single LLM call.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple SQL query generation can be powered by a single LLM call, depending on
    the UX and target user the developer has in mind.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Architecture #2: Chain'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This next architecture extends on all that by using multiple LLM calls, in a
    predefined sequence (that is, different invocations of the application do the
    same sequence of LLM calls, albeit with different inputs and results).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take as an example a text-to-SQL application, which receives as input
    from the user a natural language description of some calculation to make over
    a database. We mentioned earlier that this could be achieved with a single LLM
    call, to generate a SQL query, but we can create a more sophisticated application
    by making use of multiple LLM calls in sequence. Some authors call this architecture
    *flow engineering*.^([2](ch05.html#id709))
  prefs: []
  type: TYPE_NORMAL
- en: 'First let’s describe the flow in words:'
  prefs: []
  type: TYPE_NORMAL
- en: One LLM call to generate a SQL query from the natural language query, provided
    by the user, and a description of the database contents, provided by the developer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another LLM call to write an explanation of the query appropriate for a nontechnical
    user, given the query generated in the previous call. This one could then be used
    to enable the user to check if the generated query matches his request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You could also extend this even further (but we won’t do that here) with additional
    steps to be taken after the preceding two:'
  prefs: []
  type: TYPE_NORMAL
- en: Executes the query against the database, which returns a two-dimensional table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uses a third LLM call to summarize the query results into a textual answer to
    the original user question.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'And now let’s implement this with LangGraph:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The visual representation of the graph is shown in [Figure 5-3](#ch05_figure_3_1736545670024001).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a program  Description automatically generated](assets/lelc_0503.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-3\. The chain architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here’s an example of inputs and outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '*The output:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: First, the `generate_sql` node is executed, which populates the `sql_query`
    key in the state (which will be part of the final output) and updates the `messages`
    key with the new messages. Then the `explain_sql` node runs, taking the SQL query
    generated in the previous step and populating the `sql_explanation` key in the
    state. At this point, the graph finishes running, and the output is returned to
    the caller.
  prefs: []
  type: TYPE_NORMAL
- en: Note also the use of separate input and output schemas when creating the `StateGraph`.
    This lets you customize which parts of the state are accepted as input from the
    user and which are returned as the final output. The remaining state keys are
    used by the graph nodes internally to keep intermediate state and are made available
    to the user as part of the streaming output produced by `stream()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Architecture #3: Router'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This next architecture moves up the autonomy ladder by assigning to LLMs the
    next of the responsibilities we outlined before: deciding the next step to take.
    That is, whereas the chain architecture always executes a static sequence of steps
    (however many), the router architecture is characterized by using an LLM to choose
    between certain predefined steps.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use the example of a RAG application with access to multiple indexes of
    documents from different domains (refer to [Chapter 2](ch02.html#ch02_rag_part_i_indexing_your_data_1736545662500927)
    for more on indexing). Usually you can extract better performance from LLMs by
    avoiding the inclusion of irrelevant information in the prompt. Therefore, in
    building this application, we should try to pick the right index to use for each
    query and use only that one. The key development in this architecture is to use
    an LLM to make this decision, effectively using an LLM to evaluate each incoming
    query and decide which index it should use for that *particular* query.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Before the advent of LLMs, the usual way of solving this problem would be to
    build a classifier model using ML techniques and a dataset mapping example user
    queries to the right index. This could prove quite challenging, as it requires
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Assembling that dataset by hand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating enough *features* (quantitative attributes) from each user query
    to enable training a classifier for the task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs, given their encoding of human language, can effectively serve as this
    classifier with zero, or very few, examples or additional training.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s describe the flow in words:'
  prefs: []
  type: TYPE_NORMAL
- en: An LLM call to pick which of the available indexes to use, given the user-supplied
    query, and the developer-supplied description of the indexes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A retrieval step that queries the chosen index for the most relevant documents
    for the user query
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another LLM call to generate an answer, given the user-supplied query and the
    list of relevant documents fetched from the index
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'And now let’s implement it with LangGraph:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The visual representation is shown in [Figure 5-4](#ch05_figure_4_1736545670024020).
  prefs: []
  type: TYPE_NORMAL
- en: '![A diagram of a router  Description automatically generated](assets/lelc_0504.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5-4\. The router architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Notice how this is now starting to become more useful, as it shows the two possible
    paths through the graph, through `retrieve_medical_records` or through `retrieve_insurance_faqs`,
    and that for both of those, we first visit the `router` node and finish by visiting
    the `generate_answer` node. These two possible paths were implemented through
    the use of a conditional edge, implemented in the function `pick_retriever`, which
    maps the `domain` picked by the LLM to one of the two nodes mentioned earlier.
    The conditional edge is shown in [Figure 5-4](#ch05_figure_4_1736545670024020)
    as dotted lines from the source node to the destination nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'And now for example inputs and outputs, this time with streaming output:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Python*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '*JavaScript*'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '*The output* (the actual answer is not shown, since it would depend on your
    documents):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This output stream contains the values returned by each node that ran during
    this execution of the graph. Let’s take it one at a time. The top-level key in
    each dictionary is the name of the node, and the value for that key is what that
    node returned:'
  prefs: []
  type: TYPE_NORMAL
- en: The `router` node returned an update to `messages` (this would allow us to easily
    continue this conversation using the memory technique described earlier), and
    the `domain` the LLM picked for this user’s query, in this case `insurance`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then the `pick_retriever` function ran and returned the name of the next node
    to run, based on the `domain` identified by the LLM call in the previous step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then the `retrieve_insurance_faqs` node ran, returning a set of relevant documents
    from that index. This means that on the drawing of the graph seen earlier, we
    took the left path, as decided by the LLM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, the `generate_answer` node ran, which took those documents and the
    original user query and produced an answer to the question, which was written
    to the state (along with a final update to the `messages` key).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter talked about the key trade-off when building LLM applications:
    agency versus oversight. The more autonomous an LLM application is, the more it
    can do—but that raises the need for more mechanisms of control over its actions.
    We moved on to different cognitive architectures that strike different balances
    between agency and oversight.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 6](ch06.html#ch06_agent_architecture_1736545671750341) talks about
    the most powerful of the cognitive architectures we’ve seen so far: the agent
    architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch05.html#id699-marker)) Theodore R. Sumers et al., [“Cognitive Architectures
    for Language Agents”](https://oreil.ly/cuQnT), arXiv, September 5, 2023, updated
    March 15, 2024\.
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch05.html#id709-marker)) Tal Ridnik et al., [“Code Generation with AlphaCodium:
    From Prompt Engineering to Flow Engineering”](https://oreil.ly/0wHX4), arXiv,
    January 16, 2024\.'
  prefs: []
  type: TYPE_NORMAL
