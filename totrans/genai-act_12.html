<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">10</span> </span> <span class="chapter-title-text">Application architecture for generative AI apps</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">An overview of GenAI application architecture and the emerging GenAI app stack</li> 
    <li class="readable-text" id="p3">The different layers that make up the GenAI app stack</li> 
    <li class="readable-text" id="p4">GenAI architecture principles</li> 
    <li class="readable-text" id="p5">The benefits of orchestration frameworks and some of the popular ones</li> 
    <li class="readable-text" id="p6">Model ensemble architectures</li> 
    <li class="readable-text" id="p7">How to create a strategic framework for a cross-functional AI Center of Excellence</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p8"> 
   <p>The enterprise architecture landscape continues to change, moving inexorably toward more self-directed systems—intelligent, self-managing applications that are capable of learning from interactions and adapting in real time. Furthermore, increasing digitization fuels the AI digital transformation. This ongoing progression underscores a transformative era in enterprise technology, poised to redefine the very nature of software development and deployment.</p> 
  </div> 
  <div class="readable-text intended-text" id="p9"> 
   <p>Naturally, this is more of an ideal. However, most enterprises are still very inexperienced with AI-infused applications in general, and generative AI is still very much in its early stages. This chapter will explore how enterprise application architecture standards and best practices must adapt to the emerging generative AI technologies and use cases. The chapter introduces the concept of a <em>GenAI app stack</em> as a conceptual reference architecture for building generative AI applications, and it outlines its main components and how generative AI fits together in the broader enterprise architecture. The GenAI app stack is an evolution of cloud application architecture, with a shift toward data-centric and AI-driven architectures.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>This chapter starts by outlining what the new GenAI app stack entails, covering details of each section and, finally, bringing all the concepts together into working examples that make it real and usable. As you learn about this stack, we’ll consolidate the different aspects of the architecture described in previous chapters. One thing to note is that despite representing a big change, generative AI does not require a completely new architecture but builds on the existing cloud-based distributed architecture. This characteristic allows us to build on existing best practices and architecture principles to incorporate new GenAI-related paradigms. Let’s start by identifying the updates to enterprise application architecture.</p> 
  </div> 
  <div class="readable-text" id="p11"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_170"><span class="num-string">10.1</span> Generative AI: Application architecture </h2> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>Over the last few years, enterprise application architecture has witnessed a significant evolution, going through several transformative stages to meet the escalating demands for business agility, scalability, and intelligence. Initially, enterprises operated on monolithic systems, that is, robust but inflexible structures with tightly interwoven components, which made changes cumbersome and wide-reaching. These systems set the stage for enterprise computing but were not suitable for the rapid evolution of business needs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p13"> 
   <p>The proliferation of cloud computing and cloud-native architectures saw the rise of containerization and orchestration tools, which simplified the deployment and management of applications across diverse environments. Simultaneously, the deluge of data led to data-centric architectures that prioritize data processing and analytics as key drivers for business operations.</p> 
  </div> 
  <div class="readable-text intended-text" id="p14"> 
   <p>The evolution of enterprise application architecture for generative AI can be seen as a shift from traditional software development to data-driven software synthesis. In the traditional paradigm, software engineers write code to implement specific functionalities and logic, using frameworks and libraries that abstract away low-level details. In the generative AI paradigm, software developers provide data and high-level specifications and use large language models (LLMs) to generate code that meets the desired requirements and constraints. The following two key concepts enabled this paradigm shift: Software 2.0 and building on copilots.</p> 
  </div> 
  <div class="readable-text" id="p15"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_171"><span class="num-string">10.1.1</span> Software 2.0</h3> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>Software 2.0 is a term coined by Andrej Karpathy [1] to describe the trend of replacing handcrafted code with learned neural networks. Software 2.0 uses advances in AI, such as natural language processing (NLP), computer vision, and reinforcement learning, to create software components that can learn from data, adapt to new situations, and interact with humans naturally.</p> 
  </div> 
  <div class="readable-text intended-text" id="p17"> 
   <p>Recently, we have transitioned from writing code and managing explicit instructions for a desired goal to a more abstract approach. Developers train models on large datasets instead of writing explicit instructions or rules in a programming language. Software 2.0 also reduces the need for manual debugging, testing, and maintenance, as the neural networks can self-correct and improve over time (see figure 10.1).<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p18">  
   <img alt="figure" src="../Images/CH10_F01_Bahree.png" width="794" height="504"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.1</span> Software 1.0 versus Software 2.0</h5>
  </div> 
  <div class="readable-text" id="p19"> 
   <p>This allows the models to learn the rules or patterns themselves. Algorithms and models are crafted to learn from data, make decisions, and improve over time, effectively writing the software. This paradigm shift has transformed the role of AI from a supportive tool to a fundamental component of system architecture.</p> 
  </div> 
  <div class="readable-text" id="p20"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_172"><span class="num-string">10.1.2</span> The era of copilots</h3> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>Another key concept that facilitated the evolution of enterprise application architecture for generative AI is copilots—a concept originally proposed by Microsoft. Copilots are meant to augment humans and human capabilities and creativity. Using an airplane analogy, if we are humans, we are the pilots; instead of AI being on autopilot where we have no control or say in how it functions, this new AI plays the role of copilots that help us take on cognitive load and some of the drudgery of work. Still, we remain in charge as the pilot.</p> 
  </div> 
  <div class="readable-text intended-text" id="p22"> 
   <p>The Copilot stack is a framework for building AI applications and copilots that use LLMs to understand and generate natural language and code. Copilots are intelligent assistants that can help users with complex cognitive tasks such as writing, coding, searching, or reasoning. Microsoft has developed a range of copilots for different domains and platforms, such as GitHub Copilot, Bing Chat, Dynamics 365 Copilot, and Windows Copilot. You can also build your custom Copilot using the Copilot stack and tools, such as Azure OpenAI, Copilot Studio, and the Teams AI Library. Copilots can also be integrated into existing tools and platforms, such as GitHub, Visual Studio Code, and Jupyter Notebook, to enhance the productivity and creativity of software developers.</p> 
  </div> 
  <div class="readable-text intended-text" id="p23"> 
   <p>Copilots are based on the concept of Software 2.0, where they use LLMs to generate code from natural language descriptions instead of relying on manually written code. However, they should be seen as the GenAI application stack, similar to the LAMP stack for web development. LAMP is an acronym for the stack components: Linux (operating system); Apache (webserver); MySQL (database); and PHP, Perl, or Python (programming language).</p> 
  </div> 
  <div class="readable-text intended-text" id="p24"> 
   <p>Copilots are a useful model for enterprises to follow when designing their generative AI apps enterprise architecture because they offer several advantages (e.g., quicker and simpler development, more creativity and testing, and improved cooperation and learning, enabling enterprises to try out new concepts and opportunities or to create original solutions for difficult problems). Let’s expand on what the Copilot stack is to make it more relevant and real in concrete terms. </p> 
  </div> 
  <div class="readable-text" id="p25"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_173"><span class="num-string">10.2</span> Generative AI: Application stack</h2> 
  </div> 
  <div class="readable-text" id="p26"> 
   <p>Copilots’ architecture comprises several layers and components that work together to provide a seamless and powerful user experience, as outlined in figure 10.2. We will start from the bottom up, examine each layer and component in detail, and find out how they interact.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p27">  
   <img alt="figure" src="../Images/CH10_F02_Bahree.png" width="693" height="630"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.2</span> GenAI application stack</h5>
  </div> 
  <div class="readable-text intended-text" id="p28"> 
   <p>The AI infrastructure layer is the foundational layer that powers everything and hosts the core AI models and computational resources. It encompasses the hardware, software, and services that enable the development and deployment of AI applications and are often optimized for AI workloads. This also includes the massively scalable distributed high-performance computing (HPC), required for training the base foundational models.</p> 
  </div> 
  <div class="readable-text intended-text" id="p29"> 
   <p>The foundational model layer includes the range of supported models, from hosted foundation models to the model you train and want to deploy. The hosted foundational models are large pretrained models, such as LLMs and others (vision and speech models), and the newer small language models (SLMs) that can be used for inference; these models can be closed or open. Some of the models can be further adjusted for specific tasks or domains. These models are hosted and managed within the AI infrastructure layer to ensure high performance and availability. Users can select from various hosted foundation models based on their needs and preferences.<span class="aframe-location"/></p> 
  </div> 
  <div class="readable-text intended-text" id="p30"> 
   <p>The orchestration layer manages the interactions between the various components of the architecture, ensuring seamless operation and coordination. It is responsible for key functions such as task allocation, resource management, and workflow optimization:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p31"> The response filtering component uses the prompt engineering set of components; here, the prompts and responses are analyzed, filtered, and optimized to generate safe outputs. </li> 
   <li class="readable-text" id="p32"> The system prompt can also provide additional information or constraints for the AI model to follow. The user can express a system prompt via a simple syntax, or the system can automatically generate it. </li> 
   <li class="readable-text" id="p33"> Grounding is the implementation of retrieval-augmented generation (RAG), and it refers to the process of contextualizing the responses generated by the AI model. Grounding ensures the outputs are syntactically correct, semantically meaningful, and relevant to the given context or domain. We use plugins to get data ingested from different enterprise systems. </li> 
   <li class="readable-text" id="p34"> The plugin execution layer runs plugins that add more features to the basic AI model. Plugins are separate and reusable parts that can do different things, such as data processing, formatting, validation, or transformation. This is very important for taking in data to make embeddings and employing vector databases and indexes when we use RAG in our solutions. </li> 
  </ul> 
  <div class="readable-text" id="p35"> 
   <p>The UX layer is the interface that allows the users to use Copilot. It is easy to use and has strong tools for working with the AI features underneath. The exact nature of how the UX operates depends on which aspect of the application and workflow the Copilot is plugging into. For example, suppose one uses Copilot as part of Microsoft Office 365. The way the UX works in Word differs from that in PowerPoint and in other applications, such as GitHub Copilot, as we saw earlier.</p> 
  </div> 
  <div class="readable-text intended-text" id="p36"> 
   <p>Finally, all of this is done with AI safety, a main part of responsible AI, ensuring the technology is used ethically and responsibly. AI safety includes different methods and rules, which we will explain later in the book.</p> 
  </div> 
  <div class="readable-text" id="p37"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_174"><span class="num-string">10.2.1</span> Integrating the GenAI stack</h3> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>To integrate the GenAI stack into an enterprise application, a strategic and technical approach is required. It starts with knowing the requirements and the business challenges that can be addressed with GenAI, especially LLMs. This involves connecting specific use cases to the capabilities of these AI technologies, focusing on areas where they can offer substantial value, such as streamlining complex workflows, enhancing data analytics, or easing customer interactions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p39"> 
   <p>Keeping the use cases in mind, the next step is to create an integration architecture that fits the AI stack within the boundaries of the current enterprise system, using a service-oriented architecture (SOA) or a microservices approach for adaptability. Establishing secure, scalable, and maintainable APIs is important to facilitating communication between the application and AI services, which will be the basis for the integration.</p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p>The AI infrastructure configuration is an important stage where the organization’s policies and the data’s sensitivity determine whether to choose on-premises, cloud, or a hybrid method. The infrastructure needs hardware and data storage options to meet the use case’s demands. A robust data pipeline is also essential for efficient model inference, especially when using RAG.</p> 
  </div> 
  <div class="readable-text intended-text" id="p41"> 
   <p>With GenAI, developers can use existing models from cloud AI services or run models on their servers. Developers can build or adjust domain-specific models when custom solutions are required, ensuring they are trained on accurate, relevant datasets and adding continuous learning methods to enhance the model with new data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p42"> 
   <p>To maintain responsible AI (RAI), safety, and adherence to data protection laws, response filtering systems are used to prevent the creation of unsuitable content and compliance. The user experience is based on this UX design, which allows users to interact with the AI stack. The design process is repeated, incorporating user feedback to satisfy the enterprise’s needs efficiently.</p> 
  </div> 
  <div class="readable-text intended-text" id="p43"> 
   <p>The system allows for the inclusion of third-party integrations or custom extensions through a secure plugin architecture, which can run them without affecting the application’s reliability. An orchestration layer handles the interactions between different AI components, ensuring the system can adjust to different demand levels.</p> 
  </div> 
  <div class="readable-text intended-text" id="p44"> 
   <p>Deployment is automated to ensure consistent and reliable updates to the AI stack, and CI/CD pipelines are established to enable ongoing integration and delivery without disrupting existing functionalities. The performance and health of the AI stack are continuously monitored, with comprehensive logging and alert systems to notify of any problems.</p> 
  </div> 
  <div class="readable-text intended-text" id="p45"> 
   <p>Finally, a successful adoption and operation of the AI stack depends on well-documented guidelines and training for developers and end users, ensuring they are fully prepared to use, troubleshoot, and maintain the new system. Each step in this process requires detailed planning, cross-team collaboration, and a deep technical understanding to ensure a smooth and effective integration into the enterprise architecture. Let’s explore this GenAI stack in more detail.</p> 
  </div> 
  <div class="readable-text" id="p46"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_175"><span class="num-string">10.2.2</span> GenAI architecture principles</h3> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>When building mission-critical applications, enterprises focus on creating a robust, scalable, and secure system. Although the traditional architectural principles remain unchanged, key additional architectural aspects for generative AI are outlined in figure 10.3.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p48">  
   <img alt="figure" src="../Images/CH10_F03_Bahree.png" width="624" height="612"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.3</span> Generative AI architecture principles</h5>
  </div> 
  <div class="readable-text" id="p49"> 
   <p>Many GenAI models are accessed via an API, so the model API integration is an architecture principle that helps connect with the GenAI API. The models and APIs have different ways of formatting and sending data, as well as limits and quotas on how many requests they can handle; thus, it can be helpful to create a layer of abstraction that can adjust to changes in each API’s design. This involves handling API requests and responses and managing API limits and quotas. It is also common to have multiple models used in the same application to choose the right model for each situation. Having an abstraction layer can help protect each API’s design from changes.</p> 
  </div> 
  <div class="readable-text intended-text" id="p50"> 
   <p>As a principle, scalability and performance help the application deal with elastic scale and changing loads as they increase and decrease. This involves selecting the appropriate cloud infrastructure, balancing the load, and potentially using asynchronous processing to manage intensive tasks. Moreover, the use of containerization and microservices architecture can help with both scalability and performance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p51"> 
   <p>Hosting LLMs in an enterprise data center is not a trivial task, as it requires careful planning to achieve scalability and performance. You must choose an appropriate LLM architecture, comparing open source and proprietary alternatives that align with the business goals. A streamlined end-to-end pipeline is crucial for smooth operations, using orchestration frameworks for workflow management. The infrastructure should be solid for GPU optimization and simplified infrastructure management. LLMOps should be applied for best practices in deployment, and continuous monitoring for performance tracking should be set up. Scalability should be ensured through load balancing and auto-scaling. The data and models should be secured with encryption and access controls, and industry regulations should be followed. This comprehensive approach ensures that LLMs can serve multiple internal customers efficiently and reliably. Of course, it involves significant and continuous investment in capital expenditure and technical expertise.</p> 
  </div> 
  <div class="readable-text intended-text" id="p52"> 
   <p>Due to the data’s sensitive nature, it is crucial to implement strong data privacy and security measures, which include encrypting data both in transit and at rest, managing access controls, and ensuring compliance with regulations such as GDPR or HIPAA. In addition, it is important to have a data minimization strategy where only necessary data is collected and processed, and security audits and penetration testing should be conducted regularly to identify and address vulnerabilities proactively. Some cloud providers, such as Azure, offer robust enterprise support systems and compliance solutions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p53"> 
   <p>Error handling and monitoring do not constitute a new architecture principle; with distributed systems, if you do not plan for failure, you are planning to fail. Use effective error handling and monitoring to check the GenAI application’s health. This means logging errors, creating alerts for anomalies, and having a plan for handling downtime or API limits, including using automatic recovery strategies, such as fallback mechanisms, to ensure high availability. Distributed tracing is essential for complex, microservice-based architectures to better track problems.</p> 
  </div> 
  <div class="readable-text intended-text" id="p54"> 
   <p>LLMs are evolving cost and currency meanings. LLM usage growth can lead to unexpected expenses. To control costs, optimize API calls and use caching strategies. Have budget alerting and cost forecasting mechanisms to avoid surprises.</p> 
  </div> 
  <div class="readable-text intended-text" id="p55"> 
   <p>The GenAI UX design focuses on how users interact with the GenAI models. This would vary depending on the model type; for a language-based use case using an LLM, the UX design would be quite different from an image-based use case where you would be using Stable Diffusion or DALL-E. This includes designing intuitive interfaces, providing helpful prompts, and ensuring the model’s responses align with user expectations. In some ways, everything should not be a simple chatbot, but it should extend and enhance the experience based on the task and intent.</p> 
  </div> 
  <div class="readable-text intended-text" id="p56"> 
   <p>Consider the ethical, biased, and legal implications of GenAI apps, especially when using LLMs. Mitigate biases and prevent harmful stereotypes. Understand legal consequences in healthcare, finance, or law. Follow relevant laws and industry standards. New regulations are emerging, and chapter 12 will cover more on responsible AI use.</p> 
  </div> 
  <div class="readable-text" id="p57"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_176"><span class="num-string">10.2.3</span> GenAI application architecture: A detailed view</h3> 
  </div> 
  <div class="readable-text" id="p58"> 
   <p>Based on the high-level architecture diagram and going into more detail, figure 10.4 illustrates the overall structure of a GenAI app stack. Although we have already used many of these elements in the previous chapters, this is our first look at it holistically. There are six broad categories forming different components that constitute the GenAI app stack.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p59">  
   <img alt="figure" src="../Images/CH10_F04_Bahree.png" width="924" height="639"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.4</span> GenAI app stack</h5>
  </div> 
  <div class="readable-text" id="p60"> 
   <p>Next, we will examine each layer more closely.</p> 
  </div> 
  <div class="readable-text" id="p61"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Orchestration layer</h4> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>The orchestration layer is the central component that integrates various services and manages data flow and prompts. It is responsible for scheduling tasks, allocating resources, and handling errors resiliently. The prompt management system is a critical part of this layer, utilizing AI technology to develop prompts that elicit the best possible responses from LLMs. This involves A/B testing and machine learning (ML) models to analyze user interactions and optimize prompts for higher engagement and accuracy. Orchestration tools such as Kubernetes can manage containerized microservices and enable component deployment across cloud providers and on-premises environments to improve the system’s robustness and fault tolerance.</p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Grounding layer</h4> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>This layer is the basis of GenAI applications that deal with getting, storing, processing, and delivering data. It must work with different record systems, requiring connectors to handle various data formats and protocols. Data pipelines are the channels that link to the different source systems to take in data for applying RAG and enabling enterprises to use their data. The pipelines can connect to the system of records through APIs natively (where supported) or using different plugins. Data pipelines should be built for high speed and low delay, with the ability to handle batch and stream processing as required. The plugin runtime considers different authentication aspects, data refresh configurations, and so forth. Data preprocessing is important for changing data into a format that LLMs can use. Therefore, this layer includes ML models for creating embeddings and vector databases such as Redis, as we saw earlier in the book, or others such as Cosmos DB, Pinecone, Milvus, Qdrant, and so forth. Using distributed data-processing frameworks such as Apache Spark or Azure Fabric ensures scalability and fault tolerance in data processing.</p> 
  </div> 
  <div class="readable-text" id="p65"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Model layer</h4> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>The model layer needs to support a diverse range of models, from frontier general-purpose LLMs such as GPT-4 to highly specialized SLMs such as Phi-2 [2] and Orca 2 [3]. We will learn more about SLMs and see an example of using Phi-2 as a classifier later in the chapter. As a result, the model layer should provide a consistent interface for accessing different models, regardless of whether they are hosted internally or externally. When considering model hosting, it is essential to scale models to handle varying loads, which may require technologies such as serverless computing to allocate resources dynamically. The model catalogs serve as a registry and repository, simplifying the discovery and management of models. This layer also encompasses the model-as-a-platform concept, which allows developers to extend and customize models, similar to how platforms such as Salesforce enable application customization.</p> 
  </div> 
  <div class="readable-text" id="p67"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Response filtering</h4> 
  </div> 
  <div class="readable-text" id="p68"> 
   <p>This layer is crucial for maintaining trust in GenAI applications by ensuring quality assurance and content moderation. It involves using classifiers and NLP tools to screen the outputs for accuracy, bias, and appropriateness. Responsible AI practices are integrated into this layer, incorporating ethical considerations and ensuring compliance with regulations such as GDPR data privacy law. The caching system within this layer improves performance and enables quick rollback and suitability of outputs. Continuous monitoring and real-time evaluation of outputs ensure the AIQ is maintained throughout the application’s lifecycle. Moreover, this layer also addresses the ethical implications of GenAI technologies, which includes developing frameworks for ethical decision-making, ensuring model transparency, and incorporating fairness and inclusivity into the design of AI systems.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p69"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Additional architecture considerations</h5> 
   </div> 
   <div class="readable-text" id="p70"> 
    <p>While comprehensive, the architecture outlined earlier does not cover the following additional considerations, which are critical for production deployment and understood well by most enterprises:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p71"> <em>Integrations</em>—These applications don’t work alone and must connect with the rest of the enterprise system, which enables the smooth transfer of data and services across internal and external systems. Middleware technologies such as enterprise service buses (ESBs) or API gateways are used to handle communication and data conversion between different systems. </li> 
    <li class="readable-text" id="p72"> <em>Security</em>—Security has always been a concern, and it is the same with GenAI; all data in the GenAI ecosystem must be safeguarded from unauthorized access and breaches, which requires strong authentication and authorization methods, transit and rest encryption, and frequent security audits. </li> 
    <li class="readable-text" id="p73"> <em>Production deployment and scaling</em>—The focus here is on the strategies for deploying GenAI applications across various environments, which includes using container orchestration systems for deployment, auto-scaling services to handle dynamic loads, and infrastructure as code for repeatable and reliable provisioning of resources. </li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p74"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_177"><span class="num-string">10.3</span> Orchestration layer</h2> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>Generative AI applications require an orchestrator layer that acts as the backbone and is crucial for managing complex tasks and workflows. This is a middle-tier and integration layer between the models, enterprise data stores, other components, and applications. It coordinates and manages various components and processes that enable the generation of content by AI models within an enterprise architecture. Ensuring that the workflows involving LLMs are efficient, scalable, and reliable for generating content is essential.</p> 
  </div> 
  <div class="readable-text intended-text" id="p76"> 
   <p>The main duties of an orchestrator include managing workflows and service orchestration, but they can be expanded to include additional responsibilities. An orchestrator consists of several components. Orchestration frameworks simplify the manage-ment and interaction with LLMs by abstracting away the complexities of prompt generation, resource management, and performance monitoring. They provide a high-level interface that enables developers to focus on building their applications without getting bogged down in the technical details of LLM interaction. Table 10.1 outlines the key responsibilities.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p77"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 10.1</span> Orchestrator key responsibilities</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Area 
       </div></th> 
      <th> 
       <div>
         Descriptions 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Workflow management <br/></td> 
      <td>  Orchestrator ensures that the sequence of processes—from data ingestion and processing to AI model inference and response delivery—is executed in an orderly and efficient manner. This includes state management to coordinate dependencies between tasks, error handling, retry mechanisms, and the dynamic allocation of resources based on the task load. <br/></td> 
     </tr> 
     <tr> 
      <td>  Service orchestration <br/></td> 
      <td>  Microservices architecture is typically employed, where each service is responsible for a discrete function in the generative AI process. Service orchestration is about managing these services to scale, communicate, and function seamlessly. In addition, containerization platforms such as Docker and orchestration systems such as Kubernetes deploy, manage, and scale the microservices across various environments. <br/></td> 
     </tr> 
     <tr> 
      <td>  Data flow coordination <br/></td> 
      <td>  Ensure that data flows correctly through the system, from the initial data sources to the model and back to the end user or application. This includes preprocessing inputs, queue management for incoming requests, and routing outputs to the correct destinations. <br/></td> 
     </tr> 
     <tr> 
      <td>  Load balancing and auto-scaling <br/></td> 
      <td>  Load balancers distribute incoming AI inference requests across multiple instances to prevent any single instance from becoming a bottleneck. Auto-scaling adjusts the number of active instances based on the current load, ensuring cost-effective resource use. This also has API management components to manage rate limits and implement back-off strategies for production workloads. <br/></td> 
     </tr> 
     <tr> 
      <td>  Model versioning and rollback <br/></td> 
      <td>  Orchestration includes maintaining different versions of AI models and managing their deployment. It allows for quick rollback to previous versions if a new model exhibits unexpected behavior or poor performance. <br/></td> 
     </tr> 
     <tr> 
      <td>  Managing model context windows <br/></td> 
      <td>  Orchestrator enhances interactions by efficiently managing context windows and token counts. It tracks and dynamically adjusts conversation history within the model’s token limits and maintains coherence in responses, especially in long or complex exchanges. Best practices include efficient context management, handling edge cases, continuous performance monitoring, and incorporating user feedback for ongoing improvements. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p78"> 
   <p>These different components work together to create a strong orchestration system that serves as the foundation for the successful deployment and operation of generative AI technology in the enterprise sector. Such orchestration is necessary for the intricacy and constant changes of AI-powered applications to avoid inefficiencies, mistakes, and system breakdowns.</p> 
  </div> 
  <div class="readable-text" id="p79"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_178"><span class="num-string">10.3.1</span> Benefits of an orchestration framework</h3> 
  </div> 
  <div class="readable-text" id="p80"> 
   <p>Orchestrators are essential for managing the complex systems powering generative AI apps. These systems involve diverse processes that need careful coordination through orchestration tools. Orchestrators simplify workflows and ensure tasks are done in order, with dependencies and error-handling rules taken care of. This results in a reliable and regular operational flow, where steps for preprocessing, computation, and postprocessing are smoothly connected, ensuring data quality and consistent output generation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p81"> 
   <p>Scalability is another area where orchestration is vital. As demand fluctuates, a system that dynamically adjusts resource allocation, especially for production workloads, becomes crucial. An orchestrator can provide this agility using different techniques, such as load balancers to distribute workloads evenly and auto-scaling features to modulate computing power in real-time. This elasticity meets the load requirements and optimizes resource usage, balancing performance and cost efficiency. The orchestrators would need to manage this across different models, as well as the computational and cost profiles of those models.</p> 
  </div> 
  <div class="readable-text intended-text" id="p82"> 
   <p>Orchestrators offer a centralized management and monitoring ability. They constitute frameworks that offer dashboards and tools for monitoring LLM usage, identifying bottlenecks, and troubleshooting problems. This enhances system reliability by monitoring service health, responding to failures, and ensuring minimal downtime. Orchestrators can employ automated recovery processes, such as instance restarts or replacements, allowing for service continuity.</p> 
  </div> 
  <div class="readable-text intended-text" id="p83"> 
   <p>The default deployment model is a pay-as-you-go method for most cloud-based LLM providers. This model is shared with other customers, and incoming requests are queued and processed on a first-come, first-served basis. However, for production workloads that require a better user experience, Azure OpenAI service offers a provisioned throughput units (PTU) feature. This feature allows customers to reserve and deploy units of model processing capacity for prompt processing and generating completions. Each unit’s minimum PTU deployment, increments, and processing capacity vary depending on the model type and version. An orchestrator will manage the different deployment endpoints between regular pay-as-you-go and PTUs to ensure optimum performance and cost-effectiveness.</p> 
  </div> 
  <div class="readable-text intended-text" id="p84"> 
   <p>Orchestrators play a significant role in increasing productivity and streamlining operations, which are achieved in two ways. First, it reduces the need to write repetitive code for common tasks such as prompt construction and output processing, thus increasing developers’ productivity. Second, it automates the deployment and management of services, thus minimizing the possibility of human error. This automated process reduces manual overhead and ensures effective compute resource utilization, streamlining production operations. We will delve deeper into managing operations later in the chapter.</p> 
  </div> 
  <div class="readable-text intended-text" id="p85"> 
   <p>Compliance and governance are essential requirements for any enterprise. An orchestrator can assist in enforcing compliance by determining how data is processed, stored, and used in the workflow, which ensures that the data complies with the enterprise’s data governance policies and privacy regulations. Maintaining trust and legal compliance in enterprise operations is crucial and can be achieved through adherence to data governance policies and privacy regulations.</p> 
  </div> 
  <div class="readable-text" id="p86"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_179"><span class="num-string">10.3.2</span> Orchestration frameworks</h3> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>Many people are familiar with orchestrators and orchestration frameworks. While frameworks such as Kubernetes, Apache Airflow, and MLflow are effective general orchestration tools for software engineering and can support ML operations, they are not designed exclusively for generative AI applications. Orchestrating workflows for generative AI requires a more intimate understanding of the nuances of these complex technologies.</p> 
  </div> 
  <div class="readable-text intended-text" id="p88"> 
   <p>The choice of an orchestration framework for generative AI applications depends on the existing technology stack, the complexity of the workflows, and specific requirements. Table 10.2 outlines orchestration frameworks tailored to the specific needs of generative AI applications. These frameworks can handle traditional computational workflows; manage interactions’ state, context, and coherence; and are designed to suit the unique requirements of generative AI.</p> 
  </div> 
  <div class="browsable-container browsable-table-container framemaker-table-container" id="p89"> 
   <h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 10.2</span> Orchestration frameworks</h5> 
   <table> 
    <thead> 
     <tr> 
      <th> 
       <div>
         Name 
       </div></th> 
      <th> 
       <div>
         Notes 
       </div></th> 
     </tr> 
    </thead> 
    <tbody> 
     <tr> 
      <td>  Semantic Kernel  <br/></td> 
      <td>  Semantic Kernel is an OSS framework from Microsoft that aims to create a unified framework for semantic search and generative AI. It uses pretrained LLMs and graph-based knowledge representations to enable rich and diverse natural language interaction. <br/></td> 
     </tr> 
     <tr> 
      <td>  LangChain  <br/></td> 
      <td>  LangChain is a library that chains language models with external knowledge and capabilities. It facilitates the orchestration of LLMs such as GPT-4 with databases, APIs, and other systems to create more comprehensive AI applications. <br/></td> 
     </tr> 
     <tr> 
      <td>  PromptLayer  <br/></td> 
      <td>  PromptLayer is a platform that simplifies the creation, management, and deployment of prompts for LLMs. Users can visually edit and test prompts, compare models, log requ-ests, and monitor performance. More details can be found at <a href="https://promptlayer.com/">https://promptlayer.com/</a>. <br/></td> 
     </tr> 
     <tr> 
      <td>  Rasa  <br/></td> 
      <td>  Rasa is an enterprise conversational AI platform that lets you create chat- and voice-based AI assistants to manage various conversations for different purposes. In addition to conversation AI, it also offers a generative AI-native method for building assistants, with enterprise features such as analytics, security, observability, testing, knowledge integration, voice connectors, and so forth. More information is available at <a href="https://rasa.com/">https://rasa.com/</a>. <br/></td> 
     </tr> 
     <tr> 
      <td>  YouChat API  <br/></td> 
      <td>  The YOU API is a suite of tools that helps enterprises ground the output of LLMs in the most recent, accurate, and relevant information available. You can use the YOU API to access web search results, news articles, and RAG for LLMs. More details can be found at <a href="https://api.you.com/">https://api.you.com/</a>. <br/></td> 
     </tr> 
     <tr> 
      <td>  Ragna  <br/></td> 
      <td>  Ragna is an open source RAG-based AI orchestration framework that allows you to experiment with different aspects of a RAG model—LLMs, vector databases, tokenization strategies, and embedding models. It also allows you to create custom RAG-based web apps and extensions from different data sources. More details can be found at <a href="https://ragna.chat/">https://ragna.chat/</a>. <br/></td> 
     </tr> 
     <tr> 
      <td>  Llama-Index  <br/></td> 
      <td>  LlamaIndex is a cloud-based orchestration framework that enables you to connect your data to LLMs and generate natural language responses. It can access various LLMs. <br/></td> 
     </tr> 
     <tr> 
      <td>  Hugging Face  <br/></td> 
      <td>  Hugging Face provides a collection of pretrained models for various NLP tasks. It can be used with orchestration tools to manage the lifecycle of generative AI applications. More details can be found at <a href="https://huggingface.co/">https://huggingface.co/</a>. <br/></td> 
     </tr> 
    </tbody> 
   </table> 
  </div> 
  <div class="readable-text" id="p90"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_180"><span class="num-string">10.3.3</span> Managing operations</h3> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>An orchestrator plays a crucial role in enhancing the performance and seamless integration of generative AI models, such as LLMs, within intricate systems and workflows. Its core functionality optimizes operational efficiency and fosters a better user experience through sophisticated control mechanisms.</p> 
  </div> 
  <div class="readable-text intended-text" id="p92"> 
   <p>The orchestrator is crucial in managing the LLM’s integration into complex workflows, such as content creation pipelines. It plans and schedules the LLM’s activation to ensure smooth data collection, preprocessing, and text generation, thus simplifying the entire process from start to finish. This coordination improves the workflow and ensures that the API calls for the generated content are timely and relevant.</p> 
  </div> 
  <div class="readable-text intended-text" id="p93"> 
   <p>The orchestrator’s main role is to balance the load and resources for the LLM’s services. It effectively manages requests to avoid overloading or wasting resources. Furthermore, it can change computational resources by constantly tracking workload and performance metrics. This flexibility ensures the system stays responsive and resources are used efficiently, even when demanding changes.</p> 
  </div> 
  <div class="readable-text intended-text" id="p94"> 
   <p>The orchestrator also supervises API interactions, enforcing rate limits and controlling secure access, while managing any errors or disruptions that may occur. Simultaneously, it handles the essential tasks of data preprocessing and postprocessing. This means cleaning, formatting, and transforming data to ensure it is in the right state for processing by the LLM and then improving the output to meet set quality standards and format requirements.</p> 
  </div> 
  <div class="readable-text intended-text" id="p95"> 
   <p>For workflows requiring sequential processing, the orchestrator ensures that outputs from one phase are accurately fed into the next, maintaining the process integrity. This is complemented by its role in enforcing security and compliance measures, where it filters sensitive information and ensures adherence to legal and ethical standards, in addition to conducting audits for accountability and quality assurance.</p> 
  </div> 
  <div class="readable-text intended-text" id="p96"> 
   <p>For applications such as chatbots or digital assistants, the orchestrator manages user interactions by handling session states and queries, directing them to the LLM or other services as needed, which results in a more engaging and responsive user experience. Moreover, the orchestrator continuously monitors the LLM performance, analyzing response time, accuracy, and throughput to guide optimization efforts. It also manages updates to the LLM, ensuring that transitions to newer versions or configurations are smooth and minimally disruptive to users.</p> 
  </div> 
  <div class="readable-text intended-text" id="p97"> 
   <p>As we can see, an orchestrator can significantly enhance the efficiency, reliability, and scalability of an LLM when integrated into complex systems, providing a layer of management that coordinates between the LLM and other system components.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p98"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Building your own orchestrator framework</h5> 
   </div> 
   <div class="readable-text" id="p99"> 
    <p>Creating your own generative AI orchestrator for an enterprise can be difficult. However, it allows you to customize the framework according to your requirements and increases your understanding of the technology. This process demands extensive technical knowledge and resources. Unfortunately, no universal boilerplate code is available to develop an LLM orchestrator. Before proceeding with this project, consider the following factors:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p101"> <em>Customization</em>—Tailoring the framework to meet your specific application and performance requirements </li> 
    <li class="readable-text" id="p102"> <em>Integration with existing systems</em>—Seamlessly integrating the orchestrator with your existing infrastructure and workflows </li> 
    <li class="readable-text" id="p103"> <em>Control and visibility</em>—Maintaining complete control over the LLM technology and accessing detailed insights into its operation </li> 
    <li class="readable-text" id="p104"> <em>Flexibility and scalability</em>—Designing the framework to be flexible enough to accommodate future changes and scaling to meet growing demands </li> 
   </ul> 
   <div class="readable-text" id="p105"> 
    <p>If you want to create something entirely new, you need to understand generative AI, the different types of LLMs, how to train and fine-tune them, and how to use them for various tasks and domains. Additionally, you should know how to gather, process, and store data and knowledge that can help improve the quality and diversity of the generated outputs.</p> 
   </div> 
   <div class="readable-text" id="p106"> 
    <p>To apply these concepts in real-world scenarios, you must be able to design and implement different generative strategies, such as prompt engineering and RAG. These strategies can help control the behavior and output of the LLMs. You must also ensure that the generative models and workflows are scalable, secure, and reliable. This can be achieved using cloud services, APIs, and UIs. Expertise in distributed systems, ML, and software engineering is also required.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p107"> 
   <p>Some new frameworks used widely nowadays are Semantic Kernel, LangChain, and LlamaIndex. These frameworks enable the use of GenAI models, although they address different aspects. We will explore these in more depth.</p> 
  </div> 
  <div class="readable-text" id="p108"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Semantic kernel</h4> 
  </div> 
  <div class="readable-text" id="p109"> 
   <p>Semantic Kernel (SK) from Microsoft is an SDK that integrates LLMs with languages such as C#, Python, and Java. It simplifies the sometimes-complex process of interfacing LLMs with traditional C#, Python, or Java code. With SK, developers can define semantic functions that encapsulate specific actions their application is capable of, such as database interactions, API calls, or email operations. SK allows these functions to be orchestrated seamlessly across mixed programming language environments.</p> 
  </div> 
  <div class="readable-text intended-text" id="p110"> 
   <p>The real power of SK lies in its AI-driven orchestration capabilities. Instead of meticulously choreographing the LLM interactions by hand, SK lets developers use natural language to state a desired outcome or task. The AI automatically determines how to combine the relevant semantic functions to achieve this goal, which significantly accelerates development and lowers the skill barrier for using LLMs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p111"> 
   <p>SK can benefit enterprises when building LLM applications by simplifying the application process, reducing the cost and complexity of prompt engineering, enabling in-context learning and reinforcement learning, and supporting multimodality and multilanguage scenarios. SK provides a consistent and unified interface for different LLM providers, such as OpenAI, Azure OpenAI, and Hugging Face.</p> 
  </div> 
  <div class="readable-text intended-text" id="p112"> 
   <p>Combining simplified LLM integration with AI-powered orchestration creates a powerful platform for enterprises to use to revolutionize their applications. Furthermore, SK makes it feasible to build highly tailored, intelligent customer support systems, implement more powerful and semantically nuanced search functionality, automate routine workflows, and potentially even aid developers with code generation and refactoring tasks. Additional details on SK can be found on their site at <a href="https://aka.ms/semantic-kernel">https://aka.ms/semantic-kernel</a>.</p> 
  </div> 
  <div class="readable-text intended-text" id="p113"> 
   <p>We can illustrate this using an example. Continuing with the pet theme from the previous chapters, we have some books about dogs, which range from general topics to more specific medical advice. These books are scanned and available as PDFs and contain confidential business data we want to use for a question–answer use case. These PDFs are complex documents that contain text, images, tables, and so forth. Given that we cannot use real-world internal information, these PDFs represent proprietary internal information for an enterprise that requires RAG to handle. Suppose we want to do question–answer use cases with the PDFs we have; let’s see how that’s possible. </p> 
  </div> 
  <div class="readable-text intended-text" id="p114"> 
   <p>The first step is to use SK to install the SDK (or the package), which is not supported via conda and will require pip instead. Also note there are breaking changes with some of the SDKs, and we will want to pin the SK SDK to version 1.2.0. You can install this specific version using <code>pip install semantic-kernel==1.2.0</code>. After installing the SDK, to get started with SK at a high level, we need to follow these steps:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p115"> Create an SK instance, and register the AI services you want to use, such as OpenAI, Azure OpenAI, or Hugging Face. </li> 
   <li class="readable-text" id="p116"> Create semantic functions that are prompts with input parameters. These functions can call your existing code or other semantic functions. </li> 
   <li class="readable-text" id="p117"> Call the semantic functions with the appropriate arguments, and await the results. The results will be the output of the AI model after executing the prompt. </li> 
   <li class="readable-text" id="p118"> Optionally, we can create a planner to orchestrate multiple semantic functions based on the user input. </li> 
  </ul> 
  <div class="readable-text" id="p119"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">SK example</h4> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>Here is an example of implementing this using the SK. As we saw earlier, SK is the core component that enables the processing and understanding of natural language text. It’s a framework that provides a unified interface for various AI services and memory stores. </p> 
  </div> 
  <div class="readable-text intended-text" id="p121"> 
   <p>Our example is a simple question-answering system that uses the OpenAI API to generate embeddings for a collection of PDF documents. Then, we use those embeddings to find documents relevant to a user’s query. In our example, it is used for</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p122"> <em>Creating embeddings</em><em> </em>—SK provides a simple interface for calling the OpenAI service to generate embeddings for the text extracted from PDF documents. As we know, these embeddings are numerical representations of the text that capture its semantic meaning. </li> 
   <li class="readable-text" id="p123"> <em>Storing and retrieving information</em><em> </em>—We use a vector database (Chroma in our example) to store the text and corresponding embeddings. SK calls these persistent data stores “memory” and, depending on the provider, has methods for querying the stored information based on semantic similarity. As we know, this is used to find documents relevant to a user’s query. </li> 
   <li class="readable-text" id="p124"> <em>Text completion</em><em> </em>—We also use SK to register an OpenAI text completion service, which is used to generate completions for a given piece of text. </li> 
  </ul> 
  <div class="readable-text print-book-callout" id="p125"> 
   <p><span class="print-book-callout-head">Note</span>  We need to specifically use Chroma version 0.4.15, as at the moment, there is an incompatibility with version 0.4.16 and higher with SK that hasn’t been fixed. To do this, we can use one of the following commands depending on whether we are using conda or pip: <code>conda install chromadb=0.4.15</code> or <code>pip install chromadb==0.4.15</code>.</p> 
  </div> 
  <div class="readable-text" id="p126"> 
   <p>Listing 10.1 shows this simple application processing a collection of PDF documents, extracting their text, and then using the OpenAI API to generate embeddings for each document. These embeddings are then stored in a vector database, which can be queried to find documents that are semantically similar to a given input. The <code>load_pdfs</code> function reads PDF files from a specified directory. It uses the PyPDF2 library to open each PDF, extract the text from each page, and return a collection of those pages. </p> 
  </div> 
  <div class="browsable-container listing-container" id="p127"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.1</span> Q&amp;A over my PDFs: Extracting text from PDFs</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import asyncio
from PyPDF2 import PdfReader
import semantic_kernel as sk
from semantic_kernel.connectors.ai.open_ai import 
<span class="">↪</span>(AzureChatCompletion,AzureTextEmbedding)
from semantic_kernel.memory.semantic_text_memory
<span class="">↪</span>import SemanticTextMemory
from semantic_kernel.core_plugins.text_memory_plugin 
<span class="">↪</span>import TextMemoryPlugin
from semantic_kernel.connectors.memory.chroma import 
<span class="">↪</span>ChromaMemoryStore


# Load environment variables
AOAI_KEY = os.getenv("AOAI_KEY")
AOAI_ENDPOINT = os.getenv("AOAI_ENDPOINT")
AOAI_MODEL = "gpt-35-turbo"
AOAI_EMBEDDINGS = "text-embedding-ada-002"
API_VERSION = '2023-09-15-preview'

PERSIST_DIR = os.getenv("PERSIST_DIR")
VECTOR_DB = os.getenv("VECTOR_DB")

DOG_BOOKS = "./data/dog_books"
DEBUG = False
VECTOR_DB = "dog_books"
PERSIST_DIR = "./storage"
ALWAYS_CREATE_VECTOR_DB = False


# Load PDFs and extract text
def load_pdfs():
    docs = []
    total_docs = 0
    total_pages = 0
    filenames = [filename for filename in 
      <span class="">↪</span>os.listdir(DOG_BOOKS) if filename.endswith(".pdf")]
    with tqdm(total=len(filenames), desc="Processing PDFs") 
      <span class="">↪</span>as pbar_outer:
        for filename in filenames:
            pdf_path = os.path.join(DOG_BOOKS, filename)
            with open(pdf_path, "rb") as file:
                pdf = PdfReader(file, strict=False)
                j = 0
                total_docs += 1
                with tqdm(total=len(pdf.pages), 
                  <span class="">↪</span>desc="Loading Pages") as pbar_inner:
                    for page in pdf.pages:
                        total_pages += 1
                        j += 1
                        docs.append(page.extract_text())
                        pbar_inner.update()
                pbar_outer.update()
    print(f"Processed {total_docs} PDFs with {total_pages} pages.")
    return docs</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p128"> 
   <p>After we have extracted the text from the pages, we use the <code>populate_db()</code> function to generate embeddings and store them in Chroma, a vector database. This function takes an SK object and goes through all the pages of the PDF. Each page saves the document’s text using the SK’s memory store. When the <code>save_information()</code> function is called, it automatically creates embedding to store in the vector database, as shown in the next listing. If there is already a Chroma vector database, we use that instead of making a new one.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p129"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.2</span> Q&amp;A over my PDFs: Using SK and populating vector database</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"># Populate the DB with the PDFs
async def populate_db(memory: SemanticTextMemory, docs) -&gt; None:
    for i, doc in enumerate(tqdm_asyncio.tqdm(docs, desc="Populating DB")):
        if doc:  #Check if doc is not empty
            try:
                await memory.save_information(VECTOR_DB,id=str(i),text=doc)
            except Exception as e:
                print(f"Failed to save information for doc {i}: {e}")
                continue  # Skip to the next iteration

# Load the vector DB
async def load_vector_db(memory: SemanticTextMemory, 
  <span class="">↪</span>vector_db_name: str) -&gt; None:
    if not ALWAYS_CREATE_VECTOR_DB:
        collections = await memory.get_collections()
        if vector_db_name in collections:
            print(f" Vector DB {vector_db_name} exists in the 
                  <span class="">↪</span>collections. We will reuse this.")
            return

    print(f" Vector DB {vector_db_name} does not exist in the collections.")
    print("Reading the pdfs...")

    pdf_docs = load_pdfs()
    print("Total PDFs loaded: ", len(pdf_docs))
    print("Creating embeddings and vector db of the PDFs...")
    # This may take some time as we call embedding API for each row
    await populate_db(memory, pdf_docs)</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p130"> 
   <p>The program’s entry point is the <code>main()</code> function, as shown in listing 10.3. It sets up the SK with the OpenAI text completion and embedding services, registers a memory store, and loads the vector database. Then, it enters a loop where it prompts the user for a question, queries the memory store for relevant documents, and prints the text of the most relevant document.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p131"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.3</span> Q&amp;A over my PDFs: SK using Chroma</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">async def main():
    # Setup Semantic Kernel
    kernel = sk.Kernel()
    kernel.add_service(AzureChatCompletion(
            service_id="chat_completion",
            deployment_name=AOAI_MODEL,
            endpoint=AOAI_ENDPOINT,
            api_key=AOAI_KEY,
            api_version=API_VERSION))

    kernel.add_service(AzureTextEmbedding(
            service_id="text_embedding",
            deployment_name=AOAI_EMBEDDINGS,
            endpoint=AOAI_ENDPOINT,
            api_key=AOAI_KEY))

    # Specify the type of memory to attach to SK. 
    # Here we will use Chroma as it is easy to run it locally
    # You can specify location of Chroma DB files.
    store = ChromaMemoryStore(persist_directory=PERSIST_DIR)
    memory = SemanticTextMemory(storage=store, 
    <span class="">↪</span>embeddings_generator = kernel.get_service("text_embedding"))
    kernel.add_plugin(TextMemoryPlugin(memory), "TextMemoryPluginACDB")

    await load_vector_db(memory, VECTOR_DB)

    while True:
        prompt = check_prompt(input('Ask a question against 
        <span class="">↪</span>the PDF (type "quit" to exit):'))

        # Query the memory for most relevant match using
        # search_async specifying relevance score and 
        # "limit" of number of closest documents
        result = await memory.search(collection=VECTOR_DB, 
        <span class="">↪</span>limit=3, min_relevance_score=0.7, query=prompt)
        if result:
            print(result[0].text)
        else:
            print("No matches found.")

        print("-" * 80)


if __name__ == "__main__":
    asyncio.run(main())</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p132"> 
   <p>In our example, we use Chroma as the vector database. This is one of the many options available when using SK. We can get more details on the list of supported vector databases at <a href="https://learn.microsoft.com/en-us/semantic-kernel/concepts/vector-store-connectors/out-of-the-box-connectors/">https://learn.microsoft.com/en-us/semantic-kernel/concepts/vector-store-connectors/out-of-the-box-connectors/</a>. It is also important to note that support between C# and Python is not at parity; some vector databases are supported across both, but some are only supported in one language.</p> 
  </div> 
  <div class="readable-text intended-text" id="p133"> 
   <p>The SK is the central component for processing and understanding text. It provides a unified interface for various AI services and memory stores, simplifying the process of building complex NLP applications. Now let’s switch gears and see the same example using LangChain.</p> 
  </div> 
  <div class="readable-text" id="p134"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">LangChain</h4> 
  </div> 
  <div class="readable-text" id="p135"> 
   <p>LangChain offers a sophisticated framework designed to streamline the integration of LLMs into enterprise applications. This framework abstracts the complexities of interfacing with LLMs, allowing developers to incorporate advanced NLP capabilities without deep expertise in the field. Its library of modular components enables the construction of customized NLP solutions easily, facilitating a more efficient development process.</p> 
  </div> 
  <div class="readable-text intended-text" id="p136"> 
   <p>LangChain’s main benefit is its ability to work with different LLMs and other natural language AI services. This feature allows enterprises to select the best tools for their particular needs, avoiding the drawbacks of being tied to one vendor. The framework boosts efficiency by providing easier interfaces and ready-made components for quick deployment and supports scalability, thus enabling projects to expand smoothly from testing stages to full-fledged applications.</p> 
  </div> 
  <div class="readable-text intended-text" id="p137"> 
   <p>Additionally, LangChain helps to lower costs by minimizing the amount of specialized development and simplifying interactions with LLMs. Enterprises also gain from the strong community and support of the ecosystem around LangChain, which gives access to documentation, best practices, and cooperative problem-solving resources. This comprehensive approach makes LangChain an attractive option for businesses that want to use AI and natural language understanding in their services. It provides a way to innovate and enhance offerings through AI-driven solutions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p138"> 
   <p>Following the topic of pets from the previous chapters, in this chapter, we have a set of books related to dogs, which cover information ranging from general subjects to more specific medical advice. These books are PDFs and contain confidential business data that we want to use for a question–answer use case. </p> 
  </div> 
  <div class="readable-text intended-text" id="p139"> 
   <p>Listing 10.4 shows how this can be done easily using LangChain. In this case, we load all the PDFs from a local folder, read each PDF, split the context into 2K pieces, create embeddings (using OpenAI), and create a vector index using FAISS (Facebook AI Similarity Search). For brevity, we don’t show the code for some of the helper functions, such as <code>load_pdfs()</code>, as they are the same as the previous SK section.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p140"> 
   <p><span class="print-book-callout-head">Note</span>  FAISS is a library that allows fast and accurate vector search and clustering and can be used for various AI applications. It supports different vector comparisons and index types and can run on CPU and GPU. Facebook AI Research developed FAISS, and more details are available at <a href="https://ai.meta.com/tools/faiss/">https://ai.meta.com/tools/faiss/</a>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p141"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.4</span> Q&amp;A over my PDFs using LangChain</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from langchain_community.vector stores import FAISS
from langchain_community.docstore.document import Document
from langchain.chains.question_answering import load_qa_chain
from langchain.text_splitter import CharacterTextSplitter
...

def create_index():
    # load the documents and create the index
    docs = load_pdfs()

    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=2048,
        chunk_overlap=200,
        length_function=len
    )

    # Convert the chunks of text into embeddings
    print("Chunking and creating embeddings...")
    chunks = text_splitter.split_documents(docs)
    embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_KEY)
    vectordb = FAISS.from_documents(chunks, embeddings)

    return vectordb

def main():
    vectordb = create_index()
    llm = OpenAI(openai_api_key=OPENAI_KEY)
    chain = load_qa_chain(llm, chain_type='stuff')

    while True:
        prompt = check_prompt(input(
            'Ask a question against the PDF (type "quit" to exit):'))
        docs = vectordb.similarity_search(prompt, k=3, fetch_k=10)
        response = chain.invoke({'input_documents': docs,
                                 'question': prompt},
                                return_only_outputs=True)
        print(f"Answer:\n {response['output_text']}")

if __name__ == "__main__":
    main()</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p142"> 
   <p>On the one hand, LangChain is great and gives enterprises a big jumpstart for those just starting with LLMs and GenAI applications. LangChain simplifies the process by standardizing interactions with different LLM providers and offering tools for prompt creation, complex workflows (chains), and sophisticated AI assistants (agents). As an orchestrator, it can easily help us to connect LLMs to existing company data and systems, overcome initial hurdles, and quickly begin experimenting with LLM-driven applications.</p> 
  </div> 
  <div class="readable-text intended-text" id="p143"> 
   <p>However, LangChain comes with its challenges. Mastering concepts such as prompt design and building effective chains and agents has a learning curve. In addition, keeping the software and dependencies updated in this rapidly changing field can add some complexity. It is also essential to be aware of ethical LLM use, as powerful language models always carry the risk of incorrect or undesirable output. Finally, for production deployments where scale and performance are important, LangChain adds too many layers of abstractions and could end up hurting performance.</p> 
  </div> 
  <div class="readable-text" id="p144"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">LlamaIndex</h4> 
  </div> 
  <div class="readable-text" id="p145"> 
   <p>LlamaIndex is a data framework that enables LLMs to access and process private data sources that are not part of their pretraining corpus. This enhances their NLP capabilities and domain-specific knowledge for various use cases, such as document Q&amp;A, data-augmented chatbots, and structured analytics. LlamaIndex provides data ingestion, indexing, query interface, vector store, and database integration tools.</p> 
  </div> 
  <div class="readable-text intended-text" id="p146"> 
   <p>One of the main challenges of using LLMs for generative AI applications is the integration of different data formats (APIs, PDFs, documents, SQL, etc.) and LLM providers (OpenAI, Hugging Face, etc.). LlamaIndex simplifies this process by providing a unified interface and modular design, allowing users to easily connect their custom data sources to their preferred LLMs. LlamaIndex also supports data augmentation, which is the process of generating synthetic from existing data to improve the performance and robustness of LLMs</p> 
  </div> 
  <div class="readable-text intended-text" id="p147"> 
   <p>Another challenge of using LLMs for generative AI applications is efficient retrieval and scalability of data. LlamaIndex uses vector store and database providers to store and index data and optimize query speed and memory usage. LlamaIndex also supports various query types, such as natural language, keyword, and vector queries, to enable users to access their data conveniently and effectively.</p> 
  </div> 
  <div class="readable-text intended-text" id="p148"> 
   <p>Listing 10.5 shows the simplicity of using LlamaIndex to implement a RAG question-and-answer use case using the same pet-related books. We employ a built-in function that loads and processes all the PDFs from storage (saved in our example’s <code>data/dog_books</code> folder) and creates a built-in vector index using the OpenAI embeddings. We save this locally to save time and can reuse it in the next instance. For us to use LlamaIndex, we do need to install a couple of packages—<code>llama-index</code> and <code>llama-index-reader-files</code> as shown: <code>pip</code> <code>install</code> <code>llama-index==0.10.9</code> <code>llama-index-readers-file</code>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p149"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.5</span> An example showing RAG with LlamaIndex</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    StorageContext,
    load_index_from_storage,
    Settings
)
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.readers.file import PDFReader

PERSIST_DIR = "./storage/llamaindex"
DOG_BOOKS = "./data/dog_books/"

OPENAI_KEY = os.getenv('OPENAI_API_BOOK_KEY')                      <span class="aframe-location"/> #1
Settings.embed_model = OpenAIEmbedding(api_key=OPENAI_KEY)         #1

def load_or_create_index():
    if not os.path.exists(PERSIST_DIR):                           <span class="aframe-location"/> #2
        try:
            parser = PDFReader()
            file_extractor = {".pdf": parser}

            # load only PDFs
            required_exts = [".pdf"]                              <span class="aframe-location"/> #3
            documents = SimpleDirectoryReader(DOG_BOOKS, 
                           <span class="">↪</span>file_extractor=file_extractor, 
                           <span class="">↪</span>required_exts=required_exts).load_data()
           <span class="aframe-location"/> index = VectorStoreIndex.from_documents(             #4
                           <span class="">↪</span>documents, show_progress=True)

            # store the index for later
            index.storage_context.persist(persist_dir=PERSIST_DIR)  <span class="aframe-location"/> #5

            print("Index created and stored in", PERSIST_DIR)
        except Exception as e:
            print("Error while creating index:", e)
            exit()
    else:
        print("Loading existing index from", PERSIST_DIR)

        try:
            # load the existing index
            storage_context = StorageContext.from_defaults( 
                              <span class="">↪</span>persist_dir=PERSIST_DIR)
            index = load_index_from_storage(storage_context) <span class="aframe-location"/> #6
        except Exception as e:
            print("Error while loading index:", e)
            exit()
    return index

def main():
    index = load_or_create_index()
    query_engine = index.as_query_engine()

    while True:
        prompt = input("Ask a question about dogs:")
        response = query_engine.query(prompt)
        print(response)

if __name__ == "__main__":
    main()</pre> 
    <div class="code-annotations-overlay-container">
     #1 Loads environment variables
     <br/>#2 Checks whether storage already exists
     <br/>#3 Loads only PDFs
     <br/>#4 Loads the PDF documents and creates the index
     <br/>#5 Saves the index for later use
     <br/>#6 Loads an existing index
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p150"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_181"><span class="num-string">10.3.4</span> Prompt management</h3> 
  </div> 
  <div class="readable-text" id="p151"> 
   <p>Earlier in the book, we learned that prompt engineering plays a crucial role in communicating with LLMs, as it directly affects the output quality. A well-constructed prompt can help the LLM to generate accurate and contextually relevant responses. For this, you need to have a profound understanding of how LLMs interpret input and the ability to create prompts that the model can comprehend effectively.</p> 
  </div> 
  <div class="readable-text intended-text" id="p152"> 
   <p>Prompt management involves designing, testing, and deploying prompts or instructions for LLMs to perform various tasks. Prompts also need to work together with the evaluations and content moderation as part of the response filtering tier and RAI implementations. We will cover this aspect in more detail later in the book. As a part of the orchestration layer, prompt management provides a comprehensive approach to managing LLMs. This involves three essential components: prompt engineering, optimization, and PromptOps.</p> 
  </div> 
  <div class="readable-text intended-text" id="p153"> 
   <p>Prompt engineering encompasses the creation of custom, adaptive, and domain-specific prompts tailored to the user’s needs and the context of their queries. This involves generating custom prompts for specific tasks, such as summarizing news articles by understanding the context and requirements and adapting prompts in real-time based on user interactions to better align with their intent. Additionally, it includes developing prompts that cater to specialized fields, using the appropriate technical language and adhering to field-specific standards.</p> 
  </div> 
  <div class="readable-text intended-text" id="p154"> 
   <p>Prompt optimization focuses on improving the prompts’ effectiveness through continuous performance monitoring, data-driven refinements, and efficient resource management. This entails tracking metrics such as accuracy and relevance to gauge the prompts’ success, refining prompts based on user feedback and response quality to enhance clarity, and optimizing prompts to stay within token limits and reduce complexity, thus ensuring cost-efficiency and prompt–response generation.</p> 
  </div> 
  <div class="readable-text intended-text" id="p155"> 
   <p>PromptOps involves the operational aspects of managing prompts, including automated testing for prompt effectiveness, version control for managing different prompt versions and enabling easy rollbacks, integration with other AI system components to ensure seamless operation, and scalability and maintenance considerations to ensure the prompt management system can handle growing demands and is easy to update. This comprehensive approach to prompt management ensures that the AI system remains effective, efficient, and adaptable to user needs and technological advancements.</p> 
  </div> 
  <div class="readable-text intended-text" id="p156"> 
   <p>Prompt management (i.e., creating and optimizing prompts for LLMs) can benefit from various tools and frameworks being developed constantly. For enterprises that want to use LLMs and prompt management tools, assessing the technical features and the vendor’s adherence to security, privacy, and compliance with relevant regulations (e.g., GDPR, HIPAA) is important. Moreover, enterprises should consider the level of support, customization, and ability to integrate with existing systems and workflows. Many of these providers offer custom solutions and partnerships for businesses, ensuring that using LLMs matches enterprise needs and strategic objectives. Prompt flow (<a href="https://github.com/microsoft/promptflow">https://github.com/microsoft/promptflow</a>), a Microsoft OSS tool for prompt management, is one example. We will cover Prompt flow in more detail in the book’s next chapter.</p> 
  </div> 
  <div class="readable-text intended-text" id="p157"> 
   <p>Another example is Pezzo (<a href="https://github.com/pezzolabs/pezzo">https://github.com/pezzolabs/pezzo</a>), which can help with prompt management. LangChain and SK, which we saw earlier, also have some support for prompt management. For more details, see “Prompting Frameworks for Large Language Models: A Survey” [4].</p> 
  </div> 
  <div class="readable-text intended-text" id="p158"> 
   <p>Prompt management is an important process in ensuring the effectiveness of LLM applications. It is a dynamic and iterative process that involves designing, testing, refining, and customizing prompts for optimal outputs. The architecture of the LLM system must be flexible enough to accommodate current and future advances in prompt design. It should also provide tools for continuous improvement mechanisms to generate high-quality outputs.</p> 
  </div> 
  <div class="readable-text" id="p159"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_182"><span class="num-string">10.4</span> Grounding layer</h2> 
  </div> 
  <div class="readable-text" id="p160"> 
   <p>The grounding layer is the foundation of GenAI applications that handle data acquisition, storage, processing, and delivery. It integrates various data sources and formats with connectors, pipelines, plugins, and APIs. In addition, it performs data preprocessing, embedding, and vectorization to make the data compatible with LLMs. It employs distributed data processing frameworks for scalability and reliability. Let’s explore this in a little more detail.</p> 
  </div> 
  <div class="readable-text" id="p161"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_183"><span class="num-string">10.4.1</span> Data integration and preprocessing</h3> 
  </div> 
  <div class="readable-text" id="p162"> 
   <p>Having reliable data pipelines to combine data from different systems as seamlessly as possible is important. These pipelines must be designed to handle various data types and sources—from structured SQL database entries to unstructured text, image files, and real-time streaming data from IoT deployments. The architecture of these pipelines must be compatible with various data formats and protocols, which may require the development of custom APIs, middleware for data transformation, and scalable ETL (extract, transform, load) processes.</p> 
  </div> 
  <div class="readable-text" id="p163"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Integration</h4> 
  </div> 
  <div class="readable-text" id="p164"> 
   <p>Integrating a system of record is fundamental to the generative AI application architecture. It involves multiple layers of interaction and data management in a secure, compliant, and efficient manner, which ensures that real-time data is available for the LLM, while maintaining quality. In addition, the integration must be scalable and adaptable to changes in the enterprise data ecosystem.</p> 
  </div> 
  <div class="readable-text intended-text" id="p165"> 
   <p>The main goal for integration pipelines is to integrate them into various systems of records (SoRs) and enable access for the data from those systems to be used efficiently by GenAI models. Integrating with SoR is crucial in designing generative AI applications. These systems include SaaS platforms, customer relationship management (CRM), and enterprise resource planning (ERP) systems. They serve as the data backbone for the LLM applications, acting as repositories for the enterprise’s structured and unstructured data. This data is essential for using the LLMs as a reasoning engine, allowing it to access high-quality, domain-specific information.</p> 
  </div> 
  <div class="readable-text intended-text" id="p166"> 
   <p>This information retrieved via the SoR integration is used for RAG implementation. As we saw earlier in the book, it is one of the main ways enterprises can operate on their proprietary information. SoR integrations are the key to achieving that. The main challenge is not just the integration but also understanding the nature of the data, the frequency of change, and the computational cost.</p> 
  </div> 
  <div class="readable-text intended-text" id="p167"> 
   <p>Several tools are available to initiate this process, such as Microsoft Fabric, which offers over 145 connectors, Apache NiFi, Informatica, and so forth. These tools gather and consolidate data from different sources into a single repository that can handle various data formats and prevent data loss during data capturing.</p> 
  </div> 
  <div class="readable-text intended-text" id="p168"> 
   <p>Modern storage solutions such as Amazon S3, Azure Data Lake Storage, or the Hadoop Distributed File System (HDFS) offer secure and scalable storage options for large amounts of data. When combined with data warehousing technologies such as Snowflake, Google BigQuery, or Amazon Redshift, businesses can efficiently store, query, and manage their data, making it easier to prepare for AI integration.</p> 
  </div> 
  <div class="readable-text intended-text" id="p169"> 
   <p>Data orchestration tools, such as Apache Airflow, Data Factory in Microsoft Fabric, and AWS Glues, offer modern, code-centric methods for constructing and executing complex data workflows. These systems allow developers to define data pipelines through code, facilitating version control and testing similar to standard software development practices. Additionally, they provide scheduling, monitoring, and error management features that contribute to the reliability of data pipelines.</p> 
  </div> 
  <div class="readable-text" id="p170"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Preprocessing</h4> 
  </div> 
  <div class="readable-text" id="p171"> 
   <p>Once data has been prepared for AI use, it can be sent to processing engines or analytical platforms for further preparation. Apache Spark is a well-known platform that can handle large-scale data processing and has several libraries covering various computing requirements. Platforms such as Databricks have built upon Spark’s capabilities to ease the journey from data preparation to model deployment. In addition, architectures must include event-driven mechanisms such as webhooks or streaming services to ensure data synchronization and real-time updates.</p> 
  </div> 
  <div class="readable-text intended-text" id="p172"> 
   <p>For data to be useful in informing LLM outcomes, it must first undergo a rigorous cleansing and standardization process to ensure its quality. The architectural blueprint should include these preprocessing activities, such as deduplication, normalization, and error rectification. Integrated data quality tools should automate these tasks, providing LLMs with superior datasets.</p> 
  </div> 
  <div class="readable-text intended-text" id="p173"> 
   <p>Data handling requires strict access controls for proper security and compliance, which is vital when working with sensitive information and following regulations. Data interaction needs strong authentication and authorization protocols. Data governance frameworks should specify access rights; furthermore, encryption should protect data at rest and in motion. Frequent compliance assessments are crucial for ensuring data quality and privacy. Following GDPR, HIPAA, or CCPA regulations is also important for ethical and lawful processing of personal data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p174"> 
   <p>A plugin enabling the integration into source systems is not a one-time static component of the architecture—it changes and adapts constantly. As businesses use or improve their new SoRs, the architecture must be built to allow simple integration or movement of data sources. For this, a flexible approach to integration is required, where new data sources can be connected with little change to the current system.</p> 
  </div> 
  <div class="readable-text intended-text" id="p175"> 
   <p>The architecture should be designed to support different data formats and protocols. This ensures that data flows seamlessly from various systems to the LLM. To achieve this, custom APIs may need to be developed, middleware may have to be used for data transformation, and ETL processes capable of handling large volumes of data may have to be implemented.</p> 
  </div> 
  <div class="readable-text intended-text" id="p176"> 
   <p>The data pipeline infrastructure for generative AI is complex and requires careful planning to handle the intricacies of enterprise-grade data landscapes. These will build on existing ETL and data warehousing investments but must factor in the new data types of embeddings. By strategically using a combination of tools for data ingestion, processing, storage, orchestration, and ML, enterprises can build powerful pipelines that provide their generative AI applications with a consistent flow of quality data.</p> 
  </div> 
  <div class="readable-text" id="p177"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_184"><span class="num-string">10.4.2</span> Embeddings and vector management</h3> 
  </div> 
  <div class="readable-text" id="p178"> 
   <p>In earlier chapters of the book, we discussed the crucial role of model embeddings and representations. This is the stage where the complexity of language is distilled into machine-interpretable formats, specifically mathematical vectors. Text is transformed by embedding techniques and advanced feature extraction forms that result in a vector space representation of text. These vectors are not arbitrary; they encapsulate the semantic essence of words, phrases, or entire documents, mapping information into a compressed, information-rich, lower-dimensional space.</p> 
  </div> 
  <div class="readable-text intended-text" id="p179"> 
   <p>OpenAI Codex is a prime example of this process. It can comprehend and generate human-readable code, making it a powerful tool for embedding programming and natural languages. This is a significant advantage for code generation and automation tasks. In contrast, Hugging Face provides an extensive suite of pretrained models that are finely tuned for diverse languages and tasks. They can adeptly handle embeddings ranging from brief sentences to intricate documents.</p> 
  </div> 
  <div class="readable-text intended-text" id="p180"> 
   <p>These models distinguish themselves by their ability to grasp contextual word relationships beyond basic dictionary meanings. By considering the words in their vicinity, the generated embeddings provide a nuanced reflection of the word usage and connotations within specific contexts. This feature is essential for generative AI applications that aim to emulate human-like text production. It fosters outcomes that are not only coherent and context-aware but also semantically profound.</p> 
  </div> 
  <div class="readable-text intended-text" id="p181"> 
   <p>As we saw in earlier chapters on RAG, various libraries are available for chunking data, and some offer auto-chunking capabilities. One such library, called Unstructured (<a href="https://github.com/Unstructured-IO/unstructured">https://github.com/Unstructured-IO/unstructured</a>), provides open source libraries and APIs that can create customized preprocessing pipelines for labeling, training, or production ML pipelines. The library includes modular functions and connectors that form a cohesive system, which makes it easy to ingest, preprocess, and adapt data to different platforms. It is also efficient at transforming unstructured data into structured outputs.</p> 
  </div> 
  <div class="readable-text intended-text" id="p182"> 
   <p>An alternative solution is using LangChain and SK, which we saw earlier. These libraries support common chunking techniques for fixed size, variable size, or a combination of both. In addition, you can specify an overlap percentage to duplicate a small amount of content in each chunk, which helps preserve context.</p> 
  </div> 
  <div class="readable-text intended-text" id="p183"> 
   <p>After transforming vectors, it is crucial to manage them properly. Vector databases specially designed to store indexes and retrieve high-dimensional vector data are available. Some such databases include Redis, Azure Cosmos DB, Pinecone, and Weaviate, to name a few. These databases help with quick searches within large embedding spaces, making it easy to identify similar vectors instantly. For instance, a generative AI system can use a vector database to match a user’s query with the most semantically related questions and answers and achieve this in a fraction of a second.</p> 
  </div> 
  <div class="readable-text intended-text" id="p184"> 
   <p>Vector databases feature sophisticated indexing algorithms engineered to deftly traverse high-dimensional terrains without falling prey to the “curse of dimensionality” [5]. This attribute renders them exceptionally valuable for applications such as recommendation engines, semantic search platforms, and personalized content curation, where pinpointing relevant content quickly is critical.</p> 
  </div> 
  <div class="readable-text intended-text" id="p185"> 
   <p>Vector databases offer more than just speed; they also provide accuracy and relevance. Combining these databases allows AI models to respond quickly and precisely to user inquiries based on their learned context. Proper index management is crucial, including tasks such as index creation, update triggers, refresh rates, complex data types, and operational factors (e.g., index size, schema design, and underlying compute services). Cloud-based solutions such as Azure AI Search and Pinecone can efficiently manage these demands in a production environment.</p> 
  </div> 
  <div class="readable-text intended-text" id="p186"> 
   <p>The process of transforming textual data into a format that AI can handle has two stages: embedding and vector database management. This conversion is essential for generative AI’s intelligence, enabling it to understand and engage with the world meaningfully and in a scalable manner. Therefore, carefully choosing embedding techniques and vector databases is a technical necessity and a key factor in the success of generative AI applications. When choosing LLMs, related vector storage and retrieval engines, and embedding models, enterprises must consider the data size, origin, change rate, and scalability needs.</p> 
  </div> 
  <div class="readable-text" id="p187"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_185"><span class="num-string">10.5</span> Model layer</h2> 
  </div> 
  <div class="readable-text" id="p188"> 
   <p>The model layer is the foundation of AI cognitive capabilities. It involves a set of models, including foundational LLMs that provide general intelligence, fine-tuned LLMs specialized for specific tasks or domains, model catalogs hosting and managing access to various models, and SLMs that offer lightweight, agile alternatives for certain applications.</p> 
  </div> 
  <div class="readable-text intended-text" id="p189"> 
   <p>The significance of this layer lies in its design, as it forms the core processing units of the GenAI app stack. It allows a scalable and flexible approach to AI deployment and can efficiently address various tasks by differentiating between foundational, fine-tuned, and small models. This ensures that the architecture can cater to diverse use cases, optimize resource allocation, and maintain high performance across different scenarios.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p190"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Small language models</h5> 
   </div> 
   <div class="readable-text" id="p191"> 
    <p>SLMs such as Phi-3 and Orca 2 are designed to offer advanced language processing capabilities with fewer parameters than larger models. Both models are part of a broader initiative to make powerful language processing tools more accessible and efficient, enabling more extensive research and application possibilities. They represent a significant step in the evolution of AI language models, balancing capability with computational efficiency.</p> 
   </div> 
   <div class="readable-text" id="p192"> 
    <p>Phi-3, Phi-2, and Orca 2 are smaller-scale language models developed by Microsoft, offering advanced language processing with fewer parameters. Phi-3, which is a successor to Phi-2, is a family of models in various sizes (mini, 3.8B; small, 7B; medium, 14B parameters). Phi-2, with 2.7 billion parameters, is efficient and matches larger models in performance, while Orca 2, available in 7- and 13-billion-parameter versions, excels in reasoning tasks and can outperform much larger models. Both are designed for accessibility and computational efficiency, enabling broader research and application in AI language processing.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p193"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_186"><span class="num-string">10.5.1</span> Model ensemble architecture</h3> 
  </div> 
  <div class="readable-text" id="p194"> 
   <p>Generative AI employs a model ensemble, which combines multiple ML models to enhance performance and reliability. This approach takes advantage of the individual strengths of each model, minimizing their weaknesses. For example, one model may be great at generating technical content, while another may be better at creative storytelling. By assembling these models, an application can better cater to a wider range of user requests with greater accuracy. To create an effective model ensemble for generative AI, the architecture should include</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p195"> <em>Model selection</em><em> </em>—Criteria for choosing which models to include in the ensemble, often based on their performance, the diversity of training data, or their area of specialization. </li> 
   <li class="readable-text" id="p196"> <em>Routing logic</em><em> </em>—Routing logic is the mechanism for determining which model to use for a given input or how to combine outputs from multiple models. </li> 
   <li class="readable-text" id="p197"> <em>API integration</em><em> </em>—APIs are the main conduits through which applications interact with LLMs. API integration becomes complex when dealing with an ensemble of models as interactions with multiple endpoints must be managed. The architecture should consider API integration of throttling and rate limits, error handling, and caching responses. </li> 
   <li class="readable-text" id="p198"> <em>Scalability and redundancy</em><em> </em>—Scalable design accommodates growing user bases and spikes in demand. Load balancing and the use of API gateways can help distribute traffic effectively. Redundancy is equally critical; thus, having multiple regions for model deployments ensures the application remains functional. </li> 
   <li class="readable-text" id="p199"> <em>Queuing and stream processing</em><em> </em>—Queuing and stream processing handle asynchronous tasks and manage workloads; message queues and stream processing services can be utilized, which ensures that the system is not overwhelmed during peak times and that tasks are processed in an orderly way. </li> 
  </ul> 
  <div class="readable-text" id="p200"> 
   <p>Figure 10.5 is an example of implementing Phi-2 as a classifier. We use Phi-2, which runs locally and fast, to identify the user’s intent when asking a question. Continuing with the topic of pets and dogs, we asked Phi-2 the intent of the question and whether it had anything to do with dogs. If it was irrelevant to the current topic (i.e., dogs), we asked GPT-4 to answer. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p201">  
   <img alt="figure" src="../Images/CH10_F05_Bahree.png" width="911" height="312"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.5</span> Classifier using multiple models</h5>
  </div> 
  <div class="readable-text" id="p202"> 
   <p>Listing 10.6 shows an example of implementing a simple classifier using a lightweight model and then, based on the question’s intent, figuring out which model to call. Here, we use Phi-2, a research SML from Microsoft, as a classifier to determine whether a question is related to dogs. The Phi-2 model is a transformer-based model, trained to understand and generate human-like text. It is used here as a first-pass filter to determine the question’s intent.</p> 
  </div> 
  <div class="readable-text intended-text" id="p203"> 
   <p>The function <code>check_dog_question()</code> takes a question as input and constructs a prompt to ask the Phi-2 model whether there’s anything about dogs in the question. If Phi-2 determines that the question is about dogs, the function returns <code>True</code>. This could trigger a more expensive GPT-4 model to generate a more detailed response. If the question is not about dogs, the function returns <code>False</code>, and the more expensive model would not have to be used. We need to ensure that the following packages are installed before running this code: <code>pip</code> <code>install</code> <code>transformers==4.42.4</code> <code>torch= =2.3.1</code>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p204"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.6</span> Using Phi-2 as an intent classifier</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import openai
...

model = AutoModelForCausalLM.from_pretrained("microsoft/phi-2", 
                                             torch_dtype="auto",
                                             trust_remote_code=True)

tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-2", 
                                          trust_remote_code=True)

def check_dog_question(question):
    prompt = f"Instruct: Is there anything about dogs in the 
             <span class="">↪</span>question below? If yes, answer with 'yes' else 
             <span class="">↪</span>'no'.\nQuestion:{question}\nOutput: "

    inputs = tokenizer(prompt, return_tensors="pt",
                       return_attention_mask=False,
                       add_special_tokens=False)
    outputs = model.generate(**inputs,
                             max_length=500,
                             pad_token_id=tokenizer.eos_token_id)
    text = tokenizer.batch_decode(outputs)[0]
    regex = "^Output: Yes$"
    match = re.search(regex, text, re.MULTILINE)
    if match:
        return True

    return False

def handle_dog_question(question):
    print( "This is a response from RAG and GPT4")

    # Call OpenAI's GPT-4 to answer the question
    openai.api_key = "YOUR_API_KEY"
    response = openai.Completion.create(
      …
    )
    return response

if __name__=="__main__": 
    # Loop until the user enters "quit"
    while True:
        # Take user input
        user_prompt = input(
           "What is your question (or type 'quit' to exit):")

        if check_dog_question(user_prompt):
            print(handle_dog_question(user_prompt))
        else:
            print("You did not ask about dogs")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p205"> 
   <p>The approach employs a small model, such as Phi-2, with much less capability for more efficient use of resources, as the more expensive GPT-4 model is used only when necessary. This approach can just as easily be expanded to use more than one model.</p> 
  </div> 
  <div class="readable-text intended-text" id="p206"> 
   <p>This toy example could be better if we used a more powerful LLM, such as a smaller GPT-3 model. Figure 10.6 shows another example of using a fine-tuned GPT-3 as a classifier to help understand the user’s goal. This is for an enterprise chatbot that can answer questions on both structured and unstructured data. It can answer questions about Microsoft’s surface devices based on the user’s persona. There is fictitious sales information in a SQL database that a salesperson can chat with, and there is unstructured data that can answer technical support questions. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p207">  
   <img alt="figure" src="../Images/CH10_F06_Bahree.png" width="1008" height="414"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.6</span> Enterprise Q&amp;A bot—High-level overview</h5>
  </div> 
  <div class="readable-text" id="p208"> 
   <p>The bot uses a RAG pattern and can answer questions using information from both structured and unstructured systems based on the user’s intention. The structured data has sales information (with fake data), and the unstructured data is a crawl of different forums and official sites related to Surface devices. Listing 10.7 presents a high-level view of the architecture.</p> 
  </div> 
  <div class="readable-text intended-text" id="p209"> 
   <p>The orchestrator uses GPT-3 to implement the intent classifier and can help select the best path based on the bot’s question. Then, suitable knowledge sources are applied. This complicated workflow shows much of what an orchestrator would do in a real-world enterprise situation. The sales data is stored in a SQL database, and GPT is also used to build the SQL query against the schema to run, depending on the user’s query. What is very interesting is that the LLM is invoked multiple times in the flow, first to understand the intent of the question, and then, depending on the path, GPT also creates the SQL query to execute. Its results are passed to the prompt formulation to invoke the LLM again to create the response for the user. This mainly shows that along the flow, we can invoke the right model based on the point in time and for what it is needed, factoring in the model capability and associated computational constraints and costs.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p210"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.7</span> Using a fine-tuned GPT-3 model as a classifier</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">try:
    response = openai_client.chat_completions(
        messages=message_list,
        openai_settings=ChatCompletionsSettings(
            **bot_config["approach_classifier"]["openai_settings"]
        ),
        api_base=f"https://{AZURE_OPENAI_SERVICE}.openai.azure.com",
        api_key=AZURE_OPENAI_KEY,
    )
    except openai.error.InvalidRequestError as e:
        self.logger.error(f"AOAI API Error: {e}", exc_info=True)
        raise e

    classification_response: str = response["choices"][0] 
    <span class="">↪</span>["message"]["content"]
    self.log_aoai_response_details(
        f'Classification Prompt:{history[-1]["utterance"]}',
        f"Response: {classification_response}",
        response,
    )
    if classification_response == "1":
        return ApproachType.structured
    elif classification_response == "2":
        return ApproachType.unstructured
    elif classification_response == "3":
        return ApproachType.chit_chat
    elif classification_response == "4":
        # Continuation: Return last question type from history
        ...
        else:
            return ApproachType.unstructured
    elif classification_response == "5":
        # User has typed something that violates guardrails
        return ApproachType.inappropriate
    else:
        return ApproachType.unstructured</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p211"> 
   <p>In addition to the classifier, we must use the appropriate prompts to convey our purpose and obtain the desired behavior. The sample prompts that match the classifier are displayed in the following listing.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p212"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.8</span> Classifier meta-prompt</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">You are an intent classifier for Microsoft Surface product Sales 
<span class="">↪</span>and Marketing teams. The user will input a statement. You will focus 
<span class="">↪</span>on the main intent of the user statement and you respond with only 
<span class="">↪</span>one of four values - '1', '2', '3', '4', or '5'. 

Below is a list of Rules that you must adhere to:
Rules:
A: Stricly answer questions relating to Microsoft Surface products.
B: For tabular information return it as an html table. 
C: Do not use markdown format in your responses.
D: Do not disclose or respond to any proprietary information, IP, 
      <span class="">↪</span>secrets, keys, data center, and infrastructure details in 
      <span class="">↪</span>your response.
E: Do not mention or compare to any competitors (i.e. Apple MacBook, 
      <span class="">↪</span>Lenovo, HP, etc).
F: Note if the user asks something illegal, harmful or malicious.

You will not try to respond to the user's question, you will just 
    <span class="">↪</span>classify the user statement based on the below classification rule:
- For questions about past sales, prices, stores or stock of products 
    <span class="">↪</span>such as devices and laptops, respond with 1
- For questions on specifications of products/devices/laptops or 
    <span class="">↪</span>marketing them, respond with 2
- If the question is idle chit-chat, pleasantries such as greetings, 
    <span class="">↪</span>or sligthly off topic but doesn't break the rules, respond with 3
- If the user is asking for more details about a previous question, 
    <span class="">↪</span>respond with 4
- If the message is not in compliance with Rule F, respond with 5

Examples:
User: How much stock of this are we currently carrying?
Assistant: 1

User: Give me its specifications
Assistant: 2

User: How many MacBook Air do we have in stock?
Assistant: 3

User: Tell me more about it
Assistant: 4
...</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p213"> 
   <p>The link to the full code listing can be found in the book’s GitHub repository (<a href="https://bit.ly/GenAIBook">https://bit.ly/GenAIBook</a>). It is a fork from one of Microsoft’s published samples, found at <a href="https://bit.ly/AOAISearchDemo">https://bit.ly/AOAISearchDemo</a>.</p> 
  </div> 
  <div class="readable-text" id="p214"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_187"><span class="num-string">10.5.2</span> Model serving</h3> 
  </div> 
  <div class="readable-text" id="p215"> 
   <p>Many modern AI applications are hosted on cloud platforms due to their scalability and the wide range of services they offer. Integrating with major cloud providers such as Microsoft Azure, Amazon Web Services, or Google Cloud Platform enables developers to use a secure global network of data centers, ML managed services, and tools for application monitoring and management. Therefore, many enterprises use one of the LLMs hosted in the cloud, which is exposed via an API. This means that the cloud providers that manage the model serve to scale up or down model inference. If some models are hosted on-premise, the layer must address model operations working with LLMOps.</p> 
  </div> 
  <div class="readable-text intended-text" id="p216"> 
   <p>The model layer architecture should provide a strategic framework for using multiple LLMs to create a robust, versatile, and scalable application. This involves careful planning around model selection and API management, while ensuring security and compliance in data handling. The architecture should be flexible enough to adapt to new models and APIs as they become available.</p> 
  </div> 
  <div class="readable-text" id="p217"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_188"><span class="num-string">10.6</span> Response filtering</h2> 
  </div> 
  <div class="readable-text" id="p218"> 
   <p>In most cases, an application should not share the raw generation from the model with the end user; it should go through a processing step to help manage and filter any sensitive details—this is where the processing layer helps, and a key responsibility of this layer is to manage the LLM output. </p> 
  </div> 
  <div class="readable-text intended-text" id="p219"> 
   <p>The response filtering layer is tasked with quality assurance and content moderation, crucial for maintaining trust in GenAI applications. It involves using classifiers and NLP tools to screen the outputs for accuracy, bias, and appropriateness.</p> 
  </div> 
  <div class="readable-text intended-text" id="p220"> 
   <p>As we have seen, LLM output can vary significantly, ranging from simple text to complex data structures. Managing these outputs requires a systematic approach so they meet the application’s standards and are presented to the user in a useful format. These postprocessing steps include a few areas, as shown in figure 10.7.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p221">  
   <img alt="figure" src="../Images/CH10_F07_Bahree.png" width="1009" height="158"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.7</span> Response filtering stages</h5>
  </div> 
  <div class="readable-text" id="p222"> 
   <p>Content moderation relies on RAI practices to mitigate the potential risks of generative AI models, such as biased, offensive, or misleading content, cyber, privacy, legal, performance, and intellectual property risks. We need to adopt RAI practices to use the power of generative AI. RAI is essential for the output processing layer to address both application- and enterprise-level risks, such as regulatory and compliance requirements. In addition, RAI can enhance other aspects, such as privacy, explainability, and fairness.</p> 
  </div> 
  <div class="readable-text intended-text" id="p223"> 
   <p>There are many tools and frameworks to start with. For example, Microsoft’s InterpretML (<a href="https://interpret.ml/">https://interpret.ml/</a>) and Fairlearn (<a href="https://fairlearn.org/">https://fairlearn.org/</a>) are open source toolkits that help developers explain and improve the fairness of ML models. IBM’s AI Fairness 360 is another open source toolkit that helps detect and reduce bias in ML models. We’ll examine RAI in more depth later in the book.</p> 
  </div> 
  <div class="readable-text intended-text" id="p224"> 
   <p>Output and postprocessing are crucial for ensuring the usability and safety of content generated by LLMs. The architecture should provide a robust framework for refining and managing outputs, including formatting, content classification, validation, and caching. Quality assurance, both automated and user driven, must be an integral part of the process to maintain high standards and improve over time.</p> 
  </div> 
  <div class="readable-text intended-text" id="p225"> 
   <p>This chapter shows how GenAI can be integrated into enterprise applications using the new GenAI app stack and associated application architecture. We have also discussed the role of the Center of Excellence in facilitating this integration and addressing the technical, cultural, and ethical challenges involved. However, building an AI solution is only the first step; deploying it for production and scale requires different skills and tools. The next chapter will explore what it takes to operationalize generative AI solutions and ensure their reliability, performance, and security. We will also look at some of best practices and frameworks for managing the AI lifecycle and delivering value to the end-users and stakeholders.</p> 
  </div> 
  <div class="readable-text" id="p226"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_189">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p227"> Copilot demonstrates how generative AI architecture can build enterprise applications and solutions. It uses a different application stack that works with copilots to create the new enterprise architecture stack. This stack is for GenAI apps, which use Copilot as a counterpart to the LAMP stack. </li> 
   <li class="readable-text" id="p228"> The GenAI app stack includes four layers that cooperate to make the application stack function—the model, orchestration, grounding, and response filtering layers. </li> 
   <li class="readable-text" id="p229"> The orchestration layer is one of the critical and foundational components of the GenAI stack. It handles and organizes different processes, AI services, and platforms to enable a dependable and coherent experience. </li> 
   <li class="readable-text" id="p230"> The area of orchestration frameworks is new and evolving, with many changes and innovations taking place. Some of the frameworks that are more widely used today are SK, LangChain, and LlamaIndex. </li> 
   <li class="readable-text" id="p231"> By using plugins than can handle the intricacies of the source systems, their protocols, and other details, the grounding layer facilitates data integrations and preprocessing for RAG deployments in the enterprises. It also oversees the embeddings and the related vector databases. </li> 
   <li class="readable-text" id="p232"> The model layer offers a platform for using multiple models from various sources—from managed and fine-tuned models to BYOM (bring-your-own-model) for enterprises. These models can all be accessed through strong APIs that guarantee compliance and security. </li> 
   <li class="readable-text" id="p233"> The response filtering layer ensures quality and moderates content, essential for building confidence in GenAI applications. Furthermore, it involves using classifiers and NLP tools to check the outputs for correctness, fairness, and suitability. </li> 
   <li class="readable-text" id="p234"> An AI Center of Excellence can help enterprises comprehensively integrate LLMs and GenAI into their applications. By addressing technical, cultural, and ethical challenges, enterprises can use AI to enhance innovation and competitiveness, ensuring lasting success in an increasingly AI-powered world. </li> 
  </ul>
 </div></div></body></html>