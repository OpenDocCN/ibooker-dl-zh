- en: Chapter 1\. Introduction to PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 1 章. PyTorch 简介
- en: When it comes to creating artificial intelligence (AI), machine learning (ML)
    and deep learning are great places to begin. When you’re getting started, however,
    it’s easy to get overwhelmed by the options and all the new terminology. This
    book aims to demystify things for you as a programmer. It takes you through writing
    code to implement concepts of ML and deep learning, and it also takes you through
    building models that behave more as a human does, with scenarios like computer
    vision, natural language processing (NLP), and more. Thus, these models become
    a form of synthesized, or artificial, intelligence.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到创建人工智能（AI）时，机器学习（ML）和深度学习是很好的起点。然而，当你开始时，很容易被选项和所有新的术语所淹没。这本书旨在为你这个程序员揭开神秘的面纱。它带你通过编写代码来实现机器学习和深度学习概念，同时也带你构建行为更像人类的模型，例如计算机视觉、自然语言处理（NLP）等。因此，这些模型成为了一种合成或人工的智能形式。
- en: But when we refer to *machine learning*, what exactly is it? Let’s take a quick
    look at that and consider it from a programmer’s perspective before we go any
    further. After that, in the rest of this chapter, we’ll show you how to install
    the tools of the trade, from PyTorch itself to environments where you can code
    and debug your PyTorch-based models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 但当我们提到*机器学习*时，它究竟是什么呢？在我们继续之前，让我们快速看一下它，并从程序员的角度来考虑它。之后，在本章的其余部分，我们将向您展示如何安装所需的工具，从
    PyTorch 本身到您可以在此处编写和调试基于 PyTorch 的模型的开发环境。
- en: What Is Machine Learning?
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: Before we get into the ins and outs of ML, let’s consider how it evolved from
    traditional programming. We’ll start by examining what traditional programming
    is, and then we’ll consider cases where it’s limited. After that, we’ll see how
    ML evolved to handle those cases and thus opened up new opportunities to implement
    new scenarios, thereby unlocking many of the concepts of AI.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨机器学习的细节之前，让我们考虑它是如何从传统编程中演变而来的。我们将首先检查传统编程是什么，然后考虑它的局限性。之后，我们将看到机器学习是如何演变来处理这些情况的，从而为实施新的场景打开了新的机会，从而解锁了许多人工智能的概念。
- en: Traditional programming involves writing rules that are expressed in a programming
    language and that act on data and give us answers. This applies just about everywhere
    we can program something with code.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 传统编程涉及编写用编程语言表达、作用于数据和给出答案的规则。这几乎适用于我们可以用代码编程的任何地方。
- en: For example, consider a game like the popular Breakout. Code determines the
    movement of the ball, the score, and the various conditions for winning or losing
    the game. Think about the scenario where the ball bounces off a brick, like in
    [Figure 1-1](#ch01_figure_1_1748548870009162).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑像流行的《Breakout》这样的游戏。代码决定了球的移动、得分以及赢得或输掉游戏的各个条件。想想球在砖块上弹跳的场景，就像[图 1-1](#ch01_figure_1_1748548870009162)中展示的那样。
- en: '![](assets/aiml_0101.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0101.png)'
- en: Figure 1-1\. Code in a Breakout game
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-1\. 《Breakout》游戏中的代码
- en: Here, the motion of the ball can be determined by its `dx` and `dy` properties.
    When the ball hits a brick, the brick is removed, the velocity of the ball increases,
    and the direction of the ball’s movement changes. The code acts on data about
    the game situation.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，球的运动可以通过其 `dx` 和 `dy` 属性来确定。当球击中砖块时，砖块被移除，球的速率增加，球的运动方向改变。代码作用于游戏情况的数据。
- en: Alternatively, consider a financial services scenario. Say you have data about
    the company, such as its current stock price and earnings. By using code like
    that in [Figure 1-2](#ch01_figure_2_1748548870009198), you can calculate a valuable
    ratio called the *price-to-earnings ratio* (or P/E, which stands for price divided
    by earnings).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，考虑一个金融服务场景。比如说，你有一家公司的数据，比如它的当前股价和收益。通过使用[图 1-2](#ch01_figure_2_1748548870009198)中的代码，你可以计算出一种非常有价值的比率，称为*市盈率*（或
    P/E，代表价格除以收益）。
- en: '![](assets/aiml_0102.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0102.png)'
- en: Figure 1-2\. Code in a financial services scenario
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-2\. 金融服务场景中的代码
- en: Your code reads the price, reads the earnings, and returns a value that is the
    former divided by the latter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你的代码读取价格，读取收益，并返回一个值，即前者除以后者。
- en: If I were to try to sum up traditional programming like this in a single diagram,
    it might look like [Figure 1-3](#ch01_figure_3_1748548870009219).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我要尝试用一张图来总结传统编程，它可能看起来像[图 1-3](#ch01_figure_3_1748548870009219)。
- en: '![](assets/aiml_0103.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0103.png)'
- en: Figure 1-3\. High-level view of traditional programming
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-3\. 传统编程的高级视图
- en: As you can see, you have rules expressed in a programming language. These rules
    act on data, and the result is answers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这些规则是用编程语言表达的。这些规则作用于数据，结果是答案。
- en: Limitations of Traditional Programming
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 传统编程的局限性
- en: 'The model from [Figure 1-3](#ch01_figure_3_1748548870009219) has been the backbone
    of development since its inception. But it has an inherent limitation: namely,
    the only scenarios that you can implement are ones for which you can derive rules.
    But what about other scenarios? Usually, it’s unfeasible to develop them because
    the code is too complex. It’s just not possible to write code to handle them.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-3](#ch01_figure_3_1748548870009219)中的模型自从其诞生以来一直是发展的基石。但它有一个固有的局限性：即你只能实现那些可以推导出规则的场景。但其他场景怎么办？通常，由于代码过于复杂，开发它们是不切实际的。编写代码来处理它们是不可能的。'
- en: Consider, for example, activity detection. Fitness monitors that can detect
    our activity are a recent innovation, not just because of the availability of
    cheap and small hardware but also because the algorithms to handle detection weren’t
    previously feasible. Let’s explore why.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以活动检测为例。能够检测我们活动的健身监测器是最近的一项创新，这不仅是因为便宜且小巧的硬件的可用性，还因为处理检测的算法之前是不可行的。让我们来探讨一下原因。
- en: '[Figure 1-4](#ch01_figure_4_1748548870009237) shows a naive activity detection
    algorithm for walking. It can consider the person’s speed and if that speed is
    less than a particular value, we can determine that they are probably walking.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-4](#ch01_figure_4_1748548870009237)显示了一个用于走路的简单活动检测算法。它可以考虑人的速度，如果这个速度低于某个特定值，我们就可以确定他们可能是在走路。'
- en: '![](assets/aiml_0104.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0104.png)'
- en: Figure 1-4\. Algorithm for activity detection
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4。活动检测算法
- en: Given that our data is speed, we could also extend this to detect whether they
    are running, as in [Figure 1-5](#ch01_figure_5_1748548870009255).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们的数据是速度，我们也可以将其扩展到检测他们是否在跑步，就像[图1-5](#ch01_figure_5_1748548870009255)中所示。
- en: '![](assets/aiml_0105.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0105.png)'
- en: Figure 1-5\. Extending the algorithm for running
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-5。扩展跑步算法
- en: As you can see, going by the speed, we might say that if it is less than a particular
    value (say, 4 mph) the person is walking, and otherwise, they are running. It
    still sort of works.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，根据速度，我们可能会说如果速度低于某个特定值（比如说，4英里/小时），那么这个人是在走路，否则，他们是在跑步。这仍然有点效果。
- en: Now, suppose we want to extend this to another popular fitness activity, biking.
    The algorithm could look like the one in [Figure 1-6](#ch01_figure_6_1748548870009270).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们想要将这种方法扩展到另一种流行的健身活动——骑自行车。算法可能看起来像[图1-6](#ch01_figure_6_1748548870009270)中的那样。
- en: '![](assets/aiml_0106.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0106.png)'
- en: Figure 1-6\. Extending the algorithm for biking
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-6。扩展骑自行车算法
- en: I know this algorithm is naive in that it just detects speed—some people run
    faster than others, and you might run downhill faster than you can cycle uphill—but
    on the whole, it still works. However, what happens if we want to implement another
    scenario, such as golfing (see [Figure 1-7](#ch01_figure_7_1748548870009286))?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道这个算法很天真，因为它只是检测速度——有些人跑得比其他人快，你可能会在下坡时跑得比上坡时骑自行车快——但总体来说，它仍然有效。然而，如果我们想要实现另一个场景，比如高尔夫球（见[图1-7](#ch01_figure_7_1748548870009286)），会发生什么呢？
- en: '![How do we write a golfing algorithm?](assets/aiml_0107.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![如何编写一个高尔夫球算法？](assets/aiml_0107.png)'
- en: Figure 1-7\. How do we write a golfing algorithm?
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-7。我们如何编写一个高尔夫球算法？
- en: Now, we’re stuck. Whether a person is golfing or not, they might walk for a
    bit, stop, do some activity, walk for a bit more, stop, etc. So how can we use
    this methodology to tell whether they’re playing golf?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们陷入了困境。无论一个人是否在打高尔夫球，他们可能会走一会儿，停下来，做一些活动，再走一会儿，然后停下来，等等。那么我们如何使用这种方法来判断他们是否在打高尔夫球呢？
- en: Our ability to detect this activity using traditional rules has hit a wall.
    But maybe there’s a better way.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用传统规则检测这种活动的能力已经遇到了瓶颈。但也许有更好的方法。
- en: Enter ML.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 进入机器学习。
- en: From Programming to Learning
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从编程到学习
- en: Let’s look back at the diagram that we used to demonstrate what traditional
    programming is (see [Figure 1-8](#ch01_figure_8_1748548870009301)). Here, we have
    rules that act on data and give us answers. In our activity detection scenario,
    the data was the speed at which the person was moving—and from that, we could
    write rules to detect their activity, be it walking, biking, or running. However,
    we hit a wall when it came to golfing because we couldn’t come up with rules to
    determine what that activity looks like.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们用来演示什么是传统编程的图（见图1-8）。在这里，我们有作用于数据并给出答案的规则。在我们的活动检测场景中，数据是这个人移动的速度——从这个数据中，我们可以编写规则来检测他们的活动，无论是走路、骑自行车还是跑步。然而，当我们遇到高尔夫时遇到了障碍，因为我们无法提出规则来确定这项活动的样子。
- en: '![](assets/aiml_0108.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_0108.png](assets/aiml_0108.png)'
- en: Figure 1-8\. The traditional programming flow
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-8\. 传统编程流程
- en: But what would happen if we were to flip the axes around on this diagram? Instead
    of us coming up with the *rules*, what if we were to come up with the *answers*
    and, along with the data, have a way of figuring out what the rules might be?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们翻转这个图中的坐标轴会怎样呢？如果我们不是提出*规则*，而是提出*答案*，并且与数据一起找到可能规则的途径呢？
- en: '[Figure 1-9](#ch01_figure_9_1748548870009316) shows what this would look like,
    and we can say that this high-level diagram defines *machine learning*.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[图1-9](#ch01_figure_9_1748548870009316)展示了这会是什么样子，我们可以说这个高级图定义了*机器学习*。'
- en: '![](assets/aiml_0109.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![aiml_0109.png](assets/aiml_0109.png)'
- en: Figure 1-9\. Changing the axes to get ML
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-9\. 改变坐标轴以获得机器学习
- en: So, what are the implications of this? Well, now, instead of *us* trying to
    figure out what the rules are, we can get lots of data about our scenario and
    label that data, and then the computer can figure out what the rules are that
    make one piece of data match a particular label and another piece of data match
    a different label.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这有什么影响呢？嗯，现在，我们不再试图找出规则，而是可以收集大量关于我们场景的数据，并标记这些数据，然后计算机可以找出使一块数据与特定标签匹配，另一块数据与不同标签匹配的规则。
- en: How would this work for our activity detection scenario? Well, we can look at
    all the sensors that give us data about this person. If the person has a wearable
    device that detects information such as heart rate, location, and speed—and if
    we collect a lot of instances of this data while they’re doing different activities—then
    we end up with a scenario of having data that says, “This is what walking looks
    like,” “This is what running looks like,” and so on (see [Figure 1-10](#ch01_figure_10_1748548870009331)).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法对我们活动检测场景会有什么作用呢？嗯，我们可以查看所有为我们提供关于这个人数据的传感器。如果这个人有一个可以检测心率、位置和速度等信息的可穿戴设备——并且我们在他们进行不同活动时收集了大量此类数据实例——那么我们最终会得到一个数据场景，它会说：“这就是走路的样子”，“这就是跑步的样子”，等等（见图1-10）。
- en: '![From coding to ML: gathering and labeling data](assets/aiml_0110.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![从编码到机器学习：收集和标记数据](assets/aiml_0110.png)'
- en: 'Figure 1-10\. From coding to ML: gathering and labeling data'
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-10\. 从编码到机器学习：收集和标记数据
- en: Now, our job as programmers changes from figuring out the rules, to determining
    the activities, to writing the code that matches the data to the labels. If we
    can do this, then we can expand the scenarios that we can implement with code.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们作为程序员的任务从找出规则转变为确定活动，再到编写将数据与标签匹配的代码。如果我们能这样做，那么我们就可以扩展我们可以用代码实现的场景。
- en: ML is a technique that enables us to do this, but to get started, we’ll need
    a framework—that’s where TensorFlow enters the picture. In the next section, we’ll
    take a look at what TensorFlow is and how to install it. Then, later in this chapter,
    you’ll write your first code that learns the pattern between two values, like
    in the preceding scenario. It’s a simple “Hello World” scenario, but it has the
    same foundational code pattern that’s used in extremely complex ones.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一种使我们能够做到这一点的技术，但为了开始，我们需要一个框架——这就是TensorFlow的用武之地。在下一节中，我们将探讨TensorFlow是什么以及如何安装它。然后，在本章的后面部分，你将编写你的第一个代码，学习两个值之间的模式，就像前面的场景一样。这是一个简单的“Hello
    World”场景，但它具有在极其复杂场景中使用的相同基础代码模式。
- en: The field of AI is large and abstract, encompassing everything that has to do
    with making computers think and act the way human beings do. One of the ways a
    human takes on new behaviors is through learning by example, and the discipline
    of ML can thus be thought of as an on-ramp to the development of AI. By way of
    an ML field called *computer vision*, a machine can learn to see like a human,
    and by way of another ML field called *natural language processing*, it can learn
    to read text like a human. Many more such applications of ML are possible, and
    we’ll be covering the basics of ML in this book by using the TensorFlow framework.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能领域是庞大且抽象的，包括与使计算机像人类一样思考和行动有关的一切。人类采取新行为的一种方式是通过示例学习，因此机器学习学科可以被视为人工智能发展的入门途径。通过机器学习领域中的
    *计算机视觉*，机器可以学会像人类一样看，通过另一个机器学习领域中的 *自然语言处理*，它可以学会像人类一样阅读文本。还有许多其他可能的机器学习应用，我们将通过使用
    TensorFlow 框架来介绍机器学习的基础知识。
- en: What Is PyTorch?
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是 PyTorch？
- en: PyTorch is an ML library that is based on a previous library called *Torch*,
    which is an open source ML framework and scripting language that is itself based
    on a programming language called Lua. In 2017, development of Torch moved to PyTorch,
    which is a port of the framework in Python.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 是一个基于先前名为 *Torch* 的库的机器学习库，而 *Torch* 本身是一个开源的机器学习框架和脚本语言，它又基于一种名为 Lua
    的编程语言。2017 年，Torch 的发展转移到了 PyTorch，这是一个在 Python 中的框架移植。
- en: So, when installing PyTorch, you’ll often see it referred to as “torch.”
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当安装 PyTorch 时，您通常会看到它被称为“torch”。
- en: PyTorch was originally developed by Meta AI, but it was moved out to the Linux
    Foundation as a way of building developer confidence that it wasn’t made by and
    for a big tech company. It’s one of the two most popular ML libraries, alongside
    the TensorFlow/Keras ecosystem.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 最初由 Meta AI 开发，但为了建立开发者对其不是由大型科技公司制作和使用的信心，它被转移到了 Linux 基金会。它是两个最受欢迎的机器学习库之一，与
    TensorFlow/Keras 生态系统并列。
- en: With the emergence of generative AI, and in particular the “open sourcing” of
    generative text and image models, PyTorch has exploded in popularity. It’s often
    used for training models (which we cover in Part I of this book) as well as for
    inference of models (which we cover in Part II of this book).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 随着生成式人工智能的出现，特别是生成文本和图像模型的“开源”，PyTorch 的受欢迎程度急剧上升。它通常用于训练模型（我们在本书的第一部分中介绍）以及模型的推理（我们在本书的第二部分中介绍）。
- en: 'PyTorch could also be seen as an ecosystem of libraries, each of which is tailored
    to specific scenarios. The important libraries and scenarios to consider are as
    follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 也可以被视为一个库的生态系统，每个库都针对特定场景进行了定制。以下是一些重要的库和需要考虑的场景：
- en: TorchServe
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: TorchServe
- en: This is an easy-to-use tool that lets you deploy PyTorch models at scale. It’s
    designed to run in multiple environments, and it’s generally technology agnostic.
    It supports features such as multimodel serving, logging, metrics, and the easy
    creation of RESTful endpoints that let you do inference on models from a variety
    of clients.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个易于使用的工具，可以让您大规模部署 PyTorch 模型。它设计为在多个环境中运行，并且通常是技术无关的。它支持多模型服务、日志记录、指标以及轻松创建
    RESTful 端点，让您可以从各种客户端对模型进行推理。
- en: Distributed training
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练
- en: When larger models don’t fit onto a single chip or machine, there are technologies
    and techniques that allow you to share them across multiple devices. The `torch.distributed`
    libraries allow you easy and native support of asynchronous execution across multiple
    devices.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当较大的模型无法适应单个芯片或机器时，有一些技术和技术允许您在多个设备之间共享它们。`torch.distributed` 库允许您在多个设备上轻松且本地地支持异步执行。
- en: Mobile
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 移动
- en: An important surface for inference is, of course, mobile. It’s important for
    you to be able to deploy your AI work to Android and iOS devices, and PyTorch
    supports this through PyTorch Mobile.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推理来说，移动设备当然是一个重要的表面。您能够将您的 AI 工作部署到 Android 和 iOS 设备上非常重要，PyTorch 通过 PyTorch
    Mobile 支持这一点。
- en: Pretrained models
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型
- en: An active community of researchers and developers have created a rich ecosystem
    of models that you can simply use with one line of code, wrapped in the torchvision.models
    library.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一群活跃的研究人员和开发者创建了一个丰富的模型生态系统，您只需一行代码即可使用，这些代码被封装在 torchvision.models 库中。
- en: '[Figure 1-11](#fig-1-11) provides a high-level representation of this.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-11](#fig-1-11) 提供了这一概念的高级表示。'
- en: '![](assets/aiml_0111.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0111.png)'
- en: Figure 1-11\. PyTorch ecosystem
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-11\. PyTorch 生态系统
- en: The process of creating ML models is called *training*, and it’s where a computer
    uses a set of algorithms to learn about inputs and what distinguishes them from
    one another. So, for example, if you want a computer to recognize cats and dogs,
    you can use lots of pictures of both to create a model, and the computer will
    use that model to try to figure out what makes a cat a cat and what makes a dog
    a dog. Once the model is trained, the process of having it recognize or categorize
    future inputs is called *inference*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 创建机器学习模型的过程被称为**训练**，这是计算机使用一系列算法来了解输入以及它们之间区别的过程。所以，例如，如果你想让计算机识别猫和狗，你可以使用大量两者的图片来创建一个模型，然后计算机将使用这个模型来试图弄清楚什么使猫成为猫，什么使狗成为狗。一旦模型被训练，让模型识别或分类未来输入的过程被称为**推理**。
- en: 'So, for training models, there are several things that you need to consider,
    and we will cover them in this book. Primarily, your choice will boil down to
    one of three things:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于训练模型，你需要考虑几个方面，我们将在本书中介绍它们。主要来说，你的选择将归结为以下三个方面之一：
- en: Creating the model entirely from scratch yourself
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始自己创建模型
- en: Using someone else’s model because it’s enough for your task
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用他人的模型，因为这对你的任务来说已经足够了
- en: Using parts of another person’s model that have already been trained and building
    on top of them
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用已经训练过的他人模型的部分，并在其基础上构建
- en: The last option on the list is called *transfer learning*, and we’ll cover it
    later in the book.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的最后一个选项被称为**迁移学习**，我们将在本书的后面部分介绍它。
- en: There are many ways to train a model. For the most part, you’ll probably just
    use a single chip, whether it’s a central processing unit (CPU), a graphics processing
    unit (GPU), or something new called a tensor processing unit (TPU). In more advanced
    working and research environments, you can use parallel training across multiple
    chips, employing a distributed training where training is intelligently spanned
    across multiple chips. PyTorch supports this, too, through “distributed training”
    libraries, as shown in [Figure 1-11](#fig-1-11).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型有许多方法。大多数情况下，你可能只会使用单个芯片，无论是中央处理器（CPU）、图形处理器（GPU）还是一种新称为张量处理单元（TPU）的东西。在更高级的工作和研究环境中，你可以使用跨多个芯片的并行训练，采用智能地跨多个芯片的分布式训练。PyTorch
    也支持这一点，通过“分布式训练”库，如[图 1-11](#fig-1-11)所示。
- en: The lifeblood of any model is its data. As we discussed earlier, if you want
    to create a model that can recognize cats and dogs, you need to train it with
    lots of examples of cats and dogs. But how can you manage these examples? Over
    time, you’ll see that this can often involve a lot more coding than the creation
    of the models themselves.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 任何模型的命脉都是其数据。正如我们之前讨论的，如果你想创建一个能够识别猫和狗的模型，你需要用大量猫和狗的例子来训练它。但你怎么管理这些例子呢？随着时间的推移，你会发现这往往比创建模型本身需要更多的编码。
- en: But luckily, the PyTorch ecosystem includes a number of built-in datasets that
    make this easy for you. We will also explore these throughout this book.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 但幸运的是，PyTorch 生态系统包含了许多内置数据集，这使得这一切对你来说变得容易。我们也将在这本书中探讨这些内容。
- en: Beyond creating models, you’ll need to be able to get them into people’s hands
    so they can use them. To this end, PyTorch includes libraries for serving, where
    you can provide model inference over an HTTP connection for cloud or web users.
    For models to run on mobile or embedded systems, there’s PyTorch Mobile, which
    provides tools for model inference on Android and iOS.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 除了创建模型之外，你还需要能够将它们放到人们手中，让他们能够使用它们。为此，PyTorch 包含了用于服务的库，你可以通过 HTTP 连接为云端或网络用户提供模型推理。对于在移动或嵌入式系统上运行的模型，有
    PyTorch Mobile，它提供了在 Android 和 iOS 上进行模型推理的工具。
- en: Next, I’ll show you how to install PyTorch so that you can get started creating
    and using ML models with it!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将向您展示如何安装 PyTorch，这样您就可以开始使用它创建和使用机器学习模型了！
- en: Using PyTorch
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 PyTorch
- en: In this section, we’ll look at the three main ways you can install and use PyTorch.
    We’ll start with how to install it on your developer box using the command line.
    Then, we’ll explore using the popular PyCharm IDE to install and use PyTorch.
    Finally, we’ll look at Google Colab and how you can use it to access your PyTorch
    code with a cloud-based backend in your browser.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨您安装和使用 PyTorch 的三种主要方式。我们将从如何在您的开发箱上使用命令行安装它开始。然后，我们将探讨使用流行的 PyCharm
    IDE 来安装和使用 PyTorch。最后，我们将探讨 Google Colab 以及如何使用它通过浏览器中的基于云的后端访问您的 PyTorch 代码。
- en: Installing Porch in Python
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Python 中安装 Porch
- en: The *Py* in PyTorch stands for Python, so it’s important to have a Python environment
    already set up. If you don’t have Python already, I strongly recommend you visit
    [the Python website](https://python.org) to get up and running with it and [the
    Learn Python website](https://learnpython.org) to learn the Python language syntax.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 中的 *Py* 代表 Python，因此已经设置好 Python 环境是很重要的。如果你还没有 Python，我强烈建议你访问[Python
    网站](https://python.org)来安装它，并访问[学习 Python 网站](https://learnpython.org)来学习 Python
    语言语法。
- en: With Python, there are many ways to install frameworks, but the default one
    supported by the TensorFlow team is `pip`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python，有多种安装框架的方法，但 TensorFlow 团队支持的是 `pip`。
- en: 'So, in your Python environment, installing PyTorch is as easy as using this:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在你的 Python 环境中，安装 PyTorch 和使用以下命令一样简单：
- en: '[PRE0]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Once you’re up and running, you can test your PyTorch version with the following
    code:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你启动并运行，你可以使用以下代码测试你的 PyTorch 版本：
- en: '[PRE1]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You should then see output like that in [Figure 1-12](#ch01_figure_12_1748548870009360).
    It will print the currently running version of PyTorch—here, you can see that
    version 2.4.1 is installed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该会看到[图 1-12](#ch01_figure_12_1748548870009360)中的类似输出。它将打印当前运行的 PyTorch 版本——在这里，你可以看到已安装版本
    2.4.1。
- en: '![](assets/aiml_0112.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0112.png)'
- en: Figure 1-12\. Running PyTorch in Python
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-12\. 在 Python 中运行 PyTorch
- en: Note
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: If you look closely at [Figure 1-12](#ch01_figure_12_1748548870009360), you’ll
    see a note that shows that the torch device is “cpu.” In this case, I natively
    installed it on my Mac, and it is configured to use the CPU. However, this is
    not optimal for complex models, where an accelerator like a GPU or Metal may be
    necessary. We will cover installation of PyTorch for accelerators later in this
    book.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细看[图 1-12](#ch01_figure_12_1748548870009360)，你会看到一个注释显示 torch 设备是“cpu”。在这种情况下，我原生地在我的
    Mac 上安装了它，并且配置为使用 CPU。然而，对于需要 GPU 或 Metal 等加速器的复杂模型来说，这并不是最佳选择。我们将在本书的后面部分介绍为加速器安装
    PyTorch。
- en: Using PyTorch in PyCharm
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 PyCharm 中使用 PyTorch
- en: I’m particularly fond of using the [free community version of PyCharm](https://oreil.ly/I2mP2)
    for building models using PyTorch. PyCharm is useful for many reasons, but one
    of my favorites is that it makes the management of virtual environments easy.
    This means you can have Python environments with versions of tools such as PyTorch
    that are specific to your particular project. So, for example, if you want to
    use PyTorch 1.x in one project and PyTorch 2.x in another, you can separate them
    with virtual environments and not have to deal with installing/uninstalling dependencies
    when you switch between them. Additionally, with PyCharm, you can do step-by-step
    debugging of your Python code—which is a must, especially if you’re just getting
    started!
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我特别喜欢使用[PyCharm 的免费社区版](https://oreil.ly/I2mP2)来构建使用 PyTorch 的模型。PyCharm 有很多用途，但我最喜欢的是它使虚拟环境的管理变得容易。这意味着你可以拥有针对特定项目的
    Python 环境，其中包含特定于项目的工具版本，例如 PyTorch。例如，如果你想在一个项目中使用 PyTorch 1.x，在另一个项目中使用 PyTorch
    2.x，你可以通过虚拟环境将它们分开，在切换时不需要处理安装/卸载依赖项。此外，使用 PyCharm，你可以逐步调试你的 Python 代码——这对于初学者来说尤其重要！
- en: For example, in [Figure 1-13](#ch01_figure_13_1748548870009375), I have a new
    project that’s called *example1*, and I’m specifying that I’m going to create
    a new environment using Conda. When I create the project, I’ll have a clean, new,
    virtual Python environment into which I can install any version of PyTorch I want.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在[图 1-13](#ch01_figure_13_1748548870009375)中，我有一个名为 *example1* 的新项目，我指定我将使用
    Conda 创建一个新环境。当我创建项目时，我将拥有一个干净、新的虚拟 Python 环境，我可以安装任何版本的 PyTorch。
- en: 'Once you’ve created a project, you can open the File → Settings dialog and
    choose the entry for “Project: *<your project name>*” from the menu on the left.
    In the menu on the left, you’ll see choices to change the settings for the Python
    Interpreter and the Project Structure. If you choose the Python Interpreter link,
    you’ll see the interpreter that you’re using, as well as a list of packages that
    are installed in this virtual environment (see [Figure 1-14](#ch01_figure_14_1748548870009389)).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你创建了一个项目，你可以打开文件 → 设置对话框，并从左侧菜单中选择“项目： *<你的项目名称>*”。在左侧菜单中，你会看到更改 Python 解释器和项目结构的选项。如果你选择
    Python 解释器链接，你会看到你正在使用的解释器，以及在这个虚拟环境中安装的包列表（见[图 1-14](#ch01_figure_14_1748548870009389)）。
- en: '![](assets/aiml_0113.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0113.png)'
- en: Figure 1-13\. Creating a new virtual environment using PyCharm
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-13\. 使用 PyCharm 创建新的虚拟环境
- en: '![](assets/aiml_0114.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0114.png)'
- en: Figure 1-14\. Adding packages to a virtual environment
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-14\. 向虚拟环境添加包
- en: You can then click the + button at the upper left, and a dialog will open showing
    the packages that are currently available. Type **`torch`** into the search box
    and you’ll see all available packages with *torch* in the name (see [Figure 1-15](#ch01_figure_15_1748548870009404)).
    Remember that the name of the package is *torch*, even if the technology is PyTorch.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以点击左上角的“+”按钮，将打开一个对话框，显示当前可用的包。在搜索框中输入**`torch`**，你将看到所有名称中包含 *torch* 的可用包（见[图
    1-15](#ch01_figure_15_1748548870009404)）。记住，包的名称是 *torch*，即使技术是 PyTorch。
- en: '![](assets/aiml_0115.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0115.png)'
- en: Figure 1-15\. Installing torch with PyCharm
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-15\. 使用 PyCharm 安装 torch
- en: Once you’ve selected torch or any other package you want to install, you can
    click the Install Package button and PyCharm will do the rest. Then, once torch
    is installed, you can write and debug your PyTorch code in Python.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你选择了 torch 或你想要安装的任何其他包，你可以点击“安装包”按钮，PyCharm 将完成剩余的工作。然后，一旦安装了 torch，你就可以在
    Python 中编写和调试你的 PyTorch 代码。
- en: Using PyTorch in Google Colab
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Google Colab 中使用 PyTorch
- en: Another option, perhaps the easiest one for getting started, is to use [*Google
    Colab*](https://oreil.ly/c0lab), which is a hosted Python environment that you
    can access via a browser. What’s really neat about Colab is that it provides GPU
    and TPU backends so you can train models using state-of-the-art hardware at no
    cost.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项，可能是入门最容易的方法，是使用 [*Google Colab*](https://oreil.ly/c0lab)，这是一个可以通过浏览器访问的托管
    Python 环境。Colab 真是 neat 的地方在于它提供了 GPU 和 TPU 后端，这样你就可以免费使用最先进的硬件来训练模型。
- en: When you visit the Colab website, you’ll be given the option to open previous
    Colabs or start a new notebook (see [Figure 1-16](#ch01_figure_16_1748548870009418)).
    If you click the + New notebook button, it will open the editor, where you can
    add panes of code or text (see [Figure 1-17](#ch01_figure_17_1748548870009451)).
    You ⁠ can then execute the code by clicking the Play button (the arrow) to the
    left of the pane.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当你访问 Colab 网站，你会得到打开之前的 Colabs 或创建一个新的笔记本的选项（见[图 1-16](#ch01_figure_16_1748548870009418)）。如果你点击“+
    新笔记本”按钮，它将打开编辑器，在那里你可以添加代码或文本面板（见[图 1-17](#ch01_figure_17_1748548870009451)）。然后你可以通过点击面板左侧的播放按钮（箭头）来执行代码。
- en: '![](assets/aiml_0116.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0116.png)'
- en: Figure 1-16\. Getting started with Google Colab
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-16\. Google Colab 入门
- en: '![](assets/aiml_0117.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0117.png)'
- en: Figure 1-17\. Running PyTorch code in Colab
  id: totrans-112
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-17\. 在 Colab 中运行 PyTorch 代码
- en: It’s always a good idea to check the PyTorch version, as shown here, to be sure
    you’re running the correct version for the task at hand.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 总是检查 PyTorch 版本是个好主意，就像这里所示，以确保你正在运行适合当前任务的正确版本。
- en: You can also see that in [Figure 1-17](#ch01_figure_17_1748548870009451) the
    version shown is 2.4.1+cu121, and you might want to know what the *cu121* part
    is! The *cu* stands for *Cuda*, which is Nvidia’s library for accelerated ML on
    GPUs. So, the preceding message demonstrates that PyTorch 2.4.1 is installed,
    along with accelerators for Cuda version 12.1.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以看到在[图 1-17](#ch01_figure_17_1748548870009451)中显示的版本是 2.4.1+cu121，你可能想知道
    *cu121* 部分是什么！*cu* 代表 *Cuda*，这是 Nvidia 用于在 GPU 上加速机器学习的库。因此，前面的消息表明已安装 PyTorch
    2.4.1，以及适用于 Cuda 版本 12.1 的加速器。
- en: 'Often, Colab’s built-in versions of various libraries, including PyTorch, will
    be a version or two behind the latest release. If that’s the case, you can update
    it with `pip install` as shown earlier, by simply using a block of code like this,
    where you specify the desired version:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，Colab 内置的各种库的版本，包括 PyTorch，可能比最新版本落后一个或两个版本。如果情况如此，你可以像前面所示的那样使用 `pip install`
    更新它，只需使用一个这样的代码块，其中指定了所需的版本：
- en: '[PRE2]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once you run this command, your current environment within Colab will use the
    desired version of PyTorch. However, you should be careful when doing this in
    Colab because the version of PyTorch you change to may not have Cuda drivers installed,
    meaning you could downgrade to using the CPU.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行此命令，你的 Colab 当前环境将使用所需的 PyTorch 版本。然而，在 Colab 中进行此操作时你应该小心，因为你更改到的 PyTorch
    版本可能没有安装 Cuda 驱动程序，这意味着你可能会降级到使用 CPU。
- en: Getting Started with Machine Learning
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习入门
- en: As we saw earlier in the chapter, the ML paradigm is one in which you have data,
    that data is labeled, and you want to figure out the rules that match the data
    to the labels. The simplest possible scenario to show this in code is as follows.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面所述，机器学习范式是这样的：你拥有数据，这些数据被标记，而你想要找出匹配数据和标签的规则。在代码中展示这一点的最简单场景如下。
- en: 'Consider these two sets of numbers:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这两组数字：
- en: '[PRE3]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: There’s a relationship between the *x* and *y* values (for example, if *x* is
    –1, then *y* is –3; if *x* is 3, then *y* is 5; and so on). Can you see it?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '*x* 和 *y* 值之间存在关系（例如，如果 *x* 是 –1，那么 *y* 是 –3；如果 *x* 是 3，那么 *y* 是 5；等等）。你能看到这个关系吗？'
- en: After a few seconds, you probably saw that the pattern here is *y* = 2*x* –
    1\. How did you get that? Different people work it out in different ways, but
    I typically hear the observation that *x* increases by 1 in its sequence and *y*
    increases by 2; thus, *y* = 2*x* +/– something. Then, they look at when *x* =
    0 and see that *y* = –1, so they figure that the answer could be *y* = 2*x* –
    1\. Next, they look at the other values and see that this hypothesis “fits,” and
    the answer is *y* = 2*x* – 1.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，你可能看到了这里的模式是 *y* = 2*x* – 1。你是怎么得到这个结果的？不同的人以不同的方式解决这个问题，但通常我听到这样的观察：*x*
    在其序列中增加1，而 *y* 增加2；因此，*y* = 2*x* +/– 某个值。然后，他们看到当 *x* = 0 时，*y* = –1，所以他们推断答案可能是
    *y* = 2*x* – 1。接下来，他们查看其他值，并看到这个假设“符合”，答案是 *y* = 2*x* – 1。
- en: That’s very similar to the ML process. Let’s take a look at some code that you
    could write to have a neural network figure this out for you.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常类似于机器学习的过程。让我们看看你可以编写的代码，让神经网络为你解决这个问题。
- en: 'Here’s the full code, using PyTorch. Don’t worry if it doesn’t make sense yet;
    we’ll go through it line by line:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这是完整的代码，使用 PyTorch。如果你现在还不理解没关系；我们会逐行讲解：
- en: '[PRE4]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The first few lines are the importers that ensure the correct libraries are
    available, so let’s jump to this line:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 前几行是导入器，确保正确的库可用，所以我们跳到这一行：
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You’ve probably heard of neural networks, and you’ve probably seen diagrams
    that explain them by using layers of interconnected neurons, a little like in
    [Figure 1-18](#ch01_figure_18_1748548870009467).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能听说过神经网络，你可能见过使用相互连接的神经元层来解释它们的图表，就像[图1-18](#ch01_figure_18_1748548870009467)中那样。
- en: '![](assets/aiml_0118.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图1-18](assets/aiml_0118.png)'
- en: Figure 1-18\. A typical neural network
  id: totrans-131
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-18。一个典型的神经网络
- en: 'When you see a neural network like this, you should consider each of the circles
    to be a *neuron* and each of the columns of circles to be a *layer*. So, in [Figure 1-18](#ch01_figure_18_1748548870009467),
    there are three layers: the first has five neurons, the second has four, and the
    third has two.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看到这样的神经网络时，你应该把每个圆圈看作是一个*神经元*，每个圆圈的列看作是一个*层*。所以，在[图1-18](#ch01_figure_18_1748548870009467)中，有三个层：第一个有五个神经元，第二个有四个，第三个有两个。
- en: These layers are organized in a sequence through which the data flows from left
    to right.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层按照数据从左到右流动的顺序组织。
- en: 'Now, if we look back at our code, you’ll see that we’re defining a sequence
    of something, with what’s contained in the brackets being the definition of the
    sequence:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们回顾我们的代码，你会看到我们正在定义一个序列，括号内包含的是序列的定义：
- en: '[PRE6]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When using PyTorch, you define your layers by using a `Sequential`, and inside
    the `Sequential`, you then specify what each layer looks like. We have only one
    line inside our `Sequential`, so the neural network this code defines will have
    only one layer.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 PyTorch 时，你通过使用 `Sequential` 来定义你的层，然后在 `Sequential` 内部指定每个层的样子。我们 `Sequential`
    内部只有一行，所以这个代码定义的神经网络将只有一个层。
- en: Then, you define what the layer looks like by using the `torch.nn` libraries.
    There are lots of different layer types, but here, we’re using a `Linear` layer,
    in which a linear relationship (where the definition of a line is *y* = *wx* +
    *b*) can be defined or learned.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你通过使用 `torch.nn` 库来定义层的样子。有很多不同的层类型，但在这里，我们使用的是 `Linear` 层，其中可以定义或学习线性关系（其中线的定义是
    *y* = *wx* + *b*）。
- en: Our `Linear` layer has the (1,1) parameters specified, which indicates one feature
    “in” and one feature “out.” So ultimately, we have just one layer with one neuron
    in our entire neural network.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `Linear` 层指定了 (1,1) 参数，这表示一个特征“输入”和一个特征“输出”。所以最终，我们整个神经网络中只有一个层，包含一个神经元。
- en: In other words, the Sequential containing a Linear with the parameters (1,1)
    ultimately looks like [Figure 1-19](#ch01_figure_19_1748548870009483).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，包含参数 (1,1) 的 Sequential 最终看起来像[图1-19](#ch01_figure_19_1748548870009483)。
- en: '![](assets/aiml_0119.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图1-19](assets/aiml_0119.png)'
- en: Figure 1-19\. A neural network with one layer, containing one neuron
  id: totrans-141
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-19。一个包含一个神经元的单层神经网络
- en: 'The next lines are where the fun really begins. Let’s look at them again:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的几行是真正有趣的地方。让我们再看看它们：
- en: '[PRE7]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you’ve done anything with ML before, you’ve probably seen that it involves
    a lot of mathematics—and if you haven’t done calculus in years, it might have
    seemed like a barrier to entry. Here’s the part where the math comes in—it’s the
    core of ML.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前做过任何机器学习（ML）的事情，你可能已经看到它涉及大量的数学——如果你已经很多年没有学过微积分，它可能看起来像是一个入门的障碍。这里就是数学发挥作用的地方——它是机器学习的核心。
- en: In a scenario such as this one, the computer has *no idea* what the relationship
    between *x* and *y* is. So, it will make a guess. Say, for example, it guesses
    that *y* = 10*x* + 10\. Then, it needs to measure how good or how bad that guess
    is—and that’s the job of the *loss function*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种场景中，计算机对*x*和*y*之间的关系一无所知。因此，它会做出猜测。比如说，它猜测*y* = 10*x* + 10。然后，它需要衡量这个猜测是好是坏——这是*损失函数*的工作。
- en: The computer already knows the answers when *x* is –1, 0, 1, 2, 3, and 4, so
    the loss function can compare these to the answers for the guessed relationship.
    If it guessed *y* = 10*x* + 10, then when *x* is –1, *y* will be 0\. However,
    the correct answer there was –3, so it’s a bit off. But when *x* is 4, the guessed
    answer is 50, whereas the correct one is 7\. That’s really far off.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当*x*为-1、0、1、2、3和4时，计算机已经知道答案，因此损失函数可以将这些与猜测关系的答案进行比较。如果它猜测*y* = 10*x* + 10，那么当*x*为-1时，*y*将是0。然而，正确的答案那里是-3，所以它有点偏离。但当*x*为4时，猜测的答案是50，而正确的答案是7。这真的很远。
- en: Armed with this knowledge, the computer can then make another guess. That’s
    the job of the *optimizer*. This is where the heavy calculus is used, but with
    PyTorch, that can be hidden from you. You just pick the appropriate optimizer
    to use for different scenarios. In this case, we picked one called `sgd`, which
    stands for *stochastic gradient descent*—a complex mathematical function that,
    when given the values, the previous guess, and the results of calculating the
    errors (or loss) on that guess, can then generate another guess. Over time, its
    job is to minimize the loss, and by doing so bring the guessed formula closer
    and closer to the correct answer.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 带着这些知识，计算机可以再次做出猜测。这是*优化器*的工作。这里使用了大量的微积分，但使用PyTorch，这可以对你隐藏起来。你只需选择适用于不同场景的适当优化器。在这种情况下，我们选择了一个名为`sgd`的优化器，代表*随机梯度下降*——一个复杂的数学函数，当给定值、前一个猜测以及在该猜测上计算错误（或损失）的结果时，可以生成另一个猜测。随着时间的推移，它的任务是使损失最小化，并通过这样做使猜测的公式越来越接近正确答案。
- en: 'Next, we simply format our numbers into the data format that the layers expect:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们只需将我们的数字格式化为层所期望的数据格式：
- en: '[PRE8]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You’ll see the word *tensor* a lot in ML; it gives the TensorFlow framework
    its name. Think of a tensor as a way of storing data that’s like an array that
    is optimized for flexibility in array size. To have PyTorch understand our data,
    we will load the values into tensors representing the *x* and *y* values.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习（ML）中，你会经常看到“张量”这个词；它给了TensorFlow框架它的名字。想象一下张量就像是一种存储数据的方式，它类似于数组，但为了适应数组大小的灵活性进行了优化。为了让PyTorch理解我们的数据，我们将值加载到表示*x*和*y*值的张量中。
- en: 'The *learning* process will then begin with the training loop like this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，学习（learning）过程将以这样的训练循环开始：
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you’re new to ML, this is probably the most difficult part to understand,
    so let’s go through it line by line.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你刚开始接触机器学习（ML），这可能是最难理解的部分，所以让我们逐行分析。
- en: Remember that the ML process looks like [Figure 1-20](#ch01_figure_20_1748548870009497).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，机器学习（ML）的过程看起来像[图1-20](#ch01_figure_20_1748548870009497)。
- en: '![](assets/aiml_0120.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/aiml_0120.png)'
- en: Figure 1-20\. The ML process
  id: totrans-156
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-20。机器学习过程
- en: 'So, the preceding code implements this as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，前面的代码是这样实现的：
- en: '[PRE10]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This line reads as “zero the gradients” of the optimizer. The calculus of learning
    involves navigating down a curve to find its minimum, and to do that, we need
    the gradient of the curve. The curve is calculated when we measure our accuracy,
    so we need to reset it at the beginning of each loop:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码的意思是“将优化器的梯度置零”。学习的微积分涉及沿着曲线导航以找到其最小值，而要做到这一点，我们需要曲线的梯度。曲线是在我们测量准确度时计算的，因此我们需要在每个循环的开始时重置它：
- en: '[PRE11]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This line creates an array of the outputs that we calculate for the input *x*
    values. Even though we have given the computer the *correct* answers in our *Y*
    array, we want to measure the accuracy of the guess that the computer has made
    for the parameters defining this line. The first time through the loop, the *w*
    and *b* parameters within the neuron will be randomly initialized, so our guess
    might be *Y* = 10*x* + 10, for example:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码创建了一个数组，其中包含了我们对输入 *x* 值计算得出的输出。尽管我们在 *Y* 数组中给出了计算机的 *正确* 答案，但我们想衡量计算机对定义此线的参数做出的猜测的准确性。在循环的第一次迭代中，神经元内的
    *w* 和 *b* 参数将被随机初始化，因此我们的猜测可能是 *Y* = 10*x* + 10，例如：
- en: '[PRE12]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This line then compares the outputs (aka our guesses) with the correct answers
    to calculate the *loss*—which is effectively a value that tells us how good or
    bad the guess is:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码随后将输出（即我们的猜测）与正确答案进行比较，以计算 *损失*——这实际上是一个告诉我们猜测有多好或多坏的价值：
- en: '[PRE13]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This line is the essential part of the learning process where a process called
    *backpropagation* happens. It’s where the math from the optimizer and the loss
    function combine to figure out the gradients for the new set of parameters. In
    our case, the error from *Y* = 10*x* + 10 is really high and not even close to
    our desired values, so the calculations done in figuring out the loss will give
    us a *direction* or gradient in which we should go to get closer to our desired
    results:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这一行是学习过程中的关键部分，其中发生了一个称为 *反向传播* 的过程。这是优化器中的数学和损失函数结合来确定新参数集的梯度的位置。在我们的例子中，*Y*
    = 10*x* + 10 的误差非常高，甚至远未达到我们期望的值，因此确定损失的计算将给我们一个 *方向* 或梯度，告诉我们如何接近我们期望的结果：
- en: '[PRE14]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This line of code finishes the job by updating the model parameters to the values
    based on the gradients calculated in the preceding backpropagation step.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码通过更新模型参数到先前反向传播步骤中计算的梯度值来完成工作。
- en: We then repeat this process five hundred times, with the goal of finding a set
    of parameters for our single neuron that will give us *y* values that are close
    to our desired *y* values. If the set of parameters does so, it can then infer
    the *y* value for *x* values that the computer has never previously seen. Thus,
    it will have learned the relationship between the *x* and *y* values we provided.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们重复这个过程 500 次，目标是找到一组参数，使得单个神经元的 *y* 值接近我们期望的 *y* 值。如果这组参数做到了，它就可以推断出计算机从未见过的
    *x* 值的 *y* 值。因此，它将学习我们提供的 *x* 和 *y* 值之间的关系。
- en: '[Figure 1-21](#ch01_figure_21_1748548870009510) shows a screenshot of this
    running in a Colab notebook. Take a look at the loss values over time.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-21](#ch01_figure_21_1748548870009510) 展示了在 Colab 笔记本中运行的截图。看看损失值随时间的变化。'
- en: '![](assets/aiml_0121.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0121.png)'
- en: Figure 1-21\. Training the neural network
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-21\. 训练神经网络
- en: We can see that over the first 10 epochs, the loss went from 5.64 to 0.86\.
    That is, after only 10 tries, the network was performing about six times better
    than with its initial guess.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在前 10 个时期，损失从 5.64 降至 0.86。也就是说，经过仅仅 10 次尝试，网络的表现比其初始猜测好了大约六倍。
- en: Then take a look at what happens by the 500th epoch (see [Figure 1-22](#ch01_figure_22_1748548870009531)).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，看看在第 500 个时期发生的情况（见 [图 1-22](#ch01_figure_22_1748548870009531)）。
- en: '![](assets/aiml_0122.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/aiml_0122.png)'
- en: Figure 1-22\. Training the neural network—the last few epochs
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-22\. 训练神经网络——最后几个时期
- en: We can now see that the loss is 9.52 × 10^(-6). The loss has gotten so small
    that the model has pretty much figured out that the relationship between the numbers
    is *y* = 2*x* – 1\. This means that the *machine* has *learned* the pattern between
    them.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到损失是 9.52 × 10^(-6)。损失已经变得如此之小，以至于模型几乎已经确定出数字之间的关系是 *y* = 2*x* – 1。这意味着
    *机器* 已经 *学习* 到它们之间的模式。
- en: 'If we want our neural network to try to predict a new value, we can use code
    like this:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想让我们的神经网络尝试预测一个新值，我们可以使用如下代码：
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-179
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The term *prediction* is typically used when dealing with ML models—but don’t
    think of it as looking into the future! We use this term because we’re dealing
    with a certain amount of uncertainty. Think back to the activity detection scenario
    we spoke about earlier. When the person was moving at a certain speed, she was
    *probably* walking. Similarly, when a model learns about the patterns that exist
    between two things, it will tell us what the answer *probably* is. In other words,
    it is *predicting* the answer. (Later, you’ll also learn about *inference*, in
    which the model picks one answer among many and *infers* that it has picked the
    correct one.)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 *预测* 通常用于处理机器学习模型时——但不要把它想成是看未来！我们使用这个术语是因为我们处理了一定程度的不确定性。回想一下我们之前提到的活动检测场景。当这个人以一定的速度移动时，她很可能是走路。同样，当模型学习到两个事物之间存在的模式时，它会告诉我们答案很可能是怎样的。换句话说，它是在
    *预测* 答案。（稍后，你还将了解到 *推理*，其中模型从多个答案中选择一个，并 *推断* 它已经选择了正确的答案。）
- en: What do you think the answer will be when we ask the model to predict *y* when
    *x* is 10? You might instantly think 19, but that’s not correct. The model will
    pick a value *very close* to 19, and there are several reasons for this. First
    of all, our loss wasn’t 0\. It was a very small amount, so we should expect any
    predicted answer to be off by a very small amount. Second, the neural network
    is trained on only a small amount of data—and in this case, it’s only six pairs
    of (*x*, *y*) values.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们要求模型预测当 *x* 为 10 时的 *y* 时，你认为答案会是什么？你可能立刻想到 19，但这并不正确。模型将选择一个非常接近 19 的值，这有几个原因。首先，我们的损失不是
    0。它是一个非常小的数值，因此我们应该预期任何预测答案都会有一个非常小的误差。其次，神经网络仅在一小部分数据上进行了训练——在这个例子中，它只有六对 (*x*,
    *y*) 值。
- en: The model only has a single neuron in it, and that neuron learns a *weight*
    and a *bias* so that *y* = *wx* + *b*. This looks exactly like the desired *y*
    = 2*x* – 1 relationship, in which we want the model to learn that *w* = 2 and
    *b* = –1\. Given that the model was trained on only six items of data, we’d never
    expect the answer to be exactly these values; instead, we’d expect it to be something
    very close to them.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 模型中只有一个神经元，该神经元学习一个 *权重* 和一个 *偏差*，使得 *y* = *wx* + *b*。这看起来与期望的 *y* = 2*x* –
    1 关系完全一样，我们希望模型学习到 *w* = 2 和 *b* = –1。鉴于模型仅基于六个数据项进行训练，我们永远不会期望答案会恰好是这些值；相反，我们期望它非常接近这些值。
- en: 'Now, run the code for yourself to see what you get. I got 18.991 when I ran
    it, but your answer may differ slightly because when the neural network is first
    initialized, there’s a random element: your initial guess will be slightly different
    from mine and from a third person’s.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，亲自运行代码看看你得到什么结果。我运行时得到了 18.991，但你的答案可能略有不同，因为当神经网络首次初始化时，存在随机元素：你的初始猜测将略不同于我的，也不同于第三个人的。
- en: Seeing What the Network Learned
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 看看网络学到了什么
- en: This is obviously a very simple scenario in which we are matching *x’*s to *y’*s
    in a linear relationship. As mentioned in the previous section, neurons have weight
    and bias parameters. That makes a single neuron fine for learning a relationship
    like this; namely, when *y* = 2*x* – 1, the weight is 2 and the bias is –1.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这显然是一个非常简单的场景，其中我们在线性关系中匹配 *x’* 和 *y’*。如前一小节所述，神经元有权重和偏差参数。这使得单个神经元非常适合学习这种关系；即当
    *y* = 2*x* – 1 时，权重是 2，偏差是 –1。
- en: 'With PyTorch, we can actually take a look at the weights and biases that are
    learned, with code like this:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyTorch，我们实际上可以查看学习到的权重和偏差，如下面的代码所示：
- en: '[PRE16]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once the network finishes learning, you can print out the values (or weights)
    that the layer learned. In my case, the output was as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网络完成学习，你可以打印出该层学习到的值（或权重）。在我的情况下，输出如下：
- en: '[PRE17]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Thus, the learned relationship between *x* and *y* was *y* = 1.998695 *x* –
    0.9959542.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，学习到的 *x* 和 *y* 之间的关系是 *y* = 1.998695 *x* – 0.9959542。
- en: This is pretty close to what we’d expect (*y* = 2*x* – 1), and we could argue
    that it’s even closer to reality because we’re *assuming* that the relationship
    will hold for other values!
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常接近我们预期的 (*y* = 2*x* – 1)，我们甚至可以说它更接近现实，因为我们 *假设* 这种关系将适用于其他值！
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: That’s it for your first “Hello World” of ML. You might be thinking that this
    seems like massive overkill for something as simple as determining a linear relationship
    between two values—and you’d be right. But the cool thing about this is that the
    pattern of code we’ve created here is the same pattern that’s used for far more
    complex scenarios. You’ll see those scenarios starting in [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080),
    where we’ll explore some basic computer vision techniques—in which the machine
    will learn to “see” patterns in pictures and identify what’s in them!
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是你的第一个机器学习“Hello World”。你可能觉得这好像是对如此简单的事情——确定两个值之间的线性关系——过度杀鸡用牛刀——你是对的。但有趣的是，我们在这里创建的代码模式与用于更复杂场景的模式是相同的。你将在[第二章](ch02.html#ch02_introduction_to_computer_vision_1748548889076080)中看到这些场景，我们将探讨一些基本的计算机视觉技术——在这些技术中，机器将学会“看到”图片中的模式并识别其中的内容！
