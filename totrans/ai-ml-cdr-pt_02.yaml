- en: Chapter 1\. Introduction to PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to creating artificial intelligence (AI), machine learning (ML)
    and deep learning are great places to begin. When you’re getting started, however,
    it’s easy to get overwhelmed by the options and all the new terminology. This
    book aims to demystify things for you as a programmer. It takes you through writing
    code to implement concepts of ML and deep learning, and it also takes you through
    building models that behave more as a human does, with scenarios like computer
    vision, natural language processing (NLP), and more. Thus, these models become
    a form of synthesized, or artificial, intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: But when we refer to *machine learning*, what exactly is it? Let’s take a quick
    look at that and consider it from a programmer’s perspective before we go any
    further. After that, in the rest of this chapter, we’ll show you how to install
    the tools of the trade, from PyTorch itself to environments where you can code
    and debug your PyTorch-based models.
  prefs: []
  type: TYPE_NORMAL
- en: What Is Machine Learning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we get into the ins and outs of ML, let’s consider how it evolved from
    traditional programming. We’ll start by examining what traditional programming
    is, and then we’ll consider cases where it’s limited. After that, we’ll see how
    ML evolved to handle those cases and thus opened up new opportunities to implement
    new scenarios, thereby unlocking many of the concepts of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional programming involves writing rules that are expressed in a programming
    language and that act on data and give us answers. This applies just about everywhere
    we can program something with code.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a game like the popular Breakout. Code determines the
    movement of the ball, the score, and the various conditions for winning or losing
    the game. Think about the scenario where the ball bounces off a brick, like in
    [Figure 1-1](#ch01_figure_1_1748548870009162).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-1\. Code in a Breakout game
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, the motion of the ball can be determined by its `dx` and `dy` properties.
    When the ball hits a brick, the brick is removed, the velocity of the ball increases,
    and the direction of the ball’s movement changes. The code acts on data about
    the game situation.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, consider a financial services scenario. Say you have data about
    the company, such as its current stock price and earnings. By using code like
    that in [Figure 1-2](#ch01_figure_2_1748548870009198), you can calculate a valuable
    ratio called the *price-to-earnings ratio* (or P/E, which stands for price divided
    by earnings).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-2\. Code in a financial services scenario
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Your code reads the price, reads the earnings, and returns a value that is the
    former divided by the latter.
  prefs: []
  type: TYPE_NORMAL
- en: If I were to try to sum up traditional programming like this in a single diagram,
    it might look like [Figure 1-3](#ch01_figure_3_1748548870009219).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0103.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-3\. High-level view of traditional programming
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, you have rules expressed in a programming language. These rules
    act on data, and the result is answers.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of Traditional Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The model from [Figure 1-3](#ch01_figure_3_1748548870009219) has been the backbone
    of development since its inception. But it has an inherent limitation: namely,
    the only scenarios that you can implement are ones for which you can derive rules.
    But what about other scenarios? Usually, it’s unfeasible to develop them because
    the code is too complex. It’s just not possible to write code to handle them.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider, for example, activity detection. Fitness monitors that can detect
    our activity are a recent innovation, not just because of the availability of
    cheap and small hardware but also because the algorithms to handle detection weren’t
    previously feasible. Let’s explore why.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-4](#ch01_figure_4_1748548870009237) shows a naive activity detection
    algorithm for walking. It can consider the person’s speed and if that speed is
    less than a particular value, we can determine that they are probably walking.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0104.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-4\. Algorithm for activity detection
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given that our data is speed, we could also extend this to detect whether they
    are running, as in [Figure 1-5](#ch01_figure_5_1748548870009255).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0105.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-5\. Extending the algorithm for running
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, going by the speed, we might say that if it is less than a particular
    value (say, 4 mph) the person is walking, and otherwise, they are running. It
    still sort of works.
  prefs: []
  type: TYPE_NORMAL
- en: Now, suppose we want to extend this to another popular fitness activity, biking.
    The algorithm could look like the one in [Figure 1-6](#ch01_figure_6_1748548870009270).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0106.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-6\. Extending the algorithm for biking
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I know this algorithm is naive in that it just detects speed—some people run
    faster than others, and you might run downhill faster than you can cycle uphill—but
    on the whole, it still works. However, what happens if we want to implement another
    scenario, such as golfing (see [Figure 1-7](#ch01_figure_7_1748548870009286))?
  prefs: []
  type: TYPE_NORMAL
- en: '![How do we write a golfing algorithm?](assets/aiml_0107.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-7\. How do we write a golfing algorithm?
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, we’re stuck. Whether a person is golfing or not, they might walk for a
    bit, stop, do some activity, walk for a bit more, stop, etc. So how can we use
    this methodology to tell whether they’re playing golf?
  prefs: []
  type: TYPE_NORMAL
- en: Our ability to detect this activity using traditional rules has hit a wall.
    But maybe there’s a better way.
  prefs: []
  type: TYPE_NORMAL
- en: Enter ML.
  prefs: []
  type: TYPE_NORMAL
- en: From Programming to Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s look back at the diagram that we used to demonstrate what traditional
    programming is (see [Figure 1-8](#ch01_figure_8_1748548870009301)). Here, we have
    rules that act on data and give us answers. In our activity detection scenario,
    the data was the speed at which the person was moving—and from that, we could
    write rules to detect their activity, be it walking, biking, or running. However,
    we hit a wall when it came to golfing because we couldn’t come up with rules to
    determine what that activity looks like.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0108.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-8\. The traditional programming flow
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: But what would happen if we were to flip the axes around on this diagram? Instead
    of us coming up with the *rules*, what if we were to come up with the *answers*
    and, along with the data, have a way of figuring out what the rules might be?
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-9](#ch01_figure_9_1748548870009316) shows what this would look like,
    and we can say that this high-level diagram defines *machine learning*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0109.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-9\. Changing the axes to get ML
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: So, what are the implications of this? Well, now, instead of *us* trying to
    figure out what the rules are, we can get lots of data about our scenario and
    label that data, and then the computer can figure out what the rules are that
    make one piece of data match a particular label and another piece of data match
    a different label.
  prefs: []
  type: TYPE_NORMAL
- en: How would this work for our activity detection scenario? Well, we can look at
    all the sensors that give us data about this person. If the person has a wearable
    device that detects information such as heart rate, location, and speed—and if
    we collect a lot of instances of this data while they’re doing different activities—then
    we end up with a scenario of having data that says, “This is what walking looks
    like,” “This is what running looks like,” and so on (see [Figure 1-10](#ch01_figure_10_1748548870009331)).
  prefs: []
  type: TYPE_NORMAL
- en: '![From coding to ML: gathering and labeling data](assets/aiml_0110.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1-10\. From coding to ML: gathering and labeling data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, our job as programmers changes from figuring out the rules, to determining
    the activities, to writing the code that matches the data to the labels. If we
    can do this, then we can expand the scenarios that we can implement with code.
  prefs: []
  type: TYPE_NORMAL
- en: ML is a technique that enables us to do this, but to get started, we’ll need
    a framework—that’s where TensorFlow enters the picture. In the next section, we’ll
    take a look at what TensorFlow is and how to install it. Then, later in this chapter,
    you’ll write your first code that learns the pattern between two values, like
    in the preceding scenario. It’s a simple “Hello World” scenario, but it has the
    same foundational code pattern that’s used in extremely complex ones.
  prefs: []
  type: TYPE_NORMAL
- en: The field of AI is large and abstract, encompassing everything that has to do
    with making computers think and act the way human beings do. One of the ways a
    human takes on new behaviors is through learning by example, and the discipline
    of ML can thus be thought of as an on-ramp to the development of AI. By way of
    an ML field called *computer vision*, a machine can learn to see like a human,
    and by way of another ML field called *natural language processing*, it can learn
    to read text like a human. Many more such applications of ML are possible, and
    we’ll be covering the basics of ML in this book by using the TensorFlow framework.
  prefs: []
  type: TYPE_NORMAL
- en: What Is PyTorch?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch is an ML library that is based on a previous library called *Torch*,
    which is an open source ML framework and scripting language that is itself based
    on a programming language called Lua. In 2017, development of Torch moved to PyTorch,
    which is a port of the framework in Python.
  prefs: []
  type: TYPE_NORMAL
- en: So, when installing PyTorch, you’ll often see it referred to as “torch.”
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch was originally developed by Meta AI, but it was moved out to the Linux
    Foundation as a way of building developer confidence that it wasn’t made by and
    for a big tech company. It’s one of the two most popular ML libraries, alongside
    the TensorFlow/Keras ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: With the emergence of generative AI, and in particular the “open sourcing” of
    generative text and image models, PyTorch has exploded in popularity. It’s often
    used for training models (which we cover in Part I of this book) as well as for
    inference of models (which we cover in Part II of this book).
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch could also be seen as an ecosystem of libraries, each of which is tailored
    to specific scenarios. The important libraries and scenarios to consider are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: TorchServe
  prefs: []
  type: TYPE_NORMAL
- en: This is an easy-to-use tool that lets you deploy PyTorch models at scale. It’s
    designed to run in multiple environments, and it’s generally technology agnostic.
    It supports features such as multimodel serving, logging, metrics, and the easy
    creation of RESTful endpoints that let you do inference on models from a variety
    of clients.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed training
  prefs: []
  type: TYPE_NORMAL
- en: When larger models don’t fit onto a single chip or machine, there are technologies
    and techniques that allow you to share them across multiple devices. The `torch.distributed`
    libraries allow you easy and native support of asynchronous execution across multiple
    devices.
  prefs: []
  type: TYPE_NORMAL
- en: Mobile
  prefs: []
  type: TYPE_NORMAL
- en: An important surface for inference is, of course, mobile. It’s important for
    you to be able to deploy your AI work to Android and iOS devices, and PyTorch
    supports this through PyTorch Mobile.
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained models
  prefs: []
  type: TYPE_NORMAL
- en: An active community of researchers and developers have created a rich ecosystem
    of models that you can simply use with one line of code, wrapped in the torchvision.models
    library.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-11](#fig-1-11) provides a high-level representation of this.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0111.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-11\. PyTorch ecosystem
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The process of creating ML models is called *training*, and it’s where a computer
    uses a set of algorithms to learn about inputs and what distinguishes them from
    one another. So, for example, if you want a computer to recognize cats and dogs,
    you can use lots of pictures of both to create a model, and the computer will
    use that model to try to figure out what makes a cat a cat and what makes a dog
    a dog. Once the model is trained, the process of having it recognize or categorize
    future inputs is called *inference*.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for training models, there are several things that you need to consider,
    and we will cover them in this book. Primarily, your choice will boil down to
    one of three things:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating the model entirely from scratch yourself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using someone else’s model because it’s enough for your task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using parts of another person’s model that have already been trained and building
    on top of them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The last option on the list is called *transfer learning*, and we’ll cover it
    later in the book.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to train a model. For the most part, you’ll probably just
    use a single chip, whether it’s a central processing unit (CPU), a graphics processing
    unit (GPU), or something new called a tensor processing unit (TPU). In more advanced
    working and research environments, you can use parallel training across multiple
    chips, employing a distributed training where training is intelligently spanned
    across multiple chips. PyTorch supports this, too, through “distributed training”
    libraries, as shown in [Figure 1-11](#fig-1-11).
  prefs: []
  type: TYPE_NORMAL
- en: The lifeblood of any model is its data. As we discussed earlier, if you want
    to create a model that can recognize cats and dogs, you need to train it with
    lots of examples of cats and dogs. But how can you manage these examples? Over
    time, you’ll see that this can often involve a lot more coding than the creation
    of the models themselves.
  prefs: []
  type: TYPE_NORMAL
- en: But luckily, the PyTorch ecosystem includes a number of built-in datasets that
    make this easy for you. We will also explore these throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond creating models, you’ll need to be able to get them into people’s hands
    so they can use them. To this end, PyTorch includes libraries for serving, where
    you can provide model inference over an HTTP connection for cloud or web users.
    For models to run on mobile or embedded systems, there’s PyTorch Mobile, which
    provides tools for model inference on Android and iOS.
  prefs: []
  type: TYPE_NORMAL
- en: Next, I’ll show you how to install PyTorch so that you can get started creating
    and using ML models with it!
  prefs: []
  type: TYPE_NORMAL
- en: Using PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll look at the three main ways you can install and use PyTorch.
    We’ll start with how to install it on your developer box using the command line.
    Then, we’ll explore using the popular PyCharm IDE to install and use PyTorch.
    Finally, we’ll look at Google Colab and how you can use it to access your PyTorch
    code with a cloud-based backend in your browser.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Porch in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The *Py* in PyTorch stands for Python, so it’s important to have a Python environment
    already set up. If you don’t have Python already, I strongly recommend you visit
    [the Python website](https://python.org) to get up and running with it and [the
    Learn Python website](https://learnpython.org) to learn the Python language syntax.
  prefs: []
  type: TYPE_NORMAL
- en: With Python, there are many ways to install frameworks, but the default one
    supported by the TensorFlow team is `pip`.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in your Python environment, installing PyTorch is as easy as using this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you’re up and running, you can test your PyTorch version with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You should then see output like that in [Figure 1-12](#ch01_figure_12_1748548870009360).
    It will print the currently running version of PyTorch—here, you can see that
    version 2.4.1 is installed.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0112.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-12\. Running PyTorch in Python
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you look closely at [Figure 1-12](#ch01_figure_12_1748548870009360), you’ll
    see a note that shows that the torch device is “cpu.” In this case, I natively
    installed it on my Mac, and it is configured to use the CPU. However, this is
    not optimal for complex models, where an accelerator like a GPU or Metal may be
    necessary. We will cover installation of PyTorch for accelerators later in this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: Using PyTorch in PyCharm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I’m particularly fond of using the [free community version of PyCharm](https://oreil.ly/I2mP2)
    for building models using PyTorch. PyCharm is useful for many reasons, but one
    of my favorites is that it makes the management of virtual environments easy.
    This means you can have Python environments with versions of tools such as PyTorch
    that are specific to your particular project. So, for example, if you want to
    use PyTorch 1.x in one project and PyTorch 2.x in another, you can separate them
    with virtual environments and not have to deal with installing/uninstalling dependencies
    when you switch between them. Additionally, with PyCharm, you can do step-by-step
    debugging of your Python code—which is a must, especially if you’re just getting
    started!
  prefs: []
  type: TYPE_NORMAL
- en: For example, in [Figure 1-13](#ch01_figure_13_1748548870009375), I have a new
    project that’s called *example1*, and I’m specifying that I’m going to create
    a new environment using Conda. When I create the project, I’ll have a clean, new,
    virtual Python environment into which I can install any version of PyTorch I want.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you’ve created a project, you can open the File → Settings dialog and
    choose the entry for “Project: *<your project name>*” from the menu on the left.
    In the menu on the left, you’ll see choices to change the settings for the Python
    Interpreter and the Project Structure. If you choose the Python Interpreter link,
    you’ll see the interpreter that you’re using, as well as a list of packages that
    are installed in this virtual environment (see [Figure 1-14](#ch01_figure_14_1748548870009389)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0113.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-13\. Creating a new virtual environment using PyCharm
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](assets/aiml_0114.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-14\. Adding packages to a virtual environment
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can then click the + button at the upper left, and a dialog will open showing
    the packages that are currently available. Type **`torch`** into the search box
    and you’ll see all available packages with *torch* in the name (see [Figure 1-15](#ch01_figure_15_1748548870009404)).
    Remember that the name of the package is *torch*, even if the technology is PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0115.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-15\. Installing torch with PyCharm
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once you’ve selected torch or any other package you want to install, you can
    click the Install Package button and PyCharm will do the rest. Then, once torch
    is installed, you can write and debug your PyTorch code in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Using PyTorch in Google Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another option, perhaps the easiest one for getting started, is to use [*Google
    Colab*](https://oreil.ly/c0lab), which is a hosted Python environment that you
    can access via a browser. What’s really neat about Colab is that it provides GPU
    and TPU backends so you can train models using state-of-the-art hardware at no
    cost.
  prefs: []
  type: TYPE_NORMAL
- en: When you visit the Colab website, you’ll be given the option to open previous
    Colabs or start a new notebook (see [Figure 1-16](#ch01_figure_16_1748548870009418)).
    If you click the + New notebook button, it will open the editor, where you can
    add panes of code or text (see [Figure 1-17](#ch01_figure_17_1748548870009451)).
    You ⁠ can then execute the code by clicking the Play button (the arrow) to the
    left of the pane.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0116.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-16\. Getting started with Google Colab
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![](assets/aiml_0117.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-17\. Running PyTorch code in Colab
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It’s always a good idea to check the PyTorch version, as shown here, to be sure
    you’re running the correct version for the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: You can also see that in [Figure 1-17](#ch01_figure_17_1748548870009451) the
    version shown is 2.4.1+cu121, and you might want to know what the *cu121* part
    is! The *cu* stands for *Cuda*, which is Nvidia’s library for accelerated ML on
    GPUs. So, the preceding message demonstrates that PyTorch 2.4.1 is installed,
    along with accelerators for Cuda version 12.1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Often, Colab’s built-in versions of various libraries, including PyTorch, will
    be a version or two behind the latest release. If that’s the case, you can update
    it with `pip install` as shown earlier, by simply using a block of code like this,
    where you specify the desired version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Once you run this command, your current environment within Colab will use the
    desired version of PyTorch. However, you should be careful when doing this in
    Colab because the version of PyTorch you change to may not have Cuda drivers installed,
    meaning you could downgrade to using the CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Getting Started with Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw earlier in the chapter, the ML paradigm is one in which you have data,
    that data is labeled, and you want to figure out the rules that match the data
    to the labels. The simplest possible scenario to show this in code is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider these two sets of numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: There’s a relationship between the *x* and *y* values (for example, if *x* is
    –1, then *y* is –3; if *x* is 3, then *y* is 5; and so on). Can you see it?
  prefs: []
  type: TYPE_NORMAL
- en: After a few seconds, you probably saw that the pattern here is *y* = 2*x* –
    1\. How did you get that? Different people work it out in different ways, but
    I typically hear the observation that *x* increases by 1 in its sequence and *y*
    increases by 2; thus, *y* = 2*x* +/– something. Then, they look at when *x* =
    0 and see that *y* = –1, so they figure that the answer could be *y* = 2*x* –
    1\. Next, they look at the other values and see that this hypothesis “fits,” and
    the answer is *y* = 2*x* – 1.
  prefs: []
  type: TYPE_NORMAL
- en: That’s very similar to the ML process. Let’s take a look at some code that you
    could write to have a neural network figure this out for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the full code, using PyTorch. Don’t worry if it doesn’t make sense yet;
    we’ll go through it line by line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The first few lines are the importers that ensure the correct libraries are
    available, so let’s jump to this line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You’ve probably heard of neural networks, and you’ve probably seen diagrams
    that explain them by using layers of interconnected neurons, a little like in
    [Figure 1-18](#ch01_figure_18_1748548870009467).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0118.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-18\. A typical neural network
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When you see a neural network like this, you should consider each of the circles
    to be a *neuron* and each of the columns of circles to be a *layer*. So, in [Figure 1-18](#ch01_figure_18_1748548870009467),
    there are three layers: the first has five neurons, the second has four, and the
    third has two.'
  prefs: []
  type: TYPE_NORMAL
- en: These layers are organized in a sequence through which the data flows from left
    to right.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we look back at our code, you’ll see that we’re defining a sequence
    of something, with what’s contained in the brackets being the definition of the
    sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: When using PyTorch, you define your layers by using a `Sequential`, and inside
    the `Sequential`, you then specify what each layer looks like. We have only one
    line inside our `Sequential`, so the neural network this code defines will have
    only one layer.
  prefs: []
  type: TYPE_NORMAL
- en: Then, you define what the layer looks like by using the `torch.nn` libraries.
    There are lots of different layer types, but here, we’re using a `Linear` layer,
    in which a linear relationship (where the definition of a line is *y* = *wx* +
    *b*) can be defined or learned.
  prefs: []
  type: TYPE_NORMAL
- en: Our `Linear` layer has the (1,1) parameters specified, which indicates one feature
    “in” and one feature “out.” So ultimately, we have just one layer with one neuron
    in our entire neural network.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the Sequential containing a Linear with the parameters (1,1)
    ultimately looks like [Figure 1-19](#ch01_figure_19_1748548870009483).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0119.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-19\. A neural network with one layer, containing one neuron
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The next lines are where the fun really begins. Let’s look at them again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If you’ve done anything with ML before, you’ve probably seen that it involves
    a lot of mathematics—and if you haven’t done calculus in years, it might have
    seemed like a barrier to entry. Here’s the part where the math comes in—it’s the
    core of ML.
  prefs: []
  type: TYPE_NORMAL
- en: In a scenario such as this one, the computer has *no idea* what the relationship
    between *x* and *y* is. So, it will make a guess. Say, for example, it guesses
    that *y* = 10*x* + 10\. Then, it needs to measure how good or how bad that guess
    is—and that’s the job of the *loss function*.
  prefs: []
  type: TYPE_NORMAL
- en: The computer already knows the answers when *x* is –1, 0, 1, 2, 3, and 4, so
    the loss function can compare these to the answers for the guessed relationship.
    If it guessed *y* = 10*x* + 10, then when *x* is –1, *y* will be 0\. However,
    the correct answer there was –3, so it’s a bit off. But when *x* is 4, the guessed
    answer is 50, whereas the correct one is 7\. That’s really far off.
  prefs: []
  type: TYPE_NORMAL
- en: Armed with this knowledge, the computer can then make another guess. That’s
    the job of the *optimizer*. This is where the heavy calculus is used, but with
    PyTorch, that can be hidden from you. You just pick the appropriate optimizer
    to use for different scenarios. In this case, we picked one called `sgd`, which
    stands for *stochastic gradient descent*—a complex mathematical function that,
    when given the values, the previous guess, and the results of calculating the
    errors (or loss) on that guess, can then generate another guess. Over time, its
    job is to minimize the loss, and by doing so bring the guessed formula closer
    and closer to the correct answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we simply format our numbers into the data format that the layers expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You’ll see the word *tensor* a lot in ML; it gives the TensorFlow framework
    its name. Think of a tensor as a way of storing data that’s like an array that
    is optimized for flexibility in array size. To have PyTorch understand our data,
    we will load the values into tensors representing the *x* and *y* values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *learning* process will then begin with the training loop like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If you’re new to ML, this is probably the most difficult part to understand,
    so let’s go through it line by line.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the ML process looks like [Figure 1-20](#ch01_figure_20_1748548870009497).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0120.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-20\. The ML process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'So, the preceding code implements this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This line reads as “zero the gradients” of the optimizer. The calculus of learning
    involves navigating down a curve to find its minimum, and to do that, we need
    the gradient of the curve. The curve is calculated when we measure our accuracy,
    so we need to reset it at the beginning of each loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This line creates an array of the outputs that we calculate for the input *x*
    values. Even though we have given the computer the *correct* answers in our *Y*
    array, we want to measure the accuracy of the guess that the computer has made
    for the parameters defining this line. The first time through the loop, the *w*
    and *b* parameters within the neuron will be randomly initialized, so our guess
    might be *Y* = 10*x* + 10, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This line then compares the outputs (aka our guesses) with the correct answers
    to calculate the *loss*—which is effectively a value that tells us how good or
    bad the guess is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This line is the essential part of the learning process where a process called
    *backpropagation* happens. It’s where the math from the optimizer and the loss
    function combine to figure out the gradients for the new set of parameters. In
    our case, the error from *Y* = 10*x* + 10 is really high and not even close to
    our desired values, so the calculations done in figuring out the loss will give
    us a *direction* or gradient in which we should go to get closer to our desired
    results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This line of code finishes the job by updating the model parameters to the values
    based on the gradients calculated in the preceding backpropagation step.
  prefs: []
  type: TYPE_NORMAL
- en: We then repeat this process five hundred times, with the goal of finding a set
    of parameters for our single neuron that will give us *y* values that are close
    to our desired *y* values. If the set of parameters does so, it can then infer
    the *y* value for *x* values that the computer has never previously seen. Thus,
    it will have learned the relationship between the *x* and *y* values we provided.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 1-21](#ch01_figure_21_1748548870009510) shows a screenshot of this
    running in a Colab notebook. Take a look at the loss values over time.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0121.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-21\. Training the neural network
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that over the first 10 epochs, the loss went from 5.64 to 0.86\.
    That is, after only 10 tries, the network was performing about six times better
    than with its initial guess.
  prefs: []
  type: TYPE_NORMAL
- en: Then take a look at what happens by the 500th epoch (see [Figure 1-22](#ch01_figure_22_1748548870009531)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aiml_0122.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1-22\. Training the neural network—the last few epochs
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can now see that the loss is 9.52 × 10^(-6). The loss has gotten so small
    that the model has pretty much figured out that the relationship between the numbers
    is *y* = 2*x* – 1\. This means that the *machine* has *learned* the pattern between
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want our neural network to try to predict a new value, we can use code
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The term *prediction* is typically used when dealing with ML models—but don’t
    think of it as looking into the future! We use this term because we’re dealing
    with a certain amount of uncertainty. Think back to the activity detection scenario
    we spoke about earlier. When the person was moving at a certain speed, she was
    *probably* walking. Similarly, when a model learns about the patterns that exist
    between two things, it will tell us what the answer *probably* is. In other words,
    it is *predicting* the answer. (Later, you’ll also learn about *inference*, in
    which the model picks one answer among many and *infers* that it has picked the
    correct one.)
  prefs: []
  type: TYPE_NORMAL
- en: What do you think the answer will be when we ask the model to predict *y* when
    *x* is 10? You might instantly think 19, but that’s not correct. The model will
    pick a value *very close* to 19, and there are several reasons for this. First
    of all, our loss wasn’t 0\. It was a very small amount, so we should expect any
    predicted answer to be off by a very small amount. Second, the neural network
    is trained on only a small amount of data—and in this case, it’s only six pairs
    of (*x*, *y*) values.
  prefs: []
  type: TYPE_NORMAL
- en: The model only has a single neuron in it, and that neuron learns a *weight*
    and a *bias* so that *y* = *wx* + *b*. This looks exactly like the desired *y*
    = 2*x* – 1 relationship, in which we want the model to learn that *w* = 2 and
    *b* = –1\. Given that the model was trained on only six items of data, we’d never
    expect the answer to be exactly these values; instead, we’d expect it to be something
    very close to them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, run the code for yourself to see what you get. I got 18.991 when I ran
    it, but your answer may differ slightly because when the neural network is first
    initialized, there’s a random element: your initial guess will be slightly different
    from mine and from a third person’s.'
  prefs: []
  type: TYPE_NORMAL
- en: Seeing What the Network Learned
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is obviously a very simple scenario in which we are matching *x’*s to *y’*s
    in a linear relationship. As mentioned in the previous section, neurons have weight
    and bias parameters. That makes a single neuron fine for learning a relationship
    like this; namely, when *y* = 2*x* – 1, the weight is 2 and the bias is –1.
  prefs: []
  type: TYPE_NORMAL
- en: 'With PyTorch, we can actually take a look at the weights and biases that are
    learned, with code like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the network finishes learning, you can print out the values (or weights)
    that the layer learned. In my case, the output was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Thus, the learned relationship between *x* and *y* was *y* = 1.998695 *x* –
    0.9959542.
  prefs: []
  type: TYPE_NORMAL
- en: This is pretty close to what we’d expect (*y* = 2*x* – 1), and we could argue
    that it’s even closer to reality because we’re *assuming* that the relationship
    will hold for other values!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: That’s it for your first “Hello World” of ML. You might be thinking that this
    seems like massive overkill for something as simple as determining a linear relationship
    between two values—and you’d be right. But the cool thing about this is that the
    pattern of code we’ve created here is the same pattern that’s used for far more
    complex scenarios. You’ll see those scenarios starting in [Chapter 2](ch02.html#ch02_introduction_to_computer_vision_1748548889076080),
    where we’ll explore some basic computer vision techniques—in which the machine
    will learn to “see” patterns in pictures and identify what’s in them!
  prefs: []
  type: TYPE_NORMAL
