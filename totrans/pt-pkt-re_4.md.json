["```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models\nfrom torchvision import transforms\n```", "```py\nfrom io import BytesIO\nfrom urllib.request import urlopen\nfrom zipfile import ZipFile\n\nzipurl = 'https://pytorch.tips/bee-zip'\nwith urlopen(zipurl) as zipresp:\n  with ZipFile(BytesIO(zipresp.read())) as zfile:\n     zfile.extractall('./data')\n```", "```py\ntrain_transforms = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        [0.485, 0.456,0.406],\n        [0.229, 0.224, 0.225])])\n\nval_transforms = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        [0.485, 0.456, 0.406],\n        [0.229, 0.224, 0.225])])\n```", "```py\ntrain_dataset = datasets.ImageFolder(\n            root='data/hymenoptera_data/train',\n            transform=train_transforms)\n\nval_dataset = datasets.ImageFolder(\n            root='data/hymenoptera_data/val',\n            transform=val_transforms)\n```", "```py\ntrain_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            batch_size=4,\n            shuffle=True,\n            num_workers=4)\n\nval_loader = torch.utils.data.DataLoader(\n            val_dataset,\n            batch_size=4,\n            shuffle=True,\n            num_workers=4)\n```", "```py\nmodel = models.resnet18(pretrained=True)\n\nprint(model.fc)\n# out:\n# Linear(in_features=512, out_features=1000, bias=True)\n\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 2)\nprint(model.fc)\n# out:\n# Linear(in_features=512, out_features=2, bias=True)\n```", "```py\nfromtorch.optim.lr_schedulerimportStepLRdevice=torch.device(\"cuda:0\"iftorch.cuda.is_available()else\"cpu\")![1](Images/1.png)model=model.to(device)criterion=nn.CrossEntropyLoss()![2](Images/2.png)optimizer=optim.SGD(model.parameters(),lr=0.001,momentum=0.9)![3](Images/3.png)exp_lr_scheduler=StepLR(optimizer,step_size=7,gamma=0.1)![4](Images/4.png)\n```", "```py\nnum_epochs=25forepochinrange(num_epochs):model.train()![1](Images/1.png)running_loss=0.0running_corrects=0forinputs,labelsintrain_loader:inputs=inputs.to(device)labels=labels.to(device)optimizer.zero_grad()outputs=model(inputs)_,preds=torch.max(outputs,1)loss=criterion(outputs,labels)loss.backward()optimizer.step()running_loss+=loss.item()/inputs.size(0)running_corrects+=\\\ntorch.sum(preds==labels.data)\\\n/inputs.size(0)exp_lr_scheduler.step()![2](Images/2.png)train_epoch_loss=\\\nrunning_loss/len(train_loader)train_epoch_acc=\\\nrunning_corrects/len(train_loader)model.eval()![3](Images/3.png)running_loss=0.0running_corrects=0forinputs,labelsinval_loader:inputs=inputs.to(device)labels=labels.to(device)outputs=model(inputs)_,preds=torch.max(outputs,1)loss=criterion(outputs,labels)running_loss+=loss.item()/inputs.size(0)running_corrects+=\\\ntorch.sum(preds==labels.data)\\\n/inputs.size(0)epoch_loss=running_loss/len(val_loader)epoch_acc=\\\nrunning_corrects.double()/len(val_loader)print(\"Train: Loss: {:.4f} Acc: {:.4f}\"\" Val: Loss: {:.4f}\"\" Acc: {:.4f}\".format(train_epoch_loss,train_epoch_acc,epoch_loss,epoch_acc))\n```", "```py\nimportmatplotlib.pyplotaspltdefimshow(inp,title=None):![1](Images/1.png)inp=inp.numpy().transpose((1,2,0))![2](Images/2.png)mean=np.array([0.485,0.456,0.406])std=np.array([0.229,0.224,0.225])inp=std*inp+mean![3](Images/3.png)inp=np.clip(inp,0,1)plt.imshow(inp)iftitleisnotNone:plt.title(title)inputs,classes=next(iter(val_loader))![4](Images/4.png)out=torchvision.utils.make_grid(inputs)class_names=val_dataset.classesoutputs=model(inputs.to(device))![5](Images/5.png)_,preds=torch.max(outputs,1)![6](Images/6.png)imshow(out,title=[class_names[x]forxinpreds])![7](Images/7.png)\n```", "```py\ntorch.save(model.state_dict(), \"./resnet18.pt\")\n```", "```py\ndef generate_bigrams(x):\n  n_grams = set(zip(*[x[i:] for i in range(2)]))\n  for n_gram in n_grams:\n    x.append(' '.join(n_gram))\n  return x\n\ngenerate_bigrams([\n        'This', 'movie', 'is', 'awesome'])\n# out:\n# ['This', 'movie', 'is', 'awesome', 'This movie',\n#  'movie is', 'is awesome']\n```", "```py\nfromtorchtext.datasetsimportIMDBfromtorch.utils.data.datasetimportrandom_splittrain_iter,test_iter=IMDB(split=('train','test'))![1](Images/1.png)train_dataset=list(train_iter)![2](Images/2.png)test_data=list(test_iter)num_train=int(len(train_dataset)*0.70)train_data,valid_data=\\\nrandom_split(train_dataset,[num_train,len(train_dataset)-num_train])![3](Images/3.png)\n```", "```py\nprint(len(train_data), len(valid_data),\n  len(test_data))\n# out:17500 7500 25000\n\ndata_index = 21\nprint(train_data[data_index][0])\n# out: (your results may vary)\n#   pos\n\nprint(train_data[data_index][1])\n# out: (your results may vary)\n# ['This', 'film', 'moved', 'me', 'beyond', ...\n```", "```py\nfromtorchtext.data.utilsimportget_tokenizerfromcollectionsimportCounterfromtorchtext.vocabimportVocabtokenizer=get_tokenizer('spacy')![1](Images/1.png)counter=Counter()for(label,line)intrain_data:counter.update(generate_bigrams(tokenizer(line)))![2](Images/2.png)vocab=Vocab(counter,max_size=25000,vectors=\"glove.6B.100d\",unk_init=torch.Tensor.normal_,)![3](Images/3.png)\n```", "```py\ntext_pipeline = lambda x: [vocab[token]\n    for token in generate_bigrams(tokenizer(x))]\n\nlabel_pipeline = lambda x: 1 if x=='pos' else 0\n\nprint(text_pipeline('the movie was horrible'))\n# out:\n\nprint(label_pipeline('neg'))\n# out:\n```", "```py\n\nfrom torch.utils.data import DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\n\ndevice = torch.device(\"cuda\" if\n    torch.cuda.is_available() else \"cpu\")\n\ndef collate_batch(batch):\n    label_list, text_list = [], []\n    for (_label, _text) in batch:\n        label_list.append(label_pipeline(_label))\n        processed_text = torch.tensor(\n                           text_pipeline(_text))\n        text_list.append(processed_text)\n    return (torch.tensor(label_list,\n          dtype=torch.float64).to(device),\n          pad_sequence(text_list,\n                       padding_value=1.0).to(device))\n\nbatch_size = 64\ndef batch_sampler():\n    indices = [(i, len(tokenizer(s[1])))\n                for i, s in enumerate(train_dataset)]\n    random.shuffle(indices)\n    pooled_indices = []\n    # create pool of indices with similar lengths\n    for i in range(0, len(indices), batch_size * 100):\n        pooled_indices.extend(sorted(\n          indices[i:i + batch_size * 100], key=lambda x: x[1]))\n\n    pooled_indices = [x[0] for x in pooled_indices]\n\n    # yield indices for current batch\n    for i in range(0, len(pooled_indices),\n      batch_size):\n        yield pooled_indices[i:i + batch_size]\n\nBATCH_SIZE = 64\n\ntrain_dataloader = DataLoader(train_data,\n                  # batch_sampler=batch_sampler(),\n                  collate_fn=collate_batch,\n                  batch_size=BATCH_SIZE,\n                  shuffle=True)\n                  # collate_fn=collate_batch)\nvalid_dataloader = DataLoader(valid_data,\n                  batch_size=BATCH_SIZE,\n                  shuffle=True,\n                  collate_fn=collate_batch)\ntest_dataloader = DataLoader(test_data,\n                  batch_size=BATCH_SIZE,\n                  shuffle=True,\n                  collate_fn=collate_batch)\n\n```", "```py\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FastText(nn.Module):\n    def __init__(self,\n                 vocab_size,\n                 embedding_dim,\n                 output_dim,\n                 pad_idx):\n        super().__init__()\n        self.embedding = nn.Embedding(\n            vocab_size,\n            embedding_dim,\n            padding_idx=pad_idx)\n        self.fc = nn.Linear(embedding_dim,\n                            output_dim)\n\n    def forward(self, text):\n        embedded = self.embedding(text)\n        embedded = embedded.permute(1, 0, 2)\n        pooled = F.avg_pool2d(\n            embedded,\n            (embedded.shape[1], 1)).squeeze(1)\n        return self.fc(pooled)\n```", "```py\nmodel = FastText(\n            vocab_size = len(vocab),\n            embedding_dim = 100,\n            output_dim = 1,\n            pad_idx = vocab['<PAD>'])\n```", "```py\npretrained_embeddings=vocab.vectors![1](Images/1.png)model.embedding.weight.data.copy_(pretrained_embeddings)![2](Images/2.png)EMBEDDING_DIM=100unk_idx=vocab['<UNK>']![3](Images/3.png)pad_idx=vocab['<PAD>']model.embedding.weight.data[unk_idx]=\\\ntorch.zeros(EMBEDDING_DIM)![4](Images/4.png)model.embedding.weight.data[pad_idx]=\\\ntorch.zeros(EMBEDDING_DIM)\n```", "```py\nimport torch.optim as optim\n\noptimizer = optim.Adam(model.parameters())\ncriterion = nn.BCEWithLogitsLoss()\n\nmodel = model.to(device)\ncriterion = criterion.to(device)\n```", "```py\nfor epoch in range(5):\n  epoch_loss = 0\n  epoch_acc = 0\n\n  model.train()\n  for label, text, _ in train_dataloader:\n      optimizer.zero_grad()\n      predictions = model(text).squeeze(1)\n      loss = criterion(predictions, label)\n\n      rounded_preds = torch.round(\n          torch.sigmoid(predictions))\n      correct = \\\n        (rounded_preds == label).float()\n      acc = correct.sum() / len(correct)\n\n      loss.backward()\n      optimizer.step()\n      epoch_loss += loss.item()\n      epoch_acc += acc.item()\n\n  print(\"Epoch %d Train: Loss: %.4f Acc: %.4f\" %\n          (epoch,\n          epoch_loss / len(train_dataloader),\n          epoch_acc / len(train_dataloader)))\n\n  epoch_loss = 0\n  epoch_acc = 0\n  model.eval()\n  with torch.no_grad():\n    for label, text, _ in valid_dataloader:\n      predictions = model(text).squeeze(1)\n      loss = criterion(predictions, label)\n\n      rounded_preds = torch.round(\n          torch.sigmoid(predictions))\n      correct = \\\n        (rounded_preds == label).float()\n      acc = correct.sum() / len(correct)\n\n      epoch_loss += loss.item()\n      epoch_acc += acc.item()\n\n  print(\"Epoch %d Valid: Loss: %.4f Acc: %.4f\" %\n          (epoch,\n          epoch_loss / len(valid_dataloader),\n          epoch_acc / len(valid_dataloader)))\n\n# out: (your results may vary)\n# Epoch 0 Train: Loss: 0.6523 Acc: 0.7165\n# Epoch 0 Valid: Loss: 0.5259 Acc: 0.7474\n# Epoch 1 Train: Loss: 0.5935 Acc: 0.7765\n# Epoch 1 Valid: Loss: 0.4571 Acc: 0.7933\n# Epoch 2 Train: Loss: 0.5230 Acc: 0.8257\n# Epoch 2 Valid: Loss: 0.4103 Acc: 0.8245\n# Epoch 3 Train: Loss: 0.4559 Acc: 0.8598\n# Epoch 3 Valid: Loss: 0.3828 Acc: 0.8549\n# Epoch 4 Train: Loss: 0.4004 Acc: 0.8813\n# Epoch 4 Valid: Loss: 0.3781 Acc: 0.8675\n```", "```py\ntest_loss=0test_acc=0model.eval()![1](Images/1.png)withtorch.no_grad():![1](Images/1.png)forlabel,text,_intest_dataloader:predictions=model(text).squeeze(1)loss=criterion(predictions,label)rounded_preds=torch.round(torch.sigmoid(predictions))correct=\\\n(rounded_preds==label).float()acc=correct.sum()/len(correct)test_loss+=loss.item()test_acc+=acc.item()print(\"Test: Loss: %.4f Acc: %.4f\"%(test_loss/len(test_dataloader),test_acc/len(test_dataloader)))# out: (your results will vary)#   Test: Loss: 0.3821 Acc: 0.8599\n```", "```py\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\ndef predict_sentiment(model, sentence):\n    model.eval()\n    text = torch.tensor(text_pipeline(\n      sentence)).unsqueeze(1).to(device)\n    prediction = torch.sigmoid(model(text))\n    return prediction.item()\n\nsentiment = predict_sentiment(model,\n                  \"Don't waste your time\")\nprint(sentiment)\n# out: 4.763594888613835e-34\n\nsentiment = predict_sentiment(model,\n                  \"You gotta see this movie!\")\nprint(sentiment)\n# out: 0.941755473613739\n```", "```py\ntorch.save(model.state_dict(), 'fasttext-model.pt')\n```", "```py\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nCODING_SIZE = 100\nBATCH_SIZE = 32\nIMAGE_SIZE = 64\n\ndevice = torch.device(\"cuda:0\" if\n  torch.cuda.is_available() else \"cpu\")\n```", "```py\ntransform = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.ToTensor(),\n])\n\ndataset = datasets.FashionMNIST(\n                './',\n                train=True,\n                download=True,\n                transform=transform)\n\ndataloader = DataLoader(\n                dataset,\n                batch_size=BATCH_SIZE,\n                shuffle=True,\n                num_workers=8)\n```", "```py\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n\ndata_batch, labels_batch = next(iter(dataloader))\ngrid_img = make_grid(data_batch, nrow=8)\nplt.imshow(grid_img.permute(1, 2, 0))\n```", "```py\nimport torch.nn as nn\n\nclass Generator(nn.Module):\n    def __init__(self, coding_sz):\n        super(Generator, self).__init__()\n        self.net = nn.Sequential(\n            nn.ConvTranspose2d(coding_sz,\n                               1024, 4, 1, 0),\n            nn.BatchNorm2d(1024),\n            nn.ReLU(),\n            nn.ConvTranspose2d(1024,\n                               512, 4, 2, 1),\n            nn.BatchNorm2d(512),\n            nn.ReLU(),\n            nn.ConvTranspose2d(512,\n                               256, 4, 2, 1),\n            nn.BatchNorm2d(256),\n            nn.ReLU(),\n            nn.ConvTranspose2d(256,\n                               128, 4, 2, 1),\n            nn.BatchNorm2d(128),\n            nn.ReLU(),\n            nn.ConvTranspose2d(128,\n                               1, 4, 2, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, input):\n        return self.net(input)\n\nnetG = Generator(CODING_SIZE).to(device)\n```", "```py\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator,\n              self).__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(1, 128, 4, 2, 1),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(128, 256, 4, 2, 1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(256, 512, 4, 2, 1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(512, 1024, 4, 2, 1),\n            nn.BatchNorm2d(1024),\n            nn.LeakyReLU(0.2),\n            nn.Conv2d(1024, 1, 4, 1, 0),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.net(input)\n\nnetD = Discriminator().to(device)\n```", "```py\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\nnetG.apply(weights_init)\nnetD.apply(weights_init)\n```", "```py\nfrom torch import optim\n\ncriterion = nn.BCELoss()\n\noptimizerG = optim.Adam(netG.parameters(),\n                        lr=0.0002,\n                        betas=(0.5, 0.999))\noptimizerD = optim.Adam(netD.parameters(),\n                        lr=0.0001,\n                        betas=(0.5, 0.999))\n```", "```py\nreal_labels = torch.full((BATCH_SIZE,),\n                       1.,\n                       dtype=torch.float,\n                       device=device)\n\nfake_labels = torch.full((BATCH_SIZE,),\n                       0.,\n                       dtype=torch.float,\n                       device=device)\n```", "```py\nG_losses = []\nD_losses = []\nD_real = []\nD_fake = []\n\nz = torch.randn((\n    BATCH_SIZE, 100)).view(-1, 100, 1, 1).to(device)\ntest_out_images = []\n```", "```py\nN_EPOCHS=5forepochinrange(N_EPOCHS):print(f'Epoch: {epoch}')fori,batchinenumerate(dataloader):if(i%200==0):print(f'batch: {i} of {len(dataloader)}')# Train Discriminator with an all-real batch.netD.zero_grad()real_images=batch[0].to(device)*2.-1.output=netD(real_images).view(-1)![1](Images/1.png)errD_real=criterion(output,real_labels)D_x=output.mean().item()# Train Discriminator with an all-fake batch.noise=torch.randn((BATCH_SIZE,CODING_SIZE))noise=noise.view(-1,100,1,1).to(device)fake_images=netG(noise)output=netD(fake_images).view(-1)![2](Images/2.png)errD_fake=criterion(output,fake_labels)D_G_z1=output.mean().item()errD=errD_real+errD_fakeerrD.backward(retain_graph=True)![3](Images/3.png)optimizerD.step()# Train Generator to generate better fakes.netG.zero_grad()output=netD(fake_images).view(-1)![4](Images/4.png)errG=criterion(output,real_labels)![5](Images/5.png)errG.backward()![6](Images/6.png)D_G_z2=output.mean().item()optimizerG.step()# Save losses for plotting later.G_losses.append(errG.item())D_losses.append(errD.item())D_real.append(D_x)D_fake.append(D_G_z2)test_images=netG(z).to('cpu').detach()![7](Images/7.png)test_out_images.append(test_images)\n```", "```py\ntorch.save(netG.state_dict(), './gan.pt')\n```"]