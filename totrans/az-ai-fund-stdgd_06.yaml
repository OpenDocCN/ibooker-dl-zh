- en: Chapter 6\. Features of Computer Vision Workloads on Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll cover the essentials of computer vision, a core area
    of AI that represents 15%–20% of the exam content. This technology enables you
    to harness Microsoft Azure’s suite of tools designed to “see” and interpret the
    world through data. We’ll start with an overview of key computer vision solutions,
    techniques, and foundational concepts. Then, we’ll dive into how Azure approaches
    image classification, object detection, OCR, and facial analysis. We’ll examine
    these topics in greater depth, incorporating hands-on examples along the way.
  prefs: []
  type: TYPE_NORMAL
- en: Computer Vision Services for Azure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you’re diving into computer vision on Azure, you’ve got several options
    to explore, each catering to different goals and levels of customization:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Azure AI Vision*'
  prefs: []
  type: TYPE_NORMAL
- en: If your focus is only on computer vision, Azure AI Vision is built precisely
    for that. It’s got all the tools you need to handle visual data, analyze images
    and videos, and pull useful insights. Let’s say you’re developing a system for
    a parking garage that needs to track available spaces and detect unauthorized
    vehicles through camera feeds. Azure AI Vision gives you a dedicated, powerful
    toolkit to tackle visual processing tasks like these with options to help you
    manage your budget along the way.
  prefs: []
  type: TYPE_NORMAL
- en: '*Azure AI Services*'
  prefs: []
  type: TYPE_NORMAL
- en: If you’re interested in tapping into multiple AI capabilities beyond computer
    vision—translation or search, for example—Azure AI Services is your go-to. Think
    of it as a one-stop shop for various AI tools. You manage everything with a single
    endpoint and access key. This setup will save time. For instance, imagine you’re
    building a travel app that translates text, tags images, and detects landmarks.
    Azure AI Services lets you combine all these features. This keeps everything streamlined
    and simple to manage.
  prefs: []
  type: TYPE_NORMAL
- en: '*Azure AI Custom Vision*'
  prefs: []
  type: TYPE_NORMAL
- en: When you need tailored image recognition capabilities, Azure AI Custom Vision
    is your go-to solution. It empowers you to create and train custom image recognition
    models using tags specific to your project’s needs. For instance, if you’re in
    agriculture, you could build a model to identify crop diseases from images, helping
    farmers take proactive measures and improve yields. Custom Vision is accessible
    via SDKs, an API, or an intuitive web portal.
  prefs: []
  type: TYPE_NORMAL
- en: '*Azure AI Face Service*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This service provides advanced AI algorithms to detect, recognize, and analyze
    human faces in images—even if someone is wearing sunglasses or viewed from an
    angle. It’s an excellent tool if your projects require identity verification,
    touchless access control, or automated face blurring for privacy in public spaces.
    The service can return detailed facial analysis, which makes it useful for applications
    where in-depth facial recognition is needed. However, access is restricted: only
    Microsoft-managed customers and partners meeting specific eligibility and usage
    criteria can use the service as part of Microsoft’s responsible AI principles.
    To apply, interested users must complete the [Face Recognition intake form](https://oreil.ly/sO2xB).'
  prefs: []
  type: TYPE_NORMAL
- en: Azure AI Video Indexer
  prefs: []
  type: TYPE_NORMAL
- en: This tool allows for extracting insights from videos, such as object detection,
    OCR, and content moderation. This is done by leveraging more than 30 AI models.
    There are also audio capabilities, such as for transcription, translation, and
    emotion detection.
  prefs: []
  type: TYPE_NORMAL
- en: Each of these options offers a unique approach to computer vision, whether you
    need a broad AI toolkit, a dedicated vision platform, customized model building,
    or advanced facial recognition capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: What You Can Do with Azure’s Computer Vision Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Azure’s computer vision services offer a powerful set of tools for analyzing
    and understanding images in various ways. If you’re looking to automatically generate
    descriptions, you can start with *image captioning*. This feature doesn’t just
    identify what’s in an image—it goes a step further by giving each description
    a confidence score ranging from 0 to 1\. This lets you know how certain the model
    is about its analysis. For instance, if you have a picture of a sunny beach with
    people swimming, image captioning might generate a caption like “a beach scene
    with people swimming” and show a confidence score that indicates how likely it
    is to be accurate—say 0.9\. This makes it easy to verify how much trust to place
    in the results.
  prefs: []
  type: TYPE_NORMAL
- en: '*Tagging* is another helpful feature. This adds a layer of searchable terms
    to each image. Tagging highlights specific keywords related to elements found
    in the image, such as *beach*, *ocean*, or *sun*, and each tag includes a confidence
    score. This feature is invaluable when organizing large image libraries or quickly
    searching for images with certain characteristics.'
  prefs: []
  type: TYPE_NORMAL
- en: Azure’s computer vision also works with *object detection*. This identifies
    and locates specific objects within an image, such as cars, people, or furniture.
    Object detection goes beyond just recognizing objects by also pinpointing their
    exact locations within the image, which is great for any task that requires spatial
    awareness. This can be used in scenarios like monitoring inventory, analyzing
    traffic patterns, or even enabling automated checkout systems in retail.
  prefs: []
  type: TYPE_NORMAL
- en: '*Facial detection* and *face recognition* add even  more specialized capabilities,
    as noted before with the Azure AI Face Service. With facial detection, Azure locates
    the presence of faces within an image, but it doesn’t go further to identify who
    those faces belong to. This is ideal for applications like crowd counting or assessing
    emotions from facial expressions. Face recognition takes things further by recognizing
    individual faces—matching them to known identities. This is useful for security
    applications, personalized experiences, or any situation where you need to verify
    someone’s identity.'
  prefs: []
  type: TYPE_NORMAL
- en: Last, Azure’s *Optical Character Recognition (OCR)* service allows you to extract
    text from images. This transforms any visual text into machine-readable characters.
    OCR is ideal for digitizing printed documents, scanning receipts, or reading handwritten
    notes. Whether you’re automating data entry or making scanned documents searchable,
    OCR can simplify processes by quickly converting images of text into usable data.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 6-1](#i06_chapter6_table_1_1742068262896692) provides a summary of Azure’s
    computer vision capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6-1\. Computer vision capabilities in Azure
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature | Description | Use cases |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Image captioning | Identifies the content of an image and provides a description
    along with a confidence score from 0 to 1 | Describing photosAuto-generating captions
    |'
  prefs: []
  type: TYPE_TB
- en: '| Tagging | Adds specific key terms or labels to an image, each with a confidence
    score, making it easier to categorize and search for images based on content |
    Photo library organizationDigital assets management |'
  prefs: []
  type: TYPE_TB
- en: '| Object detection | Identifies and locates specific objects within an image,
    providing spatial data on their positions | Inventory managementTraffic analysisAutomated
    retail checkout |'
  prefs: []
  type: TYPE_TB
- en: '| Facial detection | Detects the presence of faces in an image without identifying
    individuals | Crowd countingEmotion detection in groups |'
  prefs: []
  type: TYPE_TB
- en: '| Face recognition | Recognizes individual faces, matching them to known identities
    for verification purposes | Security accessPersonalized customer experiences |'
  prefs: []
  type: TYPE_TB
- en: '| OCR | Extracts text from images, converting visual text into machine-readable
    characters | Data entry automationDocument digitizationReceipt scanning |'
  prefs: []
  type: TYPE_TB
- en: Let’s walk through an example using the Azure AI Vision service. Go to the “[Add
    captions to images” section in the Vision Studio](https://oreil.ly/aYYYy). You
    will see sample photos at the top and a place to upload your own photos, as shown
    in [Figure 6-1](#i06_chapter6_figure_1_1742068262892610).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0601.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-1\. The image caption service in Azure Vision Studio
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We’ll select the baseball player. The caption will appear: “A baseball player
    holding a bat.” Then, select JSON and review the output, which is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The JSON shows the identification information, length, content type, and model
    version for the AI. It also shows the image caption as well as the confidence
    score, which is 0.821\. Finally, you’ll see some metadata for the dimensions of
    the image.
  prefs: []
  type: TYPE_NORMAL
- en: How Computer Vision Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the heart of computer vision is the pixel. When a computer “looks” at an
    image, it’s actually reading numbers that tell it how bright each pixel is and
    what color it should be.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a more in-depth look at how computer vision works:'
  prefs: []
  type: TYPE_NORMAL
- en: Images are made of pixels
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine an image as a huge grid made up of pixels. An image that is 100 × 100
    pixels, for instance, has exactly 10,000 pixels arranged in rows and columns.
    Each pixel has a color made up of three values: red, green, and blue (RGB), ranging
    in integer value from 0 to 255\. By mixing these colors, the computer constructs
    the images we see.'
  prefs: []
  type: TYPE_NORMAL
- en: Find patterns in pixels
  prefs: []
  type: TYPE_NORMAL
- en: To recognize objects, computer vision algorithms search for patterns within
    this pixel grid. They use math-based methods, like convolution, to identify parts
    of the image, such as edges and textures. Convolution works by analyzing small
    groups of pixels (kernels) to enhance certain features, like an edge, and help
    the computer pick out shapes in the image.
  prefs: []
  type: TYPE_NORMAL
- en: Spot important details
  prefs: []
  type: TYPE_NORMAL
- en: After recognizing patterns in the pixels, computer vision models focus on key
    parts—features—of the image. These features could be the curve of a face, the
    outline of a vehicle, or the edges of a building. By narrowing down the focus,
    the model zeroes in on only the most important points. This makes it easier to
    tell what the image contains.
  prefs: []
  type: TYPE_NORMAL
- en: '*Teach a model to recognize images*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the model has picked out these features, it’s ready for the next step:
    learning. The model has been trained on loads of images, each labeled with what’s
    in it. By studying this labeled data, it learns to connect certain pixel patterns
    with objects like “cat” or “car” and later can recognize similar objects in new
    images.'
  prefs: []
  type: TYPE_NORMAL
- en: '*DL and pixel data*'
  prefs: []
  type: TYPE_NORMAL
- en: For DL models like convolutional neural networks (CNNs)—which we’ll learn more
    about later in this chapter—pixels are just the beginning. Each layer in a CNN
    digs deeper into the pixel data, with early layers detecting edges, middle layers
    recognizing shapes, and later layers identifying full objects. As the network
    learns, it adjusts the relationships between pixels. This makes the model better
    at detecting patterns each time it’s used.
  prefs: []
  type: TYPE_NORMAL
- en: '*Fine-tune for accuracy*'
  prefs: []
  type: TYPE_NORMAL
- en: In applications where accuracy is critical—like facial recognition or medical
    imaging—every pixel matters. Advanced algorithms take a closer look at these pixels,
    which improves the model’s precision. By fine-tuning pixel data, the model gradually
    detects with greater detail, getting better at interpreting images with each improvement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go further into pixels and the processing of images. For this, we’ll
    take a look at an example, which is an array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This array has seven rows and seven columns, creating a 7 × 7–pixel image—its
    resolution. Each pixel in this image is represented by a number ranging from 0
    (black) to 255 (white), with values between these extremes representing different
    shades of gray. In this case, the pixel values of 180 create a lighter gray square
    in the middle of a darker background, which is shown in [Figure 6-2](#i06_chapter6_figure_2_1742068262892648).
  prefs: []
  type: TYPE_NORMAL
- en: 'A grid of pixel values like this forms a simple, two-dimensional image by arranging
    rows and columns along x- and y-coordinates. This single layer is enough for grayscale
    images, but color images require a bit more complexity. To represent color, we
    use three separate layers. Each one represents a different color component: red,
    green, and blue. As an example, imagine we have a similar 7 *×* 7–pixel array
    in three color channels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Red:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Green:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Blue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](assets/aaif_0602.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-2\. A 7 × 7 grid display outlining each pixel
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This example is similar to [Figure 6-2](#i06_chapter6_figure_2_1742068262892648)
    in shape. But when these three layers combine, they form a color image. The greenish-blue
    squares on the outer edges come from mixing these values for each color:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Red: 120'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Green: 20'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blue: 250'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The teal squares in the middle are created by mixing different values for each
    color:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Red: 200'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Green: 180'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blue: 30'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This blending of the red, green, and blue channels brings color images to life,
    pixel by pixel.
  prefs: []
  type: TYPE_NORMAL
- en: Image Filters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you want to enhance an image, applying filters can significantly transform
    it by adjusting each pixel’s value to create various effects. Filters use *kernels*—small
    grids of numbers that define how each pixel will be altered. Different kernels
    create different effects, such as blurring or sharpening. Each serves a unique
    purpose in image processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a 3 × 3–kernel matrix for sharpening that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This kernel is applied by moving it across the image, calculating a new pixel
    value for each 3 × 3 section, and filling these results into a new version of
    the image. Let’s walk through an example using a simple grayscale image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Starting with the top left corner, each pixel in a 3 × 3 section is multiplied
    by the corresponding kernel value, then summed up to produce a new pixel value
    for the output image. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You then move the kernel one pixel to the right and repeat this process across
    the entire image. This specific kernel is used for sharpening, emphasizing details,
    and making edges more prominent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keep in mind that there are various filters to achieve different results:'
  prefs: []
  type: TYPE_NORMAL
- en: Blurring
  prefs: []
  type: TYPE_NORMAL
- en: 'Blurring filters, like a simple averaging kernel, reduce noise and smooth out
    an image. A typical blurring kernel might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This kernel averages the pixels in each 3 × 3 section, softening details and
    creating a blurred effect.
  prefs: []
  type: TYPE_NORMAL
- en: Edge detection
  prefs: []
  type: TYPE_NORMAL
- en: 'Edge detection filters, such as the Sobel or Laplacian filters, are designed
    to identify boundaries within an image by emphasizing rapid changes in pixel intensity—for
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This kernel detects edges by highlighting where pixel values change sharply,
    helping to isolate shapes and lines.
  prefs: []
  type: TYPE_NORMAL
- en: Color inversion
  prefs: []
  type: TYPE_NORMAL
- en: Color inversion filters flip the pixel values, creating a negative of the image.
    These filters don’t use a kernel like the other filters. Inverting colors means
    that each pixel’s value is subtracted from the maximum intensity, transforming
    light areas to dark and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Sharpening
  prefs: []
  type: TYPE_NORMAL
- en: The sharpening filter, as shown in the example above, enhances details by making
    edges stand out. This is useful for highlighting features or making an image appear
    crisper.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 6-3](#i06_chapter6_figure_3_1742068262892675) has a side-by-side comparison
    showing the effect of a sharpening filter on an image of an apple. This process,
    known as *convolutional filtering*, involves sweeping the filter across the image
    to apply effects like blurring, edge detection, color inversion, and sharpening.
    By experimenting with different kernels, you gain many creative possibilities
    for transforming your images.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0603.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-3\. The result of using a filter on an image to sharpen it
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Image Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Image classification* is an important aspect of AI that focuses on identifying
    the main content within an image and sorting it into specific categories. For
    example, if you want a model to distinguish between animals, it can be taught
    to identify whether an image shows a cat, dog, or bird. This capability goes beyond
    just detecting items; it’s about recognizing complex patterns. Sometimes this
    means picking up on visual details that may be too subtle for the human eye.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning is typically used to train a model for image classification.
    In this approach, the model learns from a labeled dataset—each image is paired
    with its correct label—so the model can learn what each category looks like. This
    way, the AI becomes adept at recognizing and correctly labeling new images that
    it hasn’t seen before. On the other hand, unsupervised learning, which doesn’t
    rely on labeled data, is generally less effective for tasks that need precise
    category distinctions.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, one popular application of image classification is in health care,
    where AI models can analyze medical images to detect specific abnormalities. This
    might include spotting tumors in X-rays or identifying certain conditions in MRI
    scans. Not only does this streamline the diagnostic process, but it also enables
    quicker and potentially more accurate detection, which can be crucial for early
    intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Object Detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Object detection* is an AI technique that not only identifies what objects
    are in an image but also pinpoints their exact locations. This is achieved through
    *bounding boxes*, which are rectangular outlines that frame each identified object
    and are mapped by pixel coordinates. These boxes show precisely where each object
    sits within the image, giving a more detailed understanding than simply knowing
    what objects are present.'
  prefs: []
  type: TYPE_NORMAL
- en: Azure AI’s vision service offers robust object detection capabilities. Tools
    like Vision Studio make integrating this feature into various applications more
    accessible. The Azure AI vision service allows users to detect objects and track
    their locations using bounding boxes. Object detection builds on image analysis
    models, but it involves more complex training since the model must learn not just
    to recognize objects but also to locate them accurately within the image. [Figure 6-4](#i06_chapter6_figure_4_1742068262892697)
    shows an example of using object detection with Vision Studio.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0604.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-4\. Object detection using Azure AI Vision Studio
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The uploaded image shows how the model detected individual products by drawing
    bounding boxes around each item on the shelves. Each bounding box, outlined in
    blue, represents a “detected product,” showing where each item is located within
    the image. Alongside this visual display, Vision Studio provides detailed JSON
    data on the right, which includes the pixel coordinates of each bounding box.
    What’s more, each detected object comes with a confidence score that tells us
    how sure it is that each item is a “product.”
  prefs: []
  type: TYPE_NORMAL
- en: OCR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional OCR uses pattern recognition to match text shapes, but with AI,
    things have stepped up a notch. Now, ML algorithms analyze every shape and line,
    comparing them to vast libraries of text samples. This means that AI can handle
    tricky fonts or messy handwriting better than older OCR methods ever could.
  prefs: []
  type: TYPE_NORMAL
- en: That said, OCR isn’t perfect. Sometimes, it misreads characters—like mistaking
    a lowercase “l” for the number “1.” These errors can crop up especially if the
    text is smudged or the font is unusual.
  prefs: []
  type: TYPE_NORMAL
- en: Yet OCR can provide major advantages. Let’s take an example. Suppose a health
    care provider has decades’ worth of patient records stored in dusty file cabinets.
    Digging up a file means sorting through piles of paper—a tedious, time-consuming
    process that often leads to errors or lost documents. With OCR, though, all those
    paper records can be quickly scanned, converted to digital format, and made fully
    searchable. This can mean saving substantial amounts of money and time.
  prefs: []
  type: TYPE_NORMAL
- en: Azure’s Read OCR engine is built with advanced ML models to not just pull text
    from documents but also adapt to multiple languages and formats. Read OCR is flexible,
    letting you choose between cloud-based processing or on-premises deployment. If
    you’re working with single images or photos “in the wild,” Read OCR offers a fast
    synchronous API, so you can embed it into your software.
  prefs: []
  type: TYPE_NORMAL
- en: There are two main versions of Read OCR that cover different scenarios. The
    first version is optimized for general images, such as labels, signs, or posters
    that you’d find in everyday settings. This version (OCR for Images 4.0) is ideal
    for situations that need fast text extraction.
  prefs: []
  type: TYPE_NORMAL
- en: The second version is tailored for text-heavy scanned or digital documents like
    books, reports, or articles. This Document Intelligence model uses an asynchronous
    API, which means it’s built to handle high volumes and works great if you’re automating
    large-scale document processing.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an example of the OCR capabilities of Azure AI Foundry. This example
    converts an image of a Social Security card into readable text, as shown in [Figure 6-5](#i06_chapter6_figure_5_1742068262892719).
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0605.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-5\. OCR output from Azure AI Foundry of a Social Security card image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see, Azure AI Foundry ​successfully identifies the text. If you
    select JSON, you will get an extensive set of features. First, it will show a
    line of the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'For each word, the location is also provided. Then, the OCR breaks this down
    into the values for each of the words, along with the confidence scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This OCR functionality ties into intelligent document processing (IDP), which
    is like OCR’s next-level cousin. It uses OCR as a foundation but dives deeper,
    extracting structure and key information beyond just words. Microsoft’s Document
    Intelligence model, for example, builds on Read OCR to analyze relationships,
    identify key entities, and provide detailed document insights. (We covered this
    system in the section [“Studios”](ch02.html#i02_chapter2_azure_ai_foundry_1742068260813706).)
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at an example of the Azure AI [Document Intelligence Studio](https://oreil.ly/Pd_kX).
    Select the Invoices section and you will see the dashboard. Click “Run analysis”
    and you will see the screen shown in [Figure 6-6](#i06_chapter6_figure_6_1742068262892740).
  prefs: []
  type: TYPE_NORMAL
- en: On the left side of the screen, the OCR has identified the various fields on
    the document. The Document Intelligence technology then determines what types
    of values they have. For example, there are fields for the billing and recipient
    addresses and so on. There are also confidence scores for each.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0606.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-6\. Analysis of an invoice using Document Intelligence Studio
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Facial Detection and Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Facial detection and analysis are crucial technologies for computer vision.
    Facial detection identifies human faces within an image or video feed, often as
    a preliminary step in other processes, such as unlocking your phone with your
    face. It determines whether a face is present but does not provide additional
    details. Facial analysis, on the other hand, goes a step further by interpreting
    various features of the face, such as age, gender, and emotional expressions,
    based on patterns in facial structures.
  prefs: []
  type: TYPE_NORMAL
- en: Facial detection and analysis rely on AI, particularly DL, which uses layers
    of neural networks to analyze patterns in data. Models are trained on vast datasets
    of human faces, which allows the models to recognize and interpret facial features
    accurately. Techniques like CNNs play a significant role here, scanning and learning
    from pixel patterns in images. The AI models then apply this learning to recognize
    facial features and expressions in real time or in prerecorded images, constantly
    improving with each use.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll find facial detection and analysis in many everyday applications. Security
    systems use it for surveillance and to identify individuals of interest in crowded
    places while retail settings apply it to gather demographic data on customers
    and even tailor advertisements. In health care, facial analysis is being explored
    to assess emotional well-being and identify symptoms associated with certain conditions.
    Gaming and entertainment sectors use it to enhance user experiences, adapting
    the game or content interactions based on the user’s reactions.
  prefs: []
  type: TYPE_NORMAL
- en: Azure AI Studio includes powerful face recognition capabilities. [Figure 6-7](#i06_chapter6_figure_7_1742068262892761)
    shows an example of this.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/aaif_0607.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6-7\. Azure AI Studio identifies and analyzes the face of a woman in
    an image
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In this example, the AI has recognized the face of a woman. It has created
    a bounding box for her face, and there are various dots on her face to indicate
    different features. If you select JSON, you will see information about this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This captures information about the identification and locations of the left
    pupil, right pupil, nose tip, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine trying to teach a computer to recognize animals in photos, such as whether
    the animal is a cat, a dog, or even a lion. *Convolutional neural networks (CNNs)*
    make this possible. They’re a popular type of DL model because of how they analyze
    data. Think of CNNs as digital detectives that filter through an image to pick
    out important clues like shapes, edges, and textures. They then piece these clues
    together to make a prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Here’s how it works in practice. The model’s filters start with random weights.
    With each round of training, the model learns which details help it recognize
    each feature, such as the animal type. After enough practice, it becomes highly
    accurate at picking up the clues—so much so that it can correctly label a new
    image of, say, a lion, even if it’s never seen that exact lion before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a breakdown of the CNN layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs: []
  type: TYPE_NORMAL
- en: Each filter in these layers scans the image and captures specific patterns,
    such as fur texture or ear shape. Over time, the CNN learns which patterns are
    most useful for recognizing an animal in the image.
  prefs: []
  type: TYPE_NORMAL
- en: '*Pooling layers*'
  prefs: []
  type: TYPE_NORMAL
- en: To keep from getting overloaded with data, CNNs use pooling (often max pooling),
    which shrinks the feature maps to retain only the most essential details. This
    process makes the model faster and less sensitive to minor variations, such as
    changes in an animal’s pose.
  prefs: []
  type: TYPE_NORMAL
- en: '*Fully connected layers*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, the model brings together all the information it’s collected. It’s like
    flattening a 3D puzzle and connecting all the pieces to make a single prediction.
    By this stage, the model has a strong sense of what the object is.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions
  prefs: []
  type: TYPE_NORMAL
- en: In the final steps, the model needs to weigh its options and come to a conclusion.
    Functions like `softmax` are used. They assign probabilities to each possible
    label, so the model might output probabilities like [0.3, 0.6, 0.1]—indicating
    a 60% chance that the image is a dog, for example.
  prefs: []
  type: TYPE_NORMAL
- en: '*Backpropagation and optimization*'
  prefs: []
  type: TYPE_NORMAL
- en: When the model’s guess isn’t right, it’s not the end of the road. It reviews
    where it missed the mark and adjusts the filter weights to improve its next guess.
    This feedback loop, called *backpropagation*, helps the model focus on the most
    relevant features for each type of image.
  prefs: []
  type: TYPE_NORMAL
- en: Training a CNN means feeding it thousands—often millions—of labeled images and
    letting it practice. With each training round, the model’s accuracy improves as
    it learns from its mistakes. By the end, the CNN understands how to best identify
    each type of object, even if a new image looks slightly different from the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: 'While CNNs are often used for labeling images, they’re also perfect for other
    tasks that require understanding complex visual data:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Object detection*'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than just saying “This is a cat,” CNNs can identify multiple animals
    in an image, drawing boxes around each one.
  prefs: []
  type: TYPE_NORMAL
- en: Image segmentation
  prefs: []
  type: TYPE_NORMAL
- en: In detailed tasks like medical imaging, CNNs can classify each pixel in an image,
    identifying distinct regions, which can be helpful, for instance, in spotting
    different types of tissues in a medical scan.
  prefs: []
  type: TYPE_NORMAL
- en: In short, CNNs allow machines to interpret images in ways that were previously
    impossible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Evolution of Computer Vision: From CNNs to Multimodal Models'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A new kind of architecture—called *transformers*—has been taking over the NLP
    world. Transformers function by turning words or phrases into numerical codes
    called *embeddings*. Imagine embeddings as coordinates in a virtual landscape
    where words with similar meanings gather near one another. For example, “cat”
    and “kitten” would be positioned closely together, unlike “cat” and “airplane,”
    which would sit farther apart in this semantic space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transformers’ success with language led to a new breakthrough: *multimodal
    models*. These models combine images and text, enabling them to understand both
    at the same time. Here’s how they do it:'
  prefs: []
  type: TYPE_NORMAL
- en: They analyze images to pull out key visual features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They convert text into embeddings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They learn to link these visual and text-based elements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Microsoft’s Florence is one example of a multimodal model. It’s trained on
    millions of images with captions, letting it take on various tasks like:'
  prefs: []
  type: TYPE_NORMAL
- en: Sorting images into categories
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spotting specific objects in images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crafting natural image descriptions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding relevant tags
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These multimodal models represent the cutting edge of AI today, creating exciting
    opportunities for systems that can handle both images and words with ease.
  prefs: []
  type: TYPE_NORMAL
- en: Responsible AI and Computer Vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](ch03.html#i03_chapter3_overview_of_ai_workloads_and_key_use_cases_1742068261133633),
    we examined the principles of responsible AI. This is certainly an important topic
    in the context of computer vision, where ethical issues take center stage. However,
    this technology also raises ethical concerns. Privacy is a major issue as many
    users may not know their data is being collected or how it will be used. There’s
    also the potential for misuse, such as mass surveillance without consent. Bias
    in AI models is another ethical concern because models trained on limited or nondiverse
    datasets can lead to inaccurate or unfair outcomes, especially across different
    demographic groups. For these reasons, deploying facial detection and analysis
    solutions responsibly requires transparency, strict data governance, and ongoing
    efforts to mitigate bias and protect individual privacy. Let’s dive deeper into
    the key factors that shape responsible AI practices in this category.
  prefs: []
  type: TYPE_NORMAL
- en: Fairness in Facial Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensuring fairness in facial recognition systems is a vital ethical consideration.
    Research has shown that these systems often exhibit biases, particularly against
    individuals with darker skin tones and women. A [2025 study by Ketan Kotwal and
    Sébastien Marcel](https://oreil.ly/orfFS) found that these systems often achieve
    higher accuracy for male subjects compared to female subjects. Moreover, the study
    highlighted challenges in skin tone classification, noting that lighter-skinned
    individuals are generally easier to verify than those with darker skin tones.
    This bias often arises from insufficiently diverse training datasets. Addressing
    this issue requires developing more inclusive datasets that represent a wide variety
    of skin tones, genders, and ethnic backgrounds. Moreover, employing fairness-aware
    algorithms and conducting regular audits can help identify and reduce biases.
  prefs: []
  type: TYPE_NORMAL
- en: Because of the issues, Microsoft has implemented restrictions with its face
    recognition technology. For example, it prohibits US police departments from using
    this technology. Microsoft has also retired certain features, such as those that
    infer emotions, gender, and age.
  prefs: []
  type: TYPE_NORMAL
- en: Privacy and Security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Facial recognition technology also raises pressing concerns about privacy and
    security. Unauthorized surveillance and data collection without consent infringe
    on individuals’ privacy rights. Furthermore, the storage of facial data carries
    risks, such as identity theft in the event of a breach. To address these concerns,
    organizations must establish strict guidelines requiring informed consent before
    collecting facial data. Adopting robust encryption methods and adhering to data
    protection laws like the European Union’s General Data Protection Regulation (GDPR)
    can further secure sensitive information. Providing transparency about how face
    recognition systems are used and offering individuals the option to opt out are
    additional measures that can help protect privacy and build trust.
  prefs: []
  type: TYPE_NORMAL
- en: Transparency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transparency is key for fostering trust and accountability in computer vision
    systems. Users should understand how these systems function, the types of data
    they collect, and the decision-making processes involved. To promote transparency,
    developers should provide clear documentation about their algorithms, including
    the data sources and methodologies employed. Open communication with stakeholders
    and the public can also help demystify the technology. What’s more, incorporating
    explainable AI techniques can make the decision-making processes of computer vision
    systems more accessible to nontechnical audiences, enhancing trust and understanding.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To wrap things up, Azure’s computer vision tools pack serious power, putting
    advanced AI right at your fingertips. Knowing how Azure handles everything from
    object detection to OCR and facial analysis is not only impressive but also key
    for passing the AI-900 exam. This exam expects you to understand the capabilities
    Azure offers—like how multimodal models can blend text and images to create more
    contextually aware AI. And with multimodal models opening up new possibilities,
    Azure’s computer vision tools aren’t just practical—they’re shaping the next era
    of AI, one smart application at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Quiz
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To check your answers, please refer to the [“Chapter 6 Answer Key”](app02.html#answers_chapter_6_sample_questions_1745932457451863).
  prefs: []
  type: TYPE_NORMAL
- en: Which of the following is a fundamental concept in computer vision involving
    dividing an image into a grid of colored points?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filters
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Pixels
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Labels
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which method is used in object detection to pinpoint the location of objects
    within an image?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: OCR
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Bounding boxes
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Facial detection
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Image classification
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the purpose of convolution in computer vision?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To detect color inversion
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To resize images
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To identify patterns in pixel data
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: To assign labels
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In Microsoft Azure’s computer vision tools, which service primarily handles
    tasks like object detection and facial analysis?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Azure Cognitive Search Service
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Azure AI Vision
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Azure Machine Learning
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Azure Kubernetes Service
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What type of neural network is most commonly used for computer vision tasks
    like image classification?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recurrent neural network (RNN)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Convolutional neural network (CNN)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Generative adversarial network (GAN)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Transformer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which component of a CNN reduces the size of feature maps to focus on essential
    details?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Fully connected layers
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Activation functions
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Which computer vision technique reads and interprets text within images?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Image classification
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: OCR
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Facial detection
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Object detection
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the primary ethical concern associated with facial detection and analysis
    in AI?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lack of color accuracy
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Privacy and consent issues
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: High computational costs
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Limited dataset availability
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: In Azure’s AI Vision Studio, what data accompanies object detection to indicate
    the model’s confidence?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pixel count
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Confidence score
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Bounding box color
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: File type
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What role do activation functions play in CNNs during the image recognition
    process?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identifying edge patterns
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Reducing image size
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Assigning probabilities to predictions
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Adding color to images
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
