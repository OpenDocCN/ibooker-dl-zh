- en: Chapter 18\. Autoencoders, GANs, and Diffusion Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Autoencoders are artificial neural networks capable of learning dense representations
    of the input data, called *latent representations* or *codings*, without any supervision
    (i.e., the training set is unlabeled). These codings typically have a much lower
    dimensionality than the input data, making autoencoders useful for dimensionality
    reduction (see [Chapter¬†7](ch07.html#dimensionality_chapter)), especially for
    visualization purposes. Autoencoders also act as feature detectors, and they can
    be used for unsupervised pretraining of deep neural networks (as we discussed
    in [Chapter¬†11](ch11.html#deep_chapter)). They are also commonly used for anomaly
    detection, as we will see. Lastly, some autoencoders are *generative models*:
    they are capable of randomly generating new data that looks very similar to the
    training data. For example, you could train an autoencoder on pictures of faces,
    and it would then be able to generate new faces.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Generative adversarial networks* (GANs) are also neural nets capable of generating
    data. In fact, they can generate pictures of faces so convincing that it is hard
    to believe the people they represent do not exist. You can judge for yourself
    by visiting [*https://thispersondoesnotexist.com*](https://thispersondoesnotexist.com),
    a website that shows faces generated by a GAN architecture called *StyleGAN*.
    GANs have been widely used for super resolution (increasing the resolution of
    an image), [colorization](https://github.com/jantic/DeOldify), powerful image
    editing (e.g., replacing photo bombers with realistic background), turning simple
    sketches into photorealistic images, predicting the next frames in a video, augmenting
    a dataset (to train other models), generating other types of data (such as text,
    audio, and time series), identifying the weaknesses in other models to strengthen
    them, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: However, since the early 2020s, GANs have been largely replaced by *diffusion
    models*, which can generate more diverse and higher-quality images than GANs,
    while also being much easier to train. However, diffusion models are much slower
    to run, so GANs are still useful when you need very fast generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Autoencoders, GANs, and diffusion models are all unsupervised, learn latent
    representations, can be used as generative models, and have many similar applications.
    However, they work very differently:'
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders simply learn to copy their inputs to their outputs. This may sound
    like a trivial task, but as you will see, constraining the network in various
    ways can make the task arbitrarily difficult. For example, you can limit the size
    of the latent representations, or you can add noise to the inputs and train the
    network to recover the original inputs. These constraints prevent the autoencoder
    from trivially copying the inputs directly to the outputs, which forces it to
    learn efficient ways of representing the data. In short, the codings are byproducts
    of the autoencoder learning the identity function under some constraints.
  prefs: []
  type: TYPE_NORMAL
- en: GANs
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs are composed of two neural networks: a *generator* that tries to generate
    data that looks similar to the training data, and a *discriminator* that tries
    to tell real data from fake data. This architecture is very original in deep learning
    in that the generator and the discriminator compete against each other during
    training; this is called *adversarial training*. The generator is often compared
    to a criminal trying to make realistic counterfeit money, while the discriminator
    is like the police investigator trying to tell real money from fake.'
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion models
  prefs: []
  type: TYPE_NORMAL
- en: A diffusion model is trained to gradually remove noise from an image. If you
    then take an image entirely full of random noise and repeatedly run the diffusion
    model on that image, a high-quality image will gradually emerge, similar to the
    training images (but not identical).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter we will start by exploring in more depth how autoencoders work
    and how to use them for dimensionality reduction, feature extraction, unsupervised
    pretraining, or as generative models. This will naturally lead us to GANs. We
    will build a simple GAN to generate fake images, but we will see that training
    is often quite difficult. We will discuss the main difficulties you will encounter
    with adversarial training, as well as some of the main techniques to work around
    these difficulties. And lastly, we will build and train a diffusion model‚Äîspecifically
    a *denoising diffusion probabilistic model* (DDPM)‚Äîand use it to generate images.
    Let‚Äôs start with autoencoders!
  prefs: []
  type: TYPE_NORMAL
- en: Efficient Data Representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which of the following number sequences do you find the easiest to memorize?
  prefs: []
  type: TYPE_NORMAL
- en: 40, 27, 25, 36, 81, 57, 10, 73, 19, 68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At first glance, it would seem that the first sequence should be easier, since
    it is much shorter. However, if you look carefully at the second sequence, you
    will notice that it is just the list of even numbers from 50 down to 14\. Once
    you notice this pattern, the second sequence becomes much easier to memorize than
    the first because you only need to remember the pattern (i.e., decreasing even
    numbers) and the starting and ending numbers (i.e., 50 and 14). Note that if you
    could quickly and easily memorize very long sequences, you would not care much
    about the existence of a pattern in the second sequence. You would just learn
    every number by heart, and that would be that. The fact that it is hard to memorize
    long sequences is what makes it useful to recognize patterns, and hopefully this
    clarifies why constraining an autoencoder during training pushes it to discover
    and exploit patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: The relationship among memory, perception, and pattern matching was famously
    studied by [William Chase and Herbert Simon](https://homl.info/111)‚Å†^([1](ch18.html#id4015))
    in the early 1970s. They observed that expert chess players were able to memorize
    the positions of all the pieces in a game by looking at the board for just five
    seconds, a task that most people would find impossible. However, this was only
    the case when the pieces were placed in realistic positions (from actual games),
    not when the pieces were placed randomly. Chess experts don‚Äôt have a much better
    memory than you and I; they just see chess patterns more easily, thanks to their
    experience with the game. Noticing patterns helps them store information efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like the chess players in this memory experiment, an autoencoder looks
    at the inputs, converts them to an efficient latent representation, and is then
    capable of reconstructing something that (hopefully) looks very close to the inputs.
    An autoencoder is always composed of two parts: an *encoder* (or *recognition
    network*) that converts the inputs to a latent representation, followed by a *decoder*
    (or *generative network*) that converts the internal representation to the outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: In the example shown in [Figure¬†18-1](#encoder_decoder_diagram), the autoencoder
    is a regular multilayer perceptron (MLP; see [Chapter¬†9](ch09.html#ann_chapter)).
    Since it must reconstruct its inputs, the number of neurons in the output layer
    must be equal to the number of inputs (i.e., three in this example). The lower
    part of the network is the encoder (in this case it‚Äôs a single layer with two
    neurons), and the upper part is the decoder. The outputs are often called the
    *reconstructions* because the autoencoder tries to reconstruct the inputs. The
    cost function always contains a *reconstruction loss* that penalizes the model
    when the reconstructions are different from the inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Illustration showing a chess memory experiment with a chessboard transitioning
    to a latent representation, and a diagram of an autoencoder with an encoder layer
    reducing inputs to two neurons and a decoder layer reconstructing three outputs.](assets/hmls_1801.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-1\. The chess memory experiment (left) and a simple autoencoder (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Because the internal representation has a lower dimensionality than the input
    data (in this example, it is 2D instead of 3D), the autoencoder is said to be
    *undercomplete*. An undercomplete autoencoder cannot trivially copy its inputs
    to the codings, yet it must find a way to output a copy of its inputs. It is forced
    to compress the data, thereby learning the most important features in the input
    data (and dropping the unimportant ones).
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs see how to implement a very simple undercomplete autoencoder for dimensionality
    reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Performing PCA with an Undercomplete Linear Autoencoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the autoencoder uses only linear activations and the cost function is the
    mean squared error (MSE), then it ends up performing principal component analysis
    (PCA; see [Chapter¬†7](ch07.html#dimensionality_chapter)).
  prefs: []
  type: TYPE_NORMAL
- en: The following code builds a simple linear autoencoder that takes a 3D input,
    projects it down to 2D, then projects it back up to 3D. Since we will train the
    model using targets equal to the inputs, gradient descent will have to find the
    2D plane that lies closest to the training data, just like PCA would.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This code is really not very different from all the MLPs we built in past chapters,
    but there are a few things to note:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We organized the autoencoder into two subcomponents: the encoder and the decoder,
    each composed of a single `Linear` layer in this example, and the autoencoder
    is a `Sequential` model containing the encoder followed by the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The autoencoder‚Äôs number of outputs is equal to the number of inputs (i.e.,
    3).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To perform PCA, we do not use any activation function (i.e., all neurons are
    linear), and the cost function is the MSE. That‚Äôs because PCA is a linear transformation.
    We will see more complex and nonlinear autoencoders shortly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let‚Äôs train the model on the same simple generated 3D dataset we used in
    [Chapter¬†7](ch07.html#dimensionality_chapter) and use it to encode that dataset
    (i.e., project it to 2D):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `X_train` is used as both the inputs and the targets. Next, let‚Äôs
    train the autoencoder, using the same `train()` function as in [Chapter¬†10](ch10.html#pytorch_chapter)
    (the notebook uses a slightly fancier function that prints some info and evaluates
    the model at each epoch):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the autoencoder is trained, we can use its encoder to compress 3D
    inputs to 2D. For example, let‚Äôs compress the entire training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure¬†18-2](#linear_autoencoder_pca_diagram) shows the original 3D dataset
    (on the left) and the output of the autoencoder‚Äôs hidden layer (i.e., the coding
    layer, on the right). As you can see, the autoencoder found the best 2D plane
    to project the data onto, preserving as much variance in the data as it could
    (just like PCA).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram comparing a 3D dataset and its 2D projection through an autoencoder,
    illustrating the principle of dimensionality reduction similar to PCA.](assets/hmls_1802.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-2\. Approximate PCA performed by an undercomplete linear autoencoder
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can think of an autoencoder as performing a form of self-supervised learning,
    since it is based on a supervised learning technique with automatically generated
    labels (in this case, simply equal to the inputs).
  prefs: []
  type: TYPE_NORMAL
- en: Stacked Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just like other neural networks we have discussed, autoencoders can have multiple
    hidden layers. In this case they are called *stacked autoencoders* (or *deep autoencoders*).
    Adding more layers helps the autoencoder learn more complex codings. That said,
    one must be careful not to make the autoencoder too powerful. Imagine an encoder
    so powerful that it just learns to map each input to a single arbitrary number
    (and the decoder learns the reverse mapping). Obviously such an autoencoder will
    reconstruct the training data perfectly, but it will not have learned any useful
    data representation in the process, and is unlikely to generalize well to new
    instances.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of a stacked autoencoder is typically symmetrical with regard
    to the central hidden layer (the coding layer). To put it simply, it looks like
    a sandwich. For example, an autoencoder for Fashion MNIST (introduced in [Chapter¬†9](ch09.html#ann_chapter))
    may have 784 inputs, followed by a hidden layer with 128 neurons, then a central
    hidden layer of 32 neurons, then another hidden layer with 128 neurons, and an
    output layer with 784 neurons. This stacked autoencoder is represented in [Figure¬†18-3](#stacked_autoencoder_diagram).
    Note that all hidden layers must have an activation function, such as ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of a stacked autoencoder architecture with an input layer of 784
    units, three hidden layers (128, 32, and 100 units), and an output layer of 784
    units, illustrating the symmetrical structure.](assets/hmls_1803.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-3\. Stacked autoencoder
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Implementing a Stacked Autoencoder Using PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can implement a stacked autoencoder very much like a regular deep MLP.
    For example, here is an autoencoder you can use to process Fashion MNIST images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let‚Äôs go through this code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like earlier, we split the autoencoder model into two submodels: the encoder
    and the decoder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The encoder takes 28 √ó 28 pixel grayscale images (i.e., with a single channel),
    flattens them so that each image is represented as a vector of size 784, then
    processes these vectors through 2 `Linear` layers of diminishing sizes (128 units,
    then 32 units), each followed by the ReLU activation function. For each input
    image, the encoder outputs a vector of size 32.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decoder takes codings of size 32 (output by the encoder) and processes them
    through 2 `Linear` layers of increasing sizes (128 units, then 784 units), and
    reshapes the final vectors into 1 √ó 28 √ó 28 arrays so the decoder‚Äôs outputs have
    the same shape as the encoder‚Äôs inputs. Note that we use the sigmoid function
    for the output layer instead of ReLU to ensure that the output pixel values range
    between 0 and 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can now load the Fashion MNIST dataset using the TorchVision library and
    split it into `train_data`, `valid_data`, and `test_data` (just like we did in
    [Chapter¬†10](ch10.html#pytorch_chapter)), then train the autoencoder exactly like
    the previous autoencoder, using the inputs as the targets and minimizing the MSE
    loss. Give it a try, it‚Äôs a good exercise! Don‚Äôt forget to change the targets
    so they match the inputs‚Äîwe‚Äôre training an autoencoder, not a classifier.‚Å†^([2](ch18.html#id4035))
    If you get stuck, please check out the implementation in this chapter‚Äôs notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the Reconstructions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you have trained the stacked autoencoder, how do you know if it‚Äôs any
    good? One way to check that an autoencoder is properly trained is to compare the
    inputs and the outputs: the differences should not be too significant. Let‚Äôs plot
    a few images from the validation set, as well as their reconstructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![Original images of clothing and footwear are displayed on top, with their
    slightly blurred reconstructions below, illustrating the results of a stacked
    autoencoder.](assets/hmls_1804.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-4\. Original images (top) and their reconstructions (bottom)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Figure¬†18-4](#reconstruction_plot) shows the resulting images. The reconstructions
    are recognizable, but a bit too lossy. We may need to train the model for longer,
    or make the encoder and decoder more powerful, or make the codings larger. For
    now, let‚Äôs go with this model and see how we can use it.'
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly Detection Using Autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One common use case for autoencoders is anomaly detection. Indeed, if an autoencoder
    is given an image that doesn‚Äôt look like the images it was trained on (the image
    is said to be *out of distribution*), then the reconstruction will be terrible.
    For example, [Figure¬†18-5](#bad_reconstructions_plot) shows some MNIST digits
    and their reconstructions using the model we just trained on Fashion MNIST. As
    you can see, these reconstructions are very different from the inputs. If you
    compute the reconstruction loss (i.e., the MSE between the input and the output),
    it will be very high. To use the model for anomaly detection, you simply need
    to define a threshold, then any image whose reconstruction loss is greater than
    that threshold can be considered an anomaly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Example showing poor reconstructions of MNIST digit images using a model
    trained on Fashion MNIST, highlighting how out-of-distribution inputs result in
    high reconstruction loss.](assets/hmls_1805.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-5\. Out-of-distribution images are poorly reconstructed
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: That‚Äôs all there is to it! Now let‚Äôs look at another use case for autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the Fashion MNIST Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we saw earlier in this chapter, undercomplete autoencoders can be used for
    dimensionality reduction. However, for most datasets they will not do a very good
    job at reducing the dimensionality down to two or three dimensions; they need
    enough dimensions to be able to properly reconstruct the inputs. As a result,
    they are generally not used directly for visualization. However, they are great
    at handling huge datasets, so one strategy is to use an autoencoder to reduce
    the dimensionality down to a reasonable level, then use another dimensionality
    reduction algorithm for visualization, such as those we discussed in [Chapter¬†7](ch07.html#dimensionality_chapter).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs use this strategy to visualize Fashion MNIST. First we‚Äôll use the encoder
    from our stacked autoencoder to reduce the dimensionality down to 32, then we‚Äôll
    use Scikit-Learn‚Äôs implementation of the t-SNE algorithm to reduce the dimensionality
    down to 2 for visualization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can plot the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure¬†18-6](#fashion_mnist_visualization_plot) shows the resulting scatterplot,
    beautified a bit by displaying some of the images. The t-SNE algorithm identified
    several clusters that match the classes reasonably well (each class is represented
    by a different color). Note that t-SNE‚Äôs output can vary greatly if you run it
    with a different random seed, slightly different data, or on a different platform,
    so your plot may look different.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A scatterplot visualizing Fashion MNIST data via an autoencoder and t-SNE,
    showing clusters corresponding to different clothing items marked by images.](assets/hmls_1806.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-6\. Fashion MNIST visualization using an autoencoder, followed by
    t-SNE
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Next let‚Äôs look at how we can use autoencoders for unsupervised pretraining.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Pretraining Using Stacked Autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed in [Chapter¬†11](ch11.html#deep_chapter), if you are tackling
    a complex supervised task but you do not have a lot of labeled training data,
    one solution is to find a neural network that performs a similar task and reuse
    its lower layers. This makes it possible to train a high-performance model using
    little training data because your neural network won‚Äôt have to learn all the low-level
    features; it will just reuse the feature detectors learned by the existing network.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, if you have a large dataset but most of it is unlabeled, you can
    first train a stacked autoencoder using all the data, then reuse the lower layers
    to create a neural network for your actual task and train it using the labeled
    data. For example, [Figure¬†18-7](#unsupervised_pretraining_autoencoders_diagram)
    shows how to use a stacked autoencoder to perform unsupervised pretraining for
    a classification neural network. When training the classifier, if you really don‚Äôt
    have much labeled training data, you may want to freeze the pretrained layers
    (at least the lower ones).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the process of using a stacked autoencoder for unsupervised
    pretraining, showing the transfer of trained parameters from a multi-layer setup
    in Phase 1 to a classifier in Phase 2.](assets/hmls_1807.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-7\. Unsupervised pretraining using autoencoders
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Having plenty of unlabeled data and little labeled data is common. Building
    a large unlabeled dataset is often cheap (e.g., a simple script can download millions
    of images off the internet), but labeling those images (e.g., classifying them
    as cute or not) can usually be done reliably only by humans. Labeling instances
    is time-consuming and costly, so it‚Äôs normal to have only a few thousand human-labeled
    instances, or even less. That said, there is a growing trend toward using advanced
    AIs to label datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is nothing special about the implementation: just train an autoencoder
    using all the training data (labeled plus unlabeled), then reuse its encoder layers
    to create a new neural network and train it on the labeled instances (see the
    exercises at the end of this chapter for an example).'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs now look at a few techniques for training stacked autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: Tying Weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When an autoencoder is neatly symmetrical, like the one we just built, a common
    technique is to *tie* the weights of the decoder layers to the weights of the
    encoder layers. This halves the number of weights in the model, speeding up training
    and limiting the risk of overfitting. Specifically, if the autoencoder has a total
    of *N* layers (not counting the input layer), and **W**[*L*] represents the connection
    weights of the *L*^(th) layer (e.g., layer 1 is the first hidden layer, layer
    *N*/2 is the coding layer, and layer *N* is the output layer), then the decoder
    layer weights can be defined as **W**[*L*] = **W**[*N*‚Äì*L*+1]^‚ä∫ (with *L* = *N*
    / 2 + 1, ‚Ä¶‚Äã, *N*).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, here is the same autoencoder as the previous one, except the decoder
    weights are tied to the encoder weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This model achieves a smaller reconstruction error than the previous model,
    using about half the number of parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Training One Autoencoder at a Time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rather than training the whole stacked autoencoder in one go like we just did,
    it is possible to train one shallow autoencoder at a time, then stack all of them
    into a single stacked autoencoder (hence the name), as shown in [Figure¬†18-8](#stacking_autoencoders_diagram).
    This technique is called *greedy layerwise training*.
  prefs: []
  type: TYPE_NORMAL
- en: During the first phase of training, the first autoencoder learns to reconstruct
    the inputs. Then we encode the whole training set using this first autoencoder,
    and this gives us a new (compressed) training set. We then train a second autoencoder
    on this new dataset. This is the second phase of training. Finally, we build a
    big sandwich using all these autoencoders, as shown in [Figure¬†18-8](#stacking_autoencoders_diagram)
    (i.e., we first stack the encoder layers of each autoencoder, then the decoder
    layers in reverse order). This gives us the final stacked autoencoder. We could
    easily train more autoencoders this way, building a very deep stacked autoencoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the three-phase process of training stacked autoencoders
    one layer at a time, showing the flow from input through hidden layers to output.](assets/hmls_1808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-8\. Training one autoencoder at a time
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As I mentioned in [Chapter¬†11](ch11.html#deep_chapter), one of the triggers
    of the deep learning tsunami was the discovery in 2006 by [Geoffrey Hinton et
    al.](https://homl.info/136) that deep neural networks can be pretrained in an
    unsupervised fashion using this greedy layer-wise approach. They used restricted
    Boltzmann machines (RBMs; see [*https://homl.info/extra-anns*](https://homl.info/extra-anns))
    for this purpose, but in 2007 [Yoshua Bengio et al.](https://homl.info/112)‚Å†^([3](ch18.html#id4062))
    showed that autoencoders worked just as well. For several years this was the only
    efficient way to train deep nets, until many of the techniques introduced in [Chapter¬†11](ch11.html#deep_chapter)
    made it possible to just train a deep net in one shot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Autoencoders are not limited to dense networks: you can also build convolutional
    autoencoders. Let‚Äôs look at these now.'
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you are dealing with images, then the autoencoders we have seen so far will
    not work well (unless the images are very small). As you saw in [Chapter¬†12](ch12.html#cnn_chapter),
    convolutional neural networks are far better suited than dense networks to working
    with images. So if you want to build an autoencoder for images (e.g., for unsupervised
    pretraining or dimensionality reduction), you will need to build a [*convolutional
    autoencoder*](https://homl.info/convae).‚Å†^([4](ch18.html#id4066)) The encoder
    is a regular CNN composed of convolutional layers and pooling layers. It typically
    reduces the spatial dimensionality of the inputs (i.e., height and width) while
    increasing the depth (i.e., the number of feature maps). The decoder must do the
    reverse (upscale the image and reduce its depth back to the original dimensions),
    and for this you can use transpose convolutional layers (alternatively, you could
    combine upsampling layers with convolutional layers). Here is a basic convolutional
    autoencoder for Fashion MNIST:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It‚Äôs also possible to create autoencoders with other architecture types, such
    as RNNs (see the notebook for an example).
  prefs: []
  type: TYPE_NORMAL
- en: 'OK, let‚Äôs step back for a second. So far we have looked at various kinds of
    autoencoders (basic, stacked, and convolutional) and how to train them (either
    in one shot or layer by layer). We also looked at a few applications: dimensionality
    reduction (e.g., for data visualization), anomaly detection, and unsupervised
    pretraining.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Up to now, in order to force the autoencoder to learn interesting features,
    we have limited the size of the coding layer, making it undercomplete. There are
    actually many other kinds of constraints that can be used, including ones that
    allow the coding layer to be just as large as the inputs, or even larger, resulting
    in an *overcomplete autoencoder*. So in the following sections we‚Äôll look at a
    few more kinds of autoencoders: denoising autoencoders, sparse autoencoders, and
    variational autoencoders.'
  prefs: []
  type: TYPE_NORMAL
- en: Denoising Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A simple way to force the autoencoder to learn useful features is to add noise
    to its inputs, training it to recover the original, noise-free inputs. This idea
    has been around since the 1980s (e.g., it is mentioned in Yann LeCun‚Äôs 1987 master‚Äôs
    thesis). In a [2008 paper](https://homl.info/113),‚Å†^([5](ch18.html#id4073)) Pascal
    Vincent et al. showed that autoencoders could also be used for feature extraction.
    In a [2010 paper](https://homl.info/114),‚Å†^([6](ch18.html#id4074)) Vincent et
    al. introduced *stacked denoising autoencoders*.
  prefs: []
  type: TYPE_NORMAL
- en: The noise can be pure Gaussian noise added to the inputs, or it can be randomly
    switched-off inputs, just like in dropout (introduced in [Chapter¬†11](ch11.html#deep_chapter)).
    [Figure¬†18-9](#denoising_autoencoders_diagram) shows both options.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating denoising autoencoders with Gaussian noise added to
    inputs on the left and dropout applied on the right.](assets/hmls_1809.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-9\. Denoising autoencoders, with Gaussian noise (left) or dropout
    (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The dropout implementation of the denoising autoencoder is straightforward:
    it is a regular stacked autoencoder with an additional `Dropout` layer applied
    to the encoder‚Äôs inputs (recall that the `Dropout` layer is only active during
    training). Note that the coding layer does not need to compress the data as much
    since the noise already makes the reconstruction task nontrivial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This may remind you of BERT‚Äôs MLM pretraining task (see [Chapter¬†15](ch15.html#transformer_chapter)):
    reconstructing masked inputs (except BERT isn‚Äôt split into an encoder and a decoder).'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure¬†18-10](#dropout_denoising_plot) shows a few noisy images (with half
    of the pixels turned off), and the images reconstructed by the dropout-based denoising
    autoencoder, after training. Notice how the autoencoder guesses details that are
    actually not in the input, such as the top of the rightmost shoe. As you can see,
    not only can denoising autoencoders be used for data visualization or unsupervised
    pretraining, like the other autoencoders we‚Äôve discussed so far, but they can
    also be used quite simply and efficiently to remove noise from images.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Noisy images at the top and their denoised reconstructions at the bottom,
    demonstrating the effectiveness of a dropout-based denoising autoencoder.](assets/hmls_1810.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-10\. Noisy images (top) and their reconstructions (bottom)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Sparse Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another kind of constraint that often leads to good feature extraction is *sparsity*:
    by adding an appropriate term to the cost function, the autoencoder is pushed
    to reduce the number of active neurons in the coding layer. This forces the autoencoder
    to represent each input as a combination of a small number of activations. As
    a result, each neuron in the coding layer typically ends up representing a useful
    feature (if you could speak only a few words per month, you would probably try
    to make them worth listening to).'
  prefs: []
  type: TYPE_NORMAL
- en: A basic approach is to use the sigmoid activation function in the coding layer
    (to constrain the codings to values between 0 and 1), use a large coding layer
    (e.g., with 256 units), and add some ‚Ñì[1] regularization to the coding layer‚Äôs
    activations. This means adding the ‚Ñì[1] norm of the codings (i.e., the sum of
    their absolute values) to the loss, weighted by a sparsity hyperparameter. This
    *sparsity loss* will encourage the neural network to produce codings close to
    0\. However, the total loss will still include the reconstruction loss, so the
    model will be forced to output at least a few nonzero values to reconstruct the
    inputs correctly. Using the ‚Ñì[1] norm rather than the ‚Ñì[2] norm will push the
    neural network to preserve the most important codings while eliminating the ones
    that are not needed for the input image (rather than just reducing all codings).
  prefs: []
  type: TYPE_NORMAL
- en: Another approach‚Äîwhich often yields better results‚Äîis to measure the mean sparsity
    of each neuron in the coding layer, across each training batch, and penalize the
    model when the mean sparsity differs from the target sparsity (e.g., 10%). The
    batch size must not be too small, or the mean will not be accurate. For example,
    if we measure that a neuron has an average activation of 0.3, but the target sparsity
    is 0.1, then this neuron must be penalized to activate less. One approach could
    be simply adding the squared error (0.3 ‚Äì 0.1)¬≤ to the loss function, but in practice
    it‚Äôs better to use the Kullback‚ÄìLeibler (KL) divergence (briefly discussed in
    [Chapter¬†4](ch04.html#linear_models_chapter)), since it has much stronger gradients
    than the mean squared error, as you can see in [Figure¬†18-11](#sparsity_loss_plot).
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram comparing the cost functions of KL divergence, MAE, and MSE at different
    actual sparsity levels, with a target sparsity of 0.1.](assets/hmls_1811.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-11\. Sparsity loss with target sparsity *p* = 0.1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given two discrete probability distributions *P* and *Q*, the KL divergence
    between these distributions, noted *D*[KL](*P* ‚à• *Q*), can be computed using [Equation
    18-1](#kl_divergence_equation).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 18-1\. Kullback‚ÄìLeibler divergence
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $upper D Subscript KL Baseline left-parenthesis upper P parallel-to upper Q
    right-parenthesis equals sigma-summation Underscript i Endscripts upper P left-parenthesis
    i right-parenthesis log StartFraction upper P left-parenthesis i right-parenthesis
    Over upper Q left-parenthesis i right-parenthesis EndFraction$
  prefs: []
  type: TYPE_NORMAL
- en: In our case, we want to measure the divergence between the target probability
    *p* that a neuron in the coding layer will activate, and the actual probability
    *q*, estimated by measuring the mean activation over the training batch. So, the
    KL divergence simplifies to [Equation 18-2](#kl_divergence_equation_simplified).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 18-2\. KL divergence between the target sparsity *p* and the actual
    sparsity *q*
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $upper D Subscript KL Baseline left-parenthesis p parallel-to q right-parenthesis
    equals p log StartFraction p Over q EndFraction plus left-parenthesis 1 minus
    p right-parenthesis log StartFraction 1 minus p Over 1 minus q EndFraction$
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this approach in PyTorch, we must first ensure that the autoencoder
    outputs both the reconstructions and the codings, since they are both needed to
    compute the loss. In this code, the autoencoder‚Äôs `forward()` method returns a
    `namedtuple` containing two fields‚Äî`output` (i.e., the reconstructions) and `codings`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You may need to tweak your training and evaluation functions to support these
    `namedtuple` predictions. For example, you can add `y_pred = y_pred.output` in
    the `evaluate_tm()` function, just after calling the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can define the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This function returns the reconstruction loss (MSE) plus a weighted sparsity
    loss. The sparsity loss is the KL divergence between the target sparsity and the
    mean sparsity across the batch. The `kl_weight` is a hyperparameter you can tune
    to control how much to encourage sparsity: if this hyperparameter is too high,
    the model will stick closely to the target sparsity, but it may not reconstruct
    the inputs properly, making the model useless. Conversely, if it is too low, the
    model will mostly ignore the sparsity objective and will not learn any interesting
    features. The `eps` argument is a smoothing term to avoid division by zero when
    computing the KL divergence.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we‚Äôre ready to create the model and train it (using the same `train()`
    function as earlier, from [Chapter¬†10](ch10.html#pytorch_chapter)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: After training this sparse autoencoder on Fashion MNIST, the coding layer will
    have roughly 10% sparsity. Success!
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Sparse autoencoders often produce fairly interpretable codings, where each
    component corresponds to an identifiable feature in the image. For example, you
    can plot all the images whose *n*^(th) coding is larger than usual (e.g., above
    the 90^(th) percentile): you will often notice that all the images have something
    in common (e.g., they are all shoes).'
  prefs: []
  type: TYPE_NORMAL
- en: Now let‚Äôs move on to variational autoencoders!
  prefs: []
  type: TYPE_NORMAL
- en: Variational Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'An important category of autoencoders was introduced in 2013 by [Diederik Kingma
    and Max Welling](https://homl.info/115)‚Å†^([7](ch18.html#id4088)) and quickly became
    one of the most popular variants: *variational autoencoders* (VAEs).'
  prefs: []
  type: TYPE_NORMAL
- en: 'VAEs are quite different from all the autoencoders we have discussed so far,
    in these particular ways:'
  prefs: []
  type: TYPE_NORMAL
- en: They are *probabilistic autoencoders*, meaning that their outputs are partly
    determined by chance, even after training (as opposed to denoising autoencoders,
    which use randomness only during training).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most importantly, they are *generative autoencoders*, meaning that they can
    generate new instances that look like they were sampled from the training set.‚Å†^([8](ch18.html#id4091))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let‚Äôs take a look at how VAEs work. [Figure¬†18-12](#variational_autoencoders_diagram)
    (left) shows a variational autoencoder. You can recognize the basic sandwich-like
    structure of most autoencoders, with an encoder followed by a decoder (in this
    example, they both have two hidden layers), but there is a twist: instead of directly
    producing a coding for a given input, the encoder produces a *mean coding* **Œº**
    and a standard deviation **œÉ**. The actual coding is then sampled randomly from
    a Gaussian distribution with mean **Œº** and standard deviation **œÉ**. After that,
    the decoder decodes the sampled coding normally. The right part of the diagram
    shows a training instance going through this autoencoder. First, the encoder produces
    **Œº** and **œÉ**, then a coding is sampled randomly (notice that it is not exactly
    located at **Œº**), and finally this coding is decoded. The final output resembles
    the training instance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in [Figure¬†18-12](#variational_autoencoders_diagram), although
    the inputs may have a very convoluted distribution, a variational autoencoder
    tends to produce codings that look as though they were sampled from a simple Gaussian
    distribution. During training, the cost function (discussed next) pushes the codings
    to gradually migrate within the coding space (also called the *latent space*)
    to end up looking like a cloud of multidimensional Gaussian points. One great
    consequence is that after training a variational autoencoder, you can very easily
    generate a new instance: just sample a random coding from the Gaussian distribution,
    decode it, and voil√†!'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of a variational autoencoder with an encoder and decoder, illustrating
    Gaussian noise sampling and coding space transformation.](assets/hmls_1812.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-12\. A variational autoencoder (left) and an instance going through
    it (right)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Sampling from a random distribution is not a differentiable operation, it will
    block backpropagation, so how can we hope to train the encoder? Well, using a
    *reparameterization trick*: sample **Œµ** from ùí©(0, 1) and compute **Œº** + **œÉ**
    ‚äó **Œµ** (element-wise multiplication). This is equivalent to sampling from ùí©(**Œº**,
    **œÉ**¬≤) but it separates the deterministic and stochastic parts of the process,
    allowing the gradients to flow back into the encoder through **Œº** and **œÉ**.
    The resulting encoder gradients are stochastic (due to **Œµ**), but they are unbiased
    estimates, and the randomness averages out during training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The cost function is composed of two parts. The first is the usual reconstruction
    loss that pushes the autoencoder to reproduce its inputs. We can use the MSE for
    this, as we did earlier. The second is the *latent loss* that pushes the autoencoder
    to have codings that look as though they were sampled from a simple Gaussian distribution:
    it is the KL divergence between the actual distribution of the codings and the
    desired latent distribution (i.e., the Gaussian distribution). The math is a bit
    more complex than with the sparse autoencoder, in particular because of the Gaussian
    noise, which limits the amount of information that can be transmitted to the coding
    layer. Luckily, the equations simplify, so the latent loss can be computed using
    [Equation 18-3](#var_ae_latent_loss_equation) (for the full mathematical details,
    check out the original paper on variational autoencoders, or Carl Doersch‚Äôs [great
    2016 tutorial](https://homl.info/vaetuto).)'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 18-3\. Variational autoencoder‚Äôs latent loss
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $script upper L equals minus one-half sigma-summation Underscript i equals 1
    Overscript n Endscripts left-bracket 1 plus log left-parenthesis sigma Subscript
    i Superscript 2 Baseline right-parenthesis minus sigma Subscript i Superscript
    2 Baseline minus mu Subscript i Superscript 2 Baseline right-bracket$
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, ‚Ñí is the latent loss, *n* is the codings‚Äô dimensionality,
    and *Œº*[i] and *œÉ*[i] are the mean and standard deviation of the *i*^(th) component
    of the codings. The vectors **Œº** and **œÉ** (which contain all the *Œº*[i] and
    *œÉ*[i]) are output by the encoder, as shown in [Figure¬†18-12](#variational_autoencoders_diagram)
    (left).
  prefs: []
  type: TYPE_NORMAL
- en: A common tweak to the variational autoencoder‚Äôs architecture is to make the
    encoder output **Œ≥** = log(**œÉ**¬≤) rather than **œÉ**. The latent loss can then
    be computed as shown in [Equation 18-4](#var_ae_latent_loss_equation_2). This
    approach is more numerically stable and speeds up training.
  prefs: []
  type: TYPE_NORMAL
- en: Equation 18-4\. Variational autoencoder‚Äôs latent loss, rewritten using **Œ≥**
    = log(**œÉ**¬≤)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $script upper L equals minus one-half sigma-summation Underscript i equals 1
    Overscript n Endscripts left-bracket 1 plus gamma Subscript i Baseline minus exp
    left-parenthesis gamma Subscript i Baseline right-parenthesis minus mu Subscript
    i Superscript 2 Baseline right-bracket$
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs build a variational autoencoder for Fashion MNIST, using the architecture
    shown in [Figure¬†18-12](#variational_autoencoders_diagram), except using the **Œ≥**
    tweak:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let‚Äôs go through this code:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we define `VAEOutput`. This allows the model to output a `namedtuple`
    containing the reconstructions (`output`) as well as **Œº** (`codings_mean`) and
    **Œ≥** (`codings_logvar`).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The encoder and decoder architectures strongly resemble the previous autoencoders,
    but notice that the encoder‚Äôs output is twice the size of the codings. This is
    because the encoder does not directly output the codings; instead, it outputs
    the parameters of the Gaussian distribution from which the codings will be sampled:
    the mean (**Œº**) and the logarithm of the variance (**Œ≥**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `encode()` method calls the `encoder` model and splits the output in two,
    using the `chunk()` method, to obtain **Œº** and **Œ≥**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `sample_codings()` method takes **Œº** and **Œ≥** and samples the actual codings.
    For this, it first computes `torch.exp(0.5 * codings_logvar)` to get the codings‚Äô
    standard deviation **œÉ** (you can verify that this works mathematically). Then
    it uses the `torch.randn_like()` function to sample a random vector of the same
    shape as **œÉ** from the Gaussian distribution with mean 0 and standard deviation
    1, on the same device and with the same data type. Lastly, it multiplies this
    Gaussian noise by **œÉ**, adds **Œº**, and returns the result. This is the reparameterization
    trick we discussed earlier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `decode()` method simply calls the decoder model to produce the reconstructions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `forward()` method calls the encoder to get **Œº** and **Œ≥**, then it uses
    these parameters to sample the codings, which it decodes, and finally it returns
    a `VAEOutput` containing the reconstructions and the parameters **Œº** and **Œ≥**,
    which are all needed to compute the VAE loss.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Speaking of which, let‚Äôs now define the loss function, which is the sum of
    the reconstruction loss (MSE) and the latent loss (KL divergence):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The function first uses [Equation 18-4](#var_ae_latent_loss_equation_2) to
    compute the latent loss (`kl_div`) for each instance in the batch (by summing
    over the last dimension), then it computes the mean latent loss over all the instances
    in the batch (`kl_div.mean()`). Note that the reconstruction loss is the mean
    over all instances in the batch *and* all 784 pixels: this is why we divide the
    latent loss by 784 to ensure that the reconstruction loss and the latent loss
    have the same scale.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can train the model on the Fashion MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Generating Fashion MNIST Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let‚Äôs use this VAE to generate images that look like fashion items. All
    we need to do is sample random codings from a Gaussian distribution with mean
    0 and variance 1, and decode them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[Figure¬†18-13](#vae_generated_images_plot) shows the 21 generated images.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variational autoencoder-generated images of clothing items from the Fashion
    MNIST dataset, appearing fuzzy and lacking detail.](assets/hmls_1813.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-13\. Fashion MNIST images generated by the variational autoencoder
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The majority of these images look fairly convincing, if a bit too fuzzy. The
    rest are not great, but don‚Äôt be too harsh on the autoencoder‚Äîit only had a few
    minutes to learn, and you would get much better results by using convolutional
    layers!
  prefs: []
  type: TYPE_NORMAL
- en: 'Variational autoencoders make it possible to perform *semantic interpolation*:
    instead of interpolating between two images at the pixel level, which would look
    as if the two images were just overlaid, we can interpolate at the codings level.
    For example, if we sample two random codings and interpolate between them, then
    decode all of the interpolated codings, we get a sequence of images that gradually
    go from one fashion item to another (see [Figure¬†18-14](#semantic_interpolation_plot)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![A series of progressively morphing silhouettes demonstrating semantic interpolation
    between two clothing items.](assets/hmls_1814.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-14\. Semantic interpolation
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'There are a few variants of VAEs, for example, with different distributions
    for the latent variables. One important variant is discrete VAEs: let‚Äôs discuss
    them now.'
  prefs: []
  type: TYPE_NORMAL
- en: Discrete Variational Autoencoders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A *discrete VAE* (dVAE) is much like a VAE, except the codings are discrete
    rather than continuous: each coding vector contains *latent codes* (also called
    *categories*), each of which is an integer between 0 and *k* ‚Äì 1, where *k* is
    the number of possible latent codes. The length of the coding vector is often
    denoted as *d*. For example, if you choose *k* = 10 and *d* = 6, then there are
    one million possible coding vectors (10‚Å∂), such as [3, 0, 3, 9, 1, 4]. Discrete
    VAEs are very useful for tokenizing continuous inputs for transformers and other
    models. For example, they are at the core of models like BEiT and DALL¬∑E (see
    [Chapter¬†16](ch16.html#vit_chapter)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The most natural way to make VAEs discrete is to use a categorical distribution
    instead of a Gaussian distribution. This implies a couple of changes:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the encoder must output logits rather than means and variances. For each
    input image, it outputs a tensor of shape [*d*, *k*] containing logits, for example
    [[1.2, ‚Äì0.8, 0.5], [‚Äì1.3, 0.4, 0.3]] if *d* = 2 and *k* = 3.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Second, since categorical sampling is not a differentiable operation, we must
    once again use a reparameterization trick, but we cannot reuse the same as for
    regular VAEs: we need one designed for categorical distributions. The most popular
    one is the Gumbel-softmax trick. Instead of directly sampling from the categorical
    distribution, we call the `F.gumble_softmax()` function: this implements a differentiable
    approximation of categorical sampling. Given the previous logits, this function
    might output the discrete coding vector [0, 2].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The Gumbel distribution is used to model the maximum of a set of samples from
    another distribution. For example, it can be used to estimate the probability
    that a river will overflow within the next 10 years. If you add Gumbel noise to
    the logits, then take the argmax of the result, it is mathematically equivalent
    to categorical sampling. However, the argmax operation is not differentiable,
    so we replace it with the softmax during the backward pass: this gives us a differentiable
    approximation of categorical sampling.'
  prefs: []
  type: TYPE_NORMAL
- en: This idea was proposed in 2016 almost simultaneously by two independent teams
    of researchers, one from [DeepMind and Oxford University](https://homl.info/dvae1),‚Å†^([9](ch18.html#id4106))
    the other from [Google, Cambridge University, and Stanford University](https://homl.info/dvae2).‚Å†^([10](ch18.html#id4107))
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs implement a dVAE for Fashion MNIST:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, this code is very similar to the VAE code. Note that we set
    `hard=True` when calling the `F.gumbel_softmax()` function to ensure that the
    forward pass uses Gumbel-argmax (to obtain one-hot vectors of the sampled codes),
    while the backward pass uses the Gumbel-softmax approximation. Also note that
    we pass a temperature (a scalar) to this function: the logits will be divided
    by this temperature before calling the softmax function. The lower the temperature
    is, the closer the output distribution will be to one-hot vectors (this only affects
    the backward pass). In general, we use a temperature of 1 at the beginning of
    training, then gradually reduce it during training, down to a small value such
    as 0.1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function is also similar to the regular VAE loss: it‚Äôs the sum of
    a reconstruction loss (MSE) and a weighted latent loss (KL divergence). However,
    the KL divergence equation is a bit different since the latent distribution has
    changed. It‚Äôs now a uniform categorical distribution, where all possible codes
    are equally likely, so they each have a probability of 1 / *k*. Since log(1 /
    *k*) = ‚Äìlog(*k*), we can add log(*k*) instead of subtracting log(1 / *k*) in the
    KL divergence equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now train the model. Remember to update your training loop to reduce
    the temperature gradually during training, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the model is trained, you can generate new images by sampling random codings
    from a uniform distribution, one-hot encoding them, then decoding the resulting
    one-hot distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Another popular approach to discrete VAEs is called *vector quantization* (VQ-VAE),
    [proposed by DeepMind researchers in 2017](https://homl.info/vqvae).‚Å†^([11](ch18.html#id4111))
    Instead of producing logits, the encoder outputs *d* embeddings, each of dimensionality
    *e*. Then instead of sampling from a categorical distribution, the VQ-VAE maps
    each embedding to the index of the nearest embedding in a trainable embedding
    matrix of shape [*k*, *e*], called the *codebook*. This produces the integer codes.
    Finally, these codes are embedded using the embedding matrix and passed on to
    the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since replacing an embedding with the nearest codebook embedding is not a differentiable
    operation, the backward pass pretends that the codebook lookup step is the identity
    function, so the gradients just go straight through this operation: this is why
    this trick is called the *straight-through estimator* (STE). It‚Äôs an approximation
    that assumes that the gradients around the encoder embeddings are similar to the
    gradients around the nearest codebook embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: VQ-VAEs can be a bit tricky to implement correctly, but you can use a library
    like [*https://github.com/lucidrains/vector-quantize-pytorch*](https://github.com/lucidrains/vector-quantize-pytorch).
    On the positive side, training is more stable, and the codes are a bit easier
    to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: 'Discrete VAEs work pretty well for small images, but not so much for large
    images: the small-scale features may look good, but there will often be large-scale
    inconsistencies. To improve on this, you can use the trained dVAE to encode your
    whole training set (so each instance becomes a sequence of integers), then use
    this new training set to train a transformer: just treat the codes as tokens,
    and train the transformer using next-token prediction. Intuitively, the dVAE learns
    the vocabulary, while the transformer learns the grammar. Once the transformer
    is trained, you can generate a new image by first generating a sequence of codes
    using the transformer, then passing this sequence to the dVAE‚Äôs decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This two-stage approach also makes it easier to control the image generation
    process: when training the transformer, a textual description of the image can
    be fed to the transformer, for example as a prefix to the sequence of codes. We
    say that the transformer is *conditioned* on the description, which helps it predict
    the correct next code. This way, after training, we can guide the image generation
    process by providing a description of the image we desire. The transformer will
    use this description to generate the appropriate sequence of codes. This is exactly
    how the first DALL¬∑E system worked.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, the encoder and decoder are usually convolutional networks, so
    the latent representation is often organized as a grid (but it‚Äôs still flattened
    to a sequence to train the transformer). For example, the encoder may output a
    tensor of shape [256, 32, 32]: that‚Äôs a 32 √ó 32 grid containing 256-dimensional
    embeddings in each cell (or 256 logits in the case of Gumbel-Softmax dVAEs). After
    mapping these embeddings to the indices of the nearest embeddings in the codebook
    (or after categorical sampling), each image is represented as a 32 √ó 32 grid of
    integers (codes), with codes ranging between 0 and 255\. To generate a new image,
    you use the transformer to predict a sequence of 1,024 codes, organize them into
    a 32 √ó 32 grid, replace each code with its codebook vector, then pass the result
    to the decoder to generate the final image.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To improve the image quality, you can also stack two or more dVAEs, each producing
    a smaller grid than the previous one: this is called a *hierarchical VAE* (HVAE).
    The encoders are stacked, followed by the decoders in reverse order, and all are
    trained jointly. The loss is the sum of a single reconstruction loss plus multiple
    KL divergence losses (one per dVAE).'
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs now turn our attention to GANs. They are harder to train, but when you
    manage to get them to work, they produce pretty amazing images.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Generative adversarial networks were proposed in a [2014 paper](https://homl.info/gan)‚Å†^([12](ch18.html#id4125))
    by Ian Goodfellow et al., and although the idea got researchers excited almost
    instantly, it took a few years to overcome some of the difficulties of training
    GANs. Like many great ideas, it seems simple in hindsight: make neural networks
    compete against each other in the hope that this competition will push them to
    excel. As shown in [Figure¬†18-15](#gan_diagram), a GAN is composed of two neural
    networks:'
  prefs: []
  type: TYPE_NORMAL
- en: Generator
  prefs: []
  type: TYPE_NORMAL
- en: 'Takes a random coding as input (typically sampled from a Gaussian distribution)
    and outputs some data‚Äîtypically, an image. The coding is the latent representation
    of the image to be generated. So, as you can see, the generator offers the same
    functionality as a decoder in a variational autoencoder, and it can be used in
    the same way to generate new images: just feed it a random vector, and it outputs
    a brand-new image. However, it is trained very differently, as you will soon see.'
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator
  prefs: []
  type: TYPE_NORMAL
- en: Takes either a fake image from the generator or a real image from the training
    set as input, and must guess whether the input image is fake or real.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating a generative adversarial network (GAN) where the generator
    produces fake images from a random vector to trick the discriminator, which aims
    to distinguish between fake and real images.](assets/hmls_1815.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-15\. A generative adversarial network
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'During training, the generator and the discriminator have opposite goals: the
    discriminator tries to tell fake images from real images, while the generator
    tries to produce images that look real enough to trick the discriminator. Because
    the GAN is composed of two networks with different objectives, it cannot be trained
    like a regular neural network. Each training iteration is divided into two phases:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First phase: train the discriminator'
  prefs: []
  type: TYPE_NORMAL
- en: A batch of real images is sampled from the training set and is completed with
    an equal number of fake images produced by the generator. The labels are set to
    0 for fake images and 1 for real images, and the discriminator is trained on this
    labeled batch for one step, using the binary cross-entropy loss. Importantly,
    backpropagation only optimizes the weights of the discriminator during this phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Second phase: train the generator'
  prefs: []
  type: TYPE_NORMAL
- en: We first use the generator to produce another batch of fake images, and once
    again the discriminator is used to tell whether the images are fake or real. This
    time we do not add real images to the batch, and all the labels are set to 1 (real);
    in other words, we want the generator to produce images that the discriminator
    will (wrongly) believe to be real! Crucially, the weights of the discriminator
    are frozen during this step, so backpropagation only affects the weights of the
    generator.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The generator never actually sees any real images, yet it gradually learns to
    produce convincing fake images! All it gets is the gradients flowing back through
    the discriminator. Fortunately, the better the discriminator gets, the more information
    about the real images is contained in these secondhand gradients, so the generator
    can make significant progress.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs go ahead and build a simple GAN for Fashion MNIST.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to build the generator and the discriminator. The generator
    is similar to an autoencoder‚Äôs decoder‚Äîit takes a coding vector as input and outputs
    an image‚Äîand the discriminator is a regular binary classifier‚Äîit takes an image
    as input and ends with a dense layer containing a single unit and using the sigmoid
    activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the training loop is unusual, we need a new training function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As discussed earlier, you can see the two phases at each iteration: first the
    discriminator makes a gradient descent step, then it‚Äôs the generator‚Äôs turn. We
    use a separate optimizer for each. Let‚Äôs look in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Phase one
  prefs: []
  type: TYPE_NORMAL
- en: We feed a batch of real images to the discriminator and compute the loss given
    targets equal to one; indeed, we want the discriminator to predict that these
    images are real. We then generate some random codings and feed them to the generator
    to produce some fake images. Note that we call `detach()` on these images because
    we don‚Äôt want gradient descent to affect the generator in this phase. Then we
    pass these fake images to the discriminator and compute the loss given targets
    equal to zero; we want the discriminator to predict that these images are fake.
    The total discriminator loss is the `real_loss` plus the `fake_loss`. Finally,
    we perform the gradient descent step, improving the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: Phase two
  prefs: []
  type: TYPE_NORMAL
- en: 'We generate some fake images using the generator, and we pass them to the discriminator,
    like we just did. However, this time we don‚Äôt call `detach()` on the fake images
    since we want to train the generator. Moreover, we make the discriminator untrainable
    by setting `p.required_grad = False` for each parameter `p`. We then compute the
    loss using targets equal to one: indeed, we want the generator to fool the discriminator,
    so we want the discriminator to wrongly predict that these are real images. And
    finally, we perform a gradient descent step for the generator, and we make the
    discriminator trainable again.'
  prefs: []
  type: TYPE_NORMAL
- en: 'That‚Äôs it! After training, you can randomly sample some codings from a Gaussian
    distribution and feed them to the generator to produce new images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: If you display the generated images (see [Figure¬†18-16](#gan_generated_images_plot)),
    you will see that at the end of the first epoch, they already start to look like
    (very noisy) Fashion MNIST images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Noisy, black-and-white images attempting to depict Fashion MNIST items, generated
    by a GAN after one epoch of training.](assets/hmls_1816.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-16\. Images generated by the GAN after one epoch of training
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Unfortunately, the images never really get much better than that, and you may
    even find epochs where the GAN seems to be forgetting what it learned. Why is
    that? Well, it turns out that training a GAN can be challenging. Let‚Äôs see why.
  prefs: []
  type: TYPE_NORMAL
- en: The Difficulties of Training GANs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'During training, the generator and the discriminator constantly try to outsmart
    each other in a zero-sum game. As training advances, the game may end up in a
    state that game theorists call a *Nash equilibrium*, named after the mathematician
    John Nash. This occurs when no player would be better off changing their own strategy,
    assuming the other players do not change theirs. For example, a Nash equilibrium
    is reached when everyone drives on the left side of the road: no driver would
    be better off being the only one to switch sides. Of course, there is a second
    possible Nash equilibrium: when everyone drives on the *right* side of the road.
    Different initial states and dynamics may lead to one equilibrium or the other.
    In this example, there is a single optimal strategy once an equilibrium is reached
    (i.e., driving on the same side as everyone else), but a Nash equilibrium can
    involve multiple competing strategies (e.g., a predator chases its prey, the prey
    tries to escape, and neither would be better off changing their strategy).'
  prefs: []
  type: TYPE_NORMAL
- en: 'So how does this apply to GANs? Well, the authors of the GAN paper demonstrated
    that a GAN can only reach a single Nash equilibrium: that‚Äôs when the generator
    produces perfectly realistic images, and the discriminator is forced to guess
    (50% real, 50% fake). This fact is very encouraging, as it would seem that you
    just need to train the GAN long enough and it will eventually reach this equilibrium,
    giving you a perfect generator. Unfortunately, it‚Äôs not that simple: nothing guarantees
    that the equilibrium will ever be reached.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The biggest difficulty is called *mode collapse*: when the generator‚Äôs outputs
    gradually become less diverse. How can this happen? Suppose the generator gets
    better at producing convincing shoes than any other class. It will fool the discriminator
    a bit more with shoes, and this will encourage it to produce even more images
    of shoes. Gradually, it will forget how to produce anything else. Meanwhile, the
    only fake images that the discriminator will see will be shoes, so it will also
    forget how to discriminate fake images of other classes. Eventually, when the
    discriminator manages to discriminate the fake shoes from the real ones, the generator
    will be forced to move to another class. It may then become good at shirts, forgetting
    about shoes, and the discriminator will follow. The GAN may gradually cycle across
    a few classes, never really becoming very good at any of them (see the top row
    of [Figure¬†18-17](#gan_mode_collapse_diagram)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating mode collapse in GAN training, showing clustered fake
    examples in the top row versus diverse results in the bottom row with successful
    training.](assets/hmls_1817.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-17\. Mode collapse while training a GAN (top row) versus successful
    training without mode collapse (bottom row)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Moreover, because the generator and the discriminator are constantly pushing
    against each other, their parameters may end up oscillating and becoming unstable.
    Training may begin properly, then suddenly diverge for no apparent reason due
    to these instabilities. And since many factors affect these complex dynamics,
    GANs are very sensitive to the hyperparameters: you may have to spend a lot of
    effort fine-tuning them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These problems have kept researchers very busy since 2014\. Many papers have
    been published on this topic, some proposing new cost functions‚Å†^([13](ch18.html#id4132))
    (though a [2018 paper](https://homl.info/gansequal)‚Å†^([14](ch18.html#id4133))
    by Google researchers questions their efficiency) or techniques to stabilize training
    or to avoid the mode collapse issue. For example, a popular technique called *experience
    replay* consists of storing the images produced by the generator at each iteration
    in a replay buffer (gradually dropping older generated images) and training the
    discriminator using real images plus fake images drawn from this buffer (rather
    than just fake images produced by the current generator). This reduces the chances
    that the discriminator will overfit the latest generator‚Äôs outputs. Another common
    technique is called *mini-batch discrimination*: it measures how similar images
    are across the batch and provides this statistic to the discriminator, so it can
    easily reject a whole batch of fake images that lack diversity. This encourages
    the generator to produce a greater variety of images, reducing the chance of mode
    collapse (see the bottom row of [Figure¬†18-17](#gan_mode_collapse_diagram)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, this was a very active field of research, and much progress was made
    until quite recently: from *deep Convolutional GANs* (DCGANs) based on convolutional
    layers (see the notebook for an example), to *progressively growing GANs* that
    could produce high-resolution images, or *StyleGANs* that gave the user fine-grained
    control over the image generation process, it seemed like GANs had a bright future
    ahead of them. But when diffusion models started to produce amazing images as
    well, with a much more stable training process and more diverse images, GANs were
    quickly sidelined. So let‚Äôs now turn our attention to diffusion models.'
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The ideas behind diffusion models have been around for many years, but they
    were first formalized in their modern form in a [2015 paper](https://homl.info/diffusion)‚Å†^([15](ch18.html#id4147))
    by Jascha Sohl-Dickstein et al. from Stanford University and UC Berkeley. The
    authors applied tools from statistical mechanics to model a diffusion process,
    similar to a drop of milk diffusing in a cup of tea. The core idea is to train
    a model to learn the reverse process: start from the completely mixed state and
    gradually ‚Äúunmix‚Äù the milk from the tea. Using this idea, they obtained promising
    results in image generation, but since GANs produced more convincing images back
    then, and they did so much faster, diffusion models did not get as much attention.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in 2020, [Jonathan Ho et al.](https://homl.info/ddpm), also from UC Berkeley,
    managed to build a diffusion model capable of generating highly realistic images,
    which they called a *denoising diffusion probabilistic model* (DDPM).‚Å†^([16](ch18.html#id4148))
    A few months later, a [2021 paper](https://homl.info/ddpm2)‚Å†^([17](ch18.html#id4149))
    by OpenAI researchers Alex Nichol and Prafulla Dhariwal analyzed the DDPM architecture
    and proposed several improvements that allowed DDPMs to finally beat GANs: not
    only are DDPMs much easier to train than GANs, but the generated images are more
    diverse and of even higher quality. The main downside of DDPMs, as you will see,
    is that they take a very long time to generate images, compared to GANs or VAEs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So how exactly does a DDPM work? Well, suppose you start with a picture of
    a cat (like the one in [Figure¬†18-18](#denoising_model_diagram)), noted **x**[0],
    and at each time step *t* you add a little bit of Gaussian noise to the image,
    with mean 0 and variance *Œ≤*[*t*] (a scalar). This noise is independent for each
    pixel (using the same mean and variance): we call it *isotropic*. You first obtain
    the image **x**[1], then **x**[2], and so on, until the cat is completely hidden
    by the noise, impossible to see. The last time step is noted *T*. In the original
    DDPM paper, the authors used *T* = 1,000, and they scheduled the variance *Œ≤*[*t*]
    in such a way that the cat signal fades linearly between time steps 0 and *T*.
    In the improved DDPM paper, *T* was bumped up to 4,000, and the variance schedule
    was tweaked to change more slowly at the beginning and at the end. In short, we‚Äôre
    gradually drowning the cat in noise: this is called the *forward process*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram of the denoising diffusion probabilistic model (DDPM) showing the
    forward and reverse process, where a cat image gradually becomes obscured by noise
    until details are lost, illustrating how noise is added and then removed.](assets/hmls_1818.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-18\. The forward process *q* and reverse process *p*
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we add more and more Gaussian noise in the forward process, the distribution
    of pixel values becomes more and more Gaussian. One important detail I left out
    is that the pixel values get rescaled slightly at each step, by a factor of $StartRoot
    1 minus beta Subscript t Baseline EndRoot$ . This ensures that the mean of the
    pixel values gradually approaches 0, since the scaling factor is a bit smaller
    than 1 (imagine repeatedly multiplying a number by 0.99). It also ensures that
    the variance will gradually converge to 1\. This is because the standard deviation
    of the pixel values also gets scaled by $StartRoot 1 minus beta Subscript t Baseline
    EndRoot$ , so the variance gets scaled by 1 ‚Äì *Œ≤*[*t*] (i.e., the square of the
    scaling factor). But the variance cannot shrink to 0 since we‚Äôre adding Gaussian
    noise with variance *Œ≤*[*t*] at each step. And since variances add up when you
    sum Gaussian distributions, the variance must converge to 1 ‚Äì *Œ≤*[*t*] + *Œ≤*[*t*]
    = 1.
  prefs: []
  type: TYPE_NORMAL
- en: The forward diffusion process is summarized in [Equation 18-5](#forward_process_equation).
    This equation won‚Äôt teach you anything new about the forward process, but it‚Äôs
    useful to understand this type of mathematical notation, as it‚Äôs often used in
    ML papers. This equation defines the probability distribution *q* of **x**[*t*],
    given **x**[*t*‚Äì1] as a Gaussian distribution with mean **x**[*t*‚Äì1] times the
    scaling factor, and with a covariance matrix equal to *Œ≤*[*t*]**I**. This is the
    identity matrix **I** multiplied by *Œ≤*[*t*], which means that the noise is isotropic
    with variance *Œ≤*[*t*].
  prefs: []
  type: TYPE_NORMAL
- en: Equation 18-5\. Probability distribution *q* of the forward diffusion process
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $q left-parenthesis bold x Subscript t Baseline vertical-bar bold x Subscript
    t minus 1 Baseline right-parenthesis equals script upper N left-parenthesis StartRoot
    1 minus beta Subscript t Baseline EndRoot bold x Subscript t minus 1 Baseline
    comma beta Subscript t Baseline bold upper I right-parenthesis$
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, there‚Äôs a shortcut for the forward process: it‚Äôs possible to
    sample an image **x**[*t*] given **x**[0] without having to first compute **x**[1],
    **x**[2], ‚Ä¶‚Äã, **x**[*t*‚Äì1]. Indeed, since the sum of multiple independent Gaussian
    distributions is also a Gaussian distribution, all the noise can be added in just
    one shot. If we define *Œ±*[*t*] = 1 ‚Äì *Œ≤*[*t*], and *Œ±ÃÖ*[*t*] = *Œ±*[*1*] √ó *Œ±*[*2*]
    √ó ‚Ä¶‚Äã√ó *Œ±*[*t*] = $alpha overbar Subscript t Baseline equals product Underscript
    i equals 1 Overscript t Endscripts alpha Subscript t$ , then we can compute **x**[*t*]
    using [Equation 18-6](#fast_forward_process_equation). This is the equation we
    will be using, as it is much faster.'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 18-6\. Shortcut for the forward diffusion process
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $q left-parenthesis bold x Subscript t Baseline vertical-bar bold x 0 right-parenthesis
    equals script upper N left-parenthesis StartRoot alpha overbar Subscript t Baseline
    EndRoot bold x 0 comma left-parenthesis 1 minus alpha overbar Subscript t Baseline
    right-parenthesis bold upper I right-parenthesis$
  prefs: []
  type: TYPE_NORMAL
- en: 'Our goal, of course, is not to drown cats in noise. On the contrary, we want
    to create many new cats! We can do so by training a model that can perform the
    *reverse process*: going from **x**[*t*] to **x**[*t*‚Äì1]. We can then use it to
    remove a tiny bit of noise from an image, and repeat the operation many times
    until all the noise is gone. It‚Äôs not a basic noise filter that relies only on
    the neighboring pixels: instead, when noise is removed, it is replaced with realistic
    pixels, depending on the training data. For example, if we train the model on
    a dataset containing many cat images, then we can give it a picture entirely full
    of Gaussian noise, and the model will gradually make a brand new cat appear (see
    [Figure¬†18-18](#denoising_model_diagram)).'
  prefs: []
  type: TYPE_NORMAL
- en: OK, so let‚Äôs start coding! The first thing we need to do is to code the forward
    process. For this, we will first need to implement the variance schedule. How
    can we control how fast the cat disappears? At each time step *t*, the pixel values
    get multiplied by $StartRoot 1 minus beta Subscript t Baseline EndRoot$ and noise
    with mean 0 and variance *Œ≤*[*t*] gets added (as explained earlier). So, the part
    of the image‚Äôs variance that comes from the original cat image shrinks by a factor
    of *Œ±*[*t*] = 1 ‚Äì \beta_t at each step. After *t* time steps, it will have shrunk
    by a factor of *Œ±ÃÖ*[*t*] = *Œ±*[*1*] √ó *Œ±*[*2*] √ó ‚Ä¶‚Äã √ó *Œ±*[*t*]. It‚Äôs this ‚Äúcat
    signal‚Äù factor *Œ±ÃÖ*[*t*] that we want to schedule so it shrinks down from 1 to
    0 gradually between time steps 0 and *T*. In the improved DDPM paper, the authors
    schedule *Œ±ÃÖ*[*t*] according to [Equation 18-7](#variance_schedule_equation).
    This schedule is represented in [Figure¬†18-19](#variance_schedule_plot).
  prefs: []
  type: TYPE_NORMAL
- en: Equation 18-7\. Variance schedule equation for the forward diffusion process
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: $beta Subscript t Baseline equals 1 minus StartFraction alpha overbar Subscript
    t Baseline Over alpha overbar Subscript t minus 1 Baseline EndFraction with alpha
    overbar Subscript t Baseline equals StartFraction f left-parenthesis t right-parenthesis
    Over f left-parenthesis 0 right-parenthesis EndFraction and f left-parenthesis
    t right-parenthesis equals cosine squared left-parenthesis StartStartFraction
    StartFraction t Over upper T EndFraction plus s OverOver 1 plus s EndEndFraction
    dot StartFraction pi Over 2 EndFraction right-parenthesis$
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*s* is a tiny value which prevents *Œ≤*[*t*] from being too small near *t* =
    0\. In the paper, the authors used *s* = 0.008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Œ≤*[*t*] is clipped to be no larger than 0.999 to avoid instabilities near
    *t* = *T*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![hmls 1819](assets/hmls_1819.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-19\. Noise variance schedule *Œ≤*[*t*], and the remaining signal variance
    *Œ±ÃÖ*[*t*]
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let‚Äôs create a small function to compute *Œ±*[*t*], *Œ≤*[*t*], and *Œ±ÃÖ*[*t*],
    using [Equation 18-7](#variance_schedule_equation), and call this function with
    *T* = 4,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To train our model to reverse the diffusion process, we will need noisy images
    from different time steps of the forward process. For this, let‚Äôs create a function
    that will take an image **x**[0] and a time step *t* using [Equation 18-6](#fast_forward_process_equation),
    and return a noisy image **x**[*t*]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The model will need both the noisy image **x**[*t*] and the time step *t*,
    so let‚Äôs create a small class that will hold both. We‚Äôll give it a handy `to()`
    method to move both **x**[*t*] and *t* to the GPU:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let‚Äôs create a dataset wrapper class. It takes an image dataset‚ÄîFashion
    MNIST in our case‚Äîand preprocesses the images so their pixel values range between
    ‚Äì1 and +1 (this is optional but usually works better), and it uses the `forward_diffusion()`
    function to add noise to the image. Then it wraps the resulting noisy image as
    well as the time step in a `DiffusionSample` object, and returns it along with
    the target, which is the unscaled noise `eps`, before it was scaled by $StartRoot
    1 minus alpha overbar Subscript t Baseline EndRoot$ and added to the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You may be wondering why not predict the original image directly, rather than
    the unscaled noise? One reason is empirical: the authors tried both approaches,
    and they observed that predicting the noise rather than the image led to more
    stable training and better results. The other reason is that the noise is Gaussian,
    which allows for some mathematical simplifications: in particular, the KL divergence
    between two Gaussian distributions is proportional to the squared distance between
    their means, so we can use the MSE loss, which is simple, fast, and quite stable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we‚Äôre ready to build the actual diffusion model itself. It can be any model
    you want, as long as it takes a `DiffusionSample` as input and outputs images
    of the same shape as the input images. The DDPM authors used a modified [U-Net
    architecture](https://homl.info/unet),‚Å†^([18](ch18.html#id4152)) which has many
    similarities with the FCN architecture we discussed in [Chapter¬†12](ch12.html#cnn_chapter)
    for semantic segmentation. U-Net is a convolutional neural network that gradually
    downsamples the input images, then gradually upsamples them again, with skip connections
    crossing over from each level of the downsampling part to the corresponding level
    in the upsampling part. To take into account the time steps, they were encoded
    using a fixed sinusoidal encoding (i.e., the same technique as the positional
    encodings in the original Transformer architecture). At every level in the U-Net
    architecture, they passed these time encodings through `Linear` layers and fed
    them to the U-Net. Lastly, they also used multi-head attention layers at various
    levels. See this chapter‚Äôs notebook for a basic implementation (it‚Äôs too long
    to copy here, and the details don‚Äôt matter: many other model architectures would
    work just fine).'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'For training, the authors noted that using the MAE loss worked better than
    the MSE. You can also use the Huber loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Once the model is trained, you can use it to generate new images by sampling
    **x**[*T*] randomly from a Gaussian distribution with mean 0 and variance 1, then
    using [Equation 18-8](#reverse_diffusion_equation) to get **x**[*T*‚Äì1]. Then use
    this equation 3,999 more times until you get **x**[0]. If all went well, **x**[0]
    should look like a regular Fashion MNIST image!
  prefs: []
  type: TYPE_NORMAL
- en: Equation 18-8\. Going one step in reverse in the DDPM diffusion process
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <mrow><msub><mi>ùê±</mi> <mrow><mi>t</mi> <mo>‚àí</mo> <mn>1</mn></mrow></msub>
    <mo>=</mo> <mfrac><mn>1</mn> <msqrt><msub><mi>Œ±</mi> <mi>t</mi></msub></msqrt></mfrac>
    <mrow><mo fence="true" form="prefix">(</mo> <msub><mi>ùê±</mi> <mi>t</mi></msub>
    <mo>‚àí</mo> <mfrac><msub><mi>Œ≤</mi> <mi>t</mi></msub> <msqrt><mrow><mn>1</mn> <mo>‚àí</mo>
    <msub><menclose notation="top"><mi>Œ±</mi></menclose> <mi>t</mi></msub></mrow></msqrt></mfrac>
    <msub><mi mathvariant="bold">Œµ</mi> <mi mathvariant="bold">Œ∏</mi></msub> <mo form="prefix"
    stretchy="false">(</mo> <msub><mi>ùê±</mi> <mi>t</mi></msub> <mo lspace="0%" rspace="0%"
    separator="true">,</mo> <mi>t</mi> <mo form="postfix" stretchy="false">)</mo>
    <mo fence="true" form="postfix">)</mo></mrow> <mo>+</mo> <msqrt><msub><mi>Œ≤</mi>
    <mi>t</mi></msub></msqrt> <mi>ùê≥</mi></mrow>
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation, **Œµ[Œ∏]**(**x**[*t*], *t*) represents the noise predicted
    by the model given the input image **x**[*t*] and the time step *t*. The **Œ∏**
    represents the model parameters. Moreover, **z** is Gaussian noise with mean 0
    and variance 1\. This makes the reverse process stochastic: if you run it multiple
    times, you will get different images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This works well, but it requires 4,000 iterations to generate an image! That‚Äôs
    too slow. Luckily, just a few months after the DDPM paper, researchers from Stanford
    University proposed a technique named the [denoising diffusion implicit model
    (DDIM)](https://homl.info/ddim)‚Å†^([19](ch18.html#id4157)) to generate images in
    much fewer steps: instead of going from *t* = 4,000 down to 0 one step at a time,
    DDIM can go down any number of time steps at a time, using [Equation 18-9](#ddim_equation).
    Moreover, the training process is exactly the same as for DDPM, so we can simply
    reuse our trained DDPM model.'
  prefs: []
  type: TYPE_NORMAL
- en: Equation 18-9\. Going multiple steps in reverse with DDIM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <mtable displaystyle="true" style="width:100%;"><mtr><mtd style="padding-left:1em;padding-right:0em;"><mtable
    displaystyle="true" class="tml-jot"><mtr><mtd class="tml-right" style="padding-left:0em;padding-right:0em;"><mrow><msub><mi>ùê±</mi>
    <mi>p</mi></msub> <mo>=</mo> <msqrt><msub><menclose notation="top"><mi>Œ±</mi></menclose>
    <mi>p</mi></msub></msqrt> <msub><mover><mi>ùê±</mi> <mo stretchy="false" class="wbk-acc"
    style="math-depth:0;">^</mo></mover> <mn>0</mn></msub> <mo>+</mo> <msqrt><mrow><mn>1</mn>
    <mo>‚àí</mo> <msub><menclose notation="top"><mi>Œ±</mi></menclose> <mi>p</mi></msub>
    <mo>‚àí</mo> <msup><mi>œÉ</mi> <mn>2</mn></msup></mrow></msqrt> <mo>‚ãÖ</mo> <msub><mi
    mathvariant="bold">Œµ</mi> <mi mathvariant="bold">Œ∏</mi></msub> <mo form="prefix"
    stretchy="false">(</mo> <msub><mi>ùê±</mi> <mi>t</mi></msub> <mo lspace="0%" rspace="0%"
    separator="true">,</mo> <mi>t</mi> <mo form="postfix" stretchy="false">)</mo>
    <mo>+</mo> <mi>œÉ</mi> <mi>ùê≥</mi></mrow></mtd></mtr> <mtr><mtd style="padding-left:0em;padding-right:0em;"><mrow><mtext>where
    ‚ÄÖ‚Ää</mtext> <msub><mover><mi>ùê±</mi> <mo stretchy="false" class="wbk-acc" style="math-depth:0;">^</mo></mover>
    <mn>0</mn></msub> <mo>=</mo> <mfrac><mn>1</mn> <msqrt><msub><menclose notation="top"><mi>Œ±</mi></menclose>
    <mi>t</mi></msub></msqrt></mfrac> <mrow><mo fence="true" form="prefix">(</mo>
    <msub><mi>ùê±</mi> <mi>t</mi></msub> <mo>‚àí</mo> <msqrt><mrow><mn>1</mn> <mo>‚àí</mo>
    <msub><menclose notation="top"><mi>Œ±</mi></menclose> <mi>t</mi></msub></mrow></msqrt>
    <msub><mi mathvariant="bold">Œµ</mi> <mi mathvariant="bold">Œ∏</mi></msub> <mo form="prefix"
    stretchy="false">(</mo> <msub><mi>ùê±</mi> <mi>t</mi></msub> <mo lspace="0%" rspace="0%"
    separator="true">,</mo> <mi>t</mi> <mo form="postfix" stretchy="false">)</mo>
    <mo fence="true" form="postfix">)</mo></mrow></mrow></mtd></mtr> <mtr><mtd style="padding-left:0em;padding-right:0em;"><mrow><mtext>and
    ‚ÄÖ‚Ää</mtext> <msup><mi>œÉ</mi> <mn>2</mn></msup> <mo>=</mo> <mi>Œ∑</mi> <mrow><mo
    fence="true" form="prefix">(</mo> <mfrac><mrow><mn>1</mn> <mo>‚àí</mo> <msub><menclose
    notation="top"><mi>Œ±</mi></menclose> <mi>p</mi></msub></mrow> <mrow><mn>1</mn>
    <mo>‚àí</mo> <msub><menclose notation="top"><mi>Œ±</mi></menclose> <mi>t</mi></msub></mrow></mfrac>
    <mo fence="true" form="postfix">)</mo></mrow> <msub><mi>Œ≤</mi> <mi>t</mi></msub></mrow></mtd></mtr></mtable></mtd></mtr></mtable>
  prefs: []
  type: TYPE_NORMAL
- en: 'In this equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Œµ[Œ∏]**(**x**[*t*], *t*), **Œ∏**, and **z** have the same meanings as in [Equation
    18-8](#reverse_diffusion_equation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p* represents any time step before *t*. For example, it could be *p* = *t*
    ‚Äì 50.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Œ∑* is a hyperparameter that controls how much randomness should be used during
    generation, from 0 (no randomness, fully deterministic) to 1 (just like DDPM).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let‚Äôs write a function that implements this reverse process, and call it to
    generate a few images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: This time the generation will only take a few seconds, and it will produce images
    such as the ones shown in [Figure¬†18-20](#ddim_generated_images_plot). Granted,
    they‚Äôre not very impressive, but we‚Äôve only trained the model for a few minutes
    on Fashion MNIST. Give it a try on a larger dataset and train it for a few hours
    to get more impressive results.
  prefs: []
  type: TYPE_NORMAL
- en: '![A grid of low-resolution, black-and-white images of clothing items, generated
    using a diffusion model with limited training on the Fashion MNIST dataset.](assets/hmls_1820.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-20\. Images generated by DDIM accelerated diffusion
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Diffusion models have made tremendous progress since 2020\. In particular, a
    paper published in December 2021 by [Robin Rombach, et al.](https://homl.info/latentdiff)‚Å†^([20](ch18.html#id4163))
    introduced *latent diffusion models*, where the diffusion process takes place
    in latent space, rather than in pixel space. To achieve this, a powerful autoencoder
    is used to compress each training image into a much smaller latent space, where
    the diffusion process takes place, then the autoencoder is used to decompress
    the final latent representation, generating the output image. This considerably
    speeds up image generation, and reduces training time and cost dramatically. Importantly,
    the quality of the generated images is outstanding.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the researchers also adapted various conditioning techniques to guide
    the diffusion process using text prompts, images, or any other inputs. This makes
    it possible to quickly produce any image you might fancy. You can also condition
    the image generation process using an input image. This enables many applications,
    such as outpainting‚Äîwhere an input image is extended beyond its borders‚Äîor inpainting‚Äîwhere
    holes in an image are filled in.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, a powerful pretrained latent diffusion model named *Stable Diffusion*
    (SD) was open sourced in August 2022 by a collaboration between LMU Munich and
    a few companies, including StabilityAI, and Runway, with support from EleutherAI
    and LAION. Now anyone can generate mindblowing images in seconds, for free, even
    on a regular laptop. For example, you can use the Hugging Face Diffusers library
    to load SD (e.g., the turbo variant), create an image generation pipeline for
    text-to-image, and generate an image of an orangutan reading a book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '![A digitally generated image of an orangutan closely reading an open book,
    illustrating the creative capabilities of the Stable Diffusion model.](assets/hmls_1821.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18-21\. A picture generated by Stable Diffusion using the Diffusers library
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The possibilities are endless!
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter we will move on to an entirely different branch of deep
    learning: deep reinforcement learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the main tasks that autoencoders are used for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Suppose you want to train a classifier, and you have plenty of unlabeled training
    data but only a few thousand labeled instances. How can autoencoders help? How
    would you proceed?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If an autoencoder perfectly reconstructs the inputs, is it necessarily a good
    autoencoder? How can you evaluate the performance of an autoencoder?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are undercomplete and overcomplete autoencoders? What is the main risk
    of an excessively undercomplete autoencoder? What about the main risk of an overcomplete
    autoencoder?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you tie weights in a stacked autoencoder? What is the point of doing
    so?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a generative model? Can you name a type of generative autoencoder?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a GAN? Can you name a few tasks where GANs can shine?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the main difficulties when training GANs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are diffusion models good at? What is their main limitation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Try using a denoising autoencoder to pretrain an image classifier. You can
    use MNIST (the simplest option), or a more complex image dataset such as [CIFAR10](https://homl.info/122)
    if you want a bigger challenge. Regardless of the dataset you‚Äôre using, follow
    these steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the dataset into a training set and a test set. Train a deep denoising
    autoencoder on the full training set.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Check that the images are fairly well reconstructed. Visualize the images that
    most activate each neuron in the coding layer.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a classification DNN, reusing the lower layers of the autoencoder. Train
    it using only 500 images from the training set. Does it perform better with or
    without pretraining?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a variational autoencoder on the image dataset of your choice, and use
    it to generate images. Alternatively, you can try to find an unlabeled dataset
    that you are interested in and see if you can generate new samples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a DCGAN to tackle the image dataset of your choice, and use it to generate
    images. Add experience replay and see if this helps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Train a diffusion model on your preferred image dataset (e.g., `torchvision.datasets.Flowers102`),
    and generate nice images. Next, add the image class as an extra input to the model,
    and retrain it: you should now be able to control the class of the generated image.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Solutions to these exercises are available at the end of this chapter‚Äôs notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  prefs: []
  type: TYPE_NORMAL
- en: '^([1](ch18.html#id4015-marker)) William G. Chase and Herbert A. Simon, ‚ÄúPerception
    in Chess‚Äù, *Cognitive Psychology* 4, no. 1 (1973): 55‚Äì81.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch18.html#id4035-marker)) Hint: one approach is to create a custom `AutoencoderDataset`
    class that wraps a given dataset and replaces the targets with the inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([3](ch18.html#id4062-marker)) Yoshua Bengio et al., ‚ÄúGreedy Layer-Wise Training
    of Deep Networks‚Äù, *Proceedings of the 19th International Conference on Neural
    Information Processing Systems* (2006): 153‚Äì160.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([4](ch18.html#id4066-marker)) Jonathan Masci et al., ‚ÄúStacked Convolutional
    Auto-Encoders for Hierarchical Feature Extraction‚Äù, *Proceedings of the 21st International
    Conference on Artificial Neural Networks* 1 (2011): 52‚Äì59.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch18.html#id4073-marker)) Pascal Vincent et al., ‚ÄúExtracting and Composing
    Robust Features with Denoising Autoencoders‚Äù, *Proceedings of the 25th International
    Conference on Machine Learning* (2008): 1096‚Äì1103.'
  prefs: []
  type: TYPE_NORMAL
- en: '^([6](ch18.html#id4074-marker)) Pascal Vincent et al., ‚ÄúStacked Denoising Autoencoders:
    Learning Useful Representations in a Deep Network with a Local Denoising Criterion‚Äù,
    *Journal of Machine Learning Research* 11 (2010): 3371‚Äì3408.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([7](ch18.html#id4088-marker)) Diederik Kingma and Max Welling, ‚ÄúAuto-Encoding
    Variational Bayes‚Äù, arXiv preprint arXiv:1312.6114 (2013).
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch18.html#id4091-marker)) Both these properties make VAEs rather similar
    to RBMs, but they are easier to train, and the sampling process is much faster
    (with RBMs you need to wait for the network to stabilize into a ‚Äúthermal equilibrium‚Äù
    before you can sample a new instance).
  prefs: []
  type: TYPE_NORMAL
- en: '^([9](ch18.html#id4106-marker)) Chris J. Maddison et al., ‚ÄúThe Concrete Distribution:
    A Continuous Relaxation of Discrete Random Variables‚Äù, arXiv preprint arXiv:1611.00712
    (2016).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([10](ch18.html#id4107-marker)) Eric Jang et al., ‚ÄúCategorical Reparameterization
    with Gumbel-Softmax‚Äù, arXiv preprint arXiv:1611.01144 (2016).
  prefs: []
  type: TYPE_NORMAL
- en: ^([11](ch18.html#id4111-marker)) Aaron van den Oord et al., ‚ÄúNeural Discrete
    Representation Learning‚Äù, arXiv preprint arXiv:1711.00937 (2017).
  prefs: []
  type: TYPE_NORMAL
- en: '^([12](ch18.html#id4125-marker)) Ian Goodfellow et al., ‚ÄúGenerative Adversarial
    Nets‚Äù, *Proceedings of the 27th International Conference on Neural Information
    Processing Systems* 2 (2014): 2672‚Äì2680.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([13](ch18.html#id4132-marker)) For a nice comparison of the main GAN losses,
    check out this great [GitHub project by Hwalsuk Lee](https://homl.info/ganloss).
  prefs: []
  type: TYPE_NORMAL
- en: '^([14](ch18.html#id4133-marker)) Mario Lucic et al., ‚ÄúAre GANs Created Equal?
    A Large-Scale Study‚Äù, *Proceedings of the 32nd International Conference on Neural
    Information Processing Systems* (2018): 698‚Äì707.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([15](ch18.html#id4147-marker)) Jascha Sohl-Dickstein et al., ‚ÄúDeep Unsupervised
    Learning using Nonequilibrium Thermodynamics‚Äù, arXiv preprint arXiv:1503.03585
    (2015).
  prefs: []
  type: TYPE_NORMAL
- en: ^([16](ch18.html#id4148-marker)) Jonathan Ho et al., ‚ÄúDenoising Diffusion Probabilistic
    Models‚Äù (2020).
  prefs: []
  type: TYPE_NORMAL
- en: ^([17](ch18.html#id4149-marker)) Alex Nichol and Prafulla Dhariwal, ‚ÄúImproved
    Denoising Diffusion Probabilistic Models‚Äù (2021).
  prefs: []
  type: TYPE_NORMAL
- en: '^([18](ch18.html#id4152-marker)) Olaf Ronneberger et al., ‚ÄúU-Net: Convolutional
    Networks for Biomedical Image Segmentation‚Äù, arXiv preprint arXiv:1505.04597 (2015).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([19](ch18.html#id4157-marker)) Jiaming Song et al., ‚ÄúDenoising Diffusion Implicit
    Models‚Äù, arXiv preprint arXiv:2010.02502 (2020).
  prefs: []
  type: TYPE_NORMAL
- en: ^([20](ch18.html#id4163-marker)) Robin Rombach, Andreas Blattmann, et al., ‚ÄúHigh-Resolution
    Image Synthesis with Latent Diffusion Models‚Äù, arXiv preprint arXiv:2112.10752
    (2021).
  prefs: []
  type: TYPE_NORMAL
