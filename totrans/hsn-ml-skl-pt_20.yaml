- en: Chapter 18\. Autoencoders, GANs, and Diffusion Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第18章\. 自编码器、GANs和扩散模型
- en: 'Autoencoders are artificial neural networks capable of learning dense representations
    of the input data, called *latent representations* or *codings*, without any supervision
    (i.e., the training set is unlabeled). These codings typically have a much lower
    dimensionality than the input data, making autoencoders useful for dimensionality
    reduction (see [Chapter 7](ch07.html#dimensionality_chapter)), especially for
    visualization purposes. Autoencoders also act as feature detectors, and they can
    be used for unsupervised pretraining of deep neural networks (as we discussed
    in [Chapter 11](ch11.html#deep_chapter)). They are also commonly used for anomaly
    detection, as we will see. Lastly, some autoencoders are *generative models*:
    they are capable of randomly generating new data that looks very similar to the
    training data. For example, you could train an autoencoder on pictures of faces,
    and it would then be able to generate new faces.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一种人工神经网络，能够在没有任何监督的情况下（即训练集是无标签的）学习输入数据的密集表示，称为*潜在表示*或*编码*。这些编码通常比输入数据具有更低的维度，这使得自编码器在降维（见[第7章](ch07.html#dimensionality_chapter)）方面非常有用，尤其是用于可视化目的。自编码器还充当特征检测器，并且可以用于深度神经网络的非监督预训练（正如我们在[第11章](ch11.html#deep_chapter)中讨论的那样）。它们也常用于异常检测，我们将会看到。最后，一些自编码器是*生成模型*：它们能够随机生成与训练数据非常相似的新数据。例如，你可以在面部图片上训练一个自编码器，然后它就能够生成新的面部。
- en: '*Generative adversarial networks* (GANs) are also neural nets capable of generating
    data. In fact, they can generate pictures of faces so convincing that it is hard
    to believe the people they represent do not exist. You can judge for yourself
    by visiting [*https://thispersondoesnotexist.com*](https://thispersondoesnotexist.com),
    a website that shows faces generated by a GAN architecture called *StyleGAN*.
    GANs have been widely used for super resolution (increasing the resolution of
    an image), [colorization](https://github.com/jantic/DeOldify), powerful image
    editing (e.g., replacing photo bombers with realistic background), turning simple
    sketches into photorealistic images, predicting the next frames in a video, augmenting
    a dataset (to train other models), generating other types of data (such as text,
    audio, and time series), identifying the weaknesses in other models to strengthen
    them, and more.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成对抗网络* (GANs)也是一种能够生成数据的神经网络。实际上，它们可以生成逼真的面部图片，以至于很难相信他们所代表的人不存在。你可以通过访问[*https://thispersondoesnotexist.com*](https://thispersondoesnotexist.com)，一个展示由名为*StyleGAN*的GAN架构生成的面部网站的链接，来判断自己。GANs已被广泛用于超分辨率（提高图像分辨率）、[着色](https://github.com/jantic/DeOldify)、强大的图像编辑（例如，用逼真的背景替换照片中的捣乱者）、将简单的素描变成逼真的图像、预测视频中的下一帧、增强数据集（以训练其他模型）、生成其他类型的数据（如文本、音频和时间序列）、识别其他模型的弱点以增强它们，等等。'
- en: However, since the early 2020s, GANs have been largely replaced by *diffusion
    models*, which can generate more diverse and higher-quality images than GANs,
    while also being much easier to train. However, diffusion models are much slower
    to run, so GANs are still useful when you need very fast generation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自从2020年代初以来，GANs在很大程度上已被*扩散模型*所取代，这些模型可以生成比GANs更多样化、更高品质的图像，同时训练起来也容易得多。然而，扩散模型的运行速度要慢得多，所以当你需要非常快速的生成时，GANs仍然很有用。
- en: 'Autoencoders, GANs, and diffusion models are all unsupervised, learn latent
    representations, can be used as generative models, and have many similar applications.
    However, they work very differently:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器、GANs和扩散模型都是无监督的，学习潜在表示，可以用作生成模型，并且有许多相似的应用。然而，它们的工作方式非常不同：
- en: Autoencoders
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器
- en: Autoencoders simply learn to copy their inputs to their outputs. This may sound
    like a trivial task, but as you will see, constraining the network in various
    ways can make the task arbitrarily difficult. For example, you can limit the size
    of the latent representations, or you can add noise to the inputs and train the
    network to recover the original inputs. These constraints prevent the autoencoder
    from trivially copying the inputs directly to the outputs, which forces it to
    learn efficient ways of representing the data. In short, the codings are byproducts
    of the autoencoder learning the identity function under some constraints.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器简单地学习将它们的输入复制到输出。这听起来可能像是一项微不足道的任务，但正如你将看到的，以各种方式约束网络可以使任务变得任意困难。例如，你可以限制潜在表示的大小，或者你可以向输入添加噪声，并训练网络恢复原始输入。这些约束阻止了自编码器直接将输入复制到输出，这迫使它学习高效的数据表示方法。简而言之，编码是自编码器在某种约束下学习恒等函数的副产品。
- en: GANs
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）
- en: 'GANs are composed of two neural networks: a *generator* that tries to generate
    data that looks similar to the training data, and a *discriminator* that tries
    to tell real data from fake data. This architecture is very original in deep learning
    in that the generator and the discriminator compete against each other during
    training; this is called *adversarial training*. The generator is often compared
    to a criminal trying to make realistic counterfeit money, while the discriminator
    is like the police investigator trying to tell real money from fake.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GANs）由两个神经网络组成：一个*生成器*，它试图生成看起来与训练数据相似的数据，以及一个*判别器*，它试图区分真实数据和伪造数据。这种架构在深度学习领域非常独特，因为在训练过程中生成器和判别器相互竞争；这被称为*对抗训练*。生成器通常被比作一个试图制作逼真假币的罪犯，而判别器则像是一个试图区分真币和假币的警察调查员。
- en: Diffusion models
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 演化模型
- en: A diffusion model is trained to gradually remove noise from an image. If you
    then take an image entirely full of random noise and repeatedly run the diffusion
    model on that image, a high-quality image will gradually emerge, similar to the
    training images (but not identical).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 演化模型被训练以逐渐从图像中去除噪声。如果你然后取一个完全充满随机噪声的图像，并反复在该图像上运行演化模型，一个高质量的图像将逐渐出现，类似于训练图像（但并不相同）。
- en: In this chapter we will start by exploring in more depth how autoencoders work
    and how to use them for dimensionality reduction, feature extraction, unsupervised
    pretraining, or as generative models. This will naturally lead us to GANs. We
    will build a simple GAN to generate fake images, but we will see that training
    is often quite difficult. We will discuss the main difficulties you will encounter
    with adversarial training, as well as some of the main techniques to work around
    these difficulties. And lastly, we will build and train a diffusion model—specifically
    a *denoising diffusion probabilistic model* (DDPM)—and use it to generate images.
    Let’s start with autoencoders!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更深入地探讨自编码器的工作原理以及如何使用它们进行降维、特征提取、无监督预训练或作为生成模型。这自然会引导我们到生成对抗网络（GANs）。我们将构建一个简单的GAN来生成假图像，但我们会看到训练通常相当困难。我们将讨论你在对抗训练中会遇到的主要困难，以及一些解决这些困难的主要技术。最后，我们将构建和训练一个演化模型——具体来说是一个*去噪演化概率模型*（DDPM）——并使用它来生成图像。让我们从自编码器开始吧！
- en: Efficient Data Representations
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高效的数据表示
- en: Which of the following number sequences do you find the easiest to memorize?
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你觉得以下哪个数字序列最容易记住？
- en: 40, 27, 25, 36, 81, 57, 10, 73, 19, 68
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 40, 27, 25, 36, 81, 57, 10, 73, 19, 68
- en: 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28, 26, 24, 22, 20, 18, 16, 14
- en: At first glance, it would seem that the first sequence should be easier, since
    it is much shorter. However, if you look carefully at the second sequence, you
    will notice that it is just the list of even numbers from 50 down to 14\. Once
    you notice this pattern, the second sequence becomes much easier to memorize than
    the first because you only need to remember the pattern (i.e., decreasing even
    numbers) and the starting and ending numbers (i.e., 50 and 14). Note that if you
    could quickly and easily memorize very long sequences, you would not care much
    about the existence of a pattern in the second sequence. You would just learn
    every number by heart, and that would be that. The fact that it is hard to memorize
    long sequences is what makes it useful to recognize patterns, and hopefully this
    clarifies why constraining an autoencoder during training pushes it to discover
    and exploit patterns in the data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，似乎第一个序列应该更容易，因为它要短得多。然而，如果你仔细观察第二个序列，你会注意到它只是从50到14的偶数列表。一旦你注意到这个模式，第二个序列就比第一个更容易记住，因为你只需要记住模式（即递减的偶数）以及起始和结束数字（即50和14）。请注意，如果你能够快速轻松地记住非常长的序列，你就不太会在意第二个序列中是否存在模式。你只需逐个记住每个数字，然后就可以了。难以记住长序列的事实使得识别模式变得有用，希望这能阐明为什么在训练过程中对自动编码器进行约束会推动它发现并利用数据中的模式。
- en: The relationship among memory, perception, and pattern matching was famously
    studied by [William Chase and Herbert Simon](https://homl.info/111)⁠^([1](ch18.html#id4015))
    in the early 1970s. They observed that expert chess players were able to memorize
    the positions of all the pieces in a game by looking at the board for just five
    seconds, a task that most people would find impossible. However, this was only
    the case when the pieces were placed in realistic positions (from actual games),
    not when the pieces were placed randomly. Chess experts don’t have a much better
    memory than you and I; they just see chess patterns more easily, thanks to their
    experience with the game. Noticing patterns helps them store information efficiently.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆、感知和模式匹配之间的关系在20世纪70年代初由[威廉·蔡斯和赫伯特·西蒙](https://homl.info/111)⁠^([1](ch18.html#id4015))进行了著名的研究。他们观察到，专家棋手只需看五秒钟棋盘，就能记住游戏中所有棋子的位置，这对于大多数人来说是不可能的任务。然而，这只在棋子放置在现实位置（来自实际游戏）时才成立，而不是当棋子随机放置时。象棋专家的记忆并不比我们好多少；他们只是更容易看到棋局模式，这得益于他们对游戏的经验。注意模式有助于他们有效地存储信息。
- en: 'Just like the chess players in this memory experiment, an autoencoder looks
    at the inputs, converts them to an efficient latent representation, and is then
    capable of reconstructing something that (hopefully) looks very close to the inputs.
    An autoencoder is always composed of two parts: an *encoder* (or *recognition
    network*) that converts the inputs to a latent representation, followed by a *decoder*
    (or *generative network*) that converts the internal representation to the outputs.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 就像这个记忆实验中的象棋选手一样，自动编码器查看输入，将它们转换为有效的潜在表示，然后能够重建出（希望）非常接近输入的东西。自动编码器始终由两部分组成：一个*编码器*（或*识别网络*），它将输入转换为潜在表示，然后是一个*解码器*（或*生成网络*），它将内部表示转换为输出。
- en: In the example shown in [Figure 18-1](#encoder_decoder_diagram), the autoencoder
    is a regular multilayer perceptron (MLP; see [Chapter 9](ch09.html#ann_chapter)).
    Since it must reconstruct its inputs, the number of neurons in the output layer
    must be equal to the number of inputs (i.e., three in this example). The lower
    part of the network is the encoder (in this case it’s a single layer with two
    neurons), and the upper part is the decoder. The outputs are often called the
    *reconstructions* because the autoencoder tries to reconstruct the inputs. The
    cost function always contains a *reconstruction loss* that penalizes the model
    when the reconstructions are different from the inputs.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图18-1](#encoder_decoder_diagram)所示的示例中，自动编码器是一个常规的多层感知器（MLP；参见[第9章](ch09.html#ann_chapter)）。由于它必须重建其输入，输出层的神经元数量必须等于输入的数量（即在这个例子中是三个）。网络的下半部分是编码器（在这种情况下它是一个包含两个神经元的单层），而上半部分是解码器。输出通常被称为*重建*，因为自动编码器试图重建输入。成本函数总是包含一个*重建损失*，当重建与输入不同时，会对模型进行惩罚。
- en: '![Illustration showing a chess memory experiment with a chessboard transitioning
    to a latent representation, and a diagram of an autoencoder with an encoder layer
    reducing inputs to two neurons and a decoder layer reconstructing three outputs.](assets/hmls_1801.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![展示棋盘从过渡到潜在表示的棋盘记忆实验图，以及一个自动编码器图，其中编码器层将输入减少到两个神经元，解码器层重构三个输出。](assets/hmls_1801.png)'
- en: Figure 18-1\. The chess memory experiment (left) and a simple autoencoder (right)
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-1. 棋盘记忆实验（左）和简单的自动编码器（右）
- en: Because the internal representation has a lower dimensionality than the input
    data (in this example, it is 2D instead of 3D), the autoencoder is said to be
    *undercomplete*. An undercomplete autoencoder cannot trivially copy its inputs
    to the codings, yet it must find a way to output a copy of its inputs. It is forced
    to compress the data, thereby learning the most important features in the input
    data (and dropping the unimportant ones).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因为内部表示的维度低于输入数据（在这个例子中，它是2D而不是3D），所以自动编码器被称为*欠完备的*。欠完备的自动编码器不能简单地复制其输入到编码中，但它必须找到一种方法来输出其输入的副本。它被迫压缩数据，从而学习输入数据中最重要的特征（并丢弃不重要的特征）。
- en: Let’s see how to implement a very simple undercomplete autoencoder for dimensionality
    reduction.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何实现一个非常简单的欠完备自动编码器来进行降维。
- en: Performing PCA with an Undercomplete Linear Autoencoder
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用欠完备线性自动编码器进行PCA
- en: If the autoencoder uses only linear activations and the cost function is the
    mean squared error (MSE), then it ends up performing principal component analysis
    (PCA; see [Chapter 7](ch07.html#dimensionality_chapter)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果自动编码器只使用线性激活函数，并且损失函数是均方误差（MSE），那么它最终会执行主成分分析（PCA；参见[第7章](ch07.html#dimensionality_chapter)）。
- en: The following code builds a simple linear autoencoder that takes a 3D input,
    projects it down to 2D, then projects it back up to 3D. Since we will train the
    model using targets equal to the inputs, gradient descent will have to find the
    2D plane that lies closest to the training data, just like PCA would.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码构建了一个简单的线性自动编码器，它接受3D输入，将其投影到2D，然后将其投影回3D。由于我们将使用等于输入的目标来训练模型，梯度下降必须找到最接近训练数据的2D平面，就像PCA会做的那样。
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This code is really not very different from all the MLPs we built in past chapters,
    but there are a few things to note:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码实际上与我们在过去章节中构建的所有MLP没有太大区别，但有一些需要注意的地方：
- en: 'We organized the autoencoder into two subcomponents: the encoder and the decoder,
    each composed of a single `Linear` layer in this example, and the autoencoder
    is a `Sequential` model containing the encoder followed by the decoder.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将自动编码器组织成两个子组件：编码器和解码器，每个组件由一个`Linear`层组成（在这个例子中），自动编码器是一个包含编码器后跟解码器的`Sequential`模型。
- en: The autoencoder’s number of outputs is equal to the number of inputs (i.e.,
    3).
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码器的输出数量等于输入数量（即，3）。
- en: To perform PCA, we do not use any activation function (i.e., all neurons are
    linear), and the cost function is the MSE. That’s because PCA is a linear transformation.
    We will see more complex and nonlinear autoencoders shortly.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要执行PCA，我们不使用任何激活函数（即，所有神经元都是线性的），并且损失函数是均方误差（MSE）。这是因为PCA是一个线性变换。我们将很快看到更复杂和非线性的自动编码器。
- en: 'Now let’s train the model on the same simple generated 3D dataset we used in
    [Chapter 7](ch07.html#dimensionality_chapter) and use it to encode that dataset
    (i.e., project it to 2D):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们在[第7章](ch07.html#dimensionality_chapter)中使用的相同简单的生成3D数据集上训练模型，并使用它来编码该数据集（即，将其投影到2D）：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note that `X_train` is used as both the inputs and the targets. Next, let’s
    train the autoencoder, using the same `train()` function as in [Chapter 10](ch10.html#pytorch_chapter)
    (the notebook uses a slightly fancier function that prints some info and evaluates
    the model at each epoch):'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`X_train`被用作输入和目标。接下来，让我们使用与[第10章](ch10.html#pytorch_chapter)中相同的`train()`函数训练自动编码器（笔记本使用了一个稍微复杂一些的函数，它在每个epoch打印一些信息并评估模型）：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that the autoencoder is trained, we can use its encoder to compress 3D
    inputs to 2D. For example, let’s compress the entire training set:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在自动编码器已经训练好了，我们可以使用它的编码器将3D输入压缩到2D。例如，让我们压缩整个训练集：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Figure 18-2](#linear_autoencoder_pca_diagram) shows the original 3D dataset
    (on the left) and the output of the autoencoder’s hidden layer (i.e., the coding
    layer, on the right). As you can see, the autoencoder found the best 2D plane
    to project the data onto, preserving as much variance in the data as it could
    (just like PCA).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 18-2](#linear_autoencoder_pca_diagram) 显示了原始 3D 数据集（在左侧）和自动编码器隐藏层的输出（即编码层，在右侧）。如图所示，自动编码器找到了将数据投影到其上的最佳
    2D 平面，尽可能地保留了数据中的方差（就像 PCA 一样）。'
- en: '![Diagram comparing a 3D dataset and its 2D projection through an autoencoder,
    illustrating the principle of dimensionality reduction similar to PCA.](assets/hmls_1802.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![通过自动编码器将 3D 数据集及其 2D 投影进行比较的图，说明了类似于 PCA 的降维原理。](assets/hmls_1802.png)'
- en: Figure 18-2\. Approximate PCA performed by an undercomplete linear autoencoder
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-2\. 由欠完备线性自动编码器执行的近似 PCA
- en: Note
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can think of an autoencoder as performing a form of self-supervised learning,
    since it is based on a supervised learning technique with automatically generated
    labels (in this case, simply equal to the inputs).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将自动编码器视为执行一种形式的自监督学习，因为它基于一种带有自动生成标签的监督学习技术（在这种情况下，简单地等于输入）。
- en: Stacked Autoencoders
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠自动编码器
- en: Just like other neural networks we have discussed, autoencoders can have multiple
    hidden layers. In this case they are called *stacked autoencoders* (or *deep autoencoders*).
    Adding more layers helps the autoencoder learn more complex codings. That said,
    one must be careful not to make the autoencoder too powerful. Imagine an encoder
    so powerful that it just learns to map each input to a single arbitrary number
    (and the decoder learns the reverse mapping). Obviously such an autoencoder will
    reconstruct the training data perfectly, but it will not have learned any useful
    data representation in the process, and is unlikely to generalize well to new
    instances.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前讨论的其他神经网络一样，自动编码器可以有多个隐藏层。在这种情况下，它们被称为 *堆叠自动编码器*（或 *深度自动编码器*）。增加更多层可以帮助自动编码器学习更复杂的编码。然而，我们必须小心不要使自动编码器过于强大。想象一下，一个如此强大的编码器，它只是学会了将每个输入映射到一个任意的单一数字（解码器学习反向映射）。显然，这样的自动编码器将完美地重建训练数据，但在过程中它并没有学习到任何有用的数据表示，并且不太可能很好地泛化到新的实例。
- en: The architecture of a stacked autoencoder is typically symmetrical with regard
    to the central hidden layer (the coding layer). To put it simply, it looks like
    a sandwich. For example, an autoencoder for Fashion MNIST (introduced in [Chapter 9](ch09.html#ann_chapter))
    may have 784 inputs, followed by a hidden layer with 128 neurons, then a central
    hidden layer of 32 neurons, then another hidden layer with 128 neurons, and an
    output layer with 784 neurons. This stacked autoencoder is represented in [Figure 18-3](#stacked_autoencoder_diagram).
    Note that all hidden layers must have an activation function, such as ReLU.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠自动编码器的架构通常在中心隐藏层（编码层）方面是对称的。简单来说，它看起来像一个三明治。例如，用于 Fashion MNIST（在第 9 章中介绍）的自动编码器可能有
    784 个输入，然后是一个包含 128 个神经元的隐藏层，接着是一个 32 个神经元的中心隐藏层，然后是另一个包含 128 个神经元的隐藏层，最后是一个包含
    784 个神经元的输出层。这个堆叠自动编码器在 [图 18-3](#stacked_autoencoder_diagram) 中表示。请注意，所有隐藏层都必须有一个激活函数，例如
    ReLU。
- en: '![Diagram of a stacked autoencoder architecture with an input layer of 784
    units, three hidden layers (128, 32, and 100 units), and an output layer of 784
    units, illustrating the symmetrical structure.](assets/hmls_1803.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![堆叠自动编码器架构图，输入层有 784 个单元，三个隐藏层（128、32 和 100 个单元），以及一个输出层有 784 个单元，展示了对称结构。](assets/hmls_1803.png)'
- en: Figure 18-3\. Stacked autoencoder
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-3\. 堆叠自动编码器
- en: Implementing a Stacked Autoencoder Using PyTorch
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PyTorch 实现堆叠自动编码器
- en: 'You can implement a stacked autoencoder very much like a regular deep MLP.
    For example, here is an autoencoder you can use to process Fashion MNIST images:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以非常类似于常规深度 MLP 来实现堆叠自动编码器。例如，这里有一个可以用于处理 Fashion MNIST 图像的自动编码器：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s go through this code:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这段代码：
- en: 'Just like earlier, we split the autoencoder model into two submodels: the encoder
    and the decoder.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就像之前一样，我们将自动编码器模型分为两个子模型：编码器和解码器。
- en: The encoder takes 28 × 28 pixel grayscale images (i.e., with a single channel),
    flattens them so that each image is represented as a vector of size 784, then
    processes these vectors through 2 `Linear` layers of diminishing sizes (128 units,
    then 32 units), each followed by the ReLU activation function. For each input
    image, the encoder outputs a vector of size 32.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器接收28 × 28像素的灰度图像（即单通道），将其展平，使每个图像表示为一个大小为784的向量，然后通过2个大小递减的`Linear`层（128个单元，然后32个单元）处理这些向量，每个层后面跟着ReLU激活函数。对于每个输入图像，编码器输出一个大小为32的向量。
- en: The decoder takes codings of size 32 (output by the encoder) and processes them
    through 2 `Linear` layers of increasing sizes (128 units, then 784 units), and
    reshapes the final vectors into 1 × 28 × 28 arrays so the decoder’s outputs have
    the same shape as the encoder’s inputs. Note that we use the sigmoid function
    for the output layer instead of ReLU to ensure that the output pixel values range
    between 0 and 1.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器接收编码器输出的32大小（输出）的编码，并通过2个大小递增的`Linear`层（128个单元，然后784个单元）进行处理，并将最终的向量重塑为1
    × 28 × 28数组，这样解码器的输出形状与编码器的输入相同。注意，我们使用sigmoid函数而不是ReLU作为输出层的激活函数，以确保输出像素值在0到1之间。
- en: We can now load the Fashion MNIST dataset using the TorchVision library and
    split it into `train_data`, `valid_data`, and `test_data` (just like we did in
    [Chapter 10](ch10.html#pytorch_chapter)), then train the autoencoder exactly like
    the previous autoencoder, using the inputs as the targets and minimizing the MSE
    loss. Give it a try, it’s a good exercise! Don’t forget to change the targets
    so they match the inputs—we’re training an autoencoder, not a classifier.⁠^([2](ch18.html#id4035))
    If you get stuck, please check out the implementation in this chapter’s notebook.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用TorchVision库加载Fashion MNIST数据集，并将其分为`train_data`、`valid_data`和`test_data`（就像我们在[第10章](ch10.html#pytorch_chapter)中所做的那样），然后像之前的自动编码器一样训练自动编码器，使用输入作为目标，最小化均方误差损失。试一试，这是一个很好的练习！别忘了更改目标，使它们与输入匹配——我们正在训练一个自动编码器，而不是一个分类器。[2](ch18.html#id4035)
    如果遇到困难，请查看本章笔记本中的实现。
- en: Visualizing the Reconstructions
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化重建图像
- en: 'Once you have trained the stacked autoencoder, how do you know if it’s any
    good? One way to check that an autoencoder is properly trained is to compare the
    inputs and the outputs: the differences should not be too significant. Let’s plot
    a few images from the validation set, as well as their reconstructions:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练了堆叠的自动编码器，你怎么知道它是否好呢？检查自动编码器是否正确训练的一种方法是比较输入和输出：差异不应太大。让我们绘制一些验证集的图像以及它们的重建图像：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '![Original images of clothing and footwear are displayed on top, with their
    slightly blurred reconstructions below, illustrating the results of a stacked
    autoencoder.](assets/hmls_1804.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![服装和鞋类的原始图像在上部显示，其略微模糊的重建图像在下部，展示了堆叠自动编码器的结果。](assets/hmls_1804.png)'
- en: Figure 18-4\. Original images (top) and their reconstructions (bottom)
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-4\. 原始图像（顶部）及其重建图像（底部）
- en: '[Figure 18-4](#reconstruction_plot) shows the resulting images. The reconstructions
    are recognizable, but a bit too lossy. We may need to train the model for longer,
    or make the encoder and decoder more powerful, or make the codings larger. For
    now, let’s go with this model and see how we can use it.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18-4](#reconstruction_plot) 展示了生成的图像。重建的图像可以辨认，但损失率有点高。我们可能需要更长时间训练模型，或者使编码器和解码器更强大，或者使编码更大。目前，我们先使用这个模型，看看我们如何使用它。'
- en: Anomaly Detection Using Autoencoders
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自动编码器进行异常检测
- en: One common use case for autoencoders is anomaly detection. Indeed, if an autoencoder
    is given an image that doesn’t look like the images it was trained on (the image
    is said to be *out of distribution*), then the reconstruction will be terrible.
    For example, [Figure 18-5](#bad_reconstructions_plot) shows some MNIST digits
    and their reconstructions using the model we just trained on Fashion MNIST. As
    you can see, these reconstructions are very different from the inputs. If you
    compute the reconstruction loss (i.e., the MSE between the input and the output),
    it will be very high. To use the model for anomaly detection, you simply need
    to define a threshold, then any image whose reconstruction loss is greater than
    that threshold can be considered an anomaly.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器的一个常见用例是异常检测。确实，如果自动编码器被给了一个看起来不像它训练过的图像（该图像被称为*分布外*），那么重建将非常糟糕。例如，[图18-5](#bad_reconstructions_plot)显示了使用我们在Fashion
    MNIST上训练的模型重建的一些MNIST数字及其重建。如你所见，这些重建与输入非常不同。如果你计算重建损失（即输入和输出之间的均方误差），它将非常高。要使用该模型进行异常检测，你只需定义一个阈值，然后任何重建损失大于该阈值的图像都可以被认为是异常。
- en: '![Example showing poor reconstructions of MNIST digit images using a model
    trained on Fashion MNIST, highlighting how out-of-distribution inputs result in
    high reconstruction loss.](assets/hmls_1805.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![示例展示了使用在Fashion MNIST上训练的模型对MNIST数字图像进行重建的糟糕效果，突出了分布外输入导致的高重建损失。](assets/hmls_1805.png)'
- en: Figure 18-5\. Out-of-distribution images are poorly reconstructed
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-5\. 分布外图像重建效果差
- en: That’s all there is to it! Now let’s look at another use case for autoencoders.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部内容！现在让我们看看自动编码器的另一个用例。
- en: Visualizing the Fashion MNIST Dataset
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化Fashion MNIST数据集
- en: As we saw earlier in this chapter, undercomplete autoencoders can be used for
    dimensionality reduction. However, for most datasets they will not do a very good
    job at reducing the dimensionality down to two or three dimensions; they need
    enough dimensions to be able to properly reconstruct the inputs. As a result,
    they are generally not used directly for visualization. However, they are great
    at handling huge datasets, so one strategy is to use an autoencoder to reduce
    the dimensionality down to a reasonable level, then use another dimensionality
    reduction algorithm for visualization, such as those we discussed in [Chapter 7](ch07.html#dimensionality_chapter).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们本章前面所看到的，欠完备的自动编码器可以用于降维。然而，对于大多数数据集，它们在将维度降低到两到三个维度方面不会做得很好；它们需要足够的维度来正确地重建输入。因此，它们通常不会直接用于可视化。然而，它们在处理大型数据集方面非常出色，所以一种策略是使用自动编码器将维度降低到合理的水平，然后使用另一个降维算法进行可视化，例如我们在[第7章](ch07.html#dimensionality_chapter)中讨论的那些。
- en: 'Let’s use this strategy to visualize Fashion MNIST. First we’ll use the encoder
    from our stacked autoencoder to reduce the dimensionality down to 32, then we’ll
    use Scikit-Learn’s implementation of the t-SNE algorithm to reduce the dimensionality
    down to 2 for visualization:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这种策略来可视化Fashion MNIST。首先，我们将使用堆叠自动编码器的编码器将维度降低到32，然后我们将使用Scikit-Learn对t-SNE算法的实现将维度降低到2进行可视化：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now we can plot the dataset:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以绘制数据集：
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Figure 18-6](#fashion_mnist_visualization_plot) shows the resulting scatterplot,
    beautified a bit by displaying some of the images. The t-SNE algorithm identified
    several clusters that match the classes reasonably well (each class is represented
    by a different color). Note that t-SNE’s output can vary greatly if you run it
    with a different random seed, slightly different data, or on a different platform,
    so your plot may look different.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18-6](#fashion_mnist_visualization_plot)显示了生成的散点图，通过显示一些图像进行了一些美化。t-SNE算法识别出几个与类别匹配得相当好的簇（每个类别用不同的颜色表示）。请注意，如果你使用不同的随机种子、略微不同的数据或在不同的平台上运行t-SNE，其输出可能会有很大差异，所以你的图表可能看起来不同。'
- en: '![A scatterplot visualizing Fashion MNIST data via an autoencoder and t-SNE,
    showing clusters corresponding to different clothing items marked by images.](assets/hmls_1806.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![通过自动编码器和t-SNE可视化Fashion MNIST数据，显示用图像标记的不同服装项目的簇。](assets/hmls_1806.png)'
- en: Figure 18-6\. Fashion MNIST visualization using an autoencoder, followed by
    t-SNE
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-6\. 使用自动编码器，然后是t-SNE进行Fashion MNIST可视化
- en: Next let’s look at how we can use autoencoders for unsupervised pretraining.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们看看我们如何可以使用自动编码器进行无监督预训练。
- en: Unsupervised Pretraining Using Stacked Autoencoders
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用堆叠自动编码器进行无监督预训练
- en: As we discussed in [Chapter 11](ch11.html#deep_chapter), if you are tackling
    a complex supervised task but you do not have a lot of labeled training data,
    one solution is to find a neural network that performs a similar task and reuse
    its lower layers. This makes it possible to train a high-performance model using
    little training data because your neural network won’t have to learn all the low-level
    features; it will just reuse the feature detectors learned by the existing network.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[第11章](ch11.html#deep_chapter)中讨论的那样，如果你正在处理一个复杂的监督任务，但你没有很多标记的训练数据，一个解决方案是找到一个执行类似任务的神经网络并重用其底层。这使得使用少量训练数据训练高性能模型成为可能，因为你的神经网络不需要学习所有低级特征；它只需重用现有网络学习到的特征检测器。
- en: Similarly, if you have a large dataset but most of it is unlabeled, you can
    first train a stacked autoencoder using all the data, then reuse the lower layers
    to create a neural network for your actual task and train it using the labeled
    data. For example, [Figure 18-7](#unsupervised_pretraining_autoencoders_diagram)
    shows how to use a stacked autoencoder to perform unsupervised pretraining for
    a classification neural network. When training the classifier, if you really don’t
    have much labeled training data, you may want to freeze the pretrained layers
    (at least the lower ones).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果你有一个大型数据集，但其中大部分是无标记的，你可以首先使用所有数据训练一个堆叠自动编码器，然后重用底层来创建一个用于实际任务的神经网络，并使用标记数据对其进行训练。例如，[图18-7](#unsupervised_pretraining_autoencoders_diagram)展示了如何使用堆叠自动编码器对分类神经网络进行无监督预训练。在训练分类器时，如果你真的没有很多标记的训练数据，你可能想要冻结预训练层（至少是底层的）。
- en: '![Diagram illustrating the process of using a stacked autoencoder for unsupervised
    pretraining, showing the transfer of trained parameters from a multi-layer setup
    in Phase 1 to a classifier in Phase 2.](assets/hmls_1807.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![展示使用堆叠自动编码器进行无监督预训练过程的图解，显示了从第一阶段的多层设置到第二阶段分类器的训练参数的迁移。](assets/hmls_1807.png)'
- en: Figure 18-7\. Unsupervised pretraining using autoencoders
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-7\. 使用自动编码器进行无监督预训练
- en: Note
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Having plenty of unlabeled data and little labeled data is common. Building
    a large unlabeled dataset is often cheap (e.g., a simple script can download millions
    of images off the internet), but labeling those images (e.g., classifying them
    as cute or not) can usually be done reliably only by humans. Labeling instances
    is time-consuming and costly, so it’s normal to have only a few thousand human-labeled
    instances, or even less. That said, there is a growing trend toward using advanced
    AIs to label datasets.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有大量无标记数据和少量标记数据是很常见的。构建一个大型无标记数据集通常很便宜（例如，一个简单的脚本可以从互联网上下载数百万张图片），但标记这些图片（例如，将它们分类为可爱或不可爱）通常只能由人类可靠地完成。标记实例既耗时又昂贵，因此通常只有几千个或更少的人类标记实例。尽管如此，使用高级AI来标记数据集的趋势正在增长。
- en: 'There is nothing special about the implementation: just train an autoencoder
    using all the training data (labeled plus unlabeled), then reuse its encoder layers
    to create a new neural network and train it on the labeled instances (see the
    exercises at the end of this chapter for an example).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 实现上没有特别之处：只需使用所有训练数据（标记和无标记）训练一个自动编码器，然后重用其编码器层来创建一个新的神经网络，并在标记实例上对其进行训练（参见本章末尾的练习以获取示例）。
- en: Let’s now look at a few techniques for training stacked autoencoders.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在看看一些用于训练堆叠自动编码器的技术。
- en: Tying Weights
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 束定权重
- en: When an autoencoder is neatly symmetrical, like the one we just built, a common
    technique is to *tie* the weights of the decoder layers to the weights of the
    encoder layers. This halves the number of weights in the model, speeding up training
    and limiting the risk of overfitting. Specifically, if the autoencoder has a total
    of *N* layers (not counting the input layer), and **W**[*L*] represents the connection
    weights of the *L*^(th) layer (e.g., layer 1 is the first hidden layer, layer
    *N*/2 is the coding layer, and layer *N* is the output layer), then the decoder
    layer weights can be defined as **W**[*L*] = **W**[*N*–*L*+1]^⊺ (with *L* = *N*
    / 2 + 1, …​, *N*).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当自动编码器整齐对称，就像我们刚刚构建的那样，一个常见的技巧是将解码器层的权重与编码器层的权重*绑定*。这将模型中的权重数量减半，加快了训练速度并限制了过拟合的风险。具体来说，如果自动编码器总共有*N*层（不计输入层），并且**W**[*L*]表示*L*^(th)层的连接权重（例如，层1是第一个隐藏层，层*N*/2是编码层，层*N*是输出层），那么解码器层的权重可以定义为**W**[*L*]
    = **W**[*N*–*L*+1]^⊺（其中*L* = *N* / 2 + 1，…，*N*）。
- en: 'For example, here is the same autoencoder as the previous one, except the decoder
    weights are tied to the encoder weights:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里有一个与上一个相同的自动编码器，只是解码器权重与编码器权重绑定：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This model achieves a smaller reconstruction error than the previous model,
    using about half the number of parameters.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型比之前的模型实现了更小的重建误差，使用了大约一半的参数数量。
- en: Training One Autoencoder at a Time
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 每次训练一个自动编码器
- en: Rather than training the whole stacked autoencoder in one go like we just did,
    it is possible to train one shallow autoencoder at a time, then stack all of them
    into a single stacked autoencoder (hence the name), as shown in [Figure 18-8](#stacking_autoencoders_diagram).
    This technique is called *greedy layerwise training*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们刚才一次性训练整个堆叠自动编码器不同，我们可以一次训练一个浅层自动编码器，然后将所有这些自动编码器堆叠成一个单一的堆叠自动编码器（因此得名），如图[图18-8](#stacking_autoencoders_diagram)所示。这种技术被称为*贪婪分层训练*。
- en: During the first phase of training, the first autoencoder learns to reconstruct
    the inputs. Then we encode the whole training set using this first autoencoder,
    and this gives us a new (compressed) training set. We then train a second autoencoder
    on this new dataset. This is the second phase of training. Finally, we build a
    big sandwich using all these autoencoders, as shown in [Figure 18-8](#stacking_autoencoders_diagram)
    (i.e., we first stack the encoder layers of each autoencoder, then the decoder
    layers in reverse order). This gives us the final stacked autoencoder. We could
    easily train more autoencoders this way, building a very deep stacked autoencoder.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练的第一个阶段，第一个自动编码器学会重建输入。然后我们使用这个第一个自动编码器对整个训练集进行编码，这给我们提供了一个新的（压缩的）训练集。然后我们在新的数据集上训练第二个自动编码器。这是训练的第二阶段。最后，我们使用所有这些自动编码器构建一个大的三明治，如图[图18-8](#stacking_autoencoders_diagram)所示（即我们首先堆叠每个自动编码器的编码器层，然后以相反的顺序堆叠解码器层）。这给我们带来了最终的堆叠自动编码器。我们可以很容易地以这种方式训练更多的自动编码器，构建一个非常深的堆叠自动编码器。
- en: '![Diagram illustrating the three-phase process of training stacked autoencoders
    one layer at a time, showing the flow from input through hidden layers to output.](assets/hmls_1808.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图示逐层训练堆叠自动编码器的三个阶段，展示从输入通过隐藏层到输出的流程。](assets/hmls_1808.png)'
- en: Figure 18-8\. Training one autoencoder at a time
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-8\. 逐次训练一个自动编码器
- en: As I mentioned in [Chapter 11](ch11.html#deep_chapter), one of the triggers
    of the deep learning tsunami was the discovery in 2006 by [Geoffrey Hinton et
    al.](https://homl.info/136) that deep neural networks can be pretrained in an
    unsupervised fashion using this greedy layer-wise approach. They used restricted
    Boltzmann machines (RBMs; see [*https://homl.info/extra-anns*](https://homl.info/extra-anns))
    for this purpose, but in 2007 [Yoshua Bengio et al.](https://homl.info/112)⁠^([3](ch18.html#id4062))
    showed that autoencoders worked just as well. For several years this was the only
    efficient way to train deep nets, until many of the techniques introduced in [Chapter 11](ch11.html#deep_chapter)
    made it possible to just train a deep net in one shot.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如我在[第11章](ch11.html#deep_chapter)中提到的，深度学习浪潮的一个触发因素是2006年[杰弗里·辛顿等](https://homl.info/136)发现，深度神经网络可以使用这种贪婪的分层方法进行无监督预训练。他们为此目的使用了受限玻尔兹曼机（RBMs；见[*https://homl.info/extra-anns*](https://homl.info/extra-anns)），但在2007年[约书亚·本吉奥等](https://homl.info/112)⁠^([3](ch18.html#id4062))表明自动编码器同样有效。在接下来的几年里，这是训练深度网络的唯一有效方法，直到第11章中介绍的技术使得一次性训练深度网络成为可能。
- en: 'Autoencoders are not limited to dense networks: you can also build convolutional
    autoencoders. Let’s look at these now.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器不仅限于密集网络：你还可以构建卷积自动编码器。现在让我们来看看这些。
- en: Convolutional Autoencoders
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积自动编码器
- en: 'If you are dealing with images, then the autoencoders we have seen so far will
    not work well (unless the images are very small). As you saw in [Chapter 12](ch12.html#cnn_chapter),
    convolutional neural networks are far better suited than dense networks to working
    with images. So if you want to build an autoencoder for images (e.g., for unsupervised
    pretraining or dimensionality reduction), you will need to build a [*convolutional
    autoencoder*](https://homl.info/convae).⁠^([4](ch18.html#id4066)) The encoder
    is a regular CNN composed of convolutional layers and pooling layers. It typically
    reduces the spatial dimensionality of the inputs (i.e., height and width) while
    increasing the depth (i.e., the number of feature maps). The decoder must do the
    reverse (upscale the image and reduce its depth back to the original dimensions),
    and for this you can use transpose convolutional layers (alternatively, you could
    combine upsampling layers with convolutional layers). Here is a basic convolutional
    autoencoder for Fashion MNIST:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在处理图像，那么我们之前看到的自编码器将不会很好地工作（除非图像非常小）。正如你在[第12章](ch12.html#cnn_chapter)中看到的，卷积神经网络比密集网络更适合处理图像。因此，如果你想为图像构建一个自编码器（例如，用于无监督预训练或降维），你需要构建一个[*卷积自编码器*](https://homl.info/convae)。⁠^([4](ch18.html#id4066))
    编码器是一个由卷积层和池化层组成的常规CNN。它通常减少输入的空间维度（即高度和宽度），同时增加深度（即特征图的数量）。解码器必须执行相反的操作（放大图像并减少其深度回到原始维度），为此你可以使用转置卷积层（或者，你也可以将上采样层与卷积层结合使用）。以下是一个基本的卷积自编码器示例，用于Fashion
    MNIST：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It’s also possible to create autoencoders with other architecture types, such
    as RNNs (see the notebook for an example).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以创建具有其他架构类型的自编码器，例如RNN（请参阅笔记本中的示例）。
- en: 'OK, let’s step back for a second. So far we have looked at various kinds of
    autoencoders (basic, stacked, and convolutional) and how to train them (either
    in one shot or layer by layer). We also looked at a few applications: dimensionality
    reduction (e.g., for data visualization), anomaly detection, and unsupervised
    pretraining.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们退一步。到目前为止，我们已经研究了各种类型的自编码器（基本、堆叠和卷积）以及如何训练它们（一次性或逐层）。我们还探讨了几个应用：降维（例如，用于数据可视化）、异常检测和无监督预训练。
- en: 'Up to now, in order to force the autoencoder to learn interesting features,
    we have limited the size of the coding layer, making it undercomplete. There are
    actually many other kinds of constraints that can be used, including ones that
    allow the coding layer to be just as large as the inputs, or even larger, resulting
    in an *overcomplete autoencoder*. So in the following sections we’ll look at a
    few more kinds of autoencoders: denoising autoencoders, sparse autoencoders, and
    variational autoencoders.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，为了迫使自编码器学习有趣的特征，我们限制了编码层的大小，使其不完整。实际上还有许多其他类型的约束可以使用，包括允许编码层与输入一样大，甚至更大的约束，从而产生一个*过完备自编码器*。因此，在接下来的几节中，我们将探讨更多类型的自编码器：降噪自编码器、稀疏自编码器和变分自编码器。
- en: Denoising Autoencoders
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降噪自编码器
- en: A simple way to force the autoencoder to learn useful features is to add noise
    to its inputs, training it to recover the original, noise-free inputs. This idea
    has been around since the 1980s (e.g., it is mentioned in Yann LeCun’s 1987 master’s
    thesis). In a [2008 paper](https://homl.info/113),⁠^([5](ch18.html#id4073)) Pascal
    Vincent et al. showed that autoencoders could also be used for feature extraction.
    In a [2010 paper](https://homl.info/114),⁠^([6](ch18.html#id4074)) Vincent et
    al. introduced *stacked denoising autoencoders*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 强迫自编码器学习有用特征的一种简单方法是在其输入中添加噪声，训练它恢复原始的无噪声输入。这个想法自20世纪80年代以来一直存在（例如，它在Yann LeCun的1987年硕士论文中提到）。在[2008年的一篇论文](https://homl.info/113)，⁠^([5](ch18.html#id4073))中
    Pascal Vincent等人表明自编码器也可以用于特征提取。在[2010年的一篇论文](https://homl.info/114)，⁠^([6](ch18.html#id4074))中
    Vincent等人介绍了*堆叠降噪自编码器*。
- en: The noise can be pure Gaussian noise added to the inputs, or it can be randomly
    switched-off inputs, just like in dropout (introduced in [Chapter 11](ch11.html#deep_chapter)).
    [Figure 18-9](#denoising_autoencoders_diagram) shows both options.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 噪声可以是添加到输入的纯高斯噪声，或者随机关闭的输入，就像在dropout（在第11章中介绍）中那样。[图18-9](#denoising_autoencoders_diagram)展示了这两种选项。
- en: '![Diagram illustrating denoising autoencoders with Gaussian noise added to
    inputs on the left and dropout applied on the right.](assets/hmls_1809.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![展示添加高斯噪声到左侧输入并应用右侧dropout的降噪自编码器图](assets/hmls_1809.png)'
- en: Figure 18-9\. Denoising autoencoders, with Gaussian noise (left) or dropout
    (right)
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-9\. 带有高斯噪声（左侧）或dropout（右侧）的去噪自动编码器
- en: 'The dropout implementation of the denoising autoencoder is straightforward:
    it is a regular stacked autoencoder with an additional `Dropout` layer applied
    to the encoder’s inputs (recall that the `Dropout` layer is only active during
    training). Note that the coding layer does not need to compress the data as much
    since the noise already makes the reconstruction task nontrivial:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪自动编码器的dropout实现很简单：它是一个常规的堆叠自动编码器，在编码器的输入上应用了一个额外的`Dropout`层（回想一下，`Dropout`层仅在训练期间是活跃的）。请注意，编码层不需要对数据进行太多的压缩，因为噪声已经使得重建任务变得非平凡：
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'This may remind you of BERT’s MLM pretraining task (see [Chapter 15](ch15.html#transformer_chapter)):
    reconstructing masked inputs (except BERT isn’t split into an encoder and a decoder).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能会让你想起BERT的MLM预训练任务（见[第15章](ch15.html#transformer_chapter)）：重建掩码输入（除了BERT没有分成编码器和解码器）。
- en: '[Figure 18-10](#dropout_denoising_plot) shows a few noisy images (with half
    of the pixels turned off), and the images reconstructed by the dropout-based denoising
    autoencoder, after training. Notice how the autoencoder guesses details that are
    actually not in the input, such as the top of the rightmost shoe. As you can see,
    not only can denoising autoencoders be used for data visualization or unsupervised
    pretraining, like the other autoencoders we’ve discussed so far, but they can
    also be used quite simply and efficiently to remove noise from images.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18-10](#dropout_denoising_plot)展示了几个噪声图像（一半的像素被关闭），以及经过训练后基于dropout的去噪自动编码器重建的图像。注意自动编码器如何猜测实际上不在输入中的细节，例如最右侧鞋子的顶部。正如你所看到的，去噪自动编码器不仅可以用于数据可视化或无监督预训练，就像我们之前讨论的其他自动编码器一样，而且它们还可以非常简单且高效地用于从图像中去除噪声。'
- en: '![Noisy images at the top and their denoised reconstructions at the bottom,
    demonstrating the effectiveness of a dropout-based denoising autoencoder.](assets/hmls_1810.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![顶部是噪声图像，底部是去噪重建图像，展示了基于dropout的去噪自动编码器的有效性。](assets/hmls_1810.png)'
- en: Figure 18-10\. Noisy images (top) and their reconstructions (bottom)
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-10\. 噪声图像（顶部）及其重建图像（底部）
- en: Sparse Autoencoders
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏自动编码器
- en: 'Another kind of constraint that often leads to good feature extraction is *sparsity*:
    by adding an appropriate term to the cost function, the autoencoder is pushed
    to reduce the number of active neurons in the coding layer. This forces the autoencoder
    to represent each input as a combination of a small number of activations. As
    a result, each neuron in the coding layer typically ends up representing a useful
    feature (if you could speak only a few words per month, you would probably try
    to make them worth listening to).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常导致良好特征提取的约束是*稀疏性*：通过在成本函数中添加一个适当的项，自动编码器被推动减少编码层中活跃神经元的数量。这迫使自动编码器将每个输入表示为少量激活的组合。因此，编码层中的每个神经元通常最终代表一个有用的特征（如果你每月只能讲几个词，你可能会尝试让它们值得一听）。
- en: A basic approach is to use the sigmoid activation function in the coding layer
    (to constrain the codings to values between 0 and 1), use a large coding layer
    (e.g., with 256 units), and add some ℓ[1] regularization to the coding layer’s
    activations. This means adding the ℓ[1] norm of the codings (i.e., the sum of
    their absolute values) to the loss, weighted by a sparsity hyperparameter. This
    *sparsity loss* will encourage the neural network to produce codings close to
    0\. However, the total loss will still include the reconstruction loss, so the
    model will be forced to output at least a few nonzero values to reconstruct the
    inputs correctly. Using the ℓ[1] norm rather than the ℓ[2] norm will push the
    neural network to preserve the most important codings while eliminating the ones
    that are not needed for the input image (rather than just reducing all codings).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一种基本的方法是在编码层中使用sigmoid激活函数（将编码约束在0到1之间的值），使用一个大的编码层（例如，有256个单元），并在编码层的激活上添加一些ℓ[1]正则化。这意味着将编码的ℓ[1]范数（即它们的绝对值之和）添加到损失中，并乘以一个稀疏超参数。这个*稀疏损失*将鼓励神经网络产生接近0的编码。然而，总损失仍然包括重建损失，因此模型将被迫输出至少几个非零值以正确重建输入。使用ℓ[1]范数而不是ℓ[2]范数将推动神经网络保留最重要的编码，同时消除对于输入图像不需要的编码（而不仅仅是减少所有编码）。
- en: Another approach—which often yields better results—is to measure the mean sparsity
    of each neuron in the coding layer, across each training batch, and penalize the
    model when the mean sparsity differs from the target sparsity (e.g., 10%). The
    batch size must not be too small, or the mean will not be accurate. For example,
    if we measure that a neuron has an average activation of 0.3, but the target sparsity
    is 0.1, then this neuron must be penalized to activate less. One approach could
    be simply adding the squared error (0.3 – 0.1)² to the loss function, but in practice
    it’s better to use the Kullback–Leibler (KL) divergence (briefly discussed in
    [Chapter 4](ch04.html#linear_models_chapter)), since it has much stronger gradients
    than the mean squared error, as you can see in [Figure 18-11](#sparsity_loss_plot).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法——通常会产生更好的结果——是测量编码层中每个神经元在每个训练批次中的平均稀疏度，并在平均稀疏度与目标稀疏度（例如，10%）不同时惩罚模型。批次大小不能太小，否则平均值将不准确。例如，如果我们测量到一个神经元的平均激活值为
    0.3，但目标稀疏度为 0.1，那么这个神经元必须受到惩罚以减少激活。一种方法可以是简单地将平方误差（0.3 – 0.1）²加到损失函数中，但在实践中，最好使用
    Kullback–Leibler (KL) 散度（在 [第 4 章](ch04.html#linear_models_chapter) 中简要讨论），因为它比均方误差具有更强的梯度，如
    [图 18-11](#sparsity_loss_plot) 所示。
- en: '![Diagram comparing the cost functions of KL divergence, MAE, and MSE at different
    actual sparsity levels, with a target sparsity of 0.1.](assets/hmls_1811.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![比较不同实际稀疏度水平下KL散度、MAE和MSE成本函数的图表，目标稀疏度为 0.1。](assets/hmls_1811.png)'
- en: Figure 18-11\. Sparsity loss with target sparsity *p* = 0.1
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-11\. 目标稀疏度为 *p* = 0.1 的稀疏度损失
- en: Given two discrete probability distributions *P* and *Q*, the KL divergence
    between these distributions, noted *D*[KL](*P* ∥ *Q*), can be computed using [Equation
    18-1](#kl_divergence_equation).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 给定两个离散概率分布 *P* 和 *Q*，这两个分布之间的KL散度，记作 *D*[KL](*P* ∥ *Q*)，可以使用 [方程 18-1](#kl_divergence_equation)
    来计算。
- en: Equation 18-1\. Kullback–Leibler divergence
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 18-1\. Kullback–Leibler 散度
- en: $upper D Subscript KL Baseline left-parenthesis upper P parallel-to upper Q
    right-parenthesis equals sigma-summation Underscript i Endscripts upper P left-parenthesis
    i right-parenthesis log StartFraction upper P left-parenthesis i right-parenthesis
    Over upper Q left-parenthesis i right-parenthesis EndFraction$
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: $upper D Subscript KL Baseline left-parenthesis upper P parallel-to upper Q
    right-parenthesis equals sigma-summation Underscript i Endscripts upper P left-parenthesis
    i right-parenthesis log StartFraction upper P left-parenthesis i right-parenthesis
    Over upper Q left-parenthesis i right-parenthesis EndFraction$
- en: In our case, we want to measure the divergence between the target probability
    *p* that a neuron in the coding layer will activate, and the actual probability
    *q*, estimated by measuring the mean activation over the training batch. So, the
    KL divergence simplifies to [Equation 18-2](#kl_divergence_equation_simplified).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们想要测量编码层中神经元激活的目标概率 *p* 与通过测量训练批次中的平均激活值估计的实际概率 *q* 之间的差异。因此，KL散度简化为
    [方程 18-2](#kl_divergence_equation_simplified)。
- en: Equation 18-2\. KL divergence between the target sparsity *p* and the actual
    sparsity *q*
  id: totrans-126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 18-2\. 目标稀疏度 *p* 与实际稀疏度 *q* 之间的KL散度
- en: $upper D Subscript KL Baseline left-parenthesis p parallel-to q right-parenthesis
    equals p log StartFraction p Over q EndFraction plus left-parenthesis 1 minus
    p right-parenthesis log StartFraction 1 minus p Over 1 minus q EndFraction$
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: $upper D Subscript KL Baseline left-parenthesis p parallel-to q right-parenthesis
    equals p log StartFraction p Over q EndFraction plus left-parenthesis 1 minus
    p right-parenthesis log StartFraction 1 minus p Over 1 minus q EndFraction$
- en: 'To implement this approach in PyTorch, we must first ensure that the autoencoder
    outputs both the reconstructions and the codings, since they are both needed to
    compute the loss. In this code, the autoencoder’s `forward()` method returns a
    `namedtuple` containing two fields—`output` (i.e., the reconstructions) and `codings`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 PyTorch 中实现这种方法，我们必须首先确保自动编码器输出重建和编码，因为它们都是计算损失所必需的。在这个代码中，自动编码器的 `forward()`
    方法返回一个包含两个字段的 `namedtuple`——`output`（即重建）和 `codings`：
- en: '[PRE11]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You may need to tweak your training and evaluation functions to support these
    `namedtuple` predictions. For example, you can add `y_pred = y_pred.output` in
    the `evaluate_tm()` function, just after calling the model.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要调整您的训练和评估函数以支持这些 `namedtuple` 预测。例如，您可以在调用模型后立即在 `evaluate_tm()` 函数中添加
    `y_pred = y_pred.output`。
- en: 'Next, we can define the loss function:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以定义损失函数：
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This function returns the reconstruction loss (MSE) plus a weighted sparsity
    loss. The sparsity loss is the KL divergence between the target sparsity and the
    mean sparsity across the batch. The `kl_weight` is a hyperparameter you can tune
    to control how much to encourage sparsity: if this hyperparameter is too high,
    the model will stick closely to the target sparsity, but it may not reconstruct
    the inputs properly, making the model useless. Conversely, if it is too low, the
    model will mostly ignore the sparsity objective and will not learn any interesting
    features. The `eps` argument is a smoothing term to avoid division by zero when
    computing the KL divergence.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数返回重建损失（均方误差）加上一个加权稀疏度损失。稀疏度损失是目标稀疏度与批次平均稀疏度之间的KL散度。`kl_weight`是一个超参数，你可以调整它来控制鼓励稀疏度的程度：如果这个超参数太高，模型将紧密地遵循目标稀疏度，但它可能无法正确地重建输入，使模型变得无用。相反，如果它太低，模型将主要忽略稀疏度目标，并且不会学习任何有趣的特征。`eps`参数是一个平滑项，用于避免在计算KL散度时除以零。
- en: 'Now we’re ready to create the model and train it (using the same `train()`
    function as earlier, from [Chapter 10](ch10.html#pytorch_chapter)):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好创建模型并对其进行训练（使用与之前相同的`train()`函数，来自[第10章](ch10.html#pytorch_chapter)）：
- en: '[PRE13]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: After training this sparse autoencoder on Fashion MNIST, the coding layer will
    have roughly 10% sparsity. Success!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在对Fashion MNIST上的这个稀疏自编码器进行训练后，编码层将大约有10%的稀疏度。成功！
- en: Tip
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Sparse autoencoders often produce fairly interpretable codings, where each
    component corresponds to an identifiable feature in the image. For example, you
    can plot all the images whose *n*^(th) coding is larger than usual (e.g., above
    the 90^(th) percentile): you will often notice that all the images have something
    in common (e.g., they are all shoes).'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏自编码器通常会产生相当可解释的编码，其中每个组件对应于图像中的一个可识别特征。例如，你可以绘制所有那些*n*^(th)编码比平常大的图像（例如，高于90^(th)分位数）：你通常会注意到所有图像都有共同之处（例如，它们都是鞋子）。
- en: Now let’s move on to variational autoencoders!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续讨论变分自编码器！
- en: Variational Autoencoders
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: 'An important category of autoencoders was introduced in 2013 by [Diederik Kingma
    and Max Welling](https://homl.info/115)⁠^([7](ch18.html#id4088)) and quickly became
    one of the most popular variants: *variational autoencoders* (VAEs).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 2013年由[Diederik Kingma和Max Welling](https://homl.info/115)⁠^([7](ch18.html#id4088))引入的一个重要自编码器类别迅速成为最受欢迎的变体之一：**变分自编码器**（VAEs）。
- en: 'VAEs are quite different from all the autoencoders we have discussed so far,
    in these particular ways:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前讨论的所有自编码器相比，变分自编码器在这些特定方面相当不同：
- en: They are *probabilistic autoencoders*, meaning that their outputs are partly
    determined by chance, even after training (as opposed to denoising autoencoders,
    which use randomness only during training).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是**概率自编码器**，这意味着即使经过训练，它们的输出也部分由机会决定（与仅在使用训练时使用随机性的去噪自编码器相反）。
- en: Most importantly, they are *generative autoencoders*, meaning that they can
    generate new instances that look like they were sampled from the training set.⁠^([8](ch18.html#id4091))
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最重要的是，它们是**生成式自编码器**，这意味着它们可以生成看起来像是从训练集中采样的新实例。⁠^([8](ch18.html#id4091))
- en: 'Let’s take a look at how VAEs work. [Figure 18-12](#variational_autoencoders_diagram)
    (left) shows a variational autoencoder. You can recognize the basic sandwich-like
    structure of most autoencoders, with an encoder followed by a decoder (in this
    example, they both have two hidden layers), but there is a twist: instead of directly
    producing a coding for a given input, the encoder produces a *mean coding* **μ**
    and a standard deviation **σ**. The actual coding is then sampled randomly from
    a Gaussian distribution with mean **μ** and standard deviation **σ**. After that,
    the decoder decodes the sampled coding normally. The right part of the diagram
    shows a training instance going through this autoencoder. First, the encoder produces
    **μ** and **σ**, then a coding is sampled randomly (notice that it is not exactly
    located at **μ**), and finally this coding is decoded. The final output resembles
    the training instance.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看变分自动编码器是如何工作的。[图18-12](#variational_autoencoders_diagram)（左侧）展示了一个变分自动编码器。您可以识别出大多数自动编码器的基本三明治结构，即编码器后面跟着解码器（在这个例子中，它们都有两个隐藏层），但有一个转折：编码器不是直接为给定的输入生成编码，而是生成一个*均值编码*
    **μ** 和一个标准差 **σ**。实际的编码随后从具有均值 **μ** 和标准差 **σ** 的高斯分布中随机采样。之后，解码器正常解码采样的编码。图例的右侧部分展示了训练实例通过这个自动编码器的过程。首先，编码器生成
    **μ** 和 **σ**，然后随机采样一个编码（注意它并不正好位于 **μ** 上），最后对这个编码进行解码。最终的输出类似于训练实例。
- en: 'As you can see in [Figure 18-12](#variational_autoencoders_diagram), although
    the inputs may have a very convoluted distribution, a variational autoencoder
    tends to produce codings that look as though they were sampled from a simple Gaussian
    distribution. During training, the cost function (discussed next) pushes the codings
    to gradually migrate within the coding space (also called the *latent space*)
    to end up looking like a cloud of multidimensional Gaussian points. One great
    consequence is that after training a variational autoencoder, you can very easily
    generate a new instance: just sample a random coding from the Gaussian distribution,
    decode it, and voilà!'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[图18-12](#variational_autoencoders_diagram)中看到的，尽管输入可能具有非常复杂的分布，变分自动编码器往往会生成看起来像是来自简单高斯分布的编码。在训练过程中，成本函数（将在下一节讨论）推动编码在编码空间（也称为*潜在空间*）内逐渐迁移，最终看起来像是一团多维高斯点。一个很好的结果是，在训练完变分自动编码器后，您可以非常容易地生成一个新的实例：只需从高斯分布中随机采样一个编码，解码它，然后就可以了！
- en: '![Diagram of a variational autoencoder with an encoder and decoder, illustrating
    Gaussian noise sampling and coding space transformation.](assets/hmls_1812.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![变分自动编码器中编码器和解码器的图示，说明了高斯噪声采样和编码空间转换。](assets/hmls_1812.png)'
- en: Figure 18-12\. A variational autoencoder (left) and an instance going through
    it (right)
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-12\. 变分自动编码器（左侧）及其通过实例（右侧）
- en: Note
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Sampling from a random distribution is not a differentiable operation, it will
    block backpropagation, so how can we hope to train the encoder? Well, using a
    *reparameterization trick*: sample **ε** from 𝒩(0, 1) and compute **μ** + **σ**
    ⊗ **ε** (element-wise multiplication). This is equivalent to sampling from 𝒩(**μ**,
    **σ**²) but it separates the deterministic and stochastic parts of the process,
    allowing the gradients to flow back into the encoder through **μ** and **σ**.
    The resulting encoder gradients are stochastic (due to **ε**), but they are unbiased
    estimates, and the randomness averages out during training.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从随机分布中采样不是一个可微的操作，它将阻止反向传播，那么我们如何希望训练编码器呢？嗯，使用一个*重新参数化技巧*：从𝒩(0, 1)中采样 **ε**
    并计算 **μ** + **σ** ⊗ **ε**（逐元素乘法）。这相当于从𝒩(**μ**, **σ**²)中采样，但它将过程的确定性和随机性部分分开，允许梯度通过
    **μ** 和 **σ** 流回编码器。结果编码器梯度是随机的（由于 **ε**），但它们是无偏估计，并且在训练过程中随机性会平均化。
- en: 'The cost function is composed of two parts. The first is the usual reconstruction
    loss that pushes the autoencoder to reproduce its inputs. We can use the MSE for
    this, as we did earlier. The second is the *latent loss* that pushes the autoencoder
    to have codings that look as though they were sampled from a simple Gaussian distribution:
    it is the KL divergence between the actual distribution of the codings and the
    desired latent distribution (i.e., the Gaussian distribution). The math is a bit
    more complex than with the sparse autoencoder, in particular because of the Gaussian
    noise, which limits the amount of information that can be transmitted to the coding
    layer. Luckily, the equations simplify, so the latent loss can be computed using
    [Equation 18-3](#var_ae_latent_loss_equation) (for the full mathematical details,
    check out the original paper on variational autoencoders, or Carl Doersch’s [great
    2016 tutorial](https://homl.info/vaetuto).)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数由两部分组成。第一部分是通常的重建损失，它推动自动编码器重现其输入。我们可以使用均方误差（MSE）来计算，就像我们之前做的那样。第二部分是*潜在损失*，它推动自动编码器产生看起来像是从简单高斯分布中采样的编码：它是编码的实际分布与期望的潜在分布（即高斯分布）之间的KL散度。与稀疏自动编码器相比，数学上要复杂一些，特别是由于高斯噪声，这限制了可以传输到编码层的信
    息量。幸运的是，方程式可以简化，因此可以使用[方程式18-3](#var_ae_latent_loss_equation)（对于完整的数学细节，请参阅关于变分自动编码器的原始论文，或卡尔·多尔斯的[优秀2016教程](https://homl.info/vaetuto)）来计算潜在损失。
- en: Equation 18-3\. Variational autoencoder’s latent loss
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式18-3\. 变分自动编码器的潜在损失
- en: $script upper L equals minus one-half sigma-summation Underscript i equals 1
    Overscript n Endscripts left-bracket 1 plus log left-parenthesis sigma Subscript
    i Superscript 2 Baseline right-parenthesis minus sigma Subscript i Superscript
    2 Baseline minus mu Subscript i Superscript 2 Baseline right-bracket$
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: $script upper L equals minus one-half sigma-summation Underscript i equals 1
    Overscript n Endscripts left-bracket 1 plus log left-parenthesis sigma Subscript
    i Superscript 2 Baseline right-parenthesis minus sigma Subscript i Superscript
    2 Baseline minus mu Subscript i Superscript 2 Baseline right-bracket$
- en: In this equation, ℒ is the latent loss, *n* is the codings’ dimensionality,
    and *μ*[i] and *σ*[i] are the mean and standard deviation of the *i*^(th) component
    of the codings. The vectors **μ** and **σ** (which contain all the *μ*[i] and
    *σ*[i]) are output by the encoder, as shown in [Figure 18-12](#variational_autoencoders_diagram)
    (left).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，ℒ是潜在损失，*n*是编码的维度，而*μ*[i]和*σ*[i]是编码的第*i*个分量的均值和标准差。向量**μ**和**σ**（包含所有*μ*[i]和*σ*[i]）由编码器输出，如图[图18-12](#variational_autoencoders_diagram)（左）所示。
- en: A common tweak to the variational autoencoder’s architecture is to make the
    encoder output **γ** = log(**σ**²) rather than **σ**. The latent loss can then
    be computed as shown in [Equation 18-4](#var_ae_latent_loss_equation_2). This
    approach is more numerically stable and speeds up training.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对变分自动编码器架构的一种常见调整是让编码器输出**γ** = log(σ²)而不是σ。然后可以按照[方程式18-4](#var_ae_latent_loss_equation_2)所示计算潜在损失。这种方法在数值上更稳定，并且可以加快训练速度。
- en: Equation 18-4\. Variational autoencoder’s latent loss, rewritten using **γ**
    = log(**σ**²)
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式18-4\. 使用**γ** = log(σ²)重写的变分自动编码器的潜在损失
- en: $script upper L equals minus one-half sigma-summation Underscript i equals 1
    Overscript n Endscripts left-bracket 1 plus gamma Subscript i Baseline minus exp
    left-parenthesis gamma Subscript i Baseline right-parenthesis minus mu Subscript
    i Superscript 2 Baseline right-bracket$
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: $script upper L equals minus one-half sigma-summation Underscript i equals 1
    Overscript n Endscripts left-bracket 1 plus gamma Subscript i Baseline minus exp
    left-parenthesis gamma Subscript i Baseline right-parenthesis minus mu Subscript
    i Superscript 2 Baseline right-bracket$
- en: 'Let’s build a variational autoencoder for Fashion MNIST, using the architecture
    shown in [Figure 18-12](#variational_autoencoders_diagram), except using the **γ**
    tweak:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为Fashion MNIST构建一个变分自动编码器，使用[图18-12](#variational_autoencoders_diagram)中所示的架构，除了使用**γ**调整：
- en: '[PRE14]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s go through this code:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这段代码：
- en: First, we define `VAEOutput`. This allows the model to output a `namedtuple`
    containing the reconstructions (`output`) as well as **μ** (`codings_mean`) and
    **γ** (`codings_logvar`).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们定义`VAEOutput`。这允许模型输出一个包含重建（`output`）、**μ**（`codings_mean`）和**γ**（`codings_logvar`）的`namedtuple`。
- en: 'The encoder and decoder architectures strongly resemble the previous autoencoders,
    but notice that the encoder’s output is twice the size of the codings. This is
    because the encoder does not directly output the codings; instead, it outputs
    the parameters of the Gaussian distribution from which the codings will be sampled:
    the mean (**μ**) and the logarithm of the variance (**γ**).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器和解码器架构与之前的自编码器非常相似，但请注意，编码器的输出是编码的两倍大小。这是因为编码器并不直接输出编码；相反，它输出从其中采样编码的的高斯分布的参数：均值（**μ**）和方差的对数（**γ**）。
- en: The `encode()` method calls the `encoder` model and splits the output in two,
    using the `chunk()` method, to obtain **μ** and **γ**.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`encode()`方法调用`encoder`模型，并使用`chunk()`方法将输出分成两部分，以获得**μ**和**γ**。'
- en: The `sample_codings()` method takes **μ** and **γ** and samples the actual codings.
    For this, it first computes `torch.exp(0.5 * codings_logvar)` to get the codings’
    standard deviation **σ** (you can verify that this works mathematically). Then
    it uses the `torch.randn_like()` function to sample a random vector of the same
    shape as **σ** from the Gaussian distribution with mean 0 and standard deviation
    1, on the same device and with the same data type. Lastly, it multiplies this
    Gaussian noise by **σ**, adds **μ**, and returns the result. This is the reparameterization
    trick we discussed earlier.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sample_codings()`方法接受**μ**和**γ**并采样实际的编码。为此，它首先计算`torch.exp(0.5 * codings_logvar)`以获取编码的标准差**σ**（你可以验证这在数学上是正确的）。然后它使用`torch.randn_like()`函数从均值为0、标准差为1的高斯分布中采样与**σ**相同形状的随机向量，在相同的设备上，并且具有相同的数据类型。最后，它将这个高斯噪声乘以**σ**，加上**μ**，并返回结果。这就是我们之前讨论过的重新参数化技巧。'
- en: The `decode()` method simply calls the decoder model to produce the reconstructions.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decode()`方法简单地调用解码器模型以产生重建。'
- en: The `forward()` method calls the encoder to get **μ** and **γ**, then it uses
    these parameters to sample the codings, which it decodes, and finally it returns
    a `VAEOutput` containing the reconstructions and the parameters **μ** and **γ**,
    which are all needed to compute the VAE loss.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`forward()`方法调用编码器以获取**μ**和**γ**，然后使用这些参数来采样编码，将其解码，并最终返回一个包含重建和参数**μ**和**γ**的`VAEOutput`，这些都是计算VAE损失所必需的。'
- en: 'Speaking of which, let’s now define the loss function, which is the sum of
    the reconstruction loss (MSE) and the latent loss (KL divergence):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 说到这个，我们现在定义损失函数，它是重建损失（MSE）和潜在损失（KL散度）的和：
- en: '[PRE15]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The function first uses [Equation 18-4](#var_ae_latent_loss_equation_2) to
    compute the latent loss (`kl_div`) for each instance in the batch (by summing
    over the last dimension), then it computes the mean latent loss over all the instances
    in the batch (`kl_div.mean()`). Note that the reconstruction loss is the mean
    over all instances in the batch *and* all 784 pixels: this is why we divide the
    latent loss by 784 to ensure that the reconstruction loss and the latent loss
    have the same scale.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数首先使用[方程18-4](#var_ae_latent_loss_equation_2)计算批次中每个实例的潜在损失（通过在最后一个维度上求和），然后计算批次中所有实例的平均潜在损失（`kl_div.mean()`）。请注意，重建损失是批次中所有实例以及所有784个像素的平均值：这就是为什么我们要将潜在损失除以784，以确保重建损失和潜在损失具有相同的尺度。
- en: 'Finally, we can train the model on the Fashion MNIST dataset:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以在Fashion MNIST数据集上训练模型：
- en: '[PRE16]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Generating Fashion MNIST Images
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成Fashion MNIST图像
- en: 'Now let’s use this VAE to generate images that look like fashion items. All
    we need to do is sample random codings from a Gaussian distribution with mean
    0 and variance 1, and decode them:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用这个VAE来生成看起来像时尚商品的图像。我们所需做的只是从均值为0、方差为1的高斯分布中采样随机的编码，并将它们解码：
- en: '[PRE17]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[Figure 18-13](#vae_generated_images_plot) shows the 21 generated images.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[图18-13](#vae_generated_images_plot)显示了21个生成的图像。'
- en: '![Variational autoencoder-generated images of clothing items from the Fashion
    MNIST dataset, appearing fuzzy and lacking detail.](assets/hmls_1813.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![从Fashion MNIST数据集中生成的服装物品的变分自编码器图像，看起来模糊且缺乏细节。](assets/hmls_1813.png)'
- en: Figure 18-13\. Fashion MNIST images generated by the variational autoencoder
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-13\. 由变分自编码器生成的Fashion MNIST图像
- en: The majority of these images look fairly convincing, if a bit too fuzzy. The
    rest are not great, but don’t be too harsh on the autoencoder—it only had a few
    minutes to learn, and you would get much better results by using convolutional
    layers!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些图像看起来相当令人信服，尽管有点模糊。其余的图像不是很好，但不要对自编码器太苛刻——它只有几分钟的时间来学习，如果你使用卷积层，你会得到更好的结果！
- en: 'Variational autoencoders make it possible to perform *semantic interpolation*:
    instead of interpolating between two images at the pixel level, which would look
    as if the two images were just overlaid, we can interpolate at the codings level.
    For example, if we sample two random codings and interpolate between them, then
    decode all of the interpolated codings, we get a sequence of images that gradually
    go from one fashion item to another (see [Figure 18-14](#semantic_interpolation_plot)):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器使得执行 *语义插值* 成为可能：而不是在像素级别在两个图像之间进行插值，这看起来就像两个图像只是叠加在一起，我们可以在编码级别进行插值。例如，如果我们采样两个随机编码并在它们之间进行插值，然后解码所有插值编码，我们得到一系列图像，这些图像逐渐从一个时尚物品变为另一个（见[图18-14](#semantic_interpolation_plot)）：
- en: '[PRE18]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![A series of progressively morphing silhouettes demonstrating semantic interpolation
    between two clothing items.](assets/hmls_1814.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![展示两个服装物品之间语义插值的一系列逐渐变形轮廓的图片。](assets/hmls_1814.png)'
- en: Figure 18-14\. Semantic interpolation
  id: totrans-183
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-14\. 语义插值
- en: 'There are a few variants of VAEs, for example, with different distributions
    for the latent variables. One important variant is discrete VAEs: let’s discuss
    them now.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs（变分自编码器）有几种变体，例如，对于潜在变量使用不同的分布。一个重要的变体是离散VAEs：现在让我们来讨论它们。
- en: Discrete Variational Autoencoders
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 离散变分自编码器
- en: 'A *discrete VAE* (dVAE) is much like a VAE, except the codings are discrete
    rather than continuous: each coding vector contains *latent codes* (also called
    *categories*), each of which is an integer between 0 and *k* – 1, where *k* is
    the number of possible latent codes. The length of the coding vector is often
    denoted as *d*. For example, if you choose *k* = 10 and *d* = 6, then there are
    one million possible coding vectors (10⁶), such as [3, 0, 3, 9, 1, 4]. Discrete
    VAEs are very useful for tokenizing continuous inputs for transformers and other
    models. For example, they are at the core of models like BEiT and DALL·E (see
    [Chapter 16](ch16.html#vit_chapter)).'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *离散VAE*（dVAE）与VAE非常相似，除了编码是离散的而不是连续的：每个编码向量包含 *潜在码*（也称为 *类别*），每个码是一个介于0和
    *k* – 1之间的整数，其中 *k* 是可能的潜在码的数量。编码向量的长度通常表示为 *d*。例如，如果你选择 *k* = 10 和 *d* = 6，那么就有100万个可能的编码向量（10⁶），例如
    [3, 0, 3, 9, 1, 4]。离散VAEs对于对连续输入进行标记，用于transformers和其他模型非常有用。例如，它们是BEiT和DALL·E等模型的核心（见第16章）。
- en: 'The most natural way to make VAEs discrete is to use a categorical distribution
    instead of a Gaussian distribution. This implies a couple of changes:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 要使VAEs离散化最自然的方法是使用分类分布而不是高斯分布。这暗示了一些变化：
- en: First, the encoder must output logits rather than means and variances. For each
    input image, it outputs a tensor of shape [*d*, *k*] containing logits, for example
    [[1.2, –0.8, 0.5], [–1.3, 0.4, 0.3]] if *d* = 2 and *k* = 3.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，编码器必须输出logits而不是均值和方差。对于每个输入图像，它输出一个形状为 [*d*, *k*] 的tensor，包含logits，例如 [[1.2,
    –0.8, 0.5], [–1.3, 0.4, 0.3]] 如果 *d* = 2 和 *k* = 3。
- en: 'Second, since categorical sampling is not a differentiable operation, we must
    once again use a reparameterization trick, but we cannot reuse the same as for
    regular VAEs: we need one designed for categorical distributions. The most popular
    one is the Gumbel-softmax trick. Instead of directly sampling from the categorical
    distribution, we call the `F.gumble_softmax()` function: this implements a differentiable
    approximation of categorical sampling. Given the previous logits, this function
    might output the discrete coding vector [0, 2].'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二，由于分类采样不是一个可微分的操作，我们必须再次使用重参数化技巧，但我们不能像在常规VAEs中那样重用：我们需要一个为分类分布设计的。最受欢迎的是Gumbel-softmax技巧。我们不是直接从分类分布中采样，而是调用`F.gumble_softmax()`函数：这实现了分类采样的可微分近似。给定之前的logits，这个函数可能会输出离散编码向量
    [0, 2]。
- en: Note
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'The Gumbel distribution is used to model the maximum of a set of samples from
    another distribution. For example, it can be used to estimate the probability
    that a river will overflow within the next 10 years. If you add Gumbel noise to
    the logits, then take the argmax of the result, it is mathematically equivalent
    to categorical sampling. However, the argmax operation is not differentiable,
    so we replace it with the softmax during the backward pass: this gives us a differentiable
    approximation of categorical sampling.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Gumbel分布用于模拟来自另一个分布的一组样本的最大值。例如，它可以用来估计未来10年内河流溢出的概率。如果你在logits上添加Gumbel噪声，然后取结果的argmax，这在数学上等同于分类采样。然而，argmax操作是不可微分的，所以在反向传播时我们用softmax来替换它：这给了我们分类采样的可微分近似。
- en: This idea was proposed in 2016 almost simultaneously by two independent teams
    of researchers, one from [DeepMind and Oxford University](https://homl.info/dvae1),⁠^([9](ch18.html#id4106))
    the other from [Google, Cambridge University, and Stanford University](https://homl.info/dvae2).⁠^([10](ch18.html#id4107))
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法在2016年由两个独立的研究团队几乎同时提出，一个来自[DeepMind和牛津大学](https://homl.info/dvae1)，⁠^([9](ch18.html#id4106))，另一个来自[谷歌、剑桥大学和斯坦福大学](https://homl.info/dvae2)。⁠^([10](ch18.html#id4107))
- en: 'Let’s implement a dVAE for Fashion MNIST:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为Fashion MNIST实现一个dVAE：
- en: '[PRE19]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As you can see, this code is very similar to the VAE code. Note that we set
    `hard=True` when calling the `F.gumbel_softmax()` function to ensure that the
    forward pass uses Gumbel-argmax (to obtain one-hot vectors of the sampled codes),
    while the backward pass uses the Gumbel-softmax approximation. Also note that
    we pass a temperature (a scalar) to this function: the logits will be divided
    by this temperature before calling the softmax function. The lower the temperature
    is, the closer the output distribution will be to one-hot vectors (this only affects
    the backward pass). In general, we use a temperature of 1 at the beginning of
    training, then gradually reduce it during training, down to a small value such
    as 0.1.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，此代码与VAE代码非常相似。请注意，我们在调用`F.gumbel_softmax()`函数时设置`hard=True`，以确保前向传播使用Gumbel-argmax（以获得采样的码的one-hot向量），而反向传播使用Gumbel-softmax近似。此外，请注意我们向此函数传递一个温度（一个标量）：在调用softmax函数之前，logits将被除以这个温度。温度越低，输出分布就越接近one-hot向量（这仅影响反向传播）。通常，我们在训练开始时使用温度为1，然后在训练过程中逐步降低它，直到一个较小的值，例如0.1。
- en: 'The loss function is also similar to the regular VAE loss: it’s the sum of
    a reconstruction loss (MSE) and a weighted latent loss (KL divergence). However,
    the KL divergence equation is a bit different since the latent distribution has
    changed. It’s now a uniform categorical distribution, where all possible codes
    are equally likely, so they each have a probability of 1 / *k*. Since log(1 /
    *k*) = –log(*k*), we can add log(*k*) instead of subtracting log(1 / *k*) in the
    KL divergence equation:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数也与常规VAE损失类似：它是重建损失（均方误差，MSE）和加权潜在损失（KL散度）的总和。然而，由于潜在分布已改变，KL散度方程略有不同。现在是一个均匀的分类分布，其中所有可能的码都是等可能的，因此每个码的概率为1
    / *k*。由于log(1 / *k*) = –log(*k*)，我们可以在KL散度方程中添加log(*k*)而不是减去log(1 / *k*)：
- en: '[PRE20]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You can now train the model. Remember to update your training loop to reduce
    the temperature gradually during training, for example:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以训练模型。请记住在训练过程中逐步降低温度，例如：
- en: '[PRE21]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Once the model is trained, you can generate new images by sampling random codings
    from a uniform distribution, one-hot encoding them, then decoding the resulting
    one-hot distribution:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，您可以通过从均匀分布中采样随机编码，然后对结果进行one-hot编码，然后解码得到的one-hot分布来生成新的图像：
- en: '[PRE22]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Another popular approach to discrete VAEs is called *vector quantization* (VQ-VAE),
    [proposed by DeepMind researchers in 2017](https://homl.info/vqvae).⁠^([11](ch18.html#id4111))
    Instead of producing logits, the encoder outputs *d* embeddings, each of dimensionality
    *e*. Then instead of sampling from a categorical distribution, the VQ-VAE maps
    each embedding to the index of the nearest embedding in a trainable embedding
    matrix of shape [*k*, *e*], called the *codebook*. This produces the integer codes.
    Finally, these codes are embedded using the embedding matrix and passed on to
    the decoder.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的离散变分自动编码器（VAE）方法被称为*向量量化*（VQ-VAE），[由DeepMind研究人员于2017年提出](https://homl.info/vqvae)。⁠^([11](ch18.html#id4111))
    与输出logits不同，编码器输出*d*个嵌入，每个嵌入的维度为*e*。然后，VQ-VAE不是从分类分布中采样，而是将每个嵌入映射到形状为[*k*, *e*]的可训练嵌入矩阵中最近的嵌入的索引，称为*码本*。这产生了整数码。最后，这些码通过嵌入矩阵嵌入，并传递给解码器。
- en: 'Since replacing an embedding with the nearest codebook embedding is not a differentiable
    operation, the backward pass pretends that the codebook lookup step is the identity
    function, so the gradients just go straight through this operation: this is why
    this trick is called the *straight-through estimator* (STE). It’s an approximation
    that assumes that the gradients around the encoder embeddings are similar to the
    gradients around the nearest codebook embeddings.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于用最近的码本嵌入替换嵌入不是一个可微操作，反向传播过程假装码本查找步骤是恒等函数，因此梯度直接通过这个操作：这就是为什么这个技巧被称为*直通估计器*（STE）。这是一个假设编码器嵌入周围的梯度与最近码本嵌入周围的梯度相似的近似。
- en: Tip
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: VQ-VAEs can be a bit tricky to implement correctly, but you can use a library
    like [*https://github.com/lucidrains/vector-quantize-pytorch*](https://github.com/lucidrains/vector-quantize-pytorch).
    On the positive side, training is more stable, and the codes are a bit easier
    to interpret.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: VQ-VAEs正确实现可能有点棘手，但你可以使用像[*https://github.com/lucidrains/vector-quantize-pytorch*](https://github.com/lucidrains/vector-quantize-pytorch)这样的库。从积极的一面来看，训练更加稳定，代码也更容易理解。
- en: 'Discrete VAEs work pretty well for small images, but not so much for large
    images: the small-scale features may look good, but there will often be large-scale
    inconsistencies. To improve on this, you can use the trained dVAE to encode your
    whole training set (so each instance becomes a sequence of integers), then use
    this new training set to train a transformer: just treat the codes as tokens,
    and train the transformer using next-token prediction. Intuitively, the dVAE learns
    the vocabulary, while the transformer learns the grammar. Once the transformer
    is trained, you can generate a new image by first generating a sequence of codes
    using the transformer, then passing this sequence to the dVAE’s decoder.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 离散VAEs对于小图像工作得相当好，但对于大图像则不太适用：小尺度特征可能看起来不错，但通常会有大尺度的不一致性。为了改进这一点，你可以使用训练好的dVAE来编码你的整个训练集（因此每个实例都变成一个整数序列），然后使用这个新的训练集来训练一个transformer：只需将代码视为标记，并使用下一个标记预测来训练transformer。直观地说，dVAE学习词汇表，而transformer学习语法。一旦transformer被训练，你就可以通过首先使用transformer生成一系列代码，然后将这个序列传递给dVAE的解码器来生成一个新图像。
- en: 'This two-stage approach also makes it easier to control the image generation
    process: when training the transformer, a textual description of the image can
    be fed to the transformer, for example as a prefix to the sequence of codes. We
    say that the transformer is *conditioned* on the description, which helps it predict
    the correct next code. This way, after training, we can guide the image generation
    process by providing a description of the image we desire. The transformer will
    use this description to generate the appropriate sequence of codes. This is exactly
    how the first DALL·E system worked.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这种两阶段方法也使得控制图像生成过程变得更加容易：当训练transformer时，可以将图像的文本描述输入到transformer中，例如作为代码序列的前缀。我们说transformer是*条件化*在描述上的，这有助于它预测正确的下一个代码。这样，在训练后，我们可以通过提供我们想要的图像描述来引导图像生成过程。transformer将使用这个描述来生成适当的代码序列。这正是第一个DALL·E系统工作的方式。
- en: 'In practice, the encoder and decoder are usually convolutional networks, so
    the latent representation is often organized as a grid (but it’s still flattened
    to a sequence to train the transformer). For example, the encoder may output a
    tensor of shape [256, 32, 32]: that’s a 32 × 32 grid containing 256-dimensional
    embeddings in each cell (or 256 logits in the case of Gumbel-Softmax dVAEs). After
    mapping these embeddings to the indices of the nearest embeddings in the codebook
    (or after categorical sampling), each image is represented as a 32 × 32 grid of
    integers (codes), with codes ranging between 0 and 255\. To generate a new image,
    you use the transformer to predict a sequence of 1,024 codes, organize them into
    a 32 × 32 grid, replace each code with its codebook vector, then pass the result
    to the decoder to generate the final image.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，编码器和解码器通常是卷积网络，因此潜在表示通常组织成一个网格（但仍然被展平成序列以训练transformer）。例如，编码器可能输出形状为[256,
    32, 32]的张量：这是一个32 × 32的网格，每个单元格包含256维的嵌入（或者在Gumbel-Softmax dVAE的情况下是256个logits）。将这些嵌入映射到代码簿中最近嵌入的索引（或进行分类采样）之后，每个图像都表示为一个32
    × 32的整数网格（代码），代码范围在0到255之间。要生成一个新图像，你使用transformer预测一系列1,024个代码，将它们组织成一个32 × 32的网格，将每个代码替换为其代码簿向量，然后将结果传递给解码器以生成最终的图像。
- en: Tip
  id: totrans-209
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'To improve the image quality, you can also stack two or more dVAEs, each producing
    a smaller grid than the previous one: this is called a *hierarchical VAE* (HVAE).
    The encoders are stacked, followed by the decoders in reverse order, and all are
    trained jointly. The loss is the sum of a single reconstruction loss plus multiple
    KL divergence losses (one per dVAE).'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高图像质量，你也可以堆叠两个或更多个dVAEs，每个dVAE产生的网格比前一个更小：这被称为*层次VAE*（HVAE）。编码器是堆叠的，随后是解码器，顺序相反，并且所有这些都是联合训练的。损失是单个重建损失加上多个KL散度损失（每个dVAE一个）的总和。
- en: Let’s now turn our attention to GANs. They are harder to train, but when you
    manage to get them to work, they produce pretty amazing images.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在把注意力转向GANs。它们训练起来更困难，但当你设法让它们工作的时候，它们会产生相当惊人的图像。
- en: Generative Adversarial Networks
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: 'Generative adversarial networks were proposed in a [2014 paper](https://homl.info/gan)⁠^([12](ch18.html#id4125))
    by Ian Goodfellow et al., and although the idea got researchers excited almost
    instantly, it took a few years to overcome some of the difficulties of training
    GANs. Like many great ideas, it seems simple in hindsight: make neural networks
    compete against each other in the hope that this competition will push them to
    excel. As shown in [Figure 18-15](#gan_diagram), a GAN is composed of two neural
    networks:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（Generative adversarial networks）是由 Ian Goodfellow 等人在 [2014 年的一篇论文](https://homl.info/gan)⁠^([12](ch18.html#id4125))
    中提出的，尽管这个想法几乎立刻就激起了研究者的热情，但克服训练 GAN 的一些困难却花了几年时间。像许多伟大的想法一样，事后看来这似乎很简单：让神经网络相互竞争，希望这种竞争能推动它们达到卓越。如图
    [图 18-15](#gan_diagram) 所示，GAN 由两个神经网络组成：
- en: Generator
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器
- en: 'Takes a random coding as input (typically sampled from a Gaussian distribution)
    and outputs some data—typically, an image. The coding is the latent representation
    of the image to be generated. So, as you can see, the generator offers the same
    functionality as a decoder in a variational autoencoder, and it can be used in
    the same way to generate new images: just feed it a random vector, and it outputs
    a brand-new image. However, it is trained very differently, as you will soon see.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 以随机编码作为输入（通常从高斯分布中采样）并输出一些数据——通常是图像。编码是待生成图像的潜在表示。所以，正如你所看到的，生成器在变分自编码器中的解码器具有相同的功能，并且可以以相同的方式用于生成新图像：只需给它一个随机向量，它就会输出一个全新的图像。然而，它的训练方式却非常不同，你很快就会看到。
- en: Discriminator
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器
- en: Takes either a fake image from the generator or a real image from the training
    set as input, and must guess whether the input image is fake or real.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 以生成器的一个假图像或训练集中的一个真实图像作为输入，并必须猜测输入图像是假还是真。
- en: '![Diagram illustrating a generative adversarial network (GAN) where the generator
    produces fake images from a random vector to trick the discriminator, which aims
    to distinguish between fake and real images.](assets/hmls_1815.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图示生成对抗网络（GAN），其中生成器从随机向量生成假图像以欺骗判别器，而判别器的目标是区分假图像和真实图像。](assets/hmls_1815.png)'
- en: Figure 18-15\. A generative adversarial network
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-15\. 生成对抗网络
- en: 'During training, the generator and the discriminator have opposite goals: the
    discriminator tries to tell fake images from real images, while the generator
    tries to produce images that look real enough to trick the discriminator. Because
    the GAN is composed of two networks with different objectives, it cannot be trained
    like a regular neural network. Each training iteration is divided into two phases:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，生成器和判别器有相反的目标：判别器试图区分假图像和真实图像，而生成器则试图生成足够真实的图像以欺骗判别器。由于 GAN 由两个具有不同目标的网络组成，因此不能像常规神经网络那样进行训练。每个训练迭代分为两个阶段：
- en: 'First phase: train the discriminator'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段：训练判别器
- en: A batch of real images is sampled from the training set and is completed with
    an equal number of fake images produced by the generator. The labels are set to
    0 for fake images and 1 for real images, and the discriminator is trained on this
    labeled batch for one step, using the binary cross-entropy loss. Importantly,
    backpropagation only optimizes the weights of the discriminator during this phase.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 从训练集中采样一批真实图像，并补充与生成器产生的相同数量的假图像。标签设置为 0 表示假图像，1 表示真实图像，判别器在这个标记批次上训练一步，使用二元交叉熵损失。重要的是，在这一阶段，反向传播只优化判别器的权重。
- en: 'Second phase: train the generator'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 第二阶段：训练生成器
- en: We first use the generator to produce another batch of fake images, and once
    again the discriminator is used to tell whether the images are fake or real. This
    time we do not add real images to the batch, and all the labels are set to 1 (real);
    in other words, we want the generator to produce images that the discriminator
    will (wrongly) believe to be real! Crucially, the weights of the discriminator
    are frozen during this step, so backpropagation only affects the weights of the
    generator.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先使用生成器生成另一批假图像，然后再次使用判别器来判断图像是否为假。这次我们不添加真实图像到批次中，所有标签都设置为 1（真实）；换句话说，我们希望生成器生成判别器（错误地）认为真实的图像！关键的是，在这一步中，判别器的权重被冻结，因此反向传播只影响生成器的权重。
- en: Note
  id: totrans-225
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The generator never actually sees any real images, yet it gradually learns to
    produce convincing fake images! All it gets is the gradients flowing back through
    the discriminator. Fortunately, the better the discriminator gets, the more information
    about the real images is contained in these secondhand gradients, so the generator
    can make significant progress.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器实际上从未看到任何真实图像，但它逐渐学会了产生令人信服的假图像！它所获得的一切只是通过判别器流回的梯度。幸运的是，判别器越好，这些二手梯度中包含的真实图像信息就越多，因此生成器可以取得显著的进步。
- en: Let’s go ahead and build a simple GAN for Fashion MNIST.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续构建一个简单的针对Fashion MNIST的GAN。
- en: 'First, we need to build the generator and the discriminator. The generator
    is similar to an autoencoder’s decoder—it takes a coding vector as input and outputs
    an image—and the discriminator is a regular binary classifier—it takes an image
    as input and ends with a dense layer containing a single unit and using the sigmoid
    activation function:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要构建生成器和判别器。生成器类似于自编码器的解码器——它以编码向量作为输入并输出一个图像——判别器是一个常规的二分类器——它以图像作为输入，并以包含单个单元和sigmoid激活函数的密集层结束：
- en: '[PRE23]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Since the training loop is unusual, we need a new training function:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练循环不寻常，我们需要一个新的训练函数：
- en: '[PRE24]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As discussed earlier, you can see the two phases at each iteration: first the
    discriminator makes a gradient descent step, then it’s the generator’s turn. We
    use a separate optimizer for each. Let’s look in more detail:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，你可以在每个迭代中看到两个阶段：首先判别器进行梯度下降步骤，然后轮到生成器。我们为每个阶段使用一个单独的优化器。让我们更详细地看看：
- en: Phase one
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段
- en: We feed a batch of real images to the discriminator and compute the loss given
    targets equal to one; indeed, we want the discriminator to predict that these
    images are real. We then generate some random codings and feed them to the generator
    to produce some fake images. Note that we call `detach()` on these images because
    we don’t want gradient descent to affect the generator in this phase. Then we
    pass these fake images to the discriminator and compute the loss given targets
    equal to zero; we want the discriminator to predict that these images are fake.
    The total discriminator loss is the `real_loss` plus the `fake_loss`. Finally,
    we perform the gradient descent step, improving the discriminator.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向判别器输入一批真实图像，并计算给定目标等于一的损失；实际上，我们希望判别器预测这些图像是真实的。然后我们生成一些随机编码，并将它们输入到生成器以生成一些假图像。注意，我们对这些图像调用`detach()`，因为我们不希望梯度下降影响生成器在这个阶段。然后我们将这些假图像传递给判别器，并计算给定目标等于零的损失；我们希望判别器预测这些图像是假的。判别器的总损失是`real_loss`加上`fake_loss`。最后，我们执行梯度下降步骤，改进判别器。
- en: Phase two
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 第二阶段
- en: 'We generate some fake images using the generator, and we pass them to the discriminator,
    like we just did. However, this time we don’t call `detach()` on the fake images
    since we want to train the generator. Moreover, we make the discriminator untrainable
    by setting `p.required_grad = False` for each parameter `p`. We then compute the
    loss using targets equal to one: indeed, we want the generator to fool the discriminator,
    so we want the discriminator to wrongly predict that these are real images. And
    finally, we perform a gradient descent step for the generator, and we make the
    discriminator trainable again.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用生成器生成一些假图像，并将它们传递给判别器，就像我们刚才做的那样。然而，这次我们不对假图像调用`detach()`，因为我们想训练生成器。此外，我们通过将每个参数`p`的`p.required_grad`设置为`False`来使判别器不可训练。然后我们使用目标等于一进行损失计算：实际上，我们希望生成器欺骗判别器，所以我们希望判别器错误地预测这些是真实图像。最后，我们为生成器执行梯度下降步骤，并将判别器重新设置为可训练。
- en: 'That’s it! After training, you can randomly sample some codings from a Gaussian
    distribution and feed them to the generator to produce new images:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些！训练完成后，你可以从高斯分布中随机采样一些编码，并将它们输入到生成器中以生成新的图像：
- en: '[PRE25]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: If you display the generated images (see [Figure 18-16](#gan_generated_images_plot)),
    you will see that at the end of the first epoch, they already start to look like
    (very noisy) Fashion MNIST images.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你展示了生成的图像（见[图18-16](#gan_generated_images_plot)），你会看到在第一个训练周期结束时，它们已经开始看起来像（非常嘈杂的）Fashion
    MNIST图像。
- en: '![Noisy, black-and-white images attempting to depict Fashion MNIST items, generated
    by a GAN after one epoch of training.](assets/hmls_1816.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![试图描绘Fashion MNIST物品的嘈杂、黑白图像，由经过一个训练周期后GAN生成的。](assets/hmls_1816.png)'
- en: Figure 18-16\. Images generated by the GAN after one epoch of training
  id: totrans-241
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-16\. 经过一个训练周期后GAN生成的图像
- en: Unfortunately, the images never really get much better than that, and you may
    even find epochs where the GAN seems to be forgetting what it learned. Why is
    that? Well, it turns out that training a GAN can be challenging. Let’s see why.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，图像从未真正变得比那更好，你甚至可能会发现某些时期GAN似乎在忘记它学到的知识。为什么会这样呢？嗯，事实证明，训练GAN可能具有挑战性。让我们看看原因。
- en: The Difficulties of Training GANs
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练GAN的困难
- en: 'During training, the generator and the discriminator constantly try to outsmart
    each other in a zero-sum game. As training advances, the game may end up in a
    state that game theorists call a *Nash equilibrium*, named after the mathematician
    John Nash. This occurs when no player would be better off changing their own strategy,
    assuming the other players do not change theirs. For example, a Nash equilibrium
    is reached when everyone drives on the left side of the road: no driver would
    be better off being the only one to switch sides. Of course, there is a second
    possible Nash equilibrium: when everyone drives on the *right* side of the road.
    Different initial states and dynamics may lead to one equilibrium or the other.
    In this example, there is a single optimal strategy once an equilibrium is reached
    (i.e., driving on the same side as everyone else), but a Nash equilibrium can
    involve multiple competing strategies (e.g., a predator chases its prey, the prey
    tries to escape, and neither would be better off changing their strategy).'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，生成器和判别器不断在零和游戏中试图智胜对方。随着训练的进行，游戏可能最终进入博弈论学家所说的*纳什均衡*状态，以数学家约翰·纳什的名字命名。这发生在没有任何玩家会因改变自己的策略而变得更好，假设其他玩家不改变他们的策略。例如，当每个人都开车在道路的左侧时，就会达到纳什均衡：没有司机会因为成为唯一一个改变车道的人而变得更好。当然，还有第二个可能的纳什均衡：当每个人都开车在*右侧*的道路上。不同的初始状态和动态可能导致一个均衡或另一个均衡。在这个例子中，一旦达到均衡，就有一个单一的优化策略（即与其他人一样开车），但纳什均衡可以涉及多个竞争策略（例如，捕食者追逐猎物，猎物试图逃跑，并且双方都不会因为改变策略而变得更好）。
- en: 'So how does this apply to GANs? Well, the authors of the GAN paper demonstrated
    that a GAN can only reach a single Nash equilibrium: that’s when the generator
    produces perfectly realistic images, and the discriminator is forced to guess
    (50% real, 50% fake). This fact is very encouraging, as it would seem that you
    just need to train the GAN long enough and it will eventually reach this equilibrium,
    giving you a perfect generator. Unfortunately, it’s not that simple: nothing guarantees
    that the equilibrium will ever be reached.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这如何应用到生成对抗网络（GANs）中呢？嗯，GAN论文的作者们证明了GAN只能达到一个纳什均衡：那就是生成器产生完全逼真的图像，而判别器被迫猜测（50%真实，50%伪造）。这个事实非常鼓舞人心，因为它似乎意味着你只需要训练GAN足够长的时间，它最终会达到这个均衡，给你一个完美的生成器。不幸的是，事情并没有那么简单：没有任何东西能保证均衡一定会被达到。
- en: 'The biggest difficulty is called *mode collapse*: when the generator’s outputs
    gradually become less diverse. How can this happen? Suppose the generator gets
    better at producing convincing shoes than any other class. It will fool the discriminator
    a bit more with shoes, and this will encourage it to produce even more images
    of shoes. Gradually, it will forget how to produce anything else. Meanwhile, the
    only fake images that the discriminator will see will be shoes, so it will also
    forget how to discriminate fake images of other classes. Eventually, when the
    discriminator manages to discriminate the fake shoes from the real ones, the generator
    will be forced to move to another class. It may then become good at shirts, forgetting
    about shoes, and the discriminator will follow. The GAN may gradually cycle across
    a few classes, never really becoming very good at any of them (see the top row
    of [Figure 18-17](#gan_mode_collapse_diagram)).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的困难被称为*模式崩溃*：当生成器的输出逐渐变得不那么多样化。这怎么可能发生呢？假设生成器在制作令人信服的鞋子方面比其他任何类别都做得更好。它将通过鞋子稍微欺骗判别器，这将鼓励它产生更多鞋子的图像。渐渐地，它将忘记如何制作其他任何东西。同时，判别器将看到的唯一伪造图像将是鞋子，因此它也将忘记如何区分其他类别的伪造图像。最终，当判别器设法区分伪造的鞋子与真实的鞋子时，生成器将被迫转移到另一个类别。它可能然后擅长衬衫，忘记鞋子，判别器也会跟随。GAN可能逐渐在几个类别之间循环，但从未真正擅长其中的任何一个（参见[图18-17](#gan_mode_collapse_diagram)的顶部行）。
- en: '![Diagram illustrating mode collapse in GAN training, showing clustered fake
    examples in the top row versus diverse results in the bottom row with successful
    training.](assets/hmls_1817.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![说明GAN训练中模式崩溃的图，显示顶部行中的聚类伪造示例与底部行中成功训练的多样化结果](assets/hmls_1817.png)'
- en: Figure 18-17\. Mode collapse while training a GAN (top row) versus successful
    training without mode collapse (bottom row)
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-17\. 训练GAN时的模式坍塌（顶部行）与无模式坍塌的成功训练（底部行）
- en: 'Moreover, because the generator and the discriminator are constantly pushing
    against each other, their parameters may end up oscillating and becoming unstable.
    Training may begin properly, then suddenly diverge for no apparent reason due
    to these instabilities. And since many factors affect these complex dynamics,
    GANs are very sensitive to the hyperparameters: you may have to spend a lot of
    effort fine-tuning them.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于生成器和判别器不断相互对抗，它们的参数最终可能会振荡并变得不稳定。训练可能一开始进行得很顺利，然后突然由于这些不稳定性而突然发散，没有任何明显的原因。由于许多因素会影响这些复杂的动态，GANs对超参数非常敏感：你可能需要花费大量精力来微调它们。
- en: 'These problems have kept researchers very busy since 2014\. Many papers have
    been published on this topic, some proposing new cost functions⁠^([13](ch18.html#id4132))
    (though a [2018 paper](https://homl.info/gansequal)⁠^([14](ch18.html#id4133))
    by Google researchers questions their efficiency) or techniques to stabilize training
    or to avoid the mode collapse issue. For example, a popular technique called *experience
    replay* consists of storing the images produced by the generator at each iteration
    in a replay buffer (gradually dropping older generated images) and training the
    discriminator using real images plus fake images drawn from this buffer (rather
    than just fake images produced by the current generator). This reduces the chances
    that the discriminator will overfit the latest generator’s outputs. Another common
    technique is called *mini-batch discrimination*: it measures how similar images
    are across the batch and provides this statistic to the discriminator, so it can
    easily reject a whole batch of fake images that lack diversity. This encourages
    the generator to produce a greater variety of images, reducing the chance of mode
    collapse (see the bottom row of [Figure 18-17](#gan_mode_collapse_diagram)).'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 自2014年以来，这些问题一直让研究人员非常忙碌。关于这个主题已经发表了多篇论文，其中一些提出了新的成本函数⁠^([13](ch18.html#id4132))（尽管谷歌研究人员在2018年的一篇论文⁠^([14](ch18.html#id4133))中对其效率提出了质疑）或稳定训练或避免模式坍塌问题的技术。例如，一种流行的技术称为**经验回放**，它包括在每个迭代中将生成器产生的图像存储在回放缓冲区中（逐渐丢弃较旧的生成图像），并使用真实图像加上从该缓冲区中抽取的假图像来训练判别器（而不是仅使用当前生成器产生的假图像）。这减少了判别器过度拟合最新生成器输出的可能性。另一种常见的技术称为**小批量判别**：它衡量批次中图像之间的相似性，并将此统计信息提供给判别器，以便它可以轻松拒绝缺乏多样性的整个批次假图像。这鼓励生成器产生更多样化的图像，减少模式坍塌的可能性（见[图18-17](#gan_mode_collapse_diagram)的底部行）。
- en: 'In short, this was a very active field of research, and much progress was made
    until quite recently: from *deep Convolutional GANs* (DCGANs) based on convolutional
    layers (see the notebook for an example), to *progressively growing GANs* that
    could produce high-resolution images, or *StyleGANs* that gave the user fine-grained
    control over the image generation process, it seemed like GANs had a bright future
    ahead of them. But when diffusion models started to produce amazing images as
    well, with a much more stable training process and more diverse images, GANs were
    quickly sidelined. So let’s now turn our attention to diffusion models.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这曾经是一个非常活跃的研究领域，直到最近都取得了很大的进展：从基于卷积层的**深度卷积生成对抗网络**（DCGANs）（见笔记本中的示例），到能够生成高分辨率图像的**渐进式增长生成对抗网络**，或者允许用户对图像生成过程进行精细控制的**风格生成对抗网络**（StyleGANs），GANs似乎拥有光明的未来。但当扩散模型也开始产生令人惊叹的图像，并且具有更稳定的训练过程和更多样化的图像时，GANs很快就被边缘化了。因此，现在让我们将注意力转向扩散模型。
- en: Diffusion Models
  id: totrans-252
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩散模型
- en: 'The ideas behind diffusion models have been around for many years, but they
    were first formalized in their modern form in a [2015 paper](https://homl.info/diffusion)⁠^([15](ch18.html#id4147))
    by Jascha Sohl-Dickstein et al. from Stanford University and UC Berkeley. The
    authors applied tools from statistical mechanics to model a diffusion process,
    similar to a drop of milk diffusing in a cup of tea. The core idea is to train
    a model to learn the reverse process: start from the completely mixed state and
    gradually “unmix” the milk from the tea. Using this idea, they obtained promising
    results in image generation, but since GANs produced more convincing images back
    then, and they did so much faster, diffusion models did not get as much attention.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型背后的理念已经存在很多年了，但它们首次以现代形式在斯坦福大学和加州大学伯克利分校的Jascha Sohl-Dickstein等人于2015年发表的一篇[论文](https://homl.info/diffusion)⁠^([15](ch18.html#id4147))中被正式化。作者们将统计力学的工具应用于模拟一个扩散过程，类似于一滴牛奶在茶杯中扩散的过程。核心思想是训练一个模型来学习逆向过程：从完全混合的状态开始，逐渐“分离”牛奶和茶。利用这个想法，他们在图像生成方面取得了有希望的结果，但由于当时GANs生成的图像更加令人信服，并且速度更快，扩散模型并没有得到太多的关注。
- en: 'Then, in 2020, [Jonathan Ho et al.](https://homl.info/ddpm), also from UC Berkeley,
    managed to build a diffusion model capable of generating highly realistic images,
    which they called a *denoising diffusion probabilistic model* (DDPM).⁠^([16](ch18.html#id4148))
    A few months later, a [2021 paper](https://homl.info/ddpm2)⁠^([17](ch18.html#id4149))
    by OpenAI researchers Alex Nichol and Prafulla Dhariwal analyzed the DDPM architecture
    and proposed several improvements that allowed DDPMs to finally beat GANs: not
    only are DDPMs much easier to train than GANs, but the generated images are more
    diverse and of even higher quality. The main downside of DDPMs, as you will see,
    is that they take a very long time to generate images, compared to GANs or VAEs.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在2020年，来自加州大学伯克利分校的[Jonathan Ho等人](https://homl.info/ddpm)成功构建了一个能够生成高度逼真图像的扩散模型，他们称之为*去噪扩散概率模型*（DDPM）。⁠^([16](ch18.html#id4148))
    几个月后，OpenAI的研究员Alex Nichol和Prafulla Dhariwal在[2021年的一篇论文](https://homl.info/ddpm2)⁠^([17](ch18.html#id4149))中分析了DDPM架构，并提出了几个改进，使得DDPM最终击败了GANs：DDPM不仅比GANs更容易训练，而且生成的图像更加多样化，质量更高。DDPM的主要缺点，正如你将看到的，是它们生成图像所需的时间非常长，与GANs或VAEs相比。
- en: 'So how exactly does a DDPM work? Well, suppose you start with a picture of
    a cat (like the one in [Figure 18-18](#denoising_model_diagram)), noted **x**[0],
    and at each time step *t* you add a little bit of Gaussian noise to the image,
    with mean 0 and variance *β*[*t*] (a scalar). This noise is independent for each
    pixel (using the same mean and variance): we call it *isotropic*. You first obtain
    the image **x**[1], then **x**[2], and so on, until the cat is completely hidden
    by the noise, impossible to see. The last time step is noted *T*. In the original
    DDPM paper, the authors used *T* = 1,000, and they scheduled the variance *β*[*t*]
    in such a way that the cat signal fades linearly between time steps 0 and *T*.
    In the improved DDPM paper, *T* was bumped up to 4,000, and the variance schedule
    was tweaked to change more slowly at the beginning and at the end. In short, we’re
    gradually drowning the cat in noise: this is called the *forward process*.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，DDPM究竟是如何工作的呢？好吧，假设你从一个猫的图片开始（就像[图18-18](#denoising_model_diagram)中的那样），记为**x**[0]，然后在每个时间步长
    *t* 向图片中添加一点高斯噪声，均值为0，方差为 *β*[*t*]（一个标量）。这种噪声对每个像素是独立的（使用相同的均值和方差）：我们称之为*各向同性*。你首先获得图像
    **x**[1]，然后 **x**[2]，依此类推，直到猫被噪声完全掩盖，无法辨认。最后一个时间步长记为 *T*。在原始DDPM论文中，作者们使用了 *T*
    = 1,000，并且他们安排方差 *β*[*t*] 以便猫的信号在时间步长0和 *T* 之间线性衰减。在改进的DDPM论文中，*T* 提高到4,000，方差调度被调整以在开始和结束时变化得更慢。简而言之，我们是在逐渐将猫淹没在噪声中：这被称为*前向过程*。
- en: '![Diagram of the denoising diffusion probabilistic model (DDPM) showing the
    forward and reverse process, where a cat image gradually becomes obscured by noise
    until details are lost, illustrating how noise is added and then removed.](assets/hmls_1818.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![去噪扩散概率模型（DDPM）的示意图，展示了前向和逆向过程，其中一张猫的图片逐渐被噪声模糊，直至细节消失，说明了噪声是如何添加和移除的。](assets/hmls_1818.png)'
- en: Figure 18-18\. The forward process *q* and reverse process *p*
  id: totrans-257
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-18\. 前向过程 *q* 和逆向过程 *p*
- en: As we add more and more Gaussian noise in the forward process, the distribution
    of pixel values becomes more and more Gaussian. One important detail I left out
    is that the pixel values get rescaled slightly at each step, by a factor of $StartRoot
    1 minus beta Subscript t Baseline EndRoot$ . This ensures that the mean of the
    pixel values gradually approaches 0, since the scaling factor is a bit smaller
    than 1 (imagine repeatedly multiplying a number by 0.99). It also ensures that
    the variance will gradually converge to 1\. This is because the standard deviation
    of the pixel values also gets scaled by $StartRoot 1 minus beta Subscript t Baseline
    EndRoot$ , so the variance gets scaled by 1 – *β*[*t*] (i.e., the square of the
    scaling factor). But the variance cannot shrink to 0 since we’re adding Gaussian
    noise with variance *β*[*t*] at each step. And since variances add up when you
    sum Gaussian distributions, the variance must converge to 1 – *β*[*t*] + *β*[*t*]
    = 1.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们越来越多地在正向过程中添加高斯噪声，像素值的分布变得越来越像高斯分布。我遗漏的一个重要细节是，像素值在每一步都会稍微重新缩放，缩放因子为 $StartRoot
    1 minus beta Subscript t Baseline EndRoot$ 。这确保了像素值的平均值逐渐接近 0，因为缩放因子略小于 1（想象一下反复将一个数字乘以
    0.99）。它还确保方差将逐渐收敛到 1。这是因为像素值的标准差也会被 $StartRoot 1 minus beta Subscript t Baseline
    EndRoot$ 缩放，因此方差会缩放为 1 – *β*[*t*]（即缩放因子的平方）。但由于我们在每一步都添加了具有方差 *β*[*t*] 的高斯噪声，方差不能缩小到
    0。并且由于高斯分布求和时方差相加，方差必须收敛到 1 – *β*[*t*] + *β*[*t*] = 1。
- en: The forward diffusion process is summarized in [Equation 18-5](#forward_process_equation).
    This equation won’t teach you anything new about the forward process, but it’s
    useful to understand this type of mathematical notation, as it’s often used in
    ML papers. This equation defines the probability distribution *q* of **x**[*t*],
    given **x**[*t*–1] as a Gaussian distribution with mean **x**[*t*–1] times the
    scaling factor, and with a covariance matrix equal to *β*[*t*]**I**. This is the
    identity matrix **I** multiplied by *β*[*t*], which means that the noise is isotropic
    with variance *β*[*t*].
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 正向扩散过程总结在 [方程 18-5](#forward_process_equation) 中。这个方程不会教你关于正向过程的新知识，但它有助于理解这种数学符号，因为它在机器学习论文中经常被使用。这个方程定义了
    **x**[*t*] 给定 **x**[*t*–1] 的概率分布 *q*，它是一个以缩放因子乘以 **x**[*t*–1] 为均值的正态分布，协方差矩阵等于
    *β*[*t*]**I**。这意味着协方差矩阵是 **I** 乘以 *β*[*t*]，即噪声是各向同性的，方差为 *β*[*t*]。
- en: Equation 18-5\. Probability distribution *q* of the forward diffusion process
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 18-5\. 正向扩散过程的概率分布 *q*
- en: $q left-parenthesis bold x Subscript t Baseline vertical-bar bold x Subscript
    t minus 1 Baseline right-parenthesis equals script upper N left-parenthesis StartRoot
    1 minus beta Subscript t Baseline EndRoot bold x Subscript t minus 1 Baseline
    comma beta Subscript t Baseline bold upper I right-parenthesis$
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: $q left-parenthesis bold x Subscript t Baseline vertical-bar bold x Subscript
    t minus 1 Baseline right-parenthesis equals script upper N left-parenthesis StartRoot
    1 minus beta Subscript t Baseline EndRoot bold x Subscript t minus 1 Baseline
    comma beta Subscript t Baseline bold upper I right-parenthesis$
- en: 'Interestingly, there’s a shortcut for the forward process: it’s possible to
    sample an image **x**[*t*] given **x**[0] without having to first compute **x**[1],
    **x**[2], …​, **x**[*t*–1]. Indeed, since the sum of multiple independent Gaussian
    distributions is also a Gaussian distribution, all the noise can be added in just
    one shot. If we define *α*[*t*] = 1 – *β*[*t*], and *α̅*[*t*] = *α*[*1*] × *α*[*2*]
    × …​× *α*[*t*] = $alpha overbar Subscript t Baseline equals product Underscript
    i equals 1 Overscript t Endscripts alpha Subscript t$ , then we can compute **x**[*t*]
    using [Equation 18-6](#fast_forward_process_equation). This is the equation we
    will be using, as it is much faster.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，正向过程有一个捷径：给定 **x**[0]，可以直接采样图像 **x**[*t*]，而无需先计算 **x**[1]、**x**[2]、…、**x**[*t*–1]。事实上，由于多个独立高斯分布之和也是一个高斯分布，所有噪声都可以在一次操作中添加。如果我们定义
    *α*[*t*] = 1 – *β*[*t*]，以及 *α̅*[*t*] = *α*[*1*] × *α*[*2*] × …​× *α*[*t*] = $alpha
    overbar Subscript t Baseline equals product Underscript i equals 1 Overscript
    t Endscripts alpha Subscript t$ ，那么我们可以使用 [方程 18-6](#fast_forward_process_equation)
    来计算 **x**[*t*]。这就是我们将要使用的方程，因为它要快得多。
- en: Equation 18-6\. Shortcut for the forward diffusion process
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 18-6\. 正向扩散过程的捷径
- en: $q left-parenthesis bold x Subscript t Baseline vertical-bar bold x 0 right-parenthesis
    equals script upper N left-parenthesis StartRoot alpha overbar Subscript t Baseline
    EndRoot bold x 0 comma left-parenthesis 1 minus alpha overbar Subscript t Baseline
    right-parenthesis bold upper I right-parenthesis$
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: $q left-parenthesis bold x Subscript t Baseline vertical-bar bold x 0 right-parenthesis
    equals script upper N left-parenthesis StartRoot alpha overbar Subscript t Baseline
    EndRoot bold x 0 comma left-parenthesis 1 minus alpha overbar Subscript t Baseline
    right-parenthesis bold upper I right-parenthesis$
- en: 'Our goal, of course, is not to drown cats in noise. On the contrary, we want
    to create many new cats! We can do so by training a model that can perform the
    *reverse process*: going from **x**[*t*] to **x**[*t*–1]. We can then use it to
    remove a tiny bit of noise from an image, and repeat the operation many times
    until all the noise is gone. It’s not a basic noise filter that relies only on
    the neighboring pixels: instead, when noise is removed, it is replaced with realistic
    pixels, depending on the training data. For example, if we train the model on
    a dataset containing many cat images, then we can give it a picture entirely full
    of Gaussian noise, and the model will gradually make a brand new cat appear (see
    [Figure 18-18](#denoising_model_diagram)).'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的目标不是在噪音中淹没猫。相反，我们想要创造许多新的猫！我们可以通过训练一个能够执行 *反向过程* 的模型来实现这一点：从 **x**[*t*]
    到 **x**[*t*–1]。然后我们可以用它来从图像中移除一小部分噪音，并重复此操作多次，直到所有噪音都消失。这不是一个仅依赖于相邻像素的基本噪音过滤器：相反，当移除噪音时，它会被根据训练数据替换为真实的像素。例如，如果我们在一个包含许多猫图像的数据集上训练模型，那么我们可以给它一张完全充满高斯噪音的图片，模型将逐渐使一只全新的猫出现（见
    [图 18-18](#denoising_model_diagram)）。
- en: OK, so let’s start coding! The first thing we need to do is to code the forward
    process. For this, we will first need to implement the variance schedule. How
    can we control how fast the cat disappears? At each time step *t*, the pixel values
    get multiplied by $StartRoot 1 minus beta Subscript t Baseline EndRoot$ and noise
    with mean 0 and variance *β*[*t*] gets added (as explained earlier). So, the part
    of the image’s variance that comes from the original cat image shrinks by a factor
    of *α*[*t*] = 1 – \beta_t at each step. After *t* time steps, it will have shrunk
    by a factor of *α̅*[*t*] = *α*[*1*] × *α*[*2*] × …​ × *α*[*t*]. It’s this “cat
    signal” factor *α̅*[*t*] that we want to schedule so it shrinks down from 1 to
    0 gradually between time steps 0 and *T*. In the improved DDPM paper, the authors
    schedule *α̅*[*t*] according to [Equation 18-7](#variance_schedule_equation).
    This schedule is represented in [Figure 18-19](#variance_schedule_plot).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么让我们开始编码！我们首先需要做的是编码前向过程。为此，我们首先需要实现方差调度。我们如何控制猫消失的速度？在每一个时间步 *t*，像素值会被乘以
    $StartRoot 1 minus beta Subscript t Baseline EndRoot$，并且会添加均值为 0、方差为 *β*[*t*]
    的噪音（如前所述）。因此，图像方差中来自原始猫图像的部分在每个步骤中都会以 *α*[*t*] = 1 – \beta_t 的因子缩小。经过 *t* 个时间步后，它将缩小到
    *α̅*[*t*] = *α*[*1*] × *α*[*2*] × …​ × *α*[*t*] 的因子。我们想要调度这个“猫信号”因子 *α̅*[*t*]，使其在时间步
    0 和 *T* 之间逐渐从 1 缩小到 0。在改进的 DDPM 论文中，作者根据 [方程 18-7](#variance_schedule_equation)
    调度 *α̅*[*t*]。这个调度在 [图 18-19](#variance_schedule_plot) 中表示。
- en: Equation 18-7\. Variance schedule equation for the forward diffusion process
  id: totrans-267
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 18-7\. 前向扩散过程的方差调度方程
- en: $beta Subscript t Baseline equals 1 minus StartFraction alpha overbar Subscript
    t Baseline Over alpha overbar Subscript t minus 1 Baseline EndFraction with alpha
    overbar Subscript t Baseline equals StartFraction f left-parenthesis t right-parenthesis
    Over f left-parenthesis 0 right-parenthesis EndFraction and f left-parenthesis
    t right-parenthesis equals cosine squared left-parenthesis StartStartFraction
    StartFraction t Over upper T EndFraction plus s OverOver 1 plus s EndEndFraction
    dot StartFraction pi Over 2 EndFraction right-parenthesis$
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: $beta Subscript t Baseline equals 1 minus StartFraction alpha overbar Subscript
    t Baseline Over alpha overbar Subscript t minus 1 Baseline EndFraction with alpha
    overbar Subscript t Baseline equals StartFraction f left-parenthesis t right-parenthesis
    Over f left-parenthesis 0 right-parenthesis EndFraction and f left-parenthesis
    t right-parenthesis equals cosine squared left-parenthesis StartStartFraction
    StartFraction t Over upper T EndFraction plus s OverOver 1 plus s EndEndFraction
    dot StartFraction pi Over 2 EndFraction right-parenthesis$
- en: 'In this equation:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '*s* is a tiny value which prevents *β*[*t*] from being too small near *t* =
    0\. In the paper, the authors used *s* = 0.008.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*s* 是一个非常小的值，它防止 *β*[*t*] 在 *t* = 0 附近变得太小。在论文中，作者使用了 *s* = 0.008。'
- en: '*β*[*t*] is clipped to be no larger than 0.999 to avoid instabilities near
    *t* = *T*.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*β*[*t*] 被限制在不超过 0.999，以避免在 *t* = *T* 附近的稳定性问题。'
- en: '![hmls 1819](assets/hmls_1819.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![hmls 1819](assets/hmls_1819.png)'
- en: Figure 18-19\. Noise variance schedule *β*[*t*], and the remaining signal variance
    *α̅*[*t*]
  id: totrans-273
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 18-19\. 噪声方差计划 *β*[*t*]，以及剩余的信号方差 *α̅*[*t*]
- en: 'Let’s create a small function to compute *α*[*t*], *β*[*t*], and *α̅*[*t*],
    using [Equation 18-7](#variance_schedule_equation), and call this function with
    *T* = 4,000:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个小的函数来计算 *α*[*t*]，*β*[*t*] 和 *α̅*[*t*]，使用 [方程 18-7](#variance_schedule_equation)，并将此函数调用为
    *T* = 4,000：
- en: '[PRE26]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To train our model to reverse the diffusion process, we will need noisy images
    from different time steps of the forward process. For this, let’s create a function
    that will take an image **x**[0] and a time step *t* using [Equation 18-6](#fast_forward_process_equation),
    and return a noisy image **x**[*t*]:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的模型以反转扩散过程，我们需要来自正向过程不同时间步的噪声图像。为此，让我们创建一个函数，它将使用 [方程 18-6](#fast_forward_process_equation)
    接收一个图像 **x**[0] 和一个时间步 *t*，并返回一个噪声图像 **x**[*t*]：
- en: '[PRE27]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The model will need both the noisy image **x**[*t*] and the time step *t*,
    so let’s create a small class that will hold both. We’ll give it a handy `to()`
    method to move both **x**[*t*] and *t* to the GPU:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 模型需要噪声图像 **x**[*t*] 和时间步 *t*，因此让我们创建一个小的类来保存这两个值。我们将给它一个方便的 `to()` 方法，将 **x**[*t*]
    和 *t* 移动到 GPU：
- en: '[PRE28]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, let’s create a dataset wrapper class. It takes an image dataset—Fashion
    MNIST in our case—and preprocesses the images so their pixel values range between
    –1 and +1 (this is optional but usually works better), and it uses the `forward_diffusion()`
    function to add noise to the image. Then it wraps the resulting noisy image as
    well as the time step in a `DiffusionSample` object, and returns it along with
    the target, which is the unscaled noise `eps`, before it was scaled by $StartRoot
    1 minus alpha overbar Subscript t Baseline EndRoot$ and added to the image:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个数据集包装器类。它接收一个图像数据集——在我们的例子中是 Fashion MNIST，并预处理图像，使其像素值介于 –1 和 +1
    之间（这是可选的，但通常效果更好），并使用 `forward_diffusion()` 函数向图像添加噪声。然后它将生成的噪声图像以及时间步封装在 `DiffusionSample`
    对象中，并返回它以及目标，即未缩放的噪声 `eps`，在它被 $StartRoot 1 减去 alpha bar Subscript t Baseline
    EndRoot$ 缩放并添加到图像之前：
- en: '[PRE29]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'You may be wondering why not predict the original image directly, rather than
    the unscaled noise? One reason is empirical: the authors tried both approaches,
    and they observed that predicting the noise rather than the image led to more
    stable training and better results. The other reason is that the noise is Gaussian,
    which allows for some mathematical simplifications: in particular, the KL divergence
    between two Gaussian distributions is proportional to the squared distance between
    their means, so we can use the MSE loss, which is simple, fast, and quite stable.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道为什么不直接预测原始图像，而不是未缩放的噪声？一个原因是经验性的：作者尝试了两种方法，并观察到预测噪声而不是图像导致了更稳定的训练和更好的结果。另一个原因是噪声是高斯分布，这允许进行一些数学简化：特别是，两个高斯分布之间的KL散度与它们均值之间的平方距离成正比，因此我们可以使用MSE损失，它简单、快速且相当稳定。
- en: 'Now we’re ready to build the actual diffusion model itself. It can be any model
    you want, as long as it takes a `DiffusionSample` as input and outputs images
    of the same shape as the input images. The DDPM authors used a modified [U-Net
    architecture](https://homl.info/unet),⁠^([18](ch18.html#id4152)) which has many
    similarities with the FCN architecture we discussed in [Chapter 12](ch12.html#cnn_chapter)
    for semantic segmentation. U-Net is a convolutional neural network that gradually
    downsamples the input images, then gradually upsamples them again, with skip connections
    crossing over from each level of the downsampling part to the corresponding level
    in the upsampling part. To take into account the time steps, they were encoded
    using a fixed sinusoidal encoding (i.e., the same technique as the positional
    encodings in the original Transformer architecture). At every level in the U-Net
    architecture, they passed these time encodings through `Linear` layers and fed
    them to the U-Net. Lastly, they also used multi-head attention layers at various
    levels. See this chapter’s notebook for a basic implementation (it’s too long
    to copy here, and the details don’t matter: many other model architectures would
    work just fine).'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好构建实际的扩散模型本身。它可以是你想要的任何模型，只要它接受一个 `DiffusionSample` 作为输入，并输出与输入图像相同形状的图像。DDPM的作者使用了一个修改过的
    [U-Net架构](https://homl.info/unet)，⁠^([18](ch18.html#id4152))，它与我们在[第12章](ch12.html#cnn_chapter)中讨论的用于语义分割的FCN架构有很多相似之处。U-Net是一个卷积神经网络，它逐渐下采样输入图像，然后再次逐渐上采样，通过跳过连接从下采样部分的每一级跨越到上采样部分的相应级。为了考虑时间步，他们使用固定的正弦编码（即与原始Transformer架构中的位置编码相同的技巧）进行编码。在U-Net架构的每一级，他们都通过`Linear`层传递这些时间编码，并将它们输入到U-Net中。最后，他们在各个级别也使用了多头注意力层。参见本章的笔记本以获取基本实现（这里太长了，无法复制，细节不重要：许多其他模型架构也可以很好地工作）。
- en: '[PRE30]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'For training, the authors noted that using the MAE loss worked better than
    the MSE. You can also use the Huber loss:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，作者指出使用MAE损失比MSE损失效果更好。你也可以使用Huber损失：
- en: '[PRE31]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Once the model is trained, you can use it to generate new images by sampling
    **x**[*T*] randomly from a Gaussian distribution with mean 0 and variance 1, then
    using [Equation 18-8](#reverse_diffusion_equation) to get **x**[*T*–1]. Then use
    this equation 3,999 more times until you get **x**[0]. If all went well, **x**[0]
    should look like a regular Fashion MNIST image!
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，你可以通过从均值为0，方差为1的高斯分布中随机采样 **x**[*T*] 来使用它生成新的图像，然后使用[方程18-8](#reverse_diffusion_equation)
    来得到 **x**[*T*–1]。然后使用这个方程3,999次，直到你得到 **x**[0]。如果一切顺利，**x**[0] 应该看起来像一张普通的Fashion
    MNIST图像！
- en: Equation 18-8\. Going one step in reverse in the DDPM diffusion process
  id: totrans-288
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程18-8。在DDPM扩散过程中的反向一步
- en: <mrow><msub><mi>𝐱</mi> <mrow><mi>t</mi> <mo>−</mo> <mn>1</mn></mrow></msub>
    <mo>=</mo> <mfrac><mn>1</mn> <msqrt><msub><mi>α</mi> <mi>t</mi></msub></msqrt></mfrac>
    <mrow><mo fence="true" form="prefix">(</mo> <msub><mi>𝐱</mi> <mi>t</mi></msub>
    <mo>−</mo> <mfrac><msub><mi>β</mi> <mi>t</mi></msub> <msqrt><mrow><mn>1</mn> <mo>−</mo>
    <msub><menclose notation="top"><mi>α</mi></menclose> <mi>t</mi></msub></mrow></msqrt></mfrac>
    <msub><mi mathvariant="bold">ε</mi> <mi mathvariant="bold">θ</mi></msub> <mo form="prefix"
    stretchy="false">(</mo> <msub><mi>𝐱</mi> <mi>t</mi></msub> <mo lspace="0%" rspace="0%"
    separator="true">,</mo> <mi>t</mi> <mo form="postfix" stretchy="false">)</mo>
    <mo fence="true" form="postfix">)</mo></mrow> <mo>+</mo> <msqrt><msub><mi>β</mi>
    <mi>t</mi></msub></msqrt> <mi>𝐳</mi></mrow>
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: <mrow><msub><mi>𝐱</mi> <mrow><mi>t</mi> <mo>−</mo> <mn>1</mn></mrow></msub>
    <mo>=</mo> <mfrac><mn>1</mn> <msqrt><msub><mi>α</mi> <mi>t</mi></msub></msqrt></mfrac>
    <mrow><mo fence="true" form="prefix">(</mo> <msub><mi>𝐱</mi> <mi>t</mi></msub>
    <mo>−</mo> <mfrac><msub><mi>β</mi> <mi>t</mi></msub> <msqrt><mrow><mn>1</mn> <mo>−</mo>
    <msub><menclose notation="top"><mi>α</mi></menclose> <mi>t</mi></msub></mrow></msqrt></mfrac>
    <msub><mi mathvariant="bold">ε</mi> <mi mathvariant="bold">θ</mi></msub> <mo form="prefix"
    stretchy="false">(</mo> <msub><mi>𝐱</mi> <mi>t</mi></msub> <mo lspace="0%" rspace="0%"
    separator="true">,</mo> <mi>t</mi> <mo form="postfix" stretchy="false">)</mo>
    <mo fence="true" form="postfix">)</mo></mrow> <mo>+</mo> <msqrt><msub><mi>β</mi>
    <mi>t</mi></msub></msqrt> <mi>𝐳</mi></mrow>
- en: 'In this equation, **ε[θ]**(**x**[*t*], *t*) represents the noise predicted
    by the model given the input image **x**[*t*] and the time step *t*. The **θ**
    represents the model parameters. Moreover, **z** is Gaussian noise with mean 0
    and variance 1\. This makes the reverse process stochastic: if you run it multiple
    times, you will get different images.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，**ε[θ]**(**x**[*t*], *t*) 表示模型根据输入图像 **x**[*t*] 和时间步 *t* 预测的噪声。**θ**
    表示模型参数。此外，**z** 是均值为0，方差为1的高斯噪声。这使得反向过程是随机的：如果你多次运行它，你会得到不同的图像。
- en: 'This works well, but it requires 4,000 iterations to generate an image! That’s
    too slow. Luckily, just a few months after the DDPM paper, researchers from Stanford
    University proposed a technique named the [denoising diffusion implicit model
    (DDIM)](https://homl.info/ddim)⁠^([19](ch18.html#id4157)) to generate images in
    much fewer steps: instead of going from *t* = 4,000 down to 0 one step at a time,
    DDIM can go down any number of time steps at a time, using [Equation 18-9](#ddim_equation).
    Moreover, the training process is exactly the same as for DDPM, so we can simply
    reuse our trained DDPM model.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这方法效果不错，但生成一张图片需要4,000次迭代！这太慢了。幸运的是，在DDPM论文发表后的几个月里，斯坦福大学的研究人员提出了一种名为[去噪扩散隐式模型（DDIM）](https://homl.info/ddim)的技术⁠^([19](ch18.html#id4157))，可以在更少的步骤中生成图片：DDIM可以一次下降任意多个时间步，而不是每次只下降一个时间步从*t*
    = 4,000下降到0，使用[方程18-9](#ddim_equation)。此外，训练过程与DDPM完全相同，因此我们可以简单地重用我们训练好的DDPM模型。
- en: Equation 18-9\. Going multiple steps in reverse with DDIM
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程18-9\. 使用DDIM进行多步反向操作
- en: <mtable displaystyle="true" style="width:100%;"><mtr><mtd style="padding-left:1em;padding-right:0em;"><mtable
    displaystyle="true" class="tml-jot"><mtr><mtd class="tml-right" style="padding-left:0em;padding-right:0em;"><mrow><msub><mi>𝐱</mi>
    <mi>p</mi></msub> <mo>=</mo> <msqrt><msub><menclose notation="top"><mi>α</mi></menclose>
    <mi>p</mi></msub></msqrt> <msub><mover><mi>𝐱</mi> <mo stretchy="false" class="wbk-acc"
    style="math-depth:0;">^</mo></mover> <mn>0</mn></msub> <mo>+</mo> <msqrt><mrow><mn>1</mn>
    <mo>−</mo> <msub><menclose notation="top"><mi>α</mi></menclose> <mi>p</mi></msub>
    <mo>−</mo> <msup><mi>σ</mi> <mn>2</mn></msup></mrow></msqrt> <mo>⋅</mo> <msub><mi
    mathvariant="bold">ε</mi> <mi mathvariant="bold">θ</mi></msub> <mo form="prefix"
    stretchy="false">(</mo> <msub><mi>𝐱</mi> <mi>t</mi></msub> <mo lspace="0%" rspace="0%"
    separator="true">,</mo> <mi>t</mi> <mo form="postfix" stretchy="false">)</mo>
    <mo>+</mo> <mi>σ</mi> <mi>𝐳</mi></mrow></mtd></mtr> <mtr><mtd style="padding-left:0em;padding-right:0em;"><mrow><mtext>where
      </mtext> <msub><mover><mi>𝐱</mi> <mo stretchy="false" class="wbk-acc" style="math-depth:0;">^</mo></mover>
    <mn>0</mn></msub> <mo>=</mo> <mfrac><mn>1</mn> <msqrt><msub><menclose notation="top"><mi>α</mi></menclose>
    <mi>t</mi></msub></msqrt></mfrac> <mrow><mo fence="true" form="prefix">(</mo>
    <msub><mi>𝐱</mi> <mi>t</mi></msub> <mo>−</mo> <msqrt><mrow><mn>1</mn> <mo>−</mo>
    <msub><menclose notation="top"><mi>α</mi></menclose> <mi>t</mi></msub></mrow></msqrt>
    <msub><mi mathvariant="bold">ε</mi> <mi mathvariant="bold">θ</mi></msub> <mo form="prefix"
    stretchy="false">(</mo> <msub><mi>𝐱</mi> <mi>t</mi></msub> <mo lspace="0%" rspace="0%"
    separator="true">,</mo> <mi>t</mi> <mo form="postfix" stretchy="false">)</mo>
    <mo fence="true" form="postfix">)</mo></mrow></mrow></mtd></mtr> <mtr><mtd style="padding-left:0em;padding-right:0em;"><mrow><mtext>and
      </mtext> <msup><mi>σ</mi> <mn>2</mn></msup> <mo>=</mo> <mi>η</mi> <mrow><mo
    fence="true" form="prefix">(</mo> <mfrac><mrow><mn>1</mn> <mo>−</mo> <msub><menclose
    notation="top"><mi>α</mi></menclose> <mi>p</mi></msub></mrow> <mrow><mn>1</mn>
    <mo>−</mo> <msub><menclose notation="top"><mi>α</mi></menclose> <mi>t</mi></msub></mrow></mfrac>
    <mo fence="true" form="postfix">)</mo></mrow> <msub><mi>β</mi> <mi>t</mi></msub></mrow></mtd></mtr></mtable></mtd></mtr></mtable>
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: <mtable displaystyle="true" style="width:100%;"><mtr><mtd style="padding-left:1em;padding-right:0em;"><mtable
    displaystyle="true" class="tml-jot"><mtr><mtd class="tml-right" style="padding-left:0em;padding-right:0em;"><mrow><msub><mi>𝐱</mi>
    <mi>p</mi></msub> <mo>=</mo> <msqrt><msub><menclose notation="top"><mi>α</mi></menclose>
    <mi>p</mi></msub></msqrt> <msub><mover><mi>𝐱</mi> <mo stretchy="false" class="wbk-acc"
    style="math-depth:0;">^</mo></mover> <mn>0</mn></msub> <mo>+</mo> <msqrt><mrow><mn>1</mn>
    <mo>−</mo> <msub><menclose notation="top"><mi>α</mi></menclose> <mi>p</mi></msub>
    <mo>−</mo> <msup><mi>σ</mi> <mn>2</mn></msup></mrow></msqrt> <mo>⋅</mo> <msub><mi
    mathvariant="bold">ε</mi> <mi mathvariant="bold">θ</mi></msub> <mo form="prefix"
    stretchy="false">(</mo> <msub><mi>𝐱</mi> <mi>t</mi></msub> <mo lspace="0%" rspace="0%"
    separator="true">,</mo> <mi>t</mi> <mo form="postfix" stretchy="false">)</mo>
    <mo>+</mo> <mi>σ</mi> <mi>𝐳</mi></mrow></mtd></mtr> <mtr><mtd style="padding-left:0em;padding-right:0em;"><mrow><mtext>其中</mtext>
    <msub><mover><mi>𝐱</mi> <mo stretchy="false" class="wbk-acc" style="math-depth:0;">^</mo></mover>
    <mn>0</mn></msub> <mo>=</mo> <mfrac><mn>1</mn> <msqrt><msub><menclose notation="top"><mi>α</mi></menclose>
    <mi>t</mi></msub></msqrt></mfrac> <mrow><mo fence="true" form="prefix">(</mo>
    <msub><mi>𝐱</mi> <mi>t</mi></msub> <mo>−</mo> <msqrt><mrow><mn>1</mn> <mo>−</mo>
    <msub><menclose notation="top"><mi>α</mi></menclose> <mi>t</mi></msub></mrow></msqrt>
    <msub><mi mathvariant="bold">ε</mi> <mi mathvariant="bold">θ</mi></msub> <mo form="prefix"
    stretchy="false">(</mo> <msub><mi>𝐱</mi> <mi>t</mi></msub> <mo lspace="0%" rspace="0%"
    separator="true">,</mo> <mi>t</mi> <mo form="postfix" stretchy="false">)</mo>
    <mo fence="true" form="postfix">)</mo></mrow></mrow></mtd></mtr> <mtr><mtd style="padding-left:0em;padding-right:0em;"><mrow><mtext>并且</mtext>
    <msup><mi>σ</mi> <mn>2</mn></msup> <mo>=</mo> <mi>η</mi> <mrow><mo fence="true"
    form="prefix">(</mo> <mfrac><mrow><mn>1</mn> <mo>−</mo> <msub><menclose notation="top"><mi>α</mi></menclose>
    <mi>p</mi></msub></mrow> <mrow><mn>1</mn> <mo>−</mo> <msub><menclose notation="top"><mi>α</mi></menclose>
    <mi>t</mi></msub></mrow></mfrac> <mo fence="true" form="postfix">)</mo></mrow>
    <msub><mi>β</mi> <mi>t</mi></msub></mrow></mtd></mtr></mtable></mtd></mtr></mtable>
- en: 'In this equation:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '**ε[θ]**(**x**[*t*], *t*), **θ**, and **z** have the same meanings as in [Equation
    18-8](#reverse_diffusion_equation).'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ε[θ]**(**x**[*t*], *t*), **θ**, 和 **z** 与 [方程 18-8](#reverse_diffusion_equation)
    中的含义相同。'
- en: '*p* represents any time step before *t*. For example, it could be *p* = *t*
    – 50.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p* 代表任何在 *t* 之前的时间步。例如，它可以是 *p* = *t* – 50。'
- en: '*η* is a hyperparameter that controls how much randomness should be used during
    generation, from 0 (no randomness, fully deterministic) to 1 (just like DDPM).'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*η* 是一个超参数，用于控制生成过程中应使用多少随机性，从 0（无随机性，完全确定）到 1（类似于 DDPM）。'
- en: 'Let’s write a function that implements this reverse process, and call it to
    generate a few images:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个实现这个逆过程的函数，并将其称为生成几个图像：
- en: '[PRE32]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This time the generation will only take a few seconds, and it will produce images
    such as the ones shown in [Figure 18-20](#ddim_generated_images_plot). Granted,
    they’re not very impressive, but we’ve only trained the model for a few minutes
    on Fashion MNIST. Give it a try on a larger dataset and train it for a few hours
    to get more impressive results.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这次生成将只需几秒钟，并将产生如图[图18-20](#ddim_generated_images_plot)所示的图像。诚然，它们并不非常令人印象深刻，但我们只在该模型上训练了Fashion
    MNIST几分钟。尝试在更大的数据集上训练它几个小时以获得更令人印象深刻的结果。
- en: '![A grid of low-resolution, black-and-white images of clothing items, generated
    using a diffusion model with limited training on the Fashion MNIST dataset.](assets/hmls_1820.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![使用在Fashion MNIST数据集上有限训练的扩散模型生成的低分辨率、黑白服装物品网格](assets/hmls_1820.png)'
- en: Figure 18-20\. Images generated by DDIM accelerated diffusion
  id: totrans-302
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-20。DDIM加速扩散生成的图像
- en: Diffusion models have made tremendous progress since 2020\. In particular, a
    paper published in December 2021 by [Robin Rombach, et al.](https://homl.info/latentdiff)⁠^([20](ch18.html#id4163))
    introduced *latent diffusion models*, where the diffusion process takes place
    in latent space, rather than in pixel space. To achieve this, a powerful autoencoder
    is used to compress each training image into a much smaller latent space, where
    the diffusion process takes place, then the autoencoder is used to decompress
    the final latent representation, generating the output image. This considerably
    speeds up image generation, and reduces training time and cost dramatically. Importantly,
    the quality of the generated images is outstanding.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 自2020年以来，扩散模型取得了巨大的进步。特别是，2021年12月由[Robin Rombach等人](https://homl.info/latentdiff)⁠^([20](ch18.html#id4163))发表的一篇论文介绍了*潜在扩散模型*，其中扩散过程发生在潜在空间，而不是像素空间。为了实现这一点，使用了一个强大的自动编码器将每个训练图像压缩到一个更小的潜在空间，其中扩散过程发生，然后使用自动编码器来解压缩最终的潜在表示，生成输出图像。这大大加快了图像生成速度，并显著减少了训练时间和成本。重要的是，生成的图像质量非常出色。
- en: Moreover, the researchers also adapted various conditioning techniques to guide
    the diffusion process using text prompts, images, or any other inputs. This makes
    it possible to quickly produce any image you might fancy. You can also condition
    the image generation process using an input image. This enables many applications,
    such as outpainting—where an input image is extended beyond its borders—or inpainting—where
    holes in an image are filled in.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，研究人员还采用了各种条件化技术，通过文本提示、图像或其他任何输入来引导扩散过程。这使得快速生成任何你想要的图像成为可能。你也可以使用输入图像来条件化图像生成过程。这使许多应用成为可能，例如扩展画布——将输入图像扩展到其边界之外，或者修复画布——在图像中填充洞。
- en: 'Lastly, a powerful pretrained latent diffusion model named *Stable Diffusion*
    (SD) was open sourced in August 2022 by a collaboration between LMU Munich and
    a few companies, including StabilityAI, and Runway, with support from EleutherAI
    and LAION. Now anyone can generate mindblowing images in seconds, for free, even
    on a regular laptop. For example, you can use the Hugging Face Diffusers library
    to load SD (e.g., the turbo variant), create an image generation pipeline for
    text-to-image, and generate an image of an orangutan reading a book:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一个名为*Stable Diffusion*（SD）的强大预训练潜在扩散模型在2022年8月由慕尼黑大学与包括StabilityAI和Runway在内的几家公司的合作开源，得到了EleutherAI和LAION的支持。现在任何人都可以在几秒钟内免费生成令人惊叹的图像，甚至在普通的笔记本电脑上也可以。例如，你可以使用Hugging
    Face Diffusers库来加载SD（例如，turbo变体），创建一个文本到图像的图像生成管道，并生成一只猩猩读书的图像：
- en: '[PRE33]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![A digitally generated image of an orangutan closely reading an open book,
    illustrating the creative capabilities of the Stable Diffusion model.](assets/hmls_1821.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![一个数字生成的猩猩仔细阅读一本打开的书的图像，展示了Stable Diffusion模型的创造性能力](assets/hmls_1821.png)'
- en: Figure 18-21\. A picture generated by Stable Diffusion using the Diffusers library
  id: totrans-308
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图18-21。使用Diffusers库生成的Stable Diffusion图像
- en: The possibilities are endless!
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 可能性是无限的！
- en: 'In the next chapter we will move on to an entirely different branch of deep
    learning: deep reinforcement learning.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转向深度学习的另一个完全不同的分支：深度强化学习。
- en: Exercises
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: What are the main tasks that autoencoders are used for?
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动编码器主要用于哪些主要任务？
- en: Suppose you want to train a classifier, and you have plenty of unlabeled training
    data but only a few thousand labeled instances. How can autoencoders help? How
    would you proceed?
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你想训练一个分类器，你有大量的未标记训练数据，但只有几千个标记实例。自动编码器如何帮助？你会如何进行？
- en: If an autoencoder perfectly reconstructs the inputs, is it necessarily a good
    autoencoder? How can you evaluate the performance of an autoencoder?
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个自动编码器完美地重建了输入，它是否必然是一个好的自动编码器？你如何评估自动编码器的性能？
- en: What are undercomplete and overcomplete autoencoders? What is the main risk
    of an excessively undercomplete autoencoder? What about the main risk of an overcomplete
    autoencoder?
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是欠完备和过完备的自动编码器？一个过度欠完备的自动编码器的主要风险是什么？过完备的自动编码器的主要风险又是什么？
- en: How do you tie weights in a stacked autoencoder? What is the point of doing
    so?
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何在堆叠自动编码器中绑定权重？这样做有什么意义？
- en: What is a generative model? Can you name a type of generative autoencoder?
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是生成模型？你能命名一种生成型自动编码器吗？
- en: What is a GAN? Can you name a few tasks where GANs can shine?
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是GAN？你能列举一些GAN可以大放异彩的任务吗？
- en: What are the main difficulties when training GANs?
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练GANs时主要有哪些困难？
- en: What are diffusion models good at? What is their main limitation?
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扩散模型擅长什么？它们的主要局限性是什么？
- en: 'Try using a denoising autoencoder to pretrain an image classifier. You can
    use MNIST (the simplest option), or a more complex image dataset such as [CIFAR10](https://homl.info/122)
    if you want a bigger challenge. Regardless of the dataset you’re using, follow
    these steps:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用去噪自动编码器来预训练一个图像分类器。你可以使用MNIST（最简单的选项），或者一个更复杂的图像数据集，如[CIFAR10](https://homl.info/122)，如果你想接受更大的挑战。无论你使用什么数据集，请遵循以下步骤：
- en: Split the dataset into a training set and a test set. Train a deep denoising
    autoencoder on the full training set.
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集分为训练集和测试集。在完整的训练集上训练一个深度去噪自动编码器。
- en: Check that the images are fairly well reconstructed. Visualize the images that
    most activate each neuron in the coding layer.
  id: totrans-323
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查图像是否得到了相当好的重建。可视化激活编码层中每个神经元的图像。
- en: Build a classification DNN, reusing the lower layers of the autoencoder. Train
    it using only 500 images from the training set. Does it perform better with or
    without pretraining?
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个分类深度神经网络，重用自动编码器的底层。仅使用训练集中的500张图像进行训练。是否预训练后表现更好？
- en: Train a variational autoencoder on the image dataset of your choice, and use
    it to generate images. Alternatively, you can try to find an unlabeled dataset
    that you are interested in and see if you can generate new samples.
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你选择的图像数据集上训练一个变分自动编码器，并使用它来生成图像。或者，你可以尝试找到一个你感兴趣的未标记数据集，看看你是否可以生成新的样本。
- en: Train a DCGAN to tackle the image dataset of your choice, and use it to generate
    images. Add experience replay and see if this helps.
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练一个DCGAN来处理你选择的图像数据集，并使用它来生成图像。添加经验回放并看看这是否有帮助。
- en: 'Train a diffusion model on your preferred image dataset (e.g., `torchvision.datasets.Flowers102`),
    and generate nice images. Next, add the image class as an extra input to the model,
    and retrain it: you should now be able to control the class of the generated image.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你喜欢的图像数据集上训练一个扩散模型（例如，`torchvision.datasets.Flowers102`），并生成漂亮的图像。接下来，将图像类别作为额外的输入添加到模型中，并重新训练它：你现在应该能够控制生成图像的类别。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab-p*](https://homl.info/colab-p).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可以在本章笔记本的末尾找到，在[*https://homl.info/colab-p*](https://homl.info/colab-p)。
- en: '^([1](ch18.html#id4015-marker)) William G. Chase and Herbert A. Simon, “Perception
    in Chess”, *Cognitive Psychology* 4, no. 1 (1973): 55–81.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch18.html#id4015-marker)) 威廉·G·蔡斯和赫伯特·A·西蒙，《国际象棋中的感知》，*认知心理学* 4，第1期（1973年）：55–81。
- en: '^([2](ch18.html#id4035-marker)) Hint: one approach is to create a custom `AutoencoderDataset`
    class that wraps a given dataset and replaces the targets with the inputs.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch18.html#id4035-marker)) 提示：一种方法是为给定的数据集创建一个自定义的`AutoencoderDataset`类，该类包装给定的数据集并将目标替换为输入。
- en: '^([3](ch18.html#id4062-marker)) Yoshua Bengio et al., “Greedy Layer-Wise Training
    of Deep Networks”, *Proceedings of the 19th International Conference on Neural
    Information Processing Systems* (2006): 153–160.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch18.html#id4062-marker)) 约书亚·本吉奥等人，《深度网络贪婪层逐层训练》，*第19届国际神经网络信息处理系统会议论文集*（2006年）：153–160。
- en: '^([4](ch18.html#id4066-marker)) Jonathan Masci et al., “Stacked Convolutional
    Auto-Encoders for Hierarchical Feature Extraction”, *Proceedings of the 21st International
    Conference on Artificial Neural Networks* 1 (2011): 52–59.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch18.html#id4066-marker)) 乔纳森·马斯奇等人，《用于分层特征提取的堆叠卷积自动编码器》，*第21届国际人工神经网络会议论文集*
    1（2011年）：52–59。
- en: '^([5](ch18.html#id4073-marker)) Pascal Vincent et al., “Extracting and Composing
    Robust Features with Denoising Autoencoders”, *Proceedings of the 25th International
    Conference on Machine Learning* (2008): 1096–1103.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '^([5](ch18.html#id4073-marker)) Pascal Vincent 等人, “使用去噪自编码器提取和组合鲁棒特征”, *第
    25 届国际机器学习会议论文集* (2008): 1096–1103.'
- en: '^([6](ch18.html#id4074-marker)) Pascal Vincent et al., “Stacked Denoising Autoencoders:
    Learning Useful Representations in a Deep Network with a Local Denoising Criterion”,
    *Journal of Machine Learning Research* 11 (2010): 3371–3408.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '^([6](ch18.html#id4074-marker)) Pascal Vincent 等人, “堆叠去噪自编码器：使用局部去噪标准在深度网络中学习有用的表示”,
    *机器学习研究杂志* 11 (2010): 3371–3408.'
- en: ^([7](ch18.html#id4088-marker)) Diederik Kingma and Max Welling, “Auto-Encoding
    Variational Bayes”, arXiv preprint arXiv:1312.6114 (2013).
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch18.html#id4088-marker)) Diederik Kingma 和 Max Welling, “Auto-Encoding
    Variational Bayes”, arXiv 预印本 arXiv:1312.6114 (2013).
- en: ^([8](ch18.html#id4091-marker)) Both these properties make VAEs rather similar
    to RBMs, but they are easier to train, and the sampling process is much faster
    (with RBMs you need to wait for the network to stabilize into a “thermal equilibrium”
    before you can sample a new instance).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch18.html#id4091-marker)) 这两个特性使得 VAEs 与 RBMs 非常相似，但它们更容易训练，采样过程也更快（使用
    RBMs，你需要等待网络稳定到“热平衡”状态后才能采样新的实例）。
- en: '^([9](ch18.html#id4106-marker)) Chris J. Maddison et al., “The Concrete Distribution:
    A Continuous Relaxation of Discrete Random Variables”, arXiv preprint arXiv:1611.00712
    (2016).'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch18.html#id4106-marker)) Chris J. Maddison 等人, “连续分布：离散随机变量的连续松弛”, arXiv
    预印本 arXiv:1611.00712 (2016).
- en: ^([10](ch18.html#id4107-marker)) Eric Jang et al., “Categorical Reparameterization
    with Gumbel-Softmax”, arXiv preprint arXiv:1611.01144 (2016).
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch18.html#id4107-marker)) Eric Jang 等人, “使用 Gumbel-Softmax 进行分类重参数化”,
    arXiv 预印本 arXiv:1611.01144 (2016).
- en: ^([11](ch18.html#id4111-marker)) Aaron van den Oord et al., “Neural Discrete
    Representation Learning”, arXiv preprint arXiv:1711.00937 (2017).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch18.html#id4111-marker)) Aaron van den Oord 等人, “神经离散表示学习”, arXiv 预印本
    arXiv:1711.00937 (2017).
- en: '^([12](ch18.html#id4125-marker)) Ian Goodfellow et al., “Generative Adversarial
    Nets”, *Proceedings of the 27th International Conference on Neural Information
    Processing Systems* 2 (2014): 2672–2680.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '^([12](ch18.html#id4125-marker)) Ian Goodfellow 等人, “生成对抗网络”, *第 27 届国际神经网络信息处理系统会议论文集*
    2 (2014): 2672–2680.'
- en: ^([13](ch18.html#id4132-marker)) For a nice comparison of the main GAN losses,
    check out this great [GitHub project by Hwalsuk Lee](https://homl.info/ganloss).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch18.html#id4132-marker)) 想要比较主要的 GAN 损失，可以查看 Hwalsuk Lee 的这个优秀的 [GitHub
    项目](https://homl.info/ganloss).
- en: '^([14](ch18.html#id4133-marker)) Mario Lucic et al., “Are GANs Created Equal?
    A Large-Scale Study”, *Proceedings of the 32nd International Conference on Neural
    Information Processing Systems* (2018): 698–707.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '^([14](ch18.html#id4133-marker)) Mario Lucic 等人, “GANs 是否平等？一项大规模研究”, *第 32
    届国际神经网络信息处理系统会议论文集* (2018): 698–707.'
- en: ^([15](ch18.html#id4147-marker)) Jascha Sohl-Dickstein et al., “Deep Unsupervised
    Learning using Nonequilibrium Thermodynamics”, arXiv preprint arXiv:1503.03585
    (2015).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch18.html#id4147-marker)) Jascha Sohl-Dickstein 等人, “利用非平衡热力学进行深度无监督学习”,
    arXiv 预印本 arXiv:1503.03585 (2015).
- en: ^([16](ch18.html#id4148-marker)) Jonathan Ho et al., “Denoising Diffusion Probabilistic
    Models” (2020).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch18.html#id4148-marker)) Jonathan Ho 等人, “去噪扩散概率模型” (2020).
- en: ^([17](ch18.html#id4149-marker)) Alex Nichol and Prafulla Dhariwal, “Improved
    Denoising Diffusion Probabilistic Models” (2021).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch18.html#id4149-marker)) Alex Nichol 和 Prafulla Dhariwal, “改进的去噪扩散概率模型”
    (2021).
- en: '^([18](ch18.html#id4152-marker)) Olaf Ronneberger et al., “U-Net: Convolutional
    Networks for Biomedical Image Segmentation”, arXiv preprint arXiv:1505.04597 (2015).'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch18.html#id4152-marker)) Olaf Ronneberger 等人, “U-Net：用于生物医学图像分割的卷积网络”,
    arXiv 预印本 arXiv:1505.04597 (2015).
- en: ^([19](ch18.html#id4157-marker)) Jiaming Song et al., “Denoising Diffusion Implicit
    Models”, arXiv preprint arXiv:2010.02502 (2020).
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch18.html#id4157-marker)) Jiaming Song 等人, “去噪扩散隐式模型”, arXiv 预印本 arXiv:2010.02502
    (2020).
- en: ^([20](ch18.html#id4163-marker)) Robin Rombach, Andreas Blattmann, et al., “High-Resolution
    Image Synthesis with Latent Diffusion Models”, arXiv preprint arXiv:2112.10752
    (2021).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch18.html#id4163-marker)) Robin Rombach, Andreas Blattmann 等人, “使用潜在扩散模型进行高分辨率图像合成”,
    arXiv 预印本 arXiv:2112.10752 (2021).
