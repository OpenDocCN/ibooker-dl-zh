- en: Chapter 7\. Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章。卷积神经网络
- en: Neurons in Human Vision
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人类视觉中的神经元
- en: The human sense of vision is unbelievably advanced. Within fractions of seconds,
    we can identify objects within our field of view, without thought or hesitation.
    Not only can we name objects we are looking at, we can also perceive their depth,
    perfectly distinguish their contours, and separate the objects from their backgrounds.
    Somehow our eyes take in raw voxels of color data, but our brain transforms that
    information into more meaningful primitives—lines, curves, and shapes—that might
    indicate, for example, that we’re looking at a house cat.^([1](ch07.xhtml#idm45934168179504))
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 人类的视觉感知能力是令人难以置信的先进。在几秒钟内，我们可以在视野范围内识别物体，毫不犹豫地，毫不费力。我们不仅可以命名我们看到的物体，还可以感知它们的深度，完美地区分它们的轮廓，并将物体与背景分开。不知何故，我们的眼睛接收到了原始的彩色数据体素，但我们的大脑将这些信息转化为更有意义的基元——线条、曲线和形状——这些基元可能表明，例如，我们正在看一只家猫。^([1](ch07.xhtml#idm45934168179504))
- en: Foundational to the human sense of vision is the neuron. Specialized neurons
    are responsible for capturing light information in the human eye.^([2](ch07.xhtml#idm45934168177536))
    This light information is then preprocessed, transported to the visual cortex
    of the brain, and then finally analyzed to completion. Neurons are single-handedly
    responsible for all of these functions. As a result, intuitively, it would make
    a lot of sense to extend our neural network models to build better computer vision
    systems. In this chapter, we will use our understanding of human vision to build
    effective deep learning models for image problems. But before we jump in, let’s
    take a look at more traditional approaches to image analysis and why they fall
    short.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 人类视觉的基础是神经元。专门的神经元负责在人眼中捕捉光信息。^([2](ch07.xhtml#idm45934168177536))然后，这些光信息经过预处理，传输到大脑的视觉皮层，最终被完全分析。神经元单独负责所有这些功能。因此，直觉上，将我们的神经网络模型扩展到构建更好的计算机视觉系统是有很多道理的。在本章中，我们将利用对人类视觉的理解来构建有效的深度学习模型，解决图像问题。但在我们深入研究之前，让我们看看更传统的图像分析方法以及它们为什么存在缺陷。
- en: The Shortcomings of Feature Selection
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择的缺陷
- en: Let’s begin by considering a simple computer vision problem. I give you a randomly
    selected image, such as the one in [Figure 7-1](#hypothetical_face_recognition_algorithm).
    Your task is to tell me if there is a human face in this picture. This is exactly
    the problem that Paul Viola and Michael Jones tackled in their seminal paper published
    in 2001.^([3](ch07.xhtml#idm45934168169648))
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从考虑一个简单的计算机视觉问题开始。我给你一张随机选择的图像，比如[图7-1](#hypothetical_face_recognition_algorithm)中的图像。你的任务是告诉我这张图片中是否有一个人脸。这正是保罗·维奥拉和迈克尔·琼斯在他们2001年发表的开创性论文中解决的问题。^([3](ch07.xhtml#idm45934168169648))
- en: '![](Images/fdl2_0701.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0701.png)'
- en: Figure 7-1\. A hypothetical face-recognition algorithm should detect a face
    in this photograph of former US President Barack Obama
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-1。一个假设的人脸识别算法应该能够在这张前美国总统巴拉克·奥巴马的照片中检测到一个人脸
- en: For a human like you or me, this task is completely trivial. For a computer,
    however, this is a difficult problem. How do we teach a computer that an image
    contains a face? We could try to train a traditional machine learning algorithm
    (like the one we described in [Chapter 3](ch03.xhtml#the_neural_network)) by giving
    it the raw pixel values of the image and hoping it can find an appropriate classifier.
    Turns out this doesn’t work well at all because the signal-to-noise ratio is much
    too low for any useful learning to occur. We need an alternative.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像你我这样的人类来说，这个任务完全是微不足道的。然而，对于计算机来说，这是一个困难的问题。我们如何教会计算机一张图像包含一个人脸？我们可以尝试训练一个传统的机器学习算法（就像我们在[第三章](ch03.xhtml#the_neural_network)中描述的那样），给它图像的原始像素值，并希望它能找到一个合适的分类器。结果表明，这根本行不通，因为信噪比太低，无法进行任何有用的学习。我们需要另一种方法。
- en: The compromise that was eventually reached was essentially a trade-off between
    the traditional computer program, where the human defined all of the logic, and
    a pure machine learning approach, where the computer did all of the heavy lifting.
    In this compromise, a human would choose the features (perhaps hundreds or thousands)
    that they believed were important in making a classification decision. In doing
    so, the human would be producing a lower-dimensional representation of the same
    learning problem. The machine learning algorithm would then use these new *feature
    vectors* to make classification decisions. Because the *feature extraction* process
    improves the signal-to-noise ratio (assuming the appropriate features are picked),
    this approach had quite a bit of success compared to the state-of-the-art at the
    time.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 最终达成的妥协基本上是传统计算机程序和纯机器学习方法之间的一种权衡，其中人类定义了所有逻辑，而计算机则承担了所有繁重的工作。在这种妥协中，人类将选择他们认为在做出分类决策时重要的特征（也许是数百或数千个）。这样做，人类将为同一学习问题生成一个较低维度的表示。然后，机器学习算法将使用这些新的*特征向量*来做出分类决策。由于*特征提取*过程提高了信噪比（假设选择了适当的特征），这种方法与当时的最新技术相比取得了相当大的成功。
- en: Viola and Jones had the insight that faces had certain patterns of light and
    dark patches that they could exploit. For example, there is a difference in light
    intensity between the eye region and the upper cheeks. There is also a difference
    in light intensity between the nose bridge and the two eyes on either side. These
    detectors are shown in [Figure 7-2](#illustration_of_viola_jones_intensity_detectors).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Viola和Jones有一个洞察，即人脸具有一定的光亮和暗斑块模式，他们可以利用这一点。例如，眼部区域和上颊之间的光强度有所不同。鼻梁和两侧眼睛之间的光强度也有所不同。这些检测器显示在[图7-2](#illustration_of_viola_jones_intensity_detectors)中。
- en: '![](Images/fdl2_0702.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0702.png)'
- en: Figure 7-2\. Viola-Jones intensity detectors
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-2。Viola-Jones强度检测器
- en: By themselves, each of these features is not very effective at identifying a
    face. But when used together (through a classic machine learning algorithm known
    as boosting, described in the [original manuscript](https://oreil.ly/UAgvR)),
    their combined effectiveness drastically increases. On a dataset of 130 images
    and 507 faces, the algorithm achieves a 91.4% detection rate with 50 false positives.
    The performance was unparalleled at the time, but there are fundamental limitations
    of the algorithm. If a face is partially covered with shade, the light intensity
    comparisons no longer work. Moreover, if the algorithm is looking at a face on
    a crumpled flier or the face of a cartoon character, it would most likely fail.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特征本身并不是很有效地识别面孔。但是当它们一起使用时（通过一个被称为增强的经典机器学习算法，描述在原始手稿中），它们的联合效果大大增加。在一个包含130张图像和507张面孔的数据集上，该算法实现了91.4%的检测率，有50个误报。当时的性能是无与伦比的，但该算法存在根本性的局限性。如果一个面孔部分被阴影遮盖，光强度比较将不再起作用。此外，如果算法看到的是一个被折叠的传单上的面孔或者卡通人物的面孔，它很可能会失败。
- en: The problem is the algorithm hasn’t really learned that much about what it means
    to “see” a face. Beyond differences in light intensity, our brain uses a vast
    number of visual cues to realize that our field of view contains a human face,
    including contours, relative positioning of facial features, and color. And even
    if there are slight discrepancies in one of our visual cues (for example, if parts
    of the face are blocked from view or if shade modifies light intensities), our
    visual cortex can still reliably identify faces.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 问题在于该算法并没有真正学到“看见”面孔的含义。除了光强度的差异之外，我们的大脑使用大量的视觉线索来认识到我们的视野中包含一个人脸，包括轮廓、面部特征的相对位置和颜色。即使我们的视觉线索中有轻微的差异（例如，如果面部的某些部分被遮挡或者阴影改变了光强度），我们的视觉皮层仍然可以可靠地识别面孔。
- en: To use traditional machine learning techniques to teach a computer to “see,”
    we need to provide our program with a lot more features to make accurate decisions.
    Before the advent of deep learning, huge teams of computer vision researchers
    would take years to debate about the usefulness of different features. As the
    recognition problems became more and more intricate, researchers had a difficult
    time coping with the increase in complexity.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用传统的机器学习技术教会计算机“看见”，我们需要提供程序更多的特征以做出准确的决策。在深度学习出现之前，庞大的计算机视觉研究团队需要花费数年时间讨论不同特征的有用性。随着识别问题变得越来越复杂，研究人员很难应对复杂性的增加。
- en: To illustrate the power of deep learning, consider the ImageNet challenge, one
    of the most prestigious benchmarks in computer vision (sometimes even referred
    to as the Olympics of computer vision).^([4](ch07.xhtml#idm45934168149488)) Every
    year, researchers attempt to classify images into one of 200 possible classes
    given a training dataset of approximately 450,000 images. The algorithm is given
    five guesses to get the right answer before it moves onto the next image in the
    test dataset. The goal of the competition is to push the state-of-the-art in computer
    vision to rival the accuracy of human vision itself (approximately 95% to 96%).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 展示深度学习的力量，考虑一下ImageNet挑战，这是计算机视觉中最负盛名的基准之一（有时甚至被称为计算机视觉的奥林匹克）。每年，研究人员尝试将图像分类为大约450,000张图像的训练数据集中的200个可能类别之一。算法有五次猜测正确答案的机会，然后才会继续测试数据集中的下一张图像。比赛的目标是推动计算机视觉的最新技术达到与人类视觉本身相媲美的准确度（大约95%至96%）。
- en: In 2011, the winner of the ImageNet benchmark had an error rate of 25.7%, making
    a mistake on one out of every four images.^([5](ch07.xhtml#idm45934168146976))
    Definitely a huge improvement over random guessing, but not good enough for any
    sort of commercial application. Then in 2012, Alex Krizhevsky from Geoffrey Hinton’s
    lab at the University of Toronto did the unthinkable. Pioneering a deep learning
    architecture known as a *convolutional neural network* for the first time on a
    challenge of this size and complexity, he blew the competition out of the water.
    The runner-up in the competition scored a commendable 26.1% error rate. But AlexNet,
    over the course of just a few months of work, completely crushed 50 years of traditional
    computer vision research with an error rate of approximately 16%.^([6](ch07.xhtml#idm45934168141808))
    It would be no understatement to say that AlexNet single-handedly put deep learning
    on the map for computer vision and completely revolutionized the field.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 2011年，ImageNet基准的获胜者错误率为25.7%，在四张图像中犯了一个错误。绝对是比随机猜测有了巨大的改进，但对于任何商业应用来说还不够好。然后在2012年，来自多伦多大学Geoffrey
    Hinton实验室的Alex Krizhevsky做了一件令人难以置信的事情。他首次在这样规模和复杂性的挑战中开创了一种被称为卷积神经网络的深度学习架构，他击败了竞争对手。比赛中的亚军错误率为26.1%。但是AlexNet在短短几个月的工作中，以大约16%的错误率完全击败了50年传统计算机视觉研究。毫不夸张地说，AlexNet单枪匹马地将深度学习推向了计算机视觉的前沿，并彻底改变了这一领域。
- en: Vanilla Deep Neural Networks Don’t Scale
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 香草深度神经网络不具备可扩展性
- en: The fundamental goal in applying deep learning to computer vision is to remove
    the cumbersome, and ultimately limiting, feature selection process. As we discussed
    in [Chapter 3](ch03.xhtml#the_neural_network), deep neural networks are perfect
    for this process because each layer of a neural network is responsible for learning
    and building up features to represent the input data that it receives. A naive
    approach might be for us to use a vanilla deep neural network using the network
    layer primitive we designed in [Chapter 5](ch05.xhtml#neural_networks_in_pytorch)
    for the MNIST dataset to achieve the image classification task.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 将深度学习应用于计算机视觉的基本目标是消除繁琐且最终限制性的特征选择过程。正如我们在[第3章](ch03.xhtml#the_neural_network)中讨论的，深度神经网络非常适合这个过程，因为神经网络的每一层都负责学习和构建特征，以表示其接收到的输入数据。一个天真的方法可能是使用我们在[第5章](ch05.xhtml#neural_networks_in_pytorch)中为MNIST数据集设计的网络层原语，来实现图像分类任务。
- en: If we attempt to tackle the image classification problem in this way, however,
    we’ll quickly face a pretty daunting challenge, visually demonstrated in [Figure 7-3](#density_of_connections_between_layers).
    In MNIST, our images were only 28 × 28 pixels and were black and white. As a result,
    a neuron in a fully connected hidden layer would have 784 incoming weights. This
    seems pretty tractable for the MNIST task, and our vanilla neural net performed
    quite well. This technique, however, does not scale well as our images grow larger.
    For example, for a full-color 200 × 200 pixel image, our input layer would have
    200 × 200 × 3 = 120,000 weights. And we’re going to want to have lots of these
    neurons over multiple layers, so these parameters add up quite quickly. Clearly,
    this full connectivity is not only wasteful, but also means that we’re much more
    likely to overfit to the training dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们尝试以这种方式解决图像分类问题，我们将很快面临一个相当艰巨的挑战，如[图7-3](#density_of_connections_between_layers)中形象地展示的。在MNIST中，我们的图像只有28×28像素，是黑白的。因此，完全连接的隐藏层中的一个神经元将有784个输入权重。对于MNIST任务来说，这似乎是相当可控的，我们的普通神经网络表现得相当不错。然而，随着图像变得更大，这种技术并不适用。例如，对于一个全彩色的200×200像素图像，我们的输入层将有200×200×3=120,000个权重。而且我们希望在多个层中有很多这样的神经元，因此这些参数会非常迅速地累积。显然，这种全连接不仅是浪费的，而且意味着我们更有可能过度拟合训练数据集。
- en: '![](Images/fdl2_0703.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0703.png)'
- en: Figure 7-3\. The density of connections between layers increases intractably
    as the size of the image increases
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-3。随着图像尺寸的增加，层之间的连接密度呈不可控增加
- en: The convolutional network takes advantage of the fact that we’re analyzing images,
    and sensibly constrains the architecture of the deep network so that we drastically
    reduce the number of parameters in our model. Inspired by how human vision works,
    layers of a convolutional network have neurons arranged in three dimensions, so
    layers have a width, height, and depth, as shown in [Figure 7-4](#convolutional_layers_arrange_neurons).^([7](ch07.xhtml#idm45934168123728))
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积网络利用我们分析图像的事实，并合理地限制深度网络的架构，从而大大减少模型中的参数数量。受到人类视觉工作方式的启发，卷积网络的层中的神经元以三维方式排列，因此层具有宽度、高度和深度，如[图7-4](#convolutional_layers_arrange_neurons)所示。^([7](ch07.xhtml#idm45934168123728))
- en: 'As we’ll see, the neurons in a convolutional layer are connected to only a
    small, local region of the preceding layer, so we avoid the wastefulness of fully
    connected neurons. A convolutional layer’s function can be expressed simply: it
    processes a three-dimensional volume of information to produce a new three-dimensional
    volume of information. We’ll take a closer look at how this works in the next
    section.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将看到的，卷积层中的神经元仅连接到前一层的一个小区域，因此我们避免了全连接神经元的浪费。卷积层的功能可以简单地表达为：它处理一个三维信息体积，以产生一个新的三维信息体积。我们将在下一节更详细地看一下这是如何工作的。
- en: '![](Images/fdl2_0704.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0704.png)'
- en: Figure 7-4\. Convolutional layers arrange neurons in three dimensions, so layers
    have width, height, and depth
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-4。卷积层以三维方式排列神经元，因此层具有宽度、高度和深度
- en: Filters and Feature Maps
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 滤波器和特征图
- en: In order to motivate the primitives of the convolutional layer, let’s build
    an intuition for how the human brain pieces together raw visual information into
    an understanding of the world around us. One of the most influential studies in
    this space came from David Hubel and Torsten Wiesel, who discovered that parts
    of the visual cortex are responsible for detecting edges. In 1959, they inserted
    electrodes into the brain of a cat and projected black-and-white patterns on the
    screen. They found that some neurons fired only when there were vertical lines,
    others when there were horizontal lines, and still others when the lines were
    at particular angles.^([8](ch07.xhtml#idm45934168116048))
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了激发卷积层的基本原理，让我们建立对人类大脑如何将原始视觉信息拼接成我们周围世界理解的直觉。在这个领域最有影响力的研究之一来自David Hubel和Torsten
    Wiesel，他们发现视觉皮层的部分负责检测边缘。1959年，他们在猫的大脑中插入电极，并在屏幕上投射黑白图案。他们发现一些神经元只有在有垂直线时才会发射，其他神经元只有在有水平线时才会发射，还有一些神经元只有在线条呈特定角度时才会发射。^([8](ch07.xhtml#idm45934168116048))
- en: Further work determined that the visual cortex was organized in layers. Each
    layer is responsible for building on the features detected in the previous layers—from
    lines, to contours, to shapes, to entire objects. Furthermore, within a layer
    of the visual cortex, the same feature detectors were replicated over the whole
    area in order to detect features in all parts of an image. These ideas significantly
    impacted the design of convolutional neural nets.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步的研究确定了视觉皮层是分层组织的。每一层负责在前一层检测到的特征基础上构建——从线条到轮廓，再到形状，最终到整个对象。此外，在视觉皮层的每一层中，相同的特征检测器在整个区域中复制，以便在图像的所有部分检测特征。这些想法对卷积神经网络的设计产生了重大影响。
- en: The first concept that arose was that of a *filter*, and it turns out that here,
    Viola and Jones were actually pretty close. A filter is essentially a feature
    detector, and to understand how it works, let’s consider the toy image in [Figure 7-5](#image_as_a_toy_image).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先出现的概念是*滤波器*，事实证明，Viola和Jones在这里实际上非常接近。滤波器本质上是一个特征检测器，要理解它是如何工作的，让我们考虑[图7-5](#image_as_a_toy_image)中的玩具图像。
- en: '![](Images/fdl2_0705.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0705.png)'
- en: Figure 7-5\. We’ll analyze this simple black-and-white image as a toy example
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-5。我们将分析这个简单的黑白图像作为一个玩具示例
- en: Let’s say that we want to detect vertical and horizontal lines in the image.
    One approach would be to use an appropriate feature detector, as shown in [Figure 7-6](#filters_that_detect_vertical_and_horizontal_lines).
    For example, to detect vertical lines, we would use the feature detector on the
    top, slide it across the entirety of the image, and at every step check if we
    have a match. We keep track of our answers in the matrix in the top right. If
    there’s a match, we shade the appropriate box black. If there isn’t, we leave
    it white. This result is our *feature map*, and it indicates where we’ve found
    the feature we’re looking for in the original image. We can do the same for the
    horizontal line detector (bottom), resulting in the feature map in the bottom-right
    corner.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要在图像中检测垂直和水平线。一种方法是使用适当的特征检测器，如[图7-6](#filters_that_detect_vertical_and_horizontal_lines)所示。例如，要检测垂直线，我们将使用顶部的特征检测器，在整个图像上滑动它，并在每一步检查是否有匹配。我们在右上角的矩阵中跟踪我们的答案。如果有匹配，我们将适当的框涂黑。如果没有，我们将其保留为白色。这个结果就是我们的特征图，它指示了我们在原始图像中找到我们正在寻找的特征的位置。我们可以对水平线检测器（底部）执行相同的操作，得到右下角的特征图。
- en: '![](Images/fdl2_0706.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0706.png)'
- en: Figure 7-6\. Applying filters that detect vertical and horizontal lines on our
    toy example
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-6。在我们的玩具示例上应用检测垂直和水平线的滤波器
- en: This operation is called a convolution. We take a filter and we multiply it
    over the entire area of an input image. Using the following scheme, let’s try
    to express this operation as neurons in a network. In this scheme, layers of neurons
    in a feed-forward neural net represent either the original image or a feature
    map. Filters represent combinations of connections (one such combination is highlighted
    in [Figure 7-7](#representing_filters_and_feature_maps)) that get replicated across
    the entirety of the input. In [Figure 7-7](#representing_filters_and_feature_maps),
    connections of the same color are restricted to always have the same weight. We
    can achieve this by initializing all the connections in a group with identical
    weights and by always averaging the weight updates of a group before applying
    them at the end of each iteration of backpropagation. The output layer is the
    feature map generated by this filter. A neuron in the feature map is activated
    if the filter contributing to its activity detected an appropriate feature at
    the corresponding position in the previous layer.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作被称为卷积。我们取一个滤波器，将其乘以输入图像的整个区域。使用以下方案，让我们尝试将这个操作表示为网络中的神经元。在这个方案中，前馈神经网络中的神经元层代表原始图像或特征图。滤波器代表连接的组合（其中一种组合在[图7-7](#representing_filters_and_feature_maps)中突出显示），这些连接在整个输入中被复制。在[图7-7](#representing_filters_and_feature_maps)中，相同颜色的连接被限制为始终具有相同的权重。我们可以通过使用相同权重初始化组中的所有连接，并在每次反向传播迭代结束前始终平均组的权重更新来实现这一点。输出层是由该滤波器生成的特征图。如果贡献到其活动的滤波器在前一层的相应位置检测到适当的特征，则特征图中的神经元将被激活。
- en: '![](Images/fdl2_0707.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0707.png)'
- en: Figure 7-7\. Representing filters and feature maps as neurons in a convolutional
    layer
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-7。将滤波器和特征图表示为卷积层中的神经元
- en: 'Let’s denote the <math alttext="k Superscript t h"><msup><mi>k</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    feature map in layer <math alttext="m"><mi>m</mi></math> as <math alttext="m Superscript
    k"><msup><mi>m</mi> <mi>k</mi></msup></math> . Moreover, let’s denote the corresponding
    filter by the values of its weights <math alttext="upper W"><mi>W</mi></math>
    . Then, assuming the neurons in the feature map have bias <math alttext="b Superscript
    k"><msup><mi>b</mi> <mi>k</mi></msup></math> (note that the bias is kept identical
    for all of the neurons in a feature map), we can mathematically express the feature
    map as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将第k个特征图在层m中表示为mk。此外，让我们用其权重值W表示相应的滤波器。然后，假设特征图中的神经元具有偏置bk（注意，偏置对于特征图中的所有神经元保持相同），我们可以用数学方式表示特征图如下：
- en: <math alttext="m Subscript i j Superscript k Baseline equals f left-parenthesis
    left-parenthesis upper W asterisk x right-parenthesis Subscript i j Baseline plus
    b Superscript k Baseline right-parenthesis"><mrow><msubsup><mi>m</mi> <mrow><mi>i</mi><mi>j</mi></mrow>
    <mi>k</mi></msubsup> <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mrow><mo>(</mo><mi>W</mi><mo>*</mo><mi>x</mi><mo>)</mo></mrow>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>+</mo> <msup><mi>b</mi> <mi>k</mi></msup>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="m Subscript i j Superscript k Baseline equals f left-parenthesis
    left-parenthesis upper W asterisk x right-parenthesis Subscript i j Baseline plus
    b Superscript k Baseline right-parenthesis"><mrow><msubsup><mi>m</mi> <mrow><mi>i</mi><mi>j</mi></mrow>
    <mi>k</mi></msubsup> <mo>=</mo> <mi>f</mi> <mrow><mo>(</mo> <msub><mrow><mo>(</mo><mi>W</mi><mo>*</mo><mi>x</mi><mo>)</mo></mrow>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mo>+</mo> <msup><mi>b</mi> <mi>k</mi></msup>
    <mo>)</mo></mrow></mrow></math>
- en: This mathematical description is simple and succinct, but it doesn’t completely
    describe filters as they are used in convolutional neural networks. Specifically,
    filters don’t just operate on a single feature map. They operate on the entire
    volume of feature maps that have been generated at a particular layer. For example,
    consider a situation in which we would like to detect a face at a particular layer
    of a convolutional net. And we have accumulated three feature maps, one for eyes,
    one for noses, and one for mouths. We know that a particular location contains
    a face if the corresponding locations in the primitive feature maps contain the
    appropriate features (two eyes, a nose, and a mouth). In other words, to make
    decisions about the existence of a face, we must combine evidence over multiple
    feature maps. This is equally necessary for an input image that is of full color.
    These images have pixels represented as RGB values, so we require three slices
    in the input volume (one slice for each color). As a result, feature maps must
    be able to operate over volumes, not just areas. This is shown in [Figure 7-8](#rgb_image_as_a_volume).
    Each cell in the input volume is a neuron. A local portion is multiplied with
    a filter (corresponding to weights in the convolutional layer) to produce a neuron
    in a filter map in the following volumetric layer of neurons.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数学描述简单而简洁，但并不能完全描述滤波器在卷积神经网络中的使用方式。具体来说，滤波器不仅仅作用于单个特征图。它们作用于在特定层生成的整个特征图体积。例如，考虑这样一种情况，我们想要在卷积网络的特定层检测一张脸。我们已经累积了三个特征图，一个用于眼睛，一个用于鼻子，一个用于嘴巴。我们知道，如果原始特征图中的相应位置包含适当的特征（两只眼睛，一个鼻子和一个嘴巴），那么特定位置包含一张脸。换句话说，为了对脸的存在做出决定，我们必须结合多个特征图上的证据。对于全彩输入图像同样必要。这些图像的像素表示为RGB值，因此我们需要输入体积中的三个切片（每种颜色一个切片）。因此，特征图必须能够在体积上操作，而不仅仅是在区域上。这在[图7-8](#rgb_image_as_a_volume)中显示。输入体积中的每个单元都是一个神经元。局部部分与滤波器（对应于卷积层中的权重）相乘，以在下一个神经元体积层中产生一个滤波器映射中的神经元。
- en: '![](Images/fdl2_0708.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0708.png)'
- en: Figure 7-8\. A full-color RGB image as a volume and applying a volumetric convolutional
    filter
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-8。将全彩RGB图像作为体积并应用体积卷积滤波器
- en: As we discussed in the previous section, a convolutional layer (which consists
    of a set of filters) converts one volume of values into another volume of values.
    The depth of the filter corresponds to the depth of the input volume. This is
    so that the filter can combine information from all the features that have been
    learned. The depth of the output volume of a convolutional layer is equivalent
    to the number of filters in that layer, because each filter produces its own slice.
    We visualize these relationships in [Figure 7-9](#each_filter_corresponds_to_a_slice).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中讨论的，卷积层（由一组滤波器组成）将一个值体积转换为另一个值体积。滤波器的深度对应于输入体积的深度。这样，滤波器可以结合从已学习的所有特征中获得的信息。卷积层的输出体积的深度等于该层中的滤波器数量，因为每个滤波器产生自己的切片。我们在[图7-9](#each_filter_corresponds_to_a_slice)中可视化这些关系。
- en: '![](Images/fdl2_0709.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0709.png)'
- en: Figure 7-9\. A three-dimensional visualization of a convolutional layer, where
    each filter corresponds to a slice in the resulting output volume
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-9。卷积层的三维可视化，其中每个滤波器对应于结果输出体积中的一个切片
- en: In the next section, we will use these concepts and fill in some of the gaps
    to create a full description of a convolutional layer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将利用这些概念并填补一些空白，以创建卷积层的完整描述。
- en: Full Description of the Convolutional Layer
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层的完整描述
- en: 'Let’s use the concepts we’ve developed so far to complete the description of
    the convolutional layer. First, a convolutional layer takes in an input volume.
    This input volume has the following characteristics:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用迄今为止我们已经开发的概念来完成对卷积层的描述。首先，卷积层接收一个输入体积。这个输入体积具有以下特征：
- en: Its width <math alttext="w Subscript i n"><msub><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的宽度<math alttext="w Subscript i n"><msub><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>
- en: Its height <math alttext="h Subscript i n"><msub><mi>h</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的高度<math alttext="h Subscript i n"><msub><mi>h</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>
- en: Its depth <math alttext="d Subscript i n"><msub><mi>d</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的深度<math alttext="d Subscript i n"><msub><mi>d</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>
- en: Its zero padding <math alttext="p"><mi>p</mi></math>
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它的零填充<math alttext="p"><mi>p</mi></math>
- en: 'This volume is processed by a total of <math alttext="k"><mi>k</mi></math>
    filters, which represent the weights and connections in the convolutional network.
    These filters have a number of hyperparameters, which are described as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这个体积由总共<math alttext="k"><mi>k</mi></math>个滤波器处理，代表卷积网络中的权重和连接。这些滤波器有一些超参数，描述如下：
- en: Their spatial extent <math alttext="e"><mi>e</mi></math> , which is equal to
    the filter’s height and width.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们的空间范围<math alttext="e"><mi>e</mi></math>，等于滤波器的高度和宽度。
- en: Their stride <math alttext="s"><mi>s</mi></math> , or the distance between consecutive
    applications of the filter on the input volume. If we use a stride of 1, we get
    the full convolution described in the previous section. We illustrate this in
    [Figure 7-10](#illustration_of_a_filters_stride).
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们的步幅<math alttext="s"><mi>s</mi></math>，或者滤波器在输入体积上连续应用的距离。如果我们使用步幅为1，我们得到了前一节中描述的完整卷积。我们在[图7-10](#illustration_of_a_filters_stride)中进行了说明。
- en: The bias <math alttext="b"><mi>b</mi></math> (a parameter learned like the values
    in the filter), which is added to each component of the convolution.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置<math alttext="b"><mi>b</mi></math>（像滤波器中的值一样学习的参数），添加到卷积的每个分量中。
- en: '![](Images/fdl2_0710.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0710.png)'
- en: Figure 7-10\. A filter’s stride hyperparameter
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-10。滤波器的步幅超参数
- en: 'This results in an output volume with the following characteristics:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致输出体积具有以下特征：
- en: Its function <math alttext="f"><mi>f</mi></math> , which is applied to the incoming
    logit of each neuron in the output volume to determine its final value
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用于输出体积中每个神经元的传入逻辑的函数f，以确定其最终值
- en: Its width <math alttext="w Subscript o u t Baseline equals left ceiling StartFraction
    w Subscript i n Baseline minus e plus 2 p Over s EndFraction right ceiling plus
    1"><mrow><msub><mi>w</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub> <mo>=</mo>
    <mfenced separators="" open="⌈" close="⌉"><mfrac><mrow><msub><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>-</mo><mi>e</mi><mo>+</mo><mn>2</mn><mi>p</mi></mrow> <mi>s</mi></mfrac></mfenced>
    <mo>+</mo> <mn>1</mn></mrow></math>
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出体积的宽度为输入体积减去e加2p除以s的上限值加1
- en: Its height <math alttext="h Subscript o u t Baseline equals left ceiling StartFraction
    h Subscript i n Baseline minus e plus 2 p Over s EndFraction right ceiling plus
    1"><mrow><msub><mi>h</mi> <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub> <mo>=</mo>
    <mfenced separators="" open="⌈" close="⌉"><mfrac><mrow><msub><mi>h</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>-</mo><mi>e</mi><mo>+</mo><mn>2</mn><mi>p</mi></mrow> <mi>s</mi></mfrac></mfenced>
    <mo>+</mo> <mn>1</mn></mrow></math>
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出体积的高度为输入体积减去e加2p除以s的上限值加1
- en: Its depth <math alttext="d Subscript o u t Baseline equals k"><mrow><msub><mi>d</mi>
    <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub> <mo>=</mo> <mi>k</mi></mrow></math>
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出体积的深度为k
- en: The <math alttext="m Superscript t h"><msup><mi>m</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    “depth slice” of the output volume, where <math alttext="1 less-than-or-equal-to
    m less-than-or-equal-to k"><mrow><mn>1</mn> <mo>≤</mo> <mi>m</mi> <mo>≤</mo> <mi>k</mi></mrow></math>
    , corresponds to the function <math alttext="f"><mi>f</mi></math> applied to the
    sum of the <math alttext="m Superscript t h"><msup><mi>m</mi> <mrow><mi>t</mi><mi>h</mi></mrow></msup></math>
    filter convoluted over the input volume and the bias <math alttext="b Superscript
    m"><msup><mi>b</mi> <mi>m</mi></msup></math> . Moreover, this means that per filter,
    we have <math alttext="d Subscript i n Baseline e squared"><mrow><msub><mi>d</mi>
    <mrow><mi>i</mi><mi>n</mi></mrow></msub> <msup><mi>e</mi> <mn>2</mn></msup></mrow></math>
    parameters. In total, that means the layer has <math alttext="k d Subscript i
    n Baseline e squared"><mrow><mi>k</mi> <msub><mi>d</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub>
    <msup><mi>e</mi> <mn>2</mn></msup></mrow></math> parameters and <math alttext="k"><mi>k</mi></math>
    biases. To demonstrate this in action, we provide an example of a convolutional
    layer in Figures [7-11](#convolutional_layer_with_input_volume) and [7-12](#next_value_in_first_depth_slice)
    with a 5 × 5 × 3 input volume with zero padding <math alttext="p equals 1"><mrow><mi>p</mi>
    <mo>=</mo> <mn>1</mn></mrow></math> . We’ll use two 3 × 3 × 3 filters (spatial
    extent ) with a stride <math alttext="s equals 2"><mrow><mi>s</mi> <mo>=</mo>
    <mn>2</mn></mrow></math> . We’ll use a linear function to produce the output volume,
    which will be of size 3 × 3 × 2\. We apply the first convolutional filter to the
    upper-leftmost 3 × 3 piece of the input volume to generate the upper-leftmost
    entry of the first depth slice.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 第m个“深度切片”对应于应用于输入体积上的第m个滤波器和偏置b的函数f的总和。此外，这意味着每个滤波器有d个参数。总的来说，这意味着该层有kd个参数和k个偏置。为了演示这一点，我们提供了一个卷积层的示例，其中输入体积为5×5×3，使用零填充p=1。我们将使用两个3×3×3的滤波器（空间范围），步长s=2。我们将使用线性函数生成输出体积，大小为3×3×2。我们将第一个卷积滤波器应用于输入体积的左上角3×3部分，以生成第一个深度切片的左上角条目。
- en: Generally, it’s wise to keep filter sizes small (size 3 × 3 or 5 × 5). Less
    commonly, larger sizes are used (7 × 7) but only in the first convolutional layer.
    Having more small filters is an easy way to achieve high representational power
    while also incurring a smaller number of parameters. It’s also suggested to use
    a stride of 1 to capture all useful information in the feature maps, and a zero
    padding that keeps the output volume’s height and width equivalent to the input
    volume’s height and width.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，保持滤波器尺寸较小（3×3或5×5）是明智的。较少见的是，使用较大的尺寸（7×7），但仅在第一个卷积层中使用。使用更多小滤波器是实现高表征能力的简单方法，同时也带来较少的参数。建议使用步长为1来捕获特征图中的所有有用信息，并使用零填充来保持输出体积的高度和宽度等于输入体积的高度和宽度。
- en: '![](Images/fdl2_0711.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0711.png)'
- en: Figure 7-11\. A convolutional layer with an input volume of width 5, height
    5, depth 3, zero padding 1, and 2 filters (with spatial extent 3 and applied with
    a stride of 2) results in an output volume of 3 × 3 × 2
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-11。一个宽度为5、高度为5、深度为3、零填充1、2个滤波器（空间范围为3，步长为2）的输入体积，结果为3×3×2的输出体积
- en: '![](Images/fdl2_0712.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0712.png)'
- en: Figure 7-12\. Using the same setup as [Figure 7-11](#convolutional_layer_with_input_volume),
    we generate the next value in the first depth slice of the output volume
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-12。使用与[图7-11](#convolutional_layer_with_input_volume)相同的设置，我们生成输出体积的第一个深度切片中的下一个值
- en: 'PyTorch ​provides us with a convenient operation to easily perform a 2D convolution
    on a minibatch of input volumes:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch为我们提供了一个方便的操作，可以轻松地在输入体积的小批量上执行2D卷积：
- en: '[PRE0]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, `in_channels` represents the depth, <math alttext="d Subscript i n"><msub><mi>d</mi>
    <mrow><mi>i</mi><mi>n</mi></mrow></msub></math> , or number of input planes. For
    color images, the number of input channels often equals three, representing the
    RGB channels. The `nn.Conv2d` layer will accept as an input a four-dimensional
    tensor of size, <math alttext="b Subscript i n Baseline asterisk d Subscript i
    n Baseline asterisk h Subscript i n Baseline asterisk w Subscript i n"><mrow><msub><mi>b</mi>
    <mrow><mi>i</mi><mi>n</mi></mrow></msub> <mo>*</mo> <msub><mi>d</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>*</mo> <msub><mi>h</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub> <mo>*</mo>
    <msub><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></mrow></math> , where
    <math alttext="b Subscript i n"><msub><mi>b</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub></math>
    is the number of examples in our minibatch.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，“in_channels”表示深度，<math alttext="d Subscript i n"><msub><mi>d</mi> <mrow><mi>i</mi><mi>n</m></row></msub></math>，或输入平面的数量。对于彩色图像，输入通道的数量通常等于三，表示RGB通道。`nn.Conv2d`层将接受一个四维张量作为输入，大小为<math
    alttext="b Subscript i n Baseline asterisk d Subscript i n Baseline asterisk h
    Subscript i n Baseline asterisk w Subscript i n"><mrow><msub><mi>b</mi> <mrow><mi>i</mi><mi>n</m></row></msub>
    <mo>*</mo> <msub><mi>d</mi> <mrow><mi>i</mi><mi>n</m></row></msub> <mo>*</mo>
    <msub><mi>h</mi> <mrow><mi>i</mi><mi>n</m></row></msub> <mo>*</mo> <msub><mi>w</mi>
    <mrow><mi>i</mi><mi>n</m></row></msub></mrow></math>，其中<math alttext="b Subscript
    i n"><msub><mi>b</mi> <mrow><mi>i</mi><mi>n</m></row></msub></math>是我们小批量中示例的数量。
- en: The `out_channels` argument represents the number of output planes or feature
    maps. The `kernel_size` argument determines the filter size or *spatial extent,
    <math alttext="e"><mi>e</mi></math> ,* while the `stride` and `padding` arguments
    determine the stride size, <math alttext="s"><mi>s</mi></math> , and zero padding
    size, <math alttext="p"><mi>p</mi></math> , respectively. Note that you can pass
    in equal dimension settings with a single value as shown here with stride and
    padding.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`out_channels`参数表示输出平面或特征图的数量。`kernel_size`参数确定滤波器大小或*空间范围，<math alttext="e"><mi>e</mi></math>，*而`stride`和`padding`参数确定步幅大小，<math
    alttext="s"><mi>s</mi></math>，和零填充大小，<math alttext="p"><mi>p</mi></math>，分别。请注意，您可以通过传递单个值来传递相等的维度设置，如此处所示的步幅和填充。'
- en: Max Pooling
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大池化
- en: To aggressively reduce dimensionality of feature maps and sharpen the located
    features, we sometimes insert a [*max pooling* layer](https://oreil.ly/HOYaa)
    after a convolutional layer. The essential idea behind max pooling is to break
    up each feature map into equally sized tiles. Then we create a condensed feature
    map. Specifically, we create a cell for each tile, compute the maximum value in
    the tile, and propagate this maximum value into the corresponding cell of the
    condensed feature map. This process is illustrated in [Figure 7-13](#illustration_of_how_max_pooling).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了积极减少特征图的维度并锐化定位的特征，我们有时会在卷积层之后插入一个[*最大池化*层](https://oreil.ly/HOYaa)。最大池化背后的基本思想是将每个特征图分成相同大小的瓦片。然后我们创建一个压缩的特征图。具体来说，我们为每个瓦片创建一个单元格，计算瓦片中的最大值，并将此最大值传播到压缩特征图的相应单元格中。这个过程在[图7-13](#illustration_of_how_max_pooling)中有所说明。
- en: '![](Images/fdl2_0713.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: ！[](Images/fdl2_0713.png)
- en: Figure 7-13\. Max pooling significantly reduces parameters as we move up the
    network
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-13。最大池化在网络上移动时显著减少参数
- en: 'More rigorously, we can describe a pooling layer with two parameters:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 更严格地说，我们可以用两个参数描述一个池化层：
- en: Its spatial extent <math alttext="e"><mi>e</mi></math>
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其空间范围<math alttext="e"><mi>e</mi></math>
- en: Its stride <math alttext="s"><mi>s</mi></math>
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其步幅<math alttext="s"><mi>s</mi></math>
- en: 'It’s important to note that only two major variations of the pooling layer
    are used. The first is the nonoverlapping pooling layer with <math alttext="e
    equals 2 comma s equals 2"><mrow><mi>e</mi> <mo>=</mo> <mn>2</mn> <mo>,</mo> <mi>s</mi>
    <mo>=</mo> <mn>2</mn></mrow></math> . The second is the overlapping pooling layer
    with <math alttext="e equals 3 comma s equals 2"><mrow><mi>e</mi> <mo>=</mo> <mn>3</mn>
    <mo>,</mo> <mi>s</mi> <mo>=</mo> <mn>2</mn></mrow></math> . The resulting dimensions
    of each feature map are as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，池化层仅使用了两种主要变体。第一种是非重叠池化层，其中<math alttext="e equals 2 comma s equals
    2"><mrow><mi>e</mi> <mo>=</mo> <mn>2</mn> <mo>,</mo> <mi>s</mi> <mo>=</mo> <mn>2</mn></mrow></math>。第二种是重叠池化层，其中<math
    alttext="e equals 3 comma s equals 2"><mrow><mi>e</mi> <mo>=</mo> <mn>3</mn> <mo>,</mo>
    <mi>s</mi> <mo>=</mo> <mn>2</mn></mrow></math>。每个特征图的结果维度如下：
- en: Its width <math alttext="w Subscript o u t Baseline equals left ceiling StartFraction
    w Subscript i n Baseline minus e Over s EndFraction right ceiling plus 1"><mrow><msub><mi>w</mi>
    <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub> <mo>=</mo> <mfenced separators=""
    open="⌈" close="⌉"><mfrac><mrow><msub><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>-</mo><mi>e</mi></mrow> <mi>s</mi></mfrac></mfenced> <mo>+</mo> <mn>1</mn></mrow></math>
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其宽度<math alttext="w Subscript o u t Baseline equals left ceiling StartFraction
    w Subscript i n Baseline minus e Over s EndFraction right ceiling plus 1"><mrow><msub><mi>w</mi>
    <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub> <mo>=</mo> <mfenced separators=""
    open="⌈" close="⌉"><mfrac><mrow><msub><mi>w</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>-</mo><mi>e</mi></mrow> <mi>s</mi></mfrac></mfenced> <mo>+</mo> <mn>1</mn></mrow></math>
- en: Its height <math alttext="h Subscript o u t Baseline equals left ceiling StartFraction
    h Subscript i n Baseline minus e Over s EndFraction right ceiling plus 1"><mrow><msub><mi>h</mi>
    <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub> <mo>=</mo> <mfenced separators=""
    open="⌈" close="⌉"><mfrac><mrow><msub><mi>h</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>-</mo><mi>e</mi></mrow> <mi>s</mi></mfrac></mfenced> <mo>+</mo> <mn>1</mn></mrow></math>
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其高度<math alttext="h Subscript o u t Baseline equals left ceiling StartFraction
    h Subscript i n Baseline minus e Over s EndFraction right ceiling plus 1"><mrow><msub><mi>h</mi>
    <mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub> <mo>=</mo> <mfenced separators=""
    open="⌈" close="⌉"><mfrac><mrow><msub><mi>h</mi> <mrow><mi>i</mi><mi>n</mi></mrow></msub>
    <mo>-</mo><mi>e</mi></mrow> <mi>s</mi></mfrac></mfenced> <mo>+</mo> <mn>1</mn></mrow></math>
- en: One interesting property of max pooling is that it is *locally invariant*. This
    means that even if the inputs shift around a little bit, the output of the max
    pooling layer stays constant. This has important implications for visual algorithms.
    Local invariance is a useful property if we care more about whether some feature
    is present than exactly where it is. However, enforcing large amounts of local
    invariance can destroy our network’s ability to carry important information. As
    a result, we usually keep the spatial extent of our pooling layers quite small.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化的一个有趣特性是它是*局部不变的*。这意味着即使输入稍微移动，最大池化层的输出保持不变。这对视觉算法有重要的影响。如果我们更关心某个特征是否存在而不是它确切的位置，局部不变性是一个有用的特性。然而，强制大量的局部不变性可能破坏我们网络传递重要信息的能力。因此，我们通常保持池化层的空间范围相当小。
- en: Some recent work along this line has come out of the University of Warwick from
    Graham,^([9](ch07.xhtml#idm45934163023728)) who proposes a concept called *fractional
    max pooling*. In fractional max pooling, a pseudorandom number generator is used
    to generate tilings with noninteger lengths for pooling. Here, fractional max
    pooling functions as a strong regularizer, helping prevent overfitting in convolutional
    networks.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最近有一些工作是由华威大学的格雷厄姆提出的，他提出了一个概念叫做*分数最大池化*。在分数最大池化中，使用伪随机数生成器生成具有非整数长度的瓦片进行池化。在这里，分数最大池化作为一个强正则化器，有助于防止卷积网络过拟合。
- en: Full Architectural Description of Convolution Networks
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积网络的完整架构描述
- en: Now that we’ve described the building blocks of convolutional networks, we start
    putting them together. [Figure 7-14](#convolutional_network_architectures) depicts
    several architectures that might be of practical use.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经描述了卷积网络的构建模块，我们开始将它们组合起来。[图7-14](#convolutional_network_architectures)描述了几种可能有实际用途的架构。
- en: One theme we notice as we build deeper networks is that we reduce the number
    of pooling layers and instead stack multiple convolutional layers in tandem. This
    is generally helpful because pooling operations are inherently destructive. Stacking
    several convolutional layers before each pooling layer allows us to achieve richer
    representations.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建更深层次的网络时，我们注意到一个主题是减少池化层的数量，而是在每个池化层之前堆叠多个卷积层。这通常是有帮助的，因为池化操作本质上是破坏性的。在每个池化层之前堆叠几个卷积层允许我们获得更丰富的表示。
- en: '![](Images/fdl2_0714.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0714.png)'
- en: Figure 7-14\. Various convolutional network architectures of various complexities
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-14。各种复杂性的卷积网络架构
- en: As a practical note, deep convolutional networks can take up a significant amount
    of space, and most casual practitioners are usually bottlenecked by the memory
    capacity on their GPU. The VGGNet architecture, for example, takes approximately
    90 MB of memory on the forward pass per image, and more than 180 MB of memory
    on the backward pass to update the parameters.^([10](ch07.xhtml#idm45934163009328))
    Many deep networks make a compromise by using strides and spatial extents in the
    first convolutional layer that reduce the amount of information that needs to
    be propagated up the network.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个实际的注意事项，深度卷积网络可能占用大量空间，大多数业余从业者通常受限于GPU的内存容量。例如，VGGNet架构在每个图像的前向传递中大约占用90
    MB的内存，在后向传递中更新参数时占用超过180 MB的内存。许多深度网络通过在第一个卷积层中使用步幅和空间范围来减少需要向网络上传播的信息量来做出妥协。
- en: Closing the Loop on MNIST with Convolutional Networks
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用卷积网络关闭MNIST循环
- en: Now that we have a better understanding of how to build networks that effectively
    analyze images, we’ll revisit the MNIST challenge we’ve tackled over the past
    several chapters. Here, we’ll use a convolutional network to learn how to recognize
    handwritten digits. Our feed-forward network was able to achieve a 98.2% accuracy.
    Our goal will be to push the envelope on this result.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对如何构建有效分析图像的网络有了更好的理解，我们将重新审视我们在过去几章中处理的MNIST挑战。在这里，我们将使用卷积网络来学习如何识别手写数字。我们的前馈网络能够达到98.2%的准确率。我们的目标是在这个结果上取得进展。
- en: 'To tackle this challenge, we’ll build a convolutional network with a pretty
    standard architecture (modeled after the second network in [Figure 7-14](#convolutional_network_architectures)):
    two convolutional/ReLU/maxpooling stacks, followed by a fully connected layer
    with dropout and a terminal fully connected layer. Building the network is easy
    in PyTorch using the built-in `nn` classes, as shown in the following code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个挑战，我们将构建一个具有相当标准架构的卷积网络（模仿[图7-14](#convolutional_network_architectures)中的第二个网络）：两个卷积/ReLU/最大池化堆栈，后面是一个带有丢失和终端全连接层的全连接层。在PyTorch中使用内置的`nn`类很容易构建网络，如下面的代码所示：
- en: '[PRE1]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `__init__` method generates two `Conv2d/ReLU/MaxPool` blocks followed by
    a block containing two fully connected layers. The convolutional layers are created
    with a particular shape. By default, the stride is set to be 1, while the padding
    is set to `same` to keep the width and height constant between input and output
    tensors. By default, each `nn.Conv2d` constructor automatically initializes the
    weights.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`__init__`方法生成两个`Conv2d/ReLU/MaxPool`块，然后是一个包含两个全连接层的块。卷积层是以特定形状创建的。默认情况下，步幅设置为1，而填充设置为`same`以保持输入和输出张量之间的宽度和高度恒定。默认情况下，每个`nn.Conv2d`构造函数会自动初始化权重。'
- en: The max pooling layers consist of nonoverlapping windows of size `k`. The default,
    as recommended, is `k=2`, and we’ll use this default in our MNIST convolutional
    network.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最大池化层由大小为`k`的非重叠窗口组成。默认值，如建议的，是`k=2`，我们将在我们的MNIST卷积网络中使用这个默认值。
- en: The forward method defines how our layers and blocks are connected together
    to perform the forward pass or inference.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 前向方法定义了我们的层和块如何连接在一起执行前向传递或推理。
- en: The code here is quite easy to follow. The input is expected to be a tensor
    of size <math alttext="upper N times 1 times 28 times 28"><mrow><mi>N</mi> <mo>×</mo>
    <mn>1</mn> <mo>×</mo> <mn>28</mn> <mo>×</mo> <mn>28</mn></mrow></math> , where
    N is the number of examples in a minibatch, 28 is the width and height of each
    image, and 1 is the depth (because the images are black and white; if the images
    were in RGB color, the depth would instead be 3 to represent each color map).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的代码非常容易理解。输入预期是一个大小为<math alttext="upper N times 1 times 28 times 28"><mrow><mi>N</mi>
    <mo>×</mo> <mn>1</mn> <mo>×</mo> <mn>28</mn> <mo>×</mo> <mn>28</mn></mrow></math>的张量，其中N是小批量中的示例数，28是每个图像的宽度和高度，1是深度（因为图像是黑白的；如果图像是RGB颜色，深度将是3，以表示每个颜色映射）。
- en: The first block, `conv1`, builds a convolutional layer with 32 filters that
    have spatial extent 5\. This results in taking an input volume of depth 1 and
    emitting an output tensor of depth 32\. This is then passed through a max pooling
    layer that compresses the information. The second block, `conv2`, then builds
    a second convolutional layer with 64 filters, again with spatial extent 5, taking
    an input tensor of depth 32 and emitting an output tensor of depth 64\. This,
    again, is passed through a max pooling layer to compress information.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个块`conv1`构建了一个具有32个滤波器和空间范围为5的卷积层。这导致将深度为1的输入体积转换为深度为32的输出张量。然后通过一个最大池化层传递，压缩信息。第二个块`conv2`然后构建了一个具有64个滤波器和空间范围为5的第二个卷积层，将深度为32的输入张量转换为深度为64的输出张量。同样，通过一个最大池化层传递，压缩信息。
- en: We then prepare to pass the output of the max pooling layer into a fully connected
    layer. To do this, we flatten the tensor. We can do this by computing the full
    size of each “subtensor” in the minibatch. We have 64 filters, which corresponds
    to the depth of 64\. We now have to determine the height and width after passing
    through two max pooling layers. Using the formulas we found in the previous section,
    it’s easy to confirm that each feature map has a height and width of 7\. Confirming
    this is left as an exercise for you.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们准备将最大池化层的输出传递到一个全连接层。为了做到这一点，我们需要展平张量。我们可以通过计算小批量中每个“子张量”的完整大小来实现这一点。我们有64个滤波器，对应于64的深度。现在我们需要确定通过两个最大池化层后的高度和宽度。使用我们在前一节中找到的公式，很容易确认每个特征图的高度和宽度为7。请您自行确认这一点。
- en: We use a fully connected layer to compress the flattened representation into
    a hidden state of size 1,024\. We use a dropout probability in this layer of 0.5
    during training and 1 during model evaluation (standard procedure for employing
    dropout). Finally, we send this hidden state into a output layer with 10 bins
    (the softmax is, as usual, performed in the `loss` constructor for better performance).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个全连接层将展平的表示压缩成大小为1,024的隐藏状态。在训练过程中，我们在这一层使用0.5的丢失概率，在模型评估过程中使用1（使用丢失的标准程序）。最后，我们将这个隐藏状态发送到一个具有10个箱子的输出层（通常在`loss`构造函数中执行softmax以获得更好的性能）。
- en: 'Finally, we train our network using the Adam optimizer. After several epochs
    over the dataset, we achieve an accuracy of 99.4%, which isn’t state-of-the-art
    (approximately 99.7 to 99.8%), but is respectable:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用Adam优化器训练我们的网络。经过几个纪元的数据集，我们实现了99.4%的准确率，这虽然不是最先进的（大约为99.7到99.8%），但是还是令人尊敬的：
- en: '[PRE2]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Image Preprocessing Pipelines Enable More Robust Models
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像预处理管道使模型更加健壮。
- en: So far we’ve been dealing with rather tame datasets. Why is MNIST a tame dataset?
    Well, fundamentally, MNIST has already been preprocessed so that all the images
    in the dataset resemble each other. The handwritten digits are perfectly cropped
    in just the same way; there are no color aberrations because MNIST is black and
    white; and so on. Natural images, however, are an entirely different beast.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理相当温顺的数据集。为什么MNIST是一个温顺的数据集呢？基本上，MNIST已经经过预处理，使数据集中的所有图像相互类似。手写数字被完美地裁剪，没有颜色异常，因为MNIST是黑白的，等等。然而，自然图像是一种完全不同的动物。
- en: 'Natural images are messy, and as a result, there are a number of preprocessing
    operations that we can utilize in order to make training slightly easier. Fortunately,
    PyTorch offers a package called Torchvision that includes many commonly used transforms
    for image processing. One technique that is supported out of the box in PyTorch
    is image whitening. The basic idea behind whitening is to zero-center every pixel
    in an image by subtracting out the mean of the dataset and normalizing to unit
    1 variance. This helps us correct for potential differences in dynamic range between
    images. In PyTorch, we can achieve this using the `Normalize` transform:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 自然图像是混乱的，因此有许多预处理操作可以使训练稍微容易一些。幸运的是，PyTorch提供了一个名为Torchvision的包，其中包含许多常用的图像处理变换。PyTorch中支持的一种技术是图像白化。白化背后的基本思想是通过减去数据集的均值并归一化为单位1方差来使图像中的每个像素居中为零。这有助于我们纠正图像之间动态范围的潜在差异。在PyTorch中，我们可以使用`Normalize`变换来实现这一点：
- en: '[PRE3]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The magic numbers for `mean`, 0.1307, and `std`, 0.3081, were computed over
    the entire MNIST dataset, and this technique is called dataset normalization.
    We can also expand our dataset artificially by randomly cropping the image, flipping
    the image, modifying saturation, modifying brightness, etc:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`mean`的魔术数字为0.1307，`std`为0.3081，是在整个MNIST数据集上计算得出的，这种技术称为数据集归一化。我们还可以通过随机裁剪图像、翻转图像、修改饱和度、修改亮度等方式来人为地扩展我们的数据集：'
- en: '[PRE4]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we use the `Compose` transform to create a sequence of transforms from
    a list. After applying random cropping, flipping, and color adjustments, we convert
    the image data to a PyTorch tensor and normalize the data. PyTorch models require
    the data to be in tensor format, and these last two steps are common practice
    in using PyTorch for deep learning.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用`Compose`变换来从列表中创建一系列变换。在应用随机裁剪、翻转和颜色调整后，我们将图像数据转换为PyTorch张量并对数据进行归一化。PyTorch模型要求数据以张量格式呈现，这两个最后步骤在使用PyTorch进行深度学习时是常见的做法。
- en: Applying these transformations helps us build networks that are robust to the
    different kinds of variations that are present in natural images, and make predictions
    with high fidelity in spite of potential distortions.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这些转换有助于我们构建对自然图像中存在的不同类型变化具有鲁棒性的网络，并且尽管可能存在扭曲，但可以高度准确地进行预测。
- en: Accelerating Training with Batch Normalization
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用批量归一化加速训练
- en: In 2015, researchers from Google devised an exciting way to even further accelerate
    the training of feed-forward and convolutional neural networks using a technique
    called *batch normaliza**tion*.^([11](ch07.xhtml#idm45934162940752)) We can think
    of the intuition behind batch normalization like a tower of blocks, as shown in
    [Figure 7-15](#when_blocks_become_shifted_too_drastically).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，来自谷歌的研究人员设计了一种令人兴奋的方法，进一步加速前馈和卷积神经网络的训练，使用一种称为*批量归一化*的技术。我们可以将批量归一化背后的直觉想象成一堆积木，如[图7-15](#when_blocks_become_shifted_too_drastically)所示。
- en: When a tower of blocks is stacked together neatly, the structure is stable.
    However, if we randomly shift the blocks, we could force the tower into configurations
    that are increasingly unstable. Eventually the tower falls apart.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当一堆积木被整齐地堆叠在一起时，结构是稳定的。然而，如果我们随机移动积木，我们可能会迫使塔处于越来越不稳定的配置中。最终，塔会倒塌。
- en: A similar phenomenon can happen during the training of neural networks. Imagine
    a two-layer neural network. In the process of training the weights of the network,
    the output distribution of the neurons in the bottom layer begins to shift. The
    result of the changing distribution of outputs from the bottom layer means that
    the top layer not only has to learn how to make the appropriate predictions, but
    it also needs to somehow modify itself to accommodate the shifts in incoming distribution.
    This significantly slows down training, and the magnitude of the problem compounds
    the more layers we have in our networks.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络的过程中也会出现类似的现象。想象一个两层神经网络。在训练网络的权重过程中，底层神经元的输出分布开始发生变化。底层输出分布的变化意味着顶层不仅需要学习如何进行适当的预测，还需要以某种方式调整自身以适应输入分布的变化。这显著减慢了训练速度，问题的严重程度随着网络层数的增加而增加。
- en: '![](Images/fdl2_0715.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0715.png)'
- en: Figure 7-15\. Batch normalization reduces shifts in the distribution of inputs
    of layers
  id: totrans-119
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-15。批量归一化减少了层输入的分布偏移
- en: 'Normalization of image inputs helps out the training process by making it more
    robust to variations. Batch normalization takes this a step further by normalizing
    inputs to every layer in our neural network. Specifically, we modify the architecture
    of our network to include operations that:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对图像输入进行归一化有助于训练过程，使其更具鲁棒性。批量归一化进一步通过将输入归一化到神经网络中的每一层来实现这一点。具体来说，我们修改网络的架构以包括以下操作：
- en: Grab the vector of logits incoming to a layer before they pass through the nonlinearity.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在通过非线性函数之前，获取传入层的logits向量。
- en: Normalize each component of the vector of logits across all examples of the
    minibatch by subtracting the mean and dividing by the standard deviation (we keep
    track of the moments using an exponentially weighted moving average).
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从所有小批量示例中减去均值并除以标准差来归一化logits向量的每个分量（我们使用指数加权移动平均值来跟踪这些时刻）。
- en: 'Given normalized inputs **x**̂, use an affine transform to restore representational
    power with two vectors of (trainable) parameters: γ**x**̂ + β.'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定归一化输入**x**̂，使用一个仿射变换来恢复表示能力，其中包括两个（可训练的）参数向量：γ**x**̂ + β。
- en: 'PyTorch provides a `BatchNorm2d` class to perform batch normalization for a
    convolutional layer:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了一个`BatchNorm2d`类，用于为卷积层执行批量归一化：
- en: '[PRE5]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, the `num_features` argument represents the depth, or number of channels,
    of the inputs to the batch normalization layer. Hence, batch normalization is
    performed over the channel dimension, computing the mean and variance of each
    minibatch of 2D channels. The `num_features` is the only required argument. All
    other arguments are set to their defaults.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`num_features`参数表示批量归一化层输入的深度或通道数。因此，批量归一化是在通道维度上执行的，计算每个2D通道的小批量的均值和方差。`num_features`是唯一必需的参数。所有其他参数都设置为默认值。
- en: 'The `BatchNorm2d` layer performs the following `affine` transformation:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`BatchNorm2d`层执行以下`affine`变换：'
- en: <math alttext="y equals StartFraction x minus upper E left-bracket x right-bracket
    Over StartRoot upper V a r left-bracket x right-bracket plus epsilon EndRoot EndFraction
    asterisk gamma plus beta"><mrow><mi>y</mi> <mo>=</mo> <mfrac><mrow><mi>x</mi><mo>-</mo><mi>E</mi><mo>[</mo><mi>x</mi><mo>]</mo></mrow>
    <msqrt><mrow><mi>V</mi><mi>a</mi><mi>r</mi><mo>[</mo><mi>x</mi><mo>]</mo><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac>
    <mo>*</mo> <mi>γ</mi> <mo>+</mo> <mi>β</mi></mrow></math>
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y equals StartFraction x minus upper E left-bracket x right-bracket
    Over StartRoot upper V a r left-bracket x right-bracket plus epsilon EndRoot EndFraction
    asterisk gamma plus beta"><mrow><mi>y</mi> <mo>=</mo> <mfrac><mrow><mi>x</mi><mo>-</mo><mi>E</mi><mo>[</mo><mi>x</mi><mo>]</mo></mrow>
    <msqrt><mrow><mi>V</mi><mi>a</mi><mi>r</mi><mo>[</mo><mi>x</mi><mo>]</mo><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac>
    <mo>*</mo> <mi>γ</mi> <mo>+</mo> <mi>β</mi></mrow></math>
- en: The parameters <math alttext="gamma"><mi>γ</mi></math> and <math alttext="beta"><mi>β</mi></math>
    are learnable parameters and will be trained during the training process if `affine
    = True`. Otherwise, the mean is subtracted from the inputs and divided by standard
    deviation to be normalized. The <math alttext="epsilon"><mi>ϵ</mi></math> argument
    is only used for mathematical stability.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 参数<math alttext="gamma"><mi>γ</mi></math>和<math alttext="beta"><mi>β</mi></math>是可学习的参数，如果`affine
    = True`，它们将在训练过程中进行训练。否则，从输入中减去均值并除以标准差进行归一化。参数<math alttext="epsilon"><mi>ϵ</mi></math>仅用于数学稳定性。
- en: When `track_running_stats = True`, this layer will keep track of the running
    mean and variance for use in evaluation mode. The running mean and variance are
    updated using the `momentum` value.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 当`track_running_stats = True`时，此层将跟踪用于评估模式的运行均值和方差。运行均值和方差使用`momentum`值进行更新。
- en: 'We can also express batch normalization for nonconvolutional feed-forward layers
    by using the `BatchNorm1d` constructor. Here, we set only `num_features = 32`
    and use the defaults for other arguments:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过使用`BatchNorm1d`构造函数来为非卷积前馈层表达批量归一化。在这里，我们只设置`num_features = 32`，并对其他参数使用默认值：
- en: '[PRE6]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In addition to speeding up training by preventing significant shifts in the
    distribution of inputs to each layer, batch normalization also allows us to significantly
    increase the learning rate. Moreover, batch normalization acts as a regularizer
    and removes the need for dropout and (when used) L2 regularization. Although we
    don’t leverage it here, the authors also claim that batch regularization largely
    removes the need for photometric distortions, and we can expose the network to
    more “real” images during the training process. In the next section, we will motivate
    and discuss a variant of normalization across the feature axis, rather than the
    batch.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通过防止输入到每一层的分布发生显著变化来加快训练速度外，批量归一化还允许我们显著增加学习速率。此外，批量归一化还充当正则化器，消除了对辍学和（当使用时）L2正则化的需求。尽管我们在这里没有利用它，但作者还声称，批量正则化在很大程度上消除了对光度失真的需求，我们可以在训练过程中向网络暴露更多“真实”图像。在下一节中，我们将激励并讨论一种沿特征轴而不是批处理轴的归一化变体。
- en: Group Normalization for Memory Constrained Learning Tasks
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 内存受限学习任务的组归一化
- en: Various forms of normalization in image processing have been studied and utilized
    in the last decade. The most famous of these is batch normalization. Just to recap
    from the previous section, this technique computes the channel-wise mean and variance
    of the output of each convolutional layer, normalizes each channel using the computed
    statistics, and then feeds the normalized output to the next convolutional layer.
    Thus, any given channel in the normalized output will have the same mean and variance
    (zero and one, respectively) across batches. In practice, the model will also
    learn a mean parameter β and a standard deviation parameter γ, which are then
    applied to the normalized output such that it has mean β and standard deviation
    γ before being fed into the subsequent layer. This process is used to reduce the
    shift in distribution of any given channel from one batch to the next. Note that
    this is only a reduction of the shift and not a complete removal of it, since
    the channel distribution might still look completely different from one batch
    to the next even though they have the same mean and variance. In theory, and as
    has been observed empirically, reducing this internal covariate shift stabilizes
    training and results in strong performance gains.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的十年中，图像处理中的各种归一化形式已经被研究和利用。其中最著名的是批量归一化。仅从前一节回顾一下，这种技术计算每个卷积层输出的通道均值和方差，使用计算出的统计数据对每个通道进行归一化，然后将归一化的输出馈送到下一个卷积层。因此，归一化输出中的任何给定通道在批次中将具有相同的均值和方差（分别为零和一）。在实践中，模型还将学习一个均值参数β和一个标准差参数γ，然后将它们应用于归一化的输出，使其在馈送到后续层之前具有均值β和标准差γ。这个过程用于减少任何给定通道从一个批次到下一个批次的分布偏移。请注意，这只是减少偏移而不是完全消除它，因为通道分布可能仍然在一个批次到下一个批次看起来完全不同，尽管它们具有相同的均值和方差。从理论上讲，并且根据经验观察，减少这种内部协变量偏移可以稳定训练并产生强大的性能增益。
- en: However, in cases where the batch size is large, the channel-wise mean and variance
    computations lead to large memory costs. Additionally, the size of the batch itself
    is very important for batch normalization, as smaller batch sizes degrade performance
    significantly due to noisy mean and variance estimates. To avoid the issues that
    come with computations along the batch dimension, *group normalization* was introduced.^([12](ch07.xhtml#idm45934165977056))
    Instead of performing a normalization along the batch dimension, group normalization
    is performed along the channel dimension and is thus unaffected by the aforementioned
    issues. Group normalization predefines a number of groups of channels and, for
    each instance, computes the mean μ and variance σ for each group of channels in
    each instance of the batch. Each set of computed β and γ parameters is used to
    normalize the set of entries from which they were computed. Additionally, similarly
    to batch normalization, an offset/mean parameter β and a scale/standard deviation
    parameter γ are separately learned for each entry set.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在批量大小较大的情况下，通道均值和方差的计算会导致较大的内存成本。此外，批量归一化对批量大小的大小非常重要，因为较小的批量大小会由于嘈杂的均值和方差估计而显著降低性能。为了避免沿批处理维度进行计算带来的问题，*组归一化*被引入。^([12](ch07.xhtml#idm45934165977056))组归一化不是沿着批处理维度进行归一化，而是沿着通道维度进行归一化，因此不受前述问题的影响。组归一化预定义了一些通道组，并且对于每个实例，在每个批次实例中计算每个通道组的均值μ和方差σ。每组计算出的β和γ参数集用于对归一化的条目集进行归一化。此外，类似于批量归一化，为每个条目集单独学习偏移/均值参数β和缩放/标准差参数γ。
- en: This is similar to another popular technique known as *layer normalization*,
    which is effectively batch normalization but across the full length of the channel
    dimension rather than the full length of the batch dimension. Note that layer
    normalization is also just a special case of group normalization, where the number
    of groups of channels is set to one. [Figure 7-16](#fodl_cnn) compares batch normalization
    with group normalization and layer normalization. The blocked-off section in each
    cube demonstrates the dimension along which normalization occurs and the group
    of entries that are normalized together. Note that we condense the standard 4D
    representation into 3D for visualization purposes.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于另一种流行的技术，称为*层归一化*，它实际上是批量归一化，但是跨越通道维度的整个长度，而不是整个批处理维度的长度。请注意，层归一化也只是组归一化的一个特例，其中通道组的数量设置为一。[图7-16](#fodl_cnn)比较了批量归一化、组归一化和层归一化。每个立方体中的被阻挡的部分展示了归一化发生的维度以及一起归一化的条目组。请注意，为了可视化目的，我们将标准的4D表示压缩为3D。
- en: '![](Images/fdl2_0716.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0716.png)'
- en: Figure 7-16\. Comparison of batch normalization, layer normalization, and group
    normalization
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-16。批量归一化、层归一化和组归一化的比较
- en: You may be wondering why techniques like group normalization and layer normalization
    are even effective. After all, it seems as though batch normalization is only
    useful due to forcing each feature (or channels in our case) to have the same
    mean and variance. For some insight, the initial paper on layer normalization
    states that the reason for normalizing the features for each instance separately
    is that “changes in the output of one layer will tend to cause highly correlated
    changes in the summed input to the next layer.” In summary, the neurons that make
    up every subsequent layer in the feed-forward network will see the same statistics
    from one training example to the next with layer normalization.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你会想知道为什么像组归一化和层归一化这样的技术会有效。毕竟，似乎批量归一化之所以有效是因为强制每个特征（或在我们的情况下是通道）具有相同的均值和方差。对于一些见解，关于层归一化的最初论文指出，为每个实例单独归一化特征的原因是“一层输出的变化往往会导致下一层的输入总和发生高度相关的变化”。总之，前馈网络中构成每个后续层的神经元将从一个训练示例到下一个训练示例看到相同的统计数据。
- en: Furthermore, why group normalization over layer normalization? In Wu et al.,
    the idea behind using group normalization is that it is less restrictive than
    layer normalization—a different distribution can be learned for each group of
    features, signifying the ability to learn potentially different levels of contribution
    and importance for different groups.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为什么选择组归一化而不是层归一化？在吴等人的研究中，使用组归一化的想法是它比层归一化更不受限制-可以为每组特征学习不同的分布，表示可以学习不同组的贡献和重要性水平。
- en: Now that we have sufficiently covered group normalization as a concept, its
    connection to prior work, and motivation for using group normalization in practice,
    we can now dive into some PyTorch code for implementing group normalization.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经充分介绍了作为概念的组归一化，它与先前工作的联系，以及在实践中使用组归一化的动机，我们现在可以深入一些PyTorch代码来实现组归一化。
- en: 'PyTorch provides a `torch.nn.GroupNorm` class to create group normalization
    layers:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了一个`torch.nn.GroupNorm`类来创建组归一化层：
- en: '[PRE7]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We need to specify only the number of groups and number of channels. Now that
    we’ve developed an enhanced toolkit for analyzing natural images with convolutional
    networks, we’ll build a classifier for tackling the CIFAR-10 challenge.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要指定组数和通道数。现在我们已经开发了一个增强的工具包，用于分析具有卷积网络的自然图像，我们将为解决CIFAR-10挑战构建一个分类器。
- en: Building a Convolutional Network for CIFAR-10
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为CIFAR-10构建卷积网络
- en: The CIFAR-10 challenge consists of 32 × 32 color images that belong to one of
    10 possible classes.^([13](ch07.xhtml#idm45934165957088)) This is a surprisingly
    hard challenge because it can be difficult for even a human to figure out what
    is in a picture. An example is shown in [Figure 7-17](#dog_from_the_cifar_100).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10挑战包括属于10个可能类别之一的32×32彩色图像。这是一个令人惊讶的难题，因为即使对于人类来说，弄清楚图片中的内容也可能很困难。一个示例显示在[图7-17](#dog_from_the_cifar_100)中。
- en: '![](Images/fdl2_0717.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0717.png)'
- en: Figure 7-17\. A dog from the CIFAR-10 dataset
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-17。CIFAR-10数据集中的一只狗
- en: In this section, we’ll build networks both with and without batch normalization
    as a basis of comparison. We increase the learning rate by 10-fold for the batch
    normalization network to take full advantage of its benefits. We’ll display code
    for only the batch normalization network here because building the vanilla convolutional
    network is similar.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建具有和不具有批量归一化的网络作为比较基础。我们将批量归一化网络的学习率增加10倍，以充分利用其优势。这里我们只显示批量归一化网络的代码，因为构建普通卷积网络类似。
- en: 'We distort random 24 × 24 crops of the input images to feed into our network
    for training. We use the example code provided by Google to do this. We’ll jump
    right into the network architecture. To start, let’s take a look at how we integrate
    batch normalization into the convolutional and fully connected layers. As expected,
    batch normalization happens to the logits before they’re fed into a nonlinearity:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我们扭曲输入图像的随机24×24裁剪，以供网络训练。我们使用谷歌提供的示例代码来实现这一点。让我们直接进入网络架构。首先，让我们看看如何将批量归一化集成到卷积和全连接层中。正如预期的那样，批量归一化发生在将逻辑输入非线性之前：
- en: '[PRE8]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Finally, we use the Adam optimizer to train our convolutional networks. After
    some amount of time training, our networks are able to achieve an impressive 92.3%
    accuracy on the CIFAR-10 task without batch normalization and 96.7% accuracy with
    batch normalization. This result actually matches (and potentially exceeds) current
    state-of-the-art research on this task. In the next section, we’ll take a closer
    look at learning and visualize how our networks perform.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用Adam优化器来训练我们的卷积网络。经过一段时间的训练，我们的网络能够在没有批量归一化的情况下实现92.3%的准确率，在批量归一化的情况下实现96.7%的准确率。这个结果实际上与（甚至可能超过）当前这项任务的最新研究相匹配。在下一节中，我们将更仔细地研究学习并可视化我们的网络的表现。
- en: Visualizing Learning in Convolutional Networks
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在卷积网络中可视化学习
- en: On a high level, the simplest thing that we can do to visualize training is
    plot the cost function and validation errors over time as training progresses.
    We can clearly demonstrate the benefits of batch normalization by comparing the
    rates of convergence between our two networks. Plots taken in the middle of the
    training process are shown in [Figure 7-18](#with_v_without_batch_normalization).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，我们可以做的最简单的事情是随着训练的进行，绘制成本函数和验证错误随时间的变化。通过比较两个网络之间的收敛速度，我们可以清楚地展示批量归一化的好处。训练过程中的图表显示在[图7-18](#with_v_without_batch_normalization)中。
- en: '![](Images/fdl2_0718.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0718.png)'
- en: Figure 7-18\. Training a convolutional network without batch normalization (left)
    versus with batch normalization (right)
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-18。训练一个没有批量归一化的卷积网络（左）与有批量归一化的卷积网络（右）
- en: Without batch normalization, cracking the 90% accuracy threshold requires over
    80,000 minibatches. On the other hand, with batch normalization, crossing the
    same threshold requires only slightly over 14,000 minibatches.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: We can also inspect the filters that our convolutional network learns in order
    to understand what the network finds important to its classification decisions.
    Convolutional layers learn hierarchical representations, so we’d hope that the
    first convolutional layer learns basic features (edges, simple curves, etc.),
    and the second convolutional layer will learn more complex features. Unfortunately,
    the second convolutional layer is difficult to interpret even if we decided to
    visualize it, so we only include the first layer filters in [Figure 7-19](#subset_of_the_learned_filters).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0719.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: Figure 7-19\. A subset of the learned filters in the first convolutional layer
    of our network
  id: totrans-161
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can make out a number of interesting features in our filters: vertical,
    horizontal, and diagonal edges, in addition to small dots or splotches of one
    color surrounded by another. We can be confident that our network is learning
    relevant features because the filters are not just noise.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: We can also try to visualize how our network has learned to cluster various
    kinds of images pictorially. To illustrate this, we take a large network that
    has been trained on the ImageNet challenge and then grab the hidden state of the
    fully connected layer just before the softmax for each image. We then take this
    high-dimensional representation for each image and use an algorithm known as *t-Distributed
    Stochastic Neighbor Embedding*, or *t-SNE*, to compress it to a 2D representation
    that we can visualize.^([14](ch07.xhtml#idm45934165928592)) We don’t cover the
    details of t-SNE here, but there are a number of publicly available software tools
    that will do it for us, including [the script](https://oreil.ly/7NA1K). We visualize
    the embeddings in [Figure 7-20](#tsne_embedding), and the results are quite spectacular.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_0720.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: Figure 7-20\. The t-SNE embedding (center) surrounded by zoomed-in subsegments
    of the embedding (periphery)^([15](ch07.xhtml#idm45934165923360))
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At first, on a high level, it seems that images that are similarly colored are
    closer together. This is interesting, but what’s even more striking is when we
    zoom into parts of the visualization, we realize that it’s more than just color.
    We realize that all pictures of boats are in one place, all pictures of humans
    are in another place, and all pictures of butterflies are in yet another location
    in the visualization. Quite clearly, convolutional networks have spectacular learning
    capabilities.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Residual Learning and Skip Connections for Very Deep Networks
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have made great progress in the field of computer vision over the past decade,
    and in this section we introduce one of the more recent advancements. Earlier,
    we discussed AlexNet, which was a breakthrough in neural methods applied to image
    classification. Since then, researchers have pushed toward deeper and deeper architectures
    in the hope of solving image classification. However, since AlexNet’s breakthrough,
    at least a few reputable studies tended to see *decreases* in training accuracy
    when naively stacking layers as compared to their shallower counterparts.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'It’s particularly interesting that the problem isn’t even overfitting (as is
    suggested by a low training accuracy and a high validation accuracy), which would
    be understandable for a network with such a large number of parameters. Additionally,
    we can easily construct a deep network by ourselves that has the exact same performance
    as its shallow counterpart: take the trained shallow network layers and simply
    stack layers that perform the identity operation. The fact that we do worse via
    a specialized optimization algorithm compared to our naive construction is quite
    astounding. The problem is that training stalls for some inexplicable reason,
    settling in a local minimum that we can’t get out of. Unfortunately, the theoretical
    justification for this is still a bit hazy.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，问题甚至不是过拟合（低训练准确率和高验证准确率所暗示的），对于具有如此多参数的网络来说，过拟合是可以理解的。此外，我们可以轻松地构建一个深度网络，其性能与其浅层对应物完全相同：只需取出训练好的浅层网络层，并简单地堆叠执行恒等操作的层。我们通过专门的优化算法表现更差，与我们的朴素构建相比，这是相当令人震惊的。问题在于由于某种无法解释的原因，训练停滞在一个我们无法摆脱的局部最小值。不幸的是，对于这一点的理论解释仍然有些模糊。
- en: In 2015, He et al.^([16](ch07.xhtml#idm45934165909136)) introduced the ResNet34
    architecture, a deep architecture that surpassed all of its peers in major image
    classification competitions. With a version that consisted of over 30 trainable
    layers, He et al. redefined how we train deep computer vision architectures. In
    particular, their contribution was the introduction of what we now call *skip
    connections*, which add the feature vector obtained from a layer to the feature
    vector obtained one or two layers after the current layer. More precisely, let’s
    say we are midway through the network so far and our original input *x* has been
    converted to some intermediate representation *x’*. The skip connection would
    take *x’* and add it to the result of the next layer, *F(x')*, before passing
    the representation on to the following layer *G.* So instead of seeing *F(x')*,
    *G* sees *F(x') + x’*. Note that the skip connection does not need to add the
    current representation to the result of *F*. As represented in [Figure 7-21](#the_skip_connection_here_skips_f_and_g),
    we could also add *x’* to the result of *G,* so the next layer *H* sees *G(F(x'))
    + x’* instead of just *G(F(x')).*
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年，何等人^([16](ch07.xhtml#idm45934165909136))引入了ResNet34架构，这是一个深度架构，在主要图像分类竞赛中超越了所有同行。何等人等人重新定义了我们如何训练深度计算机视觉架构，他们的贡献是引入了我们现在称之为*跳跃连接*的概念，即将从一层获得的特征向量添加到当前层之后的一层或两层获得的特征向量中。更确切地说，假设我们到目前为止已经通过网络的一半，我们的原始输入*x*已经转换为一些中间表示*x'*。跳跃连接将*x'*添加到下一层的结果*F(x')*中，然后将表示传递给下一层*G*。因此，G看到的不是*F(x')*，而是*F(x')
    + x'*。请注意，跳跃连接不需要将当前表示添加到*F*的结果中。如[图7-21](#the_skip_connection_here_skips_f_and_g)所示，我们还可以将*x'*添加到*G*的结果中，因此下一层*H*看到的是*G(F(x'))
    + x'*，而不仅仅是*G(F(x'))*。
- en: '![](Images/fdl2_0721.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0721.png)'
- en: Figure 7-21\. The skip connection here skips F and G, summing the input to F
    with the output of G, which comprises the input to H
  id: totrans-172
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-21。这里的跳跃连接跳过了F和G，将输入到F的输入与G的输出相加，这构成了传递给H的输入。
- en: Note
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: These skip connections are just the identity operation, so they add no additional
    parameters to train. Additionally, since the skip connection is the identity operation,
    it must be the case that *x’* and *G(F(x'))*, in the example where the skip connection
    skips two layers, must be the same dimension. If this were not the case, we would
    not be able to add the two feature vectors. This does place a constraint on the
    network architecture, but we hope to construct a deep network anyway, and this
    approach lends itself well to such networks since we wouldn’t want the dimensionality
    to decrease too rapidly (recall the discussion on padding).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这些跳跃连接只是恒等操作，因此它们不会增加额外的参数进行训练。此外，由于跳跃连接是恒等操作，必须是在跳过两层的情况下，*x'*和*G(F(x'))*必须具有相同的维度。如果不是这种情况，我们将无法将这两个特征向量相加。这对网络架构施加了一定的约束，但我们仍希望构建一个深度网络，这种方法非常适合这样的网络，因为我们不希望维度下降得太快（回想一下关于填充的讨论）。
- en: 'It’s natural to ask why skip connections work so well. After all, it does seem
    like a pretty simple modification to the plain deep network architecture. Let’s
    think back to the original motivation: through experimentation, researchers had
    noticed a degradation in performance as networks got deeper and deeper. However,
    it must be the case that deeper networks are able to perform at least as well
    as their shallower counterparts, since we can construct a naive solution where
    the additional layers are the identity mapping. It’s also important to note that
    the representations learned by shallower counterparts such as AlexNet are quite
    good, as they achieved state-of-the-art performance just a couple of years prior.
    If we make the assumption that representations at downstream layers in deep networks
    are only going to be slightly different from one layer to the next, which is reasonable
    due to the fact that shallower networks still can learn very good representations,
    it would instead make sense to optimize the difference between representations
    (which should be close to zero for all weights) rather than attempt to achieve
    something close to the identity operation, which is a very specific and imbalanced
    weight setting.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 自然而然地会问为什么跳跃连接效果如此好。毕竟，这似乎是对普通深度网络架构的一个相当简单的修改。让我们回想一下最初的动机：通过实验，研究人员注意到随着网络变得越来越深，性能会下降。然而，更深的网络至少应该能够像更浅的网络一样表现良好，因为我们可以构建一个简单的解决方案，其中额外的层是恒等映射。还要注意的是，像AlexNet这样的更浅的网络学习到的表示是相当好的，因为它们在几年前就实现了最先进的性能。如果我们假设深度网络中下游层的表示只会在一层到另一层之间略有不同，这是合理的，因为更浅的网络仍然可以学习到非常好的表示，那么优化表示之间的差异（对于所有权重应该接近零）而不是尝试实现接近恒等操作的目标，这是一个非常具体和不平衡的权重设置，反而更有意义。
- en: That’s where residual connections come in. The downstream layers of the neural
    network, such as *F* and *G*, are learning precisely this difference between representations
    and then adding the difference back to the incoming representation *x’* to achieve
    an only slightly different representation *G(F(x')) + x’*. This is in contrast
    with the traditional feed-forward neural network paradigm, which would attempt
    to learn a weight setting that is approximately close to identity for *F* and
    *G*, which seems like a much harder problem. In the next section, we will put
    our knowledge together to build a residual network.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是残差连接的作用。神经网络的下游层，如*F*和*G*，正在精确地学习表示之间的差异，然后将差异添加回到传入的表示*x'*中，以实现一个略有不同的表示*G(F(x'))
    + x'*。这与传统的前馈神经网络范式形成对比，后者试图学习一个对于*F*和*G*近似接近恒等的权重设置，这似乎是一个更困难的问题。在下一节中，我们将整合我们的知识来构建一个残差网络。
- en: Building a Residual Network with Superhuman Vision
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个具有超人视觉的残差网络
- en: In the previous section, we discussed residual connections and how they allow
    for improved gradient flow through deep neural networks. In this section, we will
    replicate the implementation of a neural network with residual connections, specifically
    the ResNet34 architecture from He et al.’s original.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们讨论了残差连接以及它们如何通过深度神经网络实现改进的梯度流。在本节中，我们将复制一个具有残差连接的神经网络的实现，具体来说是He等人原始的ResNet34架构。
- en: 'PyTorch’s Torchvision library provides constructors for many commonly used
    resnets. We can use it to create a ResNet34 model:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch的Torchvision库提供了许多常用ResNet的构造函数。我们可以使用它来创建一个ResNet34模型：
- en: '[PRE9]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let’s see how `resnet34` creates a residual network.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看`resnet34`如何创建一个残差网络。
- en: 'Most versions of residual networks consist of the following structure:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数版本的残差网络由以下结构组成：
- en: Convolutional block (CONV->BN->ReLU->MAXPOOL)
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积块（CONV->BN->ReLU->MAXPOOL）
- en: Four residual layers
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 四个残差层
- en: A classifier block with average pooling and a linear layer
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带有平均池化和线性层的分类器块
- en: 'Each residual layer consists of one or more residual blocks. For example, the
    layers *F* and *G* from [Figure 7-21](#the_skip_connection_here_skips_f_and_g)
    form a residual block. Here is the PyTorch code for a simplified implementation
    of a residual block for ResNet34:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 每个残差层由一个或多个残差块组成。例如，[图7-21](#the_skip_connection_here_skips_f_and_g)中的*F*和*G*层形成一个残差块。以下是一个简化的ResNet34残差块的PyTorch代码：
- en: '[PRE10]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Similarly to the previous section, each residual block in the ResNet34 architecture
    consists of two convolutional layers. The `downsample` argument allows for an
    optional downsampler function. The purpose of downsampling is to match the dimensions
    of the input with the output of the residual block, if the two are of different
    dimensions.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一节类似，ResNet34架构中的每个残差块由两个卷积层组成。`downsample`参数允许使用可选的下采样函数。下采样的目的是匹配输入的维度与残差块的输出，如果两者的维度不同。
- en: 'The following is an example of a downsampler that matches the number of channels
    of the input to that of the output of the residual block. Note that this downsampler
    does not change the size of each feature map given the `kernel_size` is 1 and
    the `stride` is also only 1, and affects the dimensions only by increasing the
    number of feature maps from 64 to 128:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例的下采样器，将输入的通道数匹配到残差块的输出通道数。请注意，这个下采样器不会改变每个特征图的大小，因为`kernel_size`为1，`stride`也只有1，它只通过增加特征图的数量从64增加到128来影响维度：
- en: '[PRE11]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The number of residual blocks for each of the four residual layers in ResNet34
    is defined as [3, 4, 6, 3], respectively. The ResNet34 architecture is named this
    way because it has 33 convolutional layers and 1 fully connected layer at the
    end, which serves as the predictor portion of the network. The 33 convolutional
    layers are arranged in four sections that have 3, 4, 6, and 3 residual blocks,
    in that order. To get to the total of 33, there is a single convolutional layer
    at the beginning that operates on the original image input, which is assumed to
    have 3 channels.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet34中的四个残差层中每个残差块的数量分别定义为[3, 4, 6, 3]。ResNet34架构之所以被命名为这样，是因为它有33个卷积层和1个最后的全连接层，作为网络的预测部分。这33个卷积层分为四个部分，分别有3、4、6和3个残差块。为了达到总共33个，一开始有一个单独的卷积层作用于原始图像输入，假设有3个通道。
- en: 'The following PyTorch code initializes each of these components, closely modeled
    after the official PyTorch implementation of the various versions presented in
    the original paper. The first component, up to the max pool, operates on the original
    input, and each of the following components requires downsampling only between
    components. This is because, within each component, the input and output of each
    `ResidualBlock` are of the same dimension. Although we won’t show it explicitly
    in this section, the combination of a `kernel_size` of 3, `stride` of 1, and `padding`
    of 1 ensures that the size of each feature map stays constant from beginning to
    end. Additionally, given the number of feature maps stays constant within each
    component, all dimensions end up remaining the same:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 以下PyTorch代码初始化了每个组件，紧密模仿了原始论文中官方PyTorch实现的各个版本。第一个组件，直到最大池，作用于原始输入，接下来的每个组件之间只需要降采样。这是因为在每个组件内，每个`ResidualBlock`的输入和输出维度相同。虽然我们在本节中不会明确展示，但`kernel_size`为3，`stride`为1，`padding`为1的组合确保每个特征图的大小从头到尾保持不变。此外，由于每个组件内特征图的数量保持不变，所有维度最终保持不变：
- en: '[PRE12]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the next section, we will present some of the latest advancements in computer
    vision regarding neural style transfer.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍关于神经风格转移的计算机视觉领域的一些最新进展。
- en: Leveraging Convolutional Filters to Replicate Artistic Styles
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用卷积滤波器复制艺术风格
- en: Over the past couple of years, we’ve also developed algorithms that leverage
    convolutional networks in much more creative ways. One of these algorithms is
    called *neural style*.^([17](ch07.xhtml#idm45934168705312)) The goal of neural
    style is to be able to take an arbitrary photograph and render it as if it were
    painted in the style of a famous artist. This seems like a daunting task, and
    it’s not exactly clear how we might approach this problem if we didn’t have a
    convolutional network. However, it turns out that clever manipulation of convolutional
    filters can produce spectacular results on this problem.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，我们还开发了利用卷积网络以更有创意方式的算法。其中一个算法被称为*神经风格*。^([17](ch07.xhtml#idm45934168705312))
    神经风格的目标是能够将任意照片呈现为以著名艺术家的风格绘制的样子。这似乎是一项艰巨的任务，如果没有卷积网络，我们不太清楚如何解决这个问题。然而，聪明地操作卷积滤波器可以在这个问题上产生惊人的结果。
- en: Let’s take a pretrained convolutional network. We’re dealing with three images.
    The first two are the source of content ***p*** and the source of style ***a***.
    The third image is the generated image ***x***. Our goal is to derive an error
    function that we can backpropagate that, when minimized, will perfectly combine
    the content of the desired photograph and the style of the desired artwork.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们拿一个预训练的卷积网络。我们处理三幅图像。前两幅是内容源***p***和风格源***a***。第三幅图像是生成的图像***x***。我们的目标是推导一个错误函数，我们可以反向传播，当最小化时，将完美地结合所需照片的内容和所需艺术作品的风格。
- en: 'We start with content first. If a layer in the network has <math alttext="k
    Subscript l"><msub><mi>k</mi> <mi>l</mi></msub></math> filters, then it produces
    a total of <math alttext="k Subscript l"><msub><mi>k</mi> <mi>l</mi></msub></math>
    feature maps. Let’s call the size of each feature map <math alttext="m Subscript
    l"><msub><mi>m</mi> <mi>l</mi></msub></math> , the height times the width of the
    feature map. This means that the activations in all the feature maps of this layer
    can be stored in a matrix ***F***^((*l*)) of size <math alttext="k Subscript l
    Baseline times m Subscript l"><mrow><msub><mi>k</mi> <mi>l</mi></msub> <mo>×</mo>
    <msub><mi>m</mi> <mi>l</mi></msub></mrow></math> . We can also represent all the
    activations of the photograph in a matrix ***P***^((*l*)) and all the activations
    of the generated image in the matrix ***X***^((*l*)). We use the `relu4_2` of
    the original VGGNet:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们先从内容开始。如果网络中的一层有<math alttext="k Subscript l"><msub><mi>k</mi> <mi>l</mi></msub></math>个滤波器，那么它会产生<math
    alttext="k Subscript l"><msub><mi>k</mi> <mi>l</mi></msub></math>个特征图。让我们称每个特征图的大小为<math
    alttext="m Subscript l"><msub><mi>m</mi> <mi>l</mi></msub></math>，即特征图的高乘以宽。这意味着该层所有特征图中的激活可以存储在大小为<math
    alttext="k Subscript l Baseline times m Subscript l"><mrow><msub><mi>k</mi> <mi>l</mi></msub>
    <mo>×</mo> <msub><mi>m</mi> <mi>l</mi></msub></mrow></math>的矩阵***F***^((*l*))中。我们还可以用矩阵***P***^((*l*))表示照片的所有激活，用矩阵***X***^((*l*))表示生成图像的所有激活。我们使用原始VGGNet的`relu4_2`：
- en: '*E*[content](***p***, ***x***) = ∑[ij](***P***[ij]^((*l*)) – ***X***[ij]^((*l*)))²'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '*E*[content](***p***, ***x***) = ∑[ij](***P***[ij]^((*l*)) – ***X***[ij]^((*l*)))²'
- en: 'Now we can try tackling style. To do this we construct a matrix known as the
    *Gram matrix*, which represents correlations between feature maps in a given layer.
    The correlations represent the texture and feel that is common among all features,
    irrespective of which features we’re looking at. Constructing the Gram matrix,
    which is of size <math alttext="k Subscript l Baseline times k Subscript l"><mrow><msub><mi>k</mi>
    <mi>l</mi></msub> <mo>×</mo> <msub><mi>k</mi> <mi>l</mi></msub></mrow></math>
    , for a given image, is done as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以尝试处理风格。为此，我们构建一个被称为*Gram矩阵*的矩阵，它代表了给定层中特征图之间的相关性。这些相关性代表了纹理和感觉，这些纹理和感觉在所有特征中都是共同的，无论我们正在查看哪些特征。构建Gram矩阵，其大小为<math
    alttext="k下标l乘以k下标l"><mrow><msub><mi>k</mi> <mi>l</mi></msub> <mo>×</mo> <msub><mi>k</mi>
    <mi>l</mi></msub></mrow></math>，对于给定的图像，可以按照以下步骤完成：
- en: '**G**^((*l*))[ij] = ∑[c = 0]^(m[l]) **F**^((*l*))[*ic*] **F**^((*l*))[*jc*]'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**G**^((*l*))[ij] = ∑[c = 0]^(m[l]) **F**^((*l*))[*ic*] **F**^((*l*))[*jc*]'
- en: 'We can compute the Gram matrices for both the artwork in matrix ***A***^((*l*))
    and the generated image in ***G***^((*l*)). We can then represent the error function
    as:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算艺术品矩阵***A***^((*l*))和生成图像矩阵***G***^((*l*))的Gram矩阵。然后我们可以将误差函数表示为：
- en: <math alttext="upper E Subscript s t y l e Baseline left-parenthesis bold a
    comma bold x right-parenthesis equals StartFraction 1 Over 4 k Subscript l Superscript
    2 Baseline m Subscript l Superscript 2 Baseline EndFraction sigma-summation Underscript
    l equals 1 Overscript upper L Endscripts sigma-summation Underscript i j Endscripts
    StartFraction 1 Over upper L EndFraction left-parenthesis upper A Subscript i
    j Superscript left-parenthesis l right-parenthesis Baseline minus upper G Subscript
    i j Superscript left-parenthesis l right-parenthesis Baseline right-parenthesis
    squared"><mrow><msub><mi>E</mi> <mrow><mi>s</mi><mi>t</mi><mi>y</mi><mi>l</mi><mi>e</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>𝐚</mi> <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mn>1</mn> <mrow><mn>4</mn><msubsup><mi>k</mi> <mi>l</mi> <mn>2</mn></msubsup>
    <msubsup><mi>m</mi> <mi>l</mi> <mn>2</mn></msubsup></mrow></mfrac> <msubsup><mo>∑</mo>
    <mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow> <mi>L</mi></msubsup> <msub><mo>∑</mo>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mfrac><mn>1</mn> <mi>L</mi></mfrac>
    <msup><mfenced separators="" open="(" close=")"><msubsup><mi>A</mi> <mrow><mi>i</mi><mi>j</mi></mrow>
    <mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup> <mo>-</mo><msubsup><mi>G</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow> <mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup></mfenced>
    <mn>2</mn></msup></mrow></math>
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper E Subscript s t y l e Baseline left-parenthesis bold a
    comma bold x right-parenthesis equals StartFraction 1 Over 4 k Subscript l Superscript
    2 Baseline m Subscript l Superscript 2 Baseline EndFraction sigma-summation Underscript
    l equals 1 Overscript upper L Endscripts sigma-summation Underscript i j Endscripts
    StartFraction 1 Over upper L EndFraction left-parenthesis upper A Subscript i
    j Superscript left-parenthesis l right-parenthesis Baseline minus upper G Subscript
    i j Superscript left-parenthesis l right-parenthesis Baseline right-parenthesis
    squared"><mrow><msub><mi>E</mi> <mrow><mi>s</mi><mi>t</mi><mi>y</mi><mi>l</mi><mi>e</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>𝐚</mi> <mo>,</mo> <mi>𝐱</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mfrac><mn>1</mn> <mrow><mn>4</mn><msubsup><mi>k</mi> <mi>l</mi> <mn>2</mn></msubsup>
    <msubsup><mi>m</mi> <mi>l</mi> <mn>2</mn></msubsup></mrow></mfrac> <msubsup><mo>∑</mo>
    <mrow><mi>l</mi><mo>=</mo><mn>1</mn></mrow> <mi>L</mi></msubsup> <msub><mo>∑</mo>
    <mrow><mi>i</mi><mi>j</mi></mrow></msub> <mfrac><mn>1</mn> <mi>L</mi></mfrac>
    <msup><mfenced separators="" open="(" close=")"><msubsup><mi>A</mi> <mrow><mi>i</mi><mi>j</mi></mrow>
    <mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup> <mo>-</mo><msubsup><mi>G</mi>
    <mrow><mi>i</mi><mi>j</mi></mrow> <mrow><mo>(</mo><mi>l</mi><mo>)</mo></mrow></msubsup></mfenced>
    <mn>2</mn></msup></mrow></math>
- en: Here, we weight each squared difference equally (dividing by the number of layers
    we want to include in our style reconstruction). Specifically, we use the `relu1_1`,
    `relu2_1`, `relu3_1`, `relu4_1`, and `relu5_1` layers of the original VGGNet.
    We omit a full discussion of the TensorFlow code for brevity, but the results,
    as shown in [Figure 7-22](#result_of_mixing_the_rain_princess), are again quite
    spectacular. We mix a photograph of the iconic MIT dome and Leonid Afremov’s *Rain
    Princess*.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们平等地权衡每个平方差（通过除以我们想要包含在风格重建中的层数）。具体来说，我们使用原始VGGNet的`relu1_1`、`relu2_1`、`relu3_1`、`relu4_1`和`relu5_1`层。为了简洁起见，我们省略了有关TensorFlow代码的完整讨论，但结果如[图7-22](#result_of_mixing_the_rain_princess)所示，再次非常壮观。我们混合了麻省理工学院标志性圆顶和列昂尼德·阿弗莫夫的*雨公主*的照片。
- en: '![](Images/fdl2_0722.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/fdl2_0722.png)'
- en: Figure 7-22\. The result of mixing the Rain Princess with a photograph of the
    MIT dome^([18](ch07.xhtml#idm45934168662448))
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图7-22。将雨公主与麻省理工学院圆顶的照片混合的结果^([18](ch07.xhtml#idm45934168662448))
- en: Learning Convolutional Filters for Other Problem Domains
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习卷积滤波器用于其他问题领域
- en: Although our examples in this chapter focus on image recognition, there are
    several other problem domains in which convolutional networks are useful. A natural
    extension of image analysis is video analysis. In fact, using five-dimensional
    tensors (including time as a dimension) and applying three-dimensional convolutions
    is an easy way to extend the convolutional paradigm to video.^([19](ch07.xhtml#idm45934168658112))
    Convolutional filters have also been successfully used to analyze audiograms.^([20](ch07.xhtml#idm45934168655488))
    In these applications, a convolutional network slides over an audiogram input
    to predict phonemes on the other side.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章中的示例侧重于图像识别，但卷积网络在几个其他问题领域中也很有用。图像分析的自然延伸是视频分析。事实上，使用五维张量（包括时间作为一个维度）并应用三维卷积是将卷积范式扩展到视频的一种简单方法。^([19](ch07.xhtml#idm45934168658112))
    卷积滤波器还成功地用于分析听力图。^([20](ch07.xhtml#idm45934168655488)) 在这些应用中，卷积网络在听力图输入上滑动，以预测另一侧的音素。
- en: Less intuitively, convolutional networks have also found some use in natural
    language processing. We’ll see some examples of this in later chapters. More exotic
    uses of convolutional networks include teaching algorithms to play board games,
    and analyzing biological molecules for drug discovery. We’ll also discuss both
    of these examples in later chapters of this book.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 不太直观的是，卷积网络还在自然语言处理中找到了一些用途。我们将在后面的章节中看到一些例子。卷积网络的更奇特用途包括教授算法玩棋盘游戏，以及分析生物分子以进行药物发现。我们还将在本书的后面章节讨论这两个例子。
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned how to build neural networks that analyze images.
    We developed the concept of a convolution, and leveraged this idea to create tractable
    networks that can analyze both simple and more complex natural images. We built
    several of these convolutional networks in TensorFlow and leveraged various image
    processing pipelines and batch normalization to make training our networks faster
    and more robust. Finally, we visualized the learning of convolutional networks
    and explored other interesting applications of the technology.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何构建分析图像的神经网络。我们发展了卷积的概念，并利用这个想法创建了可分析简单和更复杂自然图像的可操作网络。我们在TensorFlow中构建了几个这样的卷积网络，并利用各种图像处理流水线和批量归一化使训练网络更快速和更稳健。最后，我们可视化了卷积网络的学习过程，并探索了技术的其他有趣应用。
- en: Images were easy to analyze because we were able to come up with effective ways
    to represent them as tensors. In other situations (e.g., natural language), it’s
    less clear how one might represent our input data as tensors. To tackle this problem
    as a stepping stone to new deep learning models, we’ll develop some key concepts
    in vector embeddings and representation learning in the next chapter.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们能够想出有效的方法将图像表示为张量，因此图像分析变得容易。在其他情况下（例如自然语言），如何将输入数据表示为张量并不太清楚。为了作为新深度学习模型的一个基石来解决这个问题，我们将在下一章中开发一些关键概念，包括向量嵌入和表示学习。
- en: '^([1](ch07.xhtml#idm45934168179504-marker)) Hubel, David H., and Torsten N.
    Wiesel. “Receptive Fields and Functional Architecture of Monkey Striate Cortex.”
    *The Journal of Physiology* 195.1 (1968): 215-243.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '^([1](ch07.xhtml#idm45934168179504-marker)) Hubel, David H., and Torsten N.
    Wiesel. “Receptive Fields and Functional Architecture of Monkey Striate Cortex.”
    *The Journal of Physiology* 195.1 (1968): 215-243.'
- en: ^([2](ch07.xhtml#idm45934168177536-marker)) Cohen, Adolph I. “Rods and Cones.”
    *Physiology of Photoreceptor Organs*. Springer Berlin Heidelberg, 1972\. 63-110.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch07.xhtml#idm45934168177536-marker)) Cohen, Adolph I. “Rods and Cones.”
    *Physiology of Photoreceptor Organs*. Springer Berlin Heidelberg, 1972\. 63-110.
- en: ^([3](ch07.xhtml#idm45934168169648-marker)) Viola, Paul, and Michael Jones.
    “Rapid Object Detection using a Boosted Cascade of Simple Features.” Computer
    Vision and Pattern Recognition, 2001\. CVPR 2001\. *Proceedings of the 2001 IEEE
    Computer Society Conference* on. Vol. 1\. IEEE, 2001.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch07.xhtml#idm45934168169648-marker)) Viola, Paul, and Michael Jones.
    “Rapid Object Detection using a Boosted Cascade of Simple Features.” Computer
    Vision and Pattern Recognition, 2001\. CVPR 2001\. *Proceedings of the 2001 IEEE
    Computer Society Conference* on. Vol. 1\. IEEE, 2001.
- en: '^([4](ch07.xhtml#idm45934168149488-marker)) Deng, Jia, et al. “ImageNet: A
    Large-Scale Hierarchical Image Database.” *Computer Vision and Pattern Recognition*,
    2009\. CVPR 2009\. IEEE Conference. IEEE, 2009.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '^([4](ch07.xhtml#idm45934168149488-marker)) Deng, Jia, et al. “ImageNet: A
    Large-Scale Hierarchical Image Database.” *Computer Vision and Pattern Recognition*,
    2009\. CVPR 2009\. IEEE Conference. IEEE, 2009.'
- en: ^([5](ch07.xhtml#idm45934168146976-marker)) Perronnin, Florent, Jorge Sénchez,
    and Yan Liu Xerox. “Large-Scale Image Categorization with Explicit Data Embedding.”
    *Computer Vision and Pattern Recognition* (CVPR), 2010 IEEE Conference. IEEE,
    2010.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch07.xhtml#idm45934168146976-marker)) Perronnin, Florent, Jorge Sénchez,
    and Yan Liu Xerox. “Large-Scale Image Categorization with Explicit Data Embedding.”
    *Computer Vision and Pattern Recognition* (CVPR), 2010 IEEE Conference. IEEE,
    2010.
- en: ^([6](ch07.xhtml#idm45934168141808-marker)) Krizhevsky, Alex, Ilya Sutskever,
    and Geoffrey E. Hinton. “ImageNet Classification with Deep Convolutional Neural
    Networks.” *Advances in Neural Information Processing Systems*. 2012.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch07.xhtml#idm45934168141808-marker)) Krizhevsky, Alex, Ilya Sutskever,
    and Geoffrey E. Hinton. “ImageNet Classification with Deep Convolutional Neural
    Networks.” *Advances in Neural Information Processing Systems*. 2012.
- en: ^([7](ch07.xhtml#idm45934168123728-marker)) LeCun, Yann, et al. “Handwritten
    Digit Recognition with a Back-Propagation Network.” *Advances in Neural Information
    Processing Systems*. 1990.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch07.xhtml#idm45934168123728-marker)) LeCun, Yann, et al. “Handwritten
    Digit Recognition with a Back-Propagation Network.” *Advances in Neural Information
    Processing Systems*. 1990.
- en: '^([8](ch07.xhtml#idm45934168116048-marker)) Hubel, David H., and Torsten N.
    Wiesel. “Receptive Fields of Single Neurones in the Cat’s Striate Cortex.” *The
    Journal of Physiology* 148.3 (1959): 574-591.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '^([8](ch07.xhtml#idm45934168116048-marker)) Hubel, David H., and Torsten N.
    Wiesel. “Receptive Fields of Single Neurones in the Cat’s Striate Cortex.” *The
    Journal of Physiology* 148.3 (1959): 574-591.'
- en: ^([9](ch07.xhtml#idm45934163023728-marker)) Graham, Benjamin. “Fractional Max-Pooling.”
    *arXiv Preprint arXiv*:1412.6071 (2014).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch07.xhtml#idm45934163023728-marker)) Graham, Benjamin. “Fractional Max-Pooling.”
    *arXiv Preprint arXiv*:1412.6071 (2014).
- en: ^([10](ch07.xhtml#idm45934163009328-marker)) Simonyan, Karen, and Andrew Zisserman.
    “Very Deep Convolutional Networks for Large-Scale Image Recognition.” *arXiv Preprint
    arXiv*:1409.1556 (2014).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch07.xhtml#idm45934163009328-marker)) Simonyan, Karen, and Andrew Zisserman.
    “Very Deep Convolutional Networks for Large-Scale Image Recognition.” *arXiv Preprint
    arXiv*:1409.1556 (2014).
- en: '^([11](ch07.xhtml#idm45934162940752-marker)) S. Ioffe, C. Szegedy. “Batch Normalization:
    Accelerating Deep Network Training by Reducing Internal Covariate Shift.” *arXiv
    Preprint arXiv*:1502.03167\. 2015.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '^([11](ch07.xhtml#idm45934162940752-marker)) S. Ioffe, C. Szegedy. “Batch Normalization:
    Accelerating Deep Network Training by Reducing Internal Covariate Shift.” *arXiv
    Preprint arXiv*:1502.03167\. 2015.'
- en: ^([12](ch07.xhtml#idm45934165977056-marker)) Wu et. al. “Group Normalization.”
    2018\. *https://arxiv.org/abs/1803.08494*.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch07.xhtml#idm45934165977056-marker)) Wu et. al. “Group Normalization.”
    2018\. *https://arxiv.org/abs/1803.08494*.
- en: ^([13](ch07.xhtml#idm45934165957088-marker)) Krizhevsky, Alex, and Geoffrey
    Hinton. “Learning Multiple Layers of Features from Tiny Images.” University of
    Toronto (2009).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch07.xhtml#idm45934165957088-marker)) Krizhevsky, Alex, and Geoffrey
    Hinton. “Learning Multiple Layers of Features from Tiny Images.” University of
    Toronto (2009).
- en: '^([14](ch07.xhtml#idm45934165928592-marker)) Maaten, Laurens van der, and Geoffrey
    Hinton. “Visualizing Data Using t-SNE.” *Journal of Machine Learning Research*
    9\. Nov (2008): 2579-2605.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '^([14](ch07.xhtml#idm45934165928592-marker)) Maaten, Laurens van der, and Geoffrey
    Hinton. “Visualizing Data Using t-SNE.” *Journal of Machine Learning Research*
    9\. Nov (2008): 2579-2605.'
- en: '^([15](ch07.xhtml#idm45934165923360-marker)) Image credit: Andrej Karpathy.
    *http://cs.stanford.edu/people/karpathy/cnnembed*.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch07.xhtml#idm45934165923360-marker)) 图像来源：Andrej Karpathy。*http://cs.stanford.edu/people/karpathy/cnnembed*。
- en: ^([16](ch07.xhtml#idm45934165909136-marker)) He et. al. “Deep Residual Learning
    for Image Recognition.” *arXiv Preprint arXiv*:1512.03385\. 2015.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch07.xhtml#idm45934165909136-marker)) He et. al. “Deep Residual Learning
    for Image Recognition.” *arXiv Preprint arXiv*:1512.03385\. 2015.
- en: ^([17](ch07.xhtml#idm45934168705312-marker)) Gatys, Leon A., Alexander S. Ecker,
    and Matthias Bethge. “A Neural Algorithm of Artistic Style.” *arXiv Preprint arXiv*:1508.06576
    (2015).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch07.xhtml#idm45934168705312-marker)) Gatys, Leon A., Alexander S. Ecker,
    and Matthias Bethge. “A Neural Algorithm of Artistic Style.” *arXiv Preprint arXiv*:1508.06576
    (2015).
- en: '^([18](ch07.xhtml#idm45934168662448-marker)) Image credit: Anish Athalye.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ^([18](ch07.xhtml#idm45934168662448-marker)) 图像来源：Anish Athalye。
- en: ^([19](ch07.xhtml#idm45934168658112-marker)) Karpathy, Andrej, et al. “Large-scale
    Video Classification with Convolutional Neural Networks.” *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*. 2014.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ^([19](ch07.xhtml#idm45934168658112-marker)) Karpathy, Andrej, et al. “Large-scale
    Video Classification with Convolutional Neural Networks.” *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition*. 2014.
- en: ^([20](ch07.xhtml#idm45934168655488-marker)) Abdel-Hamid, Ossama, et al. “Applying
    Convolutional Neural Networks Concepts to Hybrid NN-HMM Model for Speech Recognition.”
    IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),
    Kyoto, 2012, pp. 4277-4280.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ^([20](ch07.xhtml#idm45934168655488-marker)) Abdel-Hamid, Ossama, et al. “Applying
    Convolutional Neural Networks Concepts to Hybrid NN-HMM Model for Speech Recognition.”
    IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),
    Kyoto, 2012, pp. 4277-4280.
