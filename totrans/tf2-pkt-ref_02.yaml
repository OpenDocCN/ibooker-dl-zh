- en: Chapter 2\. Data Storage and Ingestion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2章 数据存储和摄入
- en: To envision how to set up an ML model to solve a problem, you have to start
    thinking about data structure patterns. In this chapter, we’ll look at some general
    patterns in storage, data formats, and data ingestion. Typically, once you understand
    your business problem and set it up as a data science problem, you have to think
    about how to get the data into a format or structure that your model training
    process can use. Data ingestion during the training process is fundamentally a
    data transformation pipeline. Without this transformation, you won’t be able to
    deliver and serve the model in an enterprise-driven or use-case-driven setting;
    it would remain nothing more than an exploration tool and would not be able to
    scale to handle large amounts of data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 要想设想如何设置一个ML模型来解决问题，您必须开始考虑数据结构模式。在本章中，我们将看一些存储、数据格式和数据摄入中的一般模式。通常，一旦您理解了您的业务问题并将其设置为数据科学问题，您必须考虑如何将数据转换为模型训练过程可以使用的格式或结构。在训练过程中进行数据摄入基本上是一个数据转换管道。没有这种转换，您将无法在企业驱动或用例驱动的环境中交付和提供模型；它将仍然只是一个探索工具，无法扩展以处理大量数据。
- en: 'This chapter will show you how to design a data ingestion pipeline for two
    common data structures: tables and images. You will learn how to make the pipeline
    scalable by using TensorFlow’s APIs.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将向您展示如何为两种常见的数据结构（表格和图像）设计数据摄入管道。您将学习如何使用TensorFlow的API使管道可扩展。
- en: '*Data streaming* is the means by which the data is ingested in small batches
    by the model for training. Data streaming in Python is not a new concept. However,
    grasping it is fundamental to understanding how the more advanced APIs in TensorFlow
    work. Thus, this chapter will start with Python generators. Then we’ll look at
    how tabular data is stored, including how to indicate and track features and labels.
    We’ll then move to designing your data structure, and finish by discussing how
    to ingest data to your model for training and how to stream tabular data. The
    rest of the chapter covers how to organize image data for image classification
    and how to stream image data.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据流式处理*是模型在训练过程中以小批量摄入数据的方式。在Python中进行数据流式处理并不是一个新概念。然而，理解它对于理解TensorFlow中更高级API的工作方式是基本的。因此，本章将从Python生成器开始。然后我们将看一下如何存储表格数据，包括如何指示和跟踪特征和标签。然后我们将转向设计您的数据结构，并讨论如何将数据摄入到模型进行训练以及如何流式传输表格数据。本章的其余部分涵盖了如何为图像分类组织图像数据以及如何流式传输图像数据。'
- en: Streaming Data with Python Generators
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Python生成器进行数据流式处理
- en: There are times when the Python runtime’s memory is not big enough to handle
    loading the dataset in its entirety. When this happens, the recommended practice
    is to load the data in small batches. Therefore, the data is streamed into the
    model during the training process.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候Python运行时的内存不足以处理整个数据集的加载。当发生这种情况时，建议的做法是将数据分批加载。因此，在训练过程中，数据被流式传输到模型中。
- en: Sending data in small batches has many other advantages as well. One is that
    a gradient descent algorithm is applied to each batch to calculate the error (that
    is, the difference between the model output and the ground truth) and to gradually
    update the model’s weights and biases to make this error as small as possible.
    This lets us parallelize the gradient calculation, since the error calculation
    (also known as *loss calculation*) of one batch does not depend on the other.
    This is known as *mini-batch gradient descent*. At the end of each epoch, after
    a full training dataset has gone through the model, gradients from all batches
    are summed and weights are updated. Then, training starts again for the next epoch,
    with the newly updated weights and biases, and the error is calculated. This process
    repeats according to a user-defined parameter, which is known as *number of epochs
    for training*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 以小批量发送数据还有许多其他优点。其中之一是对每个批次应用梯度下降算法来计算误差（即模型输出与实际值之间的差异），并逐渐更新模型的权重和偏差以使这个误差尽可能小。这让我们可以并行计算梯度，因为一个批次的误差计算（也称为*损失计算*）不依赖于其他批次。这被称为*小批量梯度下降*。在每个时代结束时，当整个训练数据集经过模型时，所有批次的梯度被求和并更新权重。然后，使用新更新的权重和偏差重新开始训练下一个时代，并计算误差。这个过程根据用户定义的参数重复进行，这个参数被称为*训练时代数*。
- en: 'A *Python generator* is an iterator that returns an iterable. An example of
    how it works follows. Let’s start with a NumPy library for this simple demonstration
    of Python generators. I’ve created a function, `my_generator`, that accepts a
    NumPy array and iterates two records at a time in the array:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*Python生成器*是返回可迭代对象的迭代器。以下是它的工作示例。让我们从一个NumPy库开始，进行这个简单的Python生成器演示。我创建了一个名为`my_generator`的函数，它接受一个NumPy数组，并以数组中的两条记录为一组进行迭代：'
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is the test array I created, which will be passed into `my_generator`:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我创建的测试数组，将被传递给`my_generator`：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This NumPy array has four records, each consisting of two floating-point values.
    Then I pass this array to `my_generator`:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个NumPy数组有四条记录，每条记录由两个浮点值组成。然后我将这个数组传递给`my_generator`：
- en: '[PRE2]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To get output, use:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得输出，请使用：
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output should be:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该是：
- en: '[PRE4]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If you run the `next(output)` command again, the output will be different:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您再次运行`next(output)`命令，输出将不同：
- en: '[PRE5]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And if you run it yet again, the output is once again different:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您再次运行它，输出将再次不同：
- en: '[PRE6]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'And if you run it a fourth time, the output is now:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您第四次运行它，输出现在是：
- en: '[PRE7]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now that the last record is shown, you have finished streaming this data. If
    you run it again, it will return an empty array:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在最后一条记录已经显示，您已经完成了对这些数据的流式处理。如果您再次运行它，它将返回一个空数组：
- en: '[PRE8]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As you can see, the `my_generator` function streams two records in a NumPy array
    each time it is run. The unique aspect of the generator function is the use of
    the `yield` statement instead of the `return` statement. Unlike `return`, `yield`
    produces a sequence of values without storing the entire sequence in the Python
    runtime memory. `yield` continues to produce a sequence each time we invoke the
    `next` function until the end of the array is reached.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，`my_generator` 函数每次运行时都会以 NumPy 数组的形式流式传输两条记录。生成器函数的独特之处在于使用 `yield`
    语句而不是 `return` 语句。与 `return` 不同，`yield` 会生成一个值序列，而不会将整个序列存储在 Python 运行时内存中。`yield`
    在我们调用 `next` 函数时每次生成一个序列，直到到达数组的末尾。
- en: This example demonstrates how a subset of data can be generated via a generator
    function. However, in this example, the NumPy array is created on the fly and
    therefore is held in the Python runtime memory. Let’s take a look at how to iterate
    over a dataset that is stored as a file.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例演示了如何通过生成器函数生成数据子集。但是，在此示例中，NumPy 数组是即时创建的，因此保存在 Python 运行时内存中。让我们看看如何迭代存储为文件的数据集。
- en: Streaming File Content with a Generator
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用生成器流式传输文件内容
- en: To understand how a file in the storage can be streamed, you may find it easier
    to use a CSV file as an example. The file I use here, the Pima Indians Diabetes
    Dataset, is an open source dataset [available for download](https://oreil.ly/enlwY).
    Download it and store it on your local machine.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解如何流式传输存储中的文件，您可能会发现使用 CSV 文件作为示例更容易。我在这里使用的文件是 Pima 印第安人糖尿病数据集，这是一个[可供下载的开源数据集](https://oreil.ly/enlwY)。下载并将其存储在本地机器上。
- en: This file does not contain a header, so you will also need to [download](https://oreil.ly/NxIKk)
    the column names and descriptions for this dataset.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件没有包含标题，因此您还需要[下载](https://oreil.ly/NxIKk)此数据集的列名和描述。
- en: 'Briefly, the columns in this file are:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，该文件中的列是：
- en: '[PRE9]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s look at this file with the following lines of code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用以下代码行查看这个文件：
- en: '[PRE10]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The first few rows of the file are shown in [Figure 2-1](#pima_indians_diabetes_dataset).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 文件的前几行显示在 [图2-1](#pima_indians_diabetes_dataset) 中。
- en: '![Pima Indians Diabetes Dataset](Images/t2pr_0201.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![Pima Indians Diabetes Dataset](Images/t2pr_0201.png)'
- en: Figure 2-1\. Pima Indians Diabetes Dataset
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-1\. Pima 印第安人糖尿病数据集
- en: 'Since we want to stream this dataset, it is more convenient to read it as a
    CSV file and use the generator to output the rows, just like we did with the NumPy
    array in the preceding section. The way to do this is through the following code:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想要流式处理这个数据集，更方便的方法是将其作为 CSV 文件读取，并使用生成器输出行，就像我们在前一节中对 NumPy 数组所做的那样。实现这一点的方法是通过以下代码：
- en: '[PRE11]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s take a closer look at this code. We use the `with open` command to create
    a file handle object, `csvfile`, that knows where the file is stored. The next
    step is to pass it to the `reader` function in the CSV library:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看这段代码。我们使用 `with open` 命令创建一个文件句柄对象 `csvfile`，该对象知道文件存储在哪里。下一步是将其传递给 CSV
    库中的 `reader` 函数：
- en: '[PRE12]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '`f` is the entire file in the Python runtime memory. To inspect the file, execute
    this short piece of a `for` loop:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`f` 是 Python 运行时内存中的整个文件。要检查文件，执行这段简短的 `for` 循环代码：'
- en: '[PRE13]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The output of the first few rows looks like [Figure 2-2](#pima_indians_diabetes_dataset_csv_output).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 前几行的输出看起来像 [图2-2](#pima_indians_diabetes_dataset_csv_output)。
- en: '![Pima Indians Diabetes Dataset CSV output](Images/t2pr_0202.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![Pima 印第安人糖尿病数据集 CSV 输出](Images/t2pr_0202.png)'
- en: Figure 2-2\. Pima Indians Diabetes Dataset CSV output
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. Pima 印第安人糖尿病数据集 CSV 输出
- en: 'Now that you understand how to use a file handle, let’s refactor the preceding
    code so that we can use `yield` in a function, effectively making a generator
    to stream the content of the file:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了如何使用文件句柄，让我们重构前面的代码，以便可以在函数中使用 `yield`，有效地创建一个生成器来流式传输文件的内容：
- en: '[PRE14]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Recall that a Python generator is a function that uses `yield` to iterate through
    an `iterable` object. You can use `with open` to acquire a file handle as usual.
    Then we pass `handle` to a generator function `stream_file`, which contains a
    `for` loop that iterates through the file in `handle` row by row, removes newline
    code `\n`, then fills up a `holder`. Each row is passed back to the main thread’s
    `print` function by `yield` from the generator. The output is shown in [Figure 2-3](#pima_indians_diabetes_dataset_output_by).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，Python 生成器是一个使用 `yield` 来迭代通过 `iterable` 对象的函数。您可以像往常一样使用 `with open` 获取文件句柄。然后我们将
    `handle` 传递给一个包含 `for` 循环的生成器函数 `stream_file`，该循环逐行遍历 `handle` 中的文件，删除换行符 `\n`，然后填充一个
    `holder`。每行通过 `yield` 从生成器传回到主线程的 `print` 函数。输出显示在 [图2-3](#pima_indians_diabetes_dataset_output_by)
    中。
- en: '![Pima Indians Diabetes Dataset output by Python generator](Images/t2pr_0203.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![由 Python 生成器输出的 Pima 印第安人糖尿病数据集](Images/t2pr_0203.png)'
- en: Figure 2-3\. Pima Indians Diabetes Dataset output by Python generator
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-3\. 由 Python 生成器输出的 Pima 印第安人糖尿病数据集
- en: Now that you have a clear idea of how a dataset can be streamed, let’s look
    at how to apply this in TensorFlow. As it turns out, TensorFlow leverages this
    approach to build a framework for data ingestion. Streaming is usually the best
    way to ingest large amounts of data (such as hundreds of thousands of rows in
    one table, or distributed across multiple tables).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经清楚了数据集如何进行流式处理，让我们看看如何在 TensorFlow 中应用这一点。事实证明，TensorFlow 利用这种方法构建了一个用于数据摄入的框架。流式处理通常是摄入大量数据（例如一个表中的数十万行，或分布在多个表中）的最佳方式。
- en: JSON Data Structures
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: JSON 数据结构
- en: Tabular data is a common and convenient format for encoding features and labels
    for ML model training, and CSV is probably the most common tabular data format.
    You can think of each field separated by the comma delimiter as a column. Each
    column is defined with a data type, such as numeric (integer or floating point)
    or string.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表格数据是用于对 ML 模型训练的特征和标签进行编码的常见和便捷格式，CSV 可能是最常见的表格数据格式。您可以将逗号分隔的每个字段视为一列。每列都定义了一个数据类型，例如数字（整数或浮点数）或字符串。
- en: Tabular data is not the only data format that is well structured, by which I
    mean that every record follows the same convention and the order of fields in
    every record is the same. Another common data structure is JSON. JSON (JavaScript
    Object Notation) is a structure built with nested, hierarchical key-value pairs.
    You can think of keys as column names and values as the actual value of the data
    in that sample. JSON can be converted to CSV, and vice versa. Sometimes the original
    data is in JSON format and it is necessary to convert it to CSV, which is easier
    to display and inspect.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表格数据不是唯一的结构化数据格式，我指的是每个记录遵循相同约定，每个记录中字段的顺序相同。另一个常见的数据结构是JSON。JSON（JavaScript对象表示）是一个由嵌套、分层键值对构建的结构。您可以将键视为列名，将值视为该样本中数据的实际值。JSON可以转换为CSV，反之亦然。有时原始数据是以JSON格式存在，需要将其转换为CSV，这样更容易显示和检查。
- en: 'Here’s an example JSON record, showing the key-value pairs:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个示例JSON记录，显示了键值对：
- en: '[PRE15]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Notice that the key “rating” is associated with the value of an array [8, 7,
    9].
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，“rating”键与数组[8, 7, 9]的值相关联。
- en: There are plenty of examples of using a CSV file or a table as training data
    and ingesting it into the TensorFlow model training process. Typically, the data
    is read into a pandas DataFrame. However, this strategy only works if all the
    data can fit into the Python runtime memory. You can use streaming to handle data
    without the Python runtime restricting memory allocation. Since you learned how
    a Python generator works in the preceding section, you’re now ready to take a
    look at TensorFlow’s API, which operates on the same principle as a Python generator,
    and learn how to use TensorFlow’s adoption of the Python generator framework.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多例子使用CSV文件或表作为训练数据，并将其纳入TensorFlow模型训练过程。通常，数据被读入一个pandas DataFrame。然而，这种策略只有在所有数据都能适应Python运行时内存时才有效。您可以使用流处理数据，而不受Python运行时限制内存分配。由于您在前一节学习了Python生成器的工作原理，现在您可以看一下TensorFlow的API，它遵循与Python生成器相同的原理，并学习如何使用TensorFlow采用Python生成器框架。
- en: Setting Up a Pattern for Filenames
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置文件名的模式
- en: When working with a set of files, you will encounter patterns in file-naming
    conventions. To simulate an enterprise environment where new data is continuously
    being generated and stored, we will use an open source CSV file, split it into
    multiple parts by row count, then rename each part with a fixed prefix. This approach
    is similar to how the Hadoop Distributed File System (HDFS) names the parts of
    a file.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理一组文件时，您会遇到文件命名约定中的模式。为了模拟一个不断生成和存储新数据的企业环境，我们将使用一个开源CSV文件，按行数拆分成多个部分，然后使用固定前缀重命名每个部分。这种方法类似于Hadoop分布式文件系统（HDFS）如何命名文件的部分。
- en: Feel free to use your own CSV file if you have one handy. If not, you can download
    the suggested [CSV file](https://oreil.ly/8uGKL) (a COVID-19 dataset) for this
    example. (You may clone this repository if you wish.)
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有一个方便的CSV文件，可以随时使用您自己的CSV文件。如果没有，您可以为本示例下载建议的[CSV文件](https://oreil.ly/8uGKL)（一个COVID-19数据集）。（如果您愿意，您可以克隆这个存储库。）
- en: 'For now, all you need is *owid-covid-data.csv*. Once it is downloaded, inspect
    the file and determine the number of rows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你只需要*owid-covid-data.csv*。一旦下载完成，检查文件并确定行数：
- en: '[PRE16]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output indicates there are over 32,000 rows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 输出表明有超过32,000行：
- en: '[PRE17]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, inspect the first three lines of the CSV file to see if there is a header:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，检查CSV文件的前三行，看看是否有标题：
- en: '[PRE18]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Since this file contains a header, you’ll see the header in each of the part
    files. You can also look at a few rows of data to see what they actually look
    like.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这个文件包含一个标题，你会在每个部分文件中看到标题。你也可以查看一些数据行，看看它们实际上是什么样子的。
- en: Splitting a Single CSV File into Multiple CSV Files
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将单个CSV文件拆分为多个CSV文件
- en: 'Now let’s split this file into multiple CSV files, each with 330 rows. You
    should end up with 100 CSV files, each of which has the header. If you use Linux
    or macOS, use the following command:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将这个文件分割成多个CSV文件，每个文件有330行。你应该最终得到100个CSV文件，每个文件都有标题。如果你使用Linux或macOS，请使用以下命令：
- en: '[PRE19]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'For macOS, you may need to first install the `parallel` command:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于macOS，您可能需要先安装`parallel`命令：
- en: '[PRE20]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here are some of the files that are created:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些已创建的文件：
- en: '[PRE21]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This pattern represents a standard storage arrangement for multiple CSV formats.
    There is a distinct pattern to the naming convention: either all files have the
    same header, or none has any header at all.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模式代表了多个CSV格式的标准存储安排。命名约定有一个明显的模式：要么所有文件都有相同的标题，要么没有一个文件有标题。
- en: It’s a good idea to maintain a file-naming pattern, which can come in handy
    whether you have tens or hundreds of files. And when your naming pattern can be
    easily represented with wildcard notation, it’s easier to create a reference or
    file pattern object that points to all the data in storage.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 保持文件命名模式是个好主意，无论您有几十个还是几百个文件都会很方便。当您的命名模式可以很容易地用通配符表示时，更容易创建一个指向存储中所有数据的引用或文件模式对象。
- en: In the next section, we will look at how to use the TensorFlow API to create
    a file pattern object, which we’ll use to create a streaming object for this dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看看如何使用TensorFlow API创建一个文件模式对象，我们将使用它来为这个数据集创建一个流对象。
- en: Creating a File Pattern Object Using tf.io
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用tf.io创建文件模式对象
- en: 'The TensorFlow `tf.io` API is used for referencing a distributed dataset that
    contains files with a common naming pattern. This is not to say that you want
    to read the distributed dataset: what you want is a list of file paths and names
    for all the dataset files you want to read. This is not a new idea. For example,
    in Python, the [glob library](https://oreil.ly/JLtGm) is a popular choice for
    retrieving a similar list. The `tf.io` API simply leverages the glob library to
    generate a list of filenames that fit the pattern object:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow的`tf.io` API用于引用包含具有共同命名模式的文件的分布式数据集。这并不意味着您要读取分布式数据集：您想要的是一个包含您想要读取的所有数据集文件的文件路径和名称列表。这并不是一个新的想法。例如，在Python中，[glob库](https://oreil.ly/JLtGm)是检索类似列表的常用选择。`tf.io`
    API简单地利用glob库生成符合模式对象的文件名列表：
- en: '[PRE22]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`files` is a list that contains all the CSV filenames that are part of the
    original CSV, in no particular order:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`files`是一个包含所有原始CSV文件名的列表，没有特定顺序：'
- en: '[PRE23]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: This list will be the input for the next step, which is to create a streaming
    dataset object based on Python generators.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表将成为下一步的输入，即基于Python生成器创建一个流数据集对象。
- en: Creating a Streaming Dataset Object
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个流数据集对象
- en: 'Now that you have your file list ready, you can use it as the input to create
    a streaming dataset object. Note that this code is *only* meant to demonstrate
    how to convert a list of CSV files into a TensorFlow dataset object. If you were
    really going to use this data to train a supervised ML model, you would also perform
    data cleansing, normalization, and aggregation, all of which we’ll cover in [Chapter 8](ch08.xhtml#distributed_training-id00013).
    For the purposes of this example, “new_deaths” is selected as the target column:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经准备好文件列表，您可以将其用作输入来创建一个流数据集对象。请注意，此代码*仅*旨在演示如何将CSV文件列表转换为TensorFlow数据集对象。如果您真的要使用这些数据来训练一个监督式ML模型，您还将执行数据清洗、归一化和聚合等操作，所有这些我们将在[第8章](ch08.xhtml#distributed_training-id00013)中介绍。在本例中，“new_deaths”被选为目标列：
- en: '[PRE24]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding code specifies that each file in `files` contains a header. For
    convenience, as we inspect it, we set a small batch size of 5\. We also designate
    a target column with `label_name`, as if we are going to use this data for training
    a supervised ML model. `num_epochs` is used to specify how many times you want
    to stream over the entire dataset.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码指定`files`中的每个文件都包含一个标题。为了方便起见，我们将批量大小设置为5。我们还使用`label_name`指定一个目标列，就好像我们要使用这些数据来训练一个监督式ML模型。`num_epochs`用于指定您希望对整个数据集进行多少次流式传输。
- en: 'To look at actual data, you’ll need to use the `csv_dataset` object to iterate
    through the data:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看实际数据，您需要使用`csv_dataset`对象迭代数据：
- en: '[PRE25]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: This code uses the first batch of the dataset (`take(1)`), which contains five
    samples.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码使用数据集的第一个批次（`take(1)`），其中包含五个样本。
- en: 'Since you specified `label_name` to be the target column, the other columns
    are all considered to be features. In the dataset, contents are formatted as key-value
    pairs. The output from the preceding code will be similar to this:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您指定了`label_name`作为目标列，其他列都被视为特征。在数据集中，内容被格式化为键值对。前面代码的输出将类似于这样：
- en: '[PRE26]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This data is retrieved during runtime (lazy execution). As indicated by the
    batch size, each column contains five records. Next, let’s discuss how to stream
    this dataset.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据在运行时检索（延迟执行）。根据批量大小，每列包含五条记录。接下来，让我们讨论如何流式传输这个数据集。
- en: Streaming a CSV Dataset
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式传输CSV数据集
- en: 'Now that a CSV dataset object has been created, you can easily iterate over
    it in batches with this line of code, which uses the `iter` function to make an
    iterator from the CSV dataset and the `next` function to return the next item
    in the iterator:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经创建了一个CSV数据集对象，您可以使用这行代码轻松地按批次迭代它，该代码使用`iter`函数从CSV数据集创建一个迭代器，并使用`next`函数返回迭代器中的下一个项目：
- en: '[PRE27]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Remember that in this dataset there are two types of elements: `features` and
    `label`. These elements are returned as a *tuple* (similar to a list of objects,
    except that the order and the value of objects cannot be changed or reassigned).
    You can unpack a tuple by assigning the tuple elements to variables.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在这个数据集中有两种类型的元素：`features`和`label`。这些元素作为*元组*返回（类似于对象列表，不同之处在于对象的顺序和值不能被更改或重新分配）。您可以通过将元组元素分配给变量来解压元组。
- en: 'If you examine the label, you’ll see the content of the first batch:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您检查标签，您将看到第一个批次的内容：
- en: '[PRE28]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If you execute the same command again, you’ll see the second batch:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果再次执行相同的命令，您将看到第二个批次：
- en: '[PRE29]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s just take a look at `label`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下`label`：
- en: '[PRE30]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Indeed, this is the second batch of observations; it contains different values
    than the first batch. This is how a streaming CSV dataset is produced in a data
    ingestion pipeline. As each batch is sent to the model for training, the model
    computes the prediction in the *forward pass*, which computes the output by multiplying
    the input value and the current weight and bias in each node of the neural network.
    Then it compares the prediction with the label and calculates the loss function.
    Next comes the *backward pass*, where the model computes the variation with respect
    to the expected output and goes backward into each node of the network to update
    the weight and bias. The model then recalculates and updates the gradients. A
    new batch of data is sent to the model for training, and the process repeats.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 确实，这是第二批观察结果；它包含与第一批不同的值。这就是在数据摄入管道中生成流式传输CSV数据集的方式。当每个批次被发送到模型进行训练时，模型通过*前向传递*计算预测，该计算通过将输入值与神经网络中每个节点的当前权重和偏差相乘来计算输出。然后，它将预测与标签进行比较并计算损失函数。接下来是*反向传递*，模型计算与预期输出的变化，并向网络中的每个节点后退以更新权重和偏差。然后模型重新计算并更新梯度。将新的数据批次发送到模型进行训练，过程重复。
- en: Next we will look at how to organize image data for storage and stream it like
    we streamed the structured data.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将看看如何为存储组织图像数据，并像我们流式传输结构化数据一样流式传输它。
- en: Organizing Image Data
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 组织图像数据
- en: 'Image classification tasks require organizing images in certain ways because,
    unlike CSV or tabular data, attaching a label to an image requires special techniques.
    A straightforward and common pattern for organizing image files is with the following
    directory structure:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图像分类任务需要以特定方式组织图像，因为与CSV或表格数据不同，将标签附加到图像需要特殊技术。组织图像文件的一种直接和常见模式是使用以下目录结构：
- en: '[PRE31]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '*<PROJECT_NAME>* is the base directory. The first level below it contains training,
    validation, and test directories. Within each of these directories, there are
    subdirectories named with the image labels (`class_1`, `class_2`, etc., which
    in the following example are flower types), each of which contains the raw image
    files. This is shown in [Figure 2-4](#file_organization_for_image_classificati).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '*<PROJECT_NAME>*是基本目录。它的下一级包含训练、验证和测试目录。在每个目录中，都有以图像标签命名的子目录（`class_1`、`class_2`等，在下面的示例中是花卉类型），每个子目录包含原始图像文件。如[图2-4](#file_organization_for_image_classificati)所示。'
- en: This structure is common because it makes it easy to keep track of labels and
    their respective images, but by no means is it the only way to organize image
    data. Let’s look at another structure for organizing images. This is very similar
    to the previous one, except that training, testing, and validation are all separate.
    Immediately below the *<PROJECT_NAME>* directory are the directories of different
    image classes, as shown in [Figure 2-5](#figure_two-five_file_organization_for_im).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这种结构很常见，因为它可以方便地跟踪标签及其相应的图像，但这绝不是组织图像数据的唯一方式。让我们看看另一种组织图像的结构。这与之前的结构非常相似，只是训练、测试和验证都是分开的。在*<PROJECT_NAME>*目录的正下方是不同图像类别的目录，如[图2-5](#figure_two-five_file_organization_for_im)所示。
- en: '![File organization for image classification and partitioning for training
    work](Images/t2pr_0204.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![用于图像分类和训练工作的文件组织](Images/t2pr_0204.png)'
- en: Figure 2-4\. File organization for image classification and partitioning for
    training work
  id: totrans-113
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-4\. 用于图像分类和训练工作的文件组织
- en: '![File organization for images based on labels](Images/t2pr_0205.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![基于标签的图像文件组织](Images/t2pr_0205.png)'
- en: Figure 2-5\. File organization for images based on labels
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-5\. 基于标签的图像文件组织
- en: Using TensorFlow Image Generator
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用TensorFlow图像生成器
- en: Now let’s take a look at how to deal with images. Besides the nuances of file
    organization, working with images also requires certain steps to standardize and
    normalize the image files. The model architecture requires a fixed shape (fixed
    dimensions) for all images. At the pixel level, the values are normalized, typically
    to a range of [0, 1] (dividing the pixel value by 255).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何处理图像。除了文件组织的细微差别外，处理图像还需要一些步骤来标准化和归一化图像文件。模型架构需要所有图像具有固定形状（固定尺寸）。在像素级别，值通常被归一化为[0,
    1]的范围（将像素值除以255）。
- en: For this example, you’ll use an open source image set of five different types
    of flowers (or feel free to use your own image set). Let’s assume that images
    should be 224 × 224 pixels, where the dimensions correspond to height and width.
    These are the expected dimensions for input images if you want to use a pretrained
    residual neural network (ResNet) as the image classifier.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，您将使用一个包含五种不同类型的花朵的开源图像集（或者随意使用您自己的图像集）。假设图像应该是224×224像素，其中尺寸对应高度和宽度。如果您想要使用预训练的残差神经网络（ResNet）作为图像分类器，这些是输入图像的预期尺寸。
- en: 'First let’s download the images. The following code downloads five types of
    flowers, all in different dimensions, and puts them in the file structure shown
    later in [Figure 2-6](Images/#a_batch_of_reshaped_images):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们下载这些图像。以下代码下载五种不同尺寸的花朵，并将它们放入稍后在[图2-6](Images/#a_batch_of_reshaped_images)中显示的文件结构中：
- en: '[PRE32]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We will refer to `data_dir` as the base directory. It should be similar to:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`data_dir`称为基本目录。它应该类似于：
- en: '[PRE33]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'If you list the content from the base directory, you’ll see:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果列出基本目录中的内容，您将看到：
- en: '[PRE34]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'There are three steps to streaming the images. Let’s look more closely:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 流式传输图像有三个步骤。让我们更仔细地看一下：
- en: 'Create an `ImageDataGenerator` object and specify normalization parameters.
    Use the `rescale` parameter to indicate the normalization scale and the `validation_split`
    parameter to specify that 20% of the data will be set aside for cross validation:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`ImageDataGenerator`对象并指定归一化参数。使用`rescale`参数指示归一化比例，使用`validation_split`参数指定将20%的数据留出用于交叉验证：
- en: '[PRE35]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Optionally, you can wrap `rescale` and `validation_split` as a dictionary that
    consists of key-value pairs:'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可选地，您可以将`rescale`和`validation_split`包装为一个包含键值对的字典：
- en: '[PRE36]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This is a convenient way to reuse the same parameters and keep multiple input
    arguments under wrap. (Passing the dictionary data structure to a function is
    a Python technique known as *dictionary unpacking*.)
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是一种方便的方式，可以重复使用相同的参数并将多个输入参数包装在一起。 （将字典数据结构传递给函数是一种称为*字典解包*的Python技术。）
- en: 'Connect the `ImageDataGenerator` object to the data source and specify parameters
    to resize the images to a fixed dimension:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`ImageDataGenerator`对象连接到数据源，并指定参数将图像调整为固定尺寸：
- en: '[PRE37]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Prepare a map for indexing the labels. In this step, you retrieve the index
    that the generator has assigned to each label and create a dictionary that maps
    it to the actual label name. The TensorFlow generator internally keeps track of
    labels from the directory name below `data_dir`. They can be retrieved through
    `train_generator.class_indices`, which returns a key-value pair of labels and
    indices. You can take advantage of this and reverse it to deploy the model for
    scoring. The model will output the index. To implement this reverse lookup, simply
    reverse the label dictionary generated by `train_generator.class_indices`:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为索引标签准备一个映射。在这一步中，您检索生成器为每个标签分配的索引，并创建一个将其映射到实际标签名称的字典。TensorFlow生成器内部会跟踪`data_dir`下目录名称的标签。它们可以通过`train_generator.class_indices`检索，返回标签和索引的键值对。您可以利用这一点并将其反转以部署模型进行评分。模型将输出索引。要实现这种反向查找，只需反转由`train_generator.class_indices`生成的标签字典：
- en: '[PRE38]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'These are the `idx_labels`:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些是`idx_labels`：
- en: '[PRE39]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Now you can inspect the shape of the items generated by `train_generator:`
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在您可以检查`train_generator`生成的项目的形状：
- en: '[PRE40]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Expect to see the following for the first batch yielded by the generator iterating
    through the base directory:'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预计通过迭代基本目录生成器产生的第一批数据将会看到以下内容：
- en: '[PRE41]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The first tuple indicates a batch size of 32 images, each with a dimension of
    224 × 224 × 3 (height × width × depth, where depth represents the three color
    channels RGB). The second tuple indicates 32 labels, each corresponding to one
    of the five flower types. It is one-hot encoded per `idx_labels`.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一个元组表示32张图片的批量大小，每张图片的尺寸为224×224×3（高度×宽度×深度，深度代表三个颜色通道RGB）。第二个元组表示32个标签，每个标签对应五种花的其中一种。它是根据`idx_labels`进行独热编码的。
- en: Streaming Cross-Validation Images
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式交叉验证图片
- en: Recall that in creating the generator for streaming training data, you specified
    the `validation_split` parameter with a value of 0.2\. If you don’t do this, `validation_split`
    defaults to a value of 0\. If `validation_split` is set to a nonzero decimal,
    when you invoke the `flow_from_directory` method, you also have to specify `subset`
    to be either `training` or `validation`. In the preceding example, it is `subset="training".`
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在创建用于流式训练数据的生成器时，您使用了`validation_split`参数，值为0.2。如果不这样做，`validation_split`默认为0。如果`validation_split`设置为非零小数，则在调用`flow_from_directory`方法时，还必须指定`subset`为`training`或`validation`。在前面的示例中，它是`subset="training"`。
- en: 'You may be wondering how you’ll know which images belong to the `training`
    subset from our previous endeavor of creating a training generator. Well, you
    don’t have to know this if you reassign and reuse the training generator:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能想知道如何知道哪些图片属于我们之前创建的训练生成器的`training`子集。好消息是，如果您重新分配和重复使用训练生成器，您就不需要知道这一点：
- en: '[PRE42]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As you can see, a TensorFlow generator knows and keeps track of `training` and
    `validation` subsets, so you can reuse the same generator to stream over different
    subsets. The `dataflow_kwargs` dictionary is also reused. This is a convenience
    feature provided by TensorFlow generators.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，TensorFlow生成器知道并跟踪`training`和`validation`子集，因此您可以重复使用相同的生成器来流式传输不同的子集。`dataflow_kwargs`字典也被重用。这是TensorFlow生成器提供的一个便利功能。
- en: Because you reuse `train_datagen`, you can be sure that image rescaling is done
    the same way as image training. And in the `valid_datagen.flow_from_directory`
    method, you’ll pass in the same `dataflow_kwargs` dictionary to set the image
    size for cross validation to be the same as it is for the training images.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因为您重复使用`train_datagen`，所以可以确保图像重新缩放的方式与图像训练的方式相同。在`valid_datagen.flow_from_directory`方法中，您将传入相同的`dataflow_kwargs`字典，以设置交叉验证的图像大小与训练图像相同。
- en: If you prefer to organize the images into training, validation, and testing
    yourself, what you learned earlier still applies, with two exceptions. First,
    your `data_dir` is at the level of the training, validation, or testing directory.
    Second, you don’t need to specify `validation_split` in `ImageDataGenerator` and
    `subset` in `flow_from_directory`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望自行组织图片到训练、验证和测试中，之前学到的内容仍然适用，但有两个例外。首先，您的`data_dir`位于训练、验证或测试目录的级别。其次，在`ImageDataGenerator`和`flow_from_directory`中不需要指定`validation_split`和`subset`。
- en: Inspecting Resized Images
  id: totrans-149
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查调整大小的图片
- en: 'Now let’s inspect the resized images coming off the generator. Following is
    the code snippet for iterating through a batch of data streamed by a generator:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们检查生成器生成的调整大小的图片。以下是通过生成器流式传输的数据批次进行迭代的代码片段：
- en: '[PRE43]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This code will produce 32 images from the first batch coming off the generator
    (see [Figure 2-6](Images/#a_batch_of_reshaped_images)).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将从生成器中产生的第一批数据中生成32张图片（参见[图2-6](Images/#a_batch_of_reshaped_images)）。
- en: '![A batch of reshaped images](Images/t2pr_0206.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![一批调整大小的图片](Images/t2pr_0206.png)'
- en: Figure 2-6\. A batch of reshaped images
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6. 一批调整大小的图片
- en: 'Let’s examine the code:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来检查代码：
- en: '[PRE44]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This iterates over the base directory with the generator. It applies the `iter`
    function to the generator and leverages the `next` function to output the image
    batch and label batch as NumPy arrays:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这将通过生成器迭代基本目录。它将`iter`函数应用于生成器，并利用`next`函数将图像批次和标签批次输出为NumPy数组：
- en: '[PRE45]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This line sets up the number of subplots you expect, which is 32, your batch
    size:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码设置了您期望的子图数量，即32，即批量大小：
- en: '[PRE46]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Then you set the figure axes, using a `for` loop to display NumPy arrays as
    images and labels. As shown in [Figure 2-6](Images/#a_batch_of_reshaped_images),
    all the images are resized into 224 × 224-pixel squares. Although the subplot
    holder is a rectangle with `figsize=(10, 20)`, you can see that all of the images
    are squares. This means your code for resizing and normalizing images in the generator
    workflow works as expected.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您设置图形轴，使用`for`循环将NumPy数组显示为图片和标签。如[图2-6](Images/#a_batch_of_reshaped_images)所示，所有图片都被调整为224×224像素的正方形。尽管子图容器是一个尺寸为`(10,
    20)`的矩形，您可以看到所有图片都是正方形的。这意味着您在生成器工作流中调整大小和归一化图片的代码按预期工作。
- en: Wrapping Up
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned the fundamentals of streaming data using Python.
    This is a workhorse technique when working with large, distributed datasets. You
    also saw some common file organization patterns for tabular and image data.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学会了使用Python处理流数据的基础知识。这是在处理大型分布式数据集时的一种重要技术。你还看到了一些常见的表格和图像数据的文件组织模式。
- en: In the section on tabular data, you learned how choosing a good file-naming
    convention can make it easier to build a reference to all the files, regardless
    of how many there are. This means you now know how to build a scalable pipeline
    that can ingest as much data as needed into a Python runtime for any use (in this
    case, for TensorFlow to create a dataset).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格数据部分，你学会了如何选择一个良好的文件命名约定，可以更容易地构建对所有文件的引用，无论有多少个文件。这意味着你现在知道如何构建一个可扩展的流水线，可以将所需的数据输入到Python运行时中，用于任何用途（在本例中，用于TensorFlow创建数据集）。
- en: You also learned how image files are usually organized in file storage and how
    to associate images with labels. In the next chapter, you will leverage what you’ve
    learned here about data organization and streaming to integrate it with the model
    training process.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学会了图像文件通常如何在文件存储中组织，以及如何将图像与标签关联起来。在下一章中，你将利用你在这里学到的关于数据组织和流式处理的知识，将其与模型训练过程整合起来。
