- en: Chapter 2\. Data Storage and Ingestion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To envision how to set up an ML model to solve a problem, you have to start
    thinking about data structure patterns. In this chapter, we’ll look at some general
    patterns in storage, data formats, and data ingestion. Typically, once you understand
    your business problem and set it up as a data science problem, you have to think
    about how to get the data into a format or structure that your model training
    process can use. Data ingestion during the training process is fundamentally a
    data transformation pipeline. Without this transformation, you won’t be able to
    deliver and serve the model in an enterprise-driven or use-case-driven setting;
    it would remain nothing more than an exploration tool and would not be able to
    scale to handle large amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will show you how to design a data ingestion pipeline for two
    common data structures: tables and images. You will learn how to make the pipeline
    scalable by using TensorFlow’s APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data streaming* is the means by which the data is ingested in small batches
    by the model for training. Data streaming in Python is not a new concept. However,
    grasping it is fundamental to understanding how the more advanced APIs in TensorFlow
    work. Thus, this chapter will start with Python generators. Then we’ll look at
    how tabular data is stored, including how to indicate and track features and labels.
    We’ll then move to designing your data structure, and finish by discussing how
    to ingest data to your model for training and how to stream tabular data. The
    rest of the chapter covers how to organize image data for image classification
    and how to stream image data.'
  prefs: []
  type: TYPE_NORMAL
- en: Streaming Data with Python Generators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are times when the Python runtime’s memory is not big enough to handle
    loading the dataset in its entirety. When this happens, the recommended practice
    is to load the data in small batches. Therefore, the data is streamed into the
    model during the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Sending data in small batches has many other advantages as well. One is that
    a gradient descent algorithm is applied to each batch to calculate the error (that
    is, the difference between the model output and the ground truth) and to gradually
    update the model’s weights and biases to make this error as small as possible.
    This lets us parallelize the gradient calculation, since the error calculation
    (also known as *loss calculation*) of one batch does not depend on the other.
    This is known as *mini-batch gradient descent*. At the end of each epoch, after
    a full training dataset has gone through the model, gradients from all batches
    are summed and weights are updated. Then, training starts again for the next epoch,
    with the newly updated weights and biases, and the error is calculated. This process
    repeats according to a user-defined parameter, which is known as *number of epochs
    for training*.
  prefs: []
  type: TYPE_NORMAL
- en: 'A *Python generator* is an iterator that returns an iterable. An example of
    how it works follows. Let’s start with a NumPy library for this simple demonstration
    of Python generators. I’ve created a function, `my_generator`, that accepts a
    NumPy array and iterates two records at a time in the array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the test array I created, which will be passed into `my_generator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This NumPy array has four records, each consisting of two floating-point values.
    Then I pass this array to `my_generator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To get output, use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run the `next(output)` command again, the output will be different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'And if you run it yet again, the output is once again different:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'And if you run it a fourth time, the output is now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the last record is shown, you have finished streaming this data. If
    you run it again, it will return an empty array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `my_generator` function streams two records in a NumPy array
    each time it is run. The unique aspect of the generator function is the use of
    the `yield` statement instead of the `return` statement. Unlike `return`, `yield`
    produces a sequence of values without storing the entire sequence in the Python
    runtime memory. `yield` continues to produce a sequence each time we invoke the
    `next` function until the end of the array is reached.
  prefs: []
  type: TYPE_NORMAL
- en: This example demonstrates how a subset of data can be generated via a generator
    function. However, in this example, the NumPy array is created on the fly and
    therefore is held in the Python runtime memory. Let’s take a look at how to iterate
    over a dataset that is stored as a file.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming File Content with a Generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand how a file in the storage can be streamed, you may find it easier
    to use a CSV file as an example. The file I use here, the Pima Indians Diabetes
    Dataset, is an open source dataset [available for download](https://oreil.ly/enlwY).
    Download it and store it on your local machine.
  prefs: []
  type: TYPE_NORMAL
- en: This file does not contain a header, so you will also need to [download](https://oreil.ly/NxIKk)
    the column names and descriptions for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Briefly, the columns in this file are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at this file with the following lines of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The first few rows of the file are shown in [Figure 2-1](#pima_indians_diabetes_dataset).
  prefs: []
  type: TYPE_NORMAL
- en: '![Pima Indians Diabetes Dataset](Images/t2pr_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. Pima Indians Diabetes Dataset
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Since we want to stream this dataset, it is more convenient to read it as a
    CSV file and use the generator to output the rows, just like we did with the NumPy
    array in the preceding section. The way to do this is through the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a closer look at this code. We use the `with open` command to create
    a file handle object, `csvfile`, that knows where the file is stored. The next
    step is to pass it to the `reader` function in the CSV library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '`f` is the entire file in the Python runtime memory. To inspect the file, execute
    this short piece of a `for` loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The output of the first few rows looks like [Figure 2-2](#pima_indians_diabetes_dataset_csv_output).
  prefs: []
  type: TYPE_NORMAL
- en: '![Pima Indians Diabetes Dataset CSV output](Images/t2pr_0202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-2\. Pima Indians Diabetes Dataset CSV output
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now that you understand how to use a file handle, let’s refactor the preceding
    code so that we can use `yield` in a function, effectively making a generator
    to stream the content of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Recall that a Python generator is a function that uses `yield` to iterate through
    an `iterable` object. You can use `with open` to acquire a file handle as usual.
    Then we pass `handle` to a generator function `stream_file`, which contains a
    `for` loop that iterates through the file in `handle` row by row, removes newline
    code `\n`, then fills up a `holder`. Each row is passed back to the main thread’s
    `print` function by `yield` from the generator. The output is shown in [Figure 2-3](#pima_indians_diabetes_dataset_output_by).
  prefs: []
  type: TYPE_NORMAL
- en: '![Pima Indians Diabetes Dataset output by Python generator](Images/t2pr_0203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-3\. Pima Indians Diabetes Dataset output by Python generator
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now that you have a clear idea of how a dataset can be streamed, let’s look
    at how to apply this in TensorFlow. As it turns out, TensorFlow leverages this
    approach to build a framework for data ingestion. Streaming is usually the best
    way to ingest large amounts of data (such as hundreds of thousands of rows in
    one table, or distributed across multiple tables).
  prefs: []
  type: TYPE_NORMAL
- en: JSON Data Structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tabular data is a common and convenient format for encoding features and labels
    for ML model training, and CSV is probably the most common tabular data format.
    You can think of each field separated by the comma delimiter as a column. Each
    column is defined with a data type, such as numeric (integer or floating point)
    or string.
  prefs: []
  type: TYPE_NORMAL
- en: Tabular data is not the only data format that is well structured, by which I
    mean that every record follows the same convention and the order of fields in
    every record is the same. Another common data structure is JSON. JSON (JavaScript
    Object Notation) is a structure built with nested, hierarchical key-value pairs.
    You can think of keys as column names and values as the actual value of the data
    in that sample. JSON can be converted to CSV, and vice versa. Sometimes the original
    data is in JSON format and it is necessary to convert it to CSV, which is easier
    to display and inspect.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example JSON record, showing the key-value pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the key “rating” is associated with the value of an array [8, 7,
    9].
  prefs: []
  type: TYPE_NORMAL
- en: There are plenty of examples of using a CSV file or a table as training data
    and ingesting it into the TensorFlow model training process. Typically, the data
    is read into a pandas DataFrame. However, this strategy only works if all the
    data can fit into the Python runtime memory. You can use streaming to handle data
    without the Python runtime restricting memory allocation. Since you learned how
    a Python generator works in the preceding section, you’re now ready to take a
    look at TensorFlow’s API, which operates on the same principle as a Python generator,
    and learn how to use TensorFlow’s adoption of the Python generator framework.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Up a Pattern for Filenames
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When working with a set of files, you will encounter patterns in file-naming
    conventions. To simulate an enterprise environment where new data is continuously
    being generated and stored, we will use an open source CSV file, split it into
    multiple parts by row count, then rename each part with a fixed prefix. This approach
    is similar to how the Hadoop Distributed File System (HDFS) names the parts of
    a file.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to use your own CSV file if you have one handy. If not, you can download
    the suggested [CSV file](https://oreil.ly/8uGKL) (a COVID-19 dataset) for this
    example. (You may clone this repository if you wish.)
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, all you need is *owid-covid-data.csv*. Once it is downloaded, inspect
    the file and determine the number of rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output indicates there are over 32,000 rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, inspect the first three lines of the CSV file to see if there is a header:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Since this file contains a header, you’ll see the header in each of the part
    files. You can also look at a few rows of data to see what they actually look
    like.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting a Single CSV File into Multiple CSV Files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let’s split this file into multiple CSV files, each with 330 rows. You
    should end up with 100 CSV files, each of which has the header. If you use Linux
    or macOS, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'For macOS, you may need to first install the `parallel` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some of the files that are created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This pattern represents a standard storage arrangement for multiple CSV formats.
    There is a distinct pattern to the naming convention: either all files have the
    same header, or none has any header at all.'
  prefs: []
  type: TYPE_NORMAL
- en: It’s a good idea to maintain a file-naming pattern, which can come in handy
    whether you have tens or hundreds of files. And when your naming pattern can be
    easily represented with wildcard notation, it’s easier to create a reference or
    file pattern object that points to all the data in storage.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at how to use the TensorFlow API to create
    a file pattern object, which we’ll use to create a streaming object for this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a File Pattern Object Using tf.io
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The TensorFlow `tf.io` API is used for referencing a distributed dataset that
    contains files with a common naming pattern. This is not to say that you want
    to read the distributed dataset: what you want is a list of file paths and names
    for all the dataset files you want to read. This is not a new idea. For example,
    in Python, the [glob library](https://oreil.ly/JLtGm) is a popular choice for
    retrieving a similar list. The `tf.io` API simply leverages the glob library to
    generate a list of filenames that fit the pattern object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '`files` is a list that contains all the CSV filenames that are part of the
    original CSV, in no particular order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This list will be the input for the next step, which is to create a streaming
    dataset object based on Python generators.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Streaming Dataset Object
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that you have your file list ready, you can use it as the input to create
    a streaming dataset object. Note that this code is *only* meant to demonstrate
    how to convert a list of CSV files into a TensorFlow dataset object. If you were
    really going to use this data to train a supervised ML model, you would also perform
    data cleansing, normalization, and aggregation, all of which we’ll cover in [Chapter 8](ch08.xhtml#distributed_training-id00013).
    For the purposes of this example, “new_deaths” is selected as the target column:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code specifies that each file in `files` contains a header. For
    convenience, as we inspect it, we set a small batch size of 5\. We also designate
    a target column with `label_name`, as if we are going to use this data for training
    a supervised ML model. `num_epochs` is used to specify how many times you want
    to stream over the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To look at actual data, you’ll need to use the `csv_dataset` object to iterate
    through the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This code uses the first batch of the dataset (`take(1)`), which contains five
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since you specified `label_name` to be the target column, the other columns
    are all considered to be features. In the dataset, contents are formatted as key-value
    pairs. The output from the preceding code will be similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: This data is retrieved during runtime (lazy execution). As indicated by the
    batch size, each column contains five records. Next, let’s discuss how to stream
    this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming a CSV Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that a CSV dataset object has been created, you can easily iterate over
    it in batches with this line of code, which uses the `iter` function to make an
    iterator from the CSV dataset and the `next` function to return the next item
    in the iterator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that in this dataset there are two types of elements: `features` and
    `label`. These elements are returned as a *tuple* (similar to a list of objects,
    except that the order and the value of objects cannot be changed or reassigned).
    You can unpack a tuple by assigning the tuple elements to variables.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you examine the label, you’ll see the content of the first batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If you execute the same command again, you’ll see the second batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s just take a look at `label`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Indeed, this is the second batch of observations; it contains different values
    than the first batch. This is how a streaming CSV dataset is produced in a data
    ingestion pipeline. As each batch is sent to the model for training, the model
    computes the prediction in the *forward pass*, which computes the output by multiplying
    the input value and the current weight and bias in each node of the neural network.
    Then it compares the prediction with the label and calculates the loss function.
    Next comes the *backward pass*, where the model computes the variation with respect
    to the expected output and goes backward into each node of the network to update
    the weight and bias. The model then recalculates and updates the gradients. A
    new batch of data is sent to the model for training, and the process repeats.
  prefs: []
  type: TYPE_NORMAL
- en: Next we will look at how to organize image data for storage and stream it like
    we streamed the structured data.
  prefs: []
  type: TYPE_NORMAL
- en: Organizing Image Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Image classification tasks require organizing images in certain ways because,
    unlike CSV or tabular data, attaching a label to an image requires special techniques.
    A straightforward and common pattern for organizing image files is with the following
    directory structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '*<PROJECT_NAME>* is the base directory. The first level below it contains training,
    validation, and test directories. Within each of these directories, there are
    subdirectories named with the image labels (`class_1`, `class_2`, etc., which
    in the following example are flower types), each of which contains the raw image
    files. This is shown in [Figure 2-4](#file_organization_for_image_classificati).'
  prefs: []
  type: TYPE_NORMAL
- en: This structure is common because it makes it easy to keep track of labels and
    their respective images, but by no means is it the only way to organize image
    data. Let’s look at another structure for organizing images. This is very similar
    to the previous one, except that training, testing, and validation are all separate.
    Immediately below the *<PROJECT_NAME>* directory are the directories of different
    image classes, as shown in [Figure 2-5](#figure_two-five_file_organization_for_im).
  prefs: []
  type: TYPE_NORMAL
- en: '![File organization for image classification and partitioning for training
    work](Images/t2pr_0204.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-4\. File organization for image classification and partitioning for
    training work
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![File organization for images based on labels](Images/t2pr_0205.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-5\. File organization for images based on labels
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using TensorFlow Image Generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now let’s take a look at how to deal with images. Besides the nuances of file
    organization, working with images also requires certain steps to standardize and
    normalize the image files. The model architecture requires a fixed shape (fixed
    dimensions) for all images. At the pixel level, the values are normalized, typically
    to a range of [0, 1] (dividing the pixel value by 255).
  prefs: []
  type: TYPE_NORMAL
- en: For this example, you’ll use an open source image set of five different types
    of flowers (or feel free to use your own image set). Let’s assume that images
    should be 224 × 224 pixels, where the dimensions correspond to height and width.
    These are the expected dimensions for input images if you want to use a pretrained
    residual neural network (ResNet) as the image classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'First let’s download the images. The following code downloads five types of
    flowers, all in different dimensions, and puts them in the file structure shown
    later in [Figure 2-6](Images/#a_batch_of_reshaped_images):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We will refer to `data_dir` as the base directory. It should be similar to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'If you list the content from the base directory, you’ll see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'There are three steps to streaming the images. Let’s look more closely:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create an `ImageDataGenerator` object and specify normalization parameters.
    Use the `rescale` parameter to indicate the normalization scale and the `validation_split`
    parameter to specify that 20% of the data will be set aside for cross validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Optionally, you can wrap `rescale` and `validation_split` as a dictionary that
    consists of key-value pairs:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is a convenient way to reuse the same parameters and keep multiple input
    arguments under wrap. (Passing the dictionary data structure to a function is
    a Python technique known as *dictionary unpacking*.)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Connect the `ImageDataGenerator` object to the data source and specify parameters
    to resize the images to a fixed dimension:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare a map for indexing the labels. In this step, you retrieve the index
    that the generator has assigned to each label and create a dictionary that maps
    it to the actual label name. The TensorFlow generator internally keeps track of
    labels from the directory name below `data_dir`. They can be retrieved through
    `train_generator.class_indices`, which returns a key-value pair of labels and
    indices. You can take advantage of this and reverse it to deploy the model for
    scoring. The model will output the index. To implement this reverse lookup, simply
    reverse the label dictionary generated by `train_generator.class_indices`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'These are the `idx_labels`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now you can inspect the shape of the items generated by `train_generator:`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Expect to see the following for the first batch yielded by the generator iterating
    through the base directory:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first tuple indicates a batch size of 32 images, each with a dimension of
    224 × 224 × 3 (height × width × depth, where depth represents the three color
    channels RGB). The second tuple indicates 32 labels, each corresponding to one
    of the five flower types. It is one-hot encoded per `idx_labels`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Streaming Cross-Validation Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall that in creating the generator for streaming training data, you specified
    the `validation_split` parameter with a value of 0.2\. If you don’t do this, `validation_split`
    defaults to a value of 0\. If `validation_split` is set to a nonzero decimal,
    when you invoke the `flow_from_directory` method, you also have to specify `subset`
    to be either `training` or `validation`. In the preceding example, it is `subset="training".`
  prefs: []
  type: TYPE_NORMAL
- en: 'You may be wondering how you’ll know which images belong to the `training`
    subset from our previous endeavor of creating a training generator. Well, you
    don’t have to know this if you reassign and reuse the training generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, a TensorFlow generator knows and keeps track of `training` and
    `validation` subsets, so you can reuse the same generator to stream over different
    subsets. The `dataflow_kwargs` dictionary is also reused. This is a convenience
    feature provided by TensorFlow generators.
  prefs: []
  type: TYPE_NORMAL
- en: Because you reuse `train_datagen`, you can be sure that image rescaling is done
    the same way as image training. And in the `valid_datagen.flow_from_directory`
    method, you’ll pass in the same `dataflow_kwargs` dictionary to set the image
    size for cross validation to be the same as it is for the training images.
  prefs: []
  type: TYPE_NORMAL
- en: If you prefer to organize the images into training, validation, and testing
    yourself, what you learned earlier still applies, with two exceptions. First,
    your `data_dir` is at the level of the training, validation, or testing directory.
    Second, you don’t need to specify `validation_split` in `ImageDataGenerator` and
    `subset` in `flow_from_directory`.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting Resized Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let’s inspect the resized images coming off the generator. Following is
    the code snippet for iterating through a batch of data streamed by a generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This code will produce 32 images from the first batch coming off the generator
    (see [Figure 2-6](Images/#a_batch_of_reshaped_images)).
  prefs: []
  type: TYPE_NORMAL
- en: '![A batch of reshaped images](Images/t2pr_0206.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-6\. A batch of reshaped images
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s examine the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'This iterates over the base directory with the generator. It applies the `iter`
    function to the generator and leverages the `next` function to output the image
    batch and label batch as NumPy arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This line sets up the number of subplots you expect, which is 32, your batch
    size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Then you set the figure axes, using a `for` loop to display NumPy arrays as
    images and labels. As shown in [Figure 2-6](Images/#a_batch_of_reshaped_images),
    all the images are resized into 224 × 224-pixel squares. Although the subplot
    holder is a rectangle with `figsize=(10, 20)`, you can see that all of the images
    are squares. This means your code for resizing and normalizing images in the generator
    workflow works as expected.
  prefs: []
  type: TYPE_NORMAL
- en: Wrapping Up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned the fundamentals of streaming data using Python.
    This is a workhorse technique when working with large, distributed datasets. You
    also saw some common file organization patterns for tabular and image data.
  prefs: []
  type: TYPE_NORMAL
- en: In the section on tabular data, you learned how choosing a good file-naming
    convention can make it easier to build a reference to all the files, regardless
    of how many there are. This means you now know how to build a scalable pipeline
    that can ingest as much data as needed into a Python runtime for any use (in this
    case, for TensorFlow to create a dataset).
  prefs: []
  type: TYPE_NORMAL
- en: You also learned how image files are usually organized in file storage and how
    to associate images with labels. In the next chapter, you will leverage what you’ve
    learned here about data organization and streaming to integrate it with the model
    training process.
  prefs: []
  type: TYPE_NORMAL
