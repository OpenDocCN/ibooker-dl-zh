- en: '5 Training large language models: How to generate the generator'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Setting up a training environment and common libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying various training techniques, including using advanced methodologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tips and tricks to get the most out of training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be water, my friend.—Bruce Lee
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Are you ready to have some fun?! What do you mean the last four chapters weren’t
    fun? Well, I promise this one for sure will be. We’ve leveled up a lot and gained
    a ton of context that will prove invaluable now as we start to get our hands dirty.
    By training an LLM, we can create bots that can do amazing things and have unique
    personalities. Indeed, we can create new friends and play with them. In the last
    chapter, we showed you how to create a training dataset based on your Slack messages.
    Now we will show you how to take that dataset and create a persona of yourself.
    Finally, you will no longer have to talk to that one annoying coworker, and just
    like Gilfoyle, you can have your own AI Gilfoyle ([https://youtu.be/IWIusSdn1e4](https://youtu.be/IWIusSdn1e4)).
  prefs: []
  type: TYPE_NORMAL
- en: First things first, we’ll show you how to set up a training environment, as
    the process can be very resource-demanding, and without the proper equipment,
    you won’t be able to enjoy what comes next. We’ll then show you how to do the
    basics, like training from scratch and finetuning, after which we’ll get into
    some of the best-known methods to improve upon these processes, making them more
    efficient, faster, and cheaper. We’ll end the chapter with some tips and tricks
    we’ve acquired through our experience of training models in the field.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Multi-GPU environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training is a resource-intensive endeavor. A model that only takes a single
    GPU to run inference on may take 10 times that many to train if, for nothing else,
    to parallelize your work and speed things up so you aren’t waiting for a thousand
    years for it to finish training. To really take advantage of what we want to teach
    you in this chapter, we’re first going to have to get you set up in an environment
    you can use as a playground. Later in the chapter, we’ll teach some resource-optimal
    strategies as well, but you’ll need to understand how to set up a multi-GPU env
    if you want to use the largest LLMs anyway.
  prefs: []
  type: TYPE_NORMAL
- en: While you can learn a lot using smaller LLMs, what sets apart a pro from an
    amateur is often the ease and fluidity they have when working with larger models.
    And there’s a good reason for this since, on the whole, larger models outperform
    smaller models. If you want to work with the largest models, you’ll never be able
    to get started on your laptop. Even most customized gaming rigs with dual GPUs
    aren’t enough for inference, let alone training.
  prefs: []
  type: TYPE_NORMAL
- en: To this end, we wanted to share with you a few methods to acquire access to
    a multi-GPU environment in the cloud, and then we will share the tools and libraries
    necessary to utilize them. The largest models do not fit in a single GPU, so without
    these environments and tools, you’ll be stuck playing on easy mode forever.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Setting up
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It should be pointed out up front that while multi-GPU environments are powerful,
    they are also expensive. When it comes to multi-GPUs, no services we know of offer
    a free tier or offering, but you can at least take comfort in knowing that paying
    per hour will be way cheaper than purchasing the rigs wholesale. Of course, if
    you can get your company to pay the bill, we recommend it, but it is still your
    responsibility to spin down and turn off any environment you create to avoid unnecessary
    charges.
  prefs: []
  type: TYPE_NORMAL
- en: If your company is paying, it likely has chosen a hosted service that makes
    this whole process easy. For the rest of us, setting up a virtual machine (VM)
    in Google’s Compute Engine is one of the easiest methods. Once set up, we will
    then show you how to utilize it.
  prefs: []
  type: TYPE_NORMAL
- en: A note to the readers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For learning purposes, we use smaller models throughout this book in our code
    listings such that you can work with them on a single GPU either locally or using
    a service like Colab or Kaggle, which offers a free tier of a single GPU. While
    the listings could be run on CPU-only hardware, you won’t want to do it. Ultimately,
    there shouldn’t be any need to run these costly VMs throughout the book. However,
    you likely will still want to. Training with multiple GPUs is much faster, more
    efficient, and often necessary. We do encourage you to try larger LLM variations
    that require these bigger rigs, as the experience will be priceless. To make it
    easy, you should be able to recycle the code in this chapter for models and datasets
    much larger than what is presented, which will often just be a matter of changing
    a few lines.
  prefs: []
  type: TYPE_NORMAL
- en: Google virtual machine
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of the easiest ways to create a multi-GPU environment is to set up a VM
    on Google’s cloud. To get started, you’ll need to create an account, create a
    Google Cloud Project (GCP), set up billing, and download the gcloud CLI. None
    of these steps are particularly hard, but be sure to follow the documentation
    found at [https://cloud.google.com/sdk/docs/install-sdk](https://cloud.google.com/sdk/docs/install-sdk)
    for your operating system to install the SDK. The steps here also include the
    steps and how-tos for creating an account, project, and billing in the Before
    You Begin section if you don’t already have an account.
  prefs: []
  type: TYPE_NORMAL
- en: For new accounts, Google offers a $300 credit to be used for pretty much anything
    on their GCP platform except GPUs. We hate to break this news, but sadly, there’s
    just no free lunch where we are going. So you’ll need to be sure to upgrade to
    a paid GCP tier. Don’t worry; just following along should only cost a couple of
    dollars, but if you are money conscious, we recommend reading the entire section
    first and then trying it out.
  prefs: []
  type: TYPE_NORMAL
- en: After setting up your account, by default, GCP sets your GPU quotas to 0\. Quotas
    are used to manage your costs. To increase your quotas, go to [https://console.cloud.google.com/iam-admin/quotas](https://console.cloud.google.com/iam-admin/quotas).
    You’ll be looking for the gpus_all_regions quota, and since we plan to use multiple
    GPUs, go ahead and submit a request to increase it to 2 or more.
  prefs: []
  type: TYPE_NORMAL
- en: 'With all the prerequisites in place, we’ll get started by initializing and
    logging in. You’ll do this by running the following command in a terminal on your
    computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: You may have already done this step if you had to install the SDK, but if not,
    it will launch a web browser to help us log in and authorize us for the gcloud
    CLI, which allows us to select our project. We will be assuming you have just
    the one project, but if this isn’t your first rodeo and you have multiple projects,
    you’ll need to add the `--project` flag in all the subsequent commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to determine two things: the machine type (or which GPUs we want
    to use) and our container image. To pick a machine type, you can check out the
    different options at [https://cloud.google.com/compute/docs/gpus](https://cloud.google.com/compute/docs/gpus).
    For beginners, we highly recommend the NVIDIA L4 GPU, as it is an all-around fantastic
    machine. For our purposes, we’ll be using the g2-standard-24, which comes with
    two L4 GPUs and costs us about $2 per hour. This machine type isn’t in every region
    and zone, but you can find a region close to you at [https://cloud.google.com/compute/docs/regions-zones](https://cloud.google.com/compute/docs/regions-zones).
    We will be using the us-west1 region and us-west1-a zone.'
  prefs: []
  type: TYPE_NORMAL
- en: For the container image, we’ll save ourselves a lot of hassle by using one that
    has all the basics set up. Generally, this means creating your own, but Google
    has several prebuilt container images for deep learning, which are great to use
    or a great place to start as a base image to customize. These are all found in
    the `deeplearning-platform -release` project that they own. To check out the options
    available, you can run
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'NOTE  You can learn more about the container image options here: [https://cloud.google.com/deep-learning-vm/docs/images](https://cloud.google.com/deep-learning-vm/docs/images).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can pick from Base, TensorFlow, and PyTorch compiled images, along with
    the CUDA and Python versions. We’ll be using `common-gpu-v20230925-debian-11-py310`,
    which is a simple image ready for GPU with a Debian Linux distribution and Python
    3.10\. Now that we have everything we need, we can create our VM! Go ahead and
    run the following commands to set up the VM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The first command creates an environment variable to store the name of our VM
    since we’ll also be using it in several of the following commands. This name can
    be whatever you want it to be. The next command creates our VM instance. The first
    several flags (`zone`, `image`, `machine`) should make sense since we just spent
    the previous paragraphs preparing and gathering that information. The `boot-disk-size`
    sets the disk space for our VM and defaults to 200 GB, so it’s included here because
    it’s important to know for LLMs since they are large assets, and you will likely
    need to increase it—especially for LLMs that require multiple GPUs to run.
  prefs: []
  type: TYPE_NORMAL
- en: The `scopes` flag is passed to set authorization. Current GCP best practices
    recommend setting it to `cloud-platform`, which determines authorization through
    OAuth and IAM roles. The `metadata` field isn’t required but is used here as a
    trick to ensure the NVIDIA drivers are installed. It is really useful if you are
    using these commands to create a shell script to automate this process. You should
    know that it will cause a small delay between when the VM is up and when you can
    actually SSH into it, as it won’t be responsive while it installs the drivers.
    If you don’t include it, the first time you SSH in through a terminal, it will
    ask you if you want to install it, so no harm done. However, if you access the
    VM through other methods (described in the next sections), you can run into problems.
    The last two commands are standard maintenance policies.
  prefs: []
  type: TYPE_NORMAL
- en: Once that runs, you can verify the VM is up by running
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will give you a lot of information about your instance that is
    worth looking over, including a status field that should read `''RUNNING''`. Once
    you’ve confirmed that it’s up, we will SSH into it. If this is your first time
    using gcloud to SSH, an SSH key will be generated automatically. Go ahead and
    run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Your terminal will be shelled into our multi-GPU VM, and you are now in business.
    At this point, your VM is still just an empty shell, so you’ll want to bring in
    code. The easiest way to do this is to copy the files over with Secure Copy Protocol
    (SCP). You can do this for a single file or a whole directory. For example, assuming
    your project has a requirements.txt file and a subdirectory local-app-folder,
    from a new terminal, you can run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Overall, not too bad. Once you’ve gone through the process and set everything
    up, the next time you set up a VM, it will only be four commands (`create`, `describe`,
    `ssh`, `scp`) to get up and running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, these instances cost good money, so the last command you’ll want
    to know before moving on is how to delete it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: For Linux power users, this code line is likely all you need, but for the rest
    of us plebs, shelling into a VM through a terminal is less than an ideal working
    environment. We’ll show you some tips and tricks to make the most of your remote
    machine.
  prefs: []
  type: TYPE_NORMAL
- en: SSH through VS Code
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For most devs, a terminal is fine, but what we really want is an IDE. Most
    IDEs offer remote SSH capabilities, but we’ll demonstrate with VS Code. The first
    step is to install the extension Remote-SSH (you can find the extension here:
    [https://mng.bz/q0dE](https://mng.bz/q0dE)). Other extensions offer this capability,
    but Remote-SSH is maintained by Microsoft and has over 17 million installs, so
    it’s a great choice for beginners.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we are going to run a configuration command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, inside of VS Code, you can press F1 to open the command palette and run
    the Remote-SSH: Open SSH Host… command, and you should see your VM’s SSH address,
    which will look like l4-llm-example.us-west1-a.project-id-401501\. If you don’t
    see it, something went wrong with the `config-ssh` command, and you likely need
    to run `gcloud` `init` again. Select the address, and a new VS Code window should
    pop up. In the bottom corner, you’ll see that it is connecting to your remote
    machine. And you are done! Easy. From here, you can use VS Code like you would
    when using it locally.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although setting up hardware is important, none of it will work without the
    software packages that enable different points of hardware to communicate with
    each other effectively. With LLMs, the importance of the software is compounded.
    One author personally experienced having all hardware correctly configured and
    was pretty sure the software setup was likewise configured, only to start up training
    a model and be met with an estimated training time of over three years. After
    troubleshooting, the team realized this was because he had installed multiple
    versions of CUDA Toolkit, and PyTorch was looking at an incompatible (up-to-date)
    one instead of the one he had intended to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'These software packages are about more than just using the CUDA low-level communication
    with your GPU; they’re about load-balancing, quantizing, and parallelizing your
    data as it runs through each computation to make sure it’s going as fast as possible
    while still enabling a certain level of fidelity for the matrices. You wouldn’t
    want to spend a long time making sure your embedding vectors are phenomenal representations
    just to have them distorted at run time. Thus, we present the four deep-learning
    libraries every practitioner should know for multi-GPU instances: DeepSpeed, Accelerate,
    BitsandBytes, and xFormers. At the time of this writing, all complementary features
    between these libraries are experimental, so feel free to mix and match. If you
    get a setup that utilizes all four at once to their full potential without erroring,
    drop it in a reusable container so fast.'
  prefs: []
  type: TYPE_NORMAL
- en: DeepSpeed
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: DeepSpeed is an optimization library for distributed deep learning. DeepSpeed
    is powered by Microsoft and implements various enhancements for speed in training
    and inference, like handling extremely long or multiple inputs in different modalities,
    quantization, caching weights and inputs, and, probably the hottest topic right
    now, scaling up to thousands of GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Installation is fairly simple if you remember to always install the latest—but
    not nightly—version of PyTorch first. This means you also need to configure your
    CUDA Toolkit beforehand. Once you have that package, `pip` `install` `deepspeed`
    should get you right where you want to go unless, ironically, you use Microsoft’s
    other products. If you are on a Windows OS, there is only partial support, and
    there are several more steps you will need to follow to get it working for inference,
    not training, mode.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: From Hugging Face, Accelerate is made to help abstract the code for parallelizing
    and scaling to multiple GPUs away from you so that you can focus on the training
    and inference side. One huge advantage of Accelerate is that it adds only one
    import and two lines of code and changes two other lines, compared to a standard
    training loop in PyTorch in its vanilla implementation. Beyond that, Accelerate
    also has fairly easy CLI usage, allowing it to be automated along with Terraform
    or AWS CDK.
  prefs: []
  type: TYPE_NORMAL
- en: Accelerate boasts compatibility over most environments, and as long as your
    environment is Python 3.8+ and PyTorch 1.10.0+ (CUDA compatibility first), you
    should be able to use Accelerate without problems. Once that’s done, `pip` `install`
    `accelerate` should get you there. Accelerate also has experimental support for
    DeepSpeed if you would like to get the benefits of both.
  prefs: []
  type: TYPE_NORMAL
- en: BitsandBytes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'If you don’t already know the name Tim Dettmers in this field, you should become
    acquainted pretty quickly. Not many people have done as much as he has to make
    CUDA-powered computing accessible. This package is made to help practitioners
    quantize models and perform efficient matrix multiplication for inference (and
    maybe training) within different bit sizes, all the way down to INT8\. BitsandBytes
    has similar requirements and drawbacks to DeepSpeed: the requirements are Python
    3.8+ and CUDA 10.0+ on Linux and Mac environments and partial support for Windows
    with a different package.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should have little trouble installing BitsandBytes, as `pip` `install`
    `bitsandbytes` should work for most use cases. If you find yourself on Windows,
    you’re in luck: `pip` `install` `bitsandbytes-windows` will work as well. If you
    want to use it with Hugging Face’s transformers or PyTorch, you will need to edit
    some minimum requirements stated within both of those packages, as the Windows
    version does not have the same version numbers as the regular package. BitsandBytes
    offers its own implementations of optimizers like Adam and NN layers like Linear
    to allow for that 8-bit boost to run deep learning apps on smaller devices at
    greater speed with a minimal drop in accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: xFormers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The most bleeding edge of the libraries we recommend for most use cases is xFormers,
    which is made for research and production. Following a (hopefully) familiar PyTorch-like
    pattern of independent building blocks for multiple modalities, xFormers takes
    it a step further and offers components that won’t be available in PyTorch for
    quite a while. One that we’ve used quite a lot is memory-efficient exact attention,
    which speeds up inference considerably.
  prefs: []
  type: TYPE_NORMAL
- en: xFormers has more requirements than the other packages, and we’d like to stress
    once more that using one or more tools to keep track of your environment is strongly
    recommended. On Linux and Windows, you’ll need PyTorch 2.0.1, and `pip` `install`
    `-U` `xFormers` should work for you. That said, there are paths for installation
    with pretty much any other version of PyTorch, but the main ones are versions
    1.12.1, 1.13.1, and 2.0.1.
  prefs: []
  type: TYPE_NORMAL
- en: In table 5.1, we can see a heavily reduced breakdown of what each of these packages
    does and how it integrates with your code. Each package does similar things, but
    even when performing the same task, they will often perform those tasks differently
    or on different parts of your model or pipeline. There is some overlap between
    packages, and we’d encourage you to use all of them to see how they might benefit
    you. Now that you have an environment and a basic understanding of some of the
    tools we’ll be using, let’s move forward and see it in action.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.1 Comparison of optimization packages for ML
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Library | Faster training or inference | Code integration | Lower accuracy
    | Many GPUs | Quantization | Optimizations |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DeepSpeed  | Both  | CLI  | Depends  | Yes  | Supports  | Caching, gradient
    checkpointing, memory management, scaling  |'
  prefs: []
  type: TYPE_TB
- en: '| Accelerate  | Both  | CLI and Code  | Depends  | Yes  | Supports  | Automation,
    compiling, parallelization  |'
  prefs: []
  type: TYPE_TB
- en: '| BitsandBytes  | Both  | Code  | Always  | NA  | Yes but only  | Quantization,
    quantized optimizers  |'
  prefs: []
  type: TYPE_TB
- en: '| xFormers  | Training  | Code  | Depends  | NA  | Yes and more  | Efficient
    attention, memory management  |'
  prefs: []
  type: TYPE_TB
- en: 5.2 Basic training techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In training LLMs, the process typically starts with defining the architecture
    of the model, the nature and amount of data required, and the training objectives.
    We’ve already gone over these steps in the last chapter, so you should be well
    prepared already, but let’s look at a brief recap. The model architecture usually
    follows a variant of the Transformer architecture due to its effectiveness in
    capturing long-term dependencies and its parallelizable nature, making it amenable
    to large-scale computation. Data is the lifeblood of any LLM (or any ML model
    in general), which typically requires extensive corpora of diverse and representative
    text data. As the model’s purpose is to learn to predict the next word in a sequence,
    it’s crucial to ensure that the data covers a wide array of linguistic contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Because we’ll be going over various training techniques in this chapter, here’s
    a (super) quick rundown of the investments you’ll need for different types. For
    training from scratch, you’ll need VRAM greater than four times the number of
    billions of parameters to hold the model, along with the batches of training data.
    So to train a 1B parameter model from scratch, you’ll need at least 5 or 6 GB
    of VRAM, depending on your batch sizes and context length. Consider training a
    70B parameter model like Llama 2 as an exercise. How much VRAM will you need to
    fit the model, along with a 32K token context limit? If you’re coming up with
    a number around 300 GB of VRAM, you’re right. For the finetuning techniques, you’ll
    need significantly fewer resources for a couple of reasons—namely, quantization
    and amount of data needed, meaning you no longer need 4× VRAM, but can use 2×
    or 1× with the correct setup.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike traditional ML models, LLMs are often trained in stages. Figure 5.1 shows
    the basic training life cycle of an LLM, starting from scratch, then finetuning,
    and finally prompting. The first step is creating our foundation model, where
    we take a large, often unrefined, dataset and train an empty shell of a model
    on it. This training will create a model that has seen such a large corpus of
    text that it appears to have a basic understanding of language. We can then take
    that foundation model and use transfer learning techniques, generally finetuning
    on a small, highly curated dataset to create a specialized LLM for expert tasks.
    Lastly, we use prompting techniques that, while not traditional training, allow
    us to goad the model to respond in a particular fashion or format, improving the
    accuracy of our results.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 The training life cycle of an LLM. We start by creating a foundation
    model based on a large corpus of text, which we later finetune using a curated
    dataset for a specific task. We can then further improve the model by using the
    model itself and techniques like prompting to enhance or enlarge our curated dataset.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You’ll notice that the training life cycle is often a continuous loop—training
    models to understand language better and then using those models to improve our
    training datasets. Later in this chapter, we will go into more depth about other
    advanced training techniques that take advantage of this loop, like prompt tuning
    and RLHF. For now, let’s solidify our understanding of three basic steps.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 From scratch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training an LLM is computationally intensive and can take several weeks or months
    even on high-performance hardware. This process feeds chunks of data (or “batches”)
    to the model and adjusts the weights based on the calculated loss. Over time,
    this iterative process of prediction and adjustment, also known as an epoch, leads
    the model to improve its understanding of the syntactic structures and complexities
    in the data. It’s worth noting that monitoring the training process is crucial
    to avoid overfitting, where the model becomes excessively tailored to the training
    data and performs poorly on unseen data. Techniques like early stopping, dropout,
    and learning rate scheduling are used to ensure the generalizability of the model,
    but they are not silver bullets. Remember, the ultimate goal is not just to minimize
    the loss on training data but to create a model that can understand and generate
    human-like text across a broad range of contexts.
  prefs: []
  type: TYPE_NORMAL
- en: Training an LLM from scratch is a complex process that begins with defining
    the model’s architecture. This decision should be guided by the specific task
    at hand, the size of the training dataset, and the available computational resources.
    The architecture, in simple terms, is a blueprint of the model that describes
    the number and arrangement of layers, the type of layers (like attention or feed-forward
    layers), and the connections between them. Modern LLMs typically employ a variant
    of the Transformer architecture, known for its scalability and efficiency in handling
    long sequences of data.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model’s architecture is set, the next step is to compile a large and
    diverse dataset for training. The quality and variety of data fed into the model
    largely dictate the model’s ability to understand and generate human-like text.
    A common approach is to use a large corpus of internet text, ensuring a wide-ranging
    mix of styles, topics, and structures. The data is then preprocessed and tokenized,
    converting the raw text into a numerical format that the model can learn from.
    During this tokenization process, the text is split into smaller units, or tokens,
    which could be as short as a single character or as long as a word.
  prefs: []
  type: TYPE_NORMAL
- en: With a model and dataset ready, the next step is to initialize the model and
    set the learning objectives. The LLMs are trained using autoregressive semi-supervised
    learning techniques where the model learns to predict the next word in a sequence
    given the preceding words. The model’s weights are randomly initialized and then
    adjusted through backpropagation and optimization techniques such as Adam or Stochastic
    Gradient Descent based on the difference between the model’s predictions and the
    actual words in the training data. The aim is to minimize this difference, commonly
    referred to as the “loss,” to improve the model’s predictive accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training involves feeding the tokenized text into the model and adjusting the
    model’s internal parameters to minimize the loss. We said this once, but it bears
    repeating: this process is computationally demanding and may take weeks or even
    months to complete, depending on the model size and available hardware. After
    training, the model is evaluated on a separate validation dataset to ensure that
    it can generalize to unseen data. It is common to iterate on this process, finetuning
    the model parameters and adjusting the architecture as needed based on the model’s
    performance on the validation set.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore training a brand-new transformer-based language model “from scratch,”
    meaning without any previously defined architecture, embeddings, or weights. Figure
    5.2 shows this process. You shouldn’t have to train an LLM from scratch, nor would
    you normally want to, as it’s a very expensive and time-consuming endeavor; however,
    knowing how can help you immensely.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 A simplified version of all the steps necessary to train a language
    model (large or otherwise) from scratch. You must have data, then define all of
    the model behavior, and only then proceed to train.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Listing 5.1 allows you to run through the motions without training an actual
    massive model, so feel free to explore with this code. For a more complex and
    complete example, check out Andrej Karpathy’s minGPT project here: [https://github.com/karpathy/minGPT](https://github.com/karpathy/minGPT).
    You should pay attention to some things when you review the listing. You might
    recall that we talked about tokenization and embeddings in the last chapter, so
    one thing to notice is that for simplicity, we will be using a character-based
    tokenizer. Before you run the code, can you predict whether this was a good or
    bad idea? Also, pay attention to how we use both Accelerate and BitsandBytes,
    which we introduced a little bit ago; you’ll see that these libraries come in
    mighty handy. Next, watch as we slowly build up the LLMs architecture, building
    each piece in a modular fashion and later defining how many of each piece is used
    and where to put them, almost like Legos. Finally, at the very end of the code,
    you’ll see a typical model training loop, splitting our data, running epochs in
    batches, and so forth.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 An example of training from scratch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Δefines the overall GPT architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Δefines the building blocks of the model'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Helper functions for training'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Trains the model'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Parameters for our experiment'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Δataset'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Character-based pseudo-tokenization'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Instantiates the model and looks at the parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Training block'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Creates model directory'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Saves the model'
  prefs: []
  type: TYPE_NORMAL
- en: '#12 Loads the saved model'
  prefs: []
  type: TYPE_NORMAL
- en: '#13 Tests the loaded model'
  prefs: []
  type: TYPE_NORMAL
- en: In listing 5.1, we explored how the Lego blocks are put together for the GPT
    family of models and showed a training loop reminiscent of our exploration of
    language modeling in chapter 2\. Beyond showing the first part of generative pretraining
    for models, this example also illustrates why character-based modeling, whether
    convolutional or otherwise, is weak for language modeling. Did you get it right?
    Yup, character-based modeling isn’t the best. Alphabets on their own do not contain
    enough information to produce statistically significant results, regardless of
    the tuning amount. From a linguistic standpoint, this is obvious, as alphabets
    and orthography, in general, are representations of meaning generated from humans,
    which is not intrinsically captured.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the ways to help with that information capture are increasing our tokenization
    capture window through word-, subword-, or sentence-level tokenization. We can
    also complete the pretraining before showing the model our task to allow it to
    capture as much approximate representation as possible. Next, we’ll show what
    benefits combining these two steps can have on our model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Transfer learning (finetuning)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Transfer learning is an essential approach in machine learning and a cornerstone
    of training LLMs. It’s predicated on the notion that we can reuse knowledge learned
    from one problem (the source domain) and apply it to a different but related problem
    (the target domain). In the context of LLMs, this typically means using a pretrained
    model, trained on a large, diverse dataset, and adapting it to a more specific
    task or domain.
  prefs: []
  type: TYPE_NORMAL
- en: In the first step of transfer learning, an LLM is trained on a large, general-purpose
    corpus, such as the entirety of Wikipedia, books, or the internet. This pretraining
    stage allows the model to learn an extensive range of language patterns and nuances
    on a wide variety of topics. The goal here is to learn a universal representation
    of language that captures a broad understanding of syntax, semantics, and world
    knowledge. These models are often trained for many iterations and require significant
    computational resources, which is why it’s practical to use pretrained models
    provided by organizations like OpenAI or Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: After pretraining, the LLM is updated on a specific task or domain. This update
    process adapts the general-purpose language understanding of the model to a more
    specific task, such as sentiment analysis, text classification, or question answering.
    Updating usually requires significantly less computational resources than the
    initial pretraining phase because it involves training on a much smaller dataset
    specific to the task at hand. Through this process, the model is able to apply
    the vast knowledge it gained during pretraining to a specific task, often outperforming
    models trained from scratch on the same task. This process of transfer learning
    has led to many of the advances in NLP over recent years.
  prefs: []
  type: TYPE_NORMAL
- en: Finetuning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are several different transfer learning techniques, but when it comes
    to LLMs, the one everyone cares about is finetuning. Finetuning an LLM involves
    taking a pretrained model—that is, a model already trained on a large general
    corpus—and adapting it to perform a specific task or to understand a specific
    domain of data.
  prefs: []
  type: TYPE_NORMAL
- en: This technique uses the fact that the base model has already learned a significant
    amount about the language, allowing you to reap the benefits of a large-scale
    model without the associated computational cost and time. The process of finetuning
    adapts the pre-existing knowledge of the model to a specific task or domain, making
    it more suitable for your specific use case. It’s like having a generalist who
    already understands the language well and then providing specialist training for
    a particular job. This approach is often more feasible for most users due to the
    significantly reduced computational requirements and training time compared to
    training a model from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: The first step in finetuning involves choosing a suitable pretrained model.
    This decision is guided by the specific task you want the model to perform and
    by the resources available to you. Keep in mind that this means setting a goal
    for the model’s behavior before training. Once the pretrained model has been chosen,
    it’s crucial to prepare the specific dataset you want the model to learn from.
    This data could be a collection of medical texts, for example, if you’re trying
    to finetune the model to understand medical language. The data must be preprocessed
    and tokenized in a way that’s compatible with the model’s pretraining.
  prefs: []
  type: TYPE_NORMAL
- en: 'The finetuning process involves training the model on your specific dataset,
    but with a twist: instead of learning from scratch, the model’s existing knowledge
    is adjusted to better fit the new data. This finetuning is typically done with
    a smaller learning rate than in the initial training phase to prevent the model
    from forgetting its previously learned knowledge. After finetuning, the model
    is evaluated on a separate dataset to ensure it can generalize to unseen data
    in the specific domain. Similar to training from scratch, this process may involve
    several iterations to optimize the model’s performance. Finetuning offers a way
    to harness the power of LLMs for specific tasks or domains without the need for
    extensive resources or computation time. See figure 5.3.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 Finetuning differs from training from scratch in that you don’t have
    to define model behavior, you can use the exact same training loop, and you have
    a fraction of the data requirement.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In listing 5.2, we show you how to finetune a GPT model. Notice how much less
    code there is in this listing than in listing 5.1\. We don’t need to define an
    architecture or a tokenizer; we’ll just use those from the original model. Essentially,
    we get to skip ahead because weights and embeddings have already been defined.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.2 An example of finetuning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Loads and formats the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates model directory to save to'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Establishes our GPT-2 parameters (different from the paper and scratchGPT)'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Instantiates our tokenizer and our special tokens'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Instantiates our model from the config'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Creates a tokenize function'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Tokenizes our whole dataset (so we never have to do it again)'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Creates a data collator to format the data for training'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Establishes training arguments'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Instantiates the Trainer'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Trains and saves the model'
  prefs: []
  type: TYPE_NORMAL
- en: '#12 Loads the saved model'
  prefs: []
  type: TYPE_NORMAL
- en: '#13 Tests the saved model'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at listing 5.2 compared with listing 5.1, they have almost the exact
    same architecture (minus the activation function), and they’re training on exactly
    the same data. Yet, there’s a marked improvement with the finetuned GPT-2 model
    due to the lack of learned representation in the first model. Our pretrained model,
    along with subword BPE tokenization instead of character-based, helps the model
    figure out which units of statistically determined meaning are most likely to
    go together. You’ll notice, though, that GPT-2, even with pretraining, struggles
    to generate relevant longer narratives despite using a newer, better activation
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Finetuning OpenAI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We just trained a GPT model from scratch, and then we finetuned GPT-2, but
    we know many readers really want the power behind OpenAI’s larger GPT models.
    Despite being proprietary models, OpenAI has graciously created an API where we
    can finetune GPT-3 models. Currently, three models are available for finetuning
    with OpenAI’s platform, but it looks like it intends to extend that finetuning
    ability to all of its models on offer. OpenAI has written a whole guide, which
    you can find at [http://platform.openai.com/](http://platform.openai.com/), but
    once you have your dataset prepared in the necessary format, the code is pretty
    easy. Here are some snippets for various tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This first snippet uploads a training dataset in the correct format for the
    platform and specifies the purpose as finetuning, but doesn’t start the process
    yet. Next, you’ll need to create the finetuning job:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This is where you specify which training file and which model you want to finetune.
    Once OpenAI’s training loop has completed, you’ll see the finetuned model’s name
    populated when you retrieve the job details. Now you can use that model the same
    way you would have used any of the vanilla ones for chat completion or anything
    else like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it for finetuning an OpenAI model! Very simple, doesn’t take too
    long, and as of March 2023, your data is private to you. Of course, you’ll be
    ceding all of the control of how that finetuning occurs over to OpenAI. If you’d
    like to do something beyond vanilla finetuning, you’ll need to do that yourself.
    In just a minute, we’ll go over those techniques you may consider, along with
    some more advanced processes that can help with more fine-grained models and more
    complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3 Prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the main reasons why LLMs are so powerful compared to traditional ML
    is because we can train them at run time. Give them a set of instructions and
    watch them follow them to the best of their ability. This technique is called
    prompting and is used in LLMs to guide the model’s output. In essence, the prompt
    is the initial input given to the model that provides it with context or instructions
    for what it should do. For example, “translate the following English text to French”
    and “summarize the following article” are prompts. In the context of LLMs, prompting
    becomes even more critical, as these models are not explicitly programmed to perform
    specific tasks but learn to respond to a variety of tasks based on the given prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering refers to the process of crafting effective prompts to guide
    the model’s behavior. The aim is to create prompts that lead the model to provide
    the most desirable or useful output. Prompt engineering can be more complex than
    it appears, as slight changes in how a prompt is phrased can lead to vastly different
    responses from the model. Some strategies for prompt engineering include being
    more explicit in the prompt, providing an example of the desired output, or rephrasing
    the prompt in different ways to get the best results. It’s a mixture of art and
    science, requiring a good understanding of the model’s capabilities and limitations.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to focus mainly on training and finetuning, the
    steps before deployment, but we would be remiss if we didn’t first mention prompting.
    We will talk about prompting in much more depth in chapter 7.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Advanced training techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that you know how to do the basics, let’s go over some more advanced techniques.
    These techniques have been developed for a variety of reasons, such as improving
    generated text outputs, shrinking the model, providing continuous learning, speeding
    up training, and reducing costs. Depending on the needs of your organization,
    you may need to reach for a different training solution. While not a comprehensive
    list, the following techniques are often used and should be valuable tools as
    you prepare a production-ready model.
  prefs: []
  type: TYPE_NORMAL
- en: Classical ML training background
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Going over some techniques to enhance your finetuning process requires a bit
    of background. We won’t be doing a full course in ML; however, in case this is
    your first exposure, you should know some classic learning paradigms that experiments
    tend to follow—supervised, unsupervised, adversarial, and reinforcement:'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised learning involves collecting both the data to train on and the labels
    showcasing the expected output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised learning does not require labels, as the data is probed for similarity
    and grouped into clusters that are the closest comparison to each other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial learning is what’s used to train a generative adversarial network.
    It involves two models, generally referred to as the Critic model and the Forger
    model. These two models essentially play a game against each other where the forger
    tries to copy some ideal output, and the critic tries to determine whether the
    forgery is the real thing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning (RL) opts for establishing a reward function instead
    of having predefined labels for the model to learn from. By measuring the model’s
    actions, it is given a reward based on that function instead.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All LLMs must be trained using at least one of these, and they perform at a
    high level with all of them done correctly. The training techniques discussed
    in this chapter differ from those basic ones, ranging from adding some form of
    human input to the model to comparing outputs to changing how the model does matrix
    multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Prompt tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve gone over pragmatics before, but as a reminder, language models perform
    better when given real-world nonsemantic context pertaining to the tasks and expectations.
    Language modeling techniques all operate on the underlying assumption that the
    LM, given inputs and expected outputs, can divine the task to be done and do it
    in the best way within the number of parameters specified.
  prefs: []
  type: TYPE_NORMAL
- en: While the idea of the model inferring both the task and the method of completing
    it from the data showed promise, it has been shown time and time again, from BERT
    to every T5 model and now to all LLMs, that providing your model with the expected
    task and relevant information for solving the task improves model performance
    drastically. As early as 2021, Google Research, DeepMind, and OpenAI had all published
    papers about prompt tuning, or giving a model pragmatic context during training.
    The benefits of prompt tuning are reducing the amount of data required for the
    model to converge during training and, even cooler, the ability to reuse a completely
    frozen language model for new tasks without retraining or fully finetuning.
  prefs: []
  type: TYPE_NORMAL
- en: Because LLMs are so large (and getting larger), it is becoming increasingly
    difficult to share them and even more difficult to guarantee their performance
    on a given task, even one they are trained on. Prompt tuning can help nudge the
    model in the right direction without becoming a significant cost. Figure 5.4 shows
    this process.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 Prompt tuning foregoes most finetuning to allow the majority of the
    foundation model’s language understanding ability to stay exactly the same and,
    instead, focuses on changing how the model responds to specific inputs.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Listing 5.3 shows how to prompt tune a smaller variant of the BLOOMZ model from
    Big Science. BLOOMZ was released as an early competitor in the LLM space but has
    ultimately struggled to garner attention or momentum in the community because
    of its inability to generate preferred outputs despite its mathematical soundness.
    Because prompt tuning doesn’t add much to the regular finetuning structure we
    used in listing 5.2, we’ll perform Parameter-Efficient Fine-Tuning (PEFT), which
    drastically reduces the memory requirements by determining which model parameters
    need changing the most.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.3 An example of prompt tuning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Helper function to preprocess text; go ahead and skip to the training'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Model prompt tuning'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Δefines prompt tuning config; notice init_text'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Loads Δataset'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Labels the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Loads tokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Runs Tokenizer across dataset and preprocess'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Prepares data loaders'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Loads foundation model'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Δefines optimizer'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Training steps'
  prefs: []
  type: TYPE_NORMAL
- en: '#12 Creates model directory to save to'
  prefs: []
  type: TYPE_NORMAL
- en: '#13 Saving'
  prefs: []
  type: TYPE_NORMAL
- en: '#14 Inference'
  prefs: []
  type: TYPE_NORMAL
- en: Other than the changed setup, the main difference between listings 5.2 and 5.3
    is simply prepending a prompt with some sort of instruction to the beginning of
    each input, reminiscent of the T5 training method that pioneered having a prepended
    task string before every input. Prompt tuning has emerged as a powerful technique
    for finetuning large language models to specific tasks and domains. By tailoring
    prompts to the desired output and optimizing them for improved performance, we
    can make our models more versatile and effective. However, as our LLMs continue
    to grow in scale and complexity, it becomes increasingly challenging to efficiently
    finetune them on specific tasks. This is where knowledge distillation comes into
    play, offering a logical next step. Knowledge distillation allows us to transfer
    the knowledge and expertise of these highly tuned models to smaller, more practical
    versions, enabling a wider range of applications and deployment scenarios. Together,
    prompt tuning and knowledge distillation form a dynamic duo in the arsenal of
    techniques for harnessing the full potential of modern LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Finetuning with knowledge distillation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Knowledge distillation is an advanced technique that provides a more efficient
    path to finetuning an LLM. Rather than just finetuning an LLM directly, knowledge
    distillation involves transferring the knowledge from a large, complex model (the
    teacher) to a smaller, simpler model (the student). The aim is to create a more
    compact model that retains the performance characteristics of the larger model
    but is more efficient in terms of resource usage. Figure 5.5 shows this process.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 Knowledge distillation allows a smaller model to learn from a foundation
    model to replicate similar behavior with fewer parameters. The student model does
    not always learn the emergent qualities of the foundation model, so the dataset
    must be especially curated. The dotted line indicates a special relationship as
    the student model becomes the specialized LLM.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The first step in knowledge distillation is to select a pre-trained LLM as the
    teacher model. This could be any of the large models, such as Llama 2 70B or Falcon
    180B, which have been trained on vast amounts of data. You also need to create
    or select a smaller model as the student. The student model might have a similar
    architecture to the teacher’s, but with fewer layers or reduced dimensionality
    to make it smaller and faster.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the student model is trained on the same task as the teacher model. However,
    instead of learning from the raw data directly, the student model learns to mimic
    the teacher model’s outputs. This training is typically done by adding a term
    to the loss function that encourages the student model’s predictions to be similar
    to the teacher model’s predictions. Thus, the student model not only learns from
    the task-specific labels but also benefits from the rich representations learned
    by the teacher model.
  prefs: []
  type: TYPE_NORMAL
- en: Once the distillation process is complete, you’ll have a compact student model
    that can handle the specific tasks learned from the teacher model but at a fraction
    of the size and computational cost. The distilled model can then be further finetuned
    on a specific task or dataset if required. Through knowledge distillation, you
    can use the power of LLMs in situations where computational resources or response
    time are limited.
  prefs: []
  type: TYPE_NORMAL
- en: In listing 5.4, we show how to perform finetuning with knowledge distillation
    using BERT and becoming DistilBERT. As opposed to regular finetuning, pay attention
    to the size and performance of the model. Both will drop; however, size will drop
    much faster than performance.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.4 An example of knowledge distillation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Place teacher on same device as student'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Computes student output'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Computes teacher output'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Asserts size'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Returns weighted student loss'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Creates model directory to save to'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Δefines the teacher and student models'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Creates label2id, id2label dicts for nice outputs for the model'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Δefines training args'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Pushes to hub parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Δistillation parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '#12 Δefines data_collator'
  prefs: []
  type: TYPE_NORMAL
- en: '#13 Δefines model'
  prefs: []
  type: TYPE_NORMAL
- en: '#14 Δefines student model'
  prefs: []
  type: TYPE_NORMAL
- en: '#15 Δefines metrics and metrics function'
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge distillation, as exemplified by the provided `compute_loss` method,
    is a technique that enables the transfer of valuable insights from a teacher model
    to a more lightweight student model. In this process, the teacher model provides
    soft targets, offering probability distributions over possible outputs, which
    are then utilized to train the student model. The critical aspect of knowledge
    distillation lies in the alignment of these distributions, ensuring that the student
    model not only learns to mimic the teacher’s predictions but also gains a deeper
    understanding of the underlying data. This approach helps improve the student’s
    generalization capabilities and performance on various tasks, ultimately making
    it more efficient and adaptable.
  prefs: []
  type: TYPE_NORMAL
- en: As we look forward, one logical progression beyond knowledge distillation is
    the incorporation of RLHF. While knowledge distillation enhances a model’s ability
    to make predictions based on existing data, RLHF allows the model to learn directly
    from user interactions and feedback. This dynamic combination not only refines
    the model’s performance further but also enables it to adapt and improve continuously.
    By incorporating human feedback, RL can help the model adapt to real-world scenarios,
    evolving its decision-making processes based on ongoing input, making it an exciting
    and natural evolution in the development of LLM systems.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.3 Reinforcement learning with human feedback
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RLHF is a newer training technique developed to overcome one of the biggest
    challenges when it comes to RL: how to create reward systems that actually work.
    It sounds easy, but anyone who’s played around with RL knows how difficult it
    can be. Before AlphaStar, one author was building his own RL bot to play *StarCraft*,
    a war simulation game in space.'
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Check out [https://mng.bz/Dp4a](https://mng.bz/Dp4a) to learn more about
    AlphaStar.
  prefs: []
  type: TYPE_NORMAL
- en: A simple reward system based on winning or losing was taking too long, so he
    decided to give it some reasonable intermediate rewards based on growing an army.
    However, this got blocked when it failed to build Pylons, a building required
    to increase army supply limits. So he gave it a reward to build Pylons. His bot
    quickly learned that it liked to build Pylons—so much so that it learned to almost
    win but not win, crippling its opponent so that it could keep building Pylons
    unharassed and for as long as it wanted.
  prefs: []
  type: TYPE_NORMAL
- en: With a task like winning a game, even if it’s difficult, we can usually still
    come up with reasonable reward systems. But what about more abstract tasks, like
    teaching a robot how to do a backflip? These tasks get really difficult to design
    reward systems for, which is where RLHF comes in. What if instead of designing
    a system, we simply have a human make suggestions? A human knows what a backflip
    is, after all. The human will act like a tutor, picking attempts it likes more
    as the bot is training. That’s what RLHF is, and it works really well. Applied
    to LLMs, a human simply looks at generated responses to a prompt and picks which
    one they like more. See figure 5.6.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 RLHF substitutes a loss function for a reward model and proximal
    policy optimization (PPO), allowing the model a much higher ceiling for learning
    trends within the data, including what is preferred as an output instead of what
    completes the task.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: While very powerful, RLHF likely won’t stick around for very long. The reason
    is that it is incredibly computationally expensive for a result that is only incrementally
    better, especially a result that can be achieved and matched by higher-quality
    datasets with supervised learning approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some other problems with RLHF, such as that it requires hiring domain
    experts to evaluate and provide the human feedback. Not only can this get expensive,
    but it can also lead to privacy concerns since these reviewers would need to look
    at actual traffic and user interactions to grade them. To combat both of these
    concerns, you could try to outsource this directly to the users, asking for their
    feedback, but it may end up poisoning your data if your users have ill intent
    or are simply not experts in the subject matter, in which case they might upvote
    responses they like but that aren’t actually correct. This gets to the next problem:
    even experts have biases. RLHF doesn’t train a model to be more accurate or factually
    correct; it trains the model to generate human-acceptable answers.'
  prefs: []
  type: TYPE_NORMAL
- en: In production, RLHF has the advantage of allowing you to easily update your
    model on a continual basis. However, this is a two-edged sword, as it also increases
    the likelihood of your model degrading over time. OpenAI uses RLHF heavily, and
    it has led to many users complaining about their models, like GPT-4, becoming
    terrible in certain domains compared to when it first came out. One Stanford study
    found that GPT-4, when asked if a number was prime, used to get it right 98% of
    the time in March 2023, but three months later, in June 2023, it would only get
    it right 2% of the time.[¹](#footnote-245) One reason is that the June model is
    much less verbose, opting to give a simple yes or no response. Humans like these
    responses. Getting straight to the point is often better, but LLMs tend to be
    better after they have had time to reason through the answer with techniques like
    chain of thought.
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, RLHF is fantastic for applications where human-acceptable
    answers are the golden standard, and factually correct answers are less important—for
    example, a friendly chatbot or improving summarization tasks. These problems are
    intuitively syntactic in nature, essentially tasks that LLMs are already good
    at but which you want to refine by possibly creating a certain tone or personality.
  prefs: []
  type: TYPE_NORMAL
- en: Another reason for RLHF degradation is due to data leakage. Data leakage is
    when your model is trained on the test or validation dataset you use to evaluate
    it. When this happens, you are essentially allowing the model to cheat, leading
    to overfitting and poor generalization. It’s just like how LeetCode interview
    questions lead tech companies to hire programmers who have lots of experience
    solving toy problems but don’t know how to make money or do their job.
  prefs: []
  type: TYPE_NORMAL
- en: How does this happen? Well, simply. When you are running an LLM in production
    with RLHF, you know it’s going to degrade over time, so it’s best to run periodic
    evaluations to monitor the system. The more you run these evaluations, the more
    likely that one of the prompts will be picked up for human feedback and subsequent
    RL training. It could also happen by pure coincidence if your users happen to
    ask a question similar to a prompt in your evaluation dataset. Either way, without
    restrictions placed on RLHF (which generally are never done), it’s a self-defeating
    system.
  prefs: []
  type: TYPE_NORMAL
- en: The really annoying aspect of continual updates through RLHF is that these updates
    ruin downstream engineering efforts, methods like prompting or retrieval-augmented
    generation (RAG). Engineering teams can take a lot of effort to dial in a process
    or procedure to query a model and then clean up responses, but all that work can
    easily be undermined if the underlying model is changing. As a result, many teams
    prefer a static model with periodic updates to one with continual updates.
  prefs: []
  type: TYPE_NORMAL
- en: All that said, RLHF is still a powerful technique that may yield greater results
    later as it is optimized and refined. Also, it’s just really cool. We don’t recommend
    using RLHF, and we don’t have the space here to delve deeper; just know that it
    is a tool used by companies specializing in LLMs. For readers who want to understand
    RLHF better, we have included an in-depth example and code listing in appendix
    B.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.4 Mixture of experts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A mixture of experts (MoE) is functionally the same as any other model for
    training but contains a trick under the hood: sparsity. This gives the advantage
    of being able to train a bunch of models on a diverse set of data and tasks at
    once. You see, a MoE is exactly what it sounds like: an ensemble of identical
    models in the beginning. You can think of them as a group of freshman undergrads.
    Then, using some unsupervised grouping methods, such as k-means clustering, each
    of these experts “picks a major” during training. This allows the model only to
    activate some experts to answer particular inputs instead of all of them, or maybe
    the input is complex enough that it requires activating all of them. The point
    is that once training has completed, if it has been done on a representative-enough
    dataset, each of your experts will have a college degree in the major that they
    studied. Because the homogeneity of inputs is determined mathematically, those
    majors won’t always have a name that correlates to something you would major in
    at school, but we like to think of these as eccentric double minors or something
    of the sort. Maybe one of your experts majored in physics but double minored in
    advertising and Africana studies. It doesn’t really matter, but the major upside
    to designing an ensemble of models in this way is that you can effectively reduce
    computational requirements immensely while retaining specialization and training
    memory by only consulting the experts whose knowledge correlates with the tokenized
    input at inference time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In listing 5.5, we finetune a MoE model in much the same way as we did in listing
    5.2 with GPT-2, thanks to Hugging Face’s API and Google’s Switch Transformer.
    Unlike the method we described in chapter 3, where we turned a feed-forward network
    into an MoE, we’ll start with an already created MoE and train it on our own dataset.
    Training an MoE is pretty simple now, unlike when they first came out. Very smart
    people performed so much engineering that we can give an oversimplified explanation
    of these models. Google created the Switch Transformer to combat two huge problems
    they had run into while trying to train LLMs: size and instability. Google engineers
    simplified the routing algorithm (how the model decides which experts to query
    for each input) and showed how to train models with lower quantizations (in this
    case, bfloat16) for the first time—quite an amazing feat and not one to take lightly,
    as GPT-4 is likely an MoE.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.5 Example mixture of experts finetuning
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Loads and formats the dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates model directory to save to'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Instantiates our tokenizer'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Establishes our SwitchTransformers config'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Instantiates our model from the config'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Creates a tokenize function'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Tokenizes our whole dataset (so we never have to do it again)'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Creates a data collator to format the data for training'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Establishes training arguments'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Instantiates the trainer'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Trains and saves the model'
  prefs: []
  type: TYPE_NORMAL
- en: '#12 Loads the saved model'
  prefs: []
  type: TYPE_NORMAL
- en: '#13 Tests the saved model'
  prefs: []
  type: TYPE_NORMAL
- en: In this script, an MoE model is finetuned using the Switch Transformer foundation
    model. MoE models are unique during finetuning because you typically update the
    task-specific parameters, such as the gating mechanism and the parameters of the
    experts, while keeping the shared parameters intact. This allows the MoE to use
    the expertise of the different experts for better task-specific performance. Finetuning
    MoE models differs from traditional finetuning because it requires handling the
    experts and gating mechanisms, which can be more complex than regular neural network
    architectures. In our case, we’re lucky that `trainer.train()` with the right
    config covers it for finetuning, and we can just bask in the work that Google
    did before us.
  prefs: []
  type: TYPE_NORMAL
- en: A logical progression beyond MoE finetuning involves exploring Parameter-Efficient
    Fine-Tuning (PEFT) and low-rank adaptations (LoRA). PEFT aims to make the finetuning
    process more efficient by reducing the model’s size and computational demands,
    making it more suitable for resource-constrained scenarios. Techniques such as
    knowledge distillation, model pruning, quantization, and compression can be employed
    in PEFT to achieve this goal. In contrast, LoRA focuses on incorporating low-rank
    factorization methods into model architectures to reduce the number of parameters
    while maintaining or even enhancing model performance. These approaches are essential,
    as they enable the deployment of sophisticated models on devices with limited
    resources and in scenarios where computational efficiency is paramount.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.5 LoRA and PEFT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LoRA represents a significant breakthrough for machine learning in general.
    Taking advantage of a mathematical trick, LoRAs can change the output of a model
    without changing the original model weights or taking up significant space or
    cost, as shown in figure 5.7\. The reason for the significance here is that it
    makes finetuning a separate model for many different tasks or domains much more
    feasible, as has already been seen in the diffusion space with text2image LoRAs
    popping up quite often for conditioning model output without significantly altering
    the base model’s abilities or style. Put simply, if you already like your model
    and would like to change it to do the exact same thing in a new domain without
    sacrificing what it was already good at on its own, an adapter might be the path
    for you, especially if you have multiple new domains that you don’t want bleeding
    into one another.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 LoRA exemplifies the idea that you should only need to train and
    save the difference between where the foundation model is and where you want it
    to be. It does this through singular value decomposition (SVD).
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To understand LoRAs, you need to first understand how models currently adjust
    weights. Since we aren’t going to go over a complete backpropagation tutorial
    here, we can abstract it as
  prefs: []
  type: TYPE_NORMAL
- en: W = W + ΔW
  prefs: []
  type: TYPE_NORMAL
- en: So if you have a model with 100 100-dimensional layers, your weights can be
    represented by a 100 × 100 matrix. The cool part comes with singular value decomposition
    (SVD), which has been used for compression by factoring a single matrix into three
    smaller matrices. We covered this topic in depth back in chapter 3 (see listing
    3.2). So while we know the intuition for SVD with LLMs, what can we compress from
    that original formula?
  prefs: []
  type: TYPE_NORMAL
- en: ΔW = W[a] × W[b]
  prefs: []
  type: TYPE_NORMAL
- en: So if ΔW = 100 × 100, W[a] = 100 × c and W[b] = c × 100, where c < 100\. If
    c = 2, you can represent 10,000 elements using only 400 because when they’re multiplied
    together, they equal the 10,000 original elements. So the big question is, what
    does c equal for your task? The c-value is the “R” in LoRA, referring to the rank
    of the matrix of weights. There are algorithmic ways of determining that rank
    using eigenvectors and the like, but you can approximate a lot of it by knowing
    that a higher rank equals more complexity, meaning that the higher the number
    you use there, the closer you’ll get to original model accuracy, but the less
    memory you’ll save. If you think the task you’re finetuning the LoRA for isn’t
    as complex, reduce the rank.
  prefs: []
  type: TYPE_NORMAL
- en: The next listing shows you how to combine creating a LoRA and then perform inference
    with both the LoRA and your base model.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.6 Example LoRA and PEFT training
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates model directory to save to'
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that you still need to keep your base model, as shown in listing
    5.6\. The LoRA is run in addition to the foundation model; it sits on top and
    changes the weights at only the rank determined in the `LoraConfig` class (in
    this case, `16`). RoBERTa-Large was likely already decent at doing token classification
    on the bionlp dataset, but now, running with the LoRA on top, it’ll be even better.
    There are multiple types of LoRAs you can use, with QLoRA, QA-LoRA, and AWQ-LoRA
    all gaining popularity in different domains and tasks. With the transformers library,
    which can be controlled from the `LoraConfig`, we encourage you to experiment
    with different adaptation methods to find what works for your data and task.
  prefs: []
  type: TYPE_NORMAL
- en: The most attractive thing about LoRA is that the particular one we discussed
    here results in a file only 68 KB in size on disk and still has a significant
    performance boost. You could create LoRAs for each portion of your company that
    wants a model, one for the legal team that’s siloed so it doesn’t have to worry
    about any private data it is putting into it, one for your engineering team to
    help with code completion and answering questions about which data structures
    or algorithms to use, and one for anyone else. Because they’re so small, it’s
    suddenly much more feasible to store than the 1.45 GB (14.5 GB if we use Llama
    in fp16; it’s 28 GB in fp32) RoBERTa-Large model being finetuned a bunch of times.
    In the spirit of giving you more of these time- and space-saving tips, we’ll go
    over some things that aren’t mentioned anywhere else, but you may still get some
    use out of if the data science part of LLMs is what you are working with.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Training tips and tricks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While this book isn’t focused on training and researching new models, we feel
    kind of bad telling you that finetuning models is an effective strategy for teaching
    LLMs correct guardrails based on your data and then just leaving you to figure
    out how to make it work on your own stuff. With this in mind, let’s look at some
    tried-and-true tips and tricks for both training and finetuning LLMs. These tips
    will help you with some of the least-intuitive parts of training LLMs that most
    practitioners (like us) had to learn the hard way.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 Training data size notes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First off, LLMs are notorious for overfitting. If you are considering training
    a foundation model, you need to consider the amount of data you have, which should
    be roughly 20× the number of parameters you’re trying to train.[²](#footnote-246)
    For example, if you’re training a 1B parameter model, you should train it on 20B
    tokens. If you have fewer tokens than that, you will run the risk of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: If you already have a model and need to finetune it on your data, consider the
    inverse, where you should likely have ~0.000001× the number of tokens as a minimum
    (10K tokens for a 1B parameter model). We came up with this rule of thumb based
    on our experience, although it should be fairly intuitive. If you have fewer than
    1/100,000 of your model parameters in tokens, finetuning likely won’t have much
    of an effect. In this case, you should consider another strategy that won’t cost
    as much, such as LoRA (which we just discussed), RAG (which we talk about in the
    next chapter), or a system that uses both.
  prefs: []
  type: TYPE_NORMAL
- en: For both these examples, we’ve had the experience where a company we worked
    for hoped for great results with minimal data and was disappointed. One hoped
    to train an LLM from scratch with only ~1 million tokens while also disallowing
    open source datasets, and another wanted to finetune the model but only on a couple
    of hundred examples. Neither of these approaches were cost-efficient, nor did
    they create models that performed up to the standards the companies aimed for.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2 Efficient training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve so far focused on tools and methodologies for training, which should supercharge
    your ability to create the best and largest models your training system allows.
    However, other factors should be considered when setting up your training loops.
    In physics, the uncertainty principle shows that you can never perfectly know
    both the speed and position of a given particle. Machine learning’s uncertainty
    principle is that you can never perfectly optimize both your speed and your memory
    utilization. Improving speed comes at the cost of memory, and vice versa. Table
    5.2 shows some choices you can make in training and their effects on speed and
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Table 5.2 Training choices to consider
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '| Method | Improve speed | Improves memory utilization | Difficulty |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Batch size choice  | Yes  | Yes  | Easy  |'
  prefs: []
  type: TYPE_TB
- en: '| Gradient accumulation  | No  | Yes  | Medium  |'
  prefs: []
  type: TYPE_TB
- en: '| Gradient checkpointing  | No  | Yes  | Medium  |'
  prefs: []
  type: TYPE_TB
- en: '| Mixed precision  | Yes  | No  | Hard  |'
  prefs: []
  type: TYPE_TB
- en: '| Optimizer choice  | Yes  | Yes  | Easy  |'
  prefs: []
  type: TYPE_TB
- en: '| Data preloading  | Yes  | No  | Medium  |'
  prefs: []
  type: TYPE_TB
- en: '| Compiling  | Yes  | No  | Easy  |'
  prefs: []
  type: TYPE_TB
- en: Carefully consider your options and what goal you’re working toward when setting
    up your training loop. For example, your batch size should be a power of 2 to
    hit maximum speed and memory efficiency. One author remembers working on getting
    an LLM to have a single-digit milli-second response time. The team was gearing
    up to serve millions of customers as fast as possible, and every millisecond counted.
    After using every trick in the book, I was able to achieve it, and I remember
    the huge feeling of accomplishment for finally getting that within the data science
    dev environment. Yet, it turned out that there was a hard batch size of 20 in
    the production environment. It was just a nice number picked out of a hat, and
    too many systems were built around this assumption; no one wanted to refactor.
    Software engineers, am I right?
  prefs: []
  type: TYPE_NORMAL
- en: 'For the majority of these methods, the tradeoff is clear: if you go slower,
    you can fit a significantly larger model, but it will take way longer. Gradient
    accumulating and checkpointing can reduce memory usage by ~60%, but training will
    take much longer. The packages we talked about in section 5.1 can help mitigate
    these tradeoffs.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.3 Local minima traps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Local minima are hard to spot with LLMs and, as such, can be difficult to avoid.
    If you see your model converging early, be suspicious and judiciously test it
    before accepting the results. When you find that your model is converging early
    at a certain number of steps, one way to avoid it on subsequent runs is to save
    and load a checkpoint 100 or so steps before you see the errant behavior, turn
    your learning rate *way* down, train until you’re sure you’re past it, and then
    turn it back up and continue. Make sure to keep the previously saved checkpoint,
    and save a new checkpoint after that so that you have places to come back to in
    case things go wrong!
  prefs: []
  type: TYPE_NORMAL
- en: 'You can probably tell that this is a frustrating occurrence that one author
    has run into before. He was so confused; he was working on a T5 XXL model, and
    around the 25K step mark, the model was converging and stopping early. He knew
    for a fact that it wasn’t actually converged; it was only 10% through the dataset!
    This happened two or three times, where he loaded up the checkpoint at around
    20K steps and watched the exact same thing happen. It wasn’t until he loaded and
    turned the learning rate down that he finally saw the model improve past this
    point. Once he got through the patch of the local minimum, he turned it back up.
    This happened four more times throughout training this particular model, but since
    he knew what was happening, he was now able to avoid wasting lots of extra time.
    The lesson of the story? Use this rule of thumb: your LLM is not ready if it hasn’t
    trained on your full dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.4 Hyperparameter tuning tips
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hyperparameter tuning isn’t something we’ve gone over extensively in this book,
    not because it’s not interesting but because it doesn’t help nearly as much as
    changing up your data, either getting more or cleaning it further. If you want
    to tune hyperparameters, Optuna is a great package, and you can get that ~1% boost
    in accuracy or F1 score that you really need. Otherwise, if you’re looking for
    a boost in a particular metric, try representing that metric more completely within
    your dataset and maybe use some statistical tricks like oversampling.
  prefs: []
  type: TYPE_NORMAL
- en: While hyperparameter tuning is pretty cool mathematically, for LLMs, it’s not
    something that ever really needs to happen. If you need a boost in performance,
    you need more/better data, and tuning your hyperparameters will never match the
    performance boost you’d get quantizing the weights or performing any of the optimizations
    we’ve mentioned here or in chapter 3\. The biggest performance boost we’ve ever
    gotten through tuning hyperparameters was about a 4% increase in F1, and we only
    did it because we wouldn’t be able to change our dataset for a couple of weeks
    at least.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.5 A note on operating systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Windows is not the right OS to work professionally with LLMs without the Windows
    Subsystem for Linux. MacOS is great but lacks the hardware packages to really
    carry this load unless you know how to use an NVIDIA or AMD GPU with a Mac. If
    you are uncomfortable with Linux, you should take some time to familiarize yourself
    with it while your OS of choice catches up (if it ever does). A myriad of free
    online materials are available to help you learn about Bash, Linux, and the command
    line. Configuring the CUDA Toolkit and Nvidia drivers on Linux can make you want
    to pull your hair out, but it’s worth it compared to the alternatives. Along with
    this, learn about virtual environments, Docker, and cloud computing, like what’s
    in this chapter!
  prefs: []
  type: TYPE_NORMAL
- en: All in all, Windows is easy in the beginning but frustrating in the long run.
    MacOS is also easy in the beginning but currently doesn’t work at all in the long
    run. Linux is incredibly frustrating in the beginning, but once you’re through
    that, it’s smooth sailing.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.6 Activation function advice
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve neglected to really dive into activation functions so far, not because
    they aren’t useful or cool but because you generally don’t need to tweak your
    activation functions unless you’re doing research science on model performance.
    If you take vanilla GPT-2 and give it a GeGLU activation instead of the GELU that
    it comes with, you will not get a significant boost in anything. In addition,
    you’ll need to redo your pretraining, as it pretrained with a different activation
    function. Activation functions help reduce some of the mathematical weaknesses
    of each layer, be they imaginary numbers from the quadratic attention, exploding
    and vanishing gradients, or maybe the researchers noticed positional encodings
    disappearing as they went through the model and changed a little bit. You can
    learn about activation functions, and we recommend doing so; in general, you can
    trust the papers that introduce new ones.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve come a long way in this chapter, discussing setting up an environment,
    training an LLM from scratch, and looking at a multitude of finetuning techniques.
    While we recognize there are still many aspects to this process that we did not
    touch on and that you need to learn on your own, you should be more than ready
    to create your own models. Now that you have a model, in the next chapter, we’ll
    discuss making it production-ready and creating an LLM service you can use to
    serve online inference.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Training is memory intensive, and you will need to master multi-GPU environments
    for many LLM training tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Model training has the same basic steps every time:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Dataset preparation*—Acquire, clean, and curate your data.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Model preparation*—Define model behavior, architecture, loss functions, etc.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Training loop*—Initialization, tokenize, batch data, get predictions/loss,
    backpropagation, etc.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Good data has a significantly greater effect on model performance than architecture
    or the training loop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finetuning is way easier than training from scratch because it requires much
    less data and resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompting allows us to train a model on a specific task after the fact, which
    is one of the reasons LLMs are so powerful compared to traditional ML.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt tuning is a powerful way to focus your model to respond as a specialist
    to certain prompts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knowledge distillation is useful for training powerful smaller models that are
    efficient and adaptable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RLHF is great at getting a model to respond in a way that pleases human evaluators
    but increases factually incorrect results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finetuning MoE models differs from traditional finetuning because it requires
    handling the experts and gating mechanisms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LoRA is a powerful finetuning technique that adapts pretrained models to new
    tasks by creating tiny assets (low-rank matrices) that are fast to train, easy
    to maintain, and very cost-effective.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The quality and size of your data are two of the most important considerations
    for successfully training your model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The major training tradeoff is speed for memory efficiency; if you go slower,
    you can fit a significantly larger model, but it will take way longer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#footnote-source-1) L. Chen, M. Zaharia, and J. Zou, “How is ChatGPT’s
    behavior changing over time?,” arXiv.org, Jul. 18, 2023, [https://arxiv.org/abs/2307.09009](https://arxiv.org/abs/2307.09009).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#footnote-source-2) J. Hoffmann et al., “Training compute-optimal large
    language models,” arXiv:2203.15556 [cs], March 2022, [https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556).'
  prefs: []
  type: TYPE_NORMAL
