- en: '5 Training large language models: How to generate the generator'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 训练大型语言模型：如何生成生成器
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Setting up a training environment and common libraries
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置训练环境和常见库
- en: Applying various training techniques, including using advanced methodologies
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用各种训练技术，包括使用高级方法
- en: Tips and tricks to get the most out of training
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高训练效率的技巧和窍门
- en: Be water, my friend.—Bruce Lee
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 水滴石穿，我的朋友。——李小龙
- en: Are you ready to have some fun?! What do you mean the last four chapters weren’t
    fun? Well, I promise this one for sure will be. We’ve leveled up a lot and gained
    a ton of context that will prove invaluable now as we start to get our hands dirty.
    By training an LLM, we can create bots that can do amazing things and have unique
    personalities. Indeed, we can create new friends and play with them. In the last
    chapter, we showed you how to create a training dataset based on your Slack messages.
    Now we will show you how to take that dataset and create a persona of yourself.
    Finally, you will no longer have to talk to that one annoying coworker, and just
    like Gilfoyle, you can have your own AI Gilfoyle ([https://youtu.be/IWIusSdn1e4](https://youtu.be/IWIusSdn1e4)).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你准备好享受乐趣了吗？！你为什么说前四章没有乐趣？好吧，我保证这一章绝对会有。我们已经提升了很多，并且获得了大量的背景知识，这将证明在开始动手时非常有价值。通过训练LLM，我们可以创建能够做奇妙事情并具有独特个性的机器人。确实，我们可以创造新的朋友并与他们玩耍。在上一个章节中，我们向您展示了如何根据您的Slack消息创建一个训练数据集。现在，我们将向您展示如何使用这个数据集创建一个代表您自己的角色。最后，您将不再需要与那个令人讨厌的同事交谈，就像Gilfoyle一样，您可以有您自己的AI
    Gilfoyle ([https://youtu.be/IWIusSdn1e4](https://youtu.be/IWIusSdn1e4))。
- en: First things first, we’ll show you how to set up a training environment, as
    the process can be very resource-demanding, and without the proper equipment,
    you won’t be able to enjoy what comes next. We’ll then show you how to do the
    basics, like training from scratch and finetuning, after which we’ll get into
    some of the best-known methods to improve upon these processes, making them more
    efficient, faster, and cheaper. We’ll end the chapter with some tips and tricks
    we’ve acquired through our experience of training models in the field.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将向您展示如何设置一个训练环境，因为这个过程可能非常资源密集，如果没有适当的设备，您将无法享受接下来的内容。然后，我们将向您展示如何进行基础知识的学习，比如从头开始训练和微调，之后我们将介绍一些最知名的方法来改进这些过程，使它们更高效、更快、更便宜。我们将以一些我们在训练模型过程中获得的经验和技巧来结束这一章。
- en: 5.1 Multi-GPU environments
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 多GPU环境
- en: Training is a resource-intensive endeavor. A model that only takes a single
    GPU to run inference on may take 10 times that many to train if, for nothing else,
    to parallelize your work and speed things up so you aren’t waiting for a thousand
    years for it to finish training. To really take advantage of what we want to teach
    you in this chapter, we’re first going to have to get you set up in an environment
    you can use as a playground. Later in the chapter, we’ll teach some resource-optimal
    strategies as well, but you’ll need to understand how to set up a multi-GPU env
    if you want to use the largest LLMs anyway.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 训练是一项资源密集型的工作。一个模型可能只需要单个GPU进行推理，但如果要并行化工作并加快速度，以便您不必等待千年才能完成训练，那么它可能需要10倍于此的GPU。为了真正利用本章想要教给您的知识，我们首先需要让您在一个可以当作游乐场使用的环境中设置好。在本章的后面部分，我们还将教授一些资源最优策略，但如果你想要使用最大的LLM，你将需要了解如何设置多GPU环境。
- en: While you can learn a lot using smaller LLMs, what sets apart a pro from an
    amateur is often the ease and fluidity they have when working with larger models.
    And there’s a good reason for this since, on the whole, larger models outperform
    smaller models. If you want to work with the largest models, you’ll never be able
    to get started on your laptop. Even most customized gaming rigs with dual GPUs
    aren’t enough for inference, let alone training.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以使用较小的LLM学到很多东西，但专业人士和业余爱好者之间的区别往往在于他们与大型模型工作的轻松和流畅。这有一个很好的原因，因为总体而言，大型模型优于小型模型。如果您想使用最大的模型，您将永远无法在笔记本电脑上开始。即使是大多数配备双GPU的定制游戏机也不足以进行推理，更不用说训练了。
- en: To this end, we wanted to share with you a few methods to acquire access to
    a multi-GPU environment in the cloud, and then we will share the tools and libraries
    necessary to utilize them. The largest models do not fit in a single GPU, so without
    these environments and tools, you’ll be stuck playing on easy mode forever.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了这个目的，我们想与您分享一些获取云中多GPU环境访问权限的方法，然后我们将分享利用它们所需的工具和库。最大的模型无法适应单个GPU，所以没有这些环境和工具，您将永远被困在简单模式下。
- en: 5.1.1 Setting up
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 设置
- en: It should be pointed out up front that while multi-GPU environments are powerful,
    they are also expensive. When it comes to multi-GPUs, no services we know of offer
    a free tier or offering, but you can at least take comfort in knowing that paying
    per hour will be way cheaper than purchasing the rigs wholesale. Of course, if
    you can get your company to pay the bill, we recommend it, but it is still your
    responsibility to spin down and turn off any environment you create to avoid unnecessary
    charges.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 需要事先指出的是，虽然多GPU环境功能强大，但它们也很昂贵。当涉及到多GPU时，我们所知的任何服务都没有提供免费层或优惠，但至少您可以放心，按小时付费将比批发购买设备便宜得多。当然，如果您能让您的公司支付账单，我们推荐这样做，但关闭您创建的任何环境以避免不必要的费用仍然是您的责任。
- en: If your company is paying, it likely has chosen a hosted service that makes
    this whole process easy. For the rest of us, setting up a virtual machine (VM)
    in Google’s Compute Engine is one of the easiest methods. Once set up, we will
    then show you how to utilize it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的公司支付费用，那么它很可能已经选择了一个使整个过程变得简单的托管服务。对于我们其他人来说，在谷歌的计算引擎（Compute Engine）中设置一个虚拟机（VM）是其中最简单的方法之一。一旦设置完成，我们就会向您展示如何使用它。
- en: A note to the readers
  id: totrans-15
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 读者须知
- en: For learning purposes, we use smaller models throughout this book in our code
    listings such that you can work with them on a single GPU either locally or using
    a service like Colab or Kaggle, which offers a free tier of a single GPU. While
    the listings could be run on CPU-only hardware, you won’t want to do it. Ultimately,
    there shouldn’t be any need to run these costly VMs throughout the book. However,
    you likely will still want to. Training with multiple GPUs is much faster, more
    efficient, and often necessary. We do encourage you to try larger LLM variations
    that require these bigger rigs, as the experience will be priceless. To make it
    easy, you should be able to recycle the code in this chapter for models and datasets
    much larger than what is presented, which will often just be a matter of changing
    a few lines.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习目的，我们在本书的代码列表中使用了较小的模型，这样您就可以在单个GPU上本地工作，或者使用Colab或Kaggle等提供单个GPU免费层的服务。虽然这些列表可以在仅CPU的硬件上运行，但您可能不会想这样做。最终，在本书中运行这些昂贵的VM不应该有任何必要。然而，您可能仍然会想这样做。使用多个GPU进行训练要快得多，效率更高，并且通常是必要的。我们确实鼓励您尝试需要这些大型设备的大型LLM变体，因为这种体验是无价的。为了方便起见，您应该能够将本章中的代码回收用于比展示的模型和数据集更大的模型，这通常只是更改几行的问题。
- en: Google virtual machine
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 谷歌虚拟机
- en: One of the easiest ways to create a multi-GPU environment is to set up a VM
    on Google’s cloud. To get started, you’ll need to create an account, create a
    Google Cloud Project (GCP), set up billing, and download the gcloud CLI. None
    of these steps are particularly hard, but be sure to follow the documentation
    found at [https://cloud.google.com/sdk/docs/install-sdk](https://cloud.google.com/sdk/docs/install-sdk)
    for your operating system to install the SDK. The steps here also include the
    steps and how-tos for creating an account, project, and billing in the Before
    You Begin section if you don’t already have an account.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 创建多GPU环境的最简单方法之一是在谷歌云上设置一个VM。要开始，您需要创建一个账户，创建一个谷歌云项目（GCP），设置账单，并下载gcloud CLI。这些步骤都不特别困难，但请确保按照您操作系统的文档在[https://cloud.google.com/sdk/docs/install-sdk](https://cloud.google.com/sdk/docs/install-sdk)中找到的说明来安装SDK。这里的步骤还包括在“开始之前”部分中创建账户、项目和账单的步骤和操作指南，如果您还没有账户的话。
- en: For new accounts, Google offers a $300 credit to be used for pretty much anything
    on their GCP platform except GPUs. We hate to break this news, but sadly, there’s
    just no free lunch where we are going. So you’ll need to be sure to upgrade to
    a paid GCP tier. Don’t worry; just following along should only cost a couple of
    dollars, but if you are money conscious, we recommend reading the entire section
    first and then trying it out.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于新账户，谷歌提供300美元的信用额度，可用于他们在GCP平台上的几乎所有服务，除了GPU。我们很遗憾地告诉你这个消息，但很遗憾，我们这里没有免费的午餐。所以你需要确保升级到付费的GCP层级。不要担心；只需跟随操作，只需花费几美元，但如果你是精打细算的人，我们建议先阅读整个部分，然后再尝试。
- en: After setting up your account, by default, GCP sets your GPU quotas to 0\. Quotas
    are used to manage your costs. To increase your quotas, go to [https://console.cloud.google.com/iam-admin/quotas](https://console.cloud.google.com/iam-admin/quotas).
    You’ll be looking for the gpus_all_regions quota, and since we plan to use multiple
    GPUs, go ahead and submit a request to increase it to 2 or more.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好账户后，默认情况下，GCP将你的GPU配额设置为0。配额用于管理你的成本。要增加配额，请访问[https://console.cloud.google.com/iam-admin/quotas](https://console.cloud.google.com/iam-admin/quotas)。你将寻找`gpus_all_regions`配额，由于我们计划使用多个GPU，请提交一个请求将其增加到2个或更多。
- en: 'With all the prerequisites in place, we’ll get started by initializing and
    logging in. You’ll do this by running the following command in a terminal on your
    computer:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有先决条件都准备就绪的情况下，我们将开始初始化并登录。你需要在电脑上的终端中运行以下命令：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You may have already done this step if you had to install the SDK, but if not,
    it will launch a web browser to help us log in and authorize us for the gcloud
    CLI, which allows us to select our project. We will be assuming you have just
    the one project, but if this isn’t your first rodeo and you have multiple projects,
    you’ll need to add the `--project` flag in all the subsequent commands.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要安装SDK，你可能已经完成了这一步，如果没有，它将启动一个网络浏览器，帮助我们登录并授权gcloud CLI，这样我们就可以选择我们的项目。我们将假设你只有一个项目，但如果你不是第一次操作并且有多个项目，你需要在所有后续命令中添加`--project`标志。
- en: 'Next, we need to determine two things: the machine type (or which GPUs we want
    to use) and our container image. To pick a machine type, you can check out the
    different options at [https://cloud.google.com/compute/docs/gpus](https://cloud.google.com/compute/docs/gpus).
    For beginners, we highly recommend the NVIDIA L4 GPU, as it is an all-around fantastic
    machine. For our purposes, we’ll be using the g2-standard-24, which comes with
    two L4 GPUs and costs us about $2 per hour. This machine type isn’t in every region
    and zone, but you can find a region close to you at [https://cloud.google.com/compute/docs/regions-zones](https://cloud.google.com/compute/docs/regions-zones).
    We will be using the us-west1 region and us-west1-a zone.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要确定两件事：机器类型（或我们想要使用的GPU）和我们的容器镜像。要选择机器类型，你可以查看[https://cloud.google.com/compute/docs/gpus](https://cloud.google.com/compute/docs/gpus)上的不同选项。对于初学者，我们强烈推荐NVIDIA
    L4 GPU，因为它是一款全能型的出色机器。对于我们来说，我们将使用带有两个L4 GPU的g2-standard-24，每小时成本约为2美元。这种机器类型并非在每个区域和区域中都有，但你可以在[https://cloud.google.com/compute/docs/regions-zones](https://cloud.google.com/compute/docs/regions-zones)找到离你较近的区域。我们将使用us-west1区域和us-west1-a区域。
- en: For the container image, we’ll save ourselves a lot of hassle by using one that
    has all the basics set up. Generally, this means creating your own, but Google
    has several prebuilt container images for deep learning, which are great to use
    or a great place to start as a base image to customize. These are all found in
    the `deeplearning-platform -release` project that they own. To check out the options
    available, you can run
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于容器镜像，我们将使用一个已经设置好所有基本配置的镜像，这样可以节省我们很多麻烦。通常这意味着创建自己的镜像，但谷歌提供了几个预构建的深度学习容器镜像，这些镜像非常适合使用，或者作为一个基础镜像进行定制的好起点。这些镜像都可以在谷歌拥有的`deeplearning-platform
    -release`项目中找到。要查看可用的选项，你可以运行
- en: '[PRE1]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'NOTE  You can learn more about the container image options here: [https://cloud.google.com/deep-learning-vm/docs/images](https://cloud.google.com/deep-learning-vm/docs/images).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可以在[https://cloud.google.com/deep-learning-vm/docs/images](https://cloud.google.com/deep-learning-vm/docs/images)这里了解更多关于容器镜像选项的信息。
- en: 'You can pick from Base, TensorFlow, and PyTorch compiled images, along with
    the CUDA and Python versions. We’ll be using `common-gpu-v20230925-debian-11-py310`,
    which is a simple image ready for GPU with a Debian Linux distribution and Python
    3.10\. Now that we have everything we need, we can create our VM! Go ahead and
    run the following commands to set up the VM:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从基础、TensorFlow 和 PyTorch 编译的镜像中进行选择，以及 CUDA 和 Python 版本。我们将使用 `common-gpu-v20230925-debian-11-py310`，这是一个简单的镜像，适用于带有
    Debian Linux 发行版和 Python 3.10 的 GPU。现在我们已经拥有了所有需要的东西，我们可以创建我们的虚拟机了！请继续运行以下命令来设置虚拟机：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first command creates an environment variable to store the name of our VM
    since we’ll also be using it in several of the following commands. This name can
    be whatever you want it to be. The next command creates our VM instance. The first
    several flags (`zone`, `image`, `machine`) should make sense since we just spent
    the previous paragraphs preparing and gathering that information. The `boot-disk-size`
    sets the disk space for our VM and defaults to 200 GB, so it’s included here because
    it’s important to know for LLMs since they are large assets, and you will likely
    need to increase it—especially for LLMs that require multiple GPUs to run.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个命令创建一个环境变量来存储我们的虚拟机名称，因为我们还将在接下来的几个命令中使用它。这个名称可以是您想要的任何名称。下一个命令创建我们的虚拟机实例。前几个标志（`zone`、`image`、`machine`）应该是有意义的，因为我们刚刚在前面的段落中准备和收集了这些信息。`boot-disk-size`
    设置虚拟机的磁盘空间，默认为 200 GB，因此它被包括在这里，因为对于 LLMs 来说很重要，因为它们是大型资产，您可能需要增加它——特别是对于需要多个
    GPU 来运行的 LLMs。
- en: The `scopes` flag is passed to set authorization. Current GCP best practices
    recommend setting it to `cloud-platform`, which determines authorization through
    OAuth and IAM roles. The `metadata` field isn’t required but is used here as a
    trick to ensure the NVIDIA drivers are installed. It is really useful if you are
    using these commands to create a shell script to automate this process. You should
    know that it will cause a small delay between when the VM is up and when you can
    actually SSH into it, as it won’t be responsive while it installs the drivers.
    If you don’t include it, the first time you SSH in through a terminal, it will
    ask you if you want to install it, so no harm done. However, if you access the
    VM through other methods (described in the next sections), you can run into problems.
    The last two commands are standard maintenance policies.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`scopes` 标志用于设置授权。当前的 GCP 最佳实践建议将其设置为 `cloud-platform`，这通过 OAuth 和 IAM 角色确定授权。`metadata`
    字段不是必需的，但在此用作技巧以确保已安装 NVIDIA 驱动程序。如果您使用这些命令创建一个 shell 脚本来自动化此过程，它将非常有用。您应该知道，它将在虚拟机启动和您实际可以
    SSH 进入之间造成轻微的延迟，因为在安装驱动程序时它不会响应。如果您不包括它，第一次通过终端 SSH 进入时，它会询问您是否要安装它，所以没有损害。然而，如果您通过其他方法（在下文中描述）访问虚拟机，可能会遇到问题。最后两个命令是标准的维护策略。'
- en: Once that runs, you can verify the VM is up by running
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦运行，您可以通过运行以下命令来验证虚拟机是否已启动：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This command will give you a lot of information about your instance that is
    worth looking over, including a status field that should read `''RUNNING''`. Once
    you’ve confirmed that it’s up, we will SSH into it. If this is your first time
    using gcloud to SSH, an SSH key will be generated automatically. Go ahead and
    run the following command:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将为您提供有关您实例的大量信息，值得仔细查看，包括应读取为 `'RUNNING'` 的状态字段。一旦您确认它已启动，我们将通过 SSH 进入它。如果您是第一次使用
    gcloud 进行 SSH，将自动生成 SSH 密钥。请继续运行以下命令：
- en: '[PRE4]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Your terminal will be shelled into our multi-GPU VM, and you are now in business.
    At this point, your VM is still just an empty shell, so you’ll want to bring in
    code. The easiest way to do this is to copy the files over with Secure Copy Protocol
    (SCP). You can do this for a single file or a whole directory. For example, assuming
    your project has a requirements.txt file and a subdirectory local-app-folder,
    from a new terminal, you can run the following commands:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 您的终端将进入我们的多 GPU 虚拟机，您现在可以开始工作了。在这个时候，您的虚拟机仍然只是一个空壳，所以您需要引入代码。最简单的方法是使用安全复制协议（SCP）复制文件。您可以为此执行单个文件或整个目录。例如，假设您的项目有一个
    requirements.txt 文件和一个子目录 local-app-folder，从一个新的终端，您可以运行以下命令：
- en: '[PRE5]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Overall, not too bad. Once you’ve gone through the process and set everything
    up, the next time you set up a VM, it will only be four commands (`create`, `describe`,
    `ssh`, `scp`) to get up and running.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，还不错。一旦您完成了这个过程并设置了所有内容，下次设置虚拟机时，只需四个命令（`create`、`describe`、`ssh`、`scp`）即可启动和运行。
- en: 'Of course, these instances cost good money, so the last command you’ll want
    to know before moving on is how to delete it:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些实例会花费不少钱，所以在继续之前，你想要知道的最后一个命令是如何删除它：
- en: '[PRE6]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For Linux power users, this code line is likely all you need, but for the rest
    of us plebs, shelling into a VM through a terminal is less than an ideal working
    environment. We’ll show you some tips and tricks to make the most of your remote
    machine.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Linux高级用户来说，这一行代码可能就足够了，但对于我们这些普通人来说，通过终端进入虚拟机的工作环境并不理想。我们将向你展示一些技巧和窍门，以充分利用你的远程机器。
- en: SSH through VS Code
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过VS Code进行SSH
- en: 'For most devs, a terminal is fine, but what we really want is an IDE. Most
    IDEs offer remote SSH capabilities, but we’ll demonstrate with VS Code. The first
    step is to install the extension Remote-SSH (you can find the extension here:
    [https://mng.bz/q0dE](https://mng.bz/q0dE)). Other extensions offer this capability,
    but Remote-SSH is maintained by Microsoft and has over 17 million installs, so
    it’s a great choice for beginners.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数开发者来说，终端就足够了，但我们真正想要的是IDE。大多数IDE都提供远程SSH功能，但我们将使用VS Code进行演示。第一步是安装扩展程序Remote-SSH（你可以在这里找到扩展程序：[https://mng.bz/q0dE](https://mng.bz/q0dE)）。其他扩展也提供这种功能，但Remote-SSH由微软维护，并且有超过1700万次安装，因此对于初学者来说是一个很好的选择。
- en: 'Next, we are going to run a configuration command:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将运行一个配置命令：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, inside of VS Code, you can press F1 to open the command palette and run
    the Remote-SSH: Open SSH Host… command, and you should see your VM’s SSH address,
    which will look like l4-llm-example.us-west1-a.project-id-401501\. If you don’t
    see it, something went wrong with the `config-ssh` command, and you likely need
    to run `gcloud` `init` again. Select the address, and a new VS Code window should
    pop up. In the bottom corner, you’ll see that it is connecting to your remote
    machine. And you are done! Easy. From here, you can use VS Code like you would
    when using it locally.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '然后，在VS Code内部，你可以按F1键打开命令面板并运行Remote-SSH: Open SSH Host…命令，你应该会看到你的虚拟机的SSH地址，它看起来像l4-llm-example.us-west1-a.project-id-401501\.
    如果你看不到它，那么`config-ssh`命令可能出了问题，你可能需要再次运行`gcloud init`。选择地址，一个新的VS Code窗口应该会弹出。在底角，你会看到它正在连接到你的远程机器。这样你就完成了！很简单。从这里开始，你可以像在本地使用一样使用VS
    Code。'
- en: 5.1.2 Libraries
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 库
- en: Although setting up hardware is important, none of it will work without the
    software packages that enable different points of hardware to communicate with
    each other effectively. With LLMs, the importance of the software is compounded.
    One author personally experienced having all hardware correctly configured and
    was pretty sure the software setup was likewise configured, only to start up training
    a model and be met with an estimated training time of over three years. After
    troubleshooting, the team realized this was because he had installed multiple
    versions of CUDA Toolkit, and PyTorch was looking at an incompatible (up-to-date)
    one instead of the one he had intended to use.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然设置硬件很重要，但没有软件包使不同的硬件点能够有效地相互通信，任何硬件都无法工作。在使用LLMs的情况下，软件的重要性被放大了。一位作者亲身体验过所有硬件都正确配置，并且非常确信软件设置也同样配置正确，但启动训练模型时，却遇到了预计训练时间超过三年的情况。经过故障排除，团队意识到这是因为他安装了多个版本的CUDA
    Toolkit，而PyTorch正在查看一个不兼容（最新版）的版本，而不是他打算使用的那个版本。
- en: 'These software packages are about more than just using the CUDA low-level communication
    with your GPU; they’re about load-balancing, quantizing, and parallelizing your
    data as it runs through each computation to make sure it’s going as fast as possible
    while still enabling a certain level of fidelity for the matrices. You wouldn’t
    want to spend a long time making sure your embedding vectors are phenomenal representations
    just to have them distorted at run time. Thus, we present the four deep-learning
    libraries every practitioner should know for multi-GPU instances: DeepSpeed, Accelerate,
    BitsandBytes, and xFormers. At the time of this writing, all complementary features
    between these libraries are experimental, so feel free to mix and match. If you
    get a setup that utilizes all four at once to their full potential without erroring,
    drop it in a reusable container so fast.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些软件包不仅仅是为了使用CUDA与GPU的低级通信；它们还涉及到在数据通过每个计算运行时进行负载均衡、量化和并行化，以确保尽可能快地运行，同时仍然为矩阵提供一定程度的保真度。您不希望花费很长时间确保您的嵌入向量是卓越的表现，然后在运行时被扭曲。因此，我们介绍了每个从业者都应该了解的四个用于多GPU实例的深度学习库：DeepSpeed、Accelerate、BitsandBytes和xFormers。在撰写本文时，这些库之间的所有互补功能都是实验性的，因此请随意混合匹配。如果您能够一次性无错误地充分利用所有四个库，那么请快速将其放入可重用的容器中。
- en: DeepSpeed
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DeepSpeed
- en: DeepSpeed is an optimization library for distributed deep learning. DeepSpeed
    is powered by Microsoft and implements various enhancements for speed in training
    and inference, like handling extremely long or multiple inputs in different modalities,
    quantization, caching weights and inputs, and, probably the hottest topic right
    now, scaling up to thousands of GPUs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed是一个用于分布式深度学习的优化库。DeepSpeed由微软提供支持，并实现了各种增强功能，以提高训练和推理的速度，例如处理极长或不同模态的多个输入、量化、缓存权重和输入，以及目前最热门的话题之一：扩展到数千个GPU。
- en: Installation is fairly simple if you remember to always install the latest—but
    not nightly—version of PyTorch first. This means you also need to configure your
    CUDA Toolkit beforehand. Once you have that package, `pip` `install` `deepspeed`
    should get you right where you want to go unless, ironically, you use Microsoft’s
    other products. If you are on a Windows OS, there is only partial support, and
    there are several more steps you will need to follow to get it working for inference,
    not training, mode.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您记得首先始终安装最新版本的PyTorch（但不是夜间版本），那么安装过程相当简单。这意味着您还需要事先配置您的CUDA工具包。一旦有了这个包，使用`pip
    install deepspeed`应该就能让您达到想要的目的，除非，讽刺的是，您使用的是微软的其他产品。如果您使用的是Windows操作系统，那么支持仅限于部分，并且您需要遵循几个额外的步骤才能使其在推理模式下工作，而不是在训练模式下。
- en: Accelerate
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Accelerate
- en: From Hugging Face, Accelerate is made to help abstract the code for parallelizing
    and scaling to multiple GPUs away from you so that you can focus on the training
    and inference side. One huge advantage of Accelerate is that it adds only one
    import and two lines of code and changes two other lines, compared to a standard
    training loop in PyTorch in its vanilla implementation. Beyond that, Accelerate
    also has fairly easy CLI usage, allowing it to be automated along with Terraform
    or AWS CDK.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Hugging Face的Accelerate旨在帮助您抽象化并行化和扩展到多个GPU的代码，这样您就可以专注于训练和推理方面。Accelerate的一个巨大优势是，与PyTorch的标准训练循环相比，它仅添加一个导入语句和两行代码，并更改两行其他代码。除此之外，Accelerate还具有相当简单的CLI使用方式，允许它与Terraform或AWS
    CDK一起自动化。
- en: Accelerate boasts compatibility over most environments, and as long as your
    environment is Python 3.8+ and PyTorch 1.10.0+ (CUDA compatibility first), you
    should be able to use Accelerate without problems. Once that’s done, `pip` `install`
    `accelerate` should get you there. Accelerate also has experimental support for
    DeepSpeed if you would like to get the benefits of both.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Accelerate在大多数环境中都具有兼容性，只要您的环境是Python 3.8+和PyTorch 1.10.0+（首先确保CUDA兼容性），您应该能够无问题地使用Accelerate。一旦完成，使用`pip
    install accelerate`应该就能达到目的。如果您想同时获得DeepSpeed和Accelerate的好处，Accelerate也提供了对DeepSpeed的实验性支持。
- en: BitsandBytes
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BitsandBytes
- en: 'If you don’t already know the name Tim Dettmers in this field, you should become
    acquainted pretty quickly. Not many people have done as much as he has to make
    CUDA-powered computing accessible. This package is made to help practitioners
    quantize models and perform efficient matrix multiplication for inference (and
    maybe training) within different bit sizes, all the way down to INT8\. BitsandBytes
    has similar requirements and drawbacks to DeepSpeed: the requirements are Python
    3.8+ and CUDA 10.0+ on Linux and Mac environments and partial support for Windows
    with a different package.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这个领域还不认识Tim Dettmers这个名字，你应该很快就会熟悉起来。很少有人像他那样做了很多工作，使得CUDA驱动的计算变得易于访问。这个包旨在帮助实践者量化模型，并在不同的位数（甚至INT8）范围内进行高效的矩阵乘法，用于推理（以及可能训练）。
- en: 'You should have little trouble installing BitsandBytes, as `pip` `install`
    `bitsandbytes` should work for most use cases. If you find yourself on Windows,
    you’re in luck: `pip` `install` `bitsandbytes-windows` will work as well. If you
    want to use it with Hugging Face’s transformers or PyTorch, you will need to edit
    some minimum requirements stated within both of those packages, as the Windows
    version does not have the same version numbers as the regular package. BitsandBytes
    offers its own implementations of optimizers like Adam and NN layers like Linear
    to allow for that 8-bit boost to run deep learning apps on smaller devices at
    greater speed with a minimal drop in accuracy.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该不会在安装BitsandBytes时遇到太多麻烦，因为`pip install bitsandbytes`应该适用于大多数用例。如果你发现自己使用的是Windows，那么你很幸运：`pip
    install bitsandbytes-windows`同样有效。如果你想用它与Hugging Face的transformers或PyTorch一起使用，你需要编辑这两个包中声明的某些最小要求，因为Windows版本与常规包的版本号不同。BitsandBytes提供了Adam优化器和NN层（如Linear）的自己的实现，以便在较小的设备上以更快的速度运行深度学习应用，同时最小化准确性的下降。
- en: xFormers
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: xFormers
- en: The most bleeding edge of the libraries we recommend for most use cases is xFormers,
    which is made for research and production. Following a (hopefully) familiar PyTorch-like
    pattern of independent building blocks for multiple modalities, xFormers takes
    it a step further and offers components that won’t be available in PyTorch for
    quite a while. One that we’ve used quite a lot is memory-efficient exact attention,
    which speeds up inference considerably.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们推荐的大多数用例中最前沿的库是xFormers，它是为研究和生产而设计的。遵循一个（希望）熟悉的类似PyTorch的独立构建块模式，xFormers更进一步，提供了在PyTorch中相当长一段时间内都不会有的组件。我们使用得相当多的是内存高效的精确注意力，这大大加快了推理速度。
- en: xFormers has more requirements than the other packages, and we’d like to stress
    once more that using one or more tools to keep track of your environment is strongly
    recommended. On Linux and Windows, you’ll need PyTorch 2.0.1, and `pip` `install`
    `-U` `xFormers` should work for you. That said, there are paths for installation
    with pretty much any other version of PyTorch, but the main ones are versions
    1.12.1, 1.13.1, and 2.0.1.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: xFormers比其他包有更多的要求，我们再次强调，使用一个或多个工具来跟踪你的环境是强烈推荐的。在Linux和Windows上，你需要PyTorch
    2.0.1，使用`pip install -U xFormers`应该对你有效。尽管如此，使用几乎任何其他版本的PyTorch都有安装路径，但主要版本是1.12.1、1.13.1和2.0.1。
- en: In table 5.1, we can see a heavily reduced breakdown of what each of these packages
    does and how it integrates with your code. Each package does similar things, but
    even when performing the same task, they will often perform those tasks differently
    or on different parts of your model or pipeline. There is some overlap between
    packages, and we’d encourage you to use all of them to see how they might benefit
    you. Now that you have an environment and a basic understanding of some of the
    tools we’ll be using, let’s move forward and see it in action.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在表5.1中，我们可以看到每个包所做的工作及其如何与你的代码集成的详细分解。每个包都做类似的事情，但即使执行相同的任务，它们通常也会以不同的方式或在不同的模型或管道部分执行这些任务。包之间存在一些重叠，我们鼓励你使用所有这些包，看看它们可能如何对你有所帮助。现在你已经有了环境，并对我们将要使用的一些工具有了基本了解，让我们继续前进，看看它是如何运作的。
- en: Table 5.1 Comparison of optimization packages for ML
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.1 机器学习优化包比较
- en: '| Library | Faster training or inference | Code integration | Lower accuracy
    | Many GPUs | Quantization | Optimizations |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 库 | 更快的训练或推理 | 代码集成 | 降低了准确性 | 许多GPU | 量化 | 优化 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| DeepSpeed  | Both  | CLI  | Depends  | Yes  | Supports  | Caching, gradient
    checkpointing, memory management, scaling  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| DeepSpeed  | Both  | CLI  | Depends  | Yes  | Supports  | 缓存、梯度检查点、内存管理、扩展  |'
- en: '| Accelerate  | Both  | CLI and Code  | Depends  | Yes  | Supports  | Automation,
    compiling, parallelization  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| Accelerate  | Both  | CLI and Code  | Depends  | Yes  | Supports  | 自动化、编译、并行化  |'
- en: '| BitsandBytes  | Both  | Code  | Always  | NA  | Yes but only  | Quantization,
    quantized optimizers  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| BitsandBytes  | Both  | Code  | Always  | NA  | Yes but only  | 量化、量化优化器  |'
- en: '| xFormers  | Training  | Code  | Depends  | NA  | Yes and more  | Efficient
    attention, memory management  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| xFormers  | Training  | Code  | Depends  | NA  | Yes and more  | 高效的注意力机制、内存管理  |'
- en: 5.2 Basic training techniques
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 基本训练技术
- en: In training LLMs, the process typically starts with defining the architecture
    of the model, the nature and amount of data required, and the training objectives.
    We’ve already gone over these steps in the last chapter, so you should be well
    prepared already, but let’s look at a brief recap. The model architecture usually
    follows a variant of the Transformer architecture due to its effectiveness in
    capturing long-term dependencies and its parallelizable nature, making it amenable
    to large-scale computation. Data is the lifeblood of any LLM (or any ML model
    in general), which typically requires extensive corpora of diverse and representative
    text data. As the model’s purpose is to learn to predict the next word in a sequence,
    it’s crucial to ensure that the data covers a wide array of linguistic contexts.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练大型语言模型（LLM）时，通常的过程是从定义模型的架构、所需数据的性质和数量以及训练目标开始。我们已经在上一章中讨论了这些步骤，所以你应该已经做好了充分的准备，但让我们简要回顾一下。由于Transformer架构在捕捉长期依赖关系和其可并行化性质方面的有效性，模型架构通常遵循其变体，这使得它适合大规模计算。数据是任何LLM（或任何一般机器学习模型）的生命线，通常需要大量多样且具有代表性的文本数据。由于模型的目的是学习预测序列中的下一个单词，因此确保数据覆盖广泛的语言环境至关重要。
- en: Because we’ll be going over various training techniques in this chapter, here’s
    a (super) quick rundown of the investments you’ll need for different types. For
    training from scratch, you’ll need VRAM greater than four times the number of
    billions of parameters to hold the model, along with the batches of training data.
    So to train a 1B parameter model from scratch, you’ll need at least 5 or 6 GB
    of VRAM, depending on your batch sizes and context length. Consider training a
    70B parameter model like Llama 2 as an exercise. How much VRAM will you need to
    fit the model, along with a 32K token context limit? If you’re coming up with
    a number around 300 GB of VRAM, you’re right. For the finetuning techniques, you’ll
    need significantly fewer resources for a couple of reasons—namely, quantization
    and amount of data needed, meaning you no longer need 4× VRAM, but can use 2×
    or 1× with the correct setup.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在本章中我们将讨论各种训练技术，所以这里简要概述一下不同类型所需的投入。对于从头开始训练，你需要比模型参数数量多四倍以上的VRAM来存储模型，以及训练数据批次。因此，要训练一个1B参数的模型，你需要至少5或6GB的VRAM，具体取决于你的批次大小和上下文长度。以训练一个70B参数的模型如Llama
    2为例。你需要多少VRAM来适应模型，以及一个32K令牌的上下文限制？如果你得出的数字大约是300GB的VRAM，那么你是对的。对于微调技术，由于以下几个原因，你将需要显著更少的资源——即量化以及所需的数据量，这意味着你不再需要4倍的VRAM，但可以使用2倍或1倍的VRAM，只要设置正确。
- en: Unlike traditional ML models, LLMs are often trained in stages. Figure 5.1 shows
    the basic training life cycle of an LLM, starting from scratch, then finetuning,
    and finally prompting. The first step is creating our foundation model, where
    we take a large, often unrefined, dataset and train an empty shell of a model
    on it. This training will create a model that has seen such a large corpus of
    text that it appears to have a basic understanding of language. We can then take
    that foundation model and use transfer learning techniques, generally finetuning
    on a small, highly curated dataset to create a specialized LLM for expert tasks.
    Lastly, we use prompting techniques that, while not traditional training, allow
    us to goad the model to respond in a particular fashion or format, improving the
    accuracy of our results.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的机器学习模型不同，LLM通常分阶段进行训练。图5.1显示了LLM的基本训练生命周期，从零开始，然后是微调，最后是提示。第一步是创建我们的基础模型，我们在这个大型、通常未经精炼的数据集上训练一个空壳模型。这次训练将创建一个模型，它已经看到了如此大量的文本，似乎对语言有基本理解。然后我们可以使用那个基础模型，并采用迁移学习技术，通常在小型、高度精选的数据集上进行微调，以创建一个针对专家任务的专用LLM。最后，我们使用提示技术，虽然这不是传统的训练方法，但它允许我们刺激模型以特定的方式或格式响应，从而提高我们结果的准确性。
- en: '![figure](../Images/5-1.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-1.png)'
- en: Figure 5.1 The training life cycle of an LLM. We start by creating a foundation
    model based on a large corpus of text, which we later finetune using a curated
    dataset for a specific task. We can then further improve the model by using the
    model itself and techniques like prompting to enhance or enlarge our curated dataset.
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.1 LLM的训练生命周期。我们首先基于大量文本创建一个基础模型，然后使用针对特定任务的精选数据集进行微调。然后我们可以通过使用模型本身和提示等技术来增强或扩大我们的精选数据集，进一步改进模型。
- en: You’ll notice that the training life cycle is often a continuous loop—training
    models to understand language better and then using those models to improve our
    training datasets. Later in this chapter, we will go into more depth about other
    advanced training techniques that take advantage of this loop, like prompt tuning
    and RLHF. For now, let’s solidify our understanding of three basic steps.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，训练生命周期通常是一个连续的循环——训练模型以更好地理解语言，然后使用这些模型来改进我们的训练数据集。在本章的后面部分，我们将更深入地探讨其他利用这个循环的高级训练技术，如提示调整和强化学习与人类反馈（RLHF）。现在，让我们巩固对三个基本步骤的理解。
- en: 5.2.1 From scratch
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 从零开始
- en: Training an LLM is computationally intensive and can take several weeks or months
    even on high-performance hardware. This process feeds chunks of data (or “batches”)
    to the model and adjusts the weights based on the calculated loss. Over time,
    this iterative process of prediction and adjustment, also known as an epoch, leads
    the model to improve its understanding of the syntactic structures and complexities
    in the data. It’s worth noting that monitoring the training process is crucial
    to avoid overfitting, where the model becomes excessively tailored to the training
    data and performs poorly on unseen data. Techniques like early stopping, dropout,
    and learning rate scheduling are used to ensure the generalizability of the model,
    but they are not silver bullets. Remember, the ultimate goal is not just to minimize
    the loss on training data but to create a model that can understand and generate
    human-like text across a broad range of contexts.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个大型语言模型（LLM）在计算上非常密集，即使在高性能硬件上也可能需要几周或几个月的时间。这个过程会将数据块（或称为“批次”）输入到模型中，并根据计算出的损失来调整权重。随着时间的推移，这种预测和调整的迭代过程，也称为一个epoch，使模型能够提高对数据中的句法结构和复杂性的理解。值得注意的是，监控训练过程对于避免过拟合至关重要，过拟合会导致模型过度适应训练数据，在未见过的数据上表现不佳。使用诸如提前停止、dropout和学习率调度等技术来确保模型的泛化能力，但它们并非万能的解决方案。记住，最终目标不仅仅是最小化训练数据上的损失，而是创建一个能够在广泛语境下理解和生成类似人类文本的模型。
- en: Training an LLM from scratch is a complex process that begins with defining
    the model’s architecture. This decision should be guided by the specific task
    at hand, the size of the training dataset, and the available computational resources.
    The architecture, in simple terms, is a blueprint of the model that describes
    the number and arrangement of layers, the type of layers (like attention or feed-forward
    layers), and the connections between them. Modern LLMs typically employ a variant
    of the Transformer architecture, known for its scalability and efficiency in handling
    long sequences of data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始训练一个大型语言模型（LLM）是一个复杂的过程，它始于定义模型的架构。这个决定应该由具体任务、训练数据集的大小以及可用的计算资源来指导。简单来说，架构是模型的蓝图，它描述了层的数量和排列、层的类型（如注意力层或前馈层），以及它们之间的连接。现代的LLM通常采用Transformer架构的变体，这种架构以其处理长序列数据的可扩展性和效率而闻名。
- en: Once the model’s architecture is set, the next step is to compile a large and
    diverse dataset for training. The quality and variety of data fed into the model
    largely dictate the model’s ability to understand and generate human-like text.
    A common approach is to use a large corpus of internet text, ensuring a wide-ranging
    mix of styles, topics, and structures. The data is then preprocessed and tokenized,
    converting the raw text into a numerical format that the model can learn from.
    During this tokenization process, the text is split into smaller units, or tokens,
    which could be as short as a single character or as long as a word.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了模型的架构，下一步就是为训练编译一个大型且多样化的数据集。输入到模型中的数据的质量和多样性在很大程度上决定了模型理解和生成类似人类文本的能力。一种常见的方法是使用大量的网络文本语料库，确保有广泛多样的风格、主题和结构。然后对这些数据进行预处理和分词，将原始文本转换为模型可以学习的数值格式。在分词过程中，文本被分割成更小的单元，或称为标记，这些标记可能短至单个字符，也可能长至一个单词。
- en: With a model and dataset ready, the next step is to initialize the model and
    set the learning objectives. The LLMs are trained using autoregressive semi-supervised
    learning techniques where the model learns to predict the next word in a sequence
    given the preceding words. The model’s weights are randomly initialized and then
    adjusted through backpropagation and optimization techniques such as Adam or Stochastic
    Gradient Descent based on the difference between the model’s predictions and the
    actual words in the training data. The aim is to minimize this difference, commonly
    referred to as the “loss,” to improve the model’s predictive accuracy.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和数据集准备就绪后，下一步是初始化模型并设置学习目标。LLM的训练使用自回归半监督学习技术，其中模型学习根据前面的单词预测序列中的下一个单词。模型的权重随机初始化，然后通过反向传播和优化技术（如Adam或随机梯度下降）根据模型预测与训练数据中实际单词之间的差异进行调整。目标是最小化这种差异，通常称为“损失”，以提高模型的预测准确性。
- en: 'Training involves feeding the tokenized text into the model and adjusting the
    model’s internal parameters to minimize the loss. We said this once, but it bears
    repeating: this process is computationally demanding and may take weeks or even
    months to complete, depending on the model size and available hardware. After
    training, the model is evaluated on a separate validation dataset to ensure that
    it can generalize to unseen data. It is common to iterate on this process, finetuning
    the model parameters and adjusting the architecture as needed based on the model’s
    performance on the validation set.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程包括将分词后的文本输入到模型中，并调整模型的内部参数以最小化损失。我们之前已经提到过，但需要再次强调：这个过程计算量很大，可能需要数周甚至数月才能完成，具体取决于模型的大小和可用的硬件。训练完成后，模型将在一个单独的验证数据集上评估，以确保它能够泛化到未见过的数据。根据模型在验证集上的表现，通常需要迭代这个过程，微调模型参数并根据需要调整架构。
- en: Let’s explore training a brand-new transformer-based language model “from scratch,”
    meaning without any previously defined architecture, embeddings, or weights. Figure
    5.2 shows this process. You shouldn’t have to train an LLM from scratch, nor would
    you normally want to, as it’s a very expensive and time-consuming endeavor; however,
    knowing how can help you immensely.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索从零开始训练一个全新的基于Transformer的语言模型，这意味着没有任何先前定义的架构、嵌入或权重。图5.2展示了这个过程。通常你不需要从头开始训练LLM，你通常也不希望这样做，因为这是一项非常昂贵且耗时的任务；然而，了解这个过程可以极大地帮助你。
- en: '![figure](../Images/5-2.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-2.png)'
- en: Figure 5.2 A simplified version of all the steps necessary to train a language
    model (large or otherwise) from scratch. You must have data, then define all of
    the model behavior, and only then proceed to train.
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.2 从头开始训练语言模型（大或小）所需的所有步骤的简化版本。你必须有数据，然后定义所有模型行为，然后才能开始训练。
- en: 'Listing 5.1 allows you to run through the motions without training an actual
    massive model, so feel free to explore with this code. For a more complex and
    complete example, check out Andrej Karpathy’s minGPT project here: [https://github.com/karpathy/minGPT](https://github.com/karpathy/minGPT).
    You should pay attention to some things when you review the listing. You might
    recall that we talked about tokenization and embeddings in the last chapter, so
    one thing to notice is that for simplicity, we will be using a character-based
    tokenizer. Before you run the code, can you predict whether this was a good or
    bad idea? Also, pay attention to how we use both Accelerate and BitsandBytes,
    which we introduced a little bit ago; you’ll see that these libraries come in
    mighty handy. Next, watch as we slowly build up the LLMs architecture, building
    each piece in a modular fashion and later defining how many of each piece is used
    and where to put them, almost like Legos. Finally, at the very end of the code,
    you’ll see a typical model training loop, splitting our data, running epochs in
    batches, and so forth.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.1 允许你运行整个过程而不需要训练一个真正的巨大模型，所以请随意用这段代码进行探索。对于更复杂和完整的示例，请查看Andrej Karpathy的minGPT项目：[https://github.com/karpathy/minGPT](https://github.com/karpathy/minGPT)。在审查列表时，请注意一些事情。你可能还记得我们在上一章讨论了标记化和嵌入，所以要注意的一点是，为了简单起见，我们将使用基于字符的标记化器。在你运行代码之前，你能预测这是不是一个好主意吗？此外，请注意我们如何使用我们之前简要介绍过的Accelerate和BitsandBytes库；你会发现这些库非常有用。接下来，我们将逐步构建LLMs架构，以模块化的方式构建每个部分，并最终定义每个部分的使用数量和放置位置，几乎就像乐高积木一样。最后，在代码的最后一部分，你会看到一个典型的模型训练循环，包括分割数据、分批运行epoch等。
- en: Listing 5.1 An example of training from scratch
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.1 从头开始训练的示例
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '#1 Δefines the overall GPT architecture'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 Δefines the overall GPT architecture'
- en: '#2 Δefines the building blocks of the model'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 Δefines the building blocks of the model'
- en: '#3 Helper functions for training'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 训练辅助函数'
- en: '#4 Trains the model'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 训练模型'
- en: '#5 Parameters for our experiment'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 实验参数'
- en: '#6 Δataset'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 Δataset'
- en: '#7 Character-based pseudo-tokenization'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 基于字符的伪标记化'
- en: '#8 Instantiates the model and looks at the parameters'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 实例化模型并查看参数'
- en: '#9 Training block'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 训练块'
- en: '#10 Creates model directory'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 创建模型目录'
- en: '#11 Saves the model'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 保存模型'
- en: '#12 Loads the saved model'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '#12 加载已保存的模型'
- en: '#13 Tests the loaded model'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '#13 测试已加载的模型'
- en: In listing 5.1, we explored how the Lego blocks are put together for the GPT
    family of models and showed a training loop reminiscent of our exploration of
    language modeling in chapter 2\. Beyond showing the first part of generative pretraining
    for models, this example also illustrates why character-based modeling, whether
    convolutional or otherwise, is weak for language modeling. Did you get it right?
    Yup, character-based modeling isn’t the best. Alphabets on their own do not contain
    enough information to produce statistically significant results, regardless of
    the tuning amount. From a linguistic standpoint, this is obvious, as alphabets
    and orthography, in general, are representations of meaning generated from humans,
    which is not intrinsically captured.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表5.1中，我们探讨了GPT系列模型如何组合乐高积木，并展示了一个类似于我们在第二章中探索语言模型的训练循环。除了展示模型生成预训练的第一部分之外，这个例子还说明了为什么基于字符的建模，无论是卷积还是其他方式，对于语言建模来说都很弱。你理解了吗？是的，基于字符的建模并不是最好的。字母本身并不包含足够的信息来产生具有统计意义的成果，无论调整量有多大。从语言学的角度来看，这是显而易见的，因为字母和正字法，总的来说，是人类生成意义的表示，这并不是内在捕捉到的。
- en: Some of the ways to help with that information capture are increasing our tokenization
    capture window through word-, subword-, or sentence-level tokenization. We can
    also complete the pretraining before showing the model our task to allow it to
    capture as much approximate representation as possible. Next, we’ll show what
    benefits combining these two steps can have on our model’s performance.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些方法可以帮助捕捉这些信息，比如通过词、子词或句子级别的标记化来增加我们的标记化捕获窗口。我们也可以在向模型展示我们的任务之前完成预训练，以便它尽可能多地捕捉近似表示。接下来，我们将展示结合这两个步骤如何对我们的模型性能产生好处。
- en: 5.2.2 Transfer learning (finetuning)
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 迁移学习（微调）
- en: Transfer learning is an essential approach in machine learning and a cornerstone
    of training LLMs. It’s predicated on the notion that we can reuse knowledge learned
    from one problem (the source domain) and apply it to a different but related problem
    (the target domain). In the context of LLMs, this typically means using a pretrained
    model, trained on a large, diverse dataset, and adapting it to a more specific
    task or domain.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是机器学习中的一个基本方法，也是训练LLM的基石。它基于这样的观点：我们可以从一个问题（源领域）中学到的知识重新使用，并将其应用到不同但相关的问题（目标领域）上。在LLM的背景下，这通常意味着使用一个在大规模多样化数据集上训练过的预训练模型，并将其调整到更具体的任务或领域。
- en: In the first step of transfer learning, an LLM is trained on a large, general-purpose
    corpus, such as the entirety of Wikipedia, books, or the internet. This pretraining
    stage allows the model to learn an extensive range of language patterns and nuances
    on a wide variety of topics. The goal here is to learn a universal representation
    of language that captures a broad understanding of syntax, semantics, and world
    knowledge. These models are often trained for many iterations and require significant
    computational resources, which is why it’s practical to use pretrained models
    provided by organizations like OpenAI or Hugging Face.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习的第一步中，一个大型通用语料库上的LLM（大型语言模型）被训练，例如维基百科的全部内容、书籍或互联网。这个预训练阶段使得模型能够在广泛的各个主题上学习到广泛的语言模式和细微差别。这里的目的是学习一种通用的语言表示，它能够捕捉对语法、语义和世界知识的广泛理解。这些模型通常需要经过多次迭代训练，并且需要大量的计算资源，这就是为什么使用像OpenAI或Hugging
    Face这样的组织提供的预训练模型是实际可行的。
- en: After pretraining, the LLM is updated on a specific task or domain. This update
    process adapts the general-purpose language understanding of the model to a more
    specific task, such as sentiment analysis, text classification, or question answering.
    Updating usually requires significantly less computational resources than the
    initial pretraining phase because it involves training on a much smaller dataset
    specific to the task at hand. Through this process, the model is able to apply
    the vast knowledge it gained during pretraining to a specific task, often outperforming
    models trained from scratch on the same task. This process of transfer learning
    has led to many of the advances in NLP over recent years.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练之后，LLM会在特定任务或领域上进行更新。这个更新过程将模型的通用语言理解调整为更具体的任务，例如情感分析、文本分类或问答。更新通常比初始预训练阶段需要的计算资源少得多，因为它涉及到在针对手头任务的小型特定数据集上进行训练。通过这个过程，模型能够将预训练期间获得的大量知识应用到特定任务上，通常优于从头开始训练的模型。这种迁移学习的过程导致了近年来NLP（自然语言处理）的许多进步。
- en: Finetuning
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调
- en: There are several different transfer learning techniques, but when it comes
    to LLMs, the one everyone cares about is finetuning. Finetuning an LLM involves
    taking a pretrained model—that is, a model already trained on a large general
    corpus—and adapting it to perform a specific task or to understand a specific
    domain of data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习有多种不同的技术，但说到LLM，大家最关心的还是微调。微调LLM涉及取一个预训练模型——即已经在大型通用语料库上训练过的模型——并调整它以执行特定任务或理解特定数据领域。
- en: This technique uses the fact that the base model has already learned a significant
    amount about the language, allowing you to reap the benefits of a large-scale
    model without the associated computational cost and time. The process of finetuning
    adapts the pre-existing knowledge of the model to a specific task or domain, making
    it more suitable for your specific use case. It’s like having a generalist who
    already understands the language well and then providing specialist training for
    a particular job. This approach is often more feasible for most users due to the
    significantly reduced computational requirements and training time compared to
    training a model from scratch.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术利用了基础模型已经对语言有了相当多的了解的事实，让你能够在不承担相关计算成本和时间的情况下享受到大规模模型的好处。微调的过程将模型现有的知识调整为特定任务或领域，使其更适合你的特定用例。这就像有一个对语言已经理解得很透彻的通才，然后为特定工作提供专业培训。由于与从头开始训练模型相比，这种方法显著降低了计算需求和训练时间，因此对于大多数用户来说通常更可行。
- en: The first step in finetuning involves choosing a suitable pretrained model.
    This decision is guided by the specific task you want the model to perform and
    by the resources available to you. Keep in mind that this means setting a goal
    for the model’s behavior before training. Once the pretrained model has been chosen,
    it’s crucial to prepare the specific dataset you want the model to learn from.
    This data could be a collection of medical texts, for example, if you’re trying
    to finetune the model to understand medical language. The data must be preprocessed
    and tokenized in a way that’s compatible with the model’s pretraining.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 微调的第一步是选择一个合适的预训练模型。这个决定取决于你希望模型执行的具体任务以及你拥有的资源。请记住，这意味着在训练之前为模型的行为设定一个目标。一旦选择了预训练模型，就至关重要地准备你希望模型从中学习的特定数据集。例如，如果你试图微调模型以理解医学语言，这些数据可能是一组医学文本。数据必须以与模型预训练兼容的方式进行预处理和分词。
- en: 'The finetuning process involves training the model on your specific dataset,
    but with a twist: instead of learning from scratch, the model’s existing knowledge
    is adjusted to better fit the new data. This finetuning is typically done with
    a smaller learning rate than in the initial training phase to prevent the model
    from forgetting its previously learned knowledge. After finetuning, the model
    is evaluated on a separate dataset to ensure it can generalize to unseen data
    in the specific domain. Similar to training from scratch, this process may involve
    several iterations to optimize the model’s performance. Finetuning offers a way
    to harness the power of LLMs for specific tasks or domains without the need for
    extensive resources or computation time. See figure 5.3.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 微调过程涉及在您的特定数据集上训练模型，但有一个转折：不是从头开始学习，而是调整模型的现有知识以更好地适应新数据。这种微调通常使用比初始训练阶段更小的学习率进行，以防止模型忘记之前学到的知识。微调后，模型将在一个单独的数据集上评估，以确保它可以在特定领域中对未见过的数据进行泛化。类似于从头开始训练，这个过程可能需要多次迭代以优化模型性能。微调提供了一种利用LLMs（大型语言模型）在特定任务或领域中的能力，而无需大量资源或计算时间。见图5.3。
- en: '![figure](../Images/5-3.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-3.png)'
- en: Figure 5.3 Finetuning differs from training from scratch in that you don’t have
    to define model behavior, you can use the exact same training loop, and you have
    a fraction of the data requirement.
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3 微调与从头开始训练的不同之处在于，你不需要定义模型行为，你可以使用完全相同的训练循环，并且数据需求量更少。
- en: In listing 5.2, we show you how to finetune a GPT model. Notice how much less
    code there is in this listing than in listing 5.1\. We don’t need to define an
    architecture or a tokenizer; we’ll just use those from the original model. Essentially,
    we get to skip ahead because weights and embeddings have already been defined.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表5.2中，我们向您展示了如何微调一个GPT模型。注意，与列表5.1相比，这个列表中的代码要少得多。我们不需要定义架构或分词器；我们只需使用原始模型中的那些。本质上，我们得以跳过，因为权重和嵌入已经定义好了。
- en: Listing 5.2 An example of finetuning
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.2 微调示例
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 Loads and formats the dataset'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 加载并格式化数据集'
- en: '#2 Creates model directory to save to'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建模型目录以保存'
- en: '#3 Establishes our GPT-2 parameters (different from the paper and scratchGPT)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 建立我们的GPT-2参数（与论文和scratchGPT不同）'
- en: '#4 Instantiates our tokenizer and our special tokens'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 实例化我们的分词器和特殊标记'
- en: '#5 Instantiates our model from the config'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 从配置中实例化我们的模型'
- en: '#6 Creates a tokenize function'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 创建一个分词函数'
- en: '#7 Tokenizes our whole dataset (so we never have to do it again)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 对整个数据集进行分词（这样我们就不必再次进行）'
- en: '#8 Creates a data collator to format the data for training'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 创建一个数据收集器以格式化训练数据'
- en: '#9 Establishes training arguments'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 建立训练参数'
- en: '#10 Instantiates the Trainer'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 实例化训练器'
- en: '#11 Trains and saves the model'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 训练并保存模型'
- en: '#12 Loads the saved model'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '#12 加载保存的模型'
- en: '#13 Tests the saved model'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '#13 测试保存的模型'
- en: Looking at listing 5.2 compared with listing 5.1, they have almost the exact
    same architecture (minus the activation function), and they’re training on exactly
    the same data. Yet, there’s a marked improvement with the finetuned GPT-2 model
    due to the lack of learned representation in the first model. Our pretrained model,
    along with subword BPE tokenization instead of character-based, helps the model
    figure out which units of statistically determined meaning are most likely to
    go together. You’ll notice, though, that GPT-2, even with pretraining, struggles
    to generate relevant longer narratives despite using a newer, better activation
    function.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 比较列表5.2和列表5.1，它们的架构几乎完全相同（除了激活函数），并且它们在完全相同的数据上训练。然而，由于第一个模型中缺乏学习到的表示，微调后的GPT-2模型有了显著的改进。我们的预训练模型，以及与基于字符的标记化相比的子词BPE标记化，帮助模型确定哪些统计上确定的单位最有可能一起出现。不过，你会发现，即使经过预训练，GPT-2在使用更新、更好的激活函数的情况下，仍然难以生成相关的较长的叙述。
- en: Finetuning OpenAI
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调OpenAI
- en: 'We just trained a GPT model from scratch, and then we finetuned GPT-2, but
    we know many readers really want the power behind OpenAI’s larger GPT models.
    Despite being proprietary models, OpenAI has graciously created an API where we
    can finetune GPT-3 models. Currently, three models are available for finetuning
    with OpenAI’s platform, but it looks like it intends to extend that finetuning
    ability to all of its models on offer. OpenAI has written a whole guide, which
    you can find at [http://platform.openai.com/](http://platform.openai.com/), but
    once you have your dataset prepared in the necessary format, the code is pretty
    easy. Here are some snippets for various tasks:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚从头开始训练了一个GPT模型，然后微调了GPT-2，但我们知道许多读者真正想要的是OpenAI更大GPT模型背后的力量。尽管这些是专有模型，但OpenAI非常慷慨地创建了一个API，我们可以通过它微调GPT-3模型。目前，OpenAI的平台上有三个模型可供微调，但看起来它打算将这种微调能力扩展到其提供的所有模型。OpenAI编写了一整篇指南，你可以在[http://platform.openai.com/](http://platform.openai.com/)找到，但一旦你以必要的格式准备好了你的数据集，代码就相当简单。以下是针对各种任务的片段：
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This first snippet uploads a training dataset in the correct format for the
    platform and specifies the purpose as finetuning, but doesn’t start the process
    yet. Next, you’ll need to create the finetuning job:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这个片段首先上传了一个符合平台正确格式的训练数据集，并指定了用途为微调，但还没有开始这个过程。接下来，你需要创建微调作业：
- en: '[PRE11]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This is where you specify which training file and which model you want to finetune.
    Once OpenAI’s training loop has completed, you’ll see the finetuned model’s name
    populated when you retrieve the job details. Now you can use that model the same
    way you would have used any of the vanilla ones for chat completion or anything
    else like this:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你指定要微调哪个训练文件和哪个模型的地方。一旦OpenAI的训练循环完成，当你检索作业详情时，你将看到微调模型的名称被填充。现在你可以像使用任何常规模型一样使用该模型进行聊天完成或其他类似操作：
- en: '[PRE12]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: And that’s it for finetuning an OpenAI model! Very simple, doesn’t take too
    long, and as of March 2023, your data is private to you. Of course, you’ll be
    ceding all of the control of how that finetuning occurs over to OpenAI. If you’d
    like to do something beyond vanilla finetuning, you’ll need to do that yourself.
    In just a minute, we’ll go over those techniques you may consider, along with
    some more advanced processes that can help with more fine-grained models and more
    complex tasks.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是微调OpenAI模型的所有内容！非常简单，不需要太长时间，截至2023年3月，你的数据对你来说是私密的。当然，你将把微调过程的所有控制权交给OpenAI。如果你想做一些超越常规微调的事情，你需要自己来做。就在下一分钟，我们将讨论你可能考虑的技术，以及一些可以帮助更精细模型和更复杂任务的更高级过程。
- en: 5.2.3 Prompting
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 提示
- en: One of the main reasons why LLMs are so powerful compared to traditional ML
    is because we can train them at run time. Give them a set of instructions and
    watch them follow them to the best of their ability. This technique is called
    prompting and is used in LLMs to guide the model’s output. In essence, the prompt
    is the initial input given to the model that provides it with context or instructions
    for what it should do. For example, “translate the following English text to French”
    and “summarize the following article” are prompts. In the context of LLMs, prompting
    becomes even more critical, as these models are not explicitly programmed to perform
    specific tasks but learn to respond to a variety of tasks based on the given prompt.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统机器学习相比，LLMs之所以如此强大，主要原因是我们可以实时训练它们。给他们一组指令，然后观察它们尽其所能地遵循这些指令。这种技术被称为提示（prompting），在LLMs中用于引导模型输出。本质上，提示是提供给模型的初始输入，为它提供上下文或执行什么操作的指令。例如，“将以下英文文本翻译成法语”和“总结以下文章”都是提示。在LLMs的背景下，提示变得尤为重要，因为这些模型并没有被明确编程来执行特定任务，而是根据给定的提示学习响应各种任务。
- en: Prompt engineering refers to the process of crafting effective prompts to guide
    the model’s behavior. The aim is to create prompts that lead the model to provide
    the most desirable or useful output. Prompt engineering can be more complex than
    it appears, as slight changes in how a prompt is phrased can lead to vastly different
    responses from the model. Some strategies for prompt engineering include being
    more explicit in the prompt, providing an example of the desired output, or rephrasing
    the prompt in different ways to get the best results. It’s a mixture of art and
    science, requiring a good understanding of the model’s capabilities and limitations.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是指构建有效的提示来引导模型行为的过程。目标是创建能够引导模型提供最理想或最有用的输出的提示。提示工程可能比看起来更复杂，因为提示语式的微小变化可能会导致模型产生截然不同的响应。提示工程的一些策略包括在提示中更加明确，提供期望输出的示例，或者以不同的方式重新措辞提示以获得最佳结果。它是一种艺术和科学的结合，需要很好地理解模型的能力和限制。
- en: In this chapter, we are going to focus mainly on training and finetuning, the
    steps before deployment, but we would be remiss if we didn’t first mention prompting.
    We will talk about prompting in much more depth in chapter 7.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将主要关注训练和微调，这是部署前的步骤，但如果我们不首先提及提示（prompt），那将是我们的疏忽。我们将在第7章更深入地讨论提示。
- en: 5.3 Advanced training techniques
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 高级训练技术
- en: Now that you know how to do the basics, let’s go over some more advanced techniques.
    These techniques have been developed for a variety of reasons, such as improving
    generated text outputs, shrinking the model, providing continuous learning, speeding
    up training, and reducing costs. Depending on the needs of your organization,
    you may need to reach for a different training solution. While not a comprehensive
    list, the following techniques are often used and should be valuable tools as
    you prepare a production-ready model.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了如何进行基础操作，让我们来了解一下一些更高级的技术。这些技术出于各种原因而被开发，例如提高生成的文本输出质量、缩小模型大小、提供持续学习、加快训练速度以及降低成本。根据你组织的需要，你可能需要寻找不同的训练解决方案。虽然这不是一个详尽的列表，但以下技术经常被使用，并且在你准备一个生产就绪的模型时应该是有价值的工具。
- en: Classical ML training background
  id: totrans-146
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 经典机器学习训练背景
- en: 'Going over some techniques to enhance your finetuning process requires a bit
    of background. We won’t be doing a full course in ML; however, in case this is
    your first exposure, you should know some classic learning paradigms that experiments
    tend to follow—supervised, unsupervised, adversarial, and reinforcement:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解一些增强微调过程的技术需要一些背景知识。我们不会进行一次完整的机器学习课程；然而，如果你是第一次接触，你应该了解一些经典的实验学习范式，这些范式实验通常会遵循——监督学习、无监督学习、对抗学习和强化学习：
- en: Supervised learning involves collecting both the data to train on and the labels
    showcasing the expected output.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习涉及收集用于训练的数据和展示预期输出的标签。
- en: Unsupervised learning does not require labels, as the data is probed for similarity
    and grouped into clusters that are the closest comparison to each other.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习不需要标签，因为数据会被探测相似性并分组到彼此最接近的簇中。
- en: Adversarial learning is what’s used to train a generative adversarial network.
    It involves two models, generally referred to as the Critic model and the Forger
    model. These two models essentially play a game against each other where the forger
    tries to copy some ideal output, and the critic tries to determine whether the
    forgery is the real thing.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗性学习是用于训练生成对抗网络的方法。它涉及两个模型，通常被称为评论家模型（Critic model）和伪造者模型（Forger model）。这两个模型本质上是在玩一个游戏，伪造者试图复制一些理想的输出，而评论家则试图确定伪造品是否为真品。
- en: Reinforcement learning (RL) opts for establishing a reward function instead
    of having predefined labels for the model to learn from. By measuring the model’s
    actions, it is given a reward based on that function instead.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习（RL）选择建立奖励函数，而不是为模型提供预定义的标签来学习。通过测量模型的行为，根据该函数给予它奖励。
- en: All LLMs must be trained using at least one of these, and they perform at a
    high level with all of them done correctly. The training techniques discussed
    in this chapter differ from those basic ones, ranging from adding some form of
    human input to the model to comparing outputs to changing how the model does matrix
    multiplication.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所有大型语言模型（LLM）都必须使用其中至少一种进行训练，并且当所有这些训练方法都正确执行时，它们的表现都非常出色。本章讨论的训练技术不同于那些基本方法，从向模型添加某种形式的人类输入到将输出进行比较，以及改变模型执行矩阵乘法的方式。
- en: 5.3.1 Prompt tuning
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 提示调整
- en: We’ve gone over pragmatics before, but as a reminder, language models perform
    better when given real-world nonsemantic context pertaining to the tasks and expectations.
    Language modeling techniques all operate on the underlying assumption that the
    LM, given inputs and expected outputs, can divine the task to be done and do it
    in the best way within the number of parameters specified.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经讨论过语用学，但为了提醒，当语言模型被给予与任务和期望相关的真实世界非语义上下文时，它们的表现会更好。所有语言建模技术都基于这样一个基本假设：给定输入和期望输出，语言模型（LM）可以推断出要完成的任务，并在指定的参数数量内以最佳方式完成它。
- en: While the idea of the model inferring both the task and the method of completing
    it from the data showed promise, it has been shown time and time again, from BERT
    to every T5 model and now to all LLMs, that providing your model with the expected
    task and relevant information for solving the task improves model performance
    drastically. As early as 2021, Google Research, DeepMind, and OpenAI had all published
    papers about prompt tuning, or giving a model pragmatic context during training.
    The benefits of prompt tuning are reducing the amount of data required for the
    model to converge during training and, even cooler, the ability to reuse a completely
    frozen language model for new tasks without retraining or fully finetuning.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然模型从数据中推断任务和完成它的方法的想法很有希望，但时间一次又一次地表明，从BERT到每个T5模型，现在到所有LLM，为模型提供预期的任务和解决任务所需的相关信息可以极大地提高模型性能。早在2021年，谷歌研究、DeepMind和OpenAI都发表了关于提示调整或训练期间为模型提供实用上下文的论文。提示调整的好处是减少模型在训练过程中收敛所需的数据量，甚至更酷的是，能够在不重新训练或完全微调的情况下，为新的任务重用完全冻结的语言模型。
- en: Because LLMs are so large (and getting larger), it is becoming increasingly
    difficult to share them and even more difficult to guarantee their performance
    on a given task, even one they are trained on. Prompt tuning can help nudge the
    model in the right direction without becoming a significant cost. Figure 5.4 shows
    this process.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM非常大（并且还在变大），因此共享它们变得越来越困难，甚至在保证它们在特定任务上的性能方面更加困难，即使这些任务是在它们被训练时。提示调整可以帮助模型朝着正确的方向引导，而不会成为重大的成本。图5.4展示了这个过程。
- en: '![figure](../Images/5-4.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-4.png)'
- en: Figure 5.4 Prompt tuning foregoes most finetuning to allow the majority of the
    foundation model’s language understanding ability to stay exactly the same and,
    instead, focuses on changing how the model responds to specific inputs.
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4中，提示调整（Prompt tuning）放弃了大部分微调，以使大多数基础模型的语言理解能力保持完全不变，相反，它专注于改变模型对特定输入的响应方式。
- en: Listing 5.3 shows how to prompt tune a smaller variant of the BLOOMZ model from
    Big Science. BLOOMZ was released as an early competitor in the LLM space but has
    ultimately struggled to garner attention or momentum in the community because
    of its inability to generate preferred outputs despite its mathematical soundness.
    Because prompt tuning doesn’t add much to the regular finetuning structure we
    used in listing 5.2, we’ll perform Parameter-Efficient Fine-Tuning (PEFT), which
    drastically reduces the memory requirements by determining which model parameters
    need changing the most.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.3展示了如何对Big Science中的BLOOMZ模型的一个较小变体进行提示调整。BLOOMZ作为LLM空间中的早期竞争者发布，但由于其无法生成首选输出，尽管其数学上合理，但最终在社区中难以引起关注或获得动力。由于提示调整并没有给我们在列表5.2中使用的常规微调结构增加太多，我们将执行参数高效微调（PEFT），这通过确定哪些模型参数需要最大程度地改变来大幅减少内存需求。
- en: Listing 5.3 An example of prompt tuning
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.3 提示调整的示例
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Helper function to preprocess text; go ahead and skip to the training'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 辅助函数用于预处理文本；继续跳转到训练'
- en: '#2 Model prompt tuning'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 模型提示调整'
- en: '#3 Δefines prompt tuning config; notice init_text'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 Δefines prompt tuning config; notice init_text'
- en: '#4 Loads Δataset'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 加载 Δataset'
- en: '#5 Labels the dataset'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 标记数据集'
- en: '#6 Loads tokenizer'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 加载分词器'
- en: '#7 Runs Tokenizer across dataset and preprocess'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 在数据集上运行分词器并进行预处理'
- en: '#8 Prepares data loaders'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 准备数据加载器'
- en: '#9 Loads foundation model'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 加载基础模型'
- en: '#10 Δefines optimizer'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 Δefines optimizer'
- en: '#11 Training steps'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 训练步骤'
- en: '#12 Creates model directory to save to'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '#12 创建模型目录以保存'
- en: '#13 Saving'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '#13 保存'
- en: '#14 Inference'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '#14 推理'
- en: Other than the changed setup, the main difference between listings 5.2 and 5.3
    is simply prepending a prompt with some sort of instruction to the beginning of
    each input, reminiscent of the T5 training method that pioneered having a prepended
    task string before every input. Prompt tuning has emerged as a powerful technique
    for finetuning large language models to specific tasks and domains. By tailoring
    prompts to the desired output and optimizing them for improved performance, we
    can make our models more versatile and effective. However, as our LLMs continue
    to grow in scale and complexity, it becomes increasingly challenging to efficiently
    finetune them on specific tasks. This is where knowledge distillation comes into
    play, offering a logical next step. Knowledge distillation allows us to transfer
    the knowledge and expertise of these highly tuned models to smaller, more practical
    versions, enabling a wider range of applications and deployment scenarios. Together,
    prompt tuning and knowledge distillation form a dynamic duo in the arsenal of
    techniques for harnessing the full potential of modern LLMs.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 除了更改设置外，列表5.2和5.3之间的主要区别仅仅是将某种形式的指令作为提示添加到每个输入的开头，这让人联想到T5训练方法，该方法在每次输入之前都引入了一个前置任务字符串。提示调整已成为微调大型语言模型到特定任务和领域的一种强大技术。通过调整提示以适应所需的输出并优化以提高性能，我们可以使我们的模型更加灵活和有效。然而，随着我们的LLM在规模和复杂性上不断增长，在特定任务上高效微调它们变得越来越具有挑战性。这就是知识蒸馏发挥作用的地方，它提供了一个逻辑上的下一步。知识蒸馏使我们能够将这些高度调优模型的知识和专长转移到更小、更实用的版本中，从而实现更广泛的应用和部署场景。提示调整和知识蒸馏共同构成了利用现代LLM全部潜力的技术库中的动态搭档。
- en: 5.3.2 Finetuning with knowledge distillation
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 使用知识蒸馏进行微调
- en: Knowledge distillation is an advanced technique that provides a more efficient
    path to finetuning an LLM. Rather than just finetuning an LLM directly, knowledge
    distillation involves transferring the knowledge from a large, complex model (the
    teacher) to a smaller, simpler model (the student). The aim is to create a more
    compact model that retains the performance characteristics of the larger model
    but is more efficient in terms of resource usage. Figure 5.5 shows this process.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏是一种高级技术，为微调大型语言模型（LLM）提供了一条更高效的路径。而不是直接微调LLM，知识蒸馏涉及将知识从一个大型的、复杂的模型（教师模型）转移到一个小型的、简单的模型（学生模型）。目标是创建一个更紧凑的模型，它保留了大型模型的性能特征，但在资源使用方面更加高效。图5.5展示了这一过程。
- en: '![figure](../Images/5-5.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-5.png)'
- en: Figure 5.5 Knowledge distillation allows a smaller model to learn from a foundation
    model to replicate similar behavior with fewer parameters. The student model does
    not always learn the emergent qualities of the foundation model, so the dataset
    must be especially curated. The dotted line indicates a special relationship as
    the student model becomes the specialized LLM.
  id: totrans-180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5 知识蒸馏允许较小的模型从基础模型学习，以更少的参数复制类似的行为。学生模型并不总是学习基础模型涌现出的特性，因此数据集必须特别精心制作。虚线表示一种特殊关系，因为学生模型变成了专门的LLM。
- en: The first step in knowledge distillation is to select a pre-trained LLM as the
    teacher model. This could be any of the large models, such as Llama 2 70B or Falcon
    180B, which have been trained on vast amounts of data. You also need to create
    or select a smaller model as the student. The student model might have a similar
    architecture to the teacher’s, but with fewer layers or reduced dimensionality
    to make it smaller and faster.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏的第一步是选择一个预训练的LLM作为教师模型。这可以是任何大型模型，例如在大量数据上训练的Llama 2 70B或Falcon 180B。你还需要创建或选择一个较小的模型作为学生。学生模型可能具有与教师模型相似的架构，但层数更少或维度更低，以使其更小、更快。
- en: Next, the student model is trained on the same task as the teacher model. However,
    instead of learning from the raw data directly, the student model learns to mimic
    the teacher model’s outputs. This training is typically done by adding a term
    to the loss function that encourages the student model’s predictions to be similar
    to the teacher model’s predictions. Thus, the student model not only learns from
    the task-specific labels but also benefits from the rich representations learned
    by the teacher model.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，学生模型在教师模型相同的任务上进行训练。然而，学生模型不是直接从原始数据中学习，而是学习模仿教师模型的输出。这种训练通常是通过在损失函数中添加一个项来完成的，该项鼓励学生模型的预测与教师模型的预测相似。因此，学生模型不仅从任务特定的标签中学习，而且从教师模型学习到的丰富表示中受益。
- en: Once the distillation process is complete, you’ll have a compact student model
    that can handle the specific tasks learned from the teacher model but at a fraction
    of the size and computational cost. The distilled model can then be further finetuned
    on a specific task or dataset if required. Through knowledge distillation, you
    can use the power of LLMs in situations where computational resources or response
    time are limited.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦蒸馏过程完成，你将拥有一个紧凑的学生模型，它可以处理从教师模型学习到的特定任务，但大小和计算成本仅为教师模型的一小部分。如果需要，蒸馏模型可以进一步在特定任务或数据集上进行微调。通过知识蒸馏，你可以在计算资源或响应时间有限的情况下使用LLM的强大功能。
- en: In listing 5.4, we show how to perform finetuning with knowledge distillation
    using BERT and becoming DistilBERT. As opposed to regular finetuning, pay attention
    to the size and performance of the model. Both will drop; however, size will drop
    much faster than performance.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表5.4中，我们展示了如何使用BERT和DistilBERT进行知识蒸馏的微调。与常规微调不同，请注意模型的大小和性能。两者都会下降；然而，大小下降的速度将远快于性能。
- en: Listing 5.4 An example of knowledge distillation
  id: totrans-185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.4 知识蒸馏的示例
- en: '[PRE14]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 Place teacher on same device as student'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 将教师模型放置在学生模型相同的设备上'
- en: '#2 Computes student output'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 计算学生模型的输出'
- en: '#3 Computes teacher output'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 计算教师模型的输出'
- en: '#4 Asserts size'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 断言大小'
- en: '#5 Returns weighted student loss'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 返回加权的学生损失'
- en: '#6 Creates model directory to save to'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 创建模型目录以保存'
- en: '#7 Δefines the teacher and student models'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 Δ定义教师和学生模型'
- en: '#8 Creates label2id, id2label dicts for nice outputs for the model'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 创建label2id，id2label字典，以供模型输出使用'
- en: '#9 Δefines training args'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 Δ定义训练参数'
- en: '#10 Pushes to hub parameters'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 将参数推送到hub'
- en: '#11 Δistillation parameters'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 Δ定义蒸馏参数'
- en: '#12 Δefines data_collator'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '#12 Δ定义数据收集器'
- en: '#13 Δefines model'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '#13 Δ定义模型'
- en: '#14 Δefines student model'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '#14 Δ定义学生模型'
- en: '#15 Δefines metrics and metrics function'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '#15 Δ定义指标和指标函数'
- en: Knowledge distillation, as exemplified by the provided `compute_loss` method,
    is a technique that enables the transfer of valuable insights from a teacher model
    to a more lightweight student model. In this process, the teacher model provides
    soft targets, offering probability distributions over possible outputs, which
    are then utilized to train the student model. The critical aspect of knowledge
    distillation lies in the alignment of these distributions, ensuring that the student
    model not only learns to mimic the teacher’s predictions but also gains a deeper
    understanding of the underlying data. This approach helps improve the student’s
    generalization capabilities and performance on various tasks, ultimately making
    it more efficient and adaptable.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏，如提供的`compute_loss`方法所示，是一种技术，它使得从教师模型到更轻量级的学生模型转移有价值见解成为可能。在这个过程中，教师模型提供软目标，提供可能的输出上的概率分布，这些分布随后被用于训练学生模型。知识蒸馏的关键在于这些分布的对齐，确保学生模型不仅学会模仿教师的预测，而且对底层数据有更深入的理解。这种方法有助于提高学生的泛化能力和在各种任务上的表现，最终使其更高效和适应性强。
- en: As we look forward, one logical progression beyond knowledge distillation is
    the incorporation of RLHF. While knowledge distillation enhances a model’s ability
    to make predictions based on existing data, RLHF allows the model to learn directly
    from user interactions and feedback. This dynamic combination not only refines
    the model’s performance further but also enables it to adapt and improve continuously.
    By incorporating human feedback, RL can help the model adapt to real-world scenarios,
    evolving its decision-making processes based on ongoing input, making it an exciting
    and natural evolution in the development of LLM systems.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们展望未来，知识蒸馏之后的一个逻辑进步是引入强化学习与人类反馈（RLHF）。虽然知识蒸馏增强了模型基于现有数据进行预测的能力，但RLHF允许模型直接从用户交互和反馈中学习。这种动态组合不仅进一步提升了模型的表现，还使其能够持续适应和改进。通过引入人类反馈，强化学习可以帮助模型适应现实世界场景，根据持续输入调整其决策过程，这在LLM系统的发展中是一个令人兴奋且自然的演变。
- en: 5.3.3 Reinforcement learning with human feedback
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.3 带有人类反馈的强化学习
- en: 'RLHF is a newer training technique developed to overcome one of the biggest
    challenges when it comes to RL: how to create reward systems that actually work.
    It sounds easy, but anyone who’s played around with RL knows how difficult it
    can be. Before AlphaStar, one author was building his own RL bot to play *StarCraft*,
    a war simulation game in space.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF是一种新开发的训练技术，旨在克服强化学习中的一个最大挑战：如何创建真正有效的奖励系统。听起来很简单，但任何玩过强化学习的人都知道这有多么困难。在AlphaStar之前，一位作者正在构建自己的强化学习机器人来玩*星际争霸*，这是一款太空战争模拟游戏。
- en: NOTE  Check out [https://mng.bz/Dp4a](https://mng.bz/Dp4a) to learn more about
    AlphaStar.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 备注  欲了解更多关于AlphaStar的信息，请访问[https://mng.bz/Dp4a](https://mng.bz/Dp4a)。
- en: A simple reward system based on winning or losing was taking too long, so he
    decided to give it some reasonable intermediate rewards based on growing an army.
    However, this got blocked when it failed to build Pylons, a building required
    to increase army supply limits. So he gave it a reward to build Pylons. His bot
    quickly learned that it liked to build Pylons—so much so that it learned to almost
    win but not win, crippling its opponent so that it could keep building Pylons
    unharassed and for as long as it wanted.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 基于胜负的简单奖励系统耗时过长，所以他决定根据军队的成长给予一些合理的中间奖励。然而，当它未能建造Pylons（一种用于增加军队供应限制的建筑）时，这一计划受阻。因此，他给予建造Pylons的奖励。他的机器人很快学会喜欢建造Pylons——如此之喜欢，以至于它学会了几乎赢但不赢，削弱了对手，使其能够不受干扰地持续建造Pylons，直到它想要的时间。
- en: With a task like winning a game, even if it’s difficult, we can usually still
    come up with reasonable reward systems. But what about more abstract tasks, like
    teaching a robot how to do a backflip? These tasks get really difficult to design
    reward systems for, which is where RLHF comes in. What if instead of designing
    a system, we simply have a human make suggestions? A human knows what a backflip
    is, after all. The human will act like a tutor, picking attempts it likes more
    as the bot is training. That’s what RLHF is, and it works really well. Applied
    to LLMs, a human simply looks at generated responses to a prompt and picks which
    one they like more. See figure 5.6.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像赢得游戏这样的任务，即使它很困难，我们通常仍然可以想出合理的奖励系统。但更抽象的任务，比如教机器人如何做后空翻呢？这些任务设计奖励系统变得非常困难，这就是RLHF发挥作用的地方。如果我们不设计系统，而是简单地让人类提供建议会怎样？毕竟，人类知道什么是后空翻。人类将充当导师，在机器人训练过程中挑选出他们更喜欢尝试。这就是RLHF，它效果非常好。应用于LLMs时，人类只需查看对提示生成的响应，并挑选出他们更喜欢的一个。见图5.6。
- en: '![figure](../Images/5-6.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/5-6.png)'
- en: Figure 5.6 RLHF substitutes a loss function for a reward model and proximal
    policy optimization (PPO), allowing the model a much higher ceiling for learning
    trends within the data, including what is preferred as an output instead of what
    completes the task.
  id: totrans-210
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6 RLHF用损失函数替换了奖励模型和近端策略优化（PPO），使得模型在数据中的学习趋势有更高的上限，包括输出时更倾向于什么而不是完成任务。
- en: While very powerful, RLHF likely won’t stick around for very long. The reason
    is that it is incredibly computationally expensive for a result that is only incrementally
    better, especially a result that can be achieved and matched by higher-quality
    datasets with supervised learning approaches.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RLHF非常强大，但它可能不会长期存在。原因是，对于仅略有改进的结果，尤其是在可以通过监督学习方法实现并匹配更高质量数据集的结果，它计算成本极高。
- en: 'There are some other problems with RLHF, such as that it requires hiring domain
    experts to evaluate and provide the human feedback. Not only can this get expensive,
    but it can also lead to privacy concerns since these reviewers would need to look
    at actual traffic and user interactions to grade them. To combat both of these
    concerns, you could try to outsource this directly to the users, asking for their
    feedback, but it may end up poisoning your data if your users have ill intent
    or are simply not experts in the subject matter, in which case they might upvote
    responses they like but that aren’t actually correct. This gets to the next problem:
    even experts have biases. RLHF doesn’t train a model to be more accurate or factually
    correct; it trains the model to generate human-acceptable answers.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF还存在一些其他问题，例如它需要雇佣领域专家来评估并提供人类反馈。这不仅可能变得昂贵，还可能导致隐私问题，因为这些审稿人需要查看实际流量和用户交互来评分。为了应对这两个问题，你可以尝试直接将这项工作外包给用户，征求他们的反馈，但如果你的用户有恶意或只是对该主题不精通，这可能会导致你的数据中毒。在这种情况下，他们可能会为那些实际上并不正确但受欢迎的回复点赞。这引出了下一个问题：即使是专家也有偏见。RLHF并没有训练模型变得更加准确或事实正确；它训练模型生成人类可接受的答案。
- en: In production, RLHF has the advantage of allowing you to easily update your
    model on a continual basis. However, this is a two-edged sword, as it also increases
    the likelihood of your model degrading over time. OpenAI uses RLHF heavily, and
    it has led to many users complaining about their models, like GPT-4, becoming
    terrible in certain domains compared to when it first came out. One Stanford study
    found that GPT-4, when asked if a number was prime, used to get it right 98% of
    the time in March 2023, but three months later, in June 2023, it would only get
    it right 2% of the time.[¹](#footnote-245) One reason is that the June model is
    much less verbose, opting to give a simple yes or no response. Humans like these
    responses. Getting straight to the point is often better, but LLMs tend to be
    better after they have had time to reason through the answer with techniques like
    chain of thought.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产中，RLHF的优势在于它允许你轻松地持续更新你的模型。然而，这是一把双刃剑，因为它也增加了模型随时间退化的可能性。OpenAI大量使用RLHF，导致许多用户对其模型，如GPT-4，在某些领域变得非常糟糕，与最初发布时相比。一项斯坦福大学的研究发现，当在2023年3月被问及一个数字是否为素数时，GPT-4曾经有98%的正确率，但三个月后，在2023年6月，它只能正确回答2%。[¹](#footnote-245)一个原因是6月的模型更加简洁，选择给出简单的“是”或“否”回答。人类喜欢这样的回答。直接切入要点通常更好，但LLMs在经过时间用诸如思维链等技术推理答案后往往表现得更好。
- en: With this in mind, RLHF is fantastic for applications where human-acceptable
    answers are the golden standard, and factually correct answers are less important—for
    example, a friendly chatbot or improving summarization tasks. These problems are
    intuitively syntactic in nature, essentially tasks that LLMs are already good
    at but which you want to refine by possibly creating a certain tone or personality.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，RLHF对于需要人类可接受答案作为黄金标准的应用非常出色，例如友好的聊天机器人或改进摘要任务，而对于事实性正确的答案则不那么重要。这些问题在本质上具有直观的语法性质，基本上是LLMs已经擅长但希望通过可能创造某种语气或个性来改进的任务。
- en: Another reason for RLHF degradation is due to data leakage. Data leakage is
    when your model is trained on the test or validation dataset you use to evaluate
    it. When this happens, you are essentially allowing the model to cheat, leading
    to overfitting and poor generalization. It’s just like how LeetCode interview
    questions lead tech companies to hire programmers who have lots of experience
    solving toy problems but don’t know how to make money or do their job.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: RLHF退化的另一个原因是数据泄露。数据泄露是指你的模型在用于评估它的测试或验证数据集上训练。当这种情况发生时，你实际上是在允许模型作弊，导致过拟合和泛化能力差。这就像LeetCode面试题导致科技公司雇佣了大量解决玩具问题但不知道如何赚钱或完成工作的程序员一样。
- en: How does this happen? Well, simply. When you are running an LLM in production
    with RLHF, you know it’s going to degrade over time, so it’s best to run periodic
    evaluations to monitor the system. The more you run these evaluations, the more
    likely that one of the prompts will be picked up for human feedback and subsequent
    RL training. It could also happen by pure coincidence if your users happen to
    ask a question similar to a prompt in your evaluation dataset. Either way, without
    restrictions placed on RLHF (which generally are never done), it’s a self-defeating
    system.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何发生的呢？简单来说。当你使用RLHF在生产环境中运行LLM时，你知道它将随着时间的推移而退化，因此最好定期进行评估以监控系统。你运行这些评估的次数越多，就越有可能有人工反馈和随后的RL训练选择其中一个提示。如果用户恰好提出了与评估数据集中提示类似的问题，这也可能纯粹是巧合。无论如何，如果没有对RLHF（通常从未这样做）施加限制，那么它就是一个自我挫败的系统。
- en: The really annoying aspect of continual updates through RLHF is that these updates
    ruin downstream engineering efforts, methods like prompting or retrieval-augmented
    generation (RAG). Engineering teams can take a lot of effort to dial in a process
    or procedure to query a model and then clean up responses, but all that work can
    easily be undermined if the underlying model is changing. As a result, many teams
    prefer a static model with periodic updates to one with continual updates.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 通过RLHF进行的持续更新真正令人烦恼的地方在于，这些更新会破坏下游工程工作，例如提示或检索增强生成（RAG）等方法。工程团队可能需要投入大量精力来调整查询模型的过程或程序，然后清理响应，但如果底层模型发生变化，所有这些工作都可能轻易被破坏。因此，许多团队更倾向于使用具有定期更新的静态模型，而不是持续更新的模型。
- en: All that said, RLHF is still a powerful technique that may yield greater results
    later as it is optimized and refined. Also, it’s just really cool. We don’t recommend
    using RLHF, and we don’t have the space here to delve deeper; just know that it
    is a tool used by companies specializing in LLMs. For readers who want to understand
    RLHF better, we have included an in-depth example and code listing in appendix
    B.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，RLHF仍然是一种强大的技术，随着其优化和改进可能会产生更好的结果。此外，它真的很酷。我们不推荐使用RLHF，而且这里也没有足够的空间深入探讨；只需知道它是专注于LLMs的公司所使用的一种工具。对于想要更好地理解RLHF的读者，我们在附录B中包含了一个深入的示例和代码列表。
- en: 5.3.4 Mixture of experts
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.4 专家混合
- en: 'A mixture of experts (MoE) is functionally the same as any other model for
    training but contains a trick under the hood: sparsity. This gives the advantage
    of being able to train a bunch of models on a diverse set of data and tasks at
    once. You see, a MoE is exactly what it sounds like: an ensemble of identical
    models in the beginning. You can think of them as a group of freshman undergrads.
    Then, using some unsupervised grouping methods, such as k-means clustering, each
    of these experts “picks a major” during training. This allows the model only to
    activate some experts to answer particular inputs instead of all of them, or maybe
    the input is complex enough that it requires activating all of them. The point
    is that once training has completed, if it has been done on a representative-enough
    dataset, each of your experts will have a college degree in the major that they
    studied. Because the homogeneity of inputs is determined mathematically, those
    majors won’t always have a name that correlates to something you would major in
    at school, but we like to think of these as eccentric double minors or something
    of the sort. Maybe one of your experts majored in physics but double minored in
    advertising and Africana studies. It doesn’t really matter, but the major upside
    to designing an ensemble of models in this way is that you can effectively reduce
    computational requirements immensely while retaining specialization and training
    memory by only consulting the experts whose knowledge correlates with the tokenized
    input at inference time.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 混合专家（MoE）在功能上与任何其他训练模型相同，但在底层包含一个技巧：稀疏性。这赋予了同时在一组多样化的数据和任务上训练多个模型的优势。您可以看到，MoE正是其名称所暗示的：最初是一组相同的模型。您可以将其想象为一群大一新生。然后，使用一些无监督分组方法，例如k-means聚类，每个专家在训练过程中“选择一个专业”。这允许模型只激活一些专家来回答特定的输入，而不是所有专家，或者输入可能足够复杂，需要激活所有专家。重点是，一旦训练完成，如果是在足够代表性的数据集上进行的，每个专家都将拥有他们在所研究专业中的大学学位。由于输入的同质性是数学上确定的，这些专业可能不会总是有一个与学校中您所选择的主修相关的名称，但我们喜欢将其视为古怪的双专业或类似的东西。也许您的某个专家主修物理学，但辅修广告和非洲研究。这并不重要，但以这种方式设计模型集的一个主要优点是，您可以有效地大幅减少计算需求，同时通过仅在推理时咨询与标记化输入相关的专家来保留专业性和训练记忆。
- en: 'In listing 5.5, we finetune a MoE model in much the same way as we did in listing
    5.2 with GPT-2, thanks to Hugging Face’s API and Google’s Switch Transformer.
    Unlike the method we described in chapter 3, where we turned a feed-forward network
    into an MoE, we’ll start with an already created MoE and train it on our own dataset.
    Training an MoE is pretty simple now, unlike when they first came out. Very smart
    people performed so much engineering that we can give an oversimplified explanation
    of these models. Google created the Switch Transformer to combat two huge problems
    they had run into while trying to train LLMs: size and instability. Google engineers
    simplified the routing algorithm (how the model decides which experts to query
    for each input) and showed how to train models with lower quantizations (in this
    case, bfloat16) for the first time—quite an amazing feat and not one to take lightly,
    as GPT-4 is likely an MoE.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表5.5中，我们使用Hugging Face的API和Google的Switch Transformer以与列表5.2中用GPT-2进行的方式类似的方式微调MoE模型。与我们在第3章中描述的方法不同，该方法将前馈网络转换为MoE，我们将从一个已经创建的MoE开始，并在我们的数据集上对其进行训练。现在训练MoE相当简单，与它们最初出现时相比。非常聪明的人进行了大量的工程，我们可以对这些模型给出一个过于简化的解释。Google创建了Switch
    Transformer来应对他们在尝试训练LLM时遇到的两个巨大问题：大小和稳定性。Google工程师简化了路由算法（模型决定为每个输入查询哪些专家的方式）并首次展示了如何使用较低量化（在这种情况下，bfloat16）来训练模型——这是一项相当了不起的成就，而且不容小觑，因为GPT-4很可能是MoE。
- en: Listing 5.5 Example mixture of experts finetuning
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.5 示例混合专家微调
- en: '[PRE15]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '#1 Loads and formats the dataset'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 加载数据集并进行格式化'
- en: '#2 Creates model directory to save to'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 创建用于保存的模型目录'
- en: '#3 Instantiates our tokenizer'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 实例化我们的分词器'
- en: '#4 Establishes our SwitchTransformers config'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 建立我们的SwitchTransformers配置'
- en: '#5 Instantiates our model from the config'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 从配置中实例化我们的模型'
- en: '#6 Creates a tokenize function'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 创建标记化函数'
- en: '#7 Tokenizes our whole dataset (so we never have to do it again)'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 对整个数据集进行标记化（因此我们永远不必再次这样做）'
- en: '#8 Creates a data collator to format the data for training'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 创建数据收集器以格式化训练数据'
- en: '#9 Establishes training arguments'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 建立训练参数'
- en: '#10 Instantiates the trainer'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 实例化训练器'
- en: '#11 Trains and saves the model'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 训练并保存模型'
- en: '#12 Loads the saved model'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '#12 加载已保存的模型'
- en: '#13 Tests the saved model'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '#13 测试已保存的模型'
- en: In this script, an MoE model is finetuned using the Switch Transformer foundation
    model. MoE models are unique during finetuning because you typically update the
    task-specific parameters, such as the gating mechanism and the parameters of the
    experts, while keeping the shared parameters intact. This allows the MoE to use
    the expertise of the different experts for better task-specific performance. Finetuning
    MoE models differs from traditional finetuning because it requires handling the
    experts and gating mechanisms, which can be more complex than regular neural network
    architectures. In our case, we’re lucky that `trainer.train()` with the right
    config covers it for finetuning, and we can just bask in the work that Google
    did before us.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个脚本中，使用Switch Transformer基础模型对MoE模型进行微调。MoE模型在微调过程中具有独特性，因为你通常需要更新任务特定的参数，例如门控机制和专家的参数，同时保持共享参数不变。这使得MoE能够利用不同专家的专业知识，以获得更好的任务特定性能。与传统的微调相比，MoE模型的微调需要处理专家和门控机制，这比常规的神经网络架构更为复杂。在我们的案例中，我们很幸运，`trainer.train()`配合正确的配置就可以覆盖微调，我们只需享受Google在我们之前所做的工作即可。
- en: A logical progression beyond MoE finetuning involves exploring Parameter-Efficient
    Fine-Tuning (PEFT) and low-rank adaptations (LoRA). PEFT aims to make the finetuning
    process more efficient by reducing the model’s size and computational demands,
    making it more suitable for resource-constrained scenarios. Techniques such as
    knowledge distillation, model pruning, quantization, and compression can be employed
    in PEFT to achieve this goal. In contrast, LoRA focuses on incorporating low-rank
    factorization methods into model architectures to reduce the number of parameters
    while maintaining or even enhancing model performance. These approaches are essential,
    as they enable the deployment of sophisticated models on devices with limited
    resources and in scenarios where computational efficiency is paramount.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: MoE微调的逻辑延伸涉及探索参数高效微调（PEFT）和低秩适应（LoRA）。PEFT旨在通过减少模型的大小和计算需求，使微调过程更加高效，使其更适合资源受限的场景。在PEFT中可以采用知识蒸馏、模型剪枝、量化压缩等技术来实现这一目标。相比之下，LoRA专注于将低秩分解方法纳入模型架构中，以减少参数数量，同时保持或甚至提高模型性能。这些方法至关重要，因为它们使得在资源有限和计算效率至关重要的场景中部署复杂模型成为可能。
- en: 5.3.5 LoRA and PEFT
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.5 LoRA和PEFT
- en: LoRA represents a significant breakthrough for machine learning in general.
    Taking advantage of a mathematical trick, LoRAs can change the output of a model
    without changing the original model weights or taking up significant space or
    cost, as shown in figure 5.7\. The reason for the significance here is that it
    makes finetuning a separate model for many different tasks or domains much more
    feasible, as has already been seen in the diffusion space with text2image LoRAs
    popping up quite often for conditioning model output without significantly altering
    the base model’s abilities or style. Put simply, if you already like your model
    and would like to change it to do the exact same thing in a new domain without
    sacrificing what it was already good at on its own, an adapter might be the path
    for you, especially if you have multiple new domains that you don’t want bleeding
    into one another.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA对于机器学习来说是一个重大的突破。利用一个数学技巧，LoRAs可以在不改变原始模型权重或占用大量空间或成本的情况下改变模型的输出，如图5.7所示。这里之所以意义重大，是因为它使得为许多不同的任务或领域单独进行微调变得更加可行，正如在扩散空间中已经看到的，text2image
    LoRAs经常出现，用于调节模型输出，而不会显著改变基础模型的能力或风格。简单来说，如果你已经喜欢你的模型，并希望在不牺牲其本身已经擅长的东西的情况下，将其用于新的领域，适配器可能就是你的选择，尤其是如果你有多个新的领域，你不想让它们相互渗透。
- en: '![figure](../Images/5-7.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/5-7.png)'
- en: Figure 5.7 LoRA exemplifies the idea that you should only need to train and
    save the difference between where the foundation model is and where you want it
    to be. It does this through singular value decomposition (SVD).
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.7 LoRA展示了这样一个想法：你只需要训练和保存基础模型所在位置和你希望它所在位置之间的差异。它是通过奇异值分解（SVD）来实现的。
- en: To understand LoRAs, you need to first understand how models currently adjust
    weights. Since we aren’t going to go over a complete backpropagation tutorial
    here, we can abstract it as
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解LoRAs，首先需要了解模型当前如何调整权重。由于我们不会在这里详细介绍完整的反向传播教程，我们可以将其抽象为
- en: W = W + ΔW
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: W = W + ΔW
- en: So if you have a model with 100 100-dimensional layers, your weights can be
    represented by a 100 × 100 matrix. The cool part comes with singular value decomposition
    (SVD), which has been used for compression by factoring a single matrix into three
    smaller matrices. We covered this topic in depth back in chapter 3 (see listing
    3.2). So while we know the intuition for SVD with LLMs, what can we compress from
    that original formula?
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你有一个包含100个100维层的模型，你的权重可以用一个100 × 100的矩阵来表示。有趣的部分来自于奇异值分解（SVD），它通过将一个矩阵分解成三个较小的矩阵来实现压缩。我们已经在第3章中深入探讨了这一主题（参见列表3.2）。因此，虽然我们了解了LLMs中SVD的直觉，但我们能从原始公式中压缩什么？
- en: ΔW = W[a] × W[b]
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ΔW = W[a] × W[b]
- en: So if ΔW = 100 × 100, W[a] = 100 × c and W[b] = c × 100, where c < 100\. If
    c = 2, you can represent 10,000 elements using only 400 because when they’re multiplied
    together, they equal the 10,000 original elements. So the big question is, what
    does c equal for your task? The c-value is the “R” in LoRA, referring to the rank
    of the matrix of weights. There are algorithmic ways of determining that rank
    using eigenvectors and the like, but you can approximate a lot of it by knowing
    that a higher rank equals more complexity, meaning that the higher the number
    you use there, the closer you’ll get to original model accuracy, but the less
    memory you’ll save. If you think the task you’re finetuning the LoRA for isn’t
    as complex, reduce the rank.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果ΔW = 100 × 100，W[a] = 100 × c 和 W[b] = c × 100，其中 c < 100。如果 c = 2，你只需使用400个元素就能表示10,000个原始元素，因为当它们相乘时，它们等于10,000个原始元素。所以，最大的问题是，c对于你的任务等于多少？c值是LoRA中的“R”，指的是权重矩阵的秩。有算法方法可以通过特征向量等来确定那个秩，但你可以通过知道更高的秩等于更多的复杂性来近似它，这意味着你使用的数字越高，你将越接近原始模型的准确性，但节省的内存越少。如果你认为你微调LoRA的任务并不那么复杂，可以降低秩。
- en: The next listing shows you how to combine creating a LoRA and then perform inference
    with both the LoRA and your base model.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表展示了如何先创建LoRA，然后使用LoRA和你的基础模型进行推理。
- en: Listing 5.6 Example LoRA and PEFT training
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.6 示例LoRA和PEFT训练
- en: '[PRE16]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 Creates model directory to save to'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 创建模型目录以保存'
- en: Keep in mind that you still need to keep your base model, as shown in listing
    5.6\. The LoRA is run in addition to the foundation model; it sits on top and
    changes the weights at only the rank determined in the `LoraConfig` class (in
    this case, `16`). RoBERTa-Large was likely already decent at doing token classification
    on the bionlp dataset, but now, running with the LoRA on top, it’ll be even better.
    There are multiple types of LoRAs you can use, with QLoRA, QA-LoRA, and AWQ-LoRA
    all gaining popularity in different domains and tasks. With the transformers library,
    which can be controlled from the `LoraConfig`, we encourage you to experiment
    with different adaptation methods to find what works for your data and task.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，你仍然需要保留你的基础模型，如列表5.6所示。LoRA是在基础模型之上运行的；它位于顶部并仅改变`LoraConfig`类（在这种情况下，`16`）中确定的秩的权重。RoBERTa-Large可能已经在bionlp数据集上进行token分类方面做得相当不错，但现在，在LoRA之上运行，它将变得更好。你可以使用多种类型的LoRA，QLoRA、QA-LoRA和AWQ-LoRA在不同的领域和任务中都越来越受欢迎。使用可以从`LoraConfig`控制的transformers库，我们鼓励你尝试不同的适应方法，以找到适合你的数据和任务的方法。
- en: The most attractive thing about LoRA is that the particular one we discussed
    here results in a file only 68 KB in size on disk and still has a significant
    performance boost. You could create LoRAs for each portion of your company that
    wants a model, one for the legal team that’s siloed so it doesn’t have to worry
    about any private data it is putting into it, one for your engineering team to
    help with code completion and answering questions about which data structures
    or algorithms to use, and one for anyone else. Because they’re so small, it’s
    suddenly much more feasible to store than the 1.45 GB (14.5 GB if we use Llama
    in fp16; it’s 28 GB in fp32) RoBERTa-Large model being finetuned a bunch of times.
    In the spirit of giving you more of these time- and space-saving tips, we’ll go
    over some things that aren’t mentioned anywhere else, but you may still get some
    use out of if the data science part of LLMs is what you are working with.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA最吸引人的地方在于，我们在这里讨论的特定LoRA在磁盘上仅产生68 KB大小的文件，同时仍然有显著的性能提升。你可以为你的公司想要模型的部分创建LoRAs，一个用于法律团队，这样它就不必担心任何它放入其中的私人数据，一个用于工程团队以帮助代码补全和回答有关使用哪些数据结构或算法的问题，还有一个用于其他人。因为它们如此小巧，存储起来突然变得比微调多次的1.45
    GB（如果我们使用Llama在fp16中则是14.5 GB；在fp32中是28 GB）的RoBERTa-Large模型要容易得多。本着给你更多这些节省时间和空间的小窍门的精神，我们将讨论一些其他地方没有提到的事情，但如果你正在处理LLMs的数据科学部分，你可能仍然会从中得到一些帮助。
- en: 5.4 Training tips and tricks
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 训练技巧与窍门
- en: While this book isn’t focused on training and researching new models, we feel
    kind of bad telling you that finetuning models is an effective strategy for teaching
    LLMs correct guardrails based on your data and then just leaving you to figure
    out how to make it work on your own stuff. With this in mind, let’s look at some
    tried-and-true tips and tricks for both training and finetuning LLMs. These tips
    will help you with some of the least-intuitive parts of training LLMs that most
    practitioners (like us) had to learn the hard way.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这本书并不专注于训练和研究新的模型，但我们觉得有点不好意思告诉你，微调模型是教授LLMs基于你的数据正确护栏的有效策略，然后只是让你自己摸索如何将其应用于你的东西。考虑到这一点，让我们来看看一些经过验证的训练和微调LLMs的技巧和窍门。这些技巧将帮助你解决训练LLMs时最不直观的部分，这些部分大多数从业者（包括我们）都是通过艰难的方式学到的。
- en: 5.4.1 Training data size notes
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 训练数据大小注意事项
- en: First off, LLMs are notorious for overfitting. If you are considering training
    a foundation model, you need to consider the amount of data you have, which should
    be roughly 20× the number of parameters you’re trying to train.[²](#footnote-246)
    For example, if you’re training a 1B parameter model, you should train it on 20B
    tokens. If you have fewer tokens than that, you will run the risk of overfitting.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，LLMs因过拟合而闻名。如果你正在考虑训练一个基础模型，你需要考虑你拥有的数据量，这应该是你试图训练的参数数量的约20倍。[²](#footnote-246)例如，如果你正在训练一个1B参数模型，你应该在20B标记上训练它。如果你比这个数量少的标记，你将面临过拟合的风险。
- en: If you already have a model and need to finetune it on your data, consider the
    inverse, where you should likely have ~0.000001× the number of tokens as a minimum
    (10K tokens for a 1B parameter model). We came up with this rule of thumb based
    on our experience, although it should be fairly intuitive. If you have fewer than
    1/100,000 of your model parameters in tokens, finetuning likely won’t have much
    of an effect. In this case, you should consider another strategy that won’t cost
    as much, such as LoRA (which we just discussed), RAG (which we talk about in the
    next chapter), or a system that uses both.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经有一个模型并且需要在你的数据上微调它，考虑其相反情况，你很可能至少需要 ~0.000001× 的标记数量（对于1B参数模型来说是10K标记）。我们根据经验提出了这个经验法则，尽管它应该是相当直观的。如果你模型参数中的标记少于1/100,000，微调可能不会有太大效果。在这种情况下，你应该考虑另一种成本较低的策略，例如LoRA（我们刚刚讨论过）、RAG（我们将在下一章讨论），或者使用两者的系统。
- en: For both these examples, we’ve had the experience where a company we worked
    for hoped for great results with minimal data and was disappointed. One hoped
    to train an LLM from scratch with only ~1 million tokens while also disallowing
    open source datasets, and another wanted to finetune the model but only on a couple
    of hundred examples. Neither of these approaches were cost-efficient, nor did
    they create models that performed up to the standards the companies aimed for.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这两个例子，我们都有过这样的经历：我们曾经为一家公司工作，该公司希望用最少的数据获得很好的结果，但最终感到失望。一个希望从头开始训练一个LLM，只使用大约100万个标记，同时不允许使用开源数据集；另一个希望微调模型，但只在几百个示例上进行。这两种方法都不经济，也没有创建出达到公司目标标准的模型。
- en: 5.4.2 Efficient training
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 高效训练
- en: We’ve so far focused on tools and methodologies for training, which should supercharge
    your ability to create the best and largest models your training system allows.
    However, other factors should be considered when setting up your training loops.
    In physics, the uncertainty principle shows that you can never perfectly know
    both the speed and position of a given particle. Machine learning’s uncertainty
    principle is that you can never perfectly optimize both your speed and your memory
    utilization. Improving speed comes at the cost of memory, and vice versa. Table
    5.2 shows some choices you can make in training and their effects on speed and
    memory.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们到目前为止一直关注训练的工具和方法，这些应该能极大地提高你创建最佳和最大模型的能力，这些模型是你训练系统允许的。然而，在设置训练循环时，还应考虑其他因素。在物理学中，不确定性原理表明，你永远无法完美地知道给定粒子的速度和位置。机器学习的不确定性原理是，你永远无法完美地优化速度和内存利用率。提高速度是以牺牲内存为代价的，反之亦然。表5.2显示了你在训练中可以做出的选择及其对速度和内存的影响。
- en: Table 5.2 Training choices to consider
  id: totrans-262
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表5.2 需要考虑的训练选择
- en: '| Method | Improve speed | Improves memory utilization | Difficulty |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 提高速度 | 提高内存利用率 | 难度 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Batch size choice  | Yes  | Yes  | Easy  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小选择 | 是 | 是 | 简单 |'
- en: '| Gradient accumulation  | No  | Yes  | Medium  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 梯度累积 | 否 | 是 | 中等 |'
- en: '| Gradient checkpointing  | No  | Yes  | Medium  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 梯度检查点 | 否 | 是 | 中等 |'
- en: '| Mixed precision  | Yes  | No  | Hard  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 混合精度 | 是 | 否 | 困难 |'
- en: '| Optimizer choice  | Yes  | Yes  | Easy  |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 优化器选择 | 是 | 是 | 简单 |'
- en: '| Data preloading  | Yes  | No  | Medium  |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 数据预加载 | 是 | 否 | 中等 |'
- en: '| Compiling  | Yes  | No  | Easy  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 编译 | 是 | 否 | 简单 |'
- en: Carefully consider your options and what goal you’re working toward when setting
    up your training loop. For example, your batch size should be a power of 2 to
    hit maximum speed and memory efficiency. One author remembers working on getting
    an LLM to have a single-digit milli-second response time. The team was gearing
    up to serve millions of customers as fast as possible, and every millisecond counted.
    After using every trick in the book, I was able to achieve it, and I remember
    the huge feeling of accomplishment for finally getting that within the data science
    dev environment. Yet, it turned out that there was a hard batch size of 20 in
    the production environment. It was just a nice number picked out of a hat, and
    too many systems were built around this assumption; no one wanted to refactor.
    Software engineers, am I right?
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置训练循环时，仔细考虑你的选项以及你正在努力实现的目标。例如，你的批量大小应该是2的幂，以实现最大速度和内存效率。有一位作者记得他们曾努力让一个大型语言模型（LLM）拥有个位数的毫秒级响应时间。团队正准备尽可能快地服务数百万客户，每一毫秒都很重要。在用尽所有技巧后，我最终实现了这个目标，我记得在数据科学开发环境中最终实现这一点时的巨大成就感。然而，在生产环境中，有一个硬性规定，批量大小为20。这只是从帽子里随机挑选的一个好数字，而且围绕这个假设构建了太多系统；没有人愿意重构。软件工程师，对吧？
- en: 'For the majority of these methods, the tradeoff is clear: if you go slower,
    you can fit a significantly larger model, but it will take way longer. Gradient
    accumulating and checkpointing can reduce memory usage by ~60%, but training will
    take much longer. The packages we talked about in section 5.1 can help mitigate
    these tradeoffs.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些方法中的大多数，权衡是明显的：如果你放慢速度，你可以拟合一个显著更大的模型，但这将花费更长的时间。梯度累积和检查点可以将内存使用量减少约60%，但训练将花费更长的时间。我们在5.1节中讨论的包可以帮助缓解这些权衡。
- en: 5.4.3 Local minima traps
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.3 局部最小值陷阱
- en: Local minima are hard to spot with LLMs and, as such, can be difficult to avoid.
    If you see your model converging early, be suspicious and judiciously test it
    before accepting the results. When you find that your model is converging early
    at a certain number of steps, one way to avoid it on subsequent runs is to save
    and load a checkpoint 100 or so steps before you see the errant behavior, turn
    your learning rate *way* down, train until you’re sure you’re past it, and then
    turn it back up and continue. Make sure to keep the previously saved checkpoint,
    and save a new checkpoint after that so that you have places to come back to in
    case things go wrong!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLMs中，局部最小值很难被发现，因此可能很难避免。如果你看到你的模型过早收敛，要保持怀疑态度，并在接受结果之前谨慎地测试它。当你发现你的模型在某个步骤数过早收敛时，为了避免在后续运行中再次出现这种情况，一种方法是在看到错误行为之前大约100步保存和加载一个检查点，将学习率大幅降低，训练直到你确信已经过了这个阶段，然后再将其调回并继续。务必保留之前保存的检查点，并在之后保存一个新的检查点，以便在事情出错时有地方可以回退！
- en: 'You can probably tell that this is a frustrating occurrence that one author
    has run into before. He was so confused; he was working on a T5 XXL model, and
    around the 25K step mark, the model was converging and stopping early. He knew
    for a fact that it wasn’t actually converged; it was only 10% through the dataset!
    This happened two or three times, where he loaded up the checkpoint at around
    20K steps and watched the exact same thing happen. It wasn’t until he loaded and
    turned the learning rate down that he finally saw the model improve past this
    point. Once he got through the patch of the local minimum, he turned it back up.
    This happened four more times throughout training this particular model, but since
    he knew what was happening, he was now able to avoid wasting lots of extra time.
    The lesson of the story? Use this rule of thumb: your LLM is not ready if it hasn’t
    trained on your full dataset.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经察觉到，这是一个让作者之前感到沮丧的情况。他非常困惑；他正在处理一个T5 XXL模型，在大约25K步标记处，模型过早收敛并停止。他确信这实际上并没有收敛；它只完成了数据集的10%！这种情况发生了两三次，他在大约20K步时加载了检查点，并目睹了完全相同的事情发生。直到他加载并降低学习率，模型才最终在这个点上得到改善。一旦他通过了局部最小值，他就将其调回。在整个训练这个特定模型的过程中，这种情况又发生了四次，但由于他知道发生了什么，他现在能够避免浪费大量额外的时间。这个故事的经验教训是：如果你的LLM没有在完整数据集上训练，那么它还没有准备好。
- en: 5.4.4 Hyperparameter tuning tips
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.4 超参数调整技巧
- en: Hyperparameter tuning isn’t something we’ve gone over extensively in this book,
    not because it’s not interesting but because it doesn’t help nearly as much as
    changing up your data, either getting more or cleaning it further. If you want
    to tune hyperparameters, Optuna is a great package, and you can get that ~1% boost
    in accuracy or F1 score that you really need. Otherwise, if you’re looking for
    a boost in a particular metric, try representing that metric more completely within
    your dataset and maybe use some statistical tricks like oversampling.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们没有详细讨论超参数调整，这并不是因为它没有趣，而是因为它几乎不能像改变数据那样提供帮助，无论是获取更多数据还是进一步清理数据。如果你想调整超参数，Optuna是一个很好的包，你可以获得你真正需要的约1%的准确度或F1分数的提升。否则，如果你在寻找特定指标的提升，尝试在数据集中更完整地表示该指标，并可能使用一些统计技巧，如过采样。
- en: While hyperparameter tuning is pretty cool mathematically, for LLMs, it’s not
    something that ever really needs to happen. If you need a boost in performance,
    you need more/better data, and tuning your hyperparameters will never match the
    performance boost you’d get quantizing the weights or performing any of the optimizations
    we’ve mentioned here or in chapter 3\. The biggest performance boost we’ve ever
    gotten through tuning hyperparameters was about a 4% increase in F1, and we only
    did it because we wouldn’t be able to change our dataset for a couple of weeks
    at least.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然超参数调整在数学上相当酷，但对于LLMs来说，这并不是真正需要发生的事情。如果你需要提升性能，你需要更多/更好的数据，而调整超参数永远不会达到你通过量化权重或执行我们在这里或第3章中提到的任何优化所获得的性能提升。我们通过调整超参数获得的最大性能提升大约是F1分数提高了4%，我们之所以这样做，是因为我们至少在接下来的几周内无法更改我们的数据集。
- en: 5.4.5 A note on operating systems
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.5 关于操作系统的注意事项
- en: Windows is not the right OS to work professionally with LLMs without the Windows
    Subsystem for Linux. MacOS is great but lacks the hardware packages to really
    carry this load unless you know how to use an NVIDIA or AMD GPU with a Mac. If
    you are uncomfortable with Linux, you should take some time to familiarize yourself
    with it while your OS of choice catches up (if it ever does). A myriad of free
    online materials are available to help you learn about Bash, Linux, and the command
    line. Configuring the CUDA Toolkit and Nvidia drivers on Linux can make you want
    to pull your hair out, but it’s worth it compared to the alternatives. Along with
    this, learn about virtual environments, Docker, and cloud computing, like what’s
    in this chapter!
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 没有 Windows 子系统 for Linux，Windows 不是与 LLM 专业工作的正确操作系统。MacOS 很好，但除非你知道如何使用 Mac
    上的 NVIDIA 或 AMD GPU，否则它缺乏真正承担这个负载的硬件包。如果你对 Linux 不舒服，你应该在你选择的操作系统迎头赶上（如果它真的能迎头赶上）的同时，花些时间熟悉
    Linux。有大量的免费在线材料可以帮助你学习 Bash、Linux 和命令行。在 Linux 上配置 CUDA 工具包和 Nvidia 驱动器可能会让你想拔头发，但与替代方案相比，这是值得的。此外，学习虚拟环境、Docker
    和云计算，就像本章所介绍的那样！
- en: All in all, Windows is easy in the beginning but frustrating in the long run.
    MacOS is also easy in the beginning but currently doesn’t work at all in the long
    run. Linux is incredibly frustrating in the beginning, but once you’re through
    that, it’s smooth sailing.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，Windows 在开始时很容易，但长期来看会让人沮丧。MacOS 在开始时也很容易，但目前长期来看根本无法使用。Linux 在开始时非常令人沮丧，但一旦你过了那个阶段，一切都会变得顺利。
- en: 5.4.6 Activation function advice
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.6 激活函数建议
- en: We’ve neglected to really dive into activation functions so far, not because
    they aren’t useful or cool but because you generally don’t need to tweak your
    activation functions unless you’re doing research science on model performance.
    If you take vanilla GPT-2 and give it a GeGLU activation instead of the GELU that
    it comes with, you will not get a significant boost in anything. In addition,
    you’ll need to redo your pretraining, as it pretrained with a different activation
    function. Activation functions help reduce some of the mathematical weaknesses
    of each layer, be they imaginary numbers from the quadratic attention, exploding
    and vanishing gradients, or maybe the researchers noticed positional encodings
    disappearing as they went through the model and changed a little bit. You can
    learn about activation functions, and we recommend doing so; in general, you can
    trust the papers that introduce new ones.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前没有深入探讨激活函数，这并不是因为它们没有用或者不酷，而是因为你通常不需要调整你的激活函数，除非你在进行模型性能的研究科学。如果你用原味的 GPT-2，并用
    GeGLU 激活函数代替它自带的 GELU，你不会在任何一个方面得到显著的提升。此外，你还需要重新进行预训练，因为它是用不同的激活函数预训练的。激活函数有助于减少每一层的数学弱点，无论是来自二次注意力的虚数、爆炸和消失的梯度，还是研究人员注意到位置编码在通过模型时消失并略有变化。你可以学习有关激活函数的知识，我们建议这样做；一般来说，你可以信任介绍新激活函数的论文。
- en: We’ve come a long way in this chapter, discussing setting up an environment,
    training an LLM from scratch, and looking at a multitude of finetuning techniques.
    While we recognize there are still many aspects to this process that we did not
    touch on and that you need to learn on your own, you should be more than ready
    to create your own models. Now that you have a model, in the next chapter, we’ll
    discuss making it production-ready and creating an LLM service you can use to
    serve online inference.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经走了很长的路，讨论了设置环境、从头开始训练 LLM 以及查看多种微调技术。虽然我们认识到在这个过程中还有许多我们没有涉及到的方面，你需要自己学习，但你应该已经准备好创建自己的模型了。现在你有了模型，在下一章中，我们将讨论使其准备就绪并创建一个你可以用来提供在线推理的
    LLM 服务。
- en: Summary
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: Training is memory intensive, and you will need to master multi-GPU environments
    for many LLM training tasks.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练非常消耗内存，你需要掌握多 GPU 环境以进行许多 LLM 训练任务。
- en: 'Model training has the same basic steps every time:'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练每次都有相同的基本步骤：
- en: '*Dataset preparation*—Acquire, clean, and curate your data.'
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据准备*—获取、清洗和整理你的数据。'
- en: '*Model preparation*—Define model behavior, architecture, loss functions, etc.'
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型准备*—定义模型行为、架构、损失函数等。'
- en: '*Training loop*—Initialization, tokenize, batch data, get predictions/loss,
    backpropagation, etc.'
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练循环*—初始化、分词、批量数据、获取预测/损失、反向传播等。'
- en: Good data has a significantly greater effect on model performance than architecture
    or the training loop.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好的数据对模型性能的影响比架构或训练循环要大得多。
- en: Finetuning is way easier than training from scratch because it requires much
    less data and resources.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调比从头开始训练容易得多，因为它需要的数据和资源要少得多。
- en: Prompting allows us to train a model on a specific task after the fact, which
    is one of the reasons LLMs are so powerful compared to traditional ML.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示允许我们在事后对特定任务进行模型训练，这也是与传统的机器学习相比，大型语言模型如此强大的原因之一。
- en: Prompt tuning is a powerful way to focus your model to respond as a specialist
    to certain prompts.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示调整是一种强大的方法，可以使模型专注于对特定提示做出专家级响应。
- en: Knowledge distillation is useful for training powerful smaller models that are
    efficient and adaptable.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 知识蒸馏对于训练高效且适应性强的强大小型模型很有用。
- en: RLHF is great at getting a model to respond in a way that pleases human evaluators
    but increases factually incorrect results.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RLHF擅长让模型以令人满意的方式响应人类评估者，但会增加事实错误的结果。
- en: Finetuning MoE models differs from traditional finetuning because it requires
    handling the experts and gating mechanisms.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微调MoE模型与传统微调不同，因为它需要处理专家和门控机制。
- en: LoRA is a powerful finetuning technique that adapts pretrained models to new
    tasks by creating tiny assets (low-rank matrices) that are fast to train, easy
    to maintain, and very cost-effective.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LoRA是一种强大的微调技术，通过创建快速训练、易于维护且非常经济高效的微小资产（低秩矩阵）来适应预训练模型的新任务。
- en: The quality and size of your data are two of the most important considerations
    for successfully training your model.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的数据质量和大小是成功训练模型最重要的考虑因素之二。
- en: The major training tradeoff is speed for memory efficiency; if you go slower,
    you can fit a significantly larger model, but it will take way longer.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要的训练权衡是速度与内存效率；如果你放慢速度，可以拟合一个显著更大的模型，但会花费更长的时间。
- en: '[[1]](#footnote-source-1) L. Chen, M. Zaharia, and J. Zou, “How is ChatGPT’s
    behavior changing over time?,” arXiv.org, Jul. 18, 2023, [https://arxiv.org/abs/2307.09009](https://arxiv.org/abs/2307.09009).'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#脚注来源-1) L. Chen，M. Zaharia和J. Zou，“ChatGPT的行为是如何随时间变化的？”，arXiv.org，2023年7月18日，[https://arxiv.org/abs/2307.09009](https://arxiv.org/abs/2307.09009)。'
- en: '[[2]](#footnote-source-2) J. Hoffmann et al., “Training compute-optimal large
    language models,” arXiv:2203.15556 [cs], March 2022, [https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556).'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[[2]](#脚注来源-2) J. Hoffmann等人，“训练计算最优的大型语言模型”，arXiv:2203.15556 [cs]，2022年3月，[https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)。'
