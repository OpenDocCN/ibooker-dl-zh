<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 2. Pre-Training Data"><div class="chapter" id="ch02">
<h1><span class="label">Chapter 2. </span>Pre-Training Data</h1>


<p>In <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>, we introduced language models, noted their strengths and limitations<a data-type="indexterm" data-primary="pre-training of data" id="xi_pretrainingofdata24102"/>, explored current and potential use cases, and presented the scaling laws that seemingly govern progress in this field. To set the stage for the rest of this book, in the next three chapters we will discuss in detail the recipe for pre-training LLMs and the ingredients that go into them. But wait, this book is about utilizing pre-trained LLMs to design and build user applications. Why do we need to discuss the nuances of pre-training these gargantuan models from scratch, something most machine learning practitioners are never going to do in their lives?</p>

<p>Actually, this information is very important because many of the decisions made during the pre-training process heavily impact downstream performance. As we will notice in subsequent chapters, failure modes are more easily understandable when you comprehend the training process. Just like we appreciate having ingredients listed on packages at our grocery stores, we would like to know the ingredients that go into making a language model before we use it in serious applications.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Not much information is available in the public realm about some of the proprietary LLMs that are accessible only through an API. This book will provide as much information as has been made public. While the lack of information doesn’t mean that we should avoid using these models, model transparency is something that you might need to consider while making a final decision regarding what model to use.</p>
</div>






<section data-type="sect1" data-pdf-bookmark="Ingredients of an LLM"><div class="sect1" id="id20">
<h1>Ingredients of an LLM</h1>

<p>Let’s start with the ingredients that go into making an LLM<a data-type="indexterm" data-primary="pre-training of data" data-secondary="ingredients of LLM" id="xi_pretrainingofdataingredientsofLLM21660"/><a data-type="indexterm" data-primary="LLMs (Large Language Models)" data-secondary="ingredients" id="xi_LLMsLargeLanguageModelsingredients21660"/>.</p>

<p>Broadly speaking, we have:</p>
<dl>
<dt>Pre-training data: What’s it trained on?</dt>
<dd>
<p>The old computer science adage “garbage in, garbage out” is still accurate when it comes to language modeling. In this chapter  we will explore popular pre-training datasets and dig into the various preprocessing steps taken to ensure <em>high-quality</em> data is fed to the model. We will also showcase some tools that allow us to probe these datasets and understand how pre-training data composition impacts downstream tasks.</p>
</dd>
<dt>Vocabulary and tokenizer: What’s it trained over?</dt>
<dd>
<p>To build a model over a language<a data-type="indexterm" data-primary="vocabulary and tokenization" id="id494"/>, we first have to determine the vocabulary of the language we are modeling and rules to break down a stream of text into the right vocabulary units, referred to as tokenization. (We will dedicate <a data-type="xref" href="ch03.html#chapter-LLM-tokenization">Chapter 3</a> to discussing these concepts.) Linguistically, humans process language in terms of meaning-bearing words and sentences. Language models process language in terms of tokens. We will explore the downstream impact when there is a mismatch between the two.</p>
</dd>
<dt>Learning objective: What is it being trained to do?</dt>
<dd>
<p>By pre-training a language model<a data-type="indexterm" data-primary="learning objectives" id="id495"/>, we aim to imbue the language model with general skills in syntax, semantics, reasoning, and so on, that hopefully will enable it to reliably solve any task you throw at it, even if it was not specifically trained on the task. Therefore the training objectives should be sufficiently general to capture all these skills. In <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>, we will discuss the various tasks (learning objectives) that pre-trained models are trained on. You might wonder if LLMs are better suited to solving downstream tasks that are similar to the tasks the pre-trained model has been trained to solve. We will test this assumption and discuss the impact various learning objectives have on task performance.</p>
</dd>
<dt>Architecture: What’s its internal structure?</dt>
<dd>
<p>The architecture<a data-type="indexterm" data-primary="architectures" id="id496"/> of a model refers to the components of a model, how they connect and interact with each other, and how they process input. Each architecture has its own inductive bias, a set of assumptions made about the data and tasks it will be used for, biasing the model toward certain types of solutions. In <a data-type="xref" href="ch04.html#chapter_transformer-architecture">Chapter 4</a>, we will conduct a deep dive into the Transformer architecture, which, as discussed in <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>, is the predominantly used architecture currently.</p>
</dd>
</dl>

<p>Let’s look at how these ingredients fit together in <a data-type="xref" href="#ingredients-of-llm">Figure 2-1</a>.</p>

<figure><div id="ingredients-of-llm" class="figure">
<img src="assets/dllm_0201.png" alt="LLM Ingredients" width="600" height="93"/>
<h6><span class="label">Figure 2-1. </span>How all the ingredients come together to make an LLM</h6>
</div></figure>

<p>The language models<a data-type="indexterm" data-primary="base models" data-secondary="fine-tuning" id="id497"/><a data-type="indexterm" data-primary="fine-tuning models" data-secondary="base models" id="id498"/><a data-type="indexterm" data-primary="models" data-secondary="base" id="id499"/> trained using the process described in this chapter and the next are called <em>base models</em>. Lately, model providers have been augmenting the base model by fine-tuning it on much smaller datasets to steer them toward being more aligned with human needs and preferences. Some popular tuning modes<a data-type="indexterm" data-primary="fine-tuning models" data-secondary="modes for" id="id500"/> are:</p>

<ul>
<li>
<p>Supervised instruction fine-tuning (SFT)<a data-type="indexterm" data-primary="supervised fine-tuning (SFT)" id="id501"/><a data-type="indexterm" data-primary="fine-tuning models" data-secondary="supervised fine-tuning" id="id502"/>, so that the model is better at following human instructions</p>
</li>
<li>
<p>Reinforcement learning by human feedback (RLHF)<a data-type="indexterm" data-primary="reinforcement learning from human feedback (RLHF)" id="id503"/><a data-type="indexterm" data-primary="RLHF (reinforcement learning from human feedback)" id="id504"/>, so that the model is better aligned with human preferences</p>
</li>
<li>
<p>Domain-adaptive or task-adaptive continued pre-training<a data-type="indexterm" data-primary="domain-adaptive models" id="id505"/><a data-type="indexterm" data-primary="task-adaptive models" id="id506"/><a data-type="indexterm" data-primary="models" data-secondary="domain-adaptive" id="id507"/><a data-type="indexterm" data-primary="models" data-secondary="task-adaptive" id="id508"/>, so that the model is better attuned to specific domains and tasks</p>
</li>
</ul>

<p>Based on the specific augmentation carried out, the resulting models are called <em>instruct models</em>, <em>chat models</em>, and so on.</p>

<p>We will cover instruct and chat models in <a data-type="xref" href="ch06.html#llm-fine-tuning">Chapter 6</a>, and domain-adaptive and task-adaptive pre-training in <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>.</p>

<figure><div id="derivative-models" class="figure">
<img src="assets/dllm_0202.png" alt="Derivative Models" width="600" height="605"/>
<h6><span class="label">Figure 2-2. </span>The relationship between base models and their derivatives</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id509">
<h1>LLM Pre-Training Challenges</h1>
<p>Pre-training an LLM<a data-type="indexterm" data-primary="pre-training of data" data-secondary="challenges of" id="id510"/> is a very technically challenging task and requires a lot of computational resources and exceptional technical skills. For example, GPT-4’s <a href="https://oreil.ly/wcAhT">technical report</a> credits 343 unique contributors, not including the <a href="https://oreil.ly/hLJno">annotators</a> in Kenya who contributed to their <a href="https://oreil.ly/KxSI1">RLHF training</a>. Delving into every aspect of pre-training LLMs is an entire book in itself. In this chapter we will not focus on infrastructure or engineering considerations for pre-training LLMs, nor on the nuances of distributed and parallel computing. We will instead focus on aspects of the pre-training process that can directly impact your application’s behavior and performance<a data-type="indexterm" data-startref="xi_pretrainingofdataingredientsofLLM21660" id="id511"/><a data-type="indexterm" data-startref="xi_LLMsLargeLanguageModelsingredients21660" id="id512"/>.</p>

<p>However, if you are curious to read more about the challenges involved in pre-training LLMs, here are some useful resources:</p>

<ul>
<li>
<p><a href="https://oreil.ly/9BYNz">The Technology Behind BLOOM Training</a>, a blog post from from Big Science that explains the hardware, types of parallelisms employed, and optimizations used in training BLOOM, an open-source 176B parameter multilingual model.</p>
</li>
<li>
<p>Training chronicles (log book) from <a href="https://oreil.ly/j6UT8">BLOOM</a> and <a href="https://oreil.ly/3W3TH">OPT</a>, which is a 175B parameter LLM released by Meta, documenting the trials and tribulations faced during training, including hardware failures and how to recover from them, training instabilities, loss spikes, and the like.</p>
</li>
<li>
<p><a href="https://oreil.ly/lGwPa">Open Pretrained Transformers</a>, a video featuring Susan Zhang, the lead author of OPT, who discusses the OPT chronicles in detail.</p>
</li>
<li>
<p><a href="https://oreil.ly/OgYS9">Blog series by Imbue</a> chronicling their efforts to train a 70B parameter model from scratch.</p>
</li>
</ul>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Pre-Training Data Requirements"><div class="sect1" id="id21">
<h1>Pre-Training Data Requirements</h1>

<p>Although it has been shown that higher-capacity models<a data-type="indexterm" data-primary="pre-training of data" data-secondary="requirements for" id="xi_pretrainingofdatarequirementsfor26655"/><a data-type="indexterm" data-primary="sample efficiency issue" id="id513"/> are relatively more <a href="https://oreil.ly/PbN6F">sample efficient</a>, in general today’s language models are very sample inefficient, meaning they need tons of examples to learn a task. It is infeasible to create such a large supervised dataset with human annotations, hence the predominant means to pre-train language models is using <em>self-supervised</em> learning<a data-type="indexterm" data-primary="self-supervised learning" id="id514"/>, where the target labels exist within your training inputs.</p>

<p>Using this setup, virtually any type of text is fair game to be included in a pre-training dataset, and theoretically any nontextual signal with some structure can be encoded in text and included as part of a pre-training dataset.</p>

<p>From our scaling laws<a data-type="indexterm" data-primary="scaling laws" id="id515"/> discussion in <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>, we know that model performance increases by just training them longer and on more data. Also, as discussed in <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>, the <em>consolidation effect</em> at play<a data-type="indexterm" data-primary="consolidation effect" id="id516"/> in the field raises expectations on what a single language model is expected to do end-to-end. Today a single model is expected to answer factual questions about the world, employ arithmetic and logical reasoning, write code, and come up with creative ideas.</p>

<p>All this means that the data needs for language model pre-training are enormous. Now, the key question is whether textual data available in the world actually contains sufficient and relevant signals needed to learn all the skills we want LLMs to learn.</p>

<p>Note that language models that are trained solely on text only have access to the linguistic form<a data-type="indexterm" data-primary="linguistic form, grounding of" id="id517"/>, i.e., the sequence of characters making up a sentence like, “Walter White tossed the pizza onto the roof.” To understand its meaning, the linguistic form has to be mapped to the communicative intent of the writer/speaker. While a <a href="https://oreil.ly/3iYA2">section</a> of the research community argues that one cannot learn meaning from form alone, recent language models are increasingly proving otherwise.</p>

<p>To have access to the full picture, the linguistic form needs to be grounded to the real world. In the cognitive sciences, grounding is defined as:</p>
<blockquote>
  <p>The process of establishing what mutual information is required for successful communication between two interlocutors</p>
  <p data-type="attribution">Chandu et al., <a href="https://oreil.ly/kPyXu">“Grounding ‘grounding’ in NLP”</a></p>
</blockquote>

<p>Human text is generally very underspecified, with a lot of communicative intent existing outside the textual context, depending on the reader/listener to use their common sense, world knowledge, and ability to detect and understand emotional subtext to interpret it.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>It is estimated that only around <a href="https://oreil.ly/jg4tW">12% of information</a> we understand from text is explicitly mentioned in text. There are several theories explaining why we communicate thus<a data-type="indexterm" data-primary="Zipf’s principle of least effort" id="id518"/>, including <a href="https://oreil.ly/UX7Nd">Zipf’s principle of least effort</a>, which states it is “human nature to want the greatest outcome at the least amount of work.”</p>
</div>

<p>The field of NLP<a data-type="indexterm" data-primary="natural language processing (NLP)" id="id519"/><a data-type="indexterm" data-primary="NLP (natural language processing)" id="id520"/> has seen <a href="https://oreil.ly/PbIhT">a lot of work</a> in grounding language models to the real world. <a href="https://oreil.ly/ysAeM">Multimodal models</a> that combine different modalities<a data-type="indexterm" data-primary="multimodal models" id="id521"/><a data-type="indexterm" data-primary="models" data-secondary="multimodal" id="id522"/> like image, video, speech, and text are a promising avenue of research, and they are likely to see more widespread usage in the coming years. Imagine a model seeing “pizza” in the training text, but also getting signals on how it looks, how it sounds, and how it tastes!</p>

<p>But do multimodal models really help with the grounding problem? Can we instead achieve the effect of grounding by just feeding the model with massive amounts of diverse text? These are unsolved questions, and there are good arguments in both directions as shown by this <a href="https://oreil.ly/oacht">debate</a>.</p>

<p>Whether training on massive amounts of text alone can enable language models to learn skills like logical reasoning is another open question. Note that text on the internet contains a lot of text describing reasoning steps, like theorem proofs, explanations of jokes, step-by-step answers to puzzles, and so on. However, there is simply not enough of derivational text going around, which leads us to cover the shortfall by using prompting methods like CoT (described further in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>). There is <a href="https://oreil.ly/Qlntp">recent evidence</a> that process supervision<a data-type="indexterm" data-primary="process supervision" id="id523"/>, where feedback is provided for each step of the problem-solving process, as opposed to outcome supervision, where feedback is provided only on the final solution, helps improve arithmetic 
<span class="keep-together">reasoning</span>.</p>

<p>A crucial skill that language models have to learn is dealing with the inherently ambiguous nature of language. Following up on the aforementioned Zipf’s principle of least effort, ambiguity enables speakers to manage the efficiency-clarity tradeoff in communication. We can leave a lot unsaid because we have established sufficient common ground with the people we are communicating with and trust that they are able to fill in the gaps.</p>

<p>Earlier language models struggled a lot with modeling ambiguity<a data-type="indexterm" data-primary="ambiguity, challenges of modeling" id="id524"/>. I long used this sentence as a canonical example in my NLP talks to highlight ambiguity in language: “WWE’s John Cena surprises Make-A-Wish 7-year-old with cancer.”</p>

<p>While state-of-the-art models are able to correctly interpret this particular sentence and not mistakenly identify John Cena as an evil disease-spreading wizard, <a href="https://oreil.ly/BrSwb">recent work</a> shows that even the best models of today still struggle to deal with ambiguity in general. Whether just scaling up models and data is enough for LLMs to model ambiguity is an open question.</p>

<p>If our only option to resolve all these shortcomings is to scale<a data-type="indexterm" data-primary="datasets" data-secondary="scaling up to deal with ambiguity" id="id525"/> up dataset sizes, the next question is if we actually have enough data available in the world that is sufficient for  LLMs to learn these skills. Are we at risk of running out of training data anytime soon? There is a misconception in certain quarters of our field that we already have. However, lack of raw data is not yet a bottleneck in training models. For instance, there are billions of publicly available documents accessible by scraping or via a free API that haven’t yet made it into most pre-training data sets, such as parliamentary proceedings, court judgments, and most SEC filings. <a href="https://oreil.ly/XnmHL">“How much LLM training data is there, in the limit?”</a> by Educating Silicon estimates the amount of text present in the world.  On the other hand, it is true that at a sufficiently large scale, there is simply not enough naturally occurring data to feed our models.</p>

<p>Thus, there are efforts to use text generated by language models<a data-type="indexterm" data-primary="synthetic data" id="id526"/>, termed <em>synthetic data</em>, to train models, albeit with the <a href="https://oreil.ly/RdzX0">risk</a> that training on LLM-generated data can potentially be detrimental, as the model deviates from the true distribution of the data. Later in this chapter, we will learn the process behind creating synthetic data for pre-training.</p>

<p>Of course, not all data is created equal. We can achieve more sample efficiency with high-quality data, thus needing smaller dataset sizes. We can preprocess data in order to filter out low-quality data or make them higher quality.  What exactly makes data high quality is a nuanced question, which we will explore later in the chapter.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id527">
<h1>Training for Multiple Epochs</h1>
<p>An epoch<a data-type="indexterm" data-primary="epoch (training dataset)" id="id528"/><a data-type="indexterm" data-primary="datasets" data-secondary="epoch" id="id529"/> refers to the model being exposed to the complete training dataset during the training process. Earlier LLMs were trained on just one epoch or less, to protect against overfitting. Can we just increase the size of the training data by training the model for multiple epochs on the same dataset, i.e., the model sees the same dataset multiple times during training?</p>

<p>It turns out that we can. Work by <a href="https://oreil.ly/d-PUZ">Muennighoff et al.</a> show that language models can be trained up to four to five epochs with no decrease in performance. The utility of a training example diminishes if it is repeated beyond that. Therefore, we just can’t train on high-quality data over a large number of epochs and call it a day; we still need to mix in large-scale, lower-quality data as we are constrained by the size of high-quality data and the number of times it can be repeated without losing its value.</p>

<p><a href="https://oreil.ly/m1Suq">Xue et al.</a> show that larger models overfit more easily when trained on multiple epochs. They also showed that using regularization techniques like <a href="https://oreil.ly/Wo5kV">Dropout</a> can help address the overfitting problem to a certain extent.</p>

<p>The quality-quantity tradeoff we see with multi-epoch training has been quantified by <a href="https://oreil.ly/tafHo">Goyal et al.</a>, proposing a new set of data filtering scaling laws<a data-type="indexterm" data-startref="xi_pretrainingofdatarequirementsfor26655" id="id530"/>.</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Popular Pre-Training Datasets"><div class="sect1" id="id22">
<h1>Popular Pre-Training Datasets</h1>

<p>A lot of text is not freely available in public<a data-type="indexterm" data-primary="pre-training of data" data-secondary="popular datasets" id="xi_pretrainingofdatapopulardatasets212148"/><a data-type="indexterm" data-primary="datasets" id="xi_datasets212148"/>. This includes data exposed behind paywalled APIs and login screens, and paywalled books and documents, many of which may not even be digitized. Larger companies like Google and OpenAI can afford to purchase this data; for example, OpenAI has <a href="https://oreil.ly/ygIO2">struck deals</a> worth hundreds of millions of dollars with the <em>Wall Street Journal</em>, <em>Financial Times</em>, and other news organizations for access to their data. Domain-specific text is often proprietary and available only to large incumbents (for example, Bloomberg trained <a href="https://oreil.ly/87r4j">BloombergGPT</a> partly on its proprietary financial data). However, even for models trained by the largest companies, a significant proportion of training data comes from publicly available data sources.</p>

<p>Next, we will cover some of the most popular general-purpose pre-training datasets that are being used to train LLMs. While this is not a comprehensive list, most LLMs, including closed-source ones, have at least a large subset of their training data drawn from these sources. We will defer discussion of domain-specific (catered to a particular field like social media, finance, biomedical, etc.) datasets to <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>.</p>
<div data-type="tip"><h6>Tip</h6>
<p>Most general-purpose LLMs are trained to be a jack-of-all-trades—to be able to solve tasks from a variety of domains. If the data domain for your use case is included in the pre-training dataset, models trained on those datasets may show relative performance improvements on downstream tasks compared to models that aren’t, even if the pre-training data is unlabeled. This means that if you intend to use LLMs for specific, well-defined use cases in a particular domain, domain-specific models<a data-type="indexterm" data-primary="domain-specific models" id="id531"/> could prove promising. You can also perform <em>continued domain-adaptive</em> or <em>task-adaptive pre-training</em> on your domain data<a data-type="indexterm" data-primary="continued domain-adaptive or task-adaptive pre-training" id="id532"/> to leverage this phenomenon. This will be discussed in detail in <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>.</p>
</div>

<p>Here are some examples of commonly used data sources for general-purpose language models:</p>
<dl>
<dt>Common Crawl/C4</dt>
<dd>
<p>The web is the largest source<a data-type="indexterm" data-primary="Common Crawl/C4" id="id533"/> of openly available textual data, and hence forms a significant proportion of pre-training datasets. <a href="https://oreil.ly/dhBvu">Common Crawl</a> is a nonprofit that creates and publishes snapshots of all web crawl data, updated every month. However, as one could imagine, this is an extremely coarse dataset and needs to be significantly cleaned before it is ready to use. Google prepared C4 (Colossal Clean Crawled Corpus), a 750GB English-language dataset, after applying a set of preprocessing and filtering steps to a Common Crawl snapshot from 2019 and released the code for it. <a href="https://oreil.ly/bxmVR">Dodge et al.</a> used this script to reproduce C4 and have made it publicly available. C4 has been used for training several well-known LLMs including all models from the T5 family.</p>
</dd>
<dt>The Pile</dt>
<dd>
<p><a href="https://oreil.ly/7UAcY">The Pile</a> is a 825GB dataset from Eleuther AI<a data-type="indexterm" data-primary="The Pile dataset" data-primary-sortas="Pile" id="id534"/><a data-type="indexterm" data-primary="Eleuther AI" data-secondary="The Pile dataset" data-secondary-sortas="Pile dataset" id="id535"/>, which focused on publishing a dataset drawn from more diverse sources. Diversity of data is important since in-domain unlabeled data in pre-training is helpful for downstream performance on that domain, and diverse data sets also enable generalization to previously unseen tasks and domains. To this end, the data from The Pile comes not only from Common Crawl but also PubMed Central, arXiv, GitHub, the FreeLaw Project, Stack Exchange, the US Patent and Trademark Office, Ubuntu IRC, HackerNews, YouTube, PhilPapers, NIH ExPorter, Project Gutenberg, and Wikipedia, among others. The Pile and its subsets have been preferred as a data source for training several LLMs, including <a href="https://oreil.ly/_8eOD">Llama</a>.</p>
</dd>
<dt>WebText/OpenWebText/OpenWebText2</dt>
<dd>
<p>These refer to a subset of web tex<a data-type="indexterm" data-primary="WebText dataset" id="id536"/><a data-type="indexterm" data-primary="OpenWebText" id="id537"/><a data-type="indexterm" data-primary="OpenWebText2" id="id538"/>t and are limited to web pages representing outbound links on Reddit that have at least three <em>karma</em>, the absolute difference between user upvotes and downvotes. The assumption is that the wisdom of the crowd will enable only high-quality links to surface, which contain information people actually find interesting. Models that have been trained on this data include GPT-2 and GPT-3.</p>
</dd>
<dt>Wikipedia</dt>
<dd>
<p>Wikipedia<a data-type="indexterm" data-primary="Wikipedia" id="id539"/> assumes a major role in the training of just about every general-purpose LLM. A full dump of Wikipedia contains valuable encyclopedic text that provides factual knowledge to the model. Wikipedia’s editorial system ensures that the text follows a highly structured format. However, it is not diverse stylistically, as the text is written in a formal manner. Therefore, Wikipedia alone is not sufficient to train a rudimentary language model and needs to be combined with data sources comprising diverse writing styles.</p>
</dd>
<dt>BooksCorpus/BooksCorpus2</dt>
<dd>
<p>Probably the most historically influential of all pre-training datasets<a data-type="indexterm" data-primary="BooksCorpus/BooksCorpus2" id="id540"/>, this dataset was part of the training corpus for well-known models like BERT, RoBERTa, GPT-2/3, etc. The BooksCorpus contains over 7,000 free, mostly fiction books written by unpublished authors. Twenty-six percent of books in the original dataset belonged to the Romance genre. A replication of the BooksCorpus is present in The Pile as BooksCorpus2.</p>
</dd>
<dt>FineWeb</dt>
<dd>
<p>As of the book’s writing<a data-type="indexterm" data-primary="FineWeb" id="id541"/>, <a href="https://oreil.ly/1GyZd">FineWeb</a> is the world’s largest publicly available pre-training dataset. Published by Hugging Face, FineWeb has 15 trillion tokens and is drawn from 96 snapshots of Common Crawl, after a rigorous cleaning and filtering process. Hugging Face also released <a href="https://oreil.ly/8XHH-">FineWeb-Edu</a>, a subset of FineWeb composed of educational data, which is crucial in enabling LLMs to pass standardized tests and popular 
<span class="keep-together">benchmarks</span>.</p>
</dd>
</dl>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id542">
<h1>Training Data Is Disappearing from the Internet</h1>
<p>Unfortunately, many of the aforementioned data sources<a data-type="indexterm" data-primary="training data" data-secondary="copyright licensing issues" id="id543"/> are embroiled in controversy, due to copyright issues. Several books in the BooksCorpus dataset have <a href="https://oreil.ly/hGmnj">restrictive copyright licenses</a>. The original corpus<a data-type="indexterm" data-primary="restrictive copyright licenses" id="id544"/> is no longer public. Similarly, the original version of The Pile is no longer available for download from its creators, due to the presence of copyrighted content. A derivative of The Pile with copyrighted content removed has been made <a href="https://oreil.ly/6oPSC">available</a>.</p>

<p>Over time, more and more websites are updating their terms of service or <em>robots.txt</em> file to disapprove of their data being used for AI training. These terms range from restrictions on specific companies like OpenAI to a blanket ban on all forms of AI training. News, forums, and social media websites, which comprise high-quality training data, are more likely to list restrictions. <a href="https://oreil.ly/qPSOj">Longpre et al.</a> estimate that 5% of overall tokens in C4, and 28% of the important sources of C4, are restricted from being used for AI training.</p>

<p>Note that <em>robots.txt</em> is not a legally enforced standard, and companies continue to violate the terms of service of these websites, arguing that using the data for LLM training is covered under fair use grounds. In many cases, the websites only host the data, and the copyright is held by the content creators. The legal consequence for these issues is still being determined by courts.</p>
</div></aside>

<p><a data-type="xref" href="#popular-pretraining-datasets">Table 2-1</a> provides a list of some of the most commonly used datasets, their size, year of release, and the means to access them.</p>
<table id="popular-pretraining-datasets">
<caption><span class="label">Table 2-1. </span>Popular pretraining datasets</caption>
<thead>
<tr>
<th>Name</th>
<th>Data source(s)</th>
<th>Size</th>
<th>Year released</th>
<th>Public?</th>
<th>Models using this dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>C4</p></td>
<td><p>Common Crawl</p></td>
<td><p>750GB</p></td>
<td><p>2019</p></td>
<td><p>Yes (reproduced version)</p></td>
<td><p>T5, FLAN-T5, UL2, Llama, etc.</p></td>
</tr>
<tr>
<td><p>The Pile</p></td>
<td><p>Common Crawl, PubMed Central, Wikipedia, arXiv, Project Gutenberg, Stack Exchange, USPTO, GitHub, etc.</p></td>
<td><p>825GB</p></td>
<td><p>2020</p></td>
<td><p>Yes</p></td>
<td><p>GPT-NeoX, GPT-J, Cerebras-GPT, StableLM, Pythia, etc.</p></td>
</tr>
<tr>
<td><p>RedPajama</p></td>
<td><p>Common Crawl, GitHub, Wikipedia, arXiv, Stack Exchange, etc.</p></td>
<td><p>1.2T tokens</p></td>
<td><p>2023</p></td>
<td><p>Yes</p></td>
<td><p>Red Pajama-INCITE, MPT</p></td>
</tr>
<tr>
<td><p>BooksCorpus</p></td>
<td><p>Sampled from smashwords.com</p></td>
<td><p>74M sentences</p></td>
<td><p>2015</p></td>
<td><p>Original not available anymore</p></td>
<td><p>Most models including BERT, GPT, etc.</p></td>
</tr>
<tr>
<td><p>OpenWebText2</p></td>
<td><p>Outbound Reddit links</p></td>
<td><p>65GB</p></td>
<td><p>2020</p></td>
<td><p>Yes</p></td>
<td><p>GPT-2, GPT-3</p></td>
</tr>
<tr>
<td><p>ROOTS</p></td>
<td><p>Big Science Catalogue, Common Crawl, GitHub</p></td>
<td><p>1.6T tokens</p></td>
<td><p>2022</p></td>
<td><p>No (but available on request)</p></td>
<td><p>BLOOM</p></td>
</tr>
<tr>
<td><p>RefinedWeb</p></td>
<td><p>Common Crawl</p></td>
<td><p>5T tokens</p></td>
<td><p>2023</p></td>
<td><p>Yes (600B subset only)</p></td>
<td><p>Falcon</p></td>
</tr>
<tr>
<td><p>SlimPajama</p></td>
<td><p>Cleaned from RedPajama</p></td>
<td><p>627B tokens</p></td>
<td><p>2023</p></td>
<td><p>Yes</p></td>
<td><p>N/A</p></td>
</tr>
</tbody>
</table>

<p>The table highlights the fact that most models are trained on similar data sources. In this chapter, we are limiting our coverage to pre-training datasets for base models. We will cover datasets used to augment base models like instruction tuning datasets, RLHF datasets, etc. in <a data-type="xref" href="ch06.html#llm-fine-tuning">Chapter 6</a>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id545">
<h1>Copyright Issues Pertaining to Pre-Training Datasets</h1>
<p>Can LLMs be trained on copyrighted text<a data-type="indexterm" data-primary="copyright issues, pre-training datasets" id="id546"/><a data-type="indexterm" data-primary="datasets" data-secondary="copyright issues" id="id547"/> without the explicit consent of the copyright holder and without attribution? Can LLMs be trained on text that inadvertently contains sensitive personal information without legal liabilities? These are all fluid legal and moral questions. In the US, the fair use doctrine has been used to justify training LLMs on copyrighted text. However, this is currently being tested, and as of this book’s writing, a <a href="https://oreil.ly/QcIKy">class action lawsuit</a> has been filed against GitHub, Microsoft, and OpenAI for using code from GitHub repositories that were published under restrictive licenses for training the code LLMs powering GitHub Copilot. The AI community will be watching this case with interest. However, all over the world, laws are <a href="https://oreil.ly/6sgh_">fast loosening</a> to permit this type of usage and clear legal hurdles for LLM training and adoption.</p>

<p>As LLM usage expands and they become an integral part of the economy, data used to train them becomes more valuable. Reddit and Stack Overflow, both of which have been important sources of data in many influential pre-training datasets, have <a href="https://oreil.ly/JdICc">announced</a> they will start charging for data access. Expect more such announcements in the future.</p>

<p>What are the copyright implications for people and organizations using these language models downstream? We will discuss this in more detail in <a data-type="xref" href="ch05.html#chapter_utilizing_llms">Chapter 5</a>, where we will provide more background on the various types of software licenses and their degree of permissibility for commercial usage.</p>
</div></aside>

<p>Let’s explore the content of these pre-training datasets. Using a Google Colab notebook or a code editor of your choice, load the <code>realnewslike</code> subset of the C4 dataset, which consumes around 15 GB:</p>

<pre data-type="programlisting" data-code-language="python" class="less_space pagebreak-before"><code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">datasets</code> <code class="kn">import</code> <code class="n">dataset</code>
<code class="n">realnewslike</code> <code class="o">=</code> <code class="n">load_dataset</code><code class="p">(</code><code class="s2">"allenai/c4"</code><code class="p">,</code> <code class="s2">"realnewslike"</code><code class="p">,</code>
                            <code class="n">streaming</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">split</code><code class="o">=</code><code class="s2">"train"</code><code class="p">)</code>
<code class="k">for</code> <code class="n">i</code><code class="p">,</code> <code class="n">example</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">realnewslike</code><code class="p">):</code>
    <code class="k">if</code> <code class="s2">"Iceland"</code> <code class="ow">in</code> <code class="n">example</code><code class="p">[</code><code class="s2">"text"</code><code class="p">]:</code>
        <code class="nb">print</code><code class="p">(</code><code class="n">example</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">i</code> <code class="o">==</code> <code class="mi">10000</code><code class="p">:</code>  <code class="c1"># Limit to 10,000 iterations for demonstration</code>
        <code class="k">break</code></pre>

<p>Using this code, we can observe all the instances in which Iceland appears in this C4 subset<a data-type="indexterm" data-startref="xi_pretrainingofdatapopulardatasets212148" id="id548"/><a data-type="indexterm" data-startref="xi_datasets212148" id="id549"/>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id550">
<h1>Exercise</h1>
<p>Using the <code>realnewslike</code> subset of C4, prepare a word frequency counter, counting the number of times each word appears in the dataset. To make it simple, define a word as a contiguous sequence of characters separated by white space. Remove frequent function words (called stop words in NLP) like “the,” “is,” etc. from your analysis. What topics seem to be underrepresented or overrepresented?</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Synthetic Pre-Training Data"><div class="sect1" id="id23">
<h1>Synthetic Pre-Training Data</h1>

<p>An emerging trend is the use of LLMs<a data-type="indexterm" data-primary="pre-training of data" data-secondary="synthetic" id="id551"/><a data-type="indexterm" data-primary="synthetic pre-training of data" id="id552"/> to generate synthetic data that can be used for pre-training LLMs. One of the first success stories in training LLMs on datasets with a significant proportion of synthetic data is Microsoft’s<a data-type="indexterm" data-primary="Microsoft phi models" id="id553"/> <a href="https://oreil.ly/eFphR">phi series of models</a>. For the phi-1.5 model, Microsoft created 20 billion tokens of synthetic data, using 20,000 seed topics and samples from real-world web datasets in their prompts.</p>

<p>Hugging Face<a data-type="indexterm" data-primary="Hugging Face" data-secondary="Cosmopedia" id="id554"/><a data-type="indexterm" data-primary="Cosmopedia" id="id555"/> released <a href="https://oreil.ly/Pdwnw">Cosmopedia</a>, an open source synthetic dataset used to train the SmolLM series of models. Its seed data included curated sources like Stanford courses, Khan Academy, and WikiHow, as well as general web data.</p>

<p>For curated sources, synthetic data was generated by extracting outlines of courses from Khan Academy and other sources and prompting the Mistral LLM to generate lengthy, detailed textbooks for individual sections.
To generate diverse data at scale, Hugging Face issues several variants of the same prompt for each topic, like “create a textbook on this topic for young children” and “create a textbook on this topic for professionals.”</p>

<p class="pagebreak-before">For general web data, Hugging Face clustered a subset of the RefinedWeb dataset into over a hundred topics. The LLM was then prompted with web page snippets and asked to generate an extensive blog post within the context of the topic the web page fell under. The cluster visualization can be explored in <a href="https://oreil.ly/t8R-6">Nomic Atlas</a>.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id556">
<h1>Exercise</h1>
<p>Load <a href="https://oreil.ly/AVyg-">Cosmopedia-100K</a>, a subset of the Cosmopedia dataset, and explore the prompts as well as the resulting synthetic data. What does the quality of the synthetic data look like? Do you observe any factual or reasoning errors?</p>

<p>Additionally, try varying the prompts and see if you can generate more diverse data.</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Training Data Preprocessing"><div class="sect1" id="id24">
<h1>Training Data Preprocessing</h1>

<p>Once we have collected or procured data, we need to filter and clean the data by running it through a preprocessing<a data-type="indexterm" data-primary="pre-training of data" data-secondary="preprocessing" id="xi_pretrainingofdatapreprocessing2233116"/><a data-type="indexterm" data-primary="preprocessing training data" id="xi_preprocessingtrainingdata2233116"/> pipeline.  Data preprocessing is the most unglamorous and underappreciated part of the LLM training pipeline, yet perhaps the most important. Based on my experience, spending more effort and resources during this phase can lead to significant downstream performance gains. As we walk through the data processing pipeline, I hope you come to appreciate the complexity of language text and the difficulty in processing it. Note that since these datasets are enormous, any preprocessing step should also be very efficient (ideally linear time).</p>

<p><a data-type="xref" href="#data-collection">Figure 2-3</a> shows the typical preprocessing steps used to generate a pre-training dataset. The ordering of steps is not fixed, but there are dependencies between some of the steps.</p>

<figure><div id="data-collection" class="figure">
<img src="assets/dllm_0203.png" alt="Data preprocessing pipeline" width="600" height="595"/>
<h6><span class="label">Figure 2-3. </span>Data collection and preprocessing pipeline</h6>
</div></figure>

<p>Let’s go through these steps in detail.</p>








<section data-type="sect2" data-pdf-bookmark="Data Filtering and Cleaning"><div class="sect2" id="id25">
<h2>Data Filtering and Cleaning</h2>

<p>A majority of text extracted<a data-type="indexterm" data-primary="preprocessing training data" data-secondary="filtering and cleaning" id="xi_preprocessingtrainingdatafilteringandcleaning224529"/><a data-type="indexterm" data-primary="filtering and cleaning data" id="xi_filteringandcleaningdata224529"/><a data-type="indexterm" data-primary="data cleaning and filtering" id="xi_datacleaningandfiltering224529"/><a data-type="indexterm" data-primary="cleaning and filtering data" id="xi_cleaningandfilteringdata224529"/> from HTML files is gibberish, like menu text from websites, boilerplate text, and random web page artifacts. There is a significant amount of pornography and toxic/hateful language on the web as well.
For example, here is a text sample from an uncleaned version of the C4 dataset:</p>
<blockquote>Skip to Main Content Skip to Footer Skip to Email Signup Skip to Feedback Form MY REWARDS SIGN OUT SIGN IN &amp; EARN REWARDS 0 Keyboard Controls Welcome to the main navigation. This menu has three levels of product categories. Use and keys to navigate between each category in the current level. Use the key to navigate down a level. Use the key to navigate up a level. Hit the key to be taken to the selected category page. Men What’s Hot New Arrivals Brand That Unites Performance Shop Online Exclusives Express Essentials Vacation Getaway Wedding Tuxedos Military Trend 9 Pieces / 33 Looks The Edit x Express NBA Collection Express + NBA Fashion NBA Game Changers Suiting &amp; Blazers Find</blockquote>

<p>How useful do you think this text is for language and task learning?</p>

<p>Data from Common Crawl<a data-type="indexterm" data-primary="Common Crawl" id="id557"/> is made available via both raw HTML and web-extracted text (WET)<a data-type="indexterm" data-primary="web-extracted text (WET) data format" id="id558"/> format. While many dataset creators directly use the WET files, the open source organization<a data-type="indexterm" data-primary="Eleuther AI" data-secondary="WET format limitations" id="id559"/> Eleuther AI <a href="https://oreil.ly/hciZS">noticed</a> that the quality of the WET files left much to be desired, with HTML boilerplate still prominent as seen above. To create The Pile, Eleuther AI thus used the <a href="https://oreil.ly/YRFzZ">jusText library</a> to<a data-type="indexterm" data-primary="jusText library" id="id560"/> more reliably remove boilerplate text from HTML documents.</p>

<p>Let’s explore the effect of using jusText with an example. In your Google Colab or Jupyter notebook, try this:</p>

<pre data-type="programlisting" data-code-language="python"><code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">justext</code>

<code class="kn">import</code> <code class="nn">requests</code>
<code class="kn">import</code> <code class="nn">justext</code>

<code class="n">response</code> <code class="o">=</code>
  <code class="n">requests</code><code class="o">.</code><code class="n">get</code><code class="p">(</code><code class="s2">"https://en.wikipedia.org/wiki/Toronto_Transit_Commission"</code><code class="p">)</code>
<code class="n">text</code> <code class="o">=</code> <code class="n">justext</code><code class="o">.</code><code class="n">justext</code><code class="p">(</code><code class="n">response</code><code class="o">.</code><code class="n">content</code><code class="p">,</code> <code class="n">justext</code><code class="o">.</code><code class="n">get_stoplist</code><code class="p">(</code><code class="s2">"English"</code><code class="p">))</code>
<code class="k">for</code> <code class="n">content</code> <code class="ow">in</code> <code class="n">text</code><code class="p">:</code>
  <code class="k">if</code> <code class="n">content</code><code class="o">.</code><code class="n">is_boilerplate</code><code class="p">:</code>
    <code class="nb">print</code><code class="p">(</code><code class="n">content</code><code class="o">.</code><code class="n">text</code><code class="p">)</code></pre>

<p>The output displays all the boilerplate that is filtered out from a standard Wikipedia article:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">Jump</code> <code class="n">to</code> <code class="n">content</code>
<code class="n">Main</code> <code class="n">menu</code>
<code class="n">Main</code> <code class="n">menu</code>
<code class="n">Navigation</code>
<code class="n">Main</code> <code class="n">page</code>
<code class="n">Contents</code>
<code class="n">Current</code> <code class="n">events</code>
<code class="n">Random</code> <code class="n">article</code>
<code class="n">About</code> <code class="n">Wikipedia</code>
<code class="n">Contact</code> <code class="n">us</code>
<code class="n">Donate</code>
<code class="n">Contribute</code>
<code class="n">Help</code>
<code class="n">Learn</code> <code class="n">to</code> <code class="n">edit</code>
<code class="err">…</code></pre>

<p>jusText just so happens to be more aggressive in removing content, but this is generally OK for cleaning pre-trained datasets since there is an abundance of text available. Some alternative libraries used for this task include <a href="https://oreil.ly/URvsq">Dragnet</a>, <a href="https://oreil.ly/xk7Hc">html2text</a>, <a href="https://oreil.ly/6-2z1">inscriptis</a>, <a href="https://oreil.ly/LPXe1">Newspaper</a>, and <a href="https://oreil.ly/zdZxj">Trafilatura</a>. According to the 
<span class="keep-together">creators</span> of <a href="https://oreil.ly/DZG7w">The Pile</a>, dividing the extraction pipeline across multiple libraries can reduce the risk of the resulting dataset being affected by any bias introduced by one of these libraries.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id561">
<h1>Pre-Training on Raw HTML Documents</h1>
<p>Do we really need to filter out HTML tags<a data-type="indexterm" data-primary="HTML documents, tag filtering or not" id="id562"/> from raw HTML documents before pre-training? What if we pre-trained on raw HTML documents instead? This outlandish yet creative idea was implemented by <a href="https://oreil.ly/VZxiC">Aghajanyan et al.</a> in their hyper-text language model (HTLM). The structured format of HTML enables valuable metadata to be encoded with text. For example, the &lt;title&gt; tags could represent the summary, and the &lt;class&gt; tags could provide category information about the text.</p>

<p>Not all of the HTML is useful for pre-training. For example, CSS isn’t very informative for language learning. Therefore, the creators of HTLM convert the raw HTML into a simplified form, by filtering out iframes, headers, footers, forms, etc. This process<a data-type="indexterm" data-primary="minification" id="id563"/> is called <em>minification.</em></p>

<p>The results presented in their paper show the model is especially good at summarization, because the access to the category tags helps it focus on the salient aspects of the topic under discussion. However, as of this book’s writing, this pre-training paradigm hasn’t caught on yet.</p>
</div></aside>

<p>Boilerplate removal in web pages is a challenging task. Web pages may also contain code blocks, tables, and math formulas, which need careful processing. <a href="https://oreil.ly/bXELJ">Meta</a> noted that it built a custom HTML parser for preparing the dataset to train Llama 3. It also mentioned that Meta retains the <em>alt</em> attribute in images, which it found contains useful information like math content.</p>

<p>LLMs can also be utilized for accurate content extraction from web pages. However, as of this book’s writing, it is prohibitively expensive to do so, given the scale of the dataset.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id564">
<h1>Exercise</h1>
<p>Use your favorite news website and open a news article. Use any of the text extraction libraries mentioned to remove web boilerplate. Is the output desirable on your first try? What kind of additional heuristics might you need?</p>
</div></aside>

<p>Once text is extracted, the documents are passed through a series of data filtering steps. First, rudimentary filtering steps based on heuristics are applied. While the details differ across datasets, here are some of the steps typically performed:</p>
<dl>
<dt>Boilerplate removal</dt>
<dd>
<p>Only lines that end with punctuation<a data-type="indexterm" data-primary="boilerplate removal" id="id565"/>, like the period, exclamation point, and question
mark are retained. This ensures that menu text from websites is removed. Only
lines with greater than a particular threshold of words and documents with
greater than a particular threshold of sentences are retained. The latter helps in
modeling long sequences, which is an important capability for language models to
have. Documents containing “lorem ipsum…” and other boilerplate text are filtered
out.</p>
</dd>
<dt>Non-English text removal</dt>
<dd>
<p>Libraries like langdetect, langid, fasttext, and pycld2 are used to detect the language<a data-type="indexterm" data-primary="non-English text removal" id="id566"/> of the text. For example, C4 retains text that has 
<span class="keep-together">&gt; 0.99</span> probability of English as judged by langdetect. Note that these libraries can also be used to remove boilerplate and web page artifacts since they give a lower probability of English to those texts.</p>
</dd>
<dt>Search engine optimization (SEO) text/spam removal</dt>
<dd>
<p>Documents<a data-type="indexterm" data-primary="search engine optimization (SEO) text/spam removal" id="id567"/><a data-type="indexterm" data-primary="SEO (search engine optimization) text/spam removal" id="id568"/> with a lot of repeated character sequences are removed.
Documents with a low proportion of closed class words are removed. Closed class words in English are function words like “of,” “at,” “the,” and “is.” If a page is engaged in keyword stuffing and other SEO tricks, then they would have a lower closed class words ratio.</p>
</dd>
<dt>Pornographic/abusive text removal</dt>
<dd>
<p>Documents<a data-type="indexterm" data-primary="pornographic/abusive text removal" id="id569"/> containing any words from keyword lists like the <a href="https://oreil.ly/w3u_r">“List of Dirty, Naughty, Obscene or Otherwise Bad Words”</a> are removed.</p>
</dd>
</dl>

<p>Tools like langdetect and langid are helpful for speedy determination of  the language<a data-type="indexterm" data-primary="langdetect tool" id="id570"/><a data-type="indexterm" data-primary="langid tool" id="id571"/> in which the text is written at scale, but how do they deal with code-switched text (text in multiple languages, where English is often interspersed with a local 
<span class="keep-together">language</span>)?</p>

<p>You can try it! Here is an example for Taglish (Tagalog + English, which is a common mode of communication in the Philippines). In your notebook, run the 
<span class="keep-together">following</span>:</p>

<pre data-type="programlisting" data-code-language="python"><code class="err">!</code><code class="n">pip</code> <code class="n">install</code> <code class="n">langdetect</code>
<code class="kn">from</code> <code class="nn">langdetect</code> <code class="kn">import</code> <code class="n">detect_langs</code><code class="p">()</code>
<code class="n">detect_langs</code><code class="p">(</code><code class="s2">"""Pag-uwi ko galing sa paaralan, sobrang pagod ako dahil sa dami</code>
<code class="s2">ng aking ginawa sa buong araw. Ang traffic din sa kalsada, nakaka-stress</code>
<code class="s2">talaga! Pero nang makarating ako sa aking tahanan, nabuhayan ako ng loob dahil</code>
<code class="s2">sa masarap na amoy ng ulam na inihanda ni nanay. Excited na akong kumain</code>
<code class="s2">kasama ang aking pamilya at i-share ang mga kwento ko tungkol sa aking mga</code>
<code class="s2">kaibigan, guro, at mga natutunan ko sa school. After dinner, magre-relax muna</code>
<code class="s2">ako habang nanonood ng TV, and then magre-review ng lessons bago matulog. Ito</code>
<code class="s2">ang routine ko pag-uwi mula sa school, at masaya ako na dumating sa bahay namay</code>
<code class="s2">naghihintay na pamilya na handang makinig at suportahan ako sa aking</code>
<code class="s2">pag-aaral."""</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="n">tl</code><code class="p">:</code><code class="mf">0.9999984631271781</code><code class="p">]</code></pre>

<pre data-type="programlisting" data-code-language="python"><code class="n">detect_langs</code><code class="p">(</code><code class="s2">"""After a long day at school, pagod na pagod talaga ako. The</code>
<code class="s2">traffic on the way home didn't help, nakakastress na nga! But upon arriving</code>
<code class="s2">home, I felt a sense of relief dahil sa welcoming atmosphere and the delicious</code>
<code class="s2">aroma of the ulam na inihanda ni Mommy. Excited na akong mag-share ng</code>
<code class="s2">experiences ko today with my family during dinner, kasama ang mga kwento about</code>
<code class="s2">my friends, teachers, and interesting lessons sa school. After eating, it's</code>
<code class="s2">time for me to chill while watching some TV shows, and then review my lessons</code>
<code class="s2">bago ako matulog. This is my daily routine pag-uwi galing school, and I am</code>
<code class="s2">grateful na may loving family ako na handang makinig at supportahan ako sa</code>
<code class="s2">aking educational journey."""</code><code class="p">)</code></pre>

<p>Output:</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="n">en</code><code class="p">:</code><code class="mf">0.9999954357601804</code><code class="p">]</code></pre>

<p>The second paragraph would get included in the C4 dataset, as per its filtering criteria (probability of English should be greater than .99). Therefore, even datasets that claim to be English-only routinely contain text in other languages, leading to surprising multilingual behavior during inference. Ever wondered why some monolingual models seem to perform well at machine translation? This is a major reason.</p>

<p>The way langdetect is implemented makes it poor at identifying language when short sequences are provided<a data-type="indexterm" data-startref="xi_preprocessingtrainingdatafilteringandcleaning224529" id="id572"/><a data-type="indexterm" data-startref="xi_filteringandcleaningdata224529" id="id573"/><a data-type="indexterm" data-startref="xi_datacleaningandfiltering224529" id="id574"/><a data-type="indexterm" data-startref="xi_cleaningandfilteringdata224529" id="id575"/>. For example:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">detect_langs</code><code class="p">(</code><code class="s1">'I love you too.'</code><code class="p">)</code></pre>

<p>returns</p>

<pre data-type="programlisting" data-code-language="python"><code class="p">[</code><code class="n">sk</code><code class="p">:</code><code class="mf">0.8571379760844766</code><code class="p">,</code> <code class="n">en</code><code class="p">:</code><code class="mf">0.14285726700161824</code><code class="p">]</code></pre>

<p>sk refers to Slovak here.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id576">
<h1>Exercise</h1>
<p>C4 is an English language dataset, constructed by filtering out text from the raw dataset with less than 0.99 probability of being English according to  langdetect. However, a lot of non-English data persists in this dataset. If you know a second language, then use the <code>realnewslike</code> subset of C4 to find instances in which text from that language appears. In what contexts do these non-English text fragments appear? Could an LLM <em>learn</em> these languages using these leftover fragments?</p>
</div></aside>
</div></section>








<section data-type="sect2" class="less_space pagebreak-before" data-pdf-bookmark="Selecting Quality Documents"><div class="sect2" id="id26">
<h2>Selecting Quality Documents</h2>

<p>Not all data is created equal<a data-type="indexterm" data-primary="preprocessing training data" data-secondary="quality document selection" id="xi_preprocessingtrainingdataqualitydocumentselection241930"/><a data-type="indexterm" data-primary="quality document selection" id="xi_qualitydocumentselection241930"/><a data-type="indexterm" data-primary="document selection for quality" id="xi_documentselectionforquality241930"/>. Text from a high school physics textbook is considered higher quality compared to promotional text about a footwear brand. There are several ways we can operationalize the notion of quality and separate high-quality from low-quality data. In this section we will highlight a few such ways.</p>










<section data-type="sect3" data-pdf-bookmark="Token-distribution K-L divergence"><div class="sect3" id="id27">
<h3>Token-distribution K-L divergence</h3>

<p>In this method, documents with a token distribution<a data-type="indexterm" data-primary="token-distribution K-L divergence" id="id577"/><a data-type="indexterm" data-primary="Kullback-Liebler (K-L) divergence" id="id578"/> that deviates too much from a reference token distribution are removed. In effect, this removes documents that have a lot of outlier tokens. This is calculated by using the <a href="https://oreil.ly/gd5GH">Kullback-Liebler (K-L) divergence</a>.</p>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Classifier-based approaches"><div class="sect3" id="id28">
<h3>Classifier-based approaches</h3>

<p>We can also build a classifier<a data-type="indexterm" data-primary="classifier for identifying high-quality data" id="id579"/><a data-type="indexterm" data-primary="data quality, managing" id="id580"/><a data-type="indexterm" data-primary="quality of data, managing" id="id581"/> for identifying high-quality data. A simple way to build a quality-based classifier is to have examples for the positive class come from high-quality data sources like Wikipedia, and examples for the negative class to be drawn from random documents in the Common Crawl data.</p>

<p>Meta employed a variety of classifier models for high-quality data extraction for its<a data-type="indexterm" data-primary="Llama 3 LLM" id="id582"/> <a href="https://oreil.ly/O-CKF">Llama 3 model</a>. One of them was a <a href="https://oreil.ly/EWic6">fasttext classification model</a> trained<a data-type="indexterm" data-primary="fasttext classification model" id="id583"/> to identify if a text is likely to be referenced by Wikipedia. Meta also trained a classifier whose training data was generated by Llama 2 by providing it with cleaned web documents and quality requirements and asking it to determine if the quality requirements are met. To extract code and text containing reasoning steps, Meta built classifiers that can identify them.</p>

<p><a data-type="xref" href="#classifier-filtering">Figure 2-4</a> shows how a classifier can be built to discriminate between high-quality and low-quality data.</p>

<figure><div id="classifier-filtering" class="figure">
<img src="assets/dllm_0204.png" alt="classifier-filtering" width="600" height="436"/>
<h6><span class="label">Figure 2-4. </span>Classifier-based quality filtering</h6>
</div></figure>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id584">
<h1>Exercise</h1>
<p>Create a quality classifier using fasttext. Your positive examples can be drawn from Wikipedia, and the negative examples can be randomly drawn from the <a href="https://oreil.ly/EpCy7">unclean version of C4</a>. Once trained, feed documents from the <code>realnewslike</code> subset of C4 to this classifier. Is this classifier able to do a good job?</p>
</div></aside>
</div></section>










<section data-type="sect3" data-pdf-bookmark="Perplexity for quality selection"><div class="sect3" id="id29">
<h3>Perplexity for quality selection</h3>

<p><a href="https://oreil.ly/OfycZ">Perplexity</a>, an intrinsic evaluation measure<a data-type="indexterm" data-primary="perplexity metric" id="xi_perplexitymetric2443122"/> for language models, has been used for document filtering in the context of preparing pre-training datasets, notably by the creators<a data-type="indexterm" data-primary="CCNet" id="id585"/> of <a href="https://oreil.ly/VF98y">CCNet</a>. Perplexity measures how well a model can predict a given text; the lower the perplexity, the better the model.</p>

<p>Just like the classifier approach, we select documents from data sources that we deem high quality (like Wikipedia) as the positive class. We then train a 5-gram language model<a data-type="indexterm" data-primary="KenLM" id="id586"/> using <a href="https://oreil.ly/EU5r3">KenLM</a> (a library facilitating training of n-gram language models) over it. Next, we take the dataset we want to filter and calculate the perplexity of each paragraph in it over the trained language model. The lower the perplexity, the more similar it is to the positive class. We can then discard documents with high perplexity.</p>

<p>Low perplexity may not always be a good thing, however. Short, repetitive text can have low perplexity. Note that writing style gets factored into perplexity. If the reference language model is trained over Wikipedia, then documents written in an informal style may receive higher perplexity scores. Therefore, it would be beneficial to have a more involved filtering strategy.</p>

<p>To resolve this, the creators<a data-type="indexterm" data-primary="BERTIN" id="id587"/> of <a href="https://oreil.ly/uI9eV">BERTIN</a> introduced the concept of perplexity sampling. In perplexity sampling, instead of just filtering out low-perplexity text, it uses a sampling strategy that oversamples from the middle part of the perplexity probability distribution.</p>

<p><a data-type="xref" href="#perplexity-sampling">Figure 2-5</a> shows how perplexity sampling is achieved in practice.</p>

<figure><div id="perplexity-sampling" class="figure">
<img src="assets/dllm_0205.png" alt="perplexity-sampling" width="600" height="413"/>
<h6><span class="label">Figure 2-5. </span>Perplexity sampling</h6>
</div></figure>

<p>Let’s explore the perplexity scores assigned by a model trained on Wikipedia text. Download this <a href="https://oreil.ly/xwYjY">file</a>.
After placing the file in your home directory, run this code in a new file:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">model</code> <code class="kn">import</code> <code class="n">KenlmModel</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">KenlmModel</code><code class="o">.</code><code class="n">from_pretrained</code><code class="p">(</code><code class="s2">"wikipedia"</code><code class="p">,</code> <code class="s2">"en"</code><code class="p">)</code>
<code class="n">model</code><code class="o">.</code><code class="n">get_perplexity</code><code class="p">(</code><code class="s2">"She was a shriveling bumblebee, and he was a bumbling</code><code class="w"/>
<code class="n">banshee</code><code class="p">,</code> <code class="n">but</code> <code class="n">they</code> <code class="n">accepted</code> <code class="n">a</code> <code class="n">position</code> <code class="n">at</code> <code class="n">Gringotts</code> <code class="n">because</code> <code class="n">of</code> <code class="n">their</code> <code class="n">love</code> <code class="k">for</code>
<code class="n">maple</code> <code class="n">syrup</code><code class="s2">")</code><code class="w"/></pre>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id588">
<h1>Exercise</h1>
<p>Try out sentences and paragraphs in different styles and topics to see how the perplexity varies! In particular, get the perplexities of these types of text:</p>

<ul>
<li>
<p>Social media text, like X (formerly Twitter)</p>
</li>
<li>
<p>SEO spam</p>
</li>
<li>
<p>Text with a lot of slang</p>
</li>
</ul>

<p>Additionally, you can train a KenLM model on your own dataset. Use a portion of the <code>realnewslike</code> subset of C4 and train the model using the instructions provided in the <a href="https://github.com/kpu/kenlm">KenLM GitHub page</a>. You can then calculate the perplexity of each document in a subset of the unclean version of C4. Which documents have the highest perplexity? Which documents have the lowest perplexity? After manually inspecting the results, do you think perplexity sampling is a good measure of quality?</p>
</div></aside>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>According to an <a href="https://oreil.ly/Nzla7">analysis of C4</a>, the internet domain that contributed the largest proportion of text in the dataset was patents.google.com. Over 10% of the text from this domain is in fact machine translated, with patents from countries like Japan being translated from Japanese to English. So a significant amount of pre-training data is already not generated by humans!</p>

<p>Propelled by LLMs, the internet is slated to see widespread prevalence of AI-generated text<a data-type="indexterm" data-primary="AI-generated text, prevalence of" id="id589"/><a data-type="indexterm" data-primary="generative AI" data-secondary="prevalence of text" id="id590"/>. Recognizing whether text was written by a human or an LLM is a nontrivial task and certainly not feasible at scale. How this will affect future LLM performance is an open research question.</p>
</div>

<p>Despite all the data cleaning steps, the resulting dataset is still not going to be perfect at this level of scale. For example, Eleuther AI <a href="https://oreil.ly/WEBne">reported</a> that the boilerplate sentence “select the forum that you want to visit from the selection below” occurs 180K times in The Pile<a data-type="indexterm" data-startref="xi_preprocessingtrainingdataqualitydocumentselection241930" id="id591"/><a data-type="indexterm" data-startref="xi_qualitydocumentselection241930" id="id592"/><a data-type="indexterm" data-startref="xi_perplexitymetric2443122" id="id593"/><a data-type="indexterm" data-startref="xi_documentselectionforquality241930" id="id594"/>.</p>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Deduplication"><div class="sect2" id="id30">
<h2>Deduplication</h2>

<p>So far we have discussed data extraction and cleaning<a data-type="indexterm" data-primary="preprocessing training data" data-secondary="deduplication" id="xi_preprocessingtrainingdatadeduplication249354"/><a data-type="indexterm" data-primary="deduplication" id="xi_deduplication249354"/>, language identification, and quality filtering. Let’s now explore the most contentious step in the pipeline: 
<span class="keep-together">deduplication</span>.</p>

<p>We know that web-crawled text is ridden with a lot of duplicates. Duplicates form a nontrivial portion of the training dataset, so any decision made about them will have a noticeable impact on the ensuing model.</p>

<p>How do we define a duplicate? We will make a distinction between three kinds:</p>
<dl>
<dt>Exact matches</dt>
<dd>
<p>Two sequences with the same text<a data-type="indexterm" data-primary="exact matches, deduplication" id="id595"/> are exact-match duplicates. They are the easiest to handle.</p>
</dd>
<dt>Approximate matches</dt>
<dd>
<p>In many cases, there are near-duplicates<a data-type="indexterm" data-primary="approximate matches, deduplication" id="id596"/>, where sequences of text are identical except for a few characters. Sometimes these sequences are slightly different only due to HTML text extraction artifacts and other filtering processes.</p>
</dd>
<dt>Semantic duplicates</dt>
<dd>
<p>Duplicates<a data-type="indexterm" data-primary="semantic duplicates" id="id597"/> that semantically convey the same content but using different wordings. This is usually treated as out of scope.</p>
</dd>
</dl>

<p>Duplicates can also be categorized based on the granularity at which they occur:</p>
<dl>
<dt>Document-level duplicates</dt>
<dd>
<p>Duplicate documents<a data-type="indexterm" data-primary="document-level duplicates" id="id598"/> are removed during the preparation of most pre-training datasets. However, in some datasets like The Pile, certain subsets (like Wikipedia) are deliberately duplicated,  so that they are seen more often by the model.</p>
</dd>
<dt>Sequence-level duplicates</dt>
<dd>
<p>These are lines or sentences<a data-type="indexterm" data-primary="sequence-level duplicates" id="id599"/> in documents that are repeated across multiple documents. In some cases they can be massively duplicated, like terms of service text, copyright notices, website prefaces, etc.</p>
</dd>
</dl>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Dededuplication is a very complex process, typically performed using the MinHash algorithm<a data-type="indexterm" data-primary="MinHash algorithm" id="id600"/>. This writeup by <a href="https://oreil.ly/2RO9f">Cheng Hao</a> details the deduplication process followed in the Big Science and Big Code open source LLM projects.</p>
</div>

<p>Deduplicating data has several benefits:</p>

<ul>
<li>
<p>A small subset of the pre-training dataset is usually set aside for validation/test. Deduplication can ensure the removal/reduction of overlap between the train and test sets, which is essential for an unbiased evaluation. Without sequence-level deduplication, there is a high likelihood of overlap of common text sequences in the train and test sets.</p>
</li>
<li>
<p>Removing duplicate sequences reduces the overall size of the training dataset. However, <a href="https://oreil.ly/k5OwJ">Lee et al.</a> show that the perplexity of a model trained on the smaller dataset isn’t affected. Thus, the model can be trained for a shorter period yet with the same benefit.</p>
</li>
<li>
<p>Deduplication can also reduce the tendency of the model to memorize its training data. Memorization<a data-type="indexterm" data-primary="memorization by generation" id="xi_memorizationbygeneration2527102"/><a data-type="indexterm" data-primary="generative AI" data-secondary="memorization by generation" id="xi_generativeAImemorizationbygeneration2527102"/> is closely linked to model overfitting and thwarts the model’s ability to generalize. While there are many ways to quantify memorization, we will focus on <em>memorization by generation</em>, where a model is said to have memorized a sequence if it is capable of generating it verbatim. <a href="https://oreil.ly/xpoz7">Lee et al.</a> have shown that models trained on datasets that have been deduplicated at the sequence level generate ten times less verbatim training data.</p>
</li>
</ul>
<div data-type="tip"><h6>Tip</h6>
<p>One advantage of using models trained on publicly available datasets is that you can search through the datasets to see if the text generated by the model exists verbatim in the dataset.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id601">
<h1>Security Vulnerabilities in LLMs Due to Memorization</h1>
<p>Memorization makes language models<a data-type="indexterm" data-primary="security" data-secondary="memorization vulnerability" id="id602"/> vulnerable to security and privacy attacks. Two demonstrated types of attacks are:</p>
<dl>
<dt>Membership inference attack</dt>
<dd>
<p>With just closed-box access to a model<a data-type="indexterm" data-primary="membership inference attack" id="id603"/>, a membership inference attack enables an attacker to determine if a sequence of text has been used to train the model or not.</p>
</dd>
<dt>Training data extraction attack</dt>
<dd>
<p>With just closed-box access to a model<a data-type="indexterm" data-primary="training data" data-secondary="extraction attack on" id="id604"/>, the attacker can prompt the model to generate memorized sensitive information. A naive example involves prompting the model with the text “Suhas Pai’s phone number is” and asking the model to provide the continuation, with the hope that it has memorized Suhas’s number.</p>
</dd>
</dl>

<p><a href="https://oreil.ly/iIic3">Carlini et al.</a> show that larger models memorize more easily and thus are most susceptible to these types of attacks. However, it is hard to estimate how much data is memorized by the model, as some memorized data is output by the model only when prompted with a long, delicately prepared prefix. This makes models harder to audit for privacy guarantees<a data-type="indexterm" data-startref="xi_preprocessingtrainingdatadeduplication249354" id="id605"/><a data-type="indexterm" data-startref="xi_deduplication249354" id="id606"/>.</p>
</div></aside>

<p><a data-type="xref" href="#privacy-attacks-against-llms">Figure 2-6</a> demonstrates the flow of a rudimentary training-data extraction attack.</p>

<figure><div id="privacy-attacks-against-llms" class="figure">
<img src="assets/dllm_0206.png" alt="Privacy attacks" width="600" height="112"/>
<h6><span class="label">Figure 2-6. </span>Privacy attacks against LLMs</h6>
</div></figure>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Removing Personally Identifiable Information"><div class="sect2" id="id31">
<h2>Removing Personally Identifiable Information</h2>

<p>While deduplication<a data-type="indexterm" data-primary="preprocessing training data" data-secondary="PII removal" id="xi_preprocessingtrainingdataPIIremoval255320"/><a data-type="indexterm" data-primary="personally identifiable information (PII)" id="xi_personallyidentifiableinformationPII255320"/><a data-type="indexterm" data-primary="security" data-secondary="removing PII" id="xi_securityremovingPII255320"/><a data-type="indexterm" data-primary="privacy and compliance management" id="xi_privacyandcompliancemanagement255320"/> can reduce the likelihood of the model memorizing training data, it is by no means a panacea for the memorization problem. Even information that appears only once in the training set could potentially be memorized (and leaked). While a lot of content in the training data is innocuous (terms of service text) and perhaps even desirable to memorize (factual information, like the capital of Canada), memorization of personally identifiable information (PII) is a major concern.</p>

<p>Let us see what PII entails. The formal definition from <a href="https://oreil.ly/kN3J8">Cornell Law</a> is as follows:</p>
<blockquote>
<p>Information that can be used to distinguish or trace an individual’s identity, either alone or when combined with other personal or identifying information that is linked or linkable to a specific individual.</p></blockquote>

<p>Based on this definition, non-PII can become PII when another piece of information becomes public, which when combined with the non-PII can be used to uniquely identify an individual.</p>

<p>The legal definition of PII varies by jurisdiction. For example, the <a href="https://oreil.ly/F2dGL">General Data Protection Regulation (GDPR)</a> in Europe says:</p>
<blockquote>
<p>Protection should be extended to anything used to directly or indirectly identify a person (or data subject). This may be extended to include characteristics that describe “physical, physiological, genetic, mental, commercial, cultural, or social identity of a person.”</p></blockquote>

<p>Most open source models are trained on publicly available datasets. These datasets might contain PII, but one might be tempted to say, “Well it is already out in the open, so there is no need for privacy protection.” This argument overlooks the importance of consent and discoverability controls. For instance, I might have shared my PII on my blog, which resides in an obscure corner of the internet and is not easily discoverable through search engines, but if it ends up being added to a pre-training dataset, it suddenly brings this data into the spotlight, without my consent. This concept<a data-type="indexterm" data-primary="contextual integrity" id="id607"/> is called <em>contextual integrity</em>: data should only be shared in the original context in which it was shared.</p>

<p>So ideally, we would like to <em>detect</em> PII in the dataset, and then <em>remediate</em> it in some fashion, so that the PII is no longer present in the training data or at least not memorizable. The presence<a data-type="indexterm" data-primary="public-figure PII" id="id608"/> of <em>public-figure PII</em> adds a layer of complexity to this problem. We would like our model to be able to accurately answer factual questions about public figures, such as providing their birth date. The privacy expectations for public figures are lower, showcasing how the values of transparency and openness clash with privacy. Determining who is a public figure and what level of privacy they are entitled to is a complex socio-technical challenge.</p>

<p>Data considered private includes names, addresses, credit card data, government IDs, medical history and diagnosis data, email IDs, phone numbers, identity and affinity groups the person belongs to (religion, race, union membership), geolocation data, and so on.</p>

<p>Attacks can be either targeted or untargeted<a data-type="indexterm" data-primary="targeted versus untargeted PII attacks" id="id609"/>. In an untargeted attack, the attacker just generates a large body of text using the model and then runs a membership inference attack to determine text within it that is most likely to be memorized. In a targeted attack, the attacker attempts to recover personal information about a particular individual or a group of individuals. Targeted attacks are more difficult to execute, because while language models are good at memorization, they are bad<a data-type="indexterm" data-primary="association, language model challenge with" id="id610"/> at <em>association</em>, for instance, identifying that an email ID belongs to a specific person.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id611">
<h1>Exercise</h1>
<p>Use the instructions in the <a href="https://oreil.ly/sHAKY">ReadMe to run this code</a> for analyzing privacy attacks on LLMs. It goes without saying, but please do not use this in the real world! Running the code and observing the outputs will give you an understanding of the limitations of this type of attack and the type of data that is typically memorized by an LLM.</p>

<p>Additionally, try out <a href="https://oreil.ly/yqZ1C">Google’s Training Data Extraction Challenge!</a></p>
</div></aside>

<p>Most pre-training datasets have undergone little to no PII remediation. The Privacy working group (of which I was the co-lead) of the Big Science project that trained the BLOOM model developed a pipeline for PII detection and remediation, which we will discuss next.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Language models are also susceptible to training data poisoning attacks<a data-type="indexterm" data-primary="training data" data-secondary="poisoning attacks" id="id612"/>. Since a large portion of training data is sourced from web-crawled text, bad actors have an opportunity to influence the content of the training set. <a href="https://oreil.ly/g_A-d">Tramer er al.</a> have shown that one can poison less than 0.1% of the training set with data whose effect is to make it easier for other data in the training set to leak more easily.</p>

<p>As LLMs increasingly get used as search engines, the demand for LLM SEO is cropping up. For example, a company could write content on their web sites in a manner that makes it more likely to be chosen in a pre-training dataset creation process that uses perplexity filtering.</p>
</div>

<p><a data-type="xref" href="#PII-processing-pipeline">Figure 2-7</a> shows a typical PII processing pipeline.</p>

<figure><div id="PII-processing-pipeline" class="figure">
<img src="assets/dllm_0207.png" alt="PII Processing Pipeline" width="600" height="477"/>
<h6><span class="label">Figure 2-7. </span>PII processing pipeline</h6>
</div></figure>










<section data-type="sect3" class="less_space pagebreak-before" data-pdf-bookmark="PII detection"><div class="sect3" id="id357">
<h3>PII detection</h3>

<p>The task of PII detection is similar to the NLP task of NER, introduced in <a data-type="xref" href="ch01.html#chapter_llm-introduction">Chapter 1</a>. However, not all named entities constitute PII.
For our task we determined the PII tags to be PERSON, AGE, NORP (nationality, race, religion, political party affiliation, socio-economic class, and union membership), STREET_ADDRESS, CREDIT_CARD, GOVT_ID, EMAIL_ADDRESS, USER_ID, and PUBLIC_FIGURE.</p>

<p>We used the PUBLIC_FIGURE tag to identify information about public figures, since we didn’t want to filter them out. We also assigned fictional characters this tag.</p>

<p>Some of the structured tags in this list like emails and government IDs can be identified using regular expressions. For other tags, we annotated datasets that could then be used to train Transformer-based NER-like models. Interestingly, we observed a very high degree of inter-annotator disagreement (same example being annotated differently by different people) that underscored the cultural nuances of the definition of privacy and what constitutes personal information.</p>

<p>Here is the <a href="https://oreil.ly/8YwG9">regular expression</a> to detect SSN (US Social Security numbers):</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">ssn_pattern</code> <code class="o">=</code> <code class="sa">r</code><code class="s2">"(?!000|666|333)0*(?:[0-6][0-9][0-9]|[0-7][0-6][0-9]|</code><code class="w"/>
<code class="p">[</code><code class="mi">0</code><code class="o">-</code><code class="mi">7</code><code class="p">][</code><code class="mi">0</code><code class="o">-</code><code class="mi">7</code><code class="p">][</code><code class="mi">0</code><code class="o">-</code><code class="mi">2</code><code class="p">])[</code><code class="o">-</code>\ <code class="p">](</code><code class="err">?!</code><code class="mi">00</code><code class="p">)[</code><code class="mi">0</code><code class="o">-</code><code class="mi">9</code><code class="p">]{</code><code class="mi">2</code><code class="p">}[</code><code class="o">-</code>\ <code class="p">](</code><code class="err">?!</code><code class="mi">0000</code><code class="p">)[</code><code class="mi">0</code><code class="o">-</code><code class="mi">9</code><code class="p">]{</code><code class="mi">4</code><code class="p">}</code><code class="s2">"</code><code class="w"/></pre>

<p>Note that detection is not the same as validation. Not all nine-digit numbers of the form XXX-XX-XXXX are SSNs!  Validation is the process of checking if a sequence of characters maps to a valid identifier. For example, the Canadian equivalent of SSN, the social insurance number (SIN) contains a checksum digit that can be used to validate it:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">stdnum.ca</code> <code class="kn">import</code> <code class="n">sin</code>
<code class="n">sin_pattern</code> <code class="o">=</code> <code class="n">re</code><code class="o">.</code><code class="n">compile</code><code class="p">(</code><code class="sa">r</code><code class="s2">"\d</code><code class="si">{3}</code><code class="s2">[-\ ]\d</code><code class="si">{3}</code><code class="s2">[-\ ]\d</code><code class="si">{3}</code><code class="s2">"</code><code class="p">,</code> <code class="n">flags</code><code class="o">=</code><code class="n">re</code><code class="o">.</code><code class="n">X</code><code class="p">)</code>
<code class="k">for</code> <code class="n">match</code> <code class="ow">in</code> <code class="n">sin_pattern</code><code class="o">.</code><code class="n">findall</code><code class="p">(</code><code class="n">text</code><code class="p">):</code>
    <code class="k">if</code> <code class="n">sin</code><code class="o">.</code><code class="n">is_valid</code><code class="p">(</code><code class="n">match</code><code class="p">):</code>
         <code class="nb">print</code><code class="p">(</code><code class="n">match</code><code class="p">)</code></pre>

<p>The <code>is_valid()</code> function uses the <a href="https://oreil.ly/i34BW">Luhn checksum algorithm</a> to validate if the sequence of digits maps to a valid SIN. The same algorithm is also used to validate credit cards. Here is the <a href="https://oreil.ly/6uTq-">regex</a> for detecting credit card numbers:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">stdnum</code> <code class="kn">import</code> <code class="n">luhn</code>
<code class="n">cc_base_pattern</code> <code class="o">=</code>  <code class="sa">r</code><code class="s2">"\b \d (?:\d[ -]?)</code><code class="si">{14}</code><code class="s2"> \d \b"</code>
<code class="n">cc_full_pattern</code> <code class="o">=</code> <code class="sa">r</code><code class="s2">"""4[0-9]</code><code class="si">{12}</code><code class="s2">(?:[0-9]</code><code class="si">{3}</code><code class="s2">)? |</code>
<code class="s2">            (?:5[1-5][0-9]</code><code class="si">{2}</code><code class="s2">|222[1-9]|22[3-9][0-9]|2[3-6][0-9]</code><code class="si">{2}</code><code class="s2">|27[01][0-9]|</code>
<code class="s2">            2720)[0-9]</code><code class="si">{12}</code><code class="s2"> |</code>
<code class="s2">            3[47][0-9]</code><code class="si">{13}</code><code class="s2"> |</code>
<code class="s2">            3(?:0[0-5]|[68][0-9])[0-9]</code><code class="si">{11}</code><code class="s2"> |</code>
<code class="s2">            6(?:011|5[0-9]</code><code class="si">{2}</code><code class="s2">)[0-9]</code><code class="si">{12}</code><code class="s2"> |</code>
<code class="s2">            (?:2131|1800|35\d</code><code class="si">{3}</code><code class="s2">)\d</code><code class="si">{11}</code><code class="s2">"""</code></pre>

<p>The regular expression for detecting email address is as follows:</p>

<pre data-type="programlisting" data-code-language="python"><code class="n">email_pattern</code> <code class="o">=</code> <code class="sa">r</code><code class="s2">"[\w\.=-]+ @ [\w\.-]+ \. [\w]{2,3}"</code></pre>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id613">
<h1>Exercise</h1>
<p>Use  the search function in the dataset viewer for the <a href="https://oreil.ly/jto4m">RefinedWeb</a>  pre-training dataset to assess presence of PII. For example, search for “gmail.com.” What do you find?</p>
</div></aside>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Removing structured PII data while keeping the number of false positives low is hard enough, but detecting and remediating unstructured data is even harder. Due to the complexity of this task and the uncertainty about its impact on the resulting model performance, we decided to not run the Transformer model–based PII pipeline over the ROOTS dataset for training the BLOOM model.</p>
</div>
</div></section>










<section data-type="sect3" data-pdf-bookmark="PII remediation"><div class="sect3" id="id32">
<h3>PII remediation</h3>

<p>Once PII has been detected, it can be remediated. <a data-type="xref" href="#PII-remediation-options">Figure 2-8</a> depicts one of the remediation schemes.</p>

<figure><div id="PII-remediation-options" class="figure">
<img src="assets/dllm_0208.png" alt="PII Remediation Options" width="600" height="38"/>
<h6><span class="label">Figure 2-8. </span>PII remediation options</h6>
</div></figure>

<p>Here is a nonexhaustive list of remediation options:</p>
<dl>
<dt>Replace with a special token</dt>
<dd>
<p>For example, a valid phone number can be replaced by the string <code>&lt;phone</code> 
<span class="keep-together"><code>number&gt;</code></span>.</p>
</dd>
<dt>Replace with a random token of the same entity type</dt>
<dd>
<p>For example, replace the name “Clarietta Richards” with “Natasha Bridges,” or any other name.</p>
</dd>
<dt>Replace with a shuffled token</dt>
<dd>
<p>Entities detected across the dataset can be shuffled.</p>
</dd>
<dt>Remove entire document/data source</dt>
<dd>
<p>If the amount of PII detected in a single document or data source is higher than a specific threshold, it is probably best to remove it. For example, <em>pastebin.com</em> is said to contain a lot of inadvertently placed PII and is recommended to be not included in training datasets.</p>
</dd>
</dl>

<p>Each of these techniques can have a varied effect on the model’s downstream performance. How does replacing tokens affect training perplexity? Are downstream tasks like NER negatively affected when tuned on the resulting model? How does 
<span class="keep-together">replacement</span> by special tokens compare to replacement with random tokens? This is a relatively underexplored topic, and all these questions are still open.</p>

<p><a href="https://oreil.ly/K4QI_">Faker</a> is an excellent library<a data-type="indexterm" data-primary="Faker library" id="id614"/> for facilitating random token replacement. It supports random token generation for a variety of PII types including names, addresses, credit card numbers, and phone numbers.
One danger in using random tokens is that the replacement process can alter the demographic distribution of the dataset, for example, if the replacement names were all or mostly Anglo-Saxon names. Faker has localization support to enable replacement with fake data from the same geography/culture. Let’s explore the library in more detail:</p>

<pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">faker</code> <code class="kn">import</code> <code class="n">Faker</code>
<code class="n">fake</code> <code class="o">=</code> <code class="n">Faker</code><code class="p">(</code><code class="s1">'en_IN'</code><code class="p">)</code>   <code class="c1"># Indian locale</code>
<code class="n">Faker</code><code class="o">.</code><code class="n">seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">5</code><code class="p">):</code>
   <code class="nb">print</code><code class="p">(</code><code class="n">fake</code><code class="o">.</code><code class="n">aadhaar_id</code><code class="p">)</code></pre>

<p>This code generates 12-digit fake Aadhaar IDs, which are the Indian equivalent of Social Security numbers. Note that the generated IDs are all invalid but still follow the same format. Similarly:</p>

<pre data-type="programlisting" data-code-language="python"><code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">5</code><code class="p">):</code>
   <code class="nb">print</code><code class="p">(</code><code class="n">fake</code><code class="o">.</code><code class="n">address</code><code class="p">)</code></pre>

<p>generates fake but representative addresses for the selected locale.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Removing PII from training datasets is only one of several solutions to prevent data leakage from models<a data-type="indexterm" data-startref="xi_memorizationbygeneration2527102" id="id615"/><a data-type="indexterm" data-startref="xi_generativeAImemorizationbygeneration2527102" id="id616"/>. One promising technique<a data-type="indexterm" data-primary="differential privacy" id="id617"/> is <a href="https://oreil.ly/TRbsf">differential privacy</a>, which introduces randomness in the inputs or outputs to provide theoretical guarantees for privacy preservation. In neural networks, differential privacy is implemented using the <a href="https://oreil.ly/DVQkl">DP-SGD</a> algorithm<a data-type="indexterm" data-primary="DP-SGD algorithm" id="id618"/>, which involves gradient clipping and noise addition at the end of each update. However, differential privacy significantly slows training, negatively affects model performance, and disproportionately impacts minority groups in the dataset in terms of model utility degradation<a data-type="indexterm" data-startref="xi_preprocessingtrainingdataPIIremoval255320" id="id619"/><a data-type="indexterm" data-startref="xi_personallyidentifiableinformationPII255320" id="id620"/><a data-type="indexterm" data-startref="xi_securityremovingPII255320" id="id621"/><a data-type="indexterm" data-startref="xi_privacyandcompliancemanagement255320" id="id622"/>. Apart from differential privacy, other methods include adversarial training, <a href="https://oreil.ly/_AV3V">model unlearning</a>, <a href="https://oreil.ly/5p0z3">retroactive censoring, and “memfree” decoding</a>.</p>
</div>
</div></section>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Training Set Decontamination"><div class="sect2" id="id33">
<h2>Training Set Decontamination</h2>

<p>Training set decontamination<a data-type="indexterm" data-primary="preprocessing training data" data-secondary="decontamination of training set" id="id623"/><a data-type="indexterm" data-primary="decontamination of training set" id="id624"/> is a crucial data preprocessing step that helps improve LLM evaluations. A pre-training dataset is said to be contaminated if it contains data from the benchmark test sets used to evaluate its performance. Contamination can happen if the test datasets were constructed from web text, or if the dataset was uploaded on the web after creation. There are two types of contamination:<sup><a data-type="noteref" id="id625-marker" href="ch02.html#id625">1</a></sup></p>
<dl>
<dt>Input and label contamination</dt>
<dd>
<p>In this setting, both the questions (inputs) and answers (target labels) exist in the pre-training dataset.</p>
</dd>
<dt>Input contamination</dt>
<dd>
<p>In this setting, only the inputs are present in the pre-training dataset but not the target labels. We will describe the effects of input contamination and how we can leverage it for positive use in <a data-type="xref" href="ch07.html#ch07">Chapter 7</a>.</p>
</dd>
</dl>

<p><a href="https://oreil.ly/d7pHK">OpenAI</a> addressed training set contamination<a data-type="indexterm" data-primary="OpenAI" data-secondary="training set decontamination" id="id626"/> in GPT-3 by finding 13-gram overlaps between text in the test/validation set and the train set, and removing 200 characters before and after the matched texts. The n-gram matching approach is the most commonly used method for decontamination.</p>

<p>However, <a href="https://oreil.ly/JjtHS">Yang et al.</a> note that contamination can also happen if a rephrased or translation of the benchmark data is present in the training dataset. This makes data contamination very challenging to detect and remove. Most benchmark results continue to be overstated due to this problem.</p>
</div></section>








<section data-type="sect2" data-pdf-bookmark="Data Mixtures"><div class="sect2" id="id34">
<h2>Data Mixtures</h2>

<p>Pre-training datasets<a data-type="indexterm" data-primary="preprocessing training data" data-secondary="data mixtures" id="xi_preprocessingtrainingdatadatamixtures273222"/><a data-type="indexterm" data-primary="data mixtures" id="xi_datamixtures273222"/> contain data from a wide variety of domains. The final dataset is prepared such that these domains are represented in optimal proportions. For example, Wikipedia, academic texts, and smaller subsets were <a href="https://oreil.ly/hpHdw">upsampled</a> by up to three times in The Pile dataset<a data-type="indexterm" data-primary="The Pile dataset" data-primary-sortas="Pile" id="id627"/><a data-type="indexterm" data-primary="Eleuther AI" data-secondary="The Pile dataset" data-secondary-sortas="Pile dataset" id="id628"/>. More involved techniques like <a href="https://oreil.ly/5z9u1">DoReMi</a> and <a href="https://oreil.ly/VWyzt">RegMix</a> are also used to calculate the right data mixture. Meta<a data-type="indexterm" data-primary="Llama 3 LLM" id="id629"/> noted that for <a href="https://oreil.ly/fMOrb">Llama 3</a>, it empirically arrived at a data mixture where 50% of the tokens are about general knowledge, 25% are about math and reasoning, 17% represent code, and the remaining are non-English tokens.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>Many pre-training datasets these days include code, even if the model is not intended for generating code. <a href="https://oreil.ly/Vm0lH">Aryabumi et al.</a> have shown that including code in pre-training data significantly improves performance on downstream tasks that do not involve generating code.</p>
</div>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id630">
<h1>Dataset Ordering</h1>
<p>After all data preprocessing stages<a data-type="indexterm" data-primary="datasets" data-seealso="pre-training of data" id="id631"/><a data-type="indexterm" data-primary="datasets" data-secondary="ordering of" id="id632"/> have been completed, the training process can commence. The order in which the data is fed to the model does matter. The area of study to determine the optimal order is called curriculum learning<a data-type="indexterm" data-primary="curriculum learning" id="id633"/>. To our knowledge, most models do not go beyond some simple ordering heuristics.</p>

<p>One technique is to start the training with shorter training sequences and then gradually increase the sequence lengths. This can be done either by truncating initial sequences to fit a certain length or by simply reordering the dataset so that shorter sequences are ordered first.</p>

<p><a href="https://oreil.ly/QYlMI">Researchers</a> have also experimented with introducing more common words to the model first, by replacing rarer words occurring in early training examples with their part-of-speech tag or with hypernyms (for example, the hypernym of magenta is color).</p>
</div></aside>

<p>Now that we have discussed all the important data collection and preprocessing steps for preparing a pre-training dataset, let’s see how individual datasets differ in terms of the preprocessing steps they have undergone<a data-type="indexterm" data-startref="xi_pretrainingofdatapreprocessing2233116" id="id634"/><a data-type="indexterm" data-startref="xi_preprocessingtrainingdata2233116" id="id635"/>.</p>
<div data-type="tip"><h6>Tip</h6>
<p><a href="https://oreil.ly/lDFm2">DataTrove</a> by Hugging Face<a data-type="indexterm" data-primary="Hugging Face" data-secondary="DataTrove" id="id636"/><a data-type="indexterm" data-primary="DataTrove" id="id637"/> is a full-fledged pre-training dataset preprocessing pipeline code repository. You can go through the repo to understand how the concepts introduced in the chapter are implemented at scale<a data-type="indexterm" data-startref="xi_preprocessingtrainingdatadatamixtures273222" id="id638"/><a data-type="indexterm" data-startref="xi_datamixtures273222" id="id639"/><a data-type="indexterm" data-primary="datasets" data-secondary="preprocessing pipeline for" id="id640"/>.</p>
</div>

<p><a data-type="xref" href="#table2-pretraining-datasets">Table 2-2</a> provides a list of the popular pre-training datasets and the kind of preprocessing they went through.</p>
<table id="table2-pretraining-datasets">
<caption><span class="label">Table 2-2. </span>Pretraining datasets and their preprocessing pipeline</caption>
<thead>
<tr>
<th>Name</th>
<th>Extraction and cleaning</th>
<th>Quality filtering</th>
<th>Deduplication</th>
<th>Language identification</th>
<th>Models trained with this dataset</th>
</tr>
</thead>
<tbody>
<tr>
<td><p>C4</p></td>
<td><p>Remove pages containing word in blocklist, remove code, remove short lines and pages</p></td>
<td><p>-</p></td>
<td><p>Deduplication of 3-sentence spans</p></td>
<td><p>langdetect</p></td>
<td><p>T5, FLAN-T5, UL2, Llama</p></td>
</tr>
<tr>
<td><p>The Pile</p></td>
<td><p>justext library for text extraction</p></td>
<td><p>fasttext classifier</p></td>
<td><p>Document level, with MinHashLSH</p></td>
<td><p>pycld2</p></td>
<td><p>GPT-NeoX, GPT-J, Cerebras-GPT, StableLM, Pythia</p></td>
</tr>
<tr>
<td><p>CCNet</p></td>
<td><p>-</p></td>
<td><p>Perplexity filtering</p></td>
<td><p>Paragraph-level deduplication</p></td>
<td><p>fasttext</p></td>
<td/>
</tr>
<tr>
<td><p>RedPajama</p></td>
<td><p>CCNet pipeline</p></td>
<td><p>Classifier distinguishing between Wikipedia text and random C4 text</p></td>
<td><p>Paragraph-level deduplication (for Common Crawl)</p></td>
<td><p>fasttext</p></td>
<td><p>Red Pajama-INCITE, MPT</p></td>
</tr>
<tr>
<td><p>CleanPajama</p></td>
<td><p>Low-length filter, NFC normalization</p></td>
<td><p>-</p></td>
<td><p>MinHashLSH</p></td>
<td><p>-</p></td>
<td><p>-</p></td>
</tr>
<tr>
<td><p>RefinedWeb</p></td>
<td><p>URL filtering by blocklists, trafilatura library for text extraction, repetitive content removal</p></td>
<td><p>-</p></td>
<td><p>Fuzzy document-level deduplication with MinHash, exact sequence-level deduplication</p></td>
<td><p>fasttext</p></td>
<td><p>Falcon</p></td>
</tr>
<tr>
<td><p>ROOTS</p></td>
<td><p>Removal of documents with low ratio of closed class words, high ratio of blocklist words, high ratio of character/word repetition</p></td>
<td><p>Perplexity filtering</p></td>
<td><p>SimHash, Suffix Array</p></td>
<td><p>fasttext</p></td>
<td><p>BLOOM</p></td>
</tr>
</tbody>
</table>
</div></section>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Effect of Pre-Training Data on Downstream Tasks"><div class="sect1" id="id35">
<h1>Effect of Pre-Training Data on Downstream Tasks</h1>

<p>Given a pre-training dataset<a data-type="indexterm" data-primary="pre-training of data" data-secondary="effect on downstream tasks" id="id641"/><a data-type="indexterm" data-primary="downstream tasks, pre-training data effects" id="id642"/> for an LLM, what assumptions can we make from it about downstream performance? It turns out that there is a correlation between the model’s performance on a given task or input and the pre-training dataset frequency of the task or the salient words in the input, respectively. First observed by <a href="https://oreil.ly/cPYej">Razeghi et al.</a>, this phenomenon has been studied in detail in McCoy et al.’s <a href="https://oreil.ly/_O2NK">“Embers of Autoregression” paper</a>.</p>

<p>McCoy et al. show that language models perform better at tasks that are more frequently represented in the training dataset than ones that are less frequently represented. For example, language models are better at base 10 addition than base 9 addition. They are also better at sorting by alphabetical order than they are at sorting by reverse alphabetical order.</p>

<p>Similarly, McCoy et al. also show that for a given task, models perform relatively better when the output is text with high frequency in the pre-training dataset as opposed to when the text is lower frequency. This phenomenon is also observed for inputs; models do relatively better with higher-frequency inputs compared to lower-frequency inputs.</p>

<p>As an example, consider the sentence: “record a be that miles, yes, hour, per fifty clocked he.” We ask the LLM to reverse the words in the sentence, which would lead to “He clocked fifty per hour, yes, miles, that be a record,” a rather low-probability sequence, due to its odd linguistic construction.</p>

<p>As of the book’s writing, GPT-4o returns the wrong answer: “He clocked fifty miles per hour that be a record,” but you can notice that it performs relatively better when the output sequence is higher probability.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="id643">
<h1>Exercise</h1>
<p>We know that Wikipedia is used to train just about every LLM. Ask your favorite LLM (that doesn’t have access to the internet) a fact present in obscure Wikipedia pages. Is it able to answer correctly? Similarly, ask the LLM about facts present in more popular Wikipedia pages. Do you notice a difference?</p>
</div></aside>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Bias and Fairness Issues in Pre-Training Datasets"><div class="sect1" id="id36">
<h1>Bias and Fairness Issues in Pre-Training Datasets</h1>

<p>A multitude of ethical questions<a data-type="indexterm" data-primary="pre-training of data" data-secondary="bias and fairness issues" id="xi_pretrainingofdatabiasandfairnessissues279533"/><a data-type="indexterm" data-primary="bias and fairness issues" data-secondary="in pre-training datasets" data-secondary-sortas="pre-training datasets" id="xi_biasandfairnessissuesinpretrainingdatasets279533"/><a data-type="indexterm" data-primary="ethical questions, pre-training of data" id="xi_ethicalquestionspretrainingofdata279533"/><a data-type="indexterm" data-primary="datasets" data-secondary="bias and fairness issues" id="xi_datasetsbiasandfairnessissues279533"/> arise during the productization of large language models. The existence of significant bias and fairness issues in these models often leads to a no-ship condition for a large number of use cases. In this section we will go through some bias and fairness issues specifically related to the collection and filtering of pre-training data.</p>

<p>The scale of data that LLMs are fed with means that they are not just constructing models of language but also of the world we inhabit. This gives rise to the question of whether we want to model the world the way it is or the way we would like it to be. The internet is filled with hate, violence, and abusive language and is often used as an outlet for humanity’s worst impulses. The text in it implicitly encodes long-existing biases against groups of people. For example, in The Pile, an <a href="https://oreil.ly/hu3-b">analysis</a> of word co-occurrence statistics shows the word “radical” co-occurs with the word “Muslim” substantially more than it does for other religions.</p>

<p>The phenomenon<a data-type="indexterm" data-primary="bias and fairness issues" data-secondary="amplification of bias" id="id644"/> of <em>bias amplification</em> makes these problems all the more critical. It has been shown that large language models <a href="https://oreil.ly/x-ba9">amplify the biases</a> that are encoded in their pre-training data: they make biased predictions against groups of people at higher rates than what the training data statistics would suggest.</p>

<p>So, can we “fix” our training data such that we can model a world that encodes our values and principles that downstream applications will inherit? There is substantial debate in the research community about this. Opponents argue it is hard to identify and fix all societal  biases encoded in the data since there are so many dimensions of bias that intersect in complex ways. Values are not universal, and model providers would like to be value-neutral to cater to all sections of society.</p>

<p>However, as Anna Rogers<a data-type="indexterm" data-primary="Rogers, Anna" id="id645"/> describes in her <a href="https://oreil.ly/hxU_-">paper</a>, this question is already moot. Data curation is already happening, whether we like it or not, and the values and interests of model providers are already being encoded into the models. For example, only a small proportion of available data is selected to be part of the pre-training set. This selection process is not value-neutral, even if one might not explicitly think in terms of it.</p>

<p>Wikipedia<a data-type="indexterm" data-primary="Wikipedia" id="id646"/> is one of the more popular datasets used in training LLMs. While it might be a no-brainer to include Wikipedia in a pre-training dataset, let’s explore the implications. Wikipedia is edited by volunteers, a very large proportion of them being men. Since the determination of whether a topic is reputable enough to deserve a Wikipedia page rests with the editors who are largely made up of men, we see disparities like obscure male football players from lower-level leagues getting their own pages while a disproportionate number of biography articles about women are slated for deletion.</p>

<p>Similarly, the highly influential WebText dataset<a data-type="indexterm" data-primary="WebText dataset" id="id647"/> is sourced from Reddit outbound links. Reddit is a predominantly male site, with <a href="https://oreil.ly/i2RkB">74% of users</a> being men. Naturally, links posted on Reddit are more likely to be catered to male interests.</p>

<p>Bias can also be introduced during the data filtering<a data-type="indexterm" data-primary="data cleaning and filtering" id="id648"/><a data-type="indexterm" data-primary="cleaning and filtering data" id="id649"/><a data-type="indexterm" data-primary="filtering and cleaning data" id="id650"/> stages. Earlier, we noted that keyword lists are often used to filter out pornographic material and abusive text. However, using a naive keyword list is a lazy approach that not only has problems with effectiveness (false negatives) but also inadvertently <a href="https://oreil.ly/XWBjV">results in</a> filtering out positive text written by or about minority communities, as well as text written in dialects like African American English and Hispanic-aligned English. The fact that words in English have multiple senses has resulted in certain documents about breastfeeding being filtered out of the C4 
<span class="keep-together">dataset</span>.</p>

<p>Overall, whether a word is hateful, abusive, or toxic depends on the social context, the intentions of the reader, and the intended audience. Keyword-based methods simply do not capture this nuance. The question of whether it is more effective to handle these issues at the pre-training stage or further downstream is an open area of research. We will explore techniques that can be employed downstream in 
<span class="keep-together"><a data-type="xref" href="ch10.html#ch10">Chapter 10</a></span>.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p>The authors of the <a href="https://oreil.ly/r4oAT">Pythia model</a> experimented<a data-type="indexterm" data-primary="Pythia model" id="id651"/> by replacing masculine pronouns with feminine ones for the last 7% of training tokens and noticed a de-biasing impact on downstream tasks<a data-type="indexterm" data-startref="xi_pretrainingofdata24102" id="id652"/><a data-type="indexterm" data-startref="xi_pretrainingofdatabiasandfairnessissues279533" id="id653"/><a data-type="indexterm" data-startref="xi_biasandfairnessissuesinpretrainingdatasets279533" id="id654"/><a data-type="indexterm" data-startref="xi_ethicalquestionspretrainingofdata279533" id="id655"/><a data-type="indexterm" data-startref="xi_datasetsbiasandfairnessissues279533" id="id656"/>.</p>
</div>
</div></section>






<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="id358">
<h1>Summary</h1>

<p>In this chapter, we outlined the key ingredients of a language model: the pre-training data, the vocabulary and tokenizer, the language objective, and the model architecture. We walked through the steps involved in creating a pre-training dataset in detail, including language identification, text extraction and cleaning, quality filtering, deduplication, PII removal, and test set decontamination. We also provided a list of commonly used pre-training datasets and the steps taken for preprocessing each of them. In the next chapter, we will explore the vocabulary and tokenizer of the language model: the language we intend the model to learn.</p>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="id625"><sup><a href="ch02.html#id625-marker">1</a></sup> From Dodge et al., <a href="https://oreil.ly/PwtVp">“A Case Study on the Colossal Clean Crawled Corpus”</a>, EMNLP 2021.</p></div></div></section></div>
</div>
</body></html>