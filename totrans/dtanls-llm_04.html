<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class=" readable-text-h1" id="ch__openaiapi"> <span class="chapter-title-numbering"><span class="num-string">3</span></span> <span class="title-text"> The OpenAI Python library</span> </h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header">This chapter covers</h3> 
   <ul> 
    <li id="p2">Installing the OpenAI library</li> 
    <li id="p3">Invoking GPT models using Python</li> 
    <li id="p4">Configuration parameters</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p5"> 
   <p>In the last chapter, we used GPT models via the OpenAI web interface. This works well as long as we’re just trying to have a conversation or classify and summarize single reviews. However, imagine trying to classify hundreds of reviews. In that case, using the web interface manually for each review becomes very tedious (to say the least). Also, perhaps we want to use a language model in combination with other tools. For instance, we might want to use GPT models to translate questions to formal queries and then seamlessly execute those queries in the corresponding tool (without having to manually copy queries back and forth between different interfaces). In all these scenarios, we need a different interface.</p> 
  </div> 
  <div class="readable-text" id="p6"> 
   <p>In this chapter, we’ll discuss a Python library from OpenAI that lets you call OpenAI’s language models directly from Python. This enables you to integrate calls to language models as a subfunction in your code. We will be using this library in most chapters of the book. Therefore, it makes sense to at least skim this chapter before proceeding to the following chapters.</p> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>Although the current chapter focuses on OpenAI’s Python library, the libraries offered by other providers of language models (including Anthropic, Cohere, and Google) are similar.</p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <h2 class=" readable-text-h2" id="prerequisites"><span class="num-string browsable-reference-id">3.1</span> Prerequisites</h2> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>First, let’s make sure we have the right environment for OpenAI’s Python library. We will use the Python programming language, so make sure Python is installed. To do so, open a terminal, and enter the following command (this command should work for Linux, macOS, and Windows terminals):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p10"> 
   <div class="code-area-container"> 
    <pre class="code-area">python --version</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p11"> 
   <p>If this command returns an error message, try replacing <code>python</code> with <code>python3</code> in the command and running it again. If Python is installed on your system, you should see a version number in reply (e.g., “Python 3.10.13”). If not, you will get an error message. For the following examples, you will need at least Python 3.9 (or a later version). If Python is not installed on your system, or if your version is below the required one, visit <a href="http://www.python.org">www.python.org</a>, click Downloads, and follow the instructions to install Python. You may also want to install an integrated development environment (IDE). PyDev (<a href="http://www.pydev.org">www.pydev.org</a>) and PyCharm (<a href="http://www.jetbrains.com/pycharm">www.jetbrains.com/pycharm</a>) are two of the many IDEs available for Python.</p> 
  </div> 
  <div class="readable-text" id="p12"> 
   <p>Along with Python, you will need pip, a package-management system used to install Python packages (the OpenAI library comes in the form of such a package). For recent Python versions (which you will need in any case), this program is already installed by default. Nevertheless, it can’t hurt to make sure:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p13"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip --version</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p14"> 
   <p>Again, you should see a version number if everything is installed properly. Let’s make sure pip is up to date. The following command should work on Linux, macOS, and Windows:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p15"> 
   <div class="code-area-container"> 
    <pre class="code-area">python -m pip install --upgrade pip</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p16"> 
   <p>That’s it! Your system is ready to install the OpenAI Python client.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p17"> 
    <h5 class=" callout-container-h5 readable-text-h5">What if it doesn’t work?</h5> 
   </div> 
   <div class="readable-text" id="p18"> 
    <p> Don’t panic! If any of the previously mentioned steps fail, you may not be able to execute the following code on your local machine. However, as long as you have web access, you can use a cloud platform instead. For instance, the Google Colab platform, accessible at <a href="https://colab.research.google.com">https://colab.research.google.com</a>, enables you to create notebooks that can execute all of the following code samples. Figure <a href="#fig__colab">3.1</a> shows the interface after creating a cell installing the OpenAI library (upper cell) and the start of a corresponding Python program (lower cell). We will discuss library installation and usage in the following sections.</p> 
   </div> 
  </div> 
  <div class="browsable-container figure-container" id="p19">  
   <img alt="figure" src="../Images/CH03_F01_Trummer.png" width="1100" height="687"/> 
   <h5 class=" figure-container-h5" id="fig__colab"><span class="num-string">Figure <span class="browsable-reference-id">3.1</span></span> The Google Colab platform can be used to run the following examples.</h5>
  </div> 
  <div class="readable-text" id="p20"> 
   <h2 class=" readable-text-h2" id="sub__clientsetup"><span class="num-string browsable-reference-id">3.2</span> Installing OpenAI’s Python library</h2> 
  </div> 
  <div class="readable-text" id="p21"> 
   <p>Time to start using GPT like a pro! Although the ChatGPT web interface, discussed in chapter 2, is useful for conversations and trying out new prompts, it is unsuitable for implementing complex data-processing pipelines. For that, OpenAI’s Python library is a much better choice, enabling you to invoke language models directly from Python. First, let’s install the corresponding library. Enter the following command into a terminal:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p22"> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install openai==1.29</pre>  
   </div> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p23"> 
    <h5 class=" callout-container-h5 readable-text-h5">Can I use a different library version?</h5> 
   </div> 
   <div class="readable-text" id="p24"> 
    <p> You might have noticed the reference to a specific version (version 1.29) of the OpenAI library. The code presented in this and the following chapters has been tested with this version. As the syntax differs slightly across different library versions (unless you are willing to adapt the code), install this precise version.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p25"> 
   <p>Every time we use the OpenAI library, we need to provide a key giving us access to the OpenAI models (this is required for billing purposes). If you have not yet created an OpenAI account, go to <a href="https://platform.openai.com">https://platform.openai.com</a>, click Sign Up, and follow the instructions. If you have an account but are not currently logged in, provide your account credentials instead. Make sure to add a payment method in the Billing section, and charge it with a couple of dollars. After that, if you haven’t done so yet, it is time to generate your secret key.</p> 
  </div> 
  <div class="readable-text" id="p26"> 
   <p>Go to <a href="https://platform.openai.com/account/api-keys">https://platform.openai.com/account/api-keys</a>. You should see the website shown in figure <a href="#fig__openaikeys">3.2</a>.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p27">  
   <img alt="figure" src="../Images/CH03_F02_Trummer.png" width="1100" height="673"/> 
   <h5 class=" figure-container-h5" id="fig__openaikeys"><span class="num-string">Figure <span class="browsable-reference-id">3.2</span></span> Managing secret keys for accessing the OpenAI API</h5>
  </div> 
  <div class="readable-text" id="p28"> 
   <p>Click the Create New Secret Key button. The interface will show a text string representing the key. Be sure to copy and store that key! You will not be able to retrieve the full key again after closing the corresponding window.</p> 
  </div> 
  <div class="readable-text" id="p29"> 
   <p>Whenever we use the Python library, we need to provide our secret key to link our requests to the appropriate account. The easiest way to do that is to store the secret key in an environment variable named <code>OPENAI_API_KEY</code>. OpenAI will automatically extract the key from that variable if it exists. The precise command used to set environment variables depends on the operating system. For example, the following command works for Linux and macOS (replace the three dots with your key):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p30"> 
   <div class="code-area-container"> 
    <pre class="code-area">export OPENAI_API_KEY=...</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p31"> 
   <p>Alternatively, you can set the key on a per-invocation basis by prefixing your calls to Python with the corresponding assignments. For example, use the following command to call the code listing presented in the next section while setting the key at the same time (again, substitute your key for the three dots):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p32"> 
   <div class="code-area-container"> 
    <pre class="code-area">OPENAI_API_KEY=... python listing1.py</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p33"> 
   <p>Finally, if none of the other options work, you can specify your access key directly in your Python code. More precisely, right after importing OpenAI’s Python library, we can pass the API access key as a parameter when creating the <code>client</code> object (which we discuss in more detail later):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p34"> 
   <div class="code-area-container"> 
    <pre class="code-area">import openai
client = openai.OpenAI(api_key='...')</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p35"> 
   <p>As before, replace the three dots with your OpenAI access key. The following code samples assume that the access key is specified in an environment variable and will therefore omit this parameter. If environment variables don’t work for you, change the code listings by passing your access key as a parameter.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p36"> 
   <p> <span class="print-book-callout-head">Warning</span> Never share your code if it contains your OpenAI access key. Among other things, having your key would enable others to invoke OpenAI’s models while making you pay for it. </p> 
  </div> 
  <div class="readable-text" id="p37"> 
   <p>Assuming you have specified your access key in one way or another, we are now ready to start calling GPT models using OpenAI’s Python library.</p> 
  </div> 
  <div class="readable-text" id="p38"> 
   <h2 class=" readable-text-h2" id="listing-available-models"><span class="num-string browsable-reference-id">3.3</span> Listing available models</h2> 
  </div> 
  <div class="readable-text" id="p39"> 
   <p>We will use the Python library to retrieve a list of available OpenAI models. Listing <a href="#code__listmodels">3.1</a> shows the corresponding Python code. (You can download this and all of the following code listings from the book’s companion website.) First, we import the OpenAI library (<strong class="cueball">1</strong>). Then we create a client object, enabling us to access library functions (<strong class="cueball">2</strong>). Next, we query for all available OpenAI models (<strong class="cueball">3</strong>) and print out the result (<strong class="cueball">4</strong>).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p40"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__listmodels"><span class="num-string">Listing <span class="browsable-reference-id">3.1</span></span> Listing available OpenAI models</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import openai  #1
client = openai.OpenAI()  #2

models = client.models.list()  #3

for model in models.data: #4
    print(model)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Imports the OpenAI Python library
     <br/>#2 Creates an OpenAI client
     <br/>#3 Gets available OpenAI models
     <br/>#4 Prints out the retrieved models
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>You should see a result similar to the following:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p42"> 
   <div class="code-area-container"> 
    <pre class="code-area">Model(id='dall-e-3', created=1698785189, 
    object='model', owned_by='system')
Model(id='whisper-1', created=1677532384, 
    object='model', owned_by='openai-internal')
Model(id='GPT-4o-2024-05-13', created=1715368132, 
    object='model', owned_by='system')
Model(id='davinci-002', created=1692634301, 
    object='model', owned_by='system')
Model(id='GPT-4o', created=1715367049,  #1
    object='model', owned_by='system')
...
 #2
Model(id='curie:ft-personal-2022-01-10-16-52-53', 
    created=1641833573, object='model', owned_by='trummerlab')
Model(id='davinci:ft-personal-2022-01-13-19-59-51', 
    created=1642103991, object='model', owned_by='trummerlab')
Model(id='ft:gpt-3.5-turbo-0613:trummerlab::8qlJH6bV', 
    created=1707585607, object='model', owned_by='trummerlab')
...</pre> 
    <div class="code-annotations-overlay-container">
     #1 GPT-4o
     <br/>#2 Fine-tuned model version
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p43"> 
   <p>Each model is described by an ID (e.g., <code>GPT-4o</code> (<strong class="cueball">1</strong>)). We will use this ID to tell OpenAI which model we want to use to process our requests. Besides the ID, each model comes with a creation timestamp and information about model ownership (<code>owned_by</code> field). In most cases, models are owned by OpenAI (marked, for example, <code>system</code> or <code>openai-internal</code>). In some cases, however, models are owned by <code>trummerlab</code> (<strong class="cueball">2</strong>), the name of the account used by this book’s author. Those models are not publicly accessible but private to the owning account. You will not see those models when executing the code using your account. They are created by a process called <em>fine-tuning</em> from the publicly available base models.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p44"> 
    <h5 class=" callout-container-h5 readable-text-h5">What is fine-tuning?</h5> 
   </div> 
   <div class="readable-text" id="p45"> 
    <p> By default, language models such as GPT-4o are trained to be versatile, meaning they can, in principle, perform any task. But sometimes we don’t want a model that is versatile but rather a model that does very well on one specific task. Fine-tuning enables us to specialize a model for a task we care about. We discuss fine-tuning in more detail in chapter 9.</p> 
   </div> 
  </div> 
  <div class="readable-text" id="p46"> 
   <h2 class=" readable-text-h2" id="chat-completion"><span class="num-string browsable-reference-id">3.4</span> Chat completion</h2> 
  </div> 
  <div class="readable-text" id="p47"> 
   <p>Almost all the code in this book uses the same functionality of the OpenAI Python library: <em>chat completion</em>. With chat completion, your model generates a completion for a chat, provided as input. The input can contain various types of data, such as text and images. We will exploit those features in the following chapters but restrict ourselves to text for the moment. Chat completion is also used in the background of OpenAI’s ChatGPT web interface. Given the chat history as input (which includes the latest message as well as prior messages, possibly containing relevant context), the model generates the most suitable reply.</p> 
  </div> 
  <div class="readable-text" id="p48"> 
   <p>To use chat completion from Python, we first need a format to describe the chat history. This is part of the input we’re providing for chat completion. In OpenAI’s Python library, chats are represented as a list of messages. Each message in turn is represented as a Python dictionary. This Python dictionary specifies values for several important properties of the message. At the very least, we need to specify two important attributes for each message:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p49">The <code>role</code> attribute, which specifies the source of a message</li> 
   <li class="readable-text" id="p50">The <code>content</code> attribute, which specifies the content of a message</li> 
  </ul> 
  <div class="readable-text" id="p51"> 
   <p>Let’s start by discussing the <code>role</code> attribute. As you know from the last chapter, a chat with GPT models is a back-and-forth series of messages, alternating between messages written by the user and messages written by the model. Accordingly, we can specify the value <code>user</code> for the <code>role</code> attribute to identify a message as written by the user. Alternatively, we can specify the value <code>assistant</code> to mark a message as generated by the language model. A third possible value for the <code>role</code> attribute is <code>system</code>. Such messages are typically used at the beginning of a chat history. They are meant to convey generic guidelines to the model, independent of the specific tasks submitted by users. For instance, a typical system message could have the content “You are a helpful assistant,” but more specialized versions (e.g., “You are an assistant that translates questions about data sets into SQL queries”) are also possible. We will not use <code>system</code> messages in this book, but feel free to experiment and try adding your own system messages to see if they influence the model output.</p> 
  </div> 
  <div class="readable-text" id="p52"> 
   <p>The <code>content</code> attribute specifies the content of a message. In this chapter, we will restrict ourselves to text content. In later chapters, we will see how language models can be used to process more diverse types of content. In the following code samples, we will only need to specify a single message in our chat history. This message contains instructions describing a task that the language model should solve, as well as relevant context information. For instance, the following chat history encourages the model to generate a story for us:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p53"> 
   <div class="code-area-container"> 
    <pre class="code-area">[{
    'role':'user',  #1
    'content':'Tell me a story!'  #2
}]</pre> 
    <div class="code-annotations-overlay-container">
     #1 Message from user
     <br/>#2 Task specification
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p54"> 
   <p>The list of messages contains only a single message. This message is marked as originating from the user (<strong class="cueball">1</strong>) and describes the previously mentioned task in its content (<strong class="cueball">2</strong>). As a reply, we would expect the model to generate a story following the input instructions.</p> 
  </div> 
  <div class="readable-text" id="p55"> 
   <p>How can we invoke a model for chat completion? This can be realized with just a few lines of Python code. First, we need to import the OpenAI Python library (<strong class="cueball">1</strong>) and create a <code>client</code> object (<strong class="cueball">2</strong>):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p56"> 
   <div class="code-area-container"> 
    <pre class="code-area">import openai  #1
client = openai.OpenAI()  #2</pre> 
    <div class="code-annotations-overlay-container">
     #1 Imports the OpenAI Python library
     <br/>#2 Creates the OpenAI client
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p57"> 
   <p>We will use the <code>client</code> object for all of the following invocations of the language model. The previous code appears in almost all of our code samples. Remember that you may need to pass the OpenAI access key manually as a parameter when creating the client (unless you specify your access key in an environment variable, which is the recommended approach). After creating the <code>client</code>, we can issue chat completion requests as shown here:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p58"> 
   <div class="code-area-container"> 
    <pre class="code-area">result = client.chat.completions.create(
    model='GPT-4o',                  #1
    messages=[{                      #2
        'role':'user', 
        'content':'Tell me a story!'
        }])</pre> 
    <div class="code-annotations-overlay-container">
     #1 Selects a model
     <br/>#2 Specifies input messages
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p59"> 
   <p>We use the <code>client.chat.completions.create</code> function to create a new request. The <code>model</code> parameter (<strong class="cueball">1</strong>) specifies the name of the model we want to use for completion. In this case, we’re selecting OpenAI’s GPT-4o model, which can process multimodal data. We will use this model for most of the code samples in this book. Next, we specify the chat history as input via the <code>messages</code> parameter (<strong class="cueball">2</strong>). This is the chat history discussed before, instructing the model to generate a story.</p> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>Let’s put it all together. The following listing (available as listing 2 in the chapter 3 section on the book’s companion website) uses GPT-4o to generate a story.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p61"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__textcompletionsimple"><span class="num-string">Listing <span class="browsable-reference-id">3.2</span></span> Using GPT-4o for chat completion</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import openai          #1
client = openai.OpenAI()  #2

result = client.chat.completions.create(  #3
    model='GPT-4o',                    #4
    messages=[{                        #5
        'role':'user', 
        'content':'Tell me a story!'
        }])
print(result)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Imports the OpenAI Python library
     <br/>#2 Creates an OpenAI client
     <br/>#3 Invokes chat completion
     <br/>#4 Selects a model/
     <br/>#5 Specifies input messages/
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p62"> 
   <p>Running the code should produce a result such as the following (your precise story may differ due to randomization):</p> 
  </div> 
  <div class="browsable-container listing-container" id="p63"> 
   <div class="code-area-container"> 
    <pre class="code-area">ChatCompletion(
    id='chatcmpl-9YKmJCE8SITsKyI557T8KTuX3IxWN', 
    choices=[                                #1
        Choice(
            finish_reason='stop',  #2
            index=0, 
            logprobs=None, 
            message=ChatCompletionMessage(  #3
                content="Of course! Here's a story that ... ", 
                role='assistant', 
                function_call=None, 
                tool_calls=None)
            )
        ], 
        created=1717970051, 
        model='GPT-4o-2024-05-13', 
        object='chat.completion', 
        system_fingerprint='fp_319be4768e', 
        usage=CompletionUsage(          #4
            completion_tokens=810, 
            prompt_tokens=12, 
            total_tokens=822
        )
    )</pre> 
    <div class="code-annotations-overlay-container">
     #1 List of completions
     <br/>#2 Termination condition
     <br/>#3 Completion message
     <br/>#4 Token usage
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>Let’s discuss the different components of that result. First, we have a list of completion alternatives (<strong class="cueball">1</strong>) (objects of type <code>Choice</code>). In our case, that list contains only a single entry. This is the default behavior, although we can ask for multiple alternative completions by setting the right configuration parameters (discussed in the next section). The <code>finish_reason</code> flag (<strong class="cueball">2</strong>) indicates for each completion the reason to stop generating. For instance, this can be due to reaching a length limit on generated text. The <code>stop</code> value indicates that the language model was able to generate complete output (as opposed to reaching a length limit). The actual message (<strong class="cueball">3</strong>) content is abbreviated, and in all likelihood, you will see different stories if you invoke the code repeatedly.</p> 
  </div> 
  <div class="readable-text" id="p65"> 
   <p>Besides the completions themselves, the result contains metadata and usage statistics (<strong class="cueball">4</strong>). More precisely, we find values for the following properties:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p66"><code>completion_tokens</code>—The number of generated tokens</li> 
   <li class="readable-text" id="p67"><code>prompt_tokens</code>—The number of tokens in the input</li> 
   <li class="readable-text" id="p68"><code>total_tokens</code>—The number of tokens read and generated</li> 
  </ul> 
  <div class="readable-text" id="p69"> 
   <p>Why would we care about the number of tokens? Because pricing for most OpenAI models is proportional to the number of tokens read and generated. For instance, at the time of writing, using GPT-4o costs $5 per million tokens read and $15 per million tokens generated. Note the difference in pricing between tokens read and generated. Typically, as in this case, generating tokens is more expensive than reading tokens. The pricing depends not only on the number of tokens but also on the model used. For example, replacing GPT-4o with the GPT-3.5 Turbo model (a slightly less powerful GPT version) cuts costs by a factor of 10. Before analyzing large amounts of data with language models, choose the appropriate model size for your task and wallet.</p> 
  </div> 
  <div class="readable-text" id="p70"> 
   <h2 class=" readable-text-h2" id="sec__completioncustomization"><span class="num-string browsable-reference-id">3.5</span> Customizing model behavior</h2> 
  </div> 
  <div class="readable-text" id="p71"> 
   <p>You can use various parameters to influence how the model replies to your input. These parameters can be specified in addition to the <code>model</code> and <code>messages</code> parameters when invoking the <code>chat.completions.create</code> function. In this section, we discuss different categories of parameters, classifying parameters by the aspect of model behavior they influence.</p> 
  </div> 
  <div class="readable-text" id="p72"> 
   <h3 class=" readable-text-h3" id="configuring-termination-conditions"><span class="num-string browsable-reference-id">3.5.1</span> Configuring termination conditions</h3> 
  </div> 
  <div class="readable-text" id="p73"> 
   <p>When we invoke a model for chat completion, it generates output until a stopping condition is met. The two parameters discussed next enable us to configure when text generation stops.</p> 
  </div> 
  <div class="readable-text" id="p74"> 
   <p>The <code>max_tokens</code> parameter specifies the maximum number of tokens (i.e., the atomic unit at which language models represent text) generated during completion. A token corresponds to approximately four characters, and a typical paragraph contains around 100 tokens. The maximum admissible value for this parameter is determined by the model used. For instance, ada, one of the smallest GPT versions, allows up to 2,049 tokens, whereas GPT-4o supports up to 128,000 tokens. Keep in mind that the maximum number of tokens supported by the model includes tokens read and tokens generated. As <code>max_tokens</code> refers only to the number of tokens generated, you should not set it higher than the maximum number of tokens supported by the model used <em>minus the number of tokens in the prompt</em>.</p> 
  </div> 
  <div class="readable-text" id="p75"> 
   <p>As a general rule, setting a reasonable value for <code>max_tokens</code> is almost always a good idea. After all, we’re paying for each generated token, and setting a bound on the number of tokens enables you to bound monetary fees per model invocation.</p> 
  </div> 
  <div class="readable-text" id="p76"> 
   <p>In some scenarios, specific text patterns indicate the end of the desired output. For instance, when generating code, it can be a string specific to the corresponding programming language indicating the end of the program. On the other hand, when generating a fairy tale, it can be the string “and they lived happily ever after!” In those scenarios, we might want to use the <code>stop</code> parameter to configure the OpenAI library to stop generating output whenever a specific token sequence appears. In some cases, there is only one token sequence indicating termination. In those scenarios, we can directly assign the <code>stop</code> parameter to the corresponding string value. In other scenarios, there are multiple candidate sequences that indicate termination. In those cases, we can assign the <code>stop</code> parameter to a list of up to four sequences. Text generation terminates whenever any of those sequences is generated.</p> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>Note that you can use both of the previously mentioned parameters together. In those cases, output generation stops whenever the length limit is reached or one of the stop sequences appears (whichever happens first).</p> 
  </div> 
  <div class="readable-text" id="p78"> 
   <h3 class=" readable-text-h3" id="sub__ConfigureGeneration"><span class="num-string browsable-reference-id">3.5.2</span> Configuring output generation</h3> 
  </div> 
  <div class="readable-text" id="p79"> 
   <p>The parameters we just discussed enable you to choose when the output terminates. But how can you influence the output generated until that point? Here, OpenAI offers a few parameters that enable you to bias the way in which GPT models select output text.</p> 
  </div> 
  <div class="readable-text" id="p80"> 
   <p>Several parameters enable you to influence how “repetitive” the generated output should be. More precisely, those parameters allow you to influence whether generating the same tokens repeatedly is desirable or not.</p> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>The <code>presence_penalty</code> parameter enables you to penalize chat completions that use the same tokens repeatedly. The presence penalty is a value between –2 and +2 (with a default value of 0). A positive penalty encourages the model to avoid reusing the same tokens. A negative penalty, on the other hand, encourages the model to use the same tokens repeatedly. The higher the absolute value, the stronger the corresponding effect.</p> 
  </div> 
  <div class="readable-text" id="p82"> 
   <p>The <code>frequency_penalty</code> relates to the prior parameter but enables a more fine-grained penalization scheme. The <code>presence_penalty</code> parameter is based on the mere <em>presence</em> of a token. For example, we do not differentiate between a token that appears twice and one that appears hundreds of times. The frequency penalty is used as a factor, multiplying the number of prior appearances of a token when aggregating its score (which is used to determine whether the token should appear next). Hence, the more often a token was used before, the less likely it is to appear again. Similar to the presence penalty, the <code>frequency_penalty</code> parameter takes values between –2 and +2 with a default setting of 0. A positive penalty factor encourages GPT models to avoid repeating the same token, whereas a negative value encourages repetitions.</p> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>Sometimes we are only interested in one of a limited set of eligible tokens. For instance, when classifying text, the set of classes is typically determined a priori. If so, let’s tell the model about it! The <code>logit_bias</code> parameter allows mapping token IDs to a bias factor. A high bias factor encourages the model to consider the corresponding token as output. A sufficiently low bias score essentially prevents the model from using the token. A sufficiently high score almost guarantees that the corresponding token will appear in the output.</p> 
  </div> 
  <div class="readable-text" id="p84"> 
   <p>Using the <code>logit_bias</code> parameter avoids generating useless output in situations where we can narrow the set of reasonable tokens. The value for <code>logit_bias</code> is a Python dictionary that maps token IDs to values between –100 and +100. Values between –1 and +1 are more typical and still give the model room to consider tokens with a low value (or to avoid using tokens that are associated with higher values). But how do we find the token IDs associated with relevant words? For that, we can use the GPT tokenizer tool, available at <a href="https://platform.openai.com/tokenizer?view=bpe">https://platform.openai.com/tokenizer?view=bpe</a>. Simply enter the words you want to encourage (or ban), and the associated token IDs will be displayed. Note that multiple tokenizer variants are available, associated with different models. Select the right tokenizer for your model (because otherwise, the token IDs may be incorrect).</p> 
  </div> 
  <div class="readable-text" id="p85"> 
   <h3 class=" readable-text-h3" id="configuring-randomization"><span class="num-string browsable-reference-id">3.5.3</span> Configuring randomization</h3> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>How do GPT models select the next output token? At a high level of abstraction, we calculate scores for all possible output tokens and then select a token based on those scores. Although tokens with higher scores tend to have better chances of being selected, we might not always want to select the token with the maximum score. For instance, think back to chapter 2, where we were able to regenerate replies for the same input, potentially leading to different results. This can be useful if the first output does not quite satisfy our requirements. If always selecting the tokens with the highest scores, regenerating an answer would be unlikely to change the output. Hence, to enable users to get diverse replies, we need to introduce a certain degree of randomization when mapping scores to output tokens.</p> 
  </div> 
  <div class="readable-text" id="p87"> 
   <p>Of course, decoupling the output too much from token scores—that is, using too much randomization—may lead to useless output (at the extreme, the output no longer connects to the input and does not follow our instructions). On the other hand, using too little randomization can lead to outputs that are less diverse than desired. Choosing the right degree of randomization for a specific scenario can take some experimentation. In each case, OpenAI offers multiple parameters that enable you to fine-tune how token scores translate to output tokens. We will discuss those parameters next.</p> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>One of the parameters most commonly used to tune randomization is the <code>temperature</code> parameter. A higher temperature means more randomization, whereas a lower temperature corresponds to less randomization. A low degree of randomization means the token with the highest score is very likely to be selected. A very high degree of randomization means tokens are (almost) selected with equal probability, independently of the scores assigned by the model. The <code>temperature</code> parameter enables you to thread the needle between those two extremes. Values for this parameter are chosen between 0 and 2 with a default of 1.</p> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>Temperature is one possibility when choosing the degree of randomization. The <code>top_p</code> parameter is an alternative approach. (It is not recommended that you alter both <code>temperature</code> and <code>top_p</code> in the same invocation of the language model.) Based on their scores, we can associate a probability of being “correct” with each possible output token. Now imagine that we are sorting those tokens in decreasing order of probability. We can reduce the degree of randomization by focusing only on the first few tokens: we neglect tokens with lower probability. How many tokens should we consider? Instead of fixing the number of eligible tokens directly, the <code>top_p</code> parameter fixes the <em>probability mass</em> of those tokens. In other words, we add tokens to the set of eligible tokens in decreasing order of probability. Whenever the sum of probability values of all selected tokens (the probability mass) exceeds the value of <code>top_p</code>, we stop adding tokens. Finally, we pick the next output token among those eligible tokens.</p> 
  </div> 
  <div class="readable-text" id="p90"> 
   <p>As the <code>top_p</code> parameter describes a probability, its values are taken from the interval between 0 and 1. Similar to temperature, choosing a higher value leads to more randomization (because even tokens with lower probability become eligible).</p> 
  </div> 
  <div class="readable-text" id="p91"> 
   <p>As soon as we are using a certain degree of randomization, it becomes useful to generate multiple answers for the same input prompt. After that, we can choose the preferred answer via postprocessing. For instance, assume that we are generating multiple SQL queries for the same input prompt. To select the preferred answer, we can try executing them on a target database and discard the queries that result in a syntax error message. Of course, we can simply call the language model repeatedly with the same prompt. However, it is more efficient to call the language model once and configure the number of generated replies. The parameter <code>n</code> determines the number of generated replies. By default, this parameter is set to 1 (i.e., only a single answer is generated). You may choose a higher value to obtain more replies. Note that using a higher value for this parameter also increases per-invocation costs (because you pay for each token generated, counting tokens across different replies).</p> 
  </div> 
  <div class="readable-text" id="p92"> 
   <h3 class=" readable-text-h3" id="customization-example"><span class="num-string browsable-reference-id">3.5.4</span> Customization example</h3> 
  </div> 
  <div class="readable-text" id="p93"> 
   <p>Let’s try some of the parameters in our code. The following listing prompts GPT-4o to write a story, this time using some of the parameters we’ve discussed to customize chat completion.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p94"> 
   <h5 class=" listing-container-h5 browsable-container-h5" id="code__textcompletioncustom"><span class="num-string">Listing <span class="browsable-reference-id">3.3</span></span> Using GPT-4o for chat completion with custom parameter settings</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import openai
client = openai.OpenAI()

result = client.chat.completions.create(
    model='GPT-4o',
    messages=[{
        'role':'user', 
        'content':'Tell me a story!'
        }],
    max_tokens=512,        #1
    stop='happily ever after',  #2
    temperature=1.5,       #3
    presence_penalty=0.5,  #4
    logit_bias={14844:-100})         #5
print(result.choices[0].message.content)</pre> 
    <div class="code-annotations-overlay-container">
     #1 Limits the output length
     <br/>#2 Sets a stopping condition
     <br/>#3 Sets temperature
     <br/>#4 Penalizes repetitions
     <br/>#5 Adds bias
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>First, to avoid getting a lengthy story, we set the maximum number of tokens to 512 (<strong class="cueball">1</strong>). This should suffice for a few paragraphs of text. To avoid generating more content than necessary, we define a stop sequence as well: “happily ever after” (<strong class="cueball">2</strong>). We do so, hoping that any reasonable story will, of course, end with the popular expression “and they lived happily ever after.” This assumption is certainly simplifying.</p> 
  </div> 
  <div class="readable-text" id="p96"> 
   <p>Next, we set the temperature to 1.5 (<strong class="cueball">3</strong>), allowing a certain degree of randomization. Having an overly repetitive story is boring. Therefore, we set a positive presence penalty (<code>presence_penalty</code>=0.5) (<strong class="cueball">4</strong>), discouraging GPT-4o from reusing the same tokens. Finally, we don’t want a scary story (perhaps we want to read it to our kids), so we penalize the word <em>dragon</em> (we obtain the corresponding token ID as described in section <a href="#sub__ConfigureGeneration">3.5.2</a>) (<strong class="cueball">5</strong>).</p> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>Let’s see what we get! For instance, in a sample invocation, GPT-4o generates the following story:</p> 
  </div> 
  <div class="callout-container sidebar-container prompt-sidebar"> 
   <div class="readable-text response" id="p98"> 
    <p><strong class="response-head"><img alt="chatGpt" height="20" src="../Images/chatGpt.png" style="width: 20px;" width="20"/><strong><em>GPT</em> </strong></strong></p> 
   </div> 
   <div class="readable-text response" id="p99"> 
    <p><em>Once upon a time, in a quaint village nestled between rolling hills and dense forests, there lived a girl named Elara. She was known for her curiosity and adventurous spirit, traits that often led her to explore the uncharted parts of her village. One crisp autumn morning, Elara decided to venture into the Whispering Woods, a place shrouded in mystery and legend. The villagers often spoke of the woods as a place where magic lingered, but also one that nobody dared to enter beyond the edge. Equipped with her trusty satchel and a heart full of excitement, Elara set off with the sunrise. As she walked deeper into the woods, the whispers of rustling leaves seemed to greet her, and she felt an odd sense of familiarity. She wandered through thickets and climbed over fallen logs until she stumbled upon a hidden grove, bathed in shimmering light. In the center of the grove stood an ancient tree with silvery leaves, its trunk twisted like spiraling tendrils of smoke. Elara was mesmerized; it looked unlike any tree she had ever seen. She approached it cautiously and noticed a small, ornate chest nestled among its roots. Her heart racing, Elara opened the chest and found inside a delicate, golden locket. As soon as she touched it, the forest seemed to come alive. The whispers grew louder, forming words that she could understand. “Brave heart, you have found the Locket of Eternia,” the voice murmured. “It grants the bearer one wish.” Elara’s mind raced with possibilities, but she knew what her heart desired most: to bring joy and prosperity to her beloved village. Clutching the locket, she whispered her wish. In an instant, a warm glow enveloped her, and she felt a surge of energy. The grove shimmered momentarily and then returned to its tranquil stillness. Elara knew something extraordinary had happened. As she returned home, she noticed subtle changes—the crops seemed more vibrant, the animals healthier and the village air filled with a sense of contentment. The villagers welcomed her back, their faces glowing with happiness. Elara never revealed the secret of the Whispering Woods or the Locket of Eternia to anyone. She simply smiled whenever anyone commented on the newfound prosperity of the village. She knew that sometimes, the greatest magic lies not in what is seen, but in the courage to follow one’s heart. And so, Elara’s village thrived, becoming a beacon of joy and harmony. All because one brave girl dared to listen to the whispers of the woods. And she lived</em></p> 
   </div> 
  </div> 
  <div class="readable-text" id="p100"> 
   <p>Happily ever after! It turns out that our stop sequence, the expression “happily ever after,” was indeed used at the end of the story (and is therefore omitted in the output returned by GPT-4o). Try a few more parameter settings, and see how the result changes as a function of the configuration.</p> 
  </div> 
  <div class="readable-text" id="p101"> 
   <h3 class=" readable-text-h3" id="further-parameters"><span class="num-string browsable-reference-id">3.5.5</span> Further parameters</h3> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>We have discussed the most important parameters for data-analysis purposes. You can use each of them when requesting a completion from OpenAI’s GPT models. Note that there are more parameters beyond the ones mentioned in this chapter. OpenAI’s API reference documentation (<a href="https://platform.openai.com/docs/api-reference/completions">https://platform.openai.com/docs/api</a><a href="https://platform.openai.com/docs/api-reference/completions">-reference/completions</a>) describes all parameters in detail.</p> 
  </div> 
  <div class="readable-text" id="p103"> 
   <h2 class=" readable-text-h2" id="summary">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p104">You can use OpenAI’s language models via a Python API. Other providers offer similar libraries for accessing their models.</li> 
   <li class="readable-text" id="p105">To use OpenAI’s library, create a client object.</li> 
   <li class="readable-text" id="p106">You can use OpenAI’s models to complete chats. Chats to complete are specified as a list of messages.</li> 
   <li class="readable-text" id="p107">Each chat message is characterized by content and a role. Roles can be one of <code>user</code>, <code>assistant</code>, or <code>system</code>.</li> 
   <li class="readable-text" id="p108">Obtain chat completions via the <code>chat.completions.create</code> function.</li> 
   <li class="readable-text" id="p109">You can configure models using various parameters:<br/>The <code>max_tokens</code> parameter limits the number of tokens generated.<br/><code>stop</code> lets you define phrases that stop text generation.<br/>You can penalize or encourage specific tokens via <code>logit_bias</code>.<br/><code>presence_penalty</code> penalizes repetitive output.<br/><code>frequency_penalty</code> penalizes repetitive output.<br/><code>temperature</code> chooses the degree of randomization.<br/><code>top_p</code> determines the number of output tokens considered.<br/><code>n</code> chooses the number of generated completions.</li> 
  </ul>
 </div></div></body></html>