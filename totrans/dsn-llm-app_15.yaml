- en: Chapter 12\. Retrieval-Augmented Generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 10](ch10.html#ch10), we demonstrated how to vastly expand the capabilities
    of LLMs by interfacing them with external data and software. In [Chapter 11](ch11.html#chapter_llm_interfaces),
    we introduced the concept of embedding-based retrieval, a foundational technique
    for retrieving relevant data from data stores in response to queries. Armed with
    this knowledge, let’s explore the application paradigm of augmenting LLMs with
    external data, called retrieval-augmented generation (RAG), in a holistic fashion.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will take a comprehensive view of the RAG pipeline, diving
    deep into each of the steps that make up a typical workflow of a RAG application.
    We will explore the various decisions involved in operationalizing RAG, including
    what kind of data we can retrieve, how to retrieve it, and when to retrieve it.
    We will highlight how RAG can help not only during model inference but also during
    model training and fine-tuning. We will also compare RAG with other paradigms
    and discuss scenarios where RAG shines in comparison to alternatives or vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: The Need for RAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As introduced in [Chapter 10](ch10.html#ch10), RAG is an umbrella term used
    to describe a variety of techniques for using external data sources to augment
    the capabilities of an LLM. Here are some reasons we might want to use RAG:'
  prefs: []
  type: TYPE_NORMAL
- en: We need the LLMs to access our private/proprietary data, or data that was not
    part of the LLM’s pre-training datasets. Using RAG is a much more lightweight
    option than pre-training an LLM on our private data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To reduce the risk of hallucinations, we would like the LLM to refer to data
    provided through a retrieval mechanism rather than rely on its own internal knowledge.
    RAG facilitates this. RAG also enables more accurate data citations, connecting
    LLM outputs to their ground-truth sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We would like the LLM to answer questions about recent events and concepts that
    have emerged after the LLM was pre-trained. While there are memory editing techniques
    for updating LLM parameters with new knowledge like [MEMIT](https://oreil.ly/kxI3j),
    they are not yet reliable or scalable. As discussed in [Chapter 7](ch07.html#ch07),
    continually training an LLM to keep its knowledge up-to-date is expensive and
    risky.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We would like the LLM to answer queries involving long-tail entities, which
    occur only rarely in the pre-training datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typical RAG Scenarios
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have seen *why* we need RAG, let’s explore *where* we can utilize
    it. The four most popular scenarios are:'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving external knowledge
  prefs: []
  type: TYPE_NORMAL
- en: This is the predominant use case that has seen a lot of success with productionization.
    As discussed earlier in the chapter, we can use RAG to plug LLM knowledge gaps
    or to reduce hallucination risk.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving context history
  prefs: []
  type: TYPE_NORMAL
- en: LLMs have a limited context window, but often we need access to more context
    in order to answer a query than what fits in the context window. We would also
    like to have longer conversations with the LLM than what fits in the context window.
    In these cases, we could retrieve parts of the conversation history or session
    context when needed.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving in-context training examples
  prefs: []
  type: TYPE_NORMAL
- en: Few-shot learning is an effective approach to help LLMs get acquainted with
    the input-output mapping of a task. You can make few-shot learning more effective
    by dynamically selecting few-shot examples based on the current input. The few-shot
    examples can be retrieved from a training example data store at inference time.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieving tool-related information
  prefs: []
  type: TYPE_NORMAL
- en: As described in [Chapter 10](ch10.html#ch10), LLMs can invoke software tools
    as part of their workflow. The list of tools available and their description is
    stored in a tool store. The LLM can then use retrieval for tool selection, selecting
    the tool best suited to the task. Tool-related information can also include API
    documentation, for instance.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding When to Retrieve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For each step in an agentic workflow, the LLM can advance its task using one
    of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Use its internal capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose from among several data stores
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose from among several software tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There can be tasks that the LLM can fully solve using its parametric memory,
    but one or more data stores may also contain the requisite data needed to solve
    them. In these cases, should we just default to using RAG, given all its benefits
    that we presented earlier?
  prefs: []
  type: TYPE_NORMAL
- en: We have seen earlier in the chapter that LLMs struggle with long-tail information,
    and that RAG can be an effective means to answer questions about long-tail entities.
    However, [Mallen et al.](https://oreil.ly/MF7Y1) show that for queries about more
    popular entities, the LLM might sometimes be better at answering queries than
    RAG. This is because of the inevitable limitations of the retrieval model, which
    might retrieve irrelevant or incorrect information that could mislead the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a given query, you can dynamically determine whether to use retrieval or
    to rely on the LLM’s parametric memory. The rules determining the right approach
    to take include:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether the query is about a more frequently occurring entity. For example,
    the LLM is more likely to memorize the birthday of Taylor Swift than of a substitute
    drummer of a local band whose Wikipedia page is a stub.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the query has timeliness constraints, i.e., if the data needed to address
    the query may not have existed before the LLM’s knowledge cutoff date.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Whether the model has been continually pre-trained or memory tuned as described
    in [Chapter 7](ch07.html#ch07), and the given query relates to concepts over which
    the training was performed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are using LLMs for general-purpose question answering, Mallen et al.
    show that you can use sources like Wikipedia as a pseudo-popularity metric for
    entities. If the entities present in your inputs have an entity count in Wikipedia
    greater than a threshold, then the LLM can choose to answer the question on its
    own without using RAG. Note that the threshold can change across LLMs. This strategy
    works only if you have a good understanding about the datasets the LLM has been
    pre-trained on.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamically deciding when to retrieve data can also help optimize the model’s
    latency and responsiveness, as the RAG pipeline will introduce additional overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dynamic retrieval is mostly useful when you are using very large LLMs. For smaller
    models (7B or below), it is almost always beneficial to prefer using RAG rather
    than relying on the LLM’s internal memory.
  prefs: []
  type: TYPE_NORMAL
- en: The RAG Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A typical RAG application follows the *retrieve-read* framework, as discussed
    in [Chapter 11](ch11.html#chapter_llm_interfaces). In response to a query, a retrieval
    model identifies documents that are relevant to answering the query. These documents
    are then passed along to the LLM as context, which the LLM can rely on in addition
    to its internal capabilities to generate a response. In practice, we typically
    need to add a lot of bells and whistles to get RAG working in a production context.
    This involves adding several more optional stages to the retrieve-read framework.
    In practice, your pipeline stages might consist of a *rewrite-retrieve-read-refine-insert-generate*
    workflow, with some of these steps potentially comprising multiple stages. Later
    in the chapter, we will go through each of the steps in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-1](#RAG-pipeline) shows the various stages of the RAG pipeline and
    the components involved.'
  prefs: []
  type: TYPE_NORMAL
- en: '![RAG-pipeline](assets/dllm_1201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-1\. RAG pipeline
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As in the rest of the book, we refer to user or LLM requests to retrieve data
    as queries, and units of text retrieved from the data store as documents.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s illustrate with an example. Consider a RAG application that answers questions
    about Canadian politics and parliamentary activity. The application has access
    to a knowledge base containing transcripts of parliamentary proceedings. We will
    assume that the data is represented using the representation techniques described
    in [Chapter 11](ch11.html#chapter_llm_interfaces).
  prefs: []
  type: TYPE_NORMAL
- en: When a user issues a query, we might want to rephrase it before sending it to
    the retriever. Traditionally in the field of information retrieval (IR), this
    is referred to as query expansion. Query expansion is especially useful because
    of the vocabulary mismatch between the query space and the document space. The
    user might use different terminology in the query than that used in the documents.
    Rephrasing a query can help bridge the vocabulary gap. In general, we would like
    to rephrase the query in such a way that it improves the chances of the retriever
    fetching the most relevant documents. This stage is called the *rewrite* stage.
  prefs: []
  type: TYPE_NORMAL
- en: Next, in the *retrieve* stage, a retrieval model is used to retrieve the documents
    relevant to the query. In [Chapter 11](ch11.html#chapter_llm_interfaces), we discussed
    embedding-based retrieval, a popular retrieval paradigm in the LLM era. The retrieval
    stage can be an extensive multi-stage pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The retrieval can happen over a very large document space. In this case, it
    is computationally infeasible to use more advanced retrieval models. Therefore,
    retrieval is usually carried out in a two-step process, with the first step using
    faster methods (these days, typically embedding-based) to retrieve a list of potentially
    relevant documents (optimizing recall), and a second step that reranks the retrieved
    list based on relevance (optimizing precision) so that the top-k ranked documents
    are then taken as the context to be passed along to the LLM. This stage is called
    the *rerank* stage.
  prefs: []
  type: TYPE_NORMAL
- en: After identifying the top-k documents relevant to the query, they need to be
    passed along to the LLM. However, the documents may not fit into the context window
    and thus need to be shortened. They also could potentially be rephrased in a way
    that makes it more likely for the LLM to use the context to generate the answer.
    This is done during the *refine* stage.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we provide the output of the refine step to the LLM. The default approach
    is to concatenate all the documents in the prompt. However, you could also pass
    them one at a time, and then ensemble the results. How the documents are ordered
    in the prompt can also make a difference. Several such techniques determine the
    way the context is fed to the LLM. This is called the *insert* stage.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, in the *generate* stage, the LLM reads the prompt containing the query
    and the context and generates the response. The generation can happen all at once
    or the retrieval process can be interleaved with the generation, i.e., the model
    can generate a few tokens, then call the retrieval model again to retrieve additional
    content, generate a few more tokens, and then call the retrieval model again,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The output of each stage can be run through a *verify* stage to assess the quality
    of the outputs and even take corrective measures. The verify stage can employ
    either heuristics or AI-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, the query was generated by a human user. But if we consider
    RAG in the context of agentic workflows, the query might be generated by an LLM.
    In an agentic workflow, the agent can determine at any given point that it needs
    to retrieve data to progress with its task, which sets the aforementioned pipeline
    into motion.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the retrieve and generate steps, the rest of the pipeline is optional,
    and including other steps depends on your performance and latency tradeoffs.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Our example pertains to RAG when used at inference time. RAG can also be applied
    when pre-training or fine-tuning the model, which we will describe later in the
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s examine each step in the pipeline in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Rewrite
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After a query is issued, it might need to be rewritten to make it more amenable
    to retrieval. The rewriting process depends on the retrieval models used. As mentioned
    before, there is usually a mismatch between the query space and the document space,
    as the vocabulary, phrasing, and semantics used by the query might vary drastically
    from how the relevant concepts are conveyed in the document.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, consider the query: “Which politicians have complained about
    the budget not being balanced?”'
  prefs: []
  type: TYPE_NORMAL
- en: 'and the data store contains the text “Senator Paxton: ‘I just can’t stand the
    sight of our enormous deficit.’”'
  prefs: []
  type: TYPE_NORMAL
- en: If you are using traditional retrieval approaches that rely more on keywords,
    this text may not be selected as relevant during retrieval. Using embedding-based
    methods bridges the gap as embeddings of similar sentences are closer to each
    other in embedding space, but it does not entirely solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If the query is coming from the user, the user might add instructions along
    with the query, like, “Which politicians have complained about the budget not
    being balanced? Provide the results in the form of a table.” In this case you
    will have to separate the query from the instructions before feeding the query
    into the retrieval pipeline. This can be done by an LLM using prompting techniques
    like CoT, ReAct, etc., which we discussed in Chapters [5](ch05.html#chapter_utilizing_llms)
    and [10](ch10.html#ch10), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: For systems using traditional retrieval techniques, query rewriting is typically
    performed using query expansion techniques, in which the query is augmented with
    similar keywords. Basic query expansion techniques include adding synonyms of
    keywords and other topic information in your query.
  prefs: []
  type: TYPE_NORMAL
- en: A well-tested method for query expansion is pseudo-relevance feedback (PRF).
    In PRF, the original query is used to retrieve documents, and salient terms from
    these documents are extracted and added to the original query.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how PRF would help with our query, ‘‘Which politicians have complained
    about the budget not being balanced?” We use a retrieval technique like BM25 (explained
    later in the chapter) to return a candidate set of k documents. We then use a
    technique like term frequency or, more effectively, [Tf-IDf](https://oreil.ly/5be9z)
    to extract the salient terms occurring in these returned documents. For this example
    the salient phrases turn out to be “fiscal policy,” “deficit,” “financial mismanagement,”
    and “budgetary reforms.” Adding these phrases to the original query will lead
    to the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '“Senator Paxton: ‘I just can’t stand the sight of our enormous deficit!’” being
    retrieved successfully.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In recent years, LLM-driven query expansion methods are gaining more prominence.
    Two such examples are [Query2doc](https://oreil.ly/BDJAN) and [Hypothetical Document
    Embeddings (HyDE)](https://oreil.ly/tZyUV).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore how LLM-based query expansion works. We pass a given query to
    an LLM and ask it to generate a hypothetical document that contains the answer
    to the query.
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt:* For the provided query, generate a hypothetical document in 2 sentences
    or less that answers the query. Assume the documents are transcripts of Canadian
    parliamentary proceedings, containing verbatim statements of representatives in
    the House and Senate.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Query:* Which politicians have complained about the budget not being balanced?'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The prompt contains:'
  prefs: []
  type: TYPE_NORMAL
- en: Instructions to generate a hypothetical document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Desired length limit of the generated document
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Description of the document corpus containing relevant documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The original query
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The hypothetical document generated by the LLM is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*LLM:* “Honorable Senator Mark Wellington stated, *This government’s failure
    to balance the budget is unacceptable and jeopardizes our economic future.* Similarly,
    MP Emily Fraser remarked, *We cannot continue on this path of reckless spending
    without addressing the deficit—it’s time for real fiscal responsibility.*”'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Using this hypothetical document as the query will return the sentence ‘‘Senator
    Paxton: ‘I just can’t stand the sight of our enormous deficit!”’ with a higher
    similarity score.'
  prefs: []
  type: TYPE_NORMAL
- en: While this hypothetical document is not factually accurate, and laughably so
    (there are no senators or MPs in Canada named Mark Wellington and Emily Fraser),
    it does contain verbiage and semantics very similar to what an actual politician
    would say. If we use this document as the query (optionally combining with the
    original query), then the chances of it being semantically similar to actual instances
    of politicians talking about the topic is higher than if matched with the query
    alone.
  prefs: []
  type: TYPE_NORMAL
- en: The length of the hypothetical document could be similar to the typical length
    of the retrieval unit. You can use a smaller LLM to generate the hypothetical
    document, as we do not care for factuality guarantees in this setting. However,
    smaller models are also not as adept as generating quality hypothetical documents,
    so you will have to manage the tradeoff. Both LangChain and LlamaIndex provide
    implementations of hypothetical document-based query rewriting.
  prefs: []
  type: TYPE_NORMAL
- en: If the model has been pre-trained or fine-tuned on the data corpus containing
    the relevant data, then adding descriptions of the corpus in the prompt as shown
    in the example will make it more likely that the generated document follows the
    structure, format, and linguistics of that data corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One pitfall of query rewriting techniques is the risk of topic drift. In the
    case of hypothetical documents, the document may drift into irrelevant topics
    after the first few tokens. Upweighting the logits bias for tokens in the query
    can partially address this problem. PRF techniques are also susceptible to topic
    drift.
  prefs: []
  type: TYPE_NORMAL
- en: You can also combine PRF style techniques with hypothetical documents. Instead
    of generating hypothetical documents to replace or augment the query, you can
    use them to extract keywords that you can add to the original query. [Li et al.](https://oreil.ly/cOnMs)
    propose a technique called *query2document2keyword*. In this technique, the LLM
    generates a hypothetical document using the query, similar to HyDE. The LLM is
    then prompted to extract salient keywords from this document.
  prefs: []
  type: TYPE_NORMAL
- en: We can then further improve the quality of the extracted keywords by taking
    them through a filtering step. The authors propose using the *self-consistency*
    method, which we discussed in [Chapter 5](ch05.html#chapter_utilizing_llms). To
    recap, in the self-consistency method, we repeat the keyword generation multiple
    times, and then select the top keywords based on the number of generations they
    are present in.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to combine traditional retrieval with LLM-driven query rewriting
    is to first return the top-k documents from the initial retrieval step, then use
    LLMs to generate salient keywords from the returned documents and add them to
    the query.
  prefs: []
  type: TYPE_NORMAL
- en: So far we have discussed techniques that bridge the query document mismatch
    problem by modifying the query and bringing it closer to the document space. An
    alternative approach to solve the mismatch problem is to represent the documents
    in a way that brings them closer to the query space. Examples of this approach
    include [doc2query](https://oreil.ly/CGUtP) and [contextual retrieval](https://oreil.ly/ZJuIu).
    While document rewriting techniques initially have a large cost if the data stores
    are very large, they can reduce latency during inference time as no or little
    query rewriting needs to be performed. On the other hand, query rewriting techniques
    are simple to implement and integrate into a RAG workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Yet another form of query rewriting is called query decomposition. For complex
    queries in an agentic workflow, we can have the LLM divide the task into multiple
    queries that can be executed sequentially or in parallel, depending on how the
    query was decomposed. We discussed query decomposition techniques in [Chapter 10](ch10.html#ch10).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If your external data is in a structured form like databases, then the query
    needs to be rewritten into a SQL query or equivalent, as discussed in [Chapter 10](ch10.html#ch10).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have discussed the query rewriting step of the pipeline, let’s move
    on to the retrieve step.
  prefs: []
  type: TYPE_NORMAL
- en: Retrieve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The retrieve step is the most crucial stage of the RAG pipeline. It is easy
    to see why: all RAG applications are bottlenecked by the quality of retrieval.
    Even if you are working with the world’s best language model, you won’t be able
    to get the correct results if the retrieval step didn’t retrieve the correct documents
    needed to answer the query. Therefore, this step of the pipeline should focus
    on increasing recall.'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding-based retrieval, which we discussed in detail in [Chapter 11](ch11.html#chapter_llm_interfaces),
    is highly popular. However, traditional information-retrieval techniques should
    not be dismissed. The right technique to use depends on the expected nature of
    queries (can a significant proportion of them be answered by just keyword or regex
    match?), the expected degree of query-document vocabulary mismatch, latency and
    compute limitations, and performance requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The information retrieval (IR) research field has been studying these problems
    for a long time. Now that retrieval is more relevant than ever in NLP, I am noticing
    a lot of efforts to reinvent the wheel rather than reusing IR insights. For insights
    in retrieval research, check out papers from leading IR research conferences like
    SIGIR, ECIR, TREC, etc.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding-based retrieval methods are not always suitable when you would like
    all documents containing a specific word or phrase to be retrieved. Therefore
    it is customary to combine keyword-based methods with embedding methods, called
    hybrid search. The results from the two methods are combined and fed to the next
    step of the retrieval pipeline. Most vector databases support hybrid search in
    some shape or form.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 12-2](#hybrid-search) shows the retrieval stage in action, using hybrid
    search.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hybrid-Search](assets/dllm_1202.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-2\. Hybrid search
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I also highly recommend metadata filters for improving retrieval. The more metadata
    you gather during the data representation and storage phase, the better the retrieval
    results. For example, if you have performed topic modeling of your data store
    in advance, you can restrict your search results to a subset of topics, with the
    filters being applied either using a hardcoded set of rules or determined by an
    LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss promising recent advances in retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: Generative retrieval
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What if the LLM could identify the right documents(s) that need to be retrieved
    in response to a query, thus removing the need for retrieval techniques? This
    is called generative retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative retrieval is implemented by assigning identifiers to documents called
    docIDs, and teaching the LLM the association between documents and docIDs. A document
    can be associated with one or more docIDs. Typical docIDs can be:'
  prefs: []
  type: TYPE_NORMAL
- en: Single tokens
  prefs: []
  type: TYPE_NORMAL
- en: Each document can be represented by a new token in the vocabulary. This means
    that, during inference, the model needs to output only a single token for each
    document it wants to retrieve. [Pradeep et al.](https://oreil.ly/7JYOM) use a
    T5 model where the encoder vocabulary is the standard T5 vocabulary but the decoder
    vocabulary contains the docIDs. This approach is feasible only with a small document
    corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Prefix/subset tokens
  prefs: []
  type: TYPE_NORMAL
- en: '[Tay et al.](https://oreil.ly/1p1C8) use the first 64 tokens of a document
    as the docID, while [Wang et al.](https://oreil.ly/lg3g3) use 64 randomly selected
    contiguous tokens from the document.'
  prefs: []
  type: TYPE_NORMAL
- en: Cluster tokens
  prefs: []
  type: TYPE_NORMAL
- en: You can also perform hierarchical clustering of your document corpus based on
    its semantics (using embeddings, for example), and the docID can be a concatenation
    of the cluster IDs at each level of the hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: Salient keyword tokens
  prefs: []
  type: TYPE_NORMAL
- en: The docIDs can also contain salient keywords representing the topics and themes
    contained in the document. For example, a document about the Transformer architecture
    can be represented by the docID “transformer_self-attention_architecture.”
  prefs: []
  type: TYPE_NORMAL
- en: One way to teach the LLM the association between documents and docIDs is by
    fine-tuning the model. This is referred to as training-based indexing. However,
    fine-tuning needs a lot of resources and is not suitable in scenarios in which
    new documents are frequently added to the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: '[Askari et al.](https://oreil.ly/K5TAB) show that we can use few-shot learning
    to build a generative retrieval system without needing to train the model. First,
    for each document in the corpus, pseudo queries are generated using a language
    model. The pseudo queries are the queries whose answers are present in the document.
    These pseudo queries are then fed to a language model in a few-shot setting and
    asked to generate docIDs. [Figure 12-3](#generative-retrieval) shows training-free
    generative retrieval in action.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Generative-Retrieval](assets/dllm_1203.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12-3\. Generative retrieval
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: During inference, the model is provided with a query similar to the setup in
    [Figure 12-3](#generative-retrieval) and asked to generate the correct docID(s)
    that are relevant to the query. Constrained beam search is used to ensure that
    the docID generated by the model corresponds to a valid docID in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also use generative retrieval to retrieve documents based on their metadata.
    For example, the model could ask to retrieve Apple’s 2024 annual report. The model
    can be made to generate the right identifier by either fine-tuning the model or
    using few-shot learning, as shown in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, generative retrieval is suitable only if your document corpus is
    relatively small, there is limited redundancy within the corpus, or the documents
    belong to a set of well-defined categories (annual reports of all public companies
    in the US, for instance).
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s discuss tightly-coupled retrievers, another new topic in the retrieval
    space.
  prefs: []
  type: TYPE_NORMAL
- en: Tightly-coupled retrievers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As seen in [Chapter 11](ch11.html#chapter_llm_interfaces), in embedding-based
    retrieval, the embedding model is typically independent of the language model
    to which the retrieval results are fed. We will refer to them as *loosely-coupled*
    retrievers.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, a *tightly-coupled* retriever is trained such that it learns from
    LLM feedback; the model learns to retrieve text that best positions the LLM to
    generate the correct output for a given query. Tightly-coupled retrievers can
    be trained together with the generator LLM as part of a single architecture, or
    they can be trained separately using feedback from the trained LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of the latter is [Zhang et al.’s LLM-Embedder](https://oreil.ly/Q__8M),
    a unified embedding model that can support a variety of retrieval needs in a single
    model, ranging from knowledge retrieval to retrieving optimal few-shot examples.
    The model is trained from two types of signals: a contrastive learning setup typically
    used to train embedding models (presented in [Chapter 11](ch11.html#chapter_llm_interfaces))
    and LLM feedback. A retrieval candidate receives a larger reward from LLM feedback
    if it improves the performance of the LLM in answering the query.'
  prefs: []
  type: TYPE_NORMAL
- en: Tightly-coupled retrievers are another tool in your toolkit for improving retrieval.
    They are by no means a necessary step in the RAG pipeline. As always, experimentation
    will show how much of a lift (if any) they provide for your application.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s discuss GraphRAG, an up-and-coming retrieval paradigm that leverages
    knowledge graphs for better retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: GraphRAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A key limitation of the retrieval approaches we have discussed so far is their
    inability to facilitate answering questions that require drawing connections between
    different parts of the document corpus, as well as questions that involve summarizing
    high-level themes across the dataset. For example, all the retrieval techniques
    we discussed so far would do poorly on a query like, “What are the key topics
    discussed in this dataset?”
  prefs: []
  type: TYPE_NORMAL
- en: One way to address these limitations is by employing knowledge graphs. Microsoft
    released [GraphRAG](https://oreil.ly/V4n_S), a graph-based RAG system. GraphRAG
    works by creating a knowledge graph from the underlying data corpus by extraction
    entities and relationships. The graph is then used to perform hierarchical semantic
    clustering, with summaries generated for each cluster. These summaries enable
    answering of thematic questions like, “What are the key topics discussed in this
    dataset?”
  prefs: []
  type: TYPE_NORMAL
- en: GraphRAG requires a lot of initial compute to prepare the knowledge graph. This
    can be prohibitive for larger datasets. While it is easy to extract entities,
    extracting relevant relationships is harder.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explored the retrieval stage of the RAG pipeline, let’s move
    on to the rerank stage.
  prefs: []
  type: TYPE_NORMAL
- en: Rerank
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The retrieval process can be broken into a two-stage or multi-stage process,
    where the initial stage retrieves a list of documents relevant to the query, followed
    by one or more *reranking* stages that take the documents and sort them by relevance.
    The reranker is generally a more complex model than the retriever and thus is
    run only on the retrieved results (or else we would have just used the reranker
    as the retriever).
  prefs: []
  type: TYPE_NORMAL
- en: The reranker is usually a language model fine-tuned on the specific use case.
    You can use BERT-like models for building a relevance classifier, where given
    a query and a document, the model outputs the probability of the document being
    relevant to answering the query. These models are called *cross-encoders*, as
    in these models the query and document are encoded together, as opposed to embedding-based
    retrieval models we have discussed, called bi-encoders, where the query and document
    are encoded as separate vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input for a BERT model acting as a cross-encoder is of the format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The Sentence Transformers library provides access to cross-encoders, which
    can be used as rerankers in the RAG pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]`` `''On September 22, 2023, I lined up at the Central Park store for
    the launch of` [PRE2] `ranks` `=` `model``.``rank``(``query``,` `documents``)`
    `for` `rank` `in` `ranks``:`    `print``(``rank``[``''score''``],` `documents``[``rank``[``''corpus_id''``]])`
    [PRE3]` [PRE4]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5] ``Because we have set `num_labels = 1`, the model will treat it as a
    regression task, using the sigmoid activation function to output a score between
    0 and 1.    These days, more advanced models like [Contextualized Late Interaction
    over BERT (ColBERT)](https://oreil.ly/N3fOv) are used for reranking. As opposed
    to the cross-encoder setup we just discussed, ColBERT-style models allow for pre-computation
    of document representations, leading to faster inference.    In ColBERT, the query
    and documents are encoded separately using BERT, generating token-level embedding
    vectors for each token in the query and documents. For each token in the query,
    the corresponding embedding is compared to the embeddings of each of the token
    embeddings of the document, generating similarity scores. The maximum similarity
    scores for each query token are summed, resulting in the final relevance score.
    This type of architecture is called *late interaction*, since the query and document
    are not encoded together but interact together only later in the process. Late
    interaction saves time compared to traditional cross-encoders, as document embeddings
    can be created and stored in advance.    [Figure 12-4](#cross-encoders) depicts
    a ColBERT model in action, illustrating the late interaction between query and
    documents.  ![cross-encodersl](assets/dllm_1204.png)  ###### Figure 12-4\. ColBERT    Next,
    let’s explore a few advanced reranking techniques.    ### Query likelihood model
    (QLM)    A QLM estimates the probability of generating the query given a candidate
    document as input. You can treat an LLM as a QLM, utilizing its zero-shot capabilities
    to rank candidate documents based on the query token probabilities. Alternatively,
    you can fine-tune an LLM on query generation tasks to improve its suitability
    as a QLM.    A typical prompt for a QLM would look like: “Generate a question
    that is most relevant to the given document <document content>”.    After getting
    the top-k documents relevant to a query from the retrieval stage, each document
    is fed to the LLM with this prompt. The likelihood of the query tokens is then
    calculated using the model logits. The documents are then sorted by likelihood,
    providing a relevance ranking.    ###### Warning    [Zhuang et al.](https://oreil.ly/QnWWh)
    show that an instruction-tuned model that doesn’t contain query generation tasks
    in its instruction-tuning training set loses its capability to be an effective
    zero-shot QLM. This is yet another case of instruction-tuned models exhibiting
    degraded performance compared to base models, on tasks they have not been trained
    on.    Note that to calculate the probability of the query tokens, we need access
    to the model logits. Most proprietary model providers including OpenAI do not
    yet provide full access to the model logits as of this book’s writing. Thus, the
    LLM-as-a-QLM approach can be implemented only using open source models.    In
    the interest of reducing latency, you would ideally like the QLM to be as small
    a model as possible. However, smaller models are less effective QLMs. Effectively
    fine-tuning a smaller LLM for query generation might be the sweet spot.    ###
    LLM distillation for ranking    Earlier in the chapter, we saw how encoder-only
    models like BERT could serve as rerankers. More recently, decoder LLMs are also
    being trained to directly rank candidate documents in three ways:    Pointwise
    ranking      Each candidate document is fed separately to the LLM. The LLM provides
    a Boolean judgment on its relevance. Alternatively, it can also provide a numerical
    score, although this is much less reliable.      Pairwise ranking      For each
    candidate document pair, the LLM indicates which document is more relevant. To
    get a complete ranking, N² such comparisons need to be made.      Listwise ranking      All
    the candidate documents are tagged with identifiers and fed to the LLM, and the
    LLM is asked to generate a ranked list of identifiers according to decreasing
    order of relevance of corresponding documents.      In general, pointwise ranking
    is the easiest to use but may not be the [most effective](https://oreil.ly/DvmtC).
    Listwise ranking might need a large context window, while pairwise ranking needs
    lots of comparisons. Pairwise ranking is the most effective of these techniques,
    since it involves direct comparison. [Figure 12-5](#llm-rerankers) shows how pointwise,
    pairwise, and listwise rankings work.    Examples of ranking LLMs include [RankGPT](https://oreil.ly/6XoOG),
    [RankVicuna](https://oreil.ly/00Dan), and [RankZephyr](https://oreil.ly/AAbUE).    These
    models are trained by distilling from larger LLMs, a technique we first learned
    in [Chapter 9](ch09.html#ch09). For example, the process for training RankVicuna
    is:    *   Queries in the training set are fed through a first-level retriever
    like BM25 to generate a list of candidate documents.           *   This list is
    passed to a larger LLM, which generates a rank-ordered list of candidates.           *   The
    query and the rank-ordered list are used to fine-tune the smaller LLM.              The
    creators of [RankVicuna](https://oreil.ly/cFLSc) show that as the effectiveness
    of the first-level retrieval increases, the possible performance gains from RankVicuna
    decreases due to diminished returns. They also reported that augmenting the dataset
    by shuffling the input order of the candidate documents improved model performance.  ![llm-rerankers](assets/dllm_1205.png)  ######
    Figure 12-5\. Decoder LLM rerankers    ###### Tip    You can combine the results
    of the retrieve and the rerank stages to get the final relevance ranking of candidate
    documents. This is needed to enforce keyword weighting, for example. You can also
    weight your relevance ranking by metadata like published date (more recent documents
    are weighted more).    Now that we have discussed the rerank stage, let’s move
    on to the refine step of the RAG pipeline.`` [PRE6]`  [PRE7]  [PRE8]'
  prefs: []
  type: TYPE_NORMAL
