<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">12 <a id="idTextAnchor000"/><a id="idTextAnchor001"/>Training a Transformer to generate text</h1>
<p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="idIndexMarker001"/><a id="marker-264"/></p>
<ul class="calibre5">
<li class="co-summary-bullet">Building a scaled-down version of the GPT-2XL model tailored to your needs</li>
<li class="co-summary-bullet">Preparing data for training a GPT-style Transformer</li>
<li class="co-summary-bullet">Training a GPT-style Transformer from scratch</li>
<li class="co-summary-bullet">Generating text using the trained GPT model</li>
</ul>
<p class="body">In chapter 11, we developed the GPT-2XL model from scratch but were unable to train it due to its vast number of parameters. Training a model with 1.5 billion parameters requires supercomputing facilities and an enormous amount of data. Consequently, we loaded pretrained weights from OpenAI into our model and then used the GPT-2XL model to generate text.</p>
<p class="body">However, learning how to train a Transformer model from scratch is crucial for several reasons. First, while this book doesn’t directly cover fine-tuning a pretrained model, understanding how to train a Transformer equips you with the skills needed for fine-tuning. Training a model involves initializing parameters randomly, whereas fine-tuning involves loading pretrained weights and further training the model. Second, training or fine-tuning a Transformer enables you to customize the model to meet your specific needs and domain, which can significantly enhance its performance and relevance for your use case. Finally, training your own Transformer or fine-tuning an existing one provides greater control over data and privacy, which is particularly important for sensitive applications or handling proprietary data. In summary, mastering the training and fine-tuning of Transformers is essential for anyone looking to harness the power of language models for specific applications while maintaining privacy and control.</p>
<p class="body">Therefore, in this chapter, we’ll construct a scaled-down version of the GPT model with approximately 5 million parameters. This smaller model follows the architecture of the GPT-2XL model; the significant differences are its composition of only 3 decoder blocks and an embedding dimension of 256, compared to the original GPT-2XL’s 48 decoder blocks and an embedding dimension of 1,600. By scaling down the GPT model to about 5 million parameters, we can train it on a regular computer.</p>
<p class="body">The generated text’s style will depend on the training data. When training a model from scratch for text generation, both text length and variation are crucial. The training material must be extensive enough for the model to learn and mimic a particular writing style effectively. At the same time, if the training material lacks variation, the model may simply replicate passages from the training text. On the other hand, if the material is too long, training may require excessive computational resources. Therefore, we will use three novels by Ernest Hemingway as our training material: <i class="fm-italics">The Old Man and the Sea</i>, <i class="fm-italics">A Farewell to Arms</i>, and <i class="fm-italics">For Whom the Bell Tolls</i>. This selection ensures that our training data has sufficient length and variation for effective learning without being so long that training becomes impractical.<a id="marker-265"/></p>
<p class="body">Since GPT models cannot process raw text directly, we will first tokenize the text into words. We will then create a dictionary to map each unique token to a different index. Using this dictionary, we will convert the text into a long sequence of integers, ready for input into a neural network.</p>
<p class="body">We will use sequences of 128 indexes as input to train the GPT model. As in chapters 8 and 10, we will shift the input sequence by one token to the right and use it as the output. This approach forces the model to predict the next word in a sentence based on the current token and all previous tokens in the sequence.</p>
<p class="body">A key challenge is determining the optimal number of epochs for training the model. Our goal is not merely to minimize the cross-entropy loss in the training set, as doing so could lead to overfitting, where the model simply replicates passages from the training text. To tackle this problem, we plan to train the model for 40 epochs. We will save the model at 10-epoch intervals and evaluate which version can generate coherent text without merely copying passages from the training material. Alternatively, one could potentially use a validation set to assess the performance of the model and decide when to stop training, as we did in chapter 2.</p>
<p class="body">Once our GPT model is trained, we will use it to generate text autoregressively, as we did in chapter 11. We’ll test different versions of the trained model. The model trained for 40 epochs produces very coherent text, capturing Hemingway’s distinctive style. However, it may also generate text partly copied from the training material, especially if the prompt is similar to passages in the training text. The model trained for 20 epochs also generates coherent text, albeit with occasional grammatical errors, but is less likely to directly copy from the training text.</p>
<p class="body">The primary goal of this chapter is not necessarily to generate the most coherent text possible, which presents significant challenges. Instead, our objective is to teach you how to build a GPT-style model from scratch, tailored to real-world applications and your specific needs. More importantly, this chapter outlines the steps involved in training a GPT model from scratch. You will learn how to select training text based on your objectives, tokenize the text and convert it to indexes, and prepare batches of training data. You will also learn how to determine the number of epochs for training. Once the model is trained, you will learn how to generate text using the model and how to avoid generating text directly copied from the training material.<a id="idIndexMarker002"/><a id="idIndexMarker003"/></p>
<h2 class="fm-head" id="heading_id_3">12.1 Building and training a GPT from scratch</h2>
<p class="body">Our objective is to master building and training a GPT model from scratch, tailored to specific tasks. This skill is crucial for applying the concepts in this book to real-world problems.<a id="idIndexMarker004"/><a id="idIndexMarker005"/><a id="marker-266"/></p>
<p class="body">Imagine you are an avid fan of Ernest Hemingway’s work and wish to train a GPT model to generate text in Hemingway’s style. How would you approach this? This section discusses the steps involved in this task.</p>
<p class="body">The first step is to configure a GPT model suitable for training. You’ll create a GPT model with a structure similar to the GPT-2 model you built in chapter 11 but with significantly fewer parameters to make training feasible in just a few hours. As a result, you’ll need to determine key hyperparameters of the model, such as sequence length, embedding dimension, number of decoder blocks, and dropout rates. These hyperparameters are crucial as they influence both the quality of the output from the trained model and the speed of training.</p>
<p class="body">Following that, you will gather the raw text of several Hemingway novels and clean it up to ensure it is suitable for training. You will tokenize the text and assign a different integer to each unique token so that you can feed it to the model. To prepare the training data, you will break down the text into sequences of integers of a certain length and use them as inputs. You will then shift the inputs one token to the right and use them as outputs. This approach forces the model to predict the next token based on the current token and all previous tokens in the sequence.</p>
<p class="body">Once the model is trained, you will use it to generate text based on a prompt. You will first convert the text in the prompt to a sequence of indexes and feed it to the trained model. The model uses the sequence to predict the most likely next token iteratively. After that, you will convert the sequence of tokens generated by the model back to text.</p>
<p class="body">In this section, we will first discuss the architecture of the GPT model for the task. After that, we will discuss the steps involved in training the model.</p>
<h3 class="fm-head1" id="heading_id_4">12.1.1 The architecture of a GPT to generate text</h3>
<p class="body">Although GPT-2 is available in various sizes, they all share a similar architecture. The GPT model we construct in this chapter follows the same structural design as GPT-2 but is significantly smaller, making it feasible to train without the need for supercomputing facilities. Table 12.1 presents a comparison between our GPT model and the four versions of the GPT-2 models. <a id="idIndexMarker006"/><a id="marker-267"/><a id="idIndexMarker007"/></p>
<p class="fm-table-caption">Table 12.1 A comparison of our GPT with different versions of GPT-2 models</p>
<table border="1" class="contenttable-1-table" id="table001" width="100%">
<colgroup class="contenttable-0-colgroup">
<col class="contenttable-0-col" span="1" width="30%"/>
<col class="contenttable-0-col" span="1" width="14%"/>
<col class="contenttable-0-col" span="1" width="14%"/>
<col class="contenttable-0-col" span="1" width="14%"/>
<col class="contenttable-0-col" span="1" width="14%"/>
<col class="contenttable-0-col" span="1" width="14%"/>
</colgroup>
<thead class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<th class="contenttable-1-th"/>
<th class="contenttable-1-th">
<p class="fm-table-head">GPT-2S</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">GPT-2M</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">GPT-2L</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">GPT-2XL</p>
</th>
<th class="contenttable-1-th">
<p class="fm-table-head">Our GPT</p>
</th>
</tr>
</thead>
<tbody class="contenttable-1-thead">
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Embedding dimension</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">768</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">1,024</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">1,280</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">1,600</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">256</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Number of decoder layers</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">12</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">24</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">36</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">48</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">3</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Number of heads</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">12</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">16</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">20</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">25</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">4</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Sequence length</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">1,024</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">1,024</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">1,024</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">1,024</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">128</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Vocabulary size</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">50,257</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">50,257</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">50,257</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">50,257</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">10,600</p>
</td>
</tr>
<tr class="contenttable-0-tr">
<td class="contenttable-1-td">
<p class="fm-table-body">Number of parameters</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">124 million</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">350 million</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">774 million</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">1,558 million</p>
</td>
<td class="contenttable-1-td">
<p class="fm-table-body">5.12 million</p>
</td>
</tr>
</tbody>
</table>
<p class="body">In this chapter, we’ll construct a GPT model with three decoder layers and an embedding dimension of 256 (meaning each token is represented by a 256-value vector after word embedding). As we mentioned in chapter 11, GPT models use a different positional encoding method than the one used in the 2017 paper “Attention Is All You Need.” Instead, we use embedding layers to learn the positional encodings for different positions in a sequence. As a result, each position in a sequence is also represented by a 256-value vector. For calculating causal self-attention, we use four parallel attention heads to capture different aspects of the meanings of a token in the sequence. Thus, each attention head has a dimension of <span class="times">256/4 = 64</span>, similar to that in GPT-2 models. For example, in GPT-2XL, each attention head has a dimension of <span class="times">1,600/25 = 64</span>.</p>
<p class="body">The maximum sequence length in our GPT model is 128, which is much shorter than the maximum sequence length of 1,024 in GPT-2 models. This reduction is necessary to keep the number of parameters in the model manageable. However, even with 128 elements in a sequence, the model can learn the relationship between tokens in a sequence and generate coherent text.</p>
<p class="body">While GPT-2 models have a vocabulary size of 50,257, our model has a much smaller vocabulary size of 10,600. It’s important to note that the vocabulary size is mainly determined by the training data, rather than being a predefined choice. If you choose to use more text for training, you may end up with a larger vocabulary.</p>
<p class="body">Figure 12.1 illustrates the architecture of the decoder-only Transformer we will create in this chapter. It is similar to the architecture of GPT-2 that you have seen in chapter 11, except that it is smaller in size. As a result, the total number of parameters in our model is 5.12 million, compared to the 1.558 billion in the GPT-2XL model that we built in chapter 11. Figure 12.1 shows the size of the training data at each step of training.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="645" src="../../OEBPS/Images/CH12_F01_Liu.png" width="602"/></p>
<p class="figurecaption">Figure 12.1 The architecture of a decoder-only Transformer, designed to generate text. The text from three Hemingway novels is tokenized and then converted into indexes. We arrange 128 indexes into a sequence, and each batch contains 32 such sequences. The input first undergoes word embedding and positional encoding, with the input embedding being the sum of these two components. This input embedding is then processed through three decoder layers. Following this, the output undergoes layer normalization and passes through a linear layer, resulting in an output size of 10,600, which corresponds to the number of unique tokens in the vocabulary.</p>
</div>
<p class="body"><a id="marker-268"/>The input to the GPT model we create consists of input embeddings, which are illustrated at the bottom of figure 12.1. We will discuss how to calculate these embeddings in detail in the next subsection. Briefly, they are the sum of word embeddings and positional encodings from the input sequence.</p>
<p class="body">The input embedding is then passed sequentially through three decoder layers. Similar to the GPT-2XL model we built in chapter 11, each decoder layer consists of two sublayers: a causal self-attention layer and a feed-forward network. Additionally, we apply layer normalization and residual connections to each sublayer. After this, the output goes through a layer normalization and a linear layer. The number of outputs in our GPT model corresponds to the number of unique tokens in the vocabulary, which is 10,600. The output of the model is the logits for the next token. Later, we will apply the softmax function to these logits to obtain the probability distribution over the vocabulary. The model is designed to predict the next token based on the current token and all previous tokens in the sequence.<a id="idIndexMarker008"/><a id="idIndexMarker009"/></p>
<h3 class="fm-head1" id="heading_id_5">12.1.2 The training process of the GPT model to generate text</h3>
<p class="body">Now that we know how to construct the GPT model for text generation, let’s explore the steps involved in training the model. We aim to provide an overview of the training process before diving into the coding aspect of the project.<a id="marker-269"/><a id="idIndexMarker010"/><a id="idIndexMarker011"/></p>
<p class="body">The style of the generated text is influenced by the training text. Since our objective is to train the model to generate text in the style of Ernest Hemingway, we’ll use the text from three of his novels: <i class="fm-italics">The Old Man and the Sea</i>, <i class="fm-italics">A Farewell to Arms</i>, and <i class="fm-italics">For Whom the Bell Tolls</i>. If we were to choose just one novel, the training data would lack variety, leading the model to memorize passages from the novel and generate text identical to the training data. Conversely, using too many novels would increase the number of unique tokens, making it challenging to train the model effectively in a short amount of time. Therefore, we strike a balance by selecting three novels and combining them as our training data.</p>
<p class="body">Figure 12.2 illustrates the steps involved in training the GPT model to generate text.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="366" src="../../OEBPS/Images/CH12_F02_Liu.png" width="455"/></p>
<p class="figurecaption">Figure 12.2 The training process for a decoder-only Transformer to generate text, Hemingway-style.</p>
</div>
<p class="body">As in the previous three chapters, the first step in the training process is to convert text into a numerical form so that we can feed the training data to the model. Specifically, we first break down the text of the three novels into tokens using word-level tokenization, as we did in chapter 8. In this case, each token is a whole word or a punctuation mark (such as a colon, a parenthesis, or a comma). Word-level tokenization is easy to implement, and we can control the number of unique tokens. After tokenization, we assign a unique index (i.e., an integer) to each token, converting the training text into a sequence of integers (see step 1 in figure 12.2).</p>
<p class="body">Next, we transform the sequence of integers into training data by first dividing this sequence into sequences of equal length (step 2 in figure 12.2). We allow a maximum length of 128 indexes in each sequence. The choice of 128 allows us to capture long-range dependencies among tokens in a sentence while keeping the model size manageable. However, the number 128 is not magical: changing the number to, say, 100 or 150 will lead to similar results. These sequences form the features (the x variable) of our model. As we did in previous chapters, we shift the input sequence one token to the right and use it as the output in the training data (the y variable; step 3 in figure 12.2).</p>
<p class="body">The pairs of input and output serve as the training data <span class="times">(x, y)</span>. In the example of the sentence “the old man and the sea,” we use indexes corresponding to “the old man and the” as the input x. We shift the input one token to the right and use the indexes for “old man and the sea” as the output y. In the first time step, the model uses “the” to predict “old.” In the second time step, the model uses “the old” to predict “man,” and so on.</p>
<p class="body">During training, you will iterate through the training data. In the forward passes, you feed the input sequence x through the GPT model (step 4). The GPT then makes a prediction based on the current parameters in the model (step 5). You compute the cross-entropy loss by comparing the predicted next tokens with the output obtained from step 3. In other words, you compare the model’s prediction with the ground truth (step 6). Finally, you will adjust the parameters in the GPT model so that in the next iteration, the model’s predictions move closer to the actual output, minimizing the cross-entropy loss (step 7). Note that the model is essentially performing a multicategory classification problem: it’s predicting the next token from all unique tokens in the vocabulary.</p>
<p class="body">You will repeat steps 3 to 7 through many iterations. After each iteration, the model parameters are adjusted to improve the prediction of the next token. We will repeat this process for 40 epochs and save the trained model after every 10 epochs. As you will see later, if we train the model for too long, it becomes overfit, memorizing passages from the training data. The generated text then becomes identical to those in the original novels. We will test ex post which version of the model generates coherent text and, at the same time, does not simply copy from the training data.<a id="idIndexMarker012"/><a id="idIndexMarker013"/><a id="idIndexMarker014"/><a id="idIndexMarker015"/><a id="marker-270"/></p>
<h2 class="fm-head" id="heading_id_6">12.2 Tokenizing text of Hemingway novels</h2>
<p class="body">Now that you understand the architecture of the GPT model and the training process, let’s begin with the first step: tokenizing and indexing the text of Hemingway’s novels.<a id="idIndexMarker016"/><a id="idIndexMarker017"/><a id="idIndexMarker018"/><a id="idIndexMarker019"/></p>
<p class="body">First, we’ll process the text data to prepare it for training. We’ll break down the text into individual tokens, as we did in chapter 8. Since deep neural networks cannot directly process raw text, we’ll create a dictionary that assigns an index to each token, effectively mapping them to integers. After that, we’ll organize these indexes into batches of training data, which will be crucial for training the GPT model in the subsequent steps.</p>
<p class="body">We’ll use word-level tokenization for its simplicity in dividing text into words, as opposed to the more complex subword tokenization that requires a nuanced understanding of linguistic structure. Additionally, word-level tokenization results in a smaller number of unique tokens than subword tokenization, reducing the number of parameters in the GPT model.</p>
<h3 class="fm-head1" id="heading_id_7">12.2.1 Tokenizing the text</h3>
<p class="body">To train the GPT model, we’ll use the raw text files of three novels by Ernest Hemingway<i class="fm-italics">: The Old Man and the Sea</i>, <i class="fm-italics">A Farewell to Arms</i>, and <i class="fm-italics">For Whom the Bell Tolls</i>. The text files are downloaded from the Faded Page website: <a class="url" href="https://www.fadedpage.com">https://www.fadedpage.com</a>. I have cleaned up the text by removing the top and bottom paragraphs that are not part of the original book. When preparing your own training text, it’s crucial to eliminate all irrelevant information, such as vendor details, formatting, and license information. This ensures that the model focuses solely on learning the writing style present in the text. I have also removed the text between chapters that are not relevant to the main text. You can download the three files OldManAndSea.txt, FarewellToArms.txt, and ToWhomTheBellTolls.txt from the book’s GitHub repository: <a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>. Place them in the /files/ folder on your computer. <a id="idIndexMarker020"/><a id="idIndexMarker021"/><a id="marker-271"/><a id="idIndexMarker022"/></p>
<p class="body">In the text file for <i class="fm-italics">The Old Man and the Sea</i>, both the opening double quote (“) and the closing double quote (”) are represented by straight double quotes ("). This is not the case in the text files for the other two novels. Therefore, we load up the text for <i class="fm-italics">The Old Man and the Sea</i> and change straight quotes to either an opening quote or a closing quote. Doing so allows us to differentiate between the opening and closing quotes. This will also aid in formatting the generated text later on: we’ll remove the space after the opening quote and the space before the closing quote. This step is implemented as shown in the following listing.</p>
<p class="fm-code-listing-caption">Listing 12.1 Changing straight quotes to opening and closing quotes</p>
<pre class="programlisting">with open("files/OldManAndSea.txt","r", encoding='utf-8-sig') as f:
    text=f.read()
text=list(text)                                             <span class="fm-combinumeral">①</span>
for i in range(len(text)):
    if text[i]=='"':
        if text[i+1]==' ' or text[i+1]=='\n':
            text[i]='"'                                     <span class="fm-combinumeral">②</span>
        if text[i+1]!=' ' and text[i+1]!='\n':
            text[i]='"'                                     <span class="fm-combinumeral">③</span>
    if text[i]=="'":
        if text[i-1]!=' ' and text[i-1]!='\n':
            text[i]='''                                     <span class="fm-combinumeral">④</span>
text="".join(text)                                          <span class="fm-combinumeral">⑤</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Loads up the raw text and breaks it into individual characters</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> If a straight double quote is followed by a space or a line break, changes it to a closing quote</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Otherwise, changes it to an opening quote</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Converts a straight single quote to an apostrophe</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Joins individual characters back to text</p>
<p class="body">If a double quote is followed by a space or a line break, we’ll change it to a closing quote; otherwise, we’ll change it to an opening quote. The apostrophe was entered as a single straight quote, and we have changed it to an apostrophe in the form of a closing single quote in listing 12.1.</p>
<p class="body">Next, we load the text for the other two novels and combine the three novels into one single file.<a id="marker-272"/></p>
<p class="fm-code-listing-caption">Listing 12.2 Combining the text from three novels</p>
<pre class="programlisting">with open("files/ToWhomTheBellTolls.txt","r", encoding='utf-8-sig') as f:
    text1=f.read()                                            <span class="fm-combinumeral">①</span>
  
with open("files/FarewellToArms.txt","r", encoding='utf-8-sig') as f:
    text2=f.read()                                            <span class="fm-combinumeral">②</span>
  
text=text+" "+text1+" "+text2                                 <span class="fm-combinumeral">③</span>
  
with open("files/ThreeNovels.txt","w", 
          encoding='utf-8-sig') as f:
    f.write(text)                                             <span class="fm-combinumeral">④</span>
print(text[:250])</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Reads the text from the second novel</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Reads the text from the third novel</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Combines the text from the three novels</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Saves the combined text in the local folder</p>
<p class="body">We load the text from the other two novels, <i class="fm-italics">A Farewell to Arms</i> and <i class="fm-italics">For Whom the Bell Tolls</i>. We then combine the text from all three novels to use as our training data. Additionally, we save the combined text in a local file named ThreeNovels.txt so that we can later verify if the generated text is directly copied from the original text.</p>
<p class="body">The output from the preceding code listing is</p>
<pre class="programlisting">He was an old man who fished alone in a skiff in the Gulf Stream and he
had gone eighty-four days now without taking a fish. In the first
forty days a boy had been with him. But after forty days without a
fish the boy's parents had told him that th</pre>
<p class="body">The output is the first 250 characters in the combined text.</p>
<p class="body">We’ll tokenize the text by using a space as the delimiter. As seen in the preceding output, punctuation marks such as periods (.), hyphens (-), and apostrophes (’) are attached to the preceding words without a space. Therefore, we need to insert a space around all punctuation marks.</p>
<p class="body">Additionally, we’ll convert line breaks (\n) into spaces so that they are not included in the vocabulary. Converting all words to lowercase is also beneficial in our setting, as it ensures that words like “The” and “the” are recognized as the same token. This step helps reduce the number of unique tokens, thereby making the training process more efficient. To address these problems, we’ll clean up the text as shown in the following listing.</p>
<p class="fm-code-listing-caption">Listing 12.3 Adding spaces around punctuation marks</p>
<pre class="programlisting">text=text.lower().replace("\n", " ")                         <span class="fm-combinumeral">①</span>
  
chars=set(text.lower())
punctuations=[i for i in chars if i.isalpha()==False
              and i.isdigit()==False]                        <span class="fm-combinumeral">②</span>
print(punctuations)
  
for x in punctuations:
    text=text.replace(f"{x}", f" {x} ")                      <span class="fm-combinumeral">③</span>
text_tokenized=text.split()
  
unique_tokens=set(text_tokenized)
print(len(unique_tokens))                                    <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Replaces line breaks with spaces</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Identifies all punctuation marks</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Inserts spaces around punctuation marks</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Counts the number of unique tokens</p>
<p class="body"><a id="marker-273"/>We use the <code class="fm-code-in-text">set()</code> method to obtain all unique characters in the text. We then use the <code class="fm-code-in-text">isalpha()</code> and <code class="fm-code-in-text">isdigit()</code> methods to identify and remove letters and numbers from the set of unique characters, leaving us with only punctuation marks.<a id="idIndexMarker023"/><a id="idIndexMarker024"/><a id="idIndexMarker025"/></p>
<p class="body">If you execute the preceding code block, the output will be as follows:</p>
<pre class="programlisting">[')', '.', '&amp;', ':', '(', ';', '-', '!', '"', ' ', ''', '"', '?', ',', ''']
10599</pre>
<p class="body">This list includes all punctuation marks in the text. We add spaces around them and break the text into individual tokens using the <code class="fm-code-in-text">split()</code> method. The output indicates that there are 10,599 unique tokens in the text from the three novels by Hemingway, a size that’s much smaller than the 50,257 tokens in GPT-2. This will significantly reduce the model size and training time.<a id="idIndexMarker026"/></p>
<p class="body">Additionally, we’ll add one more token <code class="fm-code-in-text">"UNK"</code> to represent unknown tokens. This is useful in case we encounter a prompt with unknown tokens, allowing us to convert them to an index to feed to the model. Otherwise, we can only use a prompt with the preceding 10,599 tokens. Suppose you include the word “technology” in the prompt. Since “technology” is not one of the tokens in the dictionary <code class="fm-code-in-text">word_to_int</code>, the program will crash. By including the <code class="fm-code-in-text">"UNK"</code> token, you can prevent the program from crashing in such scenarios. When you train your own GPT, you should always include the <code class="fm-code-in-text">"UNK"</code> token since it’s impossible to include all tokens in your vocabulary. To that end, we add <code class="fm-code-in-text">"UNK"</code> to the list of unique tokens and map them to indexes.</p>
<p class="fm-code-listing-caption">Listing 12.4 Mapping tokens to indexes</p>
<pre class="programlisting">from collections import Counter   
  
word_counts=Counter(text_tokenized)    
words=sorted(word_counts, key=word_counts.get,
                      reverse=True)     
words.append("UNK")                                            <span class="fm-combinumeral">①</span>
text_length=len(text_tokenized)
ntokens=len(words)                                             <span class="fm-combinumeral">②</span>
print(f"the text contains {text_length} words")
print(f"there are {ntokens} unique tokens")  
word_to_int={v:k for k,v in enumerate(words)}                  <span class="fm-combinumeral">③</span>
int_to_word={v:k for k,v in word_to_int.items()}               <span class="fm-combinumeral">④</span>
print({k:v for k,v in word_to_int.items() if k in words[:10]})
print({k:v for k,v in int_to_word.items() if v in words[:10]})</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Adds “UNK” to the list of unique tokens</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Counts the size of the vocabulary, ntokens, which will be a hyperparamter in our model</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Maps tokens to indexes</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Maps indexes to tokens</p>
<p class="body">The output from the preceding code block is</p>
<pre class="programlisting">the text contains 698207 words
there are 10600 unique tokens
{'.': 0, 'the': 1, ',': 2, '"': 3, '"': 4, 'and': 5, 'i': 6, 'to': 7, 'he': 8, 'it': 9}
{0: '.', 1: 'the', 2: ',', 3: '"', 4: '"', 5: 'and', 6: 'i', 7: 'to', 8: 'he', 9: 'it'}</pre>
<p class="body">The text from the three novels contains 698,207 tokens. After including <code class="fm-code-in-text">"UNK"</code> in the vocabulary, the total number of <i class="fm-italics">unique</i> tokens is now 10,600. The dictionary <code class="fm-code-in-text">word_to_int</code> assigns a different index to each unique token. For example, the most frequent token, the period (.), is assigned an index of 0, and the word “the” is assigned an index of 1. The dictionary <code class="fm-code-in-text">int_to_word</code> translates an index back to a token. For example, index 3 is translated back to the opening quote (“), and index 4 is translated back to the closing quote (”).<a id="marker-274"/><a id="idIndexMarker027"/></p>
<p class="body">We print out the first 20 tokens in the text and their corresponding indexes:</p>
<pre class="programlisting">print(text_tokenized[0:20])
wordidx=[word_to_int[w] for w in text_tokenized]
print([word_to_int[w] for w in text_tokenized[0:20]])</pre>
<p class="body">The output is</p>
<pre class="programlisting">['he', 'was', 'an', 'old', 'man', 'who', 'fished', 'alone', 'in', 'a', 
'skiff', 'in', 'the', 'gulf', 'stream', 'and', 'he', 'had', 'gone',
 'eighty']
[8, 16, 98, 110, 67, 85, 6052, 314, 14, 11, 1039, 14, 1, 3193, 507, 5, 8,
25, 223, 3125] </pre>
<p class="body">Next, we’ll break the indexes into sequences of equal length to use as training data.<a id="idIndexMarker028"/><a id="idIndexMarker029"/><a id="idIndexMarker030"/></p>
<h3 class="fm-head1" id="heading_id_8">12.2.2 Creating batches for training</h3>
<p class="body">We’ll use a sequence of 128 tokens as the input to the model. We then shift the sequence one token to the right and use it as the output. <a id="idIndexMarker031"/><a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="idIndexMarker034"/></p>
<p class="body">Specifically, we create pairs of <span class="times">(x, y)</span> for training purposes. Each x is a sequence with 128 indexes. We choose 128 to strike a balance between training speed and the model’s ability to capture long-range dependencies. Setting the number too high may slow down training, while setting it too low may prevent the model from capturing long-range dependencies effectively.</p>
<p class="body">Once we have the sequence x, we slide the sequence window to the right by one token and use it as the target y. Shifting the sequence by one token to the right and using it as the output during sequence generation is a common technique in training language models, including GPTs. We have done this in chapters 8 to 10. The following code block creates the training data:</p>
<pre class="programlisting">import torch
  
seq_len=128                                                 <span class="fm-combinumeral">①</span>
xys=[]
for n in range(0, len(wordidx)-seq_len-1):
    x = wordidx[n:n+seq_len]                                <span class="fm-combinumeral">②</span>
    y = wordidx[n+1:n+seq_len+1]                            <span class="fm-combinumeral">③</span>
    xys.append((torch.tensor(x),(torch.tensor(y))))         <span class="fm-combinumeral">④</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Sets the sequence length to 128 indexes</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> The input sequence x contains 128 consecutive indexes in the training text.</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Shifts x one position to the right and uses it as output y</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Adds the pair <span class="times">(x, y)</span> to the training data.</p>
<p class="body"><a id="marker-275"/>We have created a list xys to contain pairs of <span class="times">(x, y)</span> as our training data. As we did in previous chapters, we organize the training data into batches to stabilize training. We choose a batch size of 32:</p>
<pre class="programlisting">from torch.utils.data import DataLoader
  
torch.manual_seed(42)
batch_size=32
loader = DataLoader(xys, batch_size=batch_size, shuffle=True)
  
x,y=next(iter(loader))
print(x)
print(y)
print(x.shape,y.shape)</pre>
<p class="body">We print out a pair of <span class="times">x</span> and <span class="times">y</span> as an example. The output is</p>
<pre class="programlisting">tensor([[   3,  129,    9,  ...,   11,  251,   10],
        [   5,   41,   32,  ...,  995,   52,   23],
        [   6,   25,   11,  ...,   15,    0,   24],
        ...,
        [1254,    0,    4,  ...,   15,    0,    3],
        [  17,    8, 1388,  ...,    0,    8,   16],
        [  55,   20,  156,  ...,   74,   76,   12]])
tensor([[ 129,    9,   23,  ...,  251,   10,    1],
        [  41,   32,   34,  ...,   52,   23,    1],
        [  25,   11,   59,  ...,    0,   24,   25],
        ...,
        [   0,    4,    3,  ...,    0,    3,   93],
        [   8, 1388,    1,  ...,    8,   16, 1437],
        [  20,  156,  970,  ...,   76,   12,   29]])
torch.Size([32, 128]) torch.Size([32, 128])</pre>
<p class="body">Each <span class="times">x</span> and <span class="times">y</span> have a shape of (32, 128). This means that in each batch of training data, there are 32 pairs of sequences, with each sequence<a id="idTextAnchor002"/> containing 128 indexes. When an index is passed through the <code class="fm-code-in-text">nn.Embedding()</code> layer, PyTorch looks up the corresponding row in the embedding matrix and returns the embedding vector for that index, avoiding the need to create potentially very large one-hot vectors. Therefore, when x is passed through the word embedding layer, it’s as if x is first converted to a one-hot tensor with a dimension of (32, 128, 256). Similarly, when x is passed through the positional encoding layer (which is implemented by the <code class="fm-code-in-text">nn.Embedding()</code> layer), it’s as if x is first converted to a one-hot tensor with a dimension of (32, 128, 128). <a id="idIndexMarker035"/><a id="idIndexMarker036"/><a id="idIndexMarker037"/><a id="idIndexMarker038"/><a id="idIndexMarker039"/><a id="idIndexMarker040"/></p>
<h2 class="fm-head" id="heading_id_9">12.3 Building a GPT to generate text</h2>
<p class="body">Now that we have the training data ready, we’ll create a GPT model from scratch to generate text. The model we’ll build has a similar architecture as the GPT-2XL model we built in chapter 11. However, instead of having 48 decoder layers, we’ll use only 3 decoder layers. The embedding dimensions and the vocabulary size are both much smaller, as I have explained earlier in this chapter. As a result, our GPT model will have far fewer parameters than GPT-2XL.<a id="idIndexMarker041"/><a id="marker-276"/><a id="idIndexMarker042"/></p>
<p class="body">We’ll follow the same steps as those in chapter 11. Along the way, we’ll highlight the differences between our GPT model and GPT-2XL and explain the reasons for these modifications.</p>
<h3 class="fm-head1" id="heading_id_10">12.3.1 Model hyperparameters</h3>
<p class="body">The feed-forward network in the decoder block uses the Gaussian error linear unit (GELU) activation function. GELU has been shown to enhance model performance in deep learning tasks, particularly in natural language processing. This has become a standard practice in GPT models. Therefore, we define a GELU class as follows, as we did in Chapter 11:<a id="idIndexMarker043"/><a id="idIndexMarker044"/><a id="idIndexMarker045"/><a id="idIndexMarker046"/></p>
<pre class="programlisting">import torch
from torch import nn
import math
  
device="cuda" if torch.cuda.is_available() else "cpu"
class GELU(nn.Module):
    def forward(self, x):
        return 0.5*x*(1.0+torch.tanh(math.sqrt(2.0/math.pi)*\
                       (x + 0.044715 * torch.pow(x, 3.0))))</pre>
<p class="body">In chapter 11, we didn’t use a GPU even during the text generation stage, as the model was simply too large and a regular GPU would run out of memory if we loaded the model onto it.</p>
<p class="body">In this chapter, however, our model is significantly smaller. We’ll move the model to the GPU for faster training. We’ll also generate text using the model on the GPU.</p>
<p class="body">We use a <code class="fm-code-in-text">Config()</code> class to include all the hyperparameters used in the model:<a id="idIndexMarker047"/></p>
<pre class="programlisting">class Config():
    def __init__(self):
        self.n_layer = 3
        self.n_head = 4
        self.n_embd = 256
        self.vocab_size = ntokens
        self.block_size = 128 
        self.embd_pdrop = 0.1
        self.resid_pdrop = 0.1
        self.attn_pdrop = 0.1
config=Config()</pre>
<p class="body">The attributes in the <code class="fm-code-in-text">Config()</code> class are used as hyperparameters in our GPT model. We set the <code class="fm-code-in-text">n_layer</code> attribute to 3, indicating our GPT model has three decoder layers. The <code class="fm-code-in-text">n_head</code> attribute is set to 4, meaning we’ll split the query <span class="times">Q</span>, key <span class="times">K</span>, and value <span class="times">V</span> vectors into 4 parallel heads when calculating causal self-attention. The <code class="fm-code-in-text">n_embd</code> attribute is set to 256, meaning the embedding dimension is 256: each token will be represented by a 256-value vector. The <code class="fm-code-in-text">vocab_size</code> attribute is determined by the number of unique tokens in the vocabulary. As explained in the last section, there are 10,600 unique tokens in our training text. The <code class="fm-code-in-text">block_size</code> attribute is set to 128, meaning the input sequence contains a maximum of 128 tokens. We set the dropout rates to 0.1, as we did in chapter 11. <a id="idIndexMarker048"/><a id="idIndexMarker049"/><a id="idIndexMarker050"/><a id="idIndexMarker051"/><a id="idIndexMarker052"/><a id="idIndexMarker053"/></p>
<h3 class="fm-head1" id="heading_id_11">12.3.2 Modeling the causal self-attention mechanism</h3>
<p class="body"><a id="marker-277"/>The causal self-attention is defined in the same way as in chapter 11:<a id="idIndexMarker054"/><a id="idIndexMarker055"/><a id="idIndexMarker056"/><a id="idIndexMarker057"/></p>
<pre class="programlisting">import torch.nn.functional as F
class CausalSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)
        self.c_proj = nn.Linear(config.n_embd, config.n_embd)
        self.attn_dropout = nn.Dropout(config.attn_pdrop)
        self.resid_dropout = nn.Dropout(config.resid_pdrop)
        self.register_buffer("bias", torch.tril(torch.ones(\
                   config.block_size, config.block_size))
             .view(1, 1, config.block_size, config.block_size))
        self.n_head = config.n_head
        self.n_embd = config.n_embd
  
    def forward(self, x):
        B, T, C = x.size() 
        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)
        hs = C // self.n_head
        k = k.view(B, T, self.n_head, hs).transpose(1, 2)
        q = q.view(B, T, self.n_head, hs).transpose(1, 2)
        v = v.view(B, T, self.n_head, hs).transpose(1, 2)
  
        att = (q @ k.transpose(-2, -1)) *\
            (1.0 / math.sqrt(k.size(-1)))
        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, \
                              float(‚-inf'))
        att = F.softmax(att, dim=-1)
        att = self.attn_dropout(att)
        y = att @ v 
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        y = self.resid_dropout(self.c_proj(y))
        return y</pre>
<p class="body">When calculating causal self-attention, the input embedding is passed through three neural networks to obtain the query <span class="times">Q</span>, key <span class="times">K</span>, and value <span class="times">V</span>. We then split each of them into four parallel heads and calculate masked self-attention within each head. After that, we concatenate the four attention vectors back into a single attention vector, which is then used as the output of the <code class="fm-code-in-text">CausalSelfAttention()</code> class. <a id="idIndexMarker058"/></p>
<h3 class="fm-head1" id="heading_id_12">12.3.3 Building the GPT model</h3>
<p class="body">We combine a feed-forward network with the causal self-attention sublayer to form a decoder block. The feed-forward network injects nonlinearity into the model. Without it, the Transformer would simply be a series of linear operations, constraining its capacity to capture complex data relationships. Moreover, the feed-forward network processes each position independently and uniformly, enabling the transformation of features identified by the self-attention mechanism. This facilitates the capture of diverse aspects of the input data, thereby augmenting the model’s ability to represent information. A decoder block is defined as follows:<a id="marker-278"/><a id="idIndexMarker059"/><a id="idIndexMarker060"/></p>
<pre class="programlisting">class Block(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.ln_1 = nn.LayerNorm(config.n_embd)
        self.attn = CausalSelfAttention(config)
        self.ln_2 = nn.LayerNorm(config.n_embd)
        self.mlp = nn.ModuleDict(dict(
            c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd),
            c_proj = nn.Linear(4 * config.n_embd, config.n_embd),
            act    = GELU(),
            dropout = nn.Dropout(config.resid_pdrop),
        ))
        m = self.mlp
        self.mlpf=lambda x:m.dropout(m.c_proj(m.act(m.c_fc(x)))) 
  
    def forward(self, x):
        x = x + self.attn(self.ln_1(x))
        x = x + self.mlpf(self.ln_2(x))
        return x</pre>
<p class="body">Each decoder block in our GPT model consists of two sublayers: a causal self-attention sublayer and a feed-forward network. We apply layer normalization and a residual connection to each sublayer for improved stability and performance. We then stack three decoder layers on top of each other to form the main body of our GPT model.</p>
<p class="fm-code-listing-caption">Listing 12.5 Building a GPT model</p>
<pre class="programlisting">class Model(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.block_size = config.block_size
        self.transformer = nn.ModuleDict(dict(
            wte = nn.Embedding(config.vocab_size, config.n_embd),
            wpe = nn.Embedding(config.block_size, config.n_embd),
            drop = nn.Dropout(config.embd_pdrop),
            h = nn.ModuleList([Block(config) 
                               for _ in range(config.n_layer)]),
            ln_f = nn.LayerNorm(config.n_embd),))
        self.lm_head = nn.Linear(config.n_embd,
                                 config.vocab_size, bias=False)
        for pn, p in self.named_parameters():
            if pn.endswith('c_proj.weight'):    
                torch.nn.init.normal_(p, mean=0.0, 
                  std=0.02/math.sqrt(2 * config.n_layer))
    def forward(self, idx, targets=None):
        b, t = idx.size()
        pos=torch.arange(0,t,dtype=\
            torch.long).unsqueeze(0).to(device)              <span class="fm-combinumeral">①</span>
        tok_emb = self.transformer.wte(idx) 
        pos_emb = self.transformer.wpe(pos) 
        x = self.transformer.drop(tok_emb + pos_emb)
        for block in self.transformer.h:
            x = block(x)
        x = self.transformer.ln_f(x)
        logits = self.lm_head(x)
        return logits</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Moves the positional encoding to CUDA-enabled GPU, if available</p>
<p class="body">The positional encoding is created within the <code class="fm-code-in-text">Model()</code> class. Therefore, we need to move it to a compute unified device architecture (CUDA)-enabled GPU (if available) to ensure that all inputs to the model are on the same device. Failing to do this will result in an error message.<a id="marker-279"/><a id="idIndexMarker061"/><a id="idIndexMarker062"/></p>
<p class="body">The input to the model consists of sequences of indexes corresponding to tokens in the vocabulary. We pass the input through word embedding and positional encoding and add the two to form the input embedding. The input embedding then goes through the three decoder blocks. After that, we apply layer normalization to the output and attach a linear head to it so that the number of outputs is 10,600, the size of the vocabulary. The outputs are the logits corresponding to the 10,600 tokens in the vocabulary. Later, we’ll apply the softmax activation function to the logits to obtain the probability distribution over the unique tokens in the vocabulary when generating text.</p>
<p class="body">Next, we’ll create our GPT model by instantiating the <code class="fm-code-in-text">Model()</code> class we defined earlier:<a id="idIndexMarker063"/></p>
<pre class="programlisting">model=Model(config)
model.to(device)
num=sum(p.numel() for p in model.transformer.parameters())
print("number of parameters: %.2fM" % (num/1e6,))
print(model)</pre>
<p class="body">The output is</p>
<pre class="programlisting">number of parameters: 5.12M
Model(
  (transformer): ModuleDict(
    (wte): Embedding(10600, 256)
    (wpe): Embedding(128, 256)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-2): 3 x Block(
        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (attn): CausalSelfAttention(
          (c_attn): Linear(in_features=256, out_features=768, bias=True)
          (c_proj): Linear(in_features=256, out_features=256, bias=True)
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
        (mlp): ModuleDict(
          (c_fc): Linear(in_features=256, out_features=1024, bias=True)
          (c_proj): Linear(in_features=1024, out_features=256, bias=True)
          (act): GELU()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=256, out_features=10600, bias=False)
)</pre>
<p class="body">Our GPT model has 5.12 million parameters. The structure of our model is similar to that of GPT-2XL. If you compare the output above with that from chapter 11, you’ll see that the only differences are in the hyperparameters, such as the embedding dimension, number of decoder layers, vocabulary size, and so on. <a id="idIndexMarker064"/><a id="idIndexMarker065"/><a id="idIndexMarker066"/><a id="marker-280"/><a id="idIndexMarker067"/></p>
<h2 class="fm-head" id="heading_id_13">12.4 Training the GPT model to generate text</h2>
<p class="body">In this section, you’ll train the GPT model you just built using the batches of training data we prepared earlier in this chapter. A related question is how many epochs we should train the model. While training too few epochs may lead to incoherent text, training too many epochs may lead to an overfitted model, which may generate text identical to passages in the training text. <a id="idIndexMarker068"/><a id="idIndexMarker069"/></p>
<p class="body">Therefore, we will train the model for 40 epochs. We’ll save the model after every 10 epochs and assess which version of the trained model can generate coherent text without simply copying passages from the training text. Another potential approach is to create a validation set and stop training when the model’s performance converges in the validation set, as we did in chapter 2.</p>
<h3 class="fm-head1" id="heading_id_14">12.4.1 Training the GPT model</h3>
<p class="body">As always, we’ll use the Adam optimizer. Since our GPT model is essentially performing a multicategory classification, we’ll use cross-entropy loss as our loss function:<a id="idIndexMarker070"/><a id="idIndexMarker071"/></p>
<pre class="programlisting">lr=0.0001
optimizer = torch.optim.Adam(model.parameters(), lr=lr)
loss_func = nn.CrossEntropyLoss()</pre>
<p class="body">We will train the model for 40 epochs, as shown in the following listing.</p>
<p class="fm-code-listing-caption">Listing 12.6 Training the GPT model to generate text</p>
<pre class="programlisting">model.train()  
for i in range(1,41):
    tloss = 0.
    for idx, (x,y) in enumerate(loader):                      <span class="fm-combinumeral">①</span>
        x,y=x.to(device),y.to(device)
        output = model(x)
        loss=loss_func(output.view(-1,output.size(-1)),
                           y.view(-1))                        <span class="fm-combinumeral">②</span>
        optimizer.zero_grad()
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(),1)        <span class="fm-combinumeral">③</span>
        optimizer.step()                                      <span class="fm-combinumeral">④</span>
        tloss += loss.item()
    print(f'epoch {i} loss {tloss/(idx+1)}') 
    if i%10==0:
        torch.save(model.state_dict(),f'files/GPTe{i}.pth')   <span class="fm-combinumeral">⑤</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Iterates through all batches of training data</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Compares model predictions with actual outputs</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Clips gradient norm to 1</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Tweaks model parameters to minimize loss</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Saves model after every ten epochs</p>
<p class="body">During training, we pass all the input sequences x in a batch through the model to obtain predictions. We compare these predictions with the output sequences y in the batch and calculate the cross-entropy loss. We then adjust the model parameters to minimize this loss. Note that we have clipped the gradient norm to 1 to avoid the potential problem of exploding gradients.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Gradient norm clipping</p>
<p class="fm-sidebar-text">Gradient norm clipping is a technique used in training neural networks to prevent the exploding gradient problem. This problem occurs when the gradients of the loss function with respect to the model’s parameters become excessively large, leading to unstable training and poor model performance. In gradient norm clipping, the gradients are scaled down if their norm (magnitude) exceeds a certain threshold. This ensures that the gradients do not become too large, maintaining stable training and improving convergence.<a id="idIndexMarker072"/><a id="marker-281"/></p>
</div>
<p class="body">This training process takes a couple of hours if you have a CUDA-enabled GPU. After training, four files, GPTe10.pth, GPTe20.pth, ..., GPTe40.pth, will be saved on your computer. Alternatively, you can download the trained models from my website: <a class="url" href="https://gattonweb.uky.edu/faculty/lium/gai/GPT.zip">https://gattonweb.uky.edu/faculty/lium/gai/GPT.zip</a>.</p>
<h3 class="fm-head1" id="heading_id_15">12.4.2 A function to generate text</h3>
<p class="body">Now that we have multiple versions of the trained model, we can proceed to text generation and compare the performance of different versions. We can assess which version performs the best and use that version to generate text. <a id="idIndexMarker073"/><a id="idIndexMarker074"/></p>
<p class="body">Similar to the process in GPT-2XL, text generation begins with feeding a sequence of indexes (representing tokens) to the model as a prompt. The model predicts the index of the next token, which is then appended to the prompt to form a new sequence. This new sequence is fed back into the model for further predictions, and this process is repeated until a desired number of new tokens is generated.</p>
<p class="body">To facilitate this process, we define a <code class="fm-code-in-text">sample()</code> function. This function takes a sequence of indexes as input, representing the current state of the text. It then iteratively predicts and appends new indexes to the sequence until the specified number of new tokens, <code class="fm-code-in-text">max_new_tokens</code>, is reached. The following listing shows the implementation.<a id="marker-282"/><a id="idIndexMarker075"/></p>
<p class="fm-code-listing-caption">Listing 12.7 A <code class="fm-code-in-text">sample()</code> function to predict subsequent indexes</p>
<pre class="programlisting">def sample(idx, weights, max_new_tokens, temperature=1.0, top_k=None):
    model.eval()
    model.load_state_dict(torch.load(weights,
        map_location=device))                                 <span class="fm-combinumeral">①</span>
    original_length=len(idx[0])
    for _ in range(max_new_tokens):                           <span class="fm-combinumeral">②</span>
        if idx.size(1) &lt;= config.block_size:
            idx_cond = idx  
        else:
            idx_cond = idx[:, -config.block_size:]
        logits = model(idx_cond.to(device))                   <span class="fm-combinumeral">③</span>
        logits = logits[:, -1, :] / temperature
        if top_k is not None:
            v, _ = torch.topk(logits, top_k)
            logits[logits &lt; v[:, [-1]]] = -float('Inf')
        probs = F.softmax(logits, dim=-1)
        idx_next=torch.multinomial(probs,num_samples=1)
        idx = torch.cat((idx, idx_next.cpu()), dim=1)         <span class="fm-combinumeral">④</span>
    return idx[:, original_length:]                           <span class="fm-combinumeral">⑤</span></pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Loads up a version of the trained model</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Generates a fixed number of new indexes</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Uses the model to make predictions</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Attaches the new index to the end of the sequence</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Outputs only the new indexes</p>
<p class="body">One of the arguments of the <code class="fm-code-in-text">sample()</code> function is <code class="fm-code-in-text">weights</code>, which represents the trained weights of one of the models saved on your computer. Unlike the <code class="fm-code-in-text">sample()</code> function we defined in chapter 11, our function here returns only the newly generated indexes, not including the original indexes that were fed to the <code class="fm-code-in-text">sample()</code> function. We made this change to accommodate cases where the prompt contains unknown tokens. In such cases, our <code class="fm-code-in-text">sample()</code> function ensures that the final output retains the original prompt. Otherwise, all unknown tokens would be replaced with <code class="fm-code-in-text">"UNK"</code> in the final output.<a id="idIndexMarker076"/><a id="idIndexMarker077"/><a id="idIndexMarker078"/><a id="idIndexMarker079"/></p>
<p class="body">Next, we define a <code class="fm-code-in-text">generate()</code> function to generate text based on a prompt. The function first converts the prompt to a sequence of indexes. It then uses the <code class="fm-code-in-text">sample()</code> function to generate a new sequence of indexes. After that, the <code class="fm-code-in-text">generate()</code> function concatenates all indexes together and converts them back to text. The implementation is shown in the following listing.<a id="idIndexMarker080"/><a id="idIndexMarker081"/><a id="idIndexMarker082"/></p>
<p class="fm-code-listing-caption">Listing 12.8 A function to generate text with the trained GPT model</p>
<pre class="programlisting">UNK=word_to_int["UNK"]
def generate(prompt, weights, max_new_tokens, temperature=1.0,
             top_k=None):
    assert len(prompt)&gt;0, "prompt must contain at least one token" <span class="fm-combinumeral">①</span>
    text=prompt.lower().replace("\n", " ")
    for x in punctuations:
        text=text.replace(f"{x}", f" {x} ")
    text_tokenized=text.split() 
    idx=[word_to_int.get(w,UNK) for w in text_tokenized]           <span class="fm-combinumeral">②</span>
    idx=torch.LongTensor(idx).unsqueeze(0)
    idx=sample(idx, weights, max_new_tokens, 
               temperature=1.0, top_k=None)                        <span class="fm-combinumeral">③</span>
    tokens=[int_to_word[i] for i in idx.squeeze().numpy()]         <span class="fm-combinumeral">④</span>
    text=" ".join(tokens)
    for x in '''").:;!?,-''''':
        text=text.replace(f" {x}", f"{x}") 
    for x in '''"(-''''':
        text=text.replace(f"{x} ", f"{x}")     
    return prompt+" "+text</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Makes sure the prompt is not empty</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Converts prompt into a sequence of indexes</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Uses the sample() function to generate new indexes</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Converts the new sequence of indexes back to text</p>
<p class="body">We ensure that the prompt is not empty. If it is, you’ll receive an error message saying “prompt must contain at least one token.” The <code class="fm-code-in-text">generate()</code> function allows you to select which version of the model to use by specifying the weights saved on your computer. For example, you can choose ‘files/GPTe10.pth’ as the value of the weights argument for the function. The function converts the prompt into a series of indexes, which are then fed into the model to predict the next index. After generating a fixed number of new indexes, the function converts the entire index sequence back into textual form.<a id="idIndexMarker083"/><a id="idIndexMarker084"/><a id="idIndexMarker085"/><a id="marker-283"/></p>
<h3 class="fm-head1" id="heading_id_16">12.4.3 Text generation with different versions of the trained model</h3>
<p class="body">Next, we’ll experiment with different versions of the trained model to generate text. <a id="idIndexMarker086"/><a id="idIndexMarker087"/></p>
<p class="body">We can use the unknown token <code class="fm-code-in-text">"UNK"</code> as the prompt for unconditional text generation. This is especially beneficial in our context because we want to check if the generated text is directly copied from the training text. While a unique prompt that’s very different from the training text unlikely leads to passages directly from the training text, unconditionally generated text is more likely to be from the training text.</p>
<p class="body">We first use the model after 20 epochs of training to generate text unconditionally:</p>
<pre class="programlisting">prompt="UNK"
for i in range(10):
    torch.manual_seed(i)
    print(generate(prompt,'files/GPTe20.pth',max_new_tokens=20)[4:]))</pre>
<p class="body">The output is</p>
<pre class="programlisting">way." "kümmel," i said. "it's the way to talk about it
--------------------------------------------------
," robert jordan said. "but do not realize how far he is ruined." "pero
--------------------------------------------------
in the fog, robert jordan thought. and then, without looking at last, so 
good, he 
--------------------------------------------------
pot of yellow rice and fish and the boy loved him. "no," the boy said.
--------------------------------------------------
the line now. it's wonderful." "he's crazy about the brave."
--------------------------------------------------
candle to us. "and if the maria kisses thee again i will commence kissing 
thee myself. it 
--------------------------------------------------
?" "do you have to for the moment." robert jordan got up and walked away in
--------------------------------------------------
. a uniform for my father, he thought. i'll say them later. just then he
--------------------------------------------------
and more practical to read and relax in the evening; of all the things he 
had enjoyed the next 
--------------------------------------------------
in bed and rolled himself a cigarette. when he gave them a log to a second 
grenade. " 
--------------------------------------------------</pre>
<p class="body">We set the prompt to <code class="fm-code-in-text">"UNK"</code> and ask the <code class="fm-code-in-text">generate()</code> function to unconditionally generate 20 new tokens 10 times. We use the <code class="fm-code-in-text">manual_seed()</code> method to fix the random seeds so results are reproducible. As you can see, the 10 short passages generated here are all grammatically correct, and they sound like passages from Hemingway’s novels. For example, the word “kummel” in the first passage was a type of liqueur that was mentioned in <i class="fm-italics">A Farewell to Arms</i> quite often. At the same time, none of the above 10 passages are directly copied from the training text.<a id="idIndexMarker088"/><a id="marker-284"/><a id="idIndexMarker089"/></p>
<p class="body">Next, we use the model after 40 epochs of training instead to generate text unconditionally and see what happens:</p>
<pre class="programlisting">prompt="UNK"
for i in range(10):
    torch.manual_seed(i)
    print(generate(prompt,'files/GPTe40.pth',max_new_tokens=20)[4:]))</pre>
<p class="body">The output is</p>
<pre class="programlisting">way." "kümmel, and i will enjoy the killing. they must have brought me a spit
--------------------------------------------------
," robert jordan said. "but do not tell me that he saw anything." "not
--------------------------------------------------
in the first time he had bit the ear like that and held onto it, his neck 
and jaws
--------------------------------------------------
pot of yellow rice with fish. it was cold now in the head and he could not 
see the
--------------------------------------------------
the line of his mouth. he thought." "the laughing hurt him." "i can
--------------------------------------------------
candle made? that was the worst day of my life until one other day." "don'
--------------------------------------------------
?" "do you have to for the moment." robert jordan took the glasses and 
opened the
--------------------------------------------------
. that's what they don't marry." i reached for her hand. "don
--------------------------------------------------
and more grenades. that was the last for next year. it crossed the river 
away from the front
--------------------------------------------------
in a revolutionary army," robert jordan said. "that's really nonsense. it's
--------------------------------------------------</pre>
<p class="body">The 10 short passages generated here are again all grammatically correct, and they sound like passages from Hemingway’s novels. However, if you examine them closely, a large part of the eighth passage is directly copied from the novel <i class="fm-italics">A Farewell to Arms</i>. The part <code class="fm-code-in-text">they don't marry." i reached for her hand. "don</code> appeared in the novel as well. You can verify by searching in the file ThreeNovels.txt that was saved on your computer earlier.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 12.1</p>
<p class="fm-sidebar-text">Generate a passage of text with 50 new tokens unconditionally using the model trained for 10 epochs. Set the random seed to 42 and keep the <code class="fm-code-in-text1">temperature</code> and <code class="fm-code-in-text1">top-K</code> sampling at the default setting. Examine whether the generated passage is grammatically correct and if any parts are directly copied from the training text.<a id="idIndexMarker090"/><a id="idIndexMarker091"/></p>
</div>
<p class="body"><a id="marker-285"/>Alternatively, you can use a unique prompt that’s not in the training text to generate new text. For example, you might use “the old man saw the shark near the” as the prompt and ask the <code class="fm-code-in-text">generate()</code> function to add 20 new tokens to the prompt, repeating this process 10 times:<a id="idIndexMarker092"/></p>
<pre class="programlisting">prompt="the old man saw the shark near the"
for i in range(10):
    torch.manual_seed(i)
    print(generate(prompt,'files/GPTe40.pth',max_new_tokens=20))
    print("-"*50)   </pre>
<p class="body">The output is</p>
<pre class="programlisting">the old man saw the shark near the old man's head with his tail out and the old man hit him squarely in the center of
--------------------------------------------------
the old man saw the shark near the boat with one hand. he had no feeling of
the morning but he started to pull on it gently
--------------------------------------------------
the old man saw the shark near the old man's head. then he went back to 
another man in and leaned over and dipped the
--------------------------------------------------
the old man saw the shark near the fish now, and the old man was asleep in 
the water as he rowed he was out of the
--------------------------------------------------
the old man saw the shark near the boat. it was a nice-boat. he saw the old
 man's head and he started
--------------------------------------------------
the old man saw the shark near the boat to see him clearly and he was 
afraid that he was higher out of the water and the old
--------------------------------------------------
the old man saw the shark near the old man's head and then, with his tail 
lashing and his jaws clicking, the shark plowed
--------------------------------------------------
the old man saw the shark near the line with his tail which was not sweet 
smelling it. the old man knew that the fish was coming
--------------------------------------------------
the old man saw the shark near the fish with his jaws hooked and the old 
man stabbed him in his left eye. the shark still hung
--------------------------------------------------
the old man saw the shark near the fish and he started to shake his head 
again. the old man was asleep in the stern and he
--------------------------------------------------</pre>
<p class="body">The generated text is grammatically correct and coherent, closely resembling passages from Hemingway’s novel <i class="fm-italics">The Old Man and the Sea</i>. Since we used the model trained for 40 epochs, there’s a higher likelihood of generating text that directly mirrors the training data. However, using a unique prompt can reduce this probability.</p>
<p class="body">By setting the <code class="fm-code-in-text">temperature</code> and using <code class="fm-code-in-text">top-K</code> sampling, we can further control the diversity of the generated text. In this case, with a prompt like “the old man saw the shark near the,” and a temperature of 0.9 with top-50 sampling, the output remains mostly grammatically correct:<a id="marker-286"/></p>
<pre class="programlisting">prompt="the old man saw the shark near the"
for i in range(10):
    torch.manual_seed(i)
    print(generate(prompt,'files/GPTe20.pth',max_new_tokens=20,
                  temperature=0.9,top_k=50))
    print("-"*50) </pre>
<p class="body">The output is</p>
<pre class="programlisting"> The old man saw the shark near the boat. then he swung the great fish that 
was more comfortable in the sun. the old man could
--------------------------------------------------
the old man saw the shark near the boat with one hand. he wore his overcoat
 and carried the submachine gun muzzle down, carrying it in
--------------------------------------------------
the old man saw the shark near the boat with its long dip sharply and the 
old man stabbed him in the morning. he could not see
--------------------------------------------------
the old man saw the shark near the fish that was now heavy and long and 
grave he had taken no part in. he was still under
--------------------------------------------------
the old man saw the shark near the boat. it was a nice little light. then 
he rowed out and the old man was asleep over
--------------------------------------------------
the old man saw the shark near the boat to come. "old man's shack and i'll 
fill the water with him in
--------------------------------------------------
the old man saw the shark near the boat and then rose with his lines close 
him over the stern. "no," the old man
--------------------------------------------------
the old man saw the shark near the line with his tail go under. he was 
cutting away onto the bow and his face was just a
--------------------------------------------------
the old man saw the shark near the fish with his tail that he swung him in.
 the shark's head was out of water and
--------------------------------------------------
the old man saw the shark near the boat and he started to cry. he could 
almost have them come down and whipped him in again.
--------------------------------------------------</pre>
<p class="body">Since we used the model trained for 20 epochs instead of 40 epochs, the output is less coherent, with occasional grammar errors. For example, “with its long dip sharply” in the third passage is not grammatically correct. However, the risk of generating text directly copied from the training data is also lower.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 12.2</p>
<p class="fm-sidebar-text">Generate a passage of text with 50 new tokens using the model trained for 40 epochs. Use “the old man saw the shark near the” as the prompt; set the <code class="fm-code-in-text1">random seed</code> to 42, the <code class="fm-code-in-text1">temperature</code> to 0.95, and the <code class="fm-code-in-text1">top_k</code> to 100. Check if the generated passage is grammatically correct and if any part of the text is directly copied from the training text.</p>
</div>
<p class="body"><a id="marker-287"/>In this chapter, you’ve learned how to construct and train a GPT-style Transformer model from the ground up. Specifically, you’ve created a simplified version of the GPT-2 model with only 5.12 million parameters. Using three novels by Ernest Hemingway as training data, you have successfully trained the model. You have also generated text that is coherent and stylistically consistent with Hemingway’s writing.<a id="idIndexMarker093"/><a id="idIndexMarker094"/><a id="idIndexMarker095"/><a id="idIndexMarker096"/></p>
<h2 class="fm-head" id="heading_id_17">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">The style of the generated text from a GPT model will be heavily influenced by the training data. For effective text generation, it’s important to have a balance of text length and variation in the training material. The training dataset should be sufficiently large for the model to learn and emulate a specific writing style accurately. However, if the dataset lacks diversity, the model might end up reproducing passages directly from the training text. Conversely, overly long training datasets can require excessive computational resources for training.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Choosing the right hyperparameters in the GPT model is crucial for successful model training and text generation. Setting the hyperparameters too large may lead to too many parameters. This results in longer training time and an overfitted model. Setting the hyperparameters too small may hinder the model’s ability to learn effectively and capture the writing style in the training data. This may lead to incoherent generated text.</p>
</li>
<li class="fm-list-bullet">
<p class="list">The appropriate number of training epochs is important for text generation. While training too few epochs may lead to incoherent text, training for too many epochs may lead to an overfitted model that generates text identical to passages in the training text.<a id="marker-288"/></p>
</li>
</ul>
</div></body></html>