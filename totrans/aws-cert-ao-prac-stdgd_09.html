<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 8. A Framework for Responsible AI"><div class="chapter" id="chapter_eight_a_framework_for_responsib">
<h1><span class="label">Chapter 8. </span>A Framework for Responsible AI</h1>
<p>In 2014, Microsoft launched Xiaoice,<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="about" id="id1581"/><a contenteditable="false" data-type="indexterm" data-primary="Microsoft" data-secondary="Xiaoice chatbot" id="id1582"/><a contenteditable="false" data-type="indexterm" data-primary="Microsoft" data-secondary="Tay chatbot" id="id1583"/><a contenteditable="false" data-type="indexterm" data-primary="Tay chatbot (Microsoft)" id="id1584"/><a contenteditable="false" data-type="indexterm" data-primary="Xiaoice chatbot (Microsoft)" id="id1585"/><a contenteditable="false" data-type="indexterm" data-primary="Twitter and Tay chatbot" id="id1586"/><a contenteditable="false" data-type="indexterm" data-primary="chatbots" data-secondary="Tay chatbot and Twitter" id="id1587"/> an AI-powered chatbot in China that successfully attracted over 40 million users. A year later, Microsoft released a version on Twitter called Tay. The launch was a disaster. Tay quickly began spouting racist and hostile comments, forcing Microsoft to shut it down within 24 hours.</p>
<p>The incident occurred because Microsoft had a false sense of security from its experience in China, where stricter content limitations were in place, and underestimated the freewheeling nature of Twitter, where users actively tried to manipulate the bot. In a blog post, a Microsoft executive acknowledged the company had learned valuable lessons, stating that the challenges of AI are “just as much social as they are technical” and that caution is required when iterating in public forums.</p>
<p>This event spurred Microsoft to create its own principles for responsible AI—a framework for developing and deploying AI systems in a safe, trustworthy, and ethical way. As AI becomes more integrated into critical areas like healthcare and criminal justice, this focus on responsibility has never been more important and<a contenteditable="false" data-type="indexterm" data-primary="exam for AIF-C01" data-secondary="topics covered" data-tertiary="responsible AI" id="id1588"/><a contenteditable="false" data-type="indexterm" data-primary="topics covered in exam for AIF-C01" data-secondary="responsible AI" id="id1589"/> is a key part of the AIF-C01 exam.</p>
<section data-type="sect1" data-pdf-bookmark="Risks of Generative AI"><div class="sect1" id="risks_of_generative_ai">
<h1>Risks of Generative AI</h1>
<p>As powerful as generative AI can be,<a contenteditable="false" data-type="indexterm" data-primary="generative AI" data-secondary="risks of" id="c08risk"/><a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="risks of generative AI" id="c08risk2"/><a contenteditable="false" data-type="indexterm" data-primary="responses" data-secondary="risks of generative AI" id="c08risk3"/><a contenteditable="false" data-type="indexterm" data-primary="generative AI" data-secondary="risks of" data-tertiary="about" id="id1590"/><a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="risks of generative AI" data-tertiary="about" id="id1591"/><a contenteditable="false" data-type="indexterm" data-primary="responses" data-secondary="risks of generative AI" data-tertiary="about" id="id1592"/> its capabilities also introduce a wide range of risks—some subtle, others deeply disruptive. From producing toxic content to infringing on intellectual property and contributing to job displacement, these risks can have serious societal, ethical, and legal consequences. This is why the concept of responsible AI is so important. It serves as a foundational approach to identifying, mitigating, and managing these challenges from the outset.</p>
<p>Rather than treating these issues as isolated technical flaws, responsible AI emphasizes a holistic strategy: ensuring fairness, transparency, safety, and human oversight in how AI systems are built and used. In the following sections, we’ll explore several key risk areas tied to generative AI—each illustrating why a responsible framework isn’t optional, but essential. We’ll then follow this up by understanding the core elements of responsible AI and how they serve as practical tools for addressing these risks.</p>
<section data-type="sect2" data-pdf-bookmark="Toxicity"><div class="sect2" id="toxicity">
<h2>Toxicity</h2>
<p>Managing toxicity is a <a contenteditable="false" data-type="indexterm" data-primary="generative AI" data-secondary="risks of" data-tertiary="toxicity" id="id1593"/><a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="risks of generative AI" data-tertiary="toxicity" id="id1594"/><a contenteditable="false" data-type="indexterm" data-primary="toxicity" id="id1595"/><a contenteditable="false" data-type="indexterm" data-primary="responses" data-secondary="risks of generative AI" data-tertiary="toxicity" id="id1596"/>central concern of responsible AI. If generative AI systems produce content that is offensive, harmful, or inappropriate, it can erode trust, damage brand reputation, and even cause real-world harm. Ensuring responsible AI means putting safeguards in place to minimize these risks—through thoughtful design, filtering mechanisms, and ongoing oversight.</p>
<p>But there is a major problem with toxicity: it is highly subjective. What may be offensive to one person may be perfectly fine for another. There are also age-related considerations and the differences among cultures.</p>
<p>Thus, for an AI developer, it can be incredibly difficult to develop the right filters. Inevitably, it seems impossible not to offend someone.</p>
<p>Another issue is that it can be challenging to identify toxicity. Because generative AI systems are based on complex probability systems, the content may have shades or nuances of offensiveness—that may not be picked up in a filter.</p>
<p>In fact, one approach is to have human curation of data for generative AI models. But this can be time-consuming—and is far from perfect either.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Intellectual Property"><div class="sect2" id="intellectual_property">
<h2>Intellectual Property</h2>
<p>Respecting intellectual property (IP) rights<a contenteditable="false" data-type="indexterm" data-primary="generative AI" data-secondary="risks of" data-tertiary="intellectual property theft" id="id1597"/><a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="risks of generative AI" data-tertiary="intellectual property theft" id="id1598"/><a contenteditable="false" data-type="indexterm" data-primary="responses" data-secondary="risks of generative AI" data-tertiary="intellectual property theft" id="id1599"/><a contenteditable="false" data-type="indexterm" data-primary="intellectual property (IP)" id="id1600"/><a contenteditable="false" data-type="indexterm" data-primary="content delivery" data-secondary="intellectual property" id="id1601"/> is a cornerstone of responsible AI. Generative AI systems that fail to properly attribute, license, or protect creative works can undermine industries, violate legal protections, and erode public trust. A responsible approach to AI development means being proactive about copyright, ownership, and fair compensation.</p>
<p>The 2023 Hollywood writers’ strike <a contenteditable="false" data-type="indexterm" data-primary="Hollywood writers’ strike (2023)" id="id1602"/>underscored these concerns, bringing to light the growing tensions between creative professionals and the rapid rise of AI-generated content. Central to the dispute was the concern that generative AI could replicate writers’ work without sufficient compensation or credit.<sup><a data-type="noteref" id="ch01fn26-marker" href="ch08.html#ch01fn26">1</a></sup> <a contenteditable="false" data-type="indexterm" data-primary="Writers Guild of America (WGA)" id="id1603"/>The Writers Guild of America (WGA) successfully negotiated provisions ensuring that AI-generated content cannot replace human writers and that any AI assistance used in writing processes would still require full credit and compensation for the human writers involved. It was certainly historic—and yet another example of the influence of AI on society.</p>
<p>But when it comes to generative AI, the issue of IP rights has been critical since the early days of the launch of ChatGPT. Within a few months, there were already various lawsuits. For example,<a contenteditable="false" data-type="indexterm" data-primary="New York Times IP theft complaints" id="id1604"/><a contenteditable="false" data-type="indexterm" data-primary="Microsoft" data-secondary="New York Times suing" id="id1605"/><a contenteditable="false" data-type="indexterm" data-primary="OpenAI" data-secondary="New York Times suing" id="id1606"/> the <em>New York Time</em>s filed a complaint against OpenAI and Microsoft, alleging that they used newspaper articles without permission.</p>
<p>The legal issues for generative AI and IP are complex and will likely take years to sort out. Ultimately, they may be decided by the Supreme Court.</p>
<p>In the meantime, AI developers are finding ways to address the concerns. <a contenteditable="false" data-type="indexterm" data-primary="intellectual property (IP)" data-secondary="licensing deals" id="id1607"/><a contenteditable="false" data-type="indexterm" data-primary="licensing intellectual property" id="id1608"/>One approach has been to strike licensing deals with content providers.</p>
<p>Another approach to deal with IP issues is to provide indemnification protection. This is where the AI developer will defend and cover legal costs for litigation. Some of the companies that provide this protection include OpenAI, Microsoft, and Adobe.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Plagiarism and Cheating"><div class="sect2" id="plagiarism_and_cheating">
<h2>Plagiarism and Cheating</h2>
<p>Promoting academic integrity is an<a contenteditable="false" data-type="indexterm" data-primary="generative AI" data-secondary="risks of" data-tertiary="plagiarism and cheating" id="id1609"/><a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="risks of generative AI" data-tertiary="plagiarism and cheating" id="id1610"/><a contenteditable="false" data-type="indexterm" data-primary="plagiarism and cheating" id="id1611"/><a contenteditable="false" data-type="indexterm" data-primary="cheating and plagiarism" id="id1612"/><a contenteditable="false" data-type="indexterm" data-primary="responses" data-secondary="risks of generative AI" data-tertiary="plagiarism and cheating" id="id1613"/> essential component of responsible AI. While generative AI can serve as a powerful educational tool, it must be deployed with guardrails that discourage misuse—such as plagiarism. Responsible AI means fostering transparency, accountability, and ethical behavior, especially in learning environments.</p>
<p>Generative AI tools like ChatGPT offer students the ability to explore academic subjects, ask complex questions, and receive personalized, on-demand help. This has the potential to enhance learning in significant ways.</p>
<p>But on the flip side, the same technology can be exploited to bypass genuine effort—writing essays, completing assignments, or answering exam questions. This raises serious concerns about fairness and learning outcomes.</p>
<p>The response from educational institutions has been divided. Some have implemented bans or put restrictions on its use. In other cases, the approach has been to promote the use of generative AI and include it in the curriculum, such as to learn how to better leverage the technology.</p>
<p>There have been attempts to detect AI-generated content. But this has proven extremely difficult. It’s not uncommon for these tools to give false positives. Besides, students can be creative in evading detection. For example, they may rewrite some of the content. This can even be done using an AI tool!</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Disruption of the Nature of Work"><div class="sect2" id="disruption_of_the_nature_of_work">
<h2>Disruption of the Nature of Work</h2>
<p>One of the most pressing challenges<a contenteditable="false" data-type="indexterm" data-primary="generative AI" data-secondary="risks of" data-tertiary="workforce transitions" id="id1614"/><a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="risks of generative AI" data-tertiary="workforce transitions" id="id1615"/><a contenteditable="false" data-type="indexterm" data-primary="workforce risks of generative AI" id="id1616"/><a contenteditable="false" data-type="indexterm" data-primary="responses" data-secondary="risks of generative AI" data-tertiary="workforce transitions" id="id1617"/><a contenteditable="false" data-type="indexterm" data-primary="automation and jobs" id="id1618"/><a contenteditable="false" data-type="indexterm" data-primary="jobs and automation" id="id1619"/> for responsible AI is its potential to disrupt the global workforce. As AI systems become capable of performing complex tasks once reserved for highly skilled professionals, responsible AI must account not only for safety and fairness but also for long-term economic and social impacts. That includes planning for workforce transition, supporting job augmentation, and fostering inclusive innovation.</p>
<p>In 1930, legendary economist<a contenteditable="false" data-type="indexterm" data-primary="Keynes, John Maynard" id="id1620"/> John Maynard Keynes wrote a paper about how technology would displace a huge number of jobs. He called this “technological unemployment.”</p>
<p>For many decades, his prediction was off the mark. Often, new technology led to even more jobs. But today, the fears of Keynes seem much more realistic. The fact is that generative AI can already engage effectively in complex, knowledge-based fields like software development, financial services, and law.</p>
<p>For example, research from Goldman Sachs predicts that generative AI could <a href="https://oreil.ly/M1Pxu">automate about 300 million full-time jobs globally</a> and that about two-thirds of jobs in the US are vulnerable to AI automation.</p>
<p>Then there was a report from the McKinsey Global Institute. It forecasted that by 2030 up to <a href="https://oreil.ly/e6ZQh">30% of hours currently worked in the US and Europe could be automated</a>.</p>
<p>No doubt, this is far from encouraging. If these predictions wind up being on target, there is likely to be significant disruption—economically and socially.</p>
<p>This certainly underscores the importance of developing and implementing responsible AI practices. It could mean thinking about retraining and reskilling the workforce, as well as seeing how AI can better augment work—not replace jobs.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Accuracy"><div class="sect2" id="accuracy">
<h2>Accuracy</h2>
<p>At the core of responsible AI is<a contenteditable="false" data-type="indexterm" data-primary="generative AI" data-secondary="risks of" data-tertiary="accuracy" id="id1621"/><a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="risks of generative AI" data-tertiary="accuracy" id="id1622"/><a contenteditable="false" data-type="indexterm" data-primary="accuracy" data-secondary="risk of generative AI" id="id1623"/><a contenteditable="false" data-type="indexterm" data-primary="responses" data-secondary="risks of generative AI" data-tertiary="accuracy" id="id1624"/> the accuracy of the results of a model. This is essential for reliability, safety, and trustworthiness.</p>
<p>In <a data-type="xref" href="ch03.html#chapter_three_ai_and_machine_learning">Chapter 3</a>, we learned<a contenteditable="false" data-type="indexterm" data-primary="accuracy" id="id1625"/> about some techniques to measure the accuracy of an AI model, which include:<a contenteditable="false" data-type="indexterm" data-primary="bias" data-secondary="defined" id="id1626"/><a contenteditable="false" data-type="indexterm" data-primary="variance" data-secondary="defined" id="id1627"/></p>
<dl>
<dt>Bias</dt>
<dd><p>This is the difference between the average predicted values and actual values. High bias often results in underfitting, where the model performs poorly on both training and unseen data because it cannot represent the complexity of the data.</p></dd>
<dt>Variance</dt>
<dd><p>This is where the model is sensitive to changes or noise in the training data. High variance leads to overfitting, where the model captures noise in the training data as if it were a true pattern. While adding more data can sometimes reduce overfitting, this is not guaranteed. If the model is too complex relative to the amount of data or if the data is noisy, simply increasing the dataset size may not improve accuracy.</p></dd>
</dl>
<p>Ultimately, the goal is to find a balance where both bias and variance are minimized. These are some techniques to help with the trade-off:<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="c08risk" id="id1628"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="c08risk2" id="id1629"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="c08risk3" id="id1630"/></p>
<dl>
<dt>Cross validation</dt>
<dd><p>Evaluating an AI model by training several others on subsets of the data available helps to detect overfitting.</p></dd>
<dt>Increase data</dt>
<dd><p>Add more data samples, especially those that are more diverse.</p></dd>
<dt>Regularization</dt>
<dd><p>This penalizes extreme values, which can mitigate overfitting and the variance.</p></dd>
<dt>Simpler models</dt>
<dd><p>Simple models can help with overfitting because they are less likely to capture noise in the training data. But if the model is too simple, this can lead to bias.</p></dd>
<dt>Dimensionality reduction</dt>
<dd><p>Simplification by reducing the number of features in a dataset while trying to retain as much information as possible can reduce variance.</p></dd>
<dt>Hyperparameter tuning</dt>
<dd><p>Adjusting model parameters can help balance bias and variance.</p></dd>
<dt>Feature selection</dt>
<dd><p>This can simplify the model and reduce variance.</p></dd>
</dl>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Elements of Responsible AI"><div class="sect1" id="elements_of_responsible_ai">
<h1>Elements of Responsible AI</h1>
<p>Besides Microsoft’s principles of responsible AI,<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="elements of" id="c08ele"/><a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="elements of" data-tertiary="about" id="id1631"/> there are other companies that have their own frameworks, like Google. This is also true for organizations like UNESCO and the United Nations. Even the Vatican has its own guidelines. Regardless, they generally share many of the same concepts.</p>
<p>As for the AIF-C01 exam,<a contenteditable="false" data-type="indexterm" data-primary="exam for AIF-C01" data-secondary="topics covered" data-tertiary="responsible AI" id="id1632"/><a contenteditable="false" data-type="indexterm" data-primary="topics covered in exam for AIF-C01" data-secondary="responsible AI" id="id1633"/> there are some principles you should keep in mind. But they should not be considered in isolation. Implementing one often involves considering others. For example, achieving transparency in AI systems typically requires explainability, fairness, and robust governance structures. Similarly, ensuring safety and controllability involves robust design and clear governance. In other words, there should be a holistic approach.</p>
<p>Let’s take a look at some of the principles you should know for the exam.</p>
<section data-type="sect2" data-pdf-bookmark="Fairness"><div class="sect2" id="fairness">
<h2>Fairness</h2>
<p>Fairness means that AI systems<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="elements of" data-tertiary="fairness" id="id1634"/><a contenteditable="false" data-type="indexterm" data-primary="fairness" data-secondary="responsible AI" id="id1635"/> should make decisions that are impartial. There should not be discrimination against individuals or groups, such as based on race, gender, or socioeconomic status. By incorporating fairness in an AI system, you help bolster inclusion and trust.</p>
<p>Interestingly enough, Apple and Goldman Sachs did not use gender as a factor in their AI models and a New York state investigation did not find that there was inherent bias.<sup><a data-type="noteref" id="ch01fn28-marker" href="ch08.html#ch01fn28">2</a></sup> Nevertheless, the algorithms were changed, and the results turned out to be fairer.</p>
<p>What this points out is that—even if you do not use certain data—a model can still be unfair. The reason is that related data may lead to the same results. For example, a credit scoring system may give a lower credit limit to teachers, which may have a higher representation of women.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Explainability"><div class="sect2" id="explainability">
<h2>Explainability</h2>
<p>Explainable AI (XAI) is where an AI system<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="elements of" data-tertiary="explainability" id="id1636"/><a contenteditable="false" data-type="indexterm" data-primary="explainable AI (XAI)" data-secondary="responsible AI" id="id1637"/><a contenteditable="false" data-type="indexterm" data-primary="explainable AI (XAI)" id="id1638"/> is developed to make the decision-making processes transparent and understandable. This can help improve trust and accountability.</p>
<p>In regulated sectors, XAI is critical. A system may not be able to pass regulatory muster if it does not meet certain requirements and standards. For instance, if an AI system is used to diagnose a disease and recommends treatments, it must have clear explanations for the underlying process and reasoning. Otherwise, patients could potentially be in danger.</p>
<p>Unfortunately, XAI has many challenges. Current techniques are generally done with post hoc interpretations that may not accurately reflect the mode’s actual decision-making process. Moreover, there’s a lack of standardized metrics to evaluate the quality and effectiveness of explanations, and efforts to make models more interpretable can sometimes compromise their performance.</p>
<p>But there has been considerable research in XAI, and there continues to be ongoing progress.</p>
<p>Keep in mind that there are<a contenteditable="false" data-type="indexterm" data-primary="explainable AI (XAI)" data-secondary="explainability frameworks" id="id1639"/> various explainability frameworks like SHapley Additive exPlanations (SHAP), Local Interpretable Model-Agnostic Explanations (LIME), and counterfactual explanations. These frameworks will summarize and interpret the decision making of AI systems.</p>
<p>As for AWS, there are some helpful tools like SageMaker Clarify, which we will cover later in this chapter.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Privacy and Security"><div class="sect2" id="privacy_and_security">
<h2>Privacy and Security</h2>
<p>Privacy and security ensure that<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="elements of" data-tertiary="privacy and security" id="id1640"/><a contenteditable="false" data-type="indexterm" data-primary="privacy" data-secondary="responsible AI" id="id1641"/><a contenteditable="false" data-type="indexterm" data-primary="security" data-secondary="responsible AI" id="id1642"/> individuals’ data is protected and that they maintain control over how their information is used. This involves both safeguarding data from unauthorized access and providing users with clear choices regarding their data’s usage.</p>
<p>Implementing strong privacy and security measures not only complies with legal requirements but also builds trust with users. When individuals are confident that their data is handled responsibly, they are more likely to engage with AI technologies. This helps to foster innovation and broader adoption.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Transparency"><div class="sect2" id="transparency">
<h2>Transparency</h2>
<p>Transparency is <a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="elements of" data-tertiary="transparency" id="id1643"/><a contenteditable="false" data-type="indexterm" data-primary="transparency" data-secondary="responsible AI" id="id1644"/>sharing information about how AI systems are developed, the data they use, and their decision-making processes. This openness enables stakeholders—like users, regulators, and developers—to understand the system’s capabilities and limitations. For instance, transparency can involve disclosing the sources of training data, the objectives of the AI model, and any inherent risks or biases.</p>
<p>While transparency and explainability<a contenteditable="false" data-type="indexterm" data-primary="transparency" data-secondary="explainability versus" id="id1645"/><a contenteditable="false" data-type="indexterm" data-primary="explainable AI (XAI)" data-secondary="transparency versus" id="id1646"/> are related concepts in AI, they serve distinct purposes. Transparency pertains to the overall openness about an AI system’s design, data sources, and functioning. Explainability, on the other hand, focuses on the specific reasoning for individual decisions made by the AI.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Veracity and Robustness"><div class="sect2" id="veracity_and_robustness">
<h2>Veracity and Robustness</h2>
<p>Veracity and robustness help<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="elements of" data-tertiary="veracity and robustness" id="id1647"/><a contenteditable="false" data-type="indexterm" data-primary="veracity and robustness as responsible AI" id="id1648"/><a contenteditable="false" data-type="indexterm" data-primary="robustness" data-secondary="responsible AI" id="id1649"/> to ensure that AI systems operate reliably and accurately. This is the case even when there are unexpected inputs or challenging <span class="keep-together">environments.</span></p>
<p>Veracity pertains to the truthfulness and accuracy of the AI’s outputs. Robustness is about an AI system’s ability to maintain consistent performance despite variations in input data, adversarial attacks, or unforeseen circumstances.</p>
<p>The importance of these attributes cannot be overstated, especially in critical applications such as healthcare, finance, and autonomous systems. For instance, in healthcare, an AI diagnostic tool must provide accurate assessments even when patient data is incomplete or contains anomalies. A robust AI system can handle such irregularities without compromising the quality of its output. Similarly, in finance, AI models must remain reliable amidst fluctuating market conditions and data inconsistencies.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Governance"><div class="sect2" id="governance-id000029">
<h2>Governance</h2>
<p>AI governance refers to the<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="elements of" data-tertiary="governance" id="id1650"/><a contenteditable="false" data-type="indexterm" data-primary="governance" data-secondary="responsible AI" id="id1651"/> policies and procedures that companies set up to guide the ethical and compliant development for AI systems. This includes defining clear roles and responsibilities, implementing oversight structures, and establishing protocols for risk assessment and mitigation. Effective AI governance helps organizations manage potential risks, such as bias, discrimination, and privacy violations, while promoting transparency and accountability in AI operations.</p>
<p>The dynamic nature of AI technologies requires ongoing monitoring and adaptation of governance strategies. Establishing cross-functional teams that include ethicists, legal experts, technologists, and other stakeholders can help organizations proactively identify and address emerging ethical dilemmas and compliance challenges.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Safety"><div class="sect2" id="safety">
<h2>Safety</h2>
<p>Ensuring the safety of AI involves<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="elements of" data-tertiary="safety" id="id1652"/><a contenteditable="false" data-type="indexterm" data-primary="safety as responsible AI" id="id1653"/> developing and operating AI systems to perform their intended functions without causing harm to humans or the environment. This involves addressing potential risks such as unintended behaviors, algorithmic bias, and misuse.</p>
<p>A critical aspect of AI safety is rigorous testing and validation. This includes stress testing AI systems under extreme conditions and using diverse datasets to ensure consistent performance across various scenarios. Such practices help in identifying and mitigating risks before deployment. Additionally, implementing robust safeguards and oversight mechanisms can prevent malfunctions and misuse.</p>
<p>There’s an important trade-off<a contenteditable="false" data-type="indexterm" data-primary="safety as responsible AI" data-secondary="transparency trade-off" id="id1654"/><a contenteditable="false" data-type="indexterm" data-primary="transparency" data-secondary="safety trade-off" id="id1655"/><a contenteditable="false" data-type="indexterm" data-primary="data" data-secondary="safety versus transparency" id="id1656"/> between the safety of a model and transparency. Model safety is all about protecting sensitive data, while model transparency is about making it easier to see how and why a model makes decisions. Striking the right balance between the two is often challenging. This is especially the case in environments where both privacy and accountability are critical.</p>
<p>For example, highly complex models like deep neural networks typically offer stronger performance and accuracy but are often difficult to interpret. Simpler models, such as linear regressions, are easier to explain but may not perform as well on complex tasks.</p>
<p>There are also techniques designed to protect data privacy, such as differential <span class="keep-together">privacy,</span> which helps prevent the exposure of individual data points. However, this can make it more difficult to understand how a model arrives at its conclusions—improving security at the cost of transparency. Similarly, models trained in isolated <span class="keep-together">environments—known</span> as air-gapped systems, which are physically or logically disconnected from external networks—further enhance security by preventing outside access. But this isolation can make it harder for external parties to audit or evaluate the model’s behavior. To ensure performance and resilience, AWS Bedrock allows for stress testing of models under various loads and scenarios, helping validate how well they operate in demanding environments.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Controllability"><div class="sect2" id="controllability">
<h2>Controllability</h2>
<p>Controllability is the capacity to<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="elements of" data-tertiary="controllability" id="id1657"/><a contenteditable="false" data-type="indexterm" data-primary="controllability" data-secondary="responsible AI" id="id1658"/> guide and regulate AI systems so that their actions remain aligned with human intentions and ethical standards. This involves designing AI architectures that allow for human oversight. This allows developers and users to monitor, intervene, and adjust the system’s behavior.</p>
<p>The “AI control problem” addresses the difficulty of ensuring that advanced AI systems act in accordance with human values and objectives. As AI systems become more autonomous, there’s an increased risk of them pursuing goals in unintended ways. This can potentially lead to harmful outcomes.</p>
<p>The controllability of a model also plays a key role in transparency and debugging. If a model reacts logically to adjustments in the training data, it becomes easier to understand how it’s functioning and to trace issues when something goes wrong.</p>
<p>The degree of controllability is influenced by the type of model. Simpler models like linear regressions typically allow for more direct control, while more complex models can behave in unpredictable ways. To assess a model’s controllability, you can run tests where you intentionally modify or augment data to see if the model’s outputs shift in expected ways.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="c08ele" id="id1659"/></p>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="The Benefits of Responsible AI"><div class="sect1" id="the_benefits_of_responsible_ai">
<h1>The Benefits of Responsible AI</h1>
<p>Responsible AI isn’t just <a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="benefits of" id="id1660"/>about ethics—it’s also good business. While doing the right thing should always be a priority, integrating responsible AI practices can significantly boost a company’s performance and long-term success.</p>
<p>Let’s look at other reasons:<a contenteditable="false" data-type="indexterm" data-primary="branding and AI" id="id1661"/></p>
<dl>
<dt>Building trust and enhancing brand image</dt>
<dd><p>When users believe an AI system is transparent, fair, and secure, they’re more inclined to engage with it. That confidence builds loyalty and strengthens a company’s reputation. It also means that an AI application will be more effective and useful.</p></dd>
<dt>Staying ahead of regulation</dt>
<dd><p>As governments and industry bodies develop new rules around AI, organizations with ethical frameworks already in place will find it easier to adapt.</p></dd>
<dt>Reducing risk exposure</dt>
<dd><p>Responsible AI helps companies proactively identify and mitigate dangers, such as algorithmic bias, data misuse, and security lapses. This lowers the chances of legal trouble, reputational harm, or financial losses from unintended <span class="keep-together">consequences.</span></p></dd>
<dt>Standing out in the market</dt>
<dd><p>Ethical AI can set a company apart from its rivals. As more consumers pay attention to how companies use AI, those that demonstrate responsibility and integrity can earn a stronger competitive advantage.</p></dd>
<dt>Smarter outcomes</dt>
<dd><p>When fairness and transparency are core design principles, AI systems tend to produce more dependable insights. This leads to sounder strategies and better-informed decisions.</p></dd>
<dt>Driving innovation</dt>
<dd><p>Responsible AI brings more perspectives into the conversation. This diversity can lead to more original thinking, helping teams create products and services that are both impactful and forward-thinking.</p></dd>
</dl>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Amazon Tools for Responsible AI"><div class="sect1" id="amazon_tools_for_responsible_ai">
<h1>Amazon Tools for Responsible AI</h1>
<p>For AWS AI platforms, there are<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="Amazon tools for" id="c08tool"/><a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="Amazon tools for" data-tertiary="about" id="id1662"/> extensive capabilities and tools for helping create responsible AI. This has been a major priority, which has involved much investment over the years.</p>
<p>Let’s look at these features for Amazon Bedrock, SageMaker Clarify, Amazon A2I, and SageMaker Model Monitor.</p>
<section data-type="sect2" data-pdf-bookmark="Amazon Bedrock"><div class="sect2" id="amazon_bedrock">
<h2>Amazon Bedrock</h2>
<p>With Bedrock, you can easily<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="Amazon tools for" data-tertiary="Amazon Bedrock" id="id1663"/><a contenteditable="false" data-type="indexterm" data-primary="Amazon Bedrock" data-secondary="responsible AI tools" id="id1664"/> evaluate and compare various FMs. Some of the automatic metrics include accuracy, robustness, and toxicity. But there are also human evaluations, which focus on more subjective categories like style and alignment of brand voice. This can be done with your own employees or those managed by AWS.</p>
<p>Another powerful feature<a contenteditable="false" data-type="indexterm" data-primary="Amazon Bedrock" data-secondary="Guardrails" data-tertiary="responsible AI" id="id1665"/><a contenteditable="false" data-type="indexterm" data-primary="Guardrails in Amazon Bedrock" data-secondary="responsible AI" id="id1666"/> for responsible AI is Bedrock’s guardrails, which we briefly covered in <a data-type="xref" href="ch06.html#chapter_six_building_with_amazon_bedroc">Chapter 6</a>. This system allows for controlling how users interact with FMs. You can restrict interactions by:<a contenteditable="false" data-type="indexterm" data-primary="personally identifiable information (PII)" data-secondary="Amazon Bedrock Guardrails redacting" id="id1667"/></p>
<dl>
<dt>Filtering content</dt>
<dd><p>You can create filters or use built-in versions that detect hateful, insulting, sexual, or violent content. For these, you can set the thresholds.</p></dd>
<dt>Redacting PII</dt>
<dd><p>Guardrails can detect sensitive data, like names, addresses, Social Security numbers, and so on. This information will be blocked from inputs and outputs in FMs.</p></dd>
<dt>Implementing content safety and privacy policies</dt>
<dd><p>You do not have to use a scripting language for this; you can use natural <span class="keep-together">language.</span></p></dd>
</dl>
<p>Guardrails also apply<a contenteditable="false" data-type="indexterm" data-primary="agentic AI" data-secondary="Amazon Bedrock agents" data-tertiary="Guardrails and responsible AI" id="id1668"/> to AI agents. This is particularly important since these systems can act autonomously. Thus, there is often a need to allow for human approval or feedback.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="SageMaker Clarify and Experiments"><div class="sect2" id="sagemaker_clarify_and_experiments">
<h2>SageMaker Clarify and Experiments</h2>
<p>SageMaker Clarify allows you to<a contenteditable="false" data-type="indexterm" data-primary="Amazon SageMaker" data-secondary="SageMaker Clarify" data-tertiary="responsible AI tools" id="id1669"/><a contenteditable="false" data-type="indexterm" data-primary="Amazon SageMaker" data-secondary="SageMaker Experiments" data-tertiary="responsible AI tools" id="id1670"/><a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="Amazon tools for" data-tertiary="SageMaker Clarify and Experiments" id="id1671"/> detect biases in datasets and AI models, without the need for advanced coding. You will specify factors like gender or age, and the system will conduct an analysis and produce a report.</p>
<p>Clarify has other features. For example, it can provide details about the decision making of an AI system, saying which features have the most influence on the responses of a model.</p>
<p>AWS also offers SageMaker Experiments. This helps manage the interactive nature of AI development. You can organize, track, and compare different training runs. For these, you will capture the inputs, parameters, and results. This helps to better evaluate FMs.</p>
<p>SageMaker also has various governance tools:<a contenteditable="false" data-type="indexterm" data-primary="Amazon SageMaker" data-secondary="SageMaker Role Manager" data-tertiary="governance tools" id="id1672"/><a contenteditable="false" data-type="indexterm" data-primary="Amazon SageMaker" data-secondary="SageMaker Model Cards" id="id1673"/><a contenteditable="false" data-type="indexterm" data-primary="model cards" data-secondary="SageMaker Model Cards" id="id1674"/><a contenteditable="false" data-type="indexterm" data-primary="Amazon SageMaker" data-secondary="SageMaker Model Dashboard" id="id1675"/><a contenteditable="false" data-type="indexterm" data-primary="governance" data-secondary="Amazon SageMaker tools for" id="id1676"/><a contenteditable="false" data-type="indexterm" data-primary="model cards" data-secondary="governance documentation" id="id1677"/></p>
<dl>
<dt>SageMaker Role Manager</dt>
<dd><p>This allows administrators to define user permissions efficiently.</p></dd>
<dt>SageMaker Model Cards</dt>
<dd><p>This provides the documentation of essential model information. This includes intended use cases, risk assessments, and training details.</p></dd>
<dt>SageMaker Model Dashboard</dt>
<dd><p>This provides a unified interface to monitor model performance. This integrates data from a myriad of sources to track metrics like data quality, model accuracy, and bias over time.</p></dd>
</dl>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Amazon Augmented AI (Amazon A21)"><div class="sect2" id="amazon_augmented_ai_left_parenthesisama">
<h2>Amazon Augmented AI (Amazon A21)</h2>
<p>Amazon Augmented AI (Amazon A2I) plays<a contenteditable="false" data-type="indexterm" data-primary="Amazon Augmented AI (Amazon A2I)" id="id1678"/><a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="Amazon tools for" data-tertiary="Amazon Augmented AI" id="id1679"/> a key role in responsible AI by allowing human oversight in automated decision-making processes. It helps to reduce the risk of harmful errors, improve fairness, and build trust in AI systems.</p>
<p>You can define conditions under which human reviews are triggered, such as low-confidence predictions or random sampling for auditing purposes. This flexibility allows for the incorporation of human judgment in various scenarios, including content moderation, text extraction, and translation tasks. For instance, in content <span class="keep-together">moderation,</span> images flagged with confidence scores below a certain threshold can be routed to human reviewers for further assessment.</p>
<p>Amazon A2I supports multiple workforce options. You can use your private team of reviewers, engage third-party vendors through the AWS Marketplace, or access a global workforce of over 500,000 independent contractors via Amazon Mechanical Turk.</p>
<p>You can use Amazon A2I with Amazon SageMaker, Amazon Textract, Amazon Rekognition, Amazon Comprehend, Amazon Transcribe, and Amazon Translate.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="SageMaker Model Monitor"><div class="sect2" id="sagemaker_model_monitor">
<h2>SageMaker Model Monitor</h2>
<p>In <a data-type="xref" href="ch03.html#chapter_three_ai_and_machine_learning">Chapter 3</a>, we <a contenteditable="false" data-type="indexterm" data-primary="Amazon SageMaker" data-secondary="SageMaker Model Monitor" data-tertiary="responsible AI" id="id1680"/><a contenteditable="false" data-type="indexterm" data-primary="monitoring" data-secondary="SageMaker Model Monitor" data-tertiary="responsible AI" id="id1681"/>briefly covered SageMaker Model Monitor. It is a fully managed service that allows for continuous review of AI models that are in production. It will detect different types of drift that can impact the performance of a model, including the following:<a contenteditable="false" data-type="indexterm" data-primary="data" data-secondary="data drift" data-tertiary="SageMaker Model Monitor" id="id1682"/><a contenteditable="false" data-type="indexterm" data-primary="model quality drift" id="id1683"/><a contenteditable="false" data-type="indexterm" data-primary="bias" data-secondary="bias drift" id="id1684"/><a contenteditable="false" data-type="indexterm" data-primary="feature attribution drift" id="id1685"/></p>
<dl>
<dt>Data quality drift</dt>
<dd><p>Identifies changes in the statistical properties of input data, such as shifts in mean or variance</p></dd>
<dt>Model quality drift</dt>
<dd><p>Monitors the performance metrics like accuracy and precision by comparing model predictions against actual outcomes</p></dd>
<dt>Bias drift</dt>
<dd><p>Detects unintended biases in model predictions over time</p></dd>
<dt>Feature attribution drift</dt>
<dd><p>Observes changes in the importance of input features in influencing model <span class="keep-together">predictions</span></p></dd>
</dl>
<p>With the Model Monitor, you can establish baselines using training data to define acceptable performance thresholds. Monitoring jobs can be scheduled at regular intervals or executed on-demand.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="c08tool" id="id1686"/></p>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Going Further with Responsible AI"><div class="sect1" id="going_further_with_responsible_ai">
<h1>Going Further with Responsible AI</h1>
<p>Let’s take a look at some additional considerations when it comes to creating responsible AI in the following sections.<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="further considerations" id="c08addl"/></p>
<section data-type="sect2" class="pagebreak-before" data-pdf-bookmark="Sustainability and Environmental Considerations"><div class="sect2" id="sustainability_and_environmental_consid">
<h2 class="less_space">Sustainability and Environmental Considerations</h2>
<p>Sustainability and environmental considerations refer<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="further considerations" data-tertiary="the environment" data-tertiary-sortas="environment" id="id1687"/><a contenteditable="false" data-type="indexterm" data-primary="environment and AI" id="id1688"/><a contenteditable="false" data-type="indexterm" data-primary="sustainability" id="id1689"/> to the development of AI <span class="keep-together">technologies</span> that are viable over the long term—socially, economically, and environmentally—while actively minimizing ecological harm. This involves creating systems that not only deliver performance and innovation but also support societal well-being and reduce negative impacts on the planet. It includes managing the full lifecycle of AI systems, from the energy required to train and run models to the materials used in hardware, with the goal of lowering the environmental footprint and promoting responsible, resource-efficient practices.</p>
<p>These principles are central to responsible AI, which emphasizes the ethical, transparent, and accountable development of artificial intelligence. As AI continues to scale, its environmental impact can no longer be treated as an afterthought. Responsible AI initiatives must ensure that sustainability is built into the design, deployment, and governance of AI systems.</p>
<p>One major concern is the<a contenteditable="false" data-type="indexterm" data-primary="training" data-secondary="environmental considerations" id="id1690"/> energy consumption associated with training and running large AI models. These processes can demand significant computational resources, which increases electricity use and contributes to greenhouse gas emissions. A responsible approach involves improving energy efficiency through better model architectures, using power-saving hardware, and sourcing electricity from renewable energy. For instance, optimizing training schedules to coincide with periods of low-carbon energy availability can reduce environmental impact without sacrificing <span class="keep-together">performance.</span></p>
<p>Another issue is the resource intensity of AI infrastructure. Manufacturing and deploying specialized hardware such as GPUs and TPUs often involves environmentally damaging materials and processes. Sustainable AI development promotes the reuse of existing hardware, prioritizes recyclable or longer-lasting components, and limits the production of electronic waste.</p>
<p>In addition, environmental impact assessments<a contenteditable="false" data-type="indexterm" data-primary="ML lifecycle" data-secondary="model development" data-tertiary="environmental impact assessments" id="id1691"/><a contenteditable="false" data-type="indexterm" data-primary="model development" data-secondary="ML lifecycle" data-tertiary="environmental impact assessments" id="id1692"/><a contenteditable="false" data-type="indexterm" data-primary="foundation models (FMs)" data-secondary="lifecycle" data-tertiary="environmental impact assessments" id="id1693"/><a contenteditable="false" data-type="indexterm" data-primary="machine learning (ML)" data-secondary="ML lifecycle" data-tertiary="environmental impact assessments" id="id1694"/> should be an integral part of the AI development lifecycle. These assessments evaluate both the direct effects (like energy use) and indirect effects (such as enabling high-emission industries) of deploying an AI system. Where risks are identified, mitigation strategies—such as reducing model size, leveraging cloud-based green computing, or introducing policy safeguards—should be put in place.</p>
</div></section>
<section data-type="sect2" class="pagebreak-before" data-pdf-bookmark="Data Preparation"><div class="sect2" id="data_preparation">
<h2 class="less_space">Data Preparation</h2>
<p>Creating responsible AI systems<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="further considerations" data-tertiary="data preparation" id="id1695"/><a contenteditable="false" data-type="indexterm" data-primary="data processing" data-secondary="responsible AI data preparation" id="id1696"/><a contenteditable="false" data-type="indexterm" data-primary="data" data-secondary="selecting for models" data-tertiary="responsible AI" id="id1697"/><a contenteditable="false" data-type="indexterm" data-primary="data" data-secondary="foundation models" data-tertiary="selecting data" id="id1698"/><a contenteditable="false" data-type="indexterm" data-primary="data" data-secondary="training data" data-tertiary="selecting data" id="id1699"/><a contenteditable="false" data-type="indexterm" data-primary="data" data-secondary="responsible AI" id="id1700"/> requires the thoughtful preparation of datasets to ensure fairness and accuracy. A key factor is balancing datasets so that AI models do not inadvertently favor certain groups or outcomes. For instance, in applications like hiring or lending, an unbalanced dataset could lead to biased decisions that unfairly disadvantage specific demographics.</p>
<p>To achieve balanced datasets, it’s important to collect data that is both inclusive and diverse. This means ensuring that the dataset accurately reflects the variety of <span class="keep-together">perspectives</span> and experiences relevant to the AI system’s intended use. For example, if developing a healthcare AI model focused on diagnosing conditions across all age groups, the training data should include a representative sample of patients from different age brackets. Neglecting to do so could result in a model that performs well for one age group but poorly for others.</p>
<p>Beyond collection, data curation plays an essential role in balancing datasets. This involves preprocessing steps like cleaning the data to remove inaccuracies, normalizing data to ensure consistency, and selecting relevant features that contribute meaningfully to the model’s predictions. Data augmentation techniques, such as <a contenteditable="false" data-type="indexterm" data-primary="data" data-secondary="synthetic data" data-tertiary="underrepresented groups" id="id1701"/>generating synthetic examples for underrepresented groups, can also help in achieving balance. Regular auditing of datasets is necessary to identify and correct any emerging biases over time.</p>
<p>Tools like Amazon SageMaker Clarify <a contenteditable="false" data-type="indexterm" data-primary="Amazon SageMaker" data-secondary="SageMaker Clarify" data-tertiary="responsible AI tools" id="id1702"/><a contenteditable="false" data-type="indexterm" data-primary="Amazon SageMaker" data-secondary="SageMaker Data Wrangler" data-tertiary="responsible AI tools" id="id1703"/><a contenteditable="false" data-type="indexterm" data-primary="bias" data-secondary="Amazon SageMaker Clarify and Data Wrangler" id="id1704"/>and SageMaker Data Wrangler can assist in this process. SageMaker Clarify helps identify potential biases in datasets by analyzing the distribution of different features and outcomes. If imbalances are detected, SageMaker Data Wrangler offers methods like random oversampling, random undersampling, and the Synthetic Minority Oversampling Technique (SMOTE) to rebalance the data.</p>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Interpretability Versus Explainability"><div class="sect2" id="interpretability_versus_explainability">
<h2>Interpretability Versus Explainability</h2>
<p>In the context of responsible AI,<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="further considerations" data-tertiary="interpretability versus explainability" id="id1705"/><a contenteditable="false" data-type="indexterm" data-primary="interpretability" data-secondary="responsible AI" id="id1706"/><a contenteditable="false" data-type="indexterm" data-primary="explainable AI (XAI)" data-secondary="responsible AI" id="id1707"/><a contenteditable="false" data-type="indexterm" data-primary="explainable AI (XAI)" data-secondary="interpretability versus" id="id1708"/><a contenteditable="false" data-type="indexterm" data-primary="interpretability" data-secondary="explainable AI (XAI)" data-tertiary="versus interpretability" id="id1709"/> the priority between interpretability (covered in <a data-type="xref" href="ch04.html#chapter_four_understanding_generative_a">Chapter 4</a>) and explainability depends on the risk, regulatory environment, and stakeholders involved:</p>
<dl>
<dt>Interpretability</dt>
<dd><p>This is generally favored when transparency and accountability are paramount, such as in regulated industries.</p></dd>
<dt>Explainability</dt>
<dd><p>Explainability is essential when using complex models that can’t easily be interpreted, but human oversight is still required—for example, in predictive diagnostics or automated hiring.</p></dd>
</dl>
<p>Both are important pillars of responsible AI, but interpretability is often seen as the gold standard when decisions must be clearly understood. <a data-type="xref" href="#table_eight_onedot_interpretability_ver">Table 8-1</a> shows some scenarios for this.</p>
<table class="border" id="table_eight_onedot_interpretability_ver">
<caption><span class="label">Table 8-1. </span>Interpretability versus explainability: when to use each</caption>
<thead>
<tr>
<th>Use case</th>
<th>Goal</th>
<th>Preferred approach</th>
<th>Rationale</th>
</tr>
</thead>
<tbody>
<tr>
<td>Loan approval in a bank</td>
<td>Regulatory compliance, fairness</td>
<td>Interpretability</td>
<td>Clear rules needed for auditability and legal compliance</td>
</tr>
<tr>
<td>Diagnosing rare diseases with AI</td>
<td>High accuracy with human oversight</td>
<td>Explainability</td>
<td>Complex models like deep learning used, but need explanations for decisions</td>
</tr>
<tr>
<td>Resume screening with ML</td>
<td>Bias prevention, HR transparency</td>
<td>Explainability</td>
<td>Must explain why a candidate was filtered out; internal logic may be opaque</td>
</tr>
<tr>
<td>Credit score predictions for consumers</td>
<td>Public trust, clarity</td>
<td>Interpretability</td>
<td>Consumers and regulators must understand how scores are computed</td>
</tr>
</tbody>
</table>
</div></section>
<section data-type="sect2" data-pdf-bookmark="Human-Centered Design"><div class="sect2" id="human_centered_design">
<h2>Human-Centered Design</h2>
<p>Human-centered design (HCD) is<a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="further considerations" data-tertiary="human-centered design" id="id1710"/><a contenteditable="false" data-type="indexterm" data-primary="human-centered design (HCD)" id="id1711"/><a contenteditable="false" data-type="indexterm" data-primary="HCD (human-centered design)" id="id1712"/> when technology is created with the end user in mind. It’s about prioritizing clarity, usability, and fairness. By using HCD, you can provide for amplified decision making. These are key principles:</p>
<dl>
<dt>Clarity</dt>
<dd><p>Information must be presented plainly—no jargon, no ambiguity. For example, a doctor reviewing an AI-recommended treatment plan needs a straightforward explanation of why it was suggested.</p></dd>
<dt>Simplicity</dt>
<dd><p>Less is more. Remove unnecessary data points and highlight what matters. A logistics manager doesn’t need the model’s internal math—just a clear route recommendation and a confidence level.</p></dd>
<dt>Usability</dt>
<dd><p>Interfaces should be intuitive for both tech-savvy and nontechnical users. A loan officer, for instance, should be able to navigate the AI tool without special <span class="keep-together">training.</span></p></dd>
<dt>Reflexivity</dt>
<dd><p>Tools should prompt users to think critically about the decision. A pop-up asking “Is there additional context this system may have missed?” can trigger thoughtful review.</p></dd>
<dt>Accountability</dt>
<dd><p>There must be clear ownership over AI-assisted decisions. If a hiring tool recommends a candidate, the HR professional remains responsible for the final choice.</p></dd>
<dt>Personalization</dt>
<dd><p>Tailor the experience to the user. For example, a customer service AI can adapt its tone and suggestions based on an agent’s interaction style.</p></dd>
<dt>Cognitive apprenticeship</dt>
<dd><p>Just as junior employees learn by shadowing experts, AI systems should learn from experienced users through examples and corrections.</p></dd>
<dt>User-centered tools</dt>
<dd><p>Make systems inclusive and accessible. A training platform should work equally well for an entry-level employee with a visual impairment and a senior manager with limited AI knowledge.</p></dd>
</dl>
</div></section>
<section data-type="sect2" data-pdf-bookmark="RLHF"><div class="sect2" id="rlhf">
<h2>RLHF</h2>
<p>In <a data-type="xref" href="ch04.html#chapter_four_understanding_generative_a">Chapter 4</a>, we briefly<a contenteditable="false" data-type="indexterm" data-primary="reinforcement learning from human feedback (RLHF)" data-secondary="responsible AI" id="id1713"/><a contenteditable="false" data-type="indexterm" data-primary="responsible AI" data-secondary="further considerations" data-tertiary="reinforcement learning from human feedback" id="id1714"/> covered RLHF. This is where models learn to make better decisions by incorporating human preferences. RLHF plays an important role in responsible AI by aligning model behavior with human values, ethics, and expectations, helping to reduce harmful or biased outputs. It supports the creation of AI systems that are not only more accurate but also more transparent, fair, and aligned with societal norms.</p>
<p>Imagine developing a virtual assistant designed to help users manage their daily tasks. Initially, the assistant might suggest reminders or schedule meetings based on general patterns. However, users might prefer certain suggestions over others. By observing which suggestions users accept or reject and gathering feedback on their preferences, the assistant can learn to tailor its recommendations more effectively.</p>
<p>These are some of the advantages of RLHF:</p>
<dl>
<dt>Enhanced model performance</dt>
<dd><p>Models can refine their outputs to better meet user expectations. This can lead to improved accuracy and relevance.</p></dd>
<dt>Handling complex scenarios</dt>
<dd><p>In situations where it’s challenging to define explicit rules, human feedback provides nuanced guidance.</p></dd>
<dt>Improved user satisfaction</dt>
<dd><p>Models that adapt based on user preferences tend to provide more personalized and satisfactory experiences. This helps to foster greater user trust and engagement.</p></dd>
</dl>
<p>Platforms like Amazon SageMaker Ground Truth<a contenteditable="false" data-type="indexterm" data-primary="Amazon SageMaker" data-secondary="SageMaker Ground Truth" id="id1715"/> provide capabilities to incorporate RLHF into the ML lifecycle. For instance, data annotators can review model outputs, ranking or classifying them based on quality. This feedback serves as a valuable input for training models. This allows them to align more closely with human judgments and expectations.<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="c08addl" id="id1716"/></p>
</div></section>
</div></section>
<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="conclusion-id000014">
<h1>Conclusion</h1>
<p>Responsible AI involves developing AI systems ethically, safely, and transparently. It is about managing risks like toxicity, intellectual property disputes, job displacement, and accuracy issues.</p>
<p>In this chapter, we learned about the core principles of responsible AI and their use cases. We also saw the various tools from AWS that can help with the process, like Amazon Bedrock and SageMaker Clarify.</p>
<p>In the next chapter, we’ll look at security, compliance, and governance for AI <span class="keep-together">solutions.</span></p>
</div></section>

<section data-type="sect1" data-pdf-bookmark="Quiz"><div class="sect1" id="ch8quiz">
	<h1>Quiz</h1>

<p>To check your answers, please refer to the <a data-type="xref" href="app02.html#answers_ch_8">“Chapter 8 Answer Key”</a>.</p>

<ol>
<li><p>Which of the following methods can help reduce overfitting in an AI model?</p>
<ol type="a">
<li><p>Increasing the complexity of the model</p></li>
<li><p>Adding more noisy data</p></li>
<li><p>Stopping training early</p></li>
<li><p>Avoiding hyperparameters</p></li>
</ol>
</li>

<li><p>What is one technique companies use to address intellectual property concerns in generative AI?</p>
<ol type="a">
<li><p>Removing all training data from public sources</p></li>
<li><p>Restricting access to AI tools</p></li>
<li><p>Limiting AI to internal company use only</p></li>
<li><p>Creating licensing agreements with content providers</p></li>
</ol>
</li>

<li><p>Why is accuracy in AI models considered key to responsible AI?</p>
<ol type="a">
<li><p>It improves reliability, trust, and safety.</p></li>
<li><p>Accurate models require fewer updates and patches.</p></li>
<li><p>Accuracy makes models less costly.</p></li>
<li><p>Accuracy only matters for visual models.</p></li>
</ol>
</li>
</ol>

<ol class="less_space pagebreak-before" start="4">
<li><p>What is the main purpose of fairness in AI systems?</p>
<ol type="a">
<li><p>Increasing the sophistication of models</p></li>
<li><p>Reducing latency in decision making</p></li>
<li><p>Enhancing personalization features</p></li>
<li><p>Avoiding discrimination against individuals or groups</p></li>
</ol>
</li>

<li><p>How does explainability compare to transparency?</p>
<ol type="a">
<li><p>Explainability focuses on user interface design.</p></li>
<li><p>Explainability explains model decisions; transparency shares system details.</p></li>
<li><p>They mean the same thing.</p></li>
<li><p>Transparency is only required in open source models.</p></li>
</ol>
</li>

<li><p>What do privacy and security in AI primarily aim to protect?</p>
<ol type="a">
<li><p>Model weights and parameters</p></li>
<li><p>Algorithm transparency</p></li>
<li><p>Individual data and usage control</p></li>
<li><p>Developer intellectual property</p></li>
</ol>
</li>
</ol>
</div></section>
<div data-type="footnotes"><p data-type="footnote" id="ch01fn26"><sup><a href="ch08.html#ch01fn26-marker">1</a></sup> Jake Coyle, <a href="https://oreil.ly/KrpWD">“In Hollywood Writers’ Battle Against AI, Humans Win (for Now)”</a>, Associated Press, September 27, 2023.</p><p data-type="footnote" id="ch01fn28"><sup><a href="ch08.html#ch01fn28-marker">2</a></sup> Sanya Mansoor, <a href="https://oreil.ly/lE4hb">“A Viral Tweet Accused Apple’s New Credit Card of Being ‘Sexist’”</a>, <em>Time</em>, November 12, 2019.</p></div></div></section></div></div></body></html>