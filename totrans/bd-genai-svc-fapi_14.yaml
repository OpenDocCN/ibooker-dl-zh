- en: Chapter 10\. Optimizing AI Services
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 10 章\. 优化 AI 服务
- en: In this chapter, you’ll learn to further optimize your services via prompt engineering,
    model quantization, and caching mechanisms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何通过提示工程、模型量化和缓存机制进一步优化你的服务。
- en: Optimization Techniques
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化技术
- en: The objectives of optimizing an AI service are to either improve output quality
    or performance (latency, throughput, costs, etc.).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 优化 AI 服务的目标是提高输出质量或性能（延迟、吞吐量、成本等）。
- en: 'Performance-related optimizations include the following:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 与性能相关的优化包括以下内容：
- en: Using batch processing APIs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用批处理 API
- en: Caching (keyword, semantic, context, or prompt)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存（关键字、语义、上下文或提示）
- en: Model quantization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型量化
- en: 'Quality-related optimizations include the following:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 与质量相关的优化包括以下内容：
- en: Using structured outputs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用结构化输出
- en: Prompt engineering
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程
- en: Model fine-tuning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型微调
- en: Let’s review each in more detail.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地回顾每一个。
- en: Batch Processing
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理
- en: Often you want an LLM to process batches of entries at the same time. The most
    obvious solution is to submit multiple API calls per entry. However, the obvious
    approach can be costly and slow and may lead to your model provider rate limiting
    you.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，你希望 LLM 同时处理多个条目。最明显的解决方案是每个条目提交多个 API 调用。然而，这种明显的方法可能成本高昂且速度慢，可能会导致你的模型提供商限制你的速率。
- en: 'In such cases, you can leverage two separate techniques for batch processing
    your data through an LLM:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你可以利用两种不同的技术通过 LLM 批处理你的数据：
- en: Updating your structured output schemas to return multiple examples at the same
    time
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新你的结构化输出模式以同时返回多个示例
- en: Identifying and using model provider APIs that are designed for batch processing
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别和使用为批处理设计的好模型提供商 API
- en: The first solution requires you to update your Pydantic models or template prompts
    to request a list of outputs per request. In this case, you can batch process
    your data within a handful of requests instead of one per entry.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个解决方案要求你更新你的 Pydantic 模型或模板提示以请求每个请求的输出列表。在这种情况下，你可以在少量请求中批处理你的数据，而不是每个条目一个请求。
- en: An implementation of the first solution is shown in [Example 10-1](#batch_processing_structured_outputs).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个解决方案的实现示例如 [示例 10-1](#batch_processing_structured_outputs) 所示。
- en: Example 10-1\. Updating structured output schema for parsing multiple items
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-1\. 更新解析多个项目的结构化输出模式
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO1-1)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimizing_ai_services_CO1-1)'
- en: Update the Pydantic model to include a list of `Category` models.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 更新 Pydantic 模型以包含一个 `Category` 模型的列表。
- en: You can now pass the new schema alongside a list of document titles to the OpenAI
    client to process multiple entries in a single API call. However, an alternative
    and possibly the best solution will be to use a batch API, if available.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以将新的模式与文档标题列表一起传递给 OpenAI 客户端，以在单个 API 调用中处理多个条目。然而，如果可用，使用批处理 API 可能是一个替代方案，也可能是最佳解决方案。
- en: Luckily, model providers such as OpenAI already supply relevant APIs for such
    use cases. Under the hood, these providers may run task queues to process any
    single batch job in the background while providing you with status updates until
    the batch is complete to retrieve the results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，像 OpenAI 这样的模型提供商已经为这种用例提供了相关的 API。在底层，这些提供商可能会运行任务队列以在后台处理任何单个批处理作业，同时在你获取结果之前提供状态更新。
- en: Compared to using standard endpoints directly, you’ll be able to send asynchronous
    groups of requests with lower costs (up to 50% with OpenAI^([1](ch10.html#id1069))),
    enjoy higher rate limits, and guarantee completion times. The batch job service
    is ideal for processing jobs that don’t require immediate responses such as using
    OpenAI LLMs to parse, classify, or translate large volumes of documents in the
    background.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接使用标准端点相比，你将能够发送异步的请求组，成本更低（使用 OpenAI 可降低高达 50%[1](ch10.html#id1069)），享受更高的速率限制，并保证完成时间。批处理作业服务非常适合处理不需要即时响应的作业，例如在后台使用
    OpenAI LLM 解析、分类或翻译大量文档。
- en: To submit a batch job, you’ll need a `jsonl` file where each line contains the
    details of an individual request to the API, as shown in [Example 10-2](#jsonl).
    Also as seen in this example, to create the JSONL file, you can iterate over your
    entries and dynamically generate the file.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要提交批处理作业，你需要一个 `jsonl` 文件，其中每行包含对 API 的单个请求的详细信息，如 [示例 10-2](#jsonl) 所示。同样，如本例所示，要创建
    JSONL 文件，你可以遍历你的条目并动态生成文件。
- en: Example 10-2\. Creating a JSONL file from entries
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-2\. 从条目创建 JSONL 文件
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once created, you can submit the file to the batch API for processing, as shown
    in [Example 10-3](#batch_processing_api).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建，您可以将文件提交给批处理 API 进行处理，如[示例 10-3](#batch_processing_api)所示。
- en: Example 10-3\. Processing batch jobs with the OpenAI Batch API
  id: totrans-31
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-3\. 使用 OpenAI 批处理 API 处理批处理作业
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can now leverage offline batch endpoints to process multiple entries in
    one go with guaranteed turnaround times and significant cost savings.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以利用离线批处理端点一次处理多个条目，并保证有保证的周转时间和显著的成本节约。
- en: Alongside leveraging structured outputs and batch APIs to optimize your services,
    you can also leverage caching techniques to significantly speed up response times
    and resource costs of your servers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了利用结构化输出和批处理 API 优化您的服务外，您还可以利用缓存技术显著加快服务器的响应时间和资源成本。
- en: Caching
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存
- en: In GenAI services, you’ll often rely on data/model response that require significant
    computations or long processing durations. If you have multiple users requesting
    the same data, repeating the same operations can be wasteful. Instead, you can
    use caching techniques for storing and retrieving frequently accessed data to
    help you optimize your services by speeding up response times, reducing server
    load, and saving bandwidth and operational costs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GenAI 服务中，您通常会依赖于需要大量计算或长时间处理的数据/模型响应。如果您有多个用户请求相同的数据，重复相同的操作可能是浪费的。相反，您可以使用缓存技术来存储和检索频繁访问的数据，以通过加快响应时间、减少服务器负载、节省带宽和运营成本来优化您的服务。
- en: For example, in a public FAQ chatbot where users ask mostly the same questions,
    you may want to reuse the cached responses for longer periods. On the other hand,
    for more personalized and dynamic chatbots, you can frequently refresh (i.e.,
    invalidate) the cached response.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个公共 FAQ 聊天机器人中，用户提出的问题大多相同，您可能希望更长时间地重用缓存的响应。另一方面，对于更个性化和动态的聊天机器人，您可以频繁刷新（即，使缓存失效）缓存的响应。
- en: Tip
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: You should always consider the frequency of cache refreshes based on the nature
    of the data and the acceptable level of staleness.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该始终根据数据的性质和可接受的陈旧程度来考虑缓存刷新的频率。
- en: 'The most relevant caching strategies for GenAI services include:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对 GenAI 服务最相关的缓存策略包括：
- en: Keyword caching
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键字缓存
- en: Semantic caching
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义缓存
- en: Context or prompt caching
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文或提示缓存
- en: Let’s review each in more detail.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地回顾一下。
- en: Keyword caching
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键字缓存
- en: If all you need is a simple caching mechanism for storing functions or endpoint
    responses, you can use *keyword caching*, which involves caching responses based
    on exact matches of input queries as key-value pairs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只需要一个简单的缓存机制来存储函数或端点响应，您可以使用 *关键字缓存*，它涉及根据输入查询的精确匹配作为键值对缓存响应。
- en: In FastAPI, libraries such as `fastapi-cache` can help you implement keyword
    caching in a few lines of code, on any functions or endpoints. FastAPI caches
    also give you the option to attach storage backends such as Redis for centralizing
    the cache store across your instances.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FastAPI 中，`fastapi-cache` 等库可以帮助您通过几行代码在任意函数或端点上实现关键字缓存。FastAPI 缓存还为您提供了附加存储后端（如
    Redis）以集中管理缓存存储的选项。
- en: Tip
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Alternatively, you can implement your own custom caching mechanism with a cache
    store using lower-level packages such as `cachetools`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用像 `cachetools` 这样的底层包来实现自己的自定义缓存机制，使用缓存存储。
- en: 'To get started, all you have to do is to initialize and configure the caching
    system as part of the application lifespan, as shown in [Example 10-4](#caching_lifespan).
    You can install FastAPI cache using the following command:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，您只需在应用程序生命周期内初始化和配置缓存系统，如[示例 10-4](#caching_lifespan)所示。您可以使用以下命令安装 FastAPI
    缓存：
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Example 10-4\. Configuring FastAPI cache lifespan
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-4\. 配置 FastAPI 缓存生命周期
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO2-1)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_optimizing_ai_services_CO2-1)'
- en: Initialize `FastAPICache` with a `RedisBackend` that doesn’t decode responses
    so that cached data is stored as bytes (binary). This is because decoding responses
    would break caching by altering the original response format.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `RedisBackend` 初始化 `FastAPICache`，使其不解码响应，以便缓存数据以字节（二进制）形式存储。这是因为解码响应会通过改变原始响应格式来破坏缓存。
- en: Once the caching system is configured, you can decorate your functions or endpoint
    handlers to cache their outputs, as shown in [Example 10-5](#caching_functions_endpoint).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦配置了缓存系统，您就可以装饰您的函数或端点处理器以缓存它们的输出，如[示例 10-5](#caching_functions_endpoint)所示。
- en: Example 10-5\. Function and endpoint results caching
  id: totrans-57
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-5\. 函数和端点结果缓存
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO3-1)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimizing_ai_services_CO3-1)'
- en: The `cache()` decorator must always come last. Invalidate the cache in 60 seconds
    by setting `expires=60` to recompute the outputs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`cache()` 装饰器必须始终放在最后。通过设置 `expires=60` 来在60秒内使缓存失效，以重新计算输出。'
- en: The `cache()` decorator shown in [Example 10-5](#caching_functions_endpoint)
    injects dependencies for the `Request` and `Response` objects so that it can add
    cache control headers to the outgoing response. These cache control headers instruct
    clients how to cache the responses on their side by specifying a set of directives
    (i.e., instructions).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [示例 10-5](#caching_functions_endpoint) 中显示的 `cache()` 装饰器注入了 `Request` 和 `Response`
    对象的依赖项，以便它可以向输出响应添加缓存控制头。这些缓存控制头指示客户端如何通过指定一组指令（即指令）在其端缓存响应。
- en: 'These are a few common cache control directives when sending responses:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在发送响应时的一些常见缓存控制指令：
- en: '`max-age`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`max-age`'
- en: Defines the maximum amount of time (in seconds) that a response is considered
    fresh
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 定义响应被视为新鲜的最大时间量（以秒为单位）
- en: '`no-cache`'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`no-cache`'
- en: Forces revalidation so that the clients check for constant updates with the
    server
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 强制重新验证，以便客户端检查与服务器之间的持续更新
- en: '`no-store`'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '`no-store`'
- en: Prevents caching entirely
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 完全阻止缓存
- en: '`private`'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`private`'
- en: Stores responses in a private cache (e.g., local caches in browsers)
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在私有缓存中存储响应（例如，浏览器中的本地缓存）
- en: 'A response could have cache control headers like `Cache-Control: max-age=180,
    private` to set these directives.^([2](ch10.html#id1072))'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '响应可以包含如 `Cache-Control: max-age=180, private` 这样的缓存控制头，以设置这些指令.^([2](ch10.html#id1072))'
- en: Since keyword caching works on exact matches, it’s more suitable for functions
    and APIs that expect frequently repeated matching inputs. However, in GenAI services
    that accept variable user queries, you may want to consider other caching mechanisms
    that rely on the meaning of inputs when returning a cached response. This is where
    semantic caching can prove useful.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 由于关键字缓存基于精确匹配，因此它更适合期望频繁重复匹配输入的函数和API。然而，在接受变量用户查询的GenAI服务中，您可能需要考虑其他缓存机制，这些机制在返回缓存响应时依赖于输入的意义。这正是语义缓存可以证明有用的地方。
- en: Semantic caching
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语义缓存
- en: '*Semantic caching* is a caching mechanism that returns a stored value based
    on similar inputs.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*语义缓存* 是一种缓存机制，根据相似输入返回存储的值。'
- en: Under the hood, the system uses encoders and embedding vectors to capture semantics
    and meanings of inputs. It then performs similarity searches across stored key-value
    pairs to return a cached response.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，系统使用编码器和嵌入向量来捕获输入的语义和意义。然后，它在存储的关键值对之间执行相似性搜索，以返回缓存响应。
- en: 'In comparison to keyword caching, similar inputs can return the same cached
    response. Inputs to the system don’t have to be identical to be recognized as
    similar. Even if such inputs have different sentence structures or formulations
    or contain inaccuracies, they’ll still be captured as similar for carrying the
    same meanings. And, the same response is being requested. As an example, the following
    queries are considered similar for carrying the same intent:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 与关键字缓存相比，相似输入可以返回相同的缓存响应。输入到系统中的内容不必完全相同才能被识别为相似。即使这些输入具有不同的句子结构或表述，或包含不准确之处，它们仍然会被捕获为相似，以携带相同的意义。并且，请求的是相同的响应。以下查询被认为是携带相同意图的相似查询：
- en: How do you build generative services with FastAPI?
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您如何使用FastAPI构建生成性服务？
- en: What is the process of developing FastAPI services for GenAI?
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发用于GenAI的FastAPI服务的流程是什么？
- en: This caching system contributes to significant cost savings^([3](ch10.html#id1076))
    by reducing API calls to [30–40%](https://oreil.ly/gjGz6) (i.e., 60–70% cache
    hit rate) depending on the use case and size of the user base. For instance, Q&A
    RAG applications that receive frequently asked questions across a large user base
    could reduce API calls by 69% using a semantic cache.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 该缓存系统通过减少API调用至 [30–40%](https://oreil.ly/gjGz6)（即，60–70% 缓存命中率）来显著降低成本^([3](ch10.html#id1076))，具体取决于用例和用户规模的大小。例如，接收大量用户常见问题的问答RAG应用可以使用语义缓存将API调用减少69%。
- en: 'Within a typical RAG system, there are two places where having a cache can
    reduce resource-intensive and time-consuming operations:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的RAG系统中，有两个地方拥有缓存可以减少资源密集型和耗时操作：
- en: '*Before the LLM* to return a cached response instead of generating a new one'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在LLM之前* 返回缓存响应而不是生成新的响应'
- en: '*Before the vector store* to enrich prompts with cached documents instead of
    searching and retrieving fresh ones'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在向量存储之前*，用缓存文档丰富提示而不是搜索和检索新鲜文档'
- en: 'When integrating a semantic cache component into your RAG system, you should
    consider whether returning a cached response could negatively impact your application’s
    user experience. For instance, if caching the LLM responses, both of the following
    queries would return the same response due to their high semantic similarity,
    causing the semantic caching system to treat them as nearly identical:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当将语义缓存组件集成到您的 RAG 系统中时，您应该考虑返回缓存响应是否会对您的应用程序的用户体验产生负面影响。例如，如果缓存 LLM 响应，由于它们的高度语义相似性，以下两个查询都会返回相同的响应，导致语义缓存系统将它们视为几乎相同：
- en: Summarize this text in 100 words
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用 100 个字总结此文本
- en: Summarize this text in 50 words
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用 50 个字总结此文本
- en: This makes it feel like your services aren’t responding to queries. As you may
    still want varied LLM outputs in your application, we’re going to implement a
    document retrieval semantic cache for your RAG system. [Figure 10-1](#semantic_cache)
    shows the full system architecture.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得您的服务感觉像是在对查询没有响应。由于您可能仍然希望在您的应用程序中获取不同的 LLM 输出，我们将为您的 RAG 系统实现一个文档检索语义缓存。[图
    10-1](#semantic_cache) 展示了完整的系统架构。
- en: '![bgai 1001](assets/bgai_1001.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 1001](assets/bgai_1001.png)'
- en: Figure 10-1\. Semantic caching in RAG system architecture
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. RAG 系统架构中的语义缓存
- en: Let’s start by implementing the semantic caching system from scratch first,
    and then we’ll review how to offload the functionality to an external library
    such as `gptcache`.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先从从头开始实现语义缓存系统开始，然后我们将回顾如何将功能卸载到外部库，如 `gptcache`。
- en: Building a semantic caching service from scratch
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从零开始构建语义缓存服务
- en: 'You can implement a semantic caching system by implementing the following components:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过实现以下组件来实施一个语义缓存系统：
- en: A cache store client
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存存储客户端
- en: A document vector store client
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档向量存储客户端
- en: An embedding model
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入模型
- en: '[Example 10-6](#semantic_cache_cache_store) shows how to implement the cache
    store client.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 10-6](#semantic_cache_cache_store) 展示了如何实现缓存存储客户端。'
- en: Example 10-6\. Cache store client
  id: totrans-96
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-6\. 缓存存储客户端
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO4-1)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimizing_ai_services_CO4-1)'
- en: Initialize a Qdrant client running on memory acting as a cache store.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化一个在内存上运行的 Qdrant 客户端，充当缓存存储。
- en: Once the cache store client is initialized, you can configure the document vector
    store by following [Example 10-7](#semantic_cache_doc_store).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始化缓存存储客户端，您可以通过遵循 [示例 10-7](#semantic_cache_doc_store) 来配置文档向量存储。
- en: Example 10-7\. Document store client
  id: totrans-101
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-7\. 文档存储客户端
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO5-1)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimizing_ai_services_CO5-1)'
- en: Load a collection of documents into the Qdrant vector store.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 将文档集合加载到 Qdrant 向量存储中。
- en: With both the cache and document vector store clients ready, you can now implement
    the semantic cache service, as shown in [Example 10-8](#semantic_cache_service),
    with methods to compute embeddings and performing cache searches.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在缓存和文档向量存储客户端准备就绪后，您现在可以实施语义缓存服务，如 [示例 10-8](#semantic_cache_service) 所示，包括计算嵌入和执行缓存搜索的方法。
- en: Example 10-8\. Semantic caching system
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-8\. 语义缓存系统
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO6-1)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimizing_ai_services_CO6-1)'
- en: Set a similarity threshold. Any score above this threshold will be a cache hit.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 设置一个相似度阈值。任何高于此阈值的分数都将被视为缓存命中。
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO6-2)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_optimizing_ai_services_CO6-2)'
- en: Query the document store if there is no cache hit. Cache the retrieved documents
    against the vector embedding of the query as the cache key.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有缓存命中，查询文档存储。将检索到的文档与查询的向量嵌入作为缓存键进行缓存。
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO6-3)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_optimizing_ai_services_CO6-3)'
- en: If there is no related document or cache available for the given query, return
    a canned answer.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有相关文档或缓存可用于给定的查询，则返回一个预设的答案。
- en: Now that you have a semantic caching service, you can use it to retrieve cached
    documents from memory by following [Example 10-9](#semantic_cache_qdrant_usage).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经拥有了一个语义缓存服务，您可以通过遵循 [示例 10-9](#semantic_cache_qdrant_usage) 来使用它从内存中检索缓存文档。
- en: Example 10-9\. Implementing a semantic cache in a RAG system with Qdrant
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-9\. 在 RAG 系统中使用 Qdrant 实现语义缓存
- en: '[PRE9]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You should now have a better understanding of how to implement your own custom
    semantic caching systems using a vector database client.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在应该更好地理解了如何使用向量数据库客户端实现您自己的自定义语义缓存系统。
- en: Semantic caching with GPT cache
  id: totrans-118
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 GPT 缓存的语义缓存
- en: If you don’t need to develop your own semantic caching service from scratch,
    you can also use the modular `gptcache` library that gives you the option to swap
    various storage, caching, and embedding components.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不需要从头开始开发自己的语义缓存服务，你也可以使用模块化的 `gptcache` 库，它为你提供了交换各种存储、缓存和嵌入组件的选项。
- en: 'To configure a semantic cache with `gptcache`, you first need to install the
    package:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置 `gptcache` 的语义缓存，首先需要安装该包：
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Then load the system on application start, as shown in [Example 10-10](#configrue_gptcache).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在应用程序启动时加载系统，如[示例 10-10](#configrue_gptcache)所示。
- en: Example 10-10\. Configuring the GPT cache
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-10\. 配置 GPT 缓存
- en: '[PRE11]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO7-1)'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_optimizing_ai_services_CO7-1)'
- en: Select a post-processing callback function to select a random item from the
    returned cached items.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个后处理回调函数以从返回的缓存项中随机选择一个项目。
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO7-2)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)(#co_optimizing_ai_services_CO7-2)'
- en: Select a pre-embedding callback function to use the last query for setting a
    new cache.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个预嵌入回调函数以使用最后一个查询来设置新的缓存。
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO7-3)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)(#co_optimizing_ai_services_CO7-3)'
- en: Use the ONNX embedding model for computing embedding vectors.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用ONNX嵌入模型来计算嵌入向量。
- en: '[![4](assets/4.png)](#co_optimizing_ai_services_CO7-4)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)(#co_optimizing_ai_services_CO7-4)'
- en: Use `OnnxModelEvaluation` to compute similarity scores between cached items
    and a given query.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `OnnxModelEvaluation` 来计算缓存项与给定查询之间的相似度分数。
- en: '[![5](assets/5.png)](#co_optimizing_ai_services_CO7-5)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png)(#co_optimizing_ai_services_CO7-5)'
- en: Set the caching configuration options such as a similarity threshold.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 设置缓存配置选项，例如相似度阈值。
- en: '[![6](assets/6.png)](#co_optimizing_ai_services_CO7-6)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '![6](assets/6.png)(#co_optimizing_ai_services_CO7-6)'
- en: Provide an OpenAI client API key for GPT Cache to automatically perform semantic
    caching on LLM API responses.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为 GPT 缓存提供一个 OpenAI 客户端 API 密钥，以便自动在 LLM API 响应上执行语义缓存。
- en: Once `gptcache` is initialized, it will integrate seamlessly with the OpenAI
    LLM client across your application. You can now make multiple LLM queries, as
    shown in [Example 10-11](#semantic_caching_gptcache), knowing that `gptcache`
    will be caching your LLM responses.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 `gptcache` 初始化，它将与你的应用程序中的 OpenAI LLM 客户端无缝集成。现在你可以进行多个 LLM 查询，如[示例 10-11](#semantic_caching_gptcache)所示，知道
    `gptcache` 将会缓存你的 LLM 响应。
- en: Example 10-11\. Semantic caching with the GPT cache
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-11\. 使用 GPT 缓存的语义缓存
- en: '[PRE12]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Using external libraries like `gptcache`, as shown in [Example 10-11](#semantic_caching_gptcache),
    makes implementing semantic caching straightforward.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如[示例 10-11](#semantic_caching_gptcache)所示，使用外部库如 `gptcache` 使得实现语义缓存变得简单直接。
- en: Once the caching system is up and running, you can adjust *similarity thresholds*
    to tune the system’s cache hit rates.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦缓存系统启动并运行，你可以调整*相似度阈值*来调整系统的缓存命中率。
- en: Similarity threshold
  id: totrans-142
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 相似度阈值
- en: When building a semantic caching service, you may need to adjust the similarity
    threshold based on provided queries to achieve high cache hit rates that are accurate.
    You can refer to the [interactive visualization of semantic cache clusters](https://semanticcachehit.com)
    shown in [Figure 10-2](#semantic_cache_visualization) to better understand the
    concept of similarity threshold.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建语义缓存服务时，你可能需要根据提供的查询调整相似度阈值，以实现高缓存命中率并保持准确性。你可以参考[图 10-2](#semantic_cache_visualization)中显示的[语义缓存集群的交互式可视化](https://semanticcachehit.com)，以更好地理解相似度阈值的概念。
- en: Increasing the threshold value in [Figure 10-2](#semantic_cache_visualization)
    will result in a less connected graph, while minimizing can produce false positives.
    Therefore, you may want to run a few experiments to fine-tune the similarity threshold
    for your own application.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 10-2](#semantic_cache_visualization)中增加阈值值将导致连接度较低的图，而最小化可能产生假阳性。因此，你可能需要进行一些实验来微调你应用的相似度阈值。
- en: '![bgai 1002](assets/bgai_1002.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 1002](assets/bgai_1002.png)'
- en: 'Figure 10-2\. Visualization of semantic caching (Source: [semanticcachehit.com](https://semanticcachehit.com))'
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 语义缓存的可视化（来源：[semanticcachehit.com](https://semanticcachehit.com)）
- en: Eviction policies
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 驱逐策略
- en: Another concept relevant to caching is *eviction policies* that control the
    caching behavior when the caching mechanism reaches its maximum capacity. Selecting
    the appropriate eviction policy should be appropriate for your own use case.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 与缓存相关的另一个概念是*驱逐策略*，它控制当缓存机制达到最大容量时的缓存行为。选择合适的驱逐策略应适合你的特定用例。
- en: Tip
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Since the size of cache memory stores is often limited, you can add an `evict()`
    method to the `SemanticCachingService` you implemented in [Example 10-8](#semantic_cache_service).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缓存存储的大小通常有限，你可以在你实现的 [示例 10-8](#semantic_cache_service) 中的 `SemanticCachingService`
    中添加一个 `evict()` 方法。
- en: '[Table 10-1](#eviction_policies) shows a few eviction policies you can choose
    from.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 10-1](#eviction_policies) 展示了你可以选择的一些淘汰策略。'
- en: Table 10-1\. Eviction policies
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-1\. 淘汰策略
- en: '| Policy | Description | Use case |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 策略 | 描述 | 用例 |'
- en: '| --- | --- | --- |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| First in, first out (FIFO) | Removes oldest items | When all items have the
    same priority |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 先进先出 (FIFO) | 移除最旧的项 | 当所有项具有相同的优先级时 |'
- en: '| Least recently used (LRU) | Tracks cache usage across time and removes the
    least recently accessed item | When recently accessed items are more likely to
    be accessed again |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 最近最少使用 (LRU) | 跟踪缓存使用情况随时间变化，并移除最近最少访问的项目 | 当最近访问的项目更有可能再次被访问时 |'
- en: '| Least frequently used (LFU) | Tracks cache usage across time and removes
    the least frequently accessed item | When less frequently used items should be
    removed first |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 最少使用 (LFU) | 跟踪缓存使用情况随时间变化，并移除最少访问的项目 | 当应首先移除较少使用的项目时 |'
- en: '| Most recently used (MRU) | Tracks cache usage across time and removes the
    most recently accessed item | Rarely used, used when most recently used items
    are less likely to be accessed again |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 最近最常使用 (MRU) | 跟踪缓存使用情况随时间变化，并移除最近访问的项目 | 很少使用，当最近访问的项目不太可能再次被访问时 |'
- en: '| Random replacement (RR) | Removes a random item from the cache | Simple and
    fast, used when it doesn’t impact performance |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 随机替换 (RR) | 从缓存中随机移除一个项目 | 简单快捷，在不影响性能的情况下使用 |'
- en: Choosing the right eviction policy will depend on your use case and application
    requirements. Generally, you can start with the LRU policy before switching to
    alternatives.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的淘汰策略将取决于你的用例和应用需求。通常，你可以从 LRU 策略开始，然后再切换到其他替代方案。
- en: You should now feel more confident in implementing semantic caching mechanisms
    that apply to document retrieval or model responses. Next, let’s learn about context
    or prompt caching, which optimizes queries to models based on their inputs.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，你应该更有信心实现适用于文档检索或模型响应的语义缓存机制。接下来，让我们了解上下文或提示缓存，它根据输入优化对模型的查询。 '
- en: Context/prompt caching
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文/提示缓存
- en: '*Context caching*, also known as *prompt caching*, is a caching mechanism suitable
    for scenarios where you’re referencing large amounts of context repeatedly within
    small requests. It’s designed to reuse precomputed attention states from frequently
    reused prompts, eliminating the need for redundant recomputation of the entire
    input context each time a new request is made.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '*上下文缓存*，也称为 *提示缓存*，是一种适用于在小型请求中重复引用大量上下文场景的缓存机制。它旨在重用频繁使用的提示的预计算注意力状态，从而消除每次新请求时对整个输入上下文的冗余重新计算的需求。'
- en: 'You should consider using a context cache when your services involve the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的服务涉及以下情况时，你应该考虑使用上下文缓存：
- en: Chatbots with extensive system instructions and long multiturn conversations
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有广泛系统指令和长对话的聊天机器人
- en: Repetitive analysis of lengthy video files
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对长视频文件进行重复分析
- en: Recurring queries against large document sets
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对大型文档集进行重复查询
- en: Frequent code repository analysis or bug fixing
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经常进行代码仓库分析或错误修复
- en: Document summarizations, talking to books, papers, documentation, podcast transcripts
    and other long form content
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档摘要、与书籍、论文、文档、播客文稿和其他长篇内容进行交流
- en: Providing a large number of examples in prompt (i.e., in-context learning)
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示中提供大量示例（即，上下文学习）
- en: This type of caching can help you to substantially reduce token usage costs
    by caching large context tokens. According to Anthropic, prompt caching can reduce
    costs by up to 90% and latency by up to 85% for long prompts.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的缓存可以帮助你通过缓存大型上下文标记来显著减少标记使用成本。据 Anthropic 称，提示缓存可以将成本降低高达 90%，并将延迟降低高达
    85%。
- en: 'The authors of the [prompt caching paper](https://oreil.ly/augpd) that presents
    this technique also claim that:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 提出这种技术的 [提示缓存论文](https://oreil.ly/augpd) 的作者还声称：
- en: We find that Prompt Cache significantly reduces latency in time-to-first-token,
    especially for longer prompts such as document-based question answering and recommendations.
    The improvements range from 8× for GPU-based inference to 60× for CPU-based inference,
    all while maintaining output accuracy and without the need for model parameter
    modifications.
  id: totrans-173
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们发现，提示缓存显著降低了首次标记的时间延迟，尤其是在基于文档的问答和推荐等较长的提示中。改进范围从基于GPU推理的8倍到基于CPU推理的60倍，同时保持输出准确性，且无需修改模型参数。
- en: '[Figure 10-3](#context_caching_architecture) visualizes the context caching
    system architecture.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-3](#context_caching_architecture)可视化了上下文缓存系统架构。'
- en: '![bgai 1003](assets/bgai_1003.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 1003](assets/bgai_1003.png)'
- en: Figure 10-3\. System architecture for context caching
  id: totrans-176
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-3\. 上下文缓存的系统架构
- en: At the time of writing, OpenAI automatically implements prompt caching for all
    API requests without requiring any code changes or additional costs. [Example 10-12](#context_cachin_anthropic)
    shows an example of how to use prompt caching when using the Anthropic API.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，OpenAI自动为所有API请求实现提示缓存，无需进行任何代码更改或额外费用。[示例10-12](#context_cachin_anthropic)展示了在使用Anthropic
    API时如何使用提示缓存。
- en: Example 10-12\. Context/prompt caching with Anthropic API
  id: totrans-178
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例10-12\. 使用Anthropic API进行上下文/提示缓存
- en: '[PRE13]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO8-1)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_optimizing_ai_services_CO8-1)'
- en: Prompt caching is available only with a handful of models including Claude 3.5
    Sonnet, Claude 3 Haiku, and Claude 3 Opus.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 提示缓存仅适用于包括Claude 3.5 Sonnet、Claude 3 Haiku和Claude 3 Opus在内的少数模型。
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO8-2)'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)(#co_optimizing_ai_services_CO8-2)'
- en: Use the `cache_control` parameter to reuse the large document content across
    multiple API calls without processing it each time.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`cache_control`参数在多个API调用之间重用大文档内容，而无需每次都处理它。
- en: 'Under the hood, the Anthropic client will add `anthropic-beta: prompt-caching-2024-07-31`
    to the request headers.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '在幕后，Anthropic客户端将`anthropic-beta: prompt-caching-2024-07-31`添加到请求头中。'
- en: At the time of writing, `ephemeral` is the only supported cache type, which
    corresponds to a 5-minute cache lifetime.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，`ephemeral`是唯一支持的缓存类型，对应5分钟的缓存生命周期。
- en: Note
  id: totrans-186
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: As soon as you adopt a context cache, you’re introducing statefulness in requests
    by preserving tokens across them. This means the data you submit in one request
    will affect later requests, as the model provider server can use the cached context
    to maintain continuity between interactions.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您采用上下文缓存，您就在请求中引入了状态性，通过在它们之间保留标记。这意味着您在一个请求中提交的数据将影响后续请求，因为模型提供者服务器可以使用缓存的上下文来保持交互之间的连续性。
- en: With the Gemini API’s context caching feature, you can provide content to the
    model once, cache the input tokens, and reference these cached tokens for future
    requests.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Gemini API的上下文缓存功能，您只需向模型提供一次内容，缓存输入标记，并在未来的请求中引用这些缓存的标记。
- en: Using these cached tokens can save you significant expenses if you avoid repeatedly
    passing in the same corpus of tokens in high volumes. The caching cost will depend
    on the size of the input tokens and the desired time to live (TTL) storage duration.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些缓存的标记可以为您节省大量费用，如果您避免在高量级上重复传递相同的标记语料库。缓存成本将取决于输入标记的大小和期望的TTL存储时长。
- en: Tip
  id: totrans-190
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: When you cache a set of tokens, you can specify a TTL duration, which is how
    long the cache should exist before the tokens are automatically deleted. By default,
    TTL is normally set to 1 hour.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当您缓存一组标记时，您可以指定TTL持续时间，即标记在自动删除之前应存在的时长。默认情况下，TTL通常设置为1小时。
- en: 'You can see how to use a cached system instruction in [Example 10-13](#context_caching_google).
    You will also need the Gemini API Python SDK:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[示例10-13](#context_caching_google)中看到如何使用缓存系统指令。你还需要Gemini API Python SDK：
- en: '[PRE14]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Example 10-13\. Context caching with the Google Gemini API
  id: totrans-194
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例10-13\. 使用Google Gemini API进行上下文缓存
- en: '[PRE15]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO9-1)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_optimizing_ai_services_CO9-1)'
- en: Provide a display name as a cache key or identifier.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 提供一个显示名称作为缓存键或标识符。
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO9-2)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)(#co_optimizing_ai_services_CO9-2)'
- en: Pass the corpus to the context caching system. The minimum size of a context
    cache is 32,768 tokens.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 将语料库传递给上下文缓存系统。上下文缓存的最小大小为32,768个标记。
- en: 'If you run [Example 10-13](#context_caching_google) and print the `response.usage_metadata`,
    you should receive the following output:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行[示例10-13](#context_caching_google)并打印`response.usage_metadata`，您应该收到以下输出：
- en: '[PRE16]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Notice how much of the `prompt_token_count` is now being cached when you compare
    it with the `cached_content_token_count`. The `candidates_token_count` refers
    to count of output or response tokens coming from the model, which isn’t affected
    by the caching system.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 注意比较`prompt_token_count`和`cached_content_token_count`时，现在有多少`prompt_token_count`被缓存了。`candidates_token_count`指的是来自模型的输出或响应标记的数量，这不受缓存系统的影响。
- en: Warning
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Gemini models don’t make any distinction between cached tokens and regular input
    tokens. Cached content will be prefixed to the prompt. This is why the prompt
    token count isn’t reduced when using caching.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Gemini模型在缓存标记和常规输入标记之间没有区别。缓存的内容将被添加到提示的前面。这就是为什么使用缓存时提示标记计数不会减少。
- en: With context caching, you won’t see a drastic reduction in response times but
    instead will significantly reduce operational costs as you avoid resending extensive
    system prompts and context tokens. Therefore, this caching strategy is most suitable
    when you have a large context to work with—for instance, when batch processing
    files with extensive instructions and examples.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上下文缓存，你不会看到响应时间的显著减少，但可以显著降低运营成本，因为你避免了重新发送大量的系统提示和上下文标记。因此，这种缓存策略最适合当你有一个大上下文要处理时——例如，当你批量处理包含大量指令和示例的文件时。
- en: Note
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Using the same context cache and prompt doesn’t guarantee consistent model responses
    because the responses from LLMs are nondeterministic. A context cache doesn’t
    cache any output.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的上下文缓存和提示不能保证模型响应的一致性，因为LLM的响应是非确定性的。上下文缓存不会缓存任何输出。
- en: Context caching remains an active area of research. If you want to avoid any
    vendor lock-in, there is already some progress in this field with open source
    tools such as [*MemServe*](https://oreil.ly/PXm6B), which implements context caching
    with an elastic memory pool.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文缓存仍然是研究的一个活跃领域。如果你想避免任何供应商锁定，这个领域已经有一些进展，例如开源工具[*MemServe*](https://oreil.ly/PXm6B)，它通过弹性内存池实现了上下文缓存。
- en: Beyond caching, you can also review your options for reducing model size to
    speed up response times using techniques such as *model quantization*.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 除了缓存之外，你还可以通过使用如*模型量化*等技术来审查你的选项，以减少模型大小，从而加快响应时间。
- en: Model Quantization
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型量化
- en: If you’re going to be serving models such as LLMs yourself, you should consider
    *quantizing* (i.e., compressing/shrinking) your models if possible. Often, open
    source model repositories will also supply quantized versions that you can download
    and use straightaway without having to go through the quantization process yourself.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打算自己提供LLM等模型的服务，你应该考虑如果可能的话对模型进行*量化*（即压缩/缩小）。通常，开源模型存储库也会提供量化版本，你可以直接下载和使用，而无需自己进行量化过程。
- en: '*Model quantization* is the adjustment process on the model weights and activations
    where high-precision model parameters are statistically projected into lower-precision
    values through a fine-tuning operation using scaling factors on the original parameter
    distribution. You can then perform all critical inference operations with lower
    precision, after which you can convert the outputs to higher precision to maintain
    the quality while improving performance.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型量化*是对模型权重和激活的调整过程，其中通过使用原始参数分布上的缩放因子进行微调操作，将高精度模型参数统计投影到低精度值。然后你可以使用低精度执行所有关键推理操作，之后可以将输出转换为高精度，以保持质量同时提高性能。'
- en: Reducing the precision also decreases the memory storage requirements, theoretically
    lowering energy consumption and speeding up operations like matrix multiplication
    through integer arithmetic. This also enables models to run on embedded devices,
    which may only support integer data types.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 降低精度也会降低内存存储需求，理论上降低能耗，并通过整数运算加速矩阵乘法等操作。这也使得模型能够在仅支持整数数据类型的嵌入式设备上运行。
- en: '[Figure 10-4](#quantization_process) shows the full quantization process.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-4](#quantization_process)展示了完整的量化过程。'
- en: '![bgai 1004](assets/bgai_1004.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 1004](assets/bgai_1004.png)'
- en: Figure 10-4\. Quantization process
  id: totrans-216
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-4\. 量化过程
- en: You can save more than a handful of gigabytes in GPU memory consumption as low-precision
    data types such as 8-bit integer would require significantly less RAM per parameter
    than a data type like 32-bit float.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GPU内存消耗上节省超过几个GB，因为低精度数据类型，如8位整数，每个参数所需的RAM比32位浮点数据类型少得多。
- en: Precision versus quality trade-off
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精度与质量权衡
- en: '[Figure 10-5](#quantization) compares a nonquantized model and a quantized
    model.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10-5](#quantization) 比较了未量化和量化的模型。'
- en: '![bgai 1005](assets/bgai_1005.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 1005](assets/bgai_1005.png)'
- en: Figure 10-5\. Quantization
  id: totrans-221
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-5\. 量化
- en: As each high-precision 32-bit float parameter consumes 4 bytes of GPU memory,
    a 1B-parameter model would require 4 GB of memory just for inference. If you plan
    on retraining or fine-tuning the same model, you’ll require at least 24 GB of
    GPU VRAM. This is because each parameter would also require storing information
    like gradients, training optimizer states, activations, and temporary memory space,
    consuming an additional 24 bytes together. This estimates up to 6 times the memory
    requirement compared to just loading the model weights. The same 1B model would
    then require a 24 GB GPU, which the best and most expensive consumer graphics
    cards such as NVIDIA RTX 4090 may still struggle to meet.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个高精度的 32 位浮点参数消耗 4 字节 GPU 内存，一个 1B 参数的模型仅用于推理就需要 4 GB 的内存。如果您计划重新训练或微调相同的模型，您至少需要
    24 GB 的 GPU VRAM。这是因为每个参数还需要存储诸如梯度、训练优化器状态、激活和临时内存空间等信息，总共消耗额外的 24 字节。这估计比仅加载模型权重所需的内存需求高
    6 倍。因此，同样的 1B 模型将需要 24 GB 的 GPU，而即使是最好的、最昂贵的消费级显卡，如 NVIDIA RTX 4090，也可能难以满足这一需求。
- en: 'Instead of using the standard 32-float, you can select any of the following
    formats:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 除了标准的 32 位浮点数外，您可以选择以下任何一种格式：
- en: '*16-bit floating-point (FP16)* cuts memory usage in half without much of a
    hit to model output quality.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*16 位浮点数 (FP16)* 可以将内存使用量减半，而对模型输出质量的影响不大。'
- en: '*8-bit integer (INT8)* offers huge savings in memory but with a significant
    loss in quality.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*8 位整数 (INT8)* 在内存上提供巨大的节省，但质量损失很大。'
- en: '*16-bit brain floating-point (BFLOAT16)* with a similar range to FP32 balances
    the memory and quality trade-off.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*16 位脑浮点数 (BFLOAT16)* 与 FP32 具有相似的范围，平衡了内存和质量之间的权衡。'
- en: '*4-bit integer (INT4)* provides a balance between memory efficiency and computational
    precision, making it suitable for low-power devices.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*4 位整数 (INT4)* 在内存效率和计算精度之间提供平衡，使其适合低功耗设备。'
- en: '*1-bit integer (INT1)* uses the lowest precision data type with maximum model
    size reduction. Research for creating high-quality [1-bit LLMs](https://oreil.ly/QH9nH)
    is currently under way.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*1 位整数 (INT1)* 使用最低精度数据类型，以最大程度地减少模型大小。目前正在进行创建高质量 [1 位 LLMs](https://oreil.ly/QH9nH)
    的研究。'
- en: For comparison, [Table 10-2](#quantization_comparison) shows the reduction in
    model size when you quantize the Llama family models.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，[表 10-2](#quantization_comparison) 展示了量化 Llama 家族模型时模型大小的减少。
- en: Table 10-2\. Impact of quantization on the size of Llama models^([a](ch10.html#id1097))
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-2\. 量化对 Llama 模型大小的影响^([a](ch10.html#id1097))
- en: '| Model | Original | FP16 | 8 Bit | 6 Bit | 4 Bit | 2Bit |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 原始 | FP16 | 8 位 | 6 位 | 4 位 | 2 位 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Llama 2 70B | 140 GB | 128.5 GB | 73.23 GB | 52.70 GB | 36.20 GB | 28.59
    GB |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 70B | 140 GB | 128.5 GB | 73.23 GB | 52.70 GB | 36.20 GB | 28.59
    GB |'
- en: '| Llama 3 8B | 16.07 GB | 14.97 GB | 7.96 GB | 4.34 GB | 4.34 GB | 2.96 GB
    |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Llama 3 8B | 16.07 GB | 14.97 GB | 7.96 GB | 4.34 GB | 4.34 GB | 2.96 GB
    |'
- en: '| ^([a](ch10.html#id1097-marker)) Sources: [Llama.cpp GitHub repository](https://oreil.ly/9iYtL)
    and [Tom Jobbins’s Hugging Face Llama 2 70B model card](https://oreil.ly/BMDtR)
    |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch10.html#id1097-marker)) 来源：[Llama.cpp GitHub 仓库](https://oreil.ly/9iYtL)
    和 [Tom Jobbins 的 Hugging Face Llama 2 70B 模型卡片](https://oreil.ly/BMDtR) |'
- en: Tip
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: In addition to the GPU VRAM needed to fit the model, you will also need an extra
    5 to 8 GB of GPU VRAM for overhead during model loading.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于适应模型的 GPU VRAM 外，您还需要额外的 5 到 8 GB GPU VRAM 来处理模型加载时的开销。
- en: As per the current state of research, maintaining accuracy with integer-only
    INT4 and INT1 data types is a challenge, and the performance improvement with
    INT32 or FP16 is not significant. Therefore, the most popular lower-precision
    data type is INT8 for inference.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 根据当前的研究状况，仅使用整数数据类型 INT4 和 INT1 来保持精度是一个挑战，而使用 INT32 或 FP16 的性能提升并不显著。因此，最流行的低精度数据类型是用于推理的
    INT8。
- en: According to [research](https://oreil.ly/C7Lz3), using integer-only arithmetic
    for inference will be more efficient than floating-point numbers. However, quantizing
    floating numbers to integers can be tricky. For instance, only 256 values can
    be represented in INT8, while float32 can represent a wide range of values.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 根据研究 [https://oreil.ly/C7Lz3](https://oreil.ly/C7Lz3)，使用仅整数算术进行推理将比使用浮点数更高效。然而，将浮点数量化为整数可能很复杂。例如，INT8
    只能表示 256 个值，而 float32 可以表示广泛的值。
- en: Floating-point numbers
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 浮点数
- en: To understand why projecting 32-bit floats to other formats would save so much
    in GPU memory, let’s look at how it breaks down.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解为什么将 32 位浮点数投影到其他格式可以节省如此多的 GPU 内存，让我们看看它是如何分解的。
- en: 'A 32-bit floating-point number consists of the following types of bits:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 32 位浮点数由以下类型的位组成：
- en: '*Sign* bit describing whether a number is positive or negative'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*符号位* 表示一个数是正数还是负数'
- en: '*Exponent* bits controlling the scale of the number'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*指数位* 控制数字的量级'
- en: '*Mantissa* bits holding the actual digits determining its precision (also known
    as *fraction* bits)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*尾数位* 保留实际数字，以确定其精度（也称为 *分数位*）'
- en: You can see a visualization of bits in the aforementioned floating-point numbers
    in [Figure 10-6](#quantization_bits).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [图 10-6](#quantization_bits) 中看到上述浮点数的位可视化。
- en: '![bgai 1006](assets/bgai_1006.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 1006](assets/bgai_1006.png)'
- en: Figure 10-6\. Bits in 32-bit float, 16-bit float, and bfloat16 numbers
  id: totrans-248
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-6\. 32 位浮点数、16 位浮点数和 bfloat16 数字中的位
- en: When you project the FP32 number into other formats, in effect, you’re squeezing
    it into smaller ranges, losing most of its mantissa bits and adjusting its exponent
    bits but without losing much of the precision. You can see such a phenomenon in
    action by referring to [Figure 10-7](#quantization_floating_numbers).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当您将 FP32 数字投影到其他格式时，实际上是在将其挤压到更小的范围内，丢失了大部分尾数位，并调整了指数位，但精度损失不大。您可以通过参考 [图 10-7](#quantization_floating_numbers)
    来看到这种现象的实际操作。
- en: '![bgai 1007](assets/bgai_1007.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 1007](assets/bgai_1007.png)'
- en: Figure 10-7\. Quantization of floating-point numbers to integers
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-7\. 将浮点数量化为整数的量化过程
- en: In fact, [research on the quantization strategies for pretrained LLM models](https://oreil.ly/Swfz7)
    has shown that LLMs with 4-bit quantization can maintain performance similar to
    their nonquantized counterparts. However, while quantization saves memory, it
    can also reduce the inference speed of LLMs.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，[关于预训练 LLM 量化策略的研究](https://oreil.ly/Swfz7) 表明，具有 4 位量化的 LLM 可以保持与未量化版本相似的性能。然而，虽然量化可以节省内存，但它也可能降低
    LLM 的推理速度。
- en: How to quantize pretrained LLMs
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何量化预训练的 LLM
- en: Quantization is the process of compressing large models by weight adjustment.
    One such technique called [*GPTQ*](https://oreil.ly/rHYKZ) can quantize LLMs with
    175 billion parameters in approximately 4 GPU hours, reducing the bit width to
    3 or 4 bits per weight, with a negligible accuracy drop relative to the uncompressed
    model.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是通过权重调整来压缩大型模型的过程。有一种称为 [*GPTQ*](https://oreil.ly/rHYKZ) 的技术可以在大约 4 个 GPU
    小时内量化具有 1750 亿参数的 LLM，将权重位宽降低到每权重 3 或 4 位，与未压缩模型相比，精度下降可以忽略不计。
- en: The Hugging Face `transformers` and `optimum` library authors have collaborated
    closely with the `auto-gptq` library developers to provide a simple API for applying
    GPTQ quantization on open source LLMs. Optimum is a library that provides APIs
    to perform quantization using different tools.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face `transformers` 和 `optimum` 库的作者与 `auto-gptq` 库的开发者紧密合作，为开源 LLM
    提供了一个简单的 API，用于应用 GPTQ 量化。Optimum 是一个库，它提供了使用不同工具进行量化的 API。
- en: With the GPTQ quantization, you can quantize your favorite language model to
    8, 4, 3, or even 2 bits without a big drop in performance, while maintaining faster
    inference speeds that are supported by most GPT hardware. You can follow [Example 10-14](#gptq_quantization)
    to quantize a pretrained model on your own GPU.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 GPTQ 量化，您可以将您喜欢的语言模型量化为 8、4、3 或甚至 2 位，而性能下降不大，同时保持大多数 GPT 硬件支持的更快推理速度。您可以通过
    [示例 10-14](#gptq_quantization) 了解如何在您的 GPU 上量化预训练模型。
- en: 'The dependencies you need to install to run [Example 10-14](#gptq_quantization)
    will include the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 [示例 10-14](#gptq_quantization) 所需安装的依赖项包括以下内容：
- en: '[PRE17]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Example 10-14\. GPTQ model quantization with Hugging Face and AutoGPTQ libraries
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-14\. 使用 Hugging Face 和 AutoGPTQ 库进行 GPTQ 模型量化
- en: '[PRE18]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO10-1)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimizing_ai_services_CO10-1)'
- en: Load the `float16` version of the `facebook/opt-125m` pretrained model prior
    to quantization.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在量化之前，加载 `facebook/opt-125m` 预训练模型的 `float16` 版本。
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO10-2)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_optimizing_ai_services_CO10-2)'
- en: Use the `c4` dataset to calibrate the quantization.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `c4` 数据集校准量化。
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO10-3)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_optimizing_ai_services_CO10-3)'
- en: Quantize only the model’s decoder layer blocks.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 仅量化模型的解码器层块。
- en: '[![4](assets/4.png)](#co_optimizing_ai_services_CO10-4)'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_optimizing_ai_services_CO10-4)'
- en: Use model sequence length of `2048` to process the dataset.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `2048` 的模型序列长度来处理数据集。
- en: Tip
  id: totrans-269
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: For reference, a 175B model will require 4 GPU hours on NVIDIA A100 to quantize.
    However, it’s worth searching the Hugging Face model repository for prequantized
    models, as you might find that someone has already done the work.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，一个 175B 的模型在 NVIDIA A100 上进行量化需要 4 个 GPU 小时。然而，值得在 Hugging Face 模型存储库中搜索预量化模型，因为你可能会发现有人已经完成了这项工作。
- en: Now that you understand performance optimization techniques, let’s explore how
    to enhance the quality of your GenAI services using methods like structured outputs.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了性能优化技术，让我们探索如何使用结构化输出等方法来提高你的 GenAI 服务的质量。
- en: Structured Outputs
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化输出
- en: Foundational models such as LLMs may be used as a component of a data pipeline
    or connected to downstream applications. For instance, you can use these models
    to extract and parse information from documents or to generate code that can be
    executed on other systems.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型，如 LLM，可以用作数据管道的一部分或连接到下游应用程序。例如，你可以使用这些模型从文档中提取和解析信息，或者生成可以在其他系统上执行的其他代码。
- en: You can ask the LLM to provide a textual response containing JSON information.
    You will then have to extract and parse this JSON string using tools like regex
    and Pydantic. However, there is no guarantee that the model will always adhere
    to your instructions. Since your downstream systems may rely on JSON outputs,
    they may throw exceptions and incorrectly handle invalid inputs.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以要求 LLM 提供包含 JSON 信息的文本响应。然后你必须使用正则表达式和 Pydantic 等工具提取和解析这个 JSON 字符串。然而，无法保证模型总是会遵循你的指示。由于你的下游系统可能依赖于
    JSON 输出，它们可能会抛出异常并错误地处理无效输入。
- en: Several utility packages like Instructor have been released to improve the robustness
    of LLM responses by taking a schema and making several API calls under the hood
    with various prompt templates to reach a desired output. While these solutions
    improve robustness, they also add significant costs to your solution due to subsequent
    API calls to the model providers.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 已经发布了几个实用程序包，如 Instructor，通过使用模式和执行多个 API 调用来提高 LLM 响应的鲁棒性，从而在幕后使用各种提示模板来达到期望的输出。虽然这些解决方案提高了鲁棒性，但它们也由于后续对模型提供商的
    API 调用而给你的解决方案增加了显著的成本。
- en: Most recently, model providers have added a feature for requesting structured
    outputs by supplying schemas when making API calls to the model, as you can see
    in [Example 10-15](#structured_outputs). This helps to reduce the prompting templating
    work you have to do yourself and aims to improve the model’s *alignment* to your
    intent when returning a response.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，模型提供商在向模型进行 API 调用时，通过提供模式来添加了一个请求结构化输出的功能，正如你在[示例 10-15](#structured_outputs)中看到的那样。这有助于减少你必须自己做的提示模板工作，并旨在提高模型在返回响应时与你的意图的*一致性*。
- en: Warning
  id: totrans-277
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: At the time of writing, only the most recent OpenAI SDK supports Pydantic models
    for enabling structured outputs.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，只有最新的 OpenAI SDK 支持使用 Pydantic 模型来启用结构化输出。
- en: Example 10-15\. Structured outputs
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-15\. 结构化输出
- en: '[PRE19]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO11-1)'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_optimizing_ai_services_CO11-1)'
- en: Specify a Pydantic model for structured outputs.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 为结构化输出指定一个 Pydantic 模型。
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO11-2)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)(#co_optimizing_ai_services_CO11-2)'
- en: Provide the defined schema to the model client when making the API call.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行 API 调用时，将定义的模式提供给模型客户端。
- en: If your model provider doesn’t support structured outputs natively, you can
    still leverage the model’s chat completion capabilities to increase robustness
    of structured outputs, as shown in [Example 10-16](#structured_outputs_completions).
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型提供商不支持原生结构化输出，你仍然可以利用模型的聊天完成功能来提高结构化输出的鲁棒性，如[示例 10-16](#structured_outputs_completions)所示。
- en: Example 10-16\. Structured outputs based on chat completions prefill
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-16\. 基于聊天完成的预填充结构化输出
- en: '[PRE20]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO12-1)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_optimizing_ai_services_CO12-1)'
- en: Limit the output tokens to improve robustness and speed of the structured responses
    and to reduce costs.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 限制输出令牌以提高结构化响应的鲁棒性和速度，并降低成本。
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO12-2)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)(#co_optimizing_ai_services_CO12-2)'
- en: Skip the preamble and directly return a JSON by prefilling the assistant response
    and including a `{` character.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 跳过序言部分，直接通过预填充助手响应并包含一个 `{` 字符来返回 JSON。
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO12-3)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)(#co_optimizing_ai_services_CO12-3)'
- en: Add back the prefilled `{` and then find the closing `}` and extract the JSON
    substring.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复预填充的 `{` 符号，然后找到闭合的 `}` 符号并提取 JSON 子串。
- en: '[![4](assets/4.png)](#co_optimizing_ai_services_CO12-4)'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_optimizing_ai_services_CO12-4)'
- en: Handle cases where there is no JSON in the response—e.g., if there is a refusal.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 处理响应中不存在 JSON 的情况——例如，如果出现拒绝。
- en: Following the aforementioned techniques should help you improve the robustness
    of your data pipelines if they leverage LLMs as a component.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循上述技术应该有助于提高你利用 LLM 作为组件的数据管道的鲁棒性。
- en: Prompt Engineering
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示工程
- en: Prompt engineering is the practice of crafting and refining queries to generative
    models to produce the most useful and optimized outputs. Without refining prompts,
    you’d either have to fine-tune models or train a model from scratch to optimize
    the output quality.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是构建和优化查询以生成模型以产生最有用和最优化输出的实践。如果没有优化提示，你可能不得不微调模型或从头开始训练一个模型以优化输出质量。
- en: Many argue that the field lacks the scientific rigor to consider it an engineering
    discipline. However, you can approach the problem from an engineering perspective
    when refining prompts to get the best quality outputs from your models.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人认为该领域缺乏科学严谨性，不能将其视为一个工程学科。然而，当优化提示以获得最佳质量输出时，你可以从工程角度来处理这个问题。
- en: Similar to how you communicate with others to get things done, with the optimized
    prompts, you can most effectively communicate your intent with the model to improve
    the chances of getting the responses you want. Therefore, prompting becomes not
    just an engineering problem but a communication one as well. A model can be compared
    to a knowledgeable colleague with lots of experience but limited domain knowledge,
    ready to help you but needs you to provide well-documented instructions, possibly
    with a few examples to follow and pattern match.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 与你与他人沟通以完成任务的方式类似，通过优化的提示，你可以最有效地与模型沟通你的意图，以提高获得你想要的响应的机会。因此，提示不仅是一个工程问题，也是一个沟通问题。一个模型可以比作一个经验丰富的知识渊博的同事，但领域知识有限，愿意帮助你，但需要你提供详细的指令，可能还需要一些示例来遵循和模式匹配。
- en: If your prompts are vague and generic, you’ll also get an average response.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的提示模糊且通用，你也会得到一个平均的响应。
- en: Another way of thinking about this optimization problem is to compare the task
    of model prompting to programming. Instead of writing the code yourself, you’re
    effectively “coding” a model to be a well-integrated component of a larger application
    or data pipeline. You can adopt test-driven development (TDD) approaches and refine
    your prompts until your tests pass. Or, experiment with different models to see
    which one *aligns* its outputs to your intent the best.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考这个优化问题的方式是将模型提示的任务与编程进行比较。你不必亲自编写代码，而是有效地“编码”一个模型，使其成为更大应用程序或数据管道中一个良好集成的组件。你可以采用测试驱动开发（TDD）方法，并完善你的提示，直到测试通过。或者，尝试不同的模型，看看哪一个*对齐*其输出与你的意图最佳。
- en: Note
  id: totrans-303
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Maximizing model *alignment* remains a high-priority objective of many model
    providers so that their model outputs best satisfy the user’s intent.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 最大化模型*对齐*仍然是许多模型提供商的一个高优先级目标，以便他们的模型输出最好地满足用户的意图。
- en: Prompt templates
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示模板
- en: If your system instructions aren’t methodical, clear, and don’t follow best
    prompting practices, you may be leaving potential quality and performance optimizations
    on the table.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的系统指令不系统、不清晰，并且不遵循最佳提示实践，你可能会错失潜在的质量和性能优化。
- en: 'As a minimum, you should have clear system prompts that provide specific tasks
    to the model. Best practice is to follow a systematic template. For instance,
    draft the model instructions following the *role, context, and task* (RCT) template:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，你应该有明确的系统提示，为模型提供具体的任务。最佳实践是遵循一个系统化的模板。例如，根据*角色、上下文和任务*（RCT）模板起草模型说明：
- en: Role
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 角色
- en: Describes how the model should behave given a scenario and a task. Research
    has shown that specifying roles for LLMs tends to significantly affect their outputs.
    As an example, a model may be more forgiving in grading an essay if you give it
    the role of a primary school teacher. Without a specific role, the model may assume
    you want the grading to follow university-level academic standards.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 描述了在给定场景和任务的情况下模型应该如何表现。研究表明，为 LLM 指定角色往往会显著影响其输出。例如，如果你让它扮演小学教师的角色，模型在评分作文时可能会更加宽容。没有特定的角色，模型可能会假设你希望评分遵循大学水平的学术标准。
- en: Note
  id: totrans-310
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can expand on the model’s role even further and describe a *persona* in
    detail for the model to adopt. Using a persona, the model will exactly know how
    to behave and make predictions as it has more context on what the role should
    entail.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以进一步扩展模型的角色，并为模型详细描述一个 *角色扮演*，以便模型采用。使用角色扮演，模型将确切知道如何表现和做出预测，因为它对角色应该包含的内容有更多的了解。
- en: Context
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文
- en: Sets the scenario, paints the picture, and provides any relevant and useful
    information that the model can use as a reference to making predictions. Without
    an explicit context, the model can only use an implicit context that’ll contain
    average information of its training data. In a RAG application, the context could
    be the concatenation of system prompt with the retrieved document chunks from
    a knowledge store.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 设置场景，描绘画面，并提供模型可以用来作为预测参考的相关和有用信息。如果没有明确的上下文，模型只能使用隐含的上下文，其中包含其训练数据的平均信息。在RAG应用中，上下文可以是系统提示与从知识库检索到的文档片段的串联。
- en: Task
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 任务
- en: Provides clear instructions on what you want the model to perform given a context
    and a role. When describing the task, make sure you think of the model as a bright
    and knowledgeable apprentice, ready to jump into action but needs highly clear
    and unambiguous instructions to follow, potentially with a handful of examples.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了在给定上下文和角色的情况下，你希望模型执行的操作的明确说明。在描述任务时，确保你将模型视为一个聪明且知识渊博的学徒，随时准备采取行动，但需要高度清晰且明确的指示来遵循，可能还需要一些示例。
- en: Following the aforementioned system template, you should enhance the quality
    of your model outputs with minimal effort.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 按照上述系统模板，你应该以最小的努力提高模型输出的质量。
- en: Advanced prompting techniques
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级提示技术
- en: 'Beyond the prompting fundamentals, you can use more advanced techniques that
    may better fit your use case. Based on a [recent systematic survey of prompting
    techniques](https://oreil.ly/xynPC), you can group LLM prompts into the following:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提示基础之外，你还可以使用更高级的技术，这些技术可能更适合你的用例。根据对提示技术的 [最近系统调查](https://oreil.ly/xynPC)，你可以将LLM提示分为以下几类：
- en: In-context learning
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文学习
- en: Thought generation
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思维生成
- en: Decomposition
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分解
- en: Ensembling
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成
- en: Self-criticism
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自我批评
- en: Agentic
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理
- en: Let’s review each in more detail.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地回顾一下。
- en: In-context learning
  id: totrans-326
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 上下文学习
- en: What sets foundational models such as LLMs apart from traditional machine learning
    models is their ability to respond to dynamic inputs without the constant need
    for fine-tuning or retraining.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 基础模型（如LLM）与传统机器学习模型区别开来的特点是，它们能够对动态输入做出响应，而无需不断进行微调或重新训练。
- en: When you give system instructions to an LLM, you can additionally supply several
    examples, (i.e., shots) to guide the output generation.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 当你向LLM提供系统指令时，你可以额外提供几个示例（即射击），以指导输出生成。
- en: '[*Zero-shot prompting*](https://oreil.ly/3F4wb) refers to a prompting approach
    that doesn’t specify reference examples, yet the model can still successfully
    complete the given task. If the model struggles without reference examples, you
    may have to use [*few-shot prompting*](https://oreil.ly/pOSj8) where you provide
    a handful of examples. There are also use cases where you want to use *dynamic
    few-shot* prompting where you dynamically insert examples from data fetched from
    a database or vector store.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[*零样本提示*](https://oreil.ly/3F4wb) 指的是一种提示方法，它不指定参考示例，但模型仍然可以成功完成给定的任务。如果模型在没有参考示例的情况下遇到困难，你可能需要使用
    [*少量样本提示*](https://oreil.ly/pOSj8)，其中你提供少量示例。还有一些用例，你希望使用 *动态少量样本* 提示，其中你动态地从数据库或向量存储中获取示例。'
- en: Prompting approaches where you specify examples are also termed *in-context
    learning*. You’re effectively fine-tuning the model’s outputs to your examples
    and the given task without actually modifying its model weights/parameters, whereas
    other ML models would require adjustments to their weights.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 指定示例的提示方法也被称为 *上下文学习*。你实际上是在调整模型的输出以适应你的示例和给定的任务，而不需要修改其模型权重/参数，而其他ML模型则需要调整其权重。
- en: This is what makes LLMs and foundational models so powerful, since they don’t
    always require weight adjustment to fit your data and tasks you give them. You
    can learn about several in-context learning techniques by referring to [Table 10-3](#prompting_techniques_incontext_learning).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是LLM和基础模型之所以如此强大的原因，因为它们并不总是需要调整权重来适应你提供的数据和任务。你可以通过参考 [表10-3](#prompting_techniques_incontext_learning)
    来了解几种上下文学习技术。
- en: Table 10-3\. In-context learning prompting techniques
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 表10-3\. 上下文学习提示技术
- en: '| Prompting technique | Examples | Use cases |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 提示技术 | 示例 | 应用场景 |'
- en: '| --- | --- | --- |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Zero-shot | Summarize the following…​ | Summarization, Q&A without specific
    training examples |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 零样本 | 概括以下内容… | 摘要、无特定训练示例的问答 |'
- en: '| Few-shot | Classify documents based on examples below:[Examples] | Text classification,
    sentiment analysis, data extraction with few examples |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 少样本 | 根据以下示例对文档进行分类：[示例] | 文本分类、情感分析、少量示例的数据提取 |'
- en: '| Dynamic few-shot | Classify the following documents based on examples below:<Inject
    examples from a vector store based on a query> | Personalized responses, complex
    problem-solving |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 动态少样本 | 根据以下示例对以下文档进行分类：<基于查询从向量存储中注入示例> | 个性化响应、复杂问题解决 |'
- en: In-context learning prompts are straightforward, effective, and a great starting
    point for completing a variety of tasks. For more complex tasks, you can use more
    advanced prompting approaches like thought generation, decomposition, ensembling,
    self-criticism, or agentic approaches.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文中学习的提示简单、有效，是完成各种任务的绝佳起点。对于更复杂的任务，您可以使用更高级的提示方法，如思维生成、分解、集成、自我批评或代理方法。
- en: Thought generation
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 思维生成
- en: Thought generation techniques like [chain of thought (CoT)](https://oreil.ly/BWUYQ)
    have shown to significantly improve the ability of LLMs to perform complex reasoning.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 思维生成技术，如[思维链（CoT）](https://oreil.ly/BWUYQ)，已被证明可以显著提高大型语言模型进行复杂推理的能力。
- en: In COT prompting you ask the model to explain its thought process and reasoning
    as it provides a response. Variants of CoT include zero-shot or [few-shot CoT](https://oreil.ly/1gjSH)
    depending on whether you supply examples. A more advanced thought generation technique
    is [thread of thought (ThoT)](https://oreil.ly/1KyO4) that systematically segments
    and analyzes chaotic and very complex information or tasks.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在COT提示中，您要求模型在提供响应时解释其思维过程和推理。CoT的变体包括零样本或[少样本CoT](https://oreil.ly/1gjSH)，具体取决于您是否提供示例。一种更高级的思维生成技术是[思维线索（ThoT）](https://oreil.ly/1KyO4)，它可以系统地分割和分析混乱且非常复杂的信息或任务。
- en: '[Table 10-4](#prompting_techniques_thought_generation) lists thought generation
    techniques.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '[表10-4](#prompting_techniques_thought_generation)列出了思维生成提示技术。'
- en: Table 10-4\. Thought generation prompting techniques
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 表10-4. 思维生成提示技术
- en: '| Prompting technique | Examples | Use cases |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 提示技术 | 示例 | 应用场景 |'
- en: '| --- | --- | --- |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Zero-shot chain of thought (CoT) | Let’s think step by step…​ | Mathematical
    problem-solving, logical reasoning, and multi-step decision-making. |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 零样本思维链（CoT） | 逐步思考… | 数学问题解决、逻辑推理和多步骤决策。|'
- en: '| Few-shot CoT | Let’s think step by step…​ Here are a few examples:[EXAMPLES]
    | Scenarios where a few examples can guide the model to perform better, such as
    nuanced text classification, complex question answering, and creative writing
    prompts. |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 少样本CoT | 逐步思考…以下是一些示例：[示例] | 可以通过少量示例引导模型表现更好的场景，例如细微文本分类、复杂问答和创意写作提示。|'
- en: '| Thread of thought (ThoT) | Walk me through the problem in manageable parts
    step by step, summarizing and analyzing as you go…​ | Maintaining context over
    multiple interactions, such as dialogue systems, interactive storytelling, and
    long-form content generation. |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 思维线索（ThoT） | 逐步引导我理解问题，在过程中总结和分析… | 在多个交互中保持上下文，例如对话系统、互动故事讲述和长篇内容生成。|'
- en: Decomposition
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分解
- en: Decomposition prompting techniques focus on breaking down complex tasks into
    smaller subtasks so that the model can work through them step-by-step and logically.
    You can experiment with these approaches alongside thought generation to identify
    which ones produce the best results for your use case.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 分解提示技术侧重于将复杂任务分解成更小的子任务，以便模型可以逐步、逻辑地处理它们。您可以在思维生成的同时尝试这些方法，以确定哪些方法对您的应用场景效果最佳。
- en: 'These are the most common decomposition prompting techniques:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是最常见的分解提示技术：
- en: '[Least-to-most](https://oreil.ly/HmsSN)'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '[从少到多](https://oreil.ly/HmsSN)'
- en: Ask the model to break a complex problem into smaller problems via logical reduction
    without solving them. You can then reprompt the model to solve each task one by
    one.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 通过逻辑分解将复杂问题分解成更小的问题，而不需要解决它们。然后您可以重新提示模型逐一解决每个任务。
- en: '[Plan-and-solve](https://oreil.ly/aWTzf)'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '[计划并解决](https://oreil.ly/aWTzf)'
- en: Given a task, ask for a plan to be devised, and then request the model to solve
    it.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个任务，要求制定一个计划，然后请求模型解决它。
- en: '[Tree of thoughts (ToT)](https://oreil.ly/IZdj1)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '[思维树（ToT）](https://oreil.ly/IZdj1)'
- en: Create a tree-search problem where a task is broken into multiple branches of
    steps like a tree. Then, reprompt the model to evaluate and solve each branch
    of steps.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个树搜索问题，其中任务被分解成多个步骤分支，就像树一样。然后，重新提示模型评估和解决每个步骤分支。
- en: '[Table 10-5](#prompting_techniques_decomposition) shows these decomposition
    techniques.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 10-5](#prompting_techniques_decomposition) 展示了这些分解技术。'
- en: Table 10-5\. Decomposition prompting techniques
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-5\. 分解提示技术
- en: '| Prompting technique | Examples | Use cases |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 提示技术 | 例子 | 用例 |'
- en: '| --- | --- | --- |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Least-to-most | Break down the task of…​into smaller tasks. | Complex problem-solving,
    project management, task decomposition |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 从少到多 | 将……的任务分解成更小的任务。 | 复杂问题解决、项目管理、任务分解 |'
- en: '| Plan-and-solve | Devise a plan to…​ | Algorithm development, software design,
    strategic planning |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 计划并解决 | 制定……的计划…… | 算法开发、软件设计、战略规划 |'
- en: '| Tree of thoughts (ToT) | Create a decision tree for choosing a…​ | Decision-making,
    problem-solving with multiple solutions, strategic planning with alternatives
    |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 思维树（ToT） | 为选择……创建决策树…… | 决策、具有多个解决方案的问题解决、具有替代方案的战略规划 |'
- en: Ensembling
  id: totrans-365
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 集成
- en: '*Ensembling* is the process of using multiple prompts to solve the same problem
    and then aggregating the responses into a final output. You can generate these
    responses using the same or different models.'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '*集成* 是使用多个提示来解决相同问题，然后将响应汇总为最终输出的过程。您可以使用相同或不同的模型生成这些响应。'
- en: The main idea behind ensembling is to reduce the variance of LLM outputs by
    improving accuracy in exchange for higher usage costs.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 集成背后的主要思想是通过提高准确性来降低 LLM 输出的方差，以换取更高的使用成本。
- en: 'Well-known ensembling prompting techniques include the following:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知的集成提示技术包括以下内容：
- en: '[Self-consistency](https://oreil.ly/_85WS)'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '[自我一致性](https://oreil.ly/_85WS)'
- en: Generates multiple reasoning paths and selects the most consistent output as
    the final result using a majority vote.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多数投票生成多个推理路径，并选择最一致的输出作为最终结果。
- en: '[Mixture of reasoning experts (MoRE)](https://oreil.ly/xllKs)'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '[混合推理专家（MoRE）](https://oreil.ly/xllKs)'
- en: Combines outputs from multiple LLMs with specialized prompts to improve response
    quality. Each LLM acts as an expert on an area focused on different reasoning
    tasks such as factual reasoning, logical reduction, common-sense checks, etc.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合具有专门提示的多个 LLM 的输出，以提高响应质量。每个 LLM 都作为专注于不同推理任务的领域专家，例如事实推理、逻辑简化、常识检查等。
- en: '[Demonstration ensembling (DENSE)](https://oreil.ly/lPEPz)'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[演示集成（DENSE）](https://oreil.ly/lPEPz)'
- en: Creates multiple few-shot prompts from data, then generates a final output by
    aggregating over the responses.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中创建多个少量样本提示，然后通过汇总响应生成最终输出。
- en: '[Prompt paraphrasing](https://oreil.ly/yP_ka)'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示改写](https://oreil.ly/yP_ka)'
- en: Formulates the original prompt into multiple variants via wording.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 通过措辞将原始提示制定成多个变体。
- en: '[Table 10-6](#prompting_techniques_ensemling) shows examples and use cases
    of these ensembling techniques.'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 10-6](#prompting_techniques_ensemling) 展示了这些集成技术的示例和用例。'
- en: Table 10-6\. Ensembling prompting techniques
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-6\. 集成提示技术
- en: '| Prompting technique | Examples | Use cases |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 提示技术 | 例子 | 用例 |'
- en: '| --- | --- | --- |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Self-consistency | Prompt #1 (run multiple times): Let’s think step by step
    and complete the following task…​Prompt #2: From the following responses, choose
    the best/common one by scoring them using…​ | Reducing errors or bias in arithmetic,
    common-sense tasks, and symbolic reasoning tasks |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 自我一致性 | 提示 #1（多次运行）：让我们一步一步思考并完成以下任务……提示 #2：从以下响应中选择最佳/最常见的一个，通过使用……评分它们……
    | 减少算术、常识任务和符号推理任务中的错误或偏差 |'
- en: '| Mixture of reasoning experts (MoRE) | Prompt #1 (run for each expert): You
    are a reviewer for …​, score the following based on…​Prompt #2: Choose the best
    expert answer based on an agreement score…​ | Accounting for specialized knowledge
    areas or domains |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 混合推理专家（MoRE） | 提示 #1（为每位专家运行）：您是……的审稿人，根据……评分以下内容……提示 #2：根据一致性评分选择最佳专家答案……
    | 考虑专业知识领域或领域 |'
- en: '| Demonstration ensembling (DENSE) | Create multiple few-shot examples for
    translating this text and aggregate the best responses.Generate several few-shot
    prompts for summarizing this article and combine the outputs. |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 演示集成（DENSE） | 为翻译此文本创建多个少量样本示例，并汇总最佳响应。为总结这篇文章生成几个少量样本提示，并组合输出。 |'
- en: Improving output reliability
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高输出可靠性
- en: Aggregating diverse perspectives
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合不同观点
- en: '|'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Prompt paraphrasing | Prompt #1a: Reword this proposal…​Prompt #1b: Clarify
    this proposal…​Prompt #1c: Make adjustment to this proposal…​Prompt #2: Choose
    the best proposal from the following responses based on…​ |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| 提示改写 | 提示 #1a：重述此提案…提示 #1b：澄清此提案…提示 #1c：对此提案进行调整…提示 #2：根据以下标准从以下响应中选择最佳提案…
    |'
- en: Exploring different interpretations
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索不同解释
- en: Data augmentation for ensembling
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为集成进行数据增强
- en: '|'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Self-criticism
  id: totrans-391
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自我批评
- en: '*Self-criticism* prompting techniques focus on using models as AI judges, assessors,
    or reviewers, either to perform self-checks or to assess the outputs of other
    models. The criticism or feedback from the first prompt can then be used to improve
    the response quality in follow-on prompts.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '*自我批评*提示技术侧重于使用模型作为AI法官、评估者或审稿人，无论是进行自我检查还是评估其他模型的输出。第一个提示的批评或反馈然后可以用于改进后续提示中的响应质量。'
- en: 'These are several self-criticism prompting strategies:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是几种自我批评提示策略：
- en: '[Self-calibration](https://oreil.ly/_4YEr)'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '[自我校准](https://oreil.ly/_4YEr)'
- en: Ask the LLM to assess the correctness of a response/answer against a question/answer.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 要求LLM评估一个响应/答案相对于问题/答案的正确性。
- en: '[Self-refine](https://oreil.ly/bTQJI)'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '[自我优化](https://oreil.ly/bTQJI)'
- en: Refine responses iteratively through self-checks and providing feedback.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自我检查和提供反馈迭代地细化响应。
- en: '[Reversing chain of thought (RCoT)](https://oreil.ly/6ojtr)'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '[反思思维链（RCoT）](https://oreil.ly/6ojtr)'
- en: Reconstruct the problem from a generated answer, and then generate fine-grained
    comparisons between the original problem and the reconstructed one to identify
    inconsistencies.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 从生成的答案中重建问题，然后生成原始问题和重建问题之间的细粒度比较，以识别不一致。
- en: '[Self-verification](https://oreil.ly/Fz3JH)'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '[自我验证](https://oreil.ly/Fz3JH)'
- en: Generate potential solutions with the CoT technique, and then score each by
    masking parts of the question and supplying each answer.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CoT技术生成潜在解决方案，然后通过遮蔽问题的一部分并供应每个答案来对每个答案进行评分。
- en: '[Chain of verification (COVE)](https://oreil.ly/WrrLP)'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '[验证链（COVE）](https://oreil.ly/WrrLP)'
- en: Create a list of related queries/questions to help verify the correctness of
    an answer/response.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 创建相关查询/问题列表以帮助验证答案/响应的正确性。
- en: '[Cumulative reasoning](https://oreil.ly/3Hb-6)'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '[累积推理](https://oreil.ly/3Hb-6)'
- en: Generate potential steps in responding to a query, and then ask the model to
    accept/reject each step. Finally, check whether it has arrived at the final answer
    to terminate the process; otherwise, repeat the process.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 生成响应查询的潜在步骤，然后要求模型接受/拒绝每个步骤。最后，检查它是否到达了最终答案以终止过程；否则，重复此过程。
- en: You can see examples of each self-criticism prompting technique in [Table 10-7](#prompting_techniques_self_criticism).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[表 10-7](#prompting_techniques_self_criticism)中看到每种自我批评提示技术的示例。
- en: Table 10-7\. Self-criticism prompting techniques
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-7\. 自我批评提示技术
- en: '| Prompting technique | Examples | Use cases |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 提示技术 | 示例 | 用例 |'
- en: '| --- | --- | --- |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Self-calibration | Assess the correctness of the following response: [response]
    for the following question: [question] | Gauge confidence of the answers to accept
    or revise the original answer. |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 自我校准 | 评估以下响应的正确性：[response] 对于以下问题：[question] | 测量答案的信心以接受或修改原始答案。 |'
- en: '| Self-refine | Prompt #1: What is your feedback on the response…​Prompt #2:
    Using the feedback [Feedback], refine your response on…​ | Reasoning, coding,
    and generation tasks. |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 自我优化 | 提示 #1：你对响应的反馈是什么…提示 #2：使用反馈[Feedback]，在以下方面优化你的响应… | 推理、编码和生成任务。 |'
- en: '| Reversing chain-of-thought (RCoT) | Prompt #1: Reconstruct the problem from
    this answer…​Prompt #2: Generate fine-grained comparison between these queries…​
    | Identifying inconsistencies and revising answers. |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 反思思维链（RCoT） | 提示 #1：从以下答案中重建问题…提示 #2：生成这些查询之间的细粒度比较… | 识别不一致并修改答案。 |'
- en: '| Self-verification | Prompt #1 (run multiple times): Let’s think step by step
    - generate solution for the following problem…​Prompt #2: Score each solution
    based on the [masked problem]…​ | Improve on reasoning tasks. |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 自我验证 | 提示 #1（运行多次）：让我们一步步思考 - 生成以下问题的解决方案…提示 #2：根据[遮蔽问题]对每个解决方案进行评分… | 改善推理任务。
    |'
- en: '| Chain of verification (COVE) | Prompt #1: Answer the following question…​Prompt
    #2: Formulate related questions to check this response: …​Prompt #3 (run for each
    new related question): Answer the following question: …​Prompt #4: Based on the
    following information, pick the best answer…​ | Question answering and text-generation
    tasks. |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 验证链（COVE） | 提示 #1：回答以下问题…提示 #2：提出相关问题以检查此响应：…提示 #3（针对每个新的相关问题运行）：回答以下问题：…提示
    #4：根据以下信息，选择最佳答案… | 问题回答和文本生成任务。|'
- en: '| Cumulative reasoning | Prompt #1: Outline steps to respond to the query:
    …​Prompt #2: Check the following plan and accept/reject steps relevant in responding
    to the query: …​Prompt #3: Check you’ve arrived at the final answer given the
    following information…​ | Step-by-step validation of complex queries, logical
    inference, and mathematical problems. |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 累积推理 | 提示 #1：概述响应查询的步骤：…提示 #2：检查以下计划并接受/拒绝与响应查询相关的步骤：…提示 #3：检查你已根据以下信息得出最终答案…
    | 复杂查询、逻辑推理和数学问题的逐步验证。|'
- en: Agentic
  id: totrans-416
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代理性
- en: You can take the prompting techniques discussed so far one step further and
    add access to external tools with complex evaluation algorithms. This process
    specializes LLMs as *agents*, allowing them to make plans, take actions, and use
    external systems.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将到目前为止讨论的提示技术进一步发展，并添加对具有复杂评估算法的外部工具的访问。这个过程将 LLMs 特化为*代理*，允许它们制定计划、采取行动，并使用外部系统。
- en: Prompts or *prompt sequences (chains)* drive agentic systems with an engineering
    focus on creating agent-like behavior from LLMs. These agentic workflows serve
    users by performing actions on systems that interface with the GenAI models, which
    are mostly LLMs. Tools, whether *symbolic* like a calculator, or *neural* such
    as another AI model, form a core component of agentic systems.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 提示或*提示序列（链）*驱动具有工程重点的代理系统，从 LLMs 中创建类似代理的行为。这些代理工作流程通过在连接到 GenAI 模型的系统上执行操作来为用户提供服务，这些模型主要是
    LLMs。工具，无论是*符号性*的，如计算器，还是*神经性*的，如另一个 AI 模型，都是代理系统的核心组件。
- en: Tip
  id: totrans-419
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you create a pipeline of multiple model calls with one output forwarded to
    the same or different model as input, you’ve constructed a *prompt chain*. In
    principle, you’re using the CoT prompting technique when you leverage prompt chains.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你创建了一个由多个模型调用组成的管道，其中一个输出被转发到同一模型或不同模型作为输入，你就构建了一个*提示链*。原则上，当你利用提示链时，你正在使用
    CoT 提示技术。
- en: 'A few agentic prompting techniques include:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 一些代理提示技术包括：
- en: '[Modular reasoning, knowledge, and language (MRKL)](https://oreil.ly/aWeQu)'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '[模块化推理、知识和语言（MRKL）](https://oreil.ly/aWeQu)'
- en: Simplest agentic system consisting of an LLM using multiple tools to get and
    combine information for generating an answer.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 由一个使用多个工具获取和组合信息以生成答案的 LLM 组成的最简单的代理系统。
- en: '[Self-correcting with tool-interactive critiquing (CRITIC)](https://oreil.ly/M-9YL)'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用工具交互式批评（CRITIC）进行自我纠正](https://oreil.ly/M-9YL)'
- en: Responds to queries, and then self-checks its answer without using external
    tools. Finally, uses tools to verify or amend responses.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 响应查询，然后在不使用外部工具的情况下自我检查其答案。最后，使用工具验证或修改响应。
- en: '[Program-aided language model (PAL)](https://oreil.ly/0WtKv)'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '[程序辅助语言模型（PAL）](https://oreil.ly/0WtKv)'
- en: Generates code from queries and sends directly to code interpreters such as
    Python to generate an answer.^([4](ch10.html#id1128))
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 从查询生成代码并将其直接发送到代码解释器，如 Python，以生成答案.^([4](ch10.html#id1128))
- en: '[Tool-integrated reasoning agent (ToRA)](https://oreil.ly/pbfv_)'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '[工具集成推理代理（ToRA）](https://oreil.ly/pbfv_)'
- en: Takes PAL a few steps further by interleaving code generation and reasoning
    steps as long as needed to provide a satisfactory response.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 将 PAL 进一步发展，通过交织代码生成和推理步骤，直到提供满意的响应。
- en: '[Reasoning and acting (ReAct)](https://oreil.ly/aDubr)'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '[推理和行动（ReAct）](https://oreil.ly/aDubr)'
- en: Given a problem, generates thoughts, takes actions, receives observations, and
    repeats the loop with previous information, (i.e., memory) until the problem is
    solved.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个问题，生成想法，采取行动，接收观察结果，并使用先前信息（即记忆）重复循环，直到问题得到解决。
- en: If you want to enable your LLMs to use tools, you can take advantage of *function
    calling* features from model providers, as in [Example 10-17](#function_calling).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想让你的 LLMs 使用工具，你可以利用模型提供商提供的*函数调用*功能，如[示例 10-17](#function_calling)所示。
- en: Example 10-17\. Function calling for fetching
  id: totrans-433
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-17\. 获取函数调用
- en: '[PRE21]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As you saw in [Example 10-17](#function_calling), you can create agentic systems
    by configuring specialized LLMs that have access to custom tools and functions.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[示例 10-17](#function_calling)中看到的，您可以通过配置具有访问自定义工具和函数的专用 LLM 来创建代理系统。
- en: Fine-Tuning
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调
- en: There are cases where prompt engineering alone won’t give the response quality
    you’re looking for. Fine-tuning is an optimization technique that requires you
    to adjust the parameters of your GenAI model to better fit your data. For instance,
    you may fine-tune a language model to learn content of private knowledge bases
    or to always respond with a certain tone following your brand guidelines.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些情况，仅通过提示工程无法达到您期望的响应质量。微调是一种优化技术，需要您调整 GenAI 模型的参数以更好地适应您的数据。例如，您可能需要微调语言模型以学习私有知识库的内容或始终按照品牌指南以某种语气进行响应。
- en: It’s often not the first technique you should try since it requires effort to
    collect and prepare data, in addition to training and evaluating models.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它需要收集和准备数据，以及训练和评估模型，因此这通常不是您应该尝试的第一个技术。
- en: When should you consider fine-tuning?
  id: totrans-439
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 何时应考虑微调？
- en: 'You may want to consider fine-tuning pretrained GenAI models if one of the
    following scenarios is true:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以下情况之一成立，您可能需要考虑微调预训练的 GenAI 模型：
- en: You have significant token usage costs—for instance, due to requiring extensive
    system instructions or providing lots of examples in every prompt.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您有显著的令牌使用成本——例如，由于需要大量系统指令或在每个提示中提供大量示例。
- en: Your use case relies on specialized domain expertise that the model needs to
    learn.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的使用案例依赖于模型需要学习的特定领域专业知识。
- en: You need to reduce the number of hallucinations in responses with a more fine-tuned
    conservative model.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要使用更精细调整的保守模型来减少响应中的幻觉数量。
- en: You require higher-quality responses and have sufficient data for fine-tuning.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要更高质量的响应，并且有足够的数据进行微调。
- en: You require lower latency in responses.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要降低响应的延迟。
- en: Once a model has been fine-tuned, you won’t need to provide as many examples
    in the prompt. This saves costs and enables lower-latency requests.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型经过微调，您就不需要提供那么多示例在提示中。这节省了成本并允许低延迟请求。
- en: Warning
  id: totrans-447
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Avoid fine-tuning as much as you can.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能避免微调。
- en: There are many tasks where prompt engineering alone can help you optimize the
    quality of your outputs. Iterating over prompts has a much faster feedback loop
    than iterating over fine-tuning, which relies on creating datasets and running
    training jobs.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多任务，仅通过提示工程就可以帮助您优化输出质量。与微调相比，提示工程的迭代具有更快的反馈循环，因为微调依赖于创建数据集和运行训练作业。
- en: However, if you do end up needing to fine-tune, you’ll notice that the initial
    prompt engineering efforts would contribute to producing higher-quality training
    data.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您最终确实需要微调，您会注意到最初的提示工程努力将有助于产生高质量的训练数据。
- en: 'Here are a few cases where fine-tuning can be useful:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些情况，微调可能很有用：
- en: Teaching a model to respond in a brand style, tone, format, or some other qualitative
    metric—for instance, to produce standardized reports that comply with regulatory
    requirements and internal protocols
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教模型以品牌风格、语气、格式或其他定性指标进行响应——例如，生成符合监管要求和企业内部协议的标准化报告
- en: Improving reliability of producing desired outputs such as always having responses
    conform to a given structured output
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高产生所需输出的可靠性，例如始终确保响应符合给定的结构化输出
- en: Achieving correct results to complex queries such as document classification
    and tagging from hundreds of classes
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现对复杂查询的正确结果，例如从数百个类别中进行文档分类和标记
- en: Performing domain-specific specialized tasks such as item classification or
    industry-specific data interpretation and aggregation
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行特定领域的专业任务，例如项目分类或行业特定数据解释和汇总
- en: Nuanced handling of edge cases
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 细腻处理边缘情况
- en: Performing skills or tasks that are hard to articulate in prompts such as datetime
    extraction from unstructured texts
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行难以在提示中表述的技能或任务，例如从非结构化文本中提取日期时间
- en: Reducing costs by using `gpt-40-mini` or even `gpt-3.5-turbo` instead of `gpt-4o`
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用 `gpt-40-mini` 或甚至 `gpt-3.5-turbo` 而不是 `gpt-4o` 来降低成本
- en: Teaching a model to use complex tools and APIs when using function calling
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教模型在函数调用时使用复杂工具和 API
- en: How to fine-tune a pretrained model
  id: totrans-460
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何微调预训练模型
- en: 'For any fine-tuning job, you will need to follow these steps:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何微调工作，您需要遵循以下步骤：
- en: Prepare and upload training data.
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备和上传训练数据。
- en: Submit a fine-tuning training job.
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提交微调训练作业。
- en: Evaluate and use fine-tuned model.
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估和使用微调模型。
- en: Depending on the model you’re using, the data must be prepared based on the
    model provider’s instruction.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您使用的模型，您必须根据模型提供商的说明准备数据。
- en: For instance, to fine-tune a typical chat model like `gpt-4o-2024-08-06`, you
    need to prepare your data as a message format, as shown in [Example 10-18](#fine_tune_prepare).
    At the time of writing, [OpenAI API pricing](https://oreil.ly/MmCNq) for fine-tuning
    this model is $25/1M training tokens.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要微调一个典型的聊天模型如`gpt-4o-2024-08-06`，您需要将您的数据准备为消息格式，如[示例10-18](#fine_tune_prepare)所示。在撰写本文时，[OpenAI
    API定价](https://oreil.ly/MmCNq)为微调此模型是$25/1M训练令牌。
- en: Example 10-18\. Example training data for a fine-tuning job
  id: totrans-467
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例10-18。微调作业的示例训练数据
- en: '[PRE22]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Once your data is prepared, you need to upload the `jsonl` file, get a file
    ID, and supply that when submitting a fine-tuning job, as you can see in [Example 10-19](#fine_tuning_training).
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的数据准备就绪，您需要上传`jsonl`文件，获取文件ID，并在提交微调作业时提供该ID，正如您在[示例10-19](#fine_tuning_training)中看到的那样。
- en: Example 10-19\. Submitting a fine-tuning training job
  id: totrans-470
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例10-19。提交微调训练作业
- en: '[PRE23]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Model providers that allow you to submit fine-tuning jobs will also provide
    APIs for checking the status of submitted jobs and for getting results.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 允许您提交微调作业的模型提供商也将提供用于检查提交作业状态和获取结果的API。
- en: Once the model is fine-tuned, you can retrieve the fine-tuned model ID and pass
    it to the LLM client, as shown in [Example 10-20](#fine_tuning_usage). Make sure
    to evaluate the model first before using it in production.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型经过微调，您可以检索微调模型ID并将其传递给LLM客户端，如[示例10-20](#fine_tuning_usage)所示。在使用生产环境之前，请确保首先评估该模型。
- en: Tip
  id: totrans-474
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: You can also use the testing techniques discussed in [Chapter 11](ch11.html#ch11)
    when evaluating fine-tuned models.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估微调模型时，您也可以使用[第11章](ch11.html#ch11)中讨论的测试技术。
- en: Example 10-20\. Using a fine-tuned model
  id: totrans-476
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例10-20。使用微调模型
- en: '[PRE24]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: While these examples show the fine-tuning process with OpenAI, the process will
    be similar with other providers even if the implementation details may differ.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些示例展示了使用OpenAI的微调过程，但与其他提供商相比，过程将相似，即使实现细节可能不同。
- en: Warning
  id: totrans-479
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If you decide to leverage fine-tuning, be mindful that you won’t be able to
    take advantage of the latest improvements or optimizations in new LLMs, potentially
    making the fine-tuning process a waste of your time and money.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您决定利用微调，请注意您将无法利用最新的人工智能语言模型（LLM）的改进或优化，这可能会使微调过程成为您时间和金钱的浪费。
- en: With this final optimization step, you should now feel confident in building
    GenAI services that not only meet your security and quality requirements but also
    achieve your desired throughput and latency metrics.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个最后的优化步骤，您现在应该对构建满足您安全和质量要求，同时实现您期望的吞吐量和延迟指标的人工智能服务感到自信。
- en: Summary
  id: totrans-482
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned about several optimization strategies to improve
    the throughput and quality of your services. A few optimizations you added covered
    various caching (keyword, semantic, context), prompt engineering, model quantization,
    and fine-tuning.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了关于提高您服务吞吐量和质量的一些优化策略。您添加的一些优化涵盖了各种缓存（关键词、语义、上下文）、提示工程、模型量化以及微调。
- en: 'In the next chapter, we will shift focus to the last step in building AI services:
    deploying your GenAI solution. This includes exploring deployment patterns for
    AI services and containerization with Docker.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将重点关注构建人工智能服务中的最后一步：部署您的通用人工智能解决方案。这包括探索人工智能服务的部署模式和与Docker的容器化。
- en: ^([1](ch10.html#id1069-marker)) See the OpenAI Batch API available in the [OpenAI
    API documentation](https://oreil.ly/0t59w).
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch10.html#id1069-marker)) 在[OpenAI API文档](https://oreil.ly/0t59w)中查看可用的OpenAI批处理API。
- en: ^([2](ch10.html#id1072-marker)) Learn more about cache control headers at the
    [MDN website](https://oreil.ly/-Y5JP).
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch10.html#id1072-marker)) 在[MDN网站](https://oreil.ly/-Y5JP)上了解更多关于缓存控制头的信息。
- en: ^([3](ch10.html#id1076-marker)) You may still require a trained embedder model
    for significant cost savings, as making frequent API calls to an off-the-shelf
    embedder model could incur additional costs, diminishing your overall savings.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch10.html#id1076-marker)) 您可能仍然需要训练嵌入模型以实现显著的成本节约，因为频繁调用现成的嵌入模型可能会产生额外的成本，从而减少您的总体节省。
- en: ^([4](ch10.html#id1128-marker)) For better security, you still need to sanitize
    any LLM-generated code before forwarding it to downstream systems for execution.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch10.html#id1128-marker)) 为了更好的安全性，在将任何由LLM生成的代码转发到下游系统执行之前，您仍然需要对其进行清理。
