- en: Capitolo 10\. Ottimizzazione dei servizi AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Questo lavoro è stato tradotto utilizzando l''AI. Siamo lieti di ricevere il
    tuo feedback e i tuoi commenti: [translation-feedback@oreilly.com](mailto:translation-feedback@oreilly.com)'
  prefs: []
  type: TYPE_NORMAL
- en: In questo capitolo imparerai a ottimizzare ulteriormente i tuoi servizi attraverso
    l'ingegneria del prompt, la quantificazione dei modelli e i meccanismi di caching.
  prefs: []
  type: TYPE_NORMAL
- en: Tecniche di ottimizzazione
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gli obiettivi dell'ottimizzazione di un servizio di IA sono il miglioramento
    della qualità dell'output o delle prestazioni (latenza, throughput, costi, ecc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'Le ottimizzazioni relative alle prestazioni includono le seguenti:'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizzare le API per l'elaborazione batch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching (parola chiave, semantico, contesto o prompt)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantizzazione del modello
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Le ottimizzazioni relative alla qualità includono le seguenti:'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizzo di output strutturati
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingegneria prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Messa a punto del modello
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Esaminiamo ciascuna di esse in modo più dettagliato.
  prefs: []
  type: TYPE_NORMAL
- en: Elaborazione in batch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spesso vuoi che un LLM elabori lotti di voci contemporaneamente. La soluzione
    più ovvia è quella di inviare più chiamate API per ogni voce. Tuttavia, l'approccio
    ovvio può essere costoso e lento e può portare il tuo fornitore di modelli a limitare
    le tariffe.
  prefs: []
  type: TYPE_NORMAL
- en: 'In questi casi, puoi sfruttare due tecniche distinte per elaborare i dati in
    batch attraverso un LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: Aggiornare gli schemi di output strutturato per restituire più esempi contemporaneamente
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identificare e utilizzare le API dei fornitori di modelli progettate per l'elaborazione
    in batch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: La prima soluzione prevede che tu aggiorni i modelli o i prompt di Pydantic
    in modo da richiedere un elenco di output per ogni richiesta. In questo caso,
    puoi elaborare i dati in batch con una manciata di richieste invece di una per
    voce.
  prefs: []
  type: TYPE_NORMAL
- en: Un'implementazione della prima soluzione è mostrata nell'[Esempio 10-1](#batch_processing_structured_outputs).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 10-1\. Aggiornamento dello schema di output strutturato per l'analisi
    di più elementi
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Aggiornare il modello Pydantic per includere un elenco di modelli `Category`.
  prefs: []
  type: TYPE_NORMAL
- en: Ora puoi passare il nuovo schema insieme a un elenco di titoli di documenti
    al client OpenAI per elaborare più voci in un'unica chiamata API. Tuttavia, un'alternativa
    e forse la soluzione migliore sarà quella di utilizzare un'API batch, se disponibile.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunatamente, i fornitori di modelli come OpenAI forniscono già delle API
    adatte a questi casi d'uso. Sotto il cofano, questi fornitori possono eseguire
    delle code di attività per elaborare un singolo lavoro batch in background, fornendo
    aggiornamenti sullo stato fino al completamento del batch per recuperare i risultati.
  prefs: []
  type: TYPE_NORMAL
- en: Rispetto all'utilizzo diretto degli endpoint standard, sarai in grado di inviare
    gruppi di richieste asincrone a costi inferiori (fino al 50% con OpenAI), di usufruire
    di limiti di velocità più elevati e di garantire tempi di completamento.^([1](ch10.html#id1069))Il
    servizio batch job è ideale per l'elaborazione di lavori che non richiedono risposte
    immediate, come l'utilizzo di OpenAI LLMs per analizzare, classificare o tradurre
    grandi volumi di documenti in background.
  prefs: []
  type: TYPE_NORMAL
- en: Per inviare un lavoro batch, avrai bisogno di un file `jsonl` in cui ogni riga
    contiene i dettagli di una singola richiesta all'API, come mostrato nell'[Esempio
    10-2](#jsonl). Inoltre, come si vede in questo esempio, per creare il file JSONL,
    puoi iterare le voci e generare dinamicamente il file.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 10-2\. Creare un file JSONL dalle voci
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Una volta creato, puoi inviare il file all'API batch per l'elaborazione, come
    mostrato nell'[esempio 10-3](#batch_processing_api).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 10-3\. Elaborazione di lavori batch con l'API OpenAI Batch
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Ora puoi sfruttare gli endpoint batch offline per elaborare più voci in una
    sola volta, con tempi di consegna garantiti e un notevole risparmio sui costi.
  prefs: []
  type: TYPE_NORMAL
- en: Oltre a sfruttare gli output strutturati e le API batch per ottimizzare i tuoi
    servizi, puoi anche sfruttare le tecniche di caching per accelerare in modo significativo
    i tempi di risposta e i costi delle risorse dei tuoi server.
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nei servizi GenAI, spesso ti affidi alla risposta di dati/modelli che richiedono
    calcoli significativi o lunghe elaborazioni. Se ci sono più utenti che richiedono
    gli stessi dati, ripetere le stesse operazioni può essere dispendioso. Invece,
    puoi usare le tecniche di caching per memorizzare e recuperare i dati a cui si
    accede di frequente per aiutarti a ottimizzare i tuoi servizi accelerando i tempi
    di risposta, riducendo il carico del server e risparmiando larghezza di banda
    e costi operativi.
  prefs: []
  type: TYPE_NORMAL
- en: Ad esempio, in un chatbot FAQ pubblico in cui gli utenti pongono per lo più
    le stesse domande, potresti voler riutilizzare le risposte nella cache per periodi
    più lunghi. D'altra parte, per chatbot più personalizzati e dinamici, puoi aggiornare
    frequentemente (cioè invalidare) la risposta nella cache.
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dovresti sempre considerare la frequenza di aggiornamento della cache in base
    alla natura dei dati e al livello accettabile di staleness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Le strategie di caching più importanti per i servizi GenAI includono:'
  prefs: []
  type: TYPE_NORMAL
- en: Caching delle parole chiave
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching semantico
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching del contesto o del prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Esaminiamo ciascuna di esse in modo più dettagliato.
  prefs: []
  type: TYPE_NORMAL
- en: Caching delle parole chiave
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Se hai bisogno solo di un semplice meccanismo di caching per memorizzare le
    funzioni o le risposte degli endpoint, puoi usare il *keyword caching*, che consiste
    nel memorizzare le risposte in base alle corrispondenze esatte delle query di
    input come coppie chiave-valore.
  prefs: []
  type: TYPE_NORMAL
- en: In FastAPI, librerie come `fastapi-cache` possono aiutarti a implementare la
    cache delle parole chiave in poche righe di codice, su qualsiasi funzione o endpoint.
    Le cache FastAPI ti danno anche la possibilità di collegare backend di archiviazione
    come Redis per centralizzare l'archivio della cache in tutte le tue istanze.
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In alternativa, puoi implementare un meccanismo di caching personalizzato con
    un cache store utilizzando pacchetti di livello inferiore come `cachetools`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Per iniziare, tutto ciò che devi fare è inizializzare e configurare il sistema
    di caching come parte del ciclo di vita dell''applicazione, come mostrato nell''[esempio
    10-4](#caching_lifespan). Puoi installare la cache FastAPI utilizzando il seguente
    comando:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Esempio 10-4\. Configurazione della durata della cache FastAPI
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Inizializza `FastAPICache` con un `RedisBackend` che non decodifica le risposte
    in modo che i dati in cache siano memorizzati come byte (binari). Questo perché
    la decodifica delle risposte romperebbe la cache alterando il formato originale
    della risposta.
  prefs: []
  type: TYPE_NORMAL
- en: Una volta configurato il sistema di caching, puoi decorare le tue funzioni o
    i gestori di endpoint per mettere in cache i loro output, come mostrato nell'[Esempio
    10-5](#caching_functions_endpoint).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 10-5\. Caching dei risultati delle funzioni e degli endpoint
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Il decoratore `cache()` deve sempre arrivare per ultimo. Invalida la cache tra
    60 secondi impostando `expires=60` per ricompilare gli output.
  prefs: []
  type: TYPE_NORMAL
- en: Il decoratore `cache()` mostrato nell'[Esempio 10-5](#caching_functions_endpoint)
    inietta le dipendenze per gli oggetti `Request` e `Response` in modo da poter
    aggiungere intestazioni di controllo della cache alla risposta in uscita. Queste
    intestazioni di controllo della cache istruiscono i client su come mettere in
    cache le risposte da parte loro, specificando un insieme di direttive (cioè istruzioni).
  prefs: []
  type: TYPE_NORMAL
- en: 'Queste sono alcune direttive comuni per il controllo della cache durante l''invio
    delle risposte:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max-age`'
  prefs: []
  type: TYPE_NORMAL
- en: Definisce il tempo massimo (in secondi) in cui una risposta è considerata fresca.
  prefs: []
  type: TYPE_NORMAL
- en: '`no-cache`'
  prefs: []
  type: TYPE_NORMAL
- en: Forza la riconvalida in modo che i client controllino gli aggiornamenti costanti
    con il server.
  prefs: []
  type: TYPE_NORMAL
- en: '`no-store`'
  prefs: []
  type: TYPE_NORMAL
- en: Impedisce completamente il caching
  prefs: []
  type: TYPE_NORMAL
- en: '`private`'
  prefs: []
  type: TYPE_NORMAL
- en: Memorizza le risposte in una cache privata (ad esempio, la cache locale dei
    browser).
  prefs: []
  type: TYPE_NORMAL
- en: 'Una risposta potrebbe avere intestazioni di controllo della cache come `Cache-Control:
    max-age=180, private` per impostare queste direttive.^([2](ch10.html#id1072))'
  prefs: []
  type: TYPE_NORMAL
- en: Dato che il caching delle parole chiave lavora sulle corrispondenze esatte,
    è più adatto alle funzioni e alle API che prevedono input con corrispondenze frequenti.
    Tuttavia, nei servizi GenAI che accettano query variabili da parte dell'utente,
    potresti voler prendere in considerazione altri meccanismi di caching che si basano
    sul significato degli input quando restituiscono una risposta in cache. È qui
    che la cache semantica può rivelarsi utile.
  prefs: []
  type: TYPE_NORMAL
- en: Caching semantico
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Il*caching semantico* è un meccanismo di caching che restituisce un valore memorizzato
    in base a input simili.
  prefs: []
  type: TYPE_NORMAL
- en: Sotto il cofano, il sistema utilizza codificatori e vettori di incorporamento
    per catturare la semantica e i significati degli input, quindi esegue ricerche
    di somiglianza tra le coppie chiave-valore memorizzate per restituire una risposta
    nella cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rispetto alla memorizzazione nella cache delle parole chiave, gli input simili
    possono restituire la stessa risposta memorizzata nella cache. Gli input al sistema
    non devono essere identici per essere riconosciuti come simili. Anche se tali
    input hanno strutture o formulazioni di frasi diverse o contengono imprecisioni,
    saranno comunque considerati simili perché hanno lo stesso significato. Inoltre,
    viene richiesta la stessa risposta. A titolo di esempio, le seguenti query sono
    considerate simili perché hanno lo stesso intento:'
  prefs: []
  type: TYPE_NORMAL
- en: Come si costruiscono i servizi generativi con FastAPI?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qual è il processo di sviluppo dei servizi FastAPI per Genai?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Questo sistema di caching contribuisce a un significativo risparmio sui costi^([3](ch10.html#id1076))
    riducendo le chiamate API del [30-40%](https://oreil.ly/gjGz6) (cioè con un tasso
    di risposta alla cache del 60-70%) a seconda del caso d'uso e delle dimensioni
    della base di utenti. Ad esempio, le applicazioni Q&A RAG che ricevono domande
    frequenti da una vasta base di utenti possono ridurre le chiamate API del 69%
    utilizzando una cache semantica.
  prefs: []
  type: TYPE_NORMAL
- en: 'In un tipico sistema RAG, ci sono due punti in cui la cache può ridurre le
    operazioni che richiedono risorse e tempo:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prima che il LLM* restituisca una risposta nella cache invece di generarne
    una nuova'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prima dell''archivio vettoriale* per arricchire i prompt con documenti memorizzati
    nella cache, invece di cercarne e recuperarne di nuovi.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quando integri un componente di cache semantica nel tuo sistema RAG, devi considerare
    se la restituzione di una risposta in cache potrebbe avere un impatto negativo
    sull''esperienza dell''utente dell''applicazione. Ad esempio, se le risposte di
    LLM vengono memorizzate nella cache, entrambe le query seguenti restituirebbero
    la stessa risposta a causa della loro elevata somiglianza semantica, inducendo
    il sistema di cache semantica a trattarle come quasi identiche:'
  prefs: []
  type: TYPE_NORMAL
- en: Riassumi questo testo in 100 parole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riassumi questo testo in 50 parole
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In questo modo si ha l'impressione che i servizi non rispondano alle query.
    Dal momento che potresti ancora volere diversi output LLM nella tua applicazione,
    implementeremo una cache semantica per il recupero dei documenti per il tuo sistema
    RAG.[La Figura 10-1](#semantic_cache) mostra l'architettura completa del sistema.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1001](assets/bgai_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 10-1\. Caching semantico nell'architettura del sistema RAG
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Cominciamo a implementare il sistema di caching semantico da zero e poi vedremo
    come scaricare la funzionalità a una libreria esterna come`gptcache`.
  prefs: []
  type: TYPE_NORMAL
- en: Costruire un servizio di caching semantico da zero
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Puoi implementare un sistema di caching semantico implementando i seguenti
    componenti:'
  prefs: []
  type: TYPE_NORMAL
- en: Un client di cache store
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Un client per l'archiviazione vettoriale di documenti
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Un modello di incorporazione
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L['esempio 10-6](#semantic_cache_cache_store) mostra come implementare il client
    del cache store.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 10-6\. Client del negozio di cache
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Inizializza un client Qdrant in esecuzione sulla memoria che funge da cache
    store.
  prefs: []
  type: TYPE_NORMAL
- en: Una volta inizializzato il client dell'archivio cache, puoi configurare l'archivio
    vettoriale dei documenti seguendo l'[Esempio 10-7](#semantic_cache_doc_store).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 10-7\. Client del negozio di documenti
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Carica una collezione di documenti nell'archivio vettoriale di Qdrant.
  prefs: []
  type: TYPE_NORMAL
- en: Con i client della cache e dell'archivio vettoriale di documenti pronti, puoi
    ora implementare il servizio di cache semantica, come mostrato nell'[Esempio 10-8](#semantic_cache_service),
    con i metodi per calcolare gli embeddings ed eseguire le ricerche nella cache.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 10-8\. Sistema di caching semantico
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Imposta una soglia di somiglianza. Qualsiasi punteggio superiore a questa soglia
    sarà un hit della cache.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Interroga l'archivio dei documenti se non c'è un riscontro nella cache. Memorizza
    nella cache i documenti recuperati con l'incorporazione vettoriale della query
    come chiave della cache.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Se non ci sono documenti correlati o cache disponibili per la query data, restituisce
    una risposta in scatola.
  prefs: []
  type: TYPE_NORMAL
- en: Ora che hai un servizio di caching semantico, puoi usarlo per recuperare i documenti
    in cache dalla memoria seguendo l'[Esempio 10-9](#semantic_cache_qdrant_usage).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 10-9\. Implementazione di una cache semantica in un sistema RAG con
    Qdrant
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Ora dovresti aver capito meglio come implementare i tuoi sistemi di caching
    semantico personalizzati utilizzando un client di database vettoriale.
  prefs: []
  type: TYPE_NORMAL
- en: Caching semantico con la cache GPT
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Se non hai bisogno di sviluppare il tuo servizio di caching semantico da zero,
    puoi anche utilizzare la libreria modulare `gptcache` che ti dà la possibilità
    di scambiare vari componenti di archiviazione, caching e incorporazione.
  prefs: []
  type: TYPE_NORMAL
- en: 'Per configurare una cache semantica con `gptcache`, devi prima installare il
    pacchetto:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Quindi carica il sistema all'avvio dell'applicazione, come mostrato nell'[Esempio
    10-10](#configrue_gptcache).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 10-10\. Configurazione della cache GPT
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Seleziona una funzione di callback post-elaborazione per selezionare un elemento
    casuale tra quelli restituiti nella cache.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Seleziona una funzione di callback pre-inclusione per utilizzare l'ultima query
    per impostare una nuova cache.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Usa il modello di incorporazione ONNX per calcolare i vettori di incorporazione.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_optimizing_ai_services_CO7-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizza `OnnxModelEvaluation` per calcolare i punteggi di somiglianza tra gli
    elementi in cache e una determinata query.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_optimizing_ai_services_CO7-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Imposta le opzioni di configurazione della cache, come ad esempio la soglia
    di somiglianza.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_optimizing_ai_services_CO7-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Fornisci una chiave API del client OpenAI per GPT Cache per eseguire automaticamente
    la cache semantica sulle risposte dell'API LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Una volta inizializzato, `gptcache` si integrerà perfettamente con il client
    LLM di OpenAI nella tua applicazione. Ora puoi effettuare query multiple LLM,
    come mostrato nell'[Esempio 10-11](#semantic_caching_gptcache), sapendo che `gptcache`
    metterà in cache le risposte LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 10-11\. Caching semantico con la cache GPT
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: L'utilizzo di librerie esterne come `gptcache`, come mostrato nell'[Esempio
    10-11](#semantic_caching_gptcache), rende semplice l'implementazione del caching
    semantico.
  prefs: []
  type: TYPE_NORMAL
- en: Una volta che il sistema di caching è attivo e funzionante, puoi regolare *le
    soglie di somiglianza* per ottimizzare le percentuali di successo della cache.
  prefs: []
  type: TYPE_NORMAL
- en: Soglia di somiglianza
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Quando crei un servizio di cache semantica, potresti dover regolare la soglia
    di somiglianza in base alle query fornite per ottenere tassi di risposta alla
    cache elevati e accurati. Puoi fare riferimento alla [visualizzazione interattiva
    dei cluster della cache semantica](https://semanticcachehit.com) mostrata nella
    [Figura 10-2](#semantic_cache_visualization) per capire meglio il concetto disoglia
    di somiglianza.
  prefs: []
  type: TYPE_NORMAL
- en: Aumentando il valore della soglia nella [Figura 10-2](#semantic_cache_visualization)
    si otterrà un grafo meno connesso, mentre riducendolo al minimo si possono ottenere
    dei falsi positivi. Pertanto, è consigliabile eseguire alcuni esperimenti per
    mettere a punto la soglia di somiglianza per la propria applicazione.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1002](assets/bgai_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figura 10-2\. Visualizzazione del caching semantico (Fonte: [semanticcachehit.com](https://semanticcachehit.com))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Politiche di sfratto
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Un altro concetto rilevante per la cache è quello delle *politiche di eviction*
    che controllano il comportamento della cache quando il meccanismo di caching raggiunge
    la sua capacità massima. La selezione della politica di eviction appropriata deve
    essere adatta al tuo caso d'uso.
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Dato che le dimensioni della memoria cache sono spesso limitate, puoi aggiungere
    un metodo `evict()` al `SemanticCachingService` che hai implementato nell'[Esempio
    10-8](#semantic_cache_service).
  prefs: []
  type: TYPE_NORMAL
- en: La[Tabella 10-1](#eviction_policies) mostra alcuni criteri di sfratto che puoi
    scegliere.
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 10-1\. Politiche di sfratto
  prefs: []
  type: TYPE_NORMAL
- en: '| Politica | Descrizione | Caso d''uso |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Primo entrato, primo uscito (FIFO) | Rimuove gli elementi più vecchi | Quando
    tutti gli elementi hanno la stessa priorità |'
  prefs: []
  type: TYPE_TB
- en: '| Ultimo usato (LRU) | Traccia l''utilizzo della cache nel tempo e rimuove
    l''ultimo elemento consultato. | Quando è più probabile che gli elementi consultati
    di recente vengano consultati nuovamente |'
  prefs: []
  type: TYPE_TB
- en: '| Utilizzato meno frequentemente (LFU) | Traccia l''utilizzo della cache nel
    tempo e rimuove l''elemento a cui si accede meno frequentemente. | Quando gli
    articoli usati meno frequentemente devono essere rimossi per primi |'
  prefs: []
  type: TYPE_TB
- en: '| Utilizzato più di recente (MRU) | Traccia l''utilizzo della cache nel tempo
    e rimuove l''ultimo elemento consultato. | Utilizzato raramente, quando gli articoli
    usati più di recente hanno meno probabilità di essere consultati di nuovo. |'
  prefs: []
  type: TYPE_TB
- en: '| Sostituzione casuale (RR) | Rimuove un elemento casuale dalla cache | Semplice
    e veloce, da usare quando non ha un impatto sulle prestazioni |'
  prefs: []
  type: TYPE_TB
- en: La scelta del giusto criterio di sfratto dipende dal tuo caso d'uso e dai requisiti
    dell'applicazione. In generale, puoi iniziare con il criterio LRU prima di passare
    allealternative.
  prefs: []
  type: TYPE_NORMAL
- en: Ora dovresti sentirti più sicuro nell'implementazione di meccanismi di caching
    semantico che si applicano al recupero di documenti o alle risposte dei modelli.
    Poi, impariamo a conoscere il caching contestuale o prompt, che ottimizza le query
    ai modelli in base ai loro input.
  prefs: []
  type: TYPE_NORMAL
- en: Caching del contesto e del prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Il*caching del contesto*, noto anche come *prompt caching*, è un meccanismo
    di caching adatto a scenari in cui si fa riferimento a grandi quantità di contesto
    ripetutamente all'interno di piccole richieste. È stato progettato per riutilizzare
    gli stati di attenzione precalcolati da prompt frequentemente riutilizzati, eliminando
    la necessità di ricompilare in modo ridondante l'intero contesto di input ogni
    volta che viene effettuata una nuova richiesta.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dovresti prendere in considerazione l''utilizzo di una cache contestuale quando
    i tuoi servizi prevedono quanto segue:'
  prefs: []
  type: TYPE_NORMAL
- en: Chatbot con istruzioni di sistema estese e lunghe conversazioni a più turni
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analisi ripetitiva di file video lunghi
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Query ricorrenti su set di documenti di grandi dimensioni
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analisi frequente del repository di codice o correzione di bug
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riassunti di documenti, libri, documenti, trascrizioni di podcast e altri contenuti
    di lunga durata.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fornire un gran numero di esempi in prompt (cioè l'apprendimento nel contesto).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondo Anthropic, il caching dei prompt può ridurre i costi fino al 90% e la
    latenza fino all'85% per i prompt lunghi.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gli autori del [documento sul prompt caching](https://oreil.ly/augpd) che presenta
    questa tecnica affermano anche che:'
  prefs: []
  type: TYPE_NORMAL
- en: Abbiamo scoperto che la Prompt Cache riduce significativamente la latenza nel
    time-to-first-token, soprattutto per i prompt più lunghi come le risposte alle
    domande basate su documenti e le raccomandazioni. I miglioramenti vanno da 8×
    per l'inferenza basata su GPU a 60× per l'inferenza basata su CPU, il tutto mantenendo
    l'accuratezza dell'output e senza la necessità di modificare i parametri del modello.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: La[Figura 10-3](#context_caching_architecture) visualizza l'architettura del
    sistema di caching del contesto.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1003](assets/bgai_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 10-3\. Architettura del sistema per il caching del contesto
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Al momento in cui scriviamo, OpenAI implementa automaticamente il prompt caching
    per tutte le richieste API senza richiedere modifiche al codice o costi aggiuntivi.
    L['esempio 10-12](#context_cachin_anthropic) mostra un esempio di utilizzo del
    prompt caching quando si utilizza l'API Anthropic.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 10-12\. Caching del contesto e del prompt con l'API Antropica
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Il prompt caching è disponibile solo per alcuni modelli, tra cui Claude 3.5
    Sonnet, Claude 3 Haiku e Claude 3 Opus.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizza il parametro `cache_control` per riutilizzare il contenuto del documento
    di grandi dimensioni in più chiamate API senza elaborarlo ogni volta.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sotto il cofano, il client Anthropic aggiunge `anthropic-beta: prompt-caching-2024-07-31`
    alle intestazioni delle richieste.'
  prefs: []
  type: TYPE_NORMAL
- en: Al momento in cui scriviamo, `ephemeral` è l'unico tipo di cache supportato,
    che corrisponde a una durata della cache di 5 minuti.
  prefs: []
  type: TYPE_NORMAL
- en: Nota
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Quando adotti una cache del contesto, introduci la statualità nelle richieste
    preservando i token in tutte le richieste. Ciò significa che i dati inviati in
    una richiesta influenzeranno le richieste successive, in quanto il server del
    provider del modello può utilizzare il contesto memorizzato nella cache per mantenere
    la continuità tra le interazioni.
  prefs: []
  type: TYPE_NORMAL
- en: Con la funzione di caching del contesto dell'API Gemini, puoi fornire il contenuto
    al modello una sola volta, mettere in cache i token di input e fare riferimento
    a questi token in cache per le richieste future.
  prefs: []
  type: TYPE_NORMAL
- en: L'uso di questi token nella cache può farti risparmiare una spesa significativa
    se eviti di passare ripetutamente lo stesso corpus di token in volumi elevati.Il
    costo della cache dipenderà dalle dimensioni dei token in ingresso e dalla durata
    di memorizzazione del time to live (TTL) desiderato.
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Quando metti in cache un insieme di token, puoi specificare la durata del TTL,
    ossia il tempo in cui la cache deve esistere prima che i token vengano eliminati
    automaticamente. Per impostazione predefinita, il TTL è normalmente impostato
    su 1 ora.
  prefs: []
  type: TYPE_NORMAL
- en: 'Puoi vedere come utilizzare un''istruzione di sistema nella cache nell''[Esempio
    10-13](#context_caching_google). Ti servirà anche il Gemini API Python SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Esempio 10-13\. Caching del contesto con l'API Google Gemini
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Fornisci un nome visualizzato come chiave o identificatore della cache.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Passa il corpus al sistema di cache contestuale. La dimensione minima di una
    cache contestuale è di 32.768 token.
  prefs: []
  type: TYPE_NORMAL
- en: 'Se esegui l''[Esempio 10-13](#context_caching_google) e stampi il sito `response.usage_metadata`,
    dovresti ricevere il seguente output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Si noti come la maggior parte di `prompt_token_count` viene ora messa in cache
    se la si confronta con `cached_content_token_count`. `candidates_token_count`
    si riferisce al conteggio dei token di output o di risposta provenienti dal modello,
    che non viene influenzato dal sistema di cache.
  prefs: []
  type: TYPE_NORMAL
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: I modelli Gemini non fanno distinzione tra i token memorizzati nella cache e
    i normali token di input. Il contenuto memorizzato nella cache viene anteposto
    al prompt. Per questo motivo il numero di token del prompt non viene ridotto quando
    si utilizza la cache.
  prefs: []
  type: TYPE_NORMAL
- en: Con la cache del contesto, non vedrai una drastica riduzione dei tempi di risposta,
    ma ridurrai in modo significativo i costi operativi perché eviterai di inviare
    nuovamente prompt di sistema e token contestuali. Pertanto, questa strategia di
    cache è più adatta quando hai un contesto ampio su cui lavorare, ad esempio quando
    elabori file batch con istruzioni ed esempi completi.
  prefs: []
  type: TYPE_NORMAL
- en: Nota
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: L'utilizzo della stessa cache di contesto e dello stesso prompt non garantisce
    risposte coerenti del modello perché le risposte degli LLMs non sono deterministiche.
    Una cache di contesto non memorizza alcun output.
  prefs: []
  type: TYPE_NORMAL
- en: Il context caching rimane un'area di ricerca attiva. Se vuoi evitare il vendor
    lock-in, ci sono già alcuni progressi in questo campo con strumenti open source
    come [*MemServe*](https://oreil.ly/PXm6B), che implementa il context caching con
    un pool di memoria elastica .
  prefs: []
  type: TYPE_NORMAL
- en: Oltre alla cache, puoi anche esaminare le opzioni per ridurre le dimensioni
    del modello per accelerare i tempi di risposta utilizzando tecniche come la *quantizzazione
    del modello*.
  prefs: []
  type: TYPE_NORMAL
- en: Quantizzazione del modello
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Se hai intenzione di utilizzare tu stesso modelli come gli LLMs, dovresti prendere
    in considerazione l'idea di *quantizzare* (cioè comprimere/ridurre) i tuoi modelli,
    se possibile. Spesso, i repository di modelli open source forniscono anche versioni
    quantizzate che puoi scaricare e utilizzare immediatamente senza dover affrontare
    il processo di quantizzazione.
  prefs: []
  type: TYPE_NORMAL
- en: La*quantizzazione del modello* è un processo di regolazione dei pesi e delle
    attivazioni del modello in cui i parametri del modello ad alta precisione vengono
    proiettati statisticamente in valori a bassa precisione attraverso un'operazione
    di regolazione fine che utilizza fattori di scala sulla distribuzione originale
    dei parametri. È quindi possibile eseguire tutte le operazioni di inferenza critiche
    con una precisione inferiore, dopodiché è possibile convertire gli output a una
    precisione superiore per mantenere la qualità e migliorare le prestazioni.
  prefs: []
  type: TYPE_NORMAL
- en: Riducendo la precisione si riducono anche i requisiti di memoria, riducendo
    teoricamente il consumo energetico e velocizzando operazioni come la moltiplicazione
    di matrici attraverso l'aritmetica intera. Questo permette anche di eseguire i
    modelli su dispositivi embedded, che possono supportare solo tipi di dati interi.
  prefs: []
  type: TYPE_NORMAL
- en: '[La Figura 10-4](#quantization_process) mostra il processo di quantizzazione
    completo.'
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1004](assets/bgai_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 10-4\. Processo di quantizzazione
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: È possibile risparmiare più di una manciata di gigabyte nel consumo di memoria
    della GPU, poiché i tipi di dati a bassa precisione come gli interi a 8 bit richiedono
    una quantità di RAM per parametro significativamente inferiore rispetto a un tipo
    di dati come i float a 32 bit.
  prefs: []
  type: TYPE_NORMAL
- en: Il compromesso tra precisione e qualità
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: La[Figura 10-5](#quantization) mette a confronto un modello non quantizzato
    e un modello quantizzato.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1005](assets/bgai_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 10-5\. Quantizzazione
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Poiché ogni parametro float a 32 bit ad alta precisione consuma 4 byte di memoria
    della GPU, un modello a 1B parametri richiederebbe 4 GB di memoria solo per l'inferenza.
    Se intendi riqualificare o perfezionare lo stesso modello, avrai bisogno di almeno
    24 GB di VRAM della GPU. Questo perché ogni parametro richiederebbe anche la memorizzazione
    di informazioni come i gradienti, gli stati dell'ottimizzatore dell'addestramento,
    le attivazioni e lo spazio di memoria temporaneo, consumando complessivamente
    altri 24 byte. Questo stima fino a 6 volte il fabbisogno di memoria rispetto al
    solo caricamento dei pesi del modello. Lo stesso modello 1B richiederebbe quindi
    una GPU da 24 GB, che le migliori e più costose schede grafiche consumer come
    la NVIDIA RTX 4090 potrebbero ancora faticare a soddisfare.
  prefs: []
  type: TYPE_NORMAL
- en: 'Invece di utilizzare il formato standard 32-float, puoi selezionare uno dei
    seguenti formati:'
  prefs: []
  type: TYPE_NORMAL
- en: La*virgola mobile a 16 bit (FP16) di* mezza l'utilizzo della memoria senza compromettere
    la qualità del modello.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gli*interi a 8 bit (INT8)* offrono un enorme risparmio di memoria, ma con una
    significativa perdita di qualità.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Il*brain floating-point a 16 bit (BFLOAT16)* con un intervallo simile all'FP32
    bilancia il compromesso tra memoria e qualità.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Il*numero intero a 4 bit (INT4)* offre un equilibrio tra l'efficienza della
    memoria e la precisione di calcolo, rendendolo adatto ai dispositivi a basso consumo.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L'*intero a 1 bit (INT1)* utilizza il tipo di dati a più bassa precisione con
    la massima riduzione delle dimensioni del modello. La ricerca per la creazione
    di [LLMs a 1 bit](https://oreil.ly/QH9nH) di alta qualità è attualmente in corso.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A titolo di confronto, la [Tabella 10-2](#quantization_comparison) mostra la
    riduzione delle dimensioni del modello quando si quantizzano i modelli della famiglia
    Llama.
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 10-2\. Impatto della quantizzazione sulle dimensioni dei modelli di
    Llama^([a](ch10.html#id1097))
  prefs: []
  type: TYPE_NORMAL
- en: '| Modello | Originale | FP16 | 8 Bit | 6 Bit | 4 Bit | 2Bit |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 2 70B | 140 GB | 128,5 GB | 73,23 GB | 52,70 GB | 36,20 GB | 28,59
    GB |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 3 8B | 16,07 GB | 14,97 GB | 7,96 GB | 4,34 GB | 4,34 GB | 2,96 GB
    |'
  prefs: []
  type: TYPE_TB
- en: '| ^([a](ch10.html#id1097-marker)) Fonti: [Llama.cpp repository GitHub](https://oreil.ly/9iYtL)
    e la [scheda modello Hugging Face Llama 2 70B di Tom Jobbins](https://oreil.ly/BMDtR)
    |'
  prefs: []
  type: TYPE_TB
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Oltre alla VRAM della GPU necessaria per montare il modello, avrai bisogno di
    altri 5-8 GB di VRAM della GPU per l'overhead durante il caricamento del modello.
  prefs: []
  type: TYPE_NORMAL
- en: Allo stato attuale della ricerca, mantenere l'accuratezza con i tipi di dati
    INT4 e INT1 solo interi è una sfida e il miglioramento delle prestazioni con INT32
    o FP16 non è significativo. Pertanto, il tipo di dati a bassa precisione più diffuso
    è INT8 per l'inferenza.
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondo la [ricerca](https://oreil.ly/C7Lz3), l''utilizzo dell''aritmetica
    dei soli numeri interi per l''inferenza sarà più efficiente rispetto ai numeri
    a virgola mobile. Tuttavia, quantificare i numeri a virgola mobile in numeri interi
    può essere complicato: ad esempio, solo 256 valori possono essere rappresentati
    in INT8, mentre float32 può rappresentare un''ampia gamma di valori.'
  prefs: []
  type: TYPE_NORMAL
- en: Numeri in virgola mobile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Per capire perché proiettare i float a 32 bit in altri formati farebbe risparmiare
    così tanto in termini di memoria della GPU, analizziamo come si articola il tutto.
  prefs: []
  type: TYPE_NORMAL
- en: 'Un numero in virgola mobile a 32 bit è composto dai seguenti tipi di bit:'
  prefs: []
  type: TYPE_NORMAL
- en: Bit di*segno* che descrive se un numero è positivo o negativo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bit dell*'esponente* che controllano la scala del numero
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bit di*mantissa* che contengono le cifre effettive che determinano la sua precisione
    (noti anche come bit *di frazione* )
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nella [Figura 10-6](#quantization_bits) puoi vedere una visualizzazione dei
    bit nei numeri in virgola mobile sopra citati.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1006](assets/bgai_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 10-6\. Bit dei numeri float a 32 bit, float a 16 bit e bfloat16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Quando proietti un numero FP32 in altri formati, in effetti, lo schiacci in
    intervalli più piccoli, perdendo la maggior parte dei bit della mantissa e adattando
    i bit dell'esponente, ma senza perdere gran parte della precisione. Puoi vedere
    questo fenomeno in azione facendo riferimento alla [Figura 10-7](#quantization_floating_numbers).
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1007](assets/bgai_1007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 10-7\. Quantizzazione di numeri in virgola mobile in numeri interi
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In effetti, la [ricerca sulle strategie di quantizzazione per i modelli LLM
    pre-addestrati](https://oreil.ly/Swfz7) ha dimostrato che gli LLM con quantizzazione
    a 4 bit possono mantenere prestazioni simili alle loro controparti non quantizzate.
    Tuttavia, se da un lato la quantizzazione consente di risparmiare memoria, dall'altro
    può ridurre la velocità di inferenza degli LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Come quantizzare le LLMs preaddestrate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Una di queste tecniche, chiamata [*GPTQ*](https://oreil.ly/rHYKZ), è in grado
    di quantizzare LLMs con 175 miliardi di parametri in circa 4 ore di GPU, riducendo
    la larghezza dei bit a 3 o 4 bit per peso, con un calo di precisione trascurabile
    rispetto al modello non compresso.
  prefs: []
  type: TYPE_NORMAL
- en: Gli autori delle librerie Hugging Face `transformers` e `optimum` hanno collaborato
    strettamente con gli sviluppatori della libreria `auto-gptq` per fornire una semplice
    API per applicare la quantizzazione GPTQ su LLMs open source. Optimum è una libreria
    che fornisce API per eseguire la quantizzazione utilizzando diversi strumenti.
  prefs: []
  type: TYPE_NORMAL
- en: Con la quantizzazione GPTQ, puoi quantizzare il tuo modello linguistico preferito
    a 8, 4, 3 o addirittura 2 bit senza un grosso calo di prestazioni, mantenendo
    una velocità di inferenza superiore a quella supportata dalla maggior parte dell'hardware
    GPT. Puoi seguire l'[Esempio 10-14](#gptq_quantization) per quantizzare un modello
    pre-addestrato sulla tua GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Le dipendenze che devi installare per eseguire l''[Esempio 10-14](#gptq_quantization)
    includonole seguenti:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Esempio 10-14\. Quantizzazione del modello GPTQ con le librerie Hugging Face
    e AutoGPTQ
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Carica la versione `float16` del modello preaddestrato `facebook/opt-125m` prima
    della quantizzazione.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO10-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Usa il dataset `c4` per calibrare la quantizzazione.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO10-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Quantizza solo i blocchi del livello di decodifica del modello.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_optimizing_ai_services_CO10-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Usa la lunghezza della sequenza modello di `2048` per elaborare il set di dati.
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Come riferimento, un modello 175B richiederà 4 ore di GPU su NVIDIA A100 per
    essere quantizzato. Tuttavia, vale la pena cercare nel repository dei modelli
    di Hugging Face i modelli prequantizzati, perché potresti scoprire che qualcuno
    ha già fatto il lavoro .
  prefs: []
  type: TYPE_NORMAL
- en: Ora che hai compreso le tecniche di ottimizzazione delle prestazioni, vediamo
    come migliorare la qualità dei tuoi servizi GenAI utilizzando metodi come gli
    output strutturati.
  prefs: []
  type: TYPE_NORMAL
- en: Uscite strutturate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I modelli fondamentali come gli LLMs possono essere utilizzati come componenti
    di una pipeline di dati o collegati ad applicazioni a valle. Ad esempio, puoi
    usare questi modelli per estrarre e analizzare informazioni da documenti o per
    generare codice che può essere eseguito su altri sistemi.
  prefs: []
  type: TYPE_NORMAL
- en: Puoi chiedere a LLM di fornire una risposta testuale contenente informazioni
    JSON. Dovrai quindi estrarre e analizzare questa stringa JSON utilizzando strumenti
    come regex e Pydantic. Tuttavia, non c'è alcuna garanzia che il modello si attenga
    sempre alle tue istruzioni. Poiché i tuoi sistemi a valle potrebbero basarsi su
    output JSON, potrebbero lanciare eccezioni e gestire in modo errato input non
    validi.
  prefs: []
  type: TYPE_NORMAL
- en: Sono stati rilasciati diversi pacchetti di utilità come Instructor per migliorare
    la robustezza delle risposte LLM, prendendo uno schema ed effettuando diverse
    chiamate API con vari modelli di prompt per raggiungere l'output desiderato. Se
    da un lato queste soluzioni migliorano la robustezza, dall'altro aggiungono costi
    significativi alla tua soluzione a causa delle successive chiamate API ai fornitori
    di modelli.
  prefs: []
  type: TYPE_NORMAL
- en: Recentemente, i fornitori di modelli hanno aggiunto una funzione per richiedere
    output strutturati fornendo schemi quando si effettuano chiamate API al modello,
    come puoi vedere nell'[Esempio 10-15](#structured_outputs). Questo aiuta a ridurre
    il lavoro di template del prompt che devi fare tu stesso e mira a migliorare l'*allineamento*
    del modello alle tue intenzioni quando restituisce una risposta.
  prefs: []
  type: TYPE_NORMAL
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Al momento in cui scriviamo, solo il più recente OpenAI SDK supporta i modelli
    Pydantic per abilitare gli output strutturati.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 10-15\. Uscite strutturate
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO11-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Specifica un modello Pydantic per gli output strutturati.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO11-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Fornisce lo schema definito al client del modello quando effettua la chiamata
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Se il tuo fornitore di modelli non supporta in modo nativo gli output strutturati,
    puoi comunque sfruttare le funzionalità di completamento delle chat del modello
    per aumentare la robustezza degli output strutturati, come mostrato nell'[Esempio
    10-16](#structured_outputs_completions).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 10-16\. Uscite strutturate basate sul prefill delle chat
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO12-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Limitare i token di output per migliorare la robustezza e la velocità delle
    risposte strutturate e per ridurre i costi.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO12-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Salta il preambolo e restituisce direttamente un JSON precompilando la risposta
    dell'assistente e includendo il carattere `{`.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO12-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Aggiungiamo di nuovo il prefiltrato `{` e poi troviamo la chiusura `}` ed estraiamo
    lasottostringa JSON.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_optimizing_ai_services_CO12-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Gestisce i casi in cui non c'è JSON nella risposta, ad esempio se c'è un rifiuto.
  prefs: []
  type: TYPE_NORMAL
- en: Seguire le tecniche sopra descritte dovrebbe aiutarti a migliorare la robustezza
    delle tue pipeline di dati se queste utilizzano gli LLMs come componenti.
  prefs: []
  type: TYPE_NORMAL
- en: Ingegneria prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: L'ingegneria dei prompt è la pratica di creare e perfezionare le query per i
    modelli generativi in modo da produrre i risultati più utili e ottimizzati. Senza
    perfezionare i prompt, dovresti perfezionare i modelli o addestrare un modello
    da zero per ottimizzare la qualità dei risultati.
  prefs: []
  type: TYPE_NORMAL
- en: Molti sostengono che questo campo non abbia il rigore scientifico necessario
    per essere considerato una disciplina ingegneristica. Tuttavia, è possibile affrontare
    il problema da una prospettiva ingegneristica quando si affinano i prompt per
    ottenere la migliore qualità di output dai modelli.
  prefs: []
  type: TYPE_NORMAL
- en: In modo simile a come comunichi con gli altri per ottenere le cose, con i prompt
    ottimizzati puoi comunicare in modo più efficace le tue intenzioni al modello
    per aumentare le possibilità di ottenere le risposte che desideri. Pertanto, il
    prompt non diventa solo un problema di ingegneria ma anche di comunicazione. Un
    modello può essere paragonato a un collega esperto con molta esperienza ma con
    una conoscenza limitata del dominio, pronto ad aiutarti ma che ha bisogno che
    tu fornisca istruzioni ben documentate, possibilmente con alcuni esempi da seguire
    e da abbinare.
  prefs: []
  type: TYPE_NORMAL
- en: Se i tuoi prompt sono vaghi e generici, otterrai anche una risposta media.
  prefs: []
  type: TYPE_NORMAL
- en: Un altro modo di pensare a questo problema di ottimizzazione è quello di paragonare
    l'attività di prompt del modello alla programmazione. Invece di scrivere il codice
    da solo, stai effettivamente "codificando" un modello in modo che sia un componente
    ben integrato di un'applicazione più grande o di una pipeline di dati. Puoi adottare
    approcci di sviluppo test-driven (TDD) e perfezionare i prompt fino a quando i
    test non passano. Oppure, sperimentare diversi modelli per vedere quale *allinea*
    meglio i suoi output alle tue intenzioni.
  prefs: []
  type: TYPE_NORMAL
- en: Nota
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: La massimizzazione dell'*allineamento* dei modelli rimane un obiettivo prioritario
    per molti fornitori di modelli, in modo che i loro risultati soddisfino al meglio
    l'intento dell'utente.
  prefs: []
  type: TYPE_NORMAL
- en: Modelli di prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Se le istruzioni del sistema non sono metodiche, chiare e non seguono le migliori
    pratiche di prompt, potresti lasciare sul tavolo potenziali ottimizzazioni della
    qualità e delle prestazioni.
  prefs: []
  type: TYPE_NORMAL
- en: 'Come minimo, dovresti avere dei prompt chiari che forniscano compiti specifici
    al modello. La pratica migliore è quella di seguire un modello sistematico.Per
    esempio, redigi le istruzioni del modello seguendo il modello di *ruolo, contesto
    e compito* (RCT):'
  prefs: []
  type: TYPE_NORMAL
- en: Ruolo
  prefs: []
  type: TYPE_NORMAL
- en: La ricerca ha dimostrato che specificare i ruoli per i LLMs tende a influenzare
    in modo significativo i loro risultati. Ad esempio, un modello potrebbe essere
    più indulgente nella valutazione di un saggio se gli dai il ruolo di un insegnante
    di scuola elementare. Senza un ruolo specifico, il modello potrebbe presumere
    che tu voglia che la valutazione segua gli standard accademici di livello universitario.
  prefs: []
  type: TYPE_NORMAL
- en: Nota
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Puoi ampliare ulteriormente il ruolo del modello e descrivere in modo dettagliato
    una *persona* che il modello dovrà adottare. Utilizzando una persona, il modello
    saprà esattamente come comportarsi e fare previsioni perché avrà un contesto più
    ampio su ciò che il ruolo dovrebbe comportare.
  prefs: []
  type: TYPE_NORMAL
- en: Contesto
  prefs: []
  type: TYPE_NORMAL
- en: Definisce lo scenario, dipinge il quadro e fornisce tutte le informazioni rilevanti
    e utili che il modello può utilizzare come riferimento per fare previsioni. Senza
    un contesto esplicito, il modello può utilizzare solo un contesto implicito che
    conterrà informazioni medie dei suoi dati di addestramento. In un'applicazione
    RAG, il contesto potrebbe essere la concatenazione del prompt del sistema con
    i pezzi di documenti recuperati da un archivio di conoscenza.
  prefs: []
  type: TYPE_NORMAL
- en: Compito
  prefs: []
  type: TYPE_NORMAL
- en: Quando descrivi il compito, assicurati di pensare al modello come a un apprendista
    brillante e preparato, pronto a entrare in azione ma che ha bisogno di istruzioni
    molto chiare e non ambigue da seguire, potenzialmente con una manciata di esempi.
  prefs: []
  type: TYPE_NORMAL
- en: Seguendo il modello di sistema sopra descritto, dovresti migliorare la qualità
    dei risultati del tuo modello con il minimo sforzo.
  prefs: []
  type: TYPE_NORMAL
- en: Tecniche di prompt avanzate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Oltre ai principi fondamentali del prompt, puoi utilizzare tecniche più avanzate
    che potrebbero adattarsi meglio al tuo caso d''uso. In base a una [recente indagine
    sistematica sulle tecniche di prompt](https://oreil.ly/xynPC), puoi raggruppare
    i prompt di LLM nei seguenti tipi:'
  prefs: []
  type: TYPE_NORMAL
- en: Apprendimento in contesto
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generazione di pensieri
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decomposizione
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assemblaggio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autocritica
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agente
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Esaminiamo ciascuna di esse in modo più dettagliato.
  prefs: []
  type: TYPE_NORMAL
- en: Apprendimento in contesto
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ciò che distingue i modelli fondamentali come gli LLMs dai tradizionali modelli
    di apprendimento automatico è la loro capacità di rispondere a input dinamici
    senza la necessità costante di una messa a punto o di una riqualificazione.
  prefs: []
  type: TYPE_NORMAL
- en: Quando fornisci delle istruzioni di sistema a un LLM, puoi fornire anche diversi
    esempi (ad esempio, delle inquadrature) per guidare la generazione dell'output.
  prefs: []
  type: TYPE_NORMAL
- en: Il[*prompt a zero colpi*](https://oreil.ly/3F4wb) si riferisce a un approccio
    di prompt che non specifica esempi di riferimento, ma il modello può comunque
    completare con successo il compito dato.Se il modello ha difficoltà senza esempi
    di riferimento, potresti dover usare il [*prompt a pochi colpi*](https://oreil.ly/pOSj8),
    in cui fornisci una manciata di esempi.Ci sono anche casi d'uso in cui vuoi usare
    il prompt *dinamico a pochi colpi*, in cui inserisci dinamicamente gli esempi
    dai dati recuperati da un database o da un archivio vettoriale.
  prefs: []
  type: TYPE_NORMAL
- en: Gli approcci basati sul prompt, in cui si specificano gli esempi, sono definiti
    anche *apprendimento in contesto*. In effetti, stai mettendo a punto i risultati
    del modello in base ai tuoi esempi e al compito dato senza modificare effettivamente
    i pesi/parametri del modello, mentre altri modelli di ML richiederebbero la modifica
    dei loro pesi.
  prefs: []
  type: TYPE_NORMAL
- en: Questo è ciò che rende gli LLMs e i modelli fondazionali così potenti, in quanto
    non richiedono sempre una regolazione del peso per adattarsi ai dati e ai compiti
    che gli vengono assegnati. Puoi conoscere diverse tecniche di apprendimento in
    contesto facendo riferimento alla [Tabella 10-3](#prompting_techniques_incontext_learning).
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 10-3\. Tecniche di prompt per l'apprendimento contestuale
  prefs: []
  type: TYPE_NORMAL
- en: '| Tecnica del prompt | Esempi | Casi d''uso |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Colpo zero | Riassumi i seguenti punti... | Riassunto, domande e risposte
    senza esempi di formazione specifici |'
  prefs: []
  type: TYPE_TB
- en: '| Pochi colpi | Classifica i documenti in base agli esempi riportati di seguito:[Esempi]
    | Classificazione del testo, analisi del sentiment, estrazione dei dati con alcuni
    esempi |'
  prefs: []
  type: TYPE_TB
- en: '| Scatti dinamici di pochi secondi | Classifica i seguenti documenti in base
    agli esempi riportati di seguito:<Inietta esempi da un archivio di vettori basato
    su una query>. | Risposte personalizzate, risoluzione di problemi complessi |'
  prefs: []
  type: TYPE_TB
- en: I prompt di apprendimento contestuali sono semplici, efficaci e rappresentano
    un ottimo punto di partenza per completare una serie di compiti. Per i compiti
    più complessi, puoi utilizzare approcci di prompt più avanzati come la generazione
    del pensiero, la scomposizione, l'assemblaggio, l'autocritica o gli approcci agici.
  prefs: []
  type: TYPE_NORMAL
- en: Generazione di pensieri
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Le tecniche di generazione del pensiero, come la [catena del pensiero (CoT),](https://oreil.ly/BWUYQ)
    hanno dimostrato di migliorare significativamente la capacità delle LLMs di eseguire
    ragionamenti complessi.
  prefs: []
  type: TYPE_NORMAL
- en: Nel prompt COT si chiede al modello di spiegare il suo processo di pensiero
    e il suo ragionamento mentre fornisce una risposta. Le varianti della CoT includono
    la [CoT](https://oreil.ly/1gjSH) a zero o a [pochi colpi](https://oreil.ly/1gjSH),
    a seconda che si forniscano o meno degli esempi. Una tecnica di generazione del
    pensiero più avanzata è il [thread of thought (ThoT)](https://oreil.ly/1KyO4)
    che segmenta e analizza sistematicamente informazioni o compiti caotici e molto
    complessi.
  prefs: []
  type: TYPE_NORMAL
- en: La[Tabella 10-4](#prompting_techniques_thought_generation) elenca le tecniche
    di generazione del pensiero.
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 10-4\. Tecniche di prompt per la generazione del pensiero
  prefs: []
  type: TYPE_NORMAL
- en: '| Tecnica del prompt | Esempi | Casi d''uso |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Catena di pensiero a colpo zero (CoT) | Pensiamo passo dopo passo... | Risoluzione
    di problemi matematici, ragionamento logico e processo decisionale in più fasi.
    |'
  prefs: []
  type: TYPE_TB
- en: '| CoT a pochi colpi | Pensiamo passo dopo passo... Ecco alcuni esempi:[ESEMPI]
    | Scenari in cui alcuni esempi possono guidare il modello verso prestazioni migliori,
    come la classificazione di testi sfumati, la risposta a domande complesse e i
    prompt di scrittura creativa. |'
  prefs: []
  type: TYPE_TB
- en: '| Filo del pensiero (ThoT) | Esamina il problema in parti gestibili, passo
    dopo passo, riassumendo e analizzando man mano... | Mantenere il contesto nel
    corso di interazioni multiple, come i sistemi di dialogo, la narrazione interattiva
    e la generazione di contenuti di lunga durata. |'
  prefs: []
  type: TYPE_TB
- en: Decomposizione
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Le tecniche di prompt a scomposizione si concentrano sulla suddivisione di compiti
    complessi in sottocompiti più piccoli, in modo che il modello possa affrontarli
    passo dopo passo e in modo logico. Puoi sperimentare questi approcci insieme alla
    generazione del pensiero per identificare quelli che producono i risultati migliori
    per il tuo caso d'uso.
  prefs: []
  type: TYPE_NORMAL
- en: 'Queste sono le tecniche di prompt di scomposizione più comuni:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Da meno a più](https://oreil.ly/HmsSN)'
  prefs: []
  type: TYPE_NORMAL
- en: Chiedi al modello di suddividere un problema complesso in problemi più piccoli
    tramite una riduzione logica, senza risolverli. Puoi poi chiedere al modello di
    risolvere ogni compito uno per uno.
  prefs: []
  type: TYPE_NORMAL
- en: '[Pianifica e risolvi](https://oreil.ly/aWTzf)'
  prefs: []
  type: TYPE_NORMAL
- en: Dato un compito, chiedi che venga elaborato un piano e poi chiedi al modello
    di risolverlo.
  prefs: []
  type: TYPE_NORMAL
- en: '[L''albero dei pensieri (ToT)](https://oreil.ly/IZdj1)'
  prefs: []
  type: TYPE_NORMAL
- en: Crea un problema di ricerca ad albero in cui un compito è suddiviso in più rami
    di passi come un albero. Poi, richiama il modello per valutare e risolvere ogni
    ramo di passi.
  prefs: []
  type: TYPE_NORMAL
- en: '[La Tabella 10-5](#prompting_techniques_decomposition) mostra queste tecniche
    di scomposizione.'
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 10-5\. Tecniche di prompt della decomposizione
  prefs: []
  type: TYPE_NORMAL
- en: '| Tecnica del prompt | Esempi | Casi d''uso |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Da meno a più | Suddividi il compito di... in compiti più piccoli. | Risoluzione
    di problemi complessi, gestione di progetti, decomposizione dei compiti |'
  prefs: []
  type: TYPE_TB
- en: '| Pianifica e risolvi | Ideare un piano per... | Sviluppo di algoritmi, progettazione
    di software, pianificazione strategica |'
  prefs: []
  type: TYPE_TB
- en: '| L''albero dei pensieri (ToT) | Crea un albero decisionale per la scelta di
    un... | Processo decisionale, risoluzione di problemi con soluzioni multiple,
    pianificazione strategica con alternative. |'
  prefs: []
  type: TYPE_TB
- en: Assemblaggio
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: L*'assemblaggio* è il processo che prevede l'utilizzo di più prompt per risolvere
    lo stesso problema e l'aggregazione delle risposte in un output finale. Puoi generare
    queste risposte utilizzando lo stesso modello o modelli diversi.
  prefs: []
  type: TYPE_NORMAL
- en: L'idea principale alla base dell'ensembling è quella di ridurre la varianza
    dei risultati di LLM migliorando l'accuratezza in cambio di costi di utilizzo
    più elevati.
  prefs: []
  type: TYPE_NORMAL
- en: 'Le tecniche di prompt dell''ensemble più conosciute sono le seguenti:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Autoconsistenza](https://oreil.ly/_85WS)'
  prefs: []
  type: TYPE_NORMAL
- en: Genera più percorsi di ragionamento e seleziona l'output più coerente come risultato
    finale utilizzando una votazione a maggioranza.
  prefs: []
  type: TYPE_NORMAL
- en: '[Miscela di esperti di ragionamento (MoRE)](https://oreil.ly/xllKs)'
  prefs: []
  type: TYPE_NORMAL
- en: Combina i risultati di più LLMs con prompt specializzati per migliorare la qualità
    delle risposte. Ogni LLMs agisce come un esperto di un'area focalizzata su diversi
    compiti di ragionamento come ragionamento fattuale, riduzione logica, controlli
    di buon senso, ecc.
  prefs: []
  type: TYPE_NORMAL
- en: '[Dimostrazione di assemblaggio (DENSE)](https://oreil.ly/lPEPz)'
  prefs: []
  type: TYPE_NORMAL
- en: Crea prompt multipli di pochi secondi a partire dai dati, quindi genera un output
    finale aggregando le risposte.
  prefs: []
  type: TYPE_NORMAL
- en: '[Parafrasi prompt](https://oreil.ly/yP_ka)'
  prefs: []
  type: TYPE_NORMAL
- en: Formula il prompt originale in più varianti attraverso la formulazione.
  prefs: []
  type: TYPE_NORMAL
- en: '[La Tabella 10-6](#prompting_techniques_ensemling) mostra esempi e casi d''uso
    di queste tecniche di assemblaggio.'
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 10-6\. Tecniche di prompt di gruppo
  prefs: []
  type: TYPE_NORMAL
- en: '| Tecnica del prompt | Esempi | Casi d''uso |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Autoconsistenza | prompt #1 (da eseguire più volte): Pensiamo passo dopo
    passo e completiamo la seguente attività...prompt n. 2: Tra le seguenti risposte,
    scegli quella migliore/comune assegnando un punteggio utilizzando... | Ridurre
    gli errori o le distorsioni nei compiti di aritmetica, di senso comune e di ragionamento
    simbolico. |'
  prefs: []
  type: TYPE_TB
- en: '| Miscela di esperti di ragionamento (MoRE) | prompt n. 1 (da eseguire per
    ogni esperto): Sei un recensore di ..., dai un punteggio a quanto segue in base
    a...prompt n. 2: scegli la migliore risposta dell''esperto in base al punteggio
    dell''accordo... | Contabilità per aree o domini di conoscenza specializzati |'
  prefs: []
  type: TYPE_TB
- en: '| Dimostrazione di assemblaggio (DENSE) | Crea diversi esempi di traduzione
    di questo testo e aggrega le risposte migliori.Genera diversi prompt per riassumere
    l''articolo e combina i risultati. |'
  prefs: []
  type: TYPE_TB
- en: Migliorare l'affidabilità della produzione
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregare prospettive diverse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Parafrasi prompt | Prompt #1a: Riformula questa proposta...prompt #1b: Chiarisci
    questa proposta...prompt #1c: Fai una modifica a questa proposta...prompt n. 2:
    Scegli la proposta migliore tra le seguenti risposte in base a... |'
  prefs: []
  type: TYPE_TB
- en: Esplorare diverse interpretazioni
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aumento dei dati per l'ensembling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Autocritica
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Le tecniche di*autocritica* si concentrano sull'utilizzo dei modelli come giudici,
    valutatori o revisori dell'intelligenza artificiale, sia per eseguire autocontrolli
    che per valutare i risultati di altri modelli. La critica o il feedback del primo
    prompt possono essere utilizzati per migliorare la qualità delle risposte nei
    prompt successivi.
  prefs: []
  type: TYPE_NORMAL
- en: 'Queste sono diverse strategie di prompt dell''autocritica:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Autocalibrazione](https://oreil.ly/_4YEr)'
  prefs: []
  type: TYPE_NORMAL
- en: Chiedi al LLM di valutare la correttezza di una risposta rispetto a una domanda/risposta.
  prefs: []
  type: TYPE_NORMAL
- en: '[Auto-raffinare](https://oreil.ly/bTQJI)'
  prefs: []
  type: TYPE_NORMAL
- en: Affina le risposte in modo iterativo attraverso l'autocontrollo e la fornitura
    di feedback.
  prefs: []
  type: TYPE_NORMAL
- en: '[Inversione della catena del pensiero (RCoT)](https://oreil.ly/6ojtr)'
  prefs: []
  type: TYPE_NORMAL
- en: Ricostruire il problema a partire da una risposta generata e poi generare confronti
    a grana fine tra il problema originale e quello ricostruito per identificare le
    incongruenze.
  prefs: []
  type: TYPE_NORMAL
- en: '[Autoverifica](https://oreil.ly/Fz3JH)'
  prefs: []
  type: TYPE_NORMAL
- en: Genera potenziali soluzioni con la tecnica CoT e poi assegna un punteggio a
    ciascuna di esse mascherando parti della domanda e fornendo ogni risposta.
  prefs: []
  type: TYPE_NORMAL
- en: '[Catena di verifica (COVE)](https://oreil.ly/WrrLP)'
  prefs: []
  type: TYPE_NORMAL
- en: Crea un elenco di query/domande correlate per verificare la correttezza di una
    risposta.
  prefs: []
  type: TYPE_NORMAL
- en: '[Ragionamento cumulativo](https://oreil.ly/3Hb-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Genera potenziali passi per rispondere a una query e poi chiedi al modello di
    accettare/rifiutare ogni passo. Infine, controlla se è arrivato alla risposta
    finale per terminare il processo; altrimenti, ripeti il processo.
  prefs: []
  type: TYPE_NORMAL
- en: Puoi vedere degli esempi di ciascuna tecnica di prompt dell'autocritica nella
    [Tabella 10-7](#prompting_techniques_self_criticism).
  prefs: []
  type: TYPE_NORMAL
- en: Tabella 10-7\. Tecniche di prompt per l'autocritica
  prefs: []
  type: TYPE_NORMAL
- en: '| Tecnica del prompt | Esempi | Casi d''uso |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Autocalibrazione | Valuta la correttezza della seguente risposta: [risposta]
    per la seguente domanda: [domanda] | Valuta la fiducia nelle risposte per accettare
    o rivedere la risposta originale. |'
  prefs: []
  type: TYPE_TB
- en: '| Auto-raffinare | prompt n. 1: Qual è la tua opinione sulla risposta...prompt
    n. 2: Utilizzando il feedback [Feedback], perfeziona la tua risposta su... | Attività
    di ragionamento, codifica e generazione. |'
  prefs: []
  type: TYPE_TB
- en: '| Inversione della catena del pensiero (RCoT) | prompt #1: Ricostruisci il
    problema a partire da questa risposta...prompt n. 2: Genera un confronto a grana
    fine tra queste query... | Identificare le incongruenze e rivedere le risposte.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Autoverifica | prompt #1 (da eseguire più volte): Pensiamo passo dopo passo
    - genera una soluzione per il seguente problema...prompt n. 2: assegna un punteggio
    a ciascuna soluzione in base al [problema mascherato]... | Migliorare i compiti
    di ragionamento. |'
  prefs: []
  type: TYPE_TB
- en: '| Catena di verifica (COVE) | prompt n. 1: rispondi alla seguente domanda...prompt
    #2: formula delle domande correlate per verificare questa risposta: ...prompt
    #3 (da eseguire per ogni nuova domanda correlata): Rispondi alla seguente domanda:
    ...prompt n. 4: in base alle seguenti informazioni, scegli la risposta migliore...
    | Attività di risposta alle domande e di generazione di testi. |'
  prefs: []
  type: TYPE_TB
- en: '| Ragionamento cumulativo | prompt n. 1: delinea i passaggi per rispondere
    alla query: ...Prompt #2: Controlla il seguente piano e accetta/rifiuta i passaggi
    rilevanti per rispondere alla query: ...prompt #3: verifica di essere arrivato
    alla risposta finale con le seguenti informazioni... | Convalida passo dopo passo
    di query complesse, inferenze logiche e problemi matematici. |'
  prefs: []
  type: TYPE_TB
- en: Agente
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Puoi fare un ulteriore passo avanti rispetto alle tecniche di prompt discusse
    finora e aggiungere l'accesso a strumenti esterni con algoritmi di valutazione
    complessi. Questo processo specializza gli LLMs come *agenti*, consentendo loro
    di fare piani, intraprendere azioni e utilizzaresistemi esterni.
  prefs: []
  type: TYPE_NORMAL
- en: I prompt o le *sequenze di prompt (catene)* guidano i sistemi agenziali con
    un focus ingegneristico sulla creazione di comportamenti simili a quelli degli
    agenti da LLMs. Questi flussi di lavoro agenziali servono gli utenti eseguendo
    azioni sui sistemi che si interfacciano con i modelli GenAI, che sono per lo più
    LLMs. Gli strumenti, siano essi *simbolici* come una calcolatrice o *neurali*
    come un altro modello di IA, costituiscono una componente fondamentale dei sistemi
    agenziali.
  prefs: []
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Se crei una pipeline di più chiamate al modello con un output inoltrato allo
    stesso modello o a modelli diversi come input, hai costruito una *catena di prompt*.
    In linea di principio, quando sfrutti le catene di prompt stai utilizzando la
    tecnica di prompt della CoT.
  prefs: []
  type: TYPE_NORMAL
- en: 'Alcune tecniche di prompt agenziale includono:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Ragionamento, conoscenza e linguaggio modulari (MRKL)](https://oreil.ly/aWeQu)'
  prefs: []
  type: TYPE_NORMAL
- en: Il sistema agenziale più semplice consiste in un LLM che utilizza diversi strumenti
    per ottenere e combinare le informazioni per generare una risposta.
  prefs: []
  type: TYPE_NORMAL
- en: '[Autocorrezione con critica interattiva dello strumento (CRITIC)](https://oreil.ly/M-9YL)'
  prefs: []
  type: TYPE_NORMAL
- en: Risponde alle query e poi auto-verifica la risposta senza utilizzare strumenti
    esterni. Infine, utilizza strumenti per verificare o modificare le risposte.
  prefs: []
  type: TYPE_NORMAL
- en: '[Modello linguistico assistito da programma (PAL)](https://oreil.ly/0WtKv)'
  prefs: []
  type: TYPE_NORMAL
- en: Genera codice dalle query e lo invia direttamente a interpreti di codice come
    Python per generare una risposta.^([4](ch10.html#id1128))
  prefs: []
  type: TYPE_NORMAL
- en: '[Agente di ragionamento integrato negli strumenti (ToRA)](https://oreil.ly/pbfv_)'
  prefs: []
  type: TYPE_NORMAL
- en: L'AMICO compie un ulteriore passo in avanti, intercalando le fasi di generazione
    del codice e di ragionamento per tutto il tempo necessario a fornire una risposta
    soddisfacente.
  prefs: []
  type: TYPE_NORMAL
- en: '[Ragionare e agire (React)](https://oreil.ly/aDubr)'
  prefs: []
  type: TYPE_NORMAL
- en: Dato un problema, genera pensieri, compie azioni, riceve osservazioni e ripete
    il ciclo con le informazioni precedenti (cioè la memoria) finché il problema non
    viene risolto.
  prefs: []
  type: TYPE_NORMAL
- en: Se vuoi permettere ai tuoi LLMs di utilizzare degli strumenti, puoi sfruttare
    le funzioni *di chiamata di funzione* dei fornitori di modelli, come nell'[Esempio
    10-17](#function_calling).
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 10-17\. Chiamata di funzione per il fetching
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Come hai visto nell'[Esempio 10-17](#function_calling), puoi creare sistemi
    agenziali configurando LLMs specializzati che hanno accesso a strumenti personalizzati
    e alle funzioni di .
  prefs: []
  type: TYPE_NORMAL
- en: Messa a punto
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ci sono casi in cui l'ingegneria del prompt da sola non è in grado di fornire
    la qualità di risposta che stai cercando. La messa a punto è una tecnica di ottimizzazione
    che richiede di regolare i parametri del tuo modello GenAI per adattarlo meglio
    ai tuoi dati. Ad esempio, puoi mettere a punto un modello linguistico per imparare
    il contenuto di basi di conoscenza private o per rispondere sempre con un certo
    tono seguendo le linee guida del tuo marchio.
  prefs: []
  type: TYPE_NORMAL
- en: Spesso non è la prima tecnica da provare perché richiede uno sforzo per raccogliere
    e preparare i dati, oltre che per addestrare e valutare i modelli.
  prefs: []
  type: TYPE_NORMAL
- en: Quando dovresti prendere in considerazione la messa a punto?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Potresti prendere in considerazione la possibilità di perfezionare i modelli
    GenAI pre-addestrati se si verifica uno dei seguenti scenari:'
  prefs: []
  type: TYPE_NORMAL
- en: Hai dei costi significativi per l'utilizzo dei token, ad esempio perché devi
    richiedere istruzioni di sistema dettagliate o fornire molti esempi in ogni prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Il tuo caso d'uso si basa su competenze specialistiche del dominio che il modello
    deve imparare.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: È necessario ridurre il numero di allucinazioni nelle risposte con un modello
    conservativo più preciso.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hai bisogno di risposte di qualità superiore e di dati sufficienti per la messa
    a punto.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hai bisogno di una latenza minore nelle risposte.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Una volta messo a punto il modello, non sarà più necessario fornire tanti esempi
    nel prompt, risparmiando sui costi e consentendo richieste a bassa latenza.
  prefs: []
  type: TYPE_NORMAL
- en: Avvertenze
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Evita il più possibile la messa a punto.
  prefs: []
  type: TYPE_NORMAL
- en: L'iterazione sui prompt ha un ciclo di feedback molto più rapido rispetto all'iterazione
    sulla messa a punto, che si basa sulla creazione di set di dati e sull'esecuzione
    di lavori di formazione.
  prefs: []
  type: TYPE_NORMAL
- en: Tuttavia, se alla fine avrai bisogno di una messa a punto, noterai che gli sforzi
    iniziali di prompt engineering contribuiranno a produrre dati di formazione di
    qualità superiore.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ecco alcuni casi in cui la messa a punto può essere utile:'
  prefs: []
  type: TYPE_NORMAL
- en: Insegnare a un modello a rispondere a uno stile, a un tono, a un formato o a
    un'altra metrica qualitativa, ad esempio per produrre rapporti standardizzati
    conformi ai requisiti normativi e ai protocolli interni.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Migliorare l'affidabilità della produzione degli output desiderati, come ad
    esempio avere sempre risposte conformi a un determinato output strutturato.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ottenere risultati corretti per query complesse come la classificazione di documenti
    e l'etichettatura di centinaia di classi.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Esecuzione di compiti specializzati in un dominio specifico, come la classificazione
    di articoli o l'interpretazione e l'aggregazione di dati specifici per il settore.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gestione più accurata dei casi limite
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eseguire abilità o compiti difficili da articolare nei prompt, come l'estrazione
    di date da testi non strutturati.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ridurre i costi utilizzando `gpt-40-mini` o addirittura `gpt-3.5-turbo` al posto
    di `gpt-4o`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insegnare a un modello a utilizzare strumenti e API complesse quando si usa
    la chiamata di funzione
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Come mettere a punto un modello preaddestrato
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Per qualsiasi lavoro di messa a punto, dovrai seguire questi passaggi:'
  prefs: []
  type: TYPE_NORMAL
- en: Preparare e caricare i dati della formazione.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Invia un lavoro di formazione per la messa a punto.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Valutare e utilizzare il modello perfezionato.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A seconda del modello che stai utilizzando, i dati devono essere preparati in
    base alle istruzioni del fornitore del modello.
  prefs: []
  type: TYPE_NORMAL
- en: Ad esempio, per mettere a punto un tipico modello di chat come `gpt-4o-2024-08-06`,
    devi preparare i tuoi dati in un formato di messaggio, come mostrato nell'[Esempio
    10-18](#fine_tune_prepare). Al momento in cui scriviamo, il [prezzo dell'API OpenAI](https://oreil.ly/MmCNq)
    per la messa a punto di questo modello è di $25/1M di token di addestramento.
  prefs: []
  type: TYPE_NORMAL
- en: Esempio 10-18\. Esempio di dati di allenamento per un lavoro di messa a punto
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22][PRE23][PRE24] `"messages"``:` `[` [PRE25][PRE26]`` `{` [PRE27][PRE28]`
    `"role"``:` `"system"``,` [PRE29][PRE30] `"content"``:` `"<text>"` [PRE31][PRE32]``py[PRE33]`py`
    `"role"``:` `"user"``,` [PRE34] `},` [PRE35]`` `{` [PRE36]` `"role"``:` `"assistant"``,`
    [PRE37] `"content"``:` `"<text>"` [PRE38]`py [PRE39]py`` [PRE40]py[PRE41][PRE42][PRE43][PRE44]py[PRE45]py`
    [PRE46]`py`` [PRE47]`py[PRE48][PRE49][PRE50]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51][PRE52]`` [PRE53] from openai import OpenAI client = OpenAI()  response
    = client.files.create(     file=open("mydata.jsonl", "rb"), purpose="fine-tune"
    )  client.fine_tuning.jobs.create(     training_file=response.id, model="gpt-4o-mini-2024-07-18"
    ) [PRE54] from openai import OpenAI client = OpenAI()  fine_tuning_job_id = "ftjob-abc123"
    response = client.fine_tuning.jobs.retrieve(fine_tuning_job_id) fine_tuned_model
    = response.fine_tuned_model  if fine_tuned_model is None:     raise ValueError(         f"Failed
    to retrieve the fine-tuned model - "         f"Job ID: {fine_tuning_job_id}"     )  completion
    = client.chat.completions.create(     model=fine_tuned_model,     messages=[         {"role":
    "system", "content": "You are a helpful assistant."},         {"role": "user",
    "content": "Hello!"},     ], ) print(completion.choices[0].message) [PRE55]` [PRE56][PRE57]
    `` `# Sommario    In questo capitolo hai appreso diverse strategie di ottimizzazione
    per migliorare il throughput e la qualità dei tuoi servizi. Alcune ottimizzazioni
    che hai aggiunto hanno riguardato la cache (parole chiave, semantica, contesto),
    l''ingegneria del prompt, la quantizzazione del modello e la messa a punto.    Nel
    prossimo capitolo ci concentreremo sull''ultima fase della costruzione di servizi
    di AI: la distribuzione della tua soluzione GenAI, che comprende l''esplorazione
    dei modelli di distribuzione per i servizi di AI e la containerizzazione con Docker.    ^([1](ch10.html#id1069-marker))
    Vedi l''API OpenAI Batch disponibile nella [documentazione](https://oreil.ly/0t59w)
    dell''[API OpenAI](https://oreil.ly/0t59w).    ^([2](ch10.html#id1072-marker))
    Per saperne di più sulle intestazioni di controllo della cache, visita il [sito
    MDN](https://oreil.ly/-Y5JP).    ^([3](ch10.html#id1076-marker)) Per ottenere
    un risparmio significativo sui costi, potrebbe essere necessario un modello embedder
    addestrato, in quanto le frequenti chiamate API a un modello embedder off-the-shelf
    potrebbero comportare costi aggiuntivi, riducendo il risparmio complessivo.    ^([4](ch10.html#id1128-marker))
    Per una maggiore sicurezza, devi comunque sanificare il codice generato da LLM
    prima di inoltrarlo ai sistemi a valle per l''esecuzione.` `` ```'
  prefs: []
  type: TYPE_NORMAL
