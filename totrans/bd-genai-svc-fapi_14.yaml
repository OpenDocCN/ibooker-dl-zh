- en: Chapter 10\. Optimizing AI Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn to further optimize your services via prompt engineering,
    model quantization, and caching mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objectives of optimizing an AI service are to either improve output quality
    or performance (latency, throughput, costs, etc.).
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance-related optimizations include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Using batch processing APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caching (keyword, semantic, context, or prompt)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model quantization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Quality-related optimizations include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Using structured outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prompt engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model fine-tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s review each in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Batch Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often you want an LLM to process batches of entries at the same time. The most
    obvious solution is to submit multiple API calls per entry. However, the obvious
    approach can be costly and slow and may lead to your model provider rate limiting
    you.
  prefs: []
  type: TYPE_NORMAL
- en: 'In such cases, you can leverage two separate techniques for batch processing
    your data through an LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: Updating your structured output schemas to return multiple examples at the same
    time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying and using model provider APIs that are designed for batch processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first solution requires you to update your Pydantic models or template prompts
    to request a list of outputs per request. In this case, you can batch process
    your data within a handful of requests instead of one per entry.
  prefs: []
  type: TYPE_NORMAL
- en: An implementation of the first solution is shown in [Example 10-1](#batch_processing_structured_outputs).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-1\. Updating structured output schema for parsing multiple items
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Update the Pydantic model to include a list of `Category` models.
  prefs: []
  type: TYPE_NORMAL
- en: You can now pass the new schema alongside a list of document titles to the OpenAI
    client to process multiple entries in a single API call. However, an alternative
    and possibly the best solution will be to use a batch API, if available.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, model providers such as OpenAI already supply relevant APIs for such
    use cases. Under the hood, these providers may run task queues to process any
    single batch job in the background while providing you with status updates until
    the batch is complete to retrieve the results.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to using standard endpoints directly, you’ll be able to send asynchronous
    groups of requests with lower costs (up to 50% with OpenAI^([1](ch10.html#id1069))),
    enjoy higher rate limits, and guarantee completion times. The batch job service
    is ideal for processing jobs that don’t require immediate responses such as using
    OpenAI LLMs to parse, classify, or translate large volumes of documents in the
    background.
  prefs: []
  type: TYPE_NORMAL
- en: To submit a batch job, you’ll need a `jsonl` file where each line contains the
    details of an individual request to the API, as shown in [Example 10-2](#jsonl).
    Also as seen in this example, to create the JSONL file, you can iterate over your
    entries and dynamically generate the file.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-2\. Creating a JSONL file from entries
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once created, you can submit the file to the batch API for processing, as shown
    in [Example 10-3](#batch_processing_api).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-3\. Processing batch jobs with the OpenAI Batch API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can now leverage offline batch endpoints to process multiple entries in
    one go with guaranteed turnaround times and significant cost savings.
  prefs: []
  type: TYPE_NORMAL
- en: Alongside leveraging structured outputs and batch APIs to optimize your services,
    you can also leverage caching techniques to significantly speed up response times
    and resource costs of your servers.
  prefs: []
  type: TYPE_NORMAL
- en: Caching
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In GenAI services, you’ll often rely on data/model response that require significant
    computations or long processing durations. If you have multiple users requesting
    the same data, repeating the same operations can be wasteful. Instead, you can
    use caching techniques for storing and retrieving frequently accessed data to
    help you optimize your services by speeding up response times, reducing server
    load, and saving bandwidth and operational costs.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in a public FAQ chatbot where users ask mostly the same questions,
    you may want to reuse the cached responses for longer periods. On the other hand,
    for more personalized and dynamic chatbots, you can frequently refresh (i.e.,
    invalidate) the cached response.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You should always consider the frequency of cache refreshes based on the nature
    of the data and the acceptable level of staleness.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most relevant caching strategies for GenAI services include:'
  prefs: []
  type: TYPE_NORMAL
- en: Keyword caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Context or prompt caching
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s review each in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Keyword caching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If all you need is a simple caching mechanism for storing functions or endpoint
    responses, you can use *keyword caching*, which involves caching responses based
    on exact matches of input queries as key-value pairs.
  prefs: []
  type: TYPE_NORMAL
- en: In FastAPI, libraries such as `fastapi-cache` can help you implement keyword
    caching in a few lines of code, on any functions or endpoints. FastAPI caches
    also give you the option to attach storage backends such as Redis for centralizing
    the cache store across your instances.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Alternatively, you can implement your own custom caching mechanism with a cache
    store using lower-level packages such as `cachetools`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, all you have to do is to initialize and configure the caching
    system as part of the application lifespan, as shown in [Example 10-4](#caching_lifespan).
    You can install FastAPI cache using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Example 10-4\. Configuring FastAPI cache lifespan
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize `FastAPICache` with a `RedisBackend` that doesn’t decode responses
    so that cached data is stored as bytes (binary). This is because decoding responses
    would break caching by altering the original response format.
  prefs: []
  type: TYPE_NORMAL
- en: Once the caching system is configured, you can decorate your functions or endpoint
    handlers to cache their outputs, as shown in [Example 10-5](#caching_functions_endpoint).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-5\. Function and endpoint results caching
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The `cache()` decorator must always come last. Invalidate the cache in 60 seconds
    by setting `expires=60` to recompute the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: The `cache()` decorator shown in [Example 10-5](#caching_functions_endpoint)
    injects dependencies for the `Request` and `Response` objects so that it can add
    cache control headers to the outgoing response. These cache control headers instruct
    clients how to cache the responses on their side by specifying a set of directives
    (i.e., instructions).
  prefs: []
  type: TYPE_NORMAL
- en: 'These are a few common cache control directives when sending responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max-age`'
  prefs: []
  type: TYPE_NORMAL
- en: Defines the maximum amount of time (in seconds) that a response is considered
    fresh
  prefs: []
  type: TYPE_NORMAL
- en: '`no-cache`'
  prefs: []
  type: TYPE_NORMAL
- en: Forces revalidation so that the clients check for constant updates with the
    server
  prefs: []
  type: TYPE_NORMAL
- en: '`no-store`'
  prefs: []
  type: TYPE_NORMAL
- en: Prevents caching entirely
  prefs: []
  type: TYPE_NORMAL
- en: '`private`'
  prefs: []
  type: TYPE_NORMAL
- en: Stores responses in a private cache (e.g., local caches in browsers)
  prefs: []
  type: TYPE_NORMAL
- en: 'A response could have cache control headers like `Cache-Control: max-age=180,
    private` to set these directives.^([2](ch10.html#id1072))'
  prefs: []
  type: TYPE_NORMAL
- en: Since keyword caching works on exact matches, it’s more suitable for functions
    and APIs that expect frequently repeated matching inputs. However, in GenAI services
    that accept variable user queries, you may want to consider other caching mechanisms
    that rely on the meaning of inputs when returning a cached response. This is where
    semantic caching can prove useful.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic caching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Semantic caching* is a caching mechanism that returns a stored value based
    on similar inputs.'
  prefs: []
  type: TYPE_NORMAL
- en: Under the hood, the system uses encoders and embedding vectors to capture semantics
    and meanings of inputs. It then performs similarity searches across stored key-value
    pairs to return a cached response.
  prefs: []
  type: TYPE_NORMAL
- en: 'In comparison to keyword caching, similar inputs can return the same cached
    response. Inputs to the system don’t have to be identical to be recognized as
    similar. Even if such inputs have different sentence structures or formulations
    or contain inaccuracies, they’ll still be captured as similar for carrying the
    same meanings. And, the same response is being requested. As an example, the following
    queries are considered similar for carrying the same intent:'
  prefs: []
  type: TYPE_NORMAL
- en: How do you build generative services with FastAPI?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the process of developing FastAPI services for GenAI?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This caching system contributes to significant cost savings^([3](ch10.html#id1076))
    by reducing API calls to [30–40%](https://oreil.ly/gjGz6) (i.e., 60–70% cache
    hit rate) depending on the use case and size of the user base. For instance, Q&A
    RAG applications that receive frequently asked questions across a large user base
    could reduce API calls by 69% using a semantic cache.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within a typical RAG system, there are two places where having a cache can
    reduce resource-intensive and time-consuming operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Before the LLM* to return a cached response instead of generating a new one'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Before the vector store* to enrich prompts with cached documents instead of
    searching and retrieving fresh ones'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When integrating a semantic cache component into your RAG system, you should
    consider whether returning a cached response could negatively impact your application’s
    user experience. For instance, if caching the LLM responses, both of the following
    queries would return the same response due to their high semantic similarity,
    causing the semantic caching system to treat them as nearly identical:'
  prefs: []
  type: TYPE_NORMAL
- en: Summarize this text in 100 words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarize this text in 50 words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This makes it feel like your services aren’t responding to queries. As you may
    still want varied LLM outputs in your application, we’re going to implement a
    document retrieval semantic cache for your RAG system. [Figure 10-1](#semantic_cache)
    shows the full system architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1001](assets/bgai_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. Semantic caching in RAG system architecture
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Let’s start by implementing the semantic caching system from scratch first,
    and then we’ll review how to offload the functionality to an external library
    such as `gptcache`.
  prefs: []
  type: TYPE_NORMAL
- en: Building a semantic caching service from scratch
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can implement a semantic caching system by implementing the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: A cache store client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A document vector store client
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An embedding model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Example 10-6](#semantic_cache_cache_store) shows how to implement the cache
    store client.'
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-6\. Cache store client
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a Qdrant client running on memory acting as a cache store.
  prefs: []
  type: TYPE_NORMAL
- en: Once the cache store client is initialized, you can configure the document vector
    store by following [Example 10-7](#semantic_cache_doc_store).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-7\. Document store client
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Load a collection of documents into the Qdrant vector store.
  prefs: []
  type: TYPE_NORMAL
- en: With both the cache and document vector store clients ready, you can now implement
    the semantic cache service, as shown in [Example 10-8](#semantic_cache_service),
    with methods to compute embeddings and performing cache searches.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-8\. Semantic caching system
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Set a similarity threshold. Any score above this threshold will be a cache hit.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Query the document store if there is no cache hit. Cache the retrieved documents
    against the vector embedding of the query as the cache key.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: If there is no related document or cache available for the given query, return
    a canned answer.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a semantic caching service, you can use it to retrieve cached
    documents from memory by following [Example 10-9](#semantic_cache_qdrant_usage).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-9\. Implementing a semantic cache in a RAG system with Qdrant
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: You should now have a better understanding of how to implement your own custom
    semantic caching systems using a vector database client.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic caching with GPT cache
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If you don’t need to develop your own semantic caching service from scratch,
    you can also use the modular `gptcache` library that gives you the option to swap
    various storage, caching, and embedding components.
  prefs: []
  type: TYPE_NORMAL
- en: 'To configure a semantic cache with `gptcache`, you first need to install the
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Then load the system on application start, as shown in [Example 10-10](#configrue_gptcache).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-10\. Configuring the GPT cache
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Select a post-processing callback function to select a random item from the
    returned cached items.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Select a pre-embedding callback function to use the last query for setting a
    new cache.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Use the ONNX embedding model for computing embedding vectors.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_optimizing_ai_services_CO7-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Use `OnnxModelEvaluation` to compute similarity scores between cached items
    and a given query.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_optimizing_ai_services_CO7-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Set the caching configuration options such as a similarity threshold.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_optimizing_ai_services_CO7-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Provide an OpenAI client API key for GPT Cache to automatically perform semantic
    caching on LLM API responses.
  prefs: []
  type: TYPE_NORMAL
- en: Once `gptcache` is initialized, it will integrate seamlessly with the OpenAI
    LLM client across your application. You can now make multiple LLM queries, as
    shown in [Example 10-11](#semantic_caching_gptcache), knowing that `gptcache`
    will be caching your LLM responses.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-11\. Semantic caching with the GPT cache
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Using external libraries like `gptcache`, as shown in [Example 10-11](#semantic_caching_gptcache),
    makes implementing semantic caching straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: Once the caching system is up and running, you can adjust *similarity thresholds*
    to tune the system’s cache hit rates.
  prefs: []
  type: TYPE_NORMAL
- en: Similarity threshold
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When building a semantic caching service, you may need to adjust the similarity
    threshold based on provided queries to achieve high cache hit rates that are accurate.
    You can refer to the [interactive visualization of semantic cache clusters](https://semanticcachehit.com)
    shown in [Figure 10-2](#semantic_cache_visualization) to better understand the
    concept of similarity threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the threshold value in [Figure 10-2](#semantic_cache_visualization)
    will result in a less connected graph, while minimizing can produce false positives.
    Therefore, you may want to run a few experiments to fine-tune the similarity threshold
    for your own application.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1002](assets/bgai_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10-2\. Visualization of semantic caching (Source: [semanticcachehit.com](https://semanticcachehit.com))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Eviction policies
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another concept relevant to caching is *eviction policies* that control the
    caching behavior when the caching mechanism reaches its maximum capacity. Selecting
    the appropriate eviction policy should be appropriate for your own use case.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since the size of cache memory stores is often limited, you can add an `evict()`
    method to the `SemanticCachingService` you implemented in [Example 10-8](#semantic_cache_service).
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 10-1](#eviction_policies) shows a few eviction policies you can choose
    from.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-1\. Eviction policies
  prefs: []
  type: TYPE_NORMAL
- en: '| Policy | Description | Use case |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| First in, first out (FIFO) | Removes oldest items | When all items have the
    same priority |'
  prefs: []
  type: TYPE_TB
- en: '| Least recently used (LRU) | Tracks cache usage across time and removes the
    least recently accessed item | When recently accessed items are more likely to
    be accessed again |'
  prefs: []
  type: TYPE_TB
- en: '| Least frequently used (LFU) | Tracks cache usage across time and removes
    the least frequently accessed item | When less frequently used items should be
    removed first |'
  prefs: []
  type: TYPE_TB
- en: '| Most recently used (MRU) | Tracks cache usage across time and removes the
    most recently accessed item | Rarely used, used when most recently used items
    are less likely to be accessed again |'
  prefs: []
  type: TYPE_TB
- en: '| Random replacement (RR) | Removes a random item from the cache | Simple and
    fast, used when it doesn’t impact performance |'
  prefs: []
  type: TYPE_TB
- en: Choosing the right eviction policy will depend on your use case and application
    requirements. Generally, you can start with the LRU policy before switching to
    alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: You should now feel more confident in implementing semantic caching mechanisms
    that apply to document retrieval or model responses. Next, let’s learn about context
    or prompt caching, which optimizes queries to models based on their inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Context/prompt caching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Context caching*, also known as *prompt caching*, is a caching mechanism suitable
    for scenarios where you’re referencing large amounts of context repeatedly within
    small requests. It’s designed to reuse precomputed attention states from frequently
    reused prompts, eliminating the need for redundant recomputation of the entire
    input context each time a new request is made.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You should consider using a context cache when your services involve the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Chatbots with extensive system instructions and long multiturn conversations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Repetitive analysis of lengthy video files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recurring queries against large document sets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequent code repository analysis or bug fixing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document summarizations, talking to books, papers, documentation, podcast transcripts
    and other long form content
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing a large number of examples in prompt (i.e., in-context learning)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This type of caching can help you to substantially reduce token usage costs
    by caching large context tokens. According to Anthropic, prompt caching can reduce
    costs by up to 90% and latency by up to 85% for long prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors of the [prompt caching paper](https://oreil.ly/augpd) that presents
    this technique also claim that:'
  prefs: []
  type: TYPE_NORMAL
- en: We find that Prompt Cache significantly reduces latency in time-to-first-token,
    especially for longer prompts such as document-based question answering and recommendations.
    The improvements range from 8× for GPU-based inference to 60× for CPU-based inference,
    all while maintaining output accuracy and without the need for model parameter
    modifications.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '[Figure 10-3](#context_caching_architecture) visualizes the context caching
    system architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1003](assets/bgai_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. System architecture for context caching
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At the time of writing, OpenAI automatically implements prompt caching for all
    API requests without requiring any code changes or additional costs. [Example 10-12](#context_cachin_anthropic)
    shows an example of how to use prompt caching when using the Anthropic API.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-12\. Context/prompt caching with Anthropic API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt caching is available only with a handful of models including Claude 3.5
    Sonnet, Claude 3 Haiku, and Claude 3 Opus.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Use the `cache_control` parameter to reuse the large document content across
    multiple API calls without processing it each time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the hood, the Anthropic client will add `anthropic-beta: prompt-caching-2024-07-31`
    to the request headers.'
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, `ephemeral` is the only supported cache type, which
    corresponds to a 5-minute cache lifetime.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As soon as you adopt a context cache, you’re introducing statefulness in requests
    by preserving tokens across them. This means the data you submit in one request
    will affect later requests, as the model provider server can use the cached context
    to maintain continuity between interactions.
  prefs: []
  type: TYPE_NORMAL
- en: With the Gemini API’s context caching feature, you can provide content to the
    model once, cache the input tokens, and reference these cached tokens for future
    requests.
  prefs: []
  type: TYPE_NORMAL
- en: Using these cached tokens can save you significant expenses if you avoid repeatedly
    passing in the same corpus of tokens in high volumes. The caching cost will depend
    on the size of the input tokens and the desired time to live (TTL) storage duration.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you cache a set of tokens, you can specify a TTL duration, which is how
    long the cache should exist before the tokens are automatically deleted. By default,
    TTL is normally set to 1 hour.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see how to use a cached system instruction in [Example 10-13](#context_caching_google).
    You will also need the Gemini API Python SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Example 10-13\. Context caching with the Google Gemini API
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Provide a display name as a cache key or identifier.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Pass the corpus to the context caching system. The minimum size of a context
    cache is 32,768 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run [Example 10-13](#context_caching_google) and print the `response.usage_metadata`,
    you should receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Notice how much of the `prompt_token_count` is now being cached when you compare
    it with the `cached_content_token_count`. The `candidates_token_count` refers
    to count of output or response tokens coming from the model, which isn’t affected
    by the caching system.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Gemini models don’t make any distinction between cached tokens and regular input
    tokens. Cached content will be prefixed to the prompt. This is why the prompt
    token count isn’t reduced when using caching.
  prefs: []
  type: TYPE_NORMAL
- en: With context caching, you won’t see a drastic reduction in response times but
    instead will significantly reduce operational costs as you avoid resending extensive
    system prompts and context tokens. Therefore, this caching strategy is most suitable
    when you have a large context to work with—for instance, when batch processing
    files with extensive instructions and examples.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using the same context cache and prompt doesn’t guarantee consistent model responses
    because the responses from LLMs are nondeterministic. A context cache doesn’t
    cache any output.
  prefs: []
  type: TYPE_NORMAL
- en: Context caching remains an active area of research. If you want to avoid any
    vendor lock-in, there is already some progress in this field with open source
    tools such as [*MemServe*](https://oreil.ly/PXm6B), which implements context caching
    with an elastic memory pool.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond caching, you can also review your options for reducing model size to
    speed up response times using techniques such as *model quantization*.
  prefs: []
  type: TYPE_NORMAL
- en: Model Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you’re going to be serving models such as LLMs yourself, you should consider
    *quantizing* (i.e., compressing/shrinking) your models if possible. Often, open
    source model repositories will also supply quantized versions that you can download
    and use straightaway without having to go through the quantization process yourself.
  prefs: []
  type: TYPE_NORMAL
- en: '*Model quantization* is the adjustment process on the model weights and activations
    where high-precision model parameters are statistically projected into lower-precision
    values through a fine-tuning operation using scaling factors on the original parameter
    distribution. You can then perform all critical inference operations with lower
    precision, after which you can convert the outputs to higher precision to maintain
    the quality while improving performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the precision also decreases the memory storage requirements, theoretically
    lowering energy consumption and speeding up operations like matrix multiplication
    through integer arithmetic. This also enables models to run on embedded devices,
    which may only support integer data types.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-4](#quantization_process) shows the full quantization process.'
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1004](assets/bgai_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. Quantization process
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can save more than a handful of gigabytes in GPU memory consumption as low-precision
    data types such as 8-bit integer would require significantly less RAM per parameter
    than a data type like 32-bit float.
  prefs: []
  type: TYPE_NORMAL
- en: Precision versus quality trade-off
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Figure 10-5](#quantization) compares a nonquantized model and a quantized
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1005](assets/bgai_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. Quantization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As each high-precision 32-bit float parameter consumes 4 bytes of GPU memory,
    a 1B-parameter model would require 4 GB of memory just for inference. If you plan
    on retraining or fine-tuning the same model, you’ll require at least 24 GB of
    GPU VRAM. This is because each parameter would also require storing information
    like gradients, training optimizer states, activations, and temporary memory space,
    consuming an additional 24 bytes together. This estimates up to 6 times the memory
    requirement compared to just loading the model weights. The same 1B model would
    then require a 24 GB GPU, which the best and most expensive consumer graphics
    cards such as NVIDIA RTX 4090 may still struggle to meet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of using the standard 32-float, you can select any of the following
    formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '*16-bit floating-point (FP16)* cuts memory usage in half without much of a
    hit to model output quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*8-bit integer (INT8)* offers huge savings in memory but with a significant
    loss in quality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*16-bit brain floating-point (BFLOAT16)* with a similar range to FP32 balances
    the memory and quality trade-off.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*4-bit integer (INT4)* provides a balance between memory efficiency and computational
    precision, making it suitable for low-power devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*1-bit integer (INT1)* uses the lowest precision data type with maximum model
    size reduction. Research for creating high-quality [1-bit LLMs](https://oreil.ly/QH9nH)
    is currently under way.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For comparison, [Table 10-2](#quantization_comparison) shows the reduction in
    model size when you quantize the Llama family models.
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-2\. Impact of quantization on the size of Llama models^([a](ch10.html#id1097))
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Original | FP16 | 8 Bit | 6 Bit | 4 Bit | 2Bit |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 2 70B | 140 GB | 128.5 GB | 73.23 GB | 52.70 GB | 36.20 GB | 28.59
    GB |'
  prefs: []
  type: TYPE_TB
- en: '| Llama 3 8B | 16.07 GB | 14.97 GB | 7.96 GB | 4.34 GB | 4.34 GB | 2.96 GB
    |'
  prefs: []
  type: TYPE_TB
- en: '| ^([a](ch10.html#id1097-marker)) Sources: [Llama.cpp GitHub repository](https://oreil.ly/9iYtL)
    and [Tom Jobbins’s Hugging Face Llama 2 70B model card](https://oreil.ly/BMDtR)
    |'
  prefs: []
  type: TYPE_TB
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In addition to the GPU VRAM needed to fit the model, you will also need an extra
    5 to 8 GB of GPU VRAM for overhead during model loading.
  prefs: []
  type: TYPE_NORMAL
- en: As per the current state of research, maintaining accuracy with integer-only
    INT4 and INT1 data types is a challenge, and the performance improvement with
    INT32 or FP16 is not significant. Therefore, the most popular lower-precision
    data type is INT8 for inference.
  prefs: []
  type: TYPE_NORMAL
- en: According to [research](https://oreil.ly/C7Lz3), using integer-only arithmetic
    for inference will be more efficient than floating-point numbers. However, quantizing
    floating numbers to integers can be tricky. For instance, only 256 values can
    be represented in INT8, while float32 can represent a wide range of values.
  prefs: []
  type: TYPE_NORMAL
- en: Floating-point numbers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To understand why projecting 32-bit floats to other formats would save so much
    in GPU memory, let’s look at how it breaks down.
  prefs: []
  type: TYPE_NORMAL
- en: 'A 32-bit floating-point number consists of the following types of bits:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sign* bit describing whether a number is positive or negative'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exponent* bits controlling the scale of the number'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Mantissa* bits holding the actual digits determining its precision (also known
    as *fraction* bits)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see a visualization of bits in the aforementioned floating-point numbers
    in [Figure 10-6](#quantization_bits).
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1006](assets/bgai_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. Bits in 32-bit float, 16-bit float, and bfloat16 numbers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: When you project the FP32 number into other formats, in effect, you’re squeezing
    it into smaller ranges, losing most of its mantissa bits and adjusting its exponent
    bits but without losing much of the precision. You can see such a phenomenon in
    action by referring to [Figure 10-7](#quantization_floating_numbers).
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 1007](assets/bgai_1007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. Quantization of floating-point numbers to integers
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In fact, [research on the quantization strategies for pretrained LLM models](https://oreil.ly/Swfz7)
    has shown that LLMs with 4-bit quantization can maintain performance similar to
    their nonquantized counterparts. However, while quantization saves memory, it
    can also reduce the inference speed of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: How to quantize pretrained LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Quantization is the process of compressing large models by weight adjustment.
    One such technique called [*GPTQ*](https://oreil.ly/rHYKZ) can quantize LLMs with
    175 billion parameters in approximately 4 GPU hours, reducing the bit width to
    3 or 4 bits per weight, with a negligible accuracy drop relative to the uncompressed
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The Hugging Face `transformers` and `optimum` library authors have collaborated
    closely with the `auto-gptq` library developers to provide a simple API for applying
    GPTQ quantization on open source LLMs. Optimum is a library that provides APIs
    to perform quantization using different tools.
  prefs: []
  type: TYPE_NORMAL
- en: With the GPTQ quantization, you can quantize your favorite language model to
    8, 4, 3, or even 2 bits without a big drop in performance, while maintaining faster
    inference speeds that are supported by most GPT hardware. You can follow [Example 10-14](#gptq_quantization)
    to quantize a pretrained model on your own GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dependencies you need to install to run [Example 10-14](#gptq_quantization)
    will include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Example 10-14\. GPTQ model quantization with Hugging Face and AutoGPTQ libraries
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Load the `float16` version of the `facebook/opt-125m` pretrained model prior
    to quantization.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO10-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Use the `c4` dataset to calibrate the quantization.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO10-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Quantize only the model’s decoder layer blocks.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_optimizing_ai_services_CO10-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Use model sequence length of `2048` to process the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For reference, a 175B model will require 4 GPU hours on NVIDIA A100 to quantize.
    However, it’s worth searching the Hugging Face model repository for prequantized
    models, as you might find that someone has already done the work.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand performance optimization techniques, let’s explore how
    to enhance the quality of your GenAI services using methods like structured outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Structured Outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Foundational models such as LLMs may be used as a component of a data pipeline
    or connected to downstream applications. For instance, you can use these models
    to extract and parse information from documents or to generate code that can be
    executed on other systems.
  prefs: []
  type: TYPE_NORMAL
- en: You can ask the LLM to provide a textual response containing JSON information.
    You will then have to extract and parse this JSON string using tools like regex
    and Pydantic. However, there is no guarantee that the model will always adhere
    to your instructions. Since your downstream systems may rely on JSON outputs,
    they may throw exceptions and incorrectly handle invalid inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Several utility packages like Instructor have been released to improve the robustness
    of LLM responses by taking a schema and making several API calls under the hood
    with various prompt templates to reach a desired output. While these solutions
    improve robustness, they also add significant costs to your solution due to subsequent
    API calls to the model providers.
  prefs: []
  type: TYPE_NORMAL
- en: Most recently, model providers have added a feature for requesting structured
    outputs by supplying schemas when making API calls to the model, as you can see
    in [Example 10-15](#structured_outputs). This helps to reduce the prompting templating
    work you have to do yourself and aims to improve the model’s *alignment* to your
    intent when returning a response.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: At the time of writing, only the most recent OpenAI SDK supports Pydantic models
    for enabling structured outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-15\. Structured outputs
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO11-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Specify a Pydantic model for structured outputs.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO11-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Provide the defined schema to the model client when making the API call.
  prefs: []
  type: TYPE_NORMAL
- en: If your model provider doesn’t support structured outputs natively, you can
    still leverage the model’s chat completion capabilities to increase robustness
    of structured outputs, as shown in [Example 10-16](#structured_outputs_completions).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-16\. Structured outputs based on chat completions prefill
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO12-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Limit the output tokens to improve robustness and speed of the structured responses
    and to reduce costs.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO12-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Skip the preamble and directly return a JSON by prefilling the assistant response
    and including a `{` character.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO12-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Add back the prefilled `{` and then find the closing `}` and extract the JSON
    substring.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_optimizing_ai_services_CO12-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Handle cases where there is no JSON in the response—e.g., if there is a refusal.
  prefs: []
  type: TYPE_NORMAL
- en: Following the aforementioned techniques should help you improve the robustness
    of your data pipelines if they leverage LLMs as a component.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Engineering
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompt engineering is the practice of crafting and refining queries to generative
    models to produce the most useful and optimized outputs. Without refining prompts,
    you’d either have to fine-tune models or train a model from scratch to optimize
    the output quality.
  prefs: []
  type: TYPE_NORMAL
- en: Many argue that the field lacks the scientific rigor to consider it an engineering
    discipline. However, you can approach the problem from an engineering perspective
    when refining prompts to get the best quality outputs from your models.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to how you communicate with others to get things done, with the optimized
    prompts, you can most effectively communicate your intent with the model to improve
    the chances of getting the responses you want. Therefore, prompting becomes not
    just an engineering problem but a communication one as well. A model can be compared
    to a knowledgeable colleague with lots of experience but limited domain knowledge,
    ready to help you but needs you to provide well-documented instructions, possibly
    with a few examples to follow and pattern match.
  prefs: []
  type: TYPE_NORMAL
- en: If your prompts are vague and generic, you’ll also get an average response.
  prefs: []
  type: TYPE_NORMAL
- en: Another way of thinking about this optimization problem is to compare the task
    of model prompting to programming. Instead of writing the code yourself, you’re
    effectively “coding” a model to be a well-integrated component of a larger application
    or data pipeline. You can adopt test-driven development (TDD) approaches and refine
    your prompts until your tests pass. Or, experiment with different models to see
    which one *aligns* its outputs to your intent the best.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Maximizing model *alignment* remains a high-priority objective of many model
    providers so that their model outputs best satisfy the user’s intent.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If your system instructions aren’t methodical, clear, and don’t follow best
    prompting practices, you may be leaving potential quality and performance optimizations
    on the table.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a minimum, you should have clear system prompts that provide specific tasks
    to the model. Best practice is to follow a systematic template. For instance,
    draft the model instructions following the *role, context, and task* (RCT) template:'
  prefs: []
  type: TYPE_NORMAL
- en: Role
  prefs: []
  type: TYPE_NORMAL
- en: Describes how the model should behave given a scenario and a task. Research
    has shown that specifying roles for LLMs tends to significantly affect their outputs.
    As an example, a model may be more forgiving in grading an essay if you give it
    the role of a primary school teacher. Without a specific role, the model may assume
    you want the grading to follow university-level academic standards.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can expand on the model’s role even further and describe a *persona* in
    detail for the model to adopt. Using a persona, the model will exactly know how
    to behave and make predictions as it has more context on what the role should
    entail.
  prefs: []
  type: TYPE_NORMAL
- en: Context
  prefs: []
  type: TYPE_NORMAL
- en: Sets the scenario, paints the picture, and provides any relevant and useful
    information that the model can use as a reference to making predictions. Without
    an explicit context, the model can only use an implicit context that’ll contain
    average information of its training data. In a RAG application, the context could
    be the concatenation of system prompt with the retrieved document chunks from
    a knowledge store.
  prefs: []
  type: TYPE_NORMAL
- en: Task
  prefs: []
  type: TYPE_NORMAL
- en: Provides clear instructions on what you want the model to perform given a context
    and a role. When describing the task, make sure you think of the model as a bright
    and knowledgeable apprentice, ready to jump into action but needs highly clear
    and unambiguous instructions to follow, potentially with a handful of examples.
  prefs: []
  type: TYPE_NORMAL
- en: Following the aforementioned system template, you should enhance the quality
    of your model outputs with minimal effort.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced prompting techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Beyond the prompting fundamentals, you can use more advanced techniques that
    may better fit your use case. Based on a [recent systematic survey of prompting
    techniques](https://oreil.ly/xynPC), you can group LLM prompts into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: In-context learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thought generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensembling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-criticism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agentic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s review each in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: In-context learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: What sets foundational models such as LLMs apart from traditional machine learning
    models is their ability to respond to dynamic inputs without the constant need
    for fine-tuning or retraining.
  prefs: []
  type: TYPE_NORMAL
- en: When you give system instructions to an LLM, you can additionally supply several
    examples, (i.e., shots) to guide the output generation.
  prefs: []
  type: TYPE_NORMAL
- en: '[*Zero-shot prompting*](https://oreil.ly/3F4wb) refers to a prompting approach
    that doesn’t specify reference examples, yet the model can still successfully
    complete the given task. If the model struggles without reference examples, you
    may have to use [*few-shot prompting*](https://oreil.ly/pOSj8) where you provide
    a handful of examples. There are also use cases where you want to use *dynamic
    few-shot* prompting where you dynamically insert examples from data fetched from
    a database or vector store.'
  prefs: []
  type: TYPE_NORMAL
- en: Prompting approaches where you specify examples are also termed *in-context
    learning*. You’re effectively fine-tuning the model’s outputs to your examples
    and the given task without actually modifying its model weights/parameters, whereas
    other ML models would require adjustments to their weights.
  prefs: []
  type: TYPE_NORMAL
- en: This is what makes LLMs and foundational models so powerful, since they don’t
    always require weight adjustment to fit your data and tasks you give them. You
    can learn about several in-context learning techniques by referring to [Table 10-3](#prompting_techniques_incontext_learning).
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-3\. In-context learning prompting techniques
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompting technique | Examples | Use cases |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot | Summarize the following…​ | Summarization, Q&A without specific
    training examples |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot | Classify documents based on examples below:[Examples] | Text classification,
    sentiment analysis, data extraction with few examples |'
  prefs: []
  type: TYPE_TB
- en: '| Dynamic few-shot | Classify the following documents based on examples below:<Inject
    examples from a vector store based on a query> | Personalized responses, complex
    problem-solving |'
  prefs: []
  type: TYPE_TB
- en: In-context learning prompts are straightforward, effective, and a great starting
    point for completing a variety of tasks. For more complex tasks, you can use more
    advanced prompting approaches like thought generation, decomposition, ensembling,
    self-criticism, or agentic approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Thought generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Thought generation techniques like [chain of thought (CoT)](https://oreil.ly/BWUYQ)
    have shown to significantly improve the ability of LLMs to perform complex reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: In COT prompting you ask the model to explain its thought process and reasoning
    as it provides a response. Variants of CoT include zero-shot or [few-shot CoT](https://oreil.ly/1gjSH)
    depending on whether you supply examples. A more advanced thought generation technique
    is [thread of thought (ThoT)](https://oreil.ly/1KyO4) that systematically segments
    and analyzes chaotic and very complex information or tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 10-4](#prompting_techniques_thought_generation) lists thought generation
    techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-4\. Thought generation prompting techniques
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompting technique | Examples | Use cases |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot chain of thought (CoT) | Let’s think step by step…​ | Mathematical
    problem-solving, logical reasoning, and multi-step decision-making. |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot CoT | Let’s think step by step…​ Here are a few examples:[EXAMPLES]
    | Scenarios where a few examples can guide the model to perform better, such as
    nuanced text classification, complex question answering, and creative writing
    prompts. |'
  prefs: []
  type: TYPE_TB
- en: '| Thread of thought (ThoT) | Walk me through the problem in manageable parts
    step by step, summarizing and analyzing as you go…​ | Maintaining context over
    multiple interactions, such as dialogue systems, interactive storytelling, and
    long-form content generation. |'
  prefs: []
  type: TYPE_TB
- en: Decomposition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Decomposition prompting techniques focus on breaking down complex tasks into
    smaller subtasks so that the model can work through them step-by-step and logically.
    You can experiment with these approaches alongside thought generation to identify
    which ones produce the best results for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the most common decomposition prompting techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Least-to-most](https://oreil.ly/HmsSN)'
  prefs: []
  type: TYPE_NORMAL
- en: Ask the model to break a complex problem into smaller problems via logical reduction
    without solving them. You can then reprompt the model to solve each task one by
    one.
  prefs: []
  type: TYPE_NORMAL
- en: '[Plan-and-solve](https://oreil.ly/aWTzf)'
  prefs: []
  type: TYPE_NORMAL
- en: Given a task, ask for a plan to be devised, and then request the model to solve
    it.
  prefs: []
  type: TYPE_NORMAL
- en: '[Tree of thoughts (ToT)](https://oreil.ly/IZdj1)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a tree-search problem where a task is broken into multiple branches of
    steps like a tree. Then, reprompt the model to evaluate and solve each branch
    of steps.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 10-5](#prompting_techniques_decomposition) shows these decomposition
    techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-5\. Decomposition prompting techniques
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompting technique | Examples | Use cases |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Least-to-most | Break down the task of…​into smaller tasks. | Complex problem-solving,
    project management, task decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| Plan-and-solve | Devise a plan to…​ | Algorithm development, software design,
    strategic planning |'
  prefs: []
  type: TYPE_TB
- en: '| Tree of thoughts (ToT) | Create a decision tree for choosing a…​ | Decision-making,
    problem-solving with multiple solutions, strategic planning with alternatives
    |'
  prefs: []
  type: TYPE_TB
- en: Ensembling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Ensembling* is the process of using multiple prompts to solve the same problem
    and then aggregating the responses into a final output. You can generate these
    responses using the same or different models.'
  prefs: []
  type: TYPE_NORMAL
- en: The main idea behind ensembling is to reduce the variance of LLM outputs by
    improving accuracy in exchange for higher usage costs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well-known ensembling prompting techniques include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Self-consistency](https://oreil.ly/_85WS)'
  prefs: []
  type: TYPE_NORMAL
- en: Generates multiple reasoning paths and selects the most consistent output as
    the final result using a majority vote.
  prefs: []
  type: TYPE_NORMAL
- en: '[Mixture of reasoning experts (MoRE)](https://oreil.ly/xllKs)'
  prefs: []
  type: TYPE_NORMAL
- en: Combines outputs from multiple LLMs with specialized prompts to improve response
    quality. Each LLM acts as an expert on an area focused on different reasoning
    tasks such as factual reasoning, logical reduction, common-sense checks, etc.
  prefs: []
  type: TYPE_NORMAL
- en: '[Demonstration ensembling (DENSE)](https://oreil.ly/lPEPz)'
  prefs: []
  type: TYPE_NORMAL
- en: Creates multiple few-shot prompts from data, then generates a final output by
    aggregating over the responses.
  prefs: []
  type: TYPE_NORMAL
- en: '[Prompt paraphrasing](https://oreil.ly/yP_ka)'
  prefs: []
  type: TYPE_NORMAL
- en: Formulates the original prompt into multiple variants via wording.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 10-6](#prompting_techniques_ensemling) shows examples and use cases
    of these ensembling techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-6\. Ensembling prompting techniques
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompting technique | Examples | Use cases |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Self-consistency | Prompt #1 (run multiple times): Let’s think step by step
    and complete the following task…​Prompt #2: From the following responses, choose
    the best/common one by scoring them using…​ | Reducing errors or bias in arithmetic,
    common-sense tasks, and symbolic reasoning tasks |'
  prefs: []
  type: TYPE_TB
- en: '| Mixture of reasoning experts (MoRE) | Prompt #1 (run for each expert): You
    are a reviewer for …​, score the following based on…​Prompt #2: Choose the best
    expert answer based on an agreement score…​ | Accounting for specialized knowledge
    areas or domains |'
  prefs: []
  type: TYPE_TB
- en: '| Demonstration ensembling (DENSE) | Create multiple few-shot examples for
    translating this text and aggregate the best responses.Generate several few-shot
    prompts for summarizing this article and combine the outputs. |'
  prefs: []
  type: TYPE_TB
- en: Improving output reliability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregating diverse perspectives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt paraphrasing | Prompt #1a: Reword this proposal…​Prompt #1b: Clarify
    this proposal…​Prompt #1c: Make adjustment to this proposal…​Prompt #2: Choose
    the best proposal from the following responses based on…​ |'
  prefs: []
  type: TYPE_TB
- en: Exploring different interpretations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data augmentation for ensembling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Self-criticism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Self-criticism* prompting techniques focus on using models as AI judges, assessors,
    or reviewers, either to perform self-checks or to assess the outputs of other
    models. The criticism or feedback from the first prompt can then be used to improve
    the response quality in follow-on prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are several self-criticism prompting strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Self-calibration](https://oreil.ly/_4YEr)'
  prefs: []
  type: TYPE_NORMAL
- en: Ask the LLM to assess the correctness of a response/answer against a question/answer.
  prefs: []
  type: TYPE_NORMAL
- en: '[Self-refine](https://oreil.ly/bTQJI)'
  prefs: []
  type: TYPE_NORMAL
- en: Refine responses iteratively through self-checks and providing feedback.
  prefs: []
  type: TYPE_NORMAL
- en: '[Reversing chain of thought (RCoT)](https://oreil.ly/6ojtr)'
  prefs: []
  type: TYPE_NORMAL
- en: Reconstruct the problem from a generated answer, and then generate fine-grained
    comparisons between the original problem and the reconstructed one to identify
    inconsistencies.
  prefs: []
  type: TYPE_NORMAL
- en: '[Self-verification](https://oreil.ly/Fz3JH)'
  prefs: []
  type: TYPE_NORMAL
- en: Generate potential solutions with the CoT technique, and then score each by
    masking parts of the question and supplying each answer.
  prefs: []
  type: TYPE_NORMAL
- en: '[Chain of verification (COVE)](https://oreil.ly/WrrLP)'
  prefs: []
  type: TYPE_NORMAL
- en: Create a list of related queries/questions to help verify the correctness of
    an answer/response.
  prefs: []
  type: TYPE_NORMAL
- en: '[Cumulative reasoning](https://oreil.ly/3Hb-6)'
  prefs: []
  type: TYPE_NORMAL
- en: Generate potential steps in responding to a query, and then ask the model to
    accept/reject each step. Finally, check whether it has arrived at the final answer
    to terminate the process; otherwise, repeat the process.
  prefs: []
  type: TYPE_NORMAL
- en: You can see examples of each self-criticism prompting technique in [Table 10-7](#prompting_techniques_self_criticism).
  prefs: []
  type: TYPE_NORMAL
- en: Table 10-7\. Self-criticism prompting techniques
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompting technique | Examples | Use cases |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Self-calibration | Assess the correctness of the following response: [response]
    for the following question: [question] | Gauge confidence of the answers to accept
    or revise the original answer. |'
  prefs: []
  type: TYPE_TB
- en: '| Self-refine | Prompt #1: What is your feedback on the response…​Prompt #2:
    Using the feedback [Feedback], refine your response on…​ | Reasoning, coding,
    and generation tasks. |'
  prefs: []
  type: TYPE_TB
- en: '| Reversing chain-of-thought (RCoT) | Prompt #1: Reconstruct the problem from
    this answer…​Prompt #2: Generate fine-grained comparison between these queries…​
    | Identifying inconsistencies and revising answers. |'
  prefs: []
  type: TYPE_TB
- en: '| Self-verification | Prompt #1 (run multiple times): Let’s think step by step
    - generate solution for the following problem…​Prompt #2: Score each solution
    based on the [masked problem]…​ | Improve on reasoning tasks. |'
  prefs: []
  type: TYPE_TB
- en: '| Chain of verification (COVE) | Prompt #1: Answer the following question…​Prompt
    #2: Formulate related questions to check this response: …​Prompt #3 (run for each
    new related question): Answer the following question: …​Prompt #4: Based on the
    following information, pick the best answer…​ | Question answering and text-generation
    tasks. |'
  prefs: []
  type: TYPE_TB
- en: '| Cumulative reasoning | Prompt #1: Outline steps to respond to the query:
    …​Prompt #2: Check the following plan and accept/reject steps relevant in responding
    to the query: …​Prompt #3: Check you’ve arrived at the final answer given the
    following information…​ | Step-by-step validation of complex queries, logical
    inference, and mathematical problems. |'
  prefs: []
  type: TYPE_TB
- en: Agentic
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You can take the prompting techniques discussed so far one step further and
    add access to external tools with complex evaluation algorithms. This process
    specializes LLMs as *agents*, allowing them to make plans, take actions, and use
    external systems.
  prefs: []
  type: TYPE_NORMAL
- en: Prompts or *prompt sequences (chains)* drive agentic systems with an engineering
    focus on creating agent-like behavior from LLMs. These agentic workflows serve
    users by performing actions on systems that interface with the GenAI models, which
    are mostly LLMs. Tools, whether *symbolic* like a calculator, or *neural* such
    as another AI model, form a core component of agentic systems.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you create a pipeline of multiple model calls with one output forwarded to
    the same or different model as input, you’ve constructed a *prompt chain*. In
    principle, you’re using the CoT prompting technique when you leverage prompt chains.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few agentic prompting techniques include:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Modular reasoning, knowledge, and language (MRKL)](https://oreil.ly/aWeQu)'
  prefs: []
  type: TYPE_NORMAL
- en: Simplest agentic system consisting of an LLM using multiple tools to get and
    combine information for generating an answer.
  prefs: []
  type: TYPE_NORMAL
- en: '[Self-correcting with tool-interactive critiquing (CRITIC)](https://oreil.ly/M-9YL)'
  prefs: []
  type: TYPE_NORMAL
- en: Responds to queries, and then self-checks its answer without using external
    tools. Finally, uses tools to verify or amend responses.
  prefs: []
  type: TYPE_NORMAL
- en: '[Program-aided language model (PAL)](https://oreil.ly/0WtKv)'
  prefs: []
  type: TYPE_NORMAL
- en: Generates code from queries and sends directly to code interpreters such as
    Python to generate an answer.^([4](ch10.html#id1128))
  prefs: []
  type: TYPE_NORMAL
- en: '[Tool-integrated reasoning agent (ToRA)](https://oreil.ly/pbfv_)'
  prefs: []
  type: TYPE_NORMAL
- en: Takes PAL a few steps further by interleaving code generation and reasoning
    steps as long as needed to provide a satisfactory response.
  prefs: []
  type: TYPE_NORMAL
- en: '[Reasoning and acting (ReAct)](https://oreil.ly/aDubr)'
  prefs: []
  type: TYPE_NORMAL
- en: Given a problem, generates thoughts, takes actions, receives observations, and
    repeats the loop with previous information, (i.e., memory) until the problem is
    solved.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to enable your LLMs to use tools, you can take advantage of *function
    calling* features from model providers, as in [Example 10-17](#function_calling).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-17\. Function calling for fetching
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As you saw in [Example 10-17](#function_calling), you can create agentic systems
    by configuring specialized LLMs that have access to custom tools and functions.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are cases where prompt engineering alone won’t give the response quality
    you’re looking for. Fine-tuning is an optimization technique that requires you
    to adjust the parameters of your GenAI model to better fit your data. For instance,
    you may fine-tune a language model to learn content of private knowledge bases
    or to always respond with a certain tone following your brand guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: It’s often not the first technique you should try since it requires effort to
    collect and prepare data, in addition to training and evaluating models.
  prefs: []
  type: TYPE_NORMAL
- en: When should you consider fine-tuning?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You may want to consider fine-tuning pretrained GenAI models if one of the
    following scenarios is true:'
  prefs: []
  type: TYPE_NORMAL
- en: You have significant token usage costs—for instance, due to requiring extensive
    system instructions or providing lots of examples in every prompt.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your use case relies on specialized domain expertise that the model needs to
    learn.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to reduce the number of hallucinations in responses with a more fine-tuned
    conservative model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You require higher-quality responses and have sufficient data for fine-tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You require lower latency in responses.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a model has been fine-tuned, you won’t need to provide as many examples
    in the prompt. This saves costs and enables lower-latency requests.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Avoid fine-tuning as much as you can.
  prefs: []
  type: TYPE_NORMAL
- en: There are many tasks where prompt engineering alone can help you optimize the
    quality of your outputs. Iterating over prompts has a much faster feedback loop
    than iterating over fine-tuning, which relies on creating datasets and running
    training jobs.
  prefs: []
  type: TYPE_NORMAL
- en: However, if you do end up needing to fine-tune, you’ll notice that the initial
    prompt engineering efforts would contribute to producing higher-quality training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few cases where fine-tuning can be useful:'
  prefs: []
  type: TYPE_NORMAL
- en: Teaching a model to respond in a brand style, tone, format, or some other qualitative
    metric—for instance, to produce standardized reports that comply with regulatory
    requirements and internal protocols
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving reliability of producing desired outputs such as always having responses
    conform to a given structured output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieving correct results to complex queries such as document classification
    and tagging from hundreds of classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing domain-specific specialized tasks such as item classification or
    industry-specific data interpretation and aggregation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nuanced handling of edge cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing skills or tasks that are hard to articulate in prompts such as datetime
    extraction from unstructured texts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing costs by using `gpt-40-mini` or even `gpt-3.5-turbo` instead of `gpt-4o`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teaching a model to use complex tools and APIs when using function calling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to fine-tune a pretrained model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For any fine-tuning job, you will need to follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare and upload training data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Submit a fine-tuning training job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate and use fine-tuned model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Depending on the model you’re using, the data must be prepared based on the
    model provider’s instruction.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, to fine-tune a typical chat model like `gpt-4o-2024-08-06`, you
    need to prepare your data as a message format, as shown in [Example 10-18](#fine_tune_prepare).
    At the time of writing, [OpenAI API pricing](https://oreil.ly/MmCNq) for fine-tuning
    this model is $25/1M training tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-18\. Example training data for a fine-tuning job
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Once your data is prepared, you need to upload the `jsonl` file, get a file
    ID, and supply that when submitting a fine-tuning job, as you can see in [Example 10-19](#fine_tuning_training).
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-19\. Submitting a fine-tuning training job
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Model providers that allow you to submit fine-tuning jobs will also provide
    APIs for checking the status of submitted jobs and for getting results.
  prefs: []
  type: TYPE_NORMAL
- en: Once the model is fine-tuned, you can retrieve the fine-tuned model ID and pass
    it to the LLM client, as shown in [Example 10-20](#fine_tuning_usage). Make sure
    to evaluate the model first before using it in production.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You can also use the testing techniques discussed in [Chapter 11](ch11.html#ch11)
    when evaluating fine-tuned models.
  prefs: []
  type: TYPE_NORMAL
- en: Example 10-20\. Using a fine-tuned model
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: While these examples show the fine-tuning process with OpenAI, the process will
    be similar with other providers even if the implementation details may differ.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you decide to leverage fine-tuning, be mindful that you won’t be able to
    take advantage of the latest improvements or optimizations in new LLMs, potentially
    making the fine-tuning process a waste of your time and money.
  prefs: []
  type: TYPE_NORMAL
- en: With this final optimization step, you should now feel confident in building
    GenAI services that not only meet your security and quality requirements but also
    achieve your desired throughput and latency metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about several optimization strategies to improve
    the throughput and quality of your services. A few optimizations you added covered
    various caching (keyword, semantic, context), prompt engineering, model quantization,
    and fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will shift focus to the last step in building AI services:
    deploying your GenAI solution. This includes exploring deployment patterns for
    AI services and containerization with Docker.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch10.html#id1069-marker)) See the OpenAI Batch API available in the [OpenAI
    API documentation](https://oreil.ly/0t59w).
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch10.html#id1072-marker)) Learn more about cache control headers at the
    [MDN website](https://oreil.ly/-Y5JP).
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch10.html#id1076-marker)) You may still require a trained embedder model
    for significant cost savings, as making frequent API calls to an off-the-shelf
    embedder model could incur additional costs, diminishing your overall savings.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch10.html#id1128-marker)) For better security, you still need to sanitize
    any LLM-generated code before forwarding it to downstream systems for execution.
  prefs: []
  type: TYPE_NORMAL
