- en: Capitolo 10\. Ottimizzazione dei servizi AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 10 章. AI 服务优化
- en: 'Questo lavoro è stato tradotto utilizzando l''AI. Siamo lieti di ricevere il
    tuo feedback e i tuoi commenti: [translation-feedback@oreilly.com](mailto:translation-feedback@oreilly.com)'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作是用 AI 翻译的。我们很高兴收到你的反馈和评论：[translation-feedback@oreilly.com](mailto:translation-feedback@oreilly.com)
- en: In questo capitolo imparerai a ottimizzare ulteriormente i tuoi servizi attraverso
    l'ingegneria del prompt, la quantificazione dei modelli e i meccanismi di caching.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何通过提示工程、模型量化和缓存机制进一步优化你的服务。
- en: Tecniche di ottimizzazione
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化技术
- en: Gli obiettivi dell'ottimizzazione di un servizio di IA sono il miglioramento
    della qualità dell'output o delle prestazioni (latenza, throughput, costi, ecc.).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 优化 IA 服务的目标是提高输出质量或性能（延迟、吞吐量、成本等）。
- en: 'Le ottimizzazioni relative alle prestazioni includono le seguenti:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 与性能相关的优化包括以下内容：
- en: Utilizzare le API per l'elaborazione batch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用批处理 API
- en: Caching (parola chiave, semantico, contesto o prompt)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存（关键字、语义、上下文或提示）
- en: Quantizzazione del modello
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型量化
- en: 'Le ottimizzazioni relative alla qualità includono le seguenti:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 与质量相关的优化包括以下内容：
- en: Utilizzo di output strutturati
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用结构化输出
- en: Ingegneria prompt
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程
- en: Messa a punto del modello
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型调整
- en: Esaminiamo ciascuna di esse in modo più dettagliato.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更详细地审视每一个。
- en: Elaborazione in batch
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批处理处理
- en: Spesso vuoi che un LLM elabori lotti di voci contemporaneamente. La soluzione
    più ovvia è quella di inviare più chiamate API per ogni voce. Tuttavia, l'approccio
    ovvio può essere costoso e lento e può portare il tuo fornitore di modelli a limitare
    le tariffe.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 经常希望 LLM 同时处理大量条目。最明显的方法是为每个条目发送多个 API 调用。然而，这种方法可能既昂贵又缓慢，并且可能导致你的模型提供商限制费用。
- en: 'In questi casi, puoi sfruttare due tecniche distinte per elaborare i dati in
    batch attraverso un LLM:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，你可以利用两种不同的技术通过 LLM 批量处理数据：
- en: Aggiornare gli schemi di output strutturato per restituire più esempi contemporaneamente
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新结构化输出模式以同时返回更多示例
- en: Identificare e utilizzare le API dei fornitori di modelli progettate per l'elaborazione
    in batch.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别并使用为批处理设计的模型提供商的 API。
- en: La prima soluzione prevede che tu aggiorni i modelli o i prompt di Pydantic
    in modo da richiedere un elenco di output per ogni richiesta. In questo caso,
    puoi elaborare i dati in batch con una manciata di richieste invece di una per
    voce.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个解决方案是更新模型或 Pydantic 的提示，以便每个请求都要求一个输出列表。在这种情况下，你可以通过几个请求而不是每个条目来批量处理数据。
- en: Un'implementazione della prima soluzione è mostrata nell'[Esempio 10-1](#batch_processing_structured_outputs).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个解决方案的实现示例如下所示：[示例 10-1](#batch_processing_structured_outputs)。
- en: Esempio 10-1\. Aggiornamento dello schema di output strutturato per l'analisi
    di più elementi
  id: totrans-21
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-1. 更新结构化输出模式以分析多个元素
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO1-1)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_optimizing_ai_services_CO1-1)'
- en: Aggiornare il modello Pydantic per includere un elenco di modelli `Category`.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 更新 Pydantic 模型以包含一个 `Category` 模型列表。
- en: Ora puoi passare il nuovo schema insieme a un elenco di titoli di documenti
    al client OpenAI per elaborare più voci in un'unica chiamata API. Tuttavia, un'alternativa
    e forse la soluzione migliore sarà quella di utilizzare un'API batch, se disponibile.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Ora puoi passare il nuovo schema insieme a un elenco di titoli di documenti
    al client OpenAI per elaborare più voci in un'unica chiamata API. Tuttavia, un'alternativa
    e forse la soluzione migliore sarà quella di utilizzare un'API batch, se disponibile.
- en: Fortunatamente, i fornitori di modelli come OpenAI forniscono già delle API
    adatte a questi casi d'uso. Sotto il cofano, questi fornitori possono eseguire
    delle code di attività per elaborare un singolo lavoro batch in background, fornendo
    aggiornamenti sullo stato fino al completamento del batch per recuperare i risultati.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，像 OpenAI 这样的模型提供商已经为这些用例提供了合适的 API。在幕后，这些提供商可以执行代码来在后台处理单个批处理工作，并提供状态更新，直到批处理完成以检索结果。
- en: Rispetto all'utilizzo diretto degli endpoint standard, sarai in grado di inviare
    gruppi di richieste asincrone a costi inferiori (fino al 50% con OpenAI), di usufruire
    di limiti di velocità più elevati e di garantire tempi di completamento.^([1](ch10.html#id1069))Il
    servizio batch job è ideale per l'elaborazione di lavori che non richiedono risposte
    immediate, come l'utilizzo di OpenAI LLMs per analizzare, classificare o tradurre
    grandi volumi di documenti in background.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与直接使用标准端点相比，你将能够以更低的成本（例如，与OpenAI相比，降低高达50%）发送异步请求组，享受更高的速度限制，并保证完成时间。[1](ch10.html#id1069)批处理作业服务非常适合处理不需要即时响应的工作，例如使用OpenAI
    LLMs在后台分析、分类或翻译大量文档。
- en: Per inviare un lavoro batch, avrai bisogno di un file `jsonl` in cui ogni riga
    contiene i dettagli di una singola richiesta all'API, come mostrato nell'[Esempio
    10-2](#jsonl). Inoltre, come si vede in questo esempio, per creare il file JSONL,
    puoi iterare le voci e generare dinamicamente il file.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了发送批处理工作，你需要一个`jsonl`文件，其中每一行包含单个API请求的详细信息，如[示例 10-2](#jsonl)所示。此外，正如这个示例所示，为了创建JSONL文件，你可以迭代条目并动态生成文件。
- en: Esempio 10-2\. Creare un file JSONL dalle voci
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-2. 从条目创建JSONL文件
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Una volta creato, puoi inviare il file all'API batch per l'elaborazione, come
    mostrato nell'[esempio 10-3](#batch_processing_api).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 创建后，你可以将文件发送到批处理API进行处理，如[示例 10-3](#batch_processing_api)所示。
- en: Esempio 10-3\. Elaborazione di lavori batch con l'API OpenAI Batch
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-3. 使用OpenAI Batch API处理批处理工作
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Ora puoi sfruttare gli endpoint batch offline per elaborare più voci in una
    sola volta, con tempi di consegna garantiti e un notevole risparmio sui costi.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以利用离线批处理端点一次处理多个条目，保证交付时间并显著降低成本。
- en: Oltre a sfruttare gli output strutturati e le API batch per ottimizzare i tuoi
    servizi, puoi anche sfruttare le tecniche di caching per accelerare in modo significativo
    i tempi di risposta e i costi delle risorse dei tuoi server.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了利用结构化输出和批处理API来优化你的服务外，你还可以利用缓存技术来显著加速响应时间和服务器资源的成本。
- en: Caching
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缓存
- en: Nei servizi GenAI, spesso ti affidi alla risposta di dati/modelli che richiedono
    calcoli significativi o lunghe elaborazioni. Se ci sono più utenti che richiedono
    gli stessi dati, ripetere le stesse operazioni può essere dispendioso. Invece,
    puoi usare le tecniche di caching per memorizzare e recuperare i dati a cui si
    accede di frequente per aiutarti a ottimizzare i tuoi servizi accelerando i tempi
    di risposta, riducendo il carico del server e risparmiando larghezza di banda
    e costi operativi.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在GenAI服务中，你通常依赖于需要大量计算或长时间处理的数据/模型响应。如果有多个用户需要相同的数据，重复相同的操作可能会很昂贵。相反，你可以使用缓存技术来存储和检索频繁访问的数据，以帮助你优化服务，通过加速响应时间、减少服务器负载和节省带宽和运营成本。
- en: Ad esempio, in un chatbot FAQ pubblico in cui gli utenti pongono per lo più
    le stesse domande, potresti voler riutilizzare le risposte nella cache per periodi
    più lunghi. D'altra parte, per chatbot più personalizzati e dinamici, puoi aggiornare
    frequentemente (cioè invalidare) la risposta nella cache.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个公共聊天机器人FAQ中，用户主要提出相同的问题，你可能希望将缓存中的响应重用更长时间。另一方面，对于更个性化、更动态的聊天机器人，你可以频繁更新（即使缓存中的响应失效）。
- en: Suggerimento
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 建议
- en: Dovresti sempre considerare la frequenza di aggiornamento della cache in base
    alla natura dei dati e al livello accettabile di staleness.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该始终根据数据的性质和可接受的陈旧程度来考虑缓存的更新频率。
- en: 'Le strategie di caching più importanti per i servizi GenAI includono:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GenAI服务，最重要的缓存策略包括：
- en: Caching delle parole chiave
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键字缓存
- en: Caching semantico
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语义缓存
- en: Caching del contesto o del prompt
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文或提示缓存
- en: Esaminiamo ciascuna di esse in modo più dettagliato.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更详细地考察每一个。
- en: Caching delle parole chiave
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键字缓存
- en: Se hai bisogno solo di un semplice meccanismo di caching per memorizzare le
    funzioni o le risposte degli endpoint, puoi usare il *keyword caching*, che consiste
    nel memorizzare le risposte in base alle corrispondenze esatte delle query di
    input come coppie chiave-valore.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只需要一个简单的缓存机制来存储函数或端点的响应，你可以使用*关键字缓存*，它通过存储基于输入查询的精确匹配的键值对来存储响应。
- en: In FastAPI, librerie come `fastapi-cache` possono aiutarti a implementare la
    cache delle parole chiave in poche righe di codice, su qualsiasi funzione o endpoint.
    Le cache FastAPI ti danno anche la possibilità di collegare backend di archiviazione
    come Redis per centralizzare l'archivio della cache in tutte le tue istanze.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: In FastAPI, librerie come `fastapi-cache` possono aiutarti a implementare la
    cache delle parole chiave in poche righe di codice, su qualsiasi funzione o endpoint.
    Le cache FastAPI ti danno anche la possibilità di collegare backend di archiviazione
    come Redis per centralizzare l'archivio della cache in tutte le tue istanze.
- en: Suggerimento
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: Suggerimento
- en: In alternativa, puoi implementare un meccanismo di caching personalizzato con
    un cache store utilizzando pacchetti di livello inferiore come `cachetools`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: In alternativa, puoi implementare un meccanismo di caching personalizzato con
    un cache store utilizzando pacchetti di livello inferiore come `cachetools`.
- en: 'Per iniziare, tutto ciò che devi fare è inizializzare e configurare il sistema
    di caching come parte del ciclo di vita dell''applicazione, come mostrato nell''[esempio
    10-4](#caching_lifespan). Puoi installare la cache FastAPI utilizzando il seguente
    comando:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 'Per iniziare, tutto ciò che devi fare è inizializzare e configurare il sistema
    di caching come parte del ciclo di vita dell''applicazione, come mostrato nell''[esempio
    10-4](#caching_lifespan). Puoi installare la cache FastAPI utilizzando il seguente
    comando:'
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Esempio 10-4\. Configurazione della durata della cache FastAPI
  id: totrans-53
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Esempio 10-4\. Configurazione della durata della cache FastAPI
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO2-1)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimizing_ai_services_CO2-1)'
- en: Inizializza `FastAPICache` con un `RedisBackend` che non decodifica le risposte
    in modo che i dati in cache siano memorizzati come byte (binari). Questo perché
    la decodifica delle risposte romperebbe la cache alterando il formato originale
    della risposta.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Inizializza `FastAPICache` con un `RedisBackend` che non decodifica le risposte
    in modo che i dati in cache siano memorizzati come byte (binari). Questo perché
    la decodifica delle risposte romperebbe la cache alterando il formato originale
    della risposta.
- en: Una volta configurato il sistema di caching, puoi decorare le tue funzioni o
    i gestori di endpoint per mettere in cache i loro output, come mostrato nell'[Esempio
    10-5](#caching_functions_endpoint).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Una volta configurato il sistema di caching, puoi decorare le tue funzioni o
    i gestori di endpoint per mettere in cache i loro output, come mostrato nell'[Esempio
    10-5](#caching_functions_endpoint).
- en: Esempio 10-5\. Caching dei risultati delle funzioni e degli endpoint
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Esempio 10-5\. Caching dei risultati delle funzioni e degli endpoint
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO3-1)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimizing_ai_services_CO3-1)'
- en: Il decoratore `cache()` deve sempre arrivare per ultimo. Invalida la cache tra
    60 secondi impostando `expires=60` per ricompilare gli output.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Il decoratore `cache()` deve sempre arrivare per ultimo. Invalida la cache tra
    60 secondi impostando `expires=60` per ricompilare gli output.
- en: Il decoratore `cache()` mostrato nell'[Esempio 10-5](#caching_functions_endpoint)
    inietta le dipendenze per gli oggetti `Request` e `Response` in modo da poter
    aggiungere intestazioni di controllo della cache alla risposta in uscita. Queste
    intestazioni di controllo della cache istruiscono i client su come mettere in
    cache le risposte da parte loro, specificando un insieme di direttive (cioè istruzioni).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Il decoratore `cache()` mostrato nell'[Esempio 10-5](#caching_functions_endpoint)
    inietta le dipendenze per gli oggetti `Request` e `Response` in modo da poter
    aggiungere intestazioni di controllo della cache alla risposta in uscita. Queste
    intestazioni di controllo della cache istruiscono i client su come mettere in
    cache le risposte da parte loro, specificando un insieme di direttive (cioè istruzioni).
- en: 'Queste sono alcune direttive comuni per il controllo della cache durante l''invio
    delle risposte:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 'Queste sono alcune direttive comuni per il controllo della cache durante l''invio
    delle risposte:'
- en: '`max-age`'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`max-age`'
- en: Definisce il tempo massimo (in secondi) in cui una risposta è considerata fresca.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Definisce il tempo massimo (in secondi) in cui una risposta è considerata fresca.
- en: '`no-cache`'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '`no-cache`'
- en: Forza la riconvalida in modo che i client controllino gli aggiornamenti costanti
    con il server.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Forza la riconvalida in modo che i client controllino gli aggiornamenti costanti
    con il server.
- en: '`no-store`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`no-store`'
- en: Impedisce completamente il caching
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Impedisce completamente il caching
- en: '`private`'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`private`'
- en: Memorizza le risposte in una cache privata (ad esempio, la cache locale dei
    browser).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Memorizza le risposte in una cache privata (ad esempio, la cache locale dei
    browser).
- en: 'Una risposta potrebbe avere intestazioni di controllo della cache come `Cache-Control:
    max-age=180, private` per impostare queste direttive.^([2](ch10.html#id1072))'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 'Una risposta potrebbe avere intestazioni di controllo della cache come `Cache-Control:
    max-age=180, private` per impostare queste direttive.^([2](ch10.html#id1072))'
- en: Dato che il caching delle parole chiave lavora sulle corrispondenze esatte,
    è più adatto alle funzioni e alle API che prevedono input con corrispondenze frequenti.
    Tuttavia, nei servizi GenAI che accettano query variabili da parte dell'utente,
    potresti voler prendere in considerazione altri meccanismi di caching che si basano
    sul significato degli input quando restituiscono una risposta in cache. È qui
    che la cache semantica può rivelarsi utile.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于关键字缓存基于精确匹配，因此更适合那些预期输入具有频繁匹配的功能和API。然而，在接收用户变量查询的GenAI服务中，你可能需要考虑其他基于输入意义的缓存机制，当在缓存中返回响应时。这正是语义缓存可以发挥作用的地方。
- en: Caching semantico
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语义缓存
- en: Il*caching semantico* è un meccanismo di caching che restituisce un valore memorizzato
    in base a input simili.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*语义缓存*是一种基于相似输入返回存储值的缓存机制。'
- en: Sotto il cofano, il sistema utilizza codificatori e vettori di incorporamento
    per catturare la semantica e i significati degli input, quindi esegue ricerche
    di somiglianza tra le coppie chiave-valore memorizzate per restituire una risposta
    nella cache.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在内部，系统使用编码器和嵌入向量来捕获输入的语义和意义，然后执行存储的关键值对的相似性搜索，以在缓存中返回一个响应。
- en: 'Rispetto alla memorizzazione nella cache delle parole chiave, gli input simili
    possono restituire la stessa risposta memorizzata nella cache. Gli input al sistema
    non devono essere identici per essere riconosciuti come simili. Anche se tali
    input hanno strutture o formulazioni di frasi diverse o contengono imprecisioni,
    saranno comunque considerati simili perché hanno lo stesso significato. Inoltre,
    viene richiesta la stessa risposta. A titolo di esempio, le seguenti query sono
    considerate simili perché hanno lo stesso intento:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与关键字缓存相比，相似输入可以返回相同的缓存响应。系统输入不必完全相同才能被识别为相似。即使这些输入具有不同的结构或句子结构，或者包含不精确性，只要它们具有相同的意义，它们仍然会被视为相似。此外，还要求相同的响应。例如，以下查询被认为是相似的，因为它们具有相同的意图：
- en: Come si costruiscono i servizi generativi con FastAPI?
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用FastAPI构建生成式服务？
- en: Qual è il processo di sviluppo dei servizi FastAPI per Genai?
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FastAPI生成式服务的开发过程是什么？
- en: Questo sistema di caching contribuisce a un significativo risparmio sui costi^([3](ch10.html#id1076))
    riducendo le chiamate API del [30-40%](https://oreil.ly/gjGz6) (cioè con un tasso
    di risposta alla cache del 60-70%) a seconda del caso d'uso e delle dimensioni
    della base di utenti. Ad esempio, le applicazioni Q&A RAG che ricevono domande
    frequenti da una vasta base di utenti possono ridurre le chiamate API del 69%
    utilizzando una cache semantica.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 该缓存系统通过减少API调用（根据用例和用户基数，减少30-40%），即以60-70%的缓存命中率，显著降低了成本^([3](ch10.html#id1076)))。例如，接收大量用户常见问题的Q&A
    RAG应用可以通过使用语义缓存减少69%的API调用。
- en: 'In un tipico sistema RAG, ci sono due punti in cui la cache può ridurre le
    operazioni che richiedono risorse e tempo:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的RAG系统中，有两个点可以减少需要资源和时间的操作：
- en: '*Prima che il LLM* restituisca una risposta nella cache invece di generarne
    una nuova'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在LLM返回新响应之前，先在缓存中查找*'
- en: '*Prima dell''archivio vettoriale* per arricchire i prompt con documenti memorizzati
    nella cache, invece di cercarne e recuperarne di nuovi.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在向量存储库之前*，用缓存中的文档丰富提示，而不是搜索和恢复新的文档。'
- en: 'Quando integri un componente di cache semantica nel tuo sistema RAG, devi considerare
    se la restituzione di una risposta in cache potrebbe avere un impatto negativo
    sull''esperienza dell''utente dell''applicazione. Ad esempio, se le risposte di
    LLM vengono memorizzate nella cache, entrambe le query seguenti restituirebbero
    la stessa risposta a causa della loro elevata somiglianza semantica, inducendo
    il sistema di cache semantica a trattarle come quasi identiche:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在你的RAG系统中集成语义缓存组件时，你必须考虑缓存响应的返回是否可能对应用程序的用户体验产生负面影响。例如，如果LLM的响应被存储在缓存中，由于它们的高度语义相似性，以下两个查询都会返回相同的响应，导致语义缓存系统将它们视为几乎相同：
- en: Riassumi questo testo in 100 parole
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用100个词总结这篇文章
- en: Riassumi questo testo in 50 parole
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用50个词总结这篇文章
- en: In questo modo si ha l'impressione che i servizi non rispondano alle query.
    Dal momento che potresti ancora volere diversi output LLM nella tua applicazione,
    implementeremo una cache semantica per il recupero dei documenti per il tuo sistema
    RAG.[La Figura 10-1](#semantic_cache) mostra l'architettura completa del sistema.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，你会感觉到服务没有响应用户查询。由于你可能在应用程序中还需要不同的LLM输出，我们将实现一个用于你的RAG系统的文档恢复语义缓存。[图 10-1](#semantic_cache)展示了系统的完整架构。
- en: '![bgai 1001](assets/bgai_1001.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 1001](assets/bgai_1001.png)'
- en: Figura 10-1\. Caching semantico nell'architettura del sistema RAG
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. RAG系统架构中的语义缓存
- en: Cominciamo a implementare il sistema di caching semantico da zero e poi vedremo
    come scaricare la funzionalità a una libreria esterna come`gptcache`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从零开始实现语义缓存系统，然后我们将看到如何将功能下载到外部库`gptcache`中。
- en: Costruire un servizio di caching semantico da zero
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从零开始构建一个语义缓存服务
- en: 'Puoi implementare un sistema di caching semantico implementando i seguenti
    componenti:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过实现以下组件来实施一个语义缓存系统：
- en: Un client di cache store
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存存储客户端
- en: Un client per l'archiviazione vettoriale di documenti
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于文档向量存储的客户端
- en: Un modello di incorporazione
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个嵌入模型
- en: L['esempio 10-6](#semantic_cache_cache_store) mostra come implementare il client
    del cache store.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: L['esempio 10-6](#semantic_cache_cache_store)展示了如何实现缓存存储客户端。
- en: Esempio 10-6\. Client del negozio di cache
  id: totrans-97
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Esempio 10-6\. 缓存存储客户端
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO4-1)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)[#co_optimizing_ai_services_CO4-1]'
- en: Inizializza un client Qdrant in esecuzione sulla memoria che funge da cache
    store.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化一个运行在内存上的Qdrant客户端，作为缓存存储。
- en: Una volta inizializzato il client dell'archivio cache, puoi configurare l'archivio
    vettoriale dei documenti seguendo l'[Esempio 10-7](#semantic_cache_doc_store).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始化了缓存存储客户端，你可以根据[Esempio 10-7](#semantic_cache_doc_store)配置文档向量存储。
- en: Esempio 10-7\. Client del negozio di documenti
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Esempio 10-7\. 文档存储客户端
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO5-1)'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)[#co_optimizing_ai_services_CO5-1]'
- en: Carica una collezione di documenti nell'archivio vettoriale di Qdrant.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在Qdrant向量存储中加载文档集合。
- en: Con i client della cache e dell'archivio vettoriale di documenti pronti, puoi
    ora implementare il servizio di cache semantica, come mostrato nell'[Esempio 10-8](#semantic_cache_service),
    con i metodi per calcolare gli embeddings ed eseguire le ricerche nella cache.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备好缓存和文档向量存储客户端之后，你现在可以像[Esempio 10-8](#semantic_cache_service)中展示的那样实现语义缓存服务，包括计算嵌入和执行缓存中的搜索的方法。
- en: Esempio 10-8\. Sistema di caching semantico
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Esempio 10-8\. 语义缓存系统
- en: '[PRE8]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO6-1)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)[#co_optimizing_ai_services_CO6-1]'
- en: Imposta una soglia di somiglianza. Qualsiasi punteggio superiore a questa soglia
    sarà un hit della cache.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 设置一个相似度阈值。任何高于这个阈值的分数都将被视为缓存命中。
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO6-2)'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)[#co_optimizing_ai_services_CO6-2]'
- en: Interroga l'archivio dei documenti se non c'è un riscontro nella cache. Memorizza
    nella cache i documenti recuperati con l'incorporazione vettoriale della query
    come chiave della cache.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果缓存中没有找到匹配项，则查询文档存储。将使用查询向量嵌入作为键将检索到的文档存储在缓存中。
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO6-3)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)[#co_optimizing_ai_services_CO6-3]'
- en: Se non ci sono documenti correlati o cache disponibili per la query data, restituisce
    una risposta in scatola.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有相关文档或缓存可用，则返回一个boxed的响应。
- en: Ora che hai un servizio di caching semantico, puoi usarlo per recuperare i documenti
    in cache dalla memoria seguendo l'[Esempio 10-9](#semantic_cache_qdrant_usage).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有一个语义缓存服务，你可以使用它来根据[Esempio 10-9](#semantic_cache_qdrant_usage)从内存中恢复缓存中的文档。
- en: Esempio 10-9\. Implementazione di una cache semantica in un sistema RAG con
    Qdrant
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Esempio 10-9\. 在RAG系统中使用Qdrant实现语义缓存
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Ora dovresti aver capito meglio come implementare i tuoi sistemi di caching
    semantico personalizzati utilizzando un client di database vettoriale.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该已经更好地理解了如何使用向量数据库客户端实现自定义的语义缓存系统。
- en: Caching semantico con la cache GPT
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用GPT缓存进行语义缓存
- en: Se non hai bisogno di sviluppare il tuo servizio di caching semantico da zero,
    puoi anche utilizzare la libreria modulare `gptcache` che ti dà la possibilità
    di scambiare vari componenti di archiviazione, caching e incorporazione.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不需要从头开始开发你的语义缓存服务，你也可以使用模块化库`gptcache`，它给你交换各种存储、缓存和嵌入组件的机会。
- en: 'Per configurare una cache semantica con `gptcache`, devi prima installare il
    pacchetto:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `gptcache` 配置语义缓存，你必须首先安装以下包：
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Quindi carica il sistema all'avvio dell'applicazione, come mostrato nell'[Esempio
    10-10](#configrue_gptcache).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在应用程序启动时加载系统，如 [示例 10-10](#configrue_gptcache) 所示。
- en: Esempio 10-10\. Configurazione della cache GPT
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Esempio 10-10\. 配置 GPT 缓存
- en: '[PRE11]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO7-1)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimizing_ai_services_CO7-1)'
- en: Seleziona una funzione di callback post-elaborazione per selezionare un elemento
    casuale tra quelli restituiti nella cache.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个后处理回调函数来从缓存中返回的元素中选择一个随机元素。
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO7-2)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_optimizing_ai_services_CO7-2)'
- en: Seleziona una funzione di callback pre-inclusione per utilizzare l'ultima query
    per impostare una nuova cache.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个预包含的回调函数来使用最新查询来设置新的缓存。
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO7-3)'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_optimizing_ai_services_CO7-3)'
- en: Usa il modello di incorporazione ONNX per calcolare i vettori di incorporazione.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ONNX 集成模型来计算嵌入向量。
- en: '[![4](assets/4.png)](#co_optimizing_ai_services_CO7-4)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_optimizing_ai_services_CO7-4)'
- en: Utilizza `OnnxModelEvaluation` per calcolare i punteggi di somiglianza tra gli
    elementi in cache e una determinata query.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `OnnxModelEvaluation` 来计算缓存元素与特定查询之间的相似度得分。
- en: '[![5](assets/5.png)](#co_optimizing_ai_services_CO7-5)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_optimizing_ai_services_CO7-5)'
- en: Imposta le opzioni di configurazione della cache, come ad esempio la soglia
    di somiglianza.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 设置缓存配置选项，例如相似度阈值。
- en: '[![6](assets/6.png)](#co_optimizing_ai_services_CO7-6)'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_optimizing_ai_services_CO7-6)'
- en: Fornisci una chiave API del client OpenAI per GPT Cache per eseguire automaticamente
    la cache semantica sulle risposte dell'API LLM.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为 GPT Cache 提供一个 OpenAI 客户端 API 密钥以自动在 LLM API 的响应上执行语义缓存。
- en: Una volta inizializzato, `gptcache` si integrerà perfettamente con il client
    LLM di OpenAI nella tua applicazione. Ora puoi effettuare query multiple LLM,
    come mostrato nell'[Esempio 10-11](#semantic_caching_gptcache), sapendo che `gptcache`
    metterà in cache le risposte LLM.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦初始化，`gptcache` 将完美集成到你的应用程序中的 OpenAI LLM 客户端。现在你可以执行多个 LLM 查询，如 [示例 10-11](#semantic_caching_gptcache)
    所示，知道 `gptcache` 会将 LLM 的响应放入缓存。
- en: Esempio 10-11\. Caching semantico con la cache GPT
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-11\. 使用 GPT 缓存进行语义缓存
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: L'utilizzo di librerie esterne come `gptcache`, come mostrato nell'[Esempio
    10-11](#semantic_caching_gptcache), rende semplice l'implementazione del caching
    semantico.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [示例 10-11](#semantic_caching_gptcache) 所示，使用 `gptcache` 等外部库使语义缓存的实现变得简单。
- en: Una volta che il sistema di caching è attivo e funzionante, puoi regolare *le
    soglie di somiglianza* per ottimizzare le percentuali di successo della cache.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦缓存系统启动并运行，你可以调整 *相似度阈值* 来优化缓存的成功率。
- en: Soglia di somiglianza
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 相似度阈值
- en: Quando crei un servizio di cache semantica, potresti dover regolare la soglia
    di somiglianza in base alle query fornite per ottenere tassi di risposta alla
    cache elevati e accurati. Puoi fare riferimento alla [visualizzazione interattiva
    dei cluster della cache semantica](https://semanticcachehit.com) mostrata nella
    [Figura 10-2](#semantic_cache_visualization) per capire meglio il concetto disoglia
    di somiglianza.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建语义缓存服务时，你可能需要根据提供的查询调整相似度阈值以获得高响应率和准确率。你可以参考 [图 10-2](#semantic_cache_visualization)
    中所示的 [语义缓存集群交互式可视化](https://semanticcachehit.com) 来更好地理解相似度阈值的概念。
- en: Aumentando il valore della soglia nella [Figura 10-2](#semantic_cache_visualization)
    si otterrà un grafo meno connesso, mentre riducendolo al minimo si possono ottenere
    dei falsi positivi. Pertanto, è consigliabile eseguire alcuni esperimenti per
    mettere a punto la soglia di somiglianza per la propria applicazione.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 10-2](#semantic_cache_visualization) 中增加阈值值将得到一个连接度较低的图，而将其降至最低可能会得到一些假阳性。因此，建议为你的应用程序调整相似度阈值进行一些实验。
- en: '![bgai 1002](assets/bgai_1002.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 1002](assets/bgai_1002.png)'
- en: 'Figura 10-2\. Visualizzazione del caching semantico (Fonte: [semanticcachehit.com](https://semanticcachehit.com))'
  id: totrans-147
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-2\. 语义缓存可视化（来源：[semanticcachehit.com](https://semanticcachehit.com)）
- en: Politiche di sfratto
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 清理策略
- en: Un altro concetto rilevante per la cache è quello delle *politiche di eviction*
    che controllano il comportamento della cache quando il meccanismo di caching raggiunge
    la sua capacità massima. La selezione della politica di eviction appropriata deve
    essere adatta al tuo caso d'uso.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于缓存来说，另一个相关的概念是 *驱逐策略*，它控制着缓存在缓存机制达到最大容量时的行为。选择合适的驱逐策略必须适合你的用例。
- en: Suggerimento
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 建议
- en: Dato che le dimensioni della memoria cache sono spesso limitate, puoi aggiungere
    un metodo `evict()` al `SemanticCachingService` che hai implementato nell'[Esempio
    10-8](#semantic_cache_service).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内存缓存的大小通常有限，你可以在你实现的 `[Esempio 10-8](#semantic_cache_service)` 中的 `SemanticCachingService`
    添加一个 `evict()` 方法。
- en: La[Tabella 10-1](#eviction_policies) mostra alcuni criteri di sfratto che puoi
    scegliere.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '[表10-1](#eviction_policies) 展示了一些你可以选择驱逐策略。'
- en: Tabella 10-1\. Politiche di sfratto
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表10-1\. 驱逐策略
- en: '| Politica | Descrizione | Caso d''uso |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 政策 | 描述 | 用例 |'
- en: '| --- | --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Primo entrato, primo uscito (FIFO) | Rimuove gli elementi più vecchi | Quando
    tutti gli elementi hanno la stessa priorità |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 首进先出 (FIFO) | 移除最旧的元素 | 当所有元素具有相同的优先级时 |'
- en: '| Ultimo usato (LRU) | Traccia l''utilizzo della cache nel tempo e rimuove
    l''ultimo elemento consultato. | Quando è più probabile che gli elementi consultati
    di recente vengano consultati nuovamente |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 最近最少使用 (LRU) | 跟踪缓存的使用情况，并移除最后访问的元素。 | 当最近访问的元素最有可能再次被访问时 |'
- en: '| Utilizzato meno frequentemente (LFU) | Traccia l''utilizzo della cache nel
    tempo e rimuove l''elemento a cui si accede meno frequentemente. | Quando gli
    articoli usati meno frequentemente devono essere rimossi per primi |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 使用频率最低 (LFU) | 跟踪缓存的使用情况，并移除访问频率最低的元素。 | 当使用频率最低的文章需要首先被移除时 |'
- en: '| Utilizzato più di recente (MRU) | Traccia l''utilizzo della cache nel tempo
    e rimuove l''ultimo elemento consultato. | Utilizzato raramente, quando gli articoli
    usati più di recente hanno meno probabilità di essere consultati di nuovo. |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 最近最少使用 (MRU) | 跟踪缓存的使用情况，并移除最后访问的元素。 | 使用频率低，当最近使用过的文章不太可能再次被访问时。 |'
- en: '| Sostituzione casuale (RR) | Rimuove un elemento casuale dalla cache | Semplice
    e veloce, da usare quando non ha un impatto sulle prestazioni |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 随机替换 (RR) | 从缓存中随机移除一个元素 | 简单快捷，当它不会影响性能时使用 |'
- en: La scelta del giusto criterio di sfratto dipende dal tuo caso d'uso e dai requisiti
    dell'applicazione. In generale, puoi iniziare con il criterio LRU prima di passare
    allealternative.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的驱逐策略取决于你的用例和应用程序的要求。一般来说，你可以从LRU策略开始，然后再考虑其他替代方案。
- en: Ora dovresti sentirti più sicuro nell'implementazione di meccanismi di caching
    semantico che si applicano al recupero di documenti o alle risposte dei modelli.
    Poi, impariamo a conoscere il caching contestuale o prompt, che ottimizza le query
    ai modelli in base ai loro input.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该在实现适用于文档检索或模型响应的语义缓存机制方面更有信心。然后，我们将了解上下文缓存或提示缓存，它根据模型的输入优化模型查询。
- en: Caching del contesto e del prompt
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 上下文和提示缓存
- en: Il*caching del contesto*, noto anche come *prompt caching*, è un meccanismo
    di caching adatto a scenari in cui si fa riferimento a grandi quantità di contesto
    ripetutamente all'interno di piccole richieste. È stato progettato per riutilizzare
    gli stati di attenzione precalcolati da prompt frequentemente riutilizzati, eliminando
    la necessità di ricompilare in modo ridondante l'intero contesto di input ogni
    volta che viene effettuata una nuova richiesta.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*上下文缓存*，也称为 *提示缓存*，是一种适合于在小型请求中反复引用大量上下文的缓存机制。它被设计用来重用频繁重用的提示预计算的注意力状态，从而消除每次进行新请求时重复编译整个输入上下文的必要性。'
- en: 'Dovresti prendere in considerazione l''utilizzo di una cache contestuale quando
    i tuoi servizi prevedono quanto segue:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的服务预计以下情况时，你应该考虑使用上下文相关的缓存：
- en: Chatbot con istruzioni di sistema estese e lunghe conversazioni a più turni
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有扩展系统指令和长轮询对话的聊天机器人
- en: Analisi ripetitiva di file video lunghi
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长视频文件的重复分析
- en: Query ricorrenti su set di documenti di grandi dimensioni
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对大型文档集的重复查询
- en: Analisi frequente del repository di codice o correzione di bug
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经常对代码库进行频率分析或修复错误
- en: Riassunti di documenti, libri, documenti, trascrizioni di podcast e altri contenuti
    di lunga durata.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档、书籍、录音、播客转录以及其他长期内容摘要。
- en: Fornire un gran numero di esempi in prompt (cioè l'apprendimento nel contesto).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示中提供大量示例（即上下文学习）。
- en: Secondo Anthropic, il caching dei prompt può ridurre i costi fino al 90% e la
    latenza fino all'85% per i prompt lunghi.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 Anthropic 的说法，提示缓存可以将成本降低至 90%，并将延迟降低至 85%，特别是对于较长的提示。
- en: 'Gli autori del [documento sul prompt caching](https://oreil.ly/augpd) che presenta
    questa tecnica affermano anche che:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 提出这种技术的 [提示缓存文档](https://oreil.ly/augpd) 的作者还声称：
- en: Abbiamo scoperto che la Prompt Cache riduce significativamente la latenza nel
    time-to-first-token, soprattutto per i prompt più lunghi come le risposte alle
    domande basate su documenti e le raccomandazioni. I miglioramenti vanno da 8×
    per l'inferenza basata su GPU a 60× per l'inferenza basata su CPU, il tutto mantenendo
    l'accuratezza dell'output e senza la necessità di modificare i parametri del modello.
  id: totrans-174
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我们发现，Prompt Cache 显著降低了首次令牌的延迟，尤其是在基于文档的问答和推荐等较长的提示中。改进从基于 GPU 的推理的 8 倍到基于 CPU
    的推理的 60 倍不等，同时保持了输出准确性，且无需修改模型参数。
- en: La[Figura 10-3](#context_caching_architecture) visualizza l'architettura del
    sistema di caching del contesto.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10-3 显示了上下文缓存系统的架构。
- en: '![bgai 1003](assets/bgai_1003.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 1003](assets/bgai_1003.png)'
- en: Figura 10-3\. Architettura del sistema per il caching del contesto
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-3\. 上下文缓存系统架构
- en: Al momento in cui scriviamo, OpenAI implementa automaticamente il prompt caching
    per tutte le richieste API senza richiedere modifiche al codice o costi aggiuntivi.
    L['esempio 10-12](#context_cachin_anthropic) mostra un esempio di utilizzo del
    prompt caching quando si utilizza l'API Anthropic.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们撰写本文时，OpenAI 自动为所有 API 请求实施提示缓存，无需对代码进行修改或额外成本。[示例 10-12](#context_cachin_anthropic)
    展示了在使用 Antropica API 时使用提示缓存的示例。
- en: Esempio 10-12\. Caching del contesto e del prompt con l'API Antropica
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-12\. 使用 Antropica API 缓存上下文和提示
- en: '[PRE13]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO8-1)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimizing_ai_services_CO8-1)'
- en: Il prompt caching è disponibile solo per alcuni modelli, tra cui Claude 3.5
    Sonnet, Claude 3 Haiku e Claude 3 Opus.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 提示缓存仅适用于某些模型，包括 Claude 3.5 Sonnet、Claude 3 Haiku 和 Claude 3 Opus。
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO8-2)'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_optimizing_ai_services_CO8-2)'
- en: Utilizza il parametro `cache_control` per riutilizzare il contenuto del documento
    di grandi dimensioni in più chiamate API senza elaborarlo ogni volta.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `cache_control` 参数，可以在多个 API 调用中重用大型文档的内容，而无需每次都进行处理。
- en: 'Sotto il cofano, il client Anthropic aggiunge `anthropic-beta: prompt-caching-2024-07-31`
    alle intestazioni delle richieste.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '在内部，Anthropic 客户端将 `anthropic-beta: prompt-caching-2024-07-31` 添加到请求的头部。'
- en: Al momento in cui scriviamo, `ephemeral` è l'unico tipo di cache supportato,
    che corrisponde a una durata della cache di 5 minuti.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们撰写本文时，`ephemeral` 是唯一支持的缓存类型，其对应于 5 分钟的缓存持续时间。
- en: Nota
  id: totrans-187
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Quando adotti una cache del contesto, introduci la statualità nelle richieste
    preservando i token in tutte le richieste. Ciò significa che i dati inviati in
    una richiesta influenzeranno le richieste successive, in quanto il server del
    provider del modello può utilizzare il contesto memorizzato nella cache per mantenere
    la continuità tra le interazioni.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当采用上下文缓存时，通过在所有请求中保留令牌来引入请求的稳定性。这意味着在一个请求中发送的数据将影响后续请求，因为模型提供者的服务器可以使用缓存的上下文来保持交互的连续性。
- en: Con la funzione di caching del contesto dell'API Gemini, puoi fornire il contenuto
    al modello una sola volta, mettere in cache i token di input e fare riferimento
    a questi token in cache per le richieste future.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 API Gemini 的上下文缓存功能，你可以一次性向模型提供内容，将输入令牌放入缓存，并在未来的请求中引用这些缓存中的令牌。
- en: L'uso di questi token nella cache può farti risparmiare una spesa significativa
    se eviti di passare ripetutamente lo stesso corpus di token in volumi elevati.Il
    costo della cache dipenderà dalle dimensioni dei token in ingresso e dalla durata
    di memorizzazione del time to live (TTL) desiderato.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在缓存中使用这些令牌可以显著节省费用，如果你避免重复发送相同的大量令牌语料库。缓存的成本将取决于输入令牌的大小和期望的生存时间（TTL）持续时间。
- en: Suggerimento
  id: totrans-191
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 建议
- en: Quando metti in cache un insieme di token, puoi specificare la durata del TTL,
    ossia il tempo in cui la cache deve esistere prima che i token vengano eliminati
    automaticamente. Per impostazione predefinita, il TTL è normalmente impostato
    su 1 ora.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当你缓存一组令牌时，你可以指定TTL（Time To Live，即缓存存在的时间，在此之后令牌将被自动删除）的持续时间。默认情况下，TTL通常设置为1小时。
- en: 'Puoi vedere come utilizzare un''istruzione di sistema nella cache nell''[Esempio
    10-13](#context_caching_google). Ti servirà anche il Gemini API Python SDK:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[Esempio 10-13](#context_caching_google)中看到如何使用系统指令在缓存中。你还需要Gemini API Python
    SDK：
- en: '[PRE14]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Esempio 10-13\. Caching del contesto con l'API Google Gemini
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 10-13示例。使用Google Gemini API进行上下文缓存
- en: '[PRE15]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO9-1)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_optimizing_ai_services_CO9-1)'
- en: Fornisci un nome visualizzato come chiave o identificatore della cache.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 提供一个可视名称作为缓存的关键字或标识符。
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO9-2)'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_optimizing_ai_services_CO9-2)'
- en: Passa il corpus al sistema di cache contestuale. La dimensione minima di una
    cache contestuale è di 32.768 token.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 将语料库传递给上下文缓存系统。上下文缓存的最小尺寸是32.768个令牌。
- en: 'Se esegui l''[Esempio 10-13](#context_caching_google) e stampi il sito `response.usage_metadata`,
    dovresti ricevere il seguente output:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你执行了[Esempio 10-13](#context_caching_google)并打印了`response.usage_metadata`网站，你应该会收到以下输出：
- en: '[PRE16]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Si noti come la maggior parte di `prompt_token_count` viene ora messa in cache
    se la si confronta con `cached_content_token_count`. `candidates_token_count`
    si riferisce al conteggio dei token di output o di risposta provenienti dal modello,
    che non viene influenzato dal sistema di cache.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，现在大部分`prompt_token_count`都被缓存了，如果与`cached_content_token_count`相比。`candidates_token_count`指的是来自模型的输出或响应令牌计数，它不受缓存系统的影响。
- en: Avvertenze
  id: totrans-204
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意事项
- en: I modelli Gemini non fanno distinzione tra i token memorizzati nella cache e
    i normali token di input. Il contenuto memorizzato nella cache viene anteposto
    al prompt. Per questo motivo il numero di token del prompt non viene ridotto quando
    si utilizza la cache.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Gemini模型不会区分缓存中存储的令牌和普通输入令牌。缓存中的内容会优先于提示。因此，当使用缓存时，提示的令牌数量不会减少。
- en: Con la cache del contesto, non vedrai una drastica riduzione dei tempi di risposta,
    ma ridurrai in modo significativo i costi operativi perché eviterai di inviare
    nuovamente prompt di sistema e token contestuali. Pertanto, questa strategia di
    cache è più adatta quando hai un contesto ampio su cui lavorare, ad esempio quando
    elabori file batch con istruzioni ed esempi completi.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 使用上下文缓存，你不会看到响应时间的显著减少，但你会显著降低运营成本，因为你可以避免重新发送系统提示和上下文令牌。因此，这种缓存策略更适合于你有广泛上下文的情况，例如当你处理带有完整指令和示例的批处理文件时。
- en: Nota
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: L'utilizzo della stessa cache di contesto e dello stesso prompt non garantisce
    risposte coerenti del modello perché le risposte degli LLMs non sono deterministiche.
    Una cache di contesto non memorizza alcun output.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的上下文缓存和提示并不能保证模型给出一致的回答，因为LLMs的回答不是确定性的。上下文缓存并不存储任何输出。
- en: Il context caching rimane un'area di ricerca attiva. Se vuoi evitare il vendor
    lock-in, ci sono già alcuni progressi in questo campo con strumenti open source
    come [*MemServe*](https://oreil.ly/PXm6B), che implementa il context caching con
    un pool di memoria elastica .
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文缓存仍然是一个活跃的研究领域。如果你想避免供应商锁定，在这个领域已经有一些进展，比如使用开源工具[*MemServe*](https://oreil.ly/PXm6B)，它通过一个弹性内存池实现了上下文缓存。
- en: Oltre alla cache, puoi anche esaminare le opzioni per ridurre le dimensioni
    del modello per accelerare i tempi di risposta utilizzando tecniche come la *quantizzazione
    del modello*.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 除了缓存之外，你还可以通过使用如*模型量化*等技术来减少模型尺寸，以加速响应时间。
- en: Quantizzazione del modello
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型量化
- en: Se hai intenzione di utilizzare tu stesso modelli come gli LLMs, dovresti prendere
    in considerazione l'idea di *quantizzare* (cioè comprimere/ridurre) i tuoi modelli,
    se possibile. Spesso, i repository di modelli open source forniscono anche versioni
    quantizzate che puoi scaricare e utilizzare immediatamente senza dover affrontare
    il processo di quantizzazione.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打算自己使用像LLMs这样的模型，你应该考虑将你的模型*量化*（即压缩/减少）的想法，如果可能的话。通常，开源模型仓库也提供了你可以下载并立即使用的量化版本，无需经历量化过程。
- en: La*quantizzazione del modello* è un processo di regolazione dei pesi e delle
    attivazioni del modello in cui i parametri del modello ad alta precisione vengono
    proiettati statisticamente in valori a bassa precisione attraverso un'operazione
    di regolazione fine che utilizza fattori di scala sulla distribuzione originale
    dei parametri. È quindi possibile eseguire tutte le operazioni di inferenza critiche
    con una precisione inferiore, dopodiché è possibile convertire gli output a una
    precisione superiore per mantenere la qualità e migliorare le prestazioni.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型量化*是一个调节模型权重和激活的过程，其中模型的高精度参数通过使用原始参数分布上的缩放因子进行的精细调节操作，统计地投影到低精度值。因此，可以使用较低的精度执行所有关键的推理操作，之后可以将输出转换回高精度以保持质量和提高性能。'
- en: Riducendo la precisione si riducono anche i requisiti di memoria, riducendo
    teoricamente il consumo energetico e velocizzando operazioni come la moltiplicazione
    di matrici attraverso l'aritmetica intera. Questo permette anche di eseguire i
    modelli su dispositivi embedded, che possono supportare solo tipi di dati interi.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 通过降低精度，也会减少内存需求，理论上降低能耗，并通过整数算术加速矩阵乘法等操作。这也使得模型可以在仅支持整数数据类型的嵌入式设备上运行。
- en: '[La Figura 10-4](#quantization_process) mostra il processo di quantizzazione
    completo.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-4](#quantization_process)展示了完整的量化过程。'
- en: '![bgai 1004](assets/bgai_1004.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 1004](assets/bgai_1004.png)'
- en: Figura 10-4\. Processo di quantizzazione
  id: totrans-217
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-4\. 量化过程
- en: È possibile risparmiare più di una manciata di gigabyte nel consumo di memoria
    della GPU, poiché i tipi di dati a bassa precisione come gli interi a 8 bit richiedono
    una quantità di RAM per parametro significativamente inferiore rispetto a un tipo
    di dati come i float a 32 bit.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 由于低精度数据类型如8位整数所需的RAM参数量远低于32位浮点数据类型，因此可以在GPU内存消耗上节省超过一打GB。
- en: Il compromesso tra precisione e qualità
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 精度和质量之间的折衷
- en: La[Figura 10-5](#quantization) mette a confronto un modello non quantizzato
    e un modello quantizzato.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[图10-5](#quantization)将未量化和量化的模型进行了对比。'
- en: '![bgai 1005](assets/bgai_1005.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 1005](assets/bgai_1005.png)'
- en: Figura 10-5\. Quantizzazione
  id: totrans-222
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-5\. 量化
- en: Poiché ogni parametro float a 32 bit ad alta precisione consuma 4 byte di memoria
    della GPU, un modello a 1B parametri richiederebbe 4 GB di memoria solo per l'inferenza.
    Se intendi riqualificare o perfezionare lo stesso modello, avrai bisogno di almeno
    24 GB di VRAM della GPU. Questo perché ogni parametro richiederebbe anche la memorizzazione
    di informazioni come i gradienti, gli stati dell'ottimizzatore dell'addestramento,
    le attivazioni e lo spazio di memoria temporaneo, consumando complessivamente
    altri 24 byte. Questo stima fino a 6 volte il fabbisogno di memoria rispetto al
    solo caricamento dei pesi del modello. Lo stesso modello 1B richiederebbe quindi
    una GPU da 24 GB, che le migliori e più costose schede grafiche consumer come
    la NVIDIA RTX 4090 potrebbero ancora faticare a soddisfare.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个32位高精度浮点参数消耗4个字节的GPU内存，一个包含10亿个参数的模型仅用于推理就需要4GB的内存。如果你打算重新训练或优化同一个模型，你至少需要24GB的GPU
    VRAM。这是因为每个参数还需要存储梯度、训练优化器的状态、激活和临时内存空间等信息，总共消耗额外的24字节。这估计比仅加载模型权重所需的内存需求高出6倍。因此，同样的10亿参数模型需要24GB的GPU，即使是最好的、最昂贵的显卡如NVIDIA
    RTX 4090也可能难以满足。
- en: 'Invece di utilizzare il formato standard 32-float, puoi selezionare uno dei
    seguenti formati:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用标准的32位浮点格式相比，你可以选择以下格式之一：
- en: La*virgola mobile a 16 bit (FP16) di* mezza l'utilizzo della memoria senza compromettere
    la qualità del modello.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*16位浮点（FP16）*在不过度牺牲模型质量的情况下，仅使用一半的内存。'
- en: Gli*interi a 8 bit (INT8)* offrono un enorme risparmio di memoria, ma con una
    significativa perdita di qualità.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*8位整数（INT8）*可以大幅节省内存，但会显著降低质量。'
- en: Il*brain floating-point a 16 bit (BFLOAT16)* con un intervallo simile all'FP32
    bilancia il compromesso tra memoria e qualità.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*16位半精度浮点（BFLOAT16）*与FP32具有相似的区间，平衡了内存和质量的折衷。'
- en: Il*numero intero a 4 bit (INT4)* offre un equilibrio tra l'efficienza della
    memoria e la precisione di calcolo, rendendolo adatto ai dispositivi a basso consumo.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*4位整数（INT4）*在内存效率和计算精度之间提供了平衡，使其适用于低功耗设备。'
- en: L'*intero a 1 bit (INT1)* utilizza il tipo di dati a più bassa precisione con
    la massima riduzione delle dimensioni del modello. La ricerca per la creazione
    di [LLMs a 1 bit](https://oreil.ly/QH9nH) di alta qualità è attualmente in corso.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*1位整数（INT1）* 使用最低精度的数据类型，具有最大的模型尺寸缩减。目前正在进行创建[1位LLMs](https://oreil.ly/QH9nH)的高质量研究。'
- en: A titolo di confronto, la [Tabella 10-2](#quantization_comparison) mostra la
    riduzione delle dimensioni del modello quando si quantizzano i modelli della famiglia
    Llama.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 作为对比，[表10-2](#quantization_comparison) 展示了在量化 Llama 家族模型时模型尺寸的减少。
- en: Tabella 10-2\. Impatto della quantizzazione sulle dimensioni dei modelli di
    Llama^([a](ch10.html#id1097))
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 表10-2\. 量化对Llama模型尺寸的影响^([a](ch10.html#id1097))
- en: '| Modello | Originale | FP16 | 8 Bit | 6 Bit | 4 Bit | 2Bit |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 原始 | FP16 | 8位 | 6位 | 4位 | 2位 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Llama 2 70B | 140 GB | 128,5 GB | 73,23 GB | 52,70 GB | 36,20 GB | 28,59
    GB |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Llama 2 70B | 140 GB | 128,5 GB | 73,23 GB | 52,70 GB | 36,20 GB | 28,59
    GB |'
- en: '| Llama 3 8B | 16,07 GB | 14,97 GB | 7,96 GB | 4,34 GB | 4,34 GB | 2,96 GB
    |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Llama 3 8B | 16,07 GB | 14,97 GB | 7,96 GB | 4,34 GB | 4,34 GB | 2,96 GB
    |'
- en: '| ^([a](ch10.html#id1097-marker)) Fonti: [Llama.cpp repository GitHub](https://oreil.ly/9iYtL)
    e la [scheda modello Hugging Face Llama 2 70B di Tom Jobbins](https://oreil.ly/BMDtR)
    |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch10.html#id1097-marker)) 来源：[Llama.cpp GitHub仓库](https://oreil.ly/9iYtL)
    和 [Tom Jobbins的Hugging Face Llama 2 70B模型卡](https://oreil.ly/BMDtR) |'
- en: Suggerimento
  id: totrans-237
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 建议
- en: Oltre alla VRAM della GPU necessaria per montare il modello, avrai bisogno di
    altri 5-8 GB di VRAM della GPU per l'overhead durante il caricamento del modello.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于加载模型的GPU VRAM之外，你还需要额外的5-8 GB GPU VRAM来处理模型加载过程中的开销。
- en: Allo stato attuale della ricerca, mantenere l'accuratezza con i tipi di dati
    INT4 e INT1 solo interi è una sfida e il miglioramento delle prestazioni con INT32
    o FP16 non è significativo. Pertanto, il tipo di dati a bassa precisione più diffuso
    è INT8 per l'inferenza.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在当前的研究阶段，仅使用INT4和INT1整数类型保持精度是一个挑战，而使用INT32或FP16提高性能并不显著。因此，最常用的低精度数据类型是INT8用于推理。
- en: 'Secondo la [ricerca](https://oreil.ly/C7Lz3), l''utilizzo dell''aritmetica
    dei soli numeri interi per l''inferenza sarà più efficiente rispetto ai numeri
    a virgola mobile. Tuttavia, quantificare i numeri a virgola mobile in numeri interi
    può essere complicato: ad esempio, solo 256 valori possono essere rappresentati
    in INT8, mentre float32 può rappresentare un''ampia gamma di valori.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 根据研究[链接](https://oreil.ly/C7Lz3)，仅使用整数算术进行推理将比使用浮点数更高效。然而，将浮点数量化为整数可能会很复杂：例如，INT8只能表示256个值，而float32可以表示广泛的值。
- en: Numeri in virgola mobile
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 浮点数
- en: Per capire perché proiettare i float a 32 bit in altri formati farebbe risparmiare
    così tanto in termini di memoria della GPU, analizziamo come si articola il tutto.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解为什么将浮点数投影到32位其他格式会在GPU内存方面节省这么多，我们来分析一下整个过程。
- en: 'Un numero in virgola mobile a 32 bit è composto dai seguenti tipi di bit:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 一个32位浮点数由以下类型的位组成：
- en: Bit di*segno* che descrive se un numero è positivo o negativo
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*符号*位描述一个数字是正数还是负数'
- en: Bit dell*'esponente* che controllano la scala del numero
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*指数*位控制数字的规模'
- en: Bit di*mantissa* che contengono le cifre effettive che determinano la sua precisione
    (noti anche come bit *di frazione* )
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*尾数*位包含确定其精度的实际数字（也称为分数位）'
- en: Nella [Figura 10-6](#quantization_bits) puoi vedere una visualizzazione dei
    bit nei numeri in virgola mobile sopra citati.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图10-6](#quantization_bits)中，你可以看到上述浮点数中的位表示。
- en: '![bgai 1006](assets/bgai_1006.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 1006](assets/bgai_1006.png)'
- en: Figura 10-6\. Bit dei numeri float a 32 bit, float a 16 bit e bfloat16
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-6\. 32位浮点数、16位浮点数和bfloat16的位
- en: Quando proietti un numero FP32 in altri formati, in effetti, lo schiacci in
    intervalli più piccoli, perdendo la maggior parte dei bit della mantissa e adattando
    i bit dell'esponente, ma senza perdere gran parte della precisione. Puoi vedere
    questo fenomeno in azione facendo riferimento alla [Figura 10-7](#quantization_floating_numbers).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 当将FP32数投影到其他格式时，实际上是将它们压缩到更小的区间，丢失了大部分尾数位，并调整了指数位，但并没有丢失大部分精度。你可以通过参考[图10-7](#quantization_floating_numbers)来观察这一现象。
- en: '![bgai 1007](assets/bgai_1007.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 1007](assets/bgai_1007.png)'
- en: Figura 10-7\. Quantizzazione di numeri in virgola mobile in numeri interi
  id: totrans-252
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-7\. 浮点数到整数的量化
- en: In effetti, la [ricerca sulle strategie di quantizzazione per i modelli LLM
    pre-addestrati](https://oreil.ly/Swfz7) ha dimostrato che gli LLM con quantizzazione
    a 4 bit possono mantenere prestazioni simili alle loro controparti non quantizzate.
    Tuttavia, se da un lato la quantizzazione consente di risparmiare memoria, dall'altro
    può ridurre la velocità di inferenza degli LLM.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，[关于预训练LLM量化策略的研究](https://oreil.ly/Swfz7)表明，量化到4位的LLMs可以保持与未量化版本相似的性能。然而，一方面量化可以节省内存，另一方面可能会降低LLMs的推理速度。
- en: Come quantizzare le LLMs preaddestrate
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何量化预训练的LLMs
- en: Una di queste tecniche, chiamata [*GPTQ*](https://oreil.ly/rHYKZ), è in grado
    di quantizzare LLMs con 175 miliardi di parametri in circa 4 ore di GPU, riducendo
    la larghezza dei bit a 3 o 4 bit per peso, con un calo di precisione trascurabile
    rispetto al modello non compresso.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种技术，称为[*GPTQ*](https://oreil.ly/rHYKZ)，能够在大约4小时的GPU时间内量化具有1750亿参数的LLMs，将权重位宽降低到3或4位，与未压缩模型相比，精度损失可以忽略不计。
- en: Gli autori delle librerie Hugging Face `transformers` e `optimum` hanno collaborato
    strettamente con gli sviluppatori della libreria `auto-gptq` per fornire una semplice
    API per applicare la quantizzazione GPTQ su LLMs open source. Optimum è una libreria
    che fornisce API per eseguire la quantizzazione utilizzando diversi strumenti.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face的`transformers`和`optimum`库的作者与`auto-gptq`库的开发者紧密合作，提供了一个简单的API来在LLMs开源项目中应用GPTQ量化。Optimum是一个提供API以使用不同工具执行量化的库。
- en: Con la quantizzazione GPTQ, puoi quantizzare il tuo modello linguistico preferito
    a 8, 4, 3 o addirittura 2 bit senza un grosso calo di prestazioni, mantenendo
    una velocità di inferenza superiore a quella supportata dalla maggior parte dell'hardware
    GPT. Puoi seguire l'[Esempio 10-14](#gptq_quantization) per quantizzare un modello
    pre-addestrato sulla tua GPU.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPTQ量化，你可以将你偏好的语言模型量化到8、4、3甚至2位，而不会大幅降低性能，同时保持比大多数GPT硬件支持的更高的推理速度。你可以遵循[Esempio
    10-14](#gptq_quantization)来在你的GPU上量化预训练模型。
- en: 'Le dipendenze che devi installare per eseguire l''[Esempio 10-14](#gptq_quantization)
    includonole seguenti:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行[Esempio 10-14](#gptq_quantization)示例，你需要安装以下依赖项：
- en: '[PRE17]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Esempio 10-14\. Quantizzazione del modello GPTQ con le librerie Hugging Face
    e AutoGPTQ
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Esempio 10-14. 使用Hugging Face和AutoGPTQ库量化GPTQ模型
- en: '[PRE18]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO10-1)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_optimizing_ai_services_CO10-1)'
- en: Carica la versione `float16` del modello preaddestrato `facebook/opt-125m` prima
    della quantizzazione.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在量化之前，加载预训练模型`facebook/opt-125m`的`float16`版本。
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO10-2)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)(#co_optimizing_ai_services_CO10-2)'
- en: Usa il dataset `c4` per calibrare la quantizzazione.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据集`c4`来校准量化。
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO10-3)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)(#co_optimizing_ai_services_CO10-3)'
- en: Quantizza solo i blocchi del livello di decodifica del modello.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 只量化模型的解码层块。
- en: '[![4](assets/4.png)](#co_optimizing_ai_services_CO10-4)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)(#co_optimizing_ai_services_CO10-4)'
- en: Usa la lunghezza della sequenza modello di `2048` per elaborare il set di dati.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`2048`长度的模型序列来处理数据集。
- en: Suggerimento
  id: totrans-270
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 建议
- en: Come riferimento, un modello 175B richiederà 4 ore di GPU su NVIDIA A100 per
    essere quantizzato. Tuttavia, vale la pena cercare nel repository dei modelli
    di Hugging Face i modelli prequantizzati, perché potresti scoprire che qualcuno
    ha già fatto il lavoro .
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 作为参考，一个175B的模型需要4小时的GPU时间在NVIDIA A100上进行量化。然而，值得在Hugging Face模型库中寻找预量化模型，因为你可能发现有人已经完成了这项工作。
- en: Ora che hai compreso le tecniche di ottimizzazione delle prestazioni, vediamo
    come migliorare la qualità dei tuoi servizi GenAI utilizzando metodi come gli
    output strutturati.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了性能优化的技术，让我们看看如何通过使用结构化输出等方法来提高你的GenAI服务的质量。
- en: Uscite strutturate
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构化输出
- en: I modelli fondamentali come gli LLMs possono essere utilizzati come componenti
    di una pipeline di dati o collegati ad applicazioni a valle. Ad esempio, puoi
    usare questi modelli per estrarre e analizzare informazioni da documenti o per
    generare codice che può essere eseguito su altri sistemi.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 基本模型，如LLMs，可以用作数据管道的组件或连接到下游应用。例如，你可以使用这些模型从文档中提取和分析信息，或者生成可以在其他系统上运行的代码。
- en: Puoi chiedere a LLM di fornire una risposta testuale contenente informazioni
    JSON. Dovrai quindi estrarre e analizzare questa stringa JSON utilizzando strumenti
    come regex e Pydantic. Tuttavia, non c'è alcuna garanzia che il modello si attenga
    sempre alle tue istruzioni. Poiché i tuoi sistemi a valle potrebbero basarsi su
    output JSON, potrebbero lanciare eccezioni e gestire in modo errato input non
    validi.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以要求LLM提供一个包含JSON信息的文本响应。然后，你需要使用正则表达式和Pydantic等工具提取和分析这个JSON字符串。然而，没有保证模型始终遵循你的指令。由于你的下游系统可能基于JSON输出，它们可能会抛出异常并错误地处理无效输入。
- en: Sono stati rilasciati diversi pacchetti di utilità come Instructor per migliorare
    la robustezza delle risposte LLM, prendendo uno schema ed effettuando diverse
    chiamate API con vari modelli di prompt per raggiungere l'output desiderato. Se
    da un lato queste soluzioni migliorano la robustezza, dall'altro aggiungono costi
    significativi alla tua soluzione a causa delle successive chiamate API ai fornitori
    di modelli.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 已发布了一些如Instructor之类的实用工具包，以提高LLM响应的鲁棒性，通过采用一个架构并使用不同的提示模型进行多次API调用以实现所需的输出。虽然这些解决方案提高了鲁棒性，但它们也由于后续对模型提供商的API调用而增加了你的解决方案的成本。
- en: Recentemente, i fornitori di modelli hanno aggiunto una funzione per richiedere
    output strutturati fornendo schemi quando si effettuano chiamate API al modello,
    come puoi vedere nell'[Esempio 10-15](#structured_outputs). Questo aiuta a ridurre
    il lavoro di template del prompt che devi fare tu stesso e mira a migliorare l'*allineamento*
    del modello alle tue intenzioni quando restituisce una risposta.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，模型提供商增加了一个功能，允许在调用模型API时请求结构化输出，并提供架构，如[Esempio 10-15](#structured_outputs)所示。这有助于减少你需要自己完成的提示模板工作，并旨在提高模型在返回响应时与你的意图的*对齐*。
- en: Avvertenze
  id: totrans-278
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意事项
- en: Al momento in cui scriviamo, solo il più recente OpenAI SDK supporta i modelli
    Pydantic per abilitare gli output strutturati.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们撰写本文时，只有最新的OpenAI SDK支持Pydantic模型以启用结构化输出。
- en: Esempio 10-15\. Uscite strutturate
  id: totrans-280
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-15\. 结构化输出
- en: '[PRE19]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO11-1)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)'
- en: Specifica un modello Pydantic per gli output strutturati.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 指定用于结构化输出的Pydantic模型。
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO11-2)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)'
- en: Fornisce lo schema definito al client del modello quando effettua la chiamata
    API.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用API时向模型客户端提供定义的架构。
- en: Se il tuo fornitore di modelli non supporta in modo nativo gli output strutturati,
    puoi comunque sfruttare le funzionalità di completamento delle chat del modello
    per aumentare la robustezza degli output strutturati, come mostrato nell'[Esempio
    10-16](#structured_outputs_completions).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型提供商不支持原生结构化输出，你仍然可以利用模型的聊天补全功能来提高结构化输出的鲁棒性，如[Esempio 10-16](#structured_outputs_completions)所示。
- en: Esempio 10-16\. Uscite strutturate basate sul prefill delle chat
  id: totrans-287
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-16\. 基于预填充聊天的结构化输出
- en: '[PRE20]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[![1](assets/1.png)](#co_optimizing_ai_services_CO12-1)'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)'
- en: Limitare i token di output per migliorare la robustezza e la velocità delle
    risposte strutturate e per ridurre i costi.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 限制输出token以提高结构化响应的鲁棒性和速度，并降低成本。
- en: '[![2](assets/2.png)](#co_optimizing_ai_services_CO12-2)'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)'
- en: Salta il preambolo e restituisce direttamente un JSON precompilando la risposta
    dell'assistente e includendo il carattere `{`.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 跳过前言并直接返回预编译的JSON，通过包含字符`{`来返回助手的响应。
- en: '[![3](assets/3.png)](#co_optimizing_ai_services_CO12-3)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)'
- en: Aggiungiamo di nuovo il prefiltrato `{` e poi troviamo la chiusura `}` ed estraiamo
    lasottostringa JSON.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 再次添加预过滤的`{`，然后找到关闭的`}`并提取相应的JSON字符串。
- en: '[![4](assets/4.png)](#co_optimizing_ai_services_CO12-4)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)'
- en: Gestisce i casi in cui non c'è JSON nella risposta, ad esempio se c'è un rifiuto.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 处理响应中不存在JSON的情况，例如，如果发生拒绝。
- en: Seguire le tecniche sopra descritte dovrebbe aiutarti a migliorare la robustezza
    delle tue pipeline di dati se queste utilizzano gli LLMs come componenti.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循上述描述的技术应该有助于提高你的数据管道的鲁棒性，如果这些管道使用LLMs作为组件的话。
- en: Ingegneria prompt
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ingegneria prompt
- en: L'ingegneria dei prompt è la pratica di creare e perfezionare le query per i
    modelli generativi in modo da produrre i risultati più utili e ottimizzati. Senza
    perfezionare i prompt, dovresti perfezionare i modelli o addestrare un modello
    da zero per ottimizzare la qualità dei risultati.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程是创建和优化生成模型查询的实践，以产生最有用和优化的结果。如果不完善提示，你应该完善模型或从头开始训练一个模型以优化结果质量。
- en: Molti sostengono che questo campo non abbia il rigore scientifico necessario
    per essere considerato una disciplina ingegneristica. Tuttavia, è possibile affrontare
    il problema da una prospettiva ingegneristica quando si affinano i prompt per
    ottenere la migliore qualità di output dai modelli.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人都认为这个领域缺乏必要的科学严谨性，不能被视为一个工程学科。然而，当优化提示以获得最佳输出质量时，可以从工程角度来处理这个问题。
- en: In modo simile a come comunichi con gli altri per ottenere le cose, con i prompt
    ottimizzati puoi comunicare in modo più efficace le tue intenzioni al modello
    per aumentare le possibilità di ottenere le risposte che desideri. Pertanto, il
    prompt non diventa solo un problema di ingegneria ma anche di comunicazione. Un
    modello può essere paragonato a un collega esperto con molta esperienza ma con
    una conoscenza limitata del dominio, pronto ad aiutarti ma che ha bisogno che
    tu fornisca istruzioni ben documentate, possibilmente con alcuni esempi da seguire
    e da abbinare.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 就像你与他人沟通以获得所需事物一样，通过优化的提示，你可以更有效地向模型传达你的意图，从而增加获得期望回答的可能性。因此，提示不仅是一个工程问题，也是一个沟通问题。一个模型可以比作一个经验丰富的同事，但对该领域的知识有限，愿意帮助你，但需要你提供详细的指令，可能还需要一些示例来遵循和匹配。
- en: Se i tuoi prompt sono vaghi e generici, otterrai anche una risposta media.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的提示模糊且通用，你也会得到一个平均水平的回答。
- en: Un altro modo di pensare a questo problema di ottimizzazione è quello di paragonare
    l'attività di prompt del modello alla programmazione. Invece di scrivere il codice
    da solo, stai effettivamente "codificando" un modello in modo che sia un componente
    ben integrato di un'applicazione più grande o di una pipeline di dati. Puoi adottare
    approcci di sviluppo test-driven (TDD) e perfezionare i prompt fino a quando i
    test non passano. Oppure, sperimentare diversi modelli per vedere quale *allinea*
    meglio i suoi output alle tue intenzioni.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考这个优化问题的方式是将模型的提示活动比作编程。你实际上是在“编码”一个模型，使其成为更大应用程序或数据管道中一个良好集成的组件。你可以采用测试驱动开发（TDD）的方法，直到测试通过为止来完善提示。或者，你可以尝试不同的模型，看看哪个模型更好地*对齐*其输出与你的意图。
- en: Nota
  id: totrans-304
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: La massimizzazione dell'*allineamento* dei modelli rimane un obiettivo prioritario
    per molti fornitori di modelli, in modo che i loro risultati soddisfino al meglio
    l'intento dell'utente.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多模型提供商来说，最大化模型的*一致性*仍然是首要目标，以确保他们的结果能够最好地满足用户意图。
- en: Modelli di prompt
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示模型
- en: Se le istruzioni del sistema non sono metodiche, chiare e non seguono le migliori
    pratiche di prompt, potresti lasciare sul tavolo potenziali ottimizzazioni della
    qualità e delle prestazioni.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 如果系统的指令不系统、不清晰且不遵循最佳提示实践，你可能会错失潜在的质量和性能优化。
- en: 'Come minimo, dovresti avere dei prompt chiari che forniscano compiti specifici
    al modello. La pratica migliore è quella di seguire un modello sistematico.Per
    esempio, redigi le istruzioni del modello seguendo il modello di *ruolo, contesto
    e compito* (RCT):'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，你应该有一些清晰的提示，为模型提供具体的任务。最佳实践是遵循一个系统性的方法。例如，你可以按照*角色、情境和任务*（RCT）模型来编写模型的指令：
- en: Ruolo
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 角色
- en: La ricerca ha dimostrato che specificare i ruoli per i LLMs tende a influenzare
    in modo significativo i loro risultati. Ad esempio, un modello potrebbe essere
    più indulgente nella valutazione di un saggio se gli dai il ruolo di un insegnante
    di scuola elementare. Senza un ruolo specifico, il modello potrebbe presumere
    che tu voglia che la valutazione segua gli standard accademici di livello universitario.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 研究表明，为大型语言模型（LLMs）指定角色往往会显著影响其结果。例如，如果你让一个模型扮演小学教师的角色，它可能在评估论文时更加宽容。如果没有指定具体的角色，模型可能会假设你希望评估遵循大学学术水平的标准。
- en: Nota
  id: totrans-311
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Puoi ampliare ulteriormente il ruolo del modello e descrivere in modo dettagliato
    una *persona* che il modello dovrà adottare. Utilizzando una persona, il modello
    saprà esattamente come comportarsi e fare previsioni perché avrà un contesto più
    ampio su ciò che il ruolo dovrebbe comportare.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以进一步扩展模型的角色，并详细描述模型需要扮演的*人物*。通过使用人物，模型将确切知道如何表现和做出预测，因为它将拥有更广泛的关于该角色应该表现为何种行为的上下文。
- en: Contesto
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文
- en: Definisce lo scenario, dipinge il quadro e fornisce tutte le informazioni rilevanti
    e utili che il modello può utilizzare come riferimento per fare previsioni. Senza
    un contesto esplicito, il modello può utilizzare solo un contesto implicito che
    conterrà informazioni medie dei suoi dati di addestramento. In un'applicazione
    RAG, il contesto potrebbe essere la concatenazione del prompt del sistema con
    i pezzi di documenti recuperati da un archivio di conoscenza.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 定义场景，描绘画面，并提供所有模型可以使用的相关和有用信息作为预测的参考。如果没有明确的上下文，模型只能使用隐含的上下文，其中包含其训练数据的平均信息。在一个RAG应用中，上下文可能是系统提示和从知识库中检索到的文档片段的串联。
- en: Compito
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 任务
- en: Quando descrivi il compito, assicurati di pensare al modello come a un apprendista
    brillante e preparato, pronto a entrare in azione ma che ha bisogno di istruzioni
    molto chiare e non ambigue da seguire, potenzialmente con una manciata di esempi.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 当描述任务时，确保将模型视为一个聪明且准备充分的学习者，随时准备行动，但需要非常清晰且不含糊的指示，可能还需要一些示例。
- en: Seguendo il modello di sistema sopra descritto, dovresti migliorare la qualità
    dei risultati del tuo modello con il minimo sforzo.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 按照上述系统模型进行操作，你应该能够以最小的努力提高你模型的结果质量。
- en: Tecniche di prompt avanzate
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高级提示技术
- en: 'Oltre ai principi fondamentali del prompt, puoi utilizzare tecniche più avanzate
    che potrebbero adattarsi meglio al tuo caso d''uso. In base a una [recente indagine
    sistematica sulle tecniche di prompt](https://oreil.ly/xynPC), puoi raggruppare
    i prompt di LLM nei seguenti tipi:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基本提示原则之外，你可以使用更高级的技术，这些技术可能更适合你的用例。根据一项关于提示技术的[最近系统调查](https://oreil.ly/xynPC)，你可以将LLM的提示分为以下几类：
- en: Apprendimento in contesto
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文学习
- en: Generazione di pensieri
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思维生成
- en: Decomposizione
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分解
- en: Assemblaggio
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组装
- en: Autocritica
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自我批评
- en: Agente
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理
- en: Esaminiamo ciascuna di esse in modo più dettagliato.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更详细地审视每一种。
- en: Apprendimento in contesto
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 上下文学习
- en: Ciò che distingue i modelli fondamentali come gli LLMs dai tradizionali modelli
    di apprendimento automatico è la loro capacità di rispondere a input dinamici
    senza la necessità costante di una messa a punto o di una riqualificazione.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 区分像LLMs这样的基本模型和传统机器学习模型的是它们能够对动态输入做出响应，而无需不断的调整或再训练。
- en: Quando fornisci delle istruzioni di sistema a un LLM, puoi fornire anche diversi
    esempi (ad esempio, delle inquadrature) per guidare la generazione dell'output.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 当向LLM提供系统指令时，你也可以提供不同的示例（例如，框架）来引导输出生成。
- en: Il[*prompt a zero colpi*](https://oreil.ly/3F4wb) si riferisce a un approccio
    di prompt che non specifica esempi di riferimento, ma il modello può comunque
    completare con successo il compito dato.Se il modello ha difficoltà senza esempi
    di riferimento, potresti dover usare il [*prompt a pochi colpi*](https://oreil.ly/pOSj8),
    in cui fornisci una manciata di esempi.Ci sono anche casi d'uso in cui vuoi usare
    il prompt *dinamico a pochi colpi*, in cui inserisci dinamicamente gli esempi
    dai dati recuperati da un database o da un archivio vettoriale.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[零样本提示](https://oreil.ly/3F4wb)指的是一种提示方法，它不指定参考示例，但模型仍然可以成功完成给定的任务。如果模型在没有参考示例的情况下遇到困难，你可能需要使用[少量样本提示](https://oreil.ly/pOSj8)，其中你提供一些示例。也有使用*动态少量样本提示*的情况，其中你动态地从数据库或向量库中检索数据来插入示例。'
- en: Gli approcci basati sul prompt, in cui si specificano gli esempi, sono definiti
    anche *apprendimento in contesto*. In effetti, stai mettendo a punto i risultati
    del modello in base ai tuoi esempi e al compito dato senza modificare effettivamente
    i pesi/parametri del modello, mentre altri modelli di ML richiederebbero la modifica
    dei loro pesi.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 基于提示的方法，其中指定了示例，也被定义为*上下文学习*。实际上，你正在根据你的示例和给定的任务调整模型的结果，而不实际修改模型的权重/参数，而其他ML模型则可能需要修改它们的权重。
- en: Questo è ciò che rende gli LLMs e i modelli fondazionali così potenti, in quanto
    non richiedono sempre una regolazione del peso per adattarsi ai dati e ai compiti
    che gli vengono assegnati. Puoi conoscere diverse tecniche di apprendimento in
    contesto facendo riferimento alla [Tabella 10-3](#prompting_techniques_incontext_learning).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么LLMs和基础模型如此强大的原因，因为它们不需要总是调整权重来适应分配给它们的数据和任务。你可以通过参考[表10-3](#prompting_techniques_incontext_learning)了解不同的上下文学习技术。
- en: Tabella 10-3\. Tecniche di prompt per l'apprendimento contestuale
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 表10-3. 上下文学习提示技术
- en: '| Tecnica del prompt | Esempi | Casi d''uso |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 提示技术 | 示例 | 用例 |'
- en: '| --- | --- | --- |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Colpo zero | Riassumi i seguenti punti... | Riassunto, domande e risposte
    senza esempi di formazione specifici |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 零次打击 | 概括以下要点... | 概括、问答，没有特定训练示例的问题和答案 |'
- en: '| Pochi colpi | Classifica i documenti in base agli esempi riportati di seguito:[Esempi]
    | Classificazione del testo, analisi del sentiment, estrazione dei dati con alcuni
    esempi |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 少量打击 | 根据以下示例对文档进行分类:[示例] | 文本分类，情感分析，带示例的数据提取 |'
- en: '| Scatti dinamici di pochi secondi | Classifica i seguenti documenti in base
    agli esempi riportati di seguito:<Inietta esempi da un archivio di vettori basato
    su una query>. | Risposte personalizzate, risoluzione di problemi complessi |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 动态短片段 | 根据以下示例对以下文档进行分类:<从基于查询的向量库中注入示例>. | 定制化回答，解决复杂问题 |'
- en: I prompt di apprendimento contestuali sono semplici, efficaci e rappresentano
    un ottimo punto di partenza per completare una serie di compiti. Per i compiti
    più complessi, puoi utilizzare approcci di prompt più avanzati come la generazione
    del pensiero, la scomposizione, l'assemblaggio, l'autocritica o gli approcci agici.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文学习提示简单、有效，是完成一系列任务的绝佳起点。对于更复杂的任务，你可以使用更高级的提示方法，如思维生成、分解、组装、自我批评或敏捷方法。
- en: Generazione di pensieri
  id: totrans-340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 思维生成
- en: Le tecniche di generazione del pensiero, come la [catena del pensiero (CoT),](https://oreil.ly/BWUYQ)
    hanno dimostrato di migliorare significativamente la capacità delle LLMs di eseguire
    ragionamenti complessi.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 思维生成技术，如[思维链（CoT）](https://oreil.ly/BWUYQ)，已被证明可以显著提高LLMs执行复杂推理的能力。
- en: Nel prompt COT si chiede al modello di spiegare il suo processo di pensiero
    e il suo ragionamento mentre fornisce una risposta. Le varianti della CoT includono
    la [CoT](https://oreil.ly/1gjSH) a zero o a [pochi colpi](https://oreil.ly/1gjSH),
    a seconda che si forniscano o meno degli esempi. Una tecnica di generazione del
    pensiero più avanzata è il [thread of thought (ThoT)](https://oreil.ly/1KyO4)
    che segmenta e analizza sistematicamente informazioni o compiti caotici e molto
    complessi.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 在COT提示中，要求模型在提供答案时解释其思维过程和推理。CoT的变体包括[零次CoT](https://oreil.ly/1gjSH)或[少量打击](https://oreil.ly/1gjSH)，具体取决于是否提供示例。一种更高级的思维生成技术是[思维线程（ThoT）](https://oreil.ly/1KyO4)，它系统地分割和分析混乱且非常复杂的信息或任务。
- en: La[Tabella 10-4](#prompting_techniques_thought_generation) elenca le tecniche
    di generazione del pensiero.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '[表10-4](#prompting_techniques_thought_generation)列出了思维生成技术。'
- en: Tabella 10-4\. Tecniche di prompt per la generazione del pensiero
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 表10-4. 思维生成提示技术
- en: '| Tecnica del prompt | Esempi | Casi d''uso |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 提示技术 | 示例 | 用例 |'
- en: '| --- | --- | --- |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Catena di pensiero a colpo zero (CoT) | Pensiamo passo dopo passo... | Risoluzione
    di problemi matematici, ragionamento logico e processo decisionale in più fasi.
    |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 零次打击思维链（CoT） | 逐步思考... | 解决数学问题、逻辑推理和多阶段决策过程。 |'
- en: '| CoT a pochi colpi | Pensiamo passo dopo passo... Ecco alcuni esempi:[ESEMPI]
    | Scenari in cui alcuni esempi possono guidare il modello verso prestazioni migliori,
    come la classificazione di testi sfumati, la risposta a domande complesse e i
    prompt di scrittura creativa. |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 零次打击CoT | 逐步思考...以下是一些示例:[示例] | 一些示例可以引导模型达到更好性能的场景，如模糊文本分类、复杂问题的回答和创意写作提示。
    |'
- en: '| Filo del pensiero (ThoT) | Esamina il problema in parti gestibili, passo
    dopo passo, riassumendo e analizzando man mano... | Mantenere il contesto nel
    corso di interazioni multiple, come i sistemi di dialogo, la narrazione interattiva
    e la generazione di contenuti di lunga durata. |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 思维线索 (ThoT) | 将问题分解为可管理的部分，逐步总结和分析... | 在多次交互中保持上下文，如对话系统、交互式叙事和长期内容生成。 |'
- en: Decomposizione
  id: totrans-350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 分解
- en: Le tecniche di prompt a scomposizione si concentrano sulla suddivisione di compiti
    complessi in sottocompiti più piccoli, in modo che il modello possa affrontarli
    passo dopo passo e in modo logico. Puoi sperimentare questi approcci insieme alla
    generazione del pensiero per identificare quelli che producono i risultati migliori
    per il tuo caso d'uso.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 分解提示技术集中在将复杂任务分解为更小的子任务上，以便模型可以逐步、逻辑地处理它们。你可以与思维生成一起实验这些方法，以确定哪些方法对你的用例产生最佳结果。
- en: 'Queste sono le tecniche di prompt di scomposizione più comuni:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是最常见的分解提示技术：
- en: '[Da meno a più](https://oreil.ly/HmsSN)'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '[从少到多](https://oreil.ly/HmsSN)'
- en: Chiedi al modello di suddividere un problema complesso in problemi più piccoli
    tramite una riduzione logica, senza risolverli. Puoi poi chiedere al modello di
    risolvere ogni compito uno per uno.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 要求模型通过逻辑减少将复杂问题分解为更小的问题，而不解决它们。然后，你可以要求模型逐个解决每个任务。
- en: '[Pianifica e risolvi](https://oreil.ly/aWTzf)'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '[计划和解决](https://oreil.ly/aWTzf)'
- en: Dato un compito, chiedi che venga elaborato un piano e poi chiedi al modello
    di risolverlo.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个任务，要求制定一个计划，然后要求模型解决它。
- en: '[L''albero dei pensieri (ToT)](https://oreil.ly/IZdj1)'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '[思维树 (ToT)](https://oreil.ly/IZdj1)'
- en: Crea un problema di ricerca ad albero in cui un compito è suddiviso in più rami
    di passi come un albero. Poi, richiama il modello per valutare e risolvere ogni
    ramo di passi.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个树形搜索问题，其中任务被分解为多个步骤的分支，就像一棵树。然后，调用模型来评估和解决每个步骤的分支。
- en: '[La Tabella 10-5](#prompting_techniques_decomposition) mostra queste tecniche
    di scomposizione.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '[表10-5](#prompting_techniques_decomposition) 展示了这些分解技术。'
- en: Tabella 10-5\. Tecniche di prompt della decomposizione
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 表10-5\. 分解提示技术
- en: '| Tecnica del prompt | Esempi | Casi d''uso |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 提示技术 | 示例 | 用例 |'
- en: '| --- | --- | --- |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Da meno a più | Suddividi il compito di... in compiti più piccoli. | Risoluzione
    di problemi complessi, gestione di progetti, decomposizione dei compiti |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 从少到多 | 将...任务分解为更小的任务。 | 复杂问题解决、项目管理、任务分解 |'
- en: '| Pianifica e risolvi | Ideare un piano per... | Sviluppo di algoritmi, progettazione
    di software, pianificazione strategica |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 计划和解决 | 设计一个计划来... | 算法开发、软件开发设计、战略规划 |'
- en: '| L''albero dei pensieri (ToT) | Crea un albero decisionale per la scelta di
    un... | Processo decisionale, risoluzione di problemi con soluzioni multiple,
    pianificazione strategica con alternative. |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 思维树 (ToT) | 创建一个决策树以选择一个... | 决策过程、具有多个解决方案的问题解决、具有替代方案的战略规划 |'
- en: Assemblaggio
  id: totrans-366
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 组装
- en: L*'assemblaggio* è il processo che prevede l'utilizzo di più prompt per risolvere
    lo stesso problema e l'aggregazione delle risposte in un output finale. Puoi generare
    queste risposte utilizzando lo stesso modello o modelli diversi.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '*“组装”* 是一个过程，它涉及使用多个提示来解决相同的问题，并将这些响应汇总为最终的输出。你可以使用相同的模型或不同的模型生成这些响应。'
- en: L'idea principale alla base dell'ensembling è quella di ridurre la varianza
    dei risultati di LLM migliorando l'accuratezza in cambio di costi di utilizzo
    più elevati.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 集成背后的主要思想是通过提高使用成本来减少LLM结果的变化，从而提高准确性。
- en: 'Le tecniche di prompt dell''ensemble più conosciute sono le seguenti:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的集成提示技术如下：
- en: '[Autoconsistenza](https://oreil.ly/_85WS)'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '[自洽性](https://oreil.ly/_85WS)'
- en: Genera più percorsi di ragionamento e seleziona l'output più coerente come risultato
    finale utilizzando una votazione a maggioranza.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 通过生成更多推理路径并使用多数投票选择最一致的输出作为最终结果。
- en: '[Miscela di esperti di ragionamento (MoRE)](https://oreil.ly/xllKs)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[混合推理专家 (MoRE)](https://oreil.ly/xllKs)'
- en: Combina i risultati di più LLMs con prompt specializzati per migliorare la qualità
    delle risposte. Ogni LLMs agisce come un esperto di un'area focalizzata su diversi
    compiti di ragionamento come ragionamento fattuale, riduzione logica, controlli
    di buon senso, ecc.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用专门的提示将多个LLM的结果结合起来，以提高答案的质量。每个LLM都作为专注于不同推理任务的领域专家，如事实推理、逻辑简化、常识控制等。
- en: '[Dimostrazione di assemblaggio (DENSE)](https://oreil.ly/lPEPz)'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '[组装演示（DENSE）](https://oreil.ly/lPEPz)'
- en: Crea prompt multipli di pochi secondi a partire dai dati, quindi genera un output
    finale aggregando le risposte.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中创建多个短提示，然后通过汇总答案生成最终输出。
- en: '[Parafrasi prompt](https://oreil.ly/yP_ka)'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '[提示改写](https://oreil.ly/yP_ka)'
- en: Formula il prompt originale in più varianti attraverso la formulazione.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 通过公式化提出原始提示的多个变体。
- en: '[La Tabella 10-6](#prompting_techniques_ensemling) mostra esempi e casi d''uso
    di queste tecniche di assemblaggio.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '[表10-6](#prompting_techniques_ensemling) 展示了这些组装技术的例子和用例。'
- en: Tabella 10-6\. Tecniche di prompt di gruppo
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 表10-6\. 群组提示技术
- en: '| Tecnica del prompt | Esempi | Casi d''uso |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 提示技术 | 例子 | 用例 |'
- en: '| --- | --- | --- |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Autoconsistenza | prompt #1 (da eseguire più volte): Pensiamo passo dopo
    passo e completiamo la seguente attività...prompt n. 2: Tra le seguenti risposte,
    scegli quella migliore/comune assegnando un punteggio utilizzando... | Ridurre
    gli errori o le distorsioni nei compiti di aritmetica, di senso comune e di ragionamento
    simbolico. |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 自我一致性 | 提示 #1（需要多次执行）：我们一步一步思考，完成以下活动...提示 #2：在以下答案中选择最佳/最常见的一个，并使用以下标准进行评分...
    | 减少算术、常识和符号推理任务中的错误或扭曲。 |'
- en: '| Miscela di esperti di ragionamento (MoRE) | prompt n. 1 (da eseguire per
    ogni esperto): Sei un recensore di ..., dai un punteggio a quanto segue in base
    a...prompt n. 2: scegli la migliore risposta dell''esperto in base al punteggio
    dell''accordo... | Contabilità per aree o domini di conoscenza specializzati |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 多专家推理混合（MoRE） | 提示 #1（为每位专家执行）：如果你是...的评论家，根据以下标准给以下内容打分...提示 #2：根据一致性评分选择专家的最佳答案...
    | 专门领域或知识领域的会计 |'
- en: '| Dimostrazione di assemblaggio (DENSE) | Crea diversi esempi di traduzione
    di questo testo e aggrega le risposte migliori.Genera diversi prompt per riassumere
    l''articolo e combina i risultati. |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 组装演示（DENSE） | 创建多个翻译示例并汇总最佳答案。生成多个提示来总结文章并组合结果。 |'
- en: Migliorare l'affidabilità della produzione
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高生产的可靠性
- en: Aggregare prospettive diverse
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚集不同的观点
- en: '|'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Parafrasi prompt | Prompt #1a: Riformula questa proposta...prompt #1b: Chiarisci
    questa proposta...prompt #1c: Fai una modifica a questa proposta...prompt n. 2:
    Scegli la proposta migliore tra le seguenti risposte in base a... |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 提示改写 | 提示 #1a：重新表述这个提议...提示 #1b：澄清这个提议...提示 #1c：对这个提议进行修改...提示 #2：根据以下标准从以下答案中选择最佳提议...
    |'
- en: Esplorare diverse interpretazioni
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索不同的解释
- en: Aumento dei dati per l'ensembling
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加用于组装的数据
- en: '|'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Autocritica
  id: totrans-392
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自我批评
- en: Le tecniche di*autocritica* si concentrano sull'utilizzo dei modelli come giudici,
    valutatori o revisori dell'intelligenza artificiale, sia per eseguire autocontrolli
    che per valutare i risultati di altri modelli. La critica o il feedback del primo
    prompt possono essere utilizzati per migliorare la qualità delle risposte nei
    prompt successivi.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 自我批评技术集中在使用模型作为人工智能的评判者、评估者或审稿人，无论是进行自我检查还是评估其他模型的结果。第一轮提示的批评或反馈可以用于提高后续提示中答案的质量。
- en: 'Queste sono diverse strategie di prompt dell''autocritica:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是自我批评的不同策略：
- en: '[Autocalibrazione](https://oreil.ly/_4YEr)'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '[自校准](https://oreil.ly/_4YEr)'
- en: Chiedi al LLM di valutare la correttezza di una risposta rispetto a una domanda/risposta.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 请LLM评估一个答案相对于一个问题/答案的正确性。
- en: '[Auto-raffinare](https://oreil.ly/bTQJI)'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '[自我优化](https://oreil.ly/bTQJI)'
- en: Affina le risposte in modo iterativo attraverso l'autocontrollo e la fornitura
    di feedback.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自我控制和提供反馈迭代地细化答案。
- en: '[Inversione della catena del pensiero (RCoT)](https://oreil.ly/6ojtr)'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '[思维链反转（RCoT）](https://oreil.ly/6ojtr)'
- en: Ricostruire il problema a partire da una risposta generata e poi generare confronti
    a grana fine tra il problema originale e quello ricostruito per identificare le
    incongruenze.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 从生成的答案出发重建问题，然后对原始问题和重建问题进行精细粒度的比较，以识别不一致之处。
- en: '[Autoverifica](https://oreil.ly/Fz3JH)'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '[自我验证](https://oreil.ly/Fz3JH)'
- en: Genera potenziali soluzioni con la tecnica CoT e poi assegna un punteggio a
    ciascuna di esse mascherando parti della domanda e fornendo ogni risposta.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CoT技术生成潜在解决方案，然后为每个解决方案评分，同时隐藏问题的一部分并提供每个答案。
- en: '[Catena di verifica (COVE)](https://oreil.ly/WrrLP)'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '[验证链 (COVE)](https://oreil.ly/WrrLP)'
- en: Crea un elenco di query/domande correlate per verificare la correttezza di una
    risposta.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个相关查询/问题列表以验证答案的正确性。
- en: '[Ragionamento cumulativo](https://oreil.ly/3Hb-6)'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '[累积推理](https://oreil.ly/3Hb-6)'
- en: Genera potenziali passi per rispondere a una query e poi chiedi al modello di
    accettare/rifiutare ogni passo. Infine, controlla se è arrivato alla risposta
    finale per terminare il processo; altrimenti, ripeti il processo.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 生成回答查询的潜在步骤，然后要求模型接受/拒绝每个步骤。最后，检查是否到达了最终答案以终止过程；否则，重复此过程。
- en: Puoi vedere degli esempi di ciascuna tecnica di prompt dell'autocritica nella
    [Tabella 10-7](#prompting_techniques_self_criticism).
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[表 10-7](#prompting_techniques_self_criticism)中看到自我批评的每个提示技术的示例。
- en: Tabella 10-7\. Tecniche di prompt per l'autocritica
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10-7\. 自我批评的提示技术
- en: '| Tecnica del prompt | Esempi | Casi d''uso |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 提示技术 | 示例 | 用例 |'
- en: '| --- | --- | --- |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Autocalibrazione | Valuta la correttezza della seguente risposta: [risposta]
    per la seguente domanda: [domanda] | Valuta la fiducia nelle risposte per accettare
    o rivedere la risposta originale. |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| 自我校准 | 评估以下回答的正确性：[回答]针对以下问题：[问题] | 评估对答案的信心以接受或修改原始答案。 |'
- en: '| Auto-raffinare | prompt n. 1: Qual è la tua opinione sulla risposta...prompt
    n. 2: Utilizzando il feedback [Feedback], perfeziona la tua risposta su... | Attività
    di ragionamento, codifica e generazione. |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| 自我优化 | prompt n. 1: 你对以下回答的看法是什么...prompt n. 2: 使用以下[反馈]来优化你在以下方面的回答... |
    推理、编码和生成活动。 |'
- en: '| Inversione della catena del pensiero (RCoT) | prompt #1: Ricostruisci il
    problema a partire da questa risposta...prompt n. 2: Genera un confronto a grana
    fine tra queste query... | Identificare le incongruenze e rivedere le risposte.
    |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 思维链反转 (RCoT) | prompt #1: 从这个回答中重构问题...prompt n. 2: 生成这些查询之间的精细对比... | 识别不一致之处并回顾回答。
    |'
- en: '| Autoverifica | prompt #1 (da eseguire più volte): Pensiamo passo dopo passo
    - genera una soluzione per il seguente problema...prompt n. 2: assegna un punteggio
    a ciascuna soluzione in base al [problema mascherato]... | Migliorare i compiti
    di ragionamento. |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| 自我验证 | prompt #1 (需要多次执行): 逐步思考 - 为以下问题生成解决方案...prompt n. 2: 根据以下[隐藏问题]为每个解决方案评分...
    | 提高推理任务。 |'
- en: '| Catena di verifica (COVE) | prompt n. 1: rispondi alla seguente domanda...prompt
    #2: formula delle domande correlate per verificare questa risposta: ...prompt
    #3 (da eseguire per ogni nuova domanda correlata): Rispondi alla seguente domanda:
    ...prompt n. 4: in base alle seguenti informazioni, scegli la risposta migliore...
    | Attività di risposta alle domande e di generazione di testi. |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| 验证链 (COVE) | prompt n. 1: 回答以下问题...prompt #2: 为验证这个回答提出相关问题的公式...prompt #3
    (为每个新相关问题执行): 回答以下问题: ...prompt #4: 根据以下信息，选择最佳答案... | 回答问题和生成文本活动。 |'
- en: '| Ragionamento cumulativo | prompt n. 1: delinea i passaggi per rispondere
    alla query: ...Prompt #2: Controlla il seguente piano e accetta/rifiuta i passaggi
    rilevanti per rispondere alla query: ...prompt #3: verifica di essere arrivato
    alla risposta finale con le seguenti informazioni... | Convalida passo dopo passo
    di query complesse, inferenze logiche e problemi matematici. |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| 累积推理 | prompt n. 1: 描述回答查询的步骤: ...Prompt #2: 检查以下计划并接受/拒绝回答查询的相关步骤: ...prompt
    #3: 使用以下信息验证你是否到达了最终答案... | 验证复杂查询、逻辑推理和数学问题的逐步验证。 |'
- en: Agente
  id: totrans-417
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代理
- en: Puoi fare un ulteriore passo avanti rispetto alle tecniche di prompt discusse
    finora e aggiungere l'accesso a strumenti esterni con algoritmi di valutazione
    complessi. Questo processo specializza gli LLMs come *agenti*, consentendo loro
    di fare piani, intraprendere azioni e utilizzaresistemi esterni.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以比迄今为止讨论的提示技术更进一步，并添加对具有复杂评估算法的外部工具的访问。这个过程将LLMs专业化为*代理*，允许它们制定计划、采取行动并使用外部系统。
- en: I prompt o le *sequenze di prompt (catene)* guidano i sistemi agenziali con
    un focus ingegneristico sulla creazione di comportamenti simili a quelli degli
    agenti da LLMs. Questi flussi di lavoro agenziali servono gli utenti eseguendo
    azioni sui sistemi che si interfacciano con i modelli GenAI, che sono per lo più
    LLMs. Gli strumenti, siano essi *simbolici* come una calcolatrice o *neurali*
    come un altro modello di IA, costituiscono una componente fondamentale dei sistemi
    agenziali.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 提示或*提示序列（链）*通过工程化方法引导代理系统，使其行为类似于LLMs的代理。这些代理工作流程通过在接口模型（大多是LLMs）上执行操作来服务用户。这些工具，无论是*符号的*如计算器还是*神经的*如其他IA模型，都是代理系统的一个基本组成部分。
- en: Suggerimento
  id: totrans-420
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 建议
- en: Se crei una pipeline di più chiamate al modello con un output inoltrato allo
    stesso modello o a modelli diversi come input, hai costruito una *catena di prompt*.
    In linea di principio, quando sfrutti le catene di prompt stai utilizzando la
    tecnica di prompt della CoT.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你创建了一个将输出传递到同一模型或不同模型的多个模型调用管道，你构建了一个*提示链*。原则上，当你使用提示链时，你正在使用CoT的提示技术。
- en: 'Alcune tecniche di prompt agenziale includono:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 一些代理提示技术包括：
- en: '[Ragionamento, conoscenza e linguaggio modulari (MRKL)](https://oreil.ly/aWeQu)'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '[模块化推理、知识和语言 (MRKL)](https://oreil.ly/aWeQu)'
- en: Il sistema agenziale più semplice consiste in un LLM che utilizza diversi strumenti
    per ottenere e combinare le informazioni per generare una risposta.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的代理系统由一个使用不同工具获取和组合信息以生成响应的LLM组成。
- en: '[Autocorrezione con critica interattiva dello strumento (CRITIC)](https://oreil.ly/M-9YL)'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '[带有工具交互式批评的自动更正 (CRITIC)](https://oreil.ly/M-9YL)'
- en: Risponde alle query e poi auto-verifica la risposta senza utilizzare strumenti
    esterni. Infine, utilizza strumenti per verificare o modificare le risposte.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 响应查询并随后自动验证响应，而不使用外部工具。最后，使用工具验证或修改响应。
- en: '[Modello linguistico assistito da programma (PAL)](https://oreil.ly/0WtKv)'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '[程序辅助语言模型 (PAL)](https://oreil.ly/0WtKv)'
- en: Genera codice dalle query e lo invia direttamente a interpreti di codice come
    Python per generare una risposta.^([4](ch10.html#id1128))
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 从查询生成代码并将其直接发送到Python等代码解释器以生成响应.^([4](ch10.html#id1128))
- en: '[Agente di ragionamento integrato negli strumenti (ToRA)](https://oreil.ly/pbfv_)'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '[集成在工具中的推理代理 (ToRA)](https://oreil.ly/pbfv_)'
- en: L'AMICO compie un ulteriore passo in avanti, intercalando le fasi di generazione
    del codice e di ragionamento per tutto il tempo necessario a fornire una risposta
    soddisfacente.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: AMICO通过在整个提供满意响应所需的时间内插入代码生成和推理阶段，又前进了一步。
- en: '[Ragionare e agire (React)](https://oreil.ly/aDubr)'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '[推理和行动 (React)](https://oreil.ly/aDubr)'
- en: Dato un problema, genera pensieri, compie azioni, riceve osservazioni e ripete
    il ciclo con le informazioni precedenti (cioè la memoria) finché il problema non
    viene risolto.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个问题，生成思考，执行操作，接收观察，并使用先前信息（即记忆）重复循环，直到问题得到解决。
- en: Se vuoi permettere ai tuoi LLMs di utilizzare degli strumenti, puoi sfruttare
    le funzioni *di chiamata di funzione* dei fornitori di modelli, come nell'[Esempio
    10-17](#function_calling).
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想让你的LLMs使用工具，你可以利用模型提供商的*函数调用*功能，如[示例10-17](#function_calling)所示。
- en: Esempio 10-17\. Chiamata di funzione per il fetching
  id: totrans-434
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例10-17\. 获取函数调用
- en: '[PRE21]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Come hai visto nell'[Esempio 10-17](#function_calling), puoi creare sistemi
    agenziali configurando LLMs specializzati che hanno accesso a strumenti personalizzati
    e alle funzioni di .
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[示例10-17](#function_calling)中看到的，你可以通过配置具有访问自定义工具和函数的专用LLMs来创建代理系统。
- en: Messa a punto
  id: totrans-437
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整
- en: Ci sono casi in cui l'ingegneria del prompt da sola non è in grado di fornire
    la qualità di risposta che stai cercando. La messa a punto è una tecnica di ottimizzazione
    che richiede di regolare i parametri del tuo modello GenAI per adattarlo meglio
    ai tuoi dati. Ad esempio, puoi mettere a punto un modello linguistico per imparare
    il contenuto di basi di conoscenza private o per rispondere sempre con un certo
    tono seguendo le linee guida del tuo marchio.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，仅通过提示工程无法提供你所需的响应质量。调整是优化技术，需要调整你的GenAI模型的参数以更好地适应你的数据。例如，你可以调整语言模型来学习私有知识库的内容，或者始终以遵循你品牌指南的某种语气进行响应。
- en: Spesso non è la prima tecnica da provare perché richiede uno sforzo per raccogliere
    e preparare i dati, oltre che per addestrare e valutare i modelli.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常不是第一个尝试的技术，因为它需要收集和准备数据的努力，以及训练和评估模型。
- en: Quando dovresti prendere in considerazione la messa a punto?
  id: totrans-440
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你应该在何时考虑进行微调？
- en: 'Potresti prendere in considerazione la possibilità di perfezionare i modelli
    GenAI pre-addestrati se si verifica uno dei seguenti scenari:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 如果出现以下情况之一，你可能需要考虑完善预训练的GenAI模型：
- en: Hai dei costi significativi per l'utilizzo dei token, ad esempio perché devi
    richiedere istruzioni di sistema dettagliate o fornire molti esempi in ogni prompt.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你在使用令牌时面临显著的成本，例如因为你需要提供详细的系统指令或在每个提示中提供许多示例。
- en: Il tuo caso d'uso si basa su competenze specialistiche del dominio che il modello
    deve imparare.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的用例基于模型必须学习的特定领域的专业知识。
- en: È necessario ridurre il numero di allucinazioni nelle risposte con un modello
    conservativo più preciso.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要使用更精确的保守模型来减少响应中的幻觉数量。
- en: Hai bisogno di risposte di qualità superiore e di dati sufficienti per la messa
    a punto.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要高质量的响应和足够的微调数据。
- en: Hai bisogno di una latenza minore nelle risposte.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要更低的响应延迟。
- en: Una volta messo a punto il modello, non sarà più necessario fornire tanti esempi
    nel prompt, risparmiando sui costi e consentendo richieste a bassa latenza.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型经过微调，就不再需要提供大量的示例在提示中，从而节省成本并允许低延迟请求。
- en: Avvertenze
  id: totrans-448
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意事项
- en: Evita il più possibile la messa a punto.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 尽可能避免微调。
- en: L'iterazione sui prompt ha un ciclo di feedback molto più rapido rispetto all'iterazione
    sulla messa a punto, che si basa sulla creazione di set di dati e sull'esecuzione
    di lavori di formazione.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于创建数据集和执行训练工作的微调迭代相比，提示迭代的反馈周期要快得多。
- en: Tuttavia, se alla fine avrai bisogno di una messa a punto, noterai che gli sforzi
    iniziali di prompt engineering contribuiranno a produrre dati di formazione di
    qualità superiore.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果最终你需要进行微调，你会注意到初始的提示工程努力将有助于产生高质量的训练数据。
- en: 'Ecco alcuni casi in cui la messa a punto può essere utile:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些微调可能有用的案例：
- en: Insegnare a un modello a rispondere a uno stile, a un tono, a un formato o a
    un'altra metrica qualitativa, ad esempio per produrre rapporti standardizzati
    conformi ai requisiti normativi e ai protocolli interni.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教会模型回答风格、语气、格式或其他质量指标，例如生成符合法规要求和管理内部协议的标准报告。
- en: Migliorare l'affidabilità della produzione degli output desiderati, come ad
    esempio avere sempre risposte conformi a un determinato output strutturato.
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高生成所需输出的可靠性，例如始终提供符合特定结构化输出的答案。
- en: Ottenere risultati corretti per query complesse come la classificazione di documenti
    e l'etichettatura di centinaia di classi.
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于复杂的查询，如文档分类和数百个类别的标签，获得正确的结果。
- en: Esecuzione di compiti specializzati in un dominio specifico, come la classificazione
    di articoli o l'interpretazione e l'aggregazione di dati specifici per il settore.
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特定领域执行专业任务，例如文章分类或为特定行业收集和解释数据的任务。
- en: Gestione più accurata dei casi limite
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更精确地管理边缘情况。
- en: Eseguire abilità o compiti difficili da articolare nei prompt, come l'estrazione
    di date da testi non strutturati.
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提示中执行难以表述的技能或任务，例如从非结构化文本中提取日期。
- en: Ridurre i costi utilizzando `gpt-40-mini` o addirittura `gpt-3.5-turbo` al posto
    di `gpt-4o`
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`gpt-40-mini`或甚至`gpt-3.5-turbo`代替`gpt-4o`来降低成本。
- en: Insegnare a un modello a utilizzare strumenti e API complesse quando si usa
    la chiamata di funzione
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 教会模型在调用函数时使用复杂的工具和API。
- en: Come mettere a punto un modello preaddestrato
  id: totrans-461
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 如何微调预训练模型
- en: 'Per qualsiasi lavoro di messa a punto, dovrai seguire questi passaggi:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何微调工作，你都需要遵循以下步骤：
- en: Preparare e caricare i dati della formazione.
  id: totrans-463
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备和加载训练数据。
- en: Invia un lavoro di formazione per la messa a punto.
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发送一份用于开发的培训工作。
- en: Valutare e utilizzare il modello perfezionato.
  id: totrans-465
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估和使用改进后的模型。
- en: A seconda del modello che stai utilizzando, i dati devono essere preparati in
    base alle istruzioni del fornitore del modello.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你使用的模型，数据应根据模型提供商的说明进行准备。
- en: Ad esempio, per mettere a punto un tipico modello di chat come `gpt-4o-2024-08-06`,
    devi preparare i tuoi dati in un formato di messaggio, come mostrato nell'[Esempio
    10-18](#fine_tune_prepare). Al momento in cui scriviamo, il [prezzo dell'API OpenAI](https://oreil.ly/MmCNq)
    per la messa a punto di questo modello è di $25/1M di token di addestramento.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，为了开发一个典型的聊天模型，如`gpt-4o-2024-08-06`，你需要将你的数据准备成消息格式，如[Esempio 10-18](#fine_tune_prepare)中所示。截至我们撰写本文时，用于调整此模型的OpenAI
    API价格是$25/1M个训练令牌。
- en: Esempio 10-18\. Esempio di dati di allenamento per un lavoro di messa a punto
  id: totrans-468
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Esempio 10-18. 调整工作的训练数据示例
- en: '[PRE22][PRE23][PRE24] `"messages"``:` `[` [PRE25][PRE26]`` `{` [PRE27][PRE28]`
    `"role"``:` `"system"``,` [PRE29][PRE30] `"content"``:` `"<text>"` [PRE31][PRE32]``py[PRE33]`py`
    `"role"``:` `"user"``,` [PRE34] `},` [PRE35]`` `{` [PRE36]` `"role"``:` `"assistant"``,`
    [PRE37] `"content"``:` `"<text>"` [PRE38]`py [PRE39]py`` [PRE40]py[PRE41][PRE42][PRE43][PRE44]py[PRE45]py`
    [PRE46]`py`` [PRE47]`py[PRE48][PRE49][PRE50]'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE22][PRE23][PRE24] `"messages"`: `[` [PRE25][PRE26]` `{` [PRE27][PRE28]`
    `"role"`: `"system"``,` [PRE29][PRE30]` `"content"`: `"<text>"` [PRE31][PRE32]`py[PRE33]`py`
    `"role"`: `"user"``,` [PRE34] `},` [PRE35]` `{` [PRE36]` `"role"`: `"assistant"``,`
    [PRE37]` `"content"`: `"<text>"` [PRE38]`py [PRE39]py` ` [PRE40]py[PRE41][PRE42][PRE43][PRE44]py[PRE45]py`
    [PRE46]`py` [PRE47]`py[PRE48][PRE49][PRE50]'
- en: '[PRE51][PRE52]`` [PRE53] from openai import OpenAI client = OpenAI()  response
    = client.files.create(     file=open("mydata.jsonl", "rb"), purpose="fine-tune"
    )  client.fine_tuning.jobs.create(     training_file=response.id, model="gpt-4o-mini-2024-07-18"
    ) [PRE54] from openai import OpenAI client = OpenAI()  fine_tuning_job_id = "ftjob-abc123"
    response = client.fine_tuning.jobs.retrieve(fine_tuning_job_id) fine_tuned_model
    = response.fine_tuned_model  if fine_tuned_model is None:     raise ValueError(         f"Failed
    to retrieve the fine-tuned model - "         f"Job ID: {fine_tuning_job_id}"     )  completion
    = client.chat.completions.create(     model=fine_tuned_model,     messages=[         {"role":
    "system", "content": "You are a helpful assistant."},         {"role": "user",
    "content": "Hello!"},     ], ) print(completion.choices[0].message) [PRE55]` [PRE56][PRE57]
    `` `# Sommario    In questo capitolo hai appreso diverse strategie di ottimizzazione
    per migliorare il throughput e la qualità dei tuoi servizi. Alcune ottimizzazioni
    che hai aggiunto hanno riguardato la cache (parole chiave, semantica, contesto),
    l''ingegneria del prompt, la quantizzazione del modello e la messa a punto.    Nel
    prossimo capitolo ci concentreremo sull''ultima fase della costruzione di servizi
    di AI: la distribuzione della tua soluzione GenAI, che comprende l''esplorazione
    dei modelli di distribuzione per i servizi di AI e la containerizzazione con Docker.    ^([1](ch10.html#id1069-marker))
    Vedi l''API OpenAI Batch disponibile nella [documentazione](https://oreil.ly/0t59w)
    dell''[API OpenAI](https://oreil.ly/0t59w).    ^([2](ch10.html#id1072-marker))
    Per saperne di più sulle intestazioni di controllo della cache, visita il [sito
    MDN](https://oreil.ly/-Y5JP).    ^([3](ch10.html#id1076-marker)) Per ottenere
    un risparmio significativo sui costi, potrebbe essere necessario un modello embedder
    addestrato, in quanto le frequenti chiamate API a un modello embedder off-the-shelf
    potrebbero comportare costi aggiuntivi, riducendo il risparmio complessivo.    ^([4](ch10.html#id1128-marker))
    Per una maggiore sicurezza, devi comunque sanificare il codice generato da LLM
    prima di inoltrarlo ai sistemi a valle per l''esecuzione.` `` ```'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '[PRE51][PRE52]`` [PRE53] 从 openai 导入 OpenAI'
