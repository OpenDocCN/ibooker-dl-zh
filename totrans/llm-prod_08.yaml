- en: '9 *Creating an LLM project: Reimplementing Llama 3*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Implementing Meta’s Llama3 model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a simple LLM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making improvements to it to prepare it for production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serving the model to a production endpoint you can share with your friends
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I am only coming to Princeton to research, not to teach. There is too much education
    altogether, especially in American schools. The only rational way of educating
    is to be an example.—Albert Einstein
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: For the first major project in the book, we want to start from scratch. We’ve
    been showing you how to work with LLMs from end to end, and we are going to put
    it all together in this chapter. This project includes pretraining a model, roughly
    following a research paper. We won’t dive too deeply into the actual research;
    in fact, we’ll take several shortcuts here, as this isn’t the focus of this book.
    We will, however, showcase how to train the model, prepare it for servings with
    quantization, finetune it with low-rank adaptation (LoRA) for a specific purpose
    or task, and deploy it to a production environment you can showcase to your friends.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will be very dense, but you should be more than prepared to meet
    the challenge at this point because it’s mainly a data scientist–focused project
    for production. We chose this project so that you can put all the lessons you’ve
    learned throughout the book together into one place and leave you with end-to-end,
    hands-on experience.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Implementing Meta’s Llama
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '“Llama 2: Open Foundation and Fine-Tuned Chat Models” by Touvron et al.[¹](#footnote-141)
    is an awesome paper that covers the development and release of Llama 2, one of
    the best, almost open source models currently on the market. You may have seen
    Llama 2 as the first open source model that was good enough to rival OpenAI’s
    models, at least based on the metrics of the time. Llama 3 is out now, and it
    has almost completely eclipsed Llama 2 in popularity and may very well be why
    you picked up this book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Llama 3 is amazing for a couple of reasons—namely, size and availability. With
    only 70B parameters, pretrained on only 15T tokens, and finetuned on 100K chats,
    it shouldn’t be able to beat a 176B or a 1.7T parameter model at anything. Unsurprisingly,
    it usually doesn’t. But it does beat them at one crucial thing: its availability.
    This feature has given rise to an open source software community that has made
    tooling and optimizations and even gathers data to make it better. Llama 3 is
    the ultimate showcase that architecture is less important than data, and it is
    trained on clean data.'
  prefs: []
  type: TYPE_NORMAL
- en: And we’re going to implement it.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will build a real model and understand the work
    that goes into it. Will it be as good as Meta’s Llama 3? Far from it, because
    we won’t be demonstrating with an adequate amount of data or GPUs. But we want
    to do more than simply supply you with yet another set of weights that are on
    some leaderboard somewhere. We want to give you some intuition for the steps required
    and the potential problems you may face. Instead of training a great model completely
    from scratch, which is what dozens of other books are tackling right now, we’ll
    show you how to train a below-average model and productionize it. This approach
    should have you not only learning more but demonstrating expertise beyond your
    experience level.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.1 Tokenization and configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By this point, you’ve likely already learned the importance of setting up the
    problem correctly. We want our models hitting tee-balls out of the park, not going
    up against an MLB pitcher. With that in mind, we’ll download the same tokenizer
    that Llama used. If you want, you can come back and experiment with this tokenizer
    since we are building from scratch. For example, try to use a faster tokenizer
    like tiktoken—just know you’ll be giving up the model’s ability to do math. You
    can also train your own version of the SentencePiece model, which should guarantee
    better results on whatever dataset you want to extend this with. The point is
    that this model is blank—no pretrained weights at all. So come back and do whatever
    you’d like after following along.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  Unlike other chapters where each listing was stand alone, in this chapter,
    each listing will be part of a larger notebook. You can find this notebook in
    the code repository accompanying this book.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.1 shows our initial setup for this project, including imports, device
    settings, and grabbing our tokenizer. While we’ll just be grabbing the tokenizer
    from Hugging Face, keep in mind that not all tokenizers and models use the same
    type of tokens. This is important because we’re going to train this model differently
    than the way the inference tokenizer is set up for. To correct for this discrepancy,
    we’ll need to add a padding token. Anything would do, but we’ll use `"<PAD>"`
    in our example. Once we have that, we’ll make sure to grab the vocab itself (we’ll
    need it later) and create encoding and decoding functions to help with batch processing.
    Because we’re using the Hugging Face implementation, this isn’t strictly needed
    because it has batch tokenization built in, along with a `batch_decode` method
    that works great. For learning’s sake, we’ll go through the motions anyway. It’s
    always good practice to be aware of what you’re doing, and these functions help
    lock that down.
  prefs: []
  type: TYPE_NORMAL
- en: The last part of this listing offers the most flexibility. Here, we set up a
    master config that will ultimately decide how many parameters our model has, how
    long it trains, and how much memory it will take per row in our dataset. Our default
    values are pretty small and designed to give you a good experience regardless
    of your hardware, including if you’re training on a CPU-only build. Feel free
    to experiment and crank up the numbers.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.1 Tokenize and config
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Uses Hugging Face'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Optional'
  prefs: []
  type: TYPE_NORMAL
- en: As we’ve reiterated a number of times throughout the book, remember that the
    strategy you use to tokenize and embed your inputs ultimately dictates what your
    model is able to “see” during training and inference. You should generally do
    a bit more than just choose a tokenizer; in fact, we’ll see later in this chapter
    what choosing the Llama 3 tokenizer will do to our inference.
  prefs: []
  type: TYPE_NORMAL
- en: You could opt for training a new tokenizer on your dataset or adding especially
    important tokens from your dataset to an already-robust tokenizer—preferably one
    that already generally matches the strategy you want and is trained in the domain
    you need. If you aren’t sure about any of that, any LLM tokenizer should generally
    work—that’s what they’re designed for. But don’t be surprised when the model doesn’t
    perform well when you pick a general tokenizer and want a specific task.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.2 Dataset, data loading, evaluation, and generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s get into the most important part of this process, which we will, for the
    most part, gloss over. There’s only so much we can focus on in one chapter, but
    we want to reiterate how important your dataset is to the success of your LLM.
    You’ll want to spend time gathering, evaluating, and cleaning your dataset, but
    we’ll shortcut that process in the interest of time. Instead, we’ll focus on the
    steps necessary to train the model—loading, preprocessing, batching, and so forth.
    As we go through this section, remember that your unique data sources end up future-proofing
    your model, so consider what data you have access to that no one else does and
    how you’d set that dataset up for this training.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start by loading a dataset that’s generally popular for creating toy models,
    TinyStories. If you did the work to explore your data—and we encourage you to
    do it—you’ll see that this is a smallish dataset for LLMs, containing only 30
    million rows, each containing a short story in a single paragraph. It draws from
    some oft-implemented and widely accepted datasets. While a small dataset for LLMs,
    it’s likely still too large for many computers, and many readers will likely hit
    out-of-memory errors if they try to load it into memory wholesale. Here’s the
    perfect time to use streaming. In listing 9.2, we show you how to pull the dataset
    from the Hugging Face Hub or `dataset.to_iterable_ dataset()` if working locally.
    Both methods allow for much more memory-efficient processing, as the whole dataset
    isn’t loaded all at once, sacrificing some speed.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.2 Loading and preparing the data
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Streams from the local files'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have your dataset and are able to retrieve an iteration, we’ll do
    some minimal (truly) cleaning. Then we’ll encode the whole thing so that our training
    can go quicker down the line. We’ll save the tokenization and attention masks
    as their own columns, and then we’ll shuffle the dataset and go on to dataloading.
    A quick note that’s always worth mentioning: when training any machine learning
    model, if you don’t already have your `train` and `val` splits defined, take extra
    care shuffling your dataset so that none of the data leaks into a split where
    it shouldn’t be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Minimal processing'
  prefs: []
  type: TYPE_NORMAL
- en: If you disregard our advice to stream and have a computer that can handle this
    dataset, know that loading the entire dataset into memory and then preparing it,
    even using hardware acceleration, takes over 30 minutes and more than 5 GB of
    memory. So if you have an extra 5 GB of VRAM outside of what you’ll need for your
    model, you’re good to go ahead and load it however you want. See figure 9.1.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/9-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 With over 30 million rows, this dataset is pretty small for what
    we’re trying to do, but it is still substantial on consumer hardware.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We’ll need at least one function to load our data into a ready-to-use format
    for our model, and we’re opting to use just that. Our `get_batches` function will
    take in one row of our data and return a model input and an expected output that
    can be compared against it for self-supervised learning. No labeling is needed,
    as we’ll start on a random token, then grab tokens up to our whole context window
    (32) for our input, and shift one token to the right for our expected output.
    For our model, we create a scenario that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: CB **input:** How much wood could a woodchuck chuck if a woodchuck could chuck
  prefs: []
  type: TYPE_NORMAL
- en: CB **label:** How much wood could a woodchuck chuck if a woodchuck could chuck
    wood?
  prefs: []
  type: TYPE_NORMAL
- en: 'This process allows our model to train on our task: guessing the next token
    in an utterance, given the context of the previous 31 tokens. We use this strategy
    instead of other strategies like masking because our preferred inputs will never
    contain information after the input is completed. This way, our model will get
    better and better at text completion the more and higher-quality data it trains
    on. Almost all foundation models are pretrained in this manner—only they train
    for much longer with many more parameters than we will right now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Windows users leave commented'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Adjust this lower if you''re running out of memory.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Pick random starting points.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have our data batching taken care of, we need to come up with functions
    for evaluation and inference so that we can gain insight into how the model is
    doing during training and so that we can use the model later. For our evaluation,
    we’ll take some batches and average the loss across them to get our validation
    loss. This result will not give us a real representation of our model’s performance
    but will not stop it from being useful for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Questioning your assumptions
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: When working with machine-learning models and other statistical methods, it’s
    important to understand how your assumptions will affect your results. Averages
    hamper data representation and understanding because they basically say, “For
    this comparison, we’re going to grab a made-up number, and we’re going to use
    that number in place of any of the real ones because it feels central to our distribution.”
    This approach doesn’t make it bad; made-up numbers often are more predictive than
    real ones. However, we urge you to be intentional and very open-minded about testing
    whether the average is the best marker for your users.
  prefs: []
  type: TYPE_NORMAL
- en: For generation, we’ll do something similar but better. Logits are what we get
    out of our model’s forward method. We created a tokenized version of our prompt
    previously when we tokenized our dataset, so we’re ready to pass that prompt into
    our model a number of times and see what comes out. We’ll grab the logits from
    the model given the prompt and then sample our model’s distribution for our next
    token and decode.
  prefs: []
  type: TYPE_NORMAL
- en: 'For sampling that distribution, we’ll take the model’s output (logits) for
    only the very end of the input (the unknown token we want our model to generate)
    and then divide those logits by the temperature setting (higher temperature setting
    = smaller logits). Once we have our logits from the last time step, if we use
    multinomial sampling, we can sample using `top_k` and/or `top_p`, which are sampling
    against the highest probability tokens until you reach a total number of tokens
    or a total number of probability sums. Once we have that, we use softmax for the
    tokens we’ve sampled and then argmax to get the next token. If we want more exploration
    and creativity in our output, we can use multinomial sampling instead. As an exercise,
    test `top_k` versus `top_p` with multinomial and argmax versus multinomial to
    get an idea of which works best:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Calls the model'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 All the batches (1); last time step, all the logits'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Softmax to get probabilities'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Sample from the distribution to get the next token'
  prefs: []
  type: TYPE_NORMAL
- en: And with that, we’ve concluded our setup! We have utility functions for all
    of the important parts of the model training, including tokenization, data loading,
    evaluation, inference, and data processing. If there’s anything you feel should
    be corrected, great! Do it—this is your project. If you want to use a multinomial
    for sampling instead of an argmax or want to get rid of the softmax and just argmax
    over the logits, awesome, go for it. For those of you for whom this is your first
    time, we know it can be quite a firehose, and we’d encourage you to work through
    it slowly, but don’t lose too much sleep over it. More than likely, you will not
    have to come up with what should change for your use case yourself because you’ll
    be implementing an already-created open source model. That said, it’s still a
    good idea to understand what’s going on behind the scenes and under the hood so
    that you know roughly where to look when things go wrong.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1.3 Network architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ve now completed a ton of setup for training a model but haven’t made a
    model. Model architecture and training have been iterated upon ad nauseam, so
    we’ll skip talking about it too much and jump right in. We’ll start with a two-layer
    feed-forward network with fewer than 20M parameters, and then we’ll upgrade and
    talk about the changes that turn the model into Llama. We want to be clear about
    what is actually changing between them so you’ll get a good feel for the pieces
    involved. Because we aren’t going to be completely replicating Llama 3, but rather
    approximating it, here’s the official architecture if you’d like to try pretraining
    it on our dataset: [https://mng.bz/Dp9A](https://mng.bz/Dp9A).'
  prefs: []
  type: TYPE_NORMAL
- en: In listing 9.3, we make a class for that linear model with a ReLU activation
    between the two linear layers. Here’s where we’ll also define our actual loss
    function (because in our `get_loss` function, we’re just sending inputs to the
    model). We’ll use cross entropy because we’re comparing unstructured sequences.
    Instead of getting into information theory for why cross-entropy is the answer
    to unstructured sequences, the current benchmark in the industry is called perplexity,
    which uses cross-entropy to figure out whether a model is making sense or not,
    so this loss function enables us to compete with other models in the industry.
  prefs: []
  type: TYPE_NORMAL
- en: 'There’s one thing that you may have noticed before when we tokenized our dataset:
    we’re padding in batches and not truncating, meaning each batch size should be
    the same length. We fully acknowledge that this doesn’t make any sense when pretraining;
    it’s just helping to speed things up. We do this because our longest length input
    is 997 tokens, and we don’t want to pad our entire dataset out to 997\. Even mitigating
    that, the most common token in our dataset is still `"<PAD>"`. If we leave it
    as is, the model could learn to generate only padding tokens, which seemingly
    minimizes the loss when predicting the next token. Because we have a tokenizer
    vocab we just added to, however, we can tell the loss function to `ignore_index`
    our `tokenizer .pad_token_id` so correctly predicting padding tokens doesn’t mistakenly
    help the loss go down.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.3 Simple model and training loop
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Comment this out on Windows.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have our model, we’ll write our training loop. We run the number
    of passes specified in the epochs portion of the master config, and we get our
    loss for each pass. The epochs here are more like steps, and we’d encourage you
    to run epochs through the whole dataset if you have the time. If you stick to
    the `MASTER_CONFIG` we set up previously, this original model will end up having
    18.5M parameters. You should definitely change it to be the maximum number of
    parameters that your computer can handle. You can find this number by changing
    `d_model` (and `vocab_size` if you train a bigger tokenizer) in your master config:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Look, it’s figure 9.2, which was generated from listing 9.3! Try to guess what
    it will be, and then read the blurb to see if you’re right.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/9-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 Training a simple neural network on our dataset to generate text
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Look at that! That’s a pretty smooth curve when we train for the first time.
    Considering we only did 1,000 examples from our dataset, we’d encourage you to
    try for several actual epochs—say, try three going over the whole dataset—and
    see how things go. You’ll likely get surprisingly decent results; we did. Let’s
    go ahead and check out what it creates when generating text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Not too shabby! Of course, these aren’t great results, but we weren’t expecting
    amazing results with our basic model and short training time. Reading the generated
    tokens, it almost makes sense. We’ll call that a win. Congratulations! We created
    a language model using a feed-forward network that can return tokens. Now it’s
    time to get into the changes that make Llama different from a regular feed-forward
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Simple Llama
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you check the full weights and layers as released by Meta, you may notice
    that what we are building is not exactly the same as what was released. The reason
    for this is twofold: (1) we’d like to make sure this discussion is still very
    understandable for people interacting with research for production for the first
    time, and (2) we’re considering the environments you’ll likely have access to
    when reading this book. Everything here should fit and run in Kaggle or Colab
    without problems. With that being the case, we’ll address differences in Llama
    3’s architecture and ours so that if you did have the infra and data to replicate
    the paper for production, you could.[²](#footnote-142)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Llama is different from a feed-forward network in a few ways: normalization,
    attention, activation, and number of layers. Without going too deeply into any
    of them, normalization helps stabilize training, attention helps support larger
    context lengths and uses information between layers more efficiently, activation
    helps represent nonlinearities better, and the number of layers increases the
    amount of information the model is able to represent. One other important thing
    to note is that we’re adding a scheduler this time around. The scheduler here
    is responsible for adjusting the learning rate during training, following a “schedule.”
    This addition helps us with potential exploding gradients and allows the model
    to converge more quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s change our network into a simpler version of Llama 3\. Here, we’ll skip
    over some of the theory and implementation. But look at the notebook in GitHub
    too—we want you to test it out on your own!
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.4 Simple Llama
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 New'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the original network, we’re creating a whole class for LlamaBlocks, or
    smaller self-contained networks within our larger one. Now we have `RMSNormalization`,
    along with `RoPEMaskedMultiheadAttention` and a `SwiGLU` activation instead of
    `ReLU`. We’ve included the implementations in the notebook, so feel free to check
    them out if you are curious.
  prefs: []
  type: TYPE_NORMAL
- en: 'You’ll notice that our forward function is very different from the original
    feed forward. We’re no longer just embedding and then getting the logits from
    the embedding. Now we’re normalizing, adding attention, normalizing again, and
    then adding our logits to what comes out. This process helps the model integrate
    more nonlinearities into its overall considerations for how the input and desired
    output can line up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can compare the original feed-forward network with this `SimpleLlama`
    class to get an idea of what’s changed overall. First, instead of only having
    one `Sequential` block of layers, we have a number of `LlamaBlocks` equal to `n_layers`
    in our config, which is `8`, as you’ll see in the following code snippet. Beyond
    that, we’re using the SwiGLU activation everywhere instead of a ReLU. SwiGLU adds
    some ability to handle negative numbers and helps with exploding/vanishing gradients.
    Other than that, they’re remarkably similar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '#1 New'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 New'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 New'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can make some slight adjustments to our master config to make the model
    bigger by increasing the embedding dimension, the number of layers, and the context
    window. You don’t actually have to make that change to see the performance difference.
    If you had the compute, data, and time, you could train a viable version of Llama
    3 (you can see the results of this training in figure 9.3):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 New'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/9-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 Training simple Llama on our dataset to generate text
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: So we’ve made the 10× jump to over 180M parameters. Did it give us the emergent
    behavior we were looking for, though? If you look at the generated text, it’s
    making improvements in that it’s guessing punctuation more often, but almost none
    are in the correct place. The loss is higher, too, but we’re not particularly
    worried about that part; if we spruce up our data loading and allow the model
    to go all the way through the dataset two or three times, that should get lower.
    Lastly, if we make the model bigger by increasing the context window and number
    of layers, along with increasing the tokens in our dataset, we should be able
    to get that emergent behavior. For this dataset and config, you’d have to train
    ~1,900 times to go through the dataset once, so you’d have to train almost 6,000
    times to start taking advantage of the whole dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Given a lack of time and resources, we aren’t going to worry that our model
    isn’t at the top of any leaderboards. Heck, it’s not even good enough to get on
    one. But we have created a simple model that resembles Llama, and we have done
    so from scratch. This exercise has given us insights into the process, and you
    should have an idea of how to make it better. With these things in mind, let’s
    discuss how to put the model we’ve created into production.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3 Making it better
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a model and it’s passing all of our internal benchmarks (we’ll
    pretend that we had some), it’s time to deploy the model and see how it behaves
    with customers interacting with it. Oh no! The internal tests we had aren’t representative
    of our production environment! Our first problem is that the model is way too
    big and slow to even get through the prod environment tests. Models themselves
    are often looked at as being the main ingredient to success. In contrast, the
    systems we engineer around models, including the data, are overlooked because
    “anyone can hire a good MLE to make those.” Unfortunately, that’s now the secret
    sauce that causes some companies to succeed and others to fail.
  prefs: []
  type: TYPE_NORMAL
- en: We’d like to acknowledge to everyone rushing to the comments and GitHub Issues
    that this model doesn’t work because that isn’t the point of this chapter, and
    we’d like to point you toward creators like Abi Aryan, Sebastian Raschka, and
    others who are covering the data science of pretraining LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'NOTE  If you’d like to pretrain a causal language model that generates great
    content, there are other great resources available. Check out these projects for
    more information on pretraining your own model: Llama 3 ([https://mng.bz/BgAw](https://mng.bz/BgAw)),
    Megatron LM ([https://mng.bz/dZdg](https://mng.bz/dZdg)), Hugging Face Tutorial
    ([https://mng.bz/V2RN](https://mng.bz/V2RN)), and Llama2.c ([https://mng.bz/x6j7](https://mng.bz/x6j7)).'
  prefs: []
  type: TYPE_NORMAL
- en: In the spirit of continuing with data scientist–focused production advice, we’ll
    now cover how to make your model easier to deploy and more effective once it’s
    out there. Once a data scientist has trained a model and it passes the efficacy
    tests set, it’s time to think about size.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.1 Quantization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The first problem you’ll definitely be up against is sheer size. Our 180M parameter
    model is over 700 MB on disk, which is much bigger than some companies ever plan
    on serving for any use case. How do you make sure it’s small enough and quick
    enough to run in AWS lambda or in a CPU-only instance? Compression is one way
    to help us out here, and quantization is something built into PyTorch! As we’ve
    stated before, you should get familiar with BitsandBytes, but let’s look at a
    quick implementation that quantizes the model after training using `torch`.
  prefs: []
  type: TYPE_NORMAL
- en: In the next listing, we take our model, and using PyTorch, we’ll quantize the
    model to INT8\. The rest of the code and functions are simply to compare the model
    sizes before and after. The important bit is just the first couple of lines.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.5 Quantization
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Post training dynamic quantization'
  prefs: []
  type: TYPE_NORMAL
- en: You can see at the end that we go from almost 1 GB to 18 MB on disk by just
    going down to INT8 quantization. And we can go even lower,[³](#footnote-143) which
    can help you fit almost any model in the chosen production environment; just keep
    in mind that as you compress weights, perplexity goes up, resulting in less stable
    and predictable performance of the LLM, even with great prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: So now that the model is small enough, the MLOps team puts it into the dev environment,
    and all of the tests pass, so our model finally made it to prod. All is well,
    right?
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.2 LoRA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What do we do when, one month down the road, we have data showing our model
    is unable to perform a particular task up to the standards of its environment?
    We have data drift, and because we’re a startup, we don’t have the money or time
    to go through the rigorous training process we went through before to train a
    model from scratch. There’s a bigger problem too: we don’t have enough new data
    illustrating the new distribution to finetune the model effectively. This situation
    is perfect for training a LoRA to tweak the model rather than spending all that
    time training it over again.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.6 shows you how to train a LoRA model and the adjustments we need
    to make to our Llama model. This listing shows first what adding a LoRA does to
    the inputs as they move through the model. The `LoRALayer` class is shown in clear
    PyTorch terms by Sebastian Raschka and Lightning.AI, and they have repos going
    into even more depth (see [https://github.com/rasbt/dora-from-scratch](https://github.com/rasbt/dora-from-scratch)
    and [https://mng.bz/Aa8e](https://mng.bz/Aa8e)). Next, it shows how our `SimpleLlama`
    class changes after we’ve added a LoRA to it. Lastly, we’ll go through a similar
    training process using a new instruct dataset and a new `get_batches` function.
    As a note, we use several helper functions throughout this listing to simplify
    it; you can find their definitions in the repository accompanying this book.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.6 Low-rank adaptation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 What does LoRA actually do?'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Shows how the blocks change'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 New'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 New'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 New'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 New dataset for LoRA'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Step 1: Adds LoRA to the trained model'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Step 2: Gets the LoRA params instead of the whole model''s'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Step 3: Initializes optimizer with LoRA params'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Step 4: Trains'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Step 5: Exports the params'
  prefs: []
  type: TYPE_NORMAL
- en: 'All of that results in two separate state dicts for us to save: the model and
    the LoRA. You can train LoRAs for a variety of specific tasks for which you may
    not have a large enough dataset to justify a whole finetuning. LoRA files on disk
    are usually only kilobytes even for very large models, depending on the size of
    the rank (in our case, 16).'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can inference using a LoRA generally in two ways: you can (1) load the
    original model’s state dict (ours is loaded within the `llama` variable), load
    the LoRA on top of it, and then inference as normal, or (2) merge all of the LoRA
    layers into the original Llama and essentially create a new model and inference
    normally. Here, we adopt the second option.'
  prefs: []
  type: TYPE_NORMAL
- en: '**##### Listing 9.7 Low-rank adaptation'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The generated text is
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the text still isn’t as coherent as we’d like it to be; however,
    we can see a definite change in the generation compared to the simple Llama. No
    more overzealous punctuation, “cry,” and other nonhappy small story words are
    present, and there are more clearly made-up words. If you train on a more distinct
    set—say, Shakespeare—you’ll be able to see the difference even more clearly, and
    the nice thing about LoRA is that you can simply `remove_lora()` to get the original
    functionality back.
  prefs: []
  type: TYPE_NORMAL
- en: 9.3.3 Fully sharded data parallel–quantized LoRA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building upon LoRA, quantized LoRA (QLoRA) allows for efficient fine-tuning
    of models larger than your GPU. It does this by quantizing the model and then
    training a LoRA on the frozen version of that quantized model. This technique
    is desirable when you look at how much memory it takes to finetune full-size models,
    even in half-precision. As we previously discussed, a 70B parameter model ends
    up being 140 GB on disk and will take more than five times that much memory to
    finetune because of the dataset and gradients. With QLoRA, we can train up to
    65B parameters on only 48 GB of VRAM—a very noticeable reduction. QLoRA is currently
    the most effective way of taking an absurdly large model and productionizing it
    for your use case, and it saves tons of money for that process too.
  prefs: []
  type: TYPE_NORMAL
- en: Add to this fully sharded data parallel (FSDP), and you can break the consumer
    versus enterprise barriers. Some of you have likely been asking where parallelism
    has been this whole time, and here it is. FSDP allows for both data and model
    parameter parallelism throughout the entire training process on multiple GPUs,
    and it takes care of the sharding as well as the rejoining on the other end when
    order and magnitude matter. It’s amazing work coming from the team that maintains
    PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Previously, 48 GB for QLoRA on a 70B parameter model was only possible using
    an enterprise GPU like an A100\. With FSDP, you can take full advantage of parallelism
    on consumer hardware, like two 3090s, to get the same result. FSDP is native to
    PyTorch! Unlike our previous efforts in this chapter, we will now abstract a script
    created by Jeremy Howard and Answer.AI so that you can just run it in one of the
    cells on a 7B parameter model. Instead of needing to clone an entire GitHub repo,
    you can install and import `fsdp_qlora` from PyPI, and we’ve recreated the importable
    class in the `train_utils` folder. This code will execute fully parallel QLoRA
    training on as many GPUs as you have access to.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.8 FSDP-QLORA training
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The result of this running is a fully finetuned safetensors model file trained
    using quantized weights and parallelism. Unlike our bespoke pretrained version,
    this one works. The safetensors file contains a state dict file for the trained
    model, similar to the state dict we saved for the `SimpleLlama`. Both of those
    state dicts need to be converted into a full model file or a full checkpoint file
    before they can be uploaded to a place like Hugging Face; otherwise, classes like
    `AutoModel` or `LlamaForCausalLM` won’t be able to load your model later.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4 Deploy to a Hugging Face Hub Space
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spaces are hosted containers where you can put models to allow community access,
    and they can be much more than that, depending on your needs. Spaces can be the
    place your company uses to deploy its whole model, as opposed to other cloud-hosting
    options. Spaces have a free tier and many paid tiers, depending on how compute-intensive
    your particular application is. Spaces integrate seamlessly with the most popular
    ML frontend stacks, namely Streamlit, Gradio, and FastAPI.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  We won’t be giving examples of these ML frontend stacks here, as we’ve
    given them in previous chapters, but we did include an example app in the notebook
    for this chapter. For reference, check out the documentation for Gradio ([https://www.gradio.app/guides/quickstart](https://www.gradio.app/guides/quickstart))
    and Hugging Face ([https://huggingface.co/docs/hub/spaces](https://huggingface.co/docs/hub/spaces)).
  prefs: []
  type: TYPE_NORMAL
- en: With our models, we’ll need to convert their weights and directories into a
    format easily pushed to the Hugging Face Hub for our Space. We have an easily
    modified script that you can use to make this conversion. You can also run this
    on the simple Llama LoRA trained earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.9 Converting weights for Hugging Face
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: If you already have a repo and are logged in to your Hugging Face account, you
    can go ahead and run `model.push_to_hub()`. This will create a repo for your model
    if it doesn’t already exist. The reason you would or wouldn’t push to the hub
    has to do with whether you want to share your model with the world. If you’d rather
    have a space where others can try out your model (even for free), we’ll show how
    to do that next.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first decisions to be made for a Space are how much compute your app requires
    and how you’ll maintain the code for the Space—with Git or with `huggingface-cli`.
    The first question starts with whether a GPU is required for your particular use
    case; for ours, it is not. However, when you need a speed or scale increase, you
    will likely need it, especially if you get into multiprocessing to get more performance
    out of the Space. Once you have your app and you’ve figured out your memory requirements,
    if you’ve decided to use Git, you’ll make your Space on Hugging Face, and then
    you’ll clone it the same way you would something on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Adding, committing, and pushing are the same as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: If you’re not doing it through the CLI, the following listing shows you how.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 9.10 Hugging Face Space
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '#1 If you haven’t created your repo yet'
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face Spaces
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The models, as we currently have them, require GPUs to load (especially quantized)
    and run. If you attempt to run on the free tier of HF Spaces, it will error out,
    as it did for us. You can fix this by upgrading to a paid tier or ZeroGPU. Hugging
    Face provides a version of a Gradio app that uses its own API only to provision
    a GPU for the amount of time it takes to complete a task and only when it’s requested.
    See [https://mng.bz/XV11](https://mng.bz/XV11).
  prefs: []
  type: TYPE_NORMAL
- en: As an exercise, we encourage you to think through and build out how you might
    be able to create a Hugging Face Space using our LLM that would run on the free
    tier, which is considerably easier than when we were first writing this, thanks
    to ZeroGPU.
  prefs: []
  type: TYPE_NORMAL
- en: And there we have it—a fully functioning hosted instance of any model you want
    to use or train. You can run either of the two Llama models we trained in the
    Space, but you’ll need to do a bit of engineering around it depending on your
    needs. Congratulations on finishing the first project if you ran all of this code
    with your own environment! This was one of the denser chapters, and making it
    through with a working example is something to be proud of. Hugging Face provides
    private solutions to enterprises looking to use Spaces long term, and this is
    a completely viable production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Choosing an appropriate tokenizer and embedding strategy is one of the first
    crucial decisions you’ll make when creating a model from scratch, as it determines
    what the model will see and, therefore, is capable of.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your unique data sources future-proof your model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The main differences between Llama and a simple feed-forward are the normalization,
    attention, activation layers, and number of layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Often, the first challenge to productionizing an LLM is its size: quantization
    to the rescue!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In production, it’s only a matter of time before you’ll need to update the model.
    LoRA and QLoRA are perfect solutions to make minor tweaks to your model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fully sharded data parallelism allows us to train QLoRA models cheaply on consumer
    hardware.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A great option to deploy and share your LLM project is Hugging Face Hub Spaces
    due to their ease of use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[[1]](#footnote-source-1) H. Touvron et al., “Llama 2: Open foundation and
    fine-tuned chat models,” arXiv.org, July 19, 2023, [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[2]](#footnote-source-2) [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/),
    [https://arxiv.org/pdf/2307.09288](https://arxiv.org/pdf/2307.09288), [https://arxiv.org/pdf/2302.13971](https://arxiv.org/pdf/2302.13971).'
  prefs: []
  type: TYPE_NORMAL
- en: '[[3]](#footnote-source-3) S. Ma et al., “The Era of 1-bit LLMs: All Large Language
    Models are in 1.58 Bits,” arXiv.org, Feb. 27, 2024, [https://arxiv.org/abs/2402.17764](https://arxiv.org/abs/2402.17764).**'
  prefs: []
  type: TYPE_NORMAL
