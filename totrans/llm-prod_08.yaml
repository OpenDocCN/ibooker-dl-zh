- en: '9 *Creating an LLM project: Reimplementing Llama 3*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 *创建一个LLM项目：重新实现Llama 3*
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Implementing Meta’s Llama3 model
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现Meta的Llama3模型
- en: Training a simple LLM
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个简单的LLM
- en: Making improvements to it to prepare it for production
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对其进行改进以准备生产
- en: Serving the model to a production endpoint you can share with your friends
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型服务于一个可以与朋友分享的生产端点
- en: I am only coming to Princeton to research, not to teach. There is too much education
    altogether, especially in American schools. The only rational way of educating
    is to be an example.—Albert Einstein
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我来普林斯顿只是为了研究，而不是教学。教育太多了，尤其是在美国学校。唯一合理的教育方式是树立榜样。——阿尔伯特·爱因斯坦
- en: For the first major project in the book, we want to start from scratch. We’ve
    been showing you how to work with LLMs from end to end, and we are going to put
    it all together in this chapter. This project includes pretraining a model, roughly
    following a research paper. We won’t dive too deeply into the actual research;
    in fact, we’ll take several shortcuts here, as this isn’t the focus of this book.
    We will, however, showcase how to train the model, prepare it for servings with
    quantization, finetune it with low-rank adaptation (LoRA) for a specific purpose
    or task, and deploy it to a production environment you can showcase to your friends.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的第一个主要项目中，我们希望从头开始。我们一直在向您展示如何从头到尾与LLMs（大型语言模型）一起工作，我们将在本章中将所有内容整合在一起。这个项目包括预训练一个模型，大致遵循研究论文。我们不会深入实际研究；事实上，我们会在这里走一些捷径，因为这不是本书的重点。然而，我们将展示如何训练模型，通过量化准备它以供服务，通过低秩适应（LoRA）针对特定目的或任务进行微调，并将其部署到您可以向朋友展示的生产环境中。
- en: This chapter will be very dense, but you should be more than prepared to meet
    the challenge at this point because it’s mainly a data scientist–focused project
    for production. We chose this project so that you can put all the lessons you’ve
    learned throughout the book together into one place and leave you with end-to-end,
    hands-on experience.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将非常密集，但您应该已经做好了迎接挑战的准备，因为这是一个主要面向生产的数据科学家项目。我们选择这个项目是为了让您将本书中学到的所有课程整合到一个地方，并给您留下端到端、动手实践的经验。
- en: 9.1 Implementing Meta’s Llama
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 实现Meta的Llama
- en: '“Llama 2: Open Foundation and Fine-Tuned Chat Models” by Touvron et al.[¹](#footnote-141)
    is an awesome paper that covers the development and release of Llama 2, one of
    the best, almost open source models currently on the market. You may have seen
    Llama 2 as the first open source model that was good enough to rival OpenAI’s
    models, at least based on the metrics of the time. Llama 3 is out now, and it
    has almost completely eclipsed Llama 2 in popularity and may very well be why
    you picked up this book.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: “Touvron等人撰写的《Llama 2：开放基础和微调聊天模型》[¹](#footnote-141)”是一篇很棒的论文，涵盖了Llama 2的开发和发布，这是目前市场上最好的、几乎开源的模型之一。您可能已经看到Llama
    2作为第一个足够好的开源模型，可以与OpenAI的模型相媲美，至少基于当时的指标。现在Llama 3已经发布，它几乎完全超越了Llama 2的受欢迎程度，这可能是您选择这本书的原因。
- en: 'Llama 3 is amazing for a couple of reasons—namely, size and availability. With
    only 70B parameters, pretrained on only 15T tokens, and finetuned on 100K chats,
    it shouldn’t be able to beat a 176B or a 1.7T parameter model at anything. Unsurprisingly,
    it usually doesn’t. But it does beat them at one crucial thing: its availability.
    This feature has given rise to an open source software community that has made
    tooling and optimizations and even gathers data to make it better. Llama 3 is
    the ultimate showcase that architecture is less important than data, and it is
    trained on clean data.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 3有几个令人惊叹的原因——主要是大小和可用性。只有70B参数，在15T个标记上预训练，并在10K个聊天上微调，它不应该在任何方面击败176B或1.7T参数的模型。不出所料，它通常做不到。但它确实在一件关键的事情上打败了它们：它的可用性。这个特性催生了一个开源软件社区，该社区开发了工具和优化，甚至收集数据以使其更好。Llama
    3是架构不如数据重要的终极展示，它是在干净的数据上训练的。
- en: And we’re going to implement it.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现它。
- en: By the end of this chapter, you will build a real model and understand the work
    that goes into it. Will it be as good as Meta’s Llama 3? Far from it, because
    we won’t be demonstrating with an adequate amount of data or GPUs. But we want
    to do more than simply supply you with yet another set of weights that are on
    some leaderboard somewhere. We want to give you some intuition for the steps required
    and the potential problems you may face. Instead of training a great model completely
    from scratch, which is what dozens of other books are tackling right now, we’ll
    show you how to train a below-average model and productionize it. This approach
    should have you not only learning more but demonstrating expertise beyond your
    experience level.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将构建一个真实模型并理解其背后的工作。它会是Meta的Llama 3那么好吗？远非如此，因为我们不会使用足够的数据或GPU进行演示。但我们想做的不仅仅是提供另一组在某个排行榜上的权重。我们希望给你一些对所需步骤和可能遇到的问题的直观感受。我们不会从头开始训练一个优秀的模型，这正是现在几十本书正在解决的问题，而是会向你展示如何训练一个低于平均水平的模型并将其投入生产。这种方法不仅能让你学到更多，还能展示出超出你经验水平的专长。
- en: 9.1.1 Tokenization and configuration
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 分词和配置
- en: By this point, you’ve likely already learned the importance of setting up the
    problem correctly. We want our models hitting tee-balls out of the park, not going
    up against an MLB pitcher. With that in mind, we’ll download the same tokenizer
    that Llama used. If you want, you can come back and experiment with this tokenizer
    since we are building from scratch. For example, try to use a faster tokenizer
    like tiktoken—just know you’ll be giving up the model’s ability to do math. You
    can also train your own version of the SentencePiece model, which should guarantee
    better results on whatever dataset you want to extend this with. The point is
    that this model is blank—no pretrained weights at all. So come back and do whatever
    you’d like after following along.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能已经学会了正确设置问题的重要性。我们希望我们的模型能够打出全垒打，而不是与MLB投手对决。考虑到这一点，我们将下载Llama使用的相同分词器。如果你想的话，可以在我们从零开始构建时回来尝试这个分词器。例如，尝试使用像tiktoken这样的更快分词器——只是要知道你将放弃模型进行数学运算的能力。你还可以训练自己的SentencePiece模型版本，这应该能保证在你想扩展的任何数据集上都能得到更好的结果。重点是，这个模型是空的——没有任何预训练的权重。所以，在跟随我们之后，回来做你想做的任何事情。
- en: NOTE  Unlike other chapters where each listing was stand alone, in this chapter,
    each listing will be part of a larger notebook. You can find this notebook in
    the code repository accompanying this book.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：与其他章节中每个列表都是独立的章节不同，在本章中，每个列表都将成为更大笔记本的一部分。你可以在本书附带的代码仓库中找到这个笔记本。
- en: Listing 9.1 shows our initial setup for this project, including imports, device
    settings, and grabbing our tokenizer. While we’ll just be grabbing the tokenizer
    from Hugging Face, keep in mind that not all tokenizers and models use the same
    type of tokens. This is important because we’re going to train this model differently
    than the way the inference tokenizer is set up for. To correct for this discrepancy,
    we’ll need to add a padding token. Anything would do, but we’ll use `"<PAD>"`
    in our example. Once we have that, we’ll make sure to grab the vocab itself (we’ll
    need it later) and create encoding and decoding functions to help with batch processing.
    Because we’re using the Hugging Face implementation, this isn’t strictly needed
    because it has batch tokenization built in, along with a `batch_decode` method
    that works great. For learning’s sake, we’ll go through the motions anyway. It’s
    always good practice to be aware of what you’re doing, and these functions help
    lock that down.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.1展示了我们为这个项目进行的初始设置，包括导入、设备设置和获取我们的分词器。虽然我们只是从Hugging Face获取分词器，但请记住，并非所有分词器和模型都使用相同类型的标记。这很重要，因为我们打算以不同于推理分词器设置的方式训练这个模型。为了纠正这种差异，我们需要添加一个填充标记。任何东西都可以，但我们的例子中将使用`"<PAD>"`。一旦我们有了这个，我们就会确保获取词汇表本身（我们稍后会用到它），并创建编码和解码函数以帮助批量处理。因为我们使用的是Hugging
    Face实现，所以这并不是严格必要的，因为它内置了批量分词，以及一个工作得很好的`batch_decode`方法。为了学习的目的，我们仍然会走一遍流程。了解自己在做什么总是好的实践，这些函数有助于锁定这一点。
- en: The last part of this listing offers the most flexibility. Here, we set up a
    master config that will ultimately decide how many parameters our model has, how
    long it trains, and how much memory it will take per row in our dataset. Our default
    values are pretty small and designed to give you a good experience regardless
    of your hardware, including if you’re training on a CPU-only build. Feel free
    to experiment and crank up the numbers.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 列表的最后部分提供了最大的灵活性。在这里，我们设置了一个主配置，它最终将决定我们的模型有多少参数，训练多长时间，以及每行数据集将占用多少内存。我们的默认值相当小，旨在无论您的硬件如何，都能为您提供良好的体验，包括如果您在仅
    CPU 构建的机器上训练。请随意实验并增加这些数字。
- en: Listing 9.1 Tokenize and config
  id: totrans-19
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.1 分词和配置
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '#1 Uses Hugging Face'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 使用 Hugging Face'
- en: '#2 Optional'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 可选'
- en: As we’ve reiterated a number of times throughout the book, remember that the
    strategy you use to tokenize and embed your inputs ultimately dictates what your
    model is able to “see” during training and inference. You should generally do
    a bit more than just choose a tokenizer; in fact, we’ll see later in this chapter
    what choosing the Llama 3 tokenizer will do to our inference.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本书中多次重申的那样，您用于分词和嵌入输入的策略最终决定了您的模型在训练和推理过程中能够“看到”的内容。您应该做的不仅仅是选择一个分词器；实际上，我们将在本章后面看到选择
    Llama 3 分词器会对我们的推理产生什么影响。
- en: You could opt for training a new tokenizer on your dataset or adding especially
    important tokens from your dataset to an already-robust tokenizer—preferably one
    that already generally matches the strategy you want and is trained in the domain
    you need. If you aren’t sure about any of that, any LLM tokenizer should generally
    work—that’s what they’re designed for. But don’t be surprised when the model doesn’t
    perform well when you pick a general tokenizer and want a specific task.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择在您的数据集上训练一个新的分词器，或者将您数据集中特别重要的标记添加到一个已经非常健壮的分词器中——最好是已经大致符合您想要策略并且在该领域训练过的分词器。如果您对此有任何疑问，任何
    LLM 分词器通常都适用——这就是它们被设计的目的。但是，当您选择一个通用分词器并希望执行特定任务时，如果模型表现不佳，请不要感到惊讶。
- en: 9.1.2 Dataset, data loading, evaluation, and generation
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 数据集、数据加载、评估和生成
- en: Let’s get into the most important part of this process, which we will, for the
    most part, gloss over. There’s only so much we can focus on in one chapter, but
    we want to reiterate how important your dataset is to the success of your LLM.
    You’ll want to spend time gathering, evaluating, and cleaning your dataset, but
    we’ll shortcut that process in the interest of time. Instead, we’ll focus on the
    steps necessary to train the model—loading, preprocessing, batching, and so forth.
    As we go through this section, remember that your unique data sources end up future-proofing
    your model, so consider what data you have access to that no one else does and
    how you’d set that dataset up for this training.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解这个过程中的最重要部分，我们将大部分时间都会略过。在一章中我们只能关注这么多，但我们想重申您的数据集对您的 LLM 成功的重要性。您将需要花费时间收集、评估和清理您的数据集，但为了节省时间，我们将简化这个过程。相反，我们将专注于训练模型所需的步骤——加载、预处理、批处理等等。在阅读这一部分时，请记住，您独特的数据源最终会使您的模型具有前瞻性，因此考虑您能访问到而其他人没有的数据，以及您如何为这次训练设置该数据集。
- en: We’ll start by loading a dataset that’s generally popular for creating toy models,
    TinyStories. If you did the work to explore your data—and we encourage you to
    do it—you’ll see that this is a smallish dataset for LLMs, containing only 30
    million rows, each containing a short story in a single paragraph. It draws from
    some oft-implemented and widely accepted datasets. While a small dataset for LLMs,
    it’s likely still too large for many computers, and many readers will likely hit
    out-of-memory errors if they try to load it into memory wholesale. Here’s the
    perfect time to use streaming. In listing 9.2, we show you how to pull the dataset
    from the Hugging Face Hub or `dataset.to_iterable_ dataset()` if working locally.
    Both methods allow for much more memory-efficient processing, as the whole dataset
    isn’t loaded all at once, sacrificing some speed.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先加载一个通常用于创建玩具模型的流行数据集，TinyStories。如果您已经完成了探索数据的工作——我们鼓励您这样做——您会发现这是一个相对较小的
    LLM 数据集，包含 3000 万行，每行包含一个段落中的短篇故事。它借鉴了一些经常实施且广泛接受的数据集。虽然对于 LLM 来说是一个较小的数据集，但它可能仍然对许多计算机来说太大，许多读者在尝试一次性将其加载到内存中时可能会遇到内存不足错误。这正是使用流式处理的好时机。在列表
    9.2 中，我们向您展示了如何从 Hugging Face Hub 或 `dataset.to_iterable_dataset()`（如果本地工作）中提取数据集。这两种方法都允许进行更高效的内存处理，因为整个数据集不是一次性加载的，这牺牲了一些速度。
- en: Listing 9.2 Loading and preparing the data
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.2 加载和准备数据
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '#1 Streams from the local files'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 从本地文件流中获取'
- en: 'Once you have your dataset and are able to retrieve an iteration, we’ll do
    some minimal (truly) cleaning. Then we’ll encode the whole thing so that our training
    can go quicker down the line. We’ll save the tokenization and attention masks
    as their own columns, and then we’ll shuffle the dataset and go on to dataloading.
    A quick note that’s always worth mentioning: when training any machine learning
    model, if you don’t already have your `train` and `val` splits defined, take extra
    care shuffling your dataset so that none of the data leaks into a split where
    it shouldn’t be:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了你的数据集并且能够检索到一个迭代，我们将进行一些最小（真正）的清理。然后我们将整个内容进行编码，以便我们的训练可以更快地进行。我们将保存分词和注意力掩码作为它们自己的列，然后我们将数据集进行洗牌，并继续进行数据加载。一个总是值得提到的快速提示：在训练任何机器学习模型时，如果你还没有定义你的`train`和`val`拆分，请特别注意洗牌你的数据集，以确保数据不会泄漏到不应该出现的数据拆分中：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '#1 Minimal processing'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 最小处理'
- en: If you disregard our advice to stream and have a computer that can handle this
    dataset, know that loading the entire dataset into memory and then preparing it,
    even using hardware acceleration, takes over 30 minutes and more than 5 GB of
    memory. So if you have an extra 5 GB of VRAM outside of what you’ll need for your
    model, you’re good to go ahead and load it however you want. See figure 9.1.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你忽视了我们的建议进行流式处理，并且有一台可以处理这个数据集的计算机，那么请知道，将整个数据集加载到内存中并进行准备，即使使用硬件加速，也需要超过
    30 分钟和超过 5 GB 的内存。所以如果你有额外的 5 GB VRAM，而且这超出了你为模型所需的内存，那么你可以随意加载它。见图 9.1。
- en: '![figure](../Images/9-1.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/9-1.png)'
- en: Figure 9.1 With over 30 million rows, this dataset is pretty small for what
    we’re trying to do, but it is still substantial on consumer hardware.
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.1 虽然这个数据集有超过 3000 万行，对于我们想要做的事情来说可能很小，但在消费级硬件上仍然是相当大的。
- en: 'We’ll need at least one function to load our data into a ready-to-use format
    for our model, and we’re opting to use just that. Our `get_batches` function will
    take in one row of our data and return a model input and an expected output that
    can be compared against it for self-supervised learning. No labeling is needed,
    as we’ll start on a random token, then grab tokens up to our whole context window
    (32) for our input, and shift one token to the right for our expected output.
    For our model, we create a scenario that looks like this:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们至少需要一个函数将我们的数据加载到模型可用的格式中，我们选择使用这个函数。我们的`get_batches`函数将接收我们数据的一行，并返回一个模型输入和一个预期的输出，可以与之进行比较以进行自监督学习。不需要标签，因为我们将从随机标记开始，然后抓取直到整个上下文窗口（32）的标记作为输入，并将一个标记向右移动作为预期的输出。对于我们的模型，我们创建了一个类似这样的场景：
- en: CB **input:** How much wood could a woodchuck chuck if a woodchuck could chuck
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: CB **输入:** 如果一只土拨鼠能够推木头，它能推多少木头
- en: CB **label:** How much wood could a woodchuck chuck if a woodchuck could chuck
    wood?
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: CB **标签:** 如果一只土拨鼠能够推木头，它能推多少木头？
- en: 'This process allows our model to train on our task: guessing the next token
    in an utterance, given the context of the previous 31 tokens. We use this strategy
    instead of other strategies like masking because our preferred inputs will never
    contain information after the input is completed. This way, our model will get
    better and better at text completion the more and higher-quality data it trains
    on. Almost all foundation models are pretrained in this manner—only they train
    for much longer with many more parameters than we will right now:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程允许我们的模型在任务上进行训练：根据前 31 个标记的上下文猜测一个话语中的下一个标记。我们使用这种策略而不是其他策略，如掩码，因为我们的首选输入在输入完成后永远不会包含信息。这样，我们的模型将随着训练数据的更多和更高质量而越来越好。几乎所有的基础模型都是以这种方式预训练的——只是它们训练的时间更长，参数比我们现在使用的要多得多：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '#1 Windows users leave commented'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 窗口用户请注释'
- en: '#2 Adjust this lower if you''re running out of memory.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 如果内存不足，请调整此值。'
- en: '#3 Pick random starting points.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 选择随机起始点。'
- en: 'Once we have our data batching taken care of, we need to come up with functions
    for evaluation and inference so that we can gain insight into how the model is
    doing during training and so that we can use the model later. For our evaluation,
    we’ll take some batches and average the loss across them to get our validation
    loss. This result will not give us a real representation of our model’s performance
    but will not stop it from being useful for us:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们处理好了数据批处理，我们需要为评估和推理编写函数，以便我们可以在训练过程中了解模型的表现，并且可以在之后使用该模型。对于我们的评估，我们将取一些批次，并计算这些批次之间的平均损失以获得验证损失。这个结果不会给我们模型性能的真实表示，但不会阻止它对我们有用：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Questioning your assumptions
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 对你的假设进行质疑
- en: When working with machine-learning models and other statistical methods, it’s
    important to understand how your assumptions will affect your results. Averages
    hamper data representation and understanding because they basically say, “For
    this comparison, we’re going to grab a made-up number, and we’re going to use
    that number in place of any of the real ones because it feels central to our distribution.”
    This approach doesn’t make it bad; made-up numbers often are more predictive than
    real ones. However, we urge you to be intentional and very open-minded about testing
    whether the average is the best marker for your users.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当与机器学习模型和其他统计方法一起工作时，了解你的假设如何影响你的结果非常重要。平均值阻碍了数据的表现和理解，因为它们基本上是说：“对于这个比较，我们将取一个虚构的数字，并将该数字用于替代任何真实的数字，因为它感觉是我们分布的中心。”这种方法并不一定不好；虚构的数字往往比真实的数字更有预测性。然而，我们敦促你有意并且非常开放地测试平均值是否是你用户最好的标记。
- en: For generation, we’ll do something similar but better. Logits are what we get
    out of our model’s forward method. We created a tokenized version of our prompt
    previously when we tokenized our dataset, so we’re ready to pass that prompt into
    our model a number of times and see what comes out. We’ll grab the logits from
    the model given the prompt and then sample our model’s distribution for our next
    token and decode.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于生成，我们将做类似但更好的事情。logits是我们从模型的前向方法中得到的。我们在对数据集进行标记化时创建了一个提示的标记化版本，所以我们准备多次将这个提示传递给我们的模型并查看结果。我们将从模型中获取给定提示的logits，然后为下一个标记采样模型分布并解码。
- en: 'For sampling that distribution, we’ll take the model’s output (logits) for
    only the very end of the input (the unknown token we want our model to generate)
    and then divide those logits by the temperature setting (higher temperature setting
    = smaller logits). Once we have our logits from the last time step, if we use
    multinomial sampling, we can sample using `top_k` and/or `top_p`, which are sampling
    against the highest probability tokens until you reach a total number of tokens
    or a total number of probability sums. Once we have that, we use softmax for the
    tokens we’ve sampled and then argmax to get the next token. If we want more exploration
    and creativity in our output, we can use multinomial sampling instead. As an exercise,
    test `top_k` versus `top_p` with multinomial and argmax versus multinomial to
    get an idea of which works best:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于采样这个分布，我们将只取模型输出的logits（logits）作为输入的最后一个部分（我们希望模型生成的未知标记），然后将这些logits除以温度设置（更高的温度设置=更小的logits）。一旦我们有了最后一个时间步的logits，如果我们使用多项式采样，我们可以使用`top_k`和/或`top_p`进行采样，这些采样是对最高概率标记的采样，直到达到标记总数或概率总和总数。一旦我们有了这些，我们使用softmax对采样的标记进行操作，然后使用argmax获取下一个标记。如果我们希望输出有更多的探索和创造力，我们可以使用多项式采样。作为一个练习，测试多项式采样与argmax之间的`top_k`与`top_p`，以了解哪个效果最好：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '#1 Calls the model'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 调用模型'
- en: '#2 All the batches (1); last time step, all the logits'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 所有批次（1）；上一次步骤，所有logits'
- en: '#3 Softmax to get probabilities'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 使用Softmax获取概率'
- en: '#4 Sample from the distribution to get the next token'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 从分布中采样以获取下一个标记'
- en: And with that, we’ve concluded our setup! We have utility functions for all
    of the important parts of the model training, including tokenization, data loading,
    evaluation, inference, and data processing. If there’s anything you feel should
    be corrected, great! Do it—this is your project. If you want to use a multinomial
    for sampling instead of an argmax or want to get rid of the softmax and just argmax
    over the logits, awesome, go for it. For those of you for whom this is your first
    time, we know it can be quite a firehose, and we’d encourage you to work through
    it slowly, but don’t lose too much sleep over it. More than likely, you will not
    have to come up with what should change for your use case yourself because you’ll
    be implementing an already-created open source model. That said, it’s still a
    good idea to understand what’s going on behind the scenes and under the hood so
    that you know roughly where to look when things go wrong.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们就完成了设置！我们为模型训练的所有重要部分都提供了实用函数，包括分词、数据加载、评估、推理和数据处理。如果你觉得有任何需要纠正的地方，那太好了！去做吧——这是你的项目。如果你想用多项式分布进行采样而不是argmax，或者想去掉softmax只对logits进行argmax，那太棒了，去做吧。对于那些这是你们第一次尝试的人来说，我们知道这可能会相当快节奏，我们鼓励你们慢慢来，但不要因此失眠。更有可能的是，你不需要自己想出针对你用例应该改变什么，因为你将实现一个已经创建的开源模型。话虽如此，了解幕后和底层发生的事情仍然是一个好主意，这样你知道当事情出错时大致该往哪里找。
- en: 9.1.3 Network architecture
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.3 网络架构
- en: 'We’ve now completed a ton of setup for training a model but haven’t made a
    model. Model architecture and training have been iterated upon ad nauseam, so
    we’ll skip talking about it too much and jump right in. We’ll start with a two-layer
    feed-forward network with fewer than 20M parameters, and then we’ll upgrade and
    talk about the changes that turn the model into Llama. We want to be clear about
    what is actually changing between them so you’ll get a good feel for the pieces
    involved. Because we aren’t going to be completely replicating Llama 3, but rather
    approximating it, here’s the official architecture if you’d like to try pretraining
    it on our dataset: [https://mng.bz/Dp9A](https://mng.bz/Dp9A).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经为训练模型做了大量的设置，但还没有创建模型。模型架构和训练已经被反复迭代，所以我们不会过多地讨论它，而是直接进入正题。我们将从一个少于20M参数的两层前馈网络开始，然后我们将升级并讨论将模型转变为Llama的变化。我们希望清楚地说明它们之间实际发生了什么变化，这样你就能对涉及的各个部分有一个良好的感觉。因为我们不会完全复制Llama
    3，而是近似它，如果你想在我们的数据集上尝试预训练它，这里是有官方架构的链接：[https://mng.bz/Dp9A](https://mng.bz/Dp9A)。
- en: In listing 9.3, we make a class for that linear model with a ReLU activation
    between the two linear layers. Here’s where we’ll also define our actual loss
    function (because in our `get_loss` function, we’re just sending inputs to the
    model). We’ll use cross entropy because we’re comparing unstructured sequences.
    Instead of getting into information theory for why cross-entropy is the answer
    to unstructured sequences, the current benchmark in the industry is called perplexity,
    which uses cross-entropy to figure out whether a model is making sense or not,
    so this loss function enables us to compete with other models in the industry.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表9.3中，我们为那个具有ReLU激活函数的线性模型创建了一个类，位于两个线性层之间。这里我们将定义我们的实际损失函数（因为在我们的`get_loss`函数中，我们只是将输入发送到模型）。我们将使用交叉熵，因为我们正在比较非结构化序列。而不是深入信息理论来解释为什么交叉熵是非结构化序列的答案，当前行业中的基准是困惑度，它使用交叉熵来确定模型是否在有意义，因此这个损失函数使我们能够与其他行业中的模型竞争。
- en: 'There’s one thing that you may have noticed before when we tokenized our dataset:
    we’re padding in batches and not truncating, meaning each batch size should be
    the same length. We fully acknowledge that this doesn’t make any sense when pretraining;
    it’s just helping to speed things up. We do this because our longest length input
    is 997 tokens, and we don’t want to pad our entire dataset out to 997\. Even mitigating
    that, the most common token in our dataset is still `"<PAD>"`. If we leave it
    as is, the model could learn to generate only padding tokens, which seemingly
    minimizes the loss when predicting the next token. Because we have a tokenizer
    vocab we just added to, however, we can tell the loss function to `ignore_index`
    our `tokenizer .pad_token_id` so correctly predicting padding tokens doesn’t mistakenly
    help the loss go down.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前对数据集进行标记化时，你可能已经注意到了一件事：我们在批量中填充而不是截断，这意味着每个批次的长度应该相同。我们完全承认，在预训练时这并没有什么意义；这只是为了加快速度。我们这样做是因为我们的最长长度输入是997个标记，我们不想把整个数据集填充到997个标记。即使这样，我们数据集中最常见的标记仍然是`"<PAD>"`。如果我们保持原样，模型可能会学会只生成填充标记，这在预测下一个标记时似乎可以最小化损失。然而，由于我们刚刚添加了标记化器词汇表，我们可以告诉损失函数忽略`tokenizer.pad_token_id`的`ignore_index`，这样正确预测填充标记就不会错误地帮助损失下降。
- en: Listing 9.3 Simple model and training loop
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.3 简单模型和训练循环
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '#1 Comment this out on Windows.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 在Windows上取消此注释。'
- en: 'Now that we have our model, we’ll write our training loop. We run the number
    of passes specified in the epochs portion of the master config, and we get our
    loss for each pass. The epochs here are more like steps, and we’d encourage you
    to run epochs through the whole dataset if you have the time. If you stick to
    the `MASTER_CONFIG` we set up previously, this original model will end up having
    18.5M parameters. You should definitely change it to be the maximum number of
    parameters that your computer can handle. You can find this number by changing
    `d_model` (and `vocab_size` if you train a bigger tokenizer) in your master config:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的模型，我们将编写我们的训练循环。我们运行主配置文件中指定轮次的次数，并得到每次迭代的损失。这里的轮次更像是步骤，我们鼓励你在有时间的情况下在整个数据集上运行轮次。如果你坚持使用我们之前设置的`MASTER_CONFIG`，这个原始模型最终将拥有18.5M个参数。你绝对应该将其更改为你的计算机可以处理的参数最大数量。你可以通过更改主配置文件中的`d_model`（如果你训练了一个更大的标记化器，还需要更改`vocab_size`）来找到这个数字：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Look, it’s figure 9.2, which was generated from listing 9.3! Try to guess what
    it will be, and then read the blurb to see if you’re right.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 看看，这是图9.2，它是从列表9.3生成的！试着猜猜它将会是什么，然后阅读摘要看看你是否猜对了。
- en: '![figure](../Images/9-2.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/9-2.png)'
- en: Figure 9.2 Training a simple neural network on our dataset to generate text
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.2 在我们的数据集上训练简单的神经网络以生成文本
- en: 'Look at that! That’s a pretty smooth curve when we train for the first time.
    Considering we only did 1,000 examples from our dataset, we’d encourage you to
    try for several actual epochs—say, try three going over the whole dataset—and
    see how things go. You’ll likely get surprisingly decent results; we did. Let’s
    go ahead and check out what it creates when generating text:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 看看！这是我们第一次训练时的曲线非常平滑。考虑到我们只从数据集中使用了1,000个示例，我们鼓励你尝试几个实际的轮次——比如说，尝试三次遍历整个数据集——看看结果如何。你可能会得到令人惊讶的良好结果；我们就是这样做的。让我们继续检查它在生成文本时创建的内容：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Not too shabby! Of course, these aren’t great results, but we weren’t expecting
    amazing results with our basic model and short training time. Reading the generated
    tokens, it almost makes sense. We’ll call that a win. Congratulations! We created
    a language model using a feed-forward network that can return tokens. Now it’s
    time to get into the changes that make Llama different from a regular feed-forward
    network.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 真的不赖！当然，这些结果并不算出色，但我们并没有期望我们的基础模型和短时间训练能带来惊人的结果。阅读生成的标记，几乎有点道理。我们可以称之为胜利。恭喜！我们使用前馈网络创建了一个可以返回标记的语言模型。现在，让我们深入了解使Llama与普通前馈网络不同的变化。
- en: 9.2 Simple Llama
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 简单的Llama
- en: 'If you check the full weights and layers as released by Meta, you may notice
    that what we are building is not exactly the same as what was released. The reason
    for this is twofold: (1) we’d like to make sure this discussion is still very
    understandable for people interacting with research for production for the first
    time, and (2) we’re considering the environments you’ll likely have access to
    when reading this book. Everything here should fit and run in Kaggle or Colab
    without problems. With that being the case, we’ll address differences in Llama
    3’s architecture and ours so that if you did have the infra and data to replicate
    the paper for production, you could.[²](#footnote-142)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您检查 Meta 发布的完整权重和层，您可能会注意到我们正在构建的并不完全与发布的内容相同。原因有两点：（1）我们希望确保这次讨论对于第一次与生产相关的科研人员来说仍然非常易懂，（2）我们正在考虑您在阅读这本书时可能能够访问的环境。这里的一切都应该能够在
    Kaggle 或 Colab 中无缝运行。在这种情况下，我们将讨论 Llama 3 的架构与我们的差异，以便如果您确实有基础设施和数据来复制论文以用于生产，您也可以做到。[²](#footnote-142)
- en: 'Llama is different from a feed-forward network in a few ways: normalization,
    attention, activation, and number of layers. Without going too deeply into any
    of them, normalization helps stabilize training, attention helps support larger
    context lengths and uses information between layers more efficiently, activation
    helps represent nonlinearities better, and the number of layers increases the
    amount of information the model is able to represent. One other important thing
    to note is that we’re adding a scheduler this time around. The scheduler here
    is responsible for adjusting the learning rate during training, following a “schedule.”
    This addition helps us with potential exploding gradients and allows the model
    to converge more quickly.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Llama 在几个方面与前馈网络不同：归一化、注意力、激活和层数。不过，我们不会深入探讨这些内容。归一化有助于稳定训练，注意力有助于支持更大的上下文长度并更有效地使用层间信息，激活有助于更好地表示非线性，而层数的增加使得模型能够表示更多的信息。还有一点需要注意，这次我们增加了一个调度器。这里的调度器负责在训练过程中根据“调度”调整学习率。这一增加有助于我们处理潜在的梯度爆炸问题，并使模型更快地收敛。
- en: Let’s change our network into a simpler version of Llama 3\. Here, we’ll skip
    over some of the theory and implementation. But look at the notebook in GitHub
    too—we want you to test it out on your own!
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将我们的网络改为 Llama 3 的一个更简单的版本。在这里，我们将跳过一些理论和实现细节。但也要查看 GitHub 上的笔记本——我们希望您亲自尝试一下！
- en: Listing 9.4 Simple Llama
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.4 简单的 Llama
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '#1 New'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 新增'
- en: Unlike the original network, we’re creating a whole class for LlamaBlocks, or
    smaller self-contained networks within our larger one. Now we have `RMSNormalization`,
    along with `RoPEMaskedMultiheadAttention` and a `SwiGLU` activation instead of
    `ReLU`. We’ve included the implementations in the notebook, so feel free to check
    them out if you are curious.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 与原始网络不同，我们为 LlamaBlocks 创建了一个完整的类，或者说是我们更大网络中的较小、自包含的网络。现在我们有 `RMSNormalization`，以及
    `RoPEMaskedMultiheadAttention` 和 `SwiGLU` 激活而不是 `ReLU`。我们在笔记本中包含了实现，如果您好奇，可以随时查看。
- en: 'You’ll notice that our forward function is very different from the original
    feed forward. We’re no longer just embedding and then getting the logits from
    the embedding. Now we’re normalizing, adding attention, normalizing again, and
    then adding our logits to what comes out. This process helps the model integrate
    more nonlinearities into its overall considerations for how the input and desired
    output can line up:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到我们的前向函数与原始的前馈函数非常不同。我们现在不再只是嵌入然后从嵌入中获取 logits，而是进行归一化、添加注意力、再次归一化，然后将 logits
    添加到输出中。这个过程有助于模型将更多的非线性整合到其对输入和期望输出的整体考虑中：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here, we can compare the original feed-forward network with this `SimpleLlama`
    class to get an idea of what’s changed overall. First, instead of only having
    one `Sequential` block of layers, we have a number of `LlamaBlocks` equal to `n_layers`
    in our config, which is `8`, as you’ll see in the following code snippet. Beyond
    that, we’re using the SwiGLU activation everywhere instead of a ReLU. SwiGLU adds
    some ability to handle negative numbers and helps with exploding/vanishing gradients.
    Other than that, they’re remarkably similar:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以将原始的前馈网络与这个 `SimpleLlama` 类进行比较，以了解整体上发生了哪些变化。首先，我们不再只有一个 `Sequential`
    层块，而是根据配置中的 `n_layers`（即 `8`，您将在下面的代码片段中看到）拥有多个 `LlamaBlocks`。除此之外，我们使用 SwiGLU
    激活而不是 ReLU。SwiGLU 增加了一些处理负数的能力，并有助于解决梯度爆炸/消失问题。除此之外，它们非常相似：
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '#1 New'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 新增'
- en: '#2 New'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 新增'
- en: '#3 New'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 新增'
- en: 'We can make some slight adjustments to our master config to make the model
    bigger by increasing the embedding dimension, the number of layers, and the context
    window. You don’t actually have to make that change to see the performance difference.
    If you had the compute, data, and time, you could train a viable version of Llama
    3 (you can see the results of this training in figure 9.3):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过增加嵌入维度、层数和上下文窗口来对我们的主配置文件进行一些轻微的调整，使模型更大。实际上，你不必做出这个改变就能看到性能差异。如果你有计算能力、数据和时间，你可以训练一个可行的Llama
    3版本（你可以在图9.3中看到这个训练的结果）：
- en: '[PRE12]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '#1 New'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 新'
- en: '![figure](../Images/9-3.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/9-3.png)'
- en: Figure 9.3 Training simple Llama on our dataset to generate text
  id: totrans-91
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.3 在我们的数据集上训练简单的Llama以生成文本
- en: So we’ve made the 10× jump to over 180M parameters. Did it give us the emergent
    behavior we were looking for, though? If you look at the generated text, it’s
    making improvements in that it’s guessing punctuation more often, but almost none
    are in the correct place. The loss is higher, too, but we’re not particularly
    worried about that part; if we spruce up our data loading and allow the model
    to go all the way through the dataset two or three times, that should get lower.
    Lastly, if we make the model bigger by increasing the context window and number
    of layers, along with increasing the tokens in our dataset, we should be able
    to get that emergent behavior. For this dataset and config, you’d have to train
    ~1,900 times to go through the dataset once, so you’d have to train almost 6,000
    times to start taking advantage of the whole dataset.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经将参数数量提高了10倍，超过了1.8亿个。但这真的给了我们我们期望的涌现行为吗？如果你看看生成的文本，它在猜测标点符号方面有所改进，但几乎都没有在正确的位置。损失也更高了，但我们并不特别担心这一点；如果我们改进数据加载，并允许模型将整个数据集遍历两到三次，那么损失应该会降低。最后，如果我们通过增加上下文窗口和层数，以及增加数据集中的标记数量来使模型更大，我们应该能够得到那种涌现行为。对于这个数据集和配置，你需要训练大约1,900次才能遍历一次数据集，所以你需要训练大约6,000次才能开始充分利用整个数据集。
- en: Given a lack of time and resources, we aren’t going to worry that our model
    isn’t at the top of any leaderboards. Heck, it’s not even good enough to get on
    one. But we have created a simple model that resembles Llama, and we have done
    so from scratch. This exercise has given us insights into the process, and you
    should have an idea of how to make it better. With these things in mind, let’s
    discuss how to put the model we’ve created into production.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 由于时间和资源的缺乏，我们不会担心我们的模型不在任何排行榜的顶端。实际上，它甚至不够好，无法进入任何一个排行榜。但我们已经创建了一个类似于Llama的简单模型，而且是从零开始的。这个练习让我们对过程有了深入了解，你应该对如何改进它有一个概念。考虑到这些，让我们讨论如何将我们创建的模型投入生产。
- en: 9.3 Making it better
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 让它变得更好
- en: Now that we have a model and it’s passing all of our internal benchmarks (we’ll
    pretend that we had some), it’s time to deploy the model and see how it behaves
    with customers interacting with it. Oh no! The internal tests we had aren’t representative
    of our production environment! Our first problem is that the model is way too
    big and slow to even get through the prod environment tests. Models themselves
    are often looked at as being the main ingredient to success. In contrast, the
    systems we engineer around models, including the data, are overlooked because
    “anyone can hire a good MLE to make those.” Unfortunately, that’s now the secret
    sauce that causes some companies to succeed and others to fail.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个模型，并且它通过了我们所有的内部基准测试（我们假装我们有一些），现在是时候部署这个模型并看看它在与客户互动时的表现了。哦不！我们之前的内部测试并不能代表我们的生产环境！我们的第一个问题是模型太大太慢，甚至无法通过生产环境测试。模型本身通常被视为成功的主要成分。相比之下，我们围绕模型构建的系统，包括数据，却被忽视，因为“任何人都可以雇佣一个优秀的MLE来制作这些。”不幸的是，这就是一些公司成功而其他公司失败的秘密配方。
- en: We’d like to acknowledge to everyone rushing to the comments and GitHub Issues
    that this model doesn’t work because that isn’t the point of this chapter, and
    we’d like to point you toward creators like Abi Aryan, Sebastian Raschka, and
    others who are covering the data science of pretraining LLMs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想感谢那些急于在评论和GitHub问题中提出意见的人，因为这个模型不起作用，这不是本章的目的，我们想引导你们去关注像Abi Aryan、Sebastian
    Raschka和其他人，他们正在介绍预训练LLMs的数据科学。
- en: 'NOTE  If you’d like to pretrain a causal language model that generates great
    content, there are other great resources available. Check out these projects for
    more information on pretraining your own model: Llama 3 ([https://mng.bz/BgAw](https://mng.bz/BgAw)),
    Megatron LM ([https://mng.bz/dZdg](https://mng.bz/dZdg)), Hugging Face Tutorial
    ([https://mng.bz/V2RN](https://mng.bz/V2RN)), and Llama2.c ([https://mng.bz/x6j7](https://mng.bz/x6j7)).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你想要预训练一个能够生成优质内容的因果语言模型，还有其他一些优秀的资源可供选择。查看以下项目以获取有关预训练你自己的模型的更多信息：Llama
    3 ([https://mng.bz/BgAw](https://mng.bz/BgAw))、Megatron LM ([https://mng.bz/dZdg](https://mng.bz/dZdg))、Hugging
    Face 教程 ([https://mng.bz/V2RN](https://mng.bz/V2RN)) 和 Llama2.c ([https://mng.bz/x6j7](https://mng.bz/x6j7))。
- en: In the spirit of continuing with data scientist–focused production advice, we’ll
    now cover how to make your model easier to deploy and more effective once it’s
    out there. Once a data scientist has trained a model and it passes the efficacy
    tests set, it’s time to think about size.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续提供以数据科学家为中心的生产建议的精神下，我们现在将介绍如何使你的模型在部署后更容易使用并更有效。一旦数据科学家训练好模型并通过了效验测试，就需要考虑模型的大小。
- en: 9.3.1 Quantization
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 量化
- en: The first problem you’ll definitely be up against is sheer size. Our 180M parameter
    model is over 700 MB on disk, which is much bigger than some companies ever plan
    on serving for any use case. How do you make sure it’s small enough and quick
    enough to run in AWS lambda or in a CPU-only instance? Compression is one way
    to help us out here, and quantization is something built into PyTorch! As we’ve
    stated before, you should get familiar with BitsandBytes, but let’s look at a
    quick implementation that quantizes the model after training using `torch`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你将肯定面临的首要问题是纯粹的大小。我们的 180M 参数模型在磁盘上超过 700 MB，这比一些公司计划用于任何用例的都要大得多。你如何确保它足够小且足够快，以便在
    AWS Lambda 或仅 CPU 实例上运行？压缩是我们帮助解决此问题的方法之一，而量化是 PyTorch 内置的功能！正如我们之前所述，你应该熟悉 BitsandBytes，但让我们看看一个快速实现，使用
    `torch` 在训练后对模型进行量化。
- en: In the next listing, we take our model, and using PyTorch, we’ll quantize the
    model to INT8\. The rest of the code and functions are simply to compare the model
    sizes before and after. The important bit is just the first couple of lines.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个列表中，我们使用我们的模型，并通过 PyTorch 对模型进行 INT8 量化。其余的代码和函数只是简单地比较模型在量化和未量化前后的尺寸。重要的是只有前几行。
- en: Listing 9.5 Quantization
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.5 量化
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '#1 Post training dynamic quantization'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 训练后动态量化'
- en: You can see at the end that we go from almost 1 GB to 18 MB on disk by just
    going down to INT8 quantization. And we can go even lower,[³](#footnote-143) which
    can help you fit almost any model in the chosen production environment; just keep
    in mind that as you compress weights, perplexity goes up, resulting in less stable
    and predictable performance of the LLM, even with great prompt engineering.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，通过仅进行 INT8 量化，我们的磁盘空间从近 1 GB 减少到 18 MB。我们甚至可以进一步降低，[³](#footnote-143)，这可以帮助你将几乎任何模型适应所选的生产环境；但请记住，随着权重的压缩，困惑度会增加，导致
    LLM 的性能不稳定且难以预测，即使有出色的提示工程。
- en: So now that the model is small enough, the MLOps team puts it into the dev environment,
    and all of the tests pass, so our model finally made it to prod. All is well,
    right?
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在模型已经足够小，MLOps 团队将其放入开发环境，所有测试都通过了，所以我们的模型最终进入了生产环境。一切顺利，对吧？
- en: 9.3.2 LoRA
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 LoRA
- en: 'What do we do when, one month down the road, we have data showing our model
    is unable to perform a particular task up to the standards of its environment?
    We have data drift, and because we’re a startup, we don’t have the money or time
    to go through the rigorous training process we went through before to train a
    model from scratch. There’s a bigger problem too: we don’t have enough new data
    illustrating the new distribution to finetune the model effectively. This situation
    is perfect for training a LoRA to tweak the model rather than spending all that
    time training it over again.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个月后，我们发现模型无法达到其环境标准执行特定任务时，我们该怎么办？我们遇到了数据漂移，由于我们是初创公司，我们没有足够的资金和时间去通过之前训练模型时经历的严格训练过程。还有一个更大的问题：我们没有足够的新数据来展示新的分布，以有效地微调模型。这种情况非常适合训练
    LoRA 来调整模型，而不是花费大量时间重新训练。
- en: Listing 9.6 shows you how to train a LoRA model and the adjustments we need
    to make to our Llama model. This listing shows first what adding a LoRA does to
    the inputs as they move through the model. The `LoRALayer` class is shown in clear
    PyTorch terms by Sebastian Raschka and Lightning.AI, and they have repos going
    into even more depth (see [https://github.com/rasbt/dora-from-scratch](https://github.com/rasbt/dora-from-scratch)
    and [https://mng.bz/Aa8e](https://mng.bz/Aa8e)). Next, it shows how our `SimpleLlama`
    class changes after we’ve added a LoRA to it. Lastly, we’ll go through a similar
    training process using a new instruct dataset and a new `get_batches` function.
    As a note, we use several helper functions throughout this listing to simplify
    it; you can find their definitions in the repository accompanying this book.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.6展示了如何训练LoRA模型以及我们需要对我们Llama模型进行的调整。这个列表首先展示了添加LoRA后输入如何在模型中移动。`LoRALayer`类由Sebastian
    Raschka和Lightning.AI以清晰的PyTorch术语展示，并且他们有更深入的研究仓库（见[https://github.com/rasbt/dora-from-scratch](https://github.com/rasbt/dora-from-scratch)和[https://mng.bz/Aa8e](https://mng.bz/Aa8e)）。接下来，它展示了添加LoRA后我们的`SimpleLlama`类发生了什么变化。最后，我们将使用新的指令数据集和新的`get_batches`函数进行类似的训练过程。作为备注，我们在整个列表中使用了几个辅助函数来简化它；您可以在本书附带的仓库中找到它们的定义。
- en: Listing 9.6 Low-rank adaptation
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.6 低秩自适应
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '#1 What does LoRA actually do?'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 LoRA实际上做什么？'
- en: '#2 Shows how the blocks change'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 展示了块如何变化'
- en: '#3 New'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 新增'
- en: '#4 New'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '#4 新增'
- en: '#5 New'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '#5 新增'
- en: '#6 New dataset for LoRA'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '#6 新的LoRA数据集'
- en: '#7 Step 1: Adds LoRA to the trained model'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '#7 步骤1：将LoRA添加到训练好的模型中'
- en: '#8 Step 2: Gets the LoRA params instead of the whole model''s'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#8 步骤2：获取LoRA参数而不是整个模型'
- en: '#9 Step 3: Initializes optimizer with LoRA params'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#9 步骤3：使用LoRA参数初始化优化器'
- en: '#10 Step 4: Trains'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#10 步骤4：训练'
- en: '#11 Step 5: Exports the params'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '#11 步骤5：导出参数'
- en: 'All of that results in two separate state dicts for us to save: the model and
    the LoRA. You can train LoRAs for a variety of specific tasks for which you may
    not have a large enough dataset to justify a whole finetuning. LoRA files on disk
    are usually only kilobytes even for very large models, depending on the size of
    the rank (in our case, 16).'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些导致我们有两个单独的状态字典需要保存：模型和LoRA。您可以为各种特定任务训练LoRA，对于这些任务，您可能没有足够大的数据集来证明整个微调的合理性。磁盘上的LoRA文件通常只有几千字节，即使是对于非常大的模型，这取决于秩的大小（在我们的例子中，是16）。
- en: 'You can inference using a LoRA generally in two ways: you can (1) load the
    original model’s state dict (ours is loaded within the `llama` variable), load
    the LoRA on top of it, and then inference as normal, or (2) merge all of the LoRA
    layers into the original Llama and essentially create a new model and inference
    normally. Here, we adopt the second option.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用LoRA进行推理，通常有两种方式：您可以选择（1）加载原始模型的状态字典（在我们的例子中，它被加载在`llama`变量中），然后在上面加载LoRA，然后像平常一样进行推理，或者（2）将所有的LoRA层合并到原始Llama中，从而本质上创建一个新的模型并正常推理。在这里，我们采用了第二种选项。
- en: '**##### Listing 9.7 Low-rank adaptation'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**##### 列表9.7 低秩自适应**'
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The generated text is
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的文本是
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We can see that the text still isn’t as coherent as we’d like it to be; however,
    we can see a definite change in the generation compared to the simple Llama. No
    more overzealous punctuation, “cry,” and other nonhappy small story words are
    present, and there are more clearly made-up words. If you train on a more distinct
    set—say, Shakespeare—you’ll be able to see the difference even more clearly, and
    the nice thing about LoRA is that you can simply `remove_lora()` to get the original
    functionality back.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，文本仍然没有达到我们期望的连贯性；然而，与简单的Llama相比，我们可以看到生成文本的明显变化。不再有过于热情的标点符号、“哭泣”和其他不愉快的短故事词汇，而且有更多明显是虚构的词汇。如果您在更明显的集合上训练——比如说莎士比亚——您将能够更清楚地看到差异，而且LoRA的好处是您只需简单地调用`remove_lora()`就可以恢复原始功能。
- en: 9.3.3 Fully sharded data parallel–quantized LoRA
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 完全分片数据并行-量化LoRA
- en: Building upon LoRA, quantized LoRA (QLoRA) allows for efficient fine-tuning
    of models larger than your GPU. It does this by quantizing the model and then
    training a LoRA on the frozen version of that quantized model. This technique
    is desirable when you look at how much memory it takes to finetune full-size models,
    even in half-precision. As we previously discussed, a 70B parameter model ends
    up being 140 GB on disk and will take more than five times that much memory to
    finetune because of the dataset and gradients. With QLoRA, we can train up to
    65B parameters on only 48 GB of VRAM—a very noticeable reduction. QLoRA is currently
    the most effective way of taking an absurdly large model and productionizing it
    for your use case, and it saves tons of money for that process too.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 LoRA，量化 LoRA（QLoRA）允许对大于你 GPU 的模型进行高效的微调。它是通过量化模型，然后在量化模型的冻结版本上训练 LoRA 来实现的。当你考虑到微调全尺寸模型所需的内存量时，这种技术是可取的，即使在半精度下也是如此。正如我们之前讨论的，一个
    70B 参数的模型在磁盘上最终会变成 140 GB，并且由于数据集和梯度，微调它将需要超过五倍那么多的内存。使用 QLoRA，我们可以在只有 48 GB 的
    VRAM 上训练高达 65B 参数的模型——这是一个非常明显的减少。QLoRA 目前是将一个荒谬大的模型用于生产并实现其用例的最有效方式，而且这个过程还能节省大量资金。
- en: Add to this fully sharded data parallel (FSDP), and you can break the consumer
    versus enterprise barriers. Some of you have likely been asking where parallelism
    has been this whole time, and here it is. FSDP allows for both data and model
    parameter parallelism throughout the entire training process on multiple GPUs,
    and it takes care of the sharding as well as the rejoining on the other end when
    order and magnitude matter. It’s amazing work coming from the team that maintains
    PyTorch.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 加入完全分片的数据并行（FSDP），你就可以打破消费者与企业的界限。你们中的一些人可能一直在问并行处理一直去哪里了，现在它就在这里。FSDP 允许在整个训练过程中在多个
    GPU 上进行数据和模型参数的并行处理，并且当顺序和规模很重要时，它还负责分片以及另一端的重新连接。这是来自维护 PyTorch 的团队的一项惊人的工作。
- en: Previously, 48 GB for QLoRA on a 70B parameter model was only possible using
    an enterprise GPU like an A100\. With FSDP, you can take full advantage of parallelism
    on consumer hardware, like two 3090s, to get the same result. FSDP is native to
    PyTorch! Unlike our previous efforts in this chapter, we will now abstract a script
    created by Jeremy Howard and Answer.AI so that you can just run it in one of the
    cells on a 7B parameter model. Instead of needing to clone an entire GitHub repo,
    you can install and import `fsdp_qlora` from PyPI, and we’ve recreated the importable
    class in the `train_utils` folder. This code will execute fully parallel QLoRA
    training on as many GPUs as you have access to.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，在 70B 参数模型上使用 QLoRA 需要 48 GB 的内存，这仅适用于像 A100 这样的企业级 GPU。有了 FSDP，你可以充分利用消费级硬件上的并行性，比如两个
    3090，以获得相同的结果。FSDP 是 PyTorch 的原生功能！与本章中我们之前的努力不同，我们现在将抽象化由 Jeremy Howard 和 Answer.AI
    创建的脚本，这样你就可以在 7B 参数模型的一个单元中运行它。你不需要克隆整个 GitHub 仓库，你可以从 PyPI 安装并导入 `fsdp_qlora`，我们还在
    `train_utils` 文件夹中重新创建了可导入的类。此代码将在你拥有的所有 GPU 上执行完全并行的 QLoRA 训练。
- en: Listing 9.8 FSDP-QLORA training
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.8 FSDP-QLORA 训练
- en: '[PRE17]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The result of this running is a fully finetuned safetensors model file trained
    using quantized weights and parallelism. Unlike our bespoke pretrained version,
    this one works. The safetensors file contains a state dict file for the trained
    model, similar to the state dict we saved for the `SimpleLlama`. Both of those
    state dicts need to be converted into a full model file or a full checkpoint file
    before they can be uploaded to a place like Hugging Face; otherwise, classes like
    `AutoModel` or `LlamaForCausalLM` won’t be able to load your model later.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 运行结果是一个完全微调的 safetensors 模型文件，使用量化权重和并行性进行训练。与我们的定制预训练版本不同，这个版本是可行的。safetensors
    文件包含一个训练模型的 state dict 文件，类似于我们为 `SimpleLlama` 保存的状态 dict。这两个状态 dict 都需要在上传到像
    Hugging Face 这样的地方之前转换为完整的模型文件或完整的检查点文件；否则，像 `AutoModel` 或 `LlamaForCausalLM`
    这样的类将无法在以后加载你的模型。
- en: 9.4 Deploy to a Hugging Face Hub Space
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 部署到 Hugging Face Hub 空间
- en: Spaces are hosted containers where you can put models to allow community access,
    and they can be much more than that, depending on your needs. Spaces can be the
    place your company uses to deploy its whole model, as opposed to other cloud-hosting
    options. Spaces have a free tier and many paid tiers, depending on how compute-intensive
    your particular application is. Spaces integrate seamlessly with the most popular
    ML frontend stacks, namely Streamlit, Gradio, and FastAPI.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 空间是托管容器，您可以将模型放入其中以允许社区访问，而且它们可以根据您的需求提供更多功能。空间可以是公司部署整个模型的地方，而不是其他云托管选项。空间提供免费层和许多付费层，具体取决于您的特定应用程序的计算密集程度。空间可以无缝集成到最受欢迎的机器学习前端框架中，例如
    Streamlit、Gradio 和 FastAPI。
- en: NOTE  We won’t be giving examples of these ML frontend stacks here, as we’ve
    given them in previous chapters, but we did include an example app in the notebook
    for this chapter. For reference, check out the documentation for Gradio ([https://www.gradio.app/guides/quickstart](https://www.gradio.app/guides/quickstart))
    and Hugging Face ([https://huggingface.co/docs/hub/spaces](https://huggingface.co/docs/hub/spaces)).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们不会在这里给出这些机器学习前端框架的示例，因为我们已经在之前的章节中给出了，但我们确实在笔记本中包含了一个本章节的示例应用程序。为了参考，请查看
    Gradio ([https://www.gradio.app/guides/quickstart](https://www.gradio.app/guides/quickstart))
    和 Hugging Face ([https://huggingface.co/docs/hub/spaces](https://huggingface.co/docs/hub/spaces))
    的文档。
- en: With our models, we’ll need to convert their weights and directories into a
    format easily pushed to the Hugging Face Hub for our Space. We have an easily
    modified script that you can use to make this conversion. You can also run this
    on the simple Llama LoRA trained earlier.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的模型，我们需要将它们的权重和目录转换为可以轻松推送到 Hugging Face Hub 的格式。我们有一个易于修改的脚本，您可以使用它来完成此转换。您也可以在之前简单训练的
    Llama LoRA 上运行此脚本。
- en: Listing 9.9 Converting weights for Hugging Face
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.9 为 Hugging Face 转换权重
- en: '[PRE18]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If you already have a repo and are logged in to your Hugging Face account, you
    can go ahead and run `model.push_to_hub()`. This will create a repo for your model
    if it doesn’t already exist. The reason you would or wouldn’t push to the hub
    has to do with whether you want to share your model with the world. If you’d rather
    have a space where others can try out your model (even for free), we’ll show how
    to do that next.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经有了仓库并且已登录到您的 Hugging Face 账户，您可以继续运行 `model.push_to_hub()`。如果您的模型尚未存在，这将为您创建一个仓库。您是否将模型推送到
    Hub 取决于您是否希望与世界分享您的模型。如果您更愿意有一个空间，其他人可以尝试您的模型（甚至免费），我们将在下一节展示如何做到这一点。
- en: 'The first decisions to be made for a Space are how much compute your app requires
    and how you’ll maintain the code for the Space—with Git or with `huggingface-cli`.
    The first question starts with whether a GPU is required for your particular use
    case; for ours, it is not. However, when you need a speed or scale increase, you
    will likely need it, especially if you get into multiprocessing to get more performance
    out of the Space. Once you have your app and you’ve figured out your memory requirements,
    if you’ve decided to use Git, you’ll make your Space on Hugging Face, and then
    you’ll clone it the same way you would something on GitHub:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于空间，需要做出的第一个决定是您的应用程序需要多少计算资源以及您将如何维护空间的代码——使用 Git 或使用 `huggingface-cli`。第一个问题始于是否需要为您的特定用例使用
    GPU；对于我们的用例，不需要。然而，当您需要提高速度或规模时，您可能需要 GPU，尤其是在您需要多进程以从空间中获得更多性能时。一旦您有了应用程序并且已经确定了您的内存需求，如果您决定使用
    Git，您将在 Hugging Face 上创建您的空间，然后您将以与 GitHub 上相同的方式克隆它：
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Adding, committing, and pushing are the same as well:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 添加、提交和推送的操作也是一样的：
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If you’re not doing it through the CLI, the following listing shows you how.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不是通过 CLI 来做这件事，以下列表显示了如何操作。
- en: Listing 9.10 Hugging Face Space
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.10 Hugging Face Space
- en: '[PRE21]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '#1 If you haven’t created your repo yet'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 如果您还没有创建您的仓库'
- en: Hugging Face Spaces
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hugging Face Spaces
- en: The models, as we currently have them, require GPUs to load (especially quantized)
    and run. If you attempt to run on the free tier of HF Spaces, it will error out,
    as it did for us. You can fix this by upgrading to a paid tier or ZeroGPU. Hugging
    Face provides a version of a Gradio app that uses its own API only to provision
    a GPU for the amount of time it takes to complete a task and only when it’s requested.
    See [https://mng.bz/XV11](https://mng.bz/XV11).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们目前拥有的模型需要 GPU 来加载（尤其是量化）和运行。如果您尝试在 HF Spaces 的免费层上运行，它将出错，就像我们遇到的情况一样。您可以通过升级到付费层或
    ZeroGPU 来修复此问题。Hugging Face 提供了一个使用其自己的 API 的 Gradio 应用程序版本，仅用于为完成任务所需的时间分配 GPU，并且仅在请求时分配。请参阅
    [https://mng.bz/XV11](https://mng.bz/XV11)。
- en: As an exercise, we encourage you to think through and build out how you might
    be able to create a Hugging Face Space using our LLM that would run on the free
    tier, which is considerably easier than when we were first writing this, thanks
    to ZeroGPU.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项练习，我们鼓励您思考并构建出您可能如何使用我们的 LLM 创建一个在免费层上运行的 Hugging Face Space，这比我们最初写这篇文章时容易得多，多亏了
    ZeroGPU。
- en: And there we have it—a fully functioning hosted instance of any model you want
    to use or train. You can run either of the two Llama models we trained in the
    Space, but you’ll need to do a bit of engineering around it depending on your
    needs. Congratulations on finishing the first project if you ran all of this code
    with your own environment! This was one of the denser chapters, and making it
    through with a working example is something to be proud of. Hugging Face provides
    private solutions to enterprises looking to use Spaces long term, and this is
    a completely viable production environment.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由此，我们得到了任何想要使用或训练的模型的完全功能化的托管实例。您可以在 Space 中运行我们训练的两个 Llama 模型中的任何一个，但您可能需要根据您的需求进行一些工程处理。如果您在自己的环境中运行了所有这些代码并完成了第一个项目，那么恭喜您！这是最密集的章节之一，能够通过一个工作示例而感到自豪。Hugging
    Face 为希望长期使用 Spaces 的企业提供私有解决方案，这是一个完全可行的生产环境。
- en: Summary
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Choosing an appropriate tokenizer and embedding strategy is one of the first
    crucial decisions you’ll make when creating a model from scratch, as it determines
    what the model will see and, therefore, is capable of.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择合适的分词器和嵌入策略是您在从头创建模型时将做出的第一个关键决策之一，因为它决定了模型将看到什么，因此也决定了模型的能力。
- en: Your unique data sources future-proof your model.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您独特的源数据可以为您的模型提供未来保障。
- en: The main differences between Llama and a simple feed-forward are the normalization,
    attention, activation layers, and number of layers.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Llama 与简单的前馈之间的主要区别在于归一化、注意力、激活层和层数的数量。
- en: 'Often, the first challenge to productionizing an LLM is its size: quantization
    to the rescue!'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，将 LLM 产业化面临的第一大挑战是其大小：量化技术可以拯救！
- en: In production, it’s only a matter of time before you’ll need to update the model.
    LoRA and QLoRA are perfect solutions to make minor tweaks to your model.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产环境中，您需要更新模型只是时间问题。LoRA 和 QLoRA 是对您的模型进行小幅度调整的完美解决方案。
- en: Fully sharded data parallelism allows us to train QLoRA models cheaply on consumer
    hardware.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全分片数据并行使我们能够在消费级硬件上以低成本训练 QLoRA 模型。
- en: A great option to deploy and share your LLM project is Hugging Face Hub Spaces
    due to their ease of use.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于使用简便，Hugging Face Hub Spaces 是部署和分享您的 LLM 项目的一个很好的选择。
- en: '[[1]](#footnote-source-1) H. Touvron et al., “Llama 2: Open foundation and
    fine-tuned chat models,” arXiv.org, July 19, 2023, [https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[[1]](#footnote-source-1) H. Touvron 等人，“Llama 2：开放基础和微调聊天模型，” arXiv.org，2023年7月19日，[https://arxiv.org/abs/2307.09288](https://arxiv.org/abs/2307.09288)。'
- en: '[[2]](#footnote-source-2) [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/),
    [https://arxiv.org/pdf/2307.09288](https://arxiv.org/pdf/2307.09288), [https://arxiv.org/pdf/2302.13971](https://arxiv.org/pdf/2302.13971).'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[[2]](#footnote-source-2) [https://ai.meta.com/blog/meta-llama-3/](https://ai.meta.com/blog/meta-llama-3/),
    [https://arxiv.org/pdf/2307.09288](https://arxiv.org/pdf/2307.09288), [https://arxiv.org/pdf/2302.13971](https://arxiv.org/pdf/2302.13971).'
- en: '[[3]](#footnote-source-3) S. Ma et al., “The Era of 1-bit LLMs: All Large Language
    Models are in 1.58 Bits,” arXiv.org, Feb. 27, 2024, [https://arxiv.org/abs/2402.17764](https://arxiv.org/abs/2402.17764).**'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[[3]](#footnote-source-3) S. Ma 等人，“1 位 LLM 时代：所有大型语言模型都在 1.58 位中，” arXiv.org，2024年2月27日，[https://arxiv.org/abs/2402.17764](https://arxiv.org/abs/2402.17764)。**'
