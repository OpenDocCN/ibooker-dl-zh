- en: Chapter 15\. Processing Sequences Using RNNs and CNNs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第15章。使用RNNs和CNNs处理序列
- en: Predicting the future is something you do all the time, whether you are finishing
    a friend’s sentence or anticipating the smell of coffee at breakfast. In this
    chapter we will discuss recurrent neural networks (RNNs)—a class of nets that
    can predict the future (well, up to a point). RNNs can analyze time series data,
    such as the number of daily active users on your website, the hourly temperature
    in your city, your home’s daily power consumption, the trajectories of nearby
    cars, and more. Once an RNN learns past patterns in the data, it is able to use
    its knowledge to forecast the future, assuming of course that past patterns still
    hold in the future.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 预测未来是你经常做的事情，无论是在结束朋友的句子还是预期早餐时咖啡的味道。在本章中，我们将讨论循环神经网络（RNNs）-一类可以预测未来的网络（嗯，至少在一定程度上）。RNNs可以分析时间序列数据，例如您网站上每日活跃用户的数量，您所在城市的每小时温度，您家每日的用电量，附近汽车的轨迹等等。一旦RNN学习了数据中的过去模式，它就能利用自己的知识来预测未来，当然前提是过去的模式在未来仍然成立。
- en: More generally, RNNs can work on sequences of arbitrary lengths, rather than
    on fixed-sized inputs. For example, they can take sentences, documents, or audio
    samples as input, making them extremely useful for natural language processing
    applications such as automatic translation or speech-to-text.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，RNNs可以处理任意长度的序列，而不是固定大小的输入。例如，它们可以将句子、文档或音频样本作为输入，使它们非常适用于自然语言处理应用，如自动翻译或语音转文本。
- en: 'In this chapter, we will first go through the fundamental concepts underlying
    RNNs and how to train them using backpropagation through time. Then, we will use
    them to forecast a time series. Along the way, we will look at the popular ARMA
    family of models, often used to forecast time series, and use them as baselines
    to compare with our RNNs. After that, we’ll explore the two main difficulties
    that RNNs face:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先介绍RNNs的基本概念以及如何使用时间反向传播来训练它们。然后，我们将使用它们来预测时间序列。在此过程中，我们将研究常用的ARMA模型系列，通常用于预测时间序列，并将它们用作与我们的RNNs进行比较的基准。之后，我们将探讨RNNs面临的两个主要困难：
- en: Unstable gradients (discussed in [Chapter 11](ch11.html#deep_chapter)), which
    can be alleviated using various techniques, including *recurrent dropout* and
    *recurrent layer normalization*.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不稳定的梯度（在[第11章](ch11.html#deep_chapter)中讨论），可以通过各种技术来缓解，包括*循环丢失*和*循环层归一化*。
- en: A (very) limited short-term memory, which can be extended using LSTM and GRU
    cells.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （非常）有限的短期记忆，可以使用LSTM和GRU单元进行扩展。
- en: RNNs are not the only types of neural networks capable of handling sequential
    data. For small sequences, a regular dense network can do the trick, and for very
    long sequences, such as audio samples or text, convolutional neural networks can
    actually work quite well too. We will discuss both of these possibilities, and
    we will finish this chapter by implementing a WaveNet—a CNN architecture capable
    of handling sequences of tens of thousands of time steps. Let’s get started!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs并不是处理序列数据的唯一类型的神经网络。对于小序列，常规的密集网络可以胜任，而对于非常长的序列，例如音频样本或文本，卷积神经网络也可以表现得相当不错。我们将讨论这两种可能性，并通过实现WaveNet来结束本章-一种能够处理数万个时间步的CNN架构。让我们开始吧！
- en: Recurrent Neurons and Layers
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经元和层
- en: Up to now we have focused on feedforward neural networks, where the activations
    flow only in one direction, from the input layer to the output layer. A recurrent
    neural network looks very much like a feedforward neural network, except it also
    has connections pointing backward.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经专注于前馈神经网络，其中激活仅在一个方向中流动，从输入层到输出层。循环神经网络看起来非常像前馈神经网络，只是它还有指向后方的连接。
- en: Let’s look at the simplest possible RNN, composed of one neuron receiving inputs,
    producing an output, and sending that output back to itself, as shown in [Figure 15-1](#simple_rnn_diagram)
    (left). At each *time step* *t* (also called a *frame*), this *recurrent neuron*
    receives the inputs **x**[(*t*)] as well as its own output from the previous time
    step, *ŷ*[(*t*–1)]. Since there is no previous output at the first time step,
    it is generally set to 0\. We can represent this tiny network against the time
    axis, as shown in [Figure 15-1](#simple_rnn_diagram) (right). This is called *unrolling
    the network through time* (it’s the same recurrent neuron represented once per
    time step).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看最简单的RNN，由一个神经元组成，接收输入，产生输出，并将该输出发送回自身，如[图15-1](#simple_rnn_diagram)（左）所示。在每个*时间步*
    *t*（也称为*帧*），这个*循环神经元*接收输入**x**[(*t*)]以及来自上一个时间步的自己的输出*ŷ*[(*t*–1)]。由于在第一个时间步没有先前的输出，通常将其设置为0。我们可以沿着时间轴表示这个小网络，如[图15-1](#simple_rnn_diagram)（右）所示。这被称为*将网络展开到时间轴*（每个时间步表示一个循环神经元）。
- en: '![mls3 1501](assets/mls3_1501.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1501](assets/mls3_1501.png)'
- en: Figure 15-1\. A recurrent neuron (left) unrolled through time (right)
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-1. 一个循环神经元（左）在时间轴上展开（右）
- en: You can easily create a layer of recurrent neurons. At each time step *t*, every
    neuron receives both the input vector **x**[(*t*)] and the output vector from
    the previous time step **ŷ**[(*t*–1)], as shown in [Figure 15-2](#rnn_layer_diagram).
    Note that both the inputs and outputs are now vectors (when there was just a single
    neuron, the output was a scalar).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以轻松创建一个循环神经元层。在每个时间步*t*，每个神经元都接收来自输入向量**x**[(*t*)]和上一个时间步的输出向量**ŷ**[(*t*–1)]，如[图15-2](#rnn_layer_diagram)所示。请注意，现在输入和输出都是向量（当只有一个神经元时，输出是标量）。
- en: '![mls3 1502](assets/mls3_1502.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1502](assets/mls3_1502.png)'
- en: Figure 15-2\. A layer of recurrent neurons (left) unrolled through time (right)
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-2. 一个循环神经元层（左）在时间轴上展开（右）
- en: 'Each recurrent neuron has two sets of weights: one for the inputs **x**[(*t*)]
    and the other for the outputs of the previous time step, **ŷ**[(*t*–1)]. Let’s
    call these weight vectors **w**[*x*] and **w**[*ŷ*]. If we consider the whole
    recurrent layer instead of just one recurrent neuron, we can place all the weight
    vectors in two weight matrices: **W**[*x*] and **W**[*ŷ*].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 每个递归神经元有两组权重：一组用于输入**x**[(*t*)]，另一组用于上一个时间步的输出**ŷ**[(*t*–1)]。让我们称这些权重向量为**w**[*x*]和**w**[*ŷ*]。如果我们考虑整个递归层而不仅仅是一个递归神经元，我们可以将所有权重向量放入两个权重矩阵：**W**[*x*]和**W**[*ŷ*]。
- en: The output vector of the whole recurrent layer can then be computed pretty much
    as you might expect, as shown in [Equation 15-1](#rnn_output_equation), where
    **b** is the bias vector and *ϕ*(·) is the activation function (e.g., ReLU⁠^([1](ch15.html#idm45720182136320))).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 整个递归层的输出向量可以按照你所期望的方式计算，如[方程15-1](#rnn_output_equation)所示，其中**b**是偏置向量，*ϕ*(·)是激活函数（例如，ReLU⁠^([1](ch15.html#idm45720182136320))）。
- en: Equation 15-1\. Output of a recurrent layer for a single instance
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程15-1. 单个实例的递归层输出
- en: <math><msub><mi mathvariant="bold">ŷ</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><mi>ϕ</mi><mfenced><mrow><msup><msub><mi
    mathvariant="bold">W</mi><mi>x</mi></msub><mo>⊺</mo></msup><msub><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>+</mo><msup><msub><mi
    mathvariant="bold">W</mi><mi>ŷ</mi></msub><mo>⊺</mo></msup><msub><mi mathvariant="bold">ŷ</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub><mo>+</mo><mi
    mathvariant="bold">b</mi></mrow></mfenced></math>
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: <math><msub><mi mathvariant="bold">ŷ</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><mi>ϕ</mi><mfenced><mrow><msup><msub><mi
    mathvariant="bold">W</mi><mi>x</mi></msub><mo>⊺</mo></msup><msub><mi mathvariant="bold">x</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>+</mo><msup><msub><mi
    mathvariant="bold">W</mi><mi>ŷ</mi></msub><mo>⊺</mo></msup><msub><mi mathvariant="bold">ŷ</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub><mo>+</mo><mi
    mathvariant="bold">b</mi></mrow></mfenced></math>
- en: Just as with feedforward neural networks, we can compute a recurrent layer’s
    output in one shot for an entire mini-batch by placing all the inputs at time
    step *t* into an input matrix **X**[(*t*)] (see [Equation 15-2](#rnn_output_vectorized_equation)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 就像前馈神经网络一样，我们可以通过将时间步*t*的所有输入放入输入矩阵**X**[(*t*)]（参见[方程15-2](#rnn_output_vectorized_equation)）来一次性计算整个小批量的递归层输出。
- en: Equation 15-2\. Outputs of a layer of recurrent neurons for all instances in
    a pass:[mini-batch
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程15-2. 一次传递中递归神经元层的所有实例的输出：[小批量
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi
    mathvariant="bold">Ŷ</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>ϕ</mi> <mfenced separators="" open="("
    close=")"><msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <msub><mi mathvariant="bold">W</mi> <mi>x</mi></msub> <mo>+</mo> <msub><mi mathvariant="bold">Ŷ</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub> <msub><mi
    mathvariant="bold">W</mi> <mi>ŷ</mi></msub> <mo>+</mo> <mi mathvariant="bold">b</mi></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mi>ϕ</mi> <mfenced separators=""
    open="(" close=")"><mfenced separators="" open="[" close="]"><msub><mi mathvariant="bold">X</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub> <msub><mi mathvariant="bold">Ŷ</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub></mfenced>
    <mi mathvariant="bold">W</mi> <mo>+</mo> <mi mathvariant="bold">b</mi></mfenced>
    <mtext>with</mtext> <mi mathvariant="bold">W</mi> <mo>=</mo> <mfenced separators=""
    open="[" close="]"><mtable><mtr><mtd><msub><mi mathvariant="bold">W</mi> <mi>x</mi></msub></mtd></mtr>
    <mtr><mtd><msub><mi mathvariant="bold">W</mi> <mi>ŷ</mi></msub></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math>
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi
    mathvariant="bold">Ŷ</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>ϕ</mi> <mfenced separators="" open="("
    close=")"><msub><mi mathvariant="bold">X</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <msub><mi mathvariant="bold">W</mi> <mi>x</mi></msub> <mo>+</mo> <msub><mi mathvariant="bold">Ŷ</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub> <msub><mi
    mathvariant="bold">W</mi> <mi>ŷ</mi></msub> <mo>+</mo> <mi mathvariant="bold">b</mi></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mi>ϕ</mi> <mfenced separators=""
    open="(" close=")"><mfenced separators="" open="[" close="]"><msub><mi mathvariant="bold">X</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub> <msub><mi mathvariant="bold">Ŷ</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub></mfenced>
    <mi mathvariant="bold">W</mi> <mo>+</mo> <mi mathvariant="bold">b</mi></mfenced>
    <mtext>with</mtext> <mi mathvariant="bold">W</mi> <mo>=</mo> <mfenced separators=""
    open="[" close="]"><mtable><mtr><mtd><msub><mi mathvariant="bold">W</mi> <mi>x</mi></msub></mtd></mtr>
    <mtr><mtd><msub><mi mathvariant="bold">W</mi> <mi>ŷ</mi></msub></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math>
- en: 'In this equation:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '**Ŷ**[(*t*)] is an *m* × *n*[neurons] matrix containing the layer’s outputs
    at time step *t* for each instance in the mini-batch (*m* is the number of instances
    in the mini-batch and *n*[neurons] is the number of neurons).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ŷ**[(*t*)]是一个*m*×*n*[neurons]矩阵，包含小批量中每个实例在时间步*t*的层输出（*m*是小批量中实例的数量，*n*[neurons]是神经元的数量）。'
- en: '**X**[(*t*)] is an *m* × *n*[inputs] matrix containing the inputs for all instances
    (*n*[inputs] is the number of input features).'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**X**[(*t*)]是一个*m*×*n*[inputs]矩阵，包含所有实例的输入（*n*[inputs]是输入特征的数量）。'
- en: '**W**[*x*] is an *n*[inputs] × *n*[neurons] matrix containing the connection
    weights for the inputs of the current time step.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W**[*x*]是一个*n*[inputs]×*n*[neurons]矩阵，包含当前时间步输入的连接权重。'
- en: '**W**[*ŷ*] is an *n*[neurons] × *n*[neurons] matrix containing the connection
    weights for the outputs of the previous time step.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W**[*ŷ*]是一个*n*[neurons]×*n*[neurons]矩阵，包含上一个时间步输出的连接权重。'
- en: '**b** is a vector of size *n*[neurons] containing each neuron’s bias term.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**b**是一个大小为*n*[neurons]的向量，包含每个神经元的偏置项。'
- en: The weight matrices **W**[*x*] and **W**[*ŷ*] are often concatenated vertically
    into a single weight matrix **W** of shape (*n*[inputs] + *n*[neurons]) × *n*[neurons]
    (see the second line of [Equation 15-2](#rnn_output_vectorized_equation)).
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重矩阵**W**[*x*]和**W**[*ŷ*]通常垂直连接成一个形状为(*n*[inputs] + *n*[neurons]) × *n*[neurons]的单个权重矩阵**W**（参见[方程15-2](#rnn_output_vectorized_equation)的第二行）。
- en: The notation [**X**[(*t*)] **Ŷ**[(*t*–1)]] represents the horizontal concatenation
    of the matrices **X**[(*t*)] and **Ŷ**[(*t*–1)].
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 符号[**X**[(*t*)] **Ŷ**[(*t*–1)]]表示矩阵**X**[(*t*)]和**Ŷ**[(*t*–1)]的水平连接。
- en: Notice that **Ŷ**[(*t*)] is a function of **X**[(*t*)] and **Ŷ**[(*t*–1)], which
    is a function of **X**[(*t*–1)] and **Ŷ**[(*t*–2)], which is a function of **X**[(*t*–2)]
    and **Ŷ**[(*t*–3)], and so on. This makes **Ŷ**[(*t*)] a function of all the inputs
    since time *t* = 0 (that is, **X**[(0)], **X**[(1)], …​, **X**[(*t*)]). At the
    first time step, *t* = 0, there are no previous outputs, so they are typically
    assumed to be all zeros.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，**Ŷ**[(*t*)]是**X**[(*t*)]和**Ŷ**[(*t*–1)]的函数，**X**[(*t*–1)]和**Ŷ**[(*t*–2)]的函数，**X**[(*t*–2)]和**Ŷ**[(*t*–3)]的函数，依此类推。这使得**Ŷ**[(*t*)]是自时间*t*=0（即**X**[(0)],
    **X**[(1)], …​, **X**[(*t*)]）以来所有输入的函数。在第一个时间步骤，*t*=0时，没有先前的输出，因此通常假定它们都是零。
- en: Memory Cells
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 记忆单元
- en: Since the output of a recurrent neuron at time step *t* is a function of all
    the inputs from previous time steps, you could say it has a form of *memory*.
    A part of a neural network that preserves some state across time steps is called
    a *memory cell* (or simply a *cell*). A single recurrent neuron, or a layer of
    recurrent neurons, is a very basic cell, capable of learning only short patterns
    (typically about 10 steps long, but this varies depending on the task). Later
    in this chapter, we will look at some more complex and powerful types of cells
    capable of learning longer patterns (roughly 10 times longer, but again, this
    depends on the task).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于递归神经元在时间步骤*t*的输出是前几个时间步骤的所有输入的函数，因此可以说它具有一种*记忆*形式。在时间步骤之间保留一些状态的神经网络的一部分称为*记忆单元*（或简称*单元*）。单个递归神经元或一层递归神经元是一个非常基本的单元，只能学习短模式（通常约为10个步骤长，但这取决于任务）。在本章后面，我们将看一些更复杂和强大的单元类型，能够学习更长的模式（大约长10倍，但这也取决于任务）。
- en: 'A cell’s state at time step *t*, denoted **h**[(*t*)] (the “h” stands for “hidden”),
    is a function of some inputs at that time step and its state at the previous time
    step: **h**[(*t*)] = *f*(**x**[(*t*)], **h**[(*t*–1)]). Its output at time step
    *t*, denoted **ŷ**[(*t*)], is also a function of the previous state and the current
    inputs. In the case of the basic cells we have discussed so far, the output is
    just equal to the state, but in more complex cells this is not always the case,
    as shown in [Figure 15-3](#hidden_state_diagram).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 时间步骤*t*时的单元状态，表示为**h**[(*t*)]（“h”代表“隐藏”），是该时间步骤的一些输入和上一个时间步骤的状态的函数：**h**[(*t*)]
    = *f*(**x**[(*t*)], **h**[(*t*–1)]）。在时间步骤*t*的输出，表示为**ŷ**[(*t*)]，也是前一个状态和当前输入的函数。在我们迄今讨论的基本单元的情况下，输出只等于状态，但在更复杂的单元中，情况并非总是如此，如[图15-3](#hidden_state_diagram)所示。
- en: '![mls3 1503](assets/mls3_1503.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1503](assets/mls3_1503.png)'
- en: Figure 15-3\. A cell’s hidden state and its output may be different
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-3。单元的隐藏状态和输出可能不同
- en: Input and Output Sequences
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入和输出序列
- en: 'An RNN can simultaneously take a sequence of inputs and produce a sequence
    of outputs (see the top-left network in [Figure 15-4](#seq_to_seq_diagram)). This
    type of *sequence-to-sequence network* is useful to forecast time series, such
    as your home’s daily power consumption: you feed it the data over the last *N*
    days, and you train it to output the power consumption shifted by one day into
    the future (i.e., from *N* – 1 days ago to tomorrow).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: RNN可以同时接受一系列输入并产生一系列输出（参见[图15-4](#seq_to_seq_diagram)左上方的网络）。这种*序列到序列网络*对于预测时间序列非常有用，例如您家每天的用电量：您向其提供过去*N*天的数据，并训练它输出将来一天的用电量（即从*N* – 1天前到明天）。
- en: Alternatively, you could feed the network a sequence of inputs and ignore all
    outputs except for the last one (see the top-right network in [Figure 15-4](#seq_to_seq_diagram)).
    This is a *sequence-to-vector network*. For example, you could feed the network
    a sequence of words corresponding to a movie review, and the network would output
    a sentiment score (e.g., from 0 [hate] to 1 [love]).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以向网络提供一系列输入并忽略除最后一个之外的所有输出（参见[图15-4](#seq_to_seq_diagram)右上方的网络）。这是一个*序列到向量网络*。例如，您可以向网络提供与电影评论相对应的一系列单词，网络将输出情感分数（例如，从0
    [讨厌]到1 [喜爱]）。
- en: Conversely, you could feed the network the same input vector over and over again
    at each time step and let it output a sequence (see the bottom-left network of
    [Figure 15-4](#seq_to_seq_diagram)). This is a *vector-to-sequence network*. For
    example, the input could be an image (or the output of a CNN), and the output
    could be a caption for that image.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，您可以在每个时间步骤反复向网络提供相同的输入向量，并让它输出一个序列（参见[图15-4](#seq_to_seq_diagram)左下方的网络）。这是一个*向量到序列网络*。例如，输入可以是一幅图像（或CNN的输出），输出可以是该图像的标题。
- en: 'Lastly, you could have a sequence-to-vector network, called an *encoder*, followed
    by a vector-to-sequence network, called a *decoder* (see the bottom-right network
    of [Figure 15-4](#seq_to_seq_diagram)). For example, this could be used for translating
    a sentence from one language to another. You would feed the network a sentence
    in one language, the encoder would convert this sentence into a single vector
    representation, and then the decoder would decode this vector into a sentence
    in another language. This two-step model, called an [*encoder–decoder*](https://homl.info/seq2seq),⁠^([2](ch15.html#idm45720181997536))
    works much better than trying to translate on the fly with a single sequence-to-sequence
    RNN (like the one represented at the top left): the last words of a sentence can
    affect the first words of the translation, so you need to wait until you have
    seen the whole sentence before translating it. We will go through the implementation
    of an encoder–decoder in [Chapter 16](ch16.html#nlp_chapter) (as you will see,
    it is a bit more complex than what [Figure 15-4](#seq_to_seq_diagram) suggests).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以有一个序列到向量网络，称为*编码器*，后面是一个向量到序列网络，称为*解码器*（参见[图15-4](#seq_to_seq_diagram)的右下方网络）。例如，这可以用于将一种语言的句子翻译成另一种语言。您将向网络提供一种语言的句子，编码器将把这个句子转换成一个单一的向量表示，然后解码器将把这个向量解码成另一种语言的句子。这种两步模型，称为[*编码器-解码器*](https://homl.info/seq2seq)⁠^([2](ch15.html#idm45720181997536))比尝试使用单个序列到序列的RNN实时翻译要好得多（就像左上角表示的那种）：一个句子的最后几个词可能会影响翻译的前几个词，因此您需要等到看完整个句子后再进行翻译。我们将在[第16章](ch16.html#nlp_chapter)中介绍编码器-解码器的实现（正如您将看到的，它比[图15-4](#seq_to_seq_diagram)所暗示的要复杂一些）。
- en: '![mls3 1504](assets/mls3_1504.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1504](assets/mls3_1504.png)'
- en: Figure 15-4\. Sequence-to-sequence (top left), sequence-to-vector (top right),
    vector-to-sequence (bottom left), and encoder–decoder (bottom right) networks
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-4\. 序列到序列（左上）、序列到向量（右上）、向量到序列（左下）和编码器-解码器（右下）网络
- en: This versatility sounds promising, but how do you train a recurrent neural network?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多功能性听起来很有前途，但如何训练循环神经网络呢？
- en: Training RNNs
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练RNNs
- en: To train an RNN, the trick is to unroll it through time (like we just did) and
    then use regular backpropagation (see [Figure 15-5](#bptt_diagram)). This strategy
    is called *backpropagation through time* (BPTT).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练RNN，关键是将其通过时间展开（就像我们刚刚做的那样），然后使用常规的反向传播（参见[图15-5](#bptt_diagram)）。这种策略称为*通过时间的反向传播*（BPTT）。
- en: Just like in regular backpropagation, there is a first forward pass through
    the unrolled network (represented by the dashed arrows). Then the output sequence
    is evaluated using a loss function ℒ(**Y**[(0)], **Y**[(1)], …​, **Y**[(*T*)];
    **Ŷ**[(0)], **Ŷ**[(1)], …​, **Ŷ**[(*T*)]) (where **Y**[(*i*)] is the *i*^(th)
    target, **Ŷ**[(*i*)] is the *i*^(th) prediction, and *T* is the max time step).
    Note that this loss function may ignore some outputs. For example, in a sequence-to-vector
    RNN, all outputs are ignored except for the very last one. In [Figure 15-5](#bptt_diagram),
    the loss function is computed based on the last three outputs only. The gradients
    of that loss function are then propagated backward through the unrolled network
    (represented by the solid arrows). In this example, since the outputs **Ŷ**[(0)]
    and **Ŷ**[(1)] are not used to compute the loss, the gradients do not flow backward
    through them; they only flow through **Ŷ**[(2)], **Ŷ**[(3)], and **Ŷ**[(4)]. Moreover,
    since the same parameters **W** and **b** are used at each time step, their gradients
    will be tweaked multiple times during backprop. Once the backward phase is complete
    and all the gradients have been computed, BPTT can perform a gradient descent
    step to update the parameters (this is no different from regular backprop).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 就像常规反向传播一样，首先通过展开的网络进行第一次前向传递（由虚线箭头表示）。然后使用损失函数ℒ(**Y**[(0)], **Y**[(1)], …​,
    **Y**[(*T*)]; **Ŷ**[(0)], **Ŷ**[(1)], …​, **Ŷ**[(*T*)])评估输出序列（其中**Y**[(*i*)]是第*i*个目标，**Ŷ**[(*i*)]是第*i*个预测，*T*是最大时间步长）。请注意，此损失函数可能会忽略一些输出。例如，在序列到向量的RNN中，除了最后一个输出之外，所有输出都会被忽略。在[图15-5](#bptt_diagram)中，损失函数仅基于最后三个输出计算。然后，该损失函数的梯度通过展开的网络向后传播（由实线箭头表示）。在这个例子中，由于输出**Ŷ**[(0)]和**Ŷ**[(1)]没有用于计算损失，梯度不会通过它们向后传播；它们只会通过**Ŷ**[(2)]、**Ŷ**[(3)]和**Ŷ**[(4)]向后传播。此外，由于在每个时间步骤中使用相同的参数**W**和**b**，它们的梯度将在反向传播过程中被多次调整。一旦反向阶段完成并计算出所有梯度，BPTT可以执行梯度下降步骤来更新参数（这与常规反向传播没有区别）。
- en: '![mls3 1505](assets/mls3_1505.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1505](assets/mls3_1505.png)'
- en: Figure 15-5\. Backpropagation through time
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-5\. 通过时间反向传播
- en: Fortunately, Keras takes care of all of this complexity for you, as you will
    see. But before we get there, let’s load a time series and start analyzing it
    using classical tools to better understand what we’re dealing with, and to get
    some baseline metrics.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Keras会为您处理所有这些复杂性，您将看到。但在我们到达那里之前，让我们加载一个时间序列，并开始使用传统工具进行分析，以更好地了解我们正在处理的内容，并获得一些基准指标。
- en: Forecasting a Time Series
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测时间序列
- en: All right! Let’s pretend you’ve just been hired as a data scientist by Chicago’s
    Transit Authority. Your first task is to build a model capable of forecasting
    the number of passengers that will ride on bus and rail the next day. You have
    access to daily ridership data since 2001\. Let’s walk through together how you
    would handle this. We’ll start by loading and cleaning up the data:^([3](ch15.html#idm45720181962848))
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 好了！假设您刚被芝加哥交通管理局聘为数据科学家。您的第一个任务是构建一个能够预测明天公交和轨道乘客数量的模型。您可以访问自2001年以来的日常乘客数据。让我们一起看看您将如何处理这个问题。我们将从加载和清理数据开始：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We load the CSV file, set short column names, sort the rows by date, remove
    the redundant `total` column, and drop duplicate rows. Now let’s check what the
    first few rows look like:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载CSV文件，设置短列名，按日期对行进行排序，删除多余的`total`列，并删除重复行。现在让我们看看前几行是什么样子的：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: On January 1st, 2001, 297,192 people boarded a bus in Chicago, and 126,455 boarded
    a train. The `day_type` column contains `W` for **W**eekdays, `A` for S**a**turdays,
    and `U` for S**u**ndays or holidays.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在2001年1月1日，芝加哥有297,192人乘坐公交车，126,455人乘坐火车。`day_type`列包含`W`表示**工作日**，`A`表示**周六**，`U`表示**周日**或假期。
- en: 'Now let’s plot the bus and rail ridership figures over a few months in 2019,
    to see what it looks like (see [Figure 15-6](#daily_ridership_plot)):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们绘制2019年几个月的公交和火车乘客量数据，看看它是什么样子的（参见[图15-6](#daily_ridership_plot)）：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![mls3 1506](assets/mls3_1506.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1506](assets/mls3_1506.png)'
- en: Figure 15-6\. Daily ridership in Chicago
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-6。芝加哥的日常乘客量
- en: 'Note that Pandas includes both the start and end month in the range, so this
    plots the data from the 1st of March all the way up to the 31st of May. This is
    a *time series*: data with values at different time steps, usually at regular
    intervals. More specifically, since there are multiple values per time step, this
    is called a *multivariate time series*. If we only looked at the `bus` column,
    it would be a *univariate time series*, with a single value per time step. Predicting
    future values (i.e., forecasting) is the most typical task when dealing with time
    series, and this is what we will focus on in this chapter. Other tasks include
    imputation (filling in missing past values), classification, anomaly detection,
    and more.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，Pandas在范围中包括起始月份和结束月份，因此这将绘制从3月1日到5月31日的数据。这是一个*时间序列*：在不同时间步长上具有值的数据，通常在规则间隔上。更具体地说，由于每个时间步长有多个值，因此称为*多变量时间序列*。如果我们只看`bus`列，那将是一个*单变量时间序列*，每个时间步长有一个值。在处理时间序列时，预测未来值（即预测）是最典型的任务，这也是我们将在本章中重点关注的内容。其他任务包括插补（填补缺失的过去值）、分类、异常检测等。
- en: 'Looking at [Figure 15-6](#daily_ridership_plot), we can see that a similar
    pattern is clearly repeated every week. This is called a weekly *seasonality*.
    In fact, it’s so strong in this case that forecasting tomorrow’s ridership by
    just copying the values from a week earlier will yield reasonably good results.
    This is called *naive forecasting*: simply copying a past value to make our forecast.
    Naive forecasting is often a great baseline, and it can even be tricky to beat
    in some cases.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 查看[图15-6](#daily_ridership_plot)，我们可以看到每周明显重复的类似模式。这被称为每周*季节性*。实际上，在这种情况下，季节性非常强，通过简单地复制一周前的值来预测明天的乘客量将产生相当不错的结果。这被称为*天真预测*：简单地复制过去的值来进行预测。天真预测通常是一个很好的基准，有时甚至在某些情况下很难超越。
- en: Note
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In general, naive forecasting means copying the latest known value (e.g., forecasting
    that tomorrow will be the same as today). However, in our case, copying the value
    from the previous week works better, due to the strong weekly seasonality.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，天真预测意味着复制最新已知值（例如，预测明天与今天相同）。然而，在我们的情况下，复制上周的值效果更好，因为存在强烈的每周季节性。
- en: 'To visualize these naive forecasts, let’s overlay the two time series (for
    bus and rail) as well as the same time series lagged by one week (i.e., shifted
    toward the right) using dotted lines. We’ll also plot the difference between the
    two (i.e., the value at time *t* minus the value at time *t* – 7); this is called
    *differencing* (see [Figure 15-7](#differencing_plot)):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化这些天真预测，让我们将两个时间序列（公交和火车）以及相同时间序列向右移动一周（即向右移动）的时间序列叠加使用虚线。我们还将绘制两者之间的差异（即时间*t*处的值减去时间*t*
    - 7处的值）；这称为*差分*（参见[图15-7](#differencing_plot)）：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Not too bad! Notice how closely the lagged time series track the actual time
    series. When a time series is correlated with a lagged version of itself, we say
    that the time series is *autocorrelated*. As you can see, most of the differences
    are fairly small, except at the end of May. Maybe there was a holiday at that
    time? Let’s check the `day_type` column:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 不错！注意滞后时间序列如何紧密跟踪实际时间序列。当一个时间序列与其滞后版本相关联时，我们说该时间序列是*自相关*的。正如您所看到的，大多数差异都相当小，除了五月底。也许那时有一个假期？让我们检查`day_type`列：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![mls3 1507](assets/mls3_1507.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1507](assets/mls3_1507.png)'
- en: Figure 15-7\. Time series overlaid with 7-day lagged time series (top), and
    difference between *t* and *t* – 7 (bottom)
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-7。与7天滞后时间序列叠加的时间序列（顶部），以及*t*和*t* - 7之间的差异（底部）
- en: 'Indeed, there was a long weekend back then: the Monday was the Memorial Day
    holiday. We could use this column to improve our forecasts, but for now let’s
    just measure the mean absolute error over the three-month period we’re arbitrarily
    focusing on—March, April, and May 2019—to get a rough idea:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，那时有一个长周末：周一是阵亡将士纪念日假期。我们可以使用这一列来改进我们的预测，但现在让我们只测量我们任意关注的三个月期间（2019年3月、4月和5月）的平均绝对误差，以获得一个大致的概念：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Our naive forecasts get an MAE of about 43,916 bus riders, and about 42,143
    rail riders. It’s hard to tell at a glance how good or bad this is, so let’s put
    the forecast errors into perspective by dividing them by the target values:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的天真预测得到大约43,916名公交乘客和约42,143名火车乘客的MAE。一眼看去很难判断这是好是坏，所以让我们将预测误差放入透视中，通过将它们除以目标值来进行评估：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'What we just computed is called the *mean absolute percentage error* (MAPE):
    it looks like our naive forecasts give us a MAPE of roughly 8.3% for bus and 9.0%
    for rail. It’s interesting to note that the MAE for the rail forecasts looks slightly
    better than the MAE for the bus forecasts, while the opposite is true for the
    MAPE. That’s because the bus ridership is larger than the rail ridership, so naturally
    the forecast errors are also larger, but when we put the errors into perspective,
    it turns out that the bus forecasts are actually slightly better than the rail
    forecasts.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚计算的是*平均绝对百分比误差*（MAPE）：看起来我们的天真预测为公交大约为8.3%，火车为9.0%。有趣的是，火车预测的MAE看起来比公交预测的稍好一些，而MAPE则相反。这是因为公交乘客量比火车乘客量大，因此自然预测误差也更大，但当我们将误差放入透视时，结果表明公交预测实际上略优于火车预测。
- en: Tip
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The MAE, MAPE, and MSE are among the most common metrics you can use to evaluate
    your forecasts. As always, choosing the right metric depends on the task. For
    example, if your project suffers quadratically more from large errors than from
    small ones, then the MSE may be preferable, as it strongly penalizes large errors.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: MAE、MAPE和MSE是评估预测的最常见指标之一。与往常一样，选择正确的指标取决于任务。例如，如果您的项目对大误差的影响是小误差的平方倍，那么MSE可能更可取，因为它会严厉惩罚大误差。
- en: 'Looking at the time series, there doesn’t appear to be any significant monthly
    seasonality, but let’s check whether there’s any yearly seasonality. We’ll look
    at the data from 2001 to 2019\. To reduce the risk of data snooping, we’ll ignore
    more recent data for now. Let’s also plot a 12-month rolling average for each
    series to visualize long-term trends (see [Figure 15-8](#long_term_ridership_plot)):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 观察时间序列，似乎没有明显的月度季节性，但让我们检查一下是否存在年度季节性。我们将查看2001年至2019年的数据。为了减少数据窥探的风险，我们暂时忽略更近期的数据。让我们为每个系列绘制一个12个月的滚动平均线，以可视化长期趋势（参见[图15-8](#long_term_ridership_plot)）：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![mls3 1508](assets/mls3_1508.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1508](assets/mls3_1508.png)'
- en: Figure 15-8\. Yearly seasonality and long-term trends
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-8。年度季节性和长期趋势
- en: 'Yep! There’s definitely some yearly seasonality as well, although it is noisier
    than the weekly seasonality, and more visible for the rail series than the bus
    series: we see peaks and troughs at roughly the same dates each year. Let’s check
    what we get if we plot the 12-month difference (see [Figure 15-9](#yearly_diff_plot)):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 是的！确实存在一些年度季节性，尽管比每周季节性更嘈杂，对于铁路系列而言更为明显，而不是公交系列：我们看到每年大致相同日期出现高峰和低谷。让我们看看如果绘制12个月的差分会得到什么（参见[图15-9](#yearly_diff_plot)）：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![mls3 1509](assets/mls3_1509.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1509](assets/mls3_1509.png)'
- en: Figure 15-9\. The 12-month difference
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-9。12个月的差分
- en: 'Notice how differencing not only removed the yearly seasonality, but it also
    removed the long-term trends. For example, the linear downward trend present in
    the time series from 2016 to 2019 became a roughly constant negative value in
    the differenced time series. In fact, differencing is a common technique used
    to remove trend and seasonality from a time series: it’s easier to study a *stationary*
    time series, meaning one whose statistical properties remain constant over time,
    without any seasonality or trends. Once you’re able to make accurate forecasts
    on the differenced time series, it’s easy to turn them into forecasts for the
    actual time series by just adding back the past values that were previously subtracted.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，差分不仅消除了年度季节性，还消除了长期趋势。例如，2016年至2019年时间序列中存在的线性下降趋势在差分时间序列中变为大致恒定的负值。事实上，差分是一种常用的技术，用于消除时间序列中的趋势和季节性：研究*平稳*时间序列更容易，这意味着其统计特性随时间保持不变，没有任何季节性或趋势。一旦您能够对差分时间序列进行准确的预测，只需将先前减去的过去值添加回来，就可以将其转换为实际时间序列的预测。
- en: You may be thinking that we’re only trying to predict tomorrow’s ridership,
    so the long-term patterns matter much less than the short-term ones. You’re right,
    but still, we may be able to improve performance slightly by taking long-term
    patterns into account. For example, daily bus ridership dropped by about 2,500
    in October 2017, which represents about 570 fewer passengers each week, so if
    we were at the end of October 2017, it would make sense to forecast tomorrow’s
    ridership by copying the value from last week, minus 570\. Accounting for the
    trend will make your forecasts a bit more accurate on average.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会认为我们只是试图预测明天的乘客量，因此长期模式比短期模式更不重要。您是对的，但是，通过考虑长期模式，我们可能能够稍微提高性能。例如，2017年10月，每日公交乘客量减少了约2500人，这代表每周减少约570名乘客，因此如果我们处于2017年10月底，通过从上周复制数值，减去570，来预测明天的乘客量是有道理的。考虑趋势将使您的平均预测略微更准确。
- en: Now that you’re familiar with the ridership time series, as well as some of
    the most important concepts in time series analysis, including seasonality, trend,
    differencing, and moving averages, let’s take a quick look at a very popular family
    of statistical models that are commonly used to analyze time series.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您熟悉了乘客量时间序列，以及时间序列分析中一些最重要的概念，包括季节性、趋势、差分和移动平均，让我们快速看一下一个非常流行的统计模型家族，通常用于分析时间序列。
- en: The ARMA Model Family
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ARMA模型家族
- en: 'We’ll start with the *autoregressive moving average* (ARMA) model, developed
    by Herman Wold in the 1930s: it computes its forecasts using a simple weighted
    sum of lagged values and corrects these forecasts by adding a moving average,
    very much like we just discussed. Specifically, the moving average component is
    computed using a weighted sum of the last few forecast errors. [Equation 15-3](#arma_equation)
    shows how the model makes its forecasts.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从上世纪30年代由赫尔曼·沃尔德（Herman Wold）开发的*自回归移动平均*（ARMA）模型开始：它通过对滞后值的简单加权和添加移动平均来计算其预测，非常类似我们刚刚讨论的。具体来说，移动平均分量是通过最近几个预测误差的加权和来计算的。[方程15-3](#arma_equation)展示了该模型如何进行预测。
- en: Equation 15-3\. Forecasting using an ARMA model
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第15-3方程。使用ARMA模型进行预测
- en: <math><mtable columnalign="left"><mtr><mtd><msub><mover><mi>y</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><msub><mi>α</mi><mi>i</mi></msub><msub><mi>y</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>+</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></munderover><msub><mi>θ</mi><mi>i</mi></msub><msub><mi>ϵ</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd></mtr><mtr><mtd><mtext>with </mtext><msub><mi>ϵ</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><msub><mi>y</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>-</mo><msub><mover><mi>y</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd></mtr></mtable></math>
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mtable columnalign="left"><mtr><mtd><msub><mover><mi>y</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>p</mi></munderover><msub><mi>α</mi><mi>i</mi></msub><msub><mi>y</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>i</mi><mo>)</mo></mrow></msub><mo>+</mo><munderover><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>q</mi></munderover><msub><mi>θ</mi><mi>i</mi></msub><msub><mi>ϵ</mi><mrow><mo>(</mo><mi>t</mi><mo>-</mo><mi>i</mi><mo>)</mo></mrow></msub></mtd></mtr><mtr><mtd><mtext>with </mtext><msub><mi>ϵ</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>=</mo><msub><mi>y</mi><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub><mo>-</mo><msub><mover><mi>y</mi><mo>^</mo></mover><mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd></mtr></mtable></math>
- en: 'In this equation:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '*ŷ*[(*t*)] is the model’s forecast for time step *t*.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*[(*t*)] is the time series’ value at time step *t*.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first sum is the weighted sum of the past *p* values of the time series,
    using the learned weights *α*[*i*]. The number *p* is a hyperparameter, and it
    determines how far back into the past the model should look. This sum is the *autoregressive*
    component of the model: it performs regression based on past values.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second sum is the weighted sum over the past *q* forecast errors *ε*[(*t*)],
    using the learned weights *θ*[*i*]. The number *q* is a hyperparameter. This sum
    is the moving average component of the model.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Importantly, this model assumes that the time series is stationary. If it is
    not, then differencing may help. Using differencing over a single time step will
    produce an approximation of the derivative of the time series: indeed, it will
    give the slope of the series at each time step. This means that it will eliminate
    any linear trend, transforming it into a constant value. For example, if you apply
    one-step differencing to the series [3, 5, 7, 9, 11], you get the differenced
    series [2, 2, 2, 2].'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: If the original time series has a quadratic trend instead of a linear trend,
    then a single round of differencing will not be enough. For example, the series
    [1, 4, 9, 16, 25, 36] becomes [3, 5, 7, 9, 11] after one round of differencing,
    but if you run differencing for a second round, then you get [2, 2, 2, 2]. So,
    running two rounds of differencing will eliminate quadratic trends. More generally,
    running *d* consecutive rounds of differencing computes an approximation of the
    *d*^(th) order derivative of the time series, so it will eliminate polynomial
    trends up to degree *d*. This hyperparameter *d* is called the *order of integration*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: 'Differencing is the central contribution of the *autoregressive integrated
    moving average* (ARIMA) model, introduced in 1970 by George Box and Gwilym Jenkins
    in their book *Time Series Analysis* (Wiley): this model runs *d* rounds of differencing
    to make the time series more stationary, then it applies a regular ARMA model.
    When making forecasts, it uses this ARMA model, then it adds back the terms that
    were subtracted by differencing.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: 'One last member of the ARMA family is the *seasonal ARIMA* (SARIMA) model:
    it models the time series in the same way as ARIMA, but it additionally models
    a seasonal component for a given frequency (e.g., weekly), using the exact same
    ARIMA approach. It has a total of seven hyperparameters: the same *p*, *d*, and
    *q* hyperparameters as ARIMA, plus additional *P*, *D*, and *Q* hyperparameters
    to model the seasonal pattern, and lastly the period of the seasonal pattern,
    noted *s*. The hyperparameters *P*, *D*, and *Q* are just like *p*, *d*, and *q*,
    but they are used to model the time series at *t* – *s*, *t* – 2*s*, *t* – 3*s*,
    etc.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see how to fit a SARIMA model to the rail time series, and use it to
    make a forecast for tomorrow’s ridership. We’ll pretend today is the last day
    of May 2019, and we want to forecast the rail ridership for “tomorrow”, the 1st
    of June, 2019\. For this, we can use the `statsmodels` library, which contains
    many different statistical models, including the ARMA model and its variants,
    implemented by the `ARIMA` class:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何将SARIMA模型拟合到铁路时间序列，并用它来预测明天的乘客量。我们假设今天是2019年5月的最后一天，我们想要预测“明天”，也就是2019年6月1日的铁路乘客量。为此，我们可以使用`statsmodels`库，其中包含许多不同的统计模型，包括由`ARIMA`类实现的ARMA模型及其变体：
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this code example:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码示例中：
- en: 'We start by importing the `ARIMA` class, then we take the rail ridership data
    from the start of 2019 up to “today”, and we use `asfreq("D")` to set the time
    series’ frequency to daily: this doesn’t change the data at all in this case,
    since it’s already daily, but without this the `ARIMA` class would have to guess
    the frequency, and it would display a warning.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先导入`ARIMA`类，然后我们从2019年初开始到“今天”获取铁路乘客数据，并使用`asfreq("D")`将时间序列的频率设置为每天：在这种情况下，这不会改变数据，因为它已经是每天的，但如果没有这个，`ARIMA`类将不得不猜测频率，并显示警告。
- en: 'Next, we create an `ARIMA` instance, passing it all the data until “today”,
    and we set the model hyperparameters: `order=(1, 0, 0)` means that *p* = 1, *d*
    = 0, *q* = 0, and `seasonal_order=(0, 1, 1, 7)` means that *P* = 0, *D* = 1, *Q*
    = 1, and *s* = 7\. Notice that the `statsmodels` API differs a bit from Scikit-Learn’s
    API, since we pass the data to the model at construction time, instead of passing
    it to the `fit()` method.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个`ARIMA`实例，将所有数据传递到“今天”，并设置模型超参数：`order=(1, 0, 0)`表示*p*=1，*d*=0，*q*=0，`seasonal_order=(0,
    1, 1, 7)`表示*P*=0，*D*=1，*Q*=1，*s*=7。请注意，`statsmodels` API与Scikit-Learn的API有些不同，因为我们在构建时将数据传递给模型，而不是将数据传递给`fit()`方法。
- en: Next, we fit the model, and we use it to make a forecast for “tomorrow”, the
    1st of June, 2019.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们拟合模型，并用它为“明天”，也就是2019年6月1日，做出预测。
- en: 'The forecast is 427,759 passengers, when in fact there were 379,044\. Yikes,
    we’re 12.9% off—that’s pretty bad. It’s actually slightly worse than naive forecasting,
    which forecasts 426,932, off by 12.6%. But perhaps we were just unlucky that day?
    To check this, we can run the same code in a loop to make forecasts for every
    day in March, April, and May, and compute the MAE over that period:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 预测为427,759名乘客，而实际上有379,044名。哎呀，我们偏差12.9%——这相当糟糕。实际上，这比天真预测稍微糟糕，天真预测为426,932，偏差为12.6%。但也许那天我们只是运气不好？为了检查这一点，我们可以在循环中运行相同的代码，为三月、四月和五月的每一天进行预测，并计算该期间的平均绝对误差：
- en: '[PRE10]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Ah, that’s much better! The MAE is about 32,041, which is significantly lower
    than the MAE we got with naive forecasting (42,143). So although the model is
    not perfect, it still beats naive forecasting by a large margin, on average.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 啊，好多了！平均绝对误差约为32,041，比我们用天真预测得到的平均绝对误差（42,143）显著低。因此，虽然模型并不完美，但平均而言仍然远远超过天真预测。
- en: 'At this point, you may be wondering how to pick good hyperparameters for the
    SARIMA model. There are several methods, but the simplest to understand and to
    get started with is the brute-force approach: just run a grid search. For each
    model you want to evaluate (i.e., each hyperparameter combination), you can run
    the preceding code example, changing only the hyperparameter values. Good *p*,
    *q*, *P*, and *Q* values are usually fairly small (typically 0 to 2, sometimes
    up to 5 or 6), and *d* and *D* are typically 0 or 1, sometimes 2\. As for *s*,
    it’s just the main seasonal pattern’s period: in our case it’s 7 since there’s
    a strong weekly seasonality. The model with the lowest MAE wins. Of course, you
    can replace the MAE with another metric if it better matches your business objective.
    And that’s it!^([4](ch15.html#idm45720180849712))'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您可能想知道如何为SARIMA模型选择良好的超参数。有几种方法，但最简单的方法是粗暴的方法：进行网格搜索。对于要评估的每个模型（即每个超参数组合），您可以运行前面的代码示例，仅更改超参数值。通常*p*、*q*、*P*和*Q*值较小（通常为0到2，有时可达5或6），*d*和*D*通常为0或1，有时为2。至于*s*，它只是主要季节模式的周期：在我们的情况下是7，因为有强烈的每周季节性。具有最低平均绝对误差的模型获胜。当然，如果它更符合您的业务目标，您可以用另一个指标替换平均绝对误差。就是这样！
- en: Preparing the Data for Machine Learning Models
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为机器学习模型准备数据
- en: 'Now that we have two baselines, naive forecasting and SARIMA, let’s try to
    use the machine learning models we’ve covered so far to forecast this time series,
    starting with a basic linear model. Our goal will be to forecast tomorrow’s ridership
    based on the ridership of the past 8 weeks of data (56 days). The inputs to our
    model will therefore be sequences (usually a single sequence per day once the
    model is in production), each containing 56 values from time steps *t* – 55 to
    *t*. For each input sequence, the model will output a single value: the forecast
    for time step *t* + 1.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了两个基线，天真预测和SARIMA，让我们尝试使用迄今为止涵盖的机器学习模型来预测这个时间序列，首先从基本的线性模型开始。我们的目标是根据过去8周（56天）的数据来预测明天的乘客量。因此，我们模型的输入将是序列（通常是生产中的每天一个序列），每个序列包含从时间步*t*
    - 55到*t*的56个值。对于每个输入序列，模型将输出一个值：时间步*t* + 1的预测。
- en: 'But what will we use as training data? Well, that’s the trick: we will use
    every 56-day window from the past as training data, and the target for each window
    will be the value immediately following it.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们将使用什么作为训练数据呢？嗯，这就是诀窍：我们将使用过去的每个56天窗口作为训练数据，每个窗口的目标将是紧随其后的值。
- en: 'Keras actually has a nice utility function called `tf.keras.utils.timeseries_​data⁠set_from_array()`
    to help us prepare the training set. It takes a time series as input, and it builds
    a tf.data.Dataset (introduced in [Chapter 13](ch13.html#data_chapter)) containing
    all the windows of the desired length, as well as their corresponding targets.
    Here’s an example that takes a time series containing the numbers 0 to 5 and creates
    a dataset containing all the windows of length 3, with their corresponding targets,
    grouped into batches of size 2:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Keras实际上有一个很好的实用函数称为`tf.keras.utils.timeseries_dataset_from_array()`，帮助我们准备训练集。它以时间序列作为输入，并构建一个tf.data.Dataset（在[第13章](ch13.html#data_chapter)中介绍）包含所需长度的所有窗口，以及它们对应的目标。以下是一个示例，它以包含数字0到5的时间序列为输入，并创建一个包含所有长度为3的窗口及其对应目标的数据集，分组成大小为2的批次：
- en: '[PRE11]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s inspect the contents of this dataset:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下这个数据集的内容：
- en: '[PRE12]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Each sample in the dataset is a window of length 3, along with its corresponding
    target (i.e., the value immediately after the window). The windows are [0, 1,
    2], [1, 2, 3], and [2, 3, 4], and their respective targets are 3, 4, and 5\. Since
    there are three windows in total, which is not a multiple of the batch size, the
    last batch only contains one window instead of two.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中的每个样本是长度为3的窗口，以及其对应的目标（即窗口后面的值）。窗口是[0, 1, 2]，[1, 2, 3]和[2, 3, 4]，它们各自的目标是3，4和5。由于总共有三个窗口，不是批次大小的倍数，最后一个批次只包含一个窗口而不是两个。
- en: 'Another way to get the same result is to use the `window()` method of tf.data’s
    `Dataset` class. It’s more complex, but it gives you full control, which will
    come in handy later in this chapter, so let’s see how it works. The `window()`
    method returns a dataset of window datasets:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种获得相同结果的方法是使用tf.data的`Dataset`类的`window()`方法。这更复杂，但它给了您完全的控制，这将在本章后面派上用场，让我们看看它是如何工作的。`window()`方法返回一个窗口数据集的数据集：
- en: '[PRE13]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this example, the dataset contains six windows, each shifted by one step
    compared to the previous one, and the last three windows are smaller because they’ve
    reached the end of the series. In general you’ll want to get rid of these smaller
    windows by passing `drop_remainder=True` to the `window()` method.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，数据集包含六个窗口，每个窗口相对于前一个窗口向前移动一个步骤，最后三个窗口较小，因为它们已经到达系列的末尾。通常情况下，您会希望通过向`window()`方法传递`drop_remainder=True`来摆脱这些较小的窗口。
- en: The `window()` method returns a *nested dataset*, analogous to a list of lists.
    This is useful when you want to transform each window by calling its dataset methods
    (e.g., to shuffle them or batch them). However, we cannot use a nested dataset
    directly for training, as our model will expect tensors as input, not datasets.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`window()`方法返回一个*嵌套数据集*，类似于一个列表的列表。当您想要通过调用其数据集方法（例如，对它们进行洗牌或分批处理）来转换每个窗口时，这将非常有用。然而，我们不能直接使用嵌套数据集进行训练，因为我们的模型将期望张量作为输入，而不是数据集。'
- en: 'Therefore, we must call the `flat_map()` method: it converts a nested dataset
    into a *flat dataset* (one that contains tensors, not datasets). For example,
    suppose {1, 2, 3} represents a dataset containing the sequence of tensors 1, 2,
    and 3\. If you flatten the nested dataset {{1, 2}, {3, 4, 5, 6}}, you get back
    the flat dataset {1, 2, 3, 4, 5, 6}.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们必须调用`flat_map()`方法：它将嵌套数据集转换为*平坦数据集*（包含张量而不是数据集）。例如，假设{1, 2, 3}表示包含张量1、2和3序列的数据集。如果展平嵌套数据集{{1,
    2}, {3, 4, 5, 6}}，您将得到平坦数据集{1, 2, 3, 4, 5, 6}。
- en: 'Moreover, the `flat_map()` method takes a function as an argument, which allows
    you to transform each dataset in the nested dataset before flattening. For example,
    if you pass the function `lambda ds: ds.batch(2)` to `flat_map()`, then it will
    transform the nested dataset {{1, 2}, {3, 4, 5, 6}} into the flat dataset {[1,
    2], [3, 4], [5, 6]}: it’s a dataset containing 3 tensors, each of size 2.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，`flat_map()`方法接受一个函数作为参数，允许您在展平之前转换嵌套数据集中的每个数据集。例如，如果您将函数`lambda ds: ds.batch(2)`传递给`flat_map()`，那么它将把嵌套数据集{{1,
    2}, {3, 4, 5, 6}}转换为平坦数据集{[1, 2], [3, 4], [5, 6]}：这是一个包含3个大小为2的张量的数据集。'
- en: 'With that in mind, we are ready to flatten our dataset:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，我们准备对数据集进行展平处理：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Since each window dataset contains exactly four items, calling `batch(4)` on
    a window produces a single tensor of size 4\. Great! We now have a dataset containing
    consecutive windows represented as tensors. Let’s create a little helper function
    to make it easier to extract windows from a dataset:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个窗口数据集恰好包含四个项目，对窗口调用`batch(4)`会产生一个大小为4的单个张量。太棒了！现在我们有一个包含连续窗口的数据集，表示为张量。让我们创建一个小助手函数，以便更容易地从数据集中提取窗口：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The last step is to split each window into inputs and targets, using the `map()`
    method. We can also group the resulting windows into batches of size 2:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是使用`map()`方法将每个窗口拆分为输入和目标。我们还可以将生成的窗口分组成大小为2的批次：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As you can see, we now have the same output as we got earlier with the `timeseries_dataset_from_array()`
    function (with a bit more effort, but it will be worthwhile soon).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，我们现在得到了与之前使用`timeseries_dataset_from_array()`函数相同的输出（稍微费劲一些，但很快就会值得）。
- en: 'Now, before we start training, we need to split our data into a training period,
    a validation period, and a test period. We will focus on the rail ridership for
    now. We will also scale it down by a factor of one million, to ensure the values
    are near the 0–1 range; this plays nicely with the default weight initialization
    and learning rate:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在开始训练之前，我们需要将数据分为训练期、验证期和测试期。我们现在将专注于铁路乘客量。我们还将通过一百万分之一的比例缩小它，以确保值接近0-1范围；这与默认的权重初始化和学习率很好地配合：
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note
  id: totrans-134
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: When dealing with time series, you generally want to split across time. However,
    in some cases you may be able to split along other dimensions, which will give
    you a longer time period to train on. For example, if you have data about the
    financial health of 10,000 companies from 2001 to 2019, you might be able to split
    this data across the different companies. It’s very likely that many of these
    companies will be strongly correlated, though (e.g., whole economic sectors may
    go up or down jointly), and if you have correlated companies across the training
    set and the test set, your test set will not be as useful, as its measure of the
    generalization error will be optimistically biased.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 处理时间序列时，通常希望按时间划分。但在某些情况下，您可能能够沿其他维度划分，这将使您有更长的时间段进行训练。例如，如果您有关于2001年至2019年间10,000家公司财务状况的数据，您可能能够将这些数据分割到不同的公司。然而，很可能这些公司中的许多将强相关（例如，整个经济部门可能一起上涨或下跌），如果在训练集和测试集中有相关的公司，那么您的测试集将不会那么有用，因为其泛化误差的度量将是乐观偏倚的。
- en: 'Next, let’s use `timeseries_dataset_from_array()` to create datasets for training
    and validation. Since gradient descent expects the instances in the training set
    to be independent and identically distributed (IID), as we saw in [Chapter 4](ch04.html#linear_models_chapter),
    we must set the argument `shuffle=True` to shuffle the training windows (but not
    their contents):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用`timeseries_dataset_from_array()`为训练和验证创建数据集。由于梯度下降期望训练集中的实例是独立同分布的（IID），正如我们在[第4章](ch04.html#linear_models_chapter)中看到的那样，我们必须设置参数`shuffle=True`来对训练窗口进行洗牌（但不洗牌其中的内容）：
- en: '[PRE18]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: And now we’re ready to build and train any regression model we want!
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好构建和训练任何回归模型了！
- en: Forecasting Using a Linear Model
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用线性模型进行预测
- en: 'Let’s try a basic linear model first. We will use the Huber loss, which usually
    works better than minimizing the MAE directly, as discussed in [Chapter 10](ch10.html#ann_chapter).
    We’ll also use early stopping:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先尝试一个基本的线性模型。我们将使用Huber损失，通常比直接最小化MAE效果更好，如[第10章](ch10.html#ann_chapter)中讨论的那样。我们还将使用提前停止：
- en: '[PRE19]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This model reaches a validation MAE of about 37,866 (your mileage may vary).
    That’s better than naive forecasting, but worse than the SARIMA model.^([5](ch15.html#idm45720180183168))
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型达到了约37,866的验证MAE（结果可能有所不同）。这比天真的预测要好，但比SARIMA模型要差。^([5](ch15.html#idm45720180183168))
- en: Can we do better with an RNN? Let’s see!
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能用RNN做得更好吗？让我们看看！
- en: Forecasting Using a Simple RNN
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用简单RNN进行预测
- en: 'Let’s try the most basic RNN, containing a single recurrent layer with just
    one recurrent neuron, as we saw in [Figure 15-1](#simple_rnn_diagram):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试最基本的RNN，其中包含一个具有一个循环神经元的单个循环层，就像我们在[图15-1](#simple_rnn_diagram)中看到的那样：
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'All recurrent layers in Keras expect 3D inputs of shape [*batch size*, *time
    steps*, *dimensionality*], where *dimensionality* is 1 for univariate time series
    and more for multivariate time series. Recall that the `input_shape` argument
    ignores the first dimension (i.e., the batch size), and since recurrent layers
    can accept input sequences of any length, we can set the second dimension to `None`,
    which means “any size”. Lastly, since we’re dealing with a univariate time series,
    we need the last dimension’s size to be 1\. This is why we specified the input
    shape `[None, 1]`: it means “univariate sequences of any length”. Note that the
    datasets actually contain inputs of shape [*batch size*, *time steps*], so we’re
    missing the last dimension, of size 1, but Keras is kind enough to add it for
    us in this case.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Keras中的所有循环层都期望形状为[*批量大小*，*时间步长*，*维度*]的3D输入，其中*维度*对于单变量时间序列为1，对于多变量时间序列为更多。请记住，`input_shape`参数忽略第一个维度（即批量大小），由于循环层可以接受任意长度的输入序列，因此我们可以将第二个维度设置为`None`，表示“任意大小”。最后，由于我们处理的是单变量时间序列，我们需要最后一个维度的大小为1。这就是为什么我们指定输入形状为`[None,
    1]`：它表示“任意长度的单变量序列”。请注意，数据集实际上包含形状为[*批量大小*，*时间步长*]的输入，因此我们缺少最后一个维度，大小为1，但在这种情况下，Keras很友好地为我们添加了它。
- en: 'This model works exactly as we saw earlier: the initial state *h*[(init)] is
    set to 0, and it is passed to a single recurrent neuron, along with the value
    of the first time step, *x*[(0)]. The neuron computes a weighted sum of these
    values plus the bias term, and it applies the activation function to the result,
    using the hyperbolic tangent function by default. The result is the first output,
    *y*[0]. In a simple RNN, this output is also the new state *h*[0]. This new state
    is passed to the same recurrent neuron along with the next input value, *x*[(1)],
    and the process is repeated until the last time step. At the end, the layer just
    outputs the last value: in our case the sequences are 56 steps long, so the last
    value is *y*[55]. All of this is performed simultaneously for every sequence in
    the batch, of which there are 32 in this case.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的工作方式与我们之前看到的完全相同：初始状态*h*[(init)]设置为0，并传递给一个单个的循环神经元，以及第一个时间步的值*x*[(0)]。神经元计算这些值加上偏置项的加权和，并使用默认的双曲正切函数对结果应用激活函数。结果是第一个输出*y*[0]。在简单RNN中，这个输出也是新状态*h*[0]。这个新状态传递给相同的循环神经元，以及下一个输入值*x*[(1)]，并且这个过程重复直到最后一个时间步。最后，该层只输出最后一个值：在我们的情况下，序列长度为56步，因此最后一个值是*y*[55]。所有这些都同时为批次中的每个序列执行，本例中有32个序列。
- en: Note
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: By default, recurrent layers in Keras only return the final output. To make
    them return one output per time step, you must set `return_sequences=True`, as
    you will see.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Keras中的循环层只返回最终输出。要使它们返回每个时间步的一个输出，您必须设置`return_sequences=True`，如您将看到的。
- en: So that’s our first recurrent model! It’s a sequence-to-vector model. Since
    there’s a single output neuron, the output vector has a size of 1.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的第一个循环模型！这是一个序列到向量的模型。由于只有一个输出神经元，输出向量的大小为1。
- en: 'Now if you compile, train, and evaluate this model just like the previous model,
    you will find that it’s no good at all: its validation MAE is greater than 100,000!
    Ouch. That was to be expected, for two reasons:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您编译、训练和评估这个模型，就像之前的模型一样，您会发现它一点用也没有：其验证MAE大于100,000！哎呀。这是可以预料到的，有两个原因：
- en: 'The model only has a single recurrent neuron, so the only data it can use to
    make a prediction at each time step is the input value at the current time step
    and the output value from the previous time step. That’s not much to go on! In
    other words, the RNN’s memory is extremely limited: it’s just a single number,
    its previous output. And let’s count how many parameters this model has: since
    there’s just one recurrent neuron with only two input values, the whole model
    only has three parameters (two weights plus a bias term). That’s far from enough
    for this time series. In contrast, our previous model could look at all 56 previous
    values at once, and it had a total of 57 parameters.'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型只有一个循环神经元，因此在每个时间步进行预测时，它只能使用当前时间步的输入值和上一个时间步的输出值。这不足以进行预测！换句话说，RNN的记忆极为有限：只是一个数字，它的先前输出。让我们来数一下这个模型有多少参数：由于只有一个循环神经元，只有两个输入值，整个模型只有三个参数（两个权重加上一个偏置项）。这对于这个时间序列来说远远不够。相比之下，我们之前的模型可以一次查看所有56个先前的值，并且总共有57个参数。
- en: The time series contains values from 0 to about 1.4, but since the default activation
    function is tanh, the recurrent layer can only output values between –1 and +1\.
    There’s no way it can predict values between 1.0 and 1.4.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 时间序列包含的值从0到约1.4，但由于默认激活函数是tanh，循环层只能输出-1到+1之间的值。它无法预测1.0到1.4之间的值。
- en: 'Let’s fix both of these issues: we will create a model with a larger recurrent
    layer, containing 32 recurrent neurons, and we will add a dense output layer on
    top of it with a single output neuron and no activation function. The recurrent
    layer will be able to carry much more information from one time step to the next,
    and the dense output layer will project the final output from 32 dimensions down
    to 1, without any value range constraints:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们解决这两个问题：我们将创建一个具有更大的循环层的模型，其中包含32个循环神经元，并在其顶部添加一个密集的输出层，其中只有一个输出神经元，没有激活函数。循环层将能够在一个时间步到下一个时间步传递更多信息，而密集输出层将把最终输出从32维投影到1维，没有任何值范围约束：
- en: '[PRE21]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now if you compile, fit, and evaluate this model just like the previous one,
    you will find that its validation MAE reaches 27,703\. That’s the best model we’ve
    trained so far, and it even beats the SARIMA model: we’re doing pretty well!'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果您像之前那样编译、拟合和评估这个模型，您会发现其验证MAE达到了27,703。这是迄今为止我们训练过的最佳模型，甚至击败了SARIMA模型：我们做得相当不错！
- en: Tip
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: We’ve only normalized the time series, without removing trend and seasonality,
    and yet the model still performs well. This is convenient, as it makes it possible
    to quickly search for promising models without worrying too much about preprocessing.
    However, to get the best performance, you may want to try making the time series
    more stationary; for example, using differencing.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只对时间序列进行了归一化，没有去除趋势和季节性，但模型仍然表现良好。这很方便，因为这样可以快速搜索有前途的模型，而不用太担心预处理。然而，为了获得最佳性能，您可能希望尝试使时间序列更加平稳；例如，使用差分。
- en: Forecasting Using a Deep RNN
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用深度RNN进行预测
- en: It is quite common to stack multiple layers of cells, as shown in [Figure 15-10](#deep_rnn_diagram).
    This gives you a *deep RNN*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会堆叠多层单元，如[图15-10](#deep_rnn_diagram)所示。这给你一个*深度RNN*。
- en: '![mls3 1510](assets/mls3_1510.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1510](assets/mls3_1510.png)'
- en: Figure 15-10\. A deep RNN (left) unrolled through time (right)
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-10. 深度RNN（左）在时间轴上展开（右）
- en: 'Implementing a deep RNN with Keras is straightforward: just stack recurrent
    layers. In the following example, we use three `SimpleRNN` layers (but we could
    use any other type of recurrent layer instead, such as an `LSTM` layer or a `GRU`
    layer, which we will discuss shortly). The first two are sequence-to-sequence
    layers, and the last one is a sequence-to-vector layer. Finally, the `Dense` layer
    produces the model’s forecast (you can think of it as a vector-to-vector layer).
    So this model is just like the model represented in [Figure 15-10](#deep_rnn_diagram),
    except the outputs **Ŷ**[(0)] to **Ŷ**[(*t*–1_)] are ignored, and there’s a dense
    layer on top of **Ŷ**[(*t*)], which outputs the actual forecast:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Keras实现深度RNN很简单：只需堆叠循环层。在下面的示例中，我们使用三个`SimpleRNN`层（但我们也可以使用任何其他类型的循环层，如`LSTM`层或`GRU`层，我们将很快讨论）。前两个是序列到序列层，最后一个是序列到向量层。最后，`Dense`层生成模型的预测（您可以将其视为向量到向量层）。因此，这个模型就像[图15-10](#deep_rnn_diagram)中表示的模型一样，只是忽略了**Ŷ**[(0)]到**Ŷ**[(*t*–1_)]的输出，并且在**Ŷ**[(*t*)]之上有一个密集层，输出实际预测：
- en: '[PRE22]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Warning
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Make sure to set `return_sequences=True` for all recurrent layers (except the
    last one, if you only care about the last output). If you forget to set this parameter
    for one recurrent layer, it will output a 2D array containing only the output
    of the last time step, instead of a 3D array containing outputs for all time steps.
    The next recurrent layer will complain that you are not feeding it sequences in
    the expected 3D format.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 确保对所有循环层设置`return_sequences=True`（除非您只关心最后的输出，最后一个循环层除外）。如果您忘记为一个循环层设置此参数，它将输出一个2D数组，其中仅包含最后一个时间步的输出，而不是包含所有时间步输出的3D数组。下一个循环层将抱怨您没有以预期的3D格式提供序列。
- en: If you train and evaluate this model, you will find that it reaches an MAE of
    about 31,211\. That’s better than both baselines, but it doesn’t beat our “shallower”
    RNN. It looks like this RNN is a bit too large for our task.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您训练和评估这个模型，您会发现它的MAE约为31,211。这比两个基线都要好，但它并没有击败我们的“更浅”的RNN。看起来这个RNN对我们的任务来说有点太大了。
- en: Forecasting Multivariate Time Series
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多变量时间序列预测
- en: 'A great quality of neural networks is their flexibility: in particular, they
    can deal with multivariate time series with almost no change to their architecture.
    For example, let’s try to forecast the rail time series using both the bus and
    rail data as input. In fact, let’s also throw in the day type! Since we can always
    know in advance whether tomorrow is going to be a weekday, a weekend, or a holiday,
    we can shift the day type series one day into the future, so that the model is
    given tomorrow’s day type as input. For simplicity, we’ll do this processing using
    Pandas:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的一个很大的优点是它们的灵活性：特别是，它们几乎不需要改变架构就可以处理多变量时间序列。例如，让我们尝试使用公交和铁路数据作为输入来预测铁路时间序列。事实上，让我们也加入日期类型！由于我们总是可以提前知道明天是工作日、周末还是假日，我们可以将日期类型系列向未来推移一天，这样模型就会将明天的日期类型作为输入。为简单起见，我们将使用
    Pandas 进行此处理：
- en: '[PRE23]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now `df_mulvar` is a DataFrame with five columns: the bus and rail data, plus
    three columns containing the one-hot encoding of the next day’s type (recall that
    there are three possible day types, `W`, `A`, and `U`). Next we can proceed much
    like we did earlier. First we split the data into three periods, for training,
    validation, and testing:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 `df_mulvar` 是一个包含五列的 DataFrame：公交和铁路数据，以及包含下一天类型的独热编码的三列（请记住，有三种可能的日期类型，`W`、`A`
    和 `U`）。接下来我们可以像之前一样继续。首先，我们将数据分为三个时期，用于训练、验证和测试：
- en: '[PRE24]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Then we create the datasets:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建数据集：
- en: '[PRE25]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'And finally we create the RNN:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 最后我们创建 RNN：
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Notice that the only difference from the `univar_model` RNN we built earlier
    is the input shape: at each time step, the model now receives five inputs instead
    of one. This model actually reaches a validation MAE of 22,062\. Now we’re making
    big progress!'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，与我们之前构建的 `univar_model` RNN 唯一的区别是输入形状：在每个时间步骤，模型现在接收五个输入，而不是一个。这个模型实际上达到了
    22,062 的验证 MAE。现在我们取得了很大的进展！
- en: 'In fact, it’s not too hard to make the RNN forecast both the bus and rail ridership.
    You just need to change the targets when creating the datasets, setting them to
    `mulvar_train[["bus", "rail"]][seq_length:]` for the training set, and `mulvar_valid[["bus",
    "rail"]][seq_length:]` for the validation set. You must also add an extra neuron
    in the output `Dense` layer, since it must now make two forecasts: one for tomorrow’s
    bus ridership, and the other for rail. That’s all there is to it!'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，让 RNN 预测公交和铁路乘客量并不太难。您只需要在创建数据集时更改目标，将其设置为训练集的 `mulvar_train[["bus", "rail"]][seq_length:]`，验证集的
    `mulvar_valid[["bus", "rail"]][seq_length:]`。您还必须在输出 `Dense` 层中添加一个额外的神经元，因为现在它必须进行两次预测：一次是明天的公交乘客量，另一次是铁路乘客量。就是这样！
- en: As we discussed in [Chapter 10](ch10.html#ann_chapter), using a single model
    for multiple related tasks often results in better performance than using a separate
    model for each task, since features learned for one task may be useful for the
    other tasks, and also because having to perform well across multiple tasks prevents
    the model from overfitting (it’s a form of regularization). However, it depends
    on the task, and in this particular case the multitask RNN that forecasts both
    the bus and the rail ridership doesn’t perform quite as well as dedicated models
    that forecast one or the other (using all five columns as input). Still, it reaches
    a validation MAE of 25,330 for rail and 26,369 for bus, which is pretty good.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第 10 章](ch10.html#ann_chapter)中讨论的那样，对于多个相关任务使用单个模型通常比为每个任务使用单独的模型效果更好，因为为一个任务学习的特征可能对其他任务也有用，而且因为在多个任务中表现良好可以防止模型过拟合（这是一种正则化形式）。然而，这取决于任务，在这种特殊情况下，同时预测公交和铁路乘客量的多任务
    RNN 并不像专门预测其中一个的模型表现得那么好（使用所有五列作为输入）。尽管如此，它对铁路的验证 MAE 达到了 25,330，对公交达到了 26,369，这还是相当不错的。
- en: Forecasting Several Time Steps Ahead
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提前预测多个时间步
- en: So far we have only predicted the value at the next time step, but we could
    just as easily have predicted the value several steps ahead by changing the targets
    appropriately (e.g., to predict the ridership 2 weeks from now, we could just
    change the targets to be the value 14 days ahead instead of 1 day ahead). But
    what if we want to predict the next 14 values?
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只预测了下一个时间步的值，但我们也可以通过适当更改目标来预测几个步骤之后的值（例如，要预测两周后的乘客量，我们只需将目标更改为比 1 天后提前
    14 天的值）。但是如果我们想预测接下来的 14 个值呢？
- en: 'The first option is to take the `univar_model` RNN we trained earlier for the
    rail time series, make it predict the next value, and add that value to the inputs,
    acting as if the predicted value had actually occurred; we would then use the
    model again to predict the following value, and so on, as in the following code:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种选择是取我们之前为铁路时间序列训练的 `univar_model` RNN，让它预测下一个值，并将该值添加到输入中，就好像预测的值实际上已经发生了；然后我们再次使用模型来预测下一个值，依此类推，如下面的代码所示：
- en: '[PRE27]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In this code, we take the rail ridership of the first 56 days of the validation
    period, and we convert the data to a NumPy array of shape [1, 56, 1] (recall that
    recurrent layers expect 3D inputs). Then we repeatedly use the model to forecast
    the next value, and we append each forecast to the input series, along the time
    axis (`axis=1`). The resulting forecasts are plotted in [Figure 15-11](#forecast_ahead_plot).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们取验证期间前 56 天的铁路乘客量，并将数据转换为形状为 [1, 56, 1] 的 NumPy 数组（请记住，循环层期望 3D 输入）。然后我们重复使用模型来预测下一个值，并将每个预测附加到输入系列中，沿着时间轴（`axis=1`）。生成的预测在[图
    15-11](#forecast_ahead_plot)中绘制。
- en: Warning
  id: totrans-186
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: 'If the model makes an error at one time step, then the forecasts for the following
    time steps are impacted as well: the errors tend to accumulate. So, it’s preferable
    to use this technique only for a small number of steps.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型在一个时间步骤上出现错误，那么接下来的时间步骤的预测也会受到影响：错误往往会累积。因此，最好只在少数步骤中使用这种技术。
- en: '![mls3 1511](assets/mls3_1511.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1511](assets/mls3_1511.png)'
- en: Figure 15-11\. Forecasting 14 steps ahead, 1 step at a time
  id: totrans-189
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 15-11。提前 14 步预测，一次预测一步
- en: The second option is to train an RNN to predict the next 14 values in one shot.
    We can still use a sequence-to-vector model, but it will output 14 values instead
    of 1\. However, we first need to change the targets to be vectors containing the
    next 14 values. To do this, we can use `timeseries_dataset_from_array()` again,
    but this time asking it to create datasets without targets (`targets=None`) and
    with longer sequences, of length `seq_length` + 14\. Then we can use the datasets’
    `map()` method to apply a custom function to each batch of sequences, splitting
    them into inputs and targets. In this example, we use the multivariate time series
    as input (using all five columns), and we forecast the rail ridership for the
    next 14 days:^([6](ch15.html#idm45720179430416))
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个选项是训练一个RNN一次性预测接下来的14个值。我们仍然可以使用一个序列到向量模型，但它将输出14个值而不是1。然而，我们首先需要改变目标，使其成为包含接下来14个值的向量。为此，我们可以再次使用`timeseries_dataset_from_array()`，但这次要求它创建没有目标（`targets=None`）的数据集，并且具有更长的序列，长度为`seq_length`
    + 14。然后我们可以使用数据集的`map()`方法对每个序列批次应用自定义函数，将其分成输入和目标。在这个例子中，我们使用多变量时间序列作为输入（使用所有五列），并预测未来14天的铁路乘客量。
- en: '[PRE28]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now we just need the output layer to have 14 units instead of 1:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需要将输出层的单元数从1增加到14：
- en: '[PRE29]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After training this model, you can predict the next 14 values at once like
    this:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完这个模型后，你可以像这样一次性预测接下来的14个值：
- en: '[PRE30]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This approach works quite well. Its forecasts for the next day are obviously
    better than its forecasts for 14 days into the future, but it doesn’t accumulate
    errors like the previous approach did. However, we can still do better, using
    a sequence-to-sequence (or *seq2seq*) model.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法效果相当不错。它对于第二天的预测显然比对未来14天的预测要好，但它不会像之前的方法那样累积误差。然而，我们仍然可以做得更好，使用一个序列到序列（或*seq2seq*）模型。
- en: Forecasting Using a Sequence-to-Sequence Model
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用序列到序列模型进行预测
- en: Instead of training the model to forecast the next 14 values only at the very
    last time step, we can train it to forecast the next 14 values at each and every
    time step. In other words, we can turn this sequence-to-vector RNN into a sequence-to-sequence
    RNN. The advantage of this technique is that the loss will contain a term for
    the output of the RNN at each and every time step, not just for the output at
    the last time step.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 不是只在最后一个时间步训练模型来预测接下来的14个值，而是在每一个时间步都训练它来预测接下来的14个值。换句话说，我们可以将这个序列到向量的RNN转变为一个序列到序列的RNN。这种技术的优势在于损失函数将包含RNN在每一个时间步的输出，而不仅仅是最后一个时间步的输出。
- en: This means there will be many more error gradients flowing through the model,
    and they won’t have to flow through time as much since they will come from the
    output of each time step, not just the last one. This will both stabilize and
    speed up training.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着会有更多的误差梯度通过模型流动，它们不需要像以前那样通过时间流动，因为它们将来自每一个时间步的输出，而不仅仅是最后一个时间步。这将使训练更加稳定和快速。
- en: To be clear, at time step 0 the model will output a vector containing the forecasts
    for time steps 1 to 14, then at time step 1 the model will forecast time steps
    2 to 15, and so on. In other words, the targets are sequences of consecutive windows,
    shifted by one time step at each time step. The target is not a vector anymore,
    but a sequence of the same length as the inputs, containing a 14-dimensional vector
    at each step.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 明确一点，在时间步0，模型将输出一个包含时间步1到14的预测的向量，然后在时间步1，模型将预测时间步2到15，依此类推。换句话说，目标是连续窗口的序列，每个时间步向后移动一个时间步。目标不再是一个向量，而是一个与输入相同长度的序列，每一步包含一个14维向量。
- en: 'Preparing the datasets is not trivial, since each instance has a window as
    input and a sequence of windows as output. One way to do this is to use the `to_windows()`
    utility function we created earlier, twice in a row, to get windows of consecutive
    windows. For example, let’s turn the series of numbers 0 to 6 into a dataset containing
    sequences of 4 consecutive windows, each of length 3:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 准备数据集并不是简单的，因为每个实例的输入是一个窗口，输出是窗口序列。一种方法是连续两次使用我们之前创建的`to_windows()`实用函数，以获得连续窗口的窗口。例如，让我们将数字0到6的系列转换为包含4个连续窗口的数据集，每个窗口长度为3：
- en: '[PRE31]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now we can use the `map()` method to split these windows of windows into inputs
    and targets:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用`map()`方法将这些窗口的窗口分割为输入和目标：
- en: '[PRE32]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Now the dataset contains sequences of length 4 as inputs, and the targets are
    sequences containing the next two steps, for each time step. For example, the
    first input sequence is [0, 1, 2, 3], and its corresponding targets are [[1, 2],
    [2, 3], [3, 4], [4, 5]], which are the next two values for each time step. If
    you’re like me, you will probably need a few minutes to wrap your head around
    this. Take your time!
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据集包含长度为4的输入序列，目标是包含下两个步骤的序列，每个时间步。例如，第一个输入序列是[0, 1, 2, 3]，对应的目标是[[1, 2],
    [2, 3], [3, 4], [4, 5]]，这是每个时间步的下两个值。如果你和我一样，可能需要几分钟来理解这个概念。慢慢来！
- en: Note
  id: totrans-206
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'It may be surprising that the targets contain values that appear in the inputs.
    Isn’t that cheating? Fortunately, not at all: at each time step, an RNN only knows
    about past time steps; it cannot look ahead. It is said to be a *causal* model.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 也许令人惊讶的是，目标值包含在输入中出现的值。这是不是作弊？幸运的是，完全不是：在每一个时间步，RNN只知道过去的时间步；它无法向前看。它被称为*因果*模型。
- en: 'Let’s create another little utility function to prepare the datasets for our
    sequence-to-sequence model. It will also take care of shuffling (optional) and
    batching:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建另一个小型实用函数来为我们的序列到序列模型准备数据集。它还会负责洗牌（可选）和分批处理：
- en: '[PRE33]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now we can use this function to create the datasets:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用这个函数来创建数据集：
- en: '[PRE34]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'And lastly, we can build the sequence-to-sequence model:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以构建序列到序列模型：
- en: '[PRE35]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'It is almost identical to our previous model: the only difference is that we
    set `return_sequences=True` in the `SimpleRNN` layer. This way, it will output
    a sequence of vectors (each of size 32), instead of outputting a single vector
    at the last time step. The `Dense` layer is smart enough to handle sequences as
    input: it will be applied at each time step, taking a 32-dimensional vector as
    input and outputting a 14-dimensional vector. In fact, another way to get the
    exact same result is to use a `Conv1D` layer with a kernel size of 1: `Conv1D(14,
    kernel_size=1)`.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎与我们之前的模型完全相同：唯一的区别是在`SimpleRNN`层中设置了`return_sequences=True`。这样，它将输出一个向量序列（每个大小为32），而不是在最后一个时间步输出单个向量。`Dense`层足够聪明，可以处理序列作为输入：它将在每个时间步应用，以32维向量作为输入，并输出14维向量。实际上，获得完全相同结果的另一种方法是使用具有核大小为1的`Conv1D`层：`Conv1D(14,
    kernel_size=1)`。
- en: Tip
  id: totrans-215
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Keras offers a `TimeDistributed` layer that lets you apply any vector-to-vector
    layer to every vector in the input sequences, at every time step. It does this
    efficiently, by reshaping the inputs so that each time step is treated as a separate
    instance, then it reshapes the layer’s outputs to recover the time dimension.
    In our case, we don’t need it since the `Dense` layer already supports sequences
    as inputs.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了一个`TimeDistributed`层，允许您将任何向量到向量层应用于输入序列中的每个向量，在每个时间步。它通过有效地重新塑造输入来实现这一点，以便将每个时间步视为单独的实例，然后重新塑造层的输出以恢复时间维度。在我们的情况下，我们不需要它，因为`Dense`层已经支持序列作为输入。
- en: 'The training code is the same as usual. During training, all the model’s outputs
    are used, but after training only the output of the very last time step matters,
    and the rest can be ignored. For example, we can forecast the rail ridership for
    the next 14 days like this:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 训练代码与往常一样。在训练期间，使用所有模型的输出，但在训练后，只有最后一个时间步的输出才重要，其余可以忽略。例如，我们可以这样预测未来14天的铁路乘客量：
- en: '[PRE36]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: If you evaluate this model’s forecasts for *t* + 1, you will find a validation
    MAE of 25,519\. For *t* + 2 it’s 26,274, and the performance continues to drop
    gradually as the model tries to forecast further into the future. At *t* + 14,
    the MAE is 34,322.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果评估此模型对*t* + 1的预测，您将发现验证MAE为25,519。对于*t* + 2，它为26,274，随着模型试图进一步预测未来，性能会逐渐下降。在*t*
    + 14时，MAE为34,322。
- en: Tip
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'You can combine both approaches to forecasting multiple steps ahead: for example,
    you can train a model that forecasts 14 days ahead, then take its output and append
    it to the inputs, then run the model again to get forecasts for the following
    14 days, and possibly repeat the process.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以结合两种方法来预测多步：例如，您可以训练一个模型，预测未来14天，然后将其输出附加到输入，然后再次运行模型，以获取接下来14天的预测，并可能重复该过程。
- en: Simple RNNs can be quite good at forecasting time series or handling other kinds
    of sequences, but they do not perform as well on long time series or sequences.
    Let’s discuss why and see what we can do about it.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的RNN在预测时间序列或处理其他类型的序列时可能表现得很好，但在长时间序列或序列上表现不佳。让我们讨论一下原因，并看看我们能做些什么。
- en: Handling Long Sequences
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理长序列
- en: 'To train an RNN on long sequences, we must run it over many time steps, making
    the unrolled RNN a very deep network. Just like any deep neural network it may
    suffer from the unstable gradients problem, discussed in [Chapter 11](ch11.html#deep_chapter):
    it may take forever to train, or training may be unstable. Moreover, when an RNN
    processes a long sequence, it will gradually forget the first inputs in the sequence.
    Let’s look at both these problems, starting with the unstable gradients problem.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 要在长序列上训练RNN，我们必须在许多时间步上运行它，使展开的RNN成为一个非常深的网络。就像任何深度神经网络一样，它可能会遇到不稳定的梯度问题，如[第11章](ch11.html#deep_chapter)中讨论的：可能需要很长时间来训练，或者训练可能不稳定。此外，当RNN处理长序列时，它将逐渐忘记序列中的第一个输入。让我们从不稳定的梯度问题开始，看看这两个问题。
- en: Fighting the Unstable Gradients Problem
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决不稳定梯度问题
- en: 'Many of the tricks we used in deep nets to alleviate the unstable gradients
    problem can also be used for RNNs: good parameter initialization, faster optimizers,
    dropout, and so on. However, nonsaturating activation functions (e.g., ReLU) may
    not help as much here. In fact, they may actually lead the RNN to be even more
    unstable during training. Why? Well, suppose gradient descent updates the weights
    in a way that increases the outputs slightly at the first time step. Because the
    same weights are used at every time step, the outputs at the second time step
    may also be slightly increased, and those at the third, and so on until the outputs
    explode—and a nonsaturating activation function does not prevent that.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 许多我们在深度网络中用来缓解不稳定梯度问题的技巧也可以用于RNN：良好的参数初始化，更快的优化器，辍学等。然而，非饱和激活函数（例如ReLU）在这里可能不会有太大帮助。实际上，它们可能会导致RNN在训练过程中更加不稳定。为什么？嗯，假设梯度下降以一种增加第一个时间步输出的方式更新权重。由于相同的权重在每个时间步使用，第二个时间步的输出也可能略有增加，第三个时间步也是如此，直到输出爆炸——而非饱和激活函数无法阻止这种情况。
- en: You can reduce this risk by using a smaller learning rate, or you can use a
    saturating activation function like the hyperbolic tangent (this explains why
    it’s the default).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用较小的学习率来减少这种风险，或者可以使用饱和激活函数，如双曲正切（这解释了为什么它是默认值）。
- en: In much the same way, the gradients themselves can explode. If you notice that
    training is unstable, you may want to monitor the size of the gradients (e.g.,
    using TensorBoard) and perhaps use gradient clipping.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，梯度本身也可能爆炸。如果注意到训练不稳定，可以监控梯度的大小（例如，使用TensorBoard），并可能使用梯度裁剪。
- en: Moreover, batch normalization cannot be used as efficiently with RNNs as with
    deep feedforward nets. In fact, you cannot use it between time steps, only between
    recurrent layers.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，批量归一化不能像深度前馈网络那样有效地与RNN一起使用。实际上，您不能在时间步之间使用它，只能在循环层之间使用。
- en: To be more precise, it is technically possible to add a BN layer to a memory
    cell (as you will see shortly) so that it will be applied at each time step (both
    on the inputs for that time step and on the hidden state from the previous step).
    However, the same BN layer will be used at each time step, with the same parameters,
    regardless of the actual scale and offset of the inputs and hidden state. In practice,
    this does not yield good results, as was demonstrated by César Laurent et al.
    in a [2015 paper](https://homl.info/rnnbn):⁠^([7](ch15.html#idm45720178649360))
    the authors found that BN was slightly beneficial only when it was applied to
    the layer’s inputs, not to the hidden states. In other words, it was slightly
    better than nothing when applied between recurrent layers (i.e., vertically in
    [Figure 15-10](#deep_rnn_diagram)), but not within recurrent layers (i.e., horizontally).
    In Keras, you can apply BN between layers simply by adding a `BatchNormalization`
    layer before each recurrent layer, but it will slow down training, and it may
    not help much.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 更准确地说，从技术上讲，可以在内存单元中添加一个BN层（您很快就会看到），以便在每个时间步上应用它（既在该时间步的输入上，也在上一个步骤的隐藏状态上）。然而，相同的BN层将在每个时间步上使用相同的参数，而不考虑输入和隐藏状态的实际比例和偏移。实际上，这并不会产生良好的结果，如César
    Laurent等人在[2015年的一篇论文](https://homl.info/rnnbn)中所证明的：作者发现，只有当BN应用于层的输入时才略有益处，而不是应用于隐藏状态。换句话说，当应用于循环层之间（即在[图15-10](#deep_rnn_diagram)中垂直地）时，它略好于什么都不做，但不适用于循环层内部（即水平地）。在Keras中，您可以通过在每个循环层之前添加一个
    `BatchNormalization` 层来简单地在层之间应用BN，但这会减慢训练速度，并且可能帮助不大。
- en: 'Another form of normalization often works better with RNNs: *layer normalization*.
    This idea was introduced by Jimmy Lei Ba et al. in a [2016 paper](https://homl.info/layernorm):⁠^([8](ch15.html#idm45720178641536))
    it is very similar to batch normalization, but instead of normalizing across the
    batch dimension, layer normalization normalizes across the features dimension.
    One advantage is that it can compute the required statistics on the fly, at each
    time step, independently for each instance. This also means that it behaves the
    same way during training and testing (as opposed to BN), and it does not need
    to use exponential moving averages to estimate the feature statistics across all
    instances in the training set, like BN does. Like BN, layer normalization learns
    a scale and an offset parameter for each input. In an RNN, it is typically used
    right after the linear combination of the inputs and the hidden states.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种规范化方法在RNN中通常效果更好：*层归一化*。这个想法是由Jimmy Lei Ba等人在[2016年的一篇论文](https://homl.info/layernorm)中提出的：它与批归一化非常相似，但不同的是，层归一化是在特征维度上进行归一化，而不是在批次维度上。一个优点是它可以在每个时间步上独立地为每个实例计算所需的统计数据。这也意味着它在训练和测试期间的行为是相同的（与BN相反），它不需要使用指数移动平均来估计训练集中所有实例的特征统计数据，就像BN那样。与BN类似，层归一化为每个输入学习一个比例和偏移参数。在RNN中，它通常在输入和隐藏状态的线性组合之后立即使用。
- en: 'Let’s use Keras to implement layer normalization within a simple memory cell.
    To do this, we need to define a custom memory cell, which is just like a regular
    layer, except its `call()` method takes two arguments: the `inputs` at the current
    time step and the hidden `states` from the previous time step.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用Keras在简单内存单元中实现层归一化。为此，我们需要定义一个自定义内存单元，它就像一个常规层一样，只是它的 `call()` 方法接受两个参数：当前时间步的
    `inputs` 和上一个时间步的隐藏 `states`。
- en: 'Note that the `states` argument is a list containing one or more tensors. In
    the case of a simple RNN cell it contains a single tensor equal to the outputs
    of the previous time step, but other cells may have multiple state tensors (e.g.,
    an `LSTMCell` has a long-term state and a short-term state, as you will see shortly).
    A cell must also have a `state_size` attribute and an `output_size` attribute.
    In a simple RNN, both are simply equal to the number of units. The following code
    implements a custom memory cell that will behave like a `SimpleRNNCell`, except
    it will also apply layer normalization at each time step:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`states` 参数是一个包含一个或多个张量的列表。在简单的RNN单元中，它包含一个张量，等于上一个时间步的输出，但其他单元可能有多个状态张量（例如，`LSTMCell`
    有一个长期状态和一个短期状态，您很快就会看到）。一个单元还必须有一个 `state_size` 属性和一个 `output_size` 属性。在简单的RNN中，两者都简单地等于单元的数量。以下代码实现了一个自定义内存单元，它将表现得像一个
    `SimpleRNNCell`，但它还会在每个时间步应用层归一化：
- en: '[PRE37]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let’s walk through this code:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下这段代码：
- en: Our `LNSimpleRNNCell` class inherits from the `tf.keras.layers.Layer` class,
    just like any custom layer.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的 `LNSimpleRNNCell` 类继承自 `tf.keras.layers.Layer` 类，就像任何自定义层一样。
- en: The constructor takes the number of units and the desired activation function
    and sets the `state_size` and `output_size` attributes, then creates a `SimpleRNNCell`
    with no activation function (because we want to perform layer normalization after
    the linear operation but before the activation function).⁠^([9](ch15.html#idm45720178559024))
    Then the constructor creates the `LayerNormalization` layer, and finally it fetches
    the desired activation function.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构造函数接受单位数和所需的激活函数，并设置 `state_size` 和 `output_size` 属性，然后创建一个没有激活函数的 `SimpleRNNCell`（因为我们希望在线性操作之后但在激活函数之前执行层归一化）。然后构造函数创建
    `LayerNormalization` 层，最后获取所需的激活函数。
- en: 'The `call()` method starts by applying the `simpleRNNCell`, which computes
    a linear combination of the current inputs and the previous hidden states, and
    it returns the result twice (indeed, in a `SimpleRNNCell`, the outputs are just
    equal to the hidden states: in other words, `new_states[0]` is equal to `outputs`,
    so we can safely ignore `new_states` in the rest of the `call()` method). Next,
    the `call()` method applies layer normalization, followed by the activation function.
    Finally, it returns the outputs twice: once as the outputs, and once as the new
    hidden states. To use this custom cell, all we need to do is create a `tf.keras.layers.RNN`
    layer, passing it a cell instance:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`call()`方法首先应用`simpleRNNCell`，它计算当前输入和先前隐藏状态的线性组合，并返回结果两次（实际上，在`SimpleRNNCell`中，输出就等于隐藏状态：换句话说，`new_states[0]`等于`outputs`，因此我们可以在`call()`方法的其余部分安全地忽略`new_states`）。接下来，`call()`方法应用层归一化，然后是激活函数。最后，它将输出返回两次：一次作为输出，一次作为新的隐藏状态。要使用此自定义细胞，我们只需要创建一个`tf.keras.layers.RNN`层，将其传递给一个细胞实例：'
- en: '[PRE38]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Similarly, you could create a custom cell to apply dropout between each time
    step. But there’s a simpler way: most recurrent layers and cells provided by Keras
    have `dropout` and `recurrent_dropout` hyperparameters: the former defines the
    dropout rate to apply to the inputs, and the latter defines the dropout rate for
    the hidden states, between time steps. So, there’s no need to create a custom
    cell to apply dropout at each time step in an RNN.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，您可以创建一个自定义细胞，在每个时间步之间应用dropout。但有一个更简单的方法：Keras提供的大多数循环层和细胞都有`dropout`和`recurrent_dropout`超参数：前者定义要应用于输入的dropout率，后者定义隐藏状态之间的dropout率，即时间步之间。因此，在RNN中不需要创建自定义细胞来在每个时间步应用dropout。
- en: With these techniques, you can alleviate the unstable gradients problem and
    train an RNN much more efficiently. Now let’s look at how to deal with the short-term
    memory problem.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些技术，您可以缓解不稳定梯度问题，并更有效地训练RNN。现在让我们看看如何解决短期记忆问题。
- en: Tip
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'When forecasting time series, it is often useful to have some error bars along
    with your predictions. For this, one approach is to use MC dropout, introduced
    in [Chapter 11](ch11.html#deep_chapter): use `recurrent_dropout` during training,
    then keep dropout active at inference time by calling the model using `model(X,
    training=True)`. Repeat this several times to get multiple slightly different
    forecasts, then compute the mean and standard deviation of these predictions for
    each time step.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测时间序列时，通常有必要在预测中包含一些误差范围。为此，一种方法是使用MC dropout，介绍在[第11章](ch11.html#deep_chapter)中：在训练期间使用`recurrent_dropout`，然后在推断时通过使用`model(X,
    training=True)`来保持dropout处于活动状态。多次重复此操作以获得多个略有不同的预测，然后计算每个时间步的这些预测的均值和标准差。
- en: Tackling the Short-Term Memory Problem
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决短期记忆问题
- en: 'Due to the transformations that the data goes through when traversing an RNN,
    some information is lost at each time step. After a while, the RNN’s state contains
    virtually no trace of the first inputs. This can be a showstopper. Imagine Dory
    the fish⁠^([10](ch15.html#idm45720178359088)) trying to translate a long sentence;
    by the time she’s finished reading it, she has no clue how it started. To tackle
    this problem, various types of cells with long-term memory have been introduced.
    They have proven so successful that the basic cells are not used much anymore.
    Let’s first look at the most popular of these long-term memory cells: the LSTM
    cell.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据在经过RNN时经历的转换，每个时间步都会丢失一些信息。过一段时间后，RNN的状态几乎不包含最初输入的任何痕迹。这可能是一个停滞不前的问题。想象一下多莉鱼试图翻译一句长句子；当她读完时，她已经不记得它是如何开始的。为了解决这个问题，引入了各种具有长期记忆的细胞类型。它们已经被证明非常成功，以至于基本细胞不再被广泛使用。让我们首先看看这些长期记忆细胞中最受欢迎的：LSTM细胞。
- en: LSTM cells
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSTM细胞
- en: 'The *long short-term memory* (LSTM) cell was [proposed in 1997](https://homl.info/93)⁠^([11](ch15.html#idm45720178354352))
    by Sepp Hochreiter and Jürgen Schmidhuber and gradually improved over the years
    by several researchers, such as [Alex Graves](https://homl.info/graves), [Haşim
    Sak](https://homl.info/94),⁠^([12](ch15.html#idm45720178351696)) and [Wojciech
    Zaremba](https://homl.info/95).⁠^([13](ch15.html#idm45720178350096)) If you consider
    the LSTM cell as a black box, it can be used very much like a basic cell, except
    it will perform much better; training will converge faster, and it will detect
    longer-term patterns in the data. In Keras, you can simply use the `LSTM` layer
    instead of the `SimpleRNN` layer:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '*长短期记忆*（LSTM）细胞是由Sepp Hochreiter和Jürgen Schmidhuber于1997年提出的，并在多年来逐渐得到了几位研究人员的改进，如Alex
    Graves，Haşim Sak和Wojciech Zaremba。如果将LSTM细胞视为黑匣子，它可以被用作基本细胞，只是它的性能会更好；训练会更快收敛，并且它会检测数据中更长期的模式。在Keras中，您可以简单地使用`LSTM`层而不是`SimpleRNN`层：'
- en: '[PRE39]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Alternatively, you could use the general-purpose `tf.keras.layers.RNN` layer,
    giving it an `LSTMCell` as an argument. However, the `LSTM` layer uses an optimized
    implementation when running on a GPU (see [Chapter 19](ch19.html#deployment_chapter)),
    so in general it is preferable to use it (the `RNN` layer is mostly useful when
    you define custom cells, as we did earlier).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用通用的`tf.keras.layers.RNN`层，将`LSTMCell`作为参数传递给它。但是，当在GPU上运行时，`LSTM`层使用了优化的实现（请参阅[第19章](ch19.html#deployment_chapter)），因此通常最好使用它（`RNN`层在定义自定义细胞时非常有用，就像我们之前做的那样）。
- en: 'So how does an LSTM cell work? Its architecture is shown in [Figure 15-12](#lstm_cell_diagram).
    If you don’t look at what’s inside the box, the LSTM cell looks exactly like a
    regular cell, except that its state is split into two vectors: **h**[(*t*)] and
    **c**[(*t*)] (“c” stands for “cell”). You can think of **h**[(*t*)] as the short-term
    state and **c**[(*t*)] as the long-term state.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 那么LSTM细胞是如何工作的呢？其架构显示在[图15-12](#lstm_cell_diagram)中。如果不看盒子里面的内容，LSTM细胞看起来与常规细胞完全相同，只是其状态分为两个向量：**h**[(*t*)]和**c**[(*t*)]（“c”代表“cell”）。您可以将**h**[(*t*)]视为短期状态，将**c**[(*t*)]视为长期状态。
- en: '![mls3 1512](assets/mls3_1512.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1512](assets/mls3_1512.png)'
- en: Figure 15-12\. An LSTM cell
  id: totrans-252
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-12\. LSTM单元
- en: Now let’s open the box! The key idea is that the network can learn what to store
    in the long-term state, what to throw away, and what to read from it. As the long-term
    state **c**[(*t*–1)] traverses the network from left to right, you can see that
    it first goes through a *forget gate*, dropping some memories, and then it adds
    some new memories via the addition operation (which adds the memories that were
    selected by an *input gate*). The result **c**[(*t*)] is sent straight out, without
    any further transformation. So, at each time step, some memories are dropped and
    some memories are added. Moreover, after the addition operation, the long-term
    state is copied and passed through the tanh function, and then the result is filtered
    by the *output gate*. This produces the short-term state **h**[(*t*)] (which is
    equal to the cell’s output for this time step, **y**[(*t*)]). Now let’s look at
    where new memories come from and how the gates work.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们打开盒子！关键思想是网络可以学习将什么存储在长期状态中，什么丢弃，以及从中读取什么。当长期状态**c**[(*t*–1)]从左到右穿过网络时，您可以看到它首先经过一个*遗忘门*，丢弃一些记忆，然后通过加法操作添加一些新的记忆（通过*输入门*选择的记忆）。结果**c**[(*t*)]直接发送出去，没有进一步的转换。因此，在每个时间步骤，一些记忆被丢弃，一些记忆被添加。此外，在加法操作之后，长期状态被复制并通过tanh函数传递，然后结果由*输出门*过滤。这产生了短期状态**h**[(*t*)]（这等于此时间步骤的单元输出**y**[(*t*)）。现在让我们看看新记忆来自哪里以及门是如何工作的。
- en: 'First, the current input vector **x**[(*t*)] and the previous short-term state
    **h**[(*t*–1)] are fed to four different fully connected layers. They all serve
    a different purpose:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，当前输入向量**x**[(*t*)]和先前的短期状态**h**[(*t*–1)]被馈送到四个不同的全连接层。它们各自有不同的作用：
- en: The main layer is the one that outputs **g**[(*t*)]. It has the usual role of
    analyzing the current inputs **x**[(*t*)] and the previous (short-term) state
    **h**[(*t*–1)]. In a basic cell, there is nothing other than this layer, and its
    output goes straight out to **y**[(*t*)] and **h**[(*t*)]. But in an LSTM cell,
    this layer’s output does not go straight out; instead its most important parts
    are stored in the long-term state (and the rest is dropped).
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要层是输出**g**[(*t*)]的层。它通常的作用是分析当前输入**x**[(*t*)]和先前（短期）状态**h**[(*t*–1)]。在基本单元中，除了这一层外没有其他内容，其输出直接发送到**y**[(*t*)]和**h**[(*t*)]。但在LSTM单元中，这一层的输出不会直接输出；相反，其最重要的部分存储在长期状态中（其余部分被丢弃）。
- en: 'The three other layers are *gate controllers*. Since they use the logistic
    activation function, the outputs range from 0 to 1\. As you can see, the gate
    controllers’ outputs are fed to element-wise multiplication operations: if they
    output 0s they close the gate, and if they output 1s they open it. Specifically:'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他三个层是*门控制器*。由于它们使用逻辑激活函数，输出范围从0到1。正如您所看到的，门控制器的输出被馈送到逐元素乘法操作：如果它们输出0，则关闭门，如果它们输出1，则打开门。具体来说：
- en: The *forget gate* (controlled by **f**[(*t*)]) controls which parts of the long-term
    state should be erased.
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*遗忘门*（由**f**[(*t*)]控制）控制着应该擦除长期状态的哪些部分。'
- en: The *input gate* (controlled by **i**[(*t*)]) controls which parts of **g**[(*t*)]
    should be added to the long-term state.
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入门*（由**i**[(*t*)]控制）控制着应该将**g**[(*t*)]的哪些部分添加到长期状态中。'
- en: Finally, the *output gate* (controlled by **o**[(*t*)]) controls which parts
    of the long-term state should be read and output at this time step, both to **h**[(*t*)]
    and to **y**[(*t*)].
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，*输出门*（由**o**[(*t*)]控制）控制着长期状态的哪些部分应该在此时间步骤被读取并输出，既输出到**h**[(*t*)]，也输出到**y**[(*t*)]。
- en: In short, an LSTM cell can learn to recognize an important input (that’s the
    role of the input gate), store it in the long-term state, preserve it for as long
    as it is needed (that’s the role of the forget gate), and extract it whenever
    it is needed. This explains why these cells have been amazingly successful at
    capturing long-term patterns in time series, long texts, audio recordings, and
    more.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，LSTM单元可以学习识别重要的输入（这是输入门的作用），将其存储在长期状态中，保留它直到需要（这是遗忘门的作用），并在需要时提取它。这解释了为什么这些单元在捕捉时间序列、长文本、音频记录等长期模式方面取得了惊人的成功。
- en: '[Equation 15-4](#lstm_equation) summarizes how to compute the cell’s long-term
    state, its short-term state, and its output at each time step for a single instance
    (the equations for a whole mini-batch are very similar).'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程15-4](#lstm_equation)总结了如何计算单元的长期状态、短期状态以及每个时间步骤的输出，针对单个实例（整个小批量的方程非常相似）。'
- en: Equation 15-4\. LSTM computations
  id: totrans-262
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程15-4\. LSTM计算
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi
    mathvariant="bold">i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>σ</mi> <mo>(</mo> <msup><mrow><msub><mi
    mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>i</mi></mrow></msub></mrow> <mo>⊺</mo></msup>
    <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>i</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">f</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>σ</mi> <mo>(</mo> <msup><mrow><msub><mi
    mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>f</mi></mrow></msub></mrow> <mo>⊺</mo></msup>
    <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>f</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mi>f</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">o</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>σ</mi> <mo>(</mo> <msup><mrow><msub><mi
    mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>o</mi></mrow></msub></mrow> <mo>⊺</mo></msup>
    <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>o</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mi>o</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mo form="prefix">tanh</mo> <mo>(</mo>
    <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>g</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>g</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mi>g</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi mathvariant="bold">f</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>⊗</mo> <msub><mi mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>⊗</mo> <msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">y</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>=</mo> <msub><mi mathvariant="bold">o</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>⊗</mo> <mo form="prefix">tanh</mo> <mrow><mo>(</mo> <msub><mi mathvariant="bold">c</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi
    mathvariant="bold">i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>σ</mi> <mo>(</mo> <msup><mrow><msub><mi
    mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>i</mi></mrow></msub></mrow> <mo>⊺</mo></msup>
    <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>i</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mo><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">f</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>σ</mi> <mo>(</mo> <msup><mrow><msub><mi
    mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>f</mi></mrow></msub></mrow> <mo>⊺</mo></msup>
    <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>f</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mo><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mi>f</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">o</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>σ</mi> <mo>(</mo> <msup><mrow><msub><mi
    mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>o</mi></mrow></msub></mrow> <mo>⊺</mo></msup>
    <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>o</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mo><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mi>o</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mo form="prefix">tanh</mo> <mo>(</mo>
    <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>g</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>g</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mo><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi> <mi>g</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi mathvariant="bold">f</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>⊗</mo> <msub><mi mathvariant="bold">c</mi> <mrow><mo>(</mo><mi>t</mo><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">i</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>⊗</mo> <msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">y</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>=</mo> <msub><mi mathvariant="bold">o</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>⊗</mo> <mo form="prefix">tanh</mo> <mrow><mo>(</mo> <msub><mi mathvariant="bold">c</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub> <mo>)</mo></mrow></mrow></mtd></mtr></mtable></math>
- en: 'In this equation:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中：
- en: '**W**[*xi*], **W**[*xf*], **W**[*xo*], and **W**[*xg*] are the weight matrices
    of each of the four layers for their connection to the input vector **x**[(*t*)].'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W**[*xi*]、**W**[*xf*]、**W**[*xo*]和**W**[*xg*]是每个四层的权重矩阵，用于它们与输入向量**x**[(*t*)]的连接。'
- en: '**W**[*hi*], **W**[*hf*], **W**[*ho*], and **W**[*hg*] are the weight matrices
    of each of the four layers for their connection to the previous short-term state
    **h**[(*t*–1)].'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**W**[*hi*]、**W**[*hf*]、**W**[*ho*]和**W**[*hg*]是每个四层的权重矩阵，用于它们与先前的短期状态**h**[(*t*–1)]的连接。'
- en: '**b**[*i*], **b**[*f*], **b**[*o*], and **b**[*g*] are the bias terms for each
    of the four layers. Note that TensorFlow initializes **b**[*f*] to a vector full
    of 1s instead of 0s. This prevents forgetting everything at the beginning of training.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**b**[*i*]、**b**[*f*]、**b**[*o*]和**b**[*g*]是每个四层的偏置项。请注意，TensorFlow将**b**[*f*]初始化为一个全为1的向量，而不是0。这可以防止在训练开始时忘记所有内容。'
- en: There are several variants of the LSTM cell. One particularly popular variant
    is the GRU cell, which we will look at now.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM单元有几个变体。一个特别流行的变体是GRU单元，我们现在将看一下。
- en: GRU cells
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GRU单元
- en: The *gated recurrent unit* (GRU) cell (see [Figure 15-13](#gru_cell_diagram))
    was proposed by Kyunghyun Cho et al. in a [2014 paper](https://homl.info/97)⁠^([14](ch15.html#idm45720178094032))
    that also introduced the encoder–decoder network we discussed earlier.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '*门控循环单元* (GRU)单元（见[图15-13](#gru_cell_diagram)）由Kyunghyun Cho等人在[2014年的一篇论文](https://homl.info/97)中提出，该论文还介绍了我们之前讨论过的编码器-解码器网络。'
- en: '![mls3 1513](assets/mls3_1513.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1513](assets/mls3_1513.png)'
- en: Figure 15-13\. GRU cell
  id: totrans-272
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-13. GRU单元
- en: 'The GRU cell is a simplified version of the LSTM cell, and it seems to perform
    just as well⁠^([15](ch15.html#idm45720178088432)) (which explains its growing
    popularity). These are the main simplifications:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: GRU单元是LSTM单元的简化版本，看起来表现同样出色（这解释了它日益增长的受欢迎程度）。这些是主要的简化：
- en: Both state vectors are merged into a single vector **h**[(*t*)].
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个状态向量合并成一个单一向量**h**[(*t*)]。
- en: A single gate controller **z**[(*t*)] controls both the forget gate and the
    input gate. If the gate controller outputs a 1, the forget gate is open (= 1)
    and the input gate is closed (1 – 1 = 0). If it outputs a 0, the opposite happens.
    In other words, whenever a memory must be stored, the location where it will be
    stored is erased first. This is actually a frequent variant to the LSTM cell in
    and of itself.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个单一的门控制器**z**[(*t*)]控制遗忘门和输入门。如果门控制器输出1，则遗忘门打开（= 1），输入门关闭（1 - 1 = 0）。如果输出0，则相反发生。换句话说，每当必须存储一个记忆时，将首先擦除将存储它的位置。这实际上是LSTM单元的一个常见变体。
- en: There is no output gate; the full state vector is output at every time step.
    However, there is a new gate controller **r**[(*t*)] that controls which part
    of the previous state will be shown to the main layer (**g**[(*t*)]).
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有输出门；完整状态向量在每个时间步输出。然而，有一个新的门控制器**r**[(*t*)]，控制哪部分先前状态将被显示给主层(**g**[(*t*)])。
- en: '[Equation 15-5](#gru_equation) summarizes how to compute the cell’s state at
    each time step for a single instance.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程15-5](#gru_equation)总结了如何计算每个时间步的单个实例的单元状态。'
- en: Equation 15-5\. GRU computations
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程15-5. GRU计算
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi
    mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>σ</mi> <mo>(</mo> <msup><mrow><msub><mi
    mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>z</mi></mrow></msub></mrow> <mo>⊺</mo></msup>
    <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>z</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi><mi>z</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">r</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>σ</mi> <mo>(</mo> <msup><mrow><msub><mi
    mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>r</mi></mrow></msub></mrow> <mo>⊺</mo></msup>
    <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>r</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi><mi>r</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mo form="prefix">tanh</mo> <mfenced
    separators="" open="(" close=")"><msup><mrow><msub><mi mathvariant="bold">W</mi>
    <mrow><mi>x</mi><mi>g</mi></mrow></msub></mrow> <mo>⊺</mo></msup> <msub><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub> <mo>+</mo> <msup><mrow><msub><mi
    mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>g</mi></mrow></msub></mrow> <mo>⊺</mo></msup>
    <mrow><mo>(</mo> <msub><mi mathvariant="bold">r</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>⊗</mo> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>)</mo></mrow> <mo>+</mo> <msub><mi mathvariant="bold">b</mi><mi>g</mi></msub></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>⊗</mo> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi mathvariant="bold">z</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub> <mo>)</mo></mrow> <mo>⊗</mo>
    <msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mrow></mtd></mtr></mtable></math>
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><msub><mi
    mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>σ</mi> <mo>(</mo> <msup><mrow><msub><mi
    mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>z</mi></mrow></msub></mrow> <mo>⊺</mo></msup>
    <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>z</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi><mi>z</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">r</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>σ</mi> <mo>(</mo> <msup><mrow><msub><mi
    mathvariant="bold">W</mi> <mrow><mi>x</mi><mi>r</mi></mrow></msub></mrow> <mo>⊺</mo></msup>
    <msub><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>+</mo> <msup><mrow><msub><mi mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>r</mi></mrow></msub></mrow>
    <mo>⊺</mo></msup> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <msub><mi mathvariant="bold">b</mi><mi>r</mi></msub> <mo>)</mo></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mo form="prefix">tanh</mo> <mfenced
    separators="" open="(" close=")"><msup><mrow><msub><mi mathvariant="bold">W</mi>
    <mrow><mi>x</mi><mi>g</mi></mrow></msub></mrow> <mo>⊺</mo></msup> <msub><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub> <mo>+</mo> <msup><mrow><msub><mi
    mathvariant="bold">W</mi> <mrow><mi>h</mi><mi>g</mi></mrow></msub></mrow> <mo>⊺</mo></msup>
    <mrow><mo>(</mo> <msub><mi mathvariant="bold">r</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>⊗</mo> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>)</mo></mrow> <mo>+</mo> <msub><mi mathvariant="bold">b</mi><mi>g</mi></msub></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub>
    <mo>⊗</mo> <msub><mi mathvariant="bold">h</mi> <mrow><mo>(</mo><mi>t</mi><mo>-</mo><mn>1</mn><mo>)</mo></mrow></msub>
    <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi mathvariant="bold">z</mi>
    <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub> <mo>)</mo></mrow> <mo>⊗</mo>
    <msub><mi mathvariant="bold">g</mi> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></msub></mrow></mtd></mtr></mtable></math>
- en: 'Keras provides a `tf.keras.layers.GRU` layer: using it is just a matter of
    replacing `SimpleRNN` or `LSTM` with `GRU`. It also provides a `tf.keras.layers.GRUCell`,
    in case you want to create a custom cell based on a GRU cell.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: LSTM and GRU cells are one of the main reasons behind the success of RNNs. Yet
    while they can tackle much longer sequences than simple RNNs, they still have
    a fairly limited short-term memory, and they have a hard time learning long-term
    patterns in sequences of 100 time steps or more, such as audio samples, long time
    series, or long sentences. One way to solve this is to shorten the input sequences;
    for example, using 1D convolutional layers.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Using 1D convolutional layers to process sequences
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [Chapter 14](ch14.html#cnn_chapter), we saw that a 2D convolutional layer
    works by sliding several fairly small kernels (or filters) across an image, producing
    multiple 2D feature maps (one per kernel). Similarly, a 1D convolutional layer
    slides several kernels across a sequence, producing a 1D feature map per kernel.
    Each kernel will learn to detect a single very short sequential pattern (no longer
    than the kernel size). If you use 10 kernels, then the layer’s output will be
    composed of 10 1D sequences (all of the same length), or equivalently you can
    view this output as a single 10D sequence. This means that you can build a neural
    network composed of a mix of recurrent layers and 1D convolutional layers (or
    even 1D pooling layers). If you use a 1D convolutional layer with a stride of
    1 and `"same"` padding, then the output sequence will have the same length as
    the input sequence. But if you use `"valid"` padding or a stride greater than
    1, then the output sequence will be shorter than the input sequence, so make sure
    you adjust the targets accordingly.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第14章](ch14.html#cnn_chapter)中，我们看到2D卷积层通过在图像上滑动几个相当小的卷积核（或滤波器），产生多个2D特征图（每个卷积核一个）。类似地，1D卷积层在序列上滑动多个卷积核，每个卷积核产生一个1D特征图。每个卷积核将学习检测单个非常短的连续模式（不超过卷积核大小）。如果使用10个卷积核，则该层的输出将由10个1D序列组成（长度相同），或者您可以将此输出视为单个10D序列。这意味着您可以构建一个由循环层和1D卷积层（甚至1D池化层）组成的神经网络。如果使用步幅为1和“same”填充的1D卷积层，则输出序列的长度将与输入序列的长度相同。但是，如果使用“valid”填充或大于1的步幅，则输出序列将短于输入序列，因此请确保相应调整目标。
- en: 'For example, the following model is the same as earlier, except it starts with
    a 1D convolutional layer that downsamples the input sequence by a factor of 2,
    using a stride of 2\. The kernel size is larger than the stride, so all inputs
    will be used to compute the layer’s output, and therefore the model can learn
    to preserve the useful information, dropping only the unimportant details. By
    shortening the sequences the convolutional layer may help the `GRU` layers detect
    longer patterns, so we can afford to double the input sequence length to 112 days.
    Note that we must also crop off the first three time steps in the targets: indeed,
    the kernel’s size is 4, so the first output of the convolutional layer will be
    based on the input time steps 0 to 3, and the first forecasts will be for time
    steps 4 to 17 (instead of time steps 1 to 14). Moreover, we must downsample the
    targets by a factor of 2, because of the stride:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下模型与之前的模型相同，只是它以一个1D卷积层开始，通过步幅为2对输入序列进行下采样。卷积核的大小大于步幅，因此所有输入都将用于计算该层的输出，因此模型可以学习保留有用信息，仅丢弃不重要的细节。通过缩短序列，卷积层可能有助于`GRU`层检测更长的模式，因此我们可以将输入序列长度加倍至112天。请注意，我们还必须裁剪目标中的前三个时间步：实际上，卷积核的大小为4，因此卷积层的第一个输出将基于输入时间步0到3，第一个预测将是时间步4到17（而不是时间步1到14）。此外，由于步幅，我们必须将目标下采样一半：
- en: '[PRE40]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: If you train and evaluate this model, you will find that it outperforms the
    previous model (by a small margin). In fact, it is actually possible to use only
    1D convolutional layers and drop the recurrent layers entirely!
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您训练和评估此模型，您会发现它的性能优于之前的模型（略有优势）。事实上，实际上可以仅使用1D卷积层并完全放弃循环层！
- en: WaveNet
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: WaveNet
- en: 'In a [2016 paper](https://homl.info/wavenet),⁠^([16](ch15.html#idm45720177810640))
    Aaron van den Oord and other DeepMind researchers introduced a novel architecture
    called *WaveNet*. They stacked 1D convolutional layers, doubling the dilation
    rate (how spread apart each neuron’s inputs are) at every layer: the first convolutional
    layer gets a glimpse of just two time steps at a time, while the next one sees
    four time steps (its receptive field is four time steps long), the next one sees
    eight time steps, and so on (see [Figure 15-14](#wavenet_diagram)). This way,
    the lower layers learn short-term patterns, while the higher layers learn long-term
    patterns. Thanks to the doubling dilation rate, the network can process extremely
    large sequences very efficiently.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在[2016年的一篇论文](https://homl.info/wavenet)，⁠^([16](ch15.html#idm45720177810640))
    Aaron van den Oord和其他DeepMind研究人员介绍了一种名为*WaveNet*的新颖架构。他们堆叠了1D卷积层，每一层的扩张率（每个神经元的输入间隔）都加倍：第一个卷积层一次只能看到两个时间步，而下一个卷积层则看到四个时间步（其感受野为四个时间步），下一个卷积层看到八个时间步，依此类推（参见[图15-14](#wavenet_diagram)）。通过加倍扩张率，较低层学习短期模式，而较高层学习长期模式。由于加倍扩张率，网络可以非常高效地处理极大的序列。
- en: '![mls3 1514](assets/mls3_1514.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 1514](assets/mls3_1514.png)'
- en: Figure 15-14\. WaveNet architecture
  id: totrans-290
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图15-14。WaveNet架构
- en: The authors of the paper actually stacked 10 convolutional layers with dilation
    rates of 1, 2, 4, 8, …​, 256, 512, then they stacked another group of 10 identical
    layers (also with dilation rates 1, 2, 4, 8, …​, 256, 512), then again another
    identical group of 10 layers. They justified this architecture by pointing out
    that a single stack of 10 convolutional layers with these dilation rates will
    act like a super-efficient convolutional layer with a kernel of size 1,024 (except
    way faster, more powerful, and using significantly fewer parameters). They also
    left-padded the input sequences with a number of zeros equal to the dilation rate
    before every layer, to preserve the same sequence length throughout the network.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者实际上堆叠了10个具有1、2、4、8、…、256、512扩张率的卷积层，然后他们又堆叠了另一组10个相同的层（扩张率也是1、2、4、8、…、256、512），然后再次堆叠了另一组相同的10层。他们通过指出，具有这些扩张率的单个10个卷积层堆栈将像具有大小为1,024的卷积核的超高效卷积层一样（速度更快，更强大，参数数量显著减少）。他们还在每一层之前用与扩张率相等的零填充输入序列，以保持整个网络中相同的序列长度。
- en: Here is how to implement a simplified WaveNet to tackle the same sequences as
    earlier:⁠^([17](ch15.html#idm45720177801776))
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是如何实现一个简化的WaveNet来处理与之前相同的序列的方法：⁠^([17](ch15.html#idm45720177801776))
- en: '[PRE41]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This `Sequential` model starts with an explicit input layer—this is simpler
    than trying to set `input_shape` only on the first layer. Then it continues with
    a 1D convolutional layer using `"causal"` padding, which is like `"same"` padding
    except that the zeros are appended only at the start of the input sequence, instead
    of on both sides. This ensures that the convolutional layer does not peek into
    the future when making predictions. Then we add similar pairs of layers using
    growing dilation rates: 1, 2, 4, 8, and again 1, 2, 4, 8\. Finally, we add the
    output layer: a convolutional layer with 14 filters of size 1 and without any
    activation function. As we saw earlier, such a convolutional layer is equivalent
    to a `Dense` layer with 14 units. Thanks to the causal padding, every convolutional
    layer outputs a sequence of the same length as its input sequence, so the targets
    we use during training can be the full 112-day sequences: no need to crop them
    or downsample them.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`Sequential`模型从一个明确的输入层开始——这比仅在第一层上设置`input_shape`要简单。然后，它继续使用“因果”填充的1D卷积层，这类似于“相同”填充，只是零值仅附加在输入序列的开头，而不是两侧。这确保了卷积层在进行预测时不会窥视未来。然后，我们添加使用不断增长的扩张率的类似对层：1、2、4、8，再次是1、2、4、8。最后，我们添加输出层：一个具有14个大小为1的滤波器的卷积层，没有任何激活函数。正如我们之前看到的那样，这样的卷积层等效于具有14个单元的`Dense`层。由于因果填充，每个卷积层输出与其输入序列相同长度的序列，因此我们在训练期间使用的目标可以是完整的112天序列：无需裁剪或降采样。
- en: The models we’ve discussed in this section offer similar performance for the
    ridership forecasting task, but they may vary significantly depending on the task
    and the amount of available data. In the WaveNet paper, the authors achieved state-of-the-art
    performance on various audio tasks (hence the name of the architecture), including
    text-to-speech tasks, producing incredibly realistic voices across several languages.
    They also used the model to generate music, one audio sample at a time. This feat
    is all the more impressive when you realize that a single second of audio can
    contain tens of thousands of time steps—even LSTMs and GRUs cannot handle such
    long sequences.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节讨论的模型对乘客量预测任务提供了类似的性能，但它们在任务和可用数据量方面可能会有很大差异。在WaveNet论文中，作者在各种音频任务（因此该架构的名称）上实现了最先进的性能，包括文本转语音任务，在多种语言中产生令人难以置信的逼真声音。他们还使用该模型逐个音频样本生成音乐。当您意识到一秒钟的音频可能包含成千上万个时间步时，这一壮举就显得更加令人印象深刻——即使是LSTM和GRU也无法处理如此长的序列。
- en: Warning
  id: totrans-296
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: If you evaluate our best Chicago ridership models on the test period, starting
    in 2020, you will find that they perform much worse than expected! Why is that?
    Well, that’s when the Covid-19 pandemic started, which greatly affected public
    transportation. As mentioned earlier, these models will only work well if the
    patterns they learned from the past continue in the future. In any case, before
    deploying a model to production, verify that it works well on recent data. And
    once it’s in production, make sure to monitor its performance regularly.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在测试期间评估我们最佳的芝加哥乘客量模型，从2020年开始，您会发现它们的表现远远不如预期！为什么呢？嗯，那时候是Covid-19大流行开始的时候，这对公共交通产生了很大影响。正如前面提到的，这些模型只有在它们从过去学到的模式在未来继续时才能很好地工作。无论如何，在将模型部署到生产环境之前，请验证它在最近的数据上表现良好。一旦投入生产，请确保定期监控其性能。
- en: With that, you can now tackle all sorts of time series! In [Chapter 16](ch16.html#nlp_chapter),
    we will continue to explore RNNs, and we will see how they can tackle various
    NLP tasks as well.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个，您现在可以处理各种时间序列了！在[第16章](ch16.html#nlp_chapter)中，我们将继续探索RNN，并看看它们如何处理各种NLP任务。
- en: Exercises
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Can you think of a few applications for a sequence-to-sequence RNN? What about
    a sequence-to-vector RNN, and a vector-to-sequence RNN?
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您能想到一些序列到序列RNN的应用吗？序列到向量RNN和向量到序列RNN呢？
- en: How many dimensions must the inputs of an RNN layer have? What does each dimension
    represent? What about its outputs?
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RNN层的输入必须具有多少维度？每个维度代表什么？输出呢？
- en: If you want to build a deep sequence-to-sequence RNN, which RNN layers should
    have `return_sequences=True`? What about a sequence-to-vector RNN?
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您想构建一个深度序列到序列RNN，哪些RNN层应该具有`return_sequences=True`？序列到向量RNN呢？
- en: Suppose you have a daily univariate time series, and you want to forecast the
    next seven days. Which RNN architecture should you use?
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设您有一个每日单变量时间序列，并且您想要预测接下来的七天。您应该使用哪种RNN架构？
- en: What are the main difficulties when training RNNs? How can you handle them?
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练RNN时主要的困难是什么？您如何处理它们？
- en: Can you sketch the LSTM cell’s architecture?
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您能勾画出LSTM单元的架构吗？
- en: Why would you want to use 1D convolutional layers in an RNN?
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么要在RNN中使用1D卷积层？
- en: Which neural network architecture could you use to classify videos?
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用哪种神经网络架构来对视频进行分类？
- en: Train a classification model for the SketchRNN dataset, available in TensorFlow
    Datasets.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为SketchRNN数据集训练一个分类模型，该数据集可在TensorFlow Datasets中找到。
- en: 'Download the [Bach chorales](https://homl.info/bach) dataset and unzip it.
    It is composed of 382 chorales composed by Johann Sebastian Bach. Each chorale
    is 100 to 640 time steps long, and each time step contains 4 integers, where each
    integer corresponds to a note’s index on a piano (except for the value 0, which
    means that no note is played). Train a model—recurrent, convolutional, or both—that
    can predict the next time step (four notes), given a sequence of time steps from
    a chorale. Then use this model to generate Bach-like music, one note at a time:
    you can do this by giving the model the start of a chorale and asking it to predict
    the next time step, then appending these time steps to the input sequence and
    asking the model for the next note, and so on. Also make sure to check out [Google’s
    Coconet model](https://homl.info/coconet), which was used for a nice Google doodle
    about Bach.'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载[巴赫赞美诗](https://homl.info/bach)数据集并解压缩。这是由约翰·塞巴斯蒂安·巴赫创作的382首赞美诗。每首赞美诗长100至640个时间步长，每个时间步长包含4个整数，其中每个整数对应于钢琴上的一个音符的索引（除了值为0，表示没有播放音符）。训练一个模型——循环的、卷积的，或两者兼而有之——可以预测下一个时间步长（四个音符），给定来自赞美诗的时间步长序列。然后使用这个模型生成类似巴赫的音乐，一次一个音符：您可以通过给模型提供赞美诗的开头并要求它预测下一个时间步长来实现这一点，然后将这些时间步长附加到输入序列并要求模型预测下一个音符，依此类推。还要确保查看[谷歌的Coconet模型](https://homl.info/coconet)，该模型用于关于巴赫的一个不错的谷歌涂鸦。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可在本章笔记本的末尾找到，网址为[*https://homl.info/colab3*](https://homl.info/colab3)。
- en: ^([1](ch15.html#idm45720182136320-marker)) Note that many researchers prefer
    to use the hyperbolic tangent (tanh) activation function in RNNs rather than the
    ReLU activation function. For example, see Vu Pham et al.’s [2013 paper](https://homl.info/91)
    “Dropout Improves Recurrent Neural Networks for Handwriting Recognition”. ReLU-based
    RNNs are also possible, as shown in Quoc V. Le et al.’s [2015 paper](https://homl.info/92)
    “A Simple Way to Initialize Recurrent Networks of Rectified Linear Units”.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch15.html#idm45720182136320-marker)) 请注意，许多研究人员更喜欢在RNN中使用双曲正切（tanh）激活函数，而不是ReLU激活函数。例如，参见Vu
    Pham等人的[2013年论文](https://homl.info/91)“Dropout Improves Recurrent Neural Networks
    for Handwriting Recognition”。基于ReLU的RNN也是可能的，正如Quoc V. Le等人的[2015年论文](https://homl.info/92)“初始化修正线性单元的循环网络的简单方法”中所示。
- en: '^([2](ch15.html#idm45720181997536-marker)) Nal Kalchbrenner and Phil Blunsom,
    “Recurrent Continuous Translation Models”, *Proceedings of the 2013 Conference
    on Empirical Methods in Natural Language Processing* (2013): 1700–1709.'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch15.html#idm45720181997536-marker)) Nal Kalchbrenner和Phil Blunsom，“循环连续翻译模型”，*2013年经验方法自然语言处理会议论文集*（2013）：1700–1709。
- en: ^([3](ch15.html#idm45720181962848-marker)) The latest data from the Chicago
    Transit Authority is available at the [Chicago Data Portal](https://homl.info/ridership).
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch15.html#idm45720181962848-marker)) 芝加哥交通管理局的最新数据可在[芝加哥数据门户](https://homl.info/ridership)上找到。
- en: ^([4](ch15.html#idm45720180849712-marker)) There are other more principled approaches
    to selecting good hyperparameters, based on analyzing the *autocorrelation function*
    (ACF) and *partial autocorrelation function* (PACF), or minimizing the AIC or
    BIC metrics (introduced in [Chapter 9](ch09.html#unsupervised_learning_chapter))
    to penalize models that use too many parameters and reduce the risk of overfitting
    the data, but grid search is a good place to start. For more details on the ACF-PACF
    approach, check out this very nice [post by Jason Brownlee](https://homl.info/arimatuning).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch15.html#idm45720180849712-marker)) 有其他更有原则的方法来选择好的超参数，基于分析*自相关函数*（ACF）和*偏自相关函数*（PACF），或最小化AIC或BIC指标（在[第9章](ch09.html#unsupervised_learning_chapter)中介绍）以惩罚使用太多参数的模型并减少过拟合数据的风险，但网格搜索是一个很好的起点。有关ACF-PACF方法的更多详细信息，请查看Jason
    Brownlee的这篇非常好的[文章](https://homl.info/arimatuning)。
- en: ^([5](ch15.html#idm45720180183168-marker)) Note that the validation period starts
    on the 1st of January 2019, so the first prediction is for the 26th of February
    2019, eight weeks later. When we evaluated the baseline models we used predictions
    starting on the 1st of March instead, but this should be close enough.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch15.html#idm45720180183168-marker)) 请注意，验证期从2019年1月1日开始，因此第一个预测是2019年2月26日，八周后。当我们评估基线模型时，我们使用了从3月1日开始的预测，但这应该足够接近。
- en: ^([6](ch15.html#idm45720179430416-marker)) Feel free to play around with this
    model. For example, you can try forecasting both the bus and rail ridership for
    the next 14 days. You’ll need to tweak the targets to include both, and make your
    model output 28 forecasts instead of 14.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch15.html#idm45720179430416-marker)) 随意尝试这个模型。例如，您可以尝试预测接下来14天的公交和轨道乘客量。您需要调整目标，包括两者，并使您的模型输出28个预测，而不是14个。
- en: '^([7](ch15.html#idm45720178649360-marker)) César Laurent et al., “Batch Normalized
    Recurrent Neural Networks”, *Proceedings of the IEEE International Conference
    on Acoustics, Speech, and Signal Processing* (2016): 2657–2661.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch15.html#idm45720178649360-marker)) César Laurent等人，“批量归一化循环神经网络”，*IEEE国际声学、语音和信号处理会议论文集*（2016）：2657–2661。
- en: ^([8](ch15.html#idm45720178641536-marker)) Jimmy Lei Ba et al., “Layer Normalization”,
    arXiv preprint arXiv:1607.06450 (2016).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch15.html#idm45720178641536-marker)) Jimmy Lei Ba等人，“层归一化”，arXiv预印本arXiv:1607.06450（2016）。
- en: ^([9](ch15.html#idm45720178559024-marker)) It would have been simpler to inherit
    from `SimpleRNNCell` instead so that we wouldn’t have to create an internal `SimpleRNNCell`
    or handle the `state_size` and `output_size` attributes, but the goal here was
    to show how to create a custom cell from scratch.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch15.html#idm45720178559024-marker)) 更简单的方法是继承自`SimpleRNNCell`，这样我们就不必创建内部的`SimpleRNNCell`或处理`state_size`和`output_size`属性，但这里的目标是展示如何从头开始创建自定义单元。
- en: ^([10](ch15.html#idm45720178359088-marker)) A character from the animated movies
    *Finding Nemo* and *Finding Dory* who has short-term memory loss.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch15.html#idm45720178359088-marker)) 动画电影*海底总动员*和*海底奇兵*中一个患有短期记忆丧失的角色。
- en: '^([11](ch15.html#idm45720178354352-marker)) Sepp Hochreiter and Jürgen Schmidhuber,
    “Long Short-Term Memory”, *Neural Computation* 9, no. 8 (1997): 1735–1780.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch15.html#idm45720178354352-marker)) Sepp Hochreiter和Jürgen Schmidhuber，“长短期记忆”，*神经计算*
    9，第8期（1997年）：1735–1780。
- en: ^([12](ch15.html#idm45720178351696-marker)) Haşim Sak et al., “Long Short-Term
    Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech
    Recognition”, arXiv preprint arXiv:1402.1128 (2014).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch15.html#idm45720178351696-marker)) Haşim Sak等，“基于长短期记忆的大词汇语音识别循环神经网络架构”，arXiv预印本arXiv:1402.1128（2014年）。
- en: ^([13](ch15.html#idm45720178350096-marker)) Wojciech Zaremba et al., “Recurrent
    Neural Network Regularization”, arXiv preprint arXiv:1409.2329 (2014).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch15.html#idm45720178350096-marker)) Wojciech Zaremba等，“循环神经网络正则化”，arXiv预印本arXiv:1409.2329（2014年）。
- en: '^([14](ch15.html#idm45720178094032-marker)) Kyunghyun Cho et al., “Learning
    Phrase Representations Using RNN Encoder–Decoder for Statistical Machine Translation”,
    *Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing*
    (2014): 1724–1734.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch15.html#idm45720178094032-marker)) Kyunghyun Cho等，“使用RNN编码器-解码器学习短语表示进行统计机器翻译”，*2014年经验方法自然语言处理会议论文集*（2014年）：1724–1734。
- en: '^([15](ch15.html#idm45720178088432-marker)) See Klaus Greff et al., [“LSTM:
    A Search Space Odyssey”](https://homl.info/98), *IEEE Transactions on Neural Networks
    and Learning Systems* 28, no. 10 (2017): 2222–2232.This paper seems to show that
    all LSTM variants perform roughly the same.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ^([15](ch15.html#idm45720178088432-marker)) 请参阅Klaus Greff等的[“LSTM：搜索空间奥德赛”](https://homl.info/98)，*IEEE神经网络与学习系统交易*
    28，第10期（2017年）：2222–2232。这篇论文似乎表明所有LSTM变体表现大致相同。
- en: '^([16](ch15.html#idm45720177810640-marker)) Aaron van den Oord et al., “WaveNet:
    A Generative Model for Raw Audio”, arXiv preprint arXiv:1609.03499 (2016).'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ^([16](ch15.html#idm45720177810640-marker)) Aaron van den Oord等，“WaveNet：原始音频的生成模型”，arXiv预印本arXiv:1609.03499（2016年）。
- en: ^([17](ch15.html#idm45720177801776-marker)) The complete WaveNet uses a few
    more tricks, such as skip connections like in a ResNet, and *gated activation
    units* similar to those found in a GRU cell. See this chapter’s notebook for more
    details.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ^([17](ch15.html#idm45720177801776-marker)) 完整的WaveNet使用了更多技巧，例如类似于ResNet中的跳过连接和类似于GRU单元中的*门控激活单元*。有关更多详细信息，请参阅本章的笔记本。
