- en: Preface
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前言
- en: 'When ChatGPT came out, like many of my colleagues, I was disoriented. What
    surprised me wasn’t the model’s size or capabilities. For over a decade, the AI
    community has known that scaling up a model improves it. In 2012, the AlexNet
    authors noted in [their landmark paper](https://oreil.ly/XG3mv) that: “All of
    our experiments suggest that our results can be improved simply by waiting for
    faster GPUs and bigger datasets to become available.”^([1](preface01.html#id531))^,
    ^([2](preface01.html#id532))'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当ChatGPT发布时，像许多我的同事一样，我感到迷茫。让我感到惊讶的并不是模型的大小或能力。十多年来，人工智能社区都知道，扩大模型规模可以提升其性能。2012年，AlexNet的作者在他们的标志性论文[《他们的里程碑论文》](https://oreil.ly/XG3mv)中提到：“我们所有的实验都表明，通过等待更快的GPU和更大的数据集变得可用，我们的结果可以得到改善。”^([1](preface01.html#id531))^,
    ^([2](preface01.html#id532))
- en: What surprised me was the sheer number of applications this capability boost
    unlocked. I thought a small increase in model quality metrics might result in
    a modest increase in applications. Instead, it resulted in an explosion of new
    possibilities.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我感到惊讶的是，这种能力提升解锁了大量的应用。我以为模型质量指标的微小增加可能会导致应用的小幅增加。相反，它导致了新可能性的爆炸式增长。
- en: Not only have these new AI capabilities increased the demand for AI applications,
    but they have also lowered the entry barrier for developers. It’s become so easy
    to get started with building AI applications. It’s even possible to build an application
    without writing a single line of code. This shift has transformed AI from a specialized
    discipline into a powerful development tool everyone can use.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 这些新的AI能力不仅增加了对AI应用的需求，还降低了开发者的入门门槛。开始构建AI应用变得如此简单。甚至可以构建一个不需要写一行代码的应用。这种转变将人工智能从一门专业学科转变为一个每个人都可以使用的强大开发工具。
- en: Even though AI adoption today seems new, it’s built upon techniques that have
    been around for a while. Papers about language modeling came out as early as the
    1950s. Retrieval-augmented generation (RAG) applications are built upon retrieval
    technology that has powered search and recommender systems since long before the
    term RAG was coined. The best practices for deploying traditional machine learning
    applications—systematic experimentation, rigorous evaluation, relentless optimization
    for faster and cheaper models—are still the best practices for working with foundation
    model-based applications.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管今天人工智能的采用似乎很新，但它建立在已经存在了一段时间的技术之上。关于语言建模的论文早在20世纪50年代就出现了。检索增强生成（RAG）应用建立在检索技术上，这种技术早在RAG这个术语被提出之前就已经推动了搜索和推荐系统。部署传统机器学习应用的最佳实践——系统性的实验、严格的评估、不懈的优化以实现更快和更便宜的模式——仍然是与基于基础模型的应用一起工作的最佳实践。
- en: The familiarity and ease of use of many AI engineering techniques can mislead
    people into thinking there is nothing new to AI engineering. But while many principles
    for building AI applications remain the same, the scale and improved capabilities
    of AI models introduce opportunities and challenges that require new solutions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人工智能工程技术的熟悉性和易用性可能会误导人们认为人工智能工程没有新东西。但是，尽管构建人工智能应用的原则仍然相同，人工智能模型的规模和改进的能力引入了需要新解决方案的机会和挑战。
- en: This book covers the end-to-end process of adapting foundation models to solve
    real-world problems, encompassing tried-and-true techniques from other engineering
    fields and techniques emerging with foundation models.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本书涵盖了将基础模型适应解决现实世界问题的端到端过程，包括来自其他工程领域的经过验证的技术以及随着基础模型出现的新的技术。
- en: I set out to write the book because I wanted to learn, and I did learn a lot.
    I learned from the projects I worked on, the papers I read, and the people I interviewed.
    During the process of writing this book, I used notes from over 100 conversations
    and interviews, including researchers from major AI labs (OpenAI, Google, Anthropic,
    ...), framework developers (NVIDIA, Meta, Hugging Face, Anyscale, LangChain, LlamaIndex,
    ...), executives and heads of AI/data at companies of different sizes, product
    managers, community researchers, and independent application developers (see [“Acknowledgments”](#pr01_0_acknowledgments_1730133447602131)).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我着手写这本书是因为我想学习，而且我确实学到了很多。我从我参与的项目、阅读的论文以及我采访的人那里学到了知识。在撰写这本书的过程中，我使用了超过100次对话和采访的笔记，包括来自主要人工智能实验室（OpenAI、Google、Anthropic、...）、框架开发者（NVIDIA、Meta、Hugging
    Face、Anyscale、LangChain、LlamaIndex、...）、不同规模公司的执行人员和AI/数据负责人、产品经理、社区研究人员以及独立应用开发者（见[“致谢”](#pr01_0_acknowledgments_1730133447602131))。
- en: I especially learned from early readers who tested my assumptions, introduced
    me to different perspectives, and exposed me to new problems and approaches. Some
    sections of the book have also received thousands of comments from the community
    after being shared on [my blog](https://huyenchip.com/blog/), many giving me new
    perspectives or confirming a hypothesis.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我特别从早期读者那里学到了很多，他们测试了我的假设，介绍了不同的观点，并让我接触到了新的问题和方法。本书的一些部分在[我的博客](https://huyenchip.com/blog/)上分享后，也收到了来自社区的数千条评论，许多人提供了新的观点或证实了一个假设。
- en: I hope that this learning process will continue for me now that the book is
    in your hands, as you have experiences and perspectives that are unique to you.
    Please feel free to share any feedback you might have for this book with me via
    [X](https://x.com/chipro), [LinkedIn](https://www.linkedin.com/in/chiphuyen),
    or email at [hi@huyenchip.com](mailto:hi@huyenchip.com).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这本书现在在你手中时，这个学习过程会继续下去，因为你有独特于你的经验和观点。请随时通过[X](https://x.com/chipro)、[LinkedIn](https://www.linkedin.com/in/chiphuyen)或电子邮件[hi@huyenchip.com](mailto:hi@huyenchip.com)与我分享你对该书的任何反馈。
- en: What This Book Is About
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书的内容是什么
- en: This book provides a framework for adapting foundation models, which include
    both large language models (LLMs) and large multimodal models (LMMs), to specific
    applications.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本书提供了一个框架，用于将基础模型（包括大型语言模型（LLMs）和大型多模态模型（LMMs））适应特定应用。
- en: 'There are many different ways to build an application. This book outlines various
    solutions and also raises questions you can ask to evaluate the best solution
    for your needs. Some of the many questions that this book can help you answer
    are:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 构建应用程序有许多不同的方法。本书概述了各种解决方案，并提出了你可以提出的问题来评估最适合你需求的最佳解决方案。本书可以帮助你回答的许多问题包括：
- en: Should I build this AI application?
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我是否应该构建这个 AI 应用程序？
- en: How do I evaluate my application? Can I use AI to evaluate AI outputs?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何评估我的应用程序？我能用 AI 来评估 AI 输出吗？
- en: What causes hallucinations? How do I detect and mitigate hallucinations?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 幻觉是由什么引起的？我如何检测和减轻幻觉？
- en: What are the best practices for prompt engineering?
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示工程的最佳实践是什么？
- en: Why does RAG work? What are the strategies for doing RAG?
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么 RAG 会起作用？进行 RAG 的策略有哪些？
- en: What’s an agent? How do I build and evaluate an agent?
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是代理？我如何构建和评估一个代理？
- en: When to finetune a model? When not to finetune a model?
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时微调模型？何时不微调模型？
- en: How much data do I need? How do I validate the quality of my data?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我需要多少数据？我如何验证数据的质量？
- en: How do I make my model faster, cheaper, and secure?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何使我的模型更快、更便宜、更安全？
- en: How do I create a feedback loop to improve my application continually?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何创建一个反馈循环来持续改进我的应用程序？
- en: 'The book will also help you navigate the overwhelming AI landscape: types of
    models, evaluation benchmarks, and a seemingly infinite number of use cases and
    application patterns.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本书还将帮助你导航令人眼花缭乱的 AI 环境类型、评估基准以及看似无限的应用用例和应用模式。
- en: The content in this book is illustrated using case studies, many of which I
    worked on, backed by ample references and extensively reviewed by experts from
    a wide range of backgrounds. Although the book took two years to write, it draws
    from my experience working with language models and ML systems from the last decade.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的内容通过案例研究进行说明，其中许多案例是我参与工作的，并有充足的参考文献支持，并由来自不同背景的专家进行了广泛审查。尽管这本书花费了两年时间来撰写，但它汲取了我过去十年与语言模型和机器学习系统合作的经验。
- en: Like my previous O’Reilly book, *Designing Machine Learning Systems* (DMLS),
    this book focuses on the fundamentals of AI engineering instead of any specific
    tool or API. Tools become outdated quickly, but fundamentals should last longer.^([3](preface01.html#id533))
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与我之前出版的 O’Reilly 书籍《设计机器学习系统》（DMLS）一样，这本书专注于人工智能工程的基础知识，而不是任何特定的工具或 API。工具很快就会过时，但基础知识应该更持久.^([3](preface01.html#id533))
- en: Determining whether something will last, however, is often challenging. I relied
    on three criteria. First, for a problem, I determined whether it results from
    the fundamental limitations of how AI works or if it’ll go away with better models.
    If a problem is fundamental, I’ll analyze its challenges and solutions to address
    each challenge. I’m a fan of the start-simple approach, so for many problems,
    I’ll start from the simplest solution and then progress with more complex solutions
    to address rising challenges.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，确定某物是否会持续存在，通常是一个挑战。我依赖三个标准。首先，对于一个问题，我确定它是否是由人工智能工作的基本局限性引起的，或者它是否会随着更好的模型而消失。如果问题本质上是基本的，我会分析其挑战和解决方案，以解决每个挑战。我是一个简单开始的支持者，所以对于许多问题，我会从最简单的解决方案开始，然后随着挑战的上升，逐步采用更复杂的解决方案。
- en: Second, I consulted an extensive network of researchers and engineers, who are
    smarter than I am, about what they think are the most important problems and solutions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我咨询了一个由比我聪明的学者和工程师组成的广泛网络，他们关于他们认为最重要的问题和解决方案的看法。
- en: Occasionally, I also relied on [Lindy’s Law](https://en.wikipedia.org/wiki/Lindy_effect),
    which infers that the future life expectancy of a technology is proportional to
    its current age. So if something has been around for a while, I assume that it’ll
    continue existing for a while longer.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我也会依赖[Lindy效应](https://en.wikipedia.org/wiki/Lindy_effect)，它推断出一种技术的未来预期寿命与其当前年龄成正比。所以，如果某样东西已经存在了一段时间，我会假设它还会继续存在一段时间更长。
- en: In this book, however, I occasionally included a concept that I believe to be
    temporary because it’s immediately useful for some application developers or because
    it illustrates an interesting problem-solving approach.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这本书中，我偶尔也会包含一些我认为是暂时性的概念，因为它们对某些应用开发者来说立即有用，或者因为它们展示了有趣的解决问题的方法。
- en: What This Book Is Not
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这本书不是什么
- en: This book isn’t a tutorial. While it mentions specific tools and includes pseudocode
    snippets to illustrate certain concepts, it doesn’t teach you how to use a tool.
    Instead, it offers a framework for selecting tools. It includes many discussions
    on the trade-offs between different solutions and the questions you should ask
    when evaluating a solution. When you want to use a tool, it’s usually easy to
    find tutorials for it online. AI chatbots are also pretty good at helping you
    get started with popular tools.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书不是教程。虽然它提到了特定的工具，并包括伪代码片段来展示某些概念，但它不教你如何使用工具。相反，它提供了一个选择工具的框架。它包括许多关于不同解决方案之间的权衡以及评估解决方案时应提出的问题的讨论。当你想使用一个工具时，通常很容易在网上找到它的教程。AI聊天机器人也相当擅长帮助你开始使用流行的工具。
- en: This book isn’t an ML theory book. It doesn’t explain what a neural network
    is or how to build and train a model from scratch. While it explains many theoretical
    concepts immediately relevant to the discussion, the book is a practical book
    that focuses on helping you build successful AI applications to solve real-world
    problems.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书不是一本机器学习理论书。它不解释神经网络是什么，或者如何从头开始构建和训练模型。虽然它解释了许多与讨论直接相关的理论概念，但本书是一本实用的书，专注于帮助你构建成功的AI应用来解决现实世界的问题。
- en: 'While it’s possible to build foundation model-based applications without ML
    expertise, a basic understanding of ML and statistics can help you build better
    applications and save you from unnecessary suffering. You can read this book without
    any prior ML background. However, you will be more effective while building AI
    applications if you know the following concepts:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在没有机器学习专业知识的情况下可以构建基于基础模型的应用，但了解机器学习和统计学的基本知识可以帮助你构建更好的应用，并让你免受不必要的痛苦。你可以没有先前的机器学习背景来阅读这本书。然而，如果你知道以下概念，在构建AI应用时你会更加有效：
- en: Probabilistic concepts such as sampling, determinism, and distribution.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率论概念，如抽样、确定性、分布。
- en: ML concepts such as supervision, self-supervision, log-likelihood, gradient
    descent, backpropagation, loss function, and hyperparameter tuning.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习概念，如监督学习、自监督学习、对数似然、梯度下降、反向传播、损失函数和超参数调整。
- en: Various neural network architectures, including feedforward, recurrent, and
    transformer.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 各种神经网络架构，包括前馈、循环和转换器。
- en: Metrics such as accuracy, F1, precision, recall, cosine similarity, and cross
    entropy.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标，如准确率、F1分数、精确率、召回率、余弦相似度和交叉熵。
- en: If you don’t know them yet, don’t worry—this book has either brief, high-level
    explanations or pointers to resources that can get you up to speed.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还不了解它们，不要担心——这本书要么提供了简短的高级解释，要么提供了可以让你快速掌握的资源。
- en: Who This Book Is For
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 这本书面向谁
- en: 'This book is for anyone who wants to leverage foundation models to solve real-world
    problems. This is a technical book, so the language of this book is geared toward
    technical roles, including AI engineers, ML engineers, data scientists, engineering
    managers, and technical product managers. This book is for you if you can relate
    to one of the following scenarios:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本书适合任何希望利用基础模型解决现实世界问题的人。这是一本技术书，因此本书的语言面向技术角色，包括AI工程师、ML工程师、数据科学家、工程经理和技术产品经理。如果您能理解以下任何一种情况，这本书就是为您准备的：
- en: You’re building or optimizing an AI application, whether you’re starting from
    scratch or looking to move beyond the demo phase into a production-ready stage.
    You may also be facing issues like hallucinations, security, latency, or costs,
    and need targeted solutions.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您正在构建或优化一个AI应用，无论您是从零开始还是希望从演示阶段过渡到生产就绪阶段。您可能还面临幻觉、安全性、延迟或成本等问题，并需要针对性的解决方案。
- en: You want to streamline your team’s AI development process, making it more systematic,
    faster, and reliable.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您想简化团队的人工智能开发流程，使其更加系统化、快速和可靠。
- en: You want to understand how your organization can leverage foundation models
    to improve the business’s bottom line and how to build a team to do so.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您想了解您的组织如何利用基础模型来提高业务的底线，以及如何组建一个团队来实现这一点。
- en: 'You can also benefit from the book if you belong to one of the following groups:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以从这本书中受益，如果您属于以下任何一组：
- en: Tool developers who want to identify underserved areas in AI engineering to
    position your products in the ecosystem.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具开发者，希望识别AI工程中的未充分服务的领域，以便在生态系统中定位您的产品。
- en: Researchers who want to better understand AI use cases.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 希望更好地理解AI用例的研究人员。
- en: Job candidates seeking clarity on the skills needed to pursue a career as an
    AI engineer.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻求清晰了解追求AI工程师职业所需技能的求职者。
- en: Anyone wanting to better understand AI’s capabilities and limitations, and how
    it might affect different roles.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何希望更好地理解AI的能力和局限性，以及它可能如何影响不同角色的人。
- en: I love getting to the bottom of things, so some sections dive a bit deeper into
    the technical side. While many early readers like the detail, it might not be
    for everyone. I’ll give you a heads-up before things get too technical. Feel free
    to skip ahead if it feels a little too in the weeds!
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢深入探究事物，所以有些部分会稍微深入到技术方面。虽然许多早期读者喜欢这些细节，但这可能不是每个人都适合的。在事情变得过于技术性之前，我会提前给您一个提示。如果您觉得有点太深入了，请随时跳过。
- en: Navigating This Book
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 导航本书
- en: This book is structured to follow the typical process for developing an AI application.
    Here’s what this typical process looks like and how each chapter fits into the
    process. Because this book is modular, you’re welcome to skip any section that
    you’re already familiar with or that is less relevant to you.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的结构遵循开发AI应用的典型流程。以下是这个典型流程的样子以及每一章如何融入这个过程。由于本书是模块化的，您可以跳过任何您已经熟悉或与您不太相关的部分。
- en: 'Before deciding to build an AI application, it’s necessary to understand what
    this process involves and answer questions such as: Is this application necessary?
    Is AI needed? Do I have to build this application myself? The first chapter of
    the book helps you answer these questions. It also covers a range of successful
    use cases to give a sense of what foundation models can do.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定构建一个AI应用之前，了解这个过程涉及的内容并回答以下问题是很必要的：这个应用是必要的吗？需要AI吗？我必须自己构建这个应用吗？这本书的第一章帮助您回答这些问题。它还涵盖了一系列成功的用例，以展示基础模型可以做什么。
- en: While an ML background is not necessary to build AI applications, understanding
    how a foundation model works under the hood is useful to make the most out of
    it. [Chapter 2](ch02.html#ch02_understanding_foundation_models_1730147895571359)
    analyzes the making of a foundation model and the design decisions with significant
    impacts on downstream applications, including its training data recipe, model
    architectures and scales, and how the model is trained to align to human preference.
    It then discusses how a model generates a response, which helps explain the model’s
    seemingly baffling behaviors, like inconsistency and hallucinations. Changing
    the generation setting of a model is also often a cheap and easy way to significantly
    boost the model’s performance.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然构建AI应用不需要ML背景，但了解基础模型在底层是如何工作的，对于充分利用它是有用的。[第2章](ch02.html#ch02_understanding_foundation_models_1730147895571359)分析了基础模型的制作及其对下游应用有重大影响的设计决策，包括其训练数据配方、模型架构和规模，以及模型是如何训练以与人类偏好对齐的。然后讨论了模型是如何生成响应的，这有助于解释模型看似令人困惑的行为，如不一致性和幻觉。改变模型的生成设置通常是便宜且简单的方法，可以显著提高模型的表现。
- en: Once you’ve committed to building an application with foundation models, evaluation
    will be an integral part of every step along the way. Evaluation is one of the
    hardest, if not the hardest, challenges of AI engineering. This book dedicates
    two chapters, Chapters [3](ch03.html#ch03a_evaluation_methodology_1730150757064067)
    and [4](ch04.html#ch04_evaluate_ai_systems_1730130866187863), to explore different
    evaluation methods and how to use them to create a reliable and systematic evaluation
    pipeline for your application.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你决定使用基础模型来构建一个应用，评估将贯穿整个过程的每一步。评估是AI工程中最困难，如果不是最困难的挑战之一。本书用两章，即第[3章](ch03.html#ch03a_evaluation_methodology_1730150757064067)和第[4章](ch04.html#ch04_evaluate_ai_systems_1730130866187863)，来探讨不同的评估方法和如何使用它们为你的应用创建一个可靠和系统的评估流程。
- en: 'Given a query, the quality of a model’s response depends on the following aspects
    (outside of the model’s generation setting):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个查询，模型响应的质量取决于以下方面（除了模型生成设置之外）：
- en: The instructions for how the model should behave
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指示模型应该如何行为的说明
- en: The context the model can use to respond to the query
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型可以用来响应查询的上下文
- en: The model itself
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型本身
- en: The next three chapters of the book focus on how to optimize each of these aspects
    to improve a model’s performance for an application. [Chapter 5](ch05.html#ch05a_prompt_engineering_1730156991195551)
    covers prompt engineering, starting with what a prompt is, why prompt engineering
    works, and prompt engineering best practices. It then discusses how bad actors
    can exploit your application with prompt attacks and how to defend your application
    against them.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 书的接下来的三章专注于如何优化这些方面的每一个，以提高模型在应用中的性能。[第5章](ch05.html#ch05a_prompt_engineering_1730156991195551)涵盖了提示工程，从提示是什么，为什么提示工程有效，以及提示工程的最佳实践开始。然后讨论了恶意行为者如何利用提示攻击来利用你的应用，以及如何防御这些攻击。
- en: '[Chapter 6](ch06.html#ch06_rag_and_agents_1730157386571386) explores why context
    is important for a model to generate accurate responses. It zooms into two major
    application patterns for context construction: RAG and agentic. The RAG pattern
    is better understood and has proven to work well in production. On the other hand,
    while the agentic pattern promises to be much more powerful, it’s also more complex
    and is still being explored.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[第6章](ch06.html#ch06_rag_and_agents_1730157386571386)探讨了为什么上下文对于模型生成准确响应很重要。它深入研究了两种主要的上下文构建应用模式：RAG和代理。RAG模式更容易理解，并且在生产中已经证明效果良好。另一方面，虽然代理模式承诺将具有更大的威力，但它也更复杂，并且仍在探索中。'
- en: '[Chapter 7](ch07.html#ch07) is about how to adapt a model to an application
    by changing the model itself with finetuning. Due to the scale of foundation models,
    native model finetuning is memory-intensive, and many techniques are developed
    to allow finetuning better models with less memory. The chapter covers different
    finetuning approaches, supplemented by a more experimental approach: model merging.
    This chapter contains a more technical section that shows how to calculate the
    memory footprint of a model.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '[第7章](ch07.html#ch07) 讲述了如何通过微调模型本身来适应一个应用。由于基础模型的规模，原生模型的微调是内存密集型的，因此已经开发了许多技术，以允许使用更少的内存来微调更好的模型。本章涵盖了不同的微调方法，并补充了一种更实验性的方法：模型合并。本章包含一个更技术性的部分，展示了如何计算模型的内存占用。'
- en: Due to the availability of many finetuning frameworks, the finetuning process
    itself is often straightforward. However, getting data for finetuning is hard.
    The next chapter is all about data, including data acquisition, data annotations,
    data synthesis, and data processing. Many of the topics discussed in [Chapter 8](ch08.html#ch08_dataset_engineering_1730130932019888)
    are relevant beyond finetuning, including the question of what data quality means
    and how to evaluate the quality of your data.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多微调框架的可用性，微调过程本身通常很简单。然而，获取微调数据是困难的。下一章全部关于数据，包括数据获取、数据标注、数据合成和数据处理。第[8](ch08.html#ch08_dataset_engineering_1730130932019888)章中讨论的许多主题超出了微调的范围，包括数据质量的意义以及如何评估你的数据质量。
- en: If Chapters [5](ch05.html#ch05a_prompt_engineering_1730156991195551) to [8](ch08.html#ch08_dataset_engineering_1730130932019888)
    are about improving a model’s quality, [Chapter 9](ch09.html#ch09_inference_optimization_1730130963006301)
    is about making its inference cheaper and faster. It discusses optimization both
    at the model level and inference service level. If you’re using a model API—i.e.,
    someone else hosts your model for you—this API will likely take care of inference
    optimization for you. However, if you host the model yourself—either an open source
    model or a model developed in-house—you’ll need to implement many of the techniques
    discussed in this chapter.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果第[5](ch05.html#ch05a_prompt_engineering_1730156991195551)章到第[8](ch08.html#ch08_dataset_engineering_1730130932019888)章是关于提高模型质量，那么第[9](ch09.html#ch09_inference_optimization_1730130963006301)章则是关于使其推理更便宜、更快。它讨论了在模型级别和推理服务级别的优化。如果你使用模型API——即其他人为你托管模型——这个API可能会为你处理推理优化。然而，如果你自己托管模型——无论是开源模型还是内部开发的模型——你需要实现本章讨论的许多技术。
- en: The last chapter in the book brings together the different concepts from this
    book to build an application end-to-end. The second part of the chapter is more
    product-focused, with discussions on how to design a user feedback system that
    helps you collect useful feedback while maintaining a good user experience.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 书中的最后一章将本书中的不同概念汇集起来，构建了一个端到端的应用程序。章节的第二部分更加关注产品，讨论了如何设计一个用户反馈系统，该系统能够帮助你收集有用的反馈，同时保持良好的用户体验。
- en: Note
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: I often use “we” in this book to mean you (the reader) and I. It’s a habit I
    got from my teaching days, as I saw writing as a shared learning experience for
    both the writer and the readers.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这本书中经常使用“我们”来指代你（读者）和我。这是我从教学日子里养成的习惯，因为我把写作看作是作者和读者共同的学习体验。
- en: Conventions Used in This Book
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本书使用的约定
- en: 'The following typographical conventions are used in this book:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 本书使用以下排版约定：
- en: '*Italic*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*斜体*'
- en: Indicates new terms, URLs, email addresses, filenames, and file extensions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表示新术语、URL、电子邮件地址、文件名和文件扩展名。
- en: '`Constant width`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`常宽`'
- en: Used for program listings, as well as within paragraphs to refer to program
    elements such as variable or function names, databases, data types, environment
    variables, statements, input prompts into models, and keywords.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 用于程序列表，以及段落中引用程序元素，如变量或函数名称、数据库、数据类型、环境变量、语句、输入到模型中的提示和关键字。
- en: '**`Constant width bold`**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**`常宽粗体`**'
- en: Shows commands or other text that should be typed literally by the user.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表示用户需要直接输入的命令或其他文本。
- en: '*`Constant width italic`*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*`常宽斜体`*'
- en: Shows text that should be replaced with user-supplied values or by values determined
    by context.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表示应替换为用户提供的值或由上下文确定的值的文本。
- en: Tip
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: This element signifies a tip or suggestion.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示提示或建议。
- en: Note
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: This element signifies a general note.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示一般性说明。
- en: Warning
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: This element indicates a warning or caution.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素表示警告或注意事项。
- en: Using Code Examples
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用代码示例
- en: Supplemental material (code examples, exercises, etc.) is available for download
    at [*https://github.com/chiphuyen/aie-book*](https://github.com/chiphuyen/aie-book).
    The repository contains additional resources about AI engineering, including important
    papers and helpful tools. It also covers topics that are too deep to go into in
    this book. For those interested in the process of writing this book, the GitHub
    repository also contains behind-the-scenes information and statistics about the
    book.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 补充材料（代码示例、练习等）可在[*https://github.com/chiphuyen/aie-book*](https://github.com/chiphuyen/aie-book)下载。该存储库包含关于AI工程的额外资源，包括重要论文和有用的工具。它还涵盖了本书中未能深入探讨的主题。对于那些对本书的写作过程感兴趣的人，GitHub存储库还包含了幕后信息和关于本书的统计数据。
- en: If you have a technical question or a problem using the code examples, please
    send email to [*support@oreilly.com*](mailto:support@oreilly.com).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在使用代码示例时遇到技术问题或问题，请发送电子邮件至[*support@oreilly.com*](mailto:support@oreilly.com)。
- en: This book is here to help you get your job done. In general, if example code
    is offered with this book, you may use it in your programs and documentation.
    You do not need to contact us for permission unless you’re reproducing a significant
    portion of the code. For example, writing a program that uses several chunks of
    code from this book does not require permission. Selling or distributing examples
    from O’Reilly books does require permission. Answering a question by citing this
    book and quoting example code does not require permission. Incorporating a significant
    amount of example code from this book into your product’s documentation does require
    permission.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本书旨在帮助您完成工作。一般来说，如果本书提供了示例代码，您可以在您的程序和文档中使用它。除非您正在复制代码的很大一部分，否则您不需要联系我们获得许可。例如，编写一个使用本书中几个代码块的程序不需要许可。通过引用本书并引用示例代码来回答问题不需要许可。将本书的大量示例代码纳入您产品的文档中需要许可。
- en: 'We appreciate, but generally do not require, attribution. An attribution usually
    includes the title, author, publisher, and ISBN. For example: “*AI Engineering*
    by Chip Huyen (O’Reilly). Copyright 2025 Developer Experience Advisory LLC, 978-1-098-16630-4.”'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢，但通常不需要署名。署名通常包括标题、作者、出版社和ISBN。例如：“*AI Engineering* by Chip Huyen (O’Reilly).
    版权2025 Developer Experience Advisory LLC, 978-1-098-16630-4。”
- en: If you feel your use of code examples falls outside fair use or the permission
    given above, feel free to contact us at [*permissions@oreilly.com*](mailto:permissions@oreilly.com).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您认为您对代码示例的使用超出了合理使用或上述许可的范围，请随时联系我们：[*permissions@oreilly.com*](mailto:permissions@oreilly.com)。
- en: O’Reilly Online Learning
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: O’Reilly在线学习
- en: Note
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For more than 40 years, [*O’Reilly Media*](https://oreilly.com) has provided
    technology and business training, knowledge, and insight to help companies succeed.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 40多年来，[*O’Reilly Media*](https://oreilly.com)一直为科技公司提供技术和商业培训、知识和洞察力，以帮助公司取得成功。
- en: Our unique network of experts and innovators share their knowledge and expertise
    through books, articles, and our online learning platform. O’Reilly’s online learning
    platform gives you on-demand access to live training courses, in-depth learning
    paths, interactive coding environments, and a vast collection of text and video
    from O’Reilly and 200+ other publishers. For more information, visit [*https://oreilly.com*](https://oreilly.com).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们独特的专家和创新者网络通过书籍、文章和我们的在线学习平台分享他们的知识和专业知识。O’Reilly的在线学习平台为您提供按需访问实时培训课程、深入的学习路径、交互式编码环境以及来自O’Reilly和200多家其他出版商的大量文本和视频。更多信息，请访问[*https://oreilly.com*](https://oreilly.com)。
- en: How to Contact Us
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何联系我们
- en: 'Please address comments and questions concerning this book to the publisher:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请将有关本书的评论和问题寄给出版社：
- en: O’Reilly Media, Inc.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: O’Reilly Media, Inc.
- en: 1005 Gravenstein Highway North
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1005 Gravenstein Highway North
- en: Sebastopol, CA 95472
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加州塞巴斯蒂波尔，95472
- en: 800-889-8969 (in the United States or Canada)
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 800-889-8969 (美国或加拿大境内)
- en: 707-827-7019 (international or local)
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-827-7019 (国际或本地)
- en: 707-829-0104 (fax)
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 707-829-0104 (传真)
- en: '[*support@oreilly.com*](mailto:support@oreilly.com)'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*support@oreilly.com*](mailto:support@oreilly.com)'
- en: '[*https://oreilly.com/about/contact.html*](https://oreilly.com/about/contact.html)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*https://oreilly.com/about/contact.html*](https://oreilly.com/about/contact.html)'
- en: We have a web page for this book, where we list errata, examples, and any additional
    information. You can access this page at [*https://oreil.ly/ai-engineering*](https://oreil.ly/ai-engineering).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为这本书有一个网页，其中列出了勘误表、示例和任何其他附加信息。您可以通过[*https://oreil.ly/ai-engineering*](https://oreil.ly/ai-engineering)访问此页面。
- en: For news and information about our books and courses, visit [*https://oreilly.com*](https://oreilly.com).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 了解我们书籍和课程的新闻和信息，请访问[*https://oreilly.com*](https://oreilly.com)。
- en: 'Find us on LinkedIn: [*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在LinkedIn上找到我们：[*https://linkedin.com/company/oreilly-media*](https://linkedin.com/company/oreilly-media)
- en: 'Watch us on YouTube: [*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia)'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在YouTube上关注我们：[*https://youtube.com/oreillymedia*](https://youtube.com/oreillymedia)
- en: Acknowledgments
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 致谢
- en: This book would’ve taken a lot longer to write and missed many important topics
    if it wasn’t for so many wonderful people who helped me through the process.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有这么多帮助我完成这个过程的好人，这本书的编写将会花费更多时间，并且会错过许多重要主题。
- en: Because the timeline for the project was tight—two years for a 150,000-word
    book that covers so much ground—I’m grateful to the technical reviewers who put
    aside their precious time to review this book so quickly.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由于项目的截止时间紧迫——在两年内完成一本涵盖如此广泛内容的15万字的书籍——我非常感激那些抽出宝贵时间迅速审阅这本书的技术审稿人。
- en: Luke Metz is an amazing soundboard who checked my assumptions and prevented
    me from going down the wrong path. Han-chung Lee, always up to date with the latest
    AI news and community development, pointed me toward resources that I had missed.
    Luke and Han were the first to review my drafts before I sent them to the next
    round of technical reviewers, and I’m forever indebted to them for tolerating
    my follies and mistakes.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 卢克·梅茨是一位出色的顾问，他检查了我的假设，防止我走错路。韩钟·李总是紧跟最新的AI新闻和社区发展，为我指出了我遗漏的资源。卢克和韩钟在我将草稿发送给下一轮技术审稿人之前，首先审阅了我的草稿，我永远感激他们容忍我的愚蠢和错误。
- en: Having led AI innovation at Fortune 500 companies, Vittorio Cretella and Andrei
    Lopatenko provided invaluable feedback that combined deep technical expertise
    with executive insights. Vicki Reyzelman helped me ground my content and keep
    it relevant for readers with a software engineering background.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 维托里奥·克雷特拉和安德烈·洛帕滕科在财富500强公司领导AI创新，他们提供了宝贵的反馈，结合了深厚的专业技术知识和管理层的洞察力。维基·赖兹尔曼帮助我将内容接地气，并使其对具有软件工程背景的读者保持相关性。
- en: Eugene Yan, a dear friend and amazing applied scientist, provided me with technical
    and emotional support. Shawn Wang (swyx) provided an important vibe check that
    helped me feel more confident about the book. Sanyam Bhutani, one of the best
    learners and most humble souls I know, not only gave thoughtful written feedback
    but also recorded videos to explain his feedback.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 尤金·严是一位亲爱的朋友和出色的应用科学家，为我提供了技术和情感上的支持。肖恩·王（swyx）提供了一次重要的氛围检查，帮助我更有信心地对待这本书。萨扬·布塔尼，是我所知最好的学习者之一，最谦逊的灵魂，不仅给出了深思熟虑的书面反馈，还录制了视频来解释他的反馈。
- en: Kyle Kranen is a star deep learning lead who interviewed his colleagues and
    shared with me an amazing writeup about their finetuning process, which guided
    the finetuning chapter. Mark Saroufim, an inquisitive mind who always has his
    finger on the pulse of the most interesting problems, introduced me to great resources
    on efficiency. Both Kyle and Mark’s feedback was critical in writing Chapters
    [7](ch07.html#ch07) and [9](ch09.html#ch09_inference_optimization_1730130963006301).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 凯尔·克兰恩是一位杰出的深度学习负责人，他采访了他的同事，并与我分享了他们微调过程的精彩总结，这指导了微调章节。马克·萨拉菲姆是一位好奇心旺盛的人，他总是关注最有趣的问题，并向我介绍了关于效率的宝贵资源。凯尔和马克的反馈对于撰写第[7](ch07.html#ch07)章和第[9](ch09.html#ch09_inference_optimization_1730130963006301)章至关重要。
- en: Kittipat “Bot” Kampa, in addition to answering my many questions, shared with
    me a detailed visualization of how he thinks about AI platforms. I appreciate
    Denys Linkov’s systematic approach to evaluation and platform development. Chetan
    Tekur gave great examples that helped me structure AI application patterns. I’d
    also like to thank Shengzhi (Alex) Li and Hien Luu for their thoughtful feedback
    on my draft on AI architecture.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 除了回答我许多问题外，基蒂帕特“博特”坎帕还与我分享了他对AI平台的详细可视化思考。我赞赏德尼·林科夫在评估和平台开发方面的系统方法。切坦·特库尔提供了帮助我构建AI应用模式的优秀例子。我还想感谢沈志（亚历克斯）李和阮辉恩对我的AI架构草稿的深思熟虑的反馈。
- en: Aileen Bui is a treasure who shared unique feedback and examples from a product
    manager’s perspective. Thanks to Todor Markov for the actionable advice on the
    RAG and Agents chapter. Thanks to Tal Kachman for jumping in at the last minute
    to push the Finetuning chapter over the finish line.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 艾琳·布伊是一位宝贵的财富，她从产品经理的角度分享了独特的反馈和例子。感谢托多·马尔科夫对RAG和代理章节的可操作建议。感谢塔尔·卡奇曼在最后一刻加入，推动微调章节顺利完成。
- en: There are so many wonderful people whose company and conversations gave me ideas
    that guided the content of this book. I tried my best to include the names of
    everyone who has helped me here, but due to the inherent faultiness of human memory,
    I undoubtedly neglected to mention many. If I forgot to include your name, please
    know that it wasn’t because I don’t appreciate your contribution, and please kindly
    remind me so that I can rectify this as soon as possible!
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多优秀的人，他们的陪伴和对话给了我许多灵感，指导了这本书的内容。我尽力将所有帮助过我的人的名字都包括在内，但由于人类记忆的固有缺陷，我无疑遗漏了许多人。如果我没有包括你的名字，请知道这并不是因为我没有感激你的贡献，请友好地提醒我，以便我尽快纠正这一点！
- en: Andrew Francis, Anish Nag, Anthony Galczak, Anton Bacaj, Balázs Galambosi, Charles
    Frye, Charles Packer, Chris Brousseau, Eric Hartford, Goku Mohandas, Hamel Husain,
    Harpreet Sahota, Hassan El Mghari, Huu Nguyen, Jeremy Howard, Jesse Silver, John
    Cook, Juan Pablo Bottaro, Kyle Gallatin, Lance Martin, Lucio Dery, Matt Ross,
    Maxime Labonne, Miles Brundage, Nathan Lambert, Omar Khattab, Phong Nguyen, Purnendu
    Mukherjee, Sam Reiswig, Sebastian Raschka, Shahul ES, Sharif Shameem, Soumith
    Chintala, Teknium, Tim Dettmers, Undi95, Val Andrei Fajardo, Vern Liang, Victor
    Sanh, Wing Lian, Xiquan Cui, Ying Sheng, and Kristofer.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 安德鲁·弗朗西斯、阿尼什·纳格、安东尼·加尔查克、安东·巴卡伊、巴拉兹·加兰博西、查尔斯·弗赖伊、查尔斯·帕克、克里斯·布罗萨乌、埃里克·哈特福德、古库·莫汉达斯、哈梅尔·胡赛因、哈普雷特·萨霍塔、哈桑·埃尔·穆加里、胡安·诺伊恩、杰里米·豪厄德、杰西·西尔弗、约翰·库克、胡安·帕布洛·博塔罗、凯尔·加尔蒂宁、兰斯·马丁、卢西奥·德里、马特·罗斯、马克西姆·拉邦内、迈尔斯·布兰德奇、内森·兰伯特、奥马尔·卡塔布、
    phong nguyen、普尔嫩杜·穆克赫杰、山姆·赖斯威格、塞巴斯蒂安·拉斯克卡、沙胡尔·ES、沙里夫·沙米姆、索乌米特·钦塔拉、Teknium、蒂姆·德特梅斯、Undi95、瓦莱·安德烈·法哈多、维尔·梁、维克多·桑赫、温·连、西泉·崔、英生、克里斯托弗。
- en: I’d like to thank all early readers who have also reached out with feedback.
    Douglas Bailley is a super reader who shared so much thoughtful feedback. Thanks
    to Nutan Sahoo for suggesting an elegant way to explain perplexity.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我还想感谢所有提供反馈的早期读者。道格拉斯·贝利是一位超级读者，分享了如此多的深思熟虑的反馈。感谢纳坦·萨胡建议一种优雅的方式来解释困惑度。
- en: I learned so much from the online discussions with so many. Thanks to everyone
    who’s ever answered my questions, commented on my posts, or sent me an email with
    your thoughts.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我从与许多人的在线讨论中学到了很多。感谢所有曾经回答过我问题、评论过我帖子或给我发送邮件分享你们想法的人。
- en: Of course, the book wouldn’t have been possible without the team at O’Reilly,
    especially my development editors (Melissa Potter, Corbin Collins, Jill Leonard)
    and my production editor (Elizabeth Kelly). Liz Wheeler is the most discerning
    copyeditor I’ve ever worked with. Nicole Butterfield is a force who oversaw this
    book from an idea to a final product.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，没有O’Reilly团队的共同努力，这本书是不可能完成的，特别是我的开发编辑（梅丽莎·波特、科宾·柯林斯、吉尔·莱昂纳德）和我的出版编辑（伊丽莎白·凯利）。莉兹·惠勒是我合作过的最挑剔的校对编辑。妮可·巴特菲尔德是一位强大的力量，她从想法到最终产品监督了这本书。
- en: This book, after all, is an accumulation of invaluable lessons I learned throughout
    my career. I owe these lessons to my extremely competent and patient coworkers
    and former coworkers. Every person I’ve worked with has taught me something new
    about bringing ML into the world.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，这本书是我整个职业生涯中学到的宝贵经验的积累。我非常感谢我的极富能力和耐心的同事和前同事。与我共事过的每一个人都教会了我如何将机器学习带入现实世界的新知识。
- en: ^([1](preface01.html#id531-marker)) An author of the AlexNet paper, Ilya Sutskever,
    went on to cofound OpenAI, turning this lesson into reality with GPT models.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](preface01.html#id531-marker)) AlexNet论文的作者伊利亚·苏茨克维尔继续共同创立了OpenAI，通过GPT模型将这个教训变成了现实。
- en: ^([2](preface01.html#id532-marker)) Even [my small project in 2017](https://x.com/chipro/status/937384141791698944),
    which used a language model to evaluate translation quality, concluded that we
    needed “a better language model.”
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](preface01.html#id532-marker)) 即使是我在2017年的一个小项目[我的小项目](https://x.com/chipro/status/937384141791698944)，该项目使用语言模型来评估翻译质量，也得出我们需要“更好的语言模型”的结论。
- en: ^([3](preface01.html#id533-marker)) Teaching a course on how to use TensorFlow
    in 2017 taught me a painful lesson about how quickly tools and tutorials become
    outdated.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](preface01.html#id533-marker)) 在2017年教授如何使用TensorFlow的课程让我深刻地认识到工具和教程更新换代的速度有多快。
