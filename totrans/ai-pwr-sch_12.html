<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">10</span> </span> <span class="chapter-title-text">Learning to rank for generalizable search relevance</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">An introduction to machine-learned ranking, also known as learning to rank (LTR)</li>
<li class="readable-text" id="p3">How LTR differs from other machine learning methods</li>
<li class="readable-text" id="p4">Training and deploying a ranking classifier</li>
<li class="readable-text" id="p5">Feature engineering, judgment lists, and integrating machine-learned ranking models into a search engine</li>
<li class="readable-text" id="p6">Validating an LTR model using a train/test split</li>
<li class="readable-text" id="p7">Performance tradeoffs for LTR-based ranking models</li>
</ul>
</div>
<div class="readable-text" id="p8">
<p>It’s a random Tuesday. You review your search logs, and the searches range from the frustrated runner’s <code>polar m430 running watch charger</code> query to the worried hypochondriac’s <code>weird bump</code> <code>on nose</code> <code>-</code> <code>cancer?</code> to the curious cinephile’s <code>william shatner</code> <code>first</code> <code>film</code>. Even though these may be one-off queries, you know each user expects nothing less than amazing search results.</p>
</div>
<div class="readable-text intended-text" id="p9">
<p>You feel hopeless. You know many query strings, by themselves, are distressingly rare. You have very little click data to know what’s relevant for these searches. Every day gets more challenging: trends, use cases, products, user interfaces, and even user terminology evolve. How can anyone hope to build search that amazes when users seem to constantly surprise us with new ways of searching?</p>
</div>
<div class="readable-text intended-text" id="p10">
<p>Despair not, there is hope! In this chapter, we’ll introduce generalizable relevance models. These models learn the underlying patterns that drive relevance ranking. Instead of memorizing that the article entitled “Zits: bumps on nose” is the answer for the query <code>weird</code> <code>bump</code> <code>on</code> <code>nose</code> <code>-</code> <code>cancer?</code> we observe the underlying pattern—that a strong title match corresponds to high probability of relevance. If we can learn these patterns and encode them into a model, we can give relevant results <em>even for search queries we’ve never seen</em>.</p>
</div>
<div class="readable-text intended-text" id="p11">
<p>This chapter explores <em>learning to rank</em> (LTR): a technique that uses machine learning to create generalizable relevance ranking models. We’ll prepare, train, and search with LTR models using the search engine.</p>
</div>
<div class="readable-text" id="p12">
<h2 class="readable-text-h2" id="sigil_toc_id_135"><span class="num-string">10.1</span> What is LTR?</h2>
</div>
<div class="readable-text" id="p13">
<p>Let’s explore what LTR does. We’ll see how LTR creates generalizable ranking models by finding patterns that predict relevance. We’ll then explore more of the nuts and bolts of building a model.</p>
</div>
<div class="readable-text" id="p14">
<h3 class="readable-text-h3" id="sigil_toc_id_136"><span class="num-string">10.1.1</span> Moving beyond manual relevance tuning</h3>
</div>
<div class="readable-text" id="p15">
<p>Recall manual relevance tuning from chapter 3. We observe factors that correspond with relevant results, and we combine those factors mathematically into a <em>ranking function</em>. The ranking function returns a relevance score that orders results as closely as possible to our ideal ranking.</p>
</div>
<div class="readable-text intended-text" id="p16">
<p>For example, consider a movie search engine with documents like those in the following listing. This document comes from TheMovieDB (tmdb) corpus (<a href="http://themoviedb.org">http://themoviedb.org</a>), which we’ll use in this chapter. If you wish to follow along with the code for this chapter, use this chapter’s first notebook to index the tmdb dataset.</p>
</div>
<div class="browsable-container listing-container" id="p17">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.1</span> A document for the movie <em>The Social Network</em></h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">{"title": ["The Social Network"],
 "overview": ["On a fall night in 2003, Harvard undergrad and computer
   <span class="">↪</span>programming genius Mark Zuckerberg sits down at his computer and
   <span class="">↪</span>heatedly begins working on a new idea. In a fury of blogging and
   <span class="">↪</span>programming, what begins in his dorm room as a small site among
   <span class="">↪</span>friends soon becomes a global social network and a revolution in
   <span class="">↪</span>communication. A mere six years and 500 million friends later,
   <span class="">↪</span>Mark Zuckerberg is the youngest billionaire in history... but for
   <span class="">↪</span>this entrepreneur, success leads to both personal and legal
   <span class="">↪</span>complications."],
 "tagline": ["You don't get to 500 million friends without making a few
   <span class="">↪</span>enemies."],
 "release_year": 2010}</pre>
</div>
</div>
<div class="readable-text" id="p18">
<p>Through endless iterations and tweaks, we might arrive at a generalizable movie ranking function that looks something like the next listing.</p>
</div>
<div class="browsable-container listing-container" id="p19">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.2</span> A generalizable ranking function using manual boosts</h5>
<div class="code-area-container code-area-with-html">
<pre class="code-area">keywords = "some example keywords"
{"query": f"title:({keywords})^10 overview:({keywords})^20
<span class="">↪</span>{!func}release_year^0.01"}</pre>
</div>
</div>
<div class="readable-text" id="p20">
<p>Manually optimizing the feature weights of general ranking functions like this to work over many queries can take significant effort, but such optimizations are perfect for machine learning.</p>
</div>
<div class="readable-text intended-text" id="p21">
<p>This is where LTR comes in—it takes our proposed relevance factors and learns an optimal ranking function. LTR takes several forms, from a simple set of linear weights (like the boosts here) to a complex deep learning model.</p>
</div>
<div class="readable-text intended-text" id="p22">
<p>To learn the ropes, we’ll build a simple LTR model in this chapter. We’ll find the optimal weights for <code>title</code>, <code>overview</code>, and <code>release_year</code> in a scoring function like the one in listing 10.2. With this relatively simple task, we’ll see the full lifecycle of developing an LTR solution.</p>
</div>
<div class="readable-text" id="p23">
<h3 class="readable-text-h3" id="sigil_toc_id_137"><span class="num-string">10.1.2</span> Implementing LTR in the real world</h3>
</div>
<div class="readable-text" id="p24">
<p>As we continue to define LTR at a high level, let’s quickly clarify where LTR fits into the overall picture of a search system. Then we can look at the kinds of data we’ll need to build an LTR model.</p>
</div>
<div class="readable-text intended-text" id="p25">
<p>We’ll focus on building LTR for production search systems, which can be quite different from a research context. We not only need relevant results, but results returned suitably quickly, with mainstream, well-understood search techniques.</p>
</div>
<div class="readable-text intended-text" id="p26">
<p>Conceptually, invoking LTR usually involves three high-level steps:</p>
</div>
<ol>
<li class="readable-text" id="p27"> Training an LTR model </li>
<li class="readable-text" id="p28"> Deploying the model to production </li>
<li class="readable-text" id="p29"> Using the model to rank (or rerank) search results </li>
</ol>
<div class="readable-text" id="p30">
<p>Most modern search engines support deploying ranking models directly into the search engine, allowing the LTR model to be invoked efficiently “where the data lives”. Usually, LTR models are significantly slower at ranking than basic keyword-based ranking functions like BM25, so LTR models are often only invoked for subsequent-pass ranking (or reranking) on a subset of the top search results ranked by an initial, faster ranking function. Pushing the LTR model into the engine (if supported) prevents the need to return hundreds or thousands of documents and their metadata from the search engine to an external model service for reranking, which can be slow and inefficient compared to doing the work in-engine and at scale.</p>
</div>
<div class="readable-text intended-text" id="p31">
<p>For this reason, our <code>ltr</code> library in this chapter implements pluggable support for deploying and invoking each supported search engine or vector database’s native LTR model integration capabilities when available. The code in each listing will work with any supported engine (see appendix B to change it), but the listing output you’ll see in this chapter will reflect Solr’s LTR implementation, since Solr is configured by default. If you change the engine, you’ll see the output from your chosen engine when you run the Jupyter notebooks.</p>
</div>
<div class="readable-text intended-text" id="p32">
<p>Solr was one of the first major open source search engines to natively support LTR model serving, with the capabilities later being ported to a community-developed Elasticsearch LTR plugin (<a href="https://github.com/o19s/elasticsearch-learning-to-rank">https://github.com/o19s/elasticsearch-learning-to-rank</a>) and then forked to the OpenSearch LTR plugin (<a href="https://github.com/opensearch-project/opensearch-learning-to-rank-base">https://github.com/opensearch-project/opensearch-learning-to-rank-base</a>). As such, the Elasticsearch and OpenSearch LTR plugins implement nearly identical concepts as those in Solr. Vespa implements phased ranking (reranking) and the ability to invoke models during each phase, and Weaviate also implements various reranking capabilities. Other engines that support native LTR will follow similar patterns.</p>
</div>
<div class="readable-text intended-text" id="p33">
<p>Figure 10.1 outlines the workflow for developing a practical LTR solution.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p34">
<img alt="figure" height="701" src="../Images/CH10_F01_Grainger.png" width="1016"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.1</span> LTR systems transform our training data (judgment lists) into models that generalize relevance ranking. This type of system lets us find the underlying patterns in our training data.</h5>
</div>
<div class="readable-text" id="p35">
<p>You may notice similarities between LTR and traditional machine learning–based classification or regression system workflows. But the exceptions are what make it interesting. Table 10.1 maps definitions between traditional machine learning objectives and LTR.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p36">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 10.1</span> Traditional machine learning vs. LTR</h5>
<table>
<thead>
<tr>
<th>
<div>
         Concept 
       </div></th>
<th>
<div>
         Traditional machine learning 
       </div></th>
<th>
<div>
         LTR 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Training data <br/></td>
<td>  Set of historical or “true” examples the model should try to predict, e.g., stock prices on past days, like “Apple” was $125 on June 6th, 2021. <br/></td>
<td>  A <em>judgment list</em>: A <em>judgment</em> simply labels a document as relevant or irrelevant for a query. In figure 10.2, <em>Return of the Jedi</em> is labeled relevant ( <code>grade</code> of <code>1</code>), for the query <code>star wars</code>. <br/></td>
</tr>
<tr>
<td>  Feature <br/></td>
<td>  The data we can use to predict the training data, e.g., Apple had 147,000 employees and revenue of $90 billion. <br/></td>
<td>  Data used so that relevant results rank higher than irrelevant ones and, ideally, values the search engine can compute quickly. Our features are search queries like <code>title:({keywords})</code> from listing 10.2. <br/></td>
</tr>
<tr>
<td>  Model <br/></td>
<td>  The algorithm that takes features as input to make a prediction. Given that Apple has 157,000 employees on July 6th, 2021, with $95 billion in revenue, the model might predict a stock price of $135 for that date. <br/></td>
<td>  Combines the ranking features (search queries) together to assign a relevance <em>score</em> to each potential search result. Results are sorted by score descending, hopefully placing more relevant results first. <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p37">
<p>This chapter follows the steps in figure 10.1 to train an LTR model:</p>
</div>
<ol>
<li class="readable-text" id="p38"> <em>Gather judgments</em><em> </em>—We derive judgments from clicks or other sources. We’ll cover this step in depth in chapter 11. </li>
<li class="readable-text" id="p39"> <em>Feature logging</em><em> </em>—To train a model, we must combine the judgments with features to see the overall pattern. This step requires us to ask the search engine to store and compute queries representing the features. </li>
<li class="readable-text" id="p40"> <em>Transform to a traditional machine learning problem</em><em> </em>—You’ll see that most LTR really is about translating the ranking task into something that looks more like the “traditional machine learning” column in table 10.1. </li>
<li class="readable-text" id="p41"> <em>Train and evaluate the model</em><em> </em>—Here we construct our model and confirm that it is, indeed, generalizable, and thus will perform well for queries it hasn’t seen. </li>
<li class="readable-text" id="p42"> <em>Store the model</em><em> </em>—We upload the model to our search infrastructure, tell the search engine which features to use as input, and enable it for users to use in their searches. </li>
<li class="readable-text" id="p43"> <em>Search using the model</em><em> </em>—We finally can execute searches using the model! </li>
</ol>
<div class="readable-text" id="p44">
<p>The rest of the chapter will walk through each of these steps in detail to build our first LTR implementation. Let’s get cracking!</p>
</div>
<div class="readable-text" id="p45">
<h2 class="readable-text-h2" id="sigil_toc_id_138"><span class="num-string">10.2</span> Step 1: A judgment list, starting with the training data</h2>
</div>
<div class="readable-text" id="p46">
<p>You already saw what LTR is at a high level, so let’s get into the nitty-gritty. Before implementing LTR, we must first learn about the data used to train an LTR model: the judgment list. </p>
</div>
<div class="readable-text intended-text" id="p47">
<p>A <em>judgment list</em> is a list of relevance labels or <em>grades</em>, each indicating the relevance of a document to a query. Grades can come in a variety of forms. For now, we’ll stick to simple <em>binary judgments</em><em> </em>—a <code>0</code> indicates an irrelevant document and a <code>1</code> indicates a relevant one. </p>
</div>
<div class="readable-text intended-text" id="p48">
<p>Using the <code>Judgment</code> class provided with this book’s code, we’ll label <em>The Social Network</em> as relevant for the query <code>social network</code> by creating a <code>Judgment</code>:</p>
</div>
<div class="browsable-container listing-container" id="p49">
<div class="code-area-container">
<pre class="code-area">from ltr.judgments import Judgment
Judgment(grade=1, keywords="social network", doc_id=37799)</pre>
</div>
</div>
<div class="readable-text" id="p50">
<p>It’s more interesting to look over multiple queries. In listing 10.3, we have <code>social network</code> and <code>star wars</code> as two different queries, with movies graded as relevant or irrelevant.</p>
</div>
<div class="browsable-container listing-container" id="p51">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.3</span> Labeling movie judgments as relevant or irrelevant</h5>
<div class="code-area-container">
<pre class="code-area">sample_judgments = [
  # for 'social network' query
  Judgment(1, "social network", 37799),  # The Social Network
  Judgment(0, "social network", 267752), # #chicagoGirl
  Judgment(0, "social network", 38408),  # Life As We Know It
  Judgment(0, "social network", 28303),  # The Cheyenne Social Club

  # for 'star wars' query
  Judgment(1, "star wars", 11),     # Star Wars
  Judgment(1, "star wars", 1892),   # Return of the Jedi
  Judgment(0, "star wars", 54138),  # Star Trek Into Darkness
  Judgment(0, "star wars", 85783),  # The Star
  Judgment(0, "star wars", 325553)  # Battlestar Galactica
]</pre>
</div>
</div>
<div class="readable-text" id="p52">
<p>You can see that we labeled <em>Star Trek into Darkness</em> and <em>Battlestar Galactica</em> as irrelevant for the query <code>star wars</code>, but <em>Return of the Jedi</em> as relevant.</p>
</div>
<div class="readable-text intended-text" id="p53">
<p>You’re hopefully asking yourself “where did these grades come from?” Hand labeled by movie experts? Based on user clicks? Good questions! Creating a good training set, based on user interactions with search results, is crucial for getting LTR to work well. To get training data in bulk, we usually derive these labels from click traffic using a type of algorithm known as a <em>click model</em>. As this step is so foundational, we’ll dedicate all of chapter 11 to diving deeper into the topic. In this chapter, however, we’ll start with manually labeled judgments so we can initially focus on the mechanics of LTR. </p>
</div>
<div class="readable-text intended-text" id="p54">
<p>Each judgment also has a <code>features</code> vector, which can be used to train a model. The first feature in the <code>features</code> vector could be made to correspond to the <code>title</code> BM25 score, the second to the <code>overview</code> BM25 score, and so on. We haven’t populated the <code>features</code> vectors yet, so if you inspect <code>sample_judgments[0].features</code>, it’s currently empty (<code>[]</code>). </p>
</div>
<div class="readable-text intended-text" id="p55">
<p>Let’s use the search engine to gather some features. </p>
</div>
<div class="readable-text" id="p56">
<h2 class="readable-text-h2" id="sigil_toc_id_139"><span class="num-string">10.3</span> Step 2: Feature logging and engineering</h2>
</div>
<div class="readable-text" id="p57">
<p>Feature engineering requires identifying patterns between document attributes and relevance. For example, we might hypothesize that “relevant results in our judgments correspond to strong title matches”. In this case, “title match” would be a feature we’d need to define. In this section, you’ll see what features (like “title match”) are and how to use a modern search engine to engineer and extract these features from a corpus. </p>
</div>
<div class="readable-text intended-text" id="p58">
<p>For the purposes of LTR, a <em>feature</em> is some numerical attribute of the document, the query, or the query-document relationship. Features are the mathematical building blocks we use to build a ranking function. You’ve already seen a manual ranking function with features in listing 10.2: the keyword score in the <code>title</code> field is one such feature, as are the <code>release_year</code> and <code>overview</code> keyword scores:</p>
</div>
<div class="browsable-container listing-container" id="p59">
<div class="code-area-container code-area-with-html">
<pre class="code-area">{"query": f"title:({keywords})^10 overview:({keywords})^20
<span class="">↪</span>{!func}release_year^0.01"}</pre>
</div>
</div>
<div class="readable-text" id="p60">
<p>Of course, the features you end up using could be more complex or domain-specific, such as the commute distance in a job search, or some knowledge graph relationship between query and document. Anything you can compute relatively quickly when a user searches might be a reasonable feature. </p>
</div>
<div class="readable-text intended-text" id="p61">
<p><em>Feature logging</em> takes a judgment list and computes features for each labeled query-document pair. If we computed the values of each component of listing 10.2 for the query <code>social network</code>, we would arrive at something like table 10.2.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p62">
<h5 class="browsable-container-h5 sigil_not_in_toc"><span class="num-string">Table 10.2</span> Features logged for the keywords <code>social network</code> for relevant (<code>grade=1</code>) and irrelevant (<code>grade=0</code>) documents</h5>
<table>
<thead>
<tr>
<th>
<div>
         Grade 
       </div></th>
<th>
<div>
         Movie 
       </div></th>
<th>
<div>
<strong><code>title:({keywords})</code></strong>
</div></th>
<th>
<div>
<strong><code>overview: ({keywords})</code></strong>
</div></th>
<th>
<div>
<strong><code>{!func}release_year</code></strong>
</div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  1 <br/></td>
<td>  Social Network <br/></td>
<td>  8.243603 <br/></td>
<td>  3.8143613 <br/></td>
<td>  2010.0 <br/></td>
</tr>
<tr>
<td>  0 <br/></td>
<td>  #chicagoGirl <br/></td>
<td>  0.0 <br/></td>
<td>  6.0172443 <br/></td>
<td>  2013.0 <br/></td>
</tr>
<tr>
<td>  0 <br/></td>
<td>  Life As We Know It <br/></td>
<td>  0.0 <br/></td>
<td>  4.353118 <br/></td>
<td>  2010.0 <br/></td>
</tr>
<tr>
<td>  0 <br/></td>
<td>  The Cheyenne Social Club <br/></td>
<td>  3.4286604 <br/></td>
<td>  3.1086721 <br/></td>
<td>  1970.0 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p63">
<p>A machine learning algorithm might examine the feature values from table 10.2 and converge on a good ranking function. From just the data in table 10.2, it seems such an algorithm might produce a ranking function with a higher weight for the <code>title</code> feature and lower weights for the other features. </p>
</div>
<div class="readable-text intended-text" id="p64">
<p>Before we get to the algorithms, however, we need to examine the feature logging workflow in a production search system.</p>
</div>
<div class="readable-text" id="p65">
<h3 class="readable-text-h3" id="sigil_toc_id_140"><span class="num-string">10.3.1</span> Storing features in a modern search engine</h3>
</div>
<div class="readable-text" id="p66">
<p>Modern search engines that support LTR help us store, manage, and extract features. Engines like Solr, Elasticsearch, and OpenSearch track features in a <em>feature store</em>—a list of named features. It’s crucial that we log features for training in a manner consistent with how the search engine will execute the model. </p>
</div>
<div class="readable-text intended-text" id="p67">
<p>As shown in listing 10.4, we generate and upload features to the search engine. We use a generic feature store abstraction in the book’s codebase, allowing us to generate various search-based features and upload them as a <em>feature set</em> to the feature store of a supported search engine. Here we create three features: a title field relevance score <code>title_bm25</code>, an overview field relevance score <code>overview_bm25</code>, and the value of the <code>release_year</code> field. BM25 here corresponds to the BM25-based scoring defined in chapter 3, which will be our default method for scoring term matches in text fields. </p>
</div>
<div class="browsable-container listing-container" id="p68">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.4</span> Creating three features for LTR</h5>
<div class="code-area-container">
<pre class="code-area">feature_set = [
  ltr.generate_query_feature(feature_name="title_bm25",
                             field_name="title"),
  ltr.generate_query_feature(feature_name="overview_bm25",
                             field_name="overview"),
  ltr.generate_field_value_feature(feature_name="release_year",
                                   field_name="release_year")]

ltr.upload_features(features=feature_set, model_name="movie_model")
display(feature_set)</pre>
</div>
</div>
<div class="readable-text" id="p69">
<p>Engine-specific feature set definition (for <code>engine=solr</code>):</p>
</div>
<div class="browsable-container listing-container" id="p70">
<div class="code-area-container">
<pre class="code-area">[{"name": "title_bm25", <span class="aframe-location"/> #1
  "store": "movies", <span class="aframe-location"/> #2
  "class": "org.apache.solr.ltr.feature.SolrFeature",
  "params": {"q": "title:(${keywords})"}}, <span class="aframe-location"/> #3
 {"name": "overview_bm25",<span class="aframe-location"/> #4
  "store": "movies",
  "class": "org.apache.solr.ltr.feature.SolrFeature",
  "params": {"q": "overview:(${keywords})"}},
 {"name": "release_year",<span class="aframe-location"/> #5
  "store": "movies",
  "class": "org.apache.solr.ltr.feature.SolrFeature",
  "params": {"q": "{!func}release_year"}}] <span class="aframe-location"/> #6</pre>
<div class="code-annotations-overlay-container">
     #1 The name of the feature
     <br/>#2 The feature store where the feature will be saved
     <br/>#3 A parametrized feature taking the keywords (e.g., star wars) and searching the title field
     <br/>#4 Another feature that searches against the overview field
     <br/>#5 A document-only feature, the release_year of the movie
     <br/>#6 params are the same params for a Solr query, allowing you to use the full power of Solr’s extensive Query DSL to craft features.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p71">
<p>The output of listing 10.4 shows the feature set that’s uploaded to the search engine—in this case, a Solr feature set. This output will obviously look different based on which search engine implementation you configure (as discussed in appendix B). The first two features are parameterized: they each take the search keywords (<code>social</code> <code>network</code>, <code>star wars</code>) and execute a search on the corresponding field. The final one is a field value feature utilizing the release year of a movie, which will boost more recent movies higher. </p>
</div>
<div class="readable-text" id="p72">
<h3 class="readable-text-h3" id="sigil_toc_id_141"><span class="num-string">10.3.2</span> Logging features from our search engine corpus</h3>
</div>
<div class="readable-text" id="p73">
<p>With features loaded into the search engine, our next focus will be to log features for every row in our judgment list. After we get this last bit of plumbing out of the way, we will then train a model that can observe relationships between each relevant and irrelevant document for each query. </p>
</div>
<div class="readable-text intended-text" id="p74">
<p>For each unique query in our judgment list, we need to extract the features for the query’s graded documents. For the query <code>social network</code> in the sample judgment list from listing 10.3, we have one relevant document (37799) and three irrelevant documents (267752, 38408, and 28303).</p>
</div>
<div class="readable-text intended-text" id="p75">
<p>The following listing shows an example of feature logging for the query <code>social network</code>.</p>
</div>
<div class="browsable-container listing-container" id="p76">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.5</span> Logging feature values for <code>social network</code> results</h5>
<div class="code-area-container">
<pre class="code-area">ids = ["37799", "267752", "38408", "28303"] <span class="aframe-location"/> #1
options = {"keywords": "social network"}
ltr.get_logged_features("movie_model", ids, <span class="aframe-location"/> #2
                        options=options, )  #2
                        fields=["id", "title"])  #2
display(response)</pre>
<div class="code-annotations-overlay-container">
     #1 Relevant and irrelevant documents for the “social network” query
     <br/>#2 Queries the search engine for feature values contained in the movies feature store
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p77">
<p>Engine-specific search request (for <code>engine=solr</code>):</p>
</div>
<div class="browsable-container listing-container" id="p78">
<div class="code-area-container">
<pre class="code-area">{"query": "{!terms f=id}37799,267752,38408,28303",
 "fields": ["id", "title",
   '[features store=movies efi.keywords="social network"]']} <span class="aframe-location"/> #1</pre>
<div class="code-annotations-overlay-container">
     #1 Example Solr query syntax to retrieve feature values from each returned document
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p79">
<p>Documents with logged features:</p>
</div>
<div class="browsable-container listing-container" id="p80">
<div class="code-area-container">
<pre class="code-area">[{"id": "37799",
  "title": "The Social Network",
  "[features]": {"title_bm25": 8.243603,<span class="aframe-location"/> #1
                 "overview_bm25": 3.8143613,  #1
                 "release_year": 2010.0}},  #1
 {"id": "267752",
  "title": "#chicagoGirl",
  "[features]": {"title_bm25": 0.0,
                 "overview_bm25": 6.0172443,
                 "release_year": 2013.0}},
 {"id": "38408",
  "title": "Life As We Know It",
  "[features]": {"title_bm25": 0.0,
                 "overview_bm25": 4.353118,
                 "release_year": 2010.0}},
 {"id": "28303",
  "title": "The Cheyenne Social Club",
  "[features]": {"title_bm25": 3.4286604,
                 "overview_bm25": 3.1086721,
                 "release_year": 1970.0}}]</pre>
<div class="code-annotations-overlay-container">
     #1 Each feature value logged for this movie for the “social network” query
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p81">
<p>Notice that the search request (for Solr in this case) in listing 10.5 has a return field containing square brackets. This syntax tells Solr to return an extra field on each document containing the feature data defined in the feature store (the <code>movies</code> feature store in this case). The <code>efi</code> parameter stands for <em>external feature information</em>, and it’s used here to pass the keyword query (<code>social network</code>) and any additional query-time information needed to compute each feature. The response contains the four requested documents with their corresponding features. These parameters will be different for each search engine, but the concepts will be similar. </p>
</div>
<div class="readable-text intended-text" id="p82">
<p>With some mundane Python data transformation, we can fill in the features for the query <code>social network</code> in our training set from this response. In listing 10.6, we apply feature data to judgments for the query <code>social network</code>:</p>
</div>
<div class="browsable-container listing-container" id="p83">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.6</span> Judgments with logged features for query <code>social network</code></h5>
<div class="code-area-container">
<pre class="code-area">[Judgment(grade=1, keywords="social network", doc_id=37799, qid=1,
          features=[8.243603, 3.8143613, 2010.0], weight=1), <span class="aframe-location"/> #1
 Judgment(0, "social network", 267752, 1, [0.0, 6.0172443, 2013.0], 1),
 Judgment(0, "social network", 38408, 1, [0.0, 4.353118, 2010.0],1),<span class="aframe-location"/> #2
 Judgment(0, "social network", 28303, 1, [3.4286604, 3.1086721, 1970.0], 1)]</pre>
<div class="code-annotations-overlay-container">
     #1 Judgment for the movie The Social Network relative to the “social network” query, including the logged feature values
     <br/>#2 An irrelevant document for the “social network” query (note the low first feature value, the title_bm25 score of 0.0)
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p84">
<p>In listing 10.6, as we might expect, the first feature value corresponds to the first feature in our feature store (<code>title_bm25</code>), the second value to the second feature in our feature store (<code>overview_bm25</code>), and so on. Let’s repeat the process of logging features for judgments for the query <code>star wars</code>.</p>
</div>
<div class="browsable-container listing-container" id="p85">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.7</span> Logged judgments for the query <code>star wars</code></h5>
<div class="code-area-container">
<pre class="code-area">[Judgment(1, "star wars", 11, 2, [6.7963624, 0.0, 1977.0], 1),
 Judgment(1, "star wars", 1892, 2, [0.0, 1.9681965, 1983.0], 1),
 Judgment(0, "star wars", 54138, 2, [2.444128, 0.0, 2013.0], 1),
 Judgment(0, "star wars", 85783, 2, [3.1871135, 0.0, 1952.0], 1),
 Judgment(0, "star wars", 325553, 2, [0.0, 0.0, 2003.0], 1)]</pre>
</div>
</div>
<div class="readable-text" id="p86">
<p>With the ability to generate logged judgments, let’s expand the judgment list to about a hundred movie queries, each with about 40 movies graded as relevant/irrelevant. Code for loading and logging features for this larger training set essentially repeats the search engine request shown in listing 10.5. The end result of this feature logging looks just like listing 10.7, but created from a much larger judgment list.</p>
</div>
<div class="readable-text intended-text" id="p87">
<p>We’ll move on next to consider how to handle the problem of ranking as a machine learning problem. </p>
</div>
<div class="readable-text" id="p88">
<h2 class="readable-text-h2" id="sigil_toc_id_142"><span class="num-string">10.4</span> Step 3: Transforming LTR to a traditional machine learning problem</h2>
</div>
<div class="readable-text" id="p89">
<p>In this section, we’re going to explore ranking as a machine learning problem. This will help us understand how to apply well-known, traditional machine learning concepts to our LTR task. </p>
</div>
<div class="readable-text intended-text" id="p90">
<p>The task of LTR is to look over many relevant and irrelevant training examples for a query and then build a model to bring more relevant documents to the top (and conversely push less relevant documents down). Each training example doesn’t have much value by itself; what matters is how it’s ordered alongside its peers in a query. Figure 10.2 shows this task, with two queries. The goal is to find a scoring function that can use the features to correctly order results.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p91">
<img alt="figure" height="389" src="../Images/CH10_F02_Grainger.png" width="848"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.2</span> LTR is about placing each query’s result set in the ideal order, not about predicting individual relevance grades. That means we need to look at each query as a case unto itself.</h5>
</div>
<div class="readable-text" id="p92">
<p>Contrast LTR with a more traditional pointwise machine learning task: a task like predicting a company’s stock price as mentioned in table 10.2 earlier. <em>Pointwise machine learning</em> means that we can evaluate the model’s accuracy on each example in isolation, predicting its absolute value as opposed to its relative value versus other examples. We know, just by looking at one company, how well we predicted that company’s stock price. Compare figure 10.3 showing a pointwise task to figure 10.2. Notice in figure 10.3 that the learned function attempts to predict the stock price directly, whereas with LTR, the function’s output is only meaningful for ordering items relative to their peers for a query. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p93">
<img alt="figure" height="405" src="../Images/CH10_F03_Grainger.png" width="848"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.3</span> Pointwise machine learning tries to optimize predictions of individual points (such as a stock price or the temperature). Search relevance is a different problem than pointwise prediction. Instead, we need to optimize a ranking of examples grouped by a search query.</h5>
</div>
<div class="readable-text" id="p94">
<p>LTR targets a very different objective (ranking multiple results) than pointwise machine learning (predicting specific values of results). Most LTR methods use clever alchemy to transmogrify this “ranking of pairs” task into a classification task per document that learns to predict which features and feature weights best separate “relevant” from “irrelevant” documents. This transformation is the key to building a generalizable LTR model that can operate on specific documents as opposed to only pairs of documents. We’ll look at one model’s method for transforming the ranking task in the next section by exploring a popular LTR model named SVMrank.</p>
</div>
<div class="readable-text" id="p95">
<h3 class="readable-text-h3" id="sigil_toc_id_143"><span class="num-string">10.4.1</span> SVMrank: Transforming ranking to binary classification</h3>
</div>
<div class="readable-text" id="p96">
<p>At the core of LTR is the model: the actual algorithm that learns the relationship between relevance/irrelevance and the features like <code>title_bm25</code>, <code>overview_bm25</code>, etc. In this section, we’ll explore one such model, SVMrank, first understanding what “SVM” stands for and then how it can be used to build a great, generalizable LTR model. </p>
</div>
<div class="readable-text intended-text" id="p97">
<p>SVMrank transforms relevance into a binary classification problem. <em>Binary classification</em> simply means classifying items as one of two classes (like “relevant” versus “irrelevant”, “adult” versus “child”, “dog” versus “cat”) using the available features. </p>
</div>
<div class="readable-text intended-text" id="p98">
<p>An <em>SVM</em> or <em>support vector machine</em> is one method of performing binary classification. We won’t go in-depth into SVMs, as you need not be a machine learning expert to follow the discussion. Nevertheless, if you want to get a deeper overview of SVMs, you can look at a book such as <em>Grokking Machine Learning</em> by Luis Serrano (Manning, 2021). </p>
</div>
<div class="readable-text intended-text" id="p99">
<p>Intuitively, an SVM finds the best, most generalizable hyperplane to draw between the two classes. A <em>hyperplane</em> is a boundary that separates a vector space into two parts. A 1D point can be a hyperplane separating a 2D line into two parts, just as a line can be a hyperplane separating a 3D space into two parts. A plane is usually a 3D boundary separating a 4D space. All of these, as well as boundaries even greater than three dimensions are generically referred to as hyperplanes. </p>
</div>
<div class="readable-text intended-text" id="p100">
<p>As an example, if we were trying to build a model to predict whether an animal is a dog or cat, we might look at a 2D graph of the heights and weights of known dogs or cats and draw a line separating the two classes as shown in figure 10.4.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p101">
<img alt="figure" height="556" src="../Images/CH10_F04_Grainger.png" width="774"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.4</span> SVM example: Is an animal a dog or a cat? This hyperplane (the line here) separates these two cases based on two features: height and weight. Soon you’ll see how we might do something similar to separate relevant and irrelevant search results for a query.</h5>
</div>
<div class="readable-text" id="p102">
<p>A good separating hyperplane drawn between the classes attempts to minimize the mistakes it makes in classifying the training data (fewer dogs on the cat side and vice versa). We also want a hyperplane that is <em>generalizable</em>, meaning that it will probably do a good job of classifying animals that weren’t seen during training. After all, what good is a model if it can’t make predictions about new data? It wouldn’t be very AI-powered!</p>
</div>
<div class="readable-text intended-text" id="p103">
<p>Another detail to know about SVMs is that they can be sensitive to the range of our features. For example, imagine if the <code>height</code> feature was millimeters instead of centimeters, like in figure 10.5. It forces the data to stretch out on the <em>x</em>-axis, and the separating hyperplane looks quite different!<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p104">
<img alt="figure" height="140" src="../Images/CH10_F05_Grainger.png" width="1075"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.5</span> Separating hyperplane affected by the range of one of the features. This causes SVMs to be sensitive to the range of features, and thus we need to normalize the features so one feature doesn’t create undue influence on the model.</h5>
</div>
<div class="readable-text" id="p105">
<p>SVMs work best when our data is normalized. <em>Normalization</em> just means scaling features to a comparable range. We’ll normalize our data by mapping <code>0</code> to the mean of the feature values. If the average <code>release_year</code> is <code>1990</code>, movies released in 1990 will normalize to <code>0</code>. We’ll also map <code>+1</code> and <code>-1</code> to one standard deviation above or below the mean. So if the standard deviation of movie release years is 22 years, then movies in 2012 turn into a <code>1.0</code>; movies in 1968 turn into a <code>-1.0</code>. We can repeat this for <code>title_bm25</code> and <code>overview_bm25</code> using those features’ means and standard deviations in our training data. This helps make the features a bit more comparable when finding a separating hyperplane. </p>
</div>
<div class="readable-text intended-text" id="p106">
<p>With that brief background out of the way, let’s now explore how SVMrank can create a generalizable model to distinguish relevant from irrelevant documents, even for queries it has never seen before. </p>
</div>
<div class="readable-text" id="p107">
<h3 class="readable-text-h3" id="sigil_toc_id_144"><span class="num-string">10.4.2</span> Transforming our LTR training task to binary classification</h3>
</div>
<div class="readable-text" id="p108">
<p>With LTR, we must reframe the task from ranking to a traditional machine learning task. In this section, we’ll explore how SVMrank transforms ranking into a binary classification task suitable for an SVM. </p>
</div>
<div class="readable-text intended-text" id="p109">
<p>Before we get started, let’s inspect the fully logged training set from the end of step 2 for our two favorite queries, <code>star</code> <code>wars</code> and <code>social</code> <code>network</code>. In this section, we’ll focus on just two features (<code>title_bm25</code> and <code>overview_bm25</code>) to help us explore feature relationships graphically. Figure 10.6 shows these two features for every graded document for the <code>star</code> <code>wars</code> and <code>social</code> <code>network</code> queries, labeling some prominent movies from the training set.</p>
</div>
<div class="browsable-container figure-container" id="p110">
<img alt="figure" height="728" src="../Images/CH10_F06_Grainger.png" width="765"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.6</span> Logged feature scores for <code>social network</code> and <code>star wars</code> queries<span class="aframe-location"/></h5>
</div>
<div class="readable-text" id="p111">
<h4 class="readable-text-h4 sigil_not_in_toc">First, normalize the LTR features</h4>
</div>
<div class="readable-text" id="p112">
<p>Our first step is to normalize each feature. The following listing takes the logged output from step 2 and normalizes features into <code>normed_judgments</code>. </p>
</div>
<div class="browsable-container listing-container" id="p113">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.8</span> Normalizing logged LTR training data</h5>
<div class="code-area-container">
<pre class="code-area">means, std_devs, normed_judgments = normalize_features(logged_judgments)
print(logged_judgments[360])
print(normed_judgments[360])</pre>
</div>
</div>
<div class="readable-text" id="p114">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p115">
<div class="code-area-container">
<pre class="code-area">#Judgment(grade, keywords, doc_id,
#         qid, features, weight)
Judgment(1, "social network", 37799, <span class="aframe-location"/> #1
         11, [8.244, 3.814, 2010.0], 1)  #1
Judgment(1, "social network", 37799, <span class="aframe-location"/> #2
         11, [4.483, 2.100, 0.835], 1)  #2</pre>
<div class="code-annotations-overlay-container">
     #1 Unnormalized example, with raw title_bm25, overview_bm25, and release_year
     <br/>#2 Same judgment, but normalized
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p116">
<p>You can see that the output from listing 10.8 shows first the logged BM25 scores for title and overview (<code>8.244</code>, <code>3.814</code>) alongside the release year (<code>2010</code>). These features are then normalized, where <code>8.244</code> for <code>title_bm25</code> corresponds to <code>4.483</code> standard deviations above the mean <code>title_bm25</code>, and so on for each feature.</p>
</div>
<div class="readable-text intended-text" id="p117">
<p>We’ve plotted the normalized features in figure 10.7. This looks very similar to figure 10.6, with only the scale on each axis differing.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p118">
<img alt="figure" height="663" src="../Images/CH10_F07_Grainger.png" width="765"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.7</span> Normalized <code>star wars</code> and <code>social network</code> graded movies. Each increment in the graph is a standard deviation above or below the mean.</h5>
</div>
<div class="readable-text" id="p119">
<p>Next, we’ll turn ranking into a binary classification learning problem to separate the relevant from irrelevant results. </p>
</div>
<div class="readable-text" id="p120">
<h4 class="readable-text-h4 sigil_not_in_toc">Second, compute the pairwise differences</h4>
</div>
<div class="readable-text" id="p121">
<p>With normalized data, we’ve forced features to a consistent range. Now our SVM should not be biased by features that happen to have very large ranges. In this section, we’re ready to transform the task into a binary classification problem, setting the stage for us to train our model. </p>
</div>
<div class="readable-text intended-text" id="p122">
<p>SVMrank uses a pairwise transformation to reformulate LTR to a binary classification problem. <em>Pairwise</em> simply means turning ranking into the task of minimizing out-of-order pairs for a query. </p>
</div>
<div class="readable-text intended-text" id="p123">
<p>In the rest of this section, we’ll carefully walk through SVMrank’s pairwise algorithm, outlined in listing 10.9. The SVMrank algorithm takes every judgment for each query and compares it to every other judgment for that same query. It computes the feature differences (<code>feature_deltas</code>) between every relevant and irrelevant pair for that query. When adding to <code>feature_deltas</code>, if the first judgment is more relevant than the second, it’s labeled with a <code>+1</code> in <code>predictor_deltas</code>. If the first judgment is less relevant, it is labeled with a <code>-1</code>. This pairwise transform algorithm yields training data (the <code>feature_deltas</code> and <code>predictor_deltas</code>) needed for binary classification. </p>
</div>
<div class="browsable-container listing-container" id="p124">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.9</span> Transforming features into pairwise data for SVMrank</h5>
<div class="code-area-container">
<pre class="code-area">for doc1_judgment in query_judgments:
  for doc2_judgment in query_judgments:
    j1_features = numpy.array(doc1_judgment.features)
    j2_features = numpy.array(doc2_judgment.features)

    if doc1_judgment.grade &gt; doc2_judgment.grade:
      predictor_deltas.append(+1) <span class="aframe-location"/> #1
      feature_deltas.append(j1_features - <span class="aframe-location"/> #2
                            j2_features)  #2
    elif doc1_judgment.grade &lt; doc2_judgment.grade: 
      predictor_deltas.append(-1) <span class="aframe-location"/> #3
      feature_deltas.append(j1_features -<span class="aframe-location"/> #4
                            j2_features)  #4</pre>
<div class="code-annotations-overlay-container">
     #1 Stores a label of +1 if doc1 is more relevant than doc2.
     <br/>#2 Stores the feature deltas
     <br/>#3 Stores a label of –1 if doc1 is less relevant than doc2.
     <br/>#4 Stores the feature deltas
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p125">
<p>Figure 10.8 plots the pairwise differences and highlights important points.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p126">
<img alt="figure" height="581" src="../Images/CH10_F08_Grainger.png" width="814"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.8</span> Pairwise differences after SVMrank’s transformation for <code>social</code> <code>network</code> and <code>star wars</code> documents, along with a candidate separating hyperplane.</h5>
</div>
<div class="readable-text" id="p127">
<p>You’ll notice that the positive pairwise deltas (+) tend to be toward the upper right. This means relevant documents have a higher <code>title_bm25</code> and <code>overview_bm25</code> when compared to irrelevant ones.</p>
</div>
<div class="readable-text intended-text" id="p128">
<p>That’s a lot to digest! Let’s walk through a few examples carefully, step-by-step, to see how this algorithm constructs the data points in figure 10.9. This algorithm compares relevant and irrelevant documents for each query, comparing two documents (<em>Network</em> and <em>The Social Network</em>) within the query <code>social network</code> as shown in figure 10.9.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p129">
<img alt="figure" height="437" src="../Images/CH10_F09_Grainger.png" width="653"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.9</span> Comparing <em>Network</em> to <em>The Social Network</em> for the query <code>social</code> <code>network</code></h5>
</div>
<div class="readable-text" id="p130">
<p>These are the features for <em>The Social Network</em>:</p>
</div>
<div class="browsable-container listing-container" id="p131">
<div class="code-area-container">
<pre class="code-area">#[title_bm25, overview_bm25] <span class="aframe-location"/> #1
[4.483, 2.100]  #1</pre>
<div class="code-annotations-overlay-container">
     #1 title_bm25 is 4.483 standard deviations above the mean, and overview_bm25 is 2.100 standard deviations above the mean.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p132">
<p>These are the features for <em>Network</em>:</p>
</div>
<div class="browsable-container listing-container" id="p133">
<div class="code-area-container">
<pre class="code-area">#[title_bm25, overview_bm25] <span class="aframe-location"/> #1
[3.101, 1.443]  #1</pre>
<div class="code-annotations-overlay-container">
     #1 title_bm25 is 3.101 standard deviations above the mean, and overview_bm25 is 1.443 standard deviations above the mean.
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p134">
<p>We then insert the delta between <em>The Social Network</em> and <em>Network</em> in the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p135">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.10</span> Calculating and storing the feature delta</h5>
<div class="code-area-container">
<pre class="code-area">predictor_deltas.append(+1)
feature_deltas.append([4.483, 2.100] - [3.101, 1.443])<span class="aframe-location"/> #1</pre>
<div class="code-annotations-overlay-container">
     #1 Adds [1.382, 0.657] to feature_deltas
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p136">
<p>To restate listing 10.10, we might say that here is one example of a movie, <em>The Social Network</em>, that’s more relevant than the movie <em>Network</em> for this query <code>social network</code>. Interesting! Let’s look at what makes them different. Of course, “difference” in math means subtraction, which we’ll do here. Ah yes, after taking the difference we see <em>The Social Network’</em>s <code>title_bm25</code> is <code>1.382</code> standard deviations higher than <em>Network’</em>s; similarly, the <code>overview_bm25</code> is <code>0.657</code> standard deviations higher. Indeed, note the <code>+</code> for <em>The Social Network</em> minus <em>Network</em> in figure 10.8 showing the point <code>[1.382, 0.657]</code> amongst the deltas.</p>
</div>
<div class="readable-text intended-text" id="p137">
<p>The algorithm would also note that <em>Network</em> is less relevant than <em>The Social Network</em> for the query <code>social network</code>, as shown in figure 10.10.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p138">
<img alt="figure" height="465" src="../Images/CH10_F10_Grainger.png" width="651"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.10</span> Comparing <em>Network</em> to <em>The Social Network</em> for the query <code>social network</code></h5>
</div>
<div class="readable-text" id="p139">
<p>Just as in listing 10.9, our code captures this difference in relevance between these two documents, but this time in the opposite direction (irrelevant-minus-relevant). So it’s no surprise that we see the same values, but in the negative.</p>
</div>
<div class="browsable-container listing-container" id="p140">
<div class="code-area-container">
<pre class="code-area">predictor_deltas.append(-1)
feature_deltas.append([3.101, 1.443] - [4.483, 2.100]) <span class="aframe-location"/> #1</pre>
<div class="code-annotations-overlay-container">
     #1 Evaluates to [–1.382, –0.657]
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p141">
<p>In figure 10.11, we move on to another relevant-irrelevant comparison of two documents for the query <code>social network</code>, appending another comparison to the new training set.</p>
</div>
<div class="readable-text intended-text" id="p142">
<p>Listing 10.11 shows appending both positive deltas (with the more relevant document listed first) and negative deltas (with the less relevant document listed first) for the highlighted pair of documents compared in figure 10.11.</p>
</div>
<div class="browsable-container listing-container" id="p143">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.11</span> Adding the positive and negative deltas</h5>
<div class="code-area-container">
<pre class="code-area"># Positive example
predictor_deltas.append(+1)
feature_deltas.append([4.483, 2.100] - [2.234, -0.444]) <span class="aframe-location"/> #1

# Negative example
predictor_deltas.append(-1)
feature_deltas.append([2.234, -0.444] - [4.483, 2.100]) <span class="aframe-location"/><span class="aframe-location"/> #2</pre>
<div class="code-annotations-overlay-container">
     #1 Evaluates to [2.249, 2.544]
     <br/>#2 Evaluates to [–2.249, –2.544]
     <br/>
</div>
</div>
</div>
<div class="browsable-container figure-container" id="p144">
<img alt="figure" height="467" src="../Images/CH10_F11_Grainger.png" width="653"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.11</span> Comparing <em>Social Genocide</em> to <em>The Social Network</em> for the query <code>social network</code></h5>
</div>
<div class="readable-text" id="p145">
<p>Once we iterate through every pairwise difference between documents matching the query <code>social network</code> to create a pointwise training set, we can move on to also logging differences for other queries. Figure 10.12 shows differences for a second query, this time comparing the relevance of documents matching the query <code>star wars</code>.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p146">
<img alt="figure" height="441" src="../Images/CH10_F12_Grainger.png" width="647"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.12</span> Comparing <em>Rogue One: A Star Wars Movie</em> to <em>Star!</em> for the query <code>star wars</code>. We’ve moved on from <code>social network</code> and have begun to look at patterns within another query.</h5>
</div>
<div class="browsable-container listing-container" id="p147">
<div class="code-area-container">
<pre class="code-area"># Positive example
predictor_deltas.append(+1)
feature_deltas.append([2.088, 1.024] - [1.808, -0.444]) <span class="aframe-location"/> #1

# Negative example
predictor_deltas.append(-1)
feature_deltas.append([1.808, -0.444] - [2.088, 1.024]) <span class="aframe-location"/> #2</pre>
<div class="code-annotations-overlay-container">
     #1 Rogue One features minus Star! features
     <br/>#2 Star! features minus Rogue One features
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p148">
<p>We continue this process of calculating differences between feature values for relevant versus irrelevant documents until we have calculated all the pairwise differences for our training and test queries.</p>
</div>
<div class="readable-text intended-text" id="p149">
<p>You can see back in figure 10.8 that the positive examples show a positive <code>title_bm25</code> delta, and possibly a slightly positive <code>overview_bm25</code> delta. This becomes even more clear if we calculate deltas over the full dataset of 100 queries, as shown in figure 10.13.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p150">
<img alt="figure" height="471" src="../Images/CH10_F13_Grainger.png" width="724"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.13</span> Full training set with a hyperplane separating relevant from irrelevant documents. We see a pattern! Relevant documents have a higher <code>title_bm25</code> and perhaps a modestly higher <code>overview_bm25</code>.</h5>
</div>
<div class="readable-text" id="p151">
<p>Interesting! It is now very easy to visually identify that a larger <code>title_bm25</code> score match is highly correlated with a document being relevant for a query, and that having a higher <code>overview_bm25</code> score is at least somewhat positively correlated.</p>
</div>
<div class="readable-text intended-text" id="p152">
<p>It’s worth taking a step back now and asking whether this formulation of ranking is appropriate for your domain. Different LTR models have their own method of mapping pairwise comparisons into classification problems as needed. As another example, LambdaMART—a popular LTR algorithm based on boosted trees—uses pairwise swapping and measures the change in <em>discounted cumulative gain</em> (DCG). </p>
</div>
<div class="readable-text intended-text" id="p153">
<p>Next up, we’ll train a robust model to capture the patterns in our fully transformed ranking dataset. </p>
</div>
<div class="readable-text" id="p154">
<h2 class="readable-text-h2" id="sigil_toc_id_145"><span class="num-string">10.5</span> Step 4: Training (and testing!) the model</h2>
</div>
<div class="readable-text" id="p155">
<p>Good machine learning clearly requires a lot of data preparation. Luckily, you’ve arrived at the section where we actually train a model! With the <code>feature_deltas</code> and <code>predictor_deltas</code> from the last section, we now have a training set suitable for training a ranking classifier. This model will let us predict when documents might be relevant, even for queries and documents it hasn’t seen yet. </p>
</div>
<div class="readable-text" id="p156">
<h3 class="readable-text-h3" id="sigil_toc_id_146"><span class="num-string">10.5.1</span> Turning a separating hyperplane’s vector into a scoring function</h3>
</div>
<div class="readable-text" id="p157">
<p>We’ve seen how SVMrank’s separating hyperplane can classify and differentiate irrelevant examples from the relevant ones. That’s useful, but you may remember that our task is to find <em>optimal</em> weights for our features, not just to classify documents. Let’s therefore look at how we can <em>score</em> search results using this hyperplane. </p>
</div>
<div class="readable-text intended-text" id="p158">
<p>It turns out that the separating hyperplane also gives us what we need to learn optimal weights. Any hyperplane is defined by the vector orthogonal to the plane. So when an SVM machine learning library does its work, it gives us a sense of the weights that each feature should have, as shown in figure 10.14.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p159">
<img alt="figure" height="468" src="../Images/CH10_F14_Grainger.png" width="721"/>
<h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 10.14</span> Full training set with a candidate-separating hyperplane, showing the orthogonal vector defining the hyperplane.</h5>
</div>
<div class="readable-text" id="p160">
<p>Think about what this orthogonal vector represents. This vector points in the direction of relevance! It says relevant examples are this way, and irrelevant ones are in the opposite direction. This vector <em>definitely</em> points to <code>title_bm25</code> having a strong influence on relevance, with some smaller influence coming from <code>overview_bm25</code>. This vector might be something like:</p>
</div>
<div class="browsable-container listing-container" id="p161">
<div class="code-area-container">
<pre class="code-area">[0.65, 0.40]</pre>
</div>
</div>
<div class="readable-text" id="p162">
<p>We used the pairwise transform algorithm in listing 10.9 to compute the deltas needed to perform classification between irrelevant and relevant examples. If we train an SVM on this data, as in the following listing, the model gives us the vector defining the separating hyperplane.</p>
</div>
<div class="browsable-container listing-container" id="p163">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.12</span> Training a linear SVM with scikit-learn</h5>
<div class="code-area-container">
<pre class="code-area">from sklearn import svm
model = svm.LinearSVC(max_iter=10000 <span class="aframe-location"/> #1
model.fit(feature_deltas, predictor_deltas) <span class="aframe-location"/> #2
display(model.coef_)<span class="aframe-location"/> #3</pre>
<div class="code-annotations-overlay-container">
     #1 Creates a linear model with sklearn
     <br/>#2 Fits to deltas using an SVM
     <br/>#3 The vector that defines the separating hyperplane
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p164">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p165">
<div class="code-area-container">
<pre class="code-area">array([0.40512169, 0.29006328, 0.14451715])</pre>
</div>
</div>
<div class="readable-text" id="p166">
<p>Listing 10.12 trains an SVM to separate the <code>predictor_deltas</code> (remember they’re <code>+1</code> and <code>-1</code>) using the corresponding <code>feature_deltas</code> (the deltas in the normalized <code>title_bm25</code>, <code>overview_bm25</code>, and <code>release_year</code> features). The resulting model is a vector orthogonal to the separating hyperplane. As expected, it shows a strong weight on <code>title_bm25</code>, a more modest one on <code>overview_bm25</code>, and a weaker weight on <code>release_year</code>. </p>
</div>
<div class="readable-text" id="p167">
<h3 class="readable-text-h3" id="sigil_toc_id_147"><span class="num-string">10.5.2</span> Taking the model for a test drive</h3>
</div>
<div class="readable-text" id="p168">
<p>How does this model work as a ranking function? Let’s suppose the user enters the query <code>wrath of khan</code>. How might this model score the document <em>Star Trek II: The Wrath of Khan</em> relative to this query? The unnormalized feature vector indicates a strong title and overview match for this query. </p>
</div>
<div class="browsable-container listing-container" id="p169">
<div class="code-area-container">
<pre class="code-area">[5.9217176, 3.401492, 1982.0]<span class="aframe-location"/> #1</pre>
<div class="code-annotations-overlay-container">
     #1 Raw features for “Star Trek II”
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p170">
<p>Normalizing it, each feature value is this many standard deviations above or below each feature’s mean:</p>
</div>
<div class="browsable-container listing-container" id="p171">
<div class="code-area-container">
<pre class="code-area">[3.099, 1.825, -0.568] <span class="aframe-location"/> #1</pre>
<div class="code-annotations-overlay-container">
     #1 Normalized features for “Star Trek II”
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p172">
<p>We simply multiply each normalized feature with its corresponding <code>coef_</code> value. Summing them then gives us a relevance score:</p>
</div>
<div class="browsable-container listing-container" id="p173">
<div class="code-area-container">
<pre class="code-area">(3.099 * 0.405) + (1.825 * 0.290) + (-0.568 * 0.1445) = 1.702 <span class="aframe-location"/> #1</pre>
<div class="code-annotations-overlay-container">
     #1 Relevance score calculation for “Star Trek II”
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p174">
<p>How would this model rank <em>Star Trek III: The Search for Spock</em> relative to <em>Star Trek II: The Wrath of Khan</em> for our query <code>wrath of khan</code>? Hopefully not nearly as highly! Indeed, it doesn’t:</p>
</div>
<div class="browsable-container listing-container" id="p175">
<div class="code-area-container">
<pre class="code-area">[0.0, 0.0, 1984.0] <span class="aframe-location"/> #1
[-0.432, -0.444, -0.468] <span class="aframe-location"/> #2
(-0.432 * 0.405) + (-0.444 * 0.290) + (-0.468 * 0.1445) = -0.371 <span class="aframe-location"/> #3</pre>
<div class="code-annotations-overlay-container">
     #1 Raw features for “Star Trek III”
     <br/>#2 Normalized features for “Star Trek III”
     <br/>#3 Relevance calculation for “Star Trek III”
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p176">
<p>The model seems to be correctly predicting the most relevant answer. </p>
</div>
<div class="readable-text" id="p177">
<h3 class="readable-text-h3" id="sigil_toc_id_148"><span class="num-string">10.5.3</span> Validating the model</h3>
</div>
<div class="readable-text" id="p178">
<p>Testing a couple of queries helps us spot problems, but we’d prefer a more systematic way of checking if the model is generalizable. </p>
</div>
<div class="readable-text intended-text" id="p179">
<p>One difference between LTR and traditional machine learning is that we usually evaluate queries and entire result sets, not individual data points, to prove our model is effective. We’ll perform a test/training split at the query level. This will let us spot types of queries with problems. We’ll evaluate using a simple precision metric, counting the proportion of results in the top <em>K</em> (with <code>k=5</code> in our case) that are relevant. You should choose the relevance metric best suited to your own use case.</p>
</div>
<div class="readable-text intended-text" id="p180">
<p>First, we will randomly put our queries into a test or training set, as shown in the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p181">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.13</span> Simple test/training split at the query level</h5>
<div class="code-area-container">
<pre class="code-area">all_qids = list(set([j.qid for j in normed_judgments]))
random.shuffle(all_qids) <span class="aframe-location"/> #1
proportion_train = 0.1  #1
  #1
split_idx = int(len(all_qids) * proportion_train)  #1
test_qids = all_qids[:split_index] <span class="aframe-location"/> #2
train_qids = all_qids[split_index:]  #2

train_data = []; test_data=[]
for j in normed_judgments:
  if j.qid in train_qids:  #2
    train_data.append(j)  #2
  elif j.qid in test_qids:  #2
    test_data.append(j)  #2</pre>
<div class="code-annotations-overlay-container">
     #1 Identifies a random 10% of the judgments to go into the training set
     <br/>#2 Places each judgment into training data (10%) or test set (90%)
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p182">
<p>With the training data split out, we can perform the pairwise transform trick from step 3. We can then retrain on just the training data.</p>
</div>
<div class="browsable-container listing-container" id="p183">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.14</span> Train just on training data</h5>
<div class="code-area-container">
<pre class="code-area">train_data_features, train_data_predictors = pairwise_transform(train_data)

from sklearn import svm
model = svm.LinearSVC(max_iter=10000, verbose=1)
model.fit(train_data_features, train_data_predictors) <span class="aframe-location"/> #1
display(model.coef_[0])</pre>
<div class="code-annotations-overlay-container">
     #1 Fits only to training data
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p184">
<p>Output:</p>
</div>
<div class="browsable-container listing-container" id="p185">
<div class="code-area-container">
<pre class="code-area">array([0.37486809, 0.28187458, 0.12097921])</pre>
</div>
</div>
<div class="readable-text" id="p186">
<p>So far, we have held back the test data. Just like a good teacher, we don’t want to give the student all the answers. We want to see if the model has learned anything beyond rote memorization of the training examples.</p>
</div>
<div class="readable-text intended-text" id="p187">
<p>In the next listing, we evaluate our model using the test data. This code loops over every test query and ranks every test judgment using the model. It then computes the precision for the top four judgments.</p>
</div>
<div class="browsable-container listing-container" id="p188">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.15</span> Can our model generalize beyond the training data?</h5>
<div class="code-area-container">
<pre class="code-area">def score_one(features, model):
  score = 0.0
  for idx, f in enumerate(features):
    this_coef = model.coef_[0][idx].item()
    score += f * this_coef
  return score

def rank(query_judgments, model):
  for j in query_judgments:
    j.score = score_one(j.features, model)
  return sorted(query_judgments, key=lambda j: j.score, reverse=True)

def evaluate_model(test_data, model, k=5):
  total_precision = 0
  unique_queries = groupby(test_data, lambda j: j.qid)
  num_groups = 0
  for qid, query_judgments in unique_queries:<span class="aframe-location"/> #1
    num_groups += 1
    ranked = rank(list(query_judgments), model) <span class="aframe-location"/> #2
    total_relevant = len([j for j in ranked[:k]
                            if j.grade == 1]) <span class="aframe-location"/> #3
    total_precision += total_relevant / float(k)
  return total_precision / num_groups

evaluation = evaluate_model(test_data, model)
print(evaluation)</pre>
<div class="code-annotations-overlay-container">
     #1 For each test query
     <br/>#2 Scores each judgment and ranks this query using the model
     <br/>#3 Compute the precisions for this query
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p189">
<p>Evaluation:</p>
</div>
<div class="browsable-container listing-container" id="p190">
<div class="code-area-container">
<pre class="code-area">0.36</pre>
</div>
</div>
<div class="readable-text" id="p191">
<p>On multiple runs, you should expect a precision of approximately 0.3–0.4. Not bad for our first iteration, where we just guessed at a few features (<code>title_bm25</code>, <code>overview_bm25</code>, and <code>release_year</code>)!</p>
</div>
<div class="readable-text intended-text" id="p192">
<p>In LTR, you can always look back at previous steps to see what might be improved. This precision test is the first time we’ve been able to systematically evaluate our model, so it’s a natural time to revisit the features to see how the precision might be improved in subsequent runs. Go all the way back up to step 2. See what examples are on the wrong side of the separating hyperplane. For example, if you look back at figure 10.8, the third Star Wars movie, <em>Return of the Jedi</em>, fits a pattern of a relevant document that doesn’t have a keyword match in the title. In the absence of a title, what other features might be added to help capture that a movie belongs in a specific collection like Star Wars? Perhaps there is a property within the TMDB dataset that we could experiment with.</p>
</div>
<div class="readable-text intended-text" id="p193">
<p>For now, though, let’s take the model we just built and see how we can deploy it to production. </p>
</div>
<div class="readable-text" id="p194">
<h2 class="readable-text-h2" id="sigil_toc_id_149"><span class="num-string">10.6</span> Steps 5 and 6: Upload a model and search</h2>
</div>
<div class="readable-text" id="p195">
<p>In this section, we’ll finally upload our model so that it can be applied to rank future search results. We’ll then discuss both applying the model to rank all documents, as well as applying it to rerank an already-run and likely more efficient initial query. Finally, we’ll discuss some of the performance implications of using LTR models in production. </p>
</div>
<div class="readable-text" id="p196">
<h3 class="readable-text-h3" id="sigil_toc_id_150"><span class="num-string">10.6.1</span> Deploying and using the LTR model</h3>
</div>
<div class="readable-text" id="p197">
<p>Originally, we presented our objective as finding <em>ideal</em> boosts for a hardcoded ranking function like the one in listing 10.2:</p>
</div>
<div class="browsable-container listing-container" id="p198">
<div class="code-area-container code-area-with-html">
<pre class="code-area">{"query": f"title:({keywords})^10 overview:({keywords})^20
<span class="">↪</span>{!func}release_year^0.01"}</pre>
</div>
</div>
<div class="readable-text" id="p199">
<p>This boosted query indeed multiplies each feature by a weight (the boost) and sums the results. But it turns out that we don’t want the search engine to multiply the <em>raw</em> feature values. Instead, we need the feature values to be normalized.</p>
</div>
<div class="readable-text intended-text" id="p200">
<p>Many search engines let us store a linear ranking model along with feature normalization statistics. We saved the <code>means</code> and <code>std_devs</code> of each feature, which will be used to normalize values for any document being evaluated. These coefficients are associated with each feature when uploading the model, as shown in the next listing.</p>
</div>
<div class="browsable-container listing-container" id="p201">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.16</span> Generating and uploading a linear model</h5>
<div class="code-area-container">
<pre class="code-area">model_name = "movie_model"
feature_names = ["title_bm25", "overview_bm25", "release_year"]
linear_model = ltr.generate_model(model_name, feature_names,
                                  means, std_devs, model.coef_[0])
response = ltr.upload_model(linear_model)
display(linear_model)</pre>
</div>
</div>
<div class="readable-text" id="p202">
<p>Generated linear model (for <code>engine=solr</code>):</p>
</div>
<div class="browsable-container listing-container" id="p203">
<div class="code-area-container">
<pre class="code-area">{"store": "movies", <span class="aframe-location"/> #1
 "class": "org.apache.solr.ltr.model.LinearModel",
 "name": "movie_model",
 "features": [
  <span class="aframe-location"/> {"name": "title_bm25",  #2
    "norm": {"class": "org.apache.solr.ltr.norm.StandardNormalizer", 
             "params": {"avg": "0.7245440735518126", <span class="aframe-location"/> #3
                        "std": "1.6772600303613545"}}},  #3
   {"name": "overview_bm25",
    "norm": {"class": "org.apache.solr.ltr.norm.StandardNormalizer",
             "params": {"avg": "0.6662927508611409",
                        "std": "1.4990448120673643"}}},
   {"name": "release_year",
    "norm": {"class": "org.apache.solr.ltr.norm.StandardNormalizer",
             "params": {"avg": "1993.3349740932642",
                        "std": "19.964916628520722"}}}],
 "params": {
   "weights": {
     "title_bm25": 0.3748679655554891, <span class="aframe-location"/> #4
     "overview_bm25": 0.28187459845467566,  #4
     "release_year": 0.12097924576841014}}}  #4</pre>
<div class="code-annotations-overlay-container">
     #1 Feature store to locate the features
     <br/>#2 Which feature to execute before evaluating this model
     <br/>#3 How to normalize this feature before applying the weight
     <br/>#4 The weight of each feature in the model
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p204">
<p>The <code>response</code> from listing 10.16 is Solr-specific and will change depending on which search engine you have configured. Next, we can issue a search using the uploaded LTR model, as shown in the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p205">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.17</span> Ranking all documents with LTR model for <code>harry potter</code></h5>
<div class="code-area-container">
<pre class="code-area">request = {"query_fields": ["title", "overview"],
           "return_fields": ["title", "id", "score"],
           "rerank_query": "harry potter",
           "log": True}
response = ltr.search_with_model("movie_model", **request)
display(response["docs"])</pre>
</div>
</div>
<div class="readable-text" id="p206">
<p>Engine-specific search request (for <code>engine=solr</code>):</p>
</div>
<div class="browsable-container listing-container" id="p207">
<div class="code-area-container code-area-with-html">
<pre class="code-area">{"fields": ["title", "id", "score"],
 "limit": 5,
 "query": "{!ltr reRankDocs=9999999 <span class="aframe-location"/> #1
 <span class="">↪</span>model=movie_model efi.keywords=\"harry potter\"}"}</pre>
<div class="code-annotations-overlay-container">
     #1 Executes our model over the maximum number of documents with the specified parameters
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p208">
<p>Returned documents:</p>
</div>
<div class="browsable-container listing-container" id="p209">
<div class="code-area-container">
<pre class="code-area">[{"id": "570724", "title": "The Story of Harry Potter", "score": 2.4261155},
 {"id": "116972", "title": "Discovering the Real World of Harry Potter",
  "score": 2.247846},
 {"id": "672", "title": "Harry Potter and the Chamber of Secrets",
  "score": 2.017499},
 {"id": "671", "title": "Harry Potter and the Philosopher's Stone",
  "score": 1.9944705},
 {"id": "54507", "title": "A Very Potter Musical",
  "score": 1.9833609}]</pre>
</div>
</div>
<div class="readable-text" id="p210">
<p>In listing 10.17, the LTR model ranks all the documents in the corpus using the keywords in the <code>rerank_query</code> parameter as input to the model. Since no initial <code>query</code> parameter is specified in the request, no matching filter is applied to the collection before the search results (all documents) are ranked by the LTR model. Though scoring such a large number of documents with the model will lead to nontrivial latency, it allows us to test the model directly, absent of any other matching parameters. </p>
</div>
<div class="readable-text intended-text" id="p211">
<p>Notice in listing 10.17 the use of the term “rerank” in the <code>rerank_query</code> parameter. As this term implies, LTR usually happens as a second ranking phase on results first calculated by a more efficient algorithm (such as BM25 and/or an initial Boolean match). This is to reduce the number of documents that must be scored by the more expensive LTR model. The following listing demonstrates executing a baseline search and then reranking the top <code>500</code> results with the LTR model. </p>
</div>
<div class="browsable-container listing-container" id="p212">
<h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 10.18</span> Searching for <code>harry potter</code> and reranking with the model</h5>
<div class="code-area-container">
<pre class="code-area">request = {"query": "harry potter",
           "query_fields": ["title", "overview"],
           "return_fields": ["title", "id", "score"],
           "rerank_query": "harry potter",
           "rerank_count": 500,
           "log": True}
response = ltr.search_with_model("movie_model", **request)
display(response["docs"])</pre>
</div>
</div>
<div class="readable-text" id="p213">
<p>Engine-specific search request (for <code>engine=solr</code>):</p>
</div>
<div class="browsable-container listing-container" id="p214">
<div class="code-area-container code-area-with-html">
<pre class="code-area">{"query": "harry potter",<span class="aframe-location"/> #1
 "fields": ["title", "id", "score"],
 "limit": 5,
 "params": {
   "rq": "{!ltr reRankDocs=500 model=movie_model <span class="aframe-location"/> #2
   <span class="">↪</span>efi.keywords=\"harry potter\"}", #2
   "qf": ["title", "overview"], #1
   "defType": "edismax"}}  #1</pre>
<div class="code-annotations-overlay-container">
     #1 First-pass Solr query—a simple keyword query with BM25 ranking
     <br/>#2 Reranks only the top 500 documents
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p215">
<p>Returned documents:</p>
</div>
<div class="browsable-container listing-container" id="p216">
<div class="code-area-container">
<pre class="code-area">[{"id": "570724", "title": "The Story of Harry Potter", "score": 2.4261155},
 {"id": "116972", "title": "Discovering the Real World of Harry Potter",
  "score": 2.247846},
 {"id": "672", "title": "Harry Potter and the Chamber of Secrets",
  "score": 2.017499},
 {"id": "671", "title": "Harry Potter and the Philosopher's Stone",
  "score": 1.9944705},
 {"id": "54507", "title": "A Very Potter Musical", "score": 1.9833605}]</pre>
</div>
</div>
<div class="readable-text" id="p217">
<p>This request is much faster, and it still yields the same top results when performing the cheaper initial BM25 ranking on the filtered <code>query</code> followed by the more expensive LTR-based reranking on just the top <code>500</code> results. </p>
</div>
<div class="readable-text" id="p218">
<h3 class="readable-text-h3" id="sigil_toc_id_151"><span class="num-string">10.6.2</span> A note on LTR performance</h3>
</div>
<div class="readable-text" id="p219">
<p>As you can see, many steps are required to build a real-world LTR model. Let’s close the chapter with some additional thoughts on practical performance constraints in LTR systems:</p>
</div>
<ul>
<li class="readable-text" id="p220"> <em>Model complexity</em><em> </em>—The more complex the model, the more accurate it <em>might</em> be. A simpler model can be faster and easier to understand, though perhaps less accurate. Here we’ve stuck to a very simple model (a set of linear weights). Imagine a complex deep-learning model—how well would that work? Would the complexity be worth it? Would it be as generalizable (or could it possibly be more generalizable)? </li>
<li class="readable-text" id="p221"> <em>Rerank depth</em><em> </em>—The deeper you rerank, the more you might find additional documents that could be hidden gems. On the other hand, the deeper you rerank, the more compute cycles your model spends scoring results in your live search engine cluster. </li>
<li class="readable-text" id="p222"> <em>Feature complexity</em><em> </em>—If you compute very complex features at query time, they might help your model. However, they’ll slow down evaluation and search response time. </li>
<li class="readable-text" id="p223"> <em>Number of features</em><em> </em>—A model with many features might lead to higher relevance. However, it will also take more time to compute every feature on each document, so ask yourself which features are crucial. Many academic LTR systems use hundreds. Practical LTR systems usually boil these down to dozens. You will almost always see diminishing returns for relevance ranking and rising compute and latency costs as you continue adding additional features, so prioritizing which features to include is important. </li>
</ul>
<div class="callout-container sidebar-container">
<div class="readable-text" id="p224">
<h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Cross-encoders</h5>
</div>
<div class="readable-text" id="p225">
<p>A cross-encoder is a specialized kind of machine-learned ranking model. Cross-encoders are trained to score the relevance of two pieces of input (usually text), such as a query and a document. They use a Transformer architecture to combine both pieces of input into a single representation, which is then used in search to rank the relevance of the document for the query based upon interpreting both the query and document within their shared semantic context. Cross-encoders are ranking classifiers, like other LTR models, but they are unique in that they are pretrained on a large amount of data and are generally only focused on the textual similarity between the query and document instead of other features like popularity, recency, or user behavior. While they can be fine-tuned on your dataset, they are often used as is, since they are already trained on a large amount of data and can generalize well to new textual inputs. </p>
</div>
<div class="readable-text" id="p226">
<p>Cross-encoders are very easy to use out of the box, and they’re often the easiest way to get started with machine-learned ranking without having to do your own training. Cross-encoders tend to be slow, so they’re not typically used to rerank large numbers of documents. Our focus in this chapter and in the coming chapter is on more flexible models that can use reflected intelligence, including those trained on your users’ judgments and implicit judgments from user signals, but it’s good to be familiar with cross-encoders, as they are a popular choice for many search teams, particularly when just getting started. We’ll cover cross-encoders in more detail, with example code, in section 13.7. </p>
</div>
</div>
<div class="readable-text" id="p228">
<h2 class="readable-text-h2" id="sigil_toc_id_152"><span class="num-string">10.7</span> Rinse and repeat</h2>
</div>
<div class="readable-text" id="p229">
<p>Congrats! You’ve done one full cycle of LTR! Like many data problems, though, you’ll likely need to continue iterating on the problem. There’s always something new you can do to improve. </p>
</div>
<div class="readable-text intended-text" id="p230">
<p>On your second iteration, you might consider the following:</p>
</div>
<ul>
<li class="readable-text" id="p231"> <em>New and better features</em><em> </em>—Are there types of queries or examples on which the model performs poorly, such as <code>title</code> searches where there’s no <code>title</code> mention? (“Star Wars” is not mentioned in the title of <em>Return of the Jedi</em>. What features could capture these?) Could we incorporate lessons from chapters 1–9 to construct more advanced features? </li>
<li class="readable-text" id="p232"> <em>Training data coverage of all features</em><em> </em>—The flip side of more features is more training data. As you increase the features you’d like to try, you should be wondering whether your training data has enough examples of relevant and irrelevant documents across each different combination of your features. Otherwise, your model won’t know how to use features to solve the problem. </li>
<li class="readable-text" id="p233"> <em>Different model architectures</em><em> </em>—We used a relatively simple model that expects features to linearly and independently correlate with relevance, but relevance can often be nonlinear and multidimensional. A shopper searching for <code>ipad</code> might expect the most recent Apple iPad release, except when they add the word “cable”, making the query <code>ipad cable</code>. For that query, the shopper might just want the cheapest cable they can find instead of the most recent. In this case, there may be “recency” and “price” features that activate depending on specific keyword combinations, necessitating a more complicated model architecture. </li>
</ul>
<div class="readable-text" id="p234">
<p>In the next chapter, we will focus on the foundation of good LTR: great judgments!</p>
</div>
<div class="readable-text" id="p235">
<h2 class="readable-text-h2" id="sigil_toc_id_153">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p236"> Learning to rank (LTR) builds generalized ranking functions that can be applied across all searches, using robust machine learning techniques.  </li>
<li class="readable-text" id="p237"> LTR features generally correspond to search queries. Search engines that support LTR often let you store and log features for use when training, and later applying, a ranking model. </li>
<li class="readable-text" id="p238"> We have tremendous freedom in what features we use to generalize relevance. Features could be properties of queries (like the number of terms), properties of documents (like popularity), or relationships between queries and documents (like BM25 or other relevance scores). </li>
<li class="readable-text" id="p239"> To do LTR well and apply well-known machine learning techniques, we typically reformulate the relevance ranking problem into a traditional, pointwise machine learning problem. </li>
<li class="readable-text" id="p240"> SVMrank creates simple linear weights on normalized feature values, a good first step on your LTR journey. </li>
<li class="readable-text" id="p241"> To be truly useful, we need our model to generalize beyond what it’s learned. We can confirm an LTR model’s ability to generalize by setting some judgments aside in a test dataset and not using them during training. After training, we can then evaluate the model on that previously unseen test dataset to confirm the model’s ability to generalize. </li>
<li class="readable-text" id="p242"> Once an LTR model is loaded into your search engine, be sure to consider performance (as in speed) tradeoffs with relevance. Real-life search systems require both.  </li>
</ul>
</div></body></html>