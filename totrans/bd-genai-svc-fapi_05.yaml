- en: Chapter 3\. AI Integration and Model Serving
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章\. AI集成与模型服务
- en: In this chapter, you will learn the mechanisms of various GenAI models and how
    to serve them in a FastAPI application. Additionally, using the [Streamlit UI
    package](https://oreil.ly/9BXmn), you will create a simple browser client for
    interacting with the model-serving endpoints. We will explore differing model-serving
    strategies, how to preload models for efficiency, and how to use FastAPI features
    for service monitoring.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习各种GenAI模型的机制以及如何在FastAPI应用程序中提供服务。此外，使用[Streamlit UI包](https://oreil.ly/9BXmn)，你将创建一个简单的浏览器客户端，用于与模型服务端点交互。我们将探讨不同的模型服务策略、如何预加载模型以提高效率，以及如何使用FastAPI功能进行服务监控。
- en: To solidify your learning in this chapter, we will progressively build a FastAPI
    service using open source GenAI models that generate text, images, audio, and
    3D geometries, all from scratch. In later chapters, you’ll build the functionality
    to parse documents and web content for your GenAI service so you can talk to them
    using a language model.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为了巩固本章的学习，我们将逐步构建一个FastAPI服务，使用开源GenAI模型生成文本、图像、音频和3D几何形状，全部从头开始。在后续章节中，你将构建解析文档和网页内容的功能，以便你的GenAI服务可以使用语言模型与之交流。
- en: Note
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In the previous chapter, you saw how to set up a fresh FastAPI project in Python.
    Make sure you have fresh installation ready before you read the rest of this chapter.
    Alternatively, you can clone or download the book’s [GitHub repository](https://github.com/Ali-Parandeh/building-generative-ai-services).
    Then once cloned, switch to the `ch03-start` branch, ready for the steps to follow.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何在Python中设置一个新的FastAPI项目。在阅读本章剩余内容之前，请确保你已经准备好了新的安装。或者，你可以克隆或下载本书的[GitHub仓库](https://github.com/Ali-Parandeh/building-generative-ai-services)。然后一旦克隆，切换到`ch03-start`分支，准备进行后续步骤。
- en: By the end of this chapter, you will have a FastAPI service that serves various
    open source GenAI models that you can test inside the Streamlit UI. Additionally,
    your service will be capable of logging usage data to disk using middleware.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将拥有一个FastAPI服务，该服务提供各种开源GenAI模型，你可以在Streamlit UI中进行测试。此外，你的服务将能够使用中间件将使用数据记录到磁盘。
- en: Serving Generative Models
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成模型服务
- en: Before you serve pretrained generative models in your application, it is worth
    learning how these models are trained and generate data. With this understanding,
    you can customize the internals of your application to enhance the outputs that
    you provide to the user.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的应用程序中提供预训练的生成模型之前，了解这些模型的训练方式和数据生成过程是值得的。有了这种理解，你可以定制应用程序的内部结构，以增强提供给用户的结果。
- en: 'In this chapter, I will show you how to serve models across a variety of modalities
    including:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将向你展示如何在不同模态上提供服务，包括：
- en: '*Language* models based on the transformer neural network architecture'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于转换器神经网络架构的*语言*模型
- en: '*Audio* models in text-to-speech and text-to-audio services based on the aggressive
    transformer architecture'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于激进转换器架构的文本到语音和文本到音频服务的*音频*模型
- en: '*Vision* models for text-to-image and text-to-video services based on the Stable
    Diffusion and vision transformer architectures'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Stable Diffusion和视觉转换器架构的文本到图像和文本到视频服务的*视觉*模型
- en: '*3D* models for text-to-3D services based on the conditional implicit function
    encoder and diffusion decoder architecture'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于条件隐函数编码器和扩散解码器架构的文本到3D服务的*3D*模型
- en: This list is not exhaustive and covers a handful of GenAI models. To explore
    other models, please visit the [Hugging Face model repository](https://oreil.ly/-4wlQ).^([1](ch03.html#id630))
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表并不全面，仅涵盖了少量通用人工智能（GenAI）模型。若要探索其他模型，请访问[Hugging Face模型仓库](https://oreil.ly/-4wlQ).^([1](ch03.html#id630))
- en: Language Models
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语言模型
- en: In this section, we talk about language models, including transformers and recurrent
    neural networks (RNNs).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论语言模型，包括转换器（Transformers）和循环神经网络（RNNs）。
- en: Transformers versus recurrent neural networks
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换器与循环神经网络的对决
- en: The world of AI was shaken with the release of the landmark paper “Attention
    Is All You Need.”^([2](ch03.html#id631)) In this paper, the authors proposed a
    completely different approach to natural language processing (NLP) and sequence
    modeling that differed from the existing RNN architectures.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的世界因里程碑式的论文“Attention Is All You Need”的发布而震动。在这篇论文中，作者们提出了一种完全不同的自然语言处理（NLP）和序列建模方法，与现有的RNN架构不同。
- en: '[Figure 3-1](#transformer_architecture) shows a simplified version of the proposed
    transformer architecture from the original paper.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-1](#transformer_architecture) 展示了原始论文中提出的简化版变压器架构。'
- en: '![bgai 0301](assets/bgai_0301.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0301](assets/bgai_0301.png)'
- en: Figure 3-1\. Transformer architecture
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1\. 变压器架构
- en: Historically, text generation tasks leveraged RNN models to learn patterns in
    sequential data such as free text. To process text, these models chunk text into
    small pieces such as a word or character called a *token* that can be sequentially
    processed.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，文本生成任务利用 RNN 模型来学习序列数据（如自由文本）中的模式。为了处理文本，这些模型将文本分成小块，如单词或字符，称为 *标记*，可以按顺序处理。
- en: RNNs maintain a memory store called a *state vector*, which carries information
    from one token to the next throughout the full text sequence, until the end. This
    means that by the time you get to the end of the text sequence, the impact of
    early tokens on the state vector is a lot smaller compared to the most recent
    tokens.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: RNN 维护一个称为 *状态向量* 的内存存储，它携带信息从全文序列中的一个标记传递到下一个标记，直到序列的末尾。这意味着当你到达文本序列的末尾时，早期标记对状态向量的影响与最近标记相比要小得多。
- en: Ideally, every token should be as important as the other tokens in any text.
    However, as RNNs can only predict the next item in a sequence by looking at the
    items that came before, they struggle with this ideal in capturing long-range
    dependencies and modeling patterns in large chunks of texts. As a result, they
    effectively fail to remember or comprehend essential information or context in
    large documents.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，每个标记应该和任何文本中的其他标记一样重要。然而，由于 RNN 只能通过查看前面的项目来预测序列中的下一个项目，它们在捕捉长距离依赖关系和在大块文本中建模模式方面存在困难。因此，它们实际上无法记住或理解大文档中的关键信息或上下文。
- en: With the invention of transformers, recurrent or convolutional modeling could
    now be replaced with a more efficient approach. Since transformers don’t maintain
    a hidden state memory and leverage a new capability termed *self-attention*, they’re
    capable of modeling relationships between words, no matter how far apart they
    appeared in a sentence. This self-attention component allows the model to “place
    attention” on contextually relevant words within a sentence.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 随着变压器的发明，循环或卷积建模现在可以被更有效的方法所取代。由于变压器不维护隐藏状态内存并利用一种称为 *自注意力* 的新能力，它们能够建模词语之间的关系，无论它们在句子中出现的距离有多远。这个自注意力组件允许模型在句子中对上下文相关的词语“给予关注”。
- en: While RNNs model relationships between neighboring words in a sentence, transformers
    map pairwise relationships between every word in the text.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当 RNN 模型句子中相邻词语之间的关系时，变压器映射文本中每个词语之间的成对关系。
- en: '[Figure 3-2](#rnn_vs_transformer) shows how RNNs process sentences in comparison
    to transformers.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-2](#rnn_vs_transformer) 展示了 RNN 与变压器处理句子的比较。'
- en: '![bgai 0302](assets/bgai_0302.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0302](assets/bgai_0302.png)'
- en: Figure 3-2\. RNNs versus transformers in processing sentences
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-2\. RNN 与变压器在处理句子时的比较
- en: What powers the self-attention system are specialized blocks called *attention
    heads* that capture pairwise patterns between words as *attention maps*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力系统所依赖的是称为 *注意力头* 的专用块，它们捕获词语之间的成对模式作为 *注意力图*。
- en: '[Figure 3-3](#head_attention_map) visualizes the attention map of an attention
    head.^([3](ch03.html#id635)) Connections can be bidirectional with the thickness
    representing the strength of the relationship between words in the sentence.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-3](#head_attention_map) 可视化了一个注意力头的注意力图.^([3](ch03.html#id635)) 连接可以是双向的，厚度表示句子中词语之间关系的强度。'
- en: '![bgai 0303](assets/bgai_0303.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0303](assets/bgai_0303.png)'
- en: Figure 3-3\. View of an attention map inside an attention head
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. 注意力头内的注意力图视图
- en: A transformer model contains several attention heads distributed across its
    neural network layers. Each head computes its own attention map independently
    to capture relationships between words focusing on certain patterns in the inputs.
    Using multiple attention heads, the model can simultaneously analyze the inputs
    from various angles and contexts to understand complex patterns and dependencies
    within the data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器模型在其神经网络层中包含多个分布的注意力头。每个头独立计算自己的注意力图，以捕获输入中某些模式之间的词语关系。使用多个注意力头，模型可以同时从不同角度和上下文中分析输入，以理解数据中的复杂模式和依赖关系。
- en: '[Figure 3-4](#model_attention_map) shows the attention maps for each head (i.e.,
    independent set of attention weights) within each layer of the model.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-4](#model_attention_map) 展示了模型每一层中每个头（即独立的注意力权重集合）的注意力图。'
- en: '![bgai 0304](assets/bgai_0304.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0304](assets/bgai_0304.png)'
- en: Figure 3-4\. View of the attention maps within the model
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-4\. 模型中注意力图的视图
- en: RNNs also required extensive compute power to train, as the training process
    couldn’t be parallelized on multiple GPU due to the sequential nature of their
    training algorithms. Transformers, on the other hand, process words nonsequentially,
    so they can run attention mechanisms in parallel on GPUs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的训练也需要大量的计算能力，因为它们的训练过程不能在多个GPU上并行化，因为它们的训练算法具有顺序性。另一方面，Transformers可以非顺序地处理单词，因此它们可以在GPU上并行运行注意力机制。
- en: The efficiency of the transformer architecture means that these models are more
    scalable as long as there is more data, compute power, and memory. You can build
    language models with a corpus that spans libraries of books produced by humanity.
    All you would need is ample compute power and data to train an LLM. And, that
    is exactly what OpenAI did, the company behind the famous ChatGPT application
    that was powered by several of their proprietary LLMs including GPT-4o.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer架构的高效性意味着只要数据、计算能力和内存更多，这些模型就更具可扩展性。您可以使用涵盖人类产生的书籍库的语料库构建语言模型。您所需要的只是足够的计算能力和数据来训练一个LLM。这正是OpenAI所做的事情，该公司推出了著名的ChatGPT应用程序，该应用程序由其多个专有LLM（包括GPT-4o）提供支持。
- en: At the time of this writing, the implementation details behind OpenAI’s LLMs
    remain a trade secret. While many researchers have a general understanding of
    OpenAI’s methods, they may not necessarily have the resources to replicate them.
    However, several open source alternatives for research and commercial use have
    been released since, including Llama (Facebook), Gemma (Google), Mistral, and
    Falcon to name a few.^([4](ch03.html#id637)) At the time of this writing, the
    model sizes vary between 0.05B and 480B parameters (i.e., model weights and biases)
    to suit your needs.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，OpenAI的LLM背后的实现细节仍然是一个商业机密。虽然许多研究人员对OpenAI的方法有一个大致的了解，但他们可能不一定有资源来复制它们。然而，自那时以来，已经发布了几个开源替代方案，用于研究和商业用途，包括Llama（Facebook）、Gemma（Google）、Mistral和Falcon等。在撰写本文时，模型大小在0.05B到480B参数之间（即模型权重和偏差），以满足您的需求。[4](ch03.html#id637)
- en: Serving LLMs still remains a challenge due to high memory requirements with
    requirements doubling if you need to train and fine-tune them on your own dataset.
    This is because the training process will require caching and reusing model parameters
    across training batches. As a result, most organizations may rely on lightweight
    (up to 3B) models or on APIs of LLM providers such as OpenAI, Anthropic, Cohere,
    Mistral, etc.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于高内存需求，服务LLM仍然是一个挑战，如果需要在自己的数据集上训练和微调，需求会加倍。这是因为训练过程需要在训练批次之间缓存和重用模型参数。因此，大多数组织可能依赖于轻量级（高达30亿）模型或依赖于LLM提供商（如OpenAI、Anthropic、Cohere、Mistral等）的API。
- en: As LLMs grow in popularity, it becomes even more important to understand how
    they’re trained and how they process data, so let’s discuss underlying mechanisms
    next.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM的普及，了解它们是如何训练的以及它们如何处理数据变得越来越重要，因此让我们接下来讨论其背后的机制。
- en: Tokenization and embedding
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标记化和嵌入
- en: Neural networks can’t process words directly as they’re big statistical models
    that function on numbers. To bridge that gap between language and numbers, you
    need to use *tokenization*. With tokenization, you break down text into smaller
    pieces that a model can process.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络不能直接处理单词，因为它们是运行在数字上的大型统计模型。为了弥合语言和数字之间的差距，您需要使用*标记化*。通过标记化，您将文本分解成模型可以处理的更小的部分。
- en: Any piece of text must be first sliced into a list of *tokens* that represent
    words, syllables, symbols, and punctuations. These tokens are then mapped to unique
    numbers so that patterns can be numerically modeled.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 任何文本都必须首先切割成代表单词、音节、符号和标点的*标记*列表。然后，这些标记被映射到唯一的数字，以便可以对模式进行数值建模。
- en: By providing a vector of input tokens to a trained transformer, the network
    can then predict the next best token to generate text, one word at a time.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向训练好的transformer提供输入标记的向量，网络可以预测生成文本的下一个最佳标记，一次一个单词。
- en: '[Figure 3-5](#openai_tokenizer) shows how the OpenAI tokenizer converts text
    into a sequence of tokens, assigning unique token identifiers to each.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-5](#openai_tokenizer) 展示了OpenAI标记化器如何将文本转换为标记序列，并为每个标记分配唯一的标记标识符。'
- en: '![bgai 0305](assets/bgai_0305.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0305](assets/bgai_0305.png)'
- en: 'Figure 3-5\. OpenAI tokenizer (Source: [OpenAI](https://oreil.ly/S-a9M))'
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-5. OpenAI分词器（来源：[OpenAI](https://oreil.ly/S-a9M))
- en: So what can you do after you tokenize some text? These tokens need to be processed
    further before a language model can process them.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在分词一些文本之后你能做什么呢？这些标记在语言模型处理之前需要进一步处理。
- en: After tokenization, you need to use an *embedder*^([5](ch03.html#id647)) to
    convert these tokens into dense vectors of real numbers called *embeddings*, capturing
    semantic information (i.e., meaning of each token) in a continuous vector space.
    [Figure 3-6](#embeddings) demonstrates these embeddings.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在分词之后，你需要使用一个*嵌入器*^([5](ch03.html#id647))将这些标记转换成实数密集向量，称为*嵌入*，在连续的向量空间中捕捉语义信息（即每个标记的含义）。[图3-6](#embeddings)展示了这些嵌入。
- en: '![bgai 0306](assets/bgai_0306.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0306](assets/bgai_0306.png)'
- en: Figure 3-6\. Assigning an embedding vector of size n to each token during the
    embedding process
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-6. 在嵌入过程中为每个标记分配大小为n的嵌入向量
- en: Tip
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: These embedding vectors use small *floating-point numbers* (not integers) to
    capture nuanced relationships between tokens with more flexibility and precision.
    They also tend to be *normally distributed*, so language model training and inference
    can be more stable and consistent.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这些嵌入向量使用小的*浮点数*（不是整数）来捕捉标记之间细微的关系，具有更大的灵活性和精确度。它们也倾向于*正态分布*，因此语言模型训练和推理可以更加稳定和一致。
- en: After the embedding process, each token is assigned an embedding vector filled
    with *n* numbers. Each number in the embedding vector focuses on a dimension that
    represents a specific aspect of the token’s meaning.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在嵌入过程之后，每个标记都被分配了一个包含*n*个数字的嵌入向量。嵌入向量中的每个数字都专注于表示标记含义的一个特定方面的维度。
- en: Training transformers
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练transformer
- en: Once you have a set of embedding vectors, you can train a model on your documents
    to update the values inside each embedding. During model training, the training
    algorithm updates the parameters of the embedding layers so that the embedding
    vectors describe the meaning of each token as close as possible within the input
    text.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有一组嵌入向量，你可以在你的文档上训练一个模型来更新每个嵌入中的值。在模型训练过程中，训练算法更新嵌入层的参数，使得嵌入向量尽可能准确地描述输入文本中每个标记的含义。
- en: Understanding how embedding vectors work can be challenging, so let’s try a
    visualization approach.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 理解嵌入向量的工作原理可能具有挑战性，所以让我们尝试一种可视化方法。
- en: Imagine you used a two-dimensional embedding vectors, meaning the vectors contained
    only two numbers. Then, if you plot these vectors, before and after model training,
    you will observe plots similar to [Figure 3-7](#untrained_to_trained_transformer).
    The embedding vectors of tokens, or words, with similar meanings will be closer
    to each other.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你使用了二维嵌入向量，这意味着向量只包含两个数字。然后，如果你在模型训练前后绘制这些向量，你会观察到类似于[图3-7](#untrained_to_trained_transformer)的图表。具有相似含义的标记的嵌入向量将彼此更接近。
- en: '![bgai 0307](assets/bgai_0307.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0307](assets/bgai_0307.png)'
- en: Figure 3-7\. Training latent space of transformer network using embedding vectors
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-7. 使用嵌入向量训练transformer网络的潜在空间
- en: To determine the similarity between two words, you can compute the angle between
    vectors using a calculation known as *cosine similarity*. Smaller angles imply
    higher similarity, representing similar context and meaning. After training, the
    cosine similarity calculation of two embedding vectors with similar meanings will
    validate that those vectors are close to each other.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定两个单词之间的相似性，你可以通过计算称为*余弦相似度*的向量之间的角度来计算。较小的角度意味着更高的相似性，表示相似上下文和含义。在训练后，具有相似含义的两个嵌入向量的余弦相似度计算将验证这些向量彼此接近。
- en: '[Figure 3-8](#embedding_vectors) illustrates the full tokenization, embedding,
    and training process.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-8](#embedding_vectors)展示了完整的分词、嵌入和训练过程。'
- en: '![bgai 0308](assets/bgai_0308.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0308](assets/bgai_0308.png)'
- en: Figure 3-8\. Processing sequential data like a piece of text into a vector of
    tokens and token embeddings
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-8. 将文本等序列数据处理成标记和标记嵌入的向量
- en: Once you have a trained embedding layer, you can now use it to embed any new
    input text to the transformer model shown in [Figure 3-1](#transformer_architecture).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有一个训练好的嵌入层，你现在可以使用它来嵌入任何新的输入文本到[图3-1](#transformer_architecture)中所示的transformer模型。
- en: Positional encoding
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置编码
- en: A final step before forwarding the embedding vectors to the attention layers
    in the transformer network is to implement *positional encoding*. The positional
    encoding process produces the positional embedding vectors that then are summed
    with the token embedding vectors.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在将嵌入向量转发到转换器网络中的注意力层之前的一个最终步骤是实现 *位置编码*。位置编码过程产生位置嵌入向量，然后与标记嵌入向量相加。
- en: Since transformers process words simultaneously rather than sequentially, positional
    embeddings are needed to record the word order and context within the sequential
    data, like sentences. The resultant embedding vectors capture both meaning and
    positional information of words in the sentences before they’re passed to the
    attention mechanisms of the transformer. This process ensures attention heads
    have all the information they need to learn patterns effectively.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 由于转换器是同时处理单词而不是按顺序处理，因此需要位置嵌入来记录序列数据（如句子）中的单词顺序和上下文。生成的嵌入向量捕捉了句子中单词的意义和位置信息，在它们传递到转换器的注意力机制之前。这个过程确保注意力头拥有它们学习模式所需的所有信息。
- en: '[Figure 3-9](#positional_encoding) shows the positional encoding process where
    the positional embeddings are summed with token embeddings.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-9](#positional_encoding) 展示了位置编码过程，其中位置嵌入与标记嵌入相加。'
- en: '![bgai 0309](assets/bgai_0309.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0309](assets/bgai_0309.png)'
- en: Figure 3-9\. Positional encoding
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-9\. 位置编码
- en: Autoregressive prediction
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自回归预测
- en: The transformer is an autoregressive (i.e., sequential) model as future predictions
    are based on the past values, as shown in [Figure 3-10](#autoregressive_prediction3).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器是一个自回归（即顺序）模型，因为未来的预测是基于过去值的，如图3-10所示。[自回归预测](#autoregressive_prediction3)。
- en: '![bgai 0310](assets/bgai_0310.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0310](assets/bgai_0310.png)'
- en: Figure 3-10\. Autoregressive prediction
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-10\. 自回归预测
- en: The model receives input tokens that are then embedded and passed through the
    network to make the next best token prediction. This process repeats until a `<stop>`
    or end of sentence `<eos>` token is generated.^([6](ch03.html#id658))
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 模型接收输入标记，然后将其嵌入并通过网络传递以进行下一个最佳标记预测。这个过程会重复进行，直到生成 `<stop>` 或句子结束 `<eos>` 标记.^([6](ch03.html#id658))
- en: However, there is a limit to the number of tokens that the model can store in
    its memory to generate the next token. This token limit is referred to as the
    model’s *context window*, which is an important factor to consider during the
    model selection stage for your GenAI services.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，模型在内存中存储以生成下一个标记的标记数量是有限的。这个标记限制被称为模型的 *上下文窗口*，这是在您的GenAI服务模型选择阶段需要考虑的一个重要因素。
- en: If the context window limit is reached, the model simply discards the least
    recently used tokens. This means it can *forget* the least recently used sentences
    in documents or messages in a conversation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果达到上下文窗口限制，模型将简单地丢弃最近最少使用的标记。这意味着它可以 *忘记* 文档中最近最少使用的句子或对话中的消息。
- en: Note
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of writing, the context of the least expensive OpenAI `gpt-4o-mini`
    model is around ~128,000 tokens, equivalent to more than 300 pages of text.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，最便宜的OpenAI `gpt-4o-mini` 模型的上下文大约为 ~128,000 个标记，相当于超过300页的文本。
- en: The largest context window as of March 2025 belongs to [Magic.Dev LTM-2-mini](https://oreil.ly/10Mj1)
    with 100 million tokens. This equals ~10 million lines of code of ~750 novels.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 截至2025年3月，最大的上下文窗口属于 [Magic.Dev LTM-2-mini](https://oreil.ly/10Mj1)，拥有1亿个标记。这相当于约750本小说的约1000万行代码。
- en: The context window of other models falls in the range of hundreds of thousands
    of tokens.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其他模型的上下文窗口范围在数十万个标记之间。
- en: Short windows will lead to loss of information, difficulty maintaining conversations,
    and reduced coherence with the user query.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 短窗口会导致信息丢失，难以维持对话，以及与用户查询的连贯性降低。
- en: On the other hand, long context windows have larger memory requirements and
    can lead to performance issues or slow services when scaling to thousands of concurrent
    users who are using your service. In addition, you will need to consider the costs
    of relying on models with larger context windows as they tend to be more expensive
    due to increased compute and memory requirements. The correct choice will depend
    on your budget and user needs in your use case.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，长上下文窗口需要更大的内存需求，并且当扩展到数千个同时使用您服务的并发用户时，可能会导致性能问题或服务变慢。此外，您还需要考虑依赖于具有更大上下文窗口的模型的成本，因为它们由于计算和内存需求的增加而往往更昂贵。正确的选择将取决于您的预算和用例中的用户需求。
- en: Integrating a language model into your application
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将语言模型集成到您的应用程序中
- en: You can download and use a language model within your application with a few
    lines of code. In [Example 3-1](#language_model_usage_example), you will download
    a TinyLlama model that has 1.1 billion parameters and is pretrained on 3 trillion
    tokens.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用几行代码在应用程序中下载和使用语言模型。在[示例3-1](#language_model_usage_example)中，您将下载一个具有11亿参数的TinyLlama模型，该模型在30万亿个令牌上进行了预训练。
- en: Example 3-1\. Download and load a language model from the Hugging Face repository
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-1. 从Hugging Face仓库下载并加载语言模型
- en: '[PRE0]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO1-1)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO1-1)'
- en: Check if an NVIDIA GPU is available, and if so, set `device` to the current
    CUDA-enabled GPU. Otherwise, continue using the CPU.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 检查是否有可用的NVIDIA GPU，如果有，则将`device`设置为当前的CUDA启用GPU。否则，继续使用CPU。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO1-2)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO1-2)'
- en: Download and load the TinyLlama model into memory with a `float16` tensor precision
    data type.^([9](ch03.html#id667))
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`float16`精度数据类型下载并将TinyLlama模型加载到内存中.^([9](ch03.html#id667))
- en: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO1-3)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO1-3)'
- en: Move the whole pipeline to GPU on the first load.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次加载时将整个管道移动到GPU上。
- en: '[![4](assets/4.png)](#co_ai_integration_and_model_serving_CO1-4)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_ai_integration_and_model_serving_CO1-4)'
- en: Prepare the message list, which consists of dictionaries that have role and
    content key-value pairs. The order of the dictionaries dictates the order of messages
    from older to newer in a conversation. The first message is often a system prompt
    to guide the model’s output in a conversation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 准备消息列表，该列表由具有角色和内容键值对的字典组成。字典的顺序决定了对话中从旧到新消息的顺序。第一条消息通常是系统提示，用于引导模型在对话中的输出。
- en: '[![5](assets/5.png)](#co_ai_integration_and_model_serving_CO1-5)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_ai_integration_and_model_serving_CO1-5)'
- en: Convert the list of chat messages into a list of integer tokens for the model.
    The model is then asked to generate output in textual format, not integer tokens
    `tokenize=False`. A generation prompt is also added to the end of chat messages
    (`add_generation_prompt=True`) so that the model is encouraged to generate a response
    based on the chat history.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 将聊天消息列表转换为模型使用的整数令牌列表。然后要求模型以文本格式生成输出，而不是整数令牌（`tokenize=False`）。同时，在聊天消息的末尾添加一个生成提示（`add_generation_prompt=True`），以鼓励模型根据聊天历史生成响应。
- en: '[![6](assets/6.png)](#co_ai_integration_and_model_serving_CO1-6)'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_ai_integration_and_model_serving_CO1-6)'
- en: 'The prepared prompt is passed to the model with several inference parameters
    to optimize the text generation performance. A few of these key inference parameters
    include:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 将准备好的提示与几个推理参数一起传递给模型，以优化文本生成性能。其中一些关键推理参数包括：
- en: '`max_new_tokens`: Specifies the maximum number of new tokens to generate in
    the output.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_new_tokens`：指定在输出中生成的新令牌的最大数量。'
- en: '`do_sample`: Determines, when producing output, whether to pick a token randomly
    from a list of suitable tokens (`True`) or to simply choose the most likely token
    at each step (`False`).'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`do_sample`：在生成输出时确定是否从合适的令牌列表中随机选择一个（`True`）或简单地在每个步骤中选择最可能的令牌（`False`）。'
- en: '`temperature`: Controls the randomness of the output generation. Lower values
    make the model’s outputs more precise, while higher values allow for more creative
    responses.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`temperature`：控制输出生成的随机性。较低的值会使模型的输出更精确，而较高的值则允许有更多创造性的回答。'
- en: '`top_k`: Restricts the model’s token predictions to the top K options. `top_k=50`
    means create a list of top 50 most suitable tokens to pick from in the current
    token prediction step.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_k`：限制模型的令牌预测为前K个选项。`top_k=50`表示在当前令牌预测步骤中创建一个包含前50个最合适令牌的列表以供选择。'
- en: '`top_p`: Implements *nucleus sampling* when creating a list of most suitable
    tokens. `top_p=0.95` means create a list of the top tokens until you’re satisfied
    that your list has 95% of the most suitable tokens to pick from, for the current
    token prediction step.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`top_p`：在创建最合适令牌列表时实现*nucleus sampling*。`top_p=0.95`表示创建一个列表，直到您满意列表中有95%的最合适令牌可供当前令牌预测步骤选择。'
- en: '[![7](assets/7.png)](#co_ai_integration_and_model_serving_CO1-7)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_ai_integration_and_model_serving_CO1-7)'
- en: The final output is obtained from the `predictions` object. The generated text
    from TinyLlama includes the full conversation history, with the generated response
    appended to the end. The `</s>` stop token followed by `\n<|assistant|>\n` tokens
    are used to pick the content of the last message in the conversation, which is
    the model’s response.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出是从 `predictions` 对象中获得的。TinyLlama 生成的文本包括完整的对话历史，并将生成的响应附加到末尾。使用 `\n<|assistant|>\n`
    标记跟随 `</s>` 停止标记来选择对话中的最后一条消息的内容，即模型的响应。
- en: '[Example 3-1](#language_model_usage_example) is a good starting point; you
    can still load this model on your CPU and get responses within a reasonable time.
    However, TinyLlama may also not perform as well as its larger counterparts. For
    production workloads, you will want to use bigger models for better output quality
    and performance.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-1](#language_model_usage_example) 是一个良好的起点；你仍然可以在你的 CPU 上加载此模型并在合理的时间内获得响应。然而，TinyLlama
    可能也不会像其更大的版本表现得那么好。对于生产工作负载，你将希望使用更大的模型以获得更好的输出质量和性能。'
- en: You can now use the `load_model` and `predict` functions inside a controller
    function^([10](ch03.html#id668)) and then add a route handling decorator to serve
    the model via an endpoint, as shown in [Example 3-2](#text_endpoint).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以在控制器函数内使用 `load_model` 和 `predict` 函数^([10](ch03.html#id668))，然后添加一个路由处理装饰器，通过端点提供模型，如
    [示例 3-2](#text_endpoint) 所示。
- en: Example 3-2\. Serving a language model via a FastAPI endpoint
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-2\. 通过 FastAPI 端点提供语言模型
- en: '[PRE1]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO2-1)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO2-1)'
- en: Create a FastAPI server and add a `/generate` route handler for serving the
    model.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 FastAPI 服务器并添加一个 `/generate` 路由处理程序来提供模型服务。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO2-2)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO2-2)'
- en: The `serve_language_model_controller` is responsible for taking the prompt from
    the request query parameters.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`serve_language_model_controller` 负责从请求查询参数中获取提示。'
- en: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO2-3)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO2-3)'
- en: The model is loaded into memory.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 模型已加载到内存中。
- en: '[![4](assets/4.png)](#co_ai_integration_and_model_serving_CO2-4)'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_ai_integration_and_model_serving_CO2-4)'
- en: The controller passes the query to the model to perform the prediction.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器将查询传递给模型以执行预测。
- en: '[![5](assets/5.png)](#co_ai_integration_and_model_serving_CO2-5)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_ai_integration_and_model_serving_CO2-5)'
- en: The FastAPI server sends the output as an HTTP response to the client.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: FastAPI 服务器将输出作为 HTTP 响应发送给客户端。
- en: 'Once the FastAPI service is up and running, you can visit the Swagger documentation
    page located at `http://localhost:8000/docs` to test your new endpoint:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 FastAPI 服务启动并运行，你可以访问位于 `http://localhost:8000/docs` 的 Swagger 文档页面来测试你的新端点：
- en: '[PRE2]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If you’re running the code samples on a CPU, it will take around a minute to
    receive a response from the model, as shown in [Figure 3-11](#text_gen_response).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一个 CPU 上运行代码示例，它将花费大约一分钟的时间从模型那里收到响应，如 [图 3-11](#text_gen_response) 所示。
- en: '![bgai 0311](assets/bgai_0311.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0311](assets/bgai_0311.png)'
- en: Figure 3-11\. Response from TinyLlama
  id: totrans-127
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-11\. TinyLlama 的响应
- en: Not a bad response for a small language model (SLM) that runs on a CPU in your
    own computer, except that TinyLlama has *hallucinated* that FastAPI uses Flask.
    That is an incorrect statement; FastAPI uses Starlette as the underlying web framework,
    not Flask.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在您自己的计算机上运行的 CPU 上的小型语言模型（SLM）来说，这不是一个糟糕的响应，除了 TinyLlama 幻觉了 FastAPI 使用 Flask。这是一个错误的陈述；FastAPI
    使用 Starlette 作为底层 Web 框架，而不是 Flask。
- en: '*Hallucinations* refer to outputs that aren’t grounded in the training data
    or reality. Even though open source SLMs such as TinyLlama have been trained on
    impressive number of tokens (3 trillion), a small number of model parameters may
    have restricted their ability to learn the ground truth in data. Additionally,
    some unfiltered training data may also have been used, both of which can contribute
    to more instances of hallucinations.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '*幻觉* 指的是不基于训练数据或现实的输出。尽管开源 SLMs（如 TinyLlama）已经在令人印象深刻的数量（3 万亿）的标记上进行了训练，但一小部分模型参数可能限制了它们在数据中学习真实情况的能力。此外，一些未经筛选的训练数据也可能被使用，这两者都可能导致更多幻觉实例的发生。'
- en: Warning
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When serving language models, always let your users know to fact-check the outputs
    with external sources as language models may *hallucinate* and produce incorrect
    statements.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供语言模型时，始终让您的用户知道使用外部来源对输出进行事实核查，因为语言模型可能会 *产生幻觉* 并产生不正确的陈述。
- en: You can now use a web browser client in Python to visually test your service
    with more interactivity compared to using a command-line client.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以使用 Python 的 Web 浏览器客户端以比使用命令行客户端更多的交互性来测试你的服务。
- en: A great Python package to quickly develop a user interface is [Streamlit](https://oreil.ly/9BXmn),
    which enables you to create beautiful and customizable UIs for your AI services
    with little effort.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一个优秀的 Python 包 [Streamlit](https://oreil.ly/9BXmn)，可以让你轻松快速地开发用户界面，几乎不需要任何努力就能为你的
    AI 服务创建美观且可定制的 UI。
- en: Connecting FastAPI with Streamlit UI generator
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将 FastAPI 与 Streamlit UI 生成器连接。
- en: 'Streamlit allows you to easily create a chat user interface for testing and
    prototyping with models. You can install the `streamlit` package using `pip`:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Streamlit 允许你轻松创建用于测试和原型设计的聊天用户界面。你可以使用 `pip` 安装 `streamlit` 包：
- en: '[PRE3]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[Example 3-3](#streamlit_chat_ui) shows how to develop a simple UI to connect
    with your service.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-3](#streamlit_chat_ui) 展示了如何开发一个简单的 UI 来连接你的服务。'
- en: Example 3-3\. Streamlit chat UI consuming the FastAPI /`generate` endpoint
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-3\. Streamlit 聊天 UI 消费 FastAPI 的 `/generate` 端点
- en: '[PRE4]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO3-1)'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_ai_integration_and_model_serving_CO3-1)'
- en: Add a title to your application that will be rendered to the UI.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为你的应用程序添加一个标题，该标题将被渲染到 UI 中。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO3-2)'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)(#co_ai_integration_and_model_serving_CO3-2)'
- en: Initialize the chat and keep track of the chat history.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化聊天并跟踪聊天历史。
- en: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO3-3)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)(#co_ai_integration_and_model_serving_CO3-3)'
- en: Display the chat messages from the chat history on app rerun.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用重运行时显示聊天历史中的聊天消息。
- en: '[![4](assets/4.png)](#co_ai_integration_and_model_serving_CO3-4)'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)(#co_ai_integration_and_model_serving_CO3-4)'
- en: Wait until the user has submitted a prompt via the chat input field.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 等待用户通过聊天输入字段提交提示。
- en: '[![5](assets/5.png)](#co_ai_integration_and_model_serving_CO3-5)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png)(#co_ai_integration_and_model_serving_CO3-5)'
- en: Add the user or assistant messages to the chat history.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 将用户或助手的消息添加到聊天历史中。
- en: '[![6](assets/6.png)](#co_ai_integration_and_model_serving_CO3-6)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![6](assets/6.png)(#co_ai_integration_and_model_serving_CO3-6)'
- en: Display the user message in the chat message container.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在聊天消息容器中显示用户消息。
- en: '[![7](assets/7.png)](#co_ai_integration_and_model_serving_CO3-7)'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '![7](assets/7.png)(#co_ai_integration_and_model_serving_CO3-7)'
- en: Send a `GET` request with the prompt as a query parameter to your FastAPI endpoint
    to generate a response from TinyLlama.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 向你的 FastAPI 端点发送带有提示作为查询参数的 `GET` 请求以从 TinyLlama 生成响应。
- en: '[![8](assets/8.png)](#co_ai_integration_and_model_serving_CO3-8)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '![8](assets/8.png)(#co_ai_integration_and_model_serving_CO3-8)'
- en: Validate the response is OK.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 验证响应是否正常。
- en: '[![9](assets/9.png)](#co_ai_integration_and_model_serving_CO3-9)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '![9](assets/9.png)(#co_ai_integration_and_model_serving_CO3-9)'
- en: Display the assistant message in the chat message container.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在聊天消息容器中显示助手消息。
- en: You can now start your Streamlit client application:^([11](ch03.html#id674))
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以启动你的 Streamlit 客户端应用程序:^([11](ch03.html#id674))
- en: '[PRE5]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You should now be able to interact with TinyLlama inside Streamlit, as shown
    in [Figure 3-12](#streamlit_ui_text_results). All of this was possible with a
    few short Python scripts.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该能够像 [图 3-12](#streamlit_ui_text_results) 中所示的那样在 Streamlit 中与 TinyLlama
    交互。所有这些都可以通过几个简短的 Python 脚本实现。
- en: '![bgai 0312](assets/bgai_0312.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0312](assets/bgai_0312.png)'
- en: Figure 3-12\. Streamlit client
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-12\. Streamlit 客户端
- en: '[Figure 3-13](#tiny_llama_fastapi_architecture) shows the overall system architecture
    of the solution we’ve developed so far.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-13](#tiny_llama_fastapi_architecture) 展示了我们迄今为止开发的解决方案的整体系统架构。'
- en: '![bgai 0313](assets/bgai_0313.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0313](assets/bgai_0313.png)'
- en: Figure 3-13\. FastAPI service system architecture
  id: totrans-165
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-13\. FastAPI 服务系统架构
- en: Warning
  id: totrans-166
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: While the solution in [Example 3-3](#streamlit_chat_ui) is great for prototyping
    and testing models, it is not suitable for production workloads where several
    users would need simultaneous access to the model. This is because with the current
    setup, the model is loaded and unloaded onto memory every time a request is processed.
    Having to load/unload a large model to and from memory is slow and I/O blocking.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 [示例 3-3](#streamlit_chat_ui) 中的解决方案非常适合原型设计和测试模型，但它不适合生产工作负载，因为在这种情况下，多个用户需要同时访问模型。这是因为，在当前设置中，每次处理请求时，模型都会被加载到内存中并卸载。加载/卸载大型模型到内存中既慢又阻塞
    I/O。
- en: The TinyLlama service you’ve just built used a *decoder* transformer, optimized
    for conversational and chat use cases. However, the [original paper on transformers](https://oreil.ly/RqztC)
    introduced an architecture that consisted of both an encoder and a decoder.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚构建的 TinyLlama 服务使用了一个 *解码器* 变换器，该变换器针对对话和聊天用例进行了优化。然而，[关于变换器的原始论文](https://oreil.ly/RqztC)
    介绍了一个由编码器和解码器组成的架构。
- en: You should now feel more confident in the inner workings of language models
    and how to package them in a FastAPI web server.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该对语言模型的内部工作原理以及如何在 FastAPI 网络服务器中打包它们更有信心。
- en: Language models represent just a fraction of all generative models. The upcoming
    sections will expand your knowledge to include the function and serving of models
    that generate audio, images, and videos.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型只是所有生成模型中的一小部分。接下来的几节将扩展你的知识，包括生成音频、图像和视频的模型的功能和用途。
- en: We can start working with audio models first.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以先从音频模型开始工作。
- en: Audio Models
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 音频模型
- en: In GenAI services, audio models are important for creating interactive and realistic
    sounds. Unlike text models that you’re now familiar with, which focus on processing
    and generating text, audio models can handle audio signals. With them, you can
    synthesize speech, generate music, and even create sound effects for applications
    like virtual assistants, automated dubbing, game development, and immersive audio
    environments.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GenAI 服务中，音频模型对于创建交互性和逼真的声音非常重要。与你现在熟悉的、专注于处理和生成文本的文本模型不同，音频模型可以处理音频信号。有了它们，你可以合成语音、生成音乐，甚至为虚拟助手、自动配音、游戏开发和沉浸式音频环境等应用创建音效。
- en: One of the most capable text-to-speech and text-to-audio models is the Bark
    model created by Suno AI. This transformer-based model can generate realistic
    multilingual speech and audio including music, background noise, and sound effects.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Suno AI 创建的 Bark 模型是功能最强大的文本到语音和文本到音频模型之一。这个基于变换器的模型可以生成逼真的多语言语音和音频，包括音乐、背景噪音和音效。
- en: The Bark model consists of four models chained together as a pipeline to synthesize
    audio waveforms from textual prompts, as shown in [Figure 3-15](#bark_pipeline).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Bark 模型由四个模型串联作为管道，从文本提示中合成音频波形，如图 [图 3-15](#bark_pipeline) 所示。
- en: '![bgai 0315](assets/bgai_0315.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0315](assets/bgai_0315.png)'
- en: Figure 3-15\. Bark synthesis pipeline
  id: totrans-177
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-15\. Bark 合成管道
- en: 1. Semantic text model
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 语义文本模型
- en: A causal (sequential) autoregressive transformer model accepts tokenized input
    text and captures the meaning via semantic tokens. Autoregressive models predict
    future values in a sequence by reusing their own previous outputs.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一个因果（顺序）自回归变换器模型接受标记化的输入文本，并通过语义标记捕获意义。自回归模型通过重复使用自己的先前输出来预测序列中的未来值。
- en: 2. Coarse acoustics model
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 粗略声学模型
- en: A causal autoregressive transformer receives the semantic model’s outputs and
    generates the initial audio features, which lack finer details. Each prediction
    is based on past and present information in the semantic token sequence.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一个因果自回归变换器接收语义模型的输出并生成初始音频特征，这些特征缺乏更精细的细节。每个预测都基于语义标记序列中的过去和现在信息。
- en: 3. Fine acoustics model
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 精细声学模型
- en: A noncausal auto-encoder transformer refines the audio representation by generating
    the remaining audio features. As the coarse acoustics model has generated the
    entire audio sequence, the fine model doesn’t need to be casual.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非因果自动编码器变换器通过生成剩余的音频特征来精炼音频表示。由于粗略声学模型已经生成了整个音频序列，精细模型不需要是因果的。
- en: 4. Encodec audio codec model
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 4. Encodec 音频编解码器模型
- en: The model decodes the output audio array from all previously generated audio
    codes.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 模型解码来自所有先前生成的音频代码的输出音频数组。
- en: Bark synthesizes the audio waveform by decoding the refined audio features into
    the final audio output in the form of spoken words, music, or simple audio effects.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Bark 通过将精炼的音频特征解码成最终音频输出（以语音、音乐或简单的音频效果的形式）来合成音频波形。
- en: '[Example 3-4](#small_bark) shows how to use the small Bark model.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-4](#small_bark) 展示了如何使用小的 Bark 模型。'
- en: Example 3-4\. Download and load the small Bark model from the Hugging Face repository
  id: totrans-188
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-4\. 从 Hugging Face 仓库下载并加载小的 Bark 模型
- en: '[PRE6]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO4-1)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)[#co_ai_integration_and_model_serving_CO4-1]'
- en: Specify supported voice preset options using a `Literal` type.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `Literal` 类型指定支持的语音预设选项。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO4-2)'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)[#co_ai_integration_and_model_serving_CO4-2]'
- en: Download the small Bark processor, which prepares the input text prompt for
    the core model.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 下载小型的Bark处理器，该处理器为核心模型准备输入文本提示。
- en: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO4-3)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)(#co_ai_integration_and_model_serving_CO4-3)'
- en: Download the Bark model, which will be used to generate the output audio. Both
    objects will be needed for audio generation later.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 下载Bark模型，该模型将用于生成输出音频。这两个对象在后续的音频生成中都将被需要。
- en: '[![4](assets/4.png)](#co_ai_integration_and_model_serving_CO4-4)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)(#co_ai_integration_and_model_serving_CO4-4)'
- en: Preprocess the text prompt with a speaker voice preset embedding and return
    a Pytorch tensor array of tokenized inputs using `return_tensors="pt"`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 使用说话人语音预设嵌入对文本提示进行预处理，并使用`return_tensors="pt"`返回一个标记化输入的Pytorch张量数组。
- en: '[![5](assets/5.png)](#co_ai_integration_and_model_serving_CO4-5)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png)(#co_ai_integration_and_model_serving_CO4-5)'
- en: Generate an audio array that contains amplitude values of the synthesized audio
    signal over time.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 生成包含合成音频信号随时间变化的振幅值的音频数组。
- en: '[![6](assets/6.png)](#co_ai_integration_and_model_serving_CO4-6)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![6](assets/6.png)(#co_ai_integration_and_model_serving_CO4-6)'
- en: Get the sampling rate from model generating configurations, which can be used
    to produce the audio.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 从生成音频的模型配置中获取采样率，该采样率可以用于生成音频。
- en: When you generate audio using a model, the output is a sequence of floating-point
    numbers that represent the *amplitude* (or strength) of the audio signal at each
    point in time.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用模型生成音频时，输出是一个表示音频信号在每个时间点上的*振幅*（或强度）的浮点数序列。
- en: To play back this audio, it needs to be converted to a digital format that can
    be sent to the speakers. This involves sampling the audio signal at a fixed rate
    and quantizing the amplitude values to a fixed number of bits. The `soundfile`
    library can help you here by generating the audio file using a *sampling rate*.
    The higher the sampling rate, the more samples that are taken, which enhances
    the audio quality but also increases the file size.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 要播放此音频，需要将其转换为可以发送到扬声器的数字格式。这涉及到以固定速率采样音频信号并将振幅值量化为固定数量的位。`soundfile`库可以通过使用*采样率*生成音频文件来帮助您。采样率越高，采样的样本越多，这提高了音频质量，但也增加了文件大小。
- en: 'You can install the `soundfile` audio library for writing audio files using
    `pip`:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`pip`安装`soundfile`音频库来使用`pip`写入音频文件：
- en: '[PRE7]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[Example 3-5](#audio_endpoint) shows how you can stream the audio content to
    the client.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 3-5](#audio_endpoint)展示了如何将音频内容流式传输到客户端。'
- en: Example 3-5\. FastAPI endpoint for returning generated audio
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-5\. 返回生成音频的FastAPI端点
- en: '[PRE8]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO5-1)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_ai_integration_and_model_serving_CO5-1)'
- en: Install the `soundfile` library to write the audio array to memory buffer using
    its sampling rate.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 安装`soundfile`库，使用其采样率将音频数组写入内存缓冲区。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO5-2)'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)(#co_ai_integration_and_model_serving_CO5-2)'
- en: Reset the buffer cursor to the start of the buffer and return the iterable buffer.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 将缓冲区光标重置到缓冲区起始位置，并返回可迭代的缓冲区。
- en: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO5-3)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)(#co_ai_integration_and_model_serving_CO5-3)'
- en: Create a new audio endpoint that returns the `audio/wav` content type as `StreamingResponse`.
    `StreamingResponse` is typically used when you want to stream the response data,
    such as when returning large files or when generating the response data. It allows
    you to return a generator function that yields chunks of data to be sent to the
    client.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的音频端点，该端点返回`audio/wav`内容类型作为`StreamingResponse`。`StreamingResponse`通常用于您想要流式传输响应数据时，例如返回大文件或生成响应数据时。它允许您返回一个生成器函数，该函数产生数据块以发送给客户端。
- en: '[![4](assets/4.png)](#co_ai_integration_and_model_serving_CO5-4)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)(#co_ai_integration_and_model_serving_CO5-4)'
- en: Convert the generated audio array to an iterable buffer that can be passed to
    streaming response.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 将生成的音频数组转换为可传递给流式响应的可迭代缓冲区。
- en: In [Example 3-5](#audio_endpoint), you generated an audio array using the small
    Bark model and streamed the memory buffer of the audio content. Streaming is more
    efficient for larger files as the client can consume the content as it is being
    served. In previous examples, we didn’t use streaming responses, as generated
    images or text can be fairly small compared to audio or video content.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例 3-5](#audio_endpoint)中，您使用小型的Bark模型生成音频数组，并流式传输了音频内容的内存缓冲区。对于较大的文件，流式传输更有效，因为客户端可以在内容被提供时消费内容。在先前的示例中，我们没有使用流式响应，因为生成的图像或文本与音频或视频内容相比可以相当小。
- en: Tip
  id: totrans-218
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Streaming audio content directly from a memory buffer is faster and more efficient
    than writing the audio array to a file and streaming the content from the hard
    drive.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 直接从内存缓冲区流式传输音频内容比将音频数组写入文件并从硬盘驱动器流式传输内容更快、更高效。
- en: If you need the memory available for other tasks, you can write the audio array
    to a file first and then stream from it using a file reader generator. You will
    be trading off latency for memory.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要为其他任务保留内存，您可以首先将音频数组写入文件，然后使用文件读取生成器从它流式传输。您将是在延迟和内存之间进行权衡。
- en: Now that you have an audio generation endpoint, you can update your Streamlit
    UI client code to render audio messages. Update your Streamlit client code as
    shown in [Example 3-6](#barksmall_streamlit_ui).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经有了音频生成端点，您可以更新Streamlit UI客户端代码以渲染音频消息。按照[示例3-6](#barksmall_streamlit_ui)所示更新您的Streamlit客户端代码。
- en: Example 3-6\. Streamlit audio UI consuming the FastAPI `/audio` generation endpoint
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-6\. Streamlit音频UI消耗FastAPI `/audio`生成端点
- en: '[PRE9]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO6-1)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO6-1)'
- en: Update the Streamlit client code to render audio content.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 更新Streamlit客户端代码以渲染音频内容。
- en: With Streamlit, you can swap components to render any type of content including
    images, audio, and video.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Streamlit，您可以交换组件以渲染任何类型的内容，包括图像、音频和视频。
- en: You should now be able to generate highly realistic speech audio in your updated
    Streamlit UI, as shown in [Figure 3-16](#streamlit_bark_ui).
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您应该能够在更新的Streamlit UI中生成高度逼真的语音音频，如图[图3-16](#streamlit_bark_ui)所示。
- en: '![bgai 0316](assets/bgai_0316.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0316](assets/bgai_0316.png)'
- en: Figure 3-16\. Rendering audio responses in the Streamlit UI
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-16\. 在Streamlit UI中渲染音频响应
- en: Bear in mind that you’re using the compressed version of the Bark model, but
    with the light version, you can generate speech and music audio fairly quickly
    even on a single CPU. This is in exchange for some audio generation quality.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，您正在使用Bark模型的压缩版本，但使用轻量版本，您可以在单个CPU上相当快速地生成语音和音乐音频。这是以一些音频生成质量为代价的。
- en: You should now feel more comfortable serving larger content to your users via
    streaming responses and working with audio models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在应该已经能够更舒适地通过流式响应和与音频模型协作向用户提供服务更大的内容。
- en: So far, you’ve been building conversational and text-to-speech services. Now
    let’s see how to interact with a vision model to build an image generator service.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您一直在构建对话和文本到语音服务。现在让我们看看如何与视觉模型交互以构建图像生成服务。
- en: Vision Models
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉模型
- en: Using vision models, you can generate, enhance, and understand visual information
    from prompts.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 使用视觉模型，您可以从提示中生成、增强和理解视觉信息。
- en: Since these models can produce very realistic outputs faster than any human
    and can understand and manipulate existing visual content, they’re extremely useful
    for applications like image generators and editors, object detection, image classification
    and captioning, and augmented reality.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些模型可以比任何人类更快地生成非常逼真的输出，并且可以理解和操纵现有的视觉内容，因此它们对于图像生成器和编辑器、目标检测、图像分类和字幕以及增强现实等应用非常有用。
- en: One of the most popular architectures used to train image models is called *Stable
    Diffusion* (SD).
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练图像模型的最流行的架构之一被称为*稳定扩散*（SD）。
- en: SD models are trained to encode input images into a latent space. This latent
    space is the mathematical representation of patterns in the training data that
    the model has learned. If you try to visualize an encoded image, all you would
    see is a white noise image, similar to the black and white dots you would see
    on your TV screen when it loses signal.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: SD模型被训练来将输入图像编码到潜在空间。这个潜在空间是模型学习到的训练数据中模式的数学表示。如果您尝试可视化编码后的图像，您将看到的只是一个白色噪声图像，类似于您在电视信号丢失时在电视屏幕上看到的黑白点。
- en: '[Figure 3-17](#stable_diffusion) shows the full process for training and inference
    and visualizes how images are encoded and decoded via the forward and reverse
    diffusion processes. A text encoder using text, images, and semantic maps assists
    in controlling the output via the reverse diffusion.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-17](#stable_diffusion)展示了训练和推理的完整过程，并可视化了图像通过正向和反向扩散过程进行编码和解码。一个使用文本、图像和语义图的文本编码器有助于通过反向扩散控制输出。'
- en: '![bgai 0317](assets/bgai_0317.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0317](assets/bgai_0317.png)'
- en: Figure 3-17\. Stable Diffusion training and inference
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-17\. 稳定扩散训练和推理
- en: What makes these models magical is their ability to decode noisy images back
    into original input images. Effectively, the SD models also learn to remove white
    noise from an encoded image to reproduce the original image. The model performs
    this denoising process over several iterations.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型之所以神奇，在于它们将噪声图像解码回原始输入图像的能力。实际上，SD模型还学会了从编码图像中去除白噪声以重现原始图像。模型通过多次迭代执行此去噪过程。
- en: However, you don’t want to re-create images you already have. You will want
    the model to create new, never-before-seen images. But how can an SD model achieve
    this for you? The answer lies in the latent space where the encoded noisy images
    live. You can change the noise in these images so that when the model denoises
    them and decodes them back, you get a whole new image that the model has never
    seen before.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，您不希望重新创建您已经拥有的图像。您希望模型创建新的、以前从未见过的图像。但SD模型如何为您实现这一点呢？答案在于编码的噪声图像存在的潜在空间。您可以改变这些图像中的噪声，当模型去噪并解码它们时，您会得到一个全新的图像，这是模型以前从未见过的。
- en: 'A challenge remains: how can you control the image generation process so that
    the model doesn’t produce random images? The solution is to also encode image
    descriptions alongside the image. The patterns in the latent space are then mapped
    to textual image descriptions of what is seen in each input image. Now, you use
    textual prompts to sample the noisy latent space such that the produced output
    image after the denoising process is what you want.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然存在一个挑战：如何控制图像生成过程，使得模型不会生成随机的图像？解决方案是同时将图像描述编码到图像中。然后，潜在空间中的模式被映射到每个输入图像中看到的文本图像描述。现在，您使用文本提示来采样带噪声的潜在空间，以便在去噪过程后的输出图像正是您想要的。
- en: This is how SD models can generate new images that they’ve never seen before
    in their training data. In essence, these models navigate a latent space that
    contains encoded representations of various patterns and meanings.^([12](ch03.html#id686))
    The model iteratively refines this noise through a denoising process to produce
    a novel image not present in its training dataset.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是SD模型如何生成它们在训练数据中从未见过的新的图像。本质上，这些模型在包含各种模式和意义的编码表示的潜在空间中导航。模型通过去噪过程迭代地细化噪声，以产生一个在训练数据集中不存在的创新图像。
- en: 'To download an SD model, you will need to have the Hugging Face `diffusers`
    library installed:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载SD模型，您需要安装Hugging Face的`diffusers`库：
- en: '[PRE10]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[Example 3-7](#sd_model_usage_example) shows how to load an SD model into memory.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例3-7](#sd_model_usage_example)展示了如何将SD模型加载到内存中。'
- en: Example 3-7\. Download and load an SD model from the Hugging Face repository
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-7\. 从Hugging Face仓库下载并加载SD模型
- en: '[PRE11]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO7-1)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO7-1)'
- en: 'Download and load the TinySD model into memory with the less memory efficient
    `float32` tensor type. Using `float16`, which has limited precision for large
    and complex models, leads to numerical instability and loss of accuracy. Additionally,
    hardware support for `float16` is limited, so trying to run an SD model on your
    CPU with the `float16` tensor type may not be possible. Source: [Hugging Face](https://oreil.ly/rzw8P).'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 使用内存效率较低的`float32`张量类型将TinySD模型下载并加载到内存中。使用具有有限精度的`float16`，对于大型和复杂模型会导致数值不稳定和精度损失。此外，对`float16`的硬件支持有限，因此尝试在CPU上使用`float16`张量类型运行SD模型可能不可行。来源：[Hugging
    Face](https://oreil.ly/rzw8P)。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO7-2)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO7-2)'
- en: Pass the text prompt to the model to generate a list of images and pick the
    first one. Some models allow you to generate multiple images in a single inference
    step.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 将文本提示传递给模型以生成一系列图像，并选择第一个。一些模型允许您在单个推理步骤中生成多个图像。
- en: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO7-3)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO7-3)'
- en: The `num_inference_steps=10` specifies the number of diffusion steps to perform
    during inference. In each diffusion step, a stronger noisy image is produced from
    previous diffusion steps. The model generates multiple noisy images by undertaking
    multiple diffusion steps. With these images, the model can better understand the
    patterns of noise that are present in the input data and learn to remove them
    more effectively. The more inference steps, the better results you will get, but
    at the cost of computing power needed and longer processing times.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_inference_steps=10` 指定了推理过程中要执行的扩散步骤的数量。在每一步扩散中，从之前的扩散步骤生成一个更强的噪声图像。模型通过执行多个扩散步骤生成多个噪声图像。有了这些图像，模型可以更好地理解输入数据中存在的噪声模式，并学会更有效地去除它们。推理步骤越多，你得到的结果越好，但这也需要更多的计算能力和更长的处理时间。'
- en: '[![4](assets/4.png)](#co_ai_integration_and_model_serving_CO7-4)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)(#co_ai_integration_and_model_serving_CO7-4)'
- en: The generated image will be a Python Pillow image type, so you have access to
    a variety of Pillow’s image methods for post-processing and storage. For instance,
    you can call the `image.save()` method to store the image in your filesystem.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图像将是一个 Python Pillow 图像类型，因此你可以访问 Pillow 的各种图像方法进行后期处理和存储。例如，你可以调用 `image.save()`
    方法将图像存储到你的文件系统中。
- en: Note
  id: totrans-258
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Vision models are extremely resource hungry. To load and use a small vision
    model such as TinySD on CPU, you will need around 5 GB of disk space and RAM.
    However, you can install `accelerate` using `pip install accelerate` to optimize
    resources required so that the model pipeline uses lower CPU memory usage.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉模型非常占用资源。要在 CPU 上加载和使用像 TinySD 这样的小型视觉模型，你需要大约 5 GB 的磁盘空间和 RAM。然而，你可以使用 `pip
    install accelerate` 安装 `accelerate` 来优化所需资源，以便模型管道使用更低的 CPU 内存使用量。
- en: When serving video models, you will need to use a GPU. Later in this chapter,
    I will show you how to leverage GPUs for video models.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 当提供视频模型时，你需要使用 GPU。在本章的后面部分，我将向你展示如何利用 GPU 为视频模型提供支持。
- en: You can now package this model into another endpoint as similar to [Example 3-2](#text_endpoint),
    with the difference being that the returned response will be an image binary (not
    text). Refer to [Example 3-8](#image_endpoint).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以将这个模型打包到另一个端点，就像 [示例 3-2](#text_endpoint) 一样，不同之处在于返回的响应将是一个图像二进制文件（而不是文本）。请参阅
    [示例 3-8](#image_endpoint)。
- en: Example 3-8\. FastAPI endpoint for returning a generated image
  id: totrans-262
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-8\. 返回生成图像的 FastAPI 端点
- en: '[PRE12]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO8-1)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_ai_integration_and_model_serving_CO8-1)'
- en: Create an in-memory buffer, save the image to this buffer in a given format,
    and then return the raw byte data from the buffer.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个内存中的缓冲区，以给定格式将图像保存到该缓冲区中，然后从缓冲区返回原始字节数据。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO8-2)'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)(#co_ai_integration_and_model_serving_CO8-2)'
- en: Specify the media content type and status codes for the auto-generated Swagger
    UI documentation page.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 指定自动生成的 Swagger UI 文档页面的媒体内容类型和状态码。
- en: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO8-3)'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)(#co_ai_integration_and_model_serving_CO8-3)'
- en: Specify the response class to prevent FastAPI from adding `application/json`
    as an additional acceptable response media type.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 指定响应类以防止 FastAPI 添加 `application/json` 作为另一个可接受响应媒体类型。
- en: '[![4](assets/4.png)](#co_ai_integration_and_model_serving_CO8-4)'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)(#co_ai_integration_and_model_serving_CO8-4)'
- en: The response returned from the model will be Pillow image format.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 模型返回的响应将是 Pillow 图像格式。
- en: '[![5](assets/5.png)](#co_ai_integration_and_model_serving_CO8-5)'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png)(#co_ai_integration_and_model_serving_CO8-5)'
- en: We will need to use the FastAPI `Response` class to send a special response
    carrying image bytes with a PNG media type.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将需要使用 FastAPI 的 `Response` 类来发送一个携带图像字节的特殊响应，并带有 PNG 媒体类型。
- en: '[Figure 3-18](#tinysd_swagger_docs) shows the results of testing the new `/generate/image`
    endpoint via FastAPI Swagger docs with the text prompt `A cosy living room with
    trees in it`.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-18](#tinysd_swagger_docs) 展示了通过 FastAPI Swagger 文档测试新的 `/generate/image`
    端点，使用文本提示 `一个带树木的舒适客厅`。'
- en: '![bgai 0318](assets/bgai_0318.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0318](assets/bgai_0318.png)'
- en: Figure 3-18\. TinySD FastAPI service
  id: totrans-276
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-18\. TinySD FastAPI 服务
- en: Now, connect your endpoint to a Streamlit UI for prototyping, as shown in [Example 3-9](#tinysd_streamlit_code).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将你的端点连接到 Streamlit UI 进行原型设计，如图 [示例 3-9](#tinysd_streamlit_code) 所示。
- en: Example 3-9\. Streamlit Vision UI consuming the FastAPI `*/image*` generation
    endpoint
  id: totrans-278
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-9\. Streamlit Vision UI 消费 FastAPI `*/image*` 生成端点
- en: '[PRE13]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO9-1)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_ai_integration_and_model_serving_CO9-1)'
- en: Images transferred over the HTTP protocol will be in binary format. Therefore,
    we update the display function to render binary image content. You can use the
    `st.image` method to display images to the UI.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 通过HTTP协议传输的图像将以二进制格式存在。因此，我们更新了显示函数以渲染二进制图像内容。您可以使用`st.image`方法将图像显示到UI上。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO9-2)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO9-2)'
- en: Update the `GET` request to hit the `/generate/image` endpoint. Then, render
    a textual and image message to the user.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 更新`GET`请求以访问`/generate/image`端点。然后，向用户渲染文本和图像消息。
- en: '[Figure 3-19](#tinysd_streamlitui) shows the final results of the user experience
    with the model.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-19](#tinysd_streamlitui)显示了用户与该模型的最终用户体验结果。'
- en: '![bgai 0319](assets/bgai_0319.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0319](assets/bgai_0319.png)'
- en: Figure 3-19\. Rendering image messages in the Streamlit UI
  id: totrans-286
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-19。在Streamlit UI中渲染图像消息
- en: We saw how even with a tiny SD model, you can generate reasonable looking images.
    The XL versions can produce even more realistic images but still have their own
    limitations.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，即使是一个微小的SD模型，也可以生成看起来合理的图像。XL版本可以生成更加逼真的图像，但仍然有其自身的限制。
- en: 'At the time of writing, the current open source SD models do have certain limitations:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，当前的开放源代码SD模型确实存在某些限制：
- en: Coherency
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '**连贯性**'
- en: The models can’t produce every detail described in the prompts and complex compositions.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 模型无法生成提示中描述的每个细节和复杂的构图。
- en: Output size
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出大小**'
- en: The output images can only be predefined sizes such as 512 × 512 or 1024 × 1024
    pixels.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 输出图像只能是预定义的大小，例如512 × 512或1024 × 1024像素。
- en: Composability
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '**可组合性**'
- en: You can’t fully control the generated image and define composition in the image.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 您无法完全控制生成的图像并在图像中定义构图。
- en: Photorealism
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**真实感**'
- en: The generated outputs do show details that give away they’ve been generated
    by AI.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的输出确实显示了细节，表明它们是由AI生成的。
- en: Legible text
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '**可读文本**'
- en: Some models cannot generate legible texts.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 一些模型无法生成可读的文本。
- en: The `tinysd` model you worked with is an early phase model that has undergone
    the *distillation* process (i.e., compression) from the larger V1.5 SD model.
    As a result, the generated outputs may not meet production standards or be entirely
    cohesive and could fail to incorporate all the concepts mentioned in the text
    prompts. However, the distilled models may perform well if you [*fine-tune* them
    using *Low-Rank Adaptation* (LoRA)](https://oreil.ly/Nqtkm) on specific concepts/styles.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 您所使用的`tinysd`模型是一个早期阶段模型，它已经从更大的V1.5 SD模型中经历了**蒸馏**过程（即压缩）。因此，生成的输出可能不符合生产标准，或者可能不完全连贯，并且可能无法包含文本提示中提到的所有概念。然而，如果您在特定概念/风格上使用**低秩适应**（LoRA）[*微调*](https://oreil.ly/Nqtkm)这些蒸馏模型，它们可能表现良好。
- en: You can now build both text- and image-based GenAI services. However, you may
    be wondering how to build text-to-video services based on video models. Let’s
    learn more about video models, how they work, and how to build an image animator
    service with them next.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以构建基于文本和图像的GenAI服务。然而，您可能想知道如何基于视频模型构建文本到视频服务。让我们更多地了解视频模型，了解它们的工作原理，以及如何使用它们构建图像动画服务。
- en: Video Models
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**视频模型**'
- en: Video models are some of the most resource-hungry generative models and often
    require a GPU to produce a short snippet of good quality. These models have to
    generate several tens of frames to produce a single second of video, even without
    any audio content.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 视频模型是一些最资源密集型的生成模型，通常需要GPU来生成一段高质量的视频片段。这些模型必须生成几十帧才能产生一秒的视频，即使没有任何音频内容。
- en: Stability AI has released several open source video models based on the SD architecture
    on Hugging Face. We will work with the compressed version of their image-to-video
    model for a faster image animation service.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: Stability AI已经在Hugging Face上发布了基于SD架构的几个开源视频模型。我们将使用他们图像到视频模型的压缩版本来提供更快的图像动画服务。
- en: To get started, let’s get a small image-to-video model running using [Example 3-10](#video_model_loading).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，让我们使用[示例3-10](#video_model_loading)启动一个小型的图像到视频模型。
- en: Note
  id: totrans-305
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '**注意**'
- en: To run [Example 3-10](#video_model_loading), you may need access to a CUDA-capable
    NVIDIA GPU.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行[示例3-10](#video_model_loading)，您可能需要访问具有CUDA功能的NVIDIA GPU。
- en: Also, for commercial use of the `stable-video-diffusion-img2vid` model, please
    refer to its [model card](https://oreil.ly/DM-0p).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于`stable-video-diffusion-img2vid`模型的商业用途，请参阅其[模型卡片](https://oreil.ly/DM-0p)。
- en: Example 3-10\. Download and load the Stability AI’s *img2vid* model from the
    Hugging Face repository
  id: totrans-308
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-10。从Hugging Face存储库下载并加载Stability AI的*img2vid*模型
- en: '[PRE14]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO10-1)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO10-1)'
- en: Resize the input image to a standard size expected by model input. Resizing
    will also protect against large inputs.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 将输入图像调整到模型输入所期望的标准尺寸。调整大小也将防止大输入。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO10-2)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO10-2)'
- en: Create a random tensor generator with the seed set to 42 for reproducible video
    frame generation.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个随机张量生成器，种子设置为42，以实现可重复的视频帧生成。
- en: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO10-3)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO10-3)'
- en: Run the frame generation pipeline to produce all video frames at once. Grab
    the first batch of generated frames. This step requires significant video memory.
    `num_frames` specifies the number of frames to generate, while `decode_chunk_size`
    specifies how many frames to generate at once.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 运行帧生成管道以一次性生成所有视频帧。抓取生成的第一批帧。这一步需要大量的视频内存。`num_frames`指定要生成的帧数，而`decode_chunk_size`指定一次性生成多少帧。
- en: With the model loading functions in place, you can now build the video-serving
    endpoint.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型加载函数就绪后，您现在可以构建视频服务端点。
- en: However, before you proceed with declaring the route handler, you do need a
    utility function to process the video model outputs from frames into a streamable
    video using an I/O buffer.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在声明路由处理程序之前，您确实需要一个实用函数来处理从帧到使用I/O缓冲区可流式传输的视频的视频模型输出。
- en: To export a sequence of frames to videos, you need to encode them into a video
    container using a video library such as `av`, which implements Python bindings
    to the popular `ffmpeg` video processing library.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 要将一系列帧导出为视频，您需要使用视频库（如`av`）将它们编码到视频容器中，该库实现了对流行的`ffmpeg`视频处理库的Python绑定。
- en: 'You can install the `av` library via:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下方式安装`av`库：
- en: '[PRE15]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now you can use [Example 3-11](#frames_to_videos) to create streamable video
    buffers.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用[示例3-11](#frames_to_videos)创建可流式传输的视频缓冲区。
- en: Example 3-11\. Exporting video model output from frames to a streamable video
    buffer using the `av` library
  id: totrans-322
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-11\. 使用`av`库将帧的视频模型输出导出到可流式传输的视频缓冲区
- en: '[PRE16]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO11-1)'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO11-1)'
- en: Open a buffer for writing an MP4 file and then configure a video stream with
    AV’s video multiplexer.^([13](ch03.html#id697))
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 打开一个用于写入MP4文件的缓冲区，然后配置一个使用AV的视频复用器的视频流.^([13](ch03.html#id697))
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO11-2)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO11-2)'
- en: Set the video encoding to `h264` at 30 frames per second and make sure the frame
    dimensions match the frames provided to the function.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 将视频编码设置为每秒30帧的`h264`，并确保帧的尺寸与函数提供的帧相匹配。
- en: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO11-3)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO11-3)'
- en: Set the pixel format of the video stream to `yuv444p` so that each pixel has
    the full resolution for the `y` (luminance or brightness) and both `u` and `v`
    (chrominance or color) components.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 将视频流的像素格式设置为`yuv444p`，以便每个像素都有完整的分辨率，包括`y`（亮度或亮度）以及`u`和`v`（色度或颜色）分量。
- en: '[![4](assets/4.png)](#co_ai_integration_and_model_serving_CO11-4)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](assets/4.png)](#co_ai_integration_and_model_serving_CO11-4)'
- en: Configure the stream’s constant rate factor (CRF) to control the video quality
    and compression. Set the CRF to 17 to output a lossless high-quality video with
    minimal compression.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 配置流的恒定比特率（CRF）以控制视频质量和压缩。将CRF设置为17以输出无损高质量视频，压缩量最小。
- en: '[![5](assets/5.png)](#co_ai_integration_and_model_serving_CO11-5)'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](assets/5.png)](#co_ai_integration_and_model_serving_CO11-5)'
- en: Encode the input frames into encoded packets with the configured stream video
    multiplexer.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 使用配置的视频流复用器将输入帧编码成编码数据包。
- en: '[![6](assets/6.png)](#co_ai_integration_and_model_serving_CO11-6)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](assets/6.png)](#co_ai_integration_and_model_serving_CO11-6)'
- en: Add the encoded frames into the opened video container buffer.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 将编码帧添加到打开的视频容器缓冲区中。
- en: '[![7](assets/7.png)](#co_ai_integration_and_model_serving_CO11-7)'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](assets/7.png)](#co_ai_integration_and_model_serving_CO11-7)'
- en: Flush any remaining frames in the encoder and combine the resulting packet into
    the output file before returning the buffer containing the encoded video.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在返回包含编码视频的缓冲区之前，清除编码器中剩余的帧并将生成的数据包组合到输出文件中。
- en: To use image prompts with the service as file uploads, you must install the
    `python-multipart` library:^([14](ch03.html#id698))
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用服务作为文件上传使用图像提示，您必须安装`python-multipart`库:^([14](ch03.html#id698))
- en: '[PRE17]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Once installed, you can set up the new endpoint using [Example 3-12](#video_endpoint).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，您可以使用 [示例 3-12](#video_endpoint) 设置新的端点。
- en: Example 3-12\. Serving generated videos from the image-to-video model
  id: totrans-341
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-12\. 从图像到视频模型提供生成的视频
- en: '[PRE18]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO12-1)'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO12-1)'
- en: Use the `File` object to specify `image` as a form file upload.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `File` 对象将 `image` 指定为一个表单文件上传。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO12-2)'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO12-2)'
- en: Create a Pillow `Image` object by passing the image bytes transferred to the
    service. The model pipeline expects a Pillow image format as input.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 通过传递给服务的图像字节创建一个 Pillow `Image` 对象。模型管道期望输入的是 Pillow 图像格式。
- en: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO12-3)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO12-3)'
- en: Export the generated frames as a MP4 video and stream it to the client using
    an iterable video buffer.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 将生成的帧导出为 MP4 视频并使用可迭代视频缓冲区将其流式传输到客户端。
- en: With the video endpoint set up, you can now upload images to your FastAPI service
    to animate them as videos.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好视频端点后，您现在可以上传图像到您的 FastAPI 服务中，将其动画化为视频。
- en: There are other video models available on the hub that allow you to generate
    GIFs and animations. For additional practice, you can try building a GenAI service
    with them. While open source video models can produce videos at ample quality,
    OpenAI’s announcement of a new large vision model (LVM) called Sora has shaken
    the video generation industry.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在 hub 上还有其他视频模型可供使用，允许您生成 GIF 和动画。为了进一步练习，您可以尝试使用它们构建一个 GenAI 服务。虽然开源视频模型可以产生高质量的视频，但
    OpenAI 宣布的新大型视觉模型（LVM）Sora 已经震撼了视频生成行业。
- en: OpenAI Sora
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenAI Sora
- en: 'Text-to-video models are limited in their generation capabilities. Apart from
    the immense computational power needed to sequentially generate coherent video
    frames, training these models can be challenging due to:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 文本到视频模型在生成能力上有限。除了需要巨大的计算能力来顺序生成连贯的视频帧之外，由于以下原因，训练这些模型可能具有挑战性：
- en: '*Maintaining temporal and spatial consistency across frames* to achieve realistic
    undistorted video outputs.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在帧之间保持时间和空间一致性，以实现逼真的无扭曲视频输出*。'
- en: '*Lack of training data* with high-quality caption and metadata needed to train
    video models.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缺乏高质量的标题和元数据训练视频模型所需的数据*。'
- en: '*Captioning challenges* when captioning the content of videos clearly and descriptively
    is time-consuming and moves beyond drafting short pieces of text. Captioning must
    describe the narrative and scenes for each sequence for the model to learn and
    map the rich patterns contained in the video to text.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在清晰和描述性地描述视频内容时存在标题挑战*，这需要花费时间，并且超出了起草简短文本的范围。为了使模型能够学习和映射视频中所包含的丰富模式到文本，标题必须描述每个序列的叙事和场景。'
- en: Because of these reasons, there has not been a breakthrough with video generation
    models until the announcement of OpenAI’s Sora model.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些原因，直到 OpenAI 的 Sora 模型发布之前，视频生成模型没有取得突破。
- en: Sora is a generalist large vision diffusion transformer model capable of generating
    videos and images spanning diverse durations, aspect ratios, and resolutions,
    up to a full minute of high-definition video. Its architecture is based on the
    transformers commonly used in LLMs and the diffusion process. Whereas LLMs use
    text tokens, Sora uses visual patches.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: Sora 是一个通用的大视觉扩散变换器模型，能够生成跨越不同时长、宽高比和分辨率的视频和图像，最高可达一分钟的高清视频。其架构基于在 LLM 中常用到的变换器和扩散过程。而
    LLM 使用文本标记，Sora 使用视觉块。
- en: Tip
  id: totrans-358
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: The Sora model combines elements and principles of the transformer and SD architectures,
    while in [Example 3-10](#video_model_loading), you used Stability AI’s SD model
    to generate videos.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: Sora 模型结合了变换器和 SD 架构的元素和原则，而在 [示例 3-10](#video_model_loading) 中，您使用了 Stability
    AI 的 SD 模型来生成视频。
- en: So what makes Sora different?
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，Sora 有什么不同之处？
- en: Transformers have demonstrated remarkable scalability across language models,
    computer vision, and image generation, so it made sense for Sora architecture
    to be based on transformers to handle diverse inputs like text, images, or video
    frames. Also, since transformers can understand complex patterns and long-range
    dependencies in sequential data, Sora as a vision transformer can also capture
    fine-grained temporal and spatial relationships between video frames to generate
    coherent frames with smooth transitions between them (i.e., exhibiting temporal
    consistency).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器在语言模型、计算机视觉和图像生成方面展现了卓越的可扩展性，因此索拉架构基于变压器来处理文本、图像或视频帧等多样化的输入是有意义的。此外，由于变压器能够理解序列数据中的复杂模式和长距离依赖关系，作为视觉变换器的索拉也可以捕捉视频帧之间的细粒度时间和空间关系，以生成具有平滑过渡的连贯帧（即表现出时间一致性）。
- en: Furthermore, Sora borrows capabilities of the SD models to generate high-quality
    and visually coherent video frames with precise controls using the iterative noise
    reduction process. Using the diffusion process lets Sora generate images with
    fine detail and desirable properties.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，索拉借鉴了SD模型的能力，通过迭代降噪过程以精确控制生成高质量且视觉上连贯的视频帧。使用扩散过程让索拉能够生成具有精细细节和理想特性的图像。
- en: By combining both sequential reasoning of transformers with iterative refinement
    of SD, Sora can generate high-resolution, coherent, and smooth videos from multimodal
    inputs like text and images that contain abstract concepts.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合变换器的序列推理和SD的迭代细化，索拉可以从包含抽象概念的文本和图像等多模态输入中生成高分辨率、连贯且平滑的视频。
- en: Sora’s network architecture is also designed to reduce dimensionality through
    a U-shape network where high-dimensional visual data is compressed and encoded
    into a latent noisy space. Sora can then generate patches from the latent space
    through the denoising diffusion process.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 索拉的神经网络架构也被设计为通过U形网络来降低维度，其中高维视觉数据被压缩并编码到潜在噪声空间中。然后索拉可以通过降噪扩散过程从潜在空间中生成补丁。
- en: The diffusion process is similar to image-based SD models. Instead of having
    a 2D U-Net normally used for images, OpenAI has trained a 3D U-Net where the third
    dimension is a sequence of frames across time (making a video), as shown in [Figure 3-20](#images_to_videos).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散过程与基于图像的SD模型类似。OpenAI 没有使用通常用于图像的2D U-Net，而是训练了一个3D U-Net，其中第三个维度是时间序列的帧序列（形成视频），如图[图3-20](#images_to_videos)所示。
- en: '![bgai 0320](assets/bgai_0320.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0320](assets/bgai_0320.png)'
- en: Figure 3-20\. A sequence of images forms a video
  id: totrans-367
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-20. 一系列图像形成一个视频
- en: OpenAI has demonstrated that by compressing videos into patches, as shown in
    [Figure 3-21](#videos_to_patches), the model can achieve scalability of learning
    high-dimensional representations when training on diverse types of videos and
    images varying in resolution, durations, and aspect ratios.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 已经证明，通过将视频压缩为补丁，如图[图3-21](#videos_to_patches)所示，该模型在训练时可以在不同分辨率、时长和宽高比的视频和图像上实现学习高维表示的可扩展性。
- en: '![bgai 0321](assets/bgai_0321.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0321](assets/bgai_0321.png)'
- en: Figure 3-21\. Video compression into space-time patches
  id: totrans-370
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-21. 将视频压缩为时空补丁
- en: Through the diffusion process, Sora crunches input noisy patches to generate
    clean videos and images in any aspect ratio, size, and resolution for devices
    directly in their native screen sizes.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 通过扩散过程，索拉将输入的噪声补丁压缩以生成任何宽高比、尺寸和分辨率的干净视频和图像，直接在设备的原生屏幕尺寸上。
- en: While a text transformer is predicting the next token in a text sequence, Sora’s
    vision transformer is predicting the next patch to generate an image or a video,
    as shown in [Figure 3-22](#vision_transformer_sequence).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 当文本变换器预测文本序列中的下一个标记时，索拉的视觉变换器正在预测生成图像或视频的下一个补丁，如图[图3-22](#vision_transformer_sequence)所示。
- en: '![bgai 0322](assets/bgai_0322.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0322](assets/bgai_0322.png)'
- en: Figure 3-22\. Token prediction by the vision transformer
  id: totrans-374
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-22. 视觉变换器进行的标记预测
- en: Through training on various datasets, OpenAI overcame the previously mentioned
    challenges of training vision models such as lack of quality captions, high dimensionality
    of video data, etc., to name a few.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在各个数据集上训练，OpenAI 克服了之前提到的训练视觉模型时的挑战，例如缺乏高质量的标题、视频数据的维度高等，仅举几例。
- en: 'What is fascinating about Sora and potentially other LVMs is the emerging capabilities
    they exhibit:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 关于索拉和可能的其他LVMs令人着迷的是它们展现出的新兴能力：
- en: 3D consistency
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 3D一致性
- en: Objects in the generated scenes remain consistent and adjust to perspective
    even when the camera moves and rotates around the scene.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成的场景中，即使摄像机在场景周围移动和旋转，物体也会保持一致性并调整到透视。
- en: Object permanence and large range coherence
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 物体恒存性和大范围一致性
- en: Objects and people that are occluded or leave a frame at a location will persist
    when they reappear in the field of view. In some cases, the model effectively
    remembers how to keep them consistent in the environment. This is also referred
    to as *temporal consistency* that most video models struggle with.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 被遮挡或离开画面的物体在重新出现在视野中时将保持不变。在某些情况下，模型实际上会记住如何在环境中保持它们的一致性。这也被称为 *时间一致性*，这是大多数视频模型都难以处理的。
- en: World interaction
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 世界交互
- en: Actions simulated in generated videos realistically affect the environment.
    For instance, Sora understands the action of eating a burger should leave a bite
    mark on it.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成的视频中模拟的动作可以真实地影响环境。例如，Sora 理解吃汉堡的动作应该在汉堡上留下咬痕。
- en: Simulating environments
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟环境
- en: Sora can also simulate worlds—real or fictional environments like in games—while
    adhering to the rules of interactions in those environments, such as playing a
    character in a *Minecraft* level. In other words, Sora has learned to be a data-driven
    physics engine.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: Sora 还可以模拟世界——例如游戏中的真实或虚构环境——同时遵守这些环境中交互的规则，如在一个 *Minecraft* 级别中扮演一个角色。换句话说，Sora
    已经学会了成为一个数据驱动的物理引擎。
- en: '[Figure 3-23](#sora_emerging_capabilities) illustrates these capabilities.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-23](#sora_emerging_capabilities) 展示了这些能力。'
- en: '![bgai 0323](assets/bgai_0323.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0323](assets/bgai_0323.png)'
- en: Figure 3-23\. Sora’s emergent capabilities
  id: totrans-387
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-23\. Sora 的涌现能力
- en: At the time of this writing, Sora has not yet been released as an API, but open
    source alternatives have already emerged. A promising large vision model called
    “Latte” allows you to fine-tune the LVM on your own visual data.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Sora 尚未作为 API 发布，但开源替代品已经出现。一个名为“Latte”的有前景的大视觉模型允许您在自己的视觉数据上微调 LVM。
- en: Caution
  id: totrans-389
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 谨慎
- en: You can’t yet commercialize some open source models, including Latte, at the
    time of writing. Always check the model card and the license to ensure any commercial
    use is allowed.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，您还不能商业化一些开源模型，包括 Latte。请始终检查模型卡片和许可证，以确保任何商业用途都是允许的。
- en: Combining transformers with diffusers to create LVMs is a promising area of
    research for generating complex outputs like videos. However, I imagine the same
    process can be applied for generating other types of high-dimensional data that
    can be represented as multidimensional arrays.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 将变压器与扩散器结合以创建 LVM 是生成复杂输出（如视频）的一个有希望的研究领域。然而，我想象同样的过程可以应用于生成其他类型的高维数据，这些数据可以表示为多维数组。
- en: You should now feel more comfortable building services with text, audio, vision,
    and video models. Next, let’s take a look at another set of models capable of
    generating complex data such as 3D geometries by building a 3D asset generator
    service.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该对使用文本、音频、视觉和视频模型来构建服务感到更加舒适。接下来，让我们看看另一组能够通过构建 3D 资产生成器服务生成复杂数据（如 3D 几何形状）的模型。
- en: 3D Models
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3D 模型
- en: You now understand how previously mentioned models use transformers and diffusers
    to generate any form of textual, audio, or visual data. Producing 3D geometries
    requires a different approach than image, audio, and text generation because you
    must account for spatial relationships, depth information, and geometric consistency,
    which add layers of complexity not present in other data types.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经了解了之前提到的模型是如何使用变压器和扩散器来生成任何形式的文本、音频或视觉数据的。生成 3D 几何形状需要与图像、音频和文本生成不同的方法，因为您必须考虑空间关系、深度信息和几何一致性，这些在其他数据类型中并不存在，从而增加了复杂性。
- en: For 3D geometries, *meshes* are used to define the shape of an object. Software
    packages like Autodesk 3ds Max, Maya, and SolidWorks can be used to produce, edit,
    and render these meshes.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 3D 几何形状，使用 *网格* 来定义物体的形状。可以使用像 Autodesk 3ds Max、Maya 和 SolidWorks 这样的软件包来生产、编辑和渲染这些网格。
- en: Meshes are effectively a collection of *vertices*, *edges*, and *faces* that
    reside in a 3D virtual space. Vertices are points in space that connect to form
    edges. Edges form faces (polygons) when they enclose on a flat surface, often
    in the shape of triangles or quadrilaterals. [Figure 3-24](#vertices_edges_faces)
    shows the differences between vertices, edges, and faces.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 网格实际上是一组位于三维虚拟空间中的 *顶点*、*边* 和 *面* 的集合。顶点是空间中的点，它们通过连接形成边。当边在平面上封闭时，它们形成面（多边形），通常是三角形或四边形的形状。[图3-24](#vertices_edges_faces)
    展示了顶点、边和面的区别。
- en: '![bgai 0324](assets/bgai_0324.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0324](assets/bgai_0324.png)'
- en: Figure 3-24\. Vertices, edges, and faces
  id: totrans-398
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-24\. 顶点、边和面
- en: You can define vertices by their coordinates in a 3D space, usually determined
    by a Cartesian coordinate system (x, y, z). Essentially, the arrangement and connection
    of vertices form surfaces of a 3D mesh that define a geometry.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在三维空间中定义顶点的坐标来定义顶点，通常由笛卡尔坐标系（x, y, z）确定。本质上，顶点的排列和连接形成了三维网格的表面，从而定义了几何形状。
- en: '[Figure 3-25](#mesh) shows how these features combine to define a mesh of a
    3D geometry such as a monkey’s head.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '[图3-25](#mesh) 展示了这些特征如何组合来定义一个三维几何形状（如猴头）的网格。'
- en: '![bgai 0325](assets/bgai_0325.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0325](assets/bgai_0325.png)'
- en: Figure 3-25\. Mesh for 3D geometry of a monkey head using both triangular and
    quadrilateral polygons (shown in Blender, open source 3D modeling software)
  id: totrans-402
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-25\. 使用三角形和四边形多边形（在Blender中显示，开源3D建模软件）为猴头三维几何形状创建的网格
- en: You can train and use a transformer model to predict the next token in a sequence
    where the sequence is coordinates of vertices on a 3D mesh surface. Such a generative
    model can produce 3D geometries by predicting the next set of vertices and faces
    within a 3D space that form the desired geometry. However, the geometry would
    require thousands of vertices and faces to achieve a smooth surface.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用transformer模型来训练和使用，以预测序列中的下一个标记，其中序列是三维网格表面的顶点坐标。这种生成模型可以通过预测三维空间中形成所需几何形状的下一个顶点和面的集合来生成3D几何形状。然而，为了达到平滑的表面，几何形状可能需要成千上万的顶点和面。
- en: This means for each 3D object, you need to wait for a long time for the generation
    to complete, and the results may still remain low fidelity. Because of this, the
    most capable models (i.e., OpenAI’s Shap-E) in producing 3D geometry train functions
    (with many parameters) to implicitly define surfaces and volumes in a 3D space.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着对于每个三维对象，您需要等待很长时间才能完成生成，而且结果可能仍然保持低保真度。正因为如此，在生成3D几何形状时，最强大的模型（即OpenAI的Shap-E）训练了具有许多参数的函数来隐式地定义三维空间中的表面和体积。
- en: Implicit functions are useful for creating smooth surfaces or handling intricate
    details that are challenging for discrete representations like meshes. A trained
    model can consist of an encoder that maps patterns to an implicit function. Instead
    of explicitly generating sequences of vertices and faces for a mesh, *conditional*
    3D models can evaluate the trained implicit functions across a continuous 3D space.
    As a result, the generation process has a high degree of freedom, control, and
    flexibility in producing high-fidelity outputs, becoming suitable for applications
    that require detailed and intricate 3D geometries.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 隐式函数对于创建平滑表面或处理对离散表示（如网格）具有挑战性的复杂细节非常有用。一个训练好的模型可以由一个将模式映射到隐式函数的编码器组成。与为网格显式生成顶点和面的序列不同，*条件*
    3D模型可以在连续的三维空间中评估训练好的隐式函数。因此，生成过程在产生高保真输出方面具有高度的自由度、控制性和灵活性，成为需要详细和复杂3D几何形状的应用的合适选择。
- en: Once the model’s encoder is trained to produce implicit functions, it leverages
    the *neural radiance fields* (NeRF) rendering technique, as part of the decoder,
    to construct 3D scenes. NeRF maps a pair of inputs—a 3D spatial coordinate and
    a 3D viewing direction—to an output consisting of an object density and RGB color
    via the implicit functions. To synthesize new views in a 3D scene, the NeRF method
    considers the viewport as a matrix of rays. Each pixel corresponding to a ray,
    originates from the camera position, and then extends in the viewing direction.
    The color of each ray and associated pixel is computed by evaluating the implicit
    function along the ray and integrating the results to calculate the RGB color.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型的编码器训练成产生隐函数，它就利用解码器的一部分*神经辐射场*（NeRF）渲染技术来构建3D场景。NeRF将一对输入——一个3D空间坐标和一个3D观看方向——映射到一个由对象密度和RGB颜色组成的输出，通过隐函数。为了在3D场景中合成新的视图，NeRF方法将视口视为射线矩阵。每个对应于射线的像素，从相机位置发出，然后沿观看方向延伸。每个射线和相关像素的颜色是通过在射线上评估隐函数并积分结果来计算RGB颜色的。
- en: 'Once the 3D scene is computed, *signed distance functions* (SDFs) are used
    to generate meshes, or wireframes of 3D objects by calculating the distance and
    color of any point to the nearest surface of the 3D object. Think of SDFs as a
    way to describe a 3D object by telling you how far away every point in space is
    from the object’s surface. This function gives a number for each point: if the
    point is inside the object, the number is negative; if it’s on the surface, the
    number is zero; and if it’s outside, the number is positive. The surface of the
    object is where all points have the number zero. SDFs help to turn this information
    into a 3D mesh.'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算完3D场景，就使用*符号距离函数*（SDFs）来生成网格，或者通过计算任何点到3D对象最近表面的距离和颜色来生成3D对象的线框。将SDFs视为一种通过告诉您空间中每个点到对象表面的距离来描述3D对象的方法。此函数为每个点提供一个数值：如果点在对象内部，数值为负；如果在表面上，数值为零；如果在外部，数值为正。对象的表面是所有点数值为零的地方。SDFs有助于将此信息转换为3D网格。
- en: Despite the use of implicit functions, the quality of outputs is still inferior
    to human-created 3D assets and may feel cartoonish. However, with 3D GenAI models,
    you can generate the initial 3D geometries to iterate over concepts and refine
    3D assets quickly.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用了隐函数，但输出的质量仍然不如人类创建的3D资产，可能感觉像卡通。然而，使用3D GenAI模型，您可以生成初始的3D几何形状来迭代概念并快速细化3D资产。
- en: OpenAI Shap-E
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenAI Shap-E
- en: '*Shap-E* (developed by OpenAI) is an open source model “conditioned” on input
    3D data (descriptions, parameters, partial geometries, colors, etc.) to generate
    specific 3D shapes. You can use Shap-E to create an image or text-to-3D services.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '*Shap-E*（由OpenAI开发）是一个“条件”于输入3D数据（描述、参数、部分几何形状、颜色等）的开源模型，以生成特定的3D形状。您可以使用Shap-E创建图像或文本到3D服务。'
- en: As usual, you start by downloading and loading the model from Hugging Face,
    as shown in [Example 3-13](#loading_shap-e).
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，您首先从Hugging Face下载并加载模型，如图[示例3-13](#loading_shap-e)所示。
- en: Example 3-13\. Downloading and loading OpenAI’s Shap-E model
  id: totrans-412
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-13\. 下载和加载OpenAI的Shap-E模型
- en: '[PRE19]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO13-1)'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)(#co_ai_integration_and_model_serving_CO13-1)'
- en: This specific Shap-E pipeline accepts textual prompts, but if you want to pass
    image prompts, you need to load a different pipeline.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 此特定的Shap-E管道接受文本提示，但如果您想传递图像提示，则需要加载不同的管道。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO13-2)'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)(#co_ai_integration_and_model_serving_CO13-2)'
- en: Use the `guidance_scale` parameter to fine-tune the generation process to better
    match the prompt.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`guidance_scale`参数微调生成过程，以更好地匹配提示。
- en: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO13-3)'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png)(#co_ai_integration_and_model_serving_CO13-3)'
- en: Use the `num_inference_steps` parameter to control the output resolution in
    exchange for additional computation. Requesting a higher number of inference steps
    or increasing the guidance scale can elongate the rendering time in exchange for
    higher-quality outputs that better follow the user’s request.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`num_inference_steps`参数来控制输出分辨率，以换取额外的计算。请求更多的推理步骤或增加指导比例可以延长渲染时间，以换取更高质量的输出，更好地满足用户的要求。
- en: '[![4](assets/4.png)](#co_ai_integration_and_model_serving_CO13-4)'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png)(#co_ai_integration_and_model_serving_CO13-4)'
- en: Set the `output_type` parameter to produce `mesh` tensors as output.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 将`output_type`参数设置为生成`mesh`张量作为输出。
- en: '[![5](assets/5.png)](#co_ai_integration_and_model_serving_CO13-5)'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png) (#co_ai_integration_and_model_serving_CO13-5)'
- en: By default, the Shap-E pipeline will produce a sequence of images that can be
    combined to generate a rotating GIF animation of the object. You can export this
    output to either GIFs, videos, or OBJ files that can be loaded in 3D modeling
    tools such as Blender.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Shap-E管道将生成一系列图像，可以组合成对象的旋转GIF动画。你可以将此输出导出为GIF、视频或OBJ文件，这些文件可以在Blender等3D建模工具中加载。
- en: Now that you have a model loading and 3D mesh generation functions, let’s export
    the mesh into a buffer using [Example 3-14](#mesh_to_buffer).
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经有了模型加载和3D网格生成函数，让我们使用[示例3-14](#mesh_to_buffer)将网格导出到缓冲区。
- en: Tip
  id: totrans-425
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: '`open3d` is an open source library for processing 3D data such as point clouds,
    meshes, and color images with depth information (i.e., RGB-D images). You will
    need to install `open3d` to run [Example 3-14](#mesh_to_buffer):'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '`open3d`是一个开源库，用于处理3D数据，如点云、网格和具有深度信息的颜色图像（即RGB-D图像）。你需要安装`open3d`来运行[示例3-14](#mesh_to_buffer)：'
- en: '[PRE20]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Example 3-14\. Exporting a 3D tensor mesh to a Wavefront OBJ buffer
  id: totrans-428
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-14。将3D张量网格导出到Wavefront OBJ缓冲区
- en: '[PRE21]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO14-1)'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png) (#co_ai_integration_and_model_serving_CO14-1)'
- en: Create an Open3D triangle mesh object.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个Open3D三角形网格对象。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO14-2)'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png) (#co_ai_integration_and_model_serving_CO14-2)'
- en: Convert the generated mesh from the model into an Open3D triangle mesh object.
    To do so, grab vertices and triangles from the generated 3D mesh by moving the
    mesh vertices and faces tensors to the CPU and converting them to `numpy` arrays.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 将生成的网格从模型转换为Open3D三角形网格对象。为此，通过将网格顶点和面张量移动到CPU并将它们转换为`numpy`数组，从生成的3D网格中获取顶点和三角形。
- en: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO14-4)'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png) (#co_ai_integration_and_model_serving_CO14-4)'
- en: Check if the mesh has three vertex color channels (indicating RGB color data)
    and stack these channels into a tensor.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 检查网格是否有三个顶点颜色通道（表示RGB颜色数据）并将这些通道堆叠成一个张量。
- en: '[![4](assets/4.png)](#co_ai_integration_and_model_serving_CO14-5)'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png) (#co_ai_integration_and_model_serving_CO14-5)'
- en: Convert mesh color tensor to a format compatible with Open3D for setting the
    vertex colors of the mesh.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 将网格颜色张量转换为与Open3D兼容的格式，以设置网格的顶点颜色。
- en: '[![5](assets/5.png)](#co_ai_integration_and_model_serving_CO14-6)'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png) (#co_ai_integration_and_model_serving_CO14-6)'
- en: Use a temporary file to create and return a data buffer.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 使用临时文件创建并返回数据缓冲区。
- en: '[![6](assets/6.png)](#co_ai_integration_and_model_serving_CO14-7)'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '![6](assets/6.png) (#co_ai_integration_and_model_serving_CO14-7)'
- en: Windows doesn’t support `NameTemporaryFile`’s `delete=True` option. Instead,
    manually remove the created temporary file just before returning the in-memory
    buffer.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: Windows不支持`NameTemporaryFile`的`delete=True`选项。相反，在返回内存缓冲区之前手动删除创建的临时文件。
- en: Finally, you can build the endpoints, as shown in [Example 3-15](#shap-e_endpoint).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以构建端点，如[示例3-15](#shap-e_endpoint)所示。
- en: Example 3-15\. Creating the 3D model-serving endpoint
  id: totrans-443
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-15。创建3D模型服务端点
- en: '[PRE22]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO15-1)'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png) (#co_ai_integration_and_model_serving_CO15-1)'
- en: Specify the OpenAPI specification for a successful response to include `model/obj`
    as the media content type.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 指定OpenAPI规范，以包括`model/obj`作为成功响应的媒体内容类型。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO15-2)'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png) (#co_ai_integration_and_model_serving_CO15-2)'
- en: Indicate to clients that the content of streaming response should be treated
    as an attachment.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 告知客户端，流响应的内容应被视为附件。
- en: If you send a request to the `/generate/3d` endpoint, the download of the 3D
    object as a Wavefront OBJ file should start as soon as generation is complete.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你向`/generate/3d`端点发送请求，一旦生成完成，就应该开始下载作为Wavefront OBJ文件的3D对象。
- en: You can import the OBJ file into any 3D modeling software such as Blender to
    view the 3D geometry. Using prompts such as `apple`, `car`, `phone`, and `donut`
    you can generate the 3D geometries shown in [Figure 3-26](#shape_e_blender).
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将OBJ文件导入到任何3D建模软件中，如Blender，以查看3D几何形状。使用诸如`apple`、`car`、`phone`和`donut`之类的提示，你可以生成[图3-26](#shape_e_blender)中显示的3D几何形状。
- en: '![bgai 0326](assets/bgai_0326.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0326](assets/bgai_0326.png)'
- en: Figure 3-26\. 3D geometries of a car, apple, phone, and donut imported into
    Blender
  id: totrans-452
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-26。导入到Blender中的汽车、苹果、手机和甜甜圈的3D几何形状
- en: If you isolate an object like the apple and enable the wireframe view, you can
    see all the vertices and edges that make up the apple’s mesh, represented as triangular
    polygons, as shown in [Figure 3-27](#shape_e_apple_wireframe).
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将苹果这样的对象隔离出来并启用线框视图，你可以看到构成苹果网格的所有顶点和边，它们以三角形多边形的形式表示，如图 [图 3-27](#shape_e_apple_wireframe)
    所示。
- en: '![bgai 0327](assets/bgai_0327.png)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0327](assets/bgai_0327.png)'
- en: 'Figure 3-27\. Zooming in on the generated 3D mesh to view triangular polygons;
    inset: viewing the generated apple geometry mesh (including vertices and edges)'
  id: totrans-455
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-27\. 放大生成的 3D 网格以查看三角形多边形；插入：查看生成的苹果几何网格（包括顶点和边）
- en: Shap-E supersedes another older model called *Point-E* that generates *point
    clouds* of 3D objects. This is because Shap-E, compared to Point-E, converges
    faster and reaches comparable or better generation shape quality despite modeling
    a higher-dimensional, multirepresentation output space.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: Shap-E 取代了另一个名为 *Point-E* 的较老模型，该模型生成 3D 对象的 *点云*。这是因为与 Point-E 相比，Shap-E 收敛速度更快，尽管建模的是更高维度的多表示输出空间，但生成的形状质量相当或更好。
- en: Point clouds (often used in the construction industry) are a large collection
    of point coordinates that closely represent a 3D object (such as a building structure)
    in a real-world space. Environment scanning devices including LiDAR laser scanners
    produce point clouds to represent objects within a 3D space at approximate measurements
    close to the real-world environment.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 点云（常用于建筑行业）是一大批点坐标的集合，在现实世界空间中紧密地代表了一个 3D 对象（如建筑结构），其测量值接近现实世界环境。环境扫描设备，包括激光扫描仪，产生点云以表示
    3D 空间内的对象。
- en: As 3D models improve, it may be possible to generate objects that closely represent
    their real counterparts.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 3D 模型的改进，生成与其实际对应物非常相似的对象可能成为可能。
- en: Strategies for Serving Generative AI Models
  id: totrans-459
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成 AI 模型服务策略
- en: You now should feel more confident building your own endpoints that serve a
    variety of models from the Hugging Face model repository. We touched upon a few
    different models, including those that generate text, image, video, audio, and
    3D shapes.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你应该更有信心构建自己的端点，从 Hugging Face 模型存储库中提供各种模型。我们提到了一些不同的模型，包括生成文本、图像、视频、音频和
    3D 形状的模型。
- en: The models you used were small, so they could be loaded and used on a CPU with
    reasonable outputs. However, in a production scenarios, you may want to use larger
    models to produce higher-quality results that may run only on GPUs and require
    a significant amount of video random access memory (VRAM).
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 你使用的模型较小，因此它们可以在具有合理输出的 CPU 上加载和使用。然而，在生产场景中，你可能想使用更大的模型来生成更高品质的结果，这些结果可能只能在
    GPU 上运行，并且需要大量的视频随机存取内存（VRAM）。
- en: 'In addition to leveraging GPUs, you will need to pick a model-serving strategy
    from several options:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 除了利用 GPU，你还需要从几个选项中选择一个模型服务策略：
- en: Be model agnostic
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 保持模型无关性
- en: Load models and generate outputs on every request (useful for model swapping).
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个请求中加载模型并生成输出（用于模型交换）。
- en: Be compute efficient
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 保持计算效率
- en: Use the FastAPI lifespan to preload models that can be reused for every request.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 FastAPI 生命周期预加载可以重复用于每个请求的模型。
- en: Be lean
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 保持精简
- en: Serve models externally without frameworks or work with third-party model APIs
    and interact with them via FastAPI.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有框架的情况下外部提供模型或与第三方模型 API 合作，并通过 FastAPI 与它们交互。
- en: Let’s take a look at each strategy in detail.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看每种策略。
- en: 'Be Model Agnostic: Swap Models on Every Request'
  id: totrans-470
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保持模型无关性：在每个请求中交换模型
- en: In the previous code examples, you defined the model loading and generation
    functions and then used them in route handler controllers. Using this serving
    strategy, FastAPI loads a model into RAM (or VRAM if using a GPU) and runs a generation
    process. Once FastAPI returns the results, the model is then unloaded from RAM.
    The process repeats for the next request.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码示例中，你定义了模型加载和生成函数，然后在路由处理控制器中使用它们。使用这种服务策略，FastAPI 将模型加载到 RAM（如果使用 GPU
    则为 VRAM）中并运行生成过程。一旦 FastAPI 返回结果，模型随后从 RAM 中卸载。这个过程会为下一个请求重复进行。
- en: As the model is unloaded after use, the memory is released to be used by another
    process or model. With this approach, you dynamically swap various models in a
    single request if processing time isn’t a concern. This means other concurrent
    requests must wait before the server responds to them.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用模型后，模型被卸载，内存被释放以供其他进程或模型使用。使用这种方法，如果处理时间不是问题，你可以在单个请求中动态交换各种模型。这意味着其他并发请求必须等待，直到服务器响应它们。
- en: When serving requests, FastAPI will queue incoming requests and process them
    in a first in first out (FIFO) order. This behavior will lead to long waiting
    times as a model needs to be loaded and unloaded every time. In most cases, this
    strategy is not recommended, but if you need to swap between multiple large models
    and you don’t have sufficient RAM, then you can adopt this strategy for prototyping.
    However, in production scenarios, you should never use this strategy for obvious
    reasons—your users will want to avoid the long wait times.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理请求时，FastAPI 将排队等待传入的请求，并按先入先出（FIFO）顺序处理它们。这种行为会导致长时间的等待，因为每次都需要加载和卸载模型。在大多数情况下，这种策略不推荐使用，但如果您需要在多个大型模型之间切换并且没有足够的RAM，那么您可以采用这种策略进行原型设计。然而，在生产场景中，出于明显的原因，您绝对不应该使用这种策略——您的用户会希望避免长时间的等待。
- en: '[Figure 3-28](#model_loading_on_request) shows this model service strategy.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-28](#model_loading_on_request) 展示了这种模型服务策略。'
- en: '![bgai 0329](assets/bgai_0329.png)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0329](assets/bgai_0329.png)'
- en: Figure 3-28\. Loading and using models on every request
  id: totrans-476
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-28\. 每个请求加载和使用模型
- en: If you need to use different models in each request and have limited memory,
    this method can work well for quickly trying things on a less powerful machine
    with just a few users. The trade-off is significantly slower processing time due
    to model swapping. However, in production scenarios, it is better to get larger
    RAM and use the model preloading strategy with FastAPI application lifespan.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要在每个请求中使用不同的模型并且内存有限，这种方法可以很好地用于在较弱的机器上快速尝试，并且只有少数用户。权衡是模型交换导致的处理时间显著变慢。然而，在生产场景中，最好是获得更大的RAM，并使用带有
    FastAPI 应用程序生命周期的模型预加载策略。
- en: 'Be Compute Efficient: Preload Models with the FastAPI Lifespan'
  id: totrans-478
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算效率：使用 FastAPI 生命周期预加载模型
- en: The most compute-efficient strategy for loading models in FastAPI is to use
    the application lifespan. With this approach, you load models on application startup
    and unload them on shutdown. During shutdown, you can also undertake any cleanup
    steps required, such as filesystem cleanup or logging.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 在 FastAPI 中加载模型的最计算高效策略是使用应用程序生命周期。采用这种方法，您在应用程序启动时加载模型，在关闭时卸载它们。在关闭期间，您还可以执行所需的任何清理步骤，例如文件系统清理或日志记录。
- en: The main benefit of this strategy compared to the first one mentioned is that
    you avoid reloading heavy models on each request. You can load a heavy model once
    and then make generations on every request coming using a preloaded model. As
    a result, you will save several minutes in processing time in exchange for a significant
    chunk of your RAM (or VRAM if using GPU). However, your application user experience
    will improve considerably due to shorter response times.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面提到的方法相比，这种策略的主要好处是您避免了每次请求时重新加载大型模型。您可以一次性加载一个大型模型，然后使用预加载的模型处理每个请求。因此，您将节省几分钟的处理时间，以换取您RAM（或使用GPU时的VRAM）的显著部分。然而，由于响应时间缩短，您的应用程序用户体验将显著提高。
- en: '[Figure 3-29](#model_loading_lifespan) shows the model-serving strategy that
    uses application lifespan.'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-29](#model_loading_lifespan) 展示了使用应用程序生命周期的模型服务策略。'
- en: '![bgai 0330](assets/bgai_0330.png)'
  id: totrans-482
  prefs: []
  type: TYPE_IMG
  zh: '![bgai 0330](assets/bgai_0330.png)'
- en: Figure 3-29\. Using the FastAPI application lifespan to preload models
  id: totrans-483
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-29\. 使用 FastAPI 应用程序生命周期预加载模型
- en: You can implement model preloading using the application lifespan, as shown
    in [Example 3-16](#model_preloading_lifespan).
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用应用程序生命周期实现模型预加载，如 [示例 3-16](#model_preloading_lifespan) 所示。
- en: Example 3-16\. Model preloading with application lifespan
  id: totrans-485
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 3-16\. 使用应用程序生命周期进行模型预加载
- en: '[PRE23]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO16-1)'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO16-1)'
- en: Initialize an empty mutable dictionary at the *global* application scope to
    hold one or multiple models.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 在全局应用程序范围内初始化一个空的可变字典以保存一个或多个模型。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO16-2)'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO16-2)'
- en: 'Use the `asynccontextmanager` decorator to handle startup and shutdown events
    as part of an async context manager:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `asynccontextmanager` 装饰器来处理作为异步上下文管理器一部分的启动和关闭事件：
- en: The context manager will run code before and after the `yield` keyword.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文管理器将在 `yield` 关键字之前和之后运行代码。
- en: The `yield` keyword in the decorated `lifespan` function separates the startup
    and shutdown phases.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 装饰的 `lifespan` 函数中的 `yield` 关键字将启动和关闭阶段分开。
- en: Code prior to the `yield` keyword runs at application startup before any requests
    are handled.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 `yield` 关键字之前的代码在处理任何请求之前在应用程序启动时运行。
- en: When you want to terminate the application, FastAPI will run the code after
    the `yield` keyword as part of the shutdown phase.
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你想终止应用程序时，FastAPI将在`yield`关键字之后作为关闭阶段的一部分运行代码。
- en: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO16-3)'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png) (#co_ai_integration_and_model_serving_CO16-3)'
- en: Preload the model on startup onto the `models` dictionary.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动时将模型预加载到`models`字典中。
- en: '[![4](assets/4.png)](#co_ai_integration_and_model_serving_CO16-4)'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png) (#co_ai_integration_and_model_serving_CO16-4)'
- en: Start handling requests as the startup phase is now finished.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 启动阶段完成后，开始处理请求。
- en: '[![5](assets/5.png)](#co_ai_integration_and_model_serving_CO16-5)'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png) (#co_ai_integration_and_model_serving_CO16-5)'
- en: Clear the model on application shutdown.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用程序关闭时清除模型。
- en: '[![6](assets/6.png)](#co_ai_integration_and_model_serving_CO16-6)'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '![6](assets/6.png) (#co_ai_integration_and_model_serving_CO16-6)'
- en: Create the FastAPI server and pass it the lifespan function to use.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 创建FastAPI服务器，并传递要使用的生命周期函数。
- en: '[![7](assets/7.png)](#co_ai_integration_and_model_serving_CO16-7)'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '![7](assets/7.png) (#co_ai_integration_and_model_serving_CO16-7)'
- en: Pass the global preloaded model instance to the generation function.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 将全局预加载的模型实例传递给生成函数。
- en: If you start the application now, you should immediately see model pipelines
    being loaded onto memory. Before you applied these changes, the model pipelines
    used to load only when you made your first request.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在启动应用程序，您应该立即看到模型管道被加载到内存中。在您应用这些更改之前，模型管道仅在您第一次请求时加载。
- en: Warning
  id: totrans-506
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: You can preload more than one model into memory using the lifespan model-serving
    strategy, but this isn’t practical with large GenAI models. Generative models
    can be resource hungry, and in most cases you’ll need GPUs to speed up the generation
    process. The most powerful consumer GPUs ship with only 24 GB of VRAM. Some models
    require 18 GB of memory to perform inference, so try to deploy models on separate
    application instances and GPUs instead.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用生命周期模型服务策略将多个模型预加载到内存中，但对于大型生成AI模型来说，这并不实用。生成模型可能需要大量资源，在大多数情况下，您需要GPU来加速生成过程。最强大的消费级GPU仅配备24
    GB的VRAM。一些模型需要18 GB的内存来进行推理，因此请尝试在单独的应用程序实例和GPU上部署模型。
- en: 'Be Lean: Serve Models Externally'
  id: totrans-508
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 精简：外部提供模型
- en: Another strategy to serve GenAI models is to package them as external services
    via other tools. You can then use your FastAPI application as the logical layer
    between your client and the external model server. In this logical layer, you
    can handle coordination between models, communication with APIs, management of
    users, security measures, monitoring activities, content filtering, enhancing
    prompts, or any other required logic.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种提供生成AI模型的方法是将它们作为外部服务通过其他工具打包。然后，您可以使用您的FastAPI应用程序作为客户端和外部模型服务器之间的逻辑层。在这个逻辑层中，您可以处理模型之间的协调、与API的通信、用户管理、安全措施、监控活动、内容过滤、增强提示或任何其他所需的逻辑。
- en: Cloud providers
  id: totrans-510
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 云服务提供商
- en: Cloud providers are constantly innovating serverless and dedicated compute solutions
    that you can use to serve your models externally. For instance, Azure Machine
    Learning Studio now provides a PromptFlow tool that you can use to deploy and
    customize OpenAI or open source language models. Upon deployment, you will receive
    a model endpoint run on your Azure compute ready for usage. However, there is
    a steep learning curve in using PromptFlow or similar tools as they may require
    particular dependencies and nontraditional steps to be followed.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 云服务提供商不断创新无服务器和专用计算解决方案，您可以使用这些解决方案来外部提供模型。例如，Azure Machine Learning Studio现在提供了一个PromptFlow工具，您可以使用它来部署和定制OpenAI或开源语言模型。部署后，您将收到一个在您的Azure计算上运行的模型端点，可供使用。然而，使用PromptFlow或类似工具可能需要特定的依赖项和非传统步骤，因此存在陡峭的学习曲线。
- en: BentoML
  id: totrans-512
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BentoML
- en: Another great contender for serving models external to FastAPI is BentoML. BentoML
    is inspired by FastAPI but implements a different serving strategy, purpose built
    for AI models.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个适合在FastAPI外部提供模型的优秀选择是BentoML。BentoML受到FastAPI的启发，但实现了不同的服务策略，专门为AI模型构建。
- en: A huge improvement over FastAPI for handling concurrent model requests is BentoML’s
    ability to run different requests on different worker processes. It can parallelize
    CPU-bound requests without you having to directly deal with Python multiprocessing.
    On top of this, BentoML can also batch model inferences such that the generation
    process for multiple users can be done with a single model call.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理并发模型请求方面，BentoML能够运行在不同工作进程上的不同请求，这比FastAPI有巨大的改进。它可以并行化CPU密集型请求，而无需你直接处理Python多进程。在此基础上，BentoML还可以批量处理模型推理，这样多个用户的生成过程可以通过单个模型调用完成。
- en: I covered BentoML in detail in [Chapter 2](ch02.html#ch02).
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 我在[第2章](ch02.html#ch02)中详细介绍了BentoML。
- en: Tip
  id: totrans-516
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'To run BentoML, you will need to install a few dependencies first:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行BentoML，你首先需要安装一些依赖项：
- en: '[PRE24]'
  id: totrans-518
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You can see how to start a BentoML server in [Example 3-18](#bentoml_usage).
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[示例3-18](#bentoml_usage)中看到如何启动BentoML服务器。
- en: Example 3-18\. Serving an image model with BentoML
  id: totrans-520
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-18\. 使用BentoML提供图像模型
- en: '[PRE25]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO17-1)'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)[#co_ai_integration_and_model_serving_CO17-1]'
- en: Declare a BentoML service with four allocated CPUs. The service should time
    out in 120 seconds if the model doesn’t generate in time and should run from port
    `5000`.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 声明一个具有四个分配CPU的BentoML服务。如果模型没有及时生成，服务应在120秒后超时，并从端口`5000`运行。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO17-2)'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)[#co_ai_integration_and_model_serving_CO17-2]'
- en: Declare an API controller for undertaking the core model generation process.
    This controller will hook to BentoML’s API route handler.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 声明一个API控制器，用于执行核心模型生成过程。此控制器将连接到BentoML的API路由处理程序。
- en: 'You can then run the BentoML service locally:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以在本地运行BentoML服务：
- en: '[PRE26]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Your FastAPI server can now become a client with the model being served externally.
    You can now make HTTP `POST` requests from within FastAPI to get a response, as
    shown in [Example 3-19](#fastapi_bentoml_usage).
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 你的FastAPI服务器现在可以成为一个客户端，所服务的模型在外部运行。你现在可以从FastAPI内部发送HTTP `POST`请求以获取响应，如[示例3-19](#fastapi_bentoml_usage)所示。
- en: Example 3-19\. BentoML endpoints via FastAPI
  id: totrans-529
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-19\. 通过FastAPI的BentoML端点
- en: '[PRE27]'
  id: totrans-530
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO18-1)'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)[#co_ai_integration_and_model_serving_CO18-1]'
- en: Create an asynchronous HTTP client using the `httpx` library.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`httpx`库创建一个异步HTTP客户端。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO18-2)'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png)[#co_ai_integration_and_model_serving_CO18-2]'
- en: Send a `POST` request to the BentoML image generation model endpoint.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 向BentoML图像生成模型端点发送`POST`请求。
- en: Model providers
  id: totrans-535
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型提供者
- en: Aside from BentoML and cloud providers, you can also use external model service
    providers such as OpenAI. In this case, your FastAPI application becomes a service
    wrapper over OpenAI’s API.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 除了BentoML和云提供商之外，你还可以使用外部模型服务提供商，如OpenAI。在这种情况下，你的FastAPI应用程序将成为OpenAI API的服务包装器。
- en: Luckily, integrating with model provider APIs such as OpenAI is quite straightforward,
    as shown in [Example 3-20](#openai_usage).
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，与模型提供者API（如OpenAI）的集成相当简单，如[示例3-20](#openai_usage)所示。
- en: Tip
  id: totrans-538
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: To run [Example 3-20](#openai_usage), you must get an API key and set the `OPENAI_API_KEY`
    environment variable to this key, as recommended by OpenAI.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行[示例3-20](#openai_usage)，你必须获取一个API密钥，并将`OPENAI_API_KEY`环境变量设置为这个密钥，如OpenAI所建议。
- en: Example 3-20\. Integrating with OpenAI service
  id: totrans-540
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-20\. 与OpenAI服务集成
- en: '[PRE28]'
  id: totrans-541
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO19-1)'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png)[#co_ai_integration_and_model_serving_CO19-1]'
- en: Use the `gpt-4o` model to chat with the model via the OpenAI API.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`gpt-4o`模型通过OpenAI API与模型聊天。
- en: And now you should be able to get outputs via external calls to the OpenAI service.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该能够通过对外部调用OpenAI服务来获取输出。
- en: When using external services, be mindful that data will be shared with third-party
    service providers. In this case, you may prefer self-hosted solutions if you value
    data privacy and security. With self-hosting, the trade-off will be an increased
    complexity in deploying and managing your own model servers.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用外部服务时，请注意数据将与第三方服务提供商共享。在这种情况下，如果你重视数据隐私和安全，你可能更喜欢自托管解决方案。自托管将带来部署和管理自己的模型服务器复杂性的增加。
- en: If you really want to avoid serving large models yourself, cloud providers can
    provide managed solutions where your data is never shared with third parties.
    An example is Azure OpenAI, which at the time of writing provides snapshots of
    OpenAI’s best LLMs and image generator.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你真的想避免自己提供大型模型的服务，云提供商可以提供托管解决方案，其中你的数据永远不会与第三方共享。一个例子是Azure OpenAI，在撰写本文时，它提供了OpenAI最佳LLM和图像生成器的快照。
- en: You now have a few options for model serving. One final system to implement
    before we wrap up this chapter is logging and monitoring of the service.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在有几个模型服务的选项。在我们结束本章之前，需要实现的一个最终系统是服务的日志记录和监控。
- en: The Role of Middleware in Service Monitoring
  id: totrans-548
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 中间件在服务监控中的作用
- en: You can implement a simple monitoring tool where prompts and responses can be
    logged alongside their request and response token usage. To implement the logging
    system, you can write a few logging functions inside your model-serving controller.
    However, if you have multiple models and endpoints, you may benefit from leveraging
    the FastAPI middleware mechanism.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以实施一个简单的监控工具，其中可以记录提示和响应，以及它们与请求和响应令牌的使用情况。要实现日志记录系统，您可以在模型服务控制器内部编写一些日志记录函数。然而，如果您有多个模型和端点，您可能从利用FastAPI中间件机制中受益。
- en: Middleware is an essential block of code that runs before and after a request
    is processed by any of your controllers. You can define custom middleware that
    you then attach to any API route handlers. Once the requests reach the route handlers,
    the middleware acts as an intermediary, processing the requests and responses
    between the client and server controller.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 中间件是在请求由您的任何控制器处理之前和之后运行的代码块。您可以定义自定义中间件，然后将它附加到任何API路由处理程序。一旦请求到达路由处理程序，中间件就充当中间人，在客户端和服务器控制器之间处理请求和响应。
- en: Excellent uses cases for middleware include logging and monitoring, rate limiting,
    content filtering, and cross-origin resource sharing (CORS) implementations.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 中间件的优秀用例包括日志记录和监控、速率限制、内容过滤和跨源资源共享（CORS）实现。
- en: '[Example 3-22](#middleware_monitoring_example) shows how you can monitor your
    model-serving handlers.'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例3-22](#middleware_monitoring_example) 展示了您如何监控模型服务处理程序。'
- en: Usage logging via custom middleware in production
  id: totrans-553
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过生产环境中的自定义中间件进行使用日志记录
- en: Don’t use [Example 3-22](#middleware_monitoring_example) in production as the
    monitoring logs can disappear if you run the application from a Docker container
    or a host machine that can be deleted or restarted without a mounted persistent
    volume or logging to a database.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 不要在生产环境中使用 [示例3-22](#middleware_monitoring_example)，因为如果从Docker容器或可以删除或重启且未挂载持久卷或未记录到数据库的主机机器运行应用程序，监控日志可能会消失。
- en: In [Chapter 7](ch07.html#ch07), you will integrate the monitoring system with
    a database to persist logs outside the application environment.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第7章](ch07.html#ch07) 中，您将监控系统集成到数据库中，以在应用程序环境之外持久化日志。
- en: Example 3-22\. Using middleware mechanisms to capture service usage logs
  id: totrans-556
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例3-22\. 使用中间件机制捕获服务使用日志
- en: '[PRE29]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[![1](assets/1.png)](#co_ai_integration_and_model_serving_CO21-1)'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '![1](assets/1.png) (#co_ai_integration_and_model_serving_CO21-1)'
- en: Declare a function decorated by the FastAPI HTTP middleware mechanism. The function
    must receive the `Request` object and `call_next` callback function to be considered
    valid `http` middleware.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 声明一个由FastAPI HTTP中间件机制装饰的函数。该函数必须接收`Request`对象和`call_next`回调函数，才能被认为是有效的`http`中间件。
- en: '[![2](assets/2.png)](#co_ai_integration_and_model_serving_CO21-2)'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '![2](assets/2.png) (#co_ai_integration_and_model_serving_CO21-2)'
- en: Pass the request to the route handler to process the response.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 将请求传递给路由处理程序以处理响应。
- en: '[![3](assets/3.png)](#co_ai_integration_and_model_serving_CO21-3)'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '![3](assets/3.png) (#co_ai_integration_and_model_serving_CO21-3)'
- en: Generate a request ID for tracking all incoming requests even if an error is
    raised in `call_next` during request processing.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 为跟踪所有传入请求生成请求ID，即使请求处理期间在`call_next`中引发错误。
- en: '[![4](assets/4.png)](#co_ai_integration_and_model_serving_CO21-4)'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '![4](assets/4.png) (#co_ai_integration_and_model_serving_CO21-4)'
- en: Calculate the response duration to four decimal places.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 将响应持续时间计算到小数点后四位。
- en: '[![5](assets/5.png)](#co_ai_integration_and_model_serving_CO21-5)'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '![5](assets/5.png) (#co_ai_integration_and_model_serving_CO21-5)'
- en: Set custom response headers for the processing time and request ID.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 为处理时间和请求ID设置自定义响应头。
- en: '[![6](assets/6.png)](#co_ai_integration_and_model_serving_CO21-6)'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '![6](assets/6.png) (#co_ai_integration_and_model_serving_CO21-6)'
- en: Log the URL of the endpoint triggered, request datetime and ID, client IP address,
    response processing time, and status code into a CSV file on disk in `append`
    mode.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 将触发端点的URL、请求日期和时间、客户端IP地址、响应处理时间和状态码记录到磁盘上的CSV文件中，以追加模式。
- en: In this section, you captured information about endpoint usage including processing
    time, status code, endpoint path, and client IP.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您捕获了有关端点使用的信息，包括处理时间、状态码、端点路径和客户端IP地址。
- en: Middleware is a powerful system for executing blocks of code before requests
    are passed to the route handlers and before responses are sent to the user. You
    saw an example of how middleware can be used to log model usage for any model-serving
    endpoint.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 中间件是一个强大的系统，用于在请求传递到路由处理程序之前和响应发送到用户之前执行代码块。你看到了中间件如何被用来为任何模型服务端点记录模型使用情况的示例。
- en: Accessing request and response bodies in middleware
  id: totrans-572
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在中间件中访问请求和响应体
- en: If you need to track interactions with your models, including prompts and the
    content they generate, using middleware for logging is more efficient than adding
    individual loggers to each handler. However, you should take into account data
    privacy and performance concerns when logging request and response bodies as the
    user could submit sensitive or large data to your service, which will require
    careful handling.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要跟踪与你的模型的交互，包括提示和它们生成的内容，使用中间件进行日志记录比在每个处理程序中添加单独的记录器更有效。然而，在记录请求和响应体时，你应该考虑到数据隐私和性能问题，因为用户可能会提交敏感或大量数据到你的服务，这将需要谨慎处理。
- en: Summary
  id: totrans-574
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We covered a lot of concepts in this chapter, so let’s quickly review everything
    we’ve discussed.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中涵盖了大量的概念，所以让我们快速回顾一下我们讨论过的所有内容。
- en: You saw how you can download, integrate, and serve a variety of open source
    GenAI models from the Hugging Face repository in a simple UI using the Streamlit
    package, within a few lines of code. You also reviewed several types of models
    and how to serve them via FastAPI endpoints. The models you experimented with
    were text, image, audio, video, and 3D-based, and you saw how they process data.
    You also learned the model architectures and the underlying mechanisms powering
    these models.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 你看到了如何使用Streamlit包在几行代码内下载、集成并使用Hugging Face存储库中的各种开源GenAI模型，并通过简单的用户界面进行服务。你还审查了几种模型类型以及如何通过FastAPI端点提供服务。你实验过的模型包括文本、图像、音频、视频和基于3D的，你看到了它们如何处理数据。你还学习了这些模型的架构和支撑这些模型的底层机制。
- en: Then, you reviewed several different model-serving strategies including model
    swapping on request, model preloading, and finally model serving outside the FastAPI
    application using other frameworks such as BentoML or using third-party APIs.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你审查了几种不同的模型服务策略，包括按请求进行模型交换、模型预加载，以及最终使用其他框架如BentoML或使用第三方API在FastAPI应用程序外部进行模型服务。
- en: Next, you noticed that the larger models could take some time to generate responses.
    Finally, you implemented a service monitoring mechanism for your models that leverage
    the FastAPI middleware system for every model-serving endpoint. You then wrote
    the logs to disk for future analysis.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你注意到较大的模型生成响应可能需要一些时间。最后，你为你的模型实现了一个服务监控机制，该机制利用FastAPI中间件系统为每个模型服务端点。然后，你将日志写入磁盘以供将来分析。
- en: You should now feel more confident building your own GenAI services powered
    by a variety of open source models.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在应该更有信心构建自己的由各种开源模型驱动的GenAI服务。
- en: In the next chapter, you will learn more about type safety and its role in eliminating
    application bugs and reducing uncertainty when working with external APIs and
    services. You will also see how to validate requests and response schemas to make
    your services even more reliable.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习更多关于类型安全及其在消除应用程序错误和减少与外部API和服务工作时不确定性的作用。你还将了解如何验证请求和响应模式，以使你的服务更加可靠。
- en: Additional References
  id: totrans-581
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他参考文献
- en: '[“Bark”](https://oreil.ly/HKT8O), in “Transformers” documentation, *Hugging
    Face*, accessed on 26 March 2024.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[“Bark”](https://oreil.ly/HKT8O)，在“Transformers”文档中，*Hugging Face*，于2024年3月26日访问。'
- en: 'Borsos, Z., et al. (2022). [“AudioLM: A Language Modeling Approach to Audio
    Generation”](https://oreil.ly/8YZBr). arXiv preprint arXiv:2209.03143.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borsos, Z.，等人（2022）。[“AudioLM：音频生成中的语言建模方法”](https://oreil.ly/8YZBr)。arXiv预印本
    arXiv:2209.03143。
- en: Brooks, T., et al. (2024). [“Video Generation Models as World Simulators”](https://oreil.ly/52duF).
    OpenAI.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brooks, T.，等人（2024）。[“视频生成模型作为世界模拟器”](https://oreil.ly/52duF)。OpenAI。
- en: Défossez, A., et al. (2022). [“High-Fidelity Neural Audio Compression”](https://oreil.ly/p4_-5).
    arXiv preprint arXiv:2210.13438.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Défossez, A.，等人（2022）。[“高保真神经网络音频压缩”](https://oreil.ly/p4_-5)。arXiv预印本 arXiv:2210.13438。
- en: 'Jun, H. & Nichol, A. (2023). [“Shap-E: Generating Conditional 3D Implicit Functions”](https://oreil.ly/LzLy0).
    arXiv preprint arXiv:2305.02463.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jun, H. & Nichol, A.（2023）。[“Shap-E：生成条件3D隐函数”](https://oreil.ly/LzLy0)。arXiv预印本
    arXiv:2305.02463。
- en: 'Kim, B.-K., et al. (2023). [“BK-SDM: A Lightweight, Fast, and Cheap Version
    of Stable Diffusion”](https://oreil.ly/uErOQ). arXiv preprint arXiv:2305.15798.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim, B.-K., et al. (2023). [“BK-SDM：Stable Diffusion 的轻量级、快速且经济实惠的版本”](https://oreil.ly/uErOQ).
    arXiv 预印本 arXiv:2305.15798。
- en: 'Liu, Y., et al. (2024). [“Sora: A Review on Background, Technology, Limitations,
    and Opportunities of Large Vision Models”](https://oreil.ly/Zr6bJ). arXiv preprint
    arXiv:2402.17177.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu, Y., et al. (2024). [“Sora：关于大型视觉模型背景、技术、局限性和机会的综述”](https://oreil.ly/Zr6bJ).
    arXiv 预印本 arXiv:2402.17177。
- en: 'Mildenhall, B., et al. (2020). [“NeRF: Representing Scenes as Neural Radiance
    Fields for View Synthesis”](https://oreil.ly/hBiBV). arXiv preprint arXiv:2003.08934.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mildenhall, B., et al. (2020). [“NeRF：用于视图合成的场景表示为神经辐射场”](https://oreil.ly/hBiBV).
    arXiv 预印本 arXiv:2003.08934。
- en: 'Nichol, A., et al. (2022). [“Point-E: A System for Generating 3D Point Clouds
    from Complex Prompts”](https://oreil.ly/FW-wT). arXiv preprint arXiv:2212.08751.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nichol, A., et al. (2022). [“Point-E：一个从复杂提示生成 3D 点云的系统”](https://oreil.ly/FW-wT).
    arXiv 预印本 arXiv:2212.08751。
- en: Vaswani, A., et al. (2017). [“Attention Is All You Need”](https://oreil.ly/N4MkH).
    arXiv preprint arXiv:1706.03762.
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani, A., et al. (2017). [“注意力即是所需”](https://oreil.ly/N4MkH). arXiv 预印本 arXiv:1706.03762。
- en: Wang, C., et al. (2023). [“Neural Codec Language Models are Zero-Shot Text to
    Speech Synthesizers”](https://oreil.ly/h1D0e). arXiv preprint arXiv:2301.02111.
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang, C., et al. (2023). [“神经编码语言模型是零样本文本到语音合成器”](https://oreil.ly/h1D0e). arXiv
    预印本 arXiv:2301.02111。
- en: 'Zhang, P., et al. (2024). [“TinyLlama: An Open-Source Small Language Model”](https://oreil.ly/Idi1B).
    arXiv preprint arXiv:2401.02385.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang, P., et al. (2024). [“TinyLlama：一个开源的小型语言模型”](https://oreil.ly/Idi1B).
    arXiv 预印本 arXiv:2401.02385。
- en: ^([1](ch03.html#id630-marker)) Hugging Face provides access to a wide range
    of pretrained machine learning models, datasets, and applications.
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch03.html#id630-marker)) Hugging Face 提供了访问各种预训练机器学习模型、数据集和应用的途径。
- en: ^([2](ch03.html#id631-marker)) A. Vaswani et al. (2017), [“Attention Is All
    You Need”](https://oreil.ly/sO33r), arXiv preprint arXiv:1706.03762.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch03.html#id631-marker)) A. Vaswani 等人 (2017)，[“注意力即是所需”](https://oreil.ly/sO33r)，arXiv
    预印本 arXiv:1706.03762。
- en: ^([3](ch03.html#id635-marker)) A great tool for visualizing attention maps is
    [BertViz](https://oreil.ly/e2Q7X).
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch03.html#id635-marker)) 一个用于可视化注意力图的优秀工具是 [BertViz](https://oreil.ly/e2Q7X)。
- en: ^([4](ch03.html#id637-marker)) You can find the up-to-date list of open source
    LLMs on the [Open LLM GitHub repository](https://oreil.ly/GZaEr).
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch03.html#id637-marker)) 你可以在 [Open LLM GitHub 仓库](https://oreil.ly/GZaEr)
    中找到最新的开源 LLM 列表。
- en: ^([5](ch03.html#id647-marker)) An embedding model or an embedding layer such
    as in a transformer
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch03.html#id647-marker)) 嵌入模型或嵌入层，例如在 Transformer 中。
- en: ^([6](ch03.html#id658-marker)) This sequential token generation process can
    also limit scalability for long sequences, as each token relies on the previous
    one.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch03.html#id658-marker)) 这种顺序标记生成过程也可能限制长序列的可扩展性，因为每个标记都依赖于前一个标记。
- en: ^([7](ch03.html#id665-marker)) The [Hugging Face model repository](https://huggingface.co)
    is a resource for AI developers to publish and share their pretrained models.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch03.html#id665-marker)) [Hugging Face 模型仓库](https://huggingface.co) 是
    AI 开发者发布和共享其预训练模型的一个资源。
- en: ^([8](ch03.html#id666-marker)) See the [Pytorch documentation](https://pytorch.org)
    for installation instructions.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch03.html#id666-marker)) 有关安装说明，请参阅 [Pytorch 文档](https://pytorch.org)。
- en: ^([9](ch03.html#id667-marker)) `float16` tensor precision is more memory efficient
    in memory constraint environments. The computations can be faster but precision
    is lower compared to `float32` tensor types. See the [TinyLlama model card](https://oreil.ly/rsmoB)
    for more information.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch03.html#id667-marker)) 在内存受限环境中，`float16` 张量精度在内存效率上更优。计算可能更快，但与 `float32`
    张量类型相比，精度较低。有关更多信息，请参阅 [TinyLlama 模型卡片](https://oreil.ly/rsmoB)。
- en: ^([10](ch03.html#id668-marker)) As we saw in [Chapter 2](ch02.html#ch02), controllers
    are functions that handle an API route’s incoming requests and return responses
    to the client via a logical execution of services or providers.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: ^([10](ch03.html#id668-marker)) 正如我们在 [第二章](ch02.html#ch02) 中所看到的，控制器是处理 API
    路由的传入请求并通过逻辑执行服务或提供者向客户端返回响应的函数。
- en: ^([11](ch03.html#id674-marker)) Streamlit collects usage statistics by default,
    but you can turn this off using a [configuration file](https://oreil.ly/m_Jix).
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: ^([11](ch03.html#id674-marker)) Streamlit 默认收集使用统计信息，但您可以使用 [配置文件](https://oreil.ly/m_Jix)
    关闭此功能。
- en: ^([12](ch03.html#id686-marker)) The latent space of a trained model when visualized
    may look like white noise but will contain structured representations that the
    model has learned to encode and decode.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: ^([12](ch03.html#id686-marker)) 训练好的模型的潜在空间在可视化时可能看起来像白噪声，但实际上会包含模型学习到的结构化表示，用于编码和解码。
- en: ^([13](ch03.html#id697-marker)) *Multiplexing* is the process of combining multiple
    streams (such as audio, video, and subtitles) into a single file or stream in
    a synchronized manner.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: ^([13](ch03.html#id697-marker)) *复用* 是将多个流（如音频、视频和字幕）以同步方式组合成一个文件或单个流的过程。
- en: ^([14](ch03.html#id698-marker)) The `python-multipart` library is used for parsing
    `multipart/form-data`, which is commonly used encoding in file upload form submissions.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: ^([14](ch03.html#id698-marker)) `python-multipart` 库用于解析 `multipart/form-data`，这是文件上传表单提交中常用的编码方式。
