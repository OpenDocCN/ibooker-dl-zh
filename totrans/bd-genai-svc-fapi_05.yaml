- en: Capitolo 3\. Integrazione dell'IA e servizio del modello
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Questo lavoro è stato tradotto utilizzando l''AI. Siamo lieti di ricevere il
    tuo feedback e i tuoi commenti: [translation-feedback@oreilly.com](mailto:translation-feedback@oreilly.com)'
  prefs: []
  type: TYPE_NORMAL
- en: In questo capitolo imparerai i meccanismi dei vari modelli GenAI e come servirli
    in un'applicazione FastAPI. Inoltre, utilizzando il [pacchetto Streamlit UI](https://oreil.ly/9BXmn),
    creerai un semplice client browser per interagire con gli endpoint che servono
    i modelli. Esploreremo le diverse strategie di servizio dei modelli, come precaricare
    i modelli per renderli più efficienti e come utilizzare le funzioni FastAPI per
    ilmonitoraggio dei servizi.
  prefs: []
  type: TYPE_NORMAL
- en: Per consolidare quanto appreso in questo capitolo, costruiremo progressivamente
    un servizio FastAPI utilizzando modelli GenAI open source che generano testo,
    immagini, audio e geometrie 3D, il tutto partendo da zero. Nei capitoli successivi,
    costruirai la funzionalità di analisi dei documenti e dei contenuti web per il
    tuo servizio GenAI in modo da poter dialogare con loro utilizzando un modello
    linguistico.
  prefs: []
  type: TYPE_NORMAL
- en: Nota
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Nel capitolo precedente hai visto come configurare un nuovo progetto FastAPI
    in Python. Assicurati di avere pronta un'installazione fresca prima di leggere
    il resto di questo capitolo. In alternativa, puoi clonare o scaricare il [repository
    GitHub](https://github.com/Ali-Parandeh/building-generative-ai-services) del libro.
    Poi, una volta clonato, passa al ramo `ch03-start`, pronto per i passi da seguire.
  prefs: []
  type: TYPE_NORMAL
- en: Alla fine di questo capitolo, avrai un servizio FastAPI che serve vari modelli
    GenAI open source che potrai testare all'interno dell'interfaccia utente Streamlit.
    Inoltre, il tuo servizio sarà in grado di registrare i dati di utilizzo su disco
    utilizzando il middleware.
  prefs: []
  type: TYPE_NORMAL
- en: Modelli generativi al servizio del cliente
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prima di utilizzare modelli generativi pre-addestrati nella tua applicazione,
    vale la pena di imparare come questi modelli vengono addestrati e generano dati.
    Grazie a questa conoscenza, puoi personalizzare gli interni della tua applicazione
    per migliorare i risultati che fornisci all'utente.
  prefs: []
  type: TYPE_NORMAL
- en: 'In questo capitolo ti mostrerò come servire i modelli in diverse modalità,
    tra cui:'
  prefs: []
  type: TYPE_NORMAL
- en: Modelli*linguistici* basati sull'architettura della rete neurale trasformatrice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modelli*audio* nei servizi text-to-speech e text-to-audio basati sull'architettura
    a trasformatori aggressivi
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modelli di*visione* per servizi text-to-image e text-to-video basati sulla Diffusione
    Stabile e sulle architetture di trasformazione della visione
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modelli*3D* per servizi text-to-3D basati sull'architettura del codificatore
    di funzioni implicite condizionali e del decodificatore di diffusione
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Questo elenco non è esaustivo e copre una manciata di modelli GenAI. Per esplorare
    altri modelli, visita il [repository dei modelli Hugging Face](https://oreil.ly/-4wlQ).^([1](ch03.html#id630))
  prefs: []
  type: TYPE_NORMAL
- en: Modelli linguistici
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In questa sezione parliamo dei modelli linguistici, compresi i trasformatori
    e le reti neurali ricorrenti (RNN).
  prefs: []
  type: TYPE_NORMAL
- en: Trasformatori contro reti neurali ricorrenti
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Il mondo dell'Intelligenza Artificiale è stato scosso dalla pubblicazione dell'importante
    articolo "Attention Is All You Need".^([2](ch03.html#id631)) In questo articolo,
    gli autori proponevano un approccio completamente diverso all'elaborazione del
    linguaggio naturale (NLP) e alla modellazione delle sequenze che si differenziava
    dalle architetture RNN esistenti.
  prefs: []
  type: TYPE_NORMAL
- en: La[Figura 3-1](#transformer_architecture) mostra una versione semplificata dell'architettura
    del trasformatore proposta nell'articolo originale.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0301](assets/bgai_0301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 3-1\. Architettura del trasformatore
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Storicamente, le attività di generazione del testo hanno sfruttato i modelli
    RNN per apprendere modelli in dati sequenziali come il testo libero.Per elaborare
    il testo, questi modelli lo suddividono in piccoli pezzi, come una parola o un
    carattere, chiamati *token*, che possono essere elaborati in sequenza.
  prefs: []
  type: TYPE_NORMAL
- en: Le RNN mantengono un archivio di memoria chiamato *vettore di stato*, che trasporta
    le informazioni da un token all'altro per tutta la sequenza di testo, fino alla
    fine. Questo significa che quando si arriva alla fine della sequenza di testo,
    l'impatto dei primi token sul vettore di stato è molto minore rispetto ai token
    più recenti.
  prefs: []
  type: TYPE_NORMAL
- en: Idealmente, ogni token dovrebbe avere la stessa importanza degli altri token
    in un testo. Tuttavia, poiché le RNN possono prevedere l'elemento successivo in
    una sequenza solo osservando gli elementi che l'hanno preceduto, non riescono
    a cogliere le dipendenze a lungo raggio e a modellare i modelli in grandi porzioni
    di testo. Di conseguenza, non riescono a ricordare o a comprendere le informazioni
    essenziali o il contesto in documenti di grandi dimensioni.
  prefs: []
  type: TYPE_NORMAL
- en: Con l'invenzione dei trasformatori, la modellazione ricorrente o convoluzionale
    poteva essere sostituita da un approccio più efficiente.Poiché i trasformatori
    non mantengono una memoria di stato nascosta e sfruttano una nuova capacità definita
    *auto-attenzione*, sono in grado di modellare le relazioni tra le parole, indipendentemente
    dalla distanza tra loro in una frase. Questa componente di auto-attenzione consente
    al modello di "porre l'attenzione" sulle parole contestualmente rilevanti all'interno
    di una frase.
  prefs: []
  type: TYPE_NORMAL
- en: Mentre le RNN modellano le relazioni tra parole vicine in una frase, i trasformatori
    mappano le relazioni a coppie tra ogni parola del testo.
  prefs: []
  type: TYPE_NORMAL
- en: La[Figura 3-2](#rnn_vs_transformer) mostra come le RNN elaborano le frasi rispetto
    ai trasformatori.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0302](assets/bgai_0302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 3-2\. RNN rispetto ai trasformatori nell'elaborazione delle frasi
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Ciò che alimenta il sistema di auto-attenzione sono blocchi specializzati chiamati
    *teste di attenzione* che catturano gli schemi di coppia tra le parole come *mappe
    di attenzione*.
  prefs: []
  type: TYPE_NORMAL
- en: La [Figura 3-3](#head_attention_map) visualizza la mappa di attenzione di una
    testa di attenzione.^([3](ch03.html#id635)) Le connessioni possono essere bidirezionali
    e lo spessore rappresenta la forza della relazione tra le parole della frase.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0303](assets/bgai_0303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 3-3\. Vista di una mappa di attenzione all'interno di una testa di attenzione
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Un modello trasformatore contiene diverse teste di attenzione distribuite tra
    gli strati della rete neurale. Ogni testa calcola la propria mappa di attenzione
    in modo indipendente per catturare le relazioni tra le parole concentrandosi su
    determinati schemi negli input. Utilizzando più teste di attenzione, il modello
    può analizzare simultaneamente gli input da diverse angolazioni e contesti per
    comprendere schemi complessi e dipendenze all'interno dei dati.
  prefs: []
  type: TYPE_NORMAL
- en: La[Figura 3-4](#model_attention_map) mostra le mappe di attenzione per ogni
    testa (cioè un insieme indipendente di pesi di attenzione) all'interno di ogni
    strato del modello.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0304](assets/bgai_0304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 3-4\. Vista delle mappe di attenzione all'interno del modello
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Le RNN richiedono inoltre una grande potenza di calcolo per l'addestramento,
    in quanto il processo di addestramento non può essere parallelizzato su più GPU
    a causa della natura sequenziale dei loro algoritmi di addestramento. I trasformatori,
    invece, elaborano le parole in modo non sequenziale, quindi possono eseguire i
    meccanismi di attenzione in parallelo sulle GPU.
  prefs: []
  type: TYPE_NORMAL
- en: L'efficienza dell'architettura a trasformatori significa che questi modelli
    sono più scalabili se ci sono più dati, potenza di calcolo e memoria. È possibile
    costruire modelli linguistici con un corpus che abbraccia le biblioteche di libri
    prodotti dall'umanità. Tutto ciò che serve è una grande potenza di calcolo e dati
    per addestrare un LLM. E questo è esattamente ciò che ha fatto OpenAI, l'azienda
    dietro la famosa applicazione ChatGPT che era alimentata da diversi LLM proprietari,
    tra cui GPT-4o.
  prefs: []
  type: TYPE_NORMAL
- en: Al momento in cui scriviamo, i dettagli dell'implementazione dei LLMs di OpenAI
    rimangono un segreto commerciale. Sebbene molti ricercatori abbiano una conoscenza
    generale dei metodi di OpenAI, non è detto che abbiano le risorse per replicarli.
    Tuttavia, da allora sono state rilasciate diverse alternative open source per
    la ricerca e l'uso commerciale, tra cui Llama (Facebook), Gemma (Google), Mistral
    e Falcon, solo per citarne alcune.^([4](ch03.html#id637)) Al momento in cui scriviamo,
    le dimensioni dei modelli variano da 0,05B a 480B parametri (cioè pesi e distorsioni
    del modello) per adattarsi alle tue esigenze.
  prefs: []
  type: TYPE_NORMAL
- en: Il servizio di LLM rimane una sfida a causa degli elevati requisiti di memoria,
    che raddoppiano se devi addestrarli e metterli a punto sul tuo set di dati. Questo
    perché il processo di addestramento richiede la memorizzazione nella cache e il
    riutilizzo dei parametri del modello tra i vari lotti di addestramento. Di conseguenza,
    la maggior parte delle organizzazioni può affidarsi a modelli leggeri (fino a
    3B) o alle API di fornitori di LLMs come OpenAI, Anthropic, Cohere, Mistral, ecc.
  prefs: []
  type: TYPE_NORMAL
- en: Con l'aumento della popolarità degli LLMs, diventa ancora più importante capire
    come vengono addestrati e come elaborano i dati, quindi parliamo dei meccanismi
    sottostanti.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizzazione e incorporazione
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Le reti neurali non possono elaborare direttamente le parole perché sono grandi
    modelli statistici che funzionano con i numeri. Per colmare il divario tra linguaggio
    e numeri, è necessario ricorrere alla *tokenizzazione*. Con la tokenizzazione,
    si scompone il testo in pezzi più piccoli che un modello può elaborare.
  prefs: []
  type: TYPE_NORMAL
- en: Qualsiasi testo deve essere innanzitutto suddiviso in un elenco di *token* che
    rappresentano parole, sillabe, simboli e punteggiature. Questi token vengono poi
    mappati in numeri unici in modo da poter modellare numericamente i modelli.
  prefs: []
  type: TYPE_NORMAL
- en: Fornendo un vettore di token in ingresso a un trasformatore addestrato, la rete
    può prevedere il token successivo migliore per generare il testo, una parola alla
    volta.
  prefs: []
  type: TYPE_NORMAL
- en: La[Figura 3-5](#openai_tokenizer) mostra come il tokenizer di OpenAI converte
    il testo in una sequenza di token, assegnando a ciascuno di essi degli identificatori
    unici.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0305](assets/bgai_0305.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figura 3-5\. Tokenizer di OpenAI (Fonte: [OpenAI](https://oreil.ly/S-a9M))'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Quindi, cosa si può fare dopo aver tokenizzato un testo? Questi token devono
    essere elaborati ulteriormente prima che un modello linguistico possa elaborarli.
  prefs: []
  type: TYPE_NORMAL
- en: Dopo la tokenizzazione, è necessario utilizzare un *embedder*^([5](ch03.html#id647))
    per convertire questi token in vettori densi di numeri reali chiamati *embeddings*,
    che catturano le informazioni semantiche (cioè il significato di ogni token) in
    uno spazio vettoriale continuo. La[Figura 3-6](#embeddings) mostra questi embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0306](assets/bgai_0306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 3-6\. Assegnazione di un vettore di incorporamento di dimensionen a ciascun
    token durante il processo diincorporamento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Suggerimento
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Questi vettori di incorporamento utilizzano piccoli *numeri in virgola mobile*
    (non numeri interi) per catturare le relazioni sfumate tra i token con maggiore
    flessibilità e precisione. Inoltre, tendono a essere *distribuiti normalmente*,
    quindi l'addestramento e l'inferenza del modello linguistico possono essere più
    stabili e coerenti.
  prefs: []
  type: TYPE_NORMAL
- en: Dopo il processo di embedding, a ogni token viene assegnato un vettore di embedding
    composto da *n* numeri. Ogni numero del vettore di embedding si concentra su una
    dimensione che rappresenta un aspetto specifico del significato del token.
  prefs: []
  type: TYPE_NORMAL
- en: Trasformatori di formazione
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Una volta ottenuta una serie di vettori di incorporamento, puoi addestrare un
    modello sui tuoi documenti per aggiornare i valori all'interno di ciascun incorporamento.
    Durante l'addestramento del modello, l'algoritmo di addestramento aggiorna i parametri
    degli strati di incorporamento in modo che i vettori di incorporamento descrivano
    il significato di ciascun token il più possibile all'interno del testo in ingresso.
  prefs: []
  type: TYPE_NORMAL
- en: Capire come funzionano i vettori di incorporamento può essere difficile, quindi
    proviamo un approccio di visualizzazione.
  prefs: []
  type: TYPE_NORMAL
- en: Immagina di utilizzare vettori di incorporamento bidimensionali, cioè contenenti
    solo due numeri. Se tracci questi vettori, prima e dopo l'addestramento del modello,
    osserverai dei grafici simili a quelli della [Figura 3-7](#untrained_to_trained_transformer).
    I vettori di incorporamento di token, o parole, con significati simili saranno
    più vicini tra loro.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0307](assets/bgai_0307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 3-7\. Spazio latente di addestramento della rete di trasformatori utilizzando
    i vettori di incorporazione
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Per determinare la somiglianza tra due parole, puoi calcolare l'angolo tra i
    vettori utilizzando un calcolo noto come *somiglianza del coseno*. Angoli più
    piccoli implicano una maggiore somiglianza, che rappresenta un contesto e un significato
    simili. Dopo l'addestramento, il calcolo della somiglianza del coseno di due vettori
    di incorporamento con significati simili convaliderà che questi vettori sono vicini
    tra loro.
  prefs: []
  type: TYPE_NORMAL
- en: La[Figura 3-8](#embedding_vectors) illustra l'intero processo di tokenizzazione,
    incorporazione e formazione.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0308](assets/bgai_0308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 3-8\. Elaborazione di dati sequenziali come un testo in un vettore di
    token e di incorporazioni di token
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Una volta ottenuto un livello di incorporazione addestrato, puoi utilizzarlo
    per incorporare qualsiasi nuovo testo in ingresso nel modello di trasformatore
    mostrato nella [Figura 3-1](#transformer_architecture).
  prefs: []
  type: TYPE_NORMAL
- en: Codifica posizionale
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Un'ultima fase prima di inoltrare i vettori di incorporamento agli strati di
    attenzione della rete di trasformazione consiste nell'implementare la *codifica
    posizionale*. Il processo di codifica posizionale produce i vettori di incorporamento
    posizionale che vengono poi sommati ai vettori di incorporamento dei token.
  prefs: []
  type: TYPE_NORMAL
- en: Poiché i trasformatori elaborano le parole simultaneamente anziché in sequenza,
    sono necessari embedding posizionali per registrare l'ordine delle parole e il
    contesto all'interno dei dati sequenziali, come le frasi. I vettori embedding
    risultanti catturano sia il significato che le informazioni posizionali delle
    parole nelle frasi prima di passarle ai meccanismi di attenzione del trasformatore.
    Questo processo assicura che le teste di attenzione abbiano tutte le informazioni
    necessarie per apprendere efficacemente i modelli.
  prefs: []
  type: TYPE_NORMAL
- en: La[Figura 3-9](#positional_encoding) mostra il processo di codifica posizionale
    in cui le incorporazioni posizionali vengono sommate alle incorporazioni dei token.
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0309](assets/bgai_0309.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 3-9\. Codifica posizionale
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Previsione autoregressiva
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Il trasformatore è un modello autoregressivo (cioè sequenziale), in quanto le
    previsioni future si basano sui valori passati, come mostrato nella [Figura 3-10](#autoregressive_prediction3).
  prefs: []
  type: TYPE_NORMAL
- en: '![bgai 0310](assets/bgai_0310.png)'
  prefs: []
  type: TYPE_IMG
- en: Figura 3-10\. Previsione autoregressiva
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Il modello riceve dei token in ingresso che vengono poi incorporati e passati
    attraverso la rete per fare la migliore previsione del token successivo. Questo
    processo si ripete fino a quando non viene generato un token `<stop>` o di fine
    frase `<eos>`.^([6](ch03.html#id658))
  prefs: []
  type: TYPE_NORMAL
- en: Tuttavia, c'è un limite al numero di token che il modello può immagazzinare
    nella sua memoria per generare il token successivo.Questo limite di token è indicato
    come la *finestra di contesto* del modello, un fattore importante da considerare
    durante la fase di selezione del modello per i tuoi servizi GenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Se il limite della finestra di contesto viene raggiunto, il modello scarta semplicemente
    i token utilizzati più di recente, il che significa che può *dimenticare* le frasi
    utilizzate più di recente nei documenti o nei messaggi di una conversazione.
  prefs: []
  type: TYPE_NORMAL
- en: Nota
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Al momento in cui scriviamo, il contesto del modello OpenAI `gpt-4o-mini` meno
    costoso è di circa 128.000 tokens, equivalenti a più di 300 pagine di testo.
  prefs: []
  type: TYPE_NORMAL
- en: La finestra di contesto più grande a marzo 2025 appartiene a [Magic.Dev LTM-2-mini](https://oreil.ly/10Mj1)
    con 100 milioni di gettoni. Ciò equivale a ~10 milioni di righe di codice di ~750
    romanzi.
  prefs: []
  type: TYPE_NORMAL
- en: La finestra di contesto di altri modelli si aggira intorno alle centinaia di
    migliaia di gettoni.
  prefs: []
  type: TYPE_NORMAL
- en: Finestre brevi comportano la perdita di informazioni, la difficoltà di mantenere
    le conversazioni e una minore coerenza con la query dell'utente.
  prefs: []
  type: TYPE_NORMAL
- en: D'altro canto, le finestre di contesto più lunghe hanno requisiti di memoria
    più elevati e possono causare problemi di prestazioni o rallentamenti dei servizi
    quando si scalano a migliaia di utenti contemporanei che utilizzano il tuo servizio.
    Inoltre, dovrai considerare i costi di affidarti a modelli con finestre di contesto
    più ampie, in quanto tendono ad essere più costosi a causa dei maggiori requisiti
    di calcolo e di memoria. La scelta corretta dipenderà dal tuo budget e dalle esigenze
    degli utenti nel tuo caso d'uso.
  prefs: []
  type: TYPE_NORMAL
- en: Integrare un modello linguistico nella tua applicazione
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Puoi scaricare e utilizzare un modello linguistico all'interno della tua applicazione
    con poche righe di codice.Nell'[Esempio 3-1](#language_model_usage_example), scaricherai
    un modello TinyLlama con 1,1 miliardi di parametri e preaddestrato su 3 trilioni
    di token.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0] # models.py  import torch from transformers import Pipeline, pipeline  prompt
    = "How to set up a FastAPI project?" system_prompt = """ Your name is FastAPI
    bot and you are a helpful chatbot responsible for teaching FastAPI to your users.
    Always respond in markdown. """  device = torch.device("cuda" if torch.cuda.is_available()
    else "cpu") ![1](assets/1.png)  def load_text_model():     pipe = pipeline(         "text-generation",         model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    ![2](assets/2.png)         torch_dtype=torch.bfloat16,         device=device ![3](assets/3.png)     )     return
    pipe   def generate_text(pipe: Pipeline, prompt: str, temperature: float = 0.7)
    -> str:     messages = [         {"role": "system", "content": system_prompt},         {"role":
    "user", "content": prompt},     ] ![4](assets/4.png)     prompt = pipe.tokenizer.apply_chat_template(         messages,
    tokenize=False, add_generation_prompt=True     ) ![5](assets/5.png)     predictions
    = pipe(         prompt,         temperature=temperature,         max_new_tokens=256,         do_sample=True,         top_k=50,         top_p=0.95,     )
    ![6](assets/6.png)     output = predictions[0]["generated_text"].split("</s>\n<|assistant|>\n")[-1]
    ![7](assets/7.png)     return output [PRE1] # main.py  from fastapi import FastAPI
    from models import load_text_model, generate_text  app = FastAPI()  @app.get("/generate/text")
    ![1](assets/1.png) def serve_language_model_controller(prompt: str) -> str: ![2](assets/2.png)     pipe
    = load_text_model() ![3](assets/3.png)     output = generate_text(pipe, prompt)
    ![4](assets/4.png)     return output ![5](assets/5.png) [PRE2] http://localhost:8000/generate/text?prompt="What
    is FastAPI?" [PRE3]`  [PRE4] $ pip install streamlit [PRE5]` L[''esempio 3-3](#streamlit_chat_ui)
    mostra come sviluppare una semplice interfaccia utente per connettersi al servizio.    #####
    Esempio 3-3\. L''interfaccia utente della chat di Streamlit che utilizza l''endpoint
    FastAPI /`generate`    [PRE6]    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO3-1)      Aggiungi
    un titolo alla tua applicazione che sarà reso all''interfaccia utente.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO3-2)      Inizializza
    la chat e tiene traccia della cronologia della chat.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO3-3)      Visualizza
    i messaggi della cronologia delle chat al riavvio dell''applicazione.      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO3-4)      Attendi
    che l''utente invii un prompt tramite il campo di inserimento della chat.      [![5](assets/5.png)](#co_ai_integration_and_model_serving_CO3-5)      Aggiungi
    i messaggi dell''utente o dell''assistente alla cronologia delle chat.      [![6](assets/6.png)](#co_ai_integration_and_model_serving_CO3-6)      Visualizza
    il messaggio dell''utente nel contenitore dei messaggi della chat.      [![7](assets/7.png)](#co_ai_integration_and_model_serving_CO3-7)      Invia
    una richiesta `GET` con il prompt come parametro di query al tuo endpoint FastAPI
    per generare una risposta da TinyLlama.      [![8](assets/8.png)](#co_ai_integration_and_model_serving_CO3-8)      Convalida
    che la risposta sia OK.      [![9](assets/9.png)](#co_ai_integration_and_model_serving_CO3-9)      Visualizza
    il messaggio dell''assistente nel contenitore dei messaggi della chat.      Ora
    puoi avviare l''applicazione client Streamlit:^([11](ch03.html#id674))    [PRE7]   `Ora
    dovresti essere in grado di interagire con TinyLlama all''interno di Streamlit,
    come mostrato nella [Figura 3-12](#streamlit_ui_text_results). Tutto questo è
    stato possibile con alcuni brevi script Python.  ![bgai 0312](assets/bgai_0312.png)  ######
    Figura 3-12\. Client Streamlit    La[Figura 3-13](#tiny_llama_fastapi_architecture)
    mostra l''architettura generale del sistema della soluzione che abbiamo sviluppato
    finora.  ![bgai 0313](assets/bgai_0313.png)  ###### Figura 3-13\. Architettura
    del sistema di servizi FastAPI    ###### Avvertenze    Sebbene la soluzione dell''[Esempio
    3-3](#streamlit_chat_ui) sia ottima per la prototipazione e il test dei modelli,
    non è adatta per i carichi di lavoro di produzione in cui diversi utenti hanno
    bisogno di accedere simultaneamente al modello. Questo perché con la configurazione
    attuale, il modello viene caricato e scaricato in memoria ogni volta che viene
    elaborata una richiesta. Dover caricare e scaricare un modello di grandi dimensioni
    da e verso la memoria è lento eblocca l''I/O.    Il servizio TinyLlama che hai
    appena costruito utilizza un trasformatore *decodificatore*, ottimizzato per i
    casi d''uso delle conversazioni e delle chat. Tuttavia, il [documento originale
    sui trasformatori](https://oreil.ly/RqztC) introduceva un''architettura che consisteva
    sia in un encoder che in un decoder.    A questo punto dovresti essere più sicuro
    di come funzionano i modelli linguistici e di come confezionarli in un server
    web FastAPI.    I modelli linguistici rappresentano solo una parte di tutti i
    modelli generativi. Le prossime sezioni amplieranno le tue conoscenze per includere
    il funzionamento e il servizio dei modelli che generano audio, immagini e video.    Possiamo
    iniziare a lavorare con i modelli audio.` [PRE8]``  [PRE9][PRE10]``py[PRE11]``
    ## Modelli audio    Nei servizi GenAI, i modelli audio sono importanti per la
    creazione di suoni interattivi e realistici. A differenza dei modelli di testo
    con cui hai familiarità, che si concentrano sull''elaborazione e la generazione
    del testo, i modelli audio possono gestire i segnali audio. Con essi puoisintetizzare
    il parlato, generare musica e persino creare effetti sonori per applicazioni come
    gli assistentivirtuali, il doppiaggio automatico, lo sviluppo di giochi e gliambienti
    audio immersivi.    Uno dei modelli text-to-speech e text-to-audio più efficaci
    è il modello Bark creato da Suno AI. Questo modello, basato su trasformatori,
    è in grado di generare un parlato e un audio multilingue realistico che include
    musica, rumore di fondo ed effetti sonori.    Il modello Bark è composto da quattro
    modelli concatenati tra loro come una pipeline per sintetizzare forme d''onda
    audio a partire da prompt testuali, come mostrato nella [Figura 3-15](#bark_pipeline).  ![bgai
    0315](assets/bgai_0315.png)  ###### Figura 3-15\. Pipeline di sintesi della corteccia    1\.
    Modello semantico del testo      Un modello trasformatore autoregressivo causale
    (sequenziale) accetta un testo di input tokenizzato e ne cattura il significato
    tramite tokens semantici. I modelli autoregressivi predicono i valori futuri di
    una sequenza riutilizzando i propri output precedenti.      2\. Modello acustico
    grossolano      Un trasformatore autoregressivo causale riceve le uscite del modello
    semantico e genera le caratteristiche audio iniziali, che mancano di dettagli
    più fini. Ogni previsione si basa sulle informazioni passate e presenti della
    sequenza di token semantici.      3\. Modello di acustica fine      Un trasformatore
    autocodificatore non causale perfeziona la rappresentazione audio generando le
    caratteristiche audio rimanenti. Poiché il modello acustico grossolano ha generato
    l''intera sequenza audio, il modello fine non deve essere casuale.      4\. Modello
    di codec audio Encodec      Il modello decodifica l''array audio in uscita da
    tutti i codici audio generati in precedenza.      Bark sintetizza la forma d''onda
    audio decodificando le caratteristiche audio raffinate nell''uscita audio finale
    sotto forma di parole, musica o semplici effetti audio.    L[''esempio 3-4](#small_bark)
    mostra come utilizzare il modello del piccolo Bark.    ##### Esempio 3-4\. Scarica
    e carica il modello della piccola corteccia dal repository di Hugging Face    [PRE12]    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO4-1)      Specifica
    le opzioni di preselezione vocale supportate utilizzando un tipo di `Literal`.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO4-2)      Scarica
    il piccolo processore Bark, che prepara il prompt di testo per il modello principale.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO4-3)      Scarica
    il modello Bark, che verrà utilizzato per generare l''audio in uscita. Entrambi
    gli oggetti saranno necessari per la generazione dell''audio in seguito.      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO4-4)      Preelabora
    il prompt di testo con un embedding della voce dell''altoparlante e restituisce
    un array di tensori PyTorch di input tokenizzati utilizzando `return_tensors="pt"`.      [![5](assets/5.png)](#co_ai_integration_and_model_serving_CO4-5)      Genera
    un array audio che contiene i valori di ampiezza del segnale audio sintetizzato
    nel tempo.      [![6](assets/6.png)](#co_ai_integration_and_model_serving_CO4-6)      Ottieni
    la frequenza di campionamento dalle configurazioni di generazione del modello,
    che possono essere utilizzate per produrre l''audio.      Quando generi l''audio
    utilizzando un modello, l''output è una sequenza di numeri a virgola mobile che
    rappresentano l''*ampiezza* (o la forza) del segnale audio in ogni momento.    Per
    riprodurre l''audio, è necessario convertirlo in un formato digitale che possa
    essere inviato agli altoparlanti, il che comporta il campionamento del segnale
    audio a una frequenza fissa e la quantizzazione dei valori di ampiezza a un numero
    fisso di bit. La libreria `soundfile` può aiutarti a generare il file audio utilizzando
    una *frequenza di campionamento*. Più alta è la frequenza di campionamento, più
    campioni vengono prelevati, migliorando la qualità dell''audio ma aumentando anche
    le dimensioni del file.    Puoi installare la libreria audio `soundfile` per scrivere
    file audio utilizzando `pip`:    [PRE13]   [PRE14] # utils.py  from io import
    BytesIO import soundfile import numpy as np  def audio_array_to_buffer(audio_array:
    np.array, sample_rate: int) -> BytesIO:     buffer = BytesIO()     soundfile.write(buffer,
    audio_array, sample_rate, format="wav") ![1](assets/1.png)     buffer.seek(0)     return
    buffer ![2](assets/2.png)  # main.py  from fastapi import FastAPI, status from
    fastapi.responses import StreamingResponse  from models import load_audio_model,
    generate_audio from schemas import VoicePresets from utils import audio_array_to_buffer  @app.get(     "/generate/audio",     responses={status.HTTP_200_OK:
    {"content": {"audio/wav": {}}}},     response_class=StreamingResponse, ) ![3](assets/3.png)
    def serve_text_to_audio_model_controller(     prompt: str,     preset: VoicePresets
    = "v2/en_speaker_1", ):     processor, model = load_audio_model()     output,
    sample_rate = generate_audio(processor, model, prompt, preset)     return StreamingResponse(         audio_array_to_buffer(output,
    sample_rate), media_type="audio/wav"     ) ![4](assets/4.png) [PRE15] # client.py  for
    message in st.session_state.messages:     with st.chat_message(message["role"]):         content
    = message["content"]         if isinstance(content, bytes):             st.audio(content)         else:             st.markdown(content)   if
    prompt := st.chat_input("Write your prompt in this input field"):     response
    = requests.get(         f"http://localhost:8000/generate/audio", params={"prompt":
    prompt}     )     response.raise_for_status()     with st.chat_message("assistant"):         st.text("Here
    is your generated audio")         st.audio(response.content) ![1](assets/1.png)
    [PRE16]`  [PRE17]`` ## Modelli di visione    Utilizzando i modelli di visione,
    puoi generare, migliorare e comprendere le informazioni visive contenute nei prompt.    Poiché
    questi modelli sono in grado di produrre risultati molto realistici più velocemente
    di qualsiasi essere umano e di comprendere e manipolare i contenuti visivi esistenti,
    sono estremamente utili per applicazioni come i generatori e gli editor di immagini,
    il rilevamento di oggetti, la classificazione e la didascalia delle immagini e
    la realtà aumentata.    Una delle architetture più popolari utilizzate per addestrare
    i modelli di immagini si chiama *Diffusione Stabile* (SD).    I modelli SD vengono
    addestrati per codificare le immagini in ingresso in uno spazio latente. Questo
    spazio latente è la rappresentazione matematica dei modelli nei dati di addestramento
    che il modello ha appreso. Se provi a visualizzare un''immagine codificata, tutto
    ciò che vedresti è un''immagine di rumore bianco, simile ai punti bianchi e neri
    che vedi sullo schermo della tua TV quando perde il segnale.    La[Figura 3-17](#stable_diffusion)
    mostra l''intero processo di addestramento e inferenza e visualizza come le immagini
    vengono codificate e decodificate attraverso i processi di diffusione in avanti
    e inversa. Un codificatore di testo che utilizza testo, immagini e mappe semantiche
    aiuta a controllare l''output attraverso la diffusione inversa.  ![bgai 0317](assets/bgai_0317.png)  ######
    Figura 3-17\. Formazione e inferenza della diffusione stabile    Ciò che rende
    questi modelli magici è la loro capacità di decodificare le immagini rumorose
    in immagini di input originali. In effetti, i modelli SD imparano anche a rimuovere
    il rumore bianco da un''immagine codificata per riprodurre l''immagine originale.
    Il modello esegue questo processo di denoising in diverse iterazioni.    Tuttavia,
    non vuoi ricreare immagini già esistenti, ma vuoi che il modello crei nuove immagini
    mai viste prima. Ma come può un modello SD raggiungere questo obiettivo? La risposta
    sta nello spazio latente in cui risiedono le immagini rumorose codificate. Puoi
    modificare il rumore di queste immagini in modo che quando il modello le denobilita
    e le decodifica, ottieni un''immagine completamente nuova che il modello non ha
    mai visto prima.    La soluzione consiste nel codificare anche le descrizioni
    dell''immagine accanto all''immagine stessa. I modelli nello spazio latente vengono
    quindi mappati in descrizioni testuali dell''immagine che si vede in ogni immagine
    di input. A questo punto, si utilizzano prompt testuali per campionare lo spazio
    latente rumoroso in modo che l''immagine di output prodotta dopo il processo di
    denoising sia quella desiderata.    In questo modo i modelli SD possono generare
    nuove immagini che non hanno mai visto prima nei loro dati di formazione. In sostanza,
    questi modelli navigano in uno spazio latente che contiene rappresentazioni codificate
    di vari modelli e significati.^([12](ch03.html#id686)) Il modello affina iterativamente
    questo rumore attraverso un processo di denoising per produrre un''immagine nuova
    non presente nel set di dati di addestramento.    Per scaricare un modello SD,
    è necessario che sia installata la libreria Hugging Face `diffusers`:    [PRE18]   [PRE19]
    # models.py  import torch from diffusers import DiffusionPipeline, StableDiffusionInpaintPipelineLegacy
    from PIL import Image  device = torch.device("cuda" if torch.cuda.is_available()
    else "cpu")  def load_image_model() -> StableDiffusionInpaintPipelineLegacy:     pipe
    = DiffusionPipeline.from_pretrained(         "segmind/tiny-sd", torch_dtype=torch.float32,         device=device     )
    ![1](assets/1.png)     return pipe  def generate_image(     pipe: StableDiffusionInpaintPipelineLegacy,
    prompt: str ) -> Image.Image:     output = pipe(prompt, num_inference_steps=10).images[0]
    ![2](assets/2.png) ![3](assets/3.png)     return output ![4](assets/4.png) [PRE20]
    # utils.py  from typing import Literal from PIL import Image from io import BytesIO  def
    img_to_bytes(     image: Image.Image, img_format: Literal["PNG", "JPEG"] = "PNG"
    ) -> bytes:     buffer = BytesIO()     image.save(buffer, format=img_format)     return
    buffer.getvalue() ![1](assets/1.png)  # main.py  from fastapi import FastAPI,
    Response, status from models import load_image_model, generate_image from utils
    import img_to_bytes  ...  @app.get("/generate/image",          responses={status.HTTP_200_OK:
    {"content": {"image/png": {}}}}, ![2](assets/2.png)          response_class=Response)
    ![3](assets/3.png) def serve_text_to_image_model_controller(prompt: str):     pipe
    = load_image_model()     output = generate_image(pipe, prompt) ![4](assets/4.png)     return
    Response(content=img_to_bytes(output), media_type="image/png") ![5](assets/5.png)
    [PRE21] # client.py  ...  for message in st.session_state.messages:     with st.chat_message(message["role"]):         st.image(message["content"])
    ![1](assets/1.png) ...  if prompt := st.chat_input("Write your prompt in this
    input field"):     ...     response = requests.get(         f"http://localhost:8000/generate/image",
    params={"prompt": prompt}     ) ![2](assets/2.png)     response.raise_for_status()     with
    st.chat_message("assistant"):         st.text("Here is your generated image")         st.image(response.content)      ...
    [PRE22]`  [PRE23]` ## Modelli video    I modelli video sono tra i modelli generativi
    più avidi di risorse e spesso richiedono una GPU per produrre un breve frammento
    di buona qualità. Questi modelli devono generare diverse decine di fotogrammi
    per produrre un singolo secondo di video, anche senza alcun contenuto audio.    Stability
    AI ha rilasciato diversi modelli video open source basati sull''architettura SD
    di Hugging Face. Lavoreremo con la versione compressa del loro modello immagine-video
    per un servizio di animazione delle immagini più veloce.    Per iniziare, facciamo
    funzionare un piccolo modello immagine-video usando l''[Esempio 3-10](#video_model_loading).    ######
    Nota    Per eseguire l''[Esempio 3-10](#video_model_loading), potresti aver bisogno
    di accedere a una GPU NVIDIA compatibile con CUDA.    Inoltre, per l''uso commerciale
    del modello `stable-video-diffusion-img2vid`, fai riferimento alla sua [scheda
    modello](https://oreil.ly/DM-0p).    ##### Esempio 3-10\. Scarica e carica il
    modello *img2vid* dell''IA Stability dal repository Hugging Face.    [PRE24]    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO10-1)      Ridimensiona
    l''immagine in ingresso a una dimensione standard prevista dal modello di input.
    Il ridimensionamento protegge anche da input di grandi dimensioni.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO10-2)      Crea
    un generatore di tensori casuali con il seme impostato a 42 per generare fotogrammi
    video riproducibili.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO10-3)      Esegui
    la pipeline di generazione dei fotogrammi per produrre tutti i fotogrammi video
    in una volta sola.Afferra il primo gruppo di fotogrammi generati. Questo passaggio
    richiede una notevole quantità di memoria video.`num_frames` specifica il numero
    di fotogrammi da generare, mentre`decode_chunk_size` specifica quanti fotogrammi
    generare in una volta sola.      Con le funzioni di caricamento del modello già
    pronte, puoi ora creare l''endpoint per il servizio video.    Tuttavia, prima
    di procedere con la dichiarazione del route handler, hai bisogno di una funzione
    di utilità che elabori gli output del modello video dai fotogrammi in un video
    in streaming utilizzando un buffer I/O.    Per esportare una sequenza di fotogrammi
    in video, devi codificarli in un contenitore video utilizzando una libreria video
    come `av`, che implementa i binding di Python alla popolare libreria di elaborazione
    video`ffmpeg`.    Puoi installare la libreria `av` tramite:    [PRE25]   [PRE26]
    # utils.py  from io import BytesIO from PIL import Image import av  def export_to_video_buffer(images:
    list[Image.Image]) -> BytesIO:     buffer = BytesIO()     output = av.open(buffer,
    "w", format="mp4") ![1](assets/1.png)     stream = output.add_stream("h264", 30)
    ![2](assets/2.png)     stream.width = images[0].width     stream.height = images[0].height     stream.pix_fmt
    = "yuv444p" ![3](assets/3.png)     stream.options = {"crf": "17"} ![4](assets/4.png)     for
    image in images:         frame = av.VideoFrame.from_image(image)         packet
    = stream.encode(frame)   ![5](assets/5.png)         output.mux(packet) ![6](assets/6.png)     packet
    = stream.encode(None)     output.mux(packet)     return buffer ![7](assets/7.png)
    [PRE27] $ pip install python-multipart [PRE28]`Una volta installato, puoi configurare
    il nuovo endpoint utilizzando l''[Esempio 3-12](#video_endpoint).    ##### Esempio
    3-12\. Servire i video generati dal modello immagine-video    [PRE29]    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO12-1)      Utilizza
    l''oggetto `File` per specificare `image` come caricamento del file del modulo.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO12-2)      Crea
    un oggetto Pillow `Image` passando i byte dell''immagine trasferiti al servizio.
    La pipeline del modello si aspetta un formato di immagine Pillow come input.      [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO12-3)      Esporta
    i fotogrammi generati come video MP4 e trasmettili al client utilizzando un buffer
    video iterabile.      Con l''endpoint video configurato, ora puoi caricare le
    immagini sul tuo servizio FastAPI per animarle come video.    Ci sono altri modelli
    video disponibili sull''hub che ti permettono di generare GIF e animazioni. Per
    fare ulteriore pratica, puoi provare a creare un servizio GenAI con questi modelli.Mentre
    i modelli video open source possono produrre video di ampia qualità, l''annuncio
    di OpenAI di un nuovo modello di visione di grandi dimensioni (LVM) chiamato Sora
    ha scosso il settore della generazione di video.    ### OpenAI Sora    I modelli
    text-to-video sono limitati nelle loro capacità di generazione: a parte l''immensa
    potenza di calcolo necessaria per generare sequenzialmente fotogrammi video coerenti,
    l''addestramento di questi modelli può essere impegnativo a causa di:    *   *Mantenere
    la coerenza temporale e spaziale tra i fotogrammi* per ottenere risultati video
    realistici e non distorti.           *   *Mancanza di dati di formazione* con
    didascalie e metadati di alta qualità, necessari per addestrare i modelli video.           *   Le*sfide
    legate alle didascalie* sono molteplici: la didascalia del contenuto dei video,
    chiara e descrittiva, richiede molto tempo e va oltre la stesura di brevi brani
    di testo. Le didascalie devono descrivere la narrazione e le scene di ogni sequenza
    affinché il modello possa apprendere e mappare i ricchi schemi contenuti nel video
    nel testo.              Per questi motivi, non c''è stata una svolta nei modelli
    di generazione video fino all''annuncio del modello Sora di OpenAI.    Sora è
    un modello di trasformatore generalista per la diffusione della visione di grandi
    dimensioni in grado di generare video e immagini con durate, rapporti d''aspetto
    e risoluzioni diverse, fino a un minuto intero di video ad alta definizione. La
    sua architettura si basa sui trasformatori comunemente utilizzati negli LLMs e
    sul processo di diffusione. Mentre gli LLMs utilizzano token di testo, Sora utilizza
    patch visive.    ###### Suggerimento    Il modello Sora combina elementi e principi
    delle architetture transformer e SD, mentre nell''[Esempio 3-10](#video_model_loading)
    hai utilizzato il modello SD di Stability AI per generare i video.    Quindi cosa
    rende Sora diverso?    I trasformatori hanno dimostrato una notevole scalabilità
    nei modelli linguistici, nella visione computerizzata e nella generazione di immagini,
    quindi era logico che l''architettura di Sora si basasse sui trasformatori per
    gestire input diversi come testo, immagini o fotogrammi video. Inoltre, poiché
    i trasformatori sono in grado di comprendere schemi complessi e dipendenze a lungo
    raggio nei dati sequenziali, Sora, in quanto trasformatore di visione, può anche
    catturare relazioni temporali e spaziali a grana fine tra i fotogrammi video per
    generare fotogrammi coerenti con transizioni fluide tra di essi (cioè, che mostrano
    coerenza temporale).    Inoltre, Sora prende in prestito le capacità dei modelli
    SD per generare fotogrammi video di alta qualità e visivamente coerenti, con controlli
    precisi grazie al processo iterativo di riduzione del rumore. L''utilizzo del
    processo di diffusione consente a Sora di generare immagini con dettagli fini
    e proprietà desiderabili.    Combinando il ragionamento sequenziale dei trasformatori
    con il perfezionamento iterativo di SD, Sora è in grado di generare video ad alta
    risoluzione, coerenti e fluidi da input multimodali come testo e immagini che
    contengono concetti astratti.    L''architettura di rete di Sora è stata progettata
    anche per ridurre la dimensionalità attraverso una retea forma di U in cui i dati
    visivi altamente dimensionali vengono compressi e codificati in uno spazio latente
    rumoroso. Sora può quindi generare patch dallo spazio latente attraverso il processo
    di diffusione del denoising.    Il processo di diffusione è simile a quello dei
    modelli SD basati sulle immagini. Invece di avere unarete U 2D normalmente utilizzata
    per le immagini, OpenAI ha addestrato una rete U 3D in cui la terza dimensione
    è una sequenza di fotogrammi nel tempo (un video), come mostra la [Figura 3-20](#images_to_videos).  ![bgai
    0320](assets/bgai_0320.png)  ###### Figura 3-20\. Una sequenza di immagini forma
    un video    OpenAI ha dimostrato che comprimendo i video in patch, come mostrato
    nella [Figura 3-21](#videos_to_patches), il modello può raggiungere la scalabilità
    dell''apprendimento di rappresentazioni ad alta dimensionalità durante l''addestramento
    su diversi tipi di video e immagini che variano per risoluzione, durata e rapporto
    di aspetto.  ![bgai 0321](assets/bgai_0321.png)  ###### Figura 3-21\. Compressione
    video in patch spazio-temporali    Tramite il processo di diffusione, Sora sminuzza
    le patch rumorose in ingresso per generare video e immagini pulite in qualsiasi
    rapporto di aspetto, dimensione e risoluzione per i dispositivi direttamente nelle
    loro dimensioni native.    Mentre un trasformatore di testo predice il token successivo
    in una sequenza di testo, il trasformatore di visione di Sora predice la patch
    successiva per generare un''immagine o un video, come mostrato nella [Figura 3-22](#vision_transformer_sequence).  ![bgai
    0322](assets/bgai_0322.png)  ###### Figura 3-22\. Previsione dei token da parte
    del trasformatore di visione    Grazie all''addestramento su vari set di dati,
    OpenAI ha superato le sfide precedentemente menzionate per l''addestramento dei
    modelli di visione, come la mancanza di didascalie di qualità, l''elevata dimensionalità
    dei dati video e così via, per citarne alcune.    Ciò che affascina di Sora e
    potenzialmente di altri LVM sono le capacità emergenti che mostrano:    Coerenza
    3D      Gli oggetti nelle scene generate rimangono coerenti e si adattano alla
    prospettiva anche quando la telecamera si muove e ruota intorno alla scena.      Permanenza
    dell''oggetto e coerenza ad ampio raggio      Gli oggetti e le persone che vengono
    occlusi o che escono da un''inquadratura in una determinata posizione persisteranno
    quando riappariranno nel campo visivo. In alcuni casi, il modello ricorda effettivamente
    come mantenerli coerenti nell''ambiente. Si tratta anche della *coerenza temporale*,
    che la maggior parte dei modelli video non riesce a gestire.      Interazione
    mondiale      Le azioni simulate nei video generati influenzano realisticamente
    l''ambiente. Ad esempio, Sora capisce che l''azione di mangiare un hamburger dovrebbe
    lasciare il segno di un morso su di esso.      Simulazione di ambienti      Sora
    può anche simulare mondi reali o fittizi, come nei giochi, rispettando le regole
    delle interazioni in quegli ambienti, come nel caso di un personaggio in un livello
    di *Minecraft*. In altre parole, Sora ha imparato a essere un motore fisico guidato
    dai dati.      La[Figura 3-23](#sora_emerging_capabilities) illustra queste funzionalità.  ![bgai
    0323](assets/bgai_0323.png)  ###### Figura 3-23\. Capacità emergenti di Sora    Al
    momento della stesura di questo articolo, Sora non è ancora stato rilasciato come
    API, ma sono già nate delle alternative open source. Un promettente modello di
    visione di grandi dimensioni chiamato "Latte" ti permette di mettere a punto l''LVM
    sui tuoi dati visivi.    ###### Attenzione    Al momento in cui scriviamo non
    è ancora possibile commercializzare alcuni modelli open source, tra cui Latte.
    Controlla sempre la scheda del modello e la licenza per assicurarti che l''uso
    commerciale sia consentito.    La combinazione di trasformatori e diffusori per
    creare LVM è un''area di ricerca promettente per la generazione di output complessi
    come i video. Tuttavia, immagino che lo stesso processo possa essere applicato
    per generare altri tipi di dati ad alta dimensionalità che possono essere rappresentati
    come array multidimensionali .    Ora dovresti sentirti più a tuo agio nella creazione
    di servizi con modelli di testo, audio, visione e video. Poi, diamo un''occhiata
    a un''altra serie di modelli in grado di generare dati complessi come le geometrie
    3D costruendo un servizio di generatore di asset 3D.[PRE30]``  [PRE31] [PRE32]`py
    [PRE33]py`` [PRE34]py[PRE35][PRE36][PRE37] [PRE38]py` # Strategie per servire
    i modelli di intelligenza artificiale generativa    Ora dovresti sentirti più
    sicuro nel creare i tuoi endpoint che servono una varietà di modelli dal repository
    di modelli di Hugging Face. Abbiamo toccato alcuni modelli diversi, tra cui quelli
    che generano testo, immagini, video, audio e forme 3D.    I modelli utilizzati
    erano piccoli, quindi potevano essere caricati e utilizzati su una CPU con risultati
    ragionevoli.Tuttavia, in uno scenario di produzione, potresti voler utilizzare
    modelli più grandi per produrre risultati di qualità superiore che potrebbero
    essere eseguiti solo su GPU e richiedere una quantità significativa di memoria
    video ad accesso casuale (VRAM).    Oltre a sfruttare le GPU, dovrai scegliere
    una strategia di model-serving tra diverse opzioni:    Sii agnostico rispetto
    al modello      Carica i modelli e genera gli output a ogni richiesta (utile per
    lo scambio di modelli).      Essere efficiente dal punto di vista del calcolo      Utilizza
    la FastAPI lifespan per precaricare i modelli che possono essere riutilizzati
    per ogni richiesta.      Sii snello      Servire i modelli all''esterno senza
    framework o lavorare con API di modelli di terze parti e interagire con esse tramite
    FastAPI.      Vediamo nel dettaglio ogni strategia.    ## Sii agnostico: scambia
    i modelli ad ogni richiesta    Negli esempi di codice precedenti, hai definito
    le funzioni di caricamento e generazione del modello e poi le hai utilizzate nei
    controller dei route handler. Utilizzando questa strategia di servizio, FastAPI
    carica un modello nella RAM (o nella VRAM se si utilizza una GPU) ed esegue un
    processo di generazione. Una volta che FastAPI restituisce i risultati, il modello
    viene scaricato dalla RAM. Il processo si ripete per la richiesta successiva.    Quando
    il modello viene scaricato dopo l''uso, la memoria viene liberata per essere utilizzata
    da un altro processo o modello. Con questo approccio, puoi scambiare dinamicamente
    vari modelli in un''unica richiesta se il tempo di elaborazione non è un problema.
    Ciò significa che altre richieste concorrenti devono aspettare prima che il server
    risponda.    Quando serve le richieste, FastAPI mette in coda le richieste in
    arrivo e le elabora in un ordine "first in first out" (FIFO). Questo comportamento
    comporta lunghi tempi di attesa perché un modello deve essere caricato e scaricato
    ogni volta. Nella maggior parte dei casi, questa strategia non è consigliata,
    ma se hai bisogno di scambiare tra più modelli di grandi dimensioni e non hai
    sufficiente RAM, allora puoi adottarla per la prototipazione. Tuttavia, negli
    scenari di produzione, non dovresti mai usare questa strategia per ovvie ragioni:
    i tuoi utenti vorranno evitare i lunghi tempi di attesa.    [La Figura 3-28](#model_loading_on_request)
    mostra questo modello di strategia di servizio.  ![bgai 0329](assets/bgai_0329.png)  ######
    Figura 3-28\. Caricamento e utilizzo dei modelli ad ogni richiesta    Se hai bisogno
    di usare modelli diversi per ogni richiesta e hai una memoria limitata, questo
    metodo può funzionare bene per fare delle prove veloci su un computer meno potente
    con pochi utenti. Il compromesso è un tempo di elaborazione significativamente
    più lento a causa delloscambio deimodelli.Tuttavia, negli scenari di produzione,
    è meglio avere una RAM più grande e usare la strategia di precaricamento dei modelli
    con la durata dell''applicazione FastAPI.    ## Efficienza di calcolo: Precaricare
    i modelli con la durata di vita di FastAPI    La strategia più efficiente dal
    punto di vista dei calcoli per caricare i modelli in FastAPI è quella di utilizzare
    la durata di vita dell''applicazione. Con questo approccio, carichi i modelli
    all''avvio dell''applicazione e li scarichi allo spegnimento. Durante lo spegnimento,
    puoi anche eseguire tutte le operazioni di pulizia necessarie, come la pulizia
    del filesystem o la registrazione.    Il vantaggio principale di questa strategia
    rispetto alla prima è che eviti di ricaricare i modelli pesanti a ogni richiesta.
    Puoi caricare un modello pesante una volta sola e poi eseguire le generazioni
    a ogni richiesta che arriva utilizzando un modello precaricato. Di conseguenza,
    risparmierai diversi minuti di tempo di elaborazione in cambio di una parte significativa
    della tua RAM (o VRAM se usi la GPU). Tuttavia, l''esperienza dell''utente dell''applicazione
    migliorerà notevolmente grazie ai tempi di risposta più brevi.    La[Figura 3-29](#model_loading_lifespan)
    mostra la strategia di model-serving che utilizza l''application lifespan.  ![bgai
    0330](assets/bgai_0330.png)  ###### Figura 3-29\. Utilizzo dell''applicazione
    FastAPI per precaricare i modelli    Puoi implementare il precaricamento del modello
    utilizzando l''applicazione lifespan, come mostrato nell''[Esempio 3-16](#model_preloading_lifespan).    #####
    Esempio 3-16\. Precaricamento del modello con l''applicazione lifespan    [PRE39]py    [![1](assets/1.png)](#co_ai_integration_and_model_serving_CO16-1)      Inizializza
    un dizionario mutabile vuoto nell''ambito dell''applicazione *globale* per contenere
    uno o più modelli.      [![2](assets/2.png)](#co_ai_integration_and_model_serving_CO16-2)      Utilizza
    il decoratore `asynccontextmanager` per gestire gli eventi di avvio e chiusura
    come parte di un gestore di contesto asincrono:    *   Il gestore del contesto
    eseguirà il codice prima e dopo la parola chiave `yield`.           *   La parola
    chiave `yield` nella funzione decorata `lifespan` separa le fasi di avvio e di
    arresto.           *   Il codice precedente alla parola chiave `yield` viene eseguito
    all''avvio dell''applicazione prima che vengano gestite le richieste.           *   Quando
    vuoi terminare l''applicazione, FastAPI eseguirà il codice dopo la parola chiave
    `yield` come parte della fase di arresto.                [![3](assets/3.png)](#co_ai_integration_and_model_serving_CO16-3)      Precarica
    il modello all''avvio sul dizionario `models`.      [![4](assets/4.png)](#co_ai_integration_and_model_serving_CO16-4)      Inizia
    a gestire le richieste perché la fase di avvio è terminata.      [![5](assets/5.png)](#co_ai_integration_and_model_serving_CO16-5)      Cancella
    il modello all''arresto dell''applicazione.      [![6](assets/6.png)](#co_ai_integration_and_model_serving_CO16-6)      Crea
    il server FastAPI e passagli la funzione lifespan da utilizzare.      [![7](assets/7.png)](#co_ai_integration_and_model_serving_CO16-7)      Passa
    l''istanza del modello globale precaricato alla funzione di generazione.      Se
    avvii l''applicazione ora, dovresti vedere immediatamente il caricamento delle
    pipeline del modello nella memoria. Prima di applicare queste modifiche, le pipeline
    del modello venivano caricate solo quando si effettuava la prima richiesta.    ######
    Avvertenze    Puoi precaricare più di un modello in memoria usando la strategia
    lifespan model-serving, ma questo non è pratico con modelli GenAI di grandi dimensioni.
    I modelli generativi possono essere affamati di risorse e nella maggior parte
    dei casi avrai bisogno di GPU per accelerare il processo di generazione. Le GPU
    consumer più potenti vengono fornite con soli 24 GB di VRAM. Alcuni modelli richiedono
    18 GB di memoria per eseguire l''inferenza, quindi cerca di distribuire i modelli
    su istanze dell''applicazione e GPU separate.    ## Essere snelli: servire i modelli
    all''esterno    Un''altra strategia per servire i modelli GenAI è quella di pacchettizzarli
    come servizi esterni tramite altri strumenti. Puoi quindi utilizzare la tua applicazione
    FastAPI come livello logico tra il tuo client e il server esterno del modello.
    In questo livello logico puoi gestire il coordinamento tra i modelli, la comunicazione
    con le API, la gestione degli utenti, le misure di sicurezza, le attività di monitoraggio,
    il filtraggio dei contenuti, il miglioramento dei prompt o qualsiasi altra logica
    necessaria.    ### Fornitori di Cloud    I fornitori di Cloud stanno innovando
    costantemente le soluzioni serverless e di calcolo dedicate che puoi utilizzare
    per servire i tuoi modelli all''esterno. Ad esempio, Azure Machine Learning Studio
    fornisce ora uno strumento PromptFlow che puoi utilizzare per distribuire e personalizzare
    modelli OpenAI o di linguaggi open source. Al momento della distribuzione, riceverai
    un endpoint del modello eseguito sul tuo calcolo Azure pronto per essere utilizzato.
    Tuttavia, l''utilizzo di PromptFlow o di strumenti simili richiede una curva di
    apprendimento ripida, in quanto possono richiedere dipendenze particolari e passaggi
    non tradizionali da seguire.    ### BentoML    Un altro grande concorrente per
    il servizio di modelli esterni a FastAPI è BentoML, chesi ispira a FastAPI ma
    implementa una strategia di servizio diversa, costruita appositamente per i modelli
    AI.    Un enorme miglioramento rispetto a FastAPI per la gestione delle richieste
    di modelli concorrenti è la capacitàdi BentoML di eseguire richieste diverse su
    processi worker diversi. Può parallelizzare le richieste delimitate dalla CPU
    senza dover gestire direttamente il multiprocessing di Python. Inoltre, BentoML
    può anche eseguire inferenze di modelli in batch, in modo che il processo di generazione
    per più utenti possa essere eseguito con un''unica chiamata al modello.    Ho
    trattato BentoML in dettaglio nel [Capitolo 2](ch02.html#ch02).    ###### Suggerimento    Per
    eseguire BentoML, dovrai prima installare alcune dipendenze:    [PRE40]py   [PRE41]
    [PRE42] # main.py  from fastapi import FastAPI from openai import OpenAI  app
    = FastAPI() openai_client = OpenAI() system_prompt = "You are a helpful assistant."  @app.get("/generate/openai/text")
    def serve_openai_language_model_controller(prompt: str) -> str | None:     response
    = openai_client.chat.completions.create( ![1](assets/1.png)         model="gpt-4o",         messages=[             {"role":
    "system", "content": f"{system_prompt}"},             {"role": "user", "content":
    prompt},         ],     )     return response.choices[0].message.content [PRE43]`
    [PRE44][PRE45]`` [PRE46] # main.py  import csv import time from datetime import
    datetime, timezone from uuid import uuid4 from typing import Awaitable, Callable
    from fastapi import FastAPI, Request, Response  # preload model with a lifespan
    ...  app = FastAPI(lifespan=lifespan)  csv_header = [     "Request ID", "Datetime",
    "Endpoint Triggered", "Client IP Address",     "Response Time", "Status Code",
    "Successful" ]   @app.middleware("http") ![1](assets/1.png) async def monitor_service(     req:
    Request, call_next: Callable[[Request], Awaitable[Response]] ) -> Response: ![2](assets/2.png)     request_id
    = uuid4().hex ![3](assets/3.png)     request_datetime = datetime.now(timezone.utc).isoformat()     start_time
    = time.perf_counter()     response: Response = await call_next(req)     response_time
    = round(time.perf_counter() - start_time, 4) ![4](assets/4.png)     response.headers["X-Response-Time"]
    = str(response_time)     response.headers["X-API-Request-ID"] = request_id ![5](assets/5.png)     with
    open("usage.csv", "a", newline="") as file:         writer = csv.writer(file)         if
    file.tell() == 0:             writer.writerow(csv_header)         writer.writerow(
    ![6](assets/6.png)             [                 request_id,                 request_datetime,                 req.url,                 req.client.host,                 response_time,                 response.status_code,                 response.status_code
    < 400,             ]         )     return response   # Usage Log Example  """"
    Request ID: 3d15d3d9b7124cc9be7eb690fc4c9bd5 Datetime: 2024-03-07T16:41:58.895091
    Endpoint triggered: http://localhost:8000/generate/text Client IP Address: 127.0.0.1
    Processing time: 26.7210 seconds Status Code: 200 Successful: True """  # model-serving
    handlers ... [PRE47]` [PRE48][PRE49][PRE50][PRE51]``````'
  prefs: []
  type: TYPE_NORMAL
