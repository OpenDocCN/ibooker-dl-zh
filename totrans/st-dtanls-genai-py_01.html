<html><head></head><body>
  <div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">2</span> </span> <span class="chapter-title-text">Using generative AI to ensure sufficient data quality</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Best practices for ensuring high quality of data</li> 
    <li class="readable-text" id="p3">Using generative AI to prepare a data cleaning protocol</li> 
    <li class="readable-text" id="p4">Evaluating data content quality</li> 
    <li class="readable-text" id="p5">Dealing with data errors</li> 
    <li class="readable-text" id="p6">Investigating unclear data</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>In MS Excel, you can calculate the trend line and standard deviation of a sample on the basis of just two data points. Clearly, such “data analysis” is meaningless. This chapter will help you focus your efforts on things you <em>should</em> do with data, rather than just expand on what you <em>can</em> do with it. It explains the necessary background for any analysis you may wish to perform. You will learn about best practices and non-negotiable rules, ensuring that your conclusions are related to the business activities you’re analyzing, rather than to flaws in the underlying data. </p> 
  </div> 
  <div class="readable-text intended-text" id="p8"> 
   <p>You’ll develop a structured approach to quality assessment and assurance, you’ll purge your data of artifacts, you’ll identify the blind spots, and you’ll learn to think about the benefits and risks of guesstimating missing pieces. Finally, you’ll learn to look at the collected data from a new perspective—the perspective of its usefulness for the process of analysis. </p> 
  </div> 
  <div class="readable-text" id="p9"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_19"><span class="num-string">2.1</span> On a whimsy of fortune</h2> 
  </div> 
  <div class="readable-text" id="p10"> 
   <p>Imagine you bet your business future on the flip of a coin. Let’s say you bet on tails. You threw the coin and, fortunately, got tails. Do you think it would be a good idea to do this again? And again? We most assuredly hope not. The fact that you once got the tails you bet on doesn’t mean that you will get tails again next time. </p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>In classical logic, conclusions drawn from false assumptions can be either true or false. It’s crucial to understand that you really can draw a <em>true conclusion</em> from a <em>false assumption</em>. We will call such a conclusion an <em>unreliable </em>one. It doesn’t mean you’ll be right the next time you follow that false assumption. </p> 
  </div> 
  <div class="readable-text intended-text" id="p12"> 
   <p>Coming back to our example, let’s say you weigh a coin in your hand and say to yourself, “I’m going to flip this coin. If I get tails, I’ll have more customers next month than I did this month.” You throw your coin, get tails, and have more customers next month. Was the assumption that the coin has a predictive power related to the success of your business true? Probably not. We could get into psychology, the influence of self-belief on success, and so on, but even then, it’s not the coin that brought your customers. It was your self-belief and hard work. Or perhaps luck. Or weather. Or a smart marketing strategy. Or your main competitor’s mishap. Or . . . you get the idea. Nevertheless, if you believe in the predictive power of the coin, you’d draw true conclusions from a false assumption.</p> 
  </div> 
  <div class="readable-text intended-text" id="p13"> 
   <p>Similarly, business analytics performed on false assumptions (false input data) may result in true or false conclusions. Poor quality input data leads to <em>unreliable </em>results regardless of the sophistication of the analysis, with the critical word here being <em>unreliable</em>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p14"> 
   <p>Let’s assume you risked performing an analysis on poor data, and it returned “good” results. You bet on tails and got more customers next month. It doesn’t sanctify the approach. Yes, it happens, but it doesn’t mean your results will be good next time. Like flipping the coin, you will get heads or tails, and your next analysis will yet again yield <em>unreliable</em> results. </p> 
  </div> 
  <div class="readable-text intended-text" id="p15"> 
   <p>In data analytics, this concept is often referred to as “garbage in, garbage out.” You now know that “garbage out” doesn’t mean <em>inaccurate </em>results. It’s much, much more dangerous. It means <em>unreliable</em> results. You bet 10 bucks on a flip? How about 100? Did you succeed again? How about a car or a house? </p> 
  </div> 
  <div class="readable-text intended-text" id="p16"> 
   <p>On the other hand, if you throw the coin multiple times and get your expected tails each time, the coin may not be random after all. Perhaps the quality of your data and, therefore, the following analysis is <em>sufficient</em> for your purpose. </p> 
  </div> 
  <div class="readable-text intended-text" id="p17"> 
   <p><em>Reliability</em> of results depends heavily on the quality of the input data. In many cases, we try to avoid being too absolute about the advice we give, as there is often more than one way of solving any given problem. However, if your analysis is to be used in any decision-making process, you simply cannot accept anything less but the best <em>possible </em>quality of available data. We emphasized the word <em>possible </em>for a reason. In most real-world scenarios, you will not have the luxury of working with the quality of data you’d want. But all the compromises you make here must be conscious and taken into account when evaluating the <em>reliability</em> of the final result.</p> 
  </div> 
  <div class="readable-text intended-text" id="p18"> 
   <p>Unfortunately, despite some marketing claims, there is no one-size-fits-all solution ensuring data quality. The most effective way to prepare for analysis is to closely examine the data and carefully consider the meaning of each variable within the specific context of your business. There may not be any way to entirely automate this task, but there are techniques and protocols that can help you approach data evaluation and cleaning in a structured and systematic manner.</p> 
  </div> 
  <div class="readable-text" id="p19"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_20"><span class="num-string">2.2</span> A note on best practices</h2> 
  </div> 
  <div class="readable-text" id="p20"> 
   <p>Let’s clean our data, then! Take the first data file you have on your desktop or in your project repository, and open it. Is it “clean”? (And if you do actually keep data files on the desktop, have a hearty chat with the generative AI of your choice about data management and governance.)</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p21"> 
   <p><span class="print-book-callout-head">Trust, but verify</span>  If you got your data from someone who told you that it is “clean,” ask yourself a question—do you trust this person’s data skills and (not “or”; specifically “and”!) diligence enough to hinge the success of your business on their opinion about what it means that data is “clean”? Even if you do trust them, control is the highest form of trust in the data world: better safe than sorry, look before you leap, and so on.</p> 
  </div> 
  <div class="readable-text" id="p22"> 
   <p>If you’re uncertain about whether your data is clean, we’re on a good path. Several aspects must be checked before you deem your data clean enough to be analyzed:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p23"> <em>Relevance</em><em> </em>—The data should be pertinent to the specific business question or problem being addressed. Irrelevant data can lead to confusion and detract from the focus of the analysis. </li> 
   <li class="readable-text" id="p24"> <em>Accuracy</em><em> </em>—The data should be correct, error-free, and consistent with the real-world objects or events it represents. </li> 
   <li class="readable-text" id="p25"> <em>Completeness</em><em> </em>—All <em>required </em>data points should be present. Any missing or incomplete information should be identified and addressed. At the same time, spurious data should be avoided to maintain clarity. </li> 
   <li class="readable-text" id="p26"> <em>Timeliness</em><em> </em>—Data should be up-to-date and relevant to the analyzed period. Outdated data can result in misleading or irrelevant insights. </li> 
   <li class="readable-text" id="p27"> <em>Uniqueness</em><em> </em>—Duplicate data should be identified and eliminated to prevent redundancies and inaccuracies in the analysis. </li> 
   <li class="readable-text" id="p28"> <em>Consistency</em><em> </em>—Data should be consistent across different sources, formats, and systems. Inconsistencies can lead to incorrect conclusions or insights. </li> 
   <li class="readable-text" id="p29"> <em>Integrity</em><em> </em>—The relationships between data elements should be maintained, ensuring the data is coherent and meaningful when analyzed. </li> 
  </ul> 
  <div class="readable-text" id="p30"> 
   <p>It looks like a lot of work because it is. You want to be sure that your data <em>adequately describes the slice of reality </em>you’re analyzing (first four points) and that its <em>structure allows for reliable analysis</em> (last three points).</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p31"> 
   <p><span class="print-book-callout-head">Slow is smooth, and smooth is fast</span>  Data quality is not a place to skate on thin ice. You need to get used to doing things systematically. A lot of data specialists claim that at least 80% of data work is related to data cleaning and preparation, and these are rather slow processes. Unfortunately, you may be pressed to omit it, due to a very unpleasant feedback loop. Businesses do not see the value of analysis, so they demand more of it. The more they demand, the higher the time pressure. The higher the time pressure, the more corners that are cut (usually in areas that do not get into final reports and thus are invisible or useless from the perspective of the business sponsors). The more corners that are cut, the less reliable the analysis. The less reliable the analytics, the lower the subsequent value of said analytics. The lower the value of the analytics . . . Doing things right from the start will reduce the bumps and hiccups on your analytical journey, caused by your stakeholders coming back to you, requesting repeat analyses better aligned to the reality they observe.</p> 
  </div> 
  <div class="readable-text" id="p32"> 
   <p>Ensuring data quality can be daunting—there is no silver bullet, and cutting corners can lead to unreliable data (and therefore unreliable results). But there is still hope: a good structure can guide you through this process in a finite amount of time. Knowing what to look for, and when, can help you discover the quality issues and estimate their impact on the final analysis. </p> 
  </div> 
  <div class="readable-text" id="p33"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_21"><span class="num-string">2.3</span> Getting started</h2> 
  </div> 
  <div class="readable-text" id="p34"> 
   <p>Do you still have your data file open? Yes, the one from your desktop. No? Open it. Have a look at it. Scroll it up and down, and if needed, left and right. Try to get to know your data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p35"> 
   <p>Do you think you look silly? Not as silly as you’ll look trying to explain why you added the sales volume to the physical product volume (yes, we’ve seen it). In an ideal world, you’d have richly described metadata explaining all the columns down to the expected decimal places and the name of the person who wrote the script that parses texts scraped from the web (if applicable). But most probably, you won’t. You’ll get the file with “address”, “time”, and “sales” columns, and it will be up to you to decipher whether the addresses are of customers or brick-and-mortar shop locations; whether the sales are in USD, EUR, or metric tons; and whether the time is the time of order or purchase, or perhaps a test column left by developers. </p> 
  </div> 
  <div class="readable-text intended-text" id="p36"> 
   <p>We will repeat this again. If you work in business and are not absolutely, positively sure about the exact meaning of the variables received from your IT department, don’t assume. Ask! We happened to analyze the efficiency of a service sending tens of millions of messages to the clients of a multinational company. We discovered an anomaly that would occur very rarely, where the ratio of opened-to-received messages was higher than one. Trying to understand what was <em>actually</em> defined as a received message and what was <em>actually </em>defined as an open message, we discovered that the error in the logging system was so deep that all the previous analytics on the efficiency of messaging were worthless! We cannot overemphasize the importance of close collaboration between the business and data or IT crowds.</p> 
  </div> 
  <div class="readable-text intended-text" id="p37"> 
   <p>From clay tablets to AI, tools change, but the mess in the data doesn’t. What did change with the introduction of generative AI is your access to expert knowledge, which may save you a lot of time. It won’t replace your experience and responsibility for understanding your data, but it may help you get to know it much quicker. </p> 
  </div> 
  <div class="readable-text intended-text" id="p38"> 
   <p>If you have read the available metadata, there are still a couple of steps you should take to make this initial inspection a part of a structured analytical approach. What steps? We could tell you, but this book wouldn’t be about generative AI then, would it?</p> 
  </div> 
  <div class="readable-text intended-text" id="p39"> 
   <p>We want you to accept that your AI advisor is infinitely patient and perfectly nonjudgmental. There is no question too simple to ask. “What was the name of this curve . . . you know, this looking like the hill? How do I plot two columns from a DataFrame?” Honestly, searching one’s own memory is sometimes more time consuming than dropping a quick question and running the provided code.</p> 
  </div> 
  <div class="readable-text intended-text" id="p40"> 
   <p>We also want you to forget about “prompt engineering.” It has its place, and some models, such as smaller Llama versions, are incredibly vulnerable to prompt structure. Changing a single word to a synonym can sometimes drastically change the model’s understanding of the prompt. We assume, however, that you will use a generative AI that understands the linguistic context sufficiently to not require more work on prompt engineering than it would take to find relevant information about actual analytics elsewhere. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p41"> 
   <p><span class="print-book-callout-head">Note</span>  As mentioned in the “about this book” section (before chapter 1), we mainly use the Python environment to process example datasets. Chapter 7 will guide you through using generative AI to adapt the code to your environment if needed.</p> 
  </div> 
  <div class="readable-text" id="p42"> 
   <p>Let’s see how generative AI (in this case, ChatGPT 4) deals with a plain language question to the effect of “I’m lost and need guidance.” </p> 
  </div> 
  <div class="readable-text prompt" id="p43"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>I want to perform data analysis. I imported my data into pandas DataFrames. What is the first thing I should do with it? </p> 
  </div> 
  <div class="readable-text response" id="p44"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>After importing your data into pandas DataFrames, the first thing you should do is to explore and understand your data. You can start by performing the following steps:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p45"> Inspect the first few rows of the DataFrame: </li> 
  </ol> 
  <div class="readable-text response" id="p46"> 
   <p>Use the <code>head()</code> method to display the first few rows of the DataFrame. This will give you a quick overview of the data structure, column names, and the type of data contained within each column.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p47"> 
   <div class="code-area-container"> 
    <pre class="code-area">df.head()</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p48"><span class="faux-ol-li-counter">2. </span> Check the shape of the DataFrame: </li> 
  </ol> 
  <div class="readable-text response" id="p49"> 
   <p>Use the <code>shape</code> attribute to get the dimensions of the DataFrame (rows and columns).</p> 
  </div> 
  <div class="browsable-container listing-container" id="p50"> 
   <div class="code-area-container"> 
    <pre class="code-area">df.shape</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p51"><span class="faux-ol-li-counter">3. </span> Obtain summary statistics and data types: </li> 
  </ol> 
  <div class="readable-text response" id="p52"> 
   <p>Use the <code>info()</code> method to get a summary of the data types, number of non-null values, and memory usage of the DataFrame.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p53"> 
   <div class="code-area-container"> 
    <pre class="code-area">df.info()</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p54"><span class="faux-ol-li-counter">4. </span> Generate summary statistics for numerical columns: </li> 
  </ol> 
  <div class="readable-text response" id="p55"> 
   <p>Use the <code>describe()</code> method to get summary statistics for the numerical columns in the DataFrame. This will give you a sense of the central tendency and dispersion of the numerical data.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p56"> 
   <div class="code-area-container"> 
    <pre class="code-area">df.describe()</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p57"><span class="faux-ol-li-counter">5. </span> Check for missing values: </li> 
  </ol> 
  <div class="readable-text response" id="p58"> 
   <p>Use the <code>isna()</code> method in conjunction with the <code>sum()</code> method to count the number of missing values in each column.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p59"> 
   <div class="code-area-container"> 
    <pre class="code-area">df.isna().sum()</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p60"><span class="faux-ol-li-counter">6. </span> Investigate the categorical columns: </li> 
  </ol> 
  <div class="readable-text response" id="p61"> 
   <p>For categorical columns, you can use the <code>value_counts()</code> method to get a count of unique values and their frequencies.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p62"> 
   <div class="code-area-container"> 
    <pre class="code-area">df['column_name'].value_counts()</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p63"> 
   <p>Once you have a better understanding of your data, you can proceed with data cleaning, preprocessing, and analysis. This may involve handling missing values, encoding categorical variables, feature engineering, and other tasks based on the specific requirements of your analysis.</p> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>In its first sentence, our AI advisor agreed with the importance of <em>understanding</em> your data. A careful read of the rest of the model’s answer may reveal some less impressive components. Some steps seem redundant. For example, the <code>df.isna().sum()</code> method does not offer any information not previously provided by <code>df.info()</code> (which, in fact, also includes information returned by <code>df.shape</code>).</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p65"> 
   <p><span class="print-book-callout-head">You need to decide on your priorities</span>  Efficiency versus readability is just one of the choices you’ll have to make when choosing the methods you’ll be using to reach your goals. Generative AI, when inquired, can often present you with the benefits and weaknesses of different methods. Still, you need to be conscious of these choices and sometimes explicitly request a specific method or solution, prioritizing one characteristic over the other.</p> 
  </div> 
  <div class="readable-text" id="p66"> 
   <p>Nevertheless, the overall answer is very good. Like the old carpenter’s adage says, it’s better to measure twice. We appreciate that ChatGPT errs on the side of redundancy. Provided with a wide enough toolset, you’ll soon be able to select the tools that best suit your analysis style. Equipped with insufficient tools, you may stay in the dark.</p> 
  </div> 
  <div class="readable-text intended-text" id="p67"> 
   <p>In summary, you want to check the following:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p68"> The overall “look” of the data. </li> 
   <li class="readable-text" id="p69"> The size of the tables. </li> 
   <li class="readable-text" id="p70"> The types of variables in each column. The quick check should reveal whether there are mismatched data types, such as text strings in place of numerical values or vice versa. </li> 
   <li class="readable-text" id="p71"> For numerical columns, the basic statistics, count, mean, extreme values, etc. </li> 
   <li class="readable-text" id="p72"> The number of missing values in each column. A note of a warning here: some data is inherently sparse. Simply counting the missing values in the column will not provide a definite answer on data completeness; however, if you combine it with the knowledge of data meaning, it should. </li> 
   <li class="readable-text" id="p73"> The number of unique values in each column. </li> 
  </ol> 
  <div class="readable-text" id="p74"> 
   <p>This should give you a good intuition about the data you’re working with.</p> 
  </div> 
  <div class="readable-text intended-text" id="p75"> 
   <p>Let’s load our example data so we have something to analyze: the Brazilian e-commerce public dataset by Olist (<a href="http://www.kaggle.com/datasets/olistbr/brazilian-ecommerce">www.kaggle.com/datasets/olistbr/brazilian-ecommerce</a>). As we mentioned in the preface, you’ll need to use a free Kaggle account. The following listing shows the code we used to load our data. (All the code required to perform the analytics we present is available in our GitHub repository <a href="https://github.com/mariansiwiak/Generative_AI_for_Data_Analytics">https://github.com/mariansiwiak/Generative_AI_for_Data_Analytics</a>.) </p> 
  </div> 
  <div class="browsable-container listing-container" id="p76"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.1</span> Preparing variables for further analysis later in the chapter</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import pandas as pd

df_customers = pd.read_csv('olist_customers_dataset.csv')
df_order_items = pd.read_csv('olist_order_items_dataset.csv')
df_order_payments = pd.read_
csv('olist_order_payments_dataset.csv')
df_order_reviews = pd.read_csv('olist_order_reviews_dataset.csv')
df_orders = pd.read_csv('olist_orders_dataset.csv')
df_products = pd.read_csv('olist_products_dataset.csv')Jupyter
df_sellers = pd.read_csv('olist_sellers_dataset.csv')
df_product_category_translation = 
<span class="">↪</span>pd.read_csv('product_category_name_translation.csv')</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>As the next step, we’ll look at the properties of the DataFrames in listing 2.1. Unfortunately, that means running all of the commands proposed by our generative AI advisor for all downloaded datasets. That’s a habit you should develop for every bit of data you import. </p> 
  </div> 
  <div class="readable-text intended-text" id="p78"> 
   <p>Did you just stop reading and run all the tests on all the DataFrames? Let’s try to induce in you a healthy dose of paranoia. We’ll share some alarming findings to show you how looking at data summaries can point you to potential problems. We’ll also work on identifying the solutions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p79"> 
   <p>We found two interesting points while we waded through the data structures in our example data: </p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p80"> The number of customers in the <code>df_customers</code> DataFrame and the number of orders in the <code>df_orders</code> DataFrame are identical. There are two possibilities. One is that each customer, as identified by the system, made one and only one purchase, and the second is that we have been provided with an incomplete set of data. Both possibilities have far-reaching implications related to further analytics. (The number of 99,441 rows has arbitrarily been set by whoever generated this dataset for us. In this case, reading the metadata on a dataset would tell us this, but let’s assume it’s not available.) </li> 
   <li class="readable-text" id="p81"> The <code>df_customers</code> DataFrame contains two suspiciously similar columns: <code>customer_id</code> and <code>customer_unique_id</code>. As we will probably want to analyze customer properties in other DataFrames, the column that will allow us to connect them is <code>customer_id</code>. As such, this column is of great importance, so we should stop and ask if <code>customer_id</code> is indeed a unique identification for customers. </li> 
  </ul> 
  <div class="readable-text" id="p82"> 
   <p>As we mentioned and will keep mentioning, it’s of utmost importance not to wave away anything that doesn’t look right! You could send an email with questions to whoever provided you with the data and, depending on their workload, get the answer within minutes or months. </p> 
  </div> 
  <div class="readable-text intended-text" id="p83"> 
   <p>If you’re not fluent enough in the available analytical environment, fret not! As long as you know what you want to do, you have a generative AI advisor to help you determine how.</p> 
  </div> 
  <div class="readable-text prompt" id="p84"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>I have a pandas DataFrame with <code>customer_id</code> and <code>order_id</code> columns. How can I quickly check if every customer made just one order or if there are customers who made more than one order?</p> 
  </div> 
  <div class="readable-text response" id="p85"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>You can quickly check if every customer made just one order or if there are customers who made more than one order by using the <code>value_counts()</code> method on the <code>'customer_id'</code> column and then checking the maximum count. Here’s an example:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p86"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import pandas as pd

<em># Sample DataFrame</em>
data = {'customer_id': [1, 2, 3, 4, 2],
        'order_id': [101, 102, 103, 104, 105]}

df = pd.DataFrame(data)

<em># Count orders per customer</em>
order_counts = df['customer_id'].value_counts()

<em># Check if any customer made more than one order</em>
if order_counts.max() &gt; 1:
    print("There are customers with more than one order.")
else:
    print("Every customer made just one order.")</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p87"> 
   <p>In this example, <code>df['customer_id'].value_counts()</code> returns a pandas <code>Series</code> containing the number of orders for each unique <code>'customer_id'</code>. If the maximum value in this <code>Series</code> is greater than 1, it means that at least one customer has made more than one order.</p> 
  </div> 
  <div class="readable-text" id="p88"> 
   <p>Generative AI understood what we wanted to achieve and proposed code we could adapt to our needs, replacing just the name of the DataFrame to be tested with <code>df_orders</code>, as we defined it when loading our datasets. </p> 
  </div> 
  <div class="readable-text intended-text" id="p89"> 
   <p>You can check the results in the files we placed in the repository. It shows that either we got the data from the online platform with the lowest customer retention rate in existence (one visit per user) or there is a problem with the <code>customer_id</code> column in the <code>df_orders</code> DataFrame.</p> 
  </div> 
  <div class="readable-text intended-text" id="p90"> 
   <p>We previously noted that the <code>customer_id</code> column in <code>df_customers</code> DataFrame is accompanied by a <code>customer_unique_id</code> column, which seems redundant if the <code>customer_id</code> column was constructed correctly. The following conversation will show you that working with generative AI is indeed different from using any other software you have ever encountered. All you need to do is provide a plain language explanation of the problem you need to solve. </p> 
  </div> 
  <div class="readable-text prompt" id="p91"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>In my DataFrame containing customers’ data, I have two columns: <code>customer_id</code> and <code>customer_unique_id</code>. How can I check how one relates to the other and which one contains actual identifiers? </p> 
  </div> 
  <div class="readable-text response" id="p92"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To check how the <code>customer_id</code> and <code>customer_unique_id</code> columns relate to each other and which one contains actual identifiers, you can analyze the uniqueness of the values in each column and their relationship. Here’s an example of how you can do this using pandas:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p93"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import pandas as pd

<em># Sample DataFrame</em>
data = {'customer_id': [1, 2, 3, 4, 5],
        'customer_unique_id': [1001, 1002, 1002, 1004, 1004]}

df = pd.DataFrame(data)

<em># Check the number of unique values in each column</em>
unique_customer_ids = df['customer_id'].nunique()
unique_customer_unique_ids = df['customer_unique_id'].nunique()

print(f"Unique customer_id values: {unique_customer_ids}")
print(f"Unique customer_unique_id values: {unique_customer_unique_ids}")

<em># Check the relationship between the two columns</em>
relationship = df.groupby('customer_unique_id')['customer_id'].nunique()

print("\nRelationship between customer_unique_id and customer_id:")
print(relationship)</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p94"> 
   <p>In this example, <code>df['customer_id'].nunique()</code> and <code>df['customer_unique _id'].nunique()</code> return the number of unique values in each column. Comparing these numbers can give you an idea of which column is more likely to contain actual identifiers.</p> 
  </div> 
  <div class="readable-text response" id="p95"> 
   <p>Next, <code>df.groupby('customer_unique_id')['customer_id'].nunique()</code> groups the DataFrame by the <code>customer_unique_id</code> column and computes the number of unique <code>customer_id</code> values for each group. This will help you understand the relationship between the two columns.</p> 
  </div> 
  <div class="readable-text response" id="p96"> 
   <p>Based on the results, you can determine which column contains actual identifiers and how they are related. For example, if <code>customer_unique_id</code> has fewer unique values and each <code>customer_unique_id</code> maps to multiple <code>customer_id</code> values, it may indicate that <code>customer_unique_id</code> is the actual identifier, and <code>customer_id</code> represents multiple records or transactions for each unique customer.</p> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>That’s an interesting response, mainly because it doesn’t suggest that the answer will be definitive! There is even a possibility that none of these columns are valid identifiers (see, this is how you get paranoia in this line of work). That point is essential. Don’t assume anything about data you received from an uncertain source, even if you firmly believe and logic dictates that some values mean one thing or another. In nine out of ten cases, it will be what you expect, but in that tenth case, you’ll thank us after reaching out to the data source and learning what this or that variable <em>actually </em>means.</p> 
  </div> 
  <div class="readable-text intended-text" id="p98"> 
   <p>Returning to our analysis, the resulting <code>relationship</code><em> </em><code>Series</code> is too big to analyze manually. This is another important lesson about working with generative AI. Do not settle for the answers if the result is unsatisfactory. Complain!</p> 
  </div> 
  <div class="readable-text prompt" id="p99"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>With my data, the <code>relationship</code> <code>Series</code> produced by your previous code is too long to check manually. Can you propose a method to analyze its contents? </p> 
  </div> 
  <div class="readable-text response" id="p100"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Sure! You can analyze the <code>relationship Series</code> by generating a summary of its contents, such as the minimum, maximum, mean, and count of unique <code>customer_id</code> values per <code>customer_unique_id</code>. Here’s an example of how you can do this:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p101"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import pandas as pd

<em># Sample DataFrame</em>
data = {'customer_id': [1, 2, 3, 4, 5, 6, 7],
        'customer_unique_id': [1001, 1002, 1002, 1004, 1004, 1001, 1004]}

df = pd.DataFrame(data)

<em># Check the number of unique values in each column</em>
unique_customer_ids = df['customer_id'].nunique()
unique_customer_unique_ids = df['customer_unique_id'].nunique()

print(f"Unique customer_id values: {unique_customer_ids}")
print(f"Unique customer_unique_id values: {unique_customer_unique_ids}")

<em># Check the relationship between the two columns</em>
relationship = df.groupby('customer_unique_id')['customer_id'].nunique()

print("\nSummary of the relationship between customer_unique_id and
<span class="">↪</span>customer_id:")
print(f"Min: {relationship.min()}")
print(f"Max: {relationship.max()}")
print(f"Mean: {relationship.mean()}")
print(f"Count of unique customer_id values per
<span class="">↪</span>customer_unique_id:\n{relationship.value_counts().sort_index()}")</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p102"> 
   <p>In this example, the summary of the relationship between the <code>customer_unique_id</code> and <code>customer_id</code> columns includes the minimum, maximum, and mean number of unique <code>customer_id</code> values per <code>customer_unique_id</code>. Additionally, the <code>relationship.value_counts().sort_index()</code> method returns a count of unique <code>customer_id</code> values per <code>customer_unique_id</code>, sorted by the index (i.e., the number of unique <code>customer_id</code> values).</p> 
  </div> 
  <div class="readable-text response" id="p103"> 
   <p>This summary will give you a better understanding of the relationship between the two columns without having to manually check the entire <code>relationship Series</code>.</p> 
  </div> 
  <div class="readable-text" id="p104"> 
   <p>After we run the code suggested by our generative AI advisor, the results are indeed easier to digest:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p105"> 
   <div class="code-area-container"> 
    <pre class="code-area">Unique customer_id values: 99441
Unique customer_unique_id values: 96096

Summary of the relationship between customer_unique_id and customer_id:
Min: 1
Max: 17
Mean: 1.0348089410589412
Count of unique customer_id values per customer_unique_id:
1     93099
2      2745
3       203
4        30
5         8
6         6
7         3
9         1
17        1
Name: customer_id, dtype: int64</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p106"> 
   <p>It looks like <code>customer_id</code> values uniquely identify <em>records </em>in the dataset rather than uniquely identifying <em>customers</em>. Therefore, <code>customer_unique_id</code> is more likely to serve as a customer identifier. In the actual business environment, our next step would be to check with the IT or other relevant departments whether over 93% of our clients really make only a single order in their lifetime, or whether this is indeed a data quality issue. For now, we’ll take the data at face value and continue our analysis. </p> 
  </div> 
  <div class="readable-text intended-text" id="p107"> 
   <p>The key takeaway here is that generative AI can help us choose suitable analytical methods. Still, it is up to us to ask the right questions and <em>understand the meaning of the data </em>in the context of our business domain.</p> 
  </div> 
  <div class="readable-text" id="p108"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_22"><span class="num-string">2.4</span> Quality assessment structure</h2> 
  </div> 
  <div class="readable-text" id="p109"> 
   <p>We should now know what information is encoded in the data at hand, and we should have an overview of its completeness. </p> 
  </div> 
  <div class="readable-text intended-text" id="p110"> 
   <p>We mentioned previously that you will need a structured approach to ensure sufficient data quality. In the introduction to this chapter, we also emphasized that even “successful” results from an analysis do not ensure the repeatability of this success. The following sections on data cleaning and exploratory data analysis present the main elements of such a quality assessment. This should give you a solid foundation upon which you can build a structure fit for your use case. </p> 
  </div> 
  <div class="readable-text" id="p111"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_23"><span class="num-string">2.4.1</span> Data cleaning steps</h3> 
  </div> 
  <div class="readable-text" id="p112"> 
   <p>You can compare data cleaning to preparing ingredients before cooking a meal. We strongly advise against dropping the contents of your fridge directly into a cooking pot. A good chef inspects ingredients to remove any spoiled parts, washes them to ensure cleanliness, and chops them to uniform sizes for even cooking. Similarly, data cleaning involves inspecting the dataset for inaccuracies or inconsistencies, removing or correcting these elements, and ensuring the data is in a uniform format. This preparatory step is essential to ensure that the final dish (or analytical result) is of high quality and free from elements that could compromise its taste (or integrity). We’ve emphasized multiple times that analyzing unclean data can lead to unreliable conclusions. The comparison with cooking serves this point. By cooking with unprepared ingredients, you can get something edible; we just wouldn’t advise this approach when you’re cooking to impress your date. </p> 
  </div> 
  <div class="readable-text intended-text" id="p113"> 
   <p>To ensure your data is technically clean, you should do the following: </p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p114"> Remove duplicates. </li> 
   <li class="readable-text" id="p115"> Handle missing values. </li> 
   <li class="readable-text" id="p116"> Correct data entry errors. </li> 
   <li class="readable-text" id="p117"> Validate the data. </li> 
  </ol> 
  <div class="readable-text" id="p118"> 
   <p>These steps should ensure that your tables contain data points describing business processes rather than artifacts from the data collection and preparation processes. The next phase should give you a better understanding of the data’s meaning.</p> 
  </div> 
  <div class="readable-text" id="p119"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_24"><span class="num-string">2.4.2</span> Exploratory data analysis elements</h3> 
  </div> 
  <div class="readable-text" id="p120"> 
   <p>By now, your data, or dish ingredients, are filtered and cleaned. Continuing with the cooking analogy, exploratory data analysis (EDA) can be compared to tasting and seasoning the ingredients. Is your tomato fresh and sun-ripened, or did it travel half the world in a freezer and is still so hard that you could play baseball with it? Does your variable have the normal distribution required by many algorithms, or is it skewed? Just as you would taste the dish at different stages, adjusting spices and ingredients to balance flavors and textures, EDA examines the dataset through various analytical and visualization techniques to understand its characteristics, trends, and outliers. This process allows you to adjust your analysis approach, like seasoning, to ensure the outcome is well-balanced and palatable. EDA is about getting a feel for the “flavor profile” of the data, identifying which ingredients (variables) play well together, anticipating how the final dish (analysis) might turn out, and making adjustments as needed for the best possible result.</p> 
  </div> 
  <div class="readable-text intended-text" id="p121"> 
   <p>The basic EDA structure you should employ for all data you lay your hands on could look like this:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p122"> <em>Variables distribution testing</em><em> </em>—What are the characteristics of the data samples, such as mean, standard deviation, range, and so on? </li> 
   <li class="readable-text" id="p123"> <em>Variables visualization</em><em> </em>—What can you read from visual representations of the data? </li> 
   <li class="readable-text" id="p124"> <em>Data meaning clarification</em><em> </em>—Does the data represent what you think it does? </li> 
   <li class="readable-text" id="p125"> <em>Data mapping and harmonization</em><em> </em>—Are all the data points sampled using the same units, and do the headings use standardized description vocabulary? </li> 
  </ol> 
  <div class="readable-text" id="p126"> 
   <p>Some people add extra steps to EDA, like correlation testing or text analysis. Others also add feature engineering. Depending on your data, their approach may be more suitable to your needs. </p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p127"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">One way from Sunday</h5> 
   </div> 
   <div class="readable-text" id="p128"> 
    <p>We do not think a one-size-fits-all, silver-bullet, cure-all data preparation protocol exists. The general framework we believe will lead to high quality data is as follows:</p> 
   </div> 
   <ul> 
    <li class="readable-text" id="p129"> Understand the relation of your data with reality (what each variable means and how it was generated). </li> 
    <li class="readable-text" id="p130"> Ensure the data meets some basic technical standards (that you are working on all available <em>relevant</em><em> </em>data points and can cross-relate them between data sources if needed). </li> 
    <li class="readable-text" id="p131"> Before you analyze reality using your data, analyze the data itself (ensure it adheres to either expertise- or common-sense-based expectations). </li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p132"> 
   <p>In the following sections, we will walk through an example and ensure the data describes phenomena rooted in reality.</p> 
  </div> 
  <div class="readable-text" id="p133"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_25"><span class="num-string">2.5</span> Data cleaning</h2> 
  </div> 
  <div class="readable-text" id="p134"> 
   <p>Data cleaning must be done as early as possible, as errors resulting from poor data representation, consistency, or integrity are persistent and stack up quickly. Unclean data can misguide other data preparation and evaluation steps as well as your analytics results.</p> 
  </div> 
  <div class="readable-text intended-text" id="p135"> 
   <p>Let us repeat: We don’t believe there is one best way of cleaning data or performing EDA. In your work, you may encounter data cleaning protocols, including steps we describe as a part of EDA, and vice versa. You may be given a protocol that includes steps we consider analysis. If specialists in their field prepared these protocols, there is a chance they are better for the data types they are designed for. Be flexible. Just make sure you understand what’s happening.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p136"> 
   <p><span class="print-book-callout-head">Order matters</span>  Some cleaning steps may obfuscate errors, especially in preparatory work. It is of great importance to understand how things change after each step. For example, if you fill missing data with average values from a certain column, you cannot do this before removing duplicates.</p> 
  </div> 
  <div class="readable-text" id="p137"> 
   <p>We’ll assume the worst-case scenario for this exercise. That’s the scenario where you’re unsure what to expect from your data. </p> 
  </div> 
  <div class="readable-text" id="p138"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_26"><span class="num-string">2.5.1</span> Removing duplicates</h3> 
  </div> 
  <div class="readable-text" id="p139"> 
   <p>There are many ways duplicate records can sneak into your data. From data entry mistakes (human errors) and system glitches (software errors) to poorly designed identifiers (human error), to synchronization problems in distributed systems (software error), to errors in merging datasets, to import issues . . . the list goes on. With so many things that can go wrong, some of them most likely will. In all honesty, we’re still waiting to see a duplicate-free dataset that was not previously curated for sharing. </p> 
  </div> 
  <div class="readable-text intended-text" id="p140"> 
   <p>How do you efficiently eliminate duplicate records to avoid overrepresenting certain data points, which could skew the analysis results? It depends on your analytical environment. Do you need to memorize details? Most certainly not!</p> 
  </div> 
  <div class="readable-text prompt" id="p141"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>How can I remove duplicate entries from a DataFrame? </p> 
  </div> 
  <div class="readable-text response" id="p142"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To remove duplicate entries from a DataFrame, you can use the <code>drop_duplicates()</code> method. By default, it considers all columns when identifying duplicates and keeps the first occurrence of a duplicate row. Here’s an example:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p143"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import pandas as pd

<em># Sample DataFrame with duplicate rows</em>
data = {'A': [1, 2, 2, 4, 5, 1],
        'B': [6, 7, 7, 9, 10, 6]}

df = pd.DataFrame(data)

<em># Remove duplicate rows</em>
df_no_duplicates = df.drop_duplicates()

print("Original DataFrame:")
print(df)

print("\nDataFrame without duplicates:")
print(df_no_duplicates)</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p144"> 
   <p>If you want to remove duplicates based on a specific set of columns, you can use the <code>subset</code> parameter:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p145"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><em># Remove duplicates based on column 'A'</em>
df_no_duplicates = df.drop_duplicates(subset=['A'])

print("\nDataFrame without duplicates based on column 'A':")
print(df_no_duplicates)</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p146"> 
   <p>In this example, <code>df.drop_duplicates(subset=['A'])</code> removes duplicate rows based on the values in column <code>A</code> while keeping the first occurrence of each duplicate.</p> 
  </div> 
  <div class="readable-text" id="p147"> 
   <p>This answer is technically correct and, under regular circumstances, sufficient.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p148"> 
   <p><span class="print-book-callout-head">Efficiency may not be the top priority, or it may</span>  Depending on your generative AI’s specifics, it may consider the answer’s simplicity rather than the resource-effectiveness of the solution. Quoting Grace Hopper, <em>it’s a feature, not a bug</em>. Chapter 5 describes methods for using even general-knowledge generative AIs to optimize code.</p> 
  </div> 
  <div class="readable-text" id="p149"> 
   <p>In our case, the answer is sufficient (as we didn’t have any duplicates in the first place), so we can triumphantly tick this box and move on to the next step. </p> 
  </div> 
  <div class="readable-text" id="p150"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_27"><span class="num-string">2.5.2</span> Handling missing values</h3> 
  </div> 
  <div class="readable-text" id="p151"> 
   <p>We will repeat the warning we issued earlier: some data <em>is</em> sparse. However, a list of the possible causes for missing data entries would be at least as long as the one for duplicates. Data entry errors, sensor malfunctions, copying errors, integration errors, you name it. Finally, there is the unavailability of information. </p> 
  </div> 
  <div class="readable-text intended-text" id="p152"> 
   <p>Depending on the context and the amount of missing data, you can choose to either delete the incomplete data entry or variable (the row or column in tabular formats), ignore it, or somehow fill in missing values. You could fill empty fields with a default value, an estimate (such as a mean, median, or mode), or use interpolation methods. It’s critical, however, to make these decisions consciously, depending on the meaning and importance of the data. Recently there was a case of a renowned macroeconomist filling missing data for some countries by copying them from adjacent cells. In that case, the dataset was sorted alphabetically by country names, which caused this method to make completely nonsensical adjustments. </p> 
  </div> 
  <div class="readable-text intended-text" id="p153"> 
   <p>It is paramount that the decision on what to do about missing values is based on understanding why the value is missing. Was it, for example, a sensor system error causing a missed measurement? Or was it an error in the system writing down the results? Was there an error when multiple data sources were connected together? To decide what to do with the missing values, you should first have a look at the data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p154"> 
   <p>In our example data, we should notice three DataFrames where the number of non-null values differs from the number of rows in some columns (meaning it’s not just an empty column). These are <code>df_orders</code>, <code>df_order_reviews</code>, and <code>df_products</code>. </p> 
  </div> 
  <div class="readable-text intended-text" id="p155"> 
   <p>In the <code>df_orders</code> DataFrame, we noticed that some approval and delivery dates are missing. We’ll use this DataFrame in section 2.6 to analyze the underlying process described by the data and the relation of the order status with the missing dates. </p> 
  </div> 
  <div class="readable-text intended-text" id="p156"> 
   <p>In the case of <code>df_products</code>, we are missing some category names, the same number of product names, description lengths, and pictures, and some product size data. Let’s see if we can say anything by looking at the products where some of the values are missing. </p> 
  </div> 
  <div class="readable-text intended-text" id="p157"> 
   <p>There are 611 rows where some of the data is missing. This would be a chore to analyze and address manually, and if we were dealing with 6,111 or 61,111 rows, it would become unfeasible. What can we do to automate the process somewhat? Let’s start with investigating whether the number of missing values in different columns is coincidental or if there is some structure to it.</p> 
  </div> 
  <div class="readable-text intended-text" id="p158"> 
   <p>There is a lesson here about working with generative AI. If you have any doubts about your data but are unsure how to voice them precisely, don’t. We mean, don’t voice them <em>precisely</em>. Simply voice them as best you can. </p> 
  </div> 
  <div class="readable-text prompt" id="p159"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>In my DataFrame, I have missing values. It looks like the number of missing values in some columns is correlated. How can I check if the missing values form a pattern?</p> 
  </div> 
  <div class="readable-text response" id="p160"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>You can check if the missing values form a pattern by analyzing the co-occurrence of missing values across different columns. To do this, you can use pandas to create a DataFrame indicating which values are missing and then compute the correlation between these binary indicators. Here’s an example:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p161"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

<em># Sample DataFrame with missing values</em>
data = {'A': [1, None, 3, 4, None],
        'B': [1, 2, None, 4, None],
        'C': [None, 2, 3, None, 5],
        'D': [1, 2, 3, 4, 5]}

vdf = pd.DataFrame(data)

<em># Create a DataFrame indicating which values are missing</em>
missing_indicator = df.isna()

<em># Compute the correlation between the missing value indicators</em>
missing_corr = missing_indicator.corr()

<em># Visualize the correlation matrix using a heatmap</em>
sns.heatmap(missing_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title("Correlation of Missing Value Indicators")
plt.show()</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p162"> 
   <p>In this example, <code>df.isna()</code> returns a DataFrame of the same shape as <code>df</code>, where each element is a boolean value indicating if the corresponding element in <code>df</code> is missing (<code>True</code>) or not (<code>False</code>).</p> 
  </div> 
  <div class="readable-text response" id="p163"> 
   <p><code>missing_indicator.corr()</code> computes the pairwise correlation between the columns of the missing value indicators. The resulting correlation matrix represents the correlation between the missing values across different columns. High positive correlation values indicate that the missing values in the corresponding columns tend to co-occur.</p> 
  </div> 
  <div class="readable-text response" id="p164"> 
   <p>Finally, the correlation matrix is visualized using a heatmap from the Seaborn library. This heatmap can help you quickly identify patterns in missing values, such as whether missing values in certain columns tend to coincide with missing values in other columns.</p> 
  </div> 
  <div class="readable-text" id="p165"> 
   <p>This time, we got some really heavy artillery, including visualization libraries. Skillful visualization is one of the most important skills in data analysis, and you shouldn’t be shy about leaning on your generative AI advisor to provide you with the tools you need. </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p166"> 
   <p><span class="print-book-callout-head">A picture is worth a thousand words</span>  If you’d like to learn more about the importance of proper visualization for statistical analysis, we can’t recommend timeless books by Professor Edward Tufte enough. His <em>Visual Explanations: Images and Quantities, Evidence and Narrative</em> (Graphics Press, 1997) and <em>The Visual Display of Quantitative Information </em>(Graphics Press, 1997) are technology-agnostic bibles of excellent visual communication.</p> 
  </div> 
  <div class="readable-text" id="p167"> 
   <p>Figure 2.1 shows the results of one of the analyses proposed by generative AI. While there are many types of analysis that can be performed, most of the steps would be similar, so we’ll just use this one as an example. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p168">  
   <img alt="figure" src="../Images/CH02_F01_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.1</span> The correlation between missing values in different columns</h5>
  </div> 
  <div class="readable-text" id="p169"> 
   <p>We can see that some products have missing values in all of the <code>product_category _name</code>, <code>product_name_lenght</code>, and <code>product_photos_qty</code> columns, or in all of the <code>product_weight_g</code>, <code>product_length_cm</code>, <code>product_height_cm</code>, and <code>product_width_cm</code> columns. This means that some products could either be weightless and dimensionless (services, perhaps?) or they don’t have any category, product name, or photos (a mystery box comes to mind). </p> 
  </div> 
  <div class="readable-text print-book-callout" id="p170"> 
   <p><span class="print-book-callout-head">Note</span>  The typo in the variable name <code>product_name_lenght</code> is preserved here to conform to the naming in the original dataset.</p> 
  </div> 
  <div class="readable-text" id="p171"> 
   <p>When we looked for rows in the dataset where every column except for the <code>product_id</code> was empty, we found just one: <code>product_id 5eb564652db742ff8f28759cd8 d2652a</code>. As we discussed earlier, before deciding whether to remove or keep such an item, we should check the business or data collection process behind such an oddity. </p> 
  </div> 
  <div class="readable-text intended-text" id="p172"> 
   <p>Fraud detection and forensic analysis are outside the scope of this book, but following our attempt to instill in you a bit of professional paranoia, we can’t completely ignore the fact that this case begs for an investigation. If we were to investigate such a case, we would first check the simplest explanation: someone was playing with the database and didn’t clean up after themselves. In such a case, this <code>product_id</code> should be present only in this single table. The <code>product_id</code> in question, however, seems to be connected to an order. If possible, we’d try to find the product’s website and see if it is a service (unlikely, as there is just one such case). If we felt especially inquisitive, we’d move to check this seller’s other transactions, particularly with the same buyer. In short, we would try to understand the actual event and the expected shape of the data describing it. From there, we would try to identify how the error could occur in the data (if it is, in fact, an error instead of a non-typical but a legitimate situation).</p> 
  </div> 
  <div class="readable-text intended-text" id="p173"> 
   <p>Unfortunately, we now need to take off our Sherlock Holmes caps and get back to dealing with a much more prevalent, if not so exciting, category of problem.</p> 
  </div> 
  <div class="readable-text" id="p174"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_28"><span class="num-string">2.5.3</span> Correcting data entry errors</h3> 
  </div> 
  <div class="readable-text" id="p175"> 
   <p>Is “Washington” the same thing as “Washington D.C.”? Or did an employee who was saving their precious time by typing just “SD” have in mind “Shipment Delayed” or “Standard Delivery”? You’ll encounter multiple data entry error types in your work, such as typos, mislabeling, or inconsistent formats, all of which negatively affect the dataset’s quality. You need to identify and correct these errors to ensure accurate and consistent data. </p> 
  </div> 
  <div class="readable-text intended-text" id="p176"> 
   <p>This is one of the most strenuous and least automatable parts of data preparation. Unfortunately, the best tool you can use to search for such errors is biological rather than digital. As of the time we’re writing this book, nothing beats a good old-fashioned eyeball. Some of the methods described in section 2.1 can help identify the oddities. For example, <code>df['column_name'].value_counts()</code> can help you spot values that occur only once, which may be suspicious if you expect uniform distribution. However, detecting and correcting data entry errors requires a combination of these techniques, along with domain knowledge and manual inspection.</p> 
  </div> 
  <div class="readable-text intended-text" id="p177"> 
   <p>The specific type of data entry error analysis we want to point out is outlier detection and treatment. <em>Outliers</em> are data points that deviate significantly from the rest of the dataset (for example, when almost all transactions in a set are from NY, and suddenly one is from LA). Identifying and handling outliers is very important, as they can disproportionately impact the analysis results. Alternatively, they can indicate data points of extreme importance. Outliers can be accepted, removed, capped, or transformed, depending on the context.</p> 
  </div> 
  <div class="readable-text" id="p178"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_29"><span class="num-string">2.5.4</span> Data validation</h3> 
  </div> 
  <div class="readable-text" id="p179"> 
   <p>You’d really rather not find entries like “last Tuesday, around noon” or “it was a beautiful spring morning” in the date column of your table. Validating the data ensures that the data adheres to specific rules or constraints, such as data types, value ranges, or relationships between columns. This is crucial for maintaining data integrity and avoiding inconsistencies that could lead to erroneous conclusions.</p> 
  </div> 
  <div class="readable-text intended-text" id="p180"> 
   <p>If this seems like a lot of work, that’s because it is. The good news is that this is yet another opportunity for you to get intimately familiar with analyzed data, and that’s something you can’t fake.</p> 
  </div> 
  <div class="readable-text intended-text" id="p181"> 
   <p>These are the four steps for validating your data:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p182"> Define expected values for your data. </li> 
   <li class="readable-text" id="p183"> Check if any bits of data fall outside your expectations. </li> 
   <li class="readable-text" id="p184"> If any do, do something about it. </li> 
   <li class="readable-text" id="p185"> Check the consistency of the data coming from multiple tables. </li> 
  </ol> 
  <div class="readable-text" id="p186"> 
   <p>The specifics of what values are expected will vary, depending on your dataset. There are no universal rules for determining expected versus unexpected values; that’s where your domain knowledge kicks in. You must remember, however, that data points deviating from expectations might signal either errors in data collection (worth investigating) or events of significant importance to your business (definitely worth investigating). To give you a starting point, we’ll outline three basic areas you should consider when validating your data.</p> 
  </div> 
  <div class="readable-text" id="p187"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Upfront validation rules and constraints definition </h4> 
  </div> 
  <div class="readable-text" id="p188"> 
   <p>We mentioned that the whole analytical protocol should be defined before running an analysis, to avoid having to “tweak” the analysis to fit expectations better. </p> 
  </div> 
  <div class="readable-text intended-text" id="p189"> 
   <p>Validation rules and constraints, indeed, <em>must</em> be defined upfront. If you scrolled through the files, as we advised you to do at the beginning of section 2.3, you should have a more or less solid intuition about the structure and distribution of variables in your data. You should also study your domain and related data enough to know what values are typical and expected. Suppose you don’t know the exact acceptable border values for your variables. In that case, you should be able to state your expectations in terms of deviations from the mean, percentiles, cicada chirps, horse’s lengths, or whatever applies to your data.</p> 
  </div> 
  <div class="readable-text intended-text" id="p190"> 
   <p>Now, let’s get off our high horse for a minute. Data quality assurance <em>is</em> an iterative process. We will perform an EDA, and we may learn that some of the assumptions we made after our initial analysis were not sufficiently grounded and that our intuition led us astray. In that case, we’ll go back and correct our validation rules to better discriminate between valuable data and potential artifacts.</p> 
  </div> 
  <div class="readable-text" id="p191"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Data types</h4> 
  </div> 
  <div class="readable-text" id="p192"> 
   <p>Assigning correct data types to your variables will make your life much easier down the line, as many libraries make operations on columns with well defined types (especially numerical ones) easy and efficient. In general, if you want to perform an operation like <code>111-11</code>, you’d like to get <code>100</code>, not <code>1</code> or a system error (where, as we will repeat throughout this book, the latter would be preferable).</p> 
  </div> 
  <div class="readable-text intended-text" id="p193"> 
   <p>When looking at the properties of our example DataFrames revealed by <code>df_name.dtype</code>, or by calling the <code>df_name.info()</code> method, you should notice that pandas successfully derived the types of numerical columns. However, it assigned general <em>object</em> types to columns containing dates. </p> 
  </div> 
  <div class="readable-text intended-text" id="p194"> 
   <p>Time representation in programming is far from straightforward. There are myriad methods and formats for noting something as seemingly simple as a timestamp, reflecting the complexity of time zones, calendar systems, and coding standards. In Python, there is no singular “date” type due to these complexities. Instead, for our datasets, we have found that the most suitable format is <code>datetime64ns</code>, which offers a precise timestamp down to the nanosecond:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p195"> 
   <div class="code-area-container"> 
    <pre class="code-area">df_name['column_name'] = df_name.column_name.astype('datetime64[ns]')</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p196"> 
   <p>Analyzing methods for reading different time formats from text may be beyond the scope of this book, but it’s surely not beyond the scope of knowledge of your GenAI advisor! When in doubt, provide an example of your data, and ask the generative AI how to transform it into the format you’ll be using in your pipeline.</p> 
  </div> 
  <div class="readable-text" id="p197"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Relationships between columns</h4> 
  </div> 
  <div class="readable-text" id="p198"> 
   <p>We mentioned that we’d seen sales volumes added to physical volumes. How do you think that happened? This point is critical if you work with multiple DataFrames from different data sources. Blind trust that identical column names mean that the two columns describe the same thing may very easily deprive your analysis of any functional value. </p> 
  </div> 
  <div class="readable-text intended-text" id="p199"> 
   <p>Before you start coding (or copying) a cross-reference function, you should have a good understanding of which of your DataFrames will serve as the reference—the source of so-called <em>primary keys</em>—and which DataFrames will have <em>foreign keys</em>—meaning the values reference existing primary keys. This way, you’ll create a lineage of DataFrames.</p> 
  </div> 
  <div class="readable-text intended-text" id="p200"> 
   <p>If you have a DataFrame containing the data on all your customers, their identifiers become primary keys. Suppose you have another DataFrame describing orders from some range of time, and it has information on which customer made an order; all these customers’ identifiers will exist in the DataFrame of customers. However, there is a possibility that some of the customers in the reference dataset didn’t order anything in the timeframe in question, so they don’t need to all be in the orders DataFrame.</p> 
  </div> 
  <div class="readable-text intended-text" id="p201"> 
   <p>You can use the following code to identify problems with keys within <code>df_orders</code> and <code>df_order_payments</code>.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p202"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 2.2</span> Identifying foreign key violations</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">foreign_key_violations = 
<span class="">↪</span>df_orders[~df_orders['order_id'].isin(df_order_payments['order_id'])]
if not foreign_key_violations.empty:
    print("Foreign key violations found:")
    print(foreign_key_violations)
<em>    # Handle violations (e.g., drop rows or correct the foreign key values)</em></pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p203"> 
   <p>In the course of our work with this example dataset, we’ve already discovered that there is a problem with <code>customer_id</code>, so the relation between <code>df_orders</code> and <code>df_ customers</code> here is unexpectedly 1:1, but when checking <code>df_orders</code> against <code>df_order_payments</code>, we can see the expected behavior of <code>order_id</code> being a primary key in <code>df_orders</code> and a foreign key in <code>df_order_payments</code>. There is one violation, but we’ll leave dealing with it as an exercise for you.</p> 
  </div> 
  <div class="readable-text intended-text" id="p204"> 
   <p>Yet again, there is an issue of causality, which you should identify as an analyst. By that, we mean identifying the actual cause of an unexpected data structure. Any explanation you can come up with should be confirmed with the data creators. Of course, sometimes, your ability to confirm things may be limited, and you may need to rely on cross-referencing all the DataFrames and checking if there is a logical pattern in identifier columns. Still, in cases where you work without 100% certainty on the data’s meaning, you should clearly communicate to your stakeholders the risk of analysis being performed on an unconfirmed dataset.</p> 
  </div> 
  <div class="readable-text intended-text" id="p205"> 
   <p>We now should have our data adequately cleaned. By performing a thorough data cleaning, you increase the likelihood of obtaining reliable and accurate insights from your data analysis. A clean dataset serves as a strong foundation for the subsequent stages of data preprocessing, EDA, and modeling.</p> 
  </div> 
  <div class="readable-text intended-text" id="p206"> 
   <p>The dataset we use in this book is provided in a modeling-ready state, and as such does not require additional cleaning or preprocessing, so we will move directly to EDA. In your work, you shouldn’t be so hasty!</p> 
  </div> 
  <div class="readable-text" id="p207"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_30"><span class="num-string">2.6</span> Exploratory data analysis</h2> 
  </div> 
  <div class="readable-text" id="p208"> 
   <p>Now that the data has been sliced and diced, it’s time for some seasoning, mainly of you as a data analyst. In most cases, your stakeholders will require simple analyses, and they’ll have strong opinions about the visualizations they want. You’ll be surprised how often they refuse bleeding-edge interactive, flashy, blinking, and sparkling tools and demand good old static charts. And there is nothing wrong with that. Your role is to make sure their decisions are informed. If their preferred way of acquiring the required information is through one type of chart or another, you should respect that in most cases. You’ll need to stomp down your foot, however, when you realize that some other data presentation introduces new and required information. Pick your battles. </p> 
  </div> 
  <div class="readable-text intended-text" id="p209"> 
   <p>Why do we talk about visualization types here? Because you will get many useful charts as a side-effect of your EDA. Doing them right at this stage (which means they’re useful to your stakeholders) can save you time and effort later. In this section you will find some examples of such useful charts.</p> 
  </div> 
  <div class="readable-text intended-text" id="p210"> 
   <p>In our opinion, EDA is the process that distinguishes great analysts. It’s a form of art, linking the meaning of data with its structure. In EDA, you will combine statistics, domain knowledge, and visualizations. (You may use your new visualization toys, but if you want your stakeholders to adopt them, make sure you sometimes show them what you use—luring them generally works better than forcing things down their throats.)</p> 
  </div> 
  <div class="readable-text intended-text" id="p211"> 
   <p>This mix of required knowledge may seem intimidating, but you now have access to a fantastic source of multidisciplinary know-how. Let’s ask generative AI about the DataFrame containing order reviews, aptly named <code>df_order_reviews</code>, as it has an interesting combination of numbers, text, and dates. </p> 
  </div> 
  <div class="readable-text prompt" id="p212"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Please suggest EDA techniques for the following DataFrame:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p213"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">DataFrame name: df_order_reviews

Columns:
- review_id (object): 
<span class="">↪</span>Unique identifier of each review.
- order_id (object): 
<span class="">↪</span>Identifier of the order being reviewed.
- review_score (int64): 
<span class="">↪</span>Note ranging from 1 to 5 given by the customer on a satisfaction survey.
- review_comment_title (object): 
<span class="">↪</span>Comment title from the review left by the customer in Portuguese.
- review_comment_message (object): 
<span class="">↪</span>Comment message from the review left by the customer in Portuguese.
- review_creation_date (datetime64[ns]): 
<span class="">↪</span>The timestamp of the review creation.
- review_answer_timestamp  (datetime64[ns]): 
<span class="">↪</span>I am not sure what this one actually is.</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p214"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Based on the provided DataFrame structure and content, here are some EDA techniques to better understand and analyze your <code>df_order_reviews</code> DataFrame:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p215"> Basic data overview: Use <code>.head()</code>, <code>.tail()</code>, <code>.info()</code>, and <code>.describe()</code> to get an initial understanding of the DataFrame content and structure. </li> 
   <li class="readable-text" id="p216"> Missing values analysis: Check for missing values in each column using <code>.isna().sum()</code> and decide how to handle them (drop, fill, or impute). </li> 
   <li class="readable-text" id="p217"> Review scores distribution: Use <code>.value_counts()</code> or a histogram to visualize the distribution of review scores. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p218"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">df_order_reviews['review_score'].value_counts().sort_index()
<span class="">↪</span>.plot(kind='bar')</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p219"><span class="faux-ol-li-counter">4. </span> Reviews over time: Plot the number of reviews created over time using the <code>review_creation_date</code> column. This can help you identify trends and patterns in the review data. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p220"> 
   <div class="code-area-container"> 
    <pre class="code-area">df_order_reviews['review_creation_date'].dt.to_period('M').
value_counts().sort_index().plot(kind='bar')</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p221"><span class="faux-ol-li-counter">5. </span> Review response time: Calculate the response time for each review by subtracting the <code>review_creation_date</code> from the <code>review_answer_timestamp</code>. Analyze the distribution of response times and investigate if there is a relationship between response time and review scores. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p222"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">df_order_reviews['response_time'] = 
<span class="">↪</span>df_order_reviews['review_answer_timestamp'] -
<span class="">↪</span>df_order_reviews['review_creation_date']</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p223"><span class="faux-ol-li-counter">6. </span> Text analysis: Explore the text data in the <code>review_comment_title</code> and <code>review_comment_message</code> columns. You can perform sentiment analysis, topic modeling, or word frequency analysis to identify common themes, patterns, or issues mentioned in the reviews. </li> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p224"><span class="faux-ol-li-counter">7. </span> Investigate the unclear column: To understand the <code>review_answer_timestamp</code> column, you can analyze its relationship with other columns, like <code>review_creation_date</code> or <code>review_score</code>. Plotting scatter plots, calculating correlations, or performing other statistical tests can help you identify the purpose of this column. </li> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p225"><span class="faux-ol-li-counter">8. </span> Orders with multiple reviews: Check if there are any orders with multiple reviews. This could indicate data entry errors or provide insight into the review process. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p226"> 
   <div class="code-area-container"> 
    <pre class="code-area">order_review_counts = df_order_reviews['order_id'].value_counts()
multiple_review_orders = order_review_counts[order_review_counts &gt; 1]</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p227"> 
   <p>These EDA techniques can help you gain a better understanding of the <code>df_order_reviews</code> DataFrame and identify patterns, trends, and potential issues in the data. Depending on your specific objectives and domain knowledge, you may need to adapt or extend these techniques to suit your needs.</p> 
  </div> 
  <div class="readable-text" id="p228"> 
   <p>The preceding answer comes from ChatGPT-v4, as it was the most comprehensive. It developed one solid EDA plan, touching upon multiple topics at once. It shows how different levels of analysis overlap with each other. You shouldn’t be surprised to see some of the steps we performed in the data-cleaning phase included here. Other steps, in our opinion, are elements of the data analysis itself. As we mentioned, there is no gold standard for EDA, and generative AI often provides the broadest possible answer. </p> 
  </div> 
  <div class="readable-text" id="p229"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_31"><span class="num-string">2.6.1</span> Reviewing score distribution</h3> 
  </div> 
  <div class="readable-text" id="p230"> 
   <p>Let’s get back to our coin-flip gambling problem for a second. If you recall, at the beginning of this chapter, we said, “. . . if you throw the coin multiple times and get your expected tails each time, the coin may not be random after all.” Before betting on a coin, you most definitely should check what the distribution of results is. If it’s really random, you most likely should not bet on it at all, but if it’s not random, you can use it to your advantage. You can think about reviewing the score distribution as you would about testing multidimensional coins. </p> 
  </div> 
  <div class="readable-text intended-text" id="p231"> 
   <p>The importance of score distribution cannot be overemphasized, as distribution analysis can reveal potential issues that may impact the study’s validity, such as data entry errors or biases. Visualizing the distribution through histograms, box plots, or density plots can also help to determine whether the data follows a specific pattern or conforms to a known statistical distribution. This knowledge can help you select appropriate statistical tests, predictive models, or data transformation techniques to ensure accurate and reliable results. </p> 
  </div> 
  <div class="readable-text intended-text" id="p232"> 
   <p>Reviewing the distribution typically involves comparing the observed data distribution with a theoretical distribution (for example, normal, exponential, or binomial) to assess how well the data conforms to expected patterns. You have multiple distribution testing methods at your disposal, including visual inspection with histograms or Q-Q plots, or more quantitative measures like the Kolmogorov-Smirnov test, the Shapiro-Wilk test for normality, or the Anderson-Darling test. Each method has its strengths and particularities, making it more or less suitable depending on the size of the dataset and the assumptions you’re willing to make.</p> 
  </div> 
  <div class="readable-text intended-text" id="p233"> 
   <p>If this task seems complex, that’s because it used to be. Now it’s as simple as typing:</p> 
  </div> 
  <div class="readable-text prompt" id="p234"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>How can I test what distribution does my dataset fit best? </p> 
  </div> 
  <div class="readable-text" id="p235"> 
   <p>If you’re looking for a more specific answer, you can add details about your analytical environment, data source, or an intuition you want to examine (note that we didn’t use the word “confirm”). From our experience, ChatGPT-4 beats all the other tools in its statistical prowess.</p> 
  </div> 
  <div class="readable-text intended-text" id="p236"> 
   <p>Let’s look at some popular distributions and where you may expect them. The shapes of commonly encountered distributions are presented in figure 2.2.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p237">  
   <img alt="figure" src="../Images/CH02_F02_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.2</span> Commonly encountered data distributions</h5>
  </div> 
  <div class="readable-text" id="p238"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Normal distribution</h4> 
  </div> 
  <div class="readable-text" id="p239"> 
   <p>The normal distribution is also known as Gaussian distribution or “bell curve.” It’s often the first assumption for continuous data, due to the central limit theorem, which posits that the mean of a sufficiently large number of independent random variables, each with finite mean and variance, will be approximately normally distributed, regardless of the underlying distribution. Be careful, though. Some statistical tests rely heavily on the assumption of the normality of the data distribution. If your data is skewed, your test results may get screwed as a result.</p> 
  </div> 
  <div class="readable-text intended-text" id="p240"> 
   <p>The normal distribution is particularly relevant for metrics like sales figures, heights of individuals, and errors in measurements.</p> 
  </div> 
  <div class="readable-text" id="p241"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Binomial distribution</h4> 
  </div> 
  <div class="readable-text" id="p242"> 
   <p>The binomial distribution is used for binary outcomes in data, like coin flipping. This distribution is relevant when dealing with success/failure, yes/no, or 1/0 types of outcomes in data, such as conversion rates in A/B testing, pass/fail rates in quality control, or click-through rates in online advertising.</p> 
  </div> 
  <div class="readable-text intended-text" id="p243"> 
   <p>Seeing more than two bars in a binomial distribution chart often leads to confusion, so let’s try to dispel that right away. The binomial distribution describes the number of successes in a fixed number of independent Bernoulli trials, each with the same probability of success. When you see a binomial distribution with more than two bins, it does not represent just a single trial (which would result in only two outcomes: success or failure, 1 or 0). Instead, it represents the outcomes of multiple trials, so it can have more than two bins.</p> 
  </div> 
  <div class="readable-text intended-text" id="p244"> 
   <p>For example, suppose you analyze the binomial distribution of <em>n</em>=10 different trials. In that case, the distribution doesn’t show just two outcomes but the probability of achieving 0, 1, 2, ..., up to 10 successes out of those 10 trials. Each “bar” in this context represents the number of successes: one bar for 0 successes, another for 1 success, and so on, up to 10 successes. So while each individual trial has a binary outcome, the</p> 
  </div> 
  <div class="readable-text" id="p245"> 
   <p>aggregate result across multiple trials can range from 0 successes (all failures) to <em>n</em> successes (all successes), leading to <em>n</em>+1 possible outcomes. Hence the number of bars in the distribution’s representation.</p> 
  </div> 
  <div class="readable-text" id="p246"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Uniform distribution</h4> 
  </div> 
  <div class="readable-text" id="p247"> 
   <p>A uniform distribution assumes that all outcomes are equally likely. It’s frequently a starting assumption for simulations or when modeling unbiased random variables, such as random digits, the simulation of equal-probability scenarios, or when no prior information is available. </p> 
  </div> 
  <div class="readable-text intended-text" id="p248"> 
   <p>While this distribution is less common in raw business data, it’s worth remembering.</p> 
  </div> 
  <div class="readable-text" id="p249"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Poisson distribution</h4> 
  </div> 
  <div class="readable-text" id="p250"> 
   <p>The Poisson distribution applies to count data where occurrences happen independently over a constant rate in space or time. It’s commonly used for modeling the number of times an event occurs in a fixed interval of time or space.</p> 
  </div> 
  <div class="readable-text intended-text" id="p251"> 
   <p>You may encounter this distribution when analyzing the number of customer arrivals per hour, daily demand for a product, or the number of failures of a system.</p> 
  </div> 
  <div class="readable-text" id="p252"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Exponential distribution</h4> 
  </div> 
  <div class="readable-text" id="p253"> 
   <p>An exponential distribution is often used to model the time between events in a Poisson process, representing the time until an event occurs. </p> 
  </div> 
  <div class="readable-text intended-text" id="p254"> 
   <p>Look for this distribution when working with the lifespan of machinery before failure, the time until a customer’s next purchase, or the inter-arrival times in a queuing system.</p> 
  </div> 
  <div class="readable-text" id="p255"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Log-normal distribution</h4> 
  </div> 
  <div class="readable-text" id="p256"> 
   <p>The log-normal distribution comes into play when dealing with variables that are the product of many independent random variables. It’s used when data cannot be negative and the distribution is skewed right.</p> 
  </div> 
  <div class="readable-text intended-text" id="p257"> 
   <p>You’ll encounter this distribution in datasets concerning stock or real estate property prices or income distributions. </p> 
  </div> 
  <div class="readable-text" id="p258"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Beta distribution</h4> 
  </div> 
  <div class="readable-text" id="p259"> 
   <p>The beta distribution is useful in modeling variables that are bounded on both sides, such as proportions and percentages. It’s flexible and can assume a variety of shapes, making it suitable for modeling the behavior of random variables limited to intervals. </p> 
  </div> 
  <div class="readable-text intended-text" id="p260"> 
   <p>This distribution appears when the data represents conversion rates with values between 0 and 1.</p> 
  </div> 
  <div class="readable-text" id="p261"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Gamma distribution</h4> 
  </div> 
  <div class="readable-text" id="p262"> 
   <p>The gamma distribution is related to the exponential distribution and is useful for modeling the wait time for the <em>n</em>-th event in a Poisson process.</p> 
  </div> 
  <div class="readable-text intended-text" id="p263"> 
   <p>You’ll need the gamma distribution when modeling things such as the amount of rainfall accumulated in a reservoir, service times in systems, or the reliability of systems over a continuous scale.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p264"> 
   <p><span class="print-book-callout-head">NOTE</span>  Can multimodal generative AI be used to identify distribution type from a chart? At the time of writing this book, no. We tried to feed parts of figure 2.2 to different generative AIs, and their answers were often as diverse as misleading. However, generative AIs can still guide you through proper testing procedures. </p> 
  </div> 
  <div class="readable-text" id="p265"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_32"><span class="num-string">2.6.2</span> Time series exploration</h3> 
  </div> 
  <div class="readable-text" id="p266"> 
   <p>Now we enter a grey zone. You may encounter discussions on analyzing changes over time as a part of EDA or as a part of in-depth analysis itself. On the one hand, visualizing all the time series is a part of EDA. Sometimes this will allow you to identify outliers or errors requiring cleanup. On the other hand, these outliers could encode significant issues or events in business processes. </p> 
  </div> 
  <div class="readable-text intended-text" id="p267"> 
   <p>We conclude that distinguishing between EDA and in-depth business analysis is a false dichotomy. EDA <em>is</em> an integral part of in-depth analysis. It’s not something you want someone else to do for you. Nor is EDA a once-off event—you will be coming back to it now and then. </p> 
  </div> 
  <div class="readable-text intended-text" id="p268"> 
   <p>From the process management perspective, it’s clearer when steps related to evaluating the data’s technical and structural properties are separate from those related to discovering business-related insights. However, the two are too closely related to be wholly compartmentalized. </p> 
  </div> 
  <div class="readable-text intended-text" id="p269"> 
   <p>There are two tests that we propose to run at the first EDA iteration. First, we’ll visualize and look at the data to search for outliers; second, we’ll perform cyclicity detection. </p> 
  </div> 
  <div class="readable-text" id="p270"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Changes in the number of responses: Quick data visualization</h4> 
  </div> 
  <div class="readable-text" id="p271"> 
   <p>We’ve seen situations where the success attributed to a project was, in fact, caused by the change in the overall market or the performance of another business function. Taking a good look at how different variables change over time can be very informative indeed!</p> 
  </div> 
  <div class="readable-text intended-text" id="p272"> 
   <p>This is a very basic yet important task, which should be performed on most of your date-related variables. Most business activities have a continuous character, and analyzing their performance over time is critical for understanding those activities. </p> 
  </div> 
  <div class="readable-text intended-text" id="p273"> 
   <p>Figure 2.3 presents the number of reviews recorded in our dataset per month. We can clearly see three distinct phases. The first one lasted from October 2016 to January 2017. The almost non-existent number of reviews suggests that either the business operations or review collecting system were just getting traction. Then, from February 2017 to November 2017 (or May 2018, really), we can see a phase of rapid growth in the number of reviews collected monthly. Then, from December 2017 the system stabilized and started fluctuating, or from June 2018 it had a moment of collapse and rebounded in August 2018.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p274">  
   <img alt="figure" src="../Images/CH02_F03_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.3</span> The number of reviews over time—monthly aggregation<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p275"> 
   <p>This information could be crucial if we were to quantitatively compare metrics involving the number of reviews between different points in time. Now that we’ve checked the number of reviews, let’s move on to checking the scores associated with reviews.</p> 
  </div> 
  <div class="readable-text" id="p276"> 
   <h4 class="readable-text-h4 sigil_not_in_toc">Changes in review scores over time</h4> 
  </div> 
  <div class="readable-text" id="p277"> 
   <p>Some data is expected to be cyclic, and other data, not so much. Discovering and describing cycles in data can help you identify whether the dataset you’ve provided is relevant to explaining the related business processes.</p> 
  </div> 
  <div class="readable-text intended-text" id="p278"> 
   <p>The following recording of our conversation with the AI advisor proves our point from chapter 1, related to the limitations of embedding generative AI directly into your analytics pipeline in a shoot-and-forget manner.</p> 
  </div> 
  <div class="readable-text prompt" id="p279"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>How can I detect internal cyclicity in data? </p> 
  </div> 
  <div class="readable-text response" id="p280"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Detecting internal cyclicity in data involves identifying recurring patterns or periodic fluctuations over time. Time series data is a common example where cyclicity might be present. Here are some techniques to detect cyclicity in your data:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p281"> Visual inspection: Plot the data over time to visually inspect if there’s any noticeable cyclical pattern. This can be a quick and simple way to identify obvious patterns. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p282"> 
   <div class="code-area-container"> 
    <pre class="code-area">import matplotlib.pyplot as plt

plt.plot(df['timestamp'], df['value'])
plt.xlabel('Timestamp')
plt.ylabel('Value')
plt.title('Time Series Data')
plt.show()</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p283"><span class="faux-ol-li-counter">2. </span> Seasonal decomposition: Decompose the time series data into its trend, seasonal, and residual components using methods like classical decomposition or Seasonal and Trend decomposition using Loess (STL). This can help you observe the seasonal component directly. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p284"> 
   <div class="code-area-container"> 
    <pre class="code-area">&lt;Dysfunctional code was here&gt;</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p285"><span class="faux-ol-li-counter">3. </span> Autocorrelation and partial autocorrelation: Plot the autocorrelation function (ACF) and partial autocorrelation function (PACF) to identify any significant lags that indicate cyclical patterns. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p286"> 
   <div class="code-area-container"> 
    <pre class="code-area">&lt;Dysfunctional code was here&gt;</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p287"><span class="faux-ol-li-counter">4. </span> Spectral analysis: Perform a spectral analysis using the Fast Fourier Transform (FFT) or Lomb-Scargle Periodogram to identify dominant frequencies in the data, which can correspond to cyclic patterns. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p288"> 
   <div class="code-area-container"> 
    <pre class="code-area">&lt;Dysfunctional code was here&gt;</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p289"> 
   <p>These techniques can help you detect internal cyclicity in your data. Depending on the characteristics of your data and the nature of the cyclicity, some methods may be more appropriate than others. It’s essential to understand the underlying assumptions and limitations of each technique and to choose the one that best fits your specific data and objectives.</p> 
  </div> 
  <div class="readable-text" id="p290"> 
   <p>The code provided by generative AI sometimes differs from expectations. Three of the four pieces of code we received are dysfunctional (and are therefore censored), and the other . . . Well, look at figure 2.4 to see the resulting visualization of the relationship between the score and the time of the review. </p> 
  </div> 
  <div class="readable-text intended-text" id="p291"> 
   <p>We believe that some people gifted with an affinity for postmodern art could find an application for the resulting chart. In business analytics, however, you aim to get something a tad more readable. </p> 
  </div> 
  <div class="readable-text intended-text" id="p292"> 
   <p>Let’s get back to the dysfunctional code, as there is a lesson to be learned here. The FFT and Lomb-Scargle methods didn’t work due to problems with data types. So far, we’ve advocated that you present your generative AI advisor with general queries to confirm your expertise or expand your horizons. However, sometimes you’ll encounter problems resulting from false assumptions that generative AI may make related to the data types available for analysis. That doesn’t mean the answer is useless. It means you’ll have to change gears.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p293">  
   <img alt="figure" src="../Images/CH02_F04_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.4</span> The relationship between the score and the time of the review<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text print-book-callout" id="p294"> 
   <p><span class="print-book-callout-head">Variables won’t, constants aren’t</span>  Generative AIs are language models. They operate on a general set of knowledge of mind-boggling scope. Expecting them to correctly assume the alignment of data type requirements for some advanced methods with our specific data may be overly optimistic.</p> 
  </div> 
  <div class="readable-text" id="p295"> 
   <p>As the general inquiry failed, let’s try to be more specific. </p> 
  </div> 
  <div class="readable-text prompt" id="p296"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Values in my data are categorical, with possible values of 1, 2, 3, 4, 5. They are review scores given to orders by customers. Can you propose a method to detect if there are any cyclical dependencies of the score in time? </p> 
  </div> 
  <div class="readable-text response" id="p297"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>Since your data consists of categorical values (review scores), you can analyze cyclical dependencies by examining the distribution of scores over different time periods. One approach is to use aggregated bar plots to visualize the score distribution over time.</p> 
  </div> 
  <div class="readable-text response" id="p298"> 
   <p>Here’s a step-by-step guide to analyze cyclical dependencies:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p299"> Convert the timestamp to a more suitable frequency (e.g., daily, weekly, or monthly), depending on the length and resolution of your data. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p300"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><em># Create a new column with the timestamp at the desired frequency </em>
<span class="">↪</span><em>(e.g., daily)</em>
df_order_reviews['date_daily'] = 
<span class="">↪</span>df_order_reviews['review_creation_date'].dt.date</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p301"><span class="faux-ol-li-counter">2. </span> Aggregate the scores and calculate the proportions for each score in each time period. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p302"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><em># Group by date_daily and calculate the proportion of each score</em>
score_counts = df_order_reviews.groupby(['date_daily', 
<span class="">↪</span>'review_score']).size().unstack(fill_value=0)
score_proportions = score_counts.div(score_counts.sum(axis=1), axis=0)</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p303"><span class="faux-ol-li-counter">3. </span> Create a stacked bar plot to visualize the proportions of scores over time. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p304"> 
   <div class="code-area-container"> 
    <pre class="code-area">score_proportions.plot(kind='bar', stacked=True, figsize=(15, 6))
plt.xlabel('Date')
plt.ylabel('Proportion of Scores')
plt.title('Proportion of Review Scores Over Time')
plt.legend(title='Review Score', bbox_to_anchor=(1, 1))
plt.show()</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p305"> 
   <p>Examine the stacked bar plot for any apparent cyclical patterns. If you observe any regular fluctuations in the proportions of different scores, there might be a cyclical dependency in the data.</p> 
  </div> 
  <div class="readable-text response" id="p306"> 
   <p>Please note that the choice of aggregation frequency (daily, weekly, or monthly) will impact the granularity of the analysis. You might need to experiment with different frequencies to find the most suitable one for your data.</p> 
  </div> 
  <div class="readable-text" id="p307"> 
   <p>As we used web-based generative AI, it even remembered the name of our DataFrame in the conversation context, so no code adjustment in this regard is needed. It’s equally as awesome as it may be dangerous. All of its assumptions should be taken with a pinch of salt and noted down, as they often may turn out to be a reason why some of the steps don’t work as expected.</p> 
  </div> 
  <div class="readable-text intended-text" id="p308"> 
   <p>Figure 2.5 presents the result of the proposed code. As you can see, the initially proposed frequency does not work well with our data. However, there is no obvious way of changing it. Does that mean we need to change the tools—revert to Excel and pivot tables or find something with a nice, big “change frequency” button? Or should we spend another week searching the matplotlib documentation?<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p309">  
   <img alt="figure" src="../Images/CH02_F05_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.5</span> Order scores distribution over time—daily aggregation</h5>
  </div> 
  <div class="readable-text" id="p310"> 
   <p>Maybe not. This is the brilliance of generative AI in action. You encounter a problem, you mention it to your AI advisor, and . . . </p> 
  </div> 
  <div class="readable-text prompt" id="p311"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>How would I change the frequency to monthly? </p> 
  </div> 
  <div class="readable-text response" id="p312"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To change the frequency to monthly, you’ll need to resample your data accordingly. Here’s how you can modify the code to aggregate the data on a monthly basis:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p313"> Convert the timestamp to a more suitable frequency (e.g., monthly). </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p314"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><em># Create a new column with the timestamp at the desired frequency </em>
<span class="">↪</span><em>(e.g., monthly)</em>
df_order_reviews['month'] = 
<span class="">↪</span>df_order_reviews['review_creation_date'].dt.to_period('M')</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p315"><span class="faux-ol-li-counter">2. </span> Aggregate the scores and calculate the proportions for each score in each time period. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p316"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area"><em># Group by month and calculate the proportion of each score</em>
score_counts = df_order_reviews.groupby(['month', 
<span class="">↪</span>'review_score']).size().unstack(fill_value=0)
score_proportions = score_counts.div(score_counts.sum(axis=1), axis=0)</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p317"><span class="faux-ol-li-counter">3. </span> Create a stacked bar plot to visualize the proportions of scores over time. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p318"> 
   <div class="code-area-container"> 
    <pre class="code-area">score_proportions.plot(kind='bar', stacked=True, figsize=(15, 6))
plt.xlabel('Month')
plt.ylabel('Proportion of Scores')
plt.title('Proportion of Review Scores Over Time (Monthly)')
plt.legend(title='Review Score', bbox_to_anchor=(1, 1))
plt.show()</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p319"> 
   <p>Now, the analysis will be performed at a monthly level, which can help reveal long-term cyclical patterns in the data. Feel free to adjust the frequency according to your requirements and the granularity of your data.</p> 
  </div> 
  <div class="readable-text" id="p320"> 
   <p>. . . there is a high chance that generative AI is capable of solving it. Figure 2.6 presents the distribution of review scores over time, aggregated by month.</p> 
  </div> 
  <div class="readable-text intended-text" id="p321"> 
   <p>We can see that there was a spike in low scores in November and December 2016, and maybe, just maybe, there is a cyclic rise in low scores in the spring, but the signal is so weak that we won’t dig much deeper into it unless we want to run some precise predictive analysis. </p> 
  </div> 
  <div class="readable-text intended-text" id="p322"> 
   <p>We can safely conclude from this section that generative AIs may make mistakes, but they can still help you write and modify code, such as for time series analysis, and they are not shy of using really advanced libraries and algorithms (Lomb-Scargle is an algorithm developed by biologists to analyze biological cycles, later adopted by astronomers to search for patterns in star brightness observations). We believe that with a little patience and an iterative approach to clarifying the method requirements and your data properties, generative AI will become the greatest facilitator of your analytical processes.</p> 
  </div> 
  <div class="browsable-container figure-container" id="p323">  
   <img alt="figure" src="../Images/CH02_F06_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.6</span> <em/>Order scores distribution over time—monthly aggregation<span class="aframe-location"/></h5>
  </div> 
  <div class="readable-text" id="p324"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_33"><span class="num-string">2.6.3</span> Mysterious variable investigation</h3> 
  </div> 
  <div class="readable-text" id="p325"> 
   <p>If mysterious variables don’t already plague your days as a data analyst, you can expect they will. We mentioned, and will again, that nothing beats solid metadata, but things like thorough documentation data pipelines are considered a luxury in these times of ever-increasing time pressure. Therefore, some of your data may come from a poorly documented system. As much as we discourage using unconfirmed data for any reason other than warming your server room, sometimes it is possible to deduce the meaning of some variables to a satisfying level of certainty.</p> 
  </div> 
  <div class="readable-text intended-text" id="p326"> 
   <p>To show how you can deal with an example of a mysterious variable, let’s feign ignorance as to the meaning of one of the columns in our example data. Let’s try to get some insight into the <code>review_answer_timestamp</code> column. With our Sherlock cap on, let’s see if there is any other column could be connected to it. Using intuition alone, we’ll check the relation with <code>review_creation_date</code>. Both columns relate to reviews and, in some form, to a time. </p> 
  </div> 
  <div class="readable-text intended-text" id="p327"> 
   <p>But what does it mean for two columns to be related? A knee-jerk reaction could be to check for a correlation, but this could be a risky proposition in the case of time series. Correlation measures the linear relationship between two variables, assuming that this relationship is consistent across the dataset. However, this assumption often doesn’t hold in time series, data due to factors like autocorrelation, trends, seasonality, and external shocks. <em>Autocorrelation</em> refers to the correlation of a signal with a delayed version of itself, a common characteristic of time series data that can inflate correlation measurements between two time series. Similarly, <em>trends</em> and <em>seasonality</em> can create spurious correlations that do not reflect a true causal relationship but rather the presence of underlying patterns that both series share. Moreover, <em>external shocks</em> impacting both series can temporarily increase correlation without indicating a meaningful or lasting relationship.</p> 
  </div> 
  <div class="readable-text intended-text" id="p328"> 
   <p>In our case, the critical factor is that the values in both columns relate to orders, which themselves were placed as series in time, so they <em>will </em>be correlated. Whatever these columns mean, their values will grow in time with orders. How, then, shall we define the relationship? </p> 
  </div> 
  <div class="readable-text intended-text" id="p329"> 
   <p>Maybe we won’t? We have tested generative AI for ideas and found its ability to understand the problem and propose a solution worthwhile. </p> 
  </div> 
  <div class="readable-text prompt" id="p330"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>Can you propose how I can investigate the relationship between the <code>review_answer_ timestamp</code> and the <code>review_creation_date</code> columns? </p> 
  </div> 
  <div class="readable-text response" id="p331"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To investigate the relationship between <code>review_answer_timestamp</code> and <code>review_ creation_date</code>, you can calculate the time difference between these two timestamps and analyze the distribution of the time differences. This will give you insights into how long it typically takes for a review to be answered.</p> 
  </div> 
  <div class="readable-text response" id="p332"> 
   <p>Here’s a step-by-step guide to analyze the relationship between these two columns:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p333"> Calculate the time difference between <code>review_answer_timestamp</code> and <code>review_ creation_date</code>. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p334"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">df_order_reviews['time_diff'] = 
<span class="">↪</span>(df_order_reviews['review_answer_timestamp'] – 
<span class="">↪</span>df_order_reviews['review_creation_date']).dt.total_seconds()</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p335"> 
   <p>The <code>time_diff</code> column now contains the time difference in seconds.</p> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p336"><span class="faux-ol-li-counter">2. </span> Calculate basic statistics on the time difference to get an overview of the distribution. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p337"> 
   <div class="code-area-container"> 
    <pre class="code-area">time_diff_summary = df_order_reviews['time_diff'].describe()
print(time_diff_summary)</pre>  
   </div> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p338"><span class="faux-ol-li-counter">3. </span> Visualize the distribution of time differences using a histogram or a box plot. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p339"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">import matplotlib.pyplot as plt
<em># Histogram</em>
df_order_reviews['time_diff'].plot(kind='hist', bins=50)
plt.xlabel('Time Difference (Seconds)')
plt.ylabel('Frequency')
plt.title('Distribution of Time Difference Between Review Creation and 
<span class="">↪</span>Answer')
plt.show()

<em># Box plot</em>
df_order_reviews['time_diff'].plot(kind='box', vert=False)
plt.xlabel('Time Difference (Seconds)')
plt.title('Box Plot of Time Difference Between Review Creation and
<span class="">↪</span>Answer')
plt.show()</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p340"> 
   <p>By analyzing the distribution of time differences between <code>review_answer_timestamp</code> and <code>review_creation_date</code>, you can understand the typical response time and identify any outliers or trends in the data.</p> 
  </div> 
  <div class="readable-text response" id="p341"> 
   <p>Feel free to convert the time differences to a more suitable unit (e.g., hours or days) if needed. You can also further investigate the relationship by looking at how the time difference varies with other factors, such as review scores or specific time periods (e.g., days of the week or months of the year).</p> 
  </div> 
  <div class="readable-text" id="p342"> 
   <p>It makes perfect sense to check the distribution of time differences between dates in two investigated columns. Let’s apply the proposed code.</p> 
  </div> 
  <div class="readable-text intended-text" id="p343"> 
   <p>The time difference summary analysis proves that the default timeframe of seconds may not be the most useful for the analysis.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p344"> 
   <div class="code-area-container"> 
    <pre class="code-area">count    9.922400e+04
mean     2.720730e+05
std      8.545003e+05
min      7.709000e+03
25%      8.682075e+04
50%      1.447155e+05
75%      2.681480e+05
max      4.481561e+07
Name: time_diff, dtype: float64</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p345"> 
   <p>However, our AI advisor mentioned that seconds might not be the best units for this data analysis. Fortunately, getting it to other time units is as simple as dividing the result by a constant. Feeling lazy, we asked generative AI to amend the code. It even took care of proper axis titles. The resulting images are presented in figure 2.7. </p> 
  </div> 
  <div class="readable-text intended-text" id="p346"> 
   <p>Judging by the time distribution, where some answers are provided within a few hours and others after many months, it seems very much like some human interaction drives the difference. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p347">  
   <img alt="figure" src="../Images/CH02_F07_Siwiak3.png"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 2.7</span> Distribution of the time difference between the <code>review_answer_timestamp</code> and <code>review_creation_date</code> columns represented as a histogram (a) and a box plot (b)</h5>
  </div> 
  <div class="readable-text" id="p348"> 
   <p>Let’s try to apply logic here. We have a review creation and review answer. As the data comes from an online platform, the first intuition is that the user creates the review after purchase, and the seller answers to the reviewer after some time. </p> 
  </div> 
  <div class="readable-text intended-text" id="p349"> 
   <p>This example shows that the combined might of generative AI and human expertise can easily be beaten by vague column-naming conventions and a lack of metadata. Actually, the <code>review_creation_date</code> column contains the date of sending the survey to the customer, and the <code>review_answer_timestamp</code> indicates when the customer answered it. This is yet another reason to <em>always </em>confirm the naming convention data creators used.</p> 
  </div> 
  <div class="readable-text" id="p350"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_34"><span class="num-string">2.6.4</span> Harmonizing data</h3> 
  </div> 
  <div class="readable-text" id="p351"> 
   <p>Let’s turn our attention to the structural issue of the number of reviews per . . . and here is the trick: are you actually sure what those collected reviews are concerned with? Getting multiple reviews of the same product is one thing. It’s not unusual for multiple people buy the same thing and express their, often diverse, opinions. However, getting multiple reviews of the same order would require your attention, as you would need to decide how to evaluate order satisfaction in the case of more than one score. Would you use the last review or the average? </p> 
  </div> 
  <div class="readable-text intended-text" id="p352"> 
   <p>In our dataset, 547 orders have more than one review. Now we should check if they come from the customer who ordered it, or from multiple customers, which would be worrying. To answer the question, we need to harmonize our data and connect different sources to form a coherent whole.</p> 
  </div> 
  <div class="readable-text intended-text" id="p353"> 
   <p>Let’s assume we feel lazy (that’s a reasonably safe assumption) and that we want our generative AI to do the heavy lifting.</p> 
  </div> 
  <div class="readable-text prompt" id="p354"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>I have another DataFrame named <code>df_customers</code>. There are two columns <code>customer _id</code> and <code>customer_unique_id</code>. What shall I do to create in the previously discussed <code>df_order_reviews</code><strong> </strong>an additional column with <code>customer_unique_id</code><strong> </strong>correctly assigned to <code>customer_id</code>, which is already there?</p> 
  </div> 
  <div class="readable-text response" id="p355"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To add the <code>customer_unique_id</code> column to the <code>df_order_reviews</code> DataFrame based on the <code>customer_id</code> column, you can use the <code>merge()</code> function in pandas. Here’s how to do it:</p> 
  </div> 
  <div class="readable-text response" id="p356"> 
   <p>Merge the <code>df_order_reviews</code> and <code>df_customers</code> DataFrames on the <code>customer_id</code> column.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p357"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">merged_df = df_order_reviews.merge(df_customers, on='customer_id', 
<span class="">↪</span>how='left')</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p358"> 
   <p>This will perform a left join, meaning that all rows from <code>df_order_reviews</code> will be included in the <code>merged_df</code> DataFrame, and the corresponding <code>customer_unique_id</code> values from <code>df_customers</code> will be matched based on the <code>customer_id</code> column.</p> 
  </div> 
  <div class="readable-text response" id="p359"> 
   <p>Now, the <code>merged_df</code> DataFrame contains an additional <code>customer_unique_id</code> column, with the correct values assigned to each <code>customer_id</code>. If you want to update the original <code>df_order_reviews</code> DataFrame instead of creating a new one, you can simply assign <code>merged_df</code> to <code>df_order_reviews</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p360"> 
   <div class="code-area-container"> 
    <pre class="code-area">df_orders_reviews = merged_df</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p361"> 
   <p>Now, that’s a nice bit of code with no right to work! It’s trying to assign customers to orders in the <code>df_order_reviews</code> DataFrame. However, no column in <code>df_order_ reviews</code> allows us to do this directly. We know that the only actual identifier we have is the <code>order_id</code> column, as we did our homework and learned that <code>customer_id</code> does not, in fact, identify customers.</p> 
  </div> 
  <div class="readable-text print-book-callout" id="p362"> 
   <p><span class="print-book-callout-head">The customer is always right</span>  Generative AIs will neither question nor correct you. If you tell them to believe in something, they will assume you have your reasons. Although, there is a limitation to their suspension of disbelief. We tried to convince generative AIs that the Earth is flat, and even though they agreed to keep this assumption for the sake of the discussion, they reminded us every now and then that the Earth is, in fact, round. However, their tolerance borders are set pretty far.</p> 
  </div> 
  <div class="readable-text" id="p363"> 
   <p>We can get the desired result in a two-step process by first connecting <code>df_orders</code> and <code>df_customers</code> on the <code>customer_id</code> field. This will give us a DataFrame that contains the <code>customer_unique_id</code> for each order. We can then connect this DataFrame to <code>df_order_reviews</code> on the <code>order_id</code> field. Alternatively, we could start by connecting <code>df_orders</code> and <code>df_order_reviews</code> on the <code>order_id</code> field. Then we could connect the resulting DataFrame to <code>df_customers</code> on the <code>customer_id</code> field. Hopefully you can now see why the initial data analysis is critical. You need to see all the DataFrames to harmonize data from multiple sources effectively.</p> 
  </div> 
  <div class="readable-text intended-text" id="p364"> 
   <p>Joining, merging, and concatenating tables may be obvious to you, but, for others, this bit of data engineering 101 may still be something to worry about. In the past, just knowing <em>what </em>needs to happen was a required but not sufficient condition for getting the code to perform the desired operation. We’ve seen the computer science degree requirement in many data analysts’ job descriptions. Let’s take this opportunity to check if today’s gaps in coding experience can be closed using generative AI. </p> 
  </div> 
  <div class="readable-text prompt" id="p365"> 
   <p><strong class="prompt-head-image"><img alt="image" height="67px" src="../Images/Init-MA.png" width="66px"/></strong><strong><span class="aframe-location"/></strong>I have my <code>df_order_reviews</code> DataFrame, which has the <code>order_id</code> column. I also have the <code>df_orders</code> DataFrame, where I have the <code>order_id</code> and the <code>customer_id</code> columns. I also have the <code>df_customers</code> DataFrame, where I have the <code>customer_id</code> and the <code>customer_unique_id</code> columns. How do I assign a valid <code>customer_ unique_id</code> from <code>df_customers</code> to each <code>order_id</code> in <code>df_order_reviews</code>? </p> 
  </div> 
  <div class="readable-text response" id="p366"> 
   <p><strong class="response-head-image"><img alt="image" height="60px" src="../Images/Response.png" width="59px"/></strong><strong><span class="aframe-location"/></strong>To assign the valid <code>customer_unique_id</code> from <code>df_customers</code> to each <code>order_id</code> in <code>df_order_reviews</code>, you will first need to merge <code>df_orders</code> with <code>df_customers</code> to create a DataFrame with <code>order_id</code> and <code>customer_unique_id</code> columns. Then, merge this new DataFrame with <code>df_order_reviews</code>. Here’s how to do it step by step:</p> 
  </div> 
  <ol class="response"> 
   <li class="readable-text" id="p367"> Merge the <code>df_orders</code> and <code>df_customers</code> DataFrames on the <code>customer_id</code> column. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p368"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">orders_customers = df_orders.merge(df_customers, on='customer_id', 
<span class="">↪</span>how='left')</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p369"> 
   <p>Now, the <code>orders_customers</code> DataFrame has <code>order_id</code> and <code>customer_unique_ id</code> columns.</p> 
  </div> 
  <ol class="response faux-ol-li" style="list-style: none;"> 
   <li class="readable-text faux-li has-faux-ol-li-counter" id="p370"><span class="faux-ol-li-counter">2. </span> Merge the <code>df_order_reviews</code> and <code>orders_customers</code> DataFrames on the <code>order_id</code> column. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p371"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">merged_df = df_order_reviews.merge(orders_customers[['order_id', 
<span class="">↪</span>'customer_unique_id']], on='order_id', how='left')</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p372"> 
   <p>This will perform a left join, meaning that all rows from <code>df_order_reviews</code> will be included in the <code>merged_df</code> DataFrame, and the corresponding <code>customer_unique_id</code> values from <code>orders_customers</code> will be matched based on the <code>order_id</code> column.</p> 
  </div> 
  <div class="readable-text response" id="p373"> 
   <p>Now, the <code>merged_df</code> DataFrame contains an additional <code>customer_unique_id</code> column, with the correct values assigned to each <code>order_id</code>. If you want to update the original <code>df_order_reviews</code> DataFrame instead of creating a new one, you can simply assign <code>merged_df</code> to <code>df_order_reviews</code>:</p> 
  </div> 
  <div class="browsable-container listing-container" id="p374"> 
   <div class="code-area-container"> 
    <pre class="code-area">df_order_reviews = merged_df</pre>  
   </div> 
  </div> 
  <div class="readable-text response" id="p375"> 
   <p>Now, your <code>df_order_reviews</code> DataFrame has the <code>customer_unique_id</code> column correctly assigned to each <code>order_id</code>.</p> 
  </div> 
  <div class="readable-text" id="p376"> 
   <p>This time, the code works as advertised. Generative AIs indeed can supplement you with the code; they know <em>how </em>to do it (even if they sometimes need severe prodding to get something working). The decision about <em>what </em>needs to happen is still yours (even if the initial conversations provided you with suggestions).</p> 
  </div> 
  <div class="readable-text intended-text" id="p377"> 
   <p>Now, to finish this part of the exercise, let’s check if multiple reviews do come from different customers; in such a case, you should probably inform your IT department, as there would be a strong possibility of erroneous review assignments to orders. The following code should give you the required answer (and no, we didn’t write this ourselves). </p> 
  </div> 
  <div class="browsable-container listing-container" id="p378"> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">unique_customers_per_order =
<span class="">↪</span>df_order_reviews.groupby('order_id')['customer_unique_id'].nunique()
multiple_customers_orders =
unique_customers_per_order[unique_customers_per_order &gt; 1]
if len(multiple_customers_orders) &gt; 0:
    print(f"There are {len(multiple_customers_orders)} orders with reviews 
    <span class="">↪</span>from multiple customers.")
    print("\nOrder IDs and the number of unique customers:")
    print(multiple_customers_orders)
else:
    print("There are no orders with reviews from multiple customers.")</pre>  
   </div> 
  </div> 
  <div class="readable-text" id="p379"> 
   <p>This code groups the <code>df_order_reviews</code> DataFrame by counting unique <code>customer_ unique_id</code> values, checks for cases where the count of unique <code>customer_unique_id</code> values assigned to <code>order_id</code> is greater than <code>1</code>, and finally prints the results. All the multiple reviews came from the same customers. This saves you a trip to the IT den and even allows an analysis of how prevalent an opinion about the order is over time.</p> 
  </div> 
  <div class="readable-text intended-text" id="p380"> 
   <p>We can assume that you now either know your data well enough or know how to use sufficiently eloquent and knowledgeable generative AI models to help you get a good insight into your data structure and properties. In the next chapter, we’ll focus on the business analysis itself.</p> 
  </div> 
  <div class="callout-container sidebar-container"> 
   <div class="readable-text" id="p381"> 
    <h5 class="callout-container-h5 readable-text-h5 sigil_not_in_toc">Things to ask generative AI</h5> 
   </div> 
   <ul> 
    <li class="readable-text" id="p382"> What is the typical data structure for data coming from &lt;some_business_ area&gt;? </li> 
    <li class="readable-text" id="p383"> What data quality problems should I test for in the DataFrame I have provided you? </li> 
    <li class="readable-text" id="p384"> What distribution should I expect in the column describing &lt;some_activity&gt;? </li> 
    <li class="readable-text" id="p385"> How do I deal with &lt;a_clear_description_of_the_encountered_problem&gt;? </li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p386"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_35">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p387"> Be aware of the context of your conversation with your generative AI advisor. Ask it to summarize its understanding of the context (this is particularly useful if you want to start a new session on the same topic, such as to research different approaches). </li> 
   <li class="readable-text" id="p388"> Don’t shy away from reminding your AI adviser about the purpose of the current work or the variable names used. Remember, it aims to please, not to be precise. </li> 
   <li class="readable-text" id="p389"> There is no better way to understand the data than by looking at it. </li> 
   <li class="readable-text" id="p390"> When evaluating data quality, it’s essential to check its structure, consistency, and contents. </li> 
   <li class="readable-text" id="p391"> Double-check your data, triple-check, and reach out to data providers for clarification when you’re still in doubt. </li> 
   <li class="readable-text" id="p392"> It’s a good practice to keep data quality investigation separate from the actual in-depth analysis . . . </li> 
   <li class="readable-text" id="p393"> . . . however, data cleaning and EDA <em>are </em>integral parts of your data analysis, and you’ll be repeating them sooner than you hope. </li> 
  </ul>
 </body></html>