<html><head></head><body>
<div id="book-content">
<div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Summary and Outlook"><div class="chapter" id="ch06_summary_and_outlook_1738498451046636">
      <h1><span class="label">Chapter 6. </span>Summary and Outlook</h1>
      <p>In this report, you learned about the multilayered lifecycles that govern AI projects (the AI development lifecycle in <a data-type="xref" href="ch01.html#ch01_figure_1_1738498450402392">Figure 1-1</a> and the AI model lifecycle in <a data-type="xref" href="ch03.html#ch03_figure_1_1738498450651715">Figure 3-1</a>) and open source Kubernetes-based tools that work to enable each of those phases at scale. You learned how to leverage open source tools to successfully move a generative AI model through these cycles, standardizing the process of model creation and allowing you to confidently deploy and manage AI models in production.</p>
      <p>To understand how these phases interact, which open source tools to use at each phase, and how they look in practice, let’s look at an example project.</p>
      <section data-type="sect1" data-pdf-bookmark="Personalized Healthcare Chatbot"><div class="sect1" id="ch06_personalized_healthcare_chatbot_1738498451046736">
        <h1>Personalized Healthcare Chatbot</h1>
        <p>In this example, let’s follow a fictional generative AI team at a major health insurer as it pitches and builds a personalized health chatbot. In the first phase of the AI development lifecycle, the project is initiated.</p>
        <p>During <em>project initiation</em>, the generative AI team lead meets with the team’s organization’s director of engineering, head of sales, head of IT, and director of research and development to discuss a project idea floated by a member of the team. She pitches a personalized chatbot that healthcare subscribers can interact with to navigate questions about their personal health, their insurance policy, and healthcare providers. She claims this can reduce time spent by <span class="keep-together">customer</span> agents on common, personalized tasks; reduce personally identifiable information (PII) and health information from being exposed to customer agents; and increase a subscriber’s agency over their own healthcare, reducing costs and increasing satisfaction.</p>
        <p>The business units are convinced by the case she made, but the director of engineering is skeptical. How will data be safeguarded? What kinds of technologies will be used? How can we deploy this and serve all of our customers? The team lead explains this to the director of engineering, who is satisfied with her answers. We’ll break down her plan throughout the rest of the chapter.</p>
        <p>Once the project initiation is developed and agreed upon, the next phase, shown in <a data-type="xref" href="ch01.html#ch01_figure_1_1738498450402392">Figure 1-1</a>, is <em>data preparation</em>. Our technical lead’s team gets to work collecting data to train and personalize the chatbot. The team chooses a popular <em>foundation model</em>, which it will fine-tune to have better access to nonidentifiable company-wide information, and then will use techniques such as prompt engineering and retrieval-augmented generation (RAG) at inference time with a user’s personal information to further personalize individual chatbot sessions. Within this phase is the first phase of the AI model lifecycle from  <a data-type="xref" href="ch03.html#ch03_figure_1_1738498450651715">Figure 3-1</a>: <em>gathering training/fine-tuning data</em>. The team builds data ingestion pipelines from its healthcare partners and internal systems, <em>online analytics processing (OLAP)</em> databases to store this data, and object storage technologies to store additional data and views. </p>
        <p>Next, the generative AI team enters the <em>model experimentation</em> phase of the AI development lifecycle. This is an iterative phase that includes the following phases of the AI model lifecycle (Fig. 3-1):</p>
        <ul>
          <li>
            <p>Developing training/fine-tuning code</p>
          </li>
          <li>
            <p>Executing the training/fine-tuning job</p>
          </li>
          <li>
            <p>Evaluating the trained model</p>
          </li>
        </ul>
        <p>The team spends several months working on this, building the initial model training code scaffolding on which the team will fine-tune the foundation model and test RAG techniques and different prompts. The team committed to a cloud-agnostic open source platform to allow greater flexibility across many environments, and so chose to build its infrastructure with Kubernetes. Because fine-tuning a foundation LLM is less intensive than training one from scratch, the team chose to use PyTorch libraries to fine-tune an existing smaller foundation model on a small corpus of the company’s data. Early exploratory versions were created in a small Jupyter notebook environment, but as the fine-tuning datasets, base models, and fine-tuned models grew in size, the team turned to <a href="https://www.kubeflow.org">Kubeflow</a> and the Kubeflow Training Operator to scale up the fine-tuning process on its Kubernetes cluster.</p>
        <p>As the team trained and evaluated new versions of its fine-tuned model, managing the training clusters became a headache, and so the team decided to invest time in finding a training resource management tool. The team had become more familiar with Kubernetes at this point, and wanted to ensure it had plenty of control without too many abstractions getting in its way. The team adopted <a href="https://kueue.sigs.k8s.io">Kueue</a> to queue up and prioritize resource-hungry training jobs, ensuring the highest-priority jobs would be run first.</p>
        <p>One thing the team decided early on in the project, however, was that it would need an experiment tracking tool. The team knew it would be repeating the fine-tuning/evaluation cycle frequently before the first candidate was ready to be promoted to production across many data scientists, and needed a way to understand who did what. Because the team had previously chosen Kubeflow as its platform of choice, the team was able to use <a href="https://oreil.ly/bjLBl">Kubeflow Pipelines</a> to build repeatable training jobs and the Kubeflow Model Registry to keep track of trained models. This allowed the data science team to keep track of fine-tuned model artifacts, prompt artifacts, and model evaluation metrics, making the team lead’s life easier when deciding when to bring a model into <span class="keep-together">production</span>.</p>
        <p>The key deliverables for this phase are production-ready model artifacts and a reusable training pipeline that accelerates both this phase and the periodic retraining of the deployed model. The pipeline itself is cleanly versioned using GitOps (see <a data-type="xref" href="ch03.html#ch03_making_training_repeatable_1738498450655759">Chapter 3</a>) principles and <a href="https://oreil.ly/00FxF">Argo CD</a> to manage continuous delivery of clean production pipeline versions that will be used to train production models.</p>
        <p>At the same time, engineers on the generative AI team are working together with the product engineering team to design and build APIs and artifact storage that allow the generative AI team to deploy new models autonomously and the product engineering team to build a chat interface that doesn’t need to know any details about the model. This is the <em>application integration</em> phase of the model development lifecycle, and for smaller teams, this may happen only after the first production-ready model is trained.</p>
        <p>The “last” phase (in quotes because this is a cyclical, iterative process) is putting the model into <em>production service</em>. In  <a data-type="xref" href="ch03.html#ch03_figure_1_1738498450651715">Figure 3-1</a>, this corresponds to <em>promoting the model to production for inference and monitoring the served model</em> (see <a data-type="xref" href="ch04.html#ch04_model_deployment_and_monitoring_1738498450837987">Chapter 4</a>). </p>
        <p>When a model is promoted to production, a system is put in place to deploy the chosen artifact and to then serve it behind an API. While it is running in production, metrics are monitored to ensure that the model is functioning as expected and that the serving infrastructure is returning results to users in a timely manner. Our generative AI team lead chose to use <a href="https://oreil.ly/sZzWk">KServe</a> with the <a href="https://docs.vllm.ai">vLLM</a> runtime. She chose KServe because of its tight integration with the Kubernetes ecosystem and active developer community. She chose the vLLM runtime because it is specifically built for serving LLMs at scale and has many features and optimizations to serve a high volume of inference requests quickly. This combination also comes with a standard API on the model that the product team can access to finalize application integration and canary deployments to gradually roll out and test new model versions with a small number of users. </p>
        <p>The team spent some time throughout the process to define a number of metrics to keep track of for production models. Some of these came from vLLM, others came from KServe, some were human feedback scores from customers, and still others the team built. Thanks to an integration with KServe, the team’s MLOps engineers are using <a href="https://prometheus.io">Prometheus</a> to visualize and monitor the metrics of the production model and respond right away to slow performance, traffic spikes, data drift, and outages.</p>
        <p>Using Prometheus on several occasions helped the team to catch a growing scaling issue early on, and the canary deployments provided by KServe prevented the issues from affecting more than a small number of users. Following GitOps best practices allowed the team to revert the problematic infrastructure version to a previously known good version, giving the team members time to diagnose and fix any found bugs before redeploying.</p>
        <p>During early testing, the team found that users were able to get inappropriate answers from the chatbot and that some conversations with the chatbot were perceived as rude. One of the machine learning engineers on the team had recently read about <a href="https://oreil.ly/wOv-O">TrustyAI Guardrails</a> (see <a data-type="xref" href="ch05.html#ch05_responsible_ai_1738498450930720">Chapter 5</a>) and began an initiative to build guardrails into KServe. With Prometheus, the team was able to monitor detections of inappropriate responses and interactions as well as the customer feedback for these instances. Using the Guardrails, the team was able to reduce negative interactions by a whopping 87%.</p>
        <p>The results were astounding: customer agents had more time to upskill and work on serving customers who had more complex issues, customers could get personalized information in a conversational interface without having to call or wait for a live agent, and her team created a blueprint for future generative AI initiatives throughout the organization.</p>
        <p>But the team’s work was not done yet, and in fact wouldn’t be done until the feature was retired or superseded by another technology. Because the model was fine-tuned on organization-wide data, it would have to be periodically fine-tuned, evaluated, and redeployed to make sure it had up-to-date information. Surprisingly, this would be a simple effort requiring only one or two data scientists less than a week to complete. This is all thanks to the generative AI team lead’s forward thinking by directing the creation and use of a reusable training pipeline with GitOps, model version tracking via a registry, data versioning, and using predictable data storage.</p>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Future Technology Outlook"><div class="sect1" id="ch06_future_technology_outlook_1738498451046802">
        <h1>Future Technology Outlook</h1>
        <p>What is next for this intrepid generative AI team? We foresee three broad dimensions along which innovation in AI and AI platforms will progress over the coming months: </p>
        <ul>
          <li>
            <p>Model architectures, the capabilities they yield, and the tools and techniques used to create them</p>
          </li>
          <li>
            <p>The level of integration between tools underpinning the overall MLOps lifecycle and the ability to leverage these integrated solutions to build intelligent applications more cost effectively</p>
          </li>
          <li>
            <p>Further innovation in inference optimization to reduce response latency, in areas such as quantization techniques, LoRA adapters, dynamic batching, and inference workload scheduling techniques</p>
          </li>
          <li>
            <p>The ability to build and maintain AI-enabled applications in a way that ensures the responsible and trustworthy use of AI</p>
          </li>
        </ul>
        <p>Along the first dimension, we will continue to see the largest models getting larger as compute resources become more performant, more efficient, and more readily available. At the same time, we will continue to see novel approaches for customizing smaller models with an organization’s own data in order to yield high performance results at lower costs for domain-specific use cases. General purpose AI will become more powerful and use case-specific models will get easier to create. The key to taking advantage of these innovations will be to leverage frameworks and training platforms with strong open source community adoption in order to be well-positioned at the forefront of new technological leaps.</p>
        <p>Along the second dimension, platform suites such as Kubeflow that bundle tools across the AI model and development lifecycles will make it easier for data scientists to use each component in a way that is increasingly transparent to them. For example, libraries for training models will natively integrate with experiment tracking and model registry tools so that data scientists’ experiments are automatically tracked. Additionally, these solutions will come with tools to automatically detect and respond to hardware failures during model training and serving, reducing the overall cost of developing and running models. These projects will gain and improve capabilities for managing the cost of developing models via better management of compute resources and sharing these resources across data science teams.</p>
        <p>Managing compute resources is the major theme of the third dimension. Here, we will see continued optimization of inference workloads via ongoing research on new quantization techniques (in which the weights and activations of a model are represented with lower precision data types, reducing memory usage), efficient utilization of <a href="https://oreil.ly/MOnY_">key-value (KV) caches</a>, LoRA adapters, and dynamic batching techniques (where requests to the hardware accelerator are batched based on batch size or time elapsed), all in order to make more efficient use of hardware accelerators like GPUs. We will also begin to see wholly new innovations in how inference workloads are scheduled and executed on hardware accelerators, again to use existing accelerators more efficiently.</p>
        <p>And along the final dimension, we expect to see more resources (financial, talent, etc.), research, and tooling dedicated to the ethical and safe training and use of generative AI. From training data lineage and tracking to model explainability and safety tools like guardrails and hallucination detection, generative AI has broadly expanded the potential for harmful creation and use of LLMs. Community-driven initiatives like <a href="https://oreil.ly/PXDlY">TrustyAI</a> and specifically TrustyAI Guardrails, along with <a href="http://hub.guardrailsai.com">Guardrails Hub</a>, are already making it easier than ever to protect users, PII, and enterprises from the various direct and indirect harms (such as lawsuits) that can be brought about with LLMs. Additionally, we expect more norms and tooling for the ethical collection and sharing of large datasets to protect privacy and intellectual property rights.</p>
      </div></section>
    </div></section></div>
</div>


<div id="book-content">
<div id="sbo-rt-content"><section data-type="colophon" epub:type="colophon" class="abouttheauthor" data-pdf-bookmark="About the Authors"><div class="colophon" id="id16">
  <h1>About the Authors</h1>
  <p><strong>Alex Corvin</strong> is a senior engineering manager responsible for crafting and executing capabilities for data scientist experimentation and model training within Red Hat OpenShift AI, Red Hat’s flagship AI/ML platform. Alex has orchestrated creation and enhancement of functionalities for distributed training and fine-tuning of AI models, encompassing extensive language models, utilizing tools such as Ray and PyTorch. Alex and his team contribute heavily to several prominent open source projects including Kubeflow Pipelines, Kueue, Kuberay, Feast, and Kubeflow Training Operator. Alex has spoken at numerous industry conferences like Ray Summit, DevConf, NVIDIA GTC, OpenShift Commons, and more.</p>
  
  <p><strong>Taneem Ibrahim</strong> is a senior engineering manager whose team is responsible for several projects in the development of an enterprise-class MLOps product, Red Hat OpenShift AI. As part of the product engineering work, Taneem and his team participate in several open source projects such as model serving (KServe, ModelMesh, vLLM), responsible AI (TrustyAI, AIX360), and model registry (KubeFlow, ML Metadata). Taneem has also worked with an extensive AI partner ecosystem for integration with OpenShift AI and IBM watsonx.ai. Taneem has spoken at many industry events like Ray Summit, Red Hat Summit, and KubeCon.</p>
  
  <p><strong>Kyle Stratis</strong> is a software engineer with over a decade of experience across the AI development lifecycle in a variety of domains, including computer vision, health technology, and social media analytics. Along with being an O’Reilly author, he is the founder of <a href="https://stratisdatalabs.com">Stratis Data Labs</a>, an AI and data consultancy, and was most recently the lead machine learning engineer at Vizit Labs, where he built Vizit’s internal AI platform.</p>
</div></section></div>
</div>
</body></html>