- en: 10 Introducing customized LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: How a lack of context affects an LLM’s performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How RAG works and its value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the fine-tuning of LLMs works and its value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing RAG and fine-tuning approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Over the past few chapters, we saw how to hone our skills to identify distinct,
    focused tasks that large language models (LLMs) can support. Combined with a range
    of prompt-engineering techniques, we’ve been successful in getting LLMs to return
    responses that are valuable for our testing activities. However, despite the lessons
    we learned, the responses we receive might still not be completely aligned with
    our needs and context. Although it would be foolish to think that we can completely
    align an LLM with our context, there are more advanced options that can be utilized
    along with prompt engineering to further maximize the output of an LLM in support
    of our testing. So, in this final part, we’re going to examine ways in which we
    can enhance LLMs so that they can become more embedded in our context, specifically
    focusing on retrieval-augmented generation (RAG) and fine-tuning. But before we
    dig into the specific details and actions of how these approaches work, we’ll
    first examine why more commonly used LLMs such as ChatGPT, Claude, and Gemini
    may struggle to tune to our context and then slowly familiarize ourselves with
    the more advanced topics of RAG and fine-tuning, comparing them to determine which
    one is more suitable in a given situation.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 The challenge with LLMs and context
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We learned that the key challenge of working with LLMs is getting them to respond
    with an output that
  prefs: []
  type: TYPE_NORMAL
- en: Is relevant to our context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Follows the rules of our prompt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contains no hallucinations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The prompt-engineering techniques we’ve used have helped us achieve these goals
    so far. For example, if we return to our example prompt from chapter 5, we can
    see a weak prompt such as
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Create tests for a file upload feature |'
  prefs: []
  type: TYPE_TB
- en: 'which returns a weak response with quite generalized results:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    1.  Test for File Upload Functionality:   •   Test Case 1.1: Upload a
    Valid File   −   Input: Choose a file with a valid format (e.g., .jpg, .pdf).  
    −   Expected Outcome: The file is successfully uploaded, and a confirmation message
    is displayed.   •   Test Case 1.2: Attempt to Upload an Invalid File   −   Input:
    Choose a file with an invalid format (e.g., .exe, .xhtml).   −   Expected Outcome:
    An error message is displayed, indicating that the file format is not supported.  
    2.  Test for File Size Limit:   •   Test Case 2.1: Upload a File Below the Size
    Limit   −   Input: Choose a file smaller than the maximum allowed size.   −  
    Expected Outcome: The file is successfully uploaded, and a confirmation message
    is displayed.   •   Test Case 2.2: Attempt to Upload a File Exceeding the Size
    Limit   −   Input: Choose a file larger than the maximum allowed size.   −   Expected
    Outcome: An error message is displayed, indicating that the file exceeds the maximum
    size limit. |'
  prefs: []
  type: TYPE_TB
- en: However, a prompt with more detail and the right prompt-engineering tactics
    can result in a more optimal response. A prompt such as
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are an expert software tester. Generate as many test cases as possible
    for the behavior delimited by three hashes.###An Authorization API that will take
    a token from a different API and check to see if the token is valid and in date
    before either responding positively or negatively depending on the result.###
    |'
  prefs: []
  type: TYPE_TB
- en: returns more specific results, such as
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    1.  Valid Token Check:   •   Test with a valid token that is within the
    expiration date. Expected result: API responds positively.   2.  Expired Token
    Check:   •   Test with a valid token that has expired. Expected result: API responds
    negatively.   3.  Invalid Token Check:   •   Test with an invalid token format
    (e.g., incorrect characters, wrong length). Expected result: API responds negatively.  
    4.  Empty Token Check:   •   Test with an empty token input. Expected result:
    API responds negatively or returns an appropriate error message.   5.  Null Token
    Check:   •   Test with a null token input. Expected result: API responds negatively
    or returns an appropriate error message. |'
  prefs: []
  type: TYPE_TB
- en: 'Our second prompt is more successful because we’ve shared more of our context
    within as well as used specific prompt engineering tactics. Therefore, if we want
    to get the most out of an LLM, we not only need to rely on good prompt-engineering
    techniques, but also provide the most relevant context possible. The reasons for
    this are twofold:'
  prefs: []
  type: TYPE_NORMAL
- en: It is more than likely that an LLM has not been trained on our context. Therefore,
    an LLM has no increased weighting or bias toward our context.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An LLM is trained on such a massive amount of generalized data that when asked
    a generic question, it will rely on stronger, more generalized patterns it has
    identified from the training process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, if we want to get the best out of an LLM, on the surface, the answer seems
    simple enough: provide it with as much context detail as possible (which we have
    been doing to some degree already). However, if we were to do this, we’d quickly
    run into some limitations around the amount we can send in a prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 Tokens, context windows, and limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we talk about this prompting limitation, there are some other concepts
    around LLMs that we need to be aware of—namely, tokens and context windows. Understanding
    these two aspects of LLMs will help us appreciate why current LLMs have an upper
    limit on how much context can be provided and how that affects our strategies
    of use.
  prefs: []
  type: TYPE_NORMAL
- en: Tokens
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine we are sending the following prompt to an LLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| List me five of the most populated cities in the world. |'
  prefs: []
  type: TYPE_TB
- en: 'How does an LLM, which can only interpret information using machine code, parse
    this prompt and return a response? This is done through a process known as *tokenization,*
    in which natural language text is converted into matching integers that can be
    read by an LLM. To understand how this works, let’s consider our populated cities
    prompt. If we were to put this through the tokenization process, the sentence
    would be sliced into smaller, discrete parts. For example, our prompt could be
    broken into 12 sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, each word in the sentence and the period at the end have been
    sliced into their own smaller sections, known as *tokens.* Notice how they also
    include the whitespace to the left of each word. It’s a general rule of thumb
    that each word in a sentence is split into its token; however, some tokenizers
    (the tools used for making this conversion) can sometimes break up larger words
    into individual tokens or group smaller words together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a sentence has been sliced into tokens, each one is converted into an
    integer with a unique number being used as an identifier for each word. For example,
    completing the tokenization of our prompt would result in a list of numbers (the
    commas and whitespace have been added to help readability):'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each number in this list correlates to a specific slice taken from our prompt.
    For example, the token `the` has an id of 279 and we can see that it appears twice
    in the list for each instance when `the` was used in the sentence. Once the prompt
    has completed the tokenization process, the model we use is then able to process
    the list of integers and start determining how to respond. Then the process of
    tokenization is used again to create the response text we receive from a model,
    so our model might respond with a series of integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'When converted back into text, it would result in the following response (line
    breaks are also included in the integer list, and each mention of iteration of
    11 is a line break):'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Tokyo, JapanDelhi, IndiaShanghai, ChinaSão Paulo, BrazilMumbai, India |'
  prefs: []
  type: TYPE_TB
- en: Experimenting with tokenizers
  prefs: []
  type: TYPE_NORMAL
- en: To better understand how the tokenization process works and how words, numbers,
    and symbols are sliced, we can experiment with preview tools for tokenizing such
    as the one at [https://gpt-tokenizer.dev/](https://gpt-tokenizer.dev/).
  prefs: []
  type: TYPE_NORMAL
- en: So, tokens are an important aspect of LLMs because they inform us not just of
    how a model can parse a prompt and form a response, but also of how large a prompt
    can be sent to said model before we run into problems. This brings us to context
    windows and the crux of our context challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Context windows
  prefs: []
  type: TYPE_NORMAL
- en: Given that the tokenization process takes natural language and turns it into
    a series of numbers for an LLM to process, the longer a prompt is, the more tokens
    there are to process. The problem with prompts that have larger sets of tokens
    is that it affects how effectively an LLM processes our prompt and the resources
    it consumes. The larger a prompt, the more complex it becomes to generate a response,
    which means more hardware usage. All of this will come at a cost, either in hosting
    fees if utilizing a private LLM or API costs for sending prompts (platforms such
    as OpenAI charge based on the number of sent and received tokens).
  prefs: []
  type: TYPE_NORMAL
- en: Add to this that a larger context window doesn’t necessarily mean a better-performing
    LLM, and we start to see that providers of LLMs have a tradeoff to make. As a
    result, LLMs will likely have some sort of limitation built into the model on
    the number of tokens it can receive at a given time. This is known as a model’s
    `context` `window`. Different models contain different-sized context windows,
    which are sometimes also referred to as the context length. All of this depends
    on the type of model that has been trained, the hardware it is running on, and
    how it has been deployed with other supporting applications. For example, OpenAI’s
    ChatGPT 4 is estimated to have a context window of 128k tokens, whereas that for
    Meta’s Llama-2 is 4k (before modifications are made). Therefore, when it comes
    to determining which LLM to use in a given situation, we must be aware of the
    context length. Choosing a model that is limited in size to save cost might limit
    what context we can add to a prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Not all context windows are the same
  prefs: []
  type: TYPE_NORMAL
- en: One thing to note when discussing context windows is that just because a model
    is able to take, for example, a 128k token request, it doesn’t mean that the response
    will have the same limitations. In fact, it may be that the response has a much
    smaller window to help keep costs down. This won’t necessarily affect our learning
    in the following chapters, but it’s a useful detail to keep in mind when expecting
    a model to return a large response.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 Embedding context as a solution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we understand that LLMs interpret our requests through tokenization
    and that the size of tokens we can send to an LLM is limited, we can start to
    see the problem we face when adding further context to our prompts. Although LLMs
    are developing rapidly, becoming more efficient and offering larger context windows,
    it’s simply not cost-effective to, for example, add a complete code base of an
    application with prompt’s instructions. We’d either end up hitting the upper limits
    of a model’s capabilities or burning our budget at a rapid pace. Instead, to maximize
    accuracy, we need to consider how we embed context into our prompts and our LLMs
    in an intelligent way. Fortunately, there has been a lot of work in the AI community
    that we can use to embed our context further in a way that improves accuracy and
    doesn’t break the bank (or the model) in the process.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Embedding context further into prompts and LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To improve the ability to increase an LLM’s exposure to our context, we can
    utilize one of two techniques. The first is RAG, and the second, fine-tuning.
    Throughout the remaining chapters, we’ll explore how these two approaches work,
    how they differ from one another, and how we can determine which is a more suitable
    approach to improving an LLM’s responses. Although both approaches are different
    in application, the end goal of both is similar: to help us improve a model’s
    performance by allowing us to add more context to an LLM’s workflow. Retrieval-augmented
    generation looks to solve the problem by focusing on ways in which we can enhance
    our prompting, whereas fine-tuning looks to bake our context directly into the
    model itself. Let’s take a look at both briefly so that we can become more familiar
    with them and determine which approach is more suitable in a given situation.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.1 RAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we learned earlier, if an LLM has limitations in the size of a prompt it
    can receive due to a context window, it’s inadvisable to attempt to throw all
    our context into a single prompt hoping that it will improve an LLM’s response.
    However, that doesn’t mean we can’t be selective with the type of context we provide
    in a prompt. What this means is that accuracy can be improved in an LLM not by
    brute-forcing our context onto an LLM, but by crafting our prompts so that they
    contain all the relevant information about our context to support our instructions.
    In more concrete terms, this means that if we wanted an LLM to generate boilerplate
    page objects for an automated check, it would be better to provide the specific
    HTML for a page and any relevant code for said page than add the entire code base
    to a prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the surface, this seems like an effective and simple approach: write our
    prompt, find the relevant supporting information, combine the two into a final
    prompt, and send it to an LLM. The problem though is that this can be a labor-intensive
    activity, researching and determining what information to add and what to ignore.
    Fortunately, this is where RAG can help us. As shown in figure 10.1, RAG works
    by automating the process of embedding relevant information into our prompts through
    using our prompt’s initial instructions to determine what information to add to
    our prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH10_F01_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 A high-level workflow diagram for RAG
  prefs: []
  type: TYPE_NORMAL
- en: 'To help us better understand the process, let’s return to our Page-object-generation
    prompt example. With a RAG framework, the approach would work like this:'
  prefs: []
  type: TYPE_NORMAL
- en: A corpus of information will have been created. In our example, it may contain
    labeled HTML documents for each page of our application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A prompt is created in which we ask for an LLM to generate a Page object for
    our booking listing page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Our RAG framework analyzes our prompt and programmatically finds the most relevant
    document in our corpus of HTML documents. If the RAG framework is working correctly,
    it will determine that the HTML document that contains the booking listing is
    the most relevant.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The most relevant HTML document is added to the prompt that we initially created,
    and the prompt is then sent to an LLM to return a response.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The RAG helps us contextualize our prompts further by analyzing what we’re asking
    and then automatically identifying the right type of context. What makes this
    useful is that it helps us create a prompt with the information that is of most
    use for an LLM to create a more accurate response than if the information didn’t
    exist. It’s also a useful approach because RAG allows us to embed any type of
    data that is easy to parse and search for relevancy, whether it’s code, documentation,
    database entries, or raw metrics. We also can control how relevancy is determined,
    meaning that we still have control over the type of information that might be
    added to a prompt.
  prefs: []
  type: TYPE_NORMAL
- en: For these reasons and its relative ease of setup, RAG has become a popular approach
    to enhancing our interaction with LLMs. Once we begin to appreciate how RAG works,
    we can begin to see how it might be useful in a testing context. We’ve already
    explored the idea of using RAG to extract sections of a code base to support prompts
    that are looking to create automation, but it can also be used to support queries
    around risk analysis, understanding how our products work and generating test
    ideas. There is also the potential to use testing artifacts in RAG frameworks,
    such as exploratory testing notes, test scripts, or automation code, to bolster
    the prompting ideas we explored in the previous chapters. At its core, if the
    data we want to use can be stored in a consistent format and easily queried, then
    it has the potential to be used in a RAG framework.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.2 Fine-tuning LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although RAG focuses on ways to enhance a prompt by adding targeted contextual
    material, fine-tuning focuses on enhancing the model we’re using itself. Fine-tuning
    utilizes a collection of tools and techniques used to further train a model that
    has already been initially trained in hope that it will bias the model further
    toward the data it has been fine-tuned on, as summarized in figure 10.2.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH10_F02_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 A visualization outlining the difference between training and fine-tuning
  prefs: []
  type: TYPE_NORMAL
- en: 'As the figure shows, fine-tuning takes place against a model that has already
    been trained on a large corpus of data. For example, the free version of ChatGPT
    is a fine-tuned version of OpenAI’s gpt-3.5-turbo model. gpt-3.5 went through
    a series of training processes to get its core abilities, but when interacted
    with, it lacks the tone and characteristics of a chatbot. For example, after sending
    the following prompt to gpt-3.5 (before fine-tuning):'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-MW.png)'
  prefs: []
  type: TYPE_IMG
- en: '| You are an expert travel agent. Give me a list of the top three places to
    visit in London. |'
  prefs: []
  type: TYPE_TB
- en: 'I received the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '|    1.  The British Museum - This iconic museum houses a vast collection of
    art and artifacts from all corners of the globe, including the Rosetta Stone and
    the controversial Elgin Marbles. It is a must-visit for history and culture enthusiasts.  
    2.  The Tower of London - This historic fortress on the banks of the River Thames
    has served as a royal palace, prison, and armory. Visitors can explore the Crown
    Jewels, see the famous ravens, and learn about the Tower’s fascinating history.  
    3.  The West End - London’s renowned theater district is home to world-class productions
    and musicals. Take in a show at one of the many historic venues like the Royal
    Opera House or the National Theatre, and experience the magic of London’s vibrant
    performing arts scene. |'
  prefs: []
  type: TYPE_TB
- en: 'Whereas, upon sending the prompt to ChatGPT (a fine-tuned version of gpt-3.5),
    I received the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/logo-openai.png)'
  prefs: []
  type: TYPE_IMG
- en: '| Absolutely! London offers a plethora of attractions, but here are three must-visit
    places:   1.  The British Museum: It’s a treasure trove of human history and culture,
    housing millions of artifacts spanning centuries and continents. From the Rosetta
    Stone to the Egyptian mummies and the Elgin Marbles, the British Museum is a fascinating
    journey through civilizations.   2.  The Tower of London: Steeped in history and
    legend, the Tower of London is an iconic fortress that has served as a royal palace,
    prison, and treasury over the centuries. Visitors can explore its ancient walls,
    marvel at the Crown Jewels, and hear tales of famous prisoners and executions.  
    3.  Westminster Abbey: A masterpiece of Gothic architecture, Westminster Abbey
    is not only a place of worship but also the site of royal coronations, weddings,
    and burials. Visitors can admire its stunning stained glass windows, intricate
    stone carvings, and the tombs of famous figures like Isaac Newton, Charles Darwin,
    and Elizabeth I.These attractions offer a rich glimpse into London’s history,
    culture, and heritage. |'
  prefs: []
  type: TYPE_TB
- en: Factually, they are similar. But looking at the second response, we can see
    that it has a bit more flavor to it. It has a more casual and friendly tone, all
    of which comes from the fine-tuning process it has undergone to make it feel more
    chat-like.
  prefs: []
  type: TYPE_NORMAL
- en: Qualities such as ChatGPT’s more casual tone are achieved by training a model
    with a curated dataset in hope of biasing its internal parameters toward a desired
    tone, style, or specific context. To recap, a model consists of millions or billions
    of parameters that determine the probability of which word should come after another
    word or collection of words. These parameters are in place when a fine-tuning
    session is started, and the fine-tuning process seeks to influence the parameters
    further toward an end goal. We’ll talk about this more in chapter 11, but we can
    see how this approach works in figure 10.3.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning takes information from a data set and uses it to form a prompt sent
    to our model that is being fine-tuned. The response is then evaluated against
    an expected response. If the responses don’t align, the model is tweaked so that
    its chances of an aligned response the next time are increased. This is then done
    thousands of, or perhaps millions of, times to slowly tune a model closer to our
    end goal of how we want the fine-tuned model to respond, which means that a very
    large corpus of data is required to successfully train a model.
  prefs: []
  type: TYPE_NORMAL
- en: Applying fine-tuning to a model can have a range of benefits. We have already
    seen them in the fine-tuning of GPT into ChatGPT, but we’ve also taken advantage
    of fine-tuned GPT models in the form of GitHub Copilot. These examples demonstrate
    the range of uses that fine-tuning can provide in a testing context. Thus, we
    can tune models based on natural language text that might come from documentation
    or testing artifacts. This could be used to embed domain language into its responses
    and promote responses that are more tuned to our context. They can also be tuned
    on our code base to help us with additional risk analysis, comprehension of what
    our code is doing, or acting as a more aligned code assistant.
  prefs: []
  type: TYPE_NORMAL
- en: When discussing fine-tuning, we need to be careful not to fall into the trap
    of thinking we’re teaching a model about our context. LLMs don’t think like humans
    do. But, as an analogy, teaching a model about our context is a close one. The
    challenge is that it’s not an exact process, meaning that multiple iterations
    are likely required to get the result we want, and as our context changes, further
    tuning sessions would likely be required.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH10_F03_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 A visual model of how fine-tuning works
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.3 Comparing the two approaches
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Knowing which approach to take depends very much on the goals we want to achieve,
    and the constraints put upon us. The two approaches aren’t entirely the same.
    But if we ever come into a position where we need to decide which approach to
    take, it can be beneficial to use some general attributes to help us determine
    the pros and cons of each.
  prefs: []
  type: TYPE_NORMAL
- en: Learning curve
  prefs: []
  type: TYPE_NORMAL
- en: Although learning new skills is always relative to a person’s abilities and
    skillsets, for those of us who have followed along with the book so far, getting
    comfortable with RAG frameworks is a smaller learning leap than fine-tuning. It
    can be argued that using a RAG framework is a form of advanced prompt engineering
    and many off-the-shelf tools are available that can be used to jump into using
    RAG with minimum effort.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning, however, has a steeper learning curve because it is a collection
    of different actions, tools, and considerations that are much larger in scope
    than using RAG. In the chapter on fine-tuning, you’ll learn different steps to
    take in curating and preparing data for tuning, executing a fine-tuning session,
    and evaluating success. Each part requires knowledge of tools, frameworks, and
    approaches to complete each step. Fortunately, the ecosystem around fine-tuning
    is making the fine-tuning process more accessible and easy to get started with.
  prefs: []
  type: TYPE_NORMAL
- en: Cost
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two aspects to cost that we need to consider: tooling and talent.
    As we’ve learned, the learning curve for RAG can be easier than that for fine-tuning.
    This means that the associated costs around training or hiring talent around implementing
    RAG are potentially lesser. As for tooling, the cost to get initially set up with
    RAG can be quite low. However, costs for RAG framework tools and the use of third-party
    LLMs can cause costs to balloon, especially if we are being charged for the number
    of tokens sent and received via an LLM API platform.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the popular RAG frameworks, a lot of fine-tuning tools are open source,
    which can make initial investment in tooling cheaper. Platforms are appearing
    that make the fine-tuning process easier, but they do come at a price. For tooling,
    the cost can be found more in the hardware needed to support fine-tuning. Running
    tuning sessions requires substantial CPU, GPU, and RAM resources, and if we want
    to tune at scale, then more investment is required. There is also the cost of
    hosting a tuned model once it’s ready for use. Finally, as fine-tuning consists
    of a combination of activities, training or hiring can be much more expensive,
    depending on how much detail we want to go into each part of the tuning process.
  prefs: []
  type: TYPE_NORMAL
- en: Speed to production
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the tools available to support a RAG framework out of the box, getting
    RAG set up and running can be rather fast. When iterating with RAG, the focus
    will be on the following two areas: the prompt we want to send to our LLM (that
    includes the additional data) and the data we want to store and extract relevant
    information from when required. Although there is a lot of space for improvement,
    getting these aspects of a RAG framework set up to an initial satisfactory state
    doesn’t take too much time.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning, however, can be relatively slower because there are more activities
    involved. For example, curating and preparing data sets for fine-tuning can be
    a complex activity in its own right. Depending on hardware, fine-tuning can also
    take time to complete with even small tunings taking many hours. Add to this that
    we would likely need to run multiple tunes because we tweak tuning and model settings,
    as well as the data set we are using, so it can take a while before we reach a
    satisfactory tuned model.
  prefs: []
  type: TYPE_NORMAL
- en: Control
  prefs: []
  type: TYPE_NORMAL
- en: Although most of the comparisons so far have been favorable toward RAG frameworks,
    those benefits do have a tradeoff. When we refer to control as a quality characteristic
    of using LLMs, this implies how much influence we have on improving the process,
    what insight we have into how a model is performing, and what control we have
    over the LLM behavior. Also, there are considerations for privacy controls as
    well.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the RAG tools available for purchase are hosted on platforms that can
    be quite opaque. This can mean there is less control over how data is stored for
    retrieval or how the relevancy algorithms work. For example, one technology that
    is used in RAG is vector databases. How data is stored and relationships maintained
    in vector databases can be out of our control but have a big influence on what
    relevant data is returned. Add to this that a lot of these tools tend to encourage
    us to use platforms such as OpenAI’s API, then we have even less control over
    which models we want to use and how an LLM will respond.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning is very much about experimentation, which means we must have full
    control over all aspects of how the tuning is done. Because fine-tuning contains
    many steps, we have a lot of control over what happens within each part of the
    process. We have control over what data we want to use and what format it should
    be in, and we can control which type of model we want to tune and how. Furthermore,
    because the result is a tuned model that can be deployed elsewhere, we have a
    lot more control over where a model is deployed and who has access to it, making
    it more suitable for enterprise-based applications.
  prefs: []
  type: TYPE_NORMAL
- en: These comparisons help us get a flavor of how the two approaches compare, which
    has been summarized in figure 10.4.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, these comparisons are highly context-dependent, but they do demonstrate
    that RAG can be a faster, more cost-effective approach to take on first. However,
    if we want more control over how we want an LLM to respond, then turning to fine-tuning
    can reward us further if we’re willing to invest.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2.4 Combining RAG and fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We’ve explored how these two approaches differ, but before we conclude the chapter,
    it’s worth stating that these two techniques are not mutually exclusive. Given
    that RAG focuses on prompts, whereas fine-tuning focuses on changes to a model,
    both can be combined to further improve responses. The tradeoff is that much more
    complexity is introduced into building, training, and debugging. It is much more
    expensive to bring a tuned model inside a RAG framework to production, and if
    it doesn’t work as expected (or desired), how do we determine what needs our attention?
    This is the challenge of working with indeterministic systems, whether we choose
    to focus on RAG, fine-tuning, or a combination of both. The approach to evaluating
    an LLM’s use as a testing assistant requires constant, healthy skepticism.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/CH10_F04_Winteringham2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 A quick comparison of RAG and fine-tuning
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the key challenges of using LLMs is getting them to return context-sensitive,
    valuable results.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To get a well-aligned response, an LLM needs to be given as much relevant context
    as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLMs interpret natural language text by turning text into numbers, known as
    tokens, through the tokenization process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Based on the sophistication of an LLM model and the hardware it is run on, an
    LLM will only be able to take a certain number of tokens at a given time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of tokens an LLM can take at a given time is known as the context
    window.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because LLMs have a limited context window, we must come up with different strategies
    to allow us to embed context in without incurring massive costs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two approaches that can be used to improve context awareness are retrieval-augmented
    generation (RAG) and fine-tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG is a process in which additional relevant information is added to a prompt
    to improve an LLM’s response.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG works by connecting to a corpus of data and finding the most relevant material
    based on the provided prompt query. It’s then all combined into a single prompt
    for an LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning utilizes training techniques to tune an already trained model with
    additional data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning allows us to modify the tone or detail or way in which an LLM responds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning can help us promote our context within an LLM’s parameters and make
    it more context-sensitive to our needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning how to utilize RAG frameworks tends to be faster and easier than fine-tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning requires knowledge of different processes and tools to carry out
    the full fine-tuning process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost of tooling and talent for RAG is relatively lower than that for fine-tuning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Existing RAG platforms make it easy to get set up and running with RAG.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning requires more investment in time to get a model ready for production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning offers much more control than RAG in terms of the model or framework
    we use at the end.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RAG and fine-tuning can be used together.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
