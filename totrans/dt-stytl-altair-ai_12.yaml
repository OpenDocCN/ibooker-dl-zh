- en: 10 Common issues while using generative AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hallucination, bias, and copyright
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guidelines for using generative AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crediting the sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your data story is now ready. However, before disseminating it to your audience,
    you should reflect on the possible issues associated with using generative AI.
    We will not discuss the technical details of these issues, but the concepts described
    will help you complete your overall understanding of generative AI. For more details,
    you are encouraged to read some of the detailed books on the topic—some of which
    are listed in the references of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first part of this chapter, we’ll focus on hallucinations, bias, and
    copyright. Next, we will focus on the guidelines for using generative AI. Finally,
    we’ll see how to correctly credit the sources, including data sources and generative
    AI. Before disseminating the story, let’s start with the first point: hallucination,
    bias, and copyright.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Hallucination, bias, and copyright
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI is not an omniscient oracle but is the result of human effort,
    and as such, it reflects the characteristics of humanity, including intelligence,
    scientific progress, and so on but also discrimination, inequalities, and injustices.
    Generative AI, while a product of human ingenuity and scientific advancement,
    inherently carries the biases and limitations of the data it is trained on, reflecting
    not only our knowledge but also our societal and historical prejudices. It’s crucial
    to remember that these systems are limited by their training data and algorithms
    and do not possess any innate understanding of consciousness.
  prefs: []
  type: TYPE_NORMAL
- en: All this, everything that humanity is, is therefore reflected in AI, both what
    is good and what is less good in humanity. For this reason, when generative AI
    is used as a work tool, particularly in data storytelling, it must always be handled
    carefully, with special attention paid to the benefits and possible damage it
    could cause to the most vulnerable people. Generative AI products have the potential
    to generate outputs biased against minority populations and can even be used to
    build manipulated stories with a plausible appearance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a practical idea of discrimination that may occur inadvertently while
    using Generative AI, consider the following prompt for DALL-E: *a photo of a woman
    wearing a red dress*. Figure 10.1 shows a possible output generated by DALL-E.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 A photo of a woman wearing a red dress
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Now, consider this prompt for DALL-E: *a woman wearing an orange sweater drinking
    a coffee*. Figure 10.2 shows a possible output generated by DALL-E.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 A woman wearing an orange sweater drinking a coffee
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Finally, consider the following prompt: *a man sitting on a yellow chair*.
    Figure 10.3 shows a possible output generated by DALL-E.'
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 A man sitting on a yellow chair
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We may continue generating more and more images, but we will probably always
    obtain similar results. What do figures 10.1, 10.2, and 10.3 have in common? The
    problem is that almost all the represented persons have dark hair (with the possible
    exception of the woman in image number 4 in figure 10.2). This may be a minor
    problem if we generate a small number of images. However, if we use DALL-E to
    generate images at a large scale, using only dark-haired people may generate a
    sort of bias against people with lighter hair.
  prefs: []
  type: TYPE_NORMAL
- en: This example shows a simple case of generative AI being partial. The problem
    is probably in the data used to train the generative model, which may contain
    more dark-haired than blonde-haired people. This is a relatively benign example,
    but it clearly illustrates that you may need to be mindful of bias and partiality
    in AI outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenge 1: Other types of discrimination'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Can you find other kinds of discrimination while generating images with people
    in DALL-E? For example, try to generate a woman with blonde hair. What kind of
    stereotype would you obtain?
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenge 2: Color and mood'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'DALL-E associates color and mood. For example, try the following prompts: (1)
    a woman wearing a yellow sweater, (2) a woman wearing a blue dress. Do you obtain
    different moods, although you didn’t specify any?'
  prefs: []
  type: TYPE_NORMAL
- en: These issues caused by generative AI depend on different causes, such as AI
    hallucinations, bias, and copyright. We will not delve into the technical details
    related to these issues; you can read the resources described in this chapter’s
    references for more information. In the remainder of this section, we will briefly
    describe the potential issues generative AI may introduce. Let’s start with hallucinations.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.1 AI hallucinations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Hallucination* happens when generative AI generates content that does not
    correspond to reality. Hallucinations within AI can create misleading or entirely
    fabricated data. Hallucination is a problem brought about by generative AI’s very
    design, in the sense that the large language models (LLMs) behind generative AI
    are generated without any communication of intent. It simply generates a statistically
    more probable text given the training dataset. Additionally, for instance, if
    a generative AI model is primarily trained on datasets containing English-language
    texts, it might struggle with accurately understanding or generating content in
    less-represented languages, reflecting a statistical bias toward English. Hallucinations
    can lead to ethical problems due to their potential misuse for generating content
    for user manipulation.'
  prefs: []
  type: TYPE_NORMAL
- en: Hallucination in generative AI can manifest in various ways. For example, a
    data story completely generated by AI could describe a nonexistent political scandal,
    potentially misleading the public and impacting decision making based on false
    information. To mitigate the hallucination problem, we always recommend having
    a *human-in-the-loop validation*. Before incorporating any AI-generated content
    in your story, please judge and review it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenge 3: Joking with generative AI'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Just for fun, consider the swimming pool case study in chapter 2\. As a quick
    reminder, the case study focused on the possibility of building a new swimming
    pool in a Portuguese hotel. Use the following prompt to generate the next steps:
    *Consider the following scenario: the case study focused on the possibility of
    building a new swimming pool in a Portuguese hotel. The data story showed an increasing
    number of tourists in Portugal in recent years. Write some hallucinated next steps.*
    What do you obtain?'
  prefs: []
  type: TYPE_NORMAL
- en: 'To mitigate hallucinations, you can try to set the following parameters in
    your prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Temperature*—The temperature controls the degree of randomness applied during
    the output generation process. It allows users to tailor the generated content’s
    level of creativity and unpredictability. It ranges from 0 for more structured
    and predictable outputs to 2 for more creative and unexpected results. The default
    value rests at 1\. To introduce temperature in your prompt, simply add the text
    *set temperature = N* (e.g., *use temperature = 2*). We can use a lower temperature
    value to reduce the probability that the model hallucinates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Top P*—Top P is also known as nucleus sampling or penalty-free sampling. It
    helps to control the diversity of the generated text. Use this technique to generate
    responses that don’t completely deviate from the topic. The range is between 0
    and 1\. A higher top P makes the output more diverse, while a lower value makes
    the model more deterministic. The default value is 1\. To introduce top P in your
    prompt, add the text *set top P = N* (e.g., *use top P = 1*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Usually, you set one parameter per prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenge 4: Setting temperature and P value'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Set the temperature or the top P value to 0 in the prompt of challenge 3, and
    then compare the new result with the original output of challenge 3\. Do you notice
    some differences?
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find a detailed description of how to set the temperature and the top
    P parameters in my blog post, “How to Improve Your ChatGPT Outputs Using Configuration
    Parameters” ([https://mng.bz/pp4G](https://mng.bz/pp4G)). Now that we’ve covered
    hallucinations, let’s move to the next problem: bias.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.2 Bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*Bias* is a systematic and often unconscious inclination, prejudice, or tendency
    that influences decision making, actions, perceptions, or judgments in favor of
    or against a particular person, group, object, or idea. Bias relies on human beliefs,
    stereotypes, or discrimination. Since LLMs are trained on datasets mostly created
    by humans, inevitably, these datasets contain bias. Bias is intrinsic to humans,
    so we can’t remove it from our datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Even in a hypothetical scenario where people are equal and free, and there is
    no discrimination and war, bias may occur. In fact, *bias is multifaceted*. Bias
    in AI is not limited to negative topics, like discrimination or war. Bias can
    also be in the form of cultural preferences, idiomatic expressions, or even what
    is considered “normal.” An LLM trained on data from even an ideal world might
    still develop biases based on what is prevalent or dominant in that data.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, even with ideal data, LLMs might still develop biases, due to their
    design, the algorithms they use, or the inherent limitations in understanding
    and processing human language. In other words, our LLM could still exhibit bias
    even in this hypothetical scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different perspectives to classify bias, such as those proposed by
    Baer in his book, *Understand, Manage, and Prevent Algorithmic Bias* (Apress,
    2019). One possible approach classifies bias into the following types:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Data bias*—This refers to the presence of bias in the training set. It derives
    from different causes, such as the underrepresentation of some groups in the training
    set. LLMs are trained with data extracted from the internet. However, the text
    on the web is written by a small percentage of humans. Tons of books representing
    the knowledge of all humankind from their beginning to today lie in libraries
    and are not available on the internet. This means that although big data is used
    to train LLMs, size does not guarantee diversity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Algorithm bias*—This type of bias derives from the assumptions and decisions
    made during the design, coding, or implementation of machine learning (ML) algorithms.
    This bias can arise due to feature selection, model complexity, and other technical
    problems related to the algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Measurement bias*—This occurs when the methods or tools used to collect data
    systematically misrepresent or distort the information being gathered. This bias
    can originate from different factors, such as instruments, human observers, and
    so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At the time of writing this book, there is no definitive solution to remove
    bias from generative AI tools. However, some possible techniques to mitigate bias
    could include data cleaning and balancing, inserting a human in the middle, model
    evaluation, and so on. Removing bias from generative AI would also mean first
    removing it from humanity, which would be significant progress for the world but
    is unlikely to happen anytime soon. Anyway, we could mitigate bias by always paying
    attention to the generated output and anchoring our data story to an ethical framework,
    as explained in the previous chapter. Having considered the problem of bias, let’s
    move to the next problem: copyright.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.1.3 Copyright
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generative AI models have been trained on huge quantities of data, derived especially
    from content shared freely in the public space. However, the creators of the original
    datasets used to train the models do not allow access to them, so we don’t know
    whether proprietary data has also been used to train the models. For this reason,
    AI-generated content might raise questions about intellectual property rights
    and ownership. Copyright questions may be connected to the fact that generative
    AI models are black boxes and the data used to train them are unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider an AI system trained on an extensive database of music
    that generates a piece similar to an existing copyrighted song. Determining the
    original creator becomes complex, raising questions about the ownership of AI-generated
    content and the rightful attribution of intellectual property, potentially leading
    to legal disputes between creators and AI systems. Before using generative AI,
    you should always understand copyright law, use clear licensing agreements for
    data sources, create original datasets, emphasize attribution and acknowledgment,
    implement copyright filters, and seek regular legal consultation.
  prefs: []
  type: TYPE_NORMAL
- en: All the issues described in this book remain unsolved at the time of writing.
    For this reason, I recommend consistently controlling the output produced by generative
    AI when using it for data storytelling and, in general, for all the application
    fields. It’s your responsibility to use generative AI ethically, ensuring that
    everyone is treated equally, minority populations are not underrepresented, and
    so on. To help you control the generative AI output and modify it if needed, you
    can apply the common guidelines for ethically using generative AI. So, let’s move
    on and learn them!
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Guidelines for using generative AI
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many initiatives exist to regularize the use of AI and generative AI in all
    domains, such as the European Union AI Act (European Commission, 2021) and the
    White House’s Executive Order on the Safe, Secure, and Trustworthy Development
    and Use of Artificial Intelligence (White House, 2023). You can find the links
    to these documents at the end of the chapter, in the references section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Referring to the field of data storytelling, following these guidelines means
    respecting human values. The UNESCO AI ethics guidelines emphasize four core values:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Human rights and dignity*—Every data story, including the pieces generated
    through generative AI, must respect human rights and dignity. The UNESCO guidelines
    say explicitly that “no human being or human community should be harmed or subordinated,
    whether physically, economically, socially, politically, culturally or mentally,
    during any phase of the life cycle of AI systems.”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Peaceful and just societies*—AI should be used to foster harmony and equity
    within communities, promoting fairness, transparency, and accountability in decision-making
    processes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Diversity and inclusiveness*—AI should respect the richness of human diversity
    in all its forms, including but not limited to race, gender, ethnicity, culture,
    and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Environmental flourishing*—AI should prioritize sustainability and contribute
    positively to environmental preservation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can apply the previous guidelines to check if AI-generated content is ethically
    correct. For all AI-generated content, we should answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Does the AI-generated content respect human rights?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the AI-generated content respect society?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the AI-generated content respect diversity and inclusiveness?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the AI-generated content respect the environment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To understand how to apply these guidelines to a practical data story, consider
    the case study we analyzed in chapter 4, related to selecting the optimal sports
    disciplines to train in to achieve good results in upcoming competitions. As next
    steps, we proposed to invest in cycling and rowing. Figure 10.4 shows the final
    data story.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 The sports disciplines data story described in chapter 4
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Let’s look at each part generated by AI:'
  prefs: []
  type: TYPE_NORMAL
- en: '*General title*—Unlock the Potential: Invest in Rowing and Cycling for Maximum
    Returns!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Image 1*—The white man practicing rowing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Image 2*—The white woman practicing cycling'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The only problem with the AI-generated content for this scenario regards diversity
    and inclusiveness. The two images, in fact, describe two white-skinned individuals.
    To make the data story fit the ethical guidelines, we could replace one of the
    two images with an individual of another ethnicity.
  prefs: []
  type: TYPE_NORMAL
- en: Exercise 1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Consider the case study described in chapter 5 on homelessness, shown in figure
    10.5\. The case study focused on searching for funds to finance a project on reducing
    homelessness in Italy. Now, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Identify the AI-generated content.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each element of AI-generated content, answer the following questions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the AI-generated content respect human rights?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the AI-generated content respect society?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the AI-generated content respect diversity and inclusiveness?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Does the AI-generated content respect the environment?
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![figure](../Images/10-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 The homelessness case study described in chapter 5
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Now that you have learned the guidelines for ethically using generative AI,
    let’s move on to the following step: determining your role while using it.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Crediting the sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Crediting the sources* means referencing the sources used in a data story.
    This is particularly important because it allows you to recognize the work done
    by others. It also adds credibility to the story, as the audience can personally
    check the sources used in the story. What types of sources should be credited?
    In general, any source used to make the story should be credited, but particularly
    the following sources:'
  prefs: []
  type: TYPE_NORMAL
- en: The data source
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fact that generative AI was used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any documents used for fine-tuning or retrieval augmented generation (RAG)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While you can use your creativity to place credits wherever you want, traditionally,
    we add credits to a data story in one of four places:'
  prefs: []
  type: TYPE_NORMAL
- en: Under the title/subtitle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the main chart
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Under the next steps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sideways
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s investigate each of these separately.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.1 Under the title or subtitle
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Placing the credits under the title or subtitle generates a sense of trust in
    the audience from the story’s beginning. Figure 10.6 shows an example of credits
    placed under the title or subtitle.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 An example of a data story with credits under the title or subtitle
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Use this placement if you want your audience to know the sources from the story’s
    beginning. Although this placement may generate trust, it could also be distracting,
    since the audience may leave your story to search for the sources.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.2 Under the main chart
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Placing the credits under the main chart involves adding a detail to the main
    point of the story. This helps reinforce the essential points of the story. Figure
    10.7 shows an example of credits placed under the main chart. Use this placement
    if you want to reinforce the main message of your chart.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 An example of a data story with credits under the main chart
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 10.3.3 Under the next steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this case, credit the sources at the end of your story, as an appendix to
    the next steps, as shown in figure 10.8\. Use this placement if you want to reinforce
    the next steps of your story.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 An example of a data story with credits under the next steps
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 10.3.4 Sideways
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Placing credits sideways means considering them as external to the main data
    story workflow. You can place credits either on the left or on the right, as shown
    in figure 10.9.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 An example of a data story with credits on the left and the right
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Use this placement to keep credits external to your main data story workflow
    and keep the audience concentrated on the story. Now that we’ve considered the
    various places you can place credits, let’s move on to how to implement credits
    practically.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.5 Implementing credits in Altair
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To implement credits in Altair, use `mark_text()` with a smaller font than the
    one you used for the main story. Optionally, you can include a hyperlink to the
    original source.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the case study on sports disciplines described in chapter 4 and shown
    in figure 10.4\. Let’s credit DALL-E for images and ChatGPT for the title. Add
    the following text as credits: *Images: source DALL-E. Title: source ChatGPT*.'
  prefs: []
  type: TYPE_NORMAL
- en: We will place credits on the left side. You can find the implemented code in
    the GitHub repository for the book under 10/crediting/left-sideways. The following
    listing shows the code to implement the credits section on the left.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 10.1 Adding the credits on the left
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note  Use `mark_text()` to add credits. Use the `angle` attribute to rotate
    the text 270 degrees. Adapt`x` and `y` to the chart. Try different values to obtain
    the best result.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, you can ask Copilot to generate the code for you. Figure 10.10 shows
    the resulting chart.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-10.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 The sports disciplines case study described in chapter 4, with
    credits on the left
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Exercise 2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Modify the previous example by implementing credits under the title, as shown
    in figure 10.11\.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/10-11.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 The sports disciplines case study described in chapter 4, with
    credits under the title
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You can find the solution to this exercise in the GitHub repository for the
    book under 10/crediting/under-the-title.
  prefs: []
  type: TYPE_NORMAL
- en: 'Challenge 5: Comparing Reading Flows'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Compare figures 10.10 and 10.11\. Can you distinguish any difference in terms
    of the reading flow?
  prefs: []
  type: TYPE_NORMAL
- en: So far, you have learned how to credit sources in your data story. In the next
    chapter, you’ll see how to export the final story.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using generative AI in any application may surface different issues, such as
    bias and discrimination. Thus, it is very important to review the content provided
    by generative AI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using generative AI ethically means respecting people, society, and the environment,
    according to UNESCO principles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before publishing your data story, make sure to credit sources. It’s a way to
    recognize the work done by others and to generate a greater sense of trust in
    your audience
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Generative AI issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Baer, T. (2019). *Understand, Manage, and Prevent Algorithmic Bias*. Apress.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tomczak, J. M. (2022). *Deep Generative Modeling*. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethics and AI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: EU AI Act. (2021). [https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52021PC0206](https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52021PC0206).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White House Executive Order on the Safe, Secure, and Trustworthy Development
    and Use of Artificial Intelligence. (2023). [https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: UNESCO Recommendation on the Ethics of Artificial Intelligence. (2022). [https://unesdoc.unesco.org/ark:/48223/pf0000381137](https://unesdoc.unesco.org/ark:/48223/pf0000381137)
    .
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
