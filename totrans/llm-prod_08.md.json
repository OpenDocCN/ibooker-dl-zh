["```py\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport numpy as np\nfrom numba import jit\nfrom matplotlib import pyplot as plt\nimport time\nfrom datetime import timedelta\nimport pandas as pd\nfrom collections import OrderedDict\nfrom itertools import cycle\nfrom transformers import AutoTokenizer\nfrom sentencepiece import SentencePieceProcessor\nfrom datasets import load_dataset\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ndevice_cap = torch.cuda.get_device_capability()\ndevice_type = \"cuda\" if \"cuda\" in device else \"cpu\"\ntorch.cuda.set_device(device)\ntorch.manual_seed(8855)\nprint(torch.__version__)\nprint(device, device_cap)\n# 2.1.0+cu121\n# cuda:0 (8,6)\n\ntokenizer = AutoTokenizer.from_pretrained(\"./llama3/\")      #1\ntokenizer.add_special_tokens({\"pad_token\": \"<PAD>\"})\n# tokenizer.pad_token = tokenizer.eos_token           #2\n\nvocab = tokenizer.vocab\n\ndef encode(example):\n    return tokenizer.encode(example, return_tensors=\"pt\")\n\ndef decode(example):\n    return tokenizer.batch_decode(\n    example,\n    skip_special_tokens=False,\n    clean_up_tokenization_spaces=True,\n    )[0]\n\nprint(f\"Vocab Size: {len(vocab)}\")\ndecode(\n    encode(\n    \"\"\"hello I am a specifically designed long sentence\n       to make sure this is working not only adequately,\n       but good enough for our batch functions\"\"\"\n    )\n)\n# Vocab Size: 32001\n#'<s> hello I am a specifically designed long sentence to make sure this is\nworking not only adequately, but good enough for our batch functions'\n\nMASTER_CONFIG = {\n    \"vocab_size\": len(vocab),\n    \"batch_size\": 16,\n    \"context_window\": 32,\n    \"d_model\": 288,\n    \"hidden_dim\": 768,\n    \"epochs\": 1000,\n    \"log_interval\": 50,\n    \"n_heads\": 6,\n    \"n_layers\": 6,\n}\nGLOBAL_KEEP_TRACK = []\n```", "```py\ndataset = load_dataset(      #1\n    \"text\",\n    data_files={\n        \"train\": [\"../../data/TinyStoriesv1andv2-train.txt\"],\n        \"val\": [\"../../data/TinyStoriesv1andv2-valid.txt\"],\n    },\n    streaming=True,\n)\n```", "```py\nclean_dataset = dataset.filter(lambda example: len(example[\"text\"]) > 2)   #1\n\nprompt = \"Write a short story. Possible Story: \"\ntokenized_prompt = tokenizer(prompt, return_tensors=\"pt\").input_ids\n\nencoded_dataset = clean_dataset.map(\n    lambda examples: tokenizer(\n      [prompt + x for x in examples[\"text\"]],\n       padding=True,\n       return_tensors=\"pt\",\n    ),\n    batched=True,\n)\ntrain_data = iter(encoded_dataset[\"train\"].shuffle())\nval_data = iter(encoded_dataset[\"val\"].shuffle())\ntrain_data = cycle(train_data)\nval_data = cycle(val_data)\n```", "```py\n# @torch.compile      #1\ndef get_batches(\n    data,\n    batch_size,\n    context_window,\n    config=MASTER_CONFIG,\n    debug=False,\n):\n    x = []\n    y = []\n    for _ in range(\n        batch_size    #2\n    ):\n        batch_data = next(data)\n\n        ix = torch.randint(         #3\n            0, len(batch_data[\"input_ids\"]) - context_window - 1, (2,)\n        )\n        batch_x = torch.stack(\n            [batch_data[\"input_ids\"][i : i + context_window] for i in ix]\n        ).long()\n        batch_y = torch.stack(\n            [\n                batch_data[\"input_ids\"][i + 1 : i + context_window + 1]\n                     for i in ix\n            ]\n        ).long()\n        x.append(batch_x)\n        y.append(batch_y)\n    x = torch.cat((x), 0).to(device)\n    y = torch.cat((y), 0).to(device)\n    return x, y\n```", "```py\n@torch.no_grad()\ndef get_loss(model, lora=False, config=MASTER_CONFIG):\n    out = {}\n    model.eval()\n    for name, split in zip([\"train\", \"val\"], [train_data, val_data]):\n        losses = []\n        for _ in range(10):\n                xb, yb = get_batches(\n               split,\n               config[\"batch_size\"],\n                 config[\"context_window\"],\n                )\n            _, loss = model(xb, yb)\n            losses.append(loss.item())\n        out[name] = np.mean(losses)\n    model.train()\n    return out\n```", "```py\n@torch.inference_mode()\ndef generate(\n    model,\n    config=MASTER_CONFIG,\n    temperature=1.0,\n    top_k=None,\n    max_new_tokens=30,\n    lora=False,\n):\n    idx_list = [tokenized_prompt] * 5\n    idx = torch.cat((idx_list), 0).long().to(device)\n    for _ in range(max_new_tokens):\n        logits = model(idx[:, -config[\"context_window\"] :])     #1\n        last_time_step_logits = logits[\n            :, -1, :\n        ]                      #2\n\n        last_time_step_logits = last_time_step_logits / temperature\n        if top_k is not None:\n            v, _ = torch.topk(\n                last_time_step_logits,\n                min(top_k, last_time_step_logits.size(-1)),\n            )\n            last_time_step_logits[\n                last_time_step_logits < v[:, [-1]]\n            ] = -float(\"Inf\")\n        p = F.softmax(\n            last_time_step_logits, dim=-1\n        )                                #3\n        idx_next = torch.argmax(\n            p, dim=-1, keepdims=True\n        )                                 #4\n        idx = torch.cat([idx, idx_next], dim=-1)  # append to the sequence\n    return [tokenizer.decode(x) for x in idx.tolist()]\n```", "```py\nclass SimpleFeedForwardNN(nn.Module):\n    def __init__(self, config=MASTER_CONFIG):\n        super().__init__()\n        self.config = config\n\n        self.embedding = nn.Embedding(\n            config[\"vocab_size\"], config[\"d_model\"]\n        )\n        self.linear = nn.Sequential(\n            nn.Linear(config[\"d_model\"], config[\"d_model\"]),\n            nn.ReLU(),\n            nn.Linear(config[\"d_model\"], config[\"vocab_size\"]),\n        )\n\n        print(\n            f\"model params: {sum([m.numel() for m in self.parameters()])}\"\n        )\n\n    def forward(self, idx, targets=None):\n        x = self.embedding(idx)\n        logits = self.linear(x)\n\n        if targets is not None:\n            loss = F.cross_entropy(\n                logits.view(-1, self.config[\"vocab_size\"]),\n                targets.view(-1),\n                ignore_index=tokenizer.pad_token_id,\n                # reduction=\"sum\",\n            )\n            return logits, loss\n\n        else:\n            return logits\n\nmodel = SimpleFeedForwardNN(MASTER_CONFIG).to(device)\nopt_model = torch.compile(model)           #1\n```", "```py\ndef train(\n    model,\n    optimizer,\n    scheduler=None,\n    data=None,\n    config=MASTER_CONFIG,\n    lora=False,\n    print_logs=False,\n):\n    losses = []\n    start_time = time.time()\n    for epoch in range(config[\"epochs\"]):\n        try:\n            optimizer.zero_grad()\n\n                xs, ys = get_batches(\n                    data, config[\"batch_size\"], config[\"context_window\"]\n                )\n                  for i in range(1, config[‘context_window’]+1):\n                          x = xs[:i]\n                          y = ys[:i]\n                    logits, loss = model(xs, targets=ys)\n                    loss.backward()\n                    optimizer.step()\n\n                    if scheduler:\n                            scheduler.step()\n\n            if epoch % config[\"log_interval\"] == 0:\n                batch_time = time.time() - start_time\n                x = get_loss(model, lora=lora)\n                losses += [x]\n                if print_logs:\n                    print(\n                      f\"\"\"Epoch {epoch} |\n                         train loss {x['train']:.3f} |\n                         val loss {x['val']:.3f} |\n                         Time {batch_time:.3f} |\n                         ETA: {timedelta(seconds=(batch_time * (config\n                            ['epochs'] - epoch)/config['log_interval']))}\"\"\"\n                    )\n                start_time = time.time()\n\n                if scheduler:\n                    print(\"lr: \", scheduler.get_last_lr())\n        except StopIteration:\n            print(f\"Reached end of dataset on step {epoch}\")\n            break\n\n    GLOBAL_KEEP_TRACK.append(\n       f\"{type(model).__name__} {sum([m.numel() for m in \nmodel.parameters()])} Params | Train: {losses[-1]['train']:.3f} | Val: \n{losses[-1]['val']:.3f}\"\n    )\n    print(\n        f\"training loss {losses[-1]['train']:.3f} | validation loss: \n{losses[-1]['val']:.3f}\"\n    )\n    return pd.DataFrame(losses).plot(xlabel=”Step // 50”, ylabel=”Loss”)\n\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n)\ntrain(model, optimizer, data=train_data, print_logs=True)\n#Epoch 0 | train loss 10.365 | val loss 10.341 | Time 0.122 | ETA:\n 0:00:02.431240\n#training loss 4.129 | validation loss: 4.458\n```", "```py\ngenerate(model, config=MASTER_CONFIG)\n# '<s> Write a short story. Possible Story: 3 together thisar andze Lily \nsaid exciteded and smiled. Everything because he wasning loved to the time, \nhe did not find like to',\n\nfor i in GLOBAL_KEEP_TRACK:\n    print(i)\n# SimpleFeedForwardNN 18547809 Params | Train: 4.129 | Val: 4.458\n```", "```py\nclass LlamaBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n\n        self.rms = RMSNormalization(\n            (config[\"d_model\"], config[\"d_model\"])\n        )                                                                   #1\n\n        self.attention = RoPEMaskedMultiheadAttention(config).to(device)   #1\n        self.feedforward = nn.Sequential(\n            nn.Linear(config[\"d_model\"], config[\"hidden_dim\"]),\n            SwiGLU(config[\"hidden_dim\"]),                                  #1\n            nn.Linear(config[\"hidden_dim\"], config[\"d_model\"]),\n        )\n```", "```py\n    def forward(self, x):\n        x = self.rms(x)\n        x = x + self.attention(x)\n\n        x = self.rms(x)\n        x = x + self.feedforward(x)\n        return x\n```", "```py\nclass SimpleLlama(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n\n        self.embedding = nn.Embedding(\n            config[\"vocab_size\"], config[\"d_model\"]\n        )\n        self.llama_blocks = nn.Sequential(\n            OrderedDict(\n                [\n                    (f\"llama_{i}\", LlamaBlock(config))    #1\n                    for i in range(config[\"n_layers\"])\n                ]\n            )\n        )\n\n        self.ffn = nn.Sequential(\n            nn.Linear(config[\"d_model\"], config[\"d_model\"]),\n            SwiGLU(config[\"d_model\"]),                           #2\n            nn.Linear(config[\"d_model\"], config[\"vocab_size\"]),\n        )\n\n        print(\n            f\"model params: {sum([m.numel() for m in self.parameters()])}\"\n        )\n\n    def forward(self, idx, targets=None):\n        x = self.embedding(idx)\n        x = self.llama_blocks(x)     #3\n        logits = self.ffn(x)\n\n        if targets is None:\n            return logits\n\n        else:\n            loss = F.cross_entropy(\n                logits.view(-1, self.config[\"vocab_size\"]),\n                targets.view(-1),\n                ignore_index=tokenizer.pad_token_id,\n            )\n            return logits, loss\n```", "```py\nMASTER_CONFIG[\"epochs\"] = 1000\nMASTER_CONFIG[\"batch_size\"] = 16\nMASTER_CONFIG[\"d_model\"] = 768\nMASTER_CONFIG[\"n_layers\"] = 8\nMASTER_CONFIG[\"context_window\"] = 128\n\nllama = SimpleLlama(MASTER_CONFIG).to(device)\n\nllama_optimizer = torch.optim.AdamW(\n    llama.parameters(),\n    betas=(0.9, 0.95),\n    weight_decay=1e-1,\n    eps=1e-9,\n    lr=5e-4,\n)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    llama_optimizer, 1000, eta_min=1e-5\n)                                              #1\n#Epoch 0 | train loss 10.321 | val loss 10.316 | Time 0.622 | ETA: \n0:00:12.439990\n#lr:  [0.0004999987909744553]\n#training loss 6.216 | validation loss: 6.046 \ngenerate(\n    llama,\n    config=MASTER_CONFIG,\n    temperature=1.0,\n    top_k=25,\n    max_new_tokens=50,\n)\n#'<s> Write a short story. Possible Story: the Story there One.t day. Back\n the, went to: her they Possible|. to and a said saw They:. be the She.. a. \nto They. they. to and to for He was a in with',',\n\nfor i in GLOBAL_KEEP_TRACK:\n    print(i)\n#SimpleFeedForwardNN 18547809 Params | Train: 4.129 | Val: 4.458\n#SimpleLlama 187827210 Params | Train: 6.216 | Val: 6.046\n```", "```py\nllama.to(\"cpu\")\nqconfig_dict = {\n    torch.nn.Embedding: torch.quantization.float_qparams_weight_only_qconfig,\n    torch.nn.Linear: torch.quantization.default_dynamic_qconfig,\n}\ndynamic_quantized_llama = torch.quantization.quantize_dynamic(    #1\n    llama, qconfig_dict, dtype=torch.qint8\n)\n#SimpleLlama size: 716.504MB\n#SimpleLlama size: 18.000MB\n```", "```py\nclass LoRALayer(nn.Module):                            #1\n    def __init__(self, in_dim, out_dim, rank, alpha):\n        super().__init__()\n        standard_deviation = 1 / torch.sqrt(torch.tensor(rank).float())\n        self.A = nn.Parameter(\n            torch.randn(in_dim, rank) * standard_deviation\n        )\n        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n        self.alpha = alpha\n\n    def forward(self, x):\n        x = self.alpha * (x @ self.A @ self.B)\n        return x\n\nclass LinearWithLoRA(nn.Module):\n    def __init__(self, linear, rank, alpha):\n        super().__init__()\n        self.linear = linear\n        self.lora = LoRALayer(\n            linear.in_features, linear.out_features, rank, alpha\n        )\n    def forward(self, x):\n        return self.linear(x) + self.lora(x)\n\nclass LlamaBlock(nn.Module):    #2\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n\n        self.rms = RMSNormalization(\n            (config[\"d_model\"], config[\"d_model\"])\n        ).to(device)\n\n        self.attention = RoPEMaskedMultiheadAttention(config).to(device)\n        self.feedforward = nn.Sequential(\n            LinearWithLoRA(config[\"d_model\"], config[\"d_model\"]),   #3\n            SwiGLU(config[\"d_model\"]),\n        ).to(device)\n\n    def forward(self, x):\n        x = self.rms(x)\n        x = x + self.attention(x)\n\n        x = self.rms(x)\n        x = x + self.feedforward(x)\n        return x\n\nclass SimpleLlama(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n\n        self.embedding = nn.Embedding(\n            config[\"vocab_size\"], config[\"d_model\"]\n        )\n        self.llama_blocks = nn.Sequential(\n            OrderedDict(\n                [\n                    (f\"llama_{i}\", LlamaBlock(config))\n                    for i in range(config[\"n_layers\"])\n                ]\n            )\n        )\n\n        self.ffn = nn.Sequential(\n            LinearWithLoRA(config[\"d_model\"], config[\"d_model\"]),        #4\n            SwiGLU(config[\"d_model\"]),\n            LinearWithLoRA(config[\"d_model\"], config[\"vocab_size\"]),     #5\n        )\n\n        print(\n            f\"model params: {sum([m.numel() for m in self.parameters()])}\"\n        )\n\n    def forward(self, idx, targets=None):\n        x = self.embedding(idx)\n        x = self.llama_blocks(x)\n        logits = self.ffn(x)\n\n        if targets is None:\n            return logits\n\n        else:\n            loss = F.cross_entropy(\n                logits.view(-1, self.config[\"vocab_size\"]),\n                targets.view(-1),\n                ignore_index=tokenizer.pad_token_id,\n                reduction=\"sum\",\n            )\n            return logits, loss\n\ndataset = load_dataset(     #6\n    \"text\",\n    data_files={\n        \"train\": [\"../../data/Lima-train.csv\"],\n        \"val\": [\"../../data/Lima-test.csv\"],\n    },\n    streaming=True,\n)\n\nencoded_dataset = dataset.map(\n    lambda examples: tokenizer(\n        examples[\"text\"],\n        padding=True,\n        max_length=128,\n        truncation=True,\n       \\]] return_tensors=\"pt\",\n    ),\n    batched=True,\n)\ntrain_data = iter(encoded_dataset[\"train\"].shuffle())\nval_data = iter(encoded_dataset[\"val\"].shuffle())\ntrain_data = cycle(train_data)\nval_data = cycle(val_data)\n\nllama.to(\"cpu\")          #7\nadd_lora(llama)\nllama.to(device)\n\nparameters = [{\"params\": list(get_lora_params(llama))}]     #8\nlora_optimizer = torch.optim.AdamW(parameters, lr=1e-3)     #9\n\ntrain(      #10\n    llama,\n    lora_optimizer,\n    scheduler,\n    data=train_data,\n    config=MASTER_CONFIG,\n    lora=True,\n    print_logs=True,\n)\n\nstate_dict = llama.state_dict()        #11\nlora_state_dict = {k: v for k, v in state_dict.items() if name_is_lora(k)}\ntorch.save(llama.state_dict(), \"./llama.pth\")\ntorch.save(lora_state_dict, \"./lora.pth\")\n```", "```py\n# Loading and Inferencing with LoRA\nadd_lora(llama)\n\n_ = llama.load_state_dict(lora_state_dict, strict=False)\n\nmerge_lora(llama)\n\ngenerate(llama)\n```", "```py\n#'<s> off It the played he had cry bird dayt didn pretty Jack. a she moved\nday to play was andny curiousTC bandierungism feel But'\n```", "```py\nfrom train_utils import FSDP_QLORA\n\ntrainer = FSDP_QLORA(\n    model_name='meta-llama/Llama-2-7b-hf',\n    batch_size=2,\n    context_length=2048,\n    precision='bf16',\n    train_type='qlora',\n    use_gradient_checkpointing=True,\n    dataset='guanaco',\n    reentrant_checkpointing=True,\n    save_model=True,\n    output_dir=”.”\n)\n\ntrainer.train_qlora()\n```", "```py\nfrom safetensors import safe_open\nimport torch\nfrom transformers import LlamaForCausalLM, BitsAndBytesConfig\nfrom peft import get_peft_model, LoraConfig, TaskType\n\ntensors = {}\nwith safe_open(\n    \"qlora_output/model_state_dict.safetensors\",\n    framework=\"pt\",\n    device=0\n) as f:\n    for k in f.keys():\n        tensors[k] = f.get_tensor(k)\n\nfor k in tensors:\n    if 'lora' not in k: tensors[k] = None\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=False,\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\nmodel = LlamaForCausalLM.from_pretrained(\n    \"meta-llama/Llama-2-7b-hf\",\n    use_cache=False,\n    quantization_config=bnb_config\n)\n\nfor param in model.parameters():\n    param.requires_grad = False\n\npeft_config = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    inference_mode=False,\n    r=64,\n    lora_alpha=16,\n    lora_dropout=0.1,\n    target_modules=[\n        \"k_proj\",\n        \"q_proj\",\n        \"v_proj\",\n        \"up_proj\",\n        \"down_proj\",\n        \"gate_proj\"\n    ]\n)\nmodel = get_peft_model(model, peft_config)\n\nlist(model.state_dict().keys())[:10]\n\nnew_sd = model.state_dict()\nfor k in new_sd:\n    if 'lora' in k:\n        new_sd[k] = tensors[k]\n\nmodel.load_state_dict(new_sd)\n\nmodel.save_pretrained(\"lora_adapters\")\n```", "```py\n$ git clone https://huggingface.co/spaces/your-username/your-space\n```", "```py\n$ git add files-you-need\n$ git commit -m \"Initial Commit\"\n$ git push\n```", "```py\n%pip install huggingface_hub -q\n\nfrom huggingface_hub import notebook_login, HfApi\n\nnotebook_login() #OR huggingface-cli login\n\napi = HfApi()\napi.create_repo(         #1\n    repo_id=\"your_username/your_repo\", repo_type=\"space\", space_sdk=\"gradio\"\n)\n\nstuff_to_save = [\n    \"llama.pth\",# Your model\n    \"lora.pth\",# Optional: Your LoRA\n    \"special_tokens_map.json\",\n    \"tokenizer_config.json\",\n    \"tokenizer.json\",\n    \"tokenizer.model\",\n    \"gradio_app.py\",\n]\nfor thing in stuff_to_save:\n    api.upload_file(\n        path_or_fileobj=f\"./llama2/{thing}\",\n        path_in_repo=thing,\n        repo_id=\"your_username/your_repo\",\n        repo_type=\"space\",\n    )\n```"]