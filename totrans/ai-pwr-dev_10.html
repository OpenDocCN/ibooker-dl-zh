<html><head></head><body>
  <h1 class="tochead" id="heading_id_2">7 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/><a id="idTextAnchor005"/><a id="idTextAnchor006"/>Coding infrastructure and managing deployments</h1>

  <p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="marker-143"/></p>

  <ul class="calibre5">
    <li class="co-summary-bullet">Creating a Dockerfile with the assistance of Copilot</li>

    <li class="co-summary-bullet">Drafting your infrastructure as code using large language models</li>

    <li class="co-summary-bullet">Managing Docker images with a container registry</li>

    <li class="co-summary-bullet">Harnessing the power of Kubernetes</li>

    <li class="co-summary-bullet">Releasing your code effortlessly using GitHub Actions</li>
  </ul>

  <p class="body">There is nothing more demoralizing than having an application sit unused. For this reason, fast-tracking a well-tested application to production is the stated goal of every competent developer. Because we spent the last chapter testing our product, it is now ready for launch.</p>

  <p class="body">This chapter will focus on that pivotal moment of transitioning from development to product launch. During this critical phase, understanding deployment strategies and best practices becomes essential to ensure a successful product launch.</p>

  <p class="body">With our application successfully secured and tested, it’s time to shift our attention toward launching the product. To this end, we will use the powerful capabilities of large language models (LLMs) to explore various deployment options tailored to cloud infrastructure. <a id="idIndexMarker001"/></p>

  <p class="body">By harnessing the power of LLMs and embracing their deployment options and methodologies, we can confidently navigate the complex landscape of launching our product, delivering a robust and scalable solution to our customers while using the benefits of cloud computing.</p>

  <p class="body">First, we will develop deployment files for Docker. We will explore how to create Docker images and define deployment files. Additionally, we will discuss best practices for containerizing our application and achieving seamless deployment.</p>

  <p class="body"><a id="marker-144"/>Next, we will use Terraform to define our infrastructure as code and automate the deployment of Elastic Compute Cloud (EC2) instances on Amazon Web Services (AWS). We will demonstrate how to write Terraform scripts to provision and deploy our application on EC2 instances, ensuring consistent and reproducible infrastructure setups.<a id="idIndexMarker002"/><a id="idIndexMarker003"/></p>

  <p class="body">Then we will utilize LLMs to deploy our application onto Kubernetes (AWS Elastic Kubernetes Service [EKS]/Elastic Container Service [ECS]). We will have GitHub Copilot create the appropriate Kubernetes deployment files to streamline our deployment process and efficiently manage our application’s lifecycle. Given the relative simplicity of our application, we will not need a Kubernetes package manager like Helm. However, as the complexities and dependencies of services grow, you may want to explore it as one option. Fortunately, Copilot can write Helm charts for you as well! <a id="idIndexMarker004"/><a id="idIndexMarker005"/><a id="idIndexMarker006"/></p>

  <p class="body">Finally, we will briefly showcase migrating from local to automated deployments using GitHub actions. We can automate our build and deployment processes by integrating LLMs with this widespread continuous integration and deployment (CI/CD) tool, ensuring faster and more efficient deployments. <a id="idIndexMarker007"/></p>

  <p class="fm-callout"><span class="fm-callout-head">NOTE</span> This chapter uses AWS as our cloud provider, but the principles and practices covered in the chapter can be adapted and applied to other cloud platforms and even on-premises infrastructure without virtualization (bare metal), allowing us to adapt and scale your product deployment strategy as your business needs evolve. You will find that by employing LLMs and using infrastructure as code, you can (partially) mitigate the vendor lock-in that is very common to cloud platforms.</p>

  <p class="body">Note that if you choose to deploy this (or any application) to AWS, there will be a cost associated with your activity. AWS and most cloud providers give you free trials to learn their platforms (Google Cloud Platform and Azure, for example), but once those credits have expired, you may get hit with a rather unexpectedly large bill. If you decide to follow along in this chapter, you need to set threshold alerts for an amount you can comfortably afford. Section 1.9 of Andreas Wittig and Michael Wittig’s <i class="fm-italics">Amazon Web Services in Action, Third Edition</i> (Manning, 2023; <a class="url" href="https://www.manning.com/books/amazon-web-services-in-action-third-edition">www.manning.com/books/amazon-web-services-in-action-third-edition</a>) is an excellent resource for setting up such a billing notification alert.<a id="idIndexMarker008"/><a id="idIndexMarker009"/><a id="idIndexMarker010"/><a id="idIndexMarker011"/><a id="idIndexMarker012"/><a id="idIndexMarker013"/></p>

  <h2 class="fm-head" id="heading_id_3">7.1 Building a Docker image and “deploying” it locally</h2>

  <p class="body">As you may remember from chapter 6, Docker is a containerization platform that allows you to run applications with little or no installation of an application (outside of Docker) in the traditional sense. Unlike a virtual machine, which simulates an entire operating system, a container shares the host system’s kernel (the core part of the operating system) and uses the host system’s operating system’s capabilities while isolating the application processes and file systems from the host. This lets you run multiple isolated applications on a single host system, each with its own environment and resource limits. Figure 7.1 gives you a sense of the relationship between the Docker runtime and the host.<a id="idIndexMarker014"/><a id="idIndexMarker015"/><a id="marker-145"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F01_Crocker2.png"/></p>

    <p class="figurecaption">Figure 7.1 Docker makes use of the host’s operating system while isolating each of the containers. This makes Docker containers lightweight compared to virtual machines, as they do not require a full OS to run.</p>
  </div>

  <p class="body">One of the more exciting features, from a production readiness perspective, is that Docker makes it easier to run applications that can self-heal in some sense. If they fail or fall over at runtime, you can configure them to restart without intervention. In this section, we will use Copilot to create the file (called a <i class="fm-italics">Dockerfile</i>) from which we will build our <i class="fm-italics">Docker image</i>.<a id="idIndexMarker016"/><a id="idIndexMarker017"/></p>

  <p class="fm-callout"><span class="fm-callout-head">Definition</span> <i class="fm-italics">Docker images</i> are like blueprints for Docker containers. They are portable, including all the dependencies (libraries, environment variables, code, etc.) required for the application to run.</p>

  <p class="body">Running Docker instances are called Docker <i class="fm-italics">containers</i>. Given their lightweight nature, we can run multiple containers on a single host without a problem. We can do this because the containerization technology shares the OS kernel, operating in an isolated user space. <a id="idIndexMarker018"/></p>

  <p class="fm-callout"><span class="fm-callout-head">NOTE</span> Originally, I wanted to use AWS CodeWhisperer as the LLM for this chapter. It seemed logical, given the intended cloud platform. However, at the time of this writing, AWS CodeWhisperer only supports programming in a programming language. It does not have facilities for infrastructure as code.</p>

  <p class="body">We will begin with the following prompt to have Copilot draft the Dockerfile for us:</p>
  <pre class="programlistinge"># Create a Dockerfile for this Python app. The main class is main.py. Use Python 3.10 and install
# the dependencies using the requirements.txt file in this directory. The app should run on port 8080.</pre>

  <p class="body"><a id="marker-146"/>You may be left with an empty file (other than this comment). Support for infrastructure as code is ever-evolving (not unlike the LLM ecosystem in general). According to Copilot Chat, GitHub Copilot is capable of creating a Dockerfile for you—but you have to goad it with the following steps:</p>

  <ol class="calibre9">
    <li class="fm-list-bullet">
      <p class="list">In the Dockerfile, type <code class="fm-code-in-text">FROM python:</code> and wait for Copilot to suggest a version of Python to use. Select the version you want to use.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Type <code class="fm-code-in-text">WORKDIR /app</code> to set the working directory for the container.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Type <code class="fm-code-in-text">COPY . /app</code> to copy the contents of your project into the container.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Type <code class="fm-code-in-text">RUN pip install --trusted-host pypi.python.org -r requirements.txt</code> to install the dependencies for your project.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Type <code class="fm-code-in-text">EXPOSE 8080</code> to expose port 8080 for the container.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Type <code class="fm-code-in-text">CMD ["python", "main.py"]</code> to specify the command to run when the container starts.</p>
    </li>
  </ol>

  <p class="body">Alternatively, you may want to copy and paste the same prompt that you previously wrote into the Dockerfile into the Copilot Chat prompt window. Copilot Chat will give you the desired content for the Dockerfile.</p>

  <p class="fm-code-listing-caption">Listing 7.1 Dockerfile to build a Docker image</p>
  <pre class="programlisting">FROM python:3.10-slim-buster
WORKDIR /app
COPY . /app
RUN pip install --trusted-host pypi.python.org -r requirements.txt
EXPOSE 8080
CMD ["python", "main.py"]</pre>

  <p class="body">With a Dockerfile, we will build an image for deploying and running our application. We can enter the following command to build our application (run from the directory where the Dockerfile lives, and do not forget the trailing dot). You will need internet access to download the dependencies and create the image:</p>
  <pre class="programlisting">docker build -t itam:latest .</pre>

  <p class="body">Building a Docker image can run for a few seconds to a few minutes, depending on which images and packages are installed on your system and your internet connection speed. Your patience will be rewarded, as you will shortly have an application you can install nearly anywhere from the lowliest commodity hardware to the most oversized hardware offered by your favorite cloud provider. Before running it anywhere, however, you need to try to get it running locally. If you’ve forgotten the command, Copilot Chat will happily and helpfully assist:</p>
  <pre class="programlisting">docker run -p 8000:8000 -d --name itam itam:latest</pre>

  <p class="body">You can confirm that your Docker container is running by issuing this command at the command line: <code class="fm-code-in-text">docker ps | grep itam</code>. You should see the running instance.<a id="idIndexMarker019"/><a id="idIndexMarker020"/></p>

  <h2 class="fm-head" id="heading_id_4">7.2 Standing up infrastructure by copiloting Terraform</h2>

  <p class="body"><a id="marker-147"/>Using a Docker image on your computer is useful when creating and testing an app. But when it comes time to launch your application, you need a machine with a little more heft than local computers. In this section, we’ll use GitHub Copilot to help us set up and control our AWS infrastructure by having Copilot write the requisite deployment descriptors for an infrastructure-as-code tool called Terraform. Terraform is made by HashiCorp and lets us write what we want our infrastructure to look like using a domain-specific language (DSL). This DSL saves us from having to understand all the complexities and intricacies that each cloud service provider uses to provision hardware. Additionally, it allows us to store and version our infrastructure using infrastructure as code.<a id="idIndexMarker021"/><a id="idIndexMarker022"/><a id="idIndexMarker023"/><a id="idIndexMarker024"/></p>

  <p class="body">To start, we want to create a file called ec2.tf and add the prompt to inform Copilot that we intend this to be a Terraform file and how we want our infrastructure stood up. Notice that Copilot needs us to enter the first word of a given line before it can be cajoled to continue.</p>

  <p class="fm-code-listing-caption">Listing 7.2 Example Terraform file, including instance size</p>
  <pre class="programlisting"><b class="fm-bold"># Create a Terraform file that provisions an AWS EC2 instance of type t2.micro, installs the Docker daemon, and returns the instance's hostname.</b>
provider "aws" {
  region = "us-west-2"
}
  
resource "aws_instance" "example" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t2.micro"
}</pre>

  <p class="body">You may find that Copilot skips over one small but crucial detail: it does not provide code for installing and provisioning Docker. Given that Docker is required for running our application, we need to correct this oversight. In fact, you may need to update the file manually to include the command to install Docker, like this:</p>
  <pre class="programlisting">resource "aws_instance" "example" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t2.micro"
  
   user_data = &lt;&lt;-EOF
                #!/bin/bash
                sudo yum update -y
                sudo yum install -y docker
                sudo service docker start
                sudo usermod -a -G docker ec2-user
                sudo docker run -d -p 80:80 nginx
                EOF
}</pre>

  <p class="body"><a id="marker-148"/>Copilot should produce a complete Terraform file that resembles the following listing. Your code probably does not exactly match the listing, but that’s fine as long as it contains the key features: the provider, the instance, the script to add the Docker daemon, the key pair, and the security group.</p>

  <p class="fm-code-listing-caption">Listing 7.3 Terraform file to create the smallest EC2 instance available</p>
  <pre class="programlisting"><b class="fm-bold"># Create a Terraform file that provisions an AWS EC2 instance of type t2.micro, installs the Docker daemon, and returns the hostname of the instance.</b>
provider "aws" {
  region = "us-west-2"
}
  
resource "aws_instance" "example" {
  ami           = "ami-0c55b159cbfafe1f0"
  instance_type = "t2.micro"
  
   user_data = &lt;&lt;-EOF
                #!/bin/bash
                sudo yum update -y
                sudo yum install -y docker
                sudo service docker start
                sudo usermod -a -G docker ec2-user
                sudo docker run -d -p 80:80 nginx
                EOF
    connection {
        type        = "ssh"
        user        = "ec2-user"
        private_key = file("~/.ssh/id_rsa")
        host        = self.public_ip
    }
  
    lifecycle {
        create_before_destroy = true
    }
  
    depends_on = [aws_security_group.allow_http]
}
  
resource "aws_security_group" "allow_http" {
  name        = "allow_http"
  description = "Allow HTTP inbound traffic"
  vpc_id      = "vpc-12345678"
  
  ingress {
    description = "HTTP from VPC"
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
    }
}
  
output "public_dns" {
  value = aws_instance.example.public_dns
    }</pre>

  <p class="body">If you are using the default Virtual Private Cloud (VPC), the <code class="fm-code-in-text">vpc_id</code> entry is not strictly necessary. You will find that many of the default configurations and conventions chosen by the AWS team make sense; if you have stricter security requirements, or if you know everything about your infrastructure and assume nothing, you might consider setting up a new VPC from scratch using Terraform. You need to change the key pair entry on line 21 to be a key pair to which you have access.<a id="idIndexMarker025"/><a id="marker-149"/></p>

  <p class="body">Once you have completed this file satisfactorily, run the <code class="fm-code-in-text">terraform init</code> command. This command initializes a new or existing Terraform working directory. It downloads and installs the required provider plugins and modules specified in your configuration files and gets everything ready to go.<a id="idIndexMarker026"/></p>

  <p class="body">Next you will have Terraform explain the changes that it intends to make. You do this with the <code class="fm-code-in-text">terraform plan</code> command. This command creates an execution plan for your infrastructure changes: it shows you what changes Terraform will make to your infrastructure when you apply your configuration files. The plan will show you which resources will be created, modified, or destroyed and any other changes that will be made to your infrastructure.<a id="idIndexMarker027"/></p>

  <p class="fm-callout"><span class="fm-callout-head">NOTE</span> You may get an error when running <code class="fm-code-in-text">terraform plan</code> for the first time: “Error: configuring Terraform AWS Provider: no valid credential sources for Terraform AWS Provider found.” You get this error when Terraform attempts to connect to AWS but cannot supply AWS with proper credentials. To address this problem, you will need to create (or edit) the file called ~/.aws/credentials and add your ITAM AWS Access Key ID and AWS Secret Access Key credentials. You can find more details on how to accomplish this correctly in section 4.2.2, “Configuring the CLI,” of <i class="fm-italics">Amazon Web Services in Action, Third Edition</i>.<a id="idIndexMarker028"/></p>

  <p class="body">Finally, to apply the Terraform changes, you use the <code class="fm-code-in-text">terraform apply</code> command. Terraform will then read the configuration files in the current directory and apply any changes to your infrastructure. If you have made any changes to your configuration files since the last time you ran <code class="fm-code-in-text">terraform apply</code>—for example, if you need to start up a new database instance or change the size of your EC2—Terraform will show you a preview of the changes that will be made and prompt you to confirm before applying the changes.<a id="idIndexMarker029"/></p>

  <p class="body">If you apply these changes, in a manner of minutes you will have a brand-new EC2 instance running in your VPC. However, this is only half of the equation. Having computing power at your fingertips is fantastic, but you need something to apply this power. In this case, we can use this EC2 instance to run our ISAM system. The following section briefly demonstrates transferring a locally built image to another machine.<a id="idIndexMarker030"/><a id="idIndexMarker031"/><a id="idIndexMarker032"/><a id="idIndexMarker033"/></p>

  <h2 class="fm-head" id="heading_id_5">7.3 Moving a Docker image around (the hard way)</h2>

  <p class="body"><a id="marker-150"/>First we will export a Docker image from our local machines and load it onto a remote machine. We will use the commands <code class="fm-code-in-text">docker save</code> and <code class="fm-code-in-text">load</code> to accomplish this. You can use the <code class="fm-code-in-text">docker save</code> command on your local machine to save the image to a tar archive. The following command will save the image to a tar archive named &lt;image-name&gt;.tar: <a id="idIndexMarker034"/><a id="idIndexMarker035"/><a id="idIndexMarker036"/><a id="idIndexMarker037"/><a id="idIndexMarker038"/><a id="idIndexMarker039"/></p>
  <pre class="programlisting">docker save -o &lt;image-name&gt;.tar &lt;image-name&gt;:&lt;tag&gt;</pre>

  <p class="body">Next, transfer the tar archive to the remote machine using a file transfer protocol such as Secure Copy Protocol (SCP) or Secure File Transfer Protocol (SFTP). You can use the <code class="fm-code-in-text">docker load</code> command on the remote machine to load the image from the tar archive: <code class="fm-code-in-text">docker load -i &lt;image-name&gt;.tar.</code> This will load the image into the local Docker image cache on the remote machine. Once the image has been loaded, use the <code class="fm-code-in-text">docker run</code> command to start the image and run the Docker container, as you did after you built it. Then add this image to your Docker compose file, in which you have the Postgres database and Kafka instances.<a id="idIndexMarker040"/><a id="idIndexMarker041"/></p>

  <p class="fm-callout"><span class="fm-callout-head">NOTE</span> This discussion of Terraform is heavily abridged. When you are ready to get serious with Terraform, your go-to resource should be Scott Winkler’s <i class="fm-italics">Terraform in Action</i> (Manning, 2021; <a class="url" href="https://www.manning.com/books/terraform-in-action">www.manning.com/books/terraform-in-action</a>).</p>

  <p class="body">This section examined how to package up images and load them on remote hosts. This process is scriptable, but with the advent of container registries, it is now easier than ever to manage deployments without slinging them all around the internet. In the next section, we will explore one such tool: Amazon’s Elastic Container Registry (ECR).</p>

  <h2 class="fm-head" id="heading_id_6">7.4 Moving a Docker image around (the easy way)</h2>

  <p class="body">Docker images, the blueprints for our containers, are a fundamental building block of containerized applications. Managing them correctly ensures that we maintain clean, efficient, and organized development and deployment workflows. Amazon ECR serves as a fully managed Docker container registry that makes it easy for developers to store, manage, and deploy Docker container images.<a id="idIndexMarker042"/><a id="idIndexMarker043"/><a id="idIndexMarker044"/></p>

  <p class="body">First, let’s dive into pushing Docker images to ECR. This process is vital to making your images accessible for use and deployment. We’ll walk through setting up your local environment, authenticating with ECR, and pushing your image. Before we can move an image to ECR, we must create a repository to house that image. This can be done from the AWS Management Console or, as we will do shortly, using the AWS command line interface (CLI). The command to create a new repository for an image is</p>
  <pre class="programlisting">aws ecr create-repository --repository-name itam</pre>

  <p class="body">Next you need to tag your Docker image with the ECR repository URL and the image name. You may want to call it <code class="fm-code-in-text">latest</code> or use semantic versioning. Tagging will allow you to easily roll back or forward versions of your system. Tag your application image <code class="fm-code-in-text">latest</code> using the following command:</p>
  <pre class="programlisting">docker tag itam:latest 
123456789012.dkr.ecr.us-west-2.amazonaws.com/itam:latest</pre>

  <p class="body">Now, authenticate Docker to the ECR registry using the <code class="fm-code-in-text">aws ecr get-login-password</code> command. This will generate a Docker <code class="fm-code-in-text">login</code> command that you can use to authenticate Docker to the registry. The command to log in is</p>
  <pre class="programlisting">aws ecr get-login-password --region us-west-2 | 
docker login --username AWS --password-stdin 
123456789012.dkr.ecr.us-west-2.amazonaws.com</pre>

  <p class="body">Finally, push the Docker image to the ECR registry using the <code class="fm-code-in-text">docker push</code> command:</p>
  <pre class="programlisting">docker push 123456789012.dkr.ecr.us-west-2.amazonaws.com/itam:latest</pre>

  <p class="body">Once the image is in your registry, your deployment options have greatly increased. You could, for example, write a bash script that will log on to the EC2 instance and perform a <code class="fm-code-in-text">docker pull</code> to download and run the image on that EC2. Alternatively, you may want to adopt a more bulletproof deployment pattern. In the next section, we’re going to walk through the process of setting up and launching our application on a powerful cloud service called Elastic Kubernetes Service (EKS). EKS is a managed Kubernetes service provided by AWS. Let’s dive in!</p>

  <h2 class="fm-head" id="heading_id_7">7.5 Deploying our application onto AWS Elastic Kubernetes Service</h2>

  <p class="body"><a id="marker-151"/>Kubernetes confers many benefits over simply running Docker images on EC2 instances. For one, managing and scaling our application becomes considerably more straightforward with Kubernetes. Also, with Kubernetes, we do not have to spend a lot of additional time thinking about what our infrastructure should look like. Plus, thanks to its automatic management of the lifecycles of its images, known as <i class="fm-italics">pods</i>, our application will essentially be self-healing. This means if something goes wrong, Kubernetes can automatically fix it, keeping our application running smoothly at all times. <a id="idIndexMarker045"/><a id="idIndexMarker046"/><a id="idIndexMarker047"/><a id="idIndexMarker048"/><a id="idIndexMarker049"/></p>

  <p class="body">First we need a deployment descriptor written in YAML (Yet Another Markup Language or YAML Ain’t Markup Language, depending on who you ask), which will describe the state we want our ITAM system to be in at all times. This file (typically called deployment.yaml) will provide the template against which Kubernetes will compare the current running system, making corrections as needed.</p>

  <p class="fm-code-listing-caption">Listing 7.4 Kubernetes deployment file for the ITAM system</p>
  <pre class="programlisting"><b class="fm-bold"># Create a Kubernetes deployment file for the itam application. The image name is itam:latest</b>
<b class="fm-bold"># The deployment will run on port 8000</b>
  
apiVersion: apps/v1
kind: Deployment
metadata:
  name: itam-deployment
  labels:
    app: itam
spec:
  replicas: 1
  selector:
    matchLabels:
      app: itam
  template:
    metadata:
      labels:
        app: itam
    spec:
      containers:
      - name: itam
        image: itam:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8000</pre>

  <p class="body"><a id="marker-152"/>This will not work, however. Kubernetes will not be able to find the image that we reference in the deployment descriptor file. To correct this, we need to tell Kubernetes to use our newly minted ECR. Fortunately, this is not as challenging as it may sound. We just have to update the image entry in our file to point to the ECR image, as well as grant EKS permissions to access ECR (okay, maybe it is a little trickier, but it is manageable).</p>

  <p class="body">First, update the deployment YAML to use the ECR image:</p>
  <pre class="programlisting">image: 123456789012.dkr.ecr.us-west-2.amazonaws.com/itam:latest. </pre>

  <p class="body">Then you would need to define a policy for EKS to use and apply the policy using either the AWS CLI or the Identity and Access Management (IAM) Management Console. Although applying the policy is (slightly) outside of the scope of this book, you can use Copilot to define it. The resulting policy will resemble the following listing. <a id="idIndexMarker050"/></p>

  <p class="fm-code-listing-caption">Listing 7.5 IAM policy to allow EKS to pull images from ECR</p>
  <pre class="programlisting">{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "AllowPull",
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::&lt;aws_account_id&gt;:role/&lt;role&gt;"
      },
      "Action": [
        "ecr:GetDownloadUrlForLayer",
        "ecr:BatchGetImage",
        "ecr:BatchCheckLayerAvailability"
      ],
      "Resource": "arn:aws:ecr:&lt;region&gt;:&lt;aws_account_id&gt;:
repository/&lt;repository_name&gt;"
    }
  ]
}</pre>

  <p class="body"><a id="marker-153"/>Once the EKS can pull down the image from ECR, you will see a pod start to run. However, you have no way to access this pod externally. You need to create a service. In Kubernetes, a <i class="fm-italics">service</i> is an abstraction that defines a logical set of pods (the smallest and simplest unit in the Kubernetes object model that you create or deploy) and a policy to access them.<a id="idIndexMarker051"/></p>

  <p class="body">Services enable communication between different parts of an application and between different applications. They help distribute network traffic and load balance by exposing the pods to the network and other pods in Kubernetes.</p>

  <p class="fm-code-listing-caption">Listing 7.6 Kubernetes services file to enable external access for our application</p>
  <pre class="programlisting"><b class="fm-bold"># Please create a service for the application that uses a load balancer type egress</b>
apiVersion: v1
kind: Service
metadata:
  name: itam-service
spec:
  type: LoadBalancer
  selector:
    app: itam
  ports:
  - name: http
    port: 80
    targetPort: 8000</pre>

  <p class="body">Kubernetes is responsible for routing all requests from this ingress through the service to the running pods, regardless of what host they are running on. This allows for seamless failover. Kubernetes expects things to fail. It banks on it. As a result, many of the best practices in distributed systems are baked into Kubernetes. Getting to Kube is a significant first step to having a reliable, highly available system. In the next section, we will examine how to ease the burden of getting our application onto Kubernetes repeatably and continuously. We will look at building out a small deployment pipeline using GitHub actions.<a id="idIndexMarker052"/><a id="idIndexMarker053"/><a id="idIndexMarker054"/><a id="idIndexMarker055"/></p>

  <h2 class="fm-head" id="heading_id_8">7.6 Setting up a continuous integration/continuous deployment pipeline in GitHub Actions</h2>

  <p class="body"><a id="marker-154"/>If releasing is hard, it will not be done often. This limits our ability to add value to the application and thus to our stakeholders. However, automating the deployment process significantly reduces the time to release. This allows for more frequent releases, accelerating the pace of development and enabling faster delivery of features to users. Continuous integration/continuous deployment (CI/CD) pipelines limit the risk associated with deployment. By making smaller, more frequent updates, any problems that arise can be isolated and fixed quickly, minimizing the potential effect on the end users. These pipelines facilitate seamless integration of code changes and expedite deployment, simplifying the software release process.<a id="idIndexMarker056"/><a id="idIndexMarker057"/></p>

  <p class="body">GitHub Actions allows us to construct customized CI/CD pipelines directly in our GitHub repositories. This makes the development workflow more efficient and enables the automation of various steps, freeing us to focus on coding rather than the logistics of integration and deployment.</p>

  <p class="body">This section provides a concise introduction to setting up a CI/CD pipeline using GitHub Actions and GitHub Copilot. Note that this will not be a comprehensive guide but rather a survey that introduces the potential benefits and general workflow. This should serve as a primer, giving you an insight into how these tools can be used to optimize your software development process.</p>

  <p class="body">First, create a file in your project in the path .github/workflows. Note the leading dot. You can call this file itam.yaml or whatever you desire. On the first line of this file, add the following prompt:</p>
  <pre class="programlistinge"># Create a GitHub Actions workflow that builds the ITAM application on every merge to the main branch and deploys it to EKS. </pre>

  <p class="fm-callout"><span class="fm-callout-head">NOTE</span> Like many of the infrastructure-related tasks that we have put to Copilot in this chapter, Copilot needs a lot of assistance in creating this file for us. We need to be aware of the structure of this file and how to begin every line. It makes sense in cases such as this one to ask ChatGPT or Copilot Chat to build the file for us.</p>

  <p class="body">The first part of this file outlines when this action should take place. The on:push instruction denotes that when a git push occurs to the main branch, this action should be executed. There is a single job in this file, one with several steps. This job “build” uses an embedded function <code class="fm-code-in-text">login-ecr</code> to log into our ECR. <a id="idIndexMarker058"/><a id="idIndexMarker059"/></p>

  <p class="fm-code-listing-caption">Listing 7.7 Beginning of GitHub Actions file to build our application</p>
  <pre class="programlisting"><b class="fm-bold"># Create a GitHub Actions workflow that builds the ITAM application on every merge to the main branch and deploys it to EKS.</b>
name: Build and Deploy to EKS
  
on:
  push:
    branches:
      - main
jobs:</pre>

  <p class="body">The build job will first check out the code from our GitHub repository. It uses the code written in the module <code class="fm-code-in-text">actions/checkout</code> version 2. Similarly, it will next grab the EKS CLI and configure the credentials to connect to EKS. Note that the AWS access key and secret are values that are automatically passed into the application. GitHub Actions uses a built-in secret management system to store sensitive data such as API keys, passwords, and certificates. This system is integrated into the GitHub platform and allows you to add, remove, or update secrets (and other sensitive data) at both the repository and organization levels. Secrets are encrypted before they’re stored and are not shown in logs or available for download. They’re only exposed as environment variables to the GitHub Actions runner, making it a secure way to handle sensitive data.</p>

  <p class="body"><a id="marker-155"/>Likewise, you can create environmental parameters and use them in your actions. For example, look at the variable <code class="fm-code-in-text">ECR_REGISTRY</code>. This variable is created using the output from the <code class="fm-code-in-text">login-ecr</code> function. In this case, you still need to hardcode the ECR in your Actions file. However, you should do this because of consistency and the need to manage it in only one place in the file. Most of these steps should seem familiar, as we have used them throughout the chapter. That is the magic of automation: it does it for you.<a id="idIndexMarker060"/><a id="idIndexMarker061"/></p>

  <p class="fm-code-listing-caption">Listing 7.8 Build and deploy steps of our GitHub Actions file</p>
  <pre class="programlisting">  build:
    runs-on: ubuntu-latest
  
    steps:
    - name: Checkout code
      uses: actions/checkout@v2
  
    - name: Set up EKS CLI
      uses: aws-actions/amazon-eks-cli@v0.1.0
  
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v1
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2
  
    - name: Build and push Docker image
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        ECR_REPOSITORY: itam
        IMAGE_TAG: ${{ github.sha }}
      run: |
        docker build -t $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG .
        docker push $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG
  
    - name: Deploy to EKS
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        ECR_REPOSITORY: itam
        IMAGE_TAG: ${{ github.sha }}
      run: |
        envsubst &lt; k8s/deployment.yaml | kubectl apply -f -
        envsubst &lt; k8s/service.yaml | kubectl apply -f -</pre>

  <p class="body">The final part of the file logs in to AWS ECR. The steps in the Actions file invoke this action. On completion, it returns the output to the calling function.<a id="marker-156"/></p>

  <p class="fm-code-listing-caption">Listing 7.9 A GitHub Actions file to build and deploy to EKS</p>
  <pre class="programlisting">  login-ecr:
    runs-on: ubuntu-latest
    steps:
    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v1
      with:
        registry: &lt;your-ecr-registry&gt;
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}</pre>

  <p class="body">Exploring code-as-infrastructure has enabled us to understand its vital role in any project and how it can be better managed through code. Tools like Terraform provide streamlined solutions for managing infrastructure, and GitHub’s code-centric features aid in maintaining the overall workflow.</p>

  <p class="body">Introducing CI/CD pipelines, primarily through platforms like GitHub Actions, highlights the importance of automating the software delivery process. Automating such processes increases the speed and reliability of the software development life cycle and minimizes the chances of human errors.</p>

  <p class="body">The journey of managing infrastructure as code is ever-evolving, with new tools and practices emerging. It requires a constant learning and adaptation mindset. This chapter has given you a glimpse of the benefits and possibilities.<a id="idIndexMarker062"/><a id="idIndexMarker063"/></p>

  <h2 class="fm-head" id="heading_id_9">Summary</h2>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">You learned about the transition from application development to product launch, covering deployment strategies, best practices for cloud infrastructure, and the use of Docker and Terraform for managing and containerizing applications efficiently.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">The chapter explained how to manage application deployment via Kubernetes, including creating YAML deployment descriptors, forming services for network traffic distribution, and deploying on AWS’s Elastic Kubernetes Service (EKS).</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">You discovered how to adapt deployment methods to different environments, whether on various cloud platforms or on premises, and how GitHub Copilot can assist in creating Dockerfiles and Terraform files accurately.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Finally, we explored the process of exporting Docker images to remote machines, pushing them to Amazon’s Elastic Container Registry (ECR), and migrating to automated deployments using GitHub Actions.<a id="idIndexMarker064"/><a id="marker-157"/></p>
    </li>
  </ul>
</body></html>