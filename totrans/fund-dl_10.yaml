- en: Chapter 10\. Generative Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generative models attempt to understand the *latent*, or underlying, process
    that produces the data we see. For example, when breaking down images of digits
    in the MNIST dataset, we can interpret some attributes of the underlying process
    generating each image as the digit itself (a discrete variable ranging from zero
    through nine), the orientation or angle at which it will be drawn, the size of
    the resulting image, the thickness of the lines, and some noise component (all
    of which are continuous variables). So far, we’ve been concerned with *discriminative*
    models, either in the regression or classification setting. In the classification
    setting, discriminative models take as input an example such as an image from
    the MNIST dataset and attempt to determine the most likely digit category, from
    zero through nine, that the input belongs to. Generative models instead attempt
    to fully model the data distribution, and in the process may implicitly try to
    learn some of the features mentioned previously to generate images that look as
    if they were originally from the MNIST dataset. Note that generative modeling
    is a harder problem than discriminative modeling, as a discriminative model may,
    for example, need to learn only a few features well to distinguish between different
    digits in the MNIST dataset to a satisfactory degree. Generative models come in
    many varieties, and in this chapter, we provide a glimpse into a vast research
    landscape that has begun to blossom only in the past decade.
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Generative Adversarial Networks*, or GANs for short, are a form of generative
    model designed to produce realistic samples of entities, such as images, from
    noise. They were introduced by Goodfellow et al. in 2014.^([1](ch10.xhtml#idm45934165113648))
    For the remainder of this section, we will assume we are working with an image
    dataset such as MNIST or CIFAR-10\. The original GAN architecture is broken down
    into two neural networks: the *discriminator* and the *generator.*'
  prefs: []
  type: TYPE_NORMAL
- en: The generator takes in samples from some noise distribution, such as a multivariate
    Gaussian distribution, and outputs an image. The discriminator is tasked with
    predicting whether this image was produced by the generator or was sampled from
    the original dataset. As the generator gets better and better at producing images
    that look real, the discriminator has a harder time determining whether a given
    image was produced by the generator or sampled from the dataset. We can think
    of these two networks as participating in a game, competing against each other
    to develop. Each network evolves until the generator can eventually produce images
    that look as if they were drawn directly from the original dataset, and the discriminator
    cannot distinguish between the two sets of images, i.e., predicts that any image
    is from the dataset with probability <math alttext="one-half"><mfrac><mn>1</mn>
    <mn>2</mn></mfrac></math> .
  prefs: []
  type: TYPE_NORMAL
- en: More rigorously, we define the data distribution to be <math alttext="p Subscript
    data Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mtext>data</mtext></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> . Although we can
    never really know the true data distribution, in practice we generally think of
    it as being approximated well enough by the dataset we have on hand ( <math alttext="p
    Subscript data Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <mtext>data</mtext></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    is just a uniform distribution over all of the images present in the dataset and
    zero likelihood associated with all images that are not in the dataset).
  prefs: []
  type: TYPE_NORMAL
- en: We additionally define the distribution parametrized by the generator to be
    <math alttext="p Subscript g Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>g</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    . The random variable *x* represents an entity such as an image, a collection
    of pixels that can each be thought of as their own random variables. The generator,
    which we also refer to as *G*, defines <math alttext="p Subscript g Baseline left-parenthesis
    x right-parenthesis"><mrow><msub><mi>p</mi> <mi>g</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> by mapping samples from the noise distribution,
    which we will refer to as *p(z)*, to the data space, which consists of all possible
    images (not just those in the dataset). It is important to keep in mind that *G*
    itself is a deterministic function, but implicitly defines a distribution by acting
    on the noise distribution. Note that this distribution is implicit because we
    can generate samples from it only via *G(z),* rather than being an explicit distribution
    we can work with directly and query an image for its likelihood. [Figure 10-1](#the_discriminator_is_tasked_with_determining_whether_any_input)
    shows the typical GAN architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. The discriminator determines whether any input image was sampled
    from the dataset or generator. The generator’s goal is to trick the discriminator
    into believing its images were sampled from the dataset.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An optimal generator for a given dataset would also parametrize <math alttext="p
    Subscript data Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <mtext>data</mtext></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    , as this would perfectly confuse even the best discriminator. In other words,
    if the generator parametrizes the exact same distribution as that of the dataset
    and it is equally likely to sample from either the generator or the dataset, then
    no discriminator would be able to tell where the query originated from, as both
    are always equally likely. We formalize this intuition in the next paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thinking back to [Chapter 2](ch02.xhtml#fundamentals-of-proba), given a generator
    that parametrizes the same distribution as the dataset, we have <math alttext="p
    left-parenthesis x vertical-bar y equals generator right-parenthesis equals p
    left-parenthesis x vertical-bar y equals dataset right-parenthesis comma for-all
    x"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext>
    <mo>)</mo> <mo>=</mo> <mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>y</mi> <mo>=</mo>
    <mtext>dataset</mtext> <mo>)</mo> <mo>,</mo> <mo>∀</mo> <mi>x</mi></mrow></math>
    , where *y* is a Bernoulli random variable over the two options: generator or
    dataset. Note that we use <math alttext="p Subscript g Baseline left-parenthesis
    x right-parenthesis"><mrow><msub><mi>p</mi> <mi>g</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> and <math alttext="p left-parenthesis
    x vertical-bar y equals generator right-parenthesis"><mrow><mi>p</mi> <mo>(</mo>
    <mi>x</mi> <mo>|</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>)</mo></mrow></math>
    interchangeably, and <math alttext="p Subscript data Baseline left-parenthesis
    x right-parenthesis"><mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> and <math alttext="p left-parenthesis
    x vertical-bar y equals dataset right-parenthesis"><mrow><mi>p</mi> <mo>(</mo>
    <mi>x</mi> <mo>|</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo></mrow></math>
    interchangeably, since they mean the same thing. The latter option of each allows
    us to keep in mind we are working with conditional probabilities. Again, assuming
    that sampling from the generator and sampling from the dataset are equally likely,
    or <math alttext="p left-parenthesis y equals generator right-parenthesis equals
    p left-parenthesis y equals dataset right-parenthesis"><mrow><mi>p</mi> <mo>(</mo>
    <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>)</mo> <mo>=</mo> <mi>p</mi>
    <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo></mrow></math>
    , we can use Bayes’ Rule to obtain the equality: <math alttext="p left-parenthesis
    y equals generator vertical-bar x right-parenthesis equals p left-parenthesis
    y equals dataset vertical-bar x right-parenthesis comma for-all x"><mrow><mi>p</mi>
    <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>|</mo> <mi>x</mi>
    <mo>)</mo> <mo>=</mo> <mi>p</mi> <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext>
    <mo>|</mo> <mi>x</mi> <mo>)</mo> <mo>,</mo> <mo>∀</mo> <mi>x</mi></mrow></math>
    . Since there are only two options, as *y* is a Bernoulli random variable, we
    are left with the perfectly confused discriminator alluded to earlier that predicts
    any image to be sampled from the dataset with probability <math alttext="one-half"><mfrac><mn>1</mn>
    <mn>2</mn></mfrac></math> .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowing our end goal, we can now go about designing an objective function for
    training our generator and discriminator in tandem. In the original GAN paper,
    the objective presented was:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper V left-parenthesis upper G comma upper D right-parenthesis
    equals double-struck upper E Subscript x tilde p Sub Subscript data Subscript
    left-parenthesis x right-parenthesis Baseline left-bracket log upper D left-parenthesis
    x right-parenthesis right-bracket plus double-struck upper E Subscript z tilde
    p left-parenthesis z right-parenthesis Baseline left-bracket log left-parenthesis
    1 minus upper D left-parenthesis upper G left-parenthesis z right-parenthesis
    right-parenthesis right-bracket"><mrow><mi>V</mi> <mrow><mo>(</mo> <mi>G</mi>
    <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi>
    <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <mi>D</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>p</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>D</mi>
    <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '*G(z)* represents the mapping from the noise distribution to the data space
    described earlier, and *D(x)* represents the score assigned to the input image.
    *D(x)* is interpreted as the probability that the input image was drawn from the
    dataset. Of course, the discriminator *D* would like to maximize this objective—this
    corresponds with assigning high probabilities to images drawn from the dataset
    rather than images produced by the generator *G*. *G*, on the other hand, would
    like to minimize this objective, since that corresponds with producing realistic
    images, or even images that look exactly like those from the dataset, that confuse
    *D* and cause it to return a high score for these generator-produced images. This
    idea of maximizing the objective for one network and minimizing the objective
    for the other is termed *minimax,* and the optimization procedure looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="min Subscript upper G Baseline max Subscript upper D Baseline
    double-struck upper E Subscript x tilde p Sub Subscript data Subscript left-parenthesis
    x right-parenthesis Baseline left-bracket log upper D left-parenthesis x right-parenthesis
    right-bracket plus double-struck upper E Subscript z tilde p left-parenthesis
    z right-parenthesis Baseline left-bracket log left-parenthesis 1 minus upper D
    left-parenthesis upper G left-parenthesis z right-parenthesis right-parenthesis
    right-bracket"><mrow><msub><mtext>min</mtext> <mi>G</mi></msub> <msub><mtext>max</mtext>
    <mi>D</mi></msub> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi>
    <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <mi>D</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>p</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>D</mi>
    <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper goes on to show that, for a fixed generator *G,* the optimal discriminator
    trained under this objective would output the following score:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartFraction p Subscript data Baseline left-parenthesis x right-parenthesis
    Over p Subscript data Baseline left-parenthesis x right-parenthesis plus p Subscript
    g Baseline left-parenthesis x right-parenthesis EndFraction"><mfrac><mrow><msub><mi>p</mi>
    <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi>
    <mi>g</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: for a given image *x*. First, we consider why this should even describe the
    behavior of an optimal discriminator given a fixed generator. Before we get into
    the “why,” it’s important to keep in mind that *D* can be alternatively represented
    as <math alttext="p Subscript theta Baseline left-parenthesis y equals dataset
    vertical-bar x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    , or the discriminator’s belief that the image was drawn from the dataset. Here
    <math alttext="theta"><mi>θ</mi></math> represents the parameters, or weights,
    of *D.* When we perform an update operation such as gradient descent, <math alttext="theta"><mi>θ</mi></math>
    represents the set of weights that is being updated. It is important to keep in
    mind that this distribution is distinct from <math alttext="p left-parenthesis
    y equals dataset vertical-bar x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo>
    <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    mentioned earlier—the latter is the true probability that a given image was sampled
    from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The optimal discriminator can never know the exact origin of the image unless
    it is impossible for the generator to have produced the image, i.e., <math alttext="p
    Subscript g Baseline left-parenthesis x right-parenthesis equals 0"><mrow><msub><mi>p</mi>
    <mi>g</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mn>0</mn></mrow></math>
    . We can quantify the uncertainty in the discriminator’s prediction as a function
    of the image’s likelihood under the data distribution, or <math alttext="p Subscript
    data Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mtext>data</mtext></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> , and the image’s
    likelihood under the distribution defined by *G*, or <math alttext="p Subscript
    g Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>g</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> . If the image’s likelihood
    under the distribution defined by the generator is less than that of the data
    distribution, it makes sense that the optimal discriminator should be swayed accordingly
    and should score the image closer to one than zero.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note that a quick back-of-the-envelope check shows that this property is true
    for the score <math alttext="StartFraction p Subscript data Baseline left-parenthesis
    x right-parenthesis Over p Subscript data Baseline left-parenthesis x right-parenthesis
    plus p Subscript g Baseline left-parenthesis x right-parenthesis EndFraction"><mfrac><mrow><msub><mi>p</mi>
    <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi>
    <mi>g</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac></math>
    . But why is this the exact proportion by which the property is true? Let’s take
    a more concrete look at the score <math alttext="StartFraction p Subscript data
    Baseline left-parenthesis x right-parenthesis Over p Subscript data Baseline left-parenthesis
    x right-parenthesis plus p Subscript g Baseline left-parenthesis x right-parenthesis
    EndFraction"><mfrac><mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi>
    <mi>g</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac></math>
    and determine why this is the optimal function of the two probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking some inspiration from our discussion regarding the perfectly confused
    discriminator, we can alternatively express the proposed optimal discriminator
    score in terms of conditional probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="StartFraction p left-parenthesis x vertical-bar y equals dataset
    right-parenthesis Over p left-parenthesis x vertical-bar y equals dataset right-parenthesis
    plus p left-parenthesis x vertical-bar y equals generator right-parenthesis EndFraction"><mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow>
    <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo><mo>+</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mfrac></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, making the same assumption regarding equal likelihood of sampling
    from the dataset versus sampling from the generator ( <math alttext="p left-parenthesis
    y equals dataset right-parenthesis equals p left-parenthesis y equals generator
    right-parenthesis equals 0.5"><mrow><mi>p</mi> <mo>(</mo> <mi>y</mi> <mo>=</mo>
    <mtext>dataset</mtext> <mo>)</mo> <mo>=</mo> <mi>p</mi> <mo>(</mo> <mi>y</mi>
    <mo>=</mo> <mtext>generator</mtext> <mo>)</mo> <mo>=</mo> <mn>0</mn> <mo>.</mo>
    <mn>5</mn></mrow></math> ), we can get to a much more interpretable representation
    of the optimal score:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper D Superscript asterisk Baseline left-parenthesis x right-parenthesis
    equals StartFraction p left-parenthesis x vertical-bar y equals dataset right-parenthesis
    Over p left-parenthesis x vertical-bar y equals dataset right-parenthesis plus
    p left-parenthesis x vertical-bar y equals generator right-parenthesis EndFraction"><mrow><msup><mi>D</mi>
    <mo>*</mo></msup> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow>
    <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo><mo>+</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals StartFraction p left-parenthesis x vertical-bar y equals
    dataset right-parenthesis asterisk p left-parenthesis y equals dataset right-parenthesis
    Over p left-parenthesis x vertical-bar y equals dataset right-parenthesis asterisk
    p left-parenthesis y equals dataset right-parenthesis plus p left-parenthesis
    x vertical-bar y equals generator right-parenthesis asterisk p left-parenthesis
    y equals generator right-parenthesis EndFraction"><mrow><mo>=</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo><mo>*</mo><mi>p</mi><mo>(</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow>
    <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo><mo>*</mo><mi>p</mi><mo>(</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo><mo>+</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo><mo>*</mo><mi>p</mi><mo>(</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals StartFraction p left-parenthesis x comma y equals dataset
    right-parenthesis Over p left-parenthesis x right-parenthesis EndFraction"><mrow><mo>=</mo>
    <mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow>
    <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals p left-parenthesis y equals dataset vertical-bar x right-parenthesis"><mrow><mo>=</mo>
    <mi>p</mi> <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo>
    <mi>x</mi> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The denominator in the third equality is a result of having marginalized out
    *y.* The final result is just the conditional probability of having sampled from
    the dataset given the input image. It makes sense that the optimal discriminator,
    <math alttext="p Subscript theta Sub Superscript asterisk Baseline left-parenthesis
    y equals dataset vertical-bar x right-parenthesis"><mrow><msub><mi>p</mi> <msup><mi>θ</mi>
    <mo>*</mo></msup></msub> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext>
    <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> , should strive to match
    the true probability that the input image was drawn from the dataset, <math alttext="p
    left-parenthesis y equals dataset vertical-bar x right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi>
    <mo>)</mo></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we consider why the minimax objective defined earlier is maximized by
    <math alttext="p left-parenthesis y equals dataset vertical-bar x right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi>
    <mo>)</mo></mrow></math> , or the true conditional probability of having drawn
    from the dataset given an image *x,* under the assumption of a fixed generator.
    Let’s take a closer look at the objective and try to reformulate it in a more
    informative manner that may provide us with some insight:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper V left-parenthesis upper G comma upper D right-parenthesis
    equals double-struck upper E Subscript x tilde p Sub Subscript data Subscript
    left-parenthesis x right-parenthesis Baseline left-bracket log upper D left-parenthesis
    x right-parenthesis right-bracket plus double-struck upper E Subscript z tilde
    p left-parenthesis z right-parenthesis Baseline left-bracket log left-parenthesis
    1 minus upper D left-parenthesis upper G left-parenthesis z right-parenthesis
    right-parenthesis right-bracket"><mrow><mi>V</mi> <mrow><mo>(</mo> <mi>G</mi>
    <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi>
    <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <mi>D</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><mi>z</mi><mo>∼</mo><mi>p</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mi>D</mi>
    <mrow><mo>(</mo> <mi>G</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals double-struck upper E Subscript x tilde p left-parenthesis
    x vertical-bar y equals dataset right-parenthesis Baseline left-bracket log p
    Subscript theta Baseline left-parenthesis y equals dataset vertical-bar x right-parenthesis
    right-bracket plus double-struck upper E Subscript p Sub Subscript phi Subscript
    left-parenthesis x vertical-bar y equals generator right-parenthesis Baseline
    left-bracket log left-parenthesis 1 minus p Subscript theta Baseline left-parenthesis
    y equals dataset vertical-bar x right-parenthesis right-parenthesis right-bracket"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext>
    <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals double-struck upper E Subscript x tilde p left-parenthesis
    x vertical-bar y equals dataset right-parenthesis Baseline left-bracket log p
    Subscript theta Baseline left-parenthesis y equals dataset vertical-bar x right-parenthesis
    right-bracket plus double-struck upper E Subscript p Sub Subscript phi Subscript
    left-parenthesis x vertical-bar y equals generator right-parenthesis Baseline
    left-bracket log left-parenthesis p Subscript theta Baseline left-parenthesis
    y equals generator vertical-bar x right-parenthesis right-parenthesis right-bracket"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>|</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: As usual, we have formulated the objective in terms of conditional probabilities.
    To get from the first equality to the second, we note that taking the expectation
    with respect to the noise distribution *p(z)* and then applying a function such
    as *G* to each sample is equivalent to just taking the expectation with respect
    to the distribution over the data space defined by *G’s* mapping. This is similar
    in spirit to a concept we discussed in [Chapter 2](ch02.xhtml#fundamentals-of-proba),
    where random variables can be functions of other random variables. Also note the
    addition of the letter <math alttext="phi"><mi>φ</mi></math> starting from the
    second line—this letter represents the parameters, or weights, of *G.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking a closer look at the final expression, we start to see an awful lot
    of similarities between the objective and the concepts of entropy and cross entropy
    introduced in [Chapter 2](ch02.xhtml#fundamentals-of-proba). It turns out that
    we can manipulate the objective slightly without affecting the best <math alttext="theta"><mi>θ</mi></math>
    here to obtain a sum of the negatives of two cross-entropy terms:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="theta Superscript asterisk Baseline equals argmin Subscript theta
    Baseline upper V left-parenthesis upper G comma upper D right-parenthesis"><mrow><msup><mi>θ</mi>
    <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmin</mtext> <mi>θ</mi></msub> <mi>V</mi>
    <mrow><mo>(</mo> <mi>G</mi> <mo>,</mo> <mi>D</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals argmin Subscript theta Baseline double-struck upper E
    Subscript x tilde p left-parenthesis x vertical-bar y equals dataset right-parenthesis
    Baseline left-bracket log p Subscript theta Baseline left-parenthesis y equals
    dataset vertical-bar x right-parenthesis right-bracket plus double-struck upper
    E Subscript p Sub Subscript phi Subscript left-parenthesis x vertical-bar y equals
    generator right-parenthesis Baseline left-bracket log left-parenthesis p Subscript
    theta Baseline left-parenthesis y equals generator vertical-bar x right-parenthesis
    right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mtext>argmin</mtext>
    <mi>θ</mi></msub> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>|</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals argmin Subscript theta Baseline minus upper H left-parenthesis
    p left-parenthesis x comma y equals dataset right-parenthesis comma p Subscript
    theta Baseline left-parenthesis x comma y equals dataset right-parenthesis right-parenthesis
    minus upper H left-parenthesis p left-parenthesis x comma y equals generator right-parenthesis
    comma p Subscript theta Baseline left-parenthesis x comma y equals generator right-parenthesis
    right-parenthesis"><mrow><mo>=</mo> <msub><mtext>argmin</mtext> <mi>θ</mi></msub>
    <mo>-</mo> <mi>H</mi> <mrow><mo>(</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo></mrow> <mo>,</mo>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi>
    <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo>
    <mi>H</mi> <mrow><mo>(</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>)</mo></mrow> <mo>,</mo> <msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo>
    <mtext>generator</mtext> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in [Chapter 2](ch02.xhtml#fundamentals-of-proba), the cross entropy
    between two distributions is minimized when the two distributions are exactly
    the same—here we are doing the equivalent by simply maximizing the negative cross
    entropy instead. Thus, <math alttext="theta"><mi>θ</mi></math> achieves the optimal
    set of weights <math alttext="theta Superscript asterisk"><msup><mi>θ</mi> <mo>*</mo></msup></math>
    when <math alttext="p Subscript theta Baseline left-parenthesis x comma y equals
    dataset right-parenthesis equals p left-parenthesis x comma y equals dataset right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo>
    <mtext>dataset</mtext> <mo>)</mo></mrow> <mo>=</mo> <mi>p</mi> <mrow><mo>(</mo>
    <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo></mrow></mrow></math>
    and <math alttext="p Subscript theta Baseline left-parenthesis x comma y equals
    generator right-parenthesis equals p left-parenthesis x comma y equals generator
    right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi>
    <mo>,</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext> <mo>)</mo></mrow> <mo>=</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mtext>generator</mtext>
    <mo>)</mo></mrow></mrow></math> .
  prefs: []
  type: TYPE_NORMAL
- en: As our final step, we’d like to show that at <math alttext="theta Superscript
    asterisk"><msup><mi>θ</mi> <mo>*</mo></msup></math> , <math alttext="p Subscript
    theta Sub Superscript asterisk Baseline left-parenthesis y equals dataset vertical-bar
    x right-parenthesis equals p left-parenthesis y equals dataset vertical-bar x
    right-parenthesis"><mrow><msub><mi>p</mi> <msup><mi>θ</mi> <mo>*</mo></msup></msub>
    <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>=</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>y</mi> <mo>=</mo>
    <mtext>dataset</mtext> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> as
    promised. We already know that <math alttext="p Subscript theta Sub Superscript
    asterisk Baseline left-parenthesis x comma y equals dataset right-parenthesis
    equals p left-parenthesis x comma y equals dataset right-parenthesis"><mrow><msub><mi>p</mi>
    <msup><mi>θ</mi> <mo>*</mo></msup></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo>
    <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext> <mo>)</mo></mrow> <mo>=</mo> <mi>p</mi>
    <mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>=</mo> <mtext>dataset</mtext>
    <mo>)</mo></mrow></mrow></math> from our prior work. Dividing by *p(x)* on both
    sides leaves us with the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have assumed a fixed *G*, and shown various properties regarding
    the optimal *D.* Unfortunately, we can’t assume a fixed *G* in practice, as we
    must train the generator as well as the discriminator. But now that we have shown
    some properties regarding the optimal *D,* we can begin to talk about the properties
    *G* must satisfy to achieve the global optimum—a generator that can perfectly
    confuse even the optimal discriminator. If we assume an optimal discriminator
    and plug in its score <math alttext="StartFraction p Subscript data Baseline left-parenthesis
    x right-parenthesis Over p Subscript data Baseline left-parenthesis x right-parenthesis
    plus p Subscript g Baseline left-parenthesis x right-parenthesis EndFraction"><mfrac><mrow><msub><mi>p</mi>
    <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi>
    <mi>g</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac></math>
    to the objective *V(G,D),* we obtain an objective that is solely dependent on
    the parameters, or weights, of *G*:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper C left-parenthesis upper G right-parenthesis equals double-struck
    upper E Subscript x tilde p Sub Subscript data Subscript left-parenthesis x right-parenthesis
    Baseline left-bracket log StartFraction p Subscript data Baseline left-parenthesis
    x right-parenthesis Over p Subscript data Baseline left-parenthesis x right-parenthesis
    plus p Subscript g Baseline left-parenthesis x right-parenthesis EndFraction right-bracket
    plus double-struck upper E Subscript x tilde p Sub Subscript g Subscript left-parenthesis
    x right-parenthesis Baseline left-bracket log left-parenthesis 1 minus StartFraction
    p Subscript data Baseline left-parenthesis x right-parenthesis Over p Subscript
    data Baseline left-parenthesis x right-parenthesis plus p Subscript g Baseline
    left-parenthesis x right-parenthesis EndFraction right-parenthesis right-bracket"><mrow><mi>C</mi>
    <mrow><mo>(</mo> <mi>G</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi>
    <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <mfrac><mrow><msub><mi>p</mi> <mtext>data</mtext></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>p</mi> <mtext>data</mtext></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi> <mi>g</mi></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac> <mo>]</mo></mrow> <mo>+</mo>
    <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi> <mi>g</mi></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext>
    <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <mfrac><mrow><msub><mi>p</mi> <mtext>data</mtext></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>p</mi> <mtext>data</mtext></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi> <mi>g</mi></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals double-struck upper E Subscript x tilde p Sub Subscript
    data Subscript left-parenthesis x right-parenthesis Baseline left-bracket log
    StartFraction p Subscript data Baseline left-parenthesis x right-parenthesis Over
    p Subscript data Baseline left-parenthesis x right-parenthesis plus p Subscript
    g Baseline left-parenthesis x right-parenthesis EndFraction right-bracket plus
    double-struck upper E Subscript x tilde p Sub Subscript g Subscript left-parenthesis
    x right-parenthesis Baseline left-bracket log StartFraction p Subscript g Baseline
    left-parenthesis x right-parenthesis Over p Subscript data Baseline left-parenthesis
    x right-parenthesis plus p Subscript g Baseline left-parenthesis x right-parenthesis
    EndFraction right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi>
    <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <mfrac><mrow><msub><mi>p</mi> <mtext>data</mtext></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><msub><mi>p</mi> <mtext>data</mtext></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi> <mi>g</mi></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac> <mo>]</mo></mrow> <mo>+</mo>
    <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi> <mi>g</mi></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mtext>log</mtext>
    <mfrac><mrow><msub><mi>p</mi> <mi>g</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow>
    <mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi>
    <mi>g</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow></mfrac> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals double-struck upper E Subscript x tilde p left-parenthesis
    x vertical-bar y equals dataset right-parenthesis Baseline left-bracket log StartFraction
    p left-parenthesis x vertical-bar y equals dataset right-parenthesis Over p left-parenthesis
    x vertical-bar y equals dataset right-parenthesis plus p Subscript phi Baseline
    left-parenthesis x vertical-bar y equals generator right-parenthesis EndFraction
    right-bracket plus double-struck upper E Subscript x tilde p Sub Subscript phi
    Subscript left-parenthesis x vertical-bar y equals generator right-parenthesis
    Baseline left-bracket log StartFraction p Subscript phi Baseline left-parenthesis
    x vertical-bar y equals generator right-parenthesis Over p left-parenthesis x
    vertical-bar y equals dataset right-parenthesis plus p Subscript phi Baseline
    left-parenthesis x vertical-bar y equals generator right-parenthesis EndFraction
    right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow>
    <mrow><mi>p</mi><mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mrow></mfrac>
    <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><mi>x</mi><mo>∼</mo><msub><mi>p</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mtext>log</mtext> <mfrac><mrow><msub><mi>p</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mrow>
    <mrow><mi>p</mi><mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>dataset</mtext><mo>)</mo></mrow><mo>+</mo><msub><mi>p</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>|</mo><mi>y</mi><mo>=</mo><mtext>generator</mtext><mo>)</mo></mrow></mrow></mfrac>
    <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We can now minimize this objective by optimizing over the generator weights
    <math alttext="phi"><mi>φ</mi></math> . We refer you to the original GAN paper
    for the rigorous derivation. However, as one might expect by now, it turns out
    that the optimal distribution *G* represents, or <math alttext="p Subscript g
    Sub Superscript asterisk Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <msup><mi>g</mi> <mo>*</mo></msup></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    , is equal to <math alttext="p Subscript data Baseline left-parenthesis x right-parenthesis
    comma for-all x"><mrow><msub><mi>p</mi> <mtext>data</mtext></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>,</mo> <mo>∀</mo> <mi>x</mi></mrow></math> .
    This matches our original intuition regarding the perfectly confused discriminator
    and shows that the objective function proposed in the original GAN paper does
    indeed theoretically converge to this global optimum.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an optimal generator and discriminator, how do we perform image
    generation? All we need to do is sample from our noise distribution *p(z)* and
    run each sample through the generator. The generator, being optimal, should produce
    images that look as if they were drawn from the dataset itself. It may come as
    a surprise to you that the discriminator is no longer needed in this phase—but
    it has served its purpose. The discriminator played a key role in competing with
    the generator, each evolving until the latter could produce images that perfectly
    confused the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: Note that unlike the standard interpretation of generative modeling, *z* does
    not represent a set of latent variables from which the data is generated. *z*
    simply plays the role of being a random variable distributed as one of our standard
    distributions, such as a uniform distribution or a standard multivariate Gaussian
    distribution, which are easy to sample from. *G,* when fully trained and optimal,
    is a complex, differentiable function that transforms samples from *p(z)* into
    samples from <math alttext="p Subscript data Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <mtext>data</mtext></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    , which approximates *p(x)*. In the next section, we will see the parallels between
    *G(z)* and the reparametrization trick, which also allows us to sample from a
    distribution by transforming samples (via a differentiable function) from a distribution
    that is easier to sample from.
  prefs: []
  type: TYPE_NORMAL
- en: Variational Autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In parallel to the introduction of GANs, Kingma and Welling introduced the *Variational
    Autoencoder*, or VAE for short, in their seminal paper, “Auto-Encoding Variational
    Bayes,” from 2014.^([2](ch10.xhtml#idm45934168472352)) The idea behind the VAE
    is more strongly rooted in probabilistic modeling than the aforementioned GAN.
    The VAE assumes there exists a set of unobserved latent variables, which we denote
    as *z*, that generate the data we see, which we denote as *x*. More formally,
    we say there exists a joint probability distribution *p(x,z)* over the latent
    variables *z* and the observed data *x* that factors as <math alttext="p left-parenthesis
    x vertical-bar z right-parenthesis p left-parenthesis z right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo> <mi>p</mi> <mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow></math> (see [Figure 10-2](#img1002)). Thinking back to [Chapter 2](ch02.xhtml#fundamentals-of-proba),
    this factorization is quite intuitive. Given the predefined roles of *z* and *x,*
    the universe in which *z* takes on some value and *x* is generated from this setting
    of *z* makes much more sense than the other way around.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. z represents the latent variables from which every instance of
    *x* is generated. The arrow pointing from z to x signifies this relationship.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*x* could represent any sort of continuous or discrete data, including images.
    We additionally know the domain of *x* due to our knowledge of the dataset. *z,*
    on the other hand, is much more elusive. We have no idea what *z* looks like,
    so we make some initial assumptions about it. For example, we may assume that
    it initially takes the form of a Gaussian distribution, i.e., *p(z)* is Gaussian.
    Again, thinking back to [Chapter 2](ch02.xhtml#fundamentals-of-proba), we say
    that *p(z),* or our prior on *z,* is Gaussian.'
  prefs: []
  type: TYPE_NORMAL
- en: Whenever we think about such a data-generation process, some natural probabilistic
    questions (should) come to mind. For example, what is the distribution <math alttext="p
    left-parenthesis z vertical-bar x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo>
    <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></math> , or the posterior of
    *z* having known *x*? As we observe data, our beliefs regarding the underlying
    parameters often change. Take the coin flip experiment from [Chapter 2](ch02.xhtml#fundamentals-of-proba)
    as an example. We initially assumed a 50-50 chance of flipping heads, where the
    50-50 can be thought of as our latent parameter <math alttext="alpha"><mi>α</mi></math>
    —the parameter dictating the data generation procedure of sequences of heads and
    tails. This is a little simplified—in reality, we initially have a distribution
    over <math alttext="alpha"><mi>α</mi></math> , the probability of flipping heads,
    which is our prior distribution. Of course, the domain of the prior is the range
    [0,1], where it is logical to design the prior <math alttext="p left-parenthesis
    alpha right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>α</mi> <mo>)</mo></mrow></math>
    such that <math alttext="p left-parenthesis alpha equals 0.5 right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>α</mi> <mo>=</mo> <mn>0</mn> <mo>.</mo> <mn>5</mn> <mo>)</mo></mrow></math>
    is larger than all other settings of <math alttext="alpha"><mi>α</mi></math> .
    As we observed sequences of flips, we updated our prior via Bayes’ Theorem. In
    a similar manner, we initially assume *p(z)* to be a Gaussian distribution with
    some mean and variance; but as we observe data, we recalculate our belief in the
    form of a posterior, *p(z|x)* (see [Figure 10-3](#img1003)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Another question naturally comes to mind: what is the distribution *p(x|z),*
    or the likelihood of the data *x* given a certain setting of the latent variables
    *z*? In the coin flip setting, *p(x|z)* is easy to think about. Due to our complete
    knowledge of the experiment, we know the probability of any sequence is just the
    product of the probability of each flip, which is directly defined by *z*. In
    more intricate settings such as images, however, we can assume the relationship
    between the data *x* and the latent variables *z* is much more complicated than
    that. For example, when looking at images, it is clear that the value of a given
    pixel is quite affected by the values of its neighboring pixels and sometimes
    even by pixels much farther than one might think. The simple independence assumption
    we have for coin flips will not suffice for our purposes. This is just one reason
    why we can’t simply use a method like Bayes’ Theorem to learn a posterior over
    *z*—it requires much more knowledge regarding the system than what is immediately
    available to us.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. Here we have the coin flip experiment, where the prior is designed
    such that 0.5 has the highest likelihood. Once we see a series of heads and tails,
    the posterior shifts to the right due to there being more heads than tails.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In variational autoencoders, we encode these distributions as neural networks,
    which can be seen as complex, nonlinear functions that can accurately model the
    relationships between latent variables *z* and the observed data *x*. We denote
    the neural network that outputs a distribution over the data given a setting of
    the latent variables, also termed the *decoder,* as <math alttext="p Subscript
    theta Baseline left-parenthesis x vertical-bar z right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    , where <math alttext="theta"><mi>θ</mi></math> represents the weights of the
    neural network. In other words, the setting of <math alttext="theta"><mi>θ</mi></math>
    , in addition to the predetermined architecture of the neural network, completely
    define the model’s belief of the true distribution <math alttext="p left-parenthesis
    x vertical-bar z right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo>
    <mi>z</mi> <mo>)</mo></mrow></math> . We optimize <math alttext="theta"><mi>θ</mi></math>
    to achieve a setting that is closest to that of the true distribution.
  prefs: []
  type: TYPE_NORMAL
- en: We additionally encode the posterior over *z*, or *p(z|x),* as a neural network.
    We denote this neural network, termed the *encoder,* as <math alttext="q Subscript
    phi Baseline left-parenthesis z vertical-bar x right-parenthesis"><mrow><msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    . Similarly to the decoder, we optimize <math alttext="phi"><mi>φ</mi></math>
    to achieve a setting that is closest to that of the true posterior.
  prefs: []
  type: TYPE_NORMAL
- en: Kingma and Welling made some key observations that made the variational autoencoder
    a practical means for generative modeling ([Figure 10-4](#fig1004)). The first
    was that the *evidence lower bound* (ELBO for short), which is a lower bound on
    the true log likelihood of the data <math alttext="p left-parenthesis x right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> , could be reformulated in a way
    that allowed for tractable optimization over the encoder and decoder parameters.
    The second was a reparametrization trick that enabled the computation of a low
    variance estimate of the gradient with respect to the parameters of the encoder,
    <math alttext="phi"><mi>φ</mi></math> . Although this may sound like a lot of
    jargon right now, we will go into each of these key observations in much more
    detail and concretely motivate the encoder-decoder architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. The overall VAE architecture presented in Kingma and Welling.
    Note that both z and the image after the decoder are both samples from the encoder
    distribution and decoder distribution, respectively.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let’s assume we have observed some data *x*, where each individual example
    can be denoted as <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> . Note that we are still
    under the assumption that there exist some set of latent variables *z* generating
    the data we’ve seen. We split our analysis over the observed data into one over
    each individual example <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> . We know there exists
    a true posterior over the latent variables <math alttext="p left-parenthesis z
    vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></math> , but we have no idea what that true posterior is. We
    assume it can be approximated by some distribution over the latent variables <math
    alttext="q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></math> , where *q* is a family of distributions in which
    optimization is much easier but complex enough to accurately model the true posterior.
    An example would be a multilayer neural network, which, as we’ve already seen,
    can be efficiently optimized via gradient descent and can represent complex, nonlinear
    functions. Note that each example <math alttext="x Superscript left-parenthesis
    i right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>
    we’ve seen has some true probability of occurrence, which we can write as <math
    alttext="p left-parenthesis x Superscript left-parenthesis i right-parenthesis
    Baseline right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></math> . We instead work with <math alttext="log p left-parenthesis
    x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mo
    form="prefix">log</mo> <mi>p</mi> <mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></math> , since this allows us to do some convenient decomposition
    into terms we’ve encountered before and doesn’t affect the verity of the optimization
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="log p left-parenthesis x Superscript left-parenthesis i right-parenthesis
    Baseline right-parenthesis equals log p left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline comma z right-parenthesis minus log p left-parenthesis
    z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mo
    form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>=</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo>
    <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>,</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo> <mi>p</mi>
    <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals log p left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline comma z right-parenthesis minus log p left-parenthesis
    z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis
    plus log q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis minus log q Subscript
    phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i
    right-parenthesis Baseline right-parenthesis"><mrow><mo>=</mo> <mo form="prefix">log</mo>
    <mi>p</mi> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>+</mo> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals double-struck upper E Subscript q Sub Subscript phi Subscript
    left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis
    Subscript right-parenthesis Baseline left-bracket log p left-parenthesis x Superscript
    left-parenthesis i right-parenthesis Baseline comma z right-parenthesis minus
    log p left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis
    Baseline right-parenthesis plus log q Subscript phi Baseline left-parenthesis
    z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis
    minus log q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis right-bracket"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>-</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo>
    <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow>
    <mo>+</mo> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo>
    <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals double-struck upper E Subscript q Sub Subscript phi Subscript
    left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis
    Subscript right-parenthesis Baseline left-bracket log StartFraction p left-parenthesis
    x Superscript left-parenthesis i right-parenthesis Baseline comma z right-parenthesis
    Over q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis EndFraction right-bracket plus
    double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis
    z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript
    right-parenthesis Baseline left-bracket log StartFraction q Subscript phi Baseline
    left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis
    Baseline right-parenthesis Over p left-parenthesis z vertical-bar x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis EndFraction right-bracket"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>,</mo><mi>z</mi><mo>)</mo></mrow>
    <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></mfrac>
    <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mfrac><mrow><msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow> <mrow><mi>p</mi><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mfrac> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: = <math alttext="ELBO plus KL left-parenthesis q Subscript phi Baseline left-parenthesis
    z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis
    StartAbsoluteValue EndAbsoluteValue p left-parenthesis z vertical-bar x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis right-parenthesis"><mrow><mtext>ELBO</mtext>
    <mo>+</mo> <mtext>KL</mtext> <mo>(</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo>
    <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>|</mo> <mo>|</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>z</mi>
    <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>)</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to express the marginal likelihood of the individual example
    <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> as a function of the
    example itself and the latent factors *z.* As we learned earlier, the marginal
    likelihood can be broken down into a quotient of the joint distribution <math
    alttext="p left-parenthesis z comma x Superscript left-parenthesis i right-parenthesis
    Baseline right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>z</mi> <mo>,</mo>
    <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></math>
    and the conditional distribution <math alttext="p left-parenthesis z vertical-bar
    x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></math> . The log function allows us to separate this quotient
    into a difference between the logs of the two terms. In the second step, we use
    a little trick that allows us to conveniently insert the approximate posterior
    into the equality—adding and subtracting the same term shouldn’t affect the equality.
    In the third step, we insert an expectation with respect to the approximate posterior.
    Why is this allowed? Well, a priori we know that <math alttext="log p left-parenthesis
    x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mo
    form="prefix">log</mo> <mi>p</mi> <mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></math> is a constant. It is just the log of the probability
    of the example occurring under the true distribution, which is fixed. Thus, taking
    the expectation on both sides doesn’t change anything about the left side of the
    equation, since the expectation of a constant is just the constant itself. On
    the right side, we have now gotten closer to expressing the log of the marginal
    likelihood in terms that we’ve seen before. In the second to last step, we combine
    logs back into quotients and use the linearity of expectation to arrive at a sum
    of two terms: (1) the KL divergence between the approximate posterior and the
    true posterior, and (2) the ELBO, or the evidence lower bound.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By now, you may have noticed that the form of the KL divergence is slightly
    different than what we encountered in [Chapter 2](ch02.xhtml#fundamentals-of-proba).
    Recall the standard KL divergence presented earlier, where the true distribution
    was *p(x)* and its approximation was *q(x)*. The KL divergence we defined was
    the difference between the cross entropy of the two distributions and the entropy
    of the true distribution, which was expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="double-struck upper E Subscript p left-parenthesis x right-parenthesis
    Baseline left-bracket log StartFraction p left-parenthesis x right-parenthesis
    Over q left-parenthesis x right-parenthesis EndFraction right-bracket"><mrow><msub><mi>𝔼</mi>
    <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the KL divergence in this derivation is the exact opposite.
    The expectation is with respect to the approximate posterior rather than the true
    posterior, and the numerator and denominator are flipped. Essentially what we
    see is <math alttext="double-struck upper E Subscript q left-parenthesis x right-parenthesis
    Baseline left-bracket log StartFraction q left-parenthesis x right-parenthesis
    Over p left-parenthesis x right-parenthesis EndFraction right-bracket"><mrow><msub><mi>𝔼</mi>
    <mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mfrac><mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow>
    <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>]</mo></mrow></mrow></math>
    instead of <math alttext="double-struck upper E Subscript p left-parenthesis x
    right-parenthesis Baseline left-bracket log StartFraction p left-parenthesis x
    right-parenthesis Over q left-parenthesis x right-parenthesis EndFraction right-bracket"><mrow><msub><mi>𝔼</mi>
    <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo>
    <mo form="prefix">log</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow>
    <mrow><mi>q</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mfrac> <mo>]</mo></mrow></mrow></math>
    . We call this the *reverse KL divergence,* since the roles of the model and the
    truth have been switched, and is the quantity we attempt to minimize in VAEs.
    Although this does not have as clean a physical interpretation as the standard
    KL, note that the reverse KL divergence is *just a type of KL divergence* and
    retains all the properties we discussed in [Chapter 2](ch02.xhtml#fundamentals-of-proba).Thus,
    optimizing the reverse KL divergence still achieves a unique global minimum of
    zero when <math alttext="q left-parenthesis x right-parenthesis equals p left-parenthesis
    x right-parenthesis comma for-all x"><mrow><mi>q</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo>
    <mo>=</mo> <mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>,</mo> <mo>∀</mo> <mi>x</mi></mrow></math>
    , so it is a valid objective to be optimizing over as it reaches its unique minimum
    when the approximate posterior is exactly the same as the true posterior.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reality, however, is that the true posterior <math alttext="p left-parenthesis
    z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></math> is still unknown to us. As a result, we can’t directly
    minimize any KL divergence with the true posterior. This is where the ELBO plays
    a key role. As we discussed earlier, <math alttext="log p left-parenthesis x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mo form="prefix">log</mo>
    <mi>p</mi> <mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></math> is a constant. Thus, minimizing the reverse KL divergence
    is the same as maximizing the ELBO. The name evidence lower bound should make
    more sense now—as we maximize this term, it provides a better and better lower
    bound on the true log probability of the example. If we can develop a methodology
    for maximizing the ELBO efficiently, we should be well on our way to developing
    a generative model. Let’s reformulate the ELBO into terms that might be easier
    to work with:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="double-struck upper E Subscript q Sub Subscript phi Subscript
    left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis
    Subscript right-parenthesis Baseline left-bracket log StartFraction p left-parenthesis
    x Superscript left-parenthesis i right-parenthesis Baseline comma z right-parenthesis
    Over q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis EndFraction right-bracket equals
    double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis
    z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript
    right-parenthesis Baseline left-bracket log p left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline comma z right-parenthesis minus log q Subscript phi
    Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis
    Baseline right-parenthesis right-bracket"><mrow><msub><mi>𝔼</mi> <mrow><msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mfrac><mrow><mi>p</mi><mo>(</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>,</mo><mi>z</mi><mo>)</mo></mrow>
    <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></mfrac>
    <mo>]</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mi>p</mi>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>,</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo>
    <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals double-struck upper E Subscript q Sub Subscript phi Subscript
    left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis
    Subscript right-parenthesis Baseline left-bracket log p left-parenthesis x Superscript
    left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis
    plus log p left-parenthesis z right-parenthesis minus log q Subscript phi Baseline
    left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis
    Baseline right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mi>p</mi>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>+</mo> <mo form="prefix">log</mo>
    <mi>p</mi> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo>
    <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals double-struck upper E Subscript q Sub Subscript phi Subscript
    left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis
    Subscript right-parenthesis Baseline left-bracket log p left-parenthesis x Superscript
    left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis
    right-bracket plus double-struck upper E Subscript q Sub Subscript phi Subscript
    left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis
    Subscript right-parenthesis Baseline left-bracket log p left-parenthesis z right-parenthesis
    minus log q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis right-bracket"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow> <mo>+</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mi>p</mi>
    <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>-</mo> <mo form="prefix">log</mo>
    <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals double-struck upper E Subscript q Sub Subscript phi Subscript
    left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis
    Subscript right-parenthesis Baseline left-bracket log p left-parenthesis x Superscript
    left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis
    right-bracket minus double-struck upper E Subscript q Sub Subscript phi Subscript
    left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis
    Subscript right-parenthesis Baseline left-bracket log StartFraction q Subscript
    phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i
    right-parenthesis Baseline right-parenthesis Over p left-parenthesis z right-parenthesis
    EndFraction right-bracket"><mrow><mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <mi>p</mi>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow> <mo>-</mo> <msub><mi>𝔼</mi>
    <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <mfrac><mrow><msub><mi>q</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow> <mrow><mi>p</mi><mo>(</mo><mi>z</mi><mo>)</mo></mrow></mfrac>
    <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals minus upper K upper L left-parenthesis q Subscript phi
    Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis
    Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue p left-parenthesis
    z right-parenthesis right-parenthesis plus double-struck upper E Subscript q Sub
    Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis
    i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p left-parenthesis
    x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis
    right-bracket"><mrow><mrow><mo>=</mo> <mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow>
    <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mrow><mo>|</mo>
    <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo>
    <mo>+</mo></mrow> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can start to see the beginnings of an architecture and an
    optimization procedure for maximizing the ELBO. For example, the first term is
    just the reverse KL divergence between the approximate posterior and the prior,
    which we already assumed to be a Gaussian distribution. We can use a neural network,
    or encoder, to represent the approximate posterior. The reverse KL divergence
    acts as a regularization term on the approximate posterior, since maximizing the
    negative of the reverse KL is the same as minimizing the reverse KL. Regularization
    prevents the approximate posterior from straying too far from the prior distribution.
    This is desirable since we have witnessed only a single example, and thus we don’t
    want our belief over the latent variables to shift too much from our prior. The
    second term is the expected true log likelihood of the example given a setting
    of latent variables *z,* where *z* is sampled from the approximate posterior.
    Wanting to maximize this quantity with respect to <math alttext="phi"><mi>φ</mi></math>
    is intuitively reasonable. This influences the approximate posterior to assign
    higher likelihoods to settings of *z* that, in turn, explain the input example
    <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> as well as possible.
    The balancing act between regularization, which prevents overfitting, and maximum
    likelihood estimation, which on its own would reach an optimum where <math alttext="q
    Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis"><mrow><msub><mi>q</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></math> is just a point mass over the setting of *z* that
    best describes <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> , is a classic optimization
    procedure you’ve likely encountered in many data science and machine learning
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, as noted earlier, we unfortunately don’t have access to the true conditional
    distribution <math alttext="p left-parenthesis x vertical-bar z right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></math> . Instead,
    we attempt to learn it using a second neural network—the decoder. We denote the
    parameters of the decoder as <math alttext="theta"><mi>θ</mi></math> and let the
    decoder represent the distribution <math alttext="p Subscript theta Baseline left-parenthesis
    x vertical-bar z right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math> . In summary,
    we perform the following optimization procedure:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="phi Superscript asterisk Baseline comma theta Superscript asterisk
    Baseline equals argmax Subscript phi comma theta Baseline minus upper K upper
    L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x
    Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue
    EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus
    double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis
    z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript
    right-parenthesis Baseline left-bracket log p Subscript theta Baseline left-parenthesis
    x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar z right-parenthesis
    right-bracket"><mrow><msup><mi>φ</mi> <mo>*</mo></msup> <mo>,</mo> <msup><mi>θ</mi>
    <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmax</mtext> <mrow><mi>φ</mi><mo>,</mo><mi>θ</mi></mrow></msub>
    <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo></mrow> <msub><mi>𝔼</mi>
    <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ve already discussed why this is a valid optimization procedure for the
    encoder parameters <math alttext="phi"><mi>φ</mi></math> , assuming that <math
    alttext="p Subscript theta Baseline left-parenthesis x vertical-bar z right-parenthesis
    equals p left-parenthesis x vertical-bar z right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    . Of course, this assumption is not satisfied at the beginning of training. However,
    as training progresses and <math alttext="theta"><mi>θ</mi></math> becomes more
    and more optimal, we eventually arrive at the desired theoretical optimization.
    But the question still remains: why is this a valid optimization procedure for
    <math alttext="theta"><mi>θ</mi></math> ? If we assume the encoder represents
    the true posterior distribution, we’d want to maximize the likelihood of recovering
    the original example <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> from our encoder samples
    *z*. Of course, just like the optimization of <math alttext="phi"><mi>φ</mi></math>
    , our assumption about the approximate posterior is not satisfied at the beginning
    of training—but as training progresses and two networks improve jointly, we hope
    to eventually reach our goal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads us into how to actually carry out the optimization. For <math alttext="theta"><mi>θ</mi></math>
    , it turns out we can use standard minibatch gradient descent techniques directly:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal nabla Subscript theta Baseline minus upper K upper L left-parenthesis
    q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue
    p left-parenthesis z right-parenthesis right-parenthesis plus double-struck upper
    E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub
    Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline
    left-bracket log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket"><mrow><msub><mi>∇</mi>
    <mi>θ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo></mrow> <msub><mi>𝔼</mi>
    <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals normal nabla Subscript theta Baseline minus upper K upper
    L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x
    Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue
    EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus
    normal nabla Subscript theta Baseline double-struck upper E Subscript q Sub Subscript
    phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis
    i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p Subscript
    theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis
    Baseline vertical-bar z right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>∇</mi>
    <mi>θ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo></mrow> <msub><mi>∇</mi>
    <mi>θ</mi></msub> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals normal nabla Subscript theta Baseline double-struck upper
    E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub
    Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline
    left-bracket log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket"><mrow><mo>=</mo>
    <msub><mi>∇</mi> <mi>θ</mi></msub> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals double-struck upper E Subscript q Sub Subscript phi Subscript
    left-parenthesis z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis
    Subscript right-parenthesis Baseline left-bracket normal nabla Subscript theta
    Baseline log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>∇</mi> <mi>θ</mi></msub> <mo form="prefix">log</mo>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="almost-equals StartFraction 1 Over n EndFraction sigma-summation
    Underscript j equals 1 Overscript n Endscripts normal nabla Subscript theta Baseline
    log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline vertical-bar z equals z Subscript j Baseline right-parenthesis"><mrow><mo>≈</mo>
    <mfrac><mn>1</mn> <mi>n</mi></mfrac> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msub><mi>∇</mi> <mi>θ</mi></msub> <mo form="prefix">log</mo>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>=</mo> <msub><mi>z</mi> <mi>j</mi></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: The first equality arises from the fact that the gradient of a sum of terms
    is equal to the sum of the gradients of each of the terms. Since the first term
    is not a function of <math alttext="theta"><mi>θ</mi></math> , its gradient with
    respect to <math alttext="theta"><mi>θ</mi></math> is 0, leading us to the second
    equality. From there we have the standard minibatch gradient estimate derivation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimization with respect to <math alttext="phi"><mi>φ</mi></math> is not
    as simple. If we try to do the same for <math alttext="phi"><mi>φ</mi></math>
    as we did for <math alttext="theta"><mi>θ</mi></math> , we run into an unforeseen
    issue:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal nabla Subscript phi Baseline minus upper K upper L left-parenthesis
    q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue
    p left-parenthesis z right-parenthesis right-parenthesis plus double-struck upper
    E Subscript q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub
    Superscript left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline
    left-bracket log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket"><mrow><msub><mi>∇</mi>
    <mi>φ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo></mrow> <msub><mi>𝔼</mi>
    <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals normal nabla Subscript phi Baseline minus upper K upper
    L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x
    Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue
    EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus
    normal nabla Subscript phi Baseline double-struck upper E Subscript q Sub Subscript
    phi Subscript left-parenthesis z vertical-bar x Sub Superscript left-parenthesis
    i right-parenthesis Subscript right-parenthesis Baseline left-bracket log p Subscript
    theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis
    Baseline vertical-bar z right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>∇</mi>
    <mi>φ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo></mrow> <msub><mi>∇</mi>
    <mi>φ</mi></msub> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals normal nabla Subscript phi Baseline minus upper K upper
    L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x
    Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue
    EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus
    normal nabla Subscript phi Baseline integral q Subscript phi Baseline left-parenthesis
    z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis
    log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline vertical-bar z right-parenthesis d z"><mrow><mo>=</mo>
    <msub><mi>∇</mi> <mi>φ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow>
    <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mrow><mo>|</mo>
    <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo>
    <mo>+</mo></mrow> <msub><mi>∇</mi> <mi>φ</mi></msub> <mo>∫</mo> <msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mi>d</mi> <mi>z</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals normal nabla Subscript phi Baseline minus upper K upper
    L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x
    Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue
    EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus
    integral normal nabla Subscript phi Baseline q Subscript phi Baseline left-parenthesis
    z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis
    log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline vertical-bar z right-parenthesis d z"><mrow><mo>=</mo>
    <msub><mi>∇</mi> <mi>φ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow>
    <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mrow><mo>|</mo>
    <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo>
    <mo>+</mo> <mo>∫</mo></mrow> <msub><mi>∇</mi> <mi>φ</mi></msub> <msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mi>d</mi> <mi>z</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'In the last step, we can’t express the second term as an expectation. This
    is because the gradient is with respect to the parameters of the distribution
    from which we are sampling. We can’t simply switch the order of the expectation
    and gradient as we did for <math alttext="theta"><mi>θ</mi></math> . To get around
    this, we make the following observation:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal nabla Subscript phi Baseline q Subscript phi Baseline
    left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis
    Baseline right-parenthesis equals normal nabla Subscript phi Baseline q Subscript
    phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i
    right-parenthesis Baseline right-parenthesis asterisk StartFraction q Subscript
    phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i
    right-parenthesis Baseline right-parenthesis Over q Subscript phi Baseline left-parenthesis
    z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis
    EndFraction"><mrow><msub><mi>∇</mi> <mi>φ</mi></msub> <msub><mi>q</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>∇</mi> <mi>φ</mi></msub> <msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>*</mo> <mfrac><mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow> <mrow><msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals q Subscript phi Baseline left-parenthesis z vertical-bar
    x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis
    asterisk StartFraction normal nabla Subscript phi Baseline q Subscript phi Baseline
    left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis
    Baseline right-parenthesis Over q Subscript phi Baseline left-parenthesis z vertical-bar
    x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis
    EndFraction"><mrow><mo>=</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo>
    <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>*</mo> <mfrac><mrow><msub><mi>∇</mi> <mi>φ</mi></msub> <msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></mfrac></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals q Subscript phi Baseline left-parenthesis z vertical-bar
    x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis
    normal nabla Subscript phi Baseline log q Subscript phi Baseline left-parenthesis
    z vertical-bar x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mo>=</mo>
    <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <msub><mi>∇</mi>
    <mi>φ</mi></msub> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'With a bit of calculus and algebra, we have derived an equivalent form for
    the gradient. If we substitute this reformulation into the step we were stuck
    on:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals normal nabla Subscript phi Baseline minus upper K upper
    L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x
    Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue
    EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus
    integral q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis normal nabla Subscript
    phi Baseline log q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis log p Subscript
    theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis
    Baseline vertical-bar z right-parenthesis d z"><mrow><mo>=</mo> <msub><mi>∇</mi>
    <mi>φ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo> <mo>∫</mo></mrow> <msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <msub><mi>∇</mi> <mi>φ</mi></msub> <mo form="prefix">log</mo>
    <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo form="prefix">log</mo>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mi>d</mi> <mi>z</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals normal nabla Subscript phi Baseline minus upper K upper
    L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar x
    Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis StartAbsoluteValue
    EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis plus
    double-struck upper E Subscript q Sub Subscript phi Subscript left-parenthesis
    z vertical-bar x Sub Superscript left-parenthesis i right-parenthesis Subscript
    right-parenthesis Baseline left-bracket normal nabla Subscript phi Baseline log
    q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis log p Subscript theta Baseline
    left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline vertical-bar
    z right-parenthesis right-bracket"><mrow><mo>=</mo> <msub><mi>∇</mi> <mi>φ</mi></msub>
    <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow> <msub><mi>q</mi> <mi>φ</mi></msub>
    <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mrow><mo>|</mo> <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo>
    <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo> <mo>+</mo></mrow> <msub><mi>𝔼</mi>
    <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>∇</mi> <mi>φ</mi></msub> <mo form="prefix">log</mo>
    <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mo form="prefix">log</mo>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="almost-equals normal nabla Subscript phi Baseline minus upper
    K upper L left-parenthesis q Subscript phi Baseline left-parenthesis z vertical-bar
    x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis
    StartAbsoluteValue EndAbsoluteValue p left-parenthesis z right-parenthesis right-parenthesis
    plus StartFraction 1 Over n EndFraction sigma-summation Underscript j equals 1
    Overscript n Endscripts normal nabla Subscript phi Baseline log q Subscript phi
    Baseline left-parenthesis z equals z Subscript j Baseline vertical-bar x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis log p Subscript
    theta Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis
    Baseline vertical-bar z equals z Subscript j Baseline right-parenthesis"><mrow><mo>≈</mo>
    <msub><mi>∇</mi> <mi>φ</mi></msub> <mrow><mo>-</mo> <mi>K</mi> <mi>L</mi> <mo>(</mo></mrow>
    <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow> <mrow><mo>|</mo>
    <mo>|</mo> <mi>p</mi></mrow> <mrow><mo>(</mo> <mi>z</mi> <mo>)</mo></mrow> <mrow><mo>)</mo>
    <mo>+</mo></mrow> <mfrac><mn>1</mn> <mi>n</mi></mfrac> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msub><mi>∇</mi> <mi>φ</mi></msub> <mo form="prefix">log</mo>
    <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>=</mo> <msub><mi>z</mi>
    <mi>j</mi></msub> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>=</mo> <msub><mi>z</mi> <mi>j</mi></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We can now use standard minibatch gradient estimation techniques to optimize
    our objective with respect to <math alttext="phi"><mi>φ</mi></math> . The observation
    we made is a well-known technique in the machine learning community termed the
    *log trick.* We will see this technique used again later in the chapter on reinforcement
    learning when we introduce the policy gradient method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have fully dissected the first observation that Kingma and Welling
    made, we now move to the second: the computation of a low variance estimate of
    the gradient with respect to <math alttext="phi"><mi>φ</mi></math> . As we mentioned
    earlier, the log trick allows us to estimate this gradient. However, this estimate
    has been shown to be of high variance. This means that if we were to run trials
    where, in each trial, we draw a few samples <math alttext="z Subscript j"><msub><mi>z</mi>
    <mi>j</mi></msub></math> from the approximate posterior and estimate the gradient
    with respect to <math alttext="phi"><mi>φ</mi></math> , we would expect to see
    vastly different estimates of the gradient across trials. Of course, this is undesirable,
    as we’d like trials for the same input example to be consistent with each other
    to have any confidence in our training procedure. We could try to ameliorate this
    by drawing many samples from the approximate posterior for each example, but this
    becomes computationally prohibitive for relatively little gain.'
  prefs: []
  type: TYPE_NORMAL
- en: Kingma and Welling proposed an alternative method to the log trick for getting
    around the issue of taking the gradient with respect to the weights of the network
    parametrizing distribution from which we are sampling. This method is called the
    *reparametrization trick*, and it allows us to compute a low variance estimate
    of the gradient, as opposed to the log trick. Why this is the case is beyond the
    scope of this text, but we refer you to the vast amount of academic literature
    that exists on this and similar topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reparametrization trick involves assuming the approximate posterior takes
    on some form, such as a multivariate Gaussian distribution, and then expressing
    this distribution as a function of another distribution that has no dependence
    on the weights of the encoder. Let’s assume that <math alttext="q Subscript phi
    Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis i right-parenthesis
    Baseline right-parenthesis"><mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo>
    <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></math> takes on the form <math alttext="upper N left-parenthesis
    z semicolon ModifyingAbove mu With caret Subscript phi Baseline comma ModifyingAbove
    sigma With caret Subscript phi Superscript 2 Baseline upper I right-parenthesis"><mrow><mi>N</mi>
    <mo>(</mo> <mi>z</mi> <mo>;</mo> <msub><mover accent="true"><mi>μ</mi> <mo>^</mo></mover>
    <mi>φ</mi></msub> <mo>,</mo> <msubsup><mover accent="true"><mi>σ</mi> <mo>^</mo></mover>
    <mi>φ</mi> <mn>2</mn></msubsup> <mi>I</mi> <mo>)</mo></mrow></math> . This represents
    a multivariate Gaussian distribution where each component <math alttext="z Subscript
    i"><msub><mi>z</mi> <mi>i</mi></msub></math> is independent of all other components
    and <math alttext="z Subscript i Baseline tilde upper N left-parenthesis mu Subscript
    phi comma i Baseline comma sigma Subscript phi comma i Superscript 2 Baseline
    right-parenthesis"><mrow><msub><mi>z</mi> <mi>i</mi></msub> <mo>∼</mo> <mi>N</mi>
    <mrow><mo>(</mo> <msub><mi>μ</mi> <mrow><mi>φ</mi><mo>,</mo><mi>i</mi></mrow></msub>
    <mo>,</mo> <msubsup><mi>σ</mi> <mrow><mi>φ</mi><mo>,</mo><mi>i</mi></mrow> <mn>2</mn></msubsup>
    <mo>)</mo></mrow></mrow></math> , <math alttext="for-all i"><mrow><mo>∀</mo> <mi>i</mi></mrow></math>
    . We use <math alttext="phi"><mi>φ</mi></math> in the subscript to explicitly
    show the approximate posterior’s dependence on the parameters of the encoder through
    its mean and variance vectors, which are defined by the encoder. In its current
    form, we run into the issue of not being able to switch the order of the expectation
    and the gradient that we encountered earlier. Using the reparametrization trick,
    we can rewrite the sampling procedure as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="z tilde upper N left-parenthesis ModifyingAbove mu With caret
    Subscript phi Baseline comma ModifyingAbove sigma With caret Subscript phi Superscript
    2 Baseline upper I right-parenthesis left right double arrow z equals ModifyingAbove
    mu With caret Subscript phi Baseline plus ModifyingAbove sigma With caret Subscript
    phi Baseline asterisk epsilon comma epsilon tilde upper N left-parenthesis 0 comma
    upper I right-parenthesis"><mrow><mi>z</mi> <mo>∼</mo> <mi>N</mi> <mrow><mo>(</mo>
    <msub><mover accent="true"><mi>μ</mi> <mo>^</mo></mover> <mi>φ</mi></msub> <mo>,</mo>
    <msubsup><mover accent="true"><mi>σ</mi> <mo>^</mo></mover> <mi>φ</mi> <mn>2</mn></msubsup>
    <mi>I</mi> <mo>)</mo></mrow> <mo>⇔</mo> <mi>z</mi> <mo>=</mo> <msub><mover accent="true"><mi>μ</mi>
    <mo>^</mo></mover> <mi>φ</mi></msub> <mo>+</mo> <msub><mover accent="true"><mi>σ</mi>
    <mo>^</mo></mover> <mi>φ</mi></msub> <mo>*</mo> <mi>ϵ</mi> <mo>,</mo> <mi>ϵ</mi>
    <mo>∼</mo> <mi>N</mi> <mrow><mo>(</mo> <mn>0</mn> <mo>,</mo> <mi>I</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: We highly encourage you to work out why the sampling procedure can be rewritten
    in this manner using the definition of the Gaussian distribution. It will be easier
    to consider the univariate case first, where *X* is a standard Gaussian random
    variable, and then show *Y = c*X* is a Gaussian random variable with mean zero
    and variance <math alttext="c squared"><msup><mi>c</mi> <mn>2</mn></msup></math>
    . Then, consider the general univariate case where *X* is any Gaussian random
    variable, and show *Y = X + c* is a Gaussian random variable with mean *E[X]*
    + *c* and variance *Var(X)*. Putting these steps together will get you to the
    reformulated sampling procedure described previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, we have expressed the approximate posterior as a function of a
    distribution that is independent of <math alttext="phi"><mi>φ</mi></math> , along
    with a mean vector and a standard deviation vector that are dependent on <math
    alttext="phi"><mi>φ</mi></math> . We term the random variable <math alttext="epsilon"><mi>ϵ</mi></math>
    an *auxiliary random variable.* Plugging this reformulation into our troublesome
    gradient expression from earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal nabla Subscript phi Baseline double-struck upper E Subscript
    q Sub Subscript phi Subscript left-parenthesis z vertical-bar x Sub Superscript
    left-parenthesis i right-parenthesis Subscript right-parenthesis Baseline left-bracket
    log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline vertical-bar z right-parenthesis right-bracket"><mrow><msub><mi>∇</mi>
    <mi>φ</mi></msub> <msub><mi>𝔼</mi> <mrow><msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo><mi>z</mi><mo>|</mo><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals normal nabla Subscript phi Baseline double-struck upper
    E Subscript epsilon tilde upper N left-parenthesis 0 comma upper I right-parenthesis
    Baseline left-bracket log p Subscript theta Baseline left-parenthesis x Superscript
    left-parenthesis i right-parenthesis Baseline vertical-bar g Subscript phi Baseline
    left-parenthesis epsilon right-parenthesis right-parenthesis right-bracket"><mrow><mo>=</mo>
    <msub><mi>∇</mi> <mi>φ</mi></msub> <msub><mi>𝔼</mi> <mrow><mi>ϵ</mi><mo>∼</mo><mi>N</mi><mo>(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <msub><mi>g</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>ϵ</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals double-struck upper E Subscript epsilon tilde upper N
    left-parenthesis 0 comma upper I right-parenthesis Baseline left-bracket normal
    nabla Subscript phi Baseline log p Subscript theta Baseline left-parenthesis x
    Superscript left-parenthesis i right-parenthesis Baseline vertical-bar g Subscript
    phi Baseline left-parenthesis epsilon right-parenthesis right-parenthesis right-bracket"><mrow><mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><mi>ϵ</mi><mo>∼</mo><mi>N</mi><mo>(</mo><mn>0</mn><mo>,</mo><mi>I</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo> <msub><mi>∇</mi> <mi>φ</mi></msub> <mo form="prefix">log</mo>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <msub><mi>g</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>ϵ</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="almost-equals StartFraction 1 Over n EndFraction sigma-summation
    Underscript j equals 1 Overscript n Endscripts normal nabla Subscript phi Baseline
    log p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline vertical-bar g Subscript phi Baseline left-parenthesis
    epsilon Subscript j Baseline right-parenthesis right-parenthesis"><mrow><mo>≈</mo>
    <mfrac><mn>1</mn> <mi>n</mi></mfrac> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msub><mi>∇</mi> <mi>φ</mi></msub> <mo form="prefix">log</mo>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>|</mo> <msub><mi>g</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <msub><mi>ϵ</mi>
    <mi>j</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Where <math alttext="g Subscript phi Baseline left-parenthesis epsilon right-parenthesis
    equals ModifyingAbove mu With caret Subscript phi Baseline plus ModifyingAbove
    sigma With caret Subscript phi Baseline asterisk epsilon"><mrow><msub><mi>g</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>ϵ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mover
    accent="true"><mi>μ</mi> <mo>^</mo></mover> <mi>φ</mi></msub> <mo>+</mo> <msub><mover
    accent="true"><mi>σ</mi> <mo>^</mo></mover> <mi>φ</mi></msub> <mo>*</mo> <mi>ϵ</mi></mrow></math>
    . We rewrote *z* as <math alttext="g Subscript phi Baseline left-parenthesis epsilon
    right-parenthesis"><mrow><msub><mi>g</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>ϵ</mi>
    <mo>)</mo></mrow></mrow></math> to explicitly show that the dependence on the
    encoder parameters is now only through the deterministic function applied to the
    sampling distribution, rather than the sampling distribution itself. This allows
    us to switch the order of the expectation and the gradient seamlessly, thereby
    lending it to standard minibatch gradient estimation techniques.
  prefs: []
  type: TYPE_NORMAL
- en: How does this change manifest itself in the encoder architecture? Earlier, when
    using the log trick, we could directly parametrize the approximate posterior via
    the encoder. Now, we instead have the encoder, for each example <math alttext="x
    Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>
    , output a vector of means <math alttext="ModifyingAbove mu With caret Subscript
    phi"><msub><mover accent="true"><mi>μ</mi> <mo>^</mo></mover> <mi>φ</mi></msub></math>
    , a vector of standard deviations <math alttext="ModifyingAbove sigma With caret
    Subscript phi"><msub><mover accent="true"><mi>σ</mi> <mo>^</mo></mover> <mi>φ</mi></msub></math>
    , and sample <math alttext="epsilon"><mi>ϵ</mi></math> from a standard Gaussian
    distribution that is completely separate from the encoder-decoder VAE architecture.
    Note that the reparametrization technique comes with its own restrictions—we must
    assume a form for the approximate posterior, in this case a Gaussian, that allows
    us to define a differentiable function such as <math alttext="g Subscript phi"><msub><mi>g</mi>
    <mi>φ</mi></msub></math> . However, there’s no guarantee the true posterior is
    Gaussian—it is most likely a complex distribution that cannot be represented as
    functions of our standard distributions. This is a trade-off we must make to achieve
    a low variance gradient estimate for tractable optimization ([Figure 10-5](#fig1005)).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. What the encoder looks like after the inclusion of reparametrization.
    It returns a mean and standard deviation vector, which we can combine with ϵ to
    generate the setting of z. The purpose of the circle versus rectangles is to show
    that the only sampling is happening for ϵ, completely independent of the encoder
    architecture. The mean and standard deviation vectors are produced deterministically
    from the input image. In addition, z is deterministic once we know the value of
    ϵ.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Note that the training procedure for a VAE is quite simple—the beast was in
    the motivation and mathematics behind the architecture and optimization. All we
    need to do is:'
  prefs: []
  type: TYPE_NORMAL
- en: Sample an example <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> from the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> through the encoder
    network to generate a vector of means <math alttext="ModifyingAbove mu With caret
    Subscript phi"><msub><mover accent="true"><mi>μ</mi> <mo>^</mo></mover> <mi>φ</mi></msub></math>
    and a vector of standard deviations <math alttext="ModifyingAbove sigma With caret
    Subscript phi"><msub><mover accent="true"><mi>σ</mi> <mo>^</mo></mover> <mi>φ</mi></msub></math>
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample <math alttext="epsilon"><mi>ϵ</mi></math> and calculate the result of
    <math alttext="g Subscript phi Baseline left-parenthesis epsilon right-parenthesis"><mrow><msub><mi>g</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>ϵ</mi> <mo>)</mo></mrow></mrow></math>
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the result through the decoder network, which now represents the distribution
    <math alttext="p Subscript theta Baseline left-parenthesis x vertical-bar z equals
    g Subscript phi Baseline left-parenthesis epsilon right-parenthesis right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>=</mo>
    <msub><mi>g</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>ϵ</mi> <mo>)</mo></mrow>
    <mo>)</mo></mrow></mrow></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Query this distribution with our initial example <math alttext="x Superscript
    left-parenthesis i right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math>
    and take the log of the resulting likelihood. This will be our *decoder loss.*
    If you took multiple samples of <math alttext="epsilon"><mi>ϵ</mi></math> in step
    3, run the above procedure for each sample, and average to get the decoder loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sum the decoder loss with <math alttext="minus upper K upper L left-parenthesis
    q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis StartAbsoluteValue EndAbsoluteValue
    p left-parenthesis z right-parenthesis right-parenthesis"><mrow><mo>-</mo> <mi>K</mi>
    <mi>L</mi> <mo>(</mo> <msub><mi>q</mi> <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi>
    <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>|</mo> <mo>|</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>z</mi>
    <mo>)</mo></mrow> <mo>)</mo></mrow></math> , the *encoder loss,* to get a final
    loss. Use the negative of the final loss in the next step since we want to maximize
    it instead of minimize it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform classical SGD/minibatch gradient descent to update <math alttext="phi"><mi>φ</mi></math>
    and <math alttext="theta"><mi>θ</mi></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have covered how to train a VAE, how do we utilize it as a generative
    model once it is trained? Note that we initially defined the generative process
    as <math alttext="p left-parenthesis x vertical-bar z right-parenthesis p left-parenthesis
    z right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi>
    <mo>)</mo> <mi>p</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></math> , where we
    start with some setting of the latent variables *z* sampled from the prior distribution
    and map *z* to an instance *x* in the data space via the conditional likelihood.
    We’ve already learned this generative process in the form of <math alttext="p
    Subscript theta Baseline left-parenthesis x vertical-bar z right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    , or the decoder, and assumed the prior distribution <math alttext="p left-parenthesis
    z right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>z</mi> <mo>)</mo></mrow></math>
    to be a multivariate standard Gaussian at the beginning. To generate samples from
    a VAE, we sample <math alttext="z Subscript i"><msub><mi>z</mi> <mi>i</mi></msub></math>
    from the prior distribution *p(z),* pass this sample through the decoder so it
    now represents the distribution <math alttext="p Subscript theta Baseline left-parenthesis
    x vertical-bar z equals z Subscript i Baseline right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>=</mo>
    <msub><mi>z</mi> <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math> , and finally
    sample <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>
    from <math alttext="p Subscript theta Baseline left-parenthesis x vertical-bar
    z equals z Subscript i Baseline right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>=</mo> <msub><mi>z</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math> . Note that we no longer need
    the approximate posterior at this step—however, it played a key role in the training
    of the decoder and is still useful in understanding how our latent variable distribution
    shifts after witnessing an example from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a VAE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will build a VAE from scratch in PyTorch. We will additionally
    provide some example training and testing code on the famous MNIST digits dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, here is a list of the packages you will need to reproduce
    this section on your own:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Let’s start with the encoder. As we discussed in the previous section, the encoder
    is a neural network that outputs a vector of means and a vector of standard deviations.
    Each index represents a univariate Gaussian, and the entire vector represents
    a multivariate Gaussian where each component is independent from the others. Though
    we are working with image data, for the sake of simplicity we convert each image
    into a vector by flattening it at the start. This allows us to apply standard,
    fully connected layers on the input. Since each image in the MNIST dataset is
    of size 28 × 28, each resulting representation is a 784-dimensional vector. We
    also need to decide on the number of components, or latent variables, we will
    use to represent the latent space. We can treat the number of components as a
    hyperparameter—if we notice that the decoder log likelihoods of input examples
    are consistently low even after a significant amount of training, this may indicate
    an approximate posterior that is not expressive enough. Increasing the number
    of components and retraining in this case is advisable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is example code for an encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'For the sake of simplicity, we leave out nonlinearities between the layers
    for now. Our encoder consists of two levels of layers. The first level operates
    on the input, embedding the vector into a lower dimensional representation. The
    second level operates on the 200-d representation and consists of two independent
    layers: one for determining the means of each of the univariate Gaussian components,
    and one for determining the standard deviations of each of the univariate Gaussian
    components. Here, we use 20 components. As we stated earlier, we assume <math
    alttext="q Subscript phi Baseline left-parenthesis z vertical-bar x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><msub><mi>q</mi>
    <mi>φ</mi></msub> <mrow><mo>(</mo> <mi>z</mi> <mo>|</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></math> takes the form of a multivariate Gaussian, where
    each component is independent of the others. Note that attempting to learn a full
    covariance matrix is computationally prohibitive (amongst other concerns), as
    its size grows quadratically with the number of components.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is example code for a decoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Again, we leave out the nonlinearities for the sake of simplicity. The decoder
    operates on the sampled *z*, which we know is a 20-d vector. The rest of the decoder
    architecture is symmetrical to the encoder, and outputs a distribution over the
    input data. Although not in the code just yet, there is a final sigmoid layer
    that will be applied to the output of the `recon_output` layer which, recall,
    squashes each input dimension into the range (0,1). Since we are working with
    the discrete MNIST dataset where each pixel is represented as either a zero or
    a one, the output of the final sigmoid layer is used to represent a Bernoulli
    distribution for each pixel. Recall the Bernoulli distribution from [Chapter 2](ch02.xhtml#fundamentals-of-proba),
    represented as *Ber(p),* where *p* is the probability of returning a one and 1
    *– p* is the the probability of returning a zero.
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, we have that the decoder likelihood distribution <math alttext="p
    Subscript theta Baseline left-parenthesis x vertical-bar z right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    can be rewritten as a product over each pixel:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="p Subscript theta Baseline left-parenthesis x vertical-bar z
    right-parenthesis equals product Underscript j equals 1 Overscript 784 Endscripts
    p Subscript theta Baseline left-parenthesis x Subscript j Baseline vertical-bar
    z right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo> <msubsup><mo>∏</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mn>784</mn></msubsup> <msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <msub><mi>x</mi> <mi>j</mi></msub> <mo>|</mo>
    <mi>z</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: where <math alttext="p left-parenthesis x Subscript j Baseline vertical-bar
    z right-parenthesis equals upper B e r left-parenthesis decoder left-parenthesis
    z right-parenthesis Subscript j Baseline right-parenthesis"><mrow><mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>j</mi></msub> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>B</mi> <mi>e</mi> <mi>r</mi> <mrow><mo>(</mo> <mtext>decoder</mtext> <msub><mrow><mo>(</mo><mi>z</mi><mo>)</mo></mrow>
    <mi>j</mi></msub> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Note that decoder(*z*) represents the 784-d vector after applying the sigmoid
    layer. For a given pixel <math alttext="x Subscript j Superscript left-parenthesis
    i right-parenthesis"><msubsup><mi>x</mi> <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup></math>
    in the input example <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> , we’d like its corresponding
    probability *p* to be close to one if <math alttext="x Subscript j Superscript
    left-parenthesis i right-parenthesis Baseline equals 1"><mrow><msubsup><mi>x</mi>
    <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup> <mo>=</mo> <mn>1</mn></mrow></math>
    , and its corresponding probability *p* to be close to zero if <math alttext="x
    Subscript j Superscript left-parenthesis i right-parenthesis Baseline equals 0"><mrow><msubsup><mi>x</mi>
    <mi>j</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup> <mo>=</mo> <mn>0</mn></mrow></math>
    . As you may recall from the previous section, we work with <math alttext="log
    p Subscript theta Baseline left-parenthesis x vertical-bar z right-parenthesis"><mrow><mo
    form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi>
    <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math> , which reduces to <math
    alttext="sigma-summation Underscript j equals 1 Overscript 784 Endscripts log
    p Subscript theta Baseline left-parenthesis x Subscript j Baseline vertical-bar
    z right-parenthesis"><mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mn>784</mn></msubsup> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>j</mi></msub> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can put the encoder and decoder together into a single VAE architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The call to encode is followed by the call to decode in the forward function.
    Note that decode uses only a single sample from the approximate posterior, as
    we found that a single sample is sufficient for the MNIST dataset, but this can
    be easily modified to work for multiple samples. To calculate the reverse KL,
    the forward function returns the results of the encode call in addition to the
    decoder likelihood distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is example code for computing the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We recommend you take a look at the PyTorch documentation for `nn.BCELoss`
    and verify that it is indeed computing the negative log likelihood of the input
    example <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> : <math alttext="minus
    sigma-summation Underscript j equals 1 Overscript 784 Endscripts log p Subscript
    theta Baseline left-parenthesis x Subscript j Superscript left-parenthesis i right-parenthesis
    Baseline vertical-bar z right-parenthesis"><mrow><mo>-</mo> <msubsup><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mn>784</mn></msubsup> <mo form="prefix">log</mo>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msubsup><mi>x</mi> <mi>j</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msubsup> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    . We also recommend you verify that the `kl_loss` term is the reverse KL divergence
    between two Gaussian distributions as derived in Kingma and Welling. Returning
    the sum of the negative log likelihood and the reverse KL divergence as a final
    loss term gets us to the end of step 6 from the previous section. Finally, for
    some training code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, we train the VAE for 10 epochs, saving the state of the VAE at the end
    of each epoch. Note that we set some hyperparameters fixed here, such as the learning
    rate of the optimizer and the number of latent variables. We recommend writing
    some validation code, in addition to the training code presented here, to select
    the best hyperparameter settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, how can we test the generative capabilities of our fully trained VAE?
    We know that the generative process can be written as *p(z)p(x|z),* where we first
    draw a sample <math alttext="z Subscript j"><msub><mi>z</mi> <mi>j</mi></msub></math>
    from our prior, run the sample through the decoder so the decoder’s likelihood
    distribution now represents <math alttext="p Subscript theta Baseline left-parenthesis
    x vertical-bar z equals z Subscript j Baseline right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>=</mo>
    <msub><mi>z</mi> <mi>j</mi></msub> <mo>)</mo></mrow></mrow></math> , and sample
    <math alttext="x Subscript j"><msub><mi>x</mi> <mi>j</mi></msub></math> from this
    distribution. Here is the code that puts this logic into action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `for` loop generates 100 samples from the approximate posterior, and for
    each of those samples, a sample from the corresponding decoder likelihood distribution
    over the input data. The last couple of lines of code allow us to save the samples
    in a 10 × 10 grid, depicted in [Figure 10-6](#hundred_samples_from_a_vae_trained_on_the_mnist_dataset).
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. 100 samples from a VAE trained on the MNIST dataset for 10 epochs.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Though the images are a bit blurry, we can make out digits in most of the samples.
    With more complex architectures such as RNNs, hyperparameter tuning, and longer
    training times, we will surely see even better results. In the next section, we
    introduce a slightly different take on generative models that has recently been
    achieving popularity.
  prefs: []
  type: TYPE_NORMAL
- en: Score-Based Generative Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we approach generative modeling through a slightly different
    lens than what we have encountered so far. In an optimally trained GAN, we first
    sample from some noise distribution *p(z)* and run this sample <math alttext="z
    Subscript i"><msub><mi>z</mi> <mi>i</mi></msub></math> through a generator *G,*
    which deterministically transforms <math alttext="z Subscript i"><msub><mi>z</mi>
    <mi>i</mi></msub></math> into a sample <math alttext="x Subscript i"><msub><mi>x</mi>
    <mi>i</mi></msub></math> from the true data distribution (where we approximate
    the true data distribution *p(x)* using our dataset, <math alttext="p Subscript
    data Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mtext>data</mtext></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> ). Though *G* itself
    is a deterministic function, *G(z)* is a random variable distributed as the true
    data distribution. In summary, we have implicitly defined a distribution over
    our domain via the generator’s action on samples from *p(z)*, and a way of sampling
    from the true data distribution via a simpler distribution *p(z)*, such as a multivariate
    Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: VAEs are more explicit in their probabilistic modeling. We define *z* to be
    a set of latent variables that generate the data we see, *x.* We explicitly learn
    a conditional distribution over the data <math alttext="p Subscript theta Baseline
    left-parenthesis x vertical-bar z right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    via the decoder, which we can sample from. In an optimally trained VAE, <math
    alttext="p Subscript theta Baseline left-parenthesis x vertical-bar z right-parenthesis
    equals p left-parenthesis x vertical-bar z right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow>
    <mo>=</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>)</mo></mrow></mrow></math>
    , is the true conditional likelihood of the data. To generate data using an optimally
    trained VAE, we first sample a setting of the latent variables from *p(z)* and
    run this sample <math alttext="z Subscript i"><msub><mi>z</mi> <mi>i</mi></msub></math>
    through the decoder, which now parametrizes the distribution <math alttext="p
    left-parenthesis x vertical-bar z equals z Subscript i Baseline right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>x</mi> <mo>|</mo> <mi>z</mi> <mo>=</mo> <msub><mi>z</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></math> . This is an explicit probability distribution we can
    now sample from.
  prefs: []
  type: TYPE_NORMAL
- en: Note that although GANs and VAEs themselves are quite distinct, both of their
    architectures and actions involve an additional distribution *p(z) (*whether that
    is a noise distribution in GANs or a prior over latent variables in VAEs). Is
    there a way of sampling from the true data distribution without the additional
    distribution? Score-based generative models attempt to do just that.
  prefs: []
  type: TYPE_NORMAL
- en: One method of sampling from a probability distribution is an iterative process
    called *Langevin dynamics.* This process is actually an instance of a class of
    algorithms referred to as *Markov Chain Monte Carlo (MCMC)* algorithms. Motivating
    MCMC algorithms and proving why they sample from probability distributions in
    an unbiased manner are beyond the scope of this section, but we refer you to the
    vast amount of academic literature that exists on this topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Langevin dynamics follows the process defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="x Superscript left-parenthesis i plus 1 right-parenthesis Baseline
    equals x Superscript left-parenthesis i right-parenthesis Baseline plus eta normal
    nabla Subscript x Baseline log p left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis plus StartRoot 2 eta EndRoot epsilon
    comma epsilon tilde upper N left-parenthesis 0 comma upper I right-parenthesis"><mrow><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msup> <mo>=</mo>
    <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>+</mo>
    <mi>η</mi> <msub><mi>∇</mi> <mi>x</mi></msub> <mo form="prefix">log</mo> <mi>p</mi>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>+</mo> <msqrt><mrow><mn>2</mn> <mi>η</mi></mrow></msqrt>
    <mi>ϵ</mi> <mo>,</mo> <mi>ϵ</mi> <mo>∼</mo> <mi>N</mi> <mrow><mo>(</mo> <mn>0</mn>
    <mo>,</mo> <mi>I</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> here represents a sample
    from *p(x),* and this dynamics equation shows us how to generate the next sample
    <math alttext="x Superscript left-parenthesis i plus 1 right-parenthesis"><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msup></math>
    given our current sample.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if we were to remove the Gaussian noise component at the end of the
    dynamics equation, we would just be following the gradient to a maximum of *p(x),*
    i.e., performing gradient ascent with some step-size <math alttext="eta"><mi>η</mi></math>
    . The intuition behind this dynamics equation is that the addition of the noise
    component prevents us from simply reaching the maximum *x* and instead allows
    us to explore regions with high probability, thereby exploring regions of low
    probability less ([Figure 10-7](#fig1007)). Again, why this produces samples from
    *p(x)* in an unbiased manner is beyond the scope of this text, but we highly encourage
    you to learn more from the academic literature.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. We use f here to represent a Gaussian distribution with mean (and
    also maximum) at the origin. Each of the contours represents locations with equal
    likelihood. As we can see from the diagram, the gradient points directly toward
    the maximum, but adding a bit of noise allows us to explore and sample from high-density
    regions without converging to the maximum.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although we use the gradient of the log probability instead of the gradient
    of the probability, the value(s) of *x* that maximizes <math alttext="log p left-parenthesis
    x right-parenthesis"><mrow><mo form="prefix">log</mo> <mi>p</mi> <mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></math> is the same as the value(s) of *x* that maximizes *p(x)*
    due to the log’s concaveness. More generally, the log’s concaveness also preserves
    the ordering relationships between all possible values of *x,* i.e., if <math
    alttext="p left-parenthesis x 1 right-parenthesis greater-than-or-equal-to p left-parenthesis
    x 2 right-parenthesis"><mrow><mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <mo>≥</mo> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow></mrow></math> , then <math alttext="log p left-parenthesis x
    1 right-parenthesis greater-than-or-equal-to log p left-parenthesis x 2 right-parenthesis"><mrow><mo
    form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>1</mn></msub>
    <mo>)</mo></mrow> <mo>≥</mo> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></math> , and vice
    versa. For that reason, as we saw in [“Implementing a VAE”](#vae-sect), these
    sorts of optimization processes tend to not be affected meaningfully by the inclusion
    of the log.
  prefs: []
  type: TYPE_NORMAL
- en: However, the main issue with Langevin dynamics, as we’ve encountered before
    with other generative models, is that we don’t know *p(x),* let alone the gradient
    of its log! But there may be a way to model <math alttext="normal nabla Subscript
    x Baseline log p left-parenthesis x right-parenthesis"><mrow><msub><mi>∇</mi>
    <mi>x</mi></msub> <mo form="prefix">log</mo> <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></mrow></math> , which we call <math alttext="p left-parenthesis
    x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    ’s *score function*, directly. This would allow us to simply plug the score directly
    into the Langevin dynamics equation and draw samples from *p(x)* as if we knew
    *p(x)* all along. This is the idea of score-based generative modeling.
  prefs: []
  type: TYPE_NORMAL
- en: For a moment, let’s forget the problem of sampling from an unknown distribution
    *p(x)* and instead consider the problem of learning *p(x)*. From now until the
    end of this section, we will consider only the problems of learning and sampling
    from continuous probability distributions. In the same vein of explicitly learning
    approximate probability distributions like in VAEs, we can try to approximate
    *p(x)* with a learned version <math alttext="p Subscript theta Baseline left-parenthesis
    x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> , where <math alttext="theta"><mi>θ</mi></math>
    represents the parameters of the learned model. What we envision is a learned
    function, such as a neural network, that takes as input an example *x* and outputs
    a likelihood <math alttext="p Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    . However, there is no way to ensure that <math alttext="integral p Subscript
    theta Baseline left-parenthesis x right-parenthesis d x equals 1"><mrow><mo>∫</mo>
    <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mi>d</mi> <mi>x</mi> <mo>=</mo> <mn>1</mn></mrow></math> , which is a necessary
    condition of any probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, we settle for learning what we call an unnormalized probability distribution
    <math alttext="q Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    . This is a function that takes an example *x* and outputs an unnormalized likelihood.
    We can, in theory, represent the normalized probability distribution <math alttext="p
    Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    via <math alttext="StartFraction q Subscript theta Baseline left-parenthesis x
    right-parenthesis Over upper Z left-parenthesis theta right-parenthesis EndFraction"><mfrac><mrow><msub><mi>q</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><mi>Z</mi><mo>(</mo><mi>θ</mi><mo>)</mo></mrow></mfrac></math>
    , where <math alttext="upper Z left-parenthesis theta right-parenthesis equals
    integral q Subscript theta Baseline left-parenthesis x right-parenthesis d x"><mrow><mi>Z</mi>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <mo>∫</mo> <msub><mi>q</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mi>d</mi> <mi>x</mi></mrow></math>
    . Unfortunately, this integral is generally intractable and has no closed form
    solution. Of course, there are exceptions to the rule. For example, <math alttext="upper
    Z left-parenthesis theta right-parenthesis equals sigma asterisk StartRoot 2 pi
    EndRoot"><mrow><mi>Z</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo>
    <mi>σ</mi> <mo>*</mo> <msqrt><mrow><mn>2</mn> <mi>π</mi></mrow></msqrt></mrow></math>
    for a univariate Gaussian distribution, where <math alttext="theta equals left-parenthesis
    mu comma sigma right-parenthesis"><mrow><mi>θ</mi> <mo>=</mo> <mo>(</mo> <mi>μ</mi>
    <mo>,</mo> <mi>σ</mi> <mo>)</mo></mrow></math> are the mean and standard deviation
    of the Gaussian. But if we’d like to model more expressive distributions via a
    neural network, for example, it is almost always impossible to tractably calculate
    <math alttext="upper Z left-parenthesis theta right-parenthesis"><mrow><mi>Z</mi>
    <mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></math> , which we will also refer to as
    the *partition function.*
  prefs: []
  type: TYPE_NORMAL
- en: How can we go about learning such an unnormalized probability distribution?
    Researchers have presented many approaches for learning <math alttext="q Subscript
    theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> throughout the history
    of machine learning and inference, but one particular method starts to bridge
    the gap between learning an unnormalized probability distribution and sampling
    from its normalized version via a process like Langevin dynamics. *Score matching*,
    or the idea of learning <math alttext="q Subscript theta Baseline left-parenthesis
    x right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> via minimizing the difference between
    the score function of <math alttext="q Subscript theta Baseline left-parenthesis
    x right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> and the score function of the true
    distribution <math alttext="p left-parenthesis x right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> , was first proposed by Hyvarinen
    in 2005.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we show that minimizing the difference as stated is equivalent to minimizing
    the difference between the score function of <math alttext="p Subscript theta
    Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> and the score function
    of <math alttext="p left-parenthesis x right-parenthesis colon"><mrow><mi>p</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo> <mo>:</mo></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal nabla Subscript x Baseline log q Subscript theta Baseline
    left-parenthesis x right-parenthesis equals normal nabla Subscript x Baseline
    log left-parenthesis p Subscript theta Baseline left-parenthesis x right-parenthesis
    asterisk upper Z left-parenthesis theta right-parenthesis right-parenthesis"><mrow><msub><mi>∇</mi>
    <mi>x</mi></msub> <mo form="prefix">log</mo> <msub><mi>q</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>∇</mi> <mi>x</mi></msub>
    <mo form="prefix">log</mo> <mrow><mo>(</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>*</mo> <mi>Z</mi> <mrow><mo>(</mo>
    <mi>θ</mi> <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals normal nabla Subscript x Baseline log p Subscript theta
    Baseline left-parenthesis x right-parenthesis plus normal nabla Subscript x Baseline
    log upper Z left-parenthesis theta right-parenthesis"><mrow><mo>=</mo> <msub><mi>∇</mi>
    <mi>x</mi></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow> <mo>+</mo> <msub><mi>∇</mi> <mi>x</mi></msub>
    <mo form="prefix">log</mo> <mi>Z</mi> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals normal nabla Subscript x Baseline log p Subscript theta
    Baseline left-parenthesis x right-parenthesis"><mrow><mo>=</mo> <msub><mi>∇</mi>
    <mi>x</mi></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that the score function of <math alttext="q Subscript theta Baseline
    left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> is the same as the
    score function of <math alttext="p Subscript theta Baseline left-parenthesis x
    right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow></mrow></math> , <math alttext="for-all x"><mrow><mo>∀</mo> <mi>x</mi></mrow></math>
    . This is because the log first separates the product of <math alttext="q Subscript
    theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> and the partition
    function into a sum of logs, and finally the gradient with respect to *x* eliminates
    the log of the partition function, since this term is solely dependent on the
    weights <math alttext="theta"><mi>θ</mi></math> and is not a function of *x* itself.
    Thus, the optimal <math alttext="theta"><mi>θ</mi></math> that minimizes the proposed
    difference is equivalent to the optimal <math alttext="theta"><mi>θ</mi></math>
    that minimizes the difference in scores between <math alttext="p Subscript theta
    Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> and *p(x)*. The following
    is the optimization procedure, which we call *explicit score matching:*
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper J left-parenthesis theta right-parenthesis equals double-struck
    upper E Subscript p left-parenthesis x right-parenthesis Baseline left-bracket
    one-half StartAbsoluteValue EndAbsoluteValue normal nabla Subscript x Baseline
    log q Subscript theta Baseline left-parenthesis x right-parenthesis minus normal
    nabla Subscript x Baseline log p left-parenthesis x right-parenthesis StartAbsoluteValue
    EndAbsoluteValue Subscript 2 Superscript 2 Baseline right-bracket"><mrow><mi>J</mi>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mrow><mo>|</mo>
    <mo>|</mo></mrow> <msub><mi>∇</mi> <mi>x</mi></msub> <mo form="prefix">log</mo>
    <msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>-</mo> <msub><mi>∇</mi> <mi>x</mi></msub> <msubsup><mrow><mo form="prefix">log</mo><mi>p</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>|</mo><mo>|</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup> <mrow><mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: '*<math alttext="theta Superscript asterisk Baseline equals argmin Subscript
    theta Baseline upper J left-parenthesis theta right-parenthesis"><mrow><msup><mi>θ</mi>
    <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmin</mtext> <mi>θ</mi></msub> <mi>J</mi>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>*'
  prefs: []
  type: TYPE_NORMAL
- en: The reason for the leading <math alttext="one-half"><mfrac><mn>1</mn> <mn>2</mn></mfrac></math>
    is to simplify the resulting gradient (cancels out with the 2 that will be pulled
    down from the square of the norm). Note that we have completely removed the dependence
    on the partition function in our analysis, and we now have a way to (1) learn
    an unnormalized distribution <math alttext="q Subscript theta Baseline left-parenthesis
    x right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> , and (2) calculate the score of <math
    alttext="p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></math> via our neural network. For item 1, in the case
    where we find a setting <math alttext="theta"><mi>θ</mi></math> that results in
    <math alttext="upper J left-parenthesis theta right-parenthesis equals 0"><mrow><mi>J</mi>
    <mo>(</mo> <mi>θ</mi> <mo>)</mo> <mo>=</mo> <mn>0</mn></mrow></math> , <math alttext="p
    Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    and *p(x)* are the same for all *x* since their gradients are the same for all
    *x.* Of course, in general, two functions that have the same gradients everywhere
    can still be different functions by being off from each other by a nonzero constant.
    However, in our case, these two functions cannot be off by a nonzero constant
    since they are both probability distributions that must sum to one. Thus, we have
    a valid optimization procedure for learning an unnormalized distribution that,
    when normalized, should approximate the true distribution well.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform item 2, in theory all we would need to do is first run our example
    <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> through our neural network
    to get <math alttext="q Subscript theta Baseline left-parenthesis x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><msub><mi>q</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></math> , take the log of <math alttext="q Subscript theta
    Baseline left-parenthesis x Superscript left-parenthesis i right-parenthesis Baseline
    right-parenthesis"><mrow><msub><mi>q</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>)</mo></mrow></mrow></math>
    , and backpropagate this result through our network all the way back to the input.
    We’ve already shown that the resultant score is equivalent to the score of <math
    alttext="p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></math> . Going forward, we will refer to the score function
    of <math alttext="p Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    (and <math alttext="q Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>q</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    ) as <math alttext="normal upper Psi Subscript theta Baseline left-parenthesis
    x right-parenthesis"><mrow><msub><mi>Ψ</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> , and the score function of <math alttext="p
    left-parenthesis x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    as <math alttext="normal upper Psi left-parenthesis x right-parenthesis"><mrow><mi>Ψ</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> . Using our new notation, we rewrite
    the explicit score matching objective as:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="theta Superscript asterisk Baseline equals argmin Subscript theta
    Baseline double-struck upper E Subscript p left-parenthesis x right-parenthesis
    Baseline left-bracket one-half StartAbsoluteValue EndAbsoluteValue normal upper
    Psi Subscript theta Baseline left-parenthesis x right-parenthesis minus normal
    upper Psi left-parenthesis x right-parenthesis StartAbsoluteValue EndAbsoluteValue
    Subscript 2 Superscript 2 Baseline right-bracket"><mrow><msup><mi>θ</mi> <mo>*</mo></msup>
    <mo>=</mo> <msub><mtext>argmin</mtext> <mi>θ</mi></msub> <msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mrow><mo>|</mo>
    <mo>|</mo></mrow> <msub><mi>Ψ</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>-</mo> <msubsup><mrow><mi>Ψ</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>|</mo><mo>|</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup> <mrow><mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we have gotten around the issue of the partition function, we still
    have no idea what <math alttext="normal upper Psi left-parenthesis x right-parenthesis"><mrow><mi>Ψ</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> is. Hyvarinen, in 2005, in addition
    to proposing the notion of explicit score matching, proved an amazing property
    regarding explicit score matching (satisfied under certain weak regularity conditions):'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="double-struck upper E Subscript p left-parenthesis x right-parenthesis
    Baseline left-bracket one-half StartAbsoluteValue EndAbsoluteValue normal upper
    Psi Subscript theta Baseline left-parenthesis x right-parenthesis minus normal
    upper Psi left-parenthesis x right-parenthesis StartAbsoluteValue EndAbsoluteValue
    Subscript 2 Superscript 2 Baseline right-bracket equals double-struck upper E
    Subscript p left-parenthesis x right-parenthesis Baseline left-bracket one-half
    StartAbsoluteValue EndAbsoluteValue normal upper Psi Subscript theta Baseline
    left-parenthesis x right-parenthesis StartAbsoluteValue EndAbsoluteValue Subscript
    2 Superscript 2 Baseline plus sigma-summation Underscript i equals 1 Overscript
    d Endscripts normal nabla Subscript x Sub Subscript i Subscript Baseline normal
    upper Psi Subscript theta comma i Baseline left-parenthesis x right-parenthesis
    plus c right-bracket"><mrow><msub><mi>𝔼</mi> <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub>
    <mrow><mo>[</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mrow><mo>|</mo>
    <mo>|</mo></mrow> <msub><mi>Ψ</mi> <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>-</mo> <msubsup><mrow><mi>Ψ</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>|</mo><mo>|</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup> <mrow><mo>]</mo> <mo>=</mo></mrow> <msub><mi>𝔼</mi>
    <mrow><mi>p</mi><mo>(</mo><mi>x</mi><mo>)</mo></mrow></msub> <mrow><mo>[</mo></mrow>
    <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mrow><mo>|</mo> <mo>|</mo></mrow> <msub><mi>Ψ</mi>
    <mi>θ</mi></msub> <msubsup><mrow><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>|</mo><mo>|</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>d</mi></msubsup> <msub><mi>∇</mi> <msub><mi>x</mi> <mi>i</mi></msub></msub>
    <msub><mi>Ψ</mi> <mrow><mi>θ</mi><mo>,</mo><mi>i</mi></mrow></msub> <mrow><mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>c</mi> <mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Where <math alttext="normal upper Psi Subscript theta comma i Baseline left-parenthesis
    x right-parenthesis equals normal nabla Subscript x Sub Subscript i Baseline log
    p Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>Ψ</mi>
    <mrow><mi>θ</mi><mo>,</mo><mi>i</mi></mrow></msub> <mrow><mo>(</mo> <mi>x</mi>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>∇</mi> <msub><mi>x</mi> <mi>i</mi></msub></msub>
    <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> —the score function is just a length
    *d* vector (assuming *x* is of *d* dimensions), where each index <math alttext="i"><mi>i</mi></math>
    corresponds with the partial derivative of the log probability with respect to
    <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math> . <math
    alttext="c"><mi>c</mi></math> is a constant that has no dependence on <math alttext="theta"><mi>θ</mi></math>
    , so it can simply be ignored during optimization. This is a method the community
    has come to know as *implicit score matching.*
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the equivalent expression has no dependence on the true probability
    distribution, and thus we can directly optimize <math alttext="theta"><mi>θ</mi></math>
    , using it as we would any other objective. Once we learn the optimal <math alttext="theta"><mi>θ</mi></math>
    , all we need to do to perform generative modeling is:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow the methodology presented earlier for calculating the score of <math
    alttext="p Subscript theta Baseline left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></math> : run the example through our learned network,
    take the log of the result, and backpropagate all the way to the input.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample <math alttext="epsilon"><mi>ϵ</mi></math> from *N(0,I).*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plug in the results of steps 1 and 2 into the Langevin dynamics equation to
    obtain the next sample, <math alttext="x Superscript left-parenthesis i plus 1
    right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msup></math>
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 1 through 3 with <math alttext="x Superscript left-parenthesis
    i plus 1 right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msup></math>
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This procedure allows us to draw samples from <math alttext="p Subscript theta
    Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mi>θ</mi></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> , which, as shown
    earlier, should approximate *p(x)* well once the network has been trained.
  prefs: []
  type: TYPE_NORMAL
- en: Can we do better than implicit score matching? For one, implicit score matching
    requires us to calculate second-order gradients, as can be seen from the <math
    alttext="sigma-summation Underscript i equals 1 Overscript d Endscripts normal
    nabla Subscript x Sub Subscript i Baseline normal upper Psi Subscript theta comma
    i Baseline left-parenthesis x right-parenthesis"><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>d</mi></msubsup> <msub><mi>∇</mi> <msub><mi>x</mi> <mi>i</mi></msub></msub>
    <msub><mi>Ψ</mi> <mrow><mi>θ</mi><mo>,</mo><mi>i</mi></mrow></msub> <mrow><mo>(</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> term in the implicit score matching
    objective. This can be quite computationally expensive depending on the size of
    *x.* In a framework such as PyTorch, this would require first calculating the
    first-order gradient through standard means such as backpropagation and then looping
    through each <math alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math>
    manually to compute its second-order gradient. In the next section, we will cover
    *denoising autoencoders* and *denoising score matching*, which modify the objective
    and allow us to get around these complexity issues.
  prefs: []
  type: TYPE_NORMAL
- en: Denoising Autoencoders and Score Matching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before explaining the connection between denoising autoencoders and score matching,
    we first motivate the denoising autoencoder architecture. In [Chapter 9](ch09.xhtml#ch07),
    we learned about autoencoders through the lens of representation learning. We
    used autoencoders to compress high-dimensional data, such as images, into low-dimensional
    representations that preserved the information, or useful features, necessary
    to reconstruct the original data. We additionally showed, through our experiments
    on MNIST, that we were able to reconstruct the data quite well and we generally
    saw, for instances of a given digit, clustering of its low-dimensional representations.
    This implies that if we were to train a standard classifier on these low-dimensional
    representations, with the label being their original digit categories, we’d expect
    to see great accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: However, depending on the data we try to compress, it turns out that, at times,
    our compressions aren’t able to capture useful features. In other words, when
    we use our trained autoencoder on real-world images outside of our sample that
    may be slightly corrupted, rotated, shifted, or captured under various light settings,
    our ability to classify these images using their low-dimensional representations
    takes a large dip.
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we would like our learned representations to be invariant to such noise.
    In 2008, Vincent proposed denoising autoencoders as a method for combating the
    issues we see with standard autoencoders. Denoising autoencoders first corrupt
    the original input data with noise, run the corrupted input through a standard
    autoencoder, and finally attempt to reconstruct the original input ([Figure 10-8](#fig1008)).
    The original paper used a corruption scheme that randomly zeroed out some portion
    of the input, but acknowledged that a variety of corruption schemes could be used
    instead. Intuitively, the representations learned from such a procedure should
    be much more robust to the challenges presented by real-world images. Indeed,
    the experiments on MNIST by Vincent in 2008 showed that, under various data augmentations
    such as rotation and background noise, the denoising autoencoder performed significantly
    better than the standard autoencoder in terms of classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](Images/fdl2_1008.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-8\. The denoising autoencoder architecture is the same as that of
    the standard autoencoder, except instead of minimizing the reconstruction error
    between y and the input x', we minimize the reconstruction error between y and
    the original x.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Following Vincent 2011, which first noticed the connection between denoising
    AEs and score matching, we instead define the corruption scheme to be the addition
    of Gaussian noise to the original data. Formally, we have that <math alttext="p
    left-parenthesis x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    represents the true distribution of the data, <math alttext="p Subscript d a t
    a Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> represents the distribution
    of the data using our training set, and <math alttext="p Subscript sigma Baseline
    left-parenthesis x prime vertical-bar x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>''</mo></msup> <mo>|</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> represents the conditional distribution
    of the corrupted data given the original data. In particular:'
  prefs: []
  type: TYPE_NORMAL
- en: "<math alttext=\"p Subscript sigma Baseline left-parenthesis x prime vertical-bar\
    \ x right-parenthesis equals upper N left-parenthesis x prime semicolon x comma\
    \ sigma squared upper I right-parenthesis\"><mrow><msub><mi>p</mi> <mi>σ</mi></msub>\
    \ <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow>\
    \ <mo>=</mo> <mi>N</mi> <mrow><mo>(</mo> <mi>x</mi> <mi>â</mi> <mi>\x80</mi> <mi>\x99\
    </mi> <mo>;</mo> <mi>x</mi> <mo>,</mo> <msup><mi>σ</mi> <mn>2</mn></msup> <mi>I</mi>\
    \ <mo>)</mo></mrow></mrow></math>"
  prefs: []
  type: TYPE_NORMAL
- en: 'Where the mean of the distribution is the original data and the subscript <math
    alttext="sigma"><mi>σ</mi></math> represents the standard deviation of the Gaussian
    noise applied to the original data. Note that *x’* and *x* are defined over the
    same domain (all possible images, for example). We can now calculate the distribution
    over the corrupted data:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="p Subscript sigma Baseline left-parenthesis x prime right-parenthesis
    equals sigma-summation Underscript x Endscripts p Subscript sigma Baseline left-parenthesis
    x prime vertical-bar x right-parenthesis p left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mo>∑</mo> <mi>x</mi></msub> <msub><mi>p</mi> <mi>σ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mi>p</mi> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="almost-equals sigma-summation Underscript x Endscripts p Subscript
    sigma Baseline left-parenthesis x prime vertical-bar x right-parenthesis p Subscript
    d a t a Baseline left-parenthesis x right-parenthesis"><mrow><mo>≈</mo> <msub><mo>∑</mo>
    <mi>x</mi></msub> <msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi>
    <mo>'</mo></msup> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow> <msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals StartFraction 1 Over n EndFraction sigma-summation Underscript
    i equals 1 Overscript n Endscripts p Subscript sigma Baseline left-parenthesis
    x prime vertical-bar x equals x Superscript left-parenthesis i right-parenthesis
    Baseline right-parenthesis"><mrow><mo>=</mo> <mfrac><mn>1</mn> <mi>n</mi></mfrac>
    <msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup>
    <msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup>
    <mo>|</mo> <mi>x</mi> <mo>=</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: which is the empirical average over the conditional probabilities using each
    data point from our dataset as the reference. This follows naturally from letting
    the true distribution be approximated by the distribution defined by the dataset
    (same as how this was defined in [“Generative Adversarial Networks”](#gans-sect)).
  prefs: []
  type: TYPE_NORMAL
- en: In 2011, Vincent explored the possibility of using <math alttext="p Subscript
    sigma Baseline left-parenthesis x prime right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math>
    as the reference instead of <math alttext="p left-parenthesis x right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> as we do in explicit score matching.
    The reasoning for this is that <math alttext="p Subscript sigma Baseline left-parenthesis
    x prime right-parenthesis"><mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo>
    <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math> can be viewed
    as a continuous approximation to the true distribution <math alttext="p left-parenthesis
    x right-parenthesis"><mrow><mi>p</mi> <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math>
    . The approximation defined by <math alttext="p Subscript d a t a Baseline left-parenthesis
    x right-parenthesis"><mrow><msub><mi>p</mi> <mrow><mi>d</mi><mi>a</mi><mi>t</mi><mi>a</mi></mrow></msub>
    <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math> is unbiased, but is
    unfortunately discontinuous everywhere *x* is not present in the dataset due to
    being a uniform distribution over all images in the dataset, with a likelihood
    of zero everywhere else. Of course, as <math alttext="sigma"><mi>σ</mi></math>
    gets larger, <math alttext="p Subscript sigma Baseline left-parenthesis x prime
    right-parenthesis"><mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi>
    <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math> is seen as a less and less faithful
    approximation to <math alttext="p left-parenthesis x right-parenthesis"><mrow><mi>p</mi>
    <mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></math> , so we’d like to work with small
    <math alttext="sigma"><mi>σ</mi></math> ’s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Vincent 2011 first proposed explicit score matching using <math alttext="p
    Subscript sigma Baseline left-parenthesis x prime right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>''</mo></msup> <mo>)</mo></mrow></mrow></math>
    as the reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '*<math alttext="upper J left-parenthesis theta right-parenthesis equals double-struck
    upper E Subscript p Sub Subscript sigma Subscript left-parenthesis x prime right-parenthesis
    Baseline left-bracket one-half StartAbsoluteValue EndAbsoluteValue normal nabla
    Subscript x prime Baseline log p Subscript theta Baseline left-parenthesis x prime
    right-parenthesis minus normal nabla Subscript x prime Baseline log p Subscript
    sigma Baseline left-parenthesis x prime right-parenthesis StartAbsoluteValue EndAbsoluteValue
    Subscript 2 Superscript 2 Baseline right-bracket"><mrow><mi>J</mi> <mrow><mo>(</mo>
    <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi>
    <mi>σ</mi></msub> <mrow><mo>(</mo><msup><mi>x</mi> <mo>''</mo></msup> <mo>)</mo></mrow></mrow></msub>
    <mrow><mo>[</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <mrow><mo>|</mo>
    <mo>|</mo></mrow> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>''</mo></msup></msub>
    <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <msup><mi>x</mi> <mo>''</mo></msup> <mo>)</mo></mrow> <mo>-</mo> <msub><mi>∇</mi>
    <msup><mi>x</mi> <mo>''</mo></msup></msub> <mo form="prefix">log</mo> <msub><mi>p</mi>
    <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>''</mo></msup> <mo>)</mo></mrow>
    <msubsup><mrow><mo>|</mo><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup> <mrow><mo>]</mo></mrow></mrow></math>*'
  prefs: []
  type: TYPE_NORMAL
- en: '*<math alttext="theta Superscript asterisk Baseline equals argmin Subscript
    theta Baseline upper J left-parenthesis theta right-parenthesis"><mrow><msup><mi>θ</mi>
    <mo>*</mo></msup> <mo>=</mo> <msub><mtext>argmin</mtext> <mi>θ</mi></msub> <mi>J</mi>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the same reasoning for why this is a valid optimization procedure
    for <math alttext="p Subscript theta Baseline left-parenthesis x right-parenthesis"><mrow><msub><mi>p</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
    is the same as in the previous section—the only difference here is the reference
    distribution we are trying to match. Vincent 2011 actually goes an extra step
    and shows that this optimization procedure is equivalent to:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper J Subscript DSM Baseline left-parenthesis theta right-parenthesis
    equals double-struck upper E Subscript p Sub Subscript sigma Subscript left-parenthesis
    x comma x Sub Superscript prime Subscript right-parenthesis Baseline left-bracket
    one-half StartAbsoluteValue EndAbsoluteValue normal nabla Subscript x prime Baseline
    log p Subscript theta Baseline left-parenthesis x prime right-parenthesis minus
    normal nabla Subscript x prime Baseline log p Subscript sigma Baseline left-parenthesis
    x prime vertical-bar x right-parenthesis StartAbsoluteValue EndAbsoluteValue Subscript
    2 Superscript 2 Baseline right-bracket"><mrow><msub><mi>J</mi> <mtext>DSM</mtext></msub>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi>
    <mi>σ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi> <mo>'</mo></msup>
    <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac>
    <mrow><mo>|</mo> <mo>|</mo></mrow> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub>
    <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>θ</mi></msub> <mrow><mo>(</mo>
    <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>-</mo> <msub><mi>∇</mi>
    <msup><mi>x</mi> <mo>'</mo></msup></msub> <mo form="prefix">log</mo> <msub><mi>p</mi>
    <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo>
    <mi>x</mi> <mo>)</mo></mrow> <msubsup><mrow><mo>|</mo><mo>|</mo></mrow> <mn>2</mn>
    <mn>2</mn></msubsup> <mrow><mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="theta Subscript DSM Superscript asterisk Baseline equals argmin
    Subscript theta Baseline upper J Subscript DSM Baseline left-parenthesis theta
    right-parenthesis"><mrow><msubsup><mi>θ</mi> <mtext>DSM</mtext> <mo>*</mo></msubsup>
    <mo>=</mo> <msub><mtext>argmin</mtext> <mi>θ</mi></msub> <msub><mi>J</mi> <mtext>DSM</mtext></msub>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Although we won’t show the proof here and refer you to Vincent 2011 for the
    full details, it does utilize the log trick we described in [“Implementing a VAE”](#vae-sect).
    We refer to optimizing this objective as *denoising score matching,* or *DSM*
    for short*,* and as we will show soon, it serves as the connection to denoising
    AEs.
  prefs: []
  type: TYPE_NORMAL
- en: "We know that <math alttext=\"p Subscript sigma Baseline left-parenthesis x\
    \ prime vertical-bar x right-parenthesis equals upper N left-parenthesis x prime\
    \ semicolon x comma sigma squared upper I right-parenthesis\"><mrow><msub><mi>p</mi>\
    \ <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo>\
    \ <mi>x</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>N</mi> <mrow><mo>(</mo> <mi>x</mi>\
    \ <mi>â</mi> <mi>\x80</mi> <mi>\x99</mi> <mo>;</mo> <mi>x</mi> <mo>,</mo> <msup><mi>σ</mi>\
    \ <mn>2</mn></msup> <mi>I</mi> <mo>)</mo></mrow></mrow></math> , and now compute\
    \ the gradient of its log:"
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="normal nabla Subscript x prime Baseline log p Subscript sigma
    Baseline left-parenthesis x prime vertical-bar x right-parenthesis equals normal
    nabla Subscript x prime Baseline log left-parenthesis StartFraction 1 Over StartRoot
    left-parenthesis 2 pi right-parenthesis Superscript d Baseline StartAbsoluteValue
    sigma squared upper I EndAbsoluteValue EndRoot EndFraction e Superscript StartFraction
    minus left-parenthesis x prime minus x right-parenthesis Super Superscript upper
    T Superscript left-parenthesis x prime minus x right-parenthesis Over 2 sigma
    squared EndFraction Baseline right-parenthesis"><mrow><msub><mi>∇</mi> <msup><mi>x</mi>
    <mo>'</mo></msup></msub> <mo form="prefix">log</mo> <msub><mi>p</mi> <mi>σ</mi></msub>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo> <mi>x</mi> <mo>)</mo></mrow>
    <mo>=</mo> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <mo form="prefix">log</mo>
    <mrow><mo>(</mo> <mfrac><mn>1</mn> <msqrt><mrow><msup><mrow><mo>(</mo><mn>2</mn><mi>π</mi><mo>)</mo></mrow>
    <mi>d</mi></msup> <mrow><mo>|</mo><msup><mi>σ</mi> <mn>2</mn></msup> <mi>I</mi><mo>|</mo></mrow></mrow></msqrt></mfrac>
    <msup><mi>e</mi> <mfrac><mrow><mo>-</mo><msup><mrow><mo>(</mo><msup><mi>x</mi>
    <mo>'</mo></msup> <mo>-</mo><mi>x</mi><mo>)</mo></mrow> <mi>T</mi></msup> <mrow><mo>(</mo><msup><mi>x</mi>
    <mo>'</mo></msup> <mo>-</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><mn>2</mn><msup><mi>σ</mi>
    <mn>2</mn></msup></mrow></mfrac></msup> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals normal nabla Subscript x prime Baseline log StartFraction
    1 Over StartRoot left-parenthesis 2 pi right-parenthesis Superscript d Baseline
    StartAbsoluteValue sigma squared upper I EndAbsoluteValue EndRoot EndFraction
    plus normal nabla Subscript x prime Baseline log e Superscript StartFraction minus
    left-parenthesis x prime minus x right-parenthesis Super Superscript upper T Superscript
    left-parenthesis x prime minus x right-parenthesis Over 2 sigma squared EndFraction"><mrow><mo>=</mo>
    <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <mo form="prefix">log</mo>
    <mfrac><mn>1</mn> <msqrt><mrow><msup><mrow><mo>(</mo><mn>2</mn><mi>π</mi><mo>)</mo></mrow>
    <mi>d</mi></msup> <mrow><mo>|</mo><msup><mi>σ</mi> <mn>2</mn></msup> <mi>I</mi><mo>|</mo></mrow></mrow></msqrt></mfrac>
    <mo>+</mo> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <mo form="prefix">log</mo>
    <msup><mi>e</mi> <mfrac><mrow><mo>-</mo><msup><mrow><mo>(</mo><msup><mi>x</mi>
    <mo>'</mo></msup> <mo>-</mo><mi>x</mi><mo>)</mo></mrow> <mi>T</mi></msup> <mrow><mo>(</mo><msup><mi>x</mi>
    <mo>'</mo></msup> <mo>-</mo><mi>x</mi><mo>)</mo></mrow></mrow> <mrow><mn>2</mn><msup><mi>σ</mi>
    <mn>2</mn></msup></mrow></mfrac></msup></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals minus StartFraction 1 Over 2 sigma squared EndFraction
    normal nabla Subscript x prime Baseline left-parenthesis x prime minus x right-parenthesis
    Superscript upper T Baseline left-parenthesis x prime minus x right-parenthesis"><mrow><mo>=</mo>
    <mo>-</mo> <mfrac><mn>1</mn> <mrow><mn>2</mn><msup><mi>σ</mi> <mn>2</mn></msup></mrow></mfrac>
    <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <msup><mrow><mo>(</mo><msup><mi>x</mi>
    <mo>'</mo></msup> <mo>-</mo><mi>x</mi><mo>)</mo></mrow> <mi>T</mi></msup> <mrow><mo>(</mo>
    <msup><mi>x</mi> <mo>'</mo></msup> <mo>-</mo> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals minus StartFraction 1 Over 2 sigma squared EndFraction
    left-parenthesis normal nabla Subscript x prime Baseline x Superscript prime upper
    T Baseline x prime minus 2 normal nabla Subscript x prime Baseline x Superscript
    prime upper T Baseline x plus normal nabla Subscript x prime Baseline x Superscript
    upper T Baseline x right-parenthesis"><mrow><mo>=</mo> <mo>-</mo> <mfrac><mn>1</mn>
    <mrow><mn>2</mn><msup><mi>σ</mi> <mn>2</mn></msup></mrow></mfrac> <mrow><mo>(</mo>
    <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub> <msup><mi>x</mi> <mrow><mo>'</mo><mi>T</mi></mrow></msup>
    <msup><mi>x</mi> <mo>'</mo></msup> <mo>-</mo> <mn>2</mn> <msub><mi>∇</mi> <msup><mi>x</mi>
    <mo>'</mo></msup></msub> <msup><mi>x</mi> <mrow><mo>'</mo><mi>T</mi></mrow></msup>
    <mi>x</mi> <mo>+</mo> <msub><mi>∇</mi> <msup><mi>x</mi> <mo>'</mo></msup></msub>
    <msup><mi>x</mi> <mi>T</mi></msup> <mi>x</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals StartFraction 1 Over sigma squared EndFraction left-parenthesis
    x minus x prime right-parenthesis"><mrow><mo>=</mo> <mfrac><mn>1</mn> <msup><mi>σ</mi>
    <mn>2</mn></msup></mfrac> <mrow><mo>(</mo> <mi>x</mi> <mo>-</mo> <msup><mi>x</mi>
    <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: Let’s break down the math. The first equality is simply the definition of a
    Gaussian distribution with mean *x* and variance <math alttext="sigma squared
    upper I"><mrow><msup><mi>σ</mi> <mn>2</mn></msup> <mi>I</mi></mrow></math> . The
    second equality is a result of the log breaking up the product into a sum of logs,
    and the gradient of a sum being the sum of gradients. In the third equality, we
    see the first term has been removed since it is not a function of *x’*, and thus
    its gradient is zero. Additionally, the log of *e* raised to any power is just
    the power itself, since log as used here has base *e*. Finally, we expand out
    the dot product of *x’ – x* with itself and apply the gradient to each individual
    term of the resulting sum. Note that we can simply rewrite <math alttext="minus
    x Superscript prime upper T Baseline x minus x Superscript upper T Baseline x
    prime"><mrow><mo>-</mo> <msup><mi>x</mi> <mrow><mo>'</mo><mi>T</mi></mrow></msup>
    <mi>x</mi> <mo>-</mo> <msup><mi>x</mi> <mi>T</mi></msup> <msup><mi>x</mi> <mo>'</mo></msup></mrow></math>
    as <math alttext="minus 2 x Superscript prime upper T Baseline x"><mrow><mo>-</mo>
    <mn>2</mn> <msup><mi>x</mi> <mrow><mo>'</mo><mi>T</mi></mrow></msup> <mi>x</mi></mrow></math>
    since the two terms are transposes of each other and result in the same scalar.
    We refer you to an amazing text called *The Matrix Cookbook* by KB Petersen and
    Michael Syskind Pedersen, which can serve as a guide to evaluating these gradients
    (plus more) and arrive at the final equality. The intuition for the gradient of
    <math alttext="x Superscript prime upper T Baseline x prime"><mrow><msup><mi>x</mi>
    <mrow><mo>'</mo><mi>T</mi></mrow></msup> <msup><mi>x</mi> <mo>'</mo></msup></mrow></math>
    is that it is the analog of the derivative of the square of a variable from single-variable
    calculus.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the final step, we will show that optimizing the objective for denoising
    score matching is equivalent to optimizing the objective for denoising AEs. To
    recap, a denoising AE has the same architecture as that of a standard AE—the only
    difference is in the input data and the training objective. The training objective
    of the denoising AE looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper J Subscript DAE Baseline left-parenthesis theta right-parenthesis
    equals double-struck upper E Subscript p Sub Subscript sigma Subscript left-parenthesis
    x comma x Sub Superscript prime Subscript right-parenthesis Baseline left-bracket
    StartAbsoluteValue EndAbsoluteValue decode left-parenthesis encode left-parenthesis
    x Superscript prime Baseline right-parenthesis right-parenthesis minus x StartAbsoluteValue
    EndAbsoluteValue Subscript 2 Superscript 2 Baseline right-bracket"><mrow><msub><mi>J</mi>
    <mtext>DAE</mtext></msub> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo>
    <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi>
    <mo>'</mo></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo>|</mo>
    <mo>|</mo> <mtext>decode</mtext></mrow> <mrow><mo>(</mo> <mtext>encode</mtext>
    <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>-</mo> <msubsup><mrow><mi>x</mi><mo>|</mo><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup>
    <mrow><mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="theta Subscript DAE Superscript asterisk Baseline equals argmin
    Subscript theta Baseline upper J Subscript DAE Baseline left-parenthesis theta
    right-parenthesis"><mrow><msubsup><mi>θ</mi> <mtext>DAE</mtext> <mo>*</mo></msubsup>
    <mo>=</mo> <msub><mtext>argmin</mtext> <mi>θ</mi></msub> <msub><mi>J</mi> <mtext>DAE</mtext></msub>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the parameters, or weights, of both decode() and encode() are encompassed
    by <math alttext="theta"><mi>θ</mi></math> . To summarize, we must show that <math
    alttext="theta Subscript DAE Superscript asterisk"><msubsup><mi>θ</mi> <mtext>DAE</mtext>
    <mo>*</mo></msubsup></math> and <math alttext="theta Subscript DSM Superscript
    asterisk"><msubsup><mi>θ</mi> <mtext>DSM</mtext> <mo>*</mo></msubsup></math> defined
    earlier are equivalent for some form of the unnormalized likelihood. Once again,
    following Vincent 2011, we define the denoising autoencoder as an encoder consisting
    of a single fully connected layer followed by a sigmoid layer and a decoder consisting
    solely of a single fully connected layer. Additionally, we add the constraint
    that the two fully connected layers are weight-tied so that they are transposes
    of each other. The training objective can now be specified as, where <math alttext="theta
    equals left-parenthesis upper W comma b comma c right-parenthesis"><mrow><mi>θ</mi>
    <mo>=</mo> <mo>(</mo> <mi>W</mi> <mo>,</mo> <mi>b</mi> <mo>,</mo> <mi>c</mi> <mo>)</mo></mrow></math>
    :'
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="upper J Subscript DAE Baseline left-parenthesis theta right-parenthesis
    equals double-struck upper E Subscript p Sub Subscript sigma Subscript left-parenthesis
    x comma x Sub Superscript prime Subscript right-parenthesis Baseline left-bracket
    StartAbsoluteValue EndAbsoluteValue upper W Superscript upper T Baseline left-parenthesis
    upper W x prime plus b right-parenthesis plus c minus x StartAbsoluteValue EndAbsoluteValue
    Subscript 2 Superscript 2 Baseline right-bracket"><mrow><msub><mi>J</mi> <mtext>DAE</mtext></msub>
    <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow> <mo>=</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi>
    <mi>σ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi> <mo>'</mo></msup>
    <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo> <mo>|</mo> <mo>|</mo></mrow>
    <msup><mi>W</mi> <mi>T</mi></msup> <mrow><mo>(</mo> <mi>W</mi> <msup><mi>x</mi>
    <mo>'</mo></msup> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>+</mo> <mi>c</mi>
    <mo>-</mo> <msubsup><mrow><mi>x</mi><mo>|</mo><mo>|</mo></mrow> <mn>2</mn> <mn>2</mn></msubsup>
    <mrow><mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals 2 sigma Superscript 4 Baseline asterisk double-struck
    upper E Subscript p Sub Subscript sigma Subscript left-parenthesis x comma x Sub
    Superscript prime Subscript right-parenthesis Baseline left-bracket StartFraction
    1 Over 2 sigma Superscript 4 Baseline EndFraction StartAbsoluteValue EndAbsoluteValue
    upper W Superscript upper T Baseline left-parenthesis upper W x prime plus b right-parenthesis
    plus c minus x StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline
    right-bracket"><mrow><mo>=</mo> <mn>2</mn> <msup><mi>σ</mi> <mn>4</mn></msup>
    <mo>*</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi> <mi>σ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi>
    <mo>'</mo></msup> <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo></mrow> <mfrac><mn>1</mn>
    <mrow><mn>2</mn><msup><mi>σ</mi> <mn>4</mn></msup></mrow></mfrac> <mrow><mo>|</mo>
    <mo>|</mo></mrow> <msup><mi>W</mi> <mi>T</mi></msup> <mrow><mo>(</mo> <mi>W</mi>
    <msup><mi>x</mi> <mo>'</mo></msup> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>+</mo>
    <mi>c</mi> <mo>-</mo> <msubsup><mrow><mi>x</mi><mo>|</mo><mo>|</mo></mrow> <mn>2</mn>
    <mn>2</mn></msubsup> <mrow><mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: <math alttext="equals 2 sigma Superscript 4 Baseline asterisk double-struck
    upper E Subscript p Sub Subscript sigma Subscript left-parenthesis x comma x Sub
    Superscript prime Subscript right-parenthesis Baseline left-bracket one-half StartAbsoluteValue
    EndAbsoluteValue StartFraction 1 Over sigma squared EndFraction left-parenthesis
    upper W Superscript upper T Baseline left-parenthesis upper W x prime plus b right-parenthesis
    plus c minus x prime right-parenthesis minus StartFraction 1 Over sigma squared
    EndFraction left-parenthesis x minus x prime right-parenthesis StartAbsoluteValue
    EndAbsoluteValue Subscript 2 Superscript 2 Baseline right-bracket"><mrow><mo>=</mo>
    <mn>2</mn> <msup><mi>σ</mi> <mn>4</mn></msup> <mo>*</mo> <msub><mi>𝔼</mi> <mrow><msub><mi>p</mi>
    <mi>σ</mi></msub> <mrow><mo>(</mo><mi>x</mi><mo>,</mo><msup><mi>x</mi> <mo>'</mo></msup>
    <mo>)</mo></mrow></mrow></msub> <mrow><mo>[</mo></mrow> <mfrac><mn>1</mn> <mn>2</mn></mfrac>
    <mrow><mo>|</mo> <mo>|</mo></mrow> <mfrac><mn>1</mn> <msup><mi>σ</mi> <mn>2</mn></msup></mfrac>
    <mrow><mo>(</mo> <msup><mi>W</mi> <mi>T</mi></msup> <mrow><mo>(</mo> <mi>W</mi>
    <msup><mi>x</mi> <mo>'</mo></msup> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>+</mo>
    <mi>c</mi> <mo>-</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <mo>-</mo>
    <mfrac><mn>1</mn> <msup><mi>σ</mi> <mn>2</mn></msup></mfrac> <mrow><mo>(</mo>
    <mi>x</mi> <mo>-</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow> <msubsup><mrow><mo>|</mo><mo>|</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup> <mrow><mo>]</mo></mrow></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: You may notice that our algebraic manipulation has led to the appearance of
    <math alttext="normal nabla Subscript x prime Baseline log p Subscript sigma Baseline
    left-parenthesis x prime vertical-bar x right-parenthesis"><mrow><msub><mi>∇</mi>
    <msup><mi>x</mi> <mo>'</mo></msup></msub> <mo form="prefix">log</mo> <msub><mi>p</mi>
    <mi>σ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>|</mo>
    <mi>x</mi> <mo>)</mo></mrow></mrow></math> . All we need to do now is find a form
    for the unnormalized likelihood whose gradient with respect to *x’* is <math alttext="StartFraction
    1 Over sigma squared EndFraction left-parenthesis upper W Superscript upper T
    Baseline left-parenthesis upper W x prime plus b right-parenthesis plus c minus
    x prime right-parenthesis"><mrow><mfrac><mn>1</mn> <msup><mi>σ</mi> <mn>2</mn></msup></mfrac>
    <mrow><mo>(</mo> <msup><mi>W</mi> <mi>T</mi></msup> <mrow><mo>(</mo> <mi>W</mi>
    <msup><mi>x</mi> <mo>'</mo></msup> <mo>+</mo> <mi>b</mi> <mo>)</mo></mrow> <mo>+</mo>
    <mi>c</mi> <mo>-</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math>
    .
  prefs: []
  type: TYPE_NORMAL
- en: As it turns out, if we define the unnormalized likelihood <math alttext="q Subscript
    theta Baseline left-parenthesis x prime right-parenthesis"><mrow><msub><mi>q</mi>
    <mi>θ</mi></msub> <mrow><mo>(</mo> <msup><mi>x</mi> <mo>'</mo></msup> <mo>)</mo></mrow></mrow></math>
    to be <math alttext="minus StartFraction 1 Over sigma squared EndFraction left-parenthesis
    c Superscript upper T Baseline x minus one-half StartAbsoluteValue EndAbsoluteValue
    x StartAbsoluteValue EndAbsoluteValue Subscript 2 Superscript 2 Baseline plus
    sigma-summation Underscript j equals 1 Overscript d Endscripts softplus left-parenthesis
    upper W Subscript j Superscript upper T Baseline x plus b Subscript j Baseline
    right-parenthesis right-parenthesis"><mrow><mo>-</mo> <mfrac><mn>1</mn> <msup><mi>σ</mi>
    <mn>2</mn></msup></mfrac> <mrow><mo>(</mo></mrow> <msup><mi>c</mi> <mi>T</mi></msup>
    <mi>x</mi> <mo>-</mo> <mfrac><mn>1</mn> <mn>2</mn></mfrac> <msubsup><mrow><mo>|</mo><mo>|</mo><mi>x</mi><mo>|</mo><mo>|</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>d</mi></msubsup> <mtext>softplus</mtext> <mrow><mo>(</mo> <msubsup><mi>W</mi>
    <mi>j</mi> <mi>T</mi></msubsup> <mi>x</mi> <mo>+</mo> <msub><mi>b</mi> <mi>j</mi></msub>
    <mo>)</mo></mrow> <mrow><mo>)</mo></mrow></mrow></math> and plug in this expression
    to the denoising score matching objective, we are left with an objective that
    is just <math alttext="StartFraction 1 Over 2 sigma Superscript 4 Baseline EndFraction
    upper J Subscript DAE Baseline left-parenthesis theta right-parenthesis"><mrow><mfrac><mn>1</mn>
    <mrow><mn>2</mn><msup><mi>σ</mi> <mn>4</mn></msup></mrow></mfrac> <msub><mi>J</mi>
    <mtext>DAE</mtext></msub> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
    . We refer you to Vincent 2011 to see why this is the case.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing this new objective with respect to <math alttext="theta"><mi>θ</mi></math>
    is no different from optimizing a denoising autoencoder. This is because <math
    alttext="sigma"><mi>σ</mi></math> is a positive constant and has no dependence
    on <math alttext="theta"><mi>θ</mi></math> , thus only scaling the magnitude of
    the resulting gradient rather than affecting its direction. In summary, we have
    found that training a denoising AE is the same as optimizing the denoising score
    matching objective, where the unnormalized likelihood takes the form specified
    in the previous paragraph. More simply, the weights of a trained denoising AE
    would be the same as those of an unnormalized likelihood specified by <math alttext="minus
    StartFraction 1 Over sigma squared EndFraction left-parenthesis c Superscript
    upper T Baseline x minus one-half StartAbsoluteValue EndAbsoluteValue x StartAbsoluteValue
    EndAbsoluteValue Subscript 2 Superscript 2 Baseline plus sigma-summation Underscript
    j equals 1 Overscript d Endscripts softplus left-parenthesis upper W Subscript
    j Superscript upper T Baseline x plus b Subscript j Baseline right-parenthesis
    right-parenthesis"><mrow><mo>-</mo> <mfrac><mn>1</mn> <msup><mi>σ</mi> <mn>2</mn></msup></mfrac>
    <mrow><mo>(</mo></mrow> <msup><mi>c</mi> <mi>T</mi></msup> <mi>x</mi> <mo>-</mo>
    <mfrac><mn>1</mn> <mn>2</mn></mfrac> <msubsup><mrow><mo>|</mo><mo>|</mo><mi>x</mi><mo>|</mo><mo>|</mo></mrow>
    <mn>2</mn> <mn>2</mn></msubsup> <mo>+</mo> <msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>d</mi></msubsup> <mtext>softplus</mtext> <mrow><mo>(</mo> <msubsup><mi>W</mi>
    <mi>j</mi> <mi>T</mi></msubsup> <mi>x</mi> <mo>+</mo> <msub><mi>b</mi> <mi>j</mi></msub>
    <mo>)</mo></mrow> <mrow><mo>)</mo></mrow></mrow></math> and trained via denoising
    score matching.
  prefs: []
  type: TYPE_NORMAL
- en: 'All we would need to do to perform generative modeling using a denoising AE
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: Fully train the denoising AE by minimizing <math alttext="upper J Subscript
    DAE Baseline left-parenthesis theta right-parenthesis"><mrow><msub><mi>J</mi>
    <mtext>DAE</mtext></msub> <mrow><mo>(</mo> <mi>θ</mi> <mo>)</mo></mrow></mrow></math>
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a given <math alttext="x Superscript left-parenthesis i right-parenthesis"><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></math> , calculate its score
    by evaluating <math alttext="StartFraction 1 Over sigma squared EndFraction left-parenthesis
    decode left-parenthesis encode left-parenthesis x Superscript left-parenthesis
    i right-parenthesis Baseline right-parenthesis right-parenthesis minus x Superscript
    left-parenthesis i right-parenthesis Baseline right-parenthesis"><mrow><mfrac><mn>1</mn>
    <msup><mi>σ</mi> <mn>2</mn></msup></mfrac> <mrow><mo>(</mo> <mtext>decode</mtext>
    <mrow><mo>(</mo> <mtext>encode</mtext> <mrow><mo>(</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>-</mo> <msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup>
    <mo>)</mo></mrow></mrow></math> .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample <math alttext="epsilon"><mi>ϵ</mi></math> from *N(0,I).*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plug in the results of 2 and 3 into the Langevin dynamics equation to obtain
    the next sample <math alttext="x Superscript left-parenthesis i plus 1 right-parenthesis"><msup><mi>x</mi>
    <mrow><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msup></math>
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat steps 2 through 4 with <math alttext="x Superscript left-parenthesis
    i plus 1 right-parenthesis"><msup><mi>x</mi> <mrow><mo>(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo>)</mo></mrow></msup></math>
    .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Though we’ve gotten around the issue of needing to calculate second-order gradients
    by using this method, there is still the issue of being able to sample only from
    the noisy approximation of *p(x)*. More recent work builds off of concepts from
    both implicit score matching and denoising score matching to achieve even stronger
    and more realistic generative capabilities. We highly recommend you explore the
    literature further, as most of the prerequisite material has been covered in these
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In summary, we have learned a great deal about generative models. We covered
    the motivation and mathematics behind GANs, VAEs, and a few forms of score matching,
    and even implemented a VAE from scratch. We also learned about the similarities
    and differences between these methods. For example, a GAN implicitly models a
    complex distribution that we can sample from via its generator, while a VAE explicitly
    learns distributions but is slightly more restrictive in the complexity of distributions
    it can model. Implicit score matching, similarly to GANs, allowed us to sample
    from complex distributions via Langevin dynamics (without the use of an additional
    noise distribution *p(z))*, but having to compute second-order gradients led us
    to the development of the denoising score matching and its connection with pre-existing
    denoising AEs. Additionally, VAEs took on the strongest probabilistic modeling
    approach of the three by defining a set of latent variables and explicitly learning
    an approximate posterior, given an input example, and a likelihood function, given
    a setting of latent variables. In contrast, for GANs, the additional variable
    *z’s* purpose is solely as an intermediate for sampling. Although all of these
    models tackle generative modeling from distinct perspectives and motivations,
    they have all produced strong results and have laid a solid groundwork for current
    and future research.
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch10.xhtml#idm45934165113648-marker)) Goodfellow et al. “Generative Adversarial
    Networks.” *arXiv Preprint arXiv*:1406.2661\. 2014.
  prefs: []
  type: TYPE_NORMAL
- en: ^([2](ch10.xhtml#idm45934168472352-marker)) Kingma et al. “Auto-Encoding Variational
    Bayes.” *arXiv Preprint arXiv*:1312.6114\. 2014.
  prefs: []
  type: TYPE_NORMAL
