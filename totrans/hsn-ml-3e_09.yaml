- en: Chapter 8\. Dimensionality Reduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many machine learning problems involve thousands or even millions of features
    for each training instance. Not only do all these features make training extremely
    slow, but they can also make it much harder to find a good solution, as you will
    see. This problem is often referred to as the *curse of dimensionality*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, in real-world problems, it is often possible to reduce the number
    of features considerably, turning an intractable problem into a tractable one.
    For example, consider the MNIST images (introduced in [Chapter 3](ch03.html#classification_chapter)):
    the pixels on the image borders are almost always white, so you could completely
    drop these pixels from the training set without losing much information. As we
    saw in the previous chapter, ([Figure 7-6](ch07.html#mnist_feature_importance_plot))
    confirms that these pixels are utterly unimportant for the classification task.
    Additionally, two neighboring pixels are often highly correlated: if you merge
    them into a single pixel (e.g., by taking the mean of the two pixel intensities),
    you will not lose much information.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  id: totrans-3
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Reducing dimensionality does cause some information loss, just like compressing
    an image to JPEG can degrade its quality, so even though it will speed up training,
    it may make your system perform slightly worse. It also makes your pipelines a
    bit more complex and thus harder to maintain. Therefore, I recommend you first
    try to train your system with the original data before considering using dimensionality
    reduction. In some cases, reducing the dimensionality of the training data may
    filter out some noise and unnecessary details and thus result in higher performance,
    but in general it won’t; it will just speed up training.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Apart from speeding up training, dimensionality reduction is also extremely
    useful for data visualization. Reducing the number of dimensions down to two (or
    three) makes it possible to plot a condensed view of a high-dimensional training
    set on a graph and often gain some important insights by visually detecting patterns,
    such as clusters. Moreover, data visualization is essential to communicate your
    conclusions to people who are not data scientists—in particular, decision makers
    who will use your results.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will first discuss the curse of dimensionality and get a
    sense of what goes on in high-dimensional space. Then we will consider the two
    main approaches to dimensionality reduction (projection and manifold learning),
    and we will go through three of the most popular dimensionality reduction techniques:
    PCA, random projection, and locally linear embedding (LLE).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: The Curse of Dimensionality
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are so used to living in three dimensions⁠^([1](ch08.html#idm45720209594752))
    that our intuition fails us when we try to imagine a high-dimensional space. Even
    a basic 4D hypercube is incredibly hard to picture in our minds (see [Figure 8-1](#hypercube_wikipedia)),
    let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0801](assets/mls3_0801.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
- en: Figure 8-1\. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)⁠^([2](ch08.html#idm45720209589856))
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It turns out that many things behave very differently in high-dimensional space.
    For example, if you pick a random point in a unit square (a 1 × 1 square), it
    will have only about a 0.4% chance of being located less than 0.001 from a border
    (in other words, it is very unlikely that a random point will be “extreme” along
    any dimension). But in a 10,000-dimensional unit hypercube, this probability is
    greater than 99.999999%. Most points in a high-dimensional hypercube are very
    close to the border.⁠^([3](ch08.html#idm45720209584784))
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a more troublesome difference: if you pick two points randomly in a
    unit square, the distance between these two points will be, on average, roughly
    0.52\. If you pick two random points in a 3D unit cube, the average distance will
    be roughly 0.66\. But what about two points picked randomly in a 1,000,000-dimensional
    unit hypercube? The average distance, believe it or not, will be about 408.25
    (roughly <math><msqrt><mfrac bevelled="true"><mrow><mn>1</mn><mo>,</mo><mn>000</mn><mo>,</mo><mn>000</mn></mrow><mn>6</mn></mfrac></msqrt></math>)!
    This is counterintuitive: how can two points be so far apart when they both lie
    within the same unit hypercube? Well, there’s just plenty of space in high dimensions.
    As a result, high-dimensional datasets are at risk of being very sparse: most
    training instances are likely to be far away from each other. This also means
    that a new instance will likely be far away from any training instance, making
    predictions much less reliable than in lower dimensions, since they will be based
    on much larger extrapolations. In short, the more dimensions the training set
    has, the greater the risk of overfitting it.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个更棘手的差异：如果你在单位正方形中随机选择两个点，这两点之间的距离平均约为0.52。如果你在3D单位立方体中随机选择两个点，平均距离将约为0.66。但是如果你在一个100万维单位超立方体中随机选择两个点呢？平均距离，信不信由你，将约为408.25（大约<math><msqrt><mfrac
    bevelled="true"><mrow><mn>1</mn><mo>,</mo><mn>000</mn><mo>,</mo><mn>000</mn></mrow><mn>6</mn></mfrac></msqrt></math>)！这是违反直觉的：当两点都位于同一个单位超立方体内时，它们怎么会相距如此遥远呢？嗯，在高维空间中有很多空间。因此，高维数据集很可能非常稀疏：大多数训练实例可能相互之间相距很远。这也意味着新实例很可能与任何训练实例相距很远，使得预测比在低维度中不可靠得多，因为它们将基于更大的外推。简而言之，训练集的维度越高，过拟合的风险就越大。
- en: In theory, one solution to the curse of dimensionality could be to increase
    the size of the training set to reach a sufficient density of training instances.
    Unfortunately, in practice, the number of training instances required to reach
    a given density grows exponentially with the number of dimensions. With just 100
    features—significantly fewer than in the MNIST problem—all ranging from 0 to 1,
    you would need more training instances than atoms in the observable universe in
    order for training instances to be within 0.1 of each other on average, assuming
    they were spread out uniformly across all dimensions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，解决维度灾难的一个方法可能是增加训练集的大小，以达到足够密度的训练实例。不幸的是，在实践中，达到给定密度所需的训练实例数量随着维度的增加呈指数增长。仅仅具有100个特征——明显少于MNIST问题中的特征数量——这些特征范围从0到1，你需要的训练实例数量将超过可观察宇宙中的原子数量，以便训练实例在平均情况下相距0.1，假设它们均匀分布在所有维度上。
- en: Main Approaches for Dimensionality Reduction
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维的主要方法
- en: 'Before we dive into specific dimensionality reduction algorithms, let’s take
    a look at the two main approaches to reducing dimensionality: projection and manifold
    learning.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入研究具体的降维算法之前，让我们看一看降维的两种主要方法：投影和流形学习。
- en: Projection
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投影
- en: In most real-world problems, training instances are *not* spread out uniformly
    across all dimensions. Many features are almost constant, while others are highly
    correlated (as discussed earlier for MNIST). As a result, all training instances
    lie within (or close to) a much lower-dimensional *subspace* of the high-dimensional
    space. This sounds very abstract, so let’s look at an example. In [Figure 8-2](#dataset_3d_plot)
    you can see a 3D dataset represented by small spheres.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数实际问题中，训练实例并*不*均匀分布在所有维度上。许多特征几乎是恒定的，而其他特征高度相关（正如前面讨论的MNIST）。因此，所有训练实例都位于（或接近）高维空间中的一个更低维度*子空间*内。这听起来很抽象，让我们看一个例子。在[图8-2](#dataset_3d_plot)中，你可以看到由小球表示的3D数据集。
- en: '![mls3 0802](assets/mls3_0802.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0802](assets/mls3_0802.png)'
- en: Figure 8-2\. A 3D dataset lying close to a 2D subspace
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-2\. 一个接近2D子空间的3D数据集
- en: 'Notice that all training instances lie close to a plane: this is a lower-dimensional
    (2D) subspace of the higher-dimensional (3D) space. If we project every training
    instance perpendicularly onto this subspace (as represented by the short dashed
    lines connecting the instances to the plane), we get the new 2D dataset shown
    in [Figure 8-3](#dataset_2d_plot). Ta-da! We have just reduced the dataset’s dimensionality
    from 3D to 2D. Note that the axes correspond to new features *z*[1] and *z*[2]:
    they are the coordinates of the projections on the plane.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到所有训练实例都接近一个平面：这是高维（3D）空间中的一个低维（2D）子空间。如果我们将每个训练实例垂直投影到这个子空间上（如短虚线连接实例到平面），我们得到了在[图8-3](#dataset_2d_plot)中显示的新的2D数据集。哇！我们刚刚将数据集的维度从3D降低到2D。请注意，轴对应于新特征*z*[1]和*z*[2]：它们是平面上投影的坐标。
- en: '![mls3 0803](assets/mls3_0803.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0803](assets/mls3_0803.png)'
- en: Figure 8-3\. The new 2D dataset after projection
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-3\. 投影后的新2D数据集
- en: Manifold Learning
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流形学习
- en: However, projection is not always the best approach to dimensionality reduction.
    In many cases the subspace may twist and turn, such as in the famous Swiss roll
    toy dataset represented in [Figure 8-4](#swiss_roll_plot).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，投影并不总是降维的最佳方法。在许多情况下，子空间可能扭曲变化，例如在著名的瑞士卷玩具数据集中所示的[图8-4](#swiss_roll_plot)。
- en: '![mls3 0804](assets/mls3_0804.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0804](assets/mls3_0804.png)'
- en: Figure 8-4\. Swiss roll dataset
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-4\. 瑞士卷数据集
- en: Simply projecting onto a plane (e.g., by dropping *x*[3]) would squash different
    layers of the Swiss roll together, as shown on the left side of [Figure 8-5](#squished_swiss_roll_plot).
    What you probably want instead is to unroll the Swiss roll to obtain the 2D dataset
    on the right side of [Figure 8-5](#squished_swiss_roll_plot).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地投影到平面上（例如，通过删除*x*[3]）会将瑞士卷的不同层压缩在一起，如[图8-5](#squished_swiss_roll_plot)的左侧所示。你可能想要的是展开瑞士卷，以获得[图8-5](#squished_swiss_roll_plot)右侧的2D数据集。
- en: '![mls3 0805](assets/mls3_0805.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0805](assets/mls3_0805.png)'
- en: Figure 8-5\. Squashing by projecting onto a plane (left) versus unrolling the
    Swiss roll (right)
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-5\. 投影到平面上压缩（左）与展开瑞士卷（右）
- en: 'The Swiss roll is an example of a 2D *manifold*. Put simply, a 2D manifold
    is a 2D shape that can be bent and twisted in a higher-dimensional space. More
    generally, a *d*-dimensional manifold is a part of an *n*-dimensional space (where
    *d* < *n*) that locally resembles a *d*-dimensional hyperplane. In the case of
    the Swiss roll, *d* = 2 and *n* = 3: it locally resembles a 2D plane, but it is
    rolled in the third dimension.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Many dimensionality reduction algorithms work by modeling the manifold on which
    the training instances lie; this is called *manifold learning*. It relies on the
    *manifold assumption*, also called the *manifold hypothesis*, which holds that
    most real-world high-dimensional datasets lie close to a much lower-dimensional
    manifold. This assumption is very often empirically observed.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Once again, think about the MNIST dataset: all handwritten digit images have
    some similarities. They are made of connected lines, the borders are white, and
    they are more or less centered. If you randomly generated images, only a ridiculously
    tiny fraction of them would look like handwritten digits. In other words, the
    degrees of freedom available to you if you try to create a digit image are dramatically
    lower than the degrees of freedom you have if you are allowed to generate any
    image you want. These constraints tend to squeeze the dataset into a lower-dimensional
    manifold.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'The manifold assumption is often accompanied by another implicit assumption:
    that the task at hand (e.g., classification or regression) will be simpler if
    expressed in the lower-dimensional space of the manifold. For example, in the
    top row of [Figure 8-6](#manifold_decision_boundary_plot) the Swiss roll is split
    into two classes: in the 3D space (on the left) the decision boundary would be
    fairly complex, but in the 2D unrolled manifold space (on the right) the decision
    boundary is a straight line.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: However, this implicit assumption does not always hold. For example, in the
    bottom row of [Figure 8-6](#manifold_decision_boundary_plot), the decision boundary
    is located at *x*[1] = 5\. This decision boundary looks very simple in the original
    3D space (a vertical plane), but it looks more complex in the unrolled manifold
    (a collection of four independent line segments).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: In short, reducing the dimensionality of your training set before training a
    model will usually speed up training, but it may not always lead to a better or
    simpler solution; it all depends on the dataset.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Hopefully you now have a good sense of what the curse of dimensionality is and
    how dimensionality reduction algorithms can fight it, especially when the manifold
    assumption holds. The rest of this chapter will go through some of the most popular
    algorithms for dimensionality reduction.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0806](assets/mls3_0806.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
- en: Figure 8-6\. The decision boundary may not always be simpler with lower dimensions
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PCA
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Principal component analysis* (PCA) is by far the most popular dimensionality
    reduction algorithm. First it identifies the hyperplane that lies closest to the
    data, and then it projects the data onto it, just like in [Figure 8-2](#dataset_3d_plot).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Preserving the Variance
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you can project the training set onto a lower-dimensional hyperplane,
    you first need to choose the right hyperplane. For example, a simple 2D dataset
    is represented on the left in [Figure 8-7](#pca_best_projection_plot), along with
    three different axes (i.e., 1D hyperplanes). On the right is the result of the
    projection of the dataset onto each of these axes. As you can see, the projection
    onto the solid line preserves the maximum variance (top), while the projection
    onto the dotted line preserves very little variance (bottom) and the projection
    onto the dashed line preserves an intermediate amount of variance (middle).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0807](assets/mls3_0807.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
- en: Figure 8-7\. Selecting the subspace on which to project
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It seems reasonable to select the axis that preserves the maximum amount of
    variance, as it will most likely lose less information than the other projections.
    Another way to justify this choice is that it is the axis that minimizes the mean
    squared distance between the original dataset and its projection onto that axis.
    This is the rather simple idea behind [PCA](https://homl.info/pca).⁠^([4](ch08.html#idm45720209523296))
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 选择保留最大方差量的轴似乎是合理的，因为它很可能会比其他投影丢失更少的信息。另一个证明这种选择的方法是，它是最小化原始数据集与其在该轴上的投影之间的均方距离的轴。这是[PCA](https://homl.info/pca)背后的相当简单的想法。⁠^([4](ch08.html#idm45720209523296))
- en: Principal Components
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分
- en: 'PCA identifies the axis that accounts for the largest amount of variance in
    the training set. In [Figure 8-7](#pca_best_projection_plot), it is the solid
    line. It also finds a second axis, orthogonal to the first one, that accounts
    for the largest amount of the remaining variance. In this 2D example there is
    no choice: it is the dotted line. If it were a higher-dimensional dataset, PCA
    would also find a third axis, orthogonal to both previous axes, and a fourth,
    a fifth, and so on—as many axes as the number of dimensions in the dataset.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: PCA确定在训练集中占据最大方差量的轴。在[图8-7](#pca_best_projection_plot)中，它是实线。它还找到第二个轴，与第一个轴正交，占剩余方差的最大部分。在这个2D示例中没有选择：它是虚线。如果是高维数据集，PCA还会找到第三个轴，与前两个轴正交，以及第四个、第五个等等——与数据集中的维数一样多的轴。
- en: The *i*^(th) axis is called the *i*^(th) *principal component* (PC) of the data.
    In [Figure 8-7](#pca_best_projection_plot), the first PC is the axis on which
    vector **c**[**1**] lies, and the second PC is the axis on which vector **c**[**2**]
    lies. In [Figure 8-2](#dataset_3d_plot) the first two PCs are on the projection
    plane, and the third PC is the axis orthogonal to that plane. After the projection,
    in [Figure 8-3](#dataset_2d_plot), the first PC corresponds to the *z*[1] axis,
    and the second PC corresponds to the *z*[2] axis.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 第*i*轴称为数据的第*i*个*主成分*（PC）。在[图8-7](#pca_best_projection_plot)中，第一个PC是向量**c**[**1**]所在的轴，第二个PC是向量**c**[**2**]所在的轴。在[图8-2](#dataset_3d_plot)中，前两个PC位于投影平面上，第三个PC是与该平面正交的轴。在投影之后，在[图8-3](#dataset_2d_plot)中，第一个PC对应于*z*[1]轴，第二个PC对应于*z*[2]轴。
- en: Note
  id: totrans-49
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'For each principal component, PCA finds a zero-centered unit vector pointing
    in the direction of the PC. Since two opposing unit vectors lie on the same axis,
    the direction of the unit vectors returned by PCA is not stable: if you perturb
    the training set slightly and run PCA again, the unit vectors may point in the
    opposite direction as the original vectors. However, they will generally still
    lie on the same axes. In some cases, a pair of unit vectors may even rotate or
    swap (if the variances along these two axes are very close), but the plane they
    define will generally remain the same.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个主成分，PCA找到一个指向PC方向的零中心单位向量。由于两个相对的单位向量位于同一轴上，PCA返回的单位向量的方向不稳定：如果稍微扰动训练集并再次运行PCA，则单位向量可能指向与原始向量相反的方向。但是，它们通常仍然位于相同的轴上。在某些情况下，一对单位向量甚至可能旋转或交换（如果沿这两个轴的方差非常接近），但是它们定义的平面通常保持不变。
- en: So how can you find the principal components of a training set? Luckily, there
    is a standard matrix factorization technique called *singular value decomposition*
    (SVD) that can decompose the training set matrix **X** into the matrix multiplication
    of three matrices **U** **Σ** **V**^⊺, where **V** contains the unit vectors that
    define all the principal components that you are looking for, as shown in [Equation
    8-1](#principal_components_matrix).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 那么如何找到训练集的主成分呢？幸运的是，有一种称为*奇异值分解*（SVD）的标准矩阵分解技术，可以将训练集矩阵**X**分解为三个矩阵**U** **Σ**
    **V**^⊺的矩阵乘法，其中**V**包含定义您正在寻找的所有主成分的单位向量，如[方程8-1](#principal_components_matrix)所示。
- en: Equation 8-1\. Principal components matrix
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第8-1方程。主成分矩阵
- en: <math display="block"><mrow><mi mathvariant="bold">V</mi> <mo>=</mo> <mfenced
    open="(" close=")"><mtable><mtr><mtd><mo>∣</mo></mtd> <mtd><mo>∣</mo></mtd> <mtd><mo>∣</mo></mtd></mtr>
    <mtr><mtd><msub><mi mathvariant="bold">c</mi> <mn>1</mn></msub></mtd> <mtd><msub><mi
    mathvariant="bold">c</mi> <mn>2</mn></msub></mtd> <mtd><mo>⋯</mo></mtd> <mtd><msub><mi
    mathvariant="bold">c</mi> <mi mathvariant="italic">n</mi></msub></mtd></mtr> <mtr><mtd><mo>∣</mo></mtd>
    <mtd><mo>∣</mo></mtd> <mtd><mo>∣</mo></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi mathvariant="bold">V</mi> <mo>=</mo> <mfenced
    open="(" close=")"><mtable><mtr><mtd><mo>∣</mo></mtd> <mtd><mo>∣</mo></mtd> <mtd><mo>∣</mo></mtd></mtr>
    <mtr><mtd><msub><mi mathvariant="bold">c</mi> <mn>1</mn></msub></mtd> <mtd><msub><mi
    mathvariant="bold">c</mi> <mn>2</mn></msub></td> <mtd><mo>⋯</mo></mtd> <mtd><msub><mi
    mathvariant="bold">c</mi> <mi mathvariant="italic">n</mi></msub></mtd></mtr> <mtr><mtd><mo>∣</mo></mtd>
    <mtd><mo>∣</mo></mtd> <mtd><mo>∣</mo></mtd></mtr></mtable></mfenced></mrow></math>
- en: 'The following Python code uses NumPy’s `svd()` function to obtain all the principal
    components of the 3D training set represented in [Figure 8-2](#dataset_3d_plot),
    then it extracts the two unit vectors that define the first two PCs:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python代码使用NumPy的`svd()`函数获取在[图8-2](#dataset_3d_plot)中表示的3D训练集的所有主成分，然后提取定义前两个PC的两个单位向量：
- en: '[PRE0]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Warning
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: PCA assumes that the dataset is centered around the origin. As you will see,
    Scikit-Learn’s PCA classes take care of centering the data for you. If you implement
    PCA yourself (as in the preceding example), or if you use other libraries, don’t
    forget to center the data first.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: PCA假定数据集围绕原点居中。正如您将看到的，Scikit-Learn的PCA类会为您处理数据的中心化。如果您自己实现PCA（如前面的示例中），或者使用其他库，请不要忘记首先对数据进行中心化。
- en: Projecting Down to d Dimensions
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投影到d维
- en: Once you have identified all the principal components, you can reduce the dimensionality
    of the dataset down to *d* dimensions by projecting it onto the hyperplane defined
    by the first *d* principal components. Selecting this hyperplane ensures that
    the projection will preserve as much variance as possible. For example, in [Figure 8-2](#dataset_3d_plot)
    the 3D dataset is projected down to the 2D plane defined by the first two principal
    components, preserving a large part of the dataset’s variance. As a result, the
    2D projection looks very much like the original 3D dataset.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了所有主成分，您可以通过将数据集投影到由前 *d* 个主成分定义的超平面上来将数据集的维度降低到 *d* 维。选择这个超平面可以确保投影尽可能保留更多的方差。例如，在
    [Figure 8-2](#dataset_3d_plot) 中，3D 数据集被投影到由前两个主成分定义的 2D 平面上，保留了数据集大部分的方差。因此，2D
    投影看起来非常像原始的 3D 数据集。
- en: To project the training set onto the hyperplane and obtain a reduced dataset
    **X**[*d*-proj] of dimensionality *d*, compute the matrix multiplication of the
    training set matrix **X** by the matrix **W**[*d*], defined as the matrix containing
    the first *d* columns of **V**, as shown in [Equation 8-2](#pca_projection).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将训练集投影到超平面上，并获得降维后的数据集 **X**[*d*-proj]，维度为 *d*，计算训练集矩阵 **X** 与矩阵 **W**[*d*]
    的矩阵乘法，其中 **W**[*d*] 定义为包含 **V** 的前 *d* 列的矩阵，如 [Equation 8-2](#pca_projection)
    所示。
- en: Equation 8-2\. Projecting the training set down to *d* dimensions
  id: totrans-61
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Equation 8-2\. 将训练集投影到 *d* 维
- en: <math display="block"><mrow><msub><mi mathvariant="bold">X</mi> <mrow><mi>d</mi><mtext>-proj</mtext></mrow></msub>
    <mo>=</mo> <mi mathvariant="bold">X</mi> <msub><mi mathvariant="bold">W</mi> <mi>d</mi></msub></mrow></math>
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi mathvariant="bold">X</mi> <mrow><mi>d</mi><mtext>-proj</mtext></mrow></msub>
    <mo>=</mo> <mi mathvariant="bold">X</mi> <msub><mi mathvariant="bold">W</mi> <mi>d</mi></msub></mrow></math>
- en: 'The following Python code projects the training set onto the plane defined
    by the first two principal components:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下 Python 代码将训练集投影到由前两个主成分定义的平面上：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: There you have it! You now know how to reduce the dimensionality of any dataset
    by projecting it down to any number of dimensions, while preserving as much variance
    as possible.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！现在你知道如何通过将数据集投影到任意维度来降低数据集的维度，同时尽可能保留更多的方差。
- en: Using Scikit-Learn
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Scikit-Learn
- en: 'Scikit-Learn’s `PCA` class uses SVD to implement PCA, just like we did earlier
    in this chapter. The following code applies PCA to reduce the dimensionality of
    the dataset down to two dimensions (note that it automatically takes care of centering
    the data):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn 的 `PCA` 类使用 SVD 来实现 PCA，就像我们在本章中之前所做的那样。以下代码应用 PCA 将数据集的维度降低到两个维度（请注意，它会自动处理数据的居中）：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After fitting the `PCA` transformer to the dataset, its `components_` attribute
    holds the transpose of **W**[*d*]: it contains one row for each of the first *d*
    principal components.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在将 `PCA` 转换器拟合到数据集后，其 `components_` 属性保存了 **W**[*d*] 的转置：它包含了前 *d* 个主成分的每一行。
- en: Explained Variance Ratio
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释方差比
- en: 'Another useful piece of information is the *explained variance ratio* of each
    principal component, available via the `explained_variance_ratio_` variable. The
    ratio indicates the proportion of the dataset’s variance that lies along each
    principal component. For example, let’s look at the explained variance ratios
    of the first two components of the 3D dataset represented in [Figure 8-2](#dataset_3d_plot):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的信息是每个主成分的*解释方差比*，可以通过 `explained_variance_ratio_` 变量获得。该比率表示数据集方差沿着每个主成分的比例。例如，让我们看看在
    [Figure 8-2](#dataset_3d_plot) 中表示的 3D 数据集的前两个主成分的解释方差比：
- en: '[PRE3]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This output tells us that about 76% of the dataset’s variance lies along the
    first PC, and about 15% lies along the second PC. This leaves about 9% for the
    third PC, so it is reasonable to assume that the third PC probably carries little
    information.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出告诉我们大约 76% 的数据集方差沿着第一个主成分，大约 15% 沿着第二个主成分。这留下了约 9% 给第三个主成分，因此可以合理地假设第三个主成分可能携带的信息很少。
- en: Choosing the Right Number of Dimensions
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择正确的维度数量
- en: Instead of arbitrarily choosing the number of dimensions to reduce down to,
    it is simpler to choose the number of dimensions that add up to a sufficiently
    large portion of the variance—say, 95% (An exception to this rule, of course,
    is if you are reducing dimensionality for data visualization, in which case you
    will want to reduce the dimensionality down to 2 or 3).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 不要随意选择要降维到的维度数量，更简单的方法是选择维度数量，使其总和占方差的足够大比例，比如 95%（当然，有一个例外，如果你是为了数据可视化而降维，那么你会希望将维度降低到
    2 或 3）。
- en: 'The following code loads and splits the MNIST dataset (introduced in [Chapter 3](ch03.html#classification_chapter))
    and performs PCA without reducing dimensionality, then computes the minimum number
    of dimensions required to preserve 95% of the training set’s variance:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码加载并拆分 MNIST 数据集（在 [Chapter 3](ch03.html#classification_chapter) 中介绍），并在不降维的情况下执行
    PCA，然后计算保留训练集 95% 方差所需的最小维度数量：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You could then set `n_components=d` and run PCA again, but there’s a better
    option. Instead of specifying the number of principal components you want to preserve,
    you can set `n_components` to be a float between 0.0 and 1.0, indicating the ratio
    of variance you wish to preserve:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以将 `n_components=d`，再次运行 PCA，但有一个更好的选择。而不是指定要保留的主成分数量，您可以将 `n_components`
    设置为介于 0.0 和 1.0 之间的浮点数，表示您希望保留的方差比例：
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The actual number of components is determined during training, and it is stored
    in the `n_components_` attribute:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的主成分数量是在训练过程中确定的，并存储在 `n_components_` 属性中：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Yet another option is to plot the explained variance as a function of the number
    of dimensions (simply plot `cumsum`; see [Figure 8-8](#explained_variance_plot)).
    There will usually be an elbow in the curve, where the explained variance stops
    growing fast. In this case, you can see that reducing the dimensionality down
    to about 100 dimensions wouldn’t lose too much explained variance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是将解释的方差作为维度数量的函数绘制出来（简单地绘制`cumsum`；参见[图8-8](#explained_variance_plot)）。曲线通常会出现一个拐点，解释的方差增长速度会变慢。在这种情况下，您可以看到将维度降低到约100维不会丢失太多解释的方差。
- en: '![mls3 0808](assets/mls3_0808.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0808](assets/mls3_0808.png)'
- en: Figure 8-8\. Explained variance as a function of the number of dimensions
  id: totrans-84
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-8。解释的方差作为维度数量的函数
- en: 'Lastly, if you are using dimensionality reduction as a preprocessing step for
    a supervised learning task (e.g., classification), then you can tune the number
    of dimensions as you would any other hyperparameter (see [Chapter 2](ch02.html#project_chapter)).
    For example, the following code example creates a two-step pipeline, first reducing
    dimensionality using PCA, then classifying using a random forest. Next, it uses
    `RandomizedSearchCV` to find a good combination of hyperparameters for both PCA
    and the random forest classifier. This example does a quick search, tuning only
    2 hyperparameters, training on just 1,000 instances, and running for just 10 iterations,
    but feel free to do a more thorough search if you have the time:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您将降维作为监督学习任务（例如分类）的预处理步骤，则可以像调整任何其他超参数一样调整维度数量（参见[第2章](ch02.html#project_chapter)）。例如，以下代码示例创建了一个两步流水线，首先使用PCA降低维度，然后使用随机森林进行分类。接下来，它使用`RandomizedSearchCV`来找到PCA和随机森林分类器的超参数的良好组合。此示例进行了快速搜索，仅调整了2个超参数，在仅训练了1,000个实例的情况下运行了仅10次迭代，但如果您有时间，请随时进行更彻底的搜索：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let’s look at the best hyperparameters found:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看找到的最佳超参数：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It’s interesting to note how low the optimal number of components is: we reduced
    a 784-dimensional dataset to just 23 dimensions! This is tied to the fact that
    we used a random forest, which is a pretty powerful model. If we used a linear
    model instead, such as an `SGDClassifier`, the search would find that we need
    to preserve more dimensions (about 70).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是最佳组件数量是多么低：我们将一个784维数据集减少到只有23维！这与我们使用了随机森林这一强大模型有关。如果我们改用线性模型，例如`SGDClassifier`，搜索会发现我们需要保留更多维度（约70个）。
- en: PCA for Compression
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于压缩的PCA
- en: After dimensionality reduction, the training set takes up much less space. For
    example, after applying PCA to the MNIST dataset while preserving 95% of its variance,
    we are left with 154 features, instead of the original 784 features. So the dataset
    is now less than 20% of its original size, and we only lost 5% of its variance!
    This is a reasonable compression ratio, and it’s easy to see how such a size reduction
    would speed up a classification algorithm tremendously.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 降维后，训练集占用的空间大大减少。例如，将PCA应用于MNIST数据集并保留95%的方差后，我们只剩下154个特征，而不是原始的784个特征。因此，数据集现在不到原始大小的20%，而且我们仅损失了5%的方差！这是一个合理的压缩比率，很容易看出这种大小的减小会极大地加快分类算法的速度。
- en: It is also possible to decompress the reduced dataset back to 784 dimensions
    by applying the inverse transformation of the PCA projection. This won’t give
    you back the original data, since the projection lost a bit of information (within
    the 5% variance that was dropped), but it will likely be close to the original
    data. The mean squared distance between the original data and the reconstructed
    data (compressed and then decompressed) is called the *reconstruction error*.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以通过应用PCA投影的逆变换将缩减的数据集解压缩回784维。这不会给您原始数据，因为投影丢失了一些信息（在丢弃的5%方差内），但它可能接近原始数据。原始数据和重构数据（压缩然后解压缩）之间的均方距离称为*重构误差*。
- en: 'The `inverse_transform()` method lets us decompress the reduced MNIST dataset
    back to 784 dimensions:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`inverse_transform()`方法让我们将降维后的MNIST数据集解压缩回784维：'
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[Figure 8-9](#mnist_compression_plot) shows a few digits from the original
    training set (on the left), and the corresponding digits after compression and
    decompression. You can see that there is a slight image quality loss, but the
    digits are still mostly intact.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-9](#mnist_compression_plot)展示了原始训练集中的一些数字（左侧），以及压缩和解压缩后的相应数字。您可以看到存在轻微的图像质量损失，但数字仍然基本完整。'
- en: '![mls3 0809](assets/mls3_0809.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0809](assets/mls3_0809.png)'
- en: Figure 8-9\. MNIST compression that preserves 95% of the variance
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-9。保留95%方差的MNIST压缩
- en: The equation for the inverse transformation is shown in [Equation 8-3](#inverse_pca).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 逆变换的方程式显示在[方程8-3](#inverse_pca)中。
- en: Equation 8-3\. PCA inverse transformation, back to the original number of dimensions
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程8-3。PCA逆变换，回到原始维度数量
- en: <math display="block"><mrow><msub><mi mathvariant="bold">X</mi> <mtext>recovered</mtext></msub>
    <mo>=</mo> <msub><mi mathvariant="bold">X</mi> <mrow><mi>d</mi><mtext>-proj</mtext></mrow></msub>
    <msup><mrow><msub><mi mathvariant="bold">W</mi> <mi>d</mi></msub></mrow> <mo>⊺</mo></msup></mrow></math>
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msub><mi mathvariant="bold">X</mi> <mtext>recovered</mtext></msub>
    <mo>=</mo> <msub><mi mathvariant="bold">X</mi> <mrow><mi>d</mi><mtext>-proj</mtext></mrow></msub>
    <msup><mrow><msub><mi mathvariant="bold">W</mi> <mi>d</mi></msub></mrow> <mo>⊺</mo></msup></mrow></math>
- en: Randomized PCA
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机PCA
- en: 'If you set the `svd_solver` hyperparameter to `"randomized"`, Scikit-Learn
    uses a stochastic algorithm called *randomized PCA* that quickly finds an approximation
    of the first *d* principal components. Its computational complexity is *O*(*m*
    × *d*²) + *O*(*d*³), instead of *O*(*m* × *n*²) + *O*(*n*³) for the full SVD approach,
    so it is dramatically faster than full SVD when *d* is much smaller than *n*:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将`svd_solver`超参数设置为`"randomized"`，Scikit-Learn将使用一种称为*随机PCA*的随机算法，快速找到前*d*个主成分的近似值。其计算复杂度为*O*(*m*
    × *d*²) + *O*(*d*³)，而不是完整SVD方法的*O*(*m* × *n*²) + *O*(*n*³)，因此当*d*远小于*n*时，它比完整SVD快得多：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Tip
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'By default, `svd_solver` is actually set to `"auto"`: Scikit-Learn automatically
    uses the randomized PCA algorithm if max(*m*, *n*) > 500 and `n_components` is
    an integer smaller than 80% of min(*m*, *n*), or else it uses the full SVD approach.
    So the preceding code would use the randomized PCA algorithm even if you removed
    the `svd_solver="randomized"` argument, since 154 < 0.8 × 784\. If you want to
    force Scikit-Learn to use full SVD for a slightly more precise result, you can
    set the `svd_solver` hyperparameter to `"full"`.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`svd_solver` 实际上设置为 `"auto"`：Scikit-Learn 在 max(*m*, *n*) > 500 且 `n_components`
    是小于 min(*m*, *n*) 的80%的整数时，自动使用随机化PCA算法，否则使用完整的SVD方法。因此，即使删除了 `svd_solver="randomized"`
    参数，上述代码也会使用随机化PCA算法，因为 154 < 0.8 × 784。如果要强制Scikit-Learn使用完整的SVD以获得稍微更精确的结果，可以将
    `svd_solver` 超参数设置为 `"full"`。
- en: Incremental PCA
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增量PCA
- en: One problem with the preceding implementations of PCA is that they require the
    whole training set to fit in memory in order for the algorithm to run. Fortunately,
    *incremental PCA* (IPCA) algorithms have been developed that allow you to split
    the training set into mini-batches and feed these in one mini-batch at a time.
    This is useful for large training sets and for applying PCA online (i.e., on the
    fly, as new instances arrive).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的前述实现的一个问题是，为了使算法运行，整个训练集必须适合内存。幸运的是，已经开发出了*增量PCA*（IPCA）算法，允许您将训练集分成小批次，并逐个小批次馈送这些数据。这对于大型训练集以及在线应用PCA（即，随着新实例的到来而进行）非常有用。
- en: 'The following code splits the MNIST training set into 100 mini-batches (using
    NumPy’s `array_split()` function) and feeds them to Scikit-Learn’s `IncrementalPCA`
    class⁠^([5](ch08.html#idm45720208783824)) to reduce the dimensionality of the
    MNIST dataset down to 154 dimensions, just like before. Note that you must call
    the `partial_fit()` method with each mini-batch, rather than the `fit()` method
    with the whole training set:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将MNIST训练集分成100个小批次（使用NumPy的 `array_split()` 函数），并将它们馈送给Scikit-Learn的 `IncrementalPCA`
    类来将MNIST数据集的维度降低到154维，就像以前一样。请注意，您必须对每个小批次调用 `partial_fit()` 方法，而不是对整个训练集调用 `fit()`
    方法：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Alternatively, you can use NumPy’s `memmap` class, which allows you to manipulate
    a large array stored in a binary file on disk as if it were entirely in memory;
    the class loads only the data it needs in memory, when it needs it. To demonstrate
    this, let’s first create a memory-mapped (memmap) file and copy the MNIST training
    set to it, then call `flush()` to ensure that any data still in the cache gets
    saved to disk. In real life, `X_train` would typically not fit in memory, so you
    would load it chunk by chunk and save each chunk to the right part of the memmap
    array:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用NumPy的 `memmap` 类，它允许您像在内存中一样操作存储在磁盘上的二进制文件中的大数组；该类仅在需要时将数据加载到内存中。为了演示这一点，让我们首先创建一个内存映射（memmap）文件，并将MNIST训练集复制到其中，然后调用
    `flush()` 来确保任何仍在缓存中的数据被保存到磁盘。在现实生活中，`X_train` 通常不会完全适合内存，因此您需要逐块加载它并将每个块保存到memmap数组的正确部分：
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we can load the memmap file and use it like a regular NumPy array. Let’s
    use the `IncrementalPCA` class to reduce its dimensionality. Since this algorithm
    uses only a small part of the array at any given time, memory usage remains under
    control. This makes it possible to call the usual `fit()` method instead of `partial_fit()`,
    which is quite convenient:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以加载memmap文件并像常规NumPy数组一样使用它。让我们使用 `IncrementalPCA` 类来降低其维度。由于该算法在任何给定时间仅使用数组的一小部分，内存使用保持在控制之下。这使得可以调用通常的
    `fit()` 方法而不是 `partial_fit()` 方法，这非常方便：
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Warning
  id: totrans-114
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Only the raw binary data is saved to disk, so you need to specify the data type
    and shape of the array when you load it. If you omit the shape, `np.memmap()`
    returns a 1D array.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 只有原始的二进制数据保存在磁盘上，因此在加载时需要指定数组的数据类型和形状。如果省略形状，`np.memmap()` 将返回一个一维数组。
- en: 'For very high-dimensional datasets, PCA can be too slow. As you saw earlier,
    even if you use randomized PCA its computational complexity is still *O*(*m* ×
    *d*²) + *O*(*d*³), so the target number of dimensions *d* must not be too large.
    If you are dealing with a dataset with tens of thousands of features or more (e.g.,
    images), then training may become much too slow: in this case, you should consider
    using random projection instead.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非常高维的数据集，PCA可能太慢了。正如您之前看到的，即使使用随机化PCA，其计算复杂度仍然是 *O*(*m* × *d*²) + *O*(*d*³)，因此目标维数
    *d* 不应太大。如果您处理的数据集具有成千上万个特征或更多（例如，图像），那么训练可能会变得非常缓慢：在这种情况下，您应该考虑使用随机投影。
- en: Random Projection
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机投影
- en: As its name suggests, the random projection algorithm projects the data to a
    lower-dimensional space using a random linear projection. This may sound crazy,
    but it turns out that such a random projection is actually very likely to preserve
    distances fairly well, as was demonstrated mathematically by William B. Johnson
    and Joram Lindenstrauss in a famous lemma. So, two similar instances will remain
    similar after the projection, and two very different instances will remain very
    different.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，随机投影算法使用随机线性投影将数据投影到较低维度的空间。这听起来可能很疯狂，但事实证明，这样的随机投影实际上很可能相当好地保留距离，正如William
    B. Johnson和Joram Lindenstrauss在一个著名的引理中数学上证明的那样。因此，两个相似的实例在投影后仍然保持相似，而两个非常不同的实例在投影后仍然保持非常不同。
- en: 'Obviously, the more dimensions you drop, the more information is lost, and
    the more distances get distorted. So how can you choose the optimal number of
    dimensions? Well, Johnson and Lindenstrauss came up with an equation that determines
    the minimum number of dimensions to preserve in order to ensure—with high probability—that
    distances won’t change by more than a given tolerance. For example, if you have
    a dataset containing *m* = 5,000 instances with *n* = 20,000 features each, and
    you don’t want the squared distance between any two instances to change by more
    than *ε* = 10%,^([6](ch08.html#idm45720208518368)) then you should project the
    data down to *d* dimensions, with *d* ≥ 4 log(*m*) / (½ *ε*² - ⅓ *ε*³), which
    is 7,300 dimensions. That’s quite a significant dimensionality reduction! Notice
    that the equation does not use *n*, it only relies on *m* and *ε*. This equation
    is implemented by the `johnson_lindenstrauss_min_dim()` function:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，你丢弃的维度越多，丢失的信息就越多，距离就会变得更加扭曲。那么你如何选择最佳的维度数量呢？Johnson和Lindenstrauss提出了一个方程，确定保留的最小维度数量，以确保——高概率下——距离不会改变超过给定的容差。例如，如果你有一个包含*m*=5,000个实例，每个实例有*n*=20,000个特征的数据集，你不希望任意两个实例之间的平方距离变化超过*ε*=10%，^([6](ch08.html#idm45720208518368))，那么你应该将数据投影到*d*维度，其中*d*≥4
    log(*m*) / (½ *ε*² - ⅓ *ε*³)，即7,300维度。这是一个相当显著的降维！请注意，这个方程不使用*n*，它只依赖于*m*和*ε*。这个方程由`johnson_lindenstrauss_min_dim()`函数实现：
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now we can just generate a random matrix **P** of shape [*d*, *n*], where each
    item is sampled randomly from a Gaussian distribution with mean 0 and variance
    1 / *d*, and use it to project a dataset from *n* dimensions down to *d*:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以生成一个形状为[*d*, *n*]的随机矩阵**P**，其中每个项都是从均值为0，方差为1 / *d*的高斯分布中随机抽样的，然后用它将数据集从*n*维度投影到*d*维度：
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'That’s all there is to it! It’s simple and efficient, and no training is required:
    the only thing the algorithm needs to create the random matrix is the dataset’s
    shape. The data itself is not used at all.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部！它简单高效，无需训练：算法需要创建随机矩阵的唯一信息是数据集的形状。数据本身根本没有被使用。
- en: 'Scikit-Learn offers a `GaussianRandomProjection` class to do exactly what we
    just did: when you call its `fit()` method, it uses `johnson_lindenstrauss_min_dim()`
    to determine the output dimensionality, then it generates a random matrix, which
    it stores in the `components_` attribute. Then when you call `transform()`, it
    uses this matrix to perform the projection. When creating the transformer, you
    can set `eps` if you want to tweak *ε* (it defaults to 0.1), and `n_components`
    if you want to force a specific target dimensionality *d*. The following code
    example gives the same result as the preceding code (you can also verify that
    `gaussian_rnd_proj.components_` is equal to `P`):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了一个`GaussianRandomProjection`类，可以做我们刚才做的事情：当你调用它的`fit()`方法时，它使用`johnson_lindenstrauss_min_dim()`来确定输出的维度，然后生成一个随机矩阵，存储在`components_`属性中。然后当你调用`transform()`时，它使用这个矩阵来执行投影。在创建转换器时，如果你想调整*ε*，可以设置`eps`（默认为0.1），如果你想强制特定的目标维度*d*，可以设置`n_components`。以下代码示例给出了与前面代码相同的结果（你也可以验证`gaussian_rnd_proj.components_`等于`P`）：
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Scikit-Learn also provides a second random projection transformer, known as
    `SparseRandomProjection`. It determines the target dimensionality in the same
    way, generates a random matrix of the same shape, and performs the projection
    identically. The main difference is that the random matrix is sparse. This means
    it uses much less memory: about 25 MB instead of almost 1.2 GB in the preceding
    example! And it’s also much faster, both to generate the random matrix and to
    reduce dimensionality: about 50% faster in this case. Moreover, if the input is
    sparse, the transformation keeps it sparse (unless you set `dense_output=True`).
    Lastly, it enjoys the same distance-preserving property as the previous approach,
    and the quality of the dimensionality reduction is comparable. In short, it’s
    usually preferable to use this transformer instead of the first one, especially
    for large or sparse datasets.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn还提供了第二个随机投影转换器，称为`SparseRandomProjection`。它以相同的方式确定目标维度，生成相同形状的随机矩阵，并执行相同的投影。主要区别在于随机矩阵是稀疏的。这意味着它使用的内存要少得多：在前面的例子中，约25
    MB，而不是将近1.2 GB！而且它也更快，无论是生成随机矩阵还是降维：在这种情况下，大约快50%。此外，如果输入是稀疏的，转换会保持稀疏（除非你设置`dense_output=True`）。最后，它享有与之前方法相同的保持距离性质，降维的质量是可比较的。简而言之，通常最好使用这个转换器而不是第一个，特别是对于大型或稀疏数据集。
- en: 'The ratio *r* of nonzero items in the sparse random matrix is called its *density*.
    By default, it is equal to <math><mn>1</mn><mo>/</mo><msqrt><mi>n</mi></msqrt></math>.
    With 20,000 features, this means that only 1 in ~141 cells in the random matrix
    is nonzero: that’s quite sparse! You can set the `density` hyperparameter to another
    value if you prefer. Each cell in the sparse random matrix has a probability *r*
    of being nonzero, and each nonzero value is either –*v* or +*v* (both equally
    likely), where *v* = <math><mn>1</mn><mo>/</mo><msqrt><mi>d</mi><mi>r</mi></msqrt></math>.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏随机矩阵中非零项的比率*r*称为其*密度*。默认情况下，它等于<math><mn>1</mn><mo>/</mo><msqrt><mi>n</mi></msqrt></math>。有了20,000个特征，这意味着随机矩阵中大约141个单元格中只有1个是非零的：这是相当稀疏的！如果你愿意，你可以将`density`超参数设置为另一个值。稀疏随机矩阵中的每个单元格有一个概率*r*是非零的，每个非零值要么是–*v*，要么是+*v*（两者概率相等），其中*v*=<math><mn>1</mn><mo>/</mo><msqrt><mi>d</mi><mi>r</mi></msqrt></math>。
- en: 'If you want to perform the inverse transform, you first need to compute the
    pseudo-inverse of the components matrix using SciPy’s `pinv()` function, then
    multiply the reduced data by the transpose of the pseudo-inverse:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想执行逆变换，你首先需要使用SciPy的`pinv()`函数计算组件矩阵的伪逆，然后将减少的数据乘以伪逆的转置：
- en: '[PRE17]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Warning
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Computing the pseudo-inverse may take a very long time if the components matrix
    is large, as the computational complexity of `pinv()` is *O*(*dn*²) if *d* < *n*,
    or *O*(*nd*²) otherwise.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果组件矩阵很大，计算伪逆可能需要很长时间，因为`pinv()`的计算复杂度是*O*(*dn*²)（如果*d*<*n*），否则是*O*(*nd*²)。
- en: In summary, random projection is a simple, fast, memory-efficient, and surprisingly
    powerful dimensionality reduction algorithm that you should keep in mind, especially
    when you deal with high-dimensional datasets.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Random projection is not always used to reduce the dimensionality of large
    datasets. For example, a [2017 paper](https://homl.info/flies)⁠^([7](ch08.html#idm45720208266832))
    by Sanjoy Dasgupta et al. showed that the brain of a fruit fly implements an analog
    of random projection to map dense low-dimensional olfactory inputs to sparse high-dimensional
    binary outputs: for each odor, only a small fraction of the output neurons get
    activated, but similar odors activate many of the same neurons. This is similar
    to a well-known algorithm called *locality sensitive hashing* (LSH), which is
    typically used in search engines to group similar documents.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: LLE
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Locally linear embedding* (LLE)](https://homl.info/lle)⁠^([8](ch08.html#idm45720208260816))
    is a *nonlinear dimensionality reduction* (NLDR) technique. It is a manifold learning
    technique that does not rely on projections, unlike PCA and random projection.
    In a nutshell, LLE works by first measuring how each training instance linearly
    relates to its nearest neighbors, and then looking for a low-dimensional representation
    of the training set where these local relationships are best preserved (more details
    shortly). This approach makes it particularly good at unrolling twisted manifolds,
    especially when there is not too much noise.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code makes a Swiss roll, then uses Scikit-Learn’s `LocallyLinearEmbedding`
    class to unroll it:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The variable `t` is a 1D NumPy array containing the position of each instance
    along the rolled axis of the Swiss roll. We don’t use it in this example, but
    it can be used as a target for a nonlinear regression task.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting 2D dataset is shown in [Figure 8-10](#lle_unrolling_plot). As
    you can see, the Swiss roll is completely unrolled, and the distances between
    instances are locally well preserved. However, distances are not preserved on
    a larger scale: the unrolled Swiss roll should be a rectangle, not this kind of
    stretched and twisted band. Nevertheless, LLE did a pretty good job of modeling
    the manifold.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![mls3 0810](assets/mls3_0810.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: Figure 8-10\. Unrolled Swiss roll using LLE
  id: totrans-142
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here’s how LLE works: for each training instance **x**^((*i*)), the algorithm
    identifies its *k*-nearest neighbors (in the preceding code *k* = 10), then tries
    to reconstruct **x**^((*i*)) as a linear function of these neighbors. More specifically,
    it tries to find the weights *w*[*i,j*] such that the squared distance between
    **x**^((*i*)) and <math><mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <mrow><msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup></mrow></mrow></math>
    is as small as possible, assuming *w*[*i,j*] = 0 if **x**^((*j*)) is not one of
    the *k*-nearest neighbors of **x**^((*i*)). Thus the first step of LLE is the
    constrained optimization problem described in [Equation 8-4](#lle_first_step),
    where **W** is the weight matrix containing all the weights *w*[*i,j*]. The second
    constraint simply normalizes the weights for each training instance **x**^((*i*)).'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Equation 8-4\. LLE step 1: linearly modeling local relationships'
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mover
    accent="true"><mi mathvariant="bold">W</mi> <mo>^</mo></mover> <mo>=</mo> <munder><mo
    form="prefix">argmin</mo> <mi mathvariant="bold">W</mi></munder> <mstyle scriptlevel="0"
    displaystyle="true"><munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover></mstyle> <msup><mfenced separators="" open="(" close=")"><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><munderover><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <msub><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup></mfenced> <mn>2</mn></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mtext>subject</mtext> <mtext>to</mtext> <mfenced
    separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><msub><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup> <mtext>is</mtext> <mtext>not</mtext>
    <mtext>one</mtext> <mtext>of</mtext> <mtext>the</mtext> <mi>k</mi> <mtext>n.n.</mtext>
    <mtext>of</mtext> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>for</mtext>
    <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <mi>m</mi></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math>
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mover
    accent="true"><mi mathvariant="bold">W</mi> <mo>^</mo></mover> <mo>=</mo> <munder><mo
    form="prefix">argmin</mo> <mi mathvariant="bold">W</mi></munder> <mstyle scriptlevel="0"
    displaystyle="true"><munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover></mstyle> <msup><mfenced separators="" open="(" close=")"><msup><mi
    mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mrow></msup> <mo>-</mo><munderover><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <msub><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>j</mi><mo>)</mrow></msup></mfenced> <mn>2</mn></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mtext>subject</mtext> <mtext>to</mtext> <mfenced
    separators="" open="{" close=""><mtable><mtr><mtd columnalign="left"><mrow><msub><mi>w</mi>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub> <mo>=</mo> <mn>0</mn></mrow></mtd>
    <mtd columnalign="left"><mrow><mtext>if</mtext> <msup><mi mathvariant="bold">x</mi>
    <mrow><mo>(</mo><mi>j</mi><mo>)</mrow></msup> <mtext>is</mtext> <mtext>not</mtext>
    <mtext>one</mtext> <mtext>of</mtext> <mtext>the</mtext> <mi>k</mi> <mtext>n.n.</mtext>
    <mtext>of</mtext> <msup><mi mathvariant="bold">x</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><munderover><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover> <msub><mi>w</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>=</mo> <mn>1</mn></mrow></mtd> <mtd columnalign="left"><mrow><mtext>for</mtext>
    <mi>i</mi> <mo>=</mo> <mn>1</mn> <mo>,</mo> <mn>2</mn> <mo>,</mo> <mo>⋯</mo> <mo>,</mo>
    <mi>m</mi></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math>
- en: 'After this step, the weight matrix <math><mover accent="true"><mi mathvariant="bold">W</mi>
    <mo>^</mo></mover></math> (containing the weights <math><msub><mover><mi>w</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>)
    encodes the local linear relationships between the training instances. The second
    step is to map the training instances into a *d*-dimensional space (where *d*
    < *n*) while preserving these local relationships as much as possible. If **z**^((*i*))
    is the image of **x**^((*i*)) in this *d*-dimensional space, then we want the
    squared distance between **z**^((*i*)) and <math><mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></msubsup> <mrow><msub><mover accent="true"><mi>w</mi> <mo>^</mo></mover>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub> <msup><mi mathvariant="bold">z</mi>
    <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup></mrow></mrow></math> to be
    as small as possible. This idea leads to the unconstrained optimization problem
    described in [Equation 8-5](#lle_second_step). It looks very similar to the first
    step, but instead of keeping the instances fixed and finding the optimal weights,
    we are doing the reverse: keeping the weights fixed and finding the optimal position
    of the instances’ images in the low-dimensional space. Note that **Z** is the
    matrix containing all **z**^((*i*)).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步之后，权重矩阵 <math><mover accent="true"><mi mathvariant="bold">W</mi> <mo>^</mo></mover></math>（包含权重
    <math><msub><mover><mi>w</mi><mo>^</mo></mover><mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>）编码了训练实例之间的局部线性关系。第二步是将训练实例映射到一个
    *d* 维空间（其中 *d* < *n*），同时尽可能保持这些局部关系。如果 **z**^((*i*)) 是在这个 *d* 维空间中 **x**^((*i*))
    的映像，那么我们希望 **z**^((*i*)) 和 <math><mrow><msubsup><mo>∑</mo> <mrow><mi>j</mi><mo>=</mo><mn>1</mn></row>
    <mi>m</mi></msubsup> <mrow><msub><mover accent="true"><mi>w</mi> <mo>^</mo></mover>
    <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub> <msup><mi mathvariant="bold">z</mi>
    <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup></mrow></mrow></math> 之间的平方距离尽可能小。这个想法导致了[方程8-5](#lle_second_step)中描述的无约束优化问题。它看起来与第一步非常相似，但是不是保持实例固定并找到最佳权重，而是相反的：保持权重固定并找到实例映像在低维空间中的最佳位置。注意，**Z**
    是包含所有 **z**^((*i*)) 的矩阵。
- en: 'Equation 8-5\. LLE step 2: reducing dimensionality while preserving relationships'
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程8-5\. LLE步骤2：在保持关系的同时降低维度
- en: <math display="block"><mrow><mover accent="true"><mi mathvariant="bold">Z</mi>
    <mo>^</mo></mover> <mo>=</mo> <munder><mo form="prefix">argmin</mo> <mi mathvariant="bold">Z</mi></munder>
    <mstyle scriptlevel="0" displaystyle="true"><munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover></mstyle> <msup><mfenced separators="" open="(" close=")"><msup><mi
    mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><munderover><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <msub><mover
    accent="true"><mi>w</mi> <mo>^</mo></mover> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <msup><mi mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup></mfenced>
    <mn>2</mn></msup></mrow></math>
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mover accent="true"><mi mathvariant="bold">Z</mi>
    <mo>^</mo></mover> <mo>=</mo> <munder><mo form="prefix">argmin</mo> <mi mathvariant="bold">Z</mi></munder>
    <mstyle scriptlevel="0" displaystyle="true"><munderover><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>m</mi></munderover></mstyle> <msup><mfenced separators="" open="(" close=")"><msup><mi
    mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>i</mi><mo>)</mo></mrow></msup> <mo>-</mo><munderover><mo>∑</mo>
    <mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow> <mi>m</mi></munderover> <msub><mover
    accent="true"><mi>w</mi> <mo>^</mo></mover> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <msup><mi mathvariant="bold">z</mi> <mrow><mo>(</mo><mi>j</mi><mo>)</mo></mrow></msup></mfenced>
    <mn>2</mn></msup></mrow></math>
- en: 'Scikit-Learn’s LLE implementation has the following computational complexity:
    *O*(*m* log(*m*)*n* log(*k*)) for finding the *k*-nearest neighbors, *O*(*mnk*³)
    for optimizing the weights, and *O*(*dm*²) for constructing the low-dimensional
    representations. Unfortunately, the *m*² in the last term makes this algorithm
    scale poorly to very large datasets.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn的LLE实现具有以下计算复杂度：*O*(*m* log(*m*)*n* log(*k*))用于查找*k*个最近邻居，*O*(*mnk*³)用于优化权重，*O*(*dm*²)用于构建低维表示。不幸的是，最后一项中的*m*²使得这个算法在处理非常大的数据集时效率低下。
- en: As you can see, LLE is quite different from the projection techniques, and it’s
    significantly more complex, but it can also construct much better low-dimensional
    representations, especially if the data is nonlinear.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，LLE与投影技术有很大不同，它显著更复杂，但也可以构建更好的低维表示，特别是在数据是非线性的情况下。
- en: Other Dimensionality Reduction Techniques
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他降维技术
- en: 'Before we conclude this chapter, let’s take a quick look at a few other popular
    dimensionality reduction techniques available in Scikit-Learn:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束本章之前，让我们快速看一下Scikit-Learn中提供的其他几种流行的降维技术：
- en: '`sklearn.manifold.MDS`'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.manifold.MDS`'
- en: '*Multidimensional scaling* (MDS) reduces dimensionality while trying to preserve
    the distances between the instances. Random projection does that for high-dimensional
    data, but it doesn’t work well on low-dimensional data.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*多维缩放*（MDS）在尝试保持实例之间的距离的同时降低维度。随机投影可以用于高维数据，但在低维数据上效果不佳。'
- en: '`sklearn.manifold.Isomap`'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.manifold.Isomap`'
- en: '*Isomap* creates a graph by connecting each instance to its nearest neighbors,
    then reduces dimensionality while trying to preserve the *geodesic distances*
    between the instances. The geodesic distance between two nodes in a graph is the
    number of nodes on the shortest path between these nodes.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*Isomap*通过将每个实例连接到其最近邻来创建图，然后在尝试保持实例之间的*测地距离*的同时降低维度。图中两个节点之间的测地距离是这两个节点之间最短路径上的节点数。'
- en: '`sklearn.manifold.TSNE`'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.manifold.TSNE`'
- en: '*t-distributed stochastic neighbor embedding* (t-SNE) reduces dimensionality
    while trying to keep similar instances close and dissimilar instances apart. It
    is mostly used for visualization, in particular to visualize clusters of instances
    in high-dimensional space. For example, in the exercises at the end of this chapter
    you will use t-SNE to visualize a 2D map of the MNIST images.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*t-分布随机邻域嵌入*（t-SNE）在尝试保持相似实例接近和不相似实例分开的同时降低维度。它主要用于可视化，特别是用于在高维空间中可视化实例的聚类。例如，在本章末尾的练习中，您将使用t-SNE来可视化MNIST图像的2D地图。'
- en: '`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.discriminant_analysis.LinearDiscriminantAnalysis`'
- en: '*Linear discriminant analysis* (LDA) is a linear classification algorithm that,
    during training, learns the most discriminative axes between the classes. These
    axes can then be used to define a hyperplane onto which to project the data. The
    benefit of this approach is that the projection will keep classes as far apart
    as possible, so LDA is a good technique to reduce dimensionality before running
    another classification algorithm (unless LDA alone is sufficient).'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '*线性判别分析*（LDA）是一种线性分类算法，在训练过程中学习类别之间最具区分性的轴。然后可以使用这些轴来定义一个超平面，将数据投影到该超平面上。这种方法的好处是投影将尽可能地使类别保持分开，因此LDA是在运行另一个分类算法之前降低维度的好技术（除非仅使用LDA就足够）。'
- en: '[Figure 8-11](#other_dim_reduction_plot) shows the results of MDS, Isomap,
    and t-SNE on the Swiss roll. MDS manages to flatten the Swiss roll without losing
    its global curvature, while Isomap drops it entirely. Depending on the downstream
    task, preserving the large-scale structure may be good or bad. t-SNE does a reasonable
    job of flattening the Swiss roll, preserving a bit of curvature, and it also amplifies
    clusters, tearing the roll apart. Again, this might be good or bad, depending
    on the downstream task.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '[图8-11](#other_dim_reduction_plot)展示了MDS、Isomap和t-SNE在瑞士卷上的结果。MDS成功将瑞士卷展平而不丢失其全局曲率，而Isomap则完全丢失了曲率。根据下游任务的不同，保留大尺度结构可能是好事或坏事。t-SNE在展平瑞士卷方面做得相当不错，保留了一些曲率，并且还放大了聚类，将卷撕裂开。同样，这可能是好事或坏事，取决于下游任务。'
- en: '![mls3 0811](assets/mls3_0811.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 0811](assets/mls3_0811.png)'
- en: Figure 8-11\. Using various techniques to reduce the Swiss roll to 2D
  id: totrans-163
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图8-11。使用各种技术将瑞士卷降维为2D
- en: Exercises
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: What are the main motivations for reducing a dataset’s dimensionality? What
    are the main drawbacks?
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 降低数据集维度的主要动机是什么？主要缺点是什么？
- en: What is the curse of dimensionality?
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 维度诅咒是什么？
- en: Once a dataset’s dimensionality has been reduced, is it possible to reverse
    the operation? If so, how? If not, why?
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦数据集的维度降低了，是否可以反转操作？如果可以，如何操作？如果不行，为什么？
- en: Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: PCA能用于降低高度非线性数据集的维度吗？
- en: Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained
    variance ratio to 95%. How many dimensions will the resulting dataset have?
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设您对一个1000维数据集执行PCA，将解释的方差比设置为95%。结果数据集将有多少维度？
- en: In what cases would you use regular PCA, incremental PCA, randomized PCA, or
    random projection?
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在什么情况下会使用常规PCA、增量PCA、随机PCA或随机投影？
- en: How can you evaluate the performance of a dimensionality reduction algorithm
    on your dataset?
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何评估数据集上降维算法的性能？
- en: Does it make any sense to chain two different dimensionality reduction algorithms?
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 链两种不同的降维算法有意义吗？
- en: Load the MNIST dataset (introduced in [Chapter 3](ch03.html#classification_chapter))
    and split it into a training set and a test set (take the first 60,000 instances
    for training, and the remaining 10,000 for testing). Train a random forest classifier
    on the dataset and time how long it takes, then evaluate the resulting model on
    the test set. Next, use PCA to reduce the dataset’s dimensionality, with an explained
    variance ratio of 95%. Train a new random forest classifier on the reduced dataset
    and see how long it takes. Was training much faster? Next, evaluate the classifier
    on the test set. How does it compare to the previous classifier? Try again with
    an `SGDClassifier`. How much does PCA help now?
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载MNIST数据集（在[第3章](ch03.html#classification_chapter)介绍）并将其分为训练集和测试集（取前60000个实例进行训练，剩下的10000个进行测试）。在数据集上训练一个随机森林分类器并计时，然后评估测试集上的结果模型。接下来，使用PCA降低数据集的维度，解释方差比为95%。在降维后的数据集上训练一个新的随机森林分类器并查看所需时间。训练速度快了吗？接下来，在测试集上评估分类器。与之前的分类器相比如何？再尝试使用`SGDClassifier`。现在PCA有多大帮助？
- en: Use t-SNE to reduce the first 5,000 images of the MNIST dataset down to 2 dimensions
    and plot the result using Matplotlib. You can use a scatterplot using 10 different
    colors to represent each image’s target class. Alternatively, you can replace
    each dot in the scatterplot with the corresponding instance’s class (a digit from
    0 to 9), or even plot scaled-down versions of the digit images themselves (if
    you plot all digits the visualization will be too cluttered, so you should either
    draw a random sample or plot an instance only if no other instance has already
    been plotted at a close distance). You should get a nice visualization with well-separated
    clusters of digits. Try using other dimensionality reduction algorithms, such
    as PCA, LLE, or MDS, and compare the resulting visualizations.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用t-SNE将MNIST数据集的前5000个图像降至2维，并使用Matplotlib绘制结果。您可以使用散点图，使用10种不同的颜色表示每个图像的目标类别。或者，您可以用散点图中的每个点替换为相应实例的类别（从0到9的数字），甚至绘制数字图像本身的缩小版本（如果绘制所有数字，可视化将太混乱，因此您应该绘制一个随机样本或仅在没有其他实例已经绘制在附近距离的情况下绘制一个实例）。您应该得到一个具有良好分离的数字簇的漂亮可视化效果。尝试使用其他降维算法，如PCA、LLE或MDS，并比较结果的可视化效果。
- en: Solutions to these exercises are available at the end of this chapter’s notebook,
    at [*https://homl.info/colab3*](https://homl.info/colab3).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这些练习的解决方案可在本章笔记本的末尾找到，网址为[*https://homl.info/colab3*](https://homl.info/colab3)。
- en: ^([1](ch08.html#idm45720209594752-marker)) Well, four dimensions if you count
    time, and a few more if you are a string theorist.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: （[1]）嗯，如果计算时间，那就是四维，如果你是弦理论学家，那就更多了。
- en: ^([2](ch08.html#idm45720209589856-marker)) Watch a rotating tesseract projected
    into 3D space at [*https://homl.info/30*](https://homl.info/30). Image by Wikipedia
    user NerdBoy1392 ([Creative Commons BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0)).
    Reproduced from [*https://en.wikipedia.org/wiki/Tesseract*](https://en.wikipedia.org/wiki/Tesseract).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: （[2]）在[*https://homl.info/30*](https://homl.info/30)观看一个在3D空间中投影的旋转四维立方体。图片由维基百科用户NerdBoy1392提供（[知识共享署名-相同方式共享3.0](https://creativecommons.org/licenses/by-sa/3.0)）。转载自[*https://en.wikipedia.org/wiki/Tesseract*](https://en.wikipedia.org/wiki/Tesseract)。
- en: '^([3](ch08.html#idm45720209584784-marker)) Fun fact: anyone you know is probably
    an extremist in at least one dimension (e.g., how much sugar they put in their
    coffee), if you consider enough dimensions.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: （[3]）有趣的事实：如果考虑足够多的维度，你认识的任何人可能在至少一个维度上是极端的（例如，他们在咖啡中放多少糖）。
- en: '^([4](ch08.html#idm45720209523296-marker)) Karl Pearson, “On Lines and Planes
    of Closest Fit to Systems of Points in Space”, *The London, Edinburgh, and Dublin
    Philosophical Magazine and Journal of Science* 2, no. 11 (1901): 559–572.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: （[4]）Karl Pearson，“关于空间中点系统的最佳拟合线和平面”，*伦敦、爱丁堡和都柏林哲学杂志和科学杂志* 2，第11号（1901年）：559-572。
- en: '^([5](ch08.html#idm45720208783824-marker)) Scikit-Learn uses the [algorithm](https://homl.info/32)
    described in David A. Ross et al., “Incremental Learning for Robust Visual Tracking”,
    *International Journal of Computer Vision* 77, no. 1–3 (2008): 125–141.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: （[5]）Scikit-Learn使用David A. Ross等人描述的[算法](https://homl.info/32)，“用于稳健视觉跟踪的增量学习”，*国际计算机视觉杂志*
    77，第1-3号（2008年）：125-141。
- en: ^([6](ch08.html#idm45720208518368-marker)) *ε* is the Greek letter epsilon,
    often used for tiny values.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: （[6]）*ε*是希腊字母ε，通常用于表示微小值。
- en: '^([7](ch08.html#idm45720208266832-marker)) Sanjoy Dasgupta et al., “A neural
    algorithm for a fundamental computing problem”, *Science* 358, no. 6364 (2017):
    793–796.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: （[7]）Sanjoy Dasgupta等人，“一个基本计算问题的神经算法”，*Science* 358，第6364号（2017年）：793-796。
- en: '^([8](ch08.html#idm45720208260816-marker)) Sam T. Roweis and Lawrence K. Saul,
    “Nonlinear Dimensionality Reduction by Locally Linear Embedding”, *Science* 290,
    no. 5500 (2000): 2323–2326.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: （[8]）Sam T. Roweis和Lawrence K. Saul，“通过局部线性嵌入进行非线性降维”，*Science* 290，第5500号（2000年）：2323-2326。
