<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 7. Deploying to Production"><div class="chapter" id="chapter_7_deploying_to_production_1749354011062634">
      <h1><span class="label">Chapter 7. </span>Deploying to Production</h1>
      <p>The <a href="https://oreil.ly/pTAIv">Knight Capital incident</a> of August 1, 2012, stands as a stark example<a contenteditable="false" data-type="indexterm" data-primary="Knight Capital trading incident" id="id902"/> of how a software deployment<a contenteditable="false" data-type="indexterm" data-primary="production deployment" id="xi_productiondeployment73186"/> gone wrong can have catastrophic consequences. On that day, Knight Capital, then one of the largest trading firms in the US, deployed a large update to its automated trading system. Due to a confluence of factors, including limited automation, human error in deployments, and poor feature flag management, outdated code was accidentally reactivated, causing the system to <a href="https://oreil.ly/efPYb">rapidly place erroneous orders in the stock market</a>.</p>
      <p>Within just 45 minutes, the faulty algorithm had executed over 4 million trades, resulting in a staggering loss of $460 million for the firm. This incident not only nearly bankrupted Knight Capital, leading to its eventual acquisition, but also caused significant market disruption. It highlighted the critical importance of robust deployment practices, thorough testing, governance, and fail-safe mechanisms in high-stakes software environments. </p>
      <p>Deploying to production can be a high-stakes activity. While not every application is a market-making trading platform, applications worth updating have people who depend on them, and changing anything introduces risk. Although we might prefer to avoid this risk by deploying less often, we face business demands for more frequent change. Moreover, certain types of risk increase as we delay and accumulate more and more changes into our planned release, making the continuous delivery approach more valuable. </p>
      <p>In previous chapters, as we’ve navigated the software delivery process, we’ve hit upon strategies to mitigate the risk of finally deploying to production. In <a data-type="xref" href="ch02.html#chapter_2_source_control_management_1749354010078326">Chapter 2</a>, we discussed the importance of code reviews. In <a data-type="xref" href="ch03.html#chapter_3_the_build_and_pre_deployment_testing_steps_of_cont_1749354010266208">Chapter 3</a>, we looked at how to use early scans and unit testing to detect issues quickly. <a data-type="xref" href="ch04.html#chapter_4_deploying_to_test_environments_1749354010445896">Chapter 4</a> described additional types of testing to harden your software and reviewed best practices in deploying to test environments. By using consistent tooling, pipeline steps, and deployment strategies, and by parameterizing for differences in environments, we use our deployments to test environments to vet our deployments to production. In <a data-type="xref" href="ch05.html#chapter_5_securing_applications_and_the_software_supply_chai_1749354010735711">Chapter 5</a>, we dived deep into security, reviewing the practices that help secure our production deployments. </p>
      <p>Today, artificial intelligence<a contenteditable="false" data-type="indexterm" data-primary="AI (artificial intelligence) systems" data-secondary="production deployment impacts" id="id903"/> is transforming how organizations approach production deployments to prevent such disasters. ML systems now analyze deployment patterns, detect anomalies during rollouts, and verify application health with greater precision than traditional monitoring. Unlike rule-based verification, which relies on predefined thresholds, AI systems can learn normal behavior patterns unique to each application and detect subtle deviations that might indicate emerging problems. These capabilities allow teams to deploy faster while paradoxically reducing risk—the opposite of the traditional speed-versus-safety trade-off.</p>
      <p>In this chapter, in addition to covering the transformative role of AI, we’ll look at best practices for governing production deployments and strategies to safely deploy to production, and we’ll discuss observability to validate the quality of production deployments. We’ll explore how modern AI-powered deployment tooling helps mitigate risk through intelligent verification rather than just reactive monitoring, and how AI-powered systems evaluate multiple signals simultaneously to determine deployment health, catching issues that might slip past human operators. </p>
      <section data-type="sect1" data-pdf-bookmark="Governing Production Deployments"><div class="sect1" id="chapter_7_governing_production_deployments_1749354011062749">
        <h1>Governing Production Deployments</h1>
        <p>The Knight Capital incident is a good reminder: the consequences of deploying<a contenteditable="false" data-type="indexterm" data-primary="governance" data-secondary="production deployment" id="xi_governanceproductiondeployment71189"/><a contenteditable="false" data-type="indexterm" data-primary="production deployment" data-secondary="governance" id="xi_productiondeploymentgovernance71189"/><a contenteditable="false" data-type="indexterm" data-primary="deployment" data-secondary="governance during" id="xi_deploymentgovernanceduring71189"/> software with defects can be nothing short of catastrophic financial ruin. Your organization’s trust and credibility are also at risk. For the organization, the cost of fixing defects post-deployment can skyrocket, far exceeding the expense of addressing them during development. </p>
        <p>To deploy with confidence, we need to understand what code changed and who made those changes. We need to validate that the code review processes we put in place were conducted, and understand who conducted those reviews. For any dependencies that were introduced, we want to understand them and know that they comply with our policies. We want to know if they were reviewed for any known defects. We need assurance that the builds, scans, and test processes that we require were executed against all code changes. And of course, we want to ensure that the results of the scans and tests, in fact, met our criteria for passing. Lastly, we require evidence that our development processes themselves remain in compliance with the frameworks and requirements relevant to our organization.</p>
        <p>Stringent code reviews, thorough and robust testing practices, and automated, repeatable deployment procedures are essential to avoiding deployment failures. However, without appropriate governance and controls to ensure that<em> </em>we’ve adhered to our processes, all of our efforts can be rendered ineffective. </p>
        <p>AI<a contenteditable="false" data-type="indexterm" data-primary="AI (artificial intelligence) systems" data-secondary="and governance" data-secondary-sortas="governance" id="id904"/> is beginning to transform deployment governance, though most applications are still emerging. Current AI systems focus primarily on analyzing deployment patterns to identify risk factors and policy violations rather than making autonomous decisions. These systems can process more deployment variables simultaneously than humans, helping to identify subtle correlations between code changes, deployment configurations, and historical incidents. Organizations are beginning to use these insights to refine their governance frameworks, though human oversight remains essential for final approval decisions.</p>
        <div data-type="note" epub:type="note"><h6>Note</h6>
          <p>Deployment governance is simply the systematic oversight and control of the software deployment process to ensure the rules and policies we’ve defined are enforced. Fundamentally, governance is about reducing the risk of change. Governance includes the policies, processes, and tools that organizations use to ensure that software deployments are carried out in a consistent, controlled, secure, and compliant manner. The challenge in governance is balancing the need for agility and innovation with the need for stability and risk management. </p>
        </div>
        <p>In the next few sections, we’ll discuss traditional and modern approaches to deployment governance. We’ll investigate how to automate the enforcement of our governing policies to make our delivery process more efficient. We’ll review tools and strategies that support our governance processes, and lastly, we’ll look at the future of deployment governance.</p>
        <section data-type="sect2" data-pdf-bookmark="Traditional Approaches to Deployment Governance"><div class="sect2" id="chapter_7_traditional_approaches_to_deployment_governance_1749354011062811">
          <h2>Traditional Approaches to Deployment Governance</h2>
          <p>Traditional approaches to deployment governance are those built for a pre-DevOps world. In this world, changes to production were infrequent, risky, and executed by a traditional operations team. Decision making was centralized and involved rigid processes.</p>
          <p>The Information Technology Infrastructure Library (ITIL)<a contenteditable="false" data-type="indexterm" data-primary="Information Technology Infrastructure Library (ITIL)" id="xi_InformationTechnologyInfrastructureLibraryITIL72270"/><a contenteditable="false" data-type="indexterm" data-primary="ITIL (Information Technology Infrastructure Library)" id="xi_ITILInformationTechnologyInfrastructureLibrary72270"/> is one widely adopted framework that characterizes a traditional approach. ITIL originally emerged in the 1980s as a response to the need for standardized IT management practices, evolving from a collection of best practices into a comprehensive framework. It includes several processes and practices that are directly relevant to deployment governance.</p>
          <p>One of these is the change management process<a contenteditable="false" data-type="indexterm" data-primary="change management process, traditional" id="id905"/>, which defines a structured approach for managing all changes to services and infrastructure, including deployments. It prescribes formal documentation of a proposed change, including its purpose, scope, impact, and risk assessment. A Change Advisory Board (CAB)<a contenteditable="false" data-type="indexterm" data-primary="Change Advisory Board (CAB)" id="xi_ChangeAdvisoryBoardCAB723322"/><a contenteditable="false" data-type="indexterm" data-primary="CAB (Change Advisory Board)" id="xi_CABChangeAdvisoryBoard723322"/> or a similar body assesses changes. The change request is formally authorized or denied based on its merits and potential risks. If the change is approved and executed, a <span class="keep-together">post-implementation</span> assessment is conducted to ensure the change achieved its objectives and identify any lessons learned. </p>
          <p>The release management process<a contenteditable="false" data-type="indexterm" data-primary="release management process, traditional governance (ITIL)" id="id906"/> is similarly formal and orders the planning, scheduling, and controlling of releases into production environments. It’s closely related to the change management process and is intended to ensure that deployments are executed in a controlled and transparent manner.</p>
          <p>CABs are a typical feature of approaches like those defined by ITIL. A CAB is a committee of individuals responsible for formally assessing and approving or rejecting proposed changes to software. This board might include a change manager responsible for coordinating change request reviews and tracking change implementation, as well as technical experts, business stakeholders, security specialists, compliance officers, and others. The intention is to reduce risk through thorough evaluation of requests from several perspectives. </p>
          <p>Moreover, CABs ensure accountability if anything does go wrong. While a highly functioning CAB will provide the intended oversight, at their worst CABs consist of inattentive reviewers that rubber-stamp reviews with little to no assessment. Or a CAB may be nominally effective but hopelessly inefficient. Email-driven approval processes are slowed by ignored emails, approvers being out of the office with no delegation, and review meetings getting rescheduled.</p>
          <p>Research shows that these traditional CAB processes aren’t just inefficient, they’re actually counterproductive to the stability they aim to ensure. Writing about their landmark study of high-performing organizations in their book <em>Accelerate</em>, Forsgren<a contenteditable="false" data-type="indexterm" data-primary="Accelerate: The Science of Lean Software and DevOps (Forsgren, Humble, Kim)" id="id907"/><a contenteditable="false" data-type="indexterm" data-primary="Forsgren, Nicole" id="id908"/><a contenteditable="false" data-type="indexterm" data-primary="Humble, Jez" id="id909"/><a contenteditable="false" data-type="indexterm" data-primary="Kim, Gene" id="id910"/>, Humble, and Kim explain that “external approvals were negatively correlated with lead time, deployment frequency, and restore time and had no correlation with change fail rate.” In other words, approval by external bodies like CABs demonstrably slows down delivery without improving stability. </p>
          <p>This occurs because CABs divorce responsibility from knowledge; the people with the deepest understanding of the changes aren’t the ones making approval decisions. While these committees create the appearance of due diligence, they often function as compliance theater, giving organizations someone to point to when things go wrong rather than actually preventing failures. The illusion of control they provide can even reduce vigilance among those implementing changes, since “the CAB approved it” becomes a shield against accountability.</p>
          <p>The expense of CAB meetings, coupled with the ineffectiveness and delay, was tolerable when applications were released infrequently. As release frequencies have increased, the trouble with CABs is increasingly clear<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_InformationTechnologyInfrastructureLibraryITIL72270" id="id911"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_ChangeAdvisoryBoardCAB723322" id="id912"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_CABChangeAdvisoryBoard723322" id="id913"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_ITILInformationTechnologyInfrastructureLibrary72270" id="id914"/>.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Modern Approaches to Deployment Governance"><div class="sect2" id="chapter_7_modern_approaches_to_deployment_governance_1749354011062862">
          <h2>Modern Approaches to Deployment Governance</h2>
          <p>In previous chapters, we explored how to streamline the development process by automating<a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="deployment governance" id="xi_automationdeploymentgovernance733103"/> steps at every stage, enabling faster and more frequent software releases. Modern approaches to deployment governance are similarly focused on automating the manual steps that are an unnecessary obstacle to releasing software. </p>
          <p>Instead of relying on committees and manual approvals for deployment decisions, modern approaches favor automated decision making and deployments. Because the stakes of production deployment are so high, this must be done with great care. In this section we’ll explore how.</p>
          <p>In addition to automation, modern governance approaches also leverage contemporary strategies and tools to manage compliance. We’ll look at how to use audit logs to simplify compliance, and tools like Open Policy Agent (OPA) to enforce security and regulatory standards.</p>
          <section data-type="sect3" data-pdf-bookmark="Automating decision making"><div class="sect3" id="chapter_7_automating_decision_making_1749354011062914">
            <h3>Automating decision making</h3>
            <p>With modern CI/CD tools<a contenteditable="false" data-type="indexterm" data-primary="CI/CD pipeline" data-secondary="autonomous deployment decisions" id="id915"/> we can empower our pipelines to make autonomous<a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="production deployment" id="id916"/><a contenteditable="false" data-type="indexterm" data-primary="deployment" data-secondary="automation in" id="id917"/> deployment decisions. If we can ensure that our pipelines can adequately enforce governance policies to maintain our standards, we can accelerate software delivery by removing or minimizing manual approvals.</p>
            <p>Consider these steps to automate deployment decision making in your delivery process:</p>
            <dl>
              <dt>1. Identify your “pass” criteria</dt>
              <dd>
                <p>Identifying clear criteria<a contenteditable="false" data-type="indexterm" data-primary="“pass” criteria, automating deployment process" data-primary-sortas="pass" id="id918"/> for promoting builds is crucial for automating your deployment process, but this can be challenging. One bank that we worked with documented its controls in a three-inch-thick binder containing hundreds of pages of regulations and policy. Often, decision makers may rely on both objective data and subjective judgment. Ambiguity can make it challenging to translate human decision making into a set of rigid, automated rules. For example, a decision maker might promote a build with a few minor test failures if they believe the issues are low risk and unlikely to impact users. However, translating this intuition into an automated rule that accurately assesses risk and user impact can be complex. AI has a growing role in bringing the fuzzier elements of human decision making into fully or mostly automated flows. If used this way, it should be required to explain its recommendations and insights.</p>
              </dd>
              <dt class="pagebreak-before less_space">2. Use “quality gates” to implement complex criteria to automate as many controls as possible</dt>
              <dd>
                <p>Gates<a contenteditable="false" data-type="indexterm" data-primary="“quality gates,” automating deployment process" data-primary-sortas="quality gates, automating deployment process" id="id919"/> are checkpoints within a CI/CD pipeline that evaluate specific criteria to determine whether a build should proceed to the next stage. Gates can take into account test results, code quality metrics based on static analysis results, code coverage, and adherence to coding standards, security scans results, and performance metrics. Other tools allow you to introduce a pipeline step that fails if the decision is “no,” or you can set up conditional execution based on your specific criteria. Often, the simplest approach is to configure each set of tests to fail if it doesn’t meet your standards. This way, if any step fails, the entire pipeline halts, preventing the promotion of a substandard build. </p>
              </dd>
              <dt>3. Consider historical results when automating nuanced decisions</dt>
              <dd>
                <p>For instance, security initiatives often start with a zero-tolerance policy for new high-priority issues but tolerate existing ones while the team works through them. This requires considering historical data, not just the most recent results. </p>
              </dd>
              <dt>4. Finally, standardize on that automation</dt>
              <dd>
                <p>Use the choice of standardization<a contenteditable="false" data-type="indexterm" data-primary="standardization on deployment automation, adopting" id="id920"/> or painful manual compliance as an incentive to use standardized tooling. Teams at the bank that we worked with were given a choice to deploy to production by certifying that a release complies with all of the controls detailed in the binder, or by using their standardized automated processes and tooling. This became an easy choice.</p>
              </dd>
            </dl>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Building strong audit trails to automate compliance"><div class="sect3" id="chapter_7_building_strong_audit_trails_to_automate_complianc_1749354011062963">
            <h3>Building strong audit trails to automate compliance</h3>
            <p>Deployment governance and compliance<a contenteditable="false" data-type="indexterm" data-primary="audit trails to automate compliance" id="id921"/><a contenteditable="false" data-type="indexterm" data-primary="standards adherence" data-secondary="audit trails to automate compliance" id="id922"/><a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="of audit trails for compliance" data-secondary-sortas="audit trails for compliance" id="id923"/> are closely related. Effective governance practices are crucial in achieving and maintaining compliance with various regulatory standards and frameworks. </p>
            <p>We reviewed several frameworks in <a data-type="xref" href="ch05.html#chapter_5_securing_applications_and_the_software_supply_chai_1749354010735711">Chapter 5</a>, specifically security-related ones. PCI DSS<a contenteditable="false" data-type="indexterm" data-primary="Payment Card Industry Data Security Standard (PCI DSS)" id="id924"/> is one widely applicable example. It’s used to ensure that all companies that accept, process, store, or transmit credit card information maintain a secure environment. Regardless of the size or number of transactions you process, if your organization handles cardholder data then you are subject to its requirements. The major card brands (Visa, Mastercard, etc.) may impose fines or restrict your ability to process card payments if compliance cannot be demonstrated.</p>
            <p>While PCI DSS primarily focuses on securing cardholder data, several requirements directly pertain to the software development and deployment process. This is to ensure the overall security of the environment where this data is handled. For example, PCI DSS requires that you develop and maintain secure systems and applications by taking steps such as conducting reviews of custom code prior to release to production and addressing common coding vulnerabilities. PCI DSS also includes testing requirements, mandating internal and external penetration testing after any significant infrastructure or application upgrade or modification.</p>
            <p>A strong and comprehensive audit trail is essential to demonstrating the practices that compliance requires. And while your organization may not be subject to PCI DSS requirements, many other frameworks that may be relevant will have similar requirements of your development and deployment processes.</p>
            <p>Your source control<a contenteditable="false" data-type="indexterm" data-primary="source control management (SCM)" data-secondary="for deployment governance" data-secondary-sortas="deployment governance" id="id925"/> and CI/CD<a contenteditable="false" data-type="indexterm" data-primary="CI/CD pipeline" data-secondary="audit trail for deployment governance" id="id926"/> systems play a vital role here by capturing the granular details of every action taken within the delivery pipeline, from code commits and builds to test results, deployments, and environment configurations, along with the associated user, timestamp, and any relevant metadata. This includes logging user actions, system events, artifact tracking, configuration changes, and external integrations. By storing this information in a structured and accessible format, CI/CD tools provide a versatile audit trail that is adaptable to any number of security and regulatory frameworks.</p>
            <p>Tools that support a strong audit trail allow your organization to demonstrate compliance without maintaining separate logs for each framework. It also enables you to proactively address potential security or compliance concerns.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Managing enforcement with Policy as Code"><div class="sect3" id="chapter_7_managing_enforcement_with_policy_as_code_1749354011063012">
            <h3>Managing enforcement with Policy as Code</h3>
            <p>Policy as Code (PaC)<a contenteditable="false" data-type="indexterm" data-primary="Policy-as-Code (PaC) frameworks" id="id927"/><a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="production deployment" id="id928"/><a contenteditable="false" data-type="indexterm" data-primary="deployment" data-secondary="automation in" id="id929"/><a contenteditable="false" data-type="indexterm" data-primary="PaC (Policy-as-Code) frameworks" id="id930"/> can be instrumental in automating your production deployments while maintaining robust governance. PaC is the practice of defining and managing security, compliance, and operational policies <em>as code</em>, allowing for automated enforcement. Policies are defined in a declarative language and can be managed like any other critical piece of code: versioned in source control, allowing for tracking, collaboration and required code reviews, and rollback capabilities.</p>
            <p>OPA<a contenteditable="false" data-type="indexterm" data-primary="Open Policy Agent (OPA)" id="xi_OpenPolicyAgentOPA77133"/> is a popular open source policy engine used to implement PaC. With OPA, every deployment is automatically evaluated against your defined policies, ensuring consistent enforcement without slowing down your delivery process. Imagine your deployment policy requires all container images to be scanned for critical vulnerabilities before reaching production. Using OPA, you can express this PaC, and integrate it into your pipeline. Now, every time a deployment is triggered, OPA automatically scans the image and either allows the deployment to proceed if the image is clean or halts it if vulnerabilities are found. This eliminates manual security checks and ensures consistent adherence to your security standards without human intervention.</p>
            <p>OPA’s versatility extends beyond security checks. You can codify various deployment policies, such as enforcing canary deployments, requiring approvals for specific changes, or validating resource configurations. By automating these checks, you gain confidence that every deployment adheres to your organization’s standards and regulatory requirements. This not only accelerates your delivery process but also reduces the risk of human error and noncompliance<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_automationdeploymentgovernance733103" id="id931"/>.</p>
          </div></section>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Safeguarding the Deployment Process"><div class="sect2" id="chapter_7_safeguarding_the_deployment_process_1749354011063059">
          <h2>Safeguarding the Deployment Process</h2>
          <p>Tightly controlling your governance mechanisms<a contenteditable="false" data-type="indexterm" data-primary="security considerations" data-secondary="governance mechanisms for deployment" id="id932"/> helps protect your deployment. Developer empowerment is also critical in modern deployments. In practice, you need to strike a balance between the two. While you want to enable developers to adapt their deployment pipelines, you also need to safeguard against potential risks. Malicious actors can tamper with or bypass the very governance mechanisms you put in place, or they can be corrupted by human error. Alternatively, overly tight controls on deployments can create another obstacle to efficient deployments.</p>
          <p>OPA can help here too. With OPA you apply strict controls on the policy update process itself, ensuring that any changes to your governance framework are carefully vetted and compliant. By centralizing policy rules in OPA and applying them to pipelines, you create a separation of concerns. This makes it more difficult for individual developers to circumvent policies, as they would need to modify the central OPA policies, which can be subject to stricter access controls, peer reviews, and audit trails. </p>
          <p>As we increasingly rely on AI to generate our pipelines for us, OPA policies provide both directional input to the AI as to what we want, and protection ensuring that the output of the AI is in compliance with our standards<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_OpenPolicyAgentOPA77133" id="id933"/>.</p>
          <p>Another important control in safeguarding your deployment process is implementing robust RBAC<a contenteditable="false" data-type="indexterm" data-primary="role-based access control (RBAC)" id="id934"/><a contenteditable="false" data-type="indexterm" data-primary="RBAC (role-based access control)" id="id935"/>. As discussed in <a data-type="xref" href="ch02.html#chapter_2_source_control_management_1749354010078326">Chapter 2</a>, RBAC allows you to granularly control who has access to modify pipelines and sensitive configuration settings within your CI/CD platform. This ensures that only authorized personnel can make changes to your deployment process, minimizing the risk of malicious activities.</p>
          <p>By combining these approaches, you can centralize policy enforcement, ensuring your deployments are tamper-proof and effectively monitored.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Future Trends in Deployment Governance"><div class="sect2" id="chapter_7_future_trends_in_deployment_governance_1749354011063105">
          <h2>Future Trends in Deployment Governance</h2>
          <p>As in nearly every area of software development, AI<a contenteditable="false" data-type="indexterm" data-primary="AI (artificial intelligence) systems" data-secondary="and governance" data-secondary-sortas="governance" id="id936"/> and ML will drive important future trends in deployment governance. Predictive analytics<a contenteditable="false" data-type="indexterm" data-primary="predictive analytics" id="id937"/>, for example, is a branch of data analytics that applies ML techniques for analyzing historical data to predict future outcomes. Applied to software deployments, predictive analytics can be used to identify patterns and risk factors to flag potential issues. Vendors are creating dashboards, such as Digital.ai’s “Change Risk Prediction,” based on trends like team failure rates and defects found in testing. Today, most of these solutions are relatively straightforward correlations found in broad sets. It’s not unreasonable to expect more insights from models as we go forward, especially from DevOps platforms with easy access to wider data sets.</p>
          <p>Your team can proactively address problems before they impact users. AI and ML can be used to automatically enforce governance policies in real time, analyzing code changes, configurations, and deployments to ensure compliance with security and operational standards. These advancements will empower organizations to deliver software with increased speed, confidence, and resilience.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Reconciling Traditional and Modern Approaches"><div class="sect2" id="chapter_7_reconciling_traditional_and_modern_approaches_1749354011063151">
          <h2>Reconciling Traditional and Modern Approaches</h2>
          <p>Within a traditional governance approach, ITIL defines a standard change as a pre-approved, low-risk change with a well-defined procedure, allowing for quicker implementation with minimal formal authorization. By using modern DevOps practices, relying on quality gates and modern policy enforcement, we can significantly de-risk even complex software deployments. This level of control and reliability allows these deployments to be treated as standard changes. Essentially, the inherent risk mitigation within modern DevOps practices aligns with ITIL’s goal of standardized, predictable change management, enabling faster and more frequent deployments without compromising stability or compliance.</p>
          <p>In <a data-type="xref" data-xrefstyle="select:nopage" href="#chapter_7_production_deployment_strategies_1749354011063200">“Production Deployment Strategies”</a> we’ll explore using progressive deployments to further de-risk production deployments<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_governanceproductiondeployment71189" id="id938"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_productiondeploymentgovernance71189" id="id939"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_deploymentgovernanceduring71189" id="id940"/>.</p>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Production Deployment Strategies"><div class="sect1" id="chapter_7_production_deployment_strategies_1749354011063200">
        <h1>Production Deployment Strategies</h1>
        <p>In <a data-type="xref" href="ch04.html#chapter_4_deploying_to_test_environments_1749354010445896">Chapter 4</a> we covered how to automate our deployment processes<a contenteditable="false" data-type="indexterm" data-primary="production deployment" data-secondary="strategies for" id="xi_productiondeploymentstrategiesfor796215"/>, and we have now looked at how to mitigate risk through deployment governance practices. Next we turn our attention to the actual business of deploying our software to production. In this section we’ll look at how to further mitigate risk with progressive deployment techniques. With even the strongest governance and the most cautious progressive deployments, our deployments may still fail. We must come prepared with a rollback strategy, so we’ll look at approaches to revert quickly. Lastly, we’ll look at tool selection. Choosing modern tools can help you make governance, progressive deployments, and rolling back easy.</p>
        <section data-type="sect2" data-pdf-bookmark="The Traditional Big-Bang Deployment"><div class="sect2" id="chapter_7_the_traditional_big_bang_deployment_1749354011063250">
          <h2>The Traditional Big-Bang Deployment</h2>
          <p>Before we look at modern approaches, we can remind ourselves of the traditional approach—and what still may be required for some elements of stateful applications. Traditionally, we would take our application offline, upgrade every instance of every component of the application, and start the application back up. After a quick validation, we would expose it back to users, and watch it for a period of time to make sure it looked healthy before deeming the deployment a success. If there was a problem, we would take the application back offline and roll back the application to the best of our ability—often a daunting challenge. </p>
          <p>This traditional approach required application downtime, introduced significant risk, and demanded significant attention from engineers. The opportunities to do better are abundant.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Using a Progressive Delivery Strategy"><div class="sect2" id="chapter_7_using_a_progressive_delivery_strategy_1749354011063298">
          <h2>Using a Progressive Delivery Strategy</h2>
          <p>Software deployments<a contenteditable="false" data-type="indexterm" data-primary="progressive delivery strategy" id="xi_progressivedeliverystrategy710434"/><a contenteditable="false" data-type="indexterm" data-primary="delivery process" data-secondary="progressive delivery strategy" id="xi_deliveryprocessprogressivedeliverystrategy710434"/> can be like walking a tightrope—one wrong step and the consequences can be severe. But progressive deployment strategies offer you a safety net. By gradually rolling out changes and closely monitoring their impact, these strategies minimize risks and allow for quick course correction if problems arise. In this section we’ll look at a number of popular deployment strategies including rolling updates, blue-green deployments, canary deployments, and the use of feature flags.</p>
          <section data-type="sect3" data-pdf-bookmark="Deploying rolling updates"><div class="sect3" id="chapter_7_deploying_rolling_updates_1749354011063346">
            <h3>Deploying rolling updates</h3>
            <p>Rolling deployments<a contenteditable="false" data-type="indexterm" data-primary="rolling deployments" id="id941"/> are a very common delivery strategy in which you gradually update an application or service by incrementally replacing instances of the old version with the new version. This is done in a controlled manner, ensuring that a certain number of instances are always available to handle user traffic during the update process.</p>
            <p>Rolling deployments have distinct advantages. They minimize downtime as the application remains accessible throughout the update process. Importantly, rolling deployments reduce risk. By updating instances incrementally, potential issues with the new version can be detected and addressed early on, limiting their impact. And this type of deployment can be customized to fit specific application needs, allowing for endless adjustments to the speed and scale of the update process.</p>
            <p>However, implementing and managing rolling deployments can be more complex than other deployment strategies, especially for large-scale or distributed systems. There is also potential for inconsistencies. During the update process, two different versions of the application, running simultaneously, could lead to differences in data or user experience. In addition, rolling back an ongoing deployment can be complicated, and additional steps to preserve data integrity are required.</p>
            <p>Implementation options are numerous. Kubernetes<a contenteditable="false" data-type="indexterm" data-primary="Kubernetes" data-secondary="rolling updates support" id="id942"/> provides built-in support for rolling updates through its Deployment object. New pods with the updated version are gradually created, and old pods are terminated once the new ones are ready. Container<a contenteditable="false" data-type="indexterm" data-primary="containers and containerization" data-secondary="orchestration software support for rolling updates" id="id943"/><a contenteditable="false" data-type="indexterm" data-primary="orchestration, container" id="id944"/> orchestration platforms (e.g., Docker Swarm, Nomad) offer similar mechanisms for rolling updates, allowing for incremental replacement of containers or services. Load balancers<a contenteditable="false" data-type="indexterm" data-primary="load balancers, for rolling updates" id="id945"/> can be used to implement rolling updates by gradually shifting traffic from old instances to new instances as they become available. In some cases, rolling deployments might be implemented using custom scripts or automation tools that manage the update process and monitor the health of the application.</p>
            <p>While rolling deployments require effort to implement, they offer a valuable option for minimizing downtime and risk during application updates. </p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Using blue-green deployments"><div class="sect3" id="chapter_7_using_blue_green_deployments_1749354011063393">
            <h3>Using blue-green deployments</h3>
            <p>A blue-green deployment<a contenteditable="false" data-type="indexterm" data-primary="blue-green deployments" id="xi_bluegreendeployments711539"/> is a release strategy that involves maintaining two identical environments, typically referred to as “blue” and “green.” At any given time, only one of these environments (usually blue) is live, serving production traffic. </p>
            <p>When a new version of your application is ready, it is deployed to the inactive environment (green). After testing and verification in the green environment, traffic is switched over from the blue environment to the green environment, making the new version live. Where a rolling deployment makes updates over time and different traffic will experience different versions of the service, a blue-green typically features a hard cutover. A switch is flipped and traffic, or at least new traffic, is moved immediately from the old to the new. <a data-type="xref" href="#chapter_7_figure_1_1749354011054693">Figure 7-1</a> depicts blue and green environments before and after deploying an update. </p>
            <figure><div id="chapter_7_figure_1_1749354011054693" class="figure">
              <img src="assets/ansd_0701.png" width="600" height="221"/>
              <h6><span class="label">Figure 7-1. </span>Blue-green deployments involve running two identical environments for seamless updates and rollback options (in the print book, blue appears in dark gray and green appears in light gray)</h6>
            </div></figure>
            <p>The previous live environment (now blue) can be used for the next deployment, kept as a backup in case a rollback is needed, or decommissioned. </p>
            <p>A blue-green strategy offers distinct advantages:</p>
            <dl>
              <dt>Reduced downtime</dt>
              <dd>
                <p>Traffic is switched between environments, minimizing any disruption to users and reducing downtime.</p>
              </dd>
              <dt>Easy rollbacks</dt>
              <dd>
                <p>If there are issues with the new deployment, traffic can be quickly switched back to the previous version. </p>
              </dd>
              <dt>Improved testing</dt>
              <dd>
                <p>The new version can be tested in a production-like environment before going live. </p>
              </dd>
            </dl>
            <p>The main disadvantage lies in the increased infrastructure cost, as maintaining two separate, identical environments can be expensive. Additionally, blue-green deployments might not be suitable for applications with complex state management or database schema changes, because synchronizing data between environments can be challenging. </p>
            <p>A more advanced blue-green model can overcome most of the infrastructure cost challenge by integrating IaCM<a contenteditable="false" data-type="indexterm" data-primary="IaCM (Infrastructure as Code Management) tools" id="id946"/><a contenteditable="false" data-type="indexterm" data-primary="Infrastructure as Code Management (IaCM) tools" id="id947"/> practices. During steady-state production, only one instance is in existence. At the start of the deployment, the deployment triggers an IaCM tool to provision a new instance, so both blue and green exist. At the conclusion of the process, the excess instance is de-provisioned. As a result, the excess infrastructure only needs to exist for the duration of the blue-green deployment<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_bluegreendeployments711539" id="id948"/>.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Using canary releases"><div class="sect3" id="chapter_7_using_canary_releases_1749354011063439">
            <h3>Using canary releases</h3>
            <p>Canary releases<a contenteditable="false" data-type="indexterm" data-primary="canary deployments" id="id949"/> offer another progressive strategy similar to rolling updates. A new version of the application is rolled out to a small subset of users or servers. This “canary” group acts as a test bed, allowing you to monitor the new version’s performance and stability in a real-world production environment before making it available to all users.</p>
            <p>In a typical canary deployment, only a small portion of traffic (e.g., 5% to 10%) might be directed to the newly deployed version. The performance, stability, and error rates of the new version are closely monitored and compared with those of the existing version. Metrics like response times, CPU usage, and error logs are analyzed to identify any potential issues. If the new version performs well in the canary environment, the percentage of traffic directed to it is gradually increased, allowing more users to access it. This process continues until the new version completely replaces the old one. If any issues or performance degradation are detected during the canary phase, the deployment can be rolled back quickly, minimizing the impact on users.</p>
            <p>Canary deployments may be implemented with simple metric thresholds, but they increasingly leverage AI or ML capabilities to determine whether the new version performs satisfactorily. Traditionally, canary deployments have focused on performance benchmarks, but we can expect that in the future they will increasingly also tap into business metrics, stopping the rollout if the new version of the application is harming the business, even if it is not crashing. </p>
            <p>While both canary deployments and rolling updates<a contenteditable="false" data-type="indexterm" data-primary="rolling deployments" id="id950"/> aim for gradual and controlled software releases, they differ in their focus. Rolling updates solve for minimizing downtime and service disruption across a service infrastructure. Canary deployments focus on metric-guided decision making about whether to gradually increase traffic to a new release or roll back to the previous version.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Using feature flags"><div class="sect3" id="chapter_7_using_feature_flags_1749354011063485">
            <h3>Using feature flags</h3>
            <p>Feature flags<a contenteditable="false" data-type="indexterm" data-primary="feature flags" data-secondary="deployment using" id="id951"/> provide a strategy for deploying<em> features</em> in a progressive manner. Think of feature flags like hidden switches within your code, allowing you to turn features on or off for specific users or groups without deploying new code. This gives you granular control over who sees what, enabling A/B testing and targeted rollouts. Feature flags are similar to other progressive deployment strategies in that they allow you to monitor performance and gather feedback in a real-world environment and use this information to mitigate risks. However, feature flags operate at different levels; they control functionality within a single version. Other progressive strategies test an entirely new version. </p>
            <p>Feature flags offer benefits beyond deployment risk mitigation, and we’ll return to them in <a data-type="xref" href="ch08.html#chapter_8_feature_management_and_experimentation_1749354011197288">Chapter 8</a>.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Rolling back"><div class="sect3" id="chapter_7_rolling_back_1749354011063529">
            <h3>Rolling back</h3>
            <p>We’ve explored a few progressive deployment strategies<a contenteditable="false" data-type="indexterm" data-primary="rollbacks" data-secondary="production deployment" id="id952"/>, but your options are innumerable. Variations and hybrid approaches that blend elements of rolling updates, blue-green deployments, canary releases, and feature flags are all possibilities. The common thread among these strategies is a controlled rollout, allowing you to stop a deployment and roll back to a previous version if you need to. With a strategy like blue-green deployments<a contenteditable="false" data-type="indexterm" data-primary="blue-green deployments" id="id953"/>, this is an easy proposition: your previous version stands at the ready. With a rolling update or a canary deployment<a contenteditable="false" data-type="indexterm" data-primary="canary deployments" id="id954"/>, the rollback process is a matter of removing traffic from nodes with the defective software and then systematically replacing those nodes with the previous software version.</p>
            <p>Rolling back involves not only redeploying the previous stable version of software, but also its associated configurations, dependencies, and data. Rolling back to a previous state can be as complex or more complex than the deployment itself. Certain deployment approaches will facilitate dependable rollbacks. For example, if the deployment is idempotent, meaning it can be repeated and achieve the same, nondestructive results, a redeploy of a prior version will be equivalent to a rollback. </p>
            <p>Testing<a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="of rollbacks" data-secondary-sortas="rollbacks" id="id955"/> rollbacks is crucial to ensuring you can roll back without fear. It’s not enough to simply have a rollback mechanism in place; you need to regularly validate its readiness. This involves simulating various failure scenarios and then executing the rollback procedure to ensure it swiftly and reliably restores the previous stable version. Thorough rollback testing verifies that the application, its data, and its dependencies are correctly reverted. Depending on the application and its data storage mechanisms, rollbacks may require data restoration or migration to ensure data consistency. Regularly test procedures to ensure they work as expected in all scenarios.</p>
            <p>With complete confidence in your rollback procedures, you can then configure rollbacks to trigger automatically based on deployment health. Verifying deployment health is a topic we’ll get into in the next section. Instead of relying on manual intervention, the system automatically reverts to the previous stable version when predefined thresholds are breached. This not only reduces the burden on your operations team but also takes human error out of the equation to minimize downtime.</p>
          </div></section>
          <section data-type="sect3" data-pdf-bookmark="Special considerations for specific architectures"><div class="sect3" id="chapter_7_special_considerations_for_specific_architectures_1749354011063583">
            <h3>Special considerations for specific architectures</h3>
            <p>Deployment and rollback complexities vary significantly depending on the software architecture. Monoliths<a contenteditable="false" data-type="indexterm" data-primary="monolithic software, deployment and rollback complexities" id="id956"/><a contenteditable="false" data-type="indexterm" data-primary="microservices architectures" id="id957"/>, with their tightly coupled codebase, often require complete deployments and rollbacks that impact the entire system. Microservices, on the other hand, offer more granular deployments and rollbacks, targeting individual services. However, this interconnectedness means that dependencies must be carefully managed to ensure consistency across services. Distributed monoliths share characteristics of both monolithic architecture and microservices and combine the deployment complexities of microservices with the interdependency issues of monoliths. </p>
            <p>Databases<a contenteditable="false" data-type="indexterm" data-primary="databases" data-secondary="“expand and contract” deployment strategy" data-secondary-sortas="expand and contract deployment strategy" id="id958"/> add another layer of complexity. When updates involve breaking changes to the structure of persistent data, strategies like “expand and contract” are needed. This strategy involves adding new database fields or tables alongside the existing ones, deploying the updated application to utilize the new structure, and eventually phasing out the old fields. The approach is complex to implement, but it is often required to ensure data integrity when supporting progressive deployment strategies and clean rollbacks<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_progressivedeliverystrategy710434" id="id959"/><a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_deliveryprocessprogressivedeliverystrategy710434" id="id960"/>.</p>
          </div></section>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Choosing the Right Tools"><div class="sect2" id="chapter_7_choosing_the_right_tools_1749354011063626">
          <h2>Choosing the Right Tools</h2>
          <p>Armed with a progressive deployment strategy and robust rollback capabilities, you can deploy to production with confidence. But to truly unlock the power of these strategies, you need the right tools at your disposal. Modern deployment tools make all the difference, offering seamless support for progressive deployment strategies out of the box. </p>
          <p>When selecting a tool to orchestrate your software deployments, it’s essential to look beyond the basics and consider how well a given tool aligns with your specific needs. If you’re planning a transition to automated deployment decisions alongside adopting new continuous delivery tools, understanding all the factors that go into your promotion decisions up front will help you choose the right tool with the required governance and gate capabilities. In addition, ensure that the deployment tool seamlessly integrates with your target environments, whether it’s the cloud, on-premise servers, or a hybrid setup. Equally important is the tool’s ability to handle your specific application types and architectures, including any complex database deployments or coordinated multiservice releases.</p>
          <p>Beyond infrastructure and architecture compatibility, the deployment tool should include out-of-the-box support for your preferred progressive deployment strategies, ensuring you can easily implement canary releases, rolling updates, or other <span class="keep-together">techniques</span>. Robust rollback mechanisms should be a first-class concern, because they allow you to quickly revert to a previous stable version in case of unexpected issues. Furthermore, consider whether the tool integrates with your existing feature flag management system or offers its own feature flagging capabilities, giving you granular control over feature releases<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_productiondeploymentstrategiesfor796215" id="id961"/>.</p>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Verifying Production Deployments"><div class="sect1" id="chapter_7_verifying_production_deployments_1749354011063673">
        <h1>Verifying Production Deployments</h1>
        <p>Even the most diligent governance doesn’t eliminate the need for robust practices to systematically verify<a contenteditable="false" data-type="indexterm" data-primary="production deployment" data-secondary="verification of" id="xi_productiondeploymentverificationof7174118"/> your production deployments. In this section we’ll look at the role of observability. We’ll discuss modernizing your verification processes and look at testing strategies specific to production deployment verifications. </p>
        <section data-type="sect2" data-pdf-bookmark="Observability in Deployments"><div class="sect2" id="chapter_7_observability_in_deployments_1749354011063718">
          <h2>Observability in Deployments</h2>
          <p>Verifying your deployment starts with observability<a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="verifying production deployments" id="id962"/>. <a href="https://oreil.ly/94AtC">Observability</a> simply refers to the ability to understand a system’s internal state by examining its external outputs. Observability gets you from knowing that something is wrong to understanding why it’s wrong, which enables faster troubleshooting and more effective root cause analysis. Observability data encompasses three key pillars: </p>
          <dl>
            <dt>Metrics</dt>
            <dd>
              <p>These provide quantitative measurements<a contenteditable="false" data-type="indexterm" data-primary="metrics" data-secondary="and observability" data-secondary-sortas="observability" id="id963"/><a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="and metrics" data-secondary-sortas="metrics" id="id964"/> of system performance, such as response times, error rates, and resource utilization. By tracking trends and anomalies in these metrics, teams can quickly identify potential issues and assess the impact of a new deployment. </p>
            </dd>
            <dt>Logs</dt>
            <dd>
              <p>Logs<a contenteditable="false" data-type="indexterm" data-primary="logs and logging" data-secondary="as observability pillar" data-secondary-sortas="observability pillar" id="id965"/> offer detailed records of events and errors occurring within the application and its infrastructure. Analyzing log data helps pinpoint the root cause of problems and understand the sequence of events leading to an issue. </p>
            </dd>
            <dt>Traces</dt>
            <dd>
              <p>Traces<a contenteditable="false" data-type="indexterm" data-primary="traces, observability pillar" id="id966"/> provide a visual representation of how requests flow through the system, highlighting bottlenecks, latency issues, and dependencies between different services. This helps identify performance issues and optimize application <span class="keep-together">architecture.</span> </p>
            </dd>
          </dl>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Modernizing the War Room"><div class="sect2" id="chapter_7_modernizing_the_war_room_1749354011063763">
          <h2>Modernizing the War Room</h2>
          <p>Traditional deployment verification often resembles a high-stakes war room scenario with engineers monitoring dashboards and logs, ready to manually intervene at the first sign of trouble. The process is highly manual, relying on human interpretation of observability data. It is also reactive, with teams often only scrambling to address issues after they have impacted users. </p>
          <p>This approach is not only stressful and inefficient but also prone to misses and slow response times. Moreover, it often leads to inconsistent verification procedures and limited visibility into the root cause of issues.</p>
          <p>Modernizing deployment verification involves automating<a contenteditable="false" data-type="indexterm" data-primary="automation" data-secondary="production deployment" id="id967"/><a contenteditable="false" data-type="indexterm" data-primary="deployment" data-secondary="automation in" id="id968"/> these manual tasks and human decisions. Instead of relying on engineers to monitor dashboards and logs, automated systems take over, continuously analyzing telemetry data and triggering alerts when anomalies are detected. The shift from reactive to proactive monitoring reduces the need for human intervention and accelerates response times. </p>
          <p>The trick to achieve this automation is to integrate your deployment tools with your observability<a contenteditable="false" data-type="indexterm" data-primary="observability" data-secondary="integration of deployment tools with" id="id969"/> platforms. The integration can take different forms depending on the tools used. In one approach, your CI/CD tool<a contenteditable="false" data-type="indexterm" data-primary="CI/CD pipeline" data-secondary="observability/deployment integration" id="id970"/> notifies the observability platform when a deployment is in progress, providing a “hook” that can be used to trigger a rollback<a contenteditable="false" data-type="indexterm" data-primary="rollbacks" data-secondary="observability/deployment integration for" id="id971"/>. The observability platform then analyzes telemetry data and decides whether to initiate a rollback, calling the hook provided by the CD tool. </p>
          <p>Alternatively, CD tools like Harness<a contenteditable="false" data-type="indexterm" data-primary="Harness" id="id972"/> can be configured to watch one or more observability tools for signs of trouble during the deployment process. If issues are detected, the CD tool can automatically trigger its own rollback mechanism, halting the deployment and reverting to a previous stable version. This tight integration between deployment and observability tools enables a seamless and automated verification process, minimizing downtime and ensuring faster feedback loops.</p>
          <p>In either case, the industry no longer tolerates outages and seeks to detect indicators that trouble is brewing before an application fails. As a result, AI/ML<a contenteditable="false" data-type="indexterm" data-primary="AI (artificial intelligence) systems" data-secondary="anomaly detection" id="id973"/> is used to analyze multiple data sources to identify anomalies that indicate a likelihood of failure. AI anomaly detection has become a central component in modern deployment verification. Unlike traditional monitoring, which relies on predefined thresholds, these systems build statistical models of normal application behavior across hundreds of metrics and can detect complex, multidimensional anomalies that would be impossible to define with static rules. This capability is particularly valuable during the critical minutes following a production deployment, when subtle performance issues might otherwise go unnoticed until they escalate into user-impacting incidents.</p>
          <p>Deployment verification systems integrate these AI capabilities into automated verification gates, providing continuous assessment throughout the deployment process rather than point-in-time checks. When anomalies are detected, these systems can automatically pause progressive rollouts, or even automatically trigger the rollback process.</p>
        </div></section>
        <section data-type="sect2" data-pdf-bookmark="Testing Production Deployments"><div class="sect2" id="chapter_7_testing_production_deployments_1749354011063807">
          <h2>Testing Production Deployments</h2>
          <p>We discussed testing<a contenteditable="false" data-type="indexterm" data-primary="testing" data-secondary="production deployment" id="id974"/> at length in <a data-type="xref" href="ch04.html#chapter_4_deploying_to_test_environments_1749354010445896">Chapter 4</a>. We return now to look at test strategies particularly suited to verifying in production. Verifying production deployments requires a layered testing approach. </p>
          <p>Synthetic testing<a contenteditable="false" data-type="indexterm" data-primary="synthetic testing, production deployments" id="id975"/> can be paired with phased or progressive deployments. By simulating typical user interactions and transactions in a production environment, synthetic tests run through scenarios to catch issues quickly. This allows teams to address problems early on, either by rolling back the deployment or by implementing necessary fixes. </p>
          <p>Beyond the initial deployment phase, ongoing testing in production is essential for ensuring long-term stability and performance. Synthetic testing continues to play a valuable role, providing continuous monitoring of critical user journeys and identifying any regressions or performance degradations. Chaos engineering<a contenteditable="false" data-type="indexterm" data-primary="chaos engineering" data-secondary="resiliency testing for production deployments" id="id976"/><a contenteditable="false" data-type="indexterm" data-primary="resiliency" data-secondary="testing" id="id977"/>, which we covered in <a data-type="xref" href="ch06.html#chapter_6_chaos_engineering_and_service_reliability_1749354010916149">Chapter 6</a>, takes this a step further by deliberately injecting failures into the system to test its resilience and ability to recover. </p>
          <p>Another important aspect of ongoing testing is progressive feature disclosure<a contenteditable="false" data-type="indexterm" data-primary="progressive feature disclosure" id="id978"/>. This involves gradually rolling out new features to a subset of users, allowing teams to gather feedback and monitor performance before a full release. Techniques like A/B testing enable comparisons between different versions of a feature, helping identify the most effective implementation. This controlled approach to feature releases minimizes risk and allows for data-driven decisions based on real user behavior. By combining synthetic testing, chaos engineering, and progressive feature disclosure, organizations can establish a comprehensive testing strategy that ensures continuous verification and improvement of their production deployments<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_productiondeploymentverificationof7174118" id="id979"/>.</p>
        </div></section>
      </div></section>
      <section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="chapter_7_summary_1749354011063850">
        <h1>Summary</h1>
        <p>As AI continues to transform production deployments, the connection between deployment strategies and feature management becomes increasingly important. AI-powered deployment verification systems don’t just monitor overall application health; they can now track the impact of individual features within a deployment, providing granular insights that inform both rollback decisions and future feature releases. These systems create a continuous feedback loop where deployment data feeds into feature flag decisions, and feature behavior informs deployment strategies. Modern platforms analyze feature performance patterns across deployments to recommend which features should be gradually released through feature flags versus those that can be safely deployed traditionally<a contenteditable="false" data-type="indexterm" data-primary="" data-startref="xi_productiondeployment73186" id="id980"/>. This intelligence helps teams balance development velocity with operational stability, creating a more sophisticated approach to managing both deployments and features in production environments. In <a data-type="xref" href="ch08.html#chapter_8_feature_management_and_experimentation_1749354011197288">Chapter 8</a>, we will focus in depth on feature management.</p>
      </div></section>
    </div></section></div></div></body></html>