["```py\nfrom torchvision.datasets import CIFAR10\n\ntrain_data = CIFAR10(root=\"./train/\",\n                     train=True,\n                     download=True)\n```", "```py\n\nprint(train_data)![1](Images/1.png)# out:# Dataset CIFAR10#     Number of datapoints: 50000#     Root location: ./train/#     Split: Trainprint(len(train_data))![2](Images/2.png)# out: 50000print(train_data.data.shape)# ndarray ![3](Images/3.png)# out: (50000, 32, 32, 3)print(train_data.targets)# list ![4](Images/4.png)# out: [6, 9, ...,  1, 1]print(train_data.classes)![5](Images/5.png)# out: ['airplane', 'automobile', 'bird',#       'cat', 'deer', 'dog', 'frog',#       'horse', 'ship', 'truck']print(train_data.class_to_idx)![6](Images/6.png)# out:# {'airplane': 0, 'automobile': 1, 'bird': 2,#  'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6,#  'horse': 7, 'ship': 8, 'truck': 9}\n```", "```py\nprint(type(train_data[0]))\n# out: <class 'tuple'>\n\nprint(len(train_data[0]))\n# out: 2\n\ndata, label = train_data[0]\n```", "```py\nprint(type(data))\n# out: <class 'PIL.Image.Image'>\n\nprint(data)\n# out:\n# <PIL.Image.Image image mode=RGB\n#       size=32x32 at 0x7FA61-D6F1748>\n```", "```py\nprint(type(label))\n# out: <class 'int'>\n\nprint(label)\n# out: 6\n\nprint(train_data.classes[label])\n# out: frog\n```", "```py\ntest_data = CIFAR10(root=\"./test/\",\n                    train=False,\n                    download=True)\n\nprint(test_data)\n# out:\n# Dataset CIFAR10\n#     Number of datapoints: 10000\n#     Root location: ./test/\n#     Split: Test\n\nprint(len(test_data))\n# out: 10000\n\nprint(test_data.data.shape) # ndarray\n# out: (10000, 32, 32, 3)\n```", "```py\nfromtorchvisionimporttransformstrain_transforms=transforms.Compose([transforms.RandomCrop(32,padding=4),transforms.RandomHorizontalFlip(),transforms.ToTensor(),transforms.Normalize(mean=(0.4914,0.4822,0.4465),![1](Images/1.png)std=(0.2023,0.1994,0.2010))])train_data=CIFAR10(root=\"./train/\",train=True,download=True,transform=train_transforms)![2](Images/2.png)\n```", "```py\nprint(train_data)\n# out:\n# Dataset CIFAR10\n#     Number of datapoints: 50000\n#     Root location: ./train/\n#     Split: Train\n#     StandardTransform\n# Transform: Compose(\n#                RandomCrop(size=(32, 32),\n#                  padding=4)\n#                RandomHorizontalFlip(p=0.5)\n#                ToTensor()\n#                Normalize(\n#                  mean=(0.4914, 0.4822, 0.4465),\n#                  std=(0.2023, 0.1994, 0.201))\n#            )\n\nprint(train_data.transforms)\n# out:\n# StandardTransform\n# Transform: Compose(\n#                RandomCrop(size=(32, 32),\n#                  padding=4)\n#                RandomHorizontalFlip(p=0.5)\n#                ToTensor()\n#                Normalize(\n#                  mean=(0.4914, 0.4822, 0.4465),\n#                  std=(0.2023, 0.1994, 0.201))\n```", "```py\ndata, label = train_data[0]\n\nprint(type(data))\n# out: <class 'torch.Tensor'>\n\nprint(data.size())\n# out: torch.Size([3, 32, 32])\n\nprint(data)\n# out:\n# tensor([[[-0.1416,  ..., -2.4291],\n#          [-0.0060,  ..., -2.4291],\n#          [-0.7426,  ..., -2.4291],\n#          ...,\n#          [ 0.5100, ..., -2.2214],\n#          [-2.2214, ..., -2.2214],\n#          [-2.2214, ..., -2.2214]]])\n```", "```py\ntest_transforms = transforms.Compose([\n  transforms.ToTensor(),\n  transforms.Normalize(\n      (0.4914, 0.4822, 0.4465),\n      (0.2023, 0.1994, 0.2010))])\n\ntest_data = torchvision.datasets.CIFAR10(\n      root=\"./test/\",\n      train=False,\n      transform=test_transforms)\n\nprint(test_data)\n# out:\n# Dataset CIFAR10\n#     Number of datapoints: 10000\n#     Root location: ./test/\n#     Split: Test\n#     StandardTransform\n# Transform: Compose(\n#     ToTensor()\n#     Normalize(\n#       mean=(0.4914, 0.4822, 0.4465),\n#       std=(0.2023, 0.1994, 0.201)))\n```", "```py\ntrainloader = torch.utils.data.DataLoader(\n                    train_data,\n                    batch_size=16,\n                    shuffle=True)\n```", "```py\ndata_batch, labels_batch = next(iter(trainloader))\nprint(data_batch.size())\n# out: torch.Size([16, 3, 32, 32])\n\nprint(labels_batch.size())\n# out: torch.Size([16])\n```", "```py\ntestloader = torch.utils.data.DataLoader(\n                    test_data,\n                    batch_size=16,\n                    shuffle=False)\n```", "```py\ntorch.utils.data.DataLoader(\n                dataset,\n                batch_size=1,\n                shuffle=False,\n                sampler=None,\n                batch_sampler=None,\n                num_workers=0,\n                collate_fn=None,\n                pin_memory=False,\n                drop_last=False,\n                timeout=0,\n                worker_init_fn=None,\n                multiprocessing_context=None,\n                generator=None)\n```", "```py\nfrom torchvision import models\n\nvgg16 = models.vgg16(pretrained=True)\n```", "```py\nprint(vgg16.classifier)\n\n# out:\n# Sequential(\n#   (0): Linear(in_features=25088,\n#               out_features=4096, bias=True)\n#   (1): ReLU(inplace=True)\n#   (2): Dropout(p=0.5, inplace=False)\n#   (3): Linear(in_features=4096,\n#               out_features=4096, bias=True)\n#   (4): ReLU(inplace=True)\n#   (5): Dropout(p=0.5, inplace=False)\n#   (6): Linear(in_features=4096,\n#               out_features=1000, bias=True)\n# )\n```", "```py\nwaveglow = torch.hub.load(\n    'nvidia/DeepLearningExamples:torchhub',\n    'nvidia_waveglow')\n```", "```py\ntorch.hub.list(\n      'nvidia/DeepLearningExamples:torchhub')\n\n# out:\n# ['checkpoint_from_distributed',\n#  'nvidia_ncf',\n#  'nvidia_ssd',\n#  'nvidia_ssd_processing_utils',\n#  'nvidia_tacotron2',\n#  'nvidia_waveglow',\n#  'unwrap_distributed']\n```", "```py\nimporttorch.nnasnnimporttorch.nn.functionalasFclassSimpleNet(nn.Module):def__init__(self):![1](Images/1.png)super(SimpleNet,self).__init__()![2](Images/2.png)self.fc1=nn.Linear(2048,256)self.fc2=nn.Linear(256,64)self.fc3=nn.Linear(64,2)defforward(self,x):![3](Images/3.png)x=x.view(-1,2048)x=F.relu(self.fc1(x))x=F.relu(self.fc2(x))x=F.softmax(self.fc3(x),dim=1)returnx\n```", "```py\nsimplenet=SimpleNet()![1](Images/1.png)print(simplenet)# out:# SimpleNet(#   (fc1): Linear(in_features=2048,#                 out_features=256, bias=True)#   (fc2): Linear(in_features=256,#                 out_features=64, bias=True)#   (fc3): Linear(in_features=64,#                 out_features=2, bias=True)# )input=torch.rand(2048)output=simplenet(input)![2](Images/2.png)\n```", "```py\nfromtorchimportnnimporttorch.nn.functionalasFclassLeNet5(nn.Module):![1](Images/1.png)def__init__(self):super(LeNet5,self).__init__()self.conv1=nn.Conv2d(3,6,5)self.conv2=nn.Conv2d(6,16,5)self.fc1=nn.Linear(16*5*5,120)self.fc2=nn.Linear(120,84)self.fc3=nn.Linear(84,10)defforward(self,x):x=F.max_pool2d(F.relu(self.conv1(x)),(2,2))x=F.max_pool2d(F.relu(self.conv2(x)),2)x=x.view(-1,int(x.nelement()/x.shape[0]))x=F.relu(self.fc1(x))x=F.relu(self.fc2(x))x=self.fc3(x)returnxdevice=('cuda'iftorch.cuda.is_available()else'cpu')![2](Images/2.png)model=LeNet5().to(device=device)![3](Images/3.png)\n```", "```py\nfromtorchimportoptimfromtorchimportnncriterion=nn.CrossEntropyLoss()optimizer=optim.SGD(model.parameters(),![1](Images/1.png)lr=0.001,momentum=0.9)\n```", "```py\nN_EPOCHS=10forepochinrange(N_EPOCHS):![1](Images/1.png)epoch_loss=0.0forinputs,labelsintrainloader:inputs=inputs.to(device)![2](Images/2.png)labels=labels.to(device)optimizer.zero_grad()![3](Images/3.png)outputs=model(inputs)![4](Images/4.png)loss=criterion(outputs,labels)![5](Images/5.png)loss.backward()![6](Images/6.png)optimizer.step()![7](Images/7.png)epoch_loss+=loss.item()![8](Images/8.png)print(\"Epoch: {} Loss: {}\".format(epoch,epoch_loss/len(trainloader)))# out: (results will vary and make take minutes)# Epoch: 0 Loss: 1.8982970092773437# Epoch: 1 Loss: 1.6062103009033204# Epoch: 2 Loss: 1.484384165763855# Epoch: 3 Loss: 1.3944422281837463# Epoch: 4 Loss: 1.334191104450226# Epoch: 5 Loss: 1.2834235876464843# Epoch: 6 Loss: 1.2407222446250916# Epoch: 7 Loss: 1.2081411465930938# Epoch: 8 Loss: 1.1832368299865723# Epoch: 9 Loss: 1.1534993273162841\n```", "```py\nfrom torch.utils.data import random_split\n\ntrain_set, val_set = random_split(\n                      train_data,\n                      [40000, 10000])\n\ntrainloader = torch.utils.data.DataLoader(\n                    train_set,\n                    batch_size=16,\n                    shuffle=True)\n\nvalloader = torch.utils.data.DataLoader(\n                    val_set,\n                    batch_size=16,\n                    shuffle=True)\n\nprint(len(trainloader))\n# out: 2500\nprint(len(valloader))\n# out: 625\n```", "```py\nfrom torch import optim\nfrom torch import nn\n\nmodel = LeNet5().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(),\n                      lr=0.001,\n                      momentum=0.9)\n```", "```py\nN_EPOCHS=10forepochinrange(N_EPOCHS):# Trainingtrain_loss=0.0model.train()![1](Images/1.png)forinputs,labelsintrainloader:inputs=inputs.to(device)labels=labels.to(device)optimizer.zero_grad()outputs=model(inputs)loss=criterion(outputs,labels)loss.backward()optimizer.step()train_loss+=loss.item()# Validationval_loss=0.0model.eval()![2](Images/2.png)forinputs,labelsinvalloader:inputs=inputs.to(device)labels=labels.to(device)outputs=model(inputs)loss=criterion(outputs,labels)val_loss+=loss.item()print(\"Epoch: {} Train Loss: {} Val Loss: {}\".format(epoch,train_loss/len(trainloader),val_loss/len(valloader)))\n```", "```py\n# out: (results may vary and take a few minutes)\n# Epoch: 0 Train Loss: 1.987607608 Val Loss: 1.740786979\n# Epoch: 1 Train Loss: 1.649753892 Val Loss: 1.587019552\n# Epoch: 2 Train Loss: 1.511723689 Val Loss: 1.435539366\n# Epoch: 3 Train Loss: 1.408525426 Val Loss: 1.361453659\n# Epoch: 4 Train Loss: 1.339505518 Val Loss: 1.293459154\n# Epoch: 5 Train Loss: 1.290560259 Val Loss: 1.245048282\n# Epoch: 6 Train Loss: 1.259268565 Val Loss: 1.285989610\n# Epoch: 7 Train Loss: 1.235161985 Val Loss: 1.253840940\n# Epoch: 8 Train Loss: 1.207051850 Val Loss: 1.215700019\n# Epoch: 9 Train Loss: 1.189215132 Val Loss: 1.183332257\n```", "```py\nnum_correct=0.0forx_test_batch,y_test_batchintestloader:model.eval()![1](Images/1.png)y_test_batch=y_test_batch.to(device)x_test_batch=x_test_batch.to(device)y_pred_batch=model(x_test_batch)![2](Images/2.png)_,predicted=torch.max(y_pred_batch,1)![3](Images/3.png)num_correct+=(predicted==y_test_batch).float().sum()![4](Images/4.png)accuracy=num_correct/(len(testloader)\\\n*testloader.batch_size)![5](Images/5.png)print(len(testloader),testloader.batch_size)# out: 625 16print(\"Test Accuracy: {}\".format(accuracy))# out: Test Accuracy: 0.6322000026702881\n```", "```py\ntorch.save(model.state_dict(), \"./lenet5_model.pt\")\n\nmodel = LeNet5().to(device)\nmodel.load_state_dict(torch.load(\"./lenet5_model.pt\"))\n```", "```py\nimport torch\nvgg16 = torch.hub.load('pytorch/vision',\n  'vgg16', pretrained=True)\n```", "```py\ndependencies = ['torch']\nfrom torchvision.models.vgg import vgg16\n```", "```py\ndependencies = ['torch']\nfrom torchvision.models.vgg import vgg16 as _vgg16\n\n# vgg16 is the name of the entrypoint\ndef vgg16(pretrained=False, **kwargs):\n    \"\"\" # This docstring shows up in hub.help():\n VGG16 model\n pretrained (bool): kwargs,\n load pretrained weights into the model\n \"\"\"\n    # Call the model; load pretrained weights\n    model = _vgg16(pretrained=pretrained, **kwargs)\n    return model\n```"]