["```py\nfrom fastai.text.all import *\npath = untar_data(URLs.IMDB)\n```", "```py\nfiles = get_text_files(path, folders = ['train', 'test', 'unsup'])\n```", "```py\ntxt = files[0].open().read(); txt[:75]\n```", "```py\n'This movie, which I just discovered at the video store, has apparently sit '\n```", "```py\nspacy = WordTokenizer()\ntoks = first(spacy([txt]))\nprint(coll_repr(toks, 30))\n```", "```py\n(#201) ['This','movie',',','which','I','just','discovered','at','the','video','s\n > tore',',','has','apparently','sit','around','for','a','couple','of','years','\n > without','a','distributor','.','It',\"'s\",'easy','to','see'...]\n```", "```py\nfirst(spacy(['The U.S. dollar $1 is $1.00.']))\n```", "```py\n(#9) ['The','U.S.','dollar','$','1','is','$','1.00','.']\n```", "```py\ntkn = Tokenizer(spacy)\nprint(coll_repr(tkn(txt), 31))\n```", "```py\n(#228) ['xxbos','xxmaj','this','movie',',','which','i','just','discovered','at',\n > 'the','video','store',',','has','apparently','sit','around','for','a','couple\n > ','of','years','without','a','distributor','.','xxmaj','it',\"'s\",'easy'...]\n```", "```py\ndefaults.text_proc_rules\n```", "```py\n[<function fastai.text.core.fix_html(x)>,\n <function fastai.text.core.replace_rep(t)>,\n <function fastai.text.core.replace_wrep(t)>,\n <function fastai.text.core.spec_add_spaces(t)>,\n <function fastai.text.core.rm_useless_spaces(t)>,\n <function fastai.text.core.replace_all_caps(t)>,\n <function fastai.text.core.replace_maj(t)>,\n <function fastai.text.core.lowercase(t, add_bos=True, add_eos=False)>]\n```", "```py\n??replace_rep\n```", "```py\ncoll_repr(tkn('&copy;   Fast.ai www.fast.ai/INDEX'), 31)\n```", "```py\n\"(#11) ['xxbos','\u00a9','xxmaj','fast.ai','xxrep','3','w','.fast.ai','/','xxup','ind\n > ex'...]\"\n```", "```py\ntxts = L(o.open().read() for o in files[:2000])\n```", "```py\ndef subword(sz):\n    sp = SubwordTokenizer(vocab_sz=sz)\n    sp.setup(txts)\n    return ' '.join(first(sp([txt]))[:40])\n```", "```py\nsubword(1000)\n```", "```py\n'\u2581This \u2581movie , \u2581which \u2581I \u2581just \u2581dis c over ed \u2581at \u2581the \u2581video \u2581st or e , \u2581has\n > \u2581a p par ent ly \u2581s it \u2581around \u2581for \u2581a \u2581couple \u2581of \u2581years \u2581without \u2581a \u2581dis t\n > ri but or . \u2581It'\n```", "```py\nsubword(200)\n```", "```py\n'\u2581 T h i s \u2581movie , \u2581w h i ch \u2581I \u2581 j us t \u2581 d i s c o ver ed \u2581a t \u2581the \u2581 v id e\n > o \u2581 st or e , \u2581h a s'\n```", "```py\nsubword(10000)\n```", "```py\n\"\u2581This \u2581movie , \u2581which \u2581I \u2581just \u2581discover ed \u2581at \u2581the \u2581video \u2581store , \u2581has\n > \u2581apparently \u2581sit \u2581around \u2581for \u2581a \u2581couple \u2581of \u2581years \u2581without \u2581a \u2581distributor\n > . \u2581It ' s \u2581easy \u2581to \u2581see \u2581why . \u2581The \u2581story \u2581of \u2581two \u2581friends \u2581living\"\n```", "```py\ntoks = tkn(txt)\nprint(coll_repr(tkn(txt), 31))\n```", "```py\n(#228) ['xxbos','xxmaj','this','movie',',','which','i','just','discovered','at',\n > 'the','video','store',',','has','apparently','sit','around','for','a','couple\n > ','of','years','without','a','distributor','.','xxmaj','it',\"'s\",'easy'...]\n```", "```py\ntoks200 = txts[:200].map(tkn)\ntoks200[0]\n```", "```py\n(#228)\n > ['xxbos','xxmaj','this','movie',',','which','i','just','discovered','at'...]\n```", "```py\nnum = Numericalize()\nnum.setup(toks200)\ncoll_repr(num.vocab,20)\n```", "```py\n\"(#2000) ['xxunk','xxpad','xxbos','xxeos','xxfld','xxrep','xxwrep','xxup','xxmaj\n > ','the','.',',','a','and','of','to','is','in','i','it'...]\"\n```", "```py\nnums = num(toks)[:20]; nums\n```", "```py\ntensor([  2,   8,  21,  28,  11,  90,  18,  59,   0,  45,   9, 351, 499,  11,\n > 72, 533, 584, 146,  29,  12])\n```", "```py\n' '.join(num.vocab[o] for o in nums)\n```", "```py\n'xxbos xxmaj this movie , which i just xxunk at the video store , has apparently\n > sit around for a'\n```", "```py\nnums200 = toks200.map(num)\n```", "```py\ndl = LMDataLoader(nums200)\n```", "```py\nx,y = first(dl)\nx.shape,y.shape\n```", "```py\n(torch.Size([64, 72]), torch.Size([64, 72]))\n```", "```py\n' '.join(num.vocab[o] for o in x[0][:20])\n```", "```py\n'xxbos xxmaj this movie , which i just xxunk at the video store , has apparently\n > sit around for a'\n```", "```py\n' '.join(num.vocab[o] for o in y[0][:20])\n```", "```py\n'xxmaj this movie , which i just xxunk at the video store , has apparently sit\n > around for a couple'\n```", "```py\nget_imdb = partial(get_text_files, folders=['train', 'test', 'unsup'])\n\ndls_lm = DataBlock(\n    blocks=TextBlock.from_folder(path, is_lm=True),\n    get_items=get_imdb, splitter=RandomSplitter(0.1)\n).dataloaders(path, path=path, bs=128, seq_len=80)\n```", "```py\ndls_lm.show_batch(max_n=2)\n```", "```py\nlearn = language_model_learner(\n    dls_lm, AWD_LSTM, drop_mult=0.3,\n    metrics=[accuracy, Perplexity()]).to_fp16()\n```", "```py\nlearn.fit_one_cycle(1, 2e-2)\n```", "```py\nlearn.save('1epoch')\n```", "```py\nlearn = learn.load('1epoch')\n```", "```py\nlearn.unfreeze()\nlearn.fit_one_cycle(10, 2e-3)\n```", "```py\nlearn.save_encoder('finetuned')\n```", "```py\nTEXT = \"I liked this movie because\"\nN_WORDS = 40\nN_SENTENCES = 2\npreds = [learn.predict(TEXT, N_WORDS, temperature=0.75)\n         for _ in range(N_SENTENCES)]\n```", "```py\nprint(\"\\n\".join(preds))\n```", "```py\ni liked this movie because of its story and characters . The story line was very\n > strong , very good for a sci - fi film . The main character , Alucard , was\n > very well developed and brought the whole story\ni liked this movie because i like the idea of the premise of the movie , the (\n > very ) convenient virus ( which , when you have to kill a few people , the \"\n > evil \" machine has to be used to protect\n```", "```py\ndls_clas = DataBlock(\n    blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock),\n    get_y = parent_label,\n    get_items=partial(get_text_files, folders=['train', 'test']),\n    splitter=GrandparentSplitter(valid_name='test')\n).dataloaders(path, path=path, bs=128, seq_len=72)\n```", "```py\ndls_clas.show_batch(max_n=3)\n```", "```py\nnums_samp = toks200[:10].map(num)\n```", "```py\nnums_samp.map(len)\n```", "```py\n(#10) [228,238,121,290,196,194,533,124,581,155]\n```", "```py\nlearn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5,\n                                metrics=accuracy).to_fp16()\n```", "```py\nlearn = learn.load_encoder('finetuned')\n```", "```py\nlearn.fit_one_cycle(1, 2e-2)\n```", "```py\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2))\n```", "```py\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3))\n```", "```py\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3))\n```"]