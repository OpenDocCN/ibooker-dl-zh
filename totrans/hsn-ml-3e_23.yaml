- en: Appendix B. Autodiff
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This appendix explains how TensorFlow’s autodifferentiation (autodiff) feature
    works, and how it compares to other solutions.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you define a function *f*(*x*, *y*) = *x*²*y* + *y* + 2, and you need
    its partial derivatives ∂*f*/∂*x* and ∂*f*/∂*y*, typically to perform gradient
    descent (or some other optimization algorithm). Your main options are manual differentiation,
    finite difference approximation, forward-mode autodiff, and reverse-mode autodiff.
    TensorFlow implements reverse-mode autodiff, but to understand it, it’s useful
    to look at the other options first. So let’s go through each of them, starting
    with manual differentiation.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Manual Differentiation
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first approach to compute derivatives is to pick up a pencil and a piece
    of paper and use your calculus knowledge to derive the appropriate equation. For
    the function *f*(*x*, *y*) just defined, it is not too hard; you just need to
    use five rules:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: The derivative of a constant is 0.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The derivative of *λx* is *λ* (where *λ* is a constant).
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The derivative of *x*^λ is *λx*^(*λ*) ^– ¹, so the derivative of *x*² is 2*x*.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The derivative of a sum of functions is the sum of these functions’ derivatives.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The derivative of *λ* times a function is *λ* times its derivative.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From these rules, you can derive [Equation B-1](#partial_derivatives_equations).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Equation B-1\. Partial derivatives of *f*(*x*, *y*)
  id: totrans-11
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mstyle
    scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mo>(</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mi>y</mi><mo>)</mo></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle>
    <mo>+</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>y</mi></mrow>
    <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle> <mo>+</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mi>∂</mi><mn>2</mn></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle>
    <mo>=</mo> <mi>y</mi> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mo>(</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle>
    <mo>+</mo> <mn>0</mn> <mo>+</mo> <mn>0</mn> <mo>=</mo> <mn>2</mn> <mi>x</mi> <mi>y</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow>
    <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mo>(</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mi>y</mi><mo>)</mo></mrow> <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle>
    <mo>+</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>y</mi></mrow>
    <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle> <mo>+</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mi>∂</mi><mn>2</mn></mrow> <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle>
    <mo>=</mo> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mn>1</mn> <mo>+</mo>
    <mn>0</mn> <mo>=</mo> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mn>1</mn></mrow></mtd></mtr></mtable></math>
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mstyle
    scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mo>(</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mi>y</mi><mo>)</mo></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle>
    <mo>+</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>y</mi></mrow>
    <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle> <mo>+</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mi>∂</mi><mn>2</mn></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle>
    <mo>=</mo> <mi>y</mi> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mo>(</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mo>)</mo></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle>
    <mo>+</mo> <mn>0</mn> <mo>+</mo> <mn>0</mn> <mo>=</mo> <mn>2</mn> <mi>x</mi> <mi>y</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow>
    <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mo>(</mo><msup><mi>x</mi>
    <mn>2</mn></msup> <mi>y</mi><mo>)</mo></mrow> <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle>
    <mo>+</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>y</mi></mrow>
    <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle> <mo>+</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mi>∂</mi><mn>2</mn></mrow> <mrow><mi>∂</mi><mi>y</mi></mrow></mfrac></mstyle>
    <mo>=</mo> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mn>1</mn> <mo>+</mo>
    <mn>0</mn> <mo>=</mo> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mn>1</mn></mrow></mtd></mtr></mtable></math>
- en: This approach can become very tedious for more complex functions, and you run
    the risk of making mistakes. Fortunately, there are other options. Let’s look
    at finite difference approximation now.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Finite Difference Approximation
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall that the derivative *h*′(*x*[0]) of a function *h*(*x*) at a point *x*[0]
    is the slope of the function at that point. More precisely, the derivative is
    defined as the limit of the slope of a straight line going through this point
    *x*[0] and another point *x* on the function, as *x* gets infinitely close to
    *x*[0] (see [Equation B-2](#derivative_definition)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Equation B-2\. Definition of the derivative of a function *h*(*x*) at point
    *x*[0]
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程B-2. 函数*h*(*x*)在点*x*[0]处的导数定义
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msup><mi>h</mi>
    <mo>'</mo></msup> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>0</mn></msub> <mo>)</mo></mrow></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <munder><mo movablelimits="true" form="prefix">lim</mo>
    <mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo>→</mo><msub><mi>x</mi>
    <mn>0</mn></msub></mrow></mstyle></munder> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>h</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>-</mo><mi>h</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>)</mo></mrow></mrow> <mrow><mi>x</mi><mo>-</mo><msub><mi>x</mi>
    <mn>0</mn></msub></mrow></mfrac></mstyle></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <munder><mo movablelimits="true" form="prefix">lim</mo> <mstyle scriptlevel="0"
    displaystyle="false"><mrow><mi>ε</mi><mo>→</mo><mn>0</mn></mrow></mstyle></munder>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>h</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>+</mo><mi>ε</mi><mo>)</mo></mrow><mo>-</mo><mi>h</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>)</mo></mrow></mrow> <mi>ε</mi></mfrac></mstyle></mrow></mtd></mtr></mtable></math>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msup><mi>h</mi>
    <mo>'</mo></msup> <mrow><mo>(</mo> <msub><mi>x</mi> <mn>0</mn></msub> <mo>)</mo></mrow></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <munder><mo movablelimits="true" form="prefix">lim</mo>
    <mstyle scriptlevel="0" displaystyle="false"><mrow><mi>x</mi><mo>→</mo><msub><mi>x</mi>
    <mn>0</mn></msub></mrow></mstyle></munder> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>h</mi><mrow><mo>(</mo><mi>x</mi><mo>)</mo></mrow><mo>-</mo><mi>h</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>)</mo></mrow></mrow> <mrow><mi>x</mi><mo>-</mo><msub><mi>x</mi>
    <mn>0</mn></msub></mrow></mfrac></mstyle></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <munder><mo movablelimits="true" form="prefix">lim</mo> <mstyle scriptlevel="0"
    displaystyle="false"><mrow><mi>ε</mi><mo>→</mo><mn>0</mn></mrow></mstyle></munder>
    <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>h</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>+</mo><mi>ε</mi><mo>)</mo></mrow><mo>-</mo><mi>h</mi><mrow><mo>(</mo><msub><mi>x</mi>
    <mn>0</mn></msub> <mo>)</mo></mrow></mrow> <mi>ε</mi></mfrac></mstyle></mrow></mtd></mtr></mtable></math>
- en: 'So, if we wanted to calculate the partial derivative of *f*(*x*, *y*) with
    regard to *x* at *x* = 3 and *y* = 4, we could compute *f*(3 + *ε*, 4) – *f*(3,
    4) and divide the result by *ε*, using a very small value for *ε*. This type of
    numerical approximation of the derivative is called a *finite difference approximation*,
    and this specific equation is called *Newton’s difference quotient*. That’s exactly
    what the following code does:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们想计算*f*(*x*, *y*)关于*x*在*x* = 3和*y* = 4处的偏导数，我们可以计算*f*(3 + *ε*, 4) - *f*(3,
    4)，然后将结果除以*ε*，使用一个非常小的*ε*值。这种数值逼近导数的方法称为*有限差分逼近*，这个特定的方程称为*牛顿的差商*。以下代码正是这样做的：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Unfortunately, the result is imprecise (and it gets worse for more complicated
    functions). The correct results are respectively 24 and 10, but instead we get:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，结果不够精确（对于更复杂的函数来说情况会更糟）。正确的结果分别是24和10，但实际上我们得到了：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Notice that to compute both partial derivatives, we have to call `f()` at least
    three times (we called it four times in the preceding code, but it could be optimized).
    If there were 1,000 parameters, we would need to call `f()` at least 1,001 times.
    When you are dealing with large neural networks, this makes finite difference
    approximation way too inefficient.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，要计算两个偏导数，我们至少要调用`f()`三次（在前面的代码中我们调用了四次，但可以进行优化）。如果有1,000个参数，我们至少需要调用`f()`
    1,001次。当处理大型神经网络时，这使得有限差分逼近方法过于低效。
- en: However, this method is so simple to implement that it is a great tool to check
    that the other methods are implemented correctly. For example, if it disagrees
    with your manually derived function, then your function probably contains a mistake.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法实现起来非常简单，是检查其他方法是否正确实现的好工具。例如，如果它与您手动推导的函数不一致，那么您的函数可能存在错误。
- en: 'So far, we have considered two ways to compute gradients: using manual differentiation
    and using finite difference approximation. Unfortunately, both are fatally flawed
    for training a large-scale neural network. So let’s turn to autodiff, starting
    with forward mode.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经考虑了两种计算梯度的方法：手动微分和有限差分逼近。不幸的是，这两种方法都对训练大规模神经网络有致命缺陷。因此，让我们转向自动微分，从正向模式开始。
- en: Forward-Mode Autodiff
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正向模式自动微分
- en: '[Figure B-1](#symbolic_differentiation_diagram) shows how forward-mode autodiff
    works on an even simpler function, *g*(*x*, *y*) = 5 + *xy*. The graph for that
    function is represented on the left. After forward-mode autodiff, we get the graph
    on the right, which represents the partial derivative ∂*g*/∂*x* = 0 + (0 × *x*
    + *y* × 1) = *y* (we could similarly obtain the partial derivative with regard
    to *y*).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[图B-1](#symbolic_differentiation_diagram)展示了正向模式自动微分在一个更简单的函数*g*(*x*, *y*)
    = 5 + *xy* 上的工作原理。该函数的图在左侧表示。经过正向模式自动微分后，我们得到右侧的图，表示偏导数∂*g*/∂*x* = 0 + (0 × *x*
    + *y* × 1) = *y*（我们可以类似地得到关于*y*的偏导数）。'
- en: The algorithm will go through the computation graph from the inputs to the outputs
    (hence the name “forward mode”). It starts by getting the partial derivatives
    of the leaf nodes. The constant node (5) returns the constant 0, since the derivative
    of a constant is always 0\. The variable *x* returns the constant 1 since ∂*x*/∂*x*
    = 1, and the variable *y* returns the constant 0 since ∂*y*/∂*x* = 0 (if we were
    looking for the partial derivative with regard to *y*, it would be the reverse).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法将从输入到输出遍历计算图（因此称为“正向模式”）。它从叶节点获取偏导数开始。常数节点（5）返回常数0，因为常数的导数始终为0。变量*x*返回常数1，因为∂*x*/∂*x*
    = 1，变量*y*返回常数0，因为∂*y*/∂*x* = 0（如果我们要找关于*y*的偏导数，结果将相反）。
- en: Now we have all we need to move up the graph to the multiplication node in function
    *g*. Calculus tells us that the derivative of the product of two functions *u*
    and *v* is ∂(*u* × *v*)/∂*x* = ∂*v*/∂*x* × *u* + *v* × ∂*u*/∂*x*. We can therefore
    construct a large part of the graph on the right, representing 0 × *x* + *y* ×
    1.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了所有需要的内容，可以向上移动到函数*g*中的乘法节点。微积分告诉我们，两个函数*u*和*v*的乘积的导数是∂(*u* × *v*)/∂*x*
    = ∂*v*/∂*x* × *u* + *v* × ∂*u*/∂*x*。因此，我们可以构建右侧的图的大部分，表示为0 × *x* + *y* × 1。
- en: 'Finally, we can go up to the addition node in function *g*. As mentioned, the
    derivative of a sum of functions is the sum of these functions’ derivatives, so
    we just need to create an addition node and connect it to the parts of the graph
    we have already computed. We get the correct partial derivative: ∂*g*/∂*x* = 0
    + (0 × *x* + *y* × 1).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以到达函数*g*中的加法节点。如前所述，函数和的导数是这些函数的导数之和，因此我们只需要创建一个加法节点并将其连接到我们已经计算过的图的部分。我们得到了正确的偏导数：∂*g*/∂*x*
    = 0 + (0 × *x* + *y* × 1)。
- en: '![mls3 ab01](assets/mls3_ab01.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 ab01](assets/mls3_ab01.png)'
- en: Figure B-1\. Forward-mode autodiff
  id: totrans-31
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图B-1. 正向模式自动微分
- en: 'However, this equation can be simplified (a lot). By applying a few pruning
    steps to the computation graph to get rid of all the unnecessary operations, we
    get a much smaller graph with just one node: ∂*g*/∂*x* = *y*. In this case simplification
    is fairly easy, but for a more complex function forward-mode autodiff can produce
    a huge graph that may be tough to simplify and lead to suboptimal performance.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个方程可以被简化（很多）。通过对计算图应用一些修剪步骤，摆脱所有不必要的操作，我们得到一个只有一个节点的更小的图：∂*g*/∂*x* = *y*。在这种情况下，简化相当容易，但对于更复杂的函数，正向模式自动微分可能会产生一个庞大的图，可能难以简化，并导致性能不佳。
- en: 'Note that we started with a computation graph, and forward-mode autodiff produced
    another computation graph. This is called *symbolic differentiation*, and it has
    two nice features: first, once the computation graph of the derivative has been
    produced, we can use it as many times as we want to compute the derivatives of
    the given function for any value of *x* and *y*; second, we can run forward-mode
    autodiff again on the resulting graph to get second-order derivatives if we ever
    need to (i.e., derivatives of derivatives). We could even compute third-order
    derivatives, and so on.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们从一个计算图开始，正向模式自动微分产生另一个计算图。这称为*符号微分*，它有两个好处：首先，一旦导数的计算图被生成，我们可以使用它任意次数来计算给定函数的导数，无论*x*和*y*的值是多少；其次，如果需要的话，我们可以再次在结果图上运行正向模式自动微分，以获得二阶导数（即导数的导数）。我们甚至可以计算三阶导数，依此类推。
- en: But it is also possible to run forward-mode autodiff without constructing a
    graph (i.e., numerically, not symbolically), just by computing intermediate results
    on the fly. One way to do this is to use *dual numbers*, which are weird but fascinating
    numbers of the form *a* + *bε*, where *a* and *b* are real numbers and *ε* is
    an infinitesimal number such that *ε*² = 0 (but *ε* ≠ 0). You can think of the
    dual number 42 + 24*ε* as something akin to 42.0000⋯000024 with an infinite number
    of 0s (but of course this is simplified just to give you some idea of what dual
    numbers are). A dual number is represented in memory as a pair of floats. For
    example, 42 + 24*ε* is represented by the pair (42.0, 24.0).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但也可以在不构建图形的情况下运行正向模式自动微分（即数值上，而不是符号上），只需在运行时计算中间结果。其中一种方法是使用*双数*，它们是形式为*a* +
    *bε*的奇怪但迷人的数字，其中*a*和*b*是实数，*ε*是一个无穷小数，使得*ε*² = 0（但*ε* ≠ 0）。您可以将双数42 + 24*ε*看作类似于42.0000⋯000024，其中有无限多个0（但当然这只是简化，只是为了让您对双数有一些概念）。双数在内存中表示为一对浮点数。例如，42
    + 24*ε*由一对(42.0, 24.0)表示。
- en: Dual numbers can be added, multiplied, and so on, as shown in [Equation B-3](#dual_numbers_operations).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 双数可以相加、相乘等，如[Equation B-3](#dual_numbers_operations)所示。
- en: Equation B-3\. A few operations with dual numbers
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Equation B-3. 双数的一些操作
- en: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mi>λ</mi>
    <mo>(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>ε</mi> <mo>)</mo> <mo>=</mo> <mi>λ</mi>
    <mi>a</mi> <mo>+</mo> <mi>λ</mi> <mi>b</mi> <mi>ε</mi></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>ε</mi>
    <mo>)</mo> <mo>+</mo> <mo>(</mo> <mi>c</mi> <mo>+</mo> <mi>d</mi> <mi>ε</mi> <mo>)</mo>
    <mo>=</mo> <mo>(</mo> <mi>a</mi> <mo>+</mo> <mi>c</mi> <mo>)</mo> <mo>+</mo> <mo>(</mo>
    <mi>b</mi> <mo>+</mo> <mi>d</mi> <mo>)</mo> <mi>ε</mi></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mrow><mo>(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>ε</mi>
    <mo>)</mo></mrow> <mo>×</mo> <mrow><mo>(</mo> <mi>c</mi> <mo>+</mo> <mi>d</mi>
    <mi>ε</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>a</mi> <mi>c</mi> <mo>+</mo> <mrow><mo>(</mo>
    <mi>a</mi> <mi>d</mi> <mo>+</mo> <mi>b</mi> <mi>c</mi> <mo>)</mo></mrow> <mi>ε</mi>
    <mo>+</mo> <mrow><mo>(</mo> <mi>b</mi> <mi>d</mi> <mo>)</mo></mrow> <msup><mi>ε</mi>
    <mn>2</mn></msup> <mo>=</mo> <mi>a</mi> <mi>c</mi> <mo>+</mo> <mrow><mo>(</mo>
    <mi>a</mi> <mi>d</mi> <mo>+</mo> <mi>b</mi> <mi>c</mi> <mo>)</mo></mrow> <mi>ε</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mtable displaystyle="true"><mtr><mtd columnalign="left"><mrow><mi>λ</mi>
    <mo>(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>ε</mi> <mo>)</mo> <mo>=</mo> <mi>λ</mi>
    <mi>a</mi> <mo>+</mo> <mi>λ</mi> <mi>b</mi> <mi>ε</mi></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mo>(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>ε</mi>
    <mo>)</mo> <mo>+</mo> <mo>(</mo> <mi>c</mi> <mo>+</mo> <mi>d</mi> <mi>ε</mi> <mo>)</mo>
    <mo>=</mo> <mo>(</mo> <mi>a</mi> <mo>+</mo> <mi>c</mi> <mo>)</mo> <mo>+</mo> <mo>(</mo>
    <mi>b</mi> <mo>+</mo> <mi>d</mi> <mo>)</mo> <mi>ε</mi></mrow></mtd></mtr> <mtr><mtd
    columnalign="left"><mrow><mrow><mo>(</mo> <mi>a</mi> <mo>+</mo> <mi>b</mi> <mi>ε</mi>
    <mo>)</mo></mrow> <mo>×</mo> <mrow><mo>(</mo> <mi>c</mi> <mo>+</mo> <mi>d</mi>
    <mi>ε</mi> <mo>)</mo></mrow> <mo>=</mo> <mi>a</mi> <mi>c</mi> <mo>+</mo> <mrow><mo>(</mo>
    <mi>a</mi> <mi>d</mi> <mo>+</mo> <mi>b</mi> <mi>c</mi> <mo>)</mo></mrow> <mi>ε</mi>
    <mo>+</mo> <mrow><mo>(</mo> <mi>b</mi> <mi>d</mi> <mo>)</mo></mrow> <msup><mi>ε</mi>
    <mn>2</mn></msup> <mo>=</mo> <mi>a</mi> <mi>c</mi> <mo>+</mo> <mrow><mo>(</mo>
    <mi>a</mi> <mi>d</mi> <mo>+</mo> <mi>b</mi> <mi>c</mi> <mo>)</mo></mrow> <mi>ε</mi></mrow></mtd></mtr></mtable></math>
- en: Most importantly, it can be shown that *h*(*a* + *bε*) = *h*(*a*) + *b* × *h*′(*a*)*ε*,
    so computing *h*(*a* + *ε*) gives you both *h*(*a*) and the derivative *h*′(*a*)
    in just one shot. [Figure B-2](#autodiff_forward_diagram) shows that the partial
    derivative of *f*(*x*, *y*) with regard to *x* at *x* = 3 and *y* = 4 (which I
    will write ∂*f*/∂*x* (3, 4)) can be computed using dual numbers. All we need to
    do is compute *f*(3 + *ε*, 4); this will output a dual number whose first component
    is equal to *f*(3, 4) and whose second component is equal to ∂*f*/∂*x* (3, 4).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，可以证明*h*(*a* + *bε*) = *h*(*a*) + *b* × *h*′(*a*)*ε*，因此计算*h*(*a* + *ε*)可以一次性得到*h*(*a*)和导数*h*′(*a*)。[图B-2](#autodiff_forward_diagram)显示了使用双重数计算*f*(*x*,
    *y*)对*x*在*x* = 3和*y* = 4时的偏导数（我将写为∂*f*/∂*x* (3, 4))。我们只需要计算*f*(3 + *ε*, 4)；这将输出一个双重数，其第一个分量等于*f*(3,
    4)，第二个分量等于∂*f*/∂*x* (3, 4)。
- en: '![mls3 ab02](assets/mls3_ab02.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 ab02](assets/mls3_ab02.png)'
- en: Figure B-2\. Forward-mode autodiff using dual numbers
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图B-2\. 使用双重数进行正向模式自动微分
- en: To compute ∂*f*/∂*y* (3, 4) we would have to go through the graph again, but
    this time with *x* = 3 and *y* = 4 + *ε*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算∂*f*/∂*y* (3, 4)，我们需要再次通过图进行计算，但这次是在*x* = 3和*y* = 4 + *ε*的情况下。
- en: 'So, forward-mode autodiff is much more accurate than finite difference approximation,
    but it suffers from the same major flaw, at least when there are many inputs and
    few outputs (as is the case when dealing with neural networks): if there were
    1,000 parameters, it would require 1,000 passes through the graph to compute all
    the partial derivatives. This is where reverse-mode autodiff shines: it can compute
    all of them in just two passes through the graph. Let’s see how.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正向模式自动微分比有限差分逼近更准确，但至少在输入较多而输出较少时存在相同的主要缺陷（例如在处理神经网络时）：如果有1,000个参数，将需要通过图进行1,000次传递来计算所有偏导数。这就是逆向模式自动微分的优势所在：它可以在通过图进行两次传递中计算出所有偏导数。让我们看看如何做到的。
- en: Reverse-Mode Autodiff
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逆向模式自动微分
- en: 'Reverse-mode autodiff is the solution implemented by TensorFlow. It first goes
    through the graph in the forward direction (i.e., from the inputs to the output)
    to compute the value of each node. Then it does a second pass, this time in the
    reverse direction (i.e., from the output to the inputs), to compute all the partial
    derivatives. The name “reverse mode” comes from this second pass through the graph,
    where gradients flow in the reverse direction. [Figure B-3](#autodiff_reverse_diagram)
    represents the second pass. During the first pass, all the node values were computed,
    starting from *x* = 3 and *y* = 4\. You can see those values at the bottom right
    of each node (e.g., *x* × *x* = 9). The nodes are labeled *n*[1] to *n*[7] for
    clarity. The output node is *n*[7]: *f*(3, 4) = *n*[7] = 42.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 逆向模式自动微分是TensorFlow实现的解决方案。它首先沿着图的正向方向（即从输入到输出）进行第一次传递，计算每个节点的值。然后进行第二次传递，这次是在反向方向（即从输出到输入）进行，计算所有偏导数。名称“逆向模式”来自于这个对图的第二次传递，在这个传递中，梯度以相反方向流动。[图B-3](#autodiff_reverse_diagram)代表了第二次传递。在第一次传递中，所有节点值都是从*x*
    = 3和*y* = 4开始计算的。您可以在每个节点的右下角看到这些值（例如，*x* × *x* = 9）。为了清晰起见，节点标记为*n*[1]到*n*[7]。输出节点是*n*[7]：*f*(3,
    4) = *n*[7] = 42。
- en: '![mls3 ab03](assets/mls3_ab03.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![mls3 ab03](assets/mls3_ab03.png)'
- en: Figure B-3\. Reverse-mode autodiff
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图B-3\. 逆向模式自动微分
- en: The idea is to gradually go down the graph, computing the partial derivative
    of *f*(*x*, *y*) with regard to each consecutive node, until we reach the variable
    nodes. For this, reverse-mode autodiff relies heavily on the *chain rule*, shown
    in [Equation B-4](#chain_rule).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是逐渐沿着图向下走，计算*f*(*x*, *y*)对每个连续节点的偏导数，直到达到变量节点。为此，逆向模式自动微分在[方程B-4](#chain_rule)中大量依赖于*链式法则*。
- en: Equation B-4\. Chain rule
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程B-4\. 链式法则
- en: <math display="block"><mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow>
    <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow> <mrow><mi>∂</mi><msub><mi>n</mi>
    <mi>i</mi></msub></mrow></mfrac></mstyle> <mo>×</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><msub><mi>n</mi>
    <mi>i</mi></msub></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle></mrow></math>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow>
    <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle> <mo>=</mo> <mstyle scriptlevel="0"
    displaystyle="true"><mfrac><mrow><mi>∂</mi><mi>f</mi></mrow> <mrow><mi>∂</mi><msub><mi>n</mi>
    <mi>i</mi></msub></mrow></mfrac></mstyle> <mo>×</mo> <mstyle scriptlevel="0" displaystyle="true"><mfrac><mrow><mi>∂</mi><msub><mi>n</mi>
    <mi>i</mi></msub></mrow> <mrow><mi>∂</mi><mi>x</mi></mrow></mfrac></mstyle></mrow></math>
- en: Since *n*[7] is the output node, *f* = *n*[7] so ∂*f* / ∂*n*[7] = 1.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 由于*n*[7]是输出节点，*f* = *n*[7]，所以∂*f* / ∂*n*[7] = 1。
- en: 'Let’s continue down the graph to *n*[5]: how much does *f* vary when *n*[5]
    varies? The answer is ∂*f* / ∂*n*[5] = ∂*f* / ∂*n*[7] × ∂*n*[7] / ∂*n*[5]. We
    already know that ∂*f* / ∂*n*[7] = 1, so all we need is ∂*n*[7] / ∂*n*[5]. Since
    *n*[7] simply performs the sum *n*[5] + *n*[6], we find that ∂*n*[7] / ∂*n*[5]
    = 1, so ∂*f* / ∂*n*[5] = 1 × 1 = 1.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续沿着图向下走到*n*[5]：当*n*[5]变化时，*f*会变化多少？答案是∂*f* / ∂*n*[5] = ∂*f* / ∂*n*[7] ×
    ∂*n*[7] / ∂*n*[5]。我们已经知道∂*f* / ∂*n*[7] = 1，所以我们只需要∂*n*[7] / ∂*n*[5]。由于*n*[7]只是执行*n*[5]
    + *n*[6]的求和，我们发现∂*n*[7] / ∂*n*[5] = 1，所以∂*f* / ∂*n*[5] = 1 × 1 = 1。
- en: 'Now we can proceed to node *n*[4]: how much does *f* vary when *n*[4] varies?
    The answer is ∂*f* / ∂*n*[4] = ∂*f* / ∂*n*[5] × ∂*n*[5] / ∂*n*[4]. Since *n*[5]
    = *n*[4] × *n*[2], we find that ∂*n*[5] / ∂*n*[4] = *n*[2], so ∂*f* / ∂*n*[4]
    = 1 × *n*[2] = 4.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以继续到节点*n*[4]：当*n*[4]变化时，*f*会变化多少？答案是∂*f* / ∂*n*[4] = ∂*f* / ∂*n*[5] × ∂*n*[5]
    / ∂*n*[4]。由于*n*[5] = *n*[4] × *n*[2]，我们发现∂*n*[5] / ∂*n*[4] = *n*[2]，所以∂*f* / ∂*n*[4]
    = 1 × *n*[2] = 4。
- en: The process continues until we reach the bottom of the graph. At that point
    we will have calculated all the partial derivatives of *f*(*x*, *y*) at the point
    *x* = 3 and *y* = 4\. In this example, we find ∂*f* / ∂*x* = 24 and ∂*f* / ∂*y*
    = 10\. Sounds about right!
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程一直持续到我们到达图的底部。在那一点上，我们将计算出*f*(*x*, *y*)在*x* = 3和*y* = 4时的所有偏导数。在这个例子中，我们发现∂*f*
    / ∂*x* = 24和∂*f* / ∂*y* = 10。听起来没错！
- en: Reverse-mode autodiff is a very powerful and accurate technique, especially
    when there are many inputs and few outputs, since it requires only one forward
    pass plus one reverse pass per output to compute all the partial derivatives for
    all outputs with regard to all the inputs. When training neural networks, we generally
    want to minimize the loss, so there is a single output (the loss), and hence only
    two passes through the graph are needed to compute the gradients. Reverse-mode
    autodiff can also handle functions that are not entirely differentiable, as long
    as you ask it to compute the partial derivatives at points that are differentiable.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 反向模式自动微分是一种非常强大和准确的技术，特别是当输入很多而输出很少时，因为它只需要一个前向传递加上一个反向传递来计算所有输出相对于所有输入的所有偏导数。在训练神经网络时，我们通常希望最小化损失，因此只有一个输出（损失），因此只需要通过图两次来计算梯度。反向模式自动微分还可以处理不完全可微的函数，只要您要求它在可微分的点计算偏导数。
- en: 'In [Figure B-3](#autodiff_reverse_diagram), the numerical results are computed
    on the fly, at each node. However, that’s not exactly what TensorFlow does: instead,
    it creates a new computation graph. In other words, it implements *symbolic* reverse-mode
    autodiff. This way, the computation graph to compute the gradients of the loss
    with regard to all the parameters in the neural network only needs to be generated
    once, and then it can be executed over and over again, whenever the optimizer
    needs to compute the gradients. Moreover, this makes it possible to compute higher-order
    derivatives if needed.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图B-3](#autodiff_reverse_diagram)中，数值结果是在每个节点上实时计算的。然而，这并不完全是TensorFlow的做法：相反，它创建了一个新的计算图。换句话说，它实现了*符号*反向模式自动微分。这样，只需要生成一次计算图来计算神经网络中所有参数相对于损失的梯度，然后每当优化器需要计算梯度时，就可以一遍又一遍地执行它。此外，这使得在需要时可以计算高阶导数。
- en: Tip
  id: totrans-56
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: 'If you ever want to implement a new type of low-level TensorFlow operation
    in C++, and you want to make it compatible with autodiff, then you will need to
    provide a function that returns the partial derivatives of the function’s outputs
    with regard to its inputs. For example, suppose you implement a function that
    computes the square of its input: *f*(*x*) = *x*². In that case you would need
    to provide the corresponding derivative function: *f*′(*x*) = 2*x*.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想在C++中实现一种新类型的低级TensorFlow操作，并且希望使其与自动微分兼容，那么您需要提供一个函数，该函数返回函数输出相对于其输入的偏导数。例如，假设您实现了一个计算其输入平方的函数：*f*(*x*)
    = *x*²。在这种情况下，您需要提供相应的导数函数：*f*′(*x*) = 2*x*。
