["```py\ndef true_dgp(jenny_inclination, brian_inclination, window_strength):     #1\n    jenny_throws_rock = jenny_inclination > 0.5     #2\n    brian_throws_rock = brian_inclination > 0.5   #2\n    if jenny_throws_rock and brian_throws_rock:     #3\n        strength_of_impact = 0.8    #3\n    elif jenny_throws_rock or brian_throws_rock:     #4\n        strength_of_impact = 0.6    #4\n    else:     #5\n        strength_of_impact = 0.0   #5\n    window_breaks = window_strength < strength_of_impact     #6\n    return jenny_throws_rock, brian_throws_rock, window_breaks\n```", "```py\nfrom pgmpy.models import BayesianNetwork\nmodel = BayesianNetwork(    #1\n       [\n        ('A', 'E'),    #2\n        ('S', 'E'),   #2\n        ('E', 'O'),   #2\n        ('E', 'R'),   #2\n        ('O', 'T'),   #2\n        ('R', 'T')    #2\n     ]\n)\n```", "```py\nBuilding a probabilistic machine learning model on a causal DAG\n```", "```py\nimport pandas as pd\nurl='https://raw.githubusercontent.com/altdeep/causalML/master/datasets\n↪/transportation_survey.csv'    #1\ndata = pd.read_csv(url)\ndata\n```", "```py\nfrom pgmpy.models import BayesianNetwork\nmodel = BayesianNetwork(\n      [\n        ('A', 'E'),\n        ('S', 'E'),\n        ('E', 'O'),\n        ('E', 'R'),\n        ('O', 'T'),\n        ('R', 'T')\n     ]\n)\nmodel.fit(data)    #1\ncausal_markov_kernels = model.get_cpds()     #2\nprint(causal_markov_kernels)  #2\n```", "```py\n[<TabularCPD representing P(A:3) at 0x7fb030dd1050>,\n <TabularCPD representing P(E:2 | A:3, S:2) at 0x7fb0318121d0>,\n <TabularCPD representing P(S:2) at 0x7fb03189fe90>,\n <TabularCPD representing P(O:2 | E:2) at 0x7fb030de85d0>,\n <TabularCPD representing P(R:2 | E:2) at 0x7fb030dfa890>,\n <TabularCPD representing P(T:3 | O:2, R:2) at 0x7fb0316c9110>]\n```", "```py\ncmk_T = causal_markov_kernels[-1]\nprint(cmk_T)\n```", "```py\n+----------+---------+----------+---------+----------+\n| O        | O(emp)  | O(emp)   | O(self) | O(self)  |\n+----------+---------+----------+---------+----------+\n| R        | R(big)  | R(small) | R(big)  | R(small) |\n+----------+---------+----------+---------+----------+\n| T(car)   | 0.70343 | 0.52439  | 0.44444 | 1.0      |\n+----------+---------+----------+---------+--------- +\n| T(other) | 0.13480 | 0.08536  | 0.33333 | 0.0      |\n+----------+---------+----------+---------+----------+\n| T(train) | 0.16176 | 0.39024  | 0.22222 | 0.0      |\n+----------+---------+----------+---------+----------+\n```", "```py\n+------+-------+\n| S(F) | 0.517 |\n+------+-------+\n| S(M) | 0.473 |\n+------+-------+\n| S(O) | 0.010 |\n+------+-------+\n```", "```py\nfrom pgmpy.estimators import BayesianEstimator #1\nmodel.fit(\n    data,\n    estimator=BayesianEstimator,    #2\n    prior_type=\"dirichlet\",\n    pseudo_counts=1    #3\n) \ncausal_markov_kernels = model.get_cpds()      #4\ncmk_T = causal_markov_kernels[-1]     #4\nprint(cmk_T)  #4\n```", "```py\n+----------+--------------------+-----+--------------------+----------+\n| O        | O(emp)             | ... | O(self)            | O(self)  |\n+----------+--------------------+-----+--------------------+----------+\n| R        | R(big)             | ... | R(big)             | R(small) |\n+----------+--------------------+-----+--------------------+----------+\n| T(car)   | 0.7007299270072993 | ... | 0.4166666666666667 | 0.5      |\n+----------+--------------------+-----+--------------------+----------+\n| T(other) | 0.1362530413625304 | ... | 0.3333333333333333 | 0.25     |\n+----------+--------------------+-----+--------------------+----------+\n| T(train) | 0.1630170316301703 | ... | 0.25               | 0.25     |\n+----------+--------------------+-----+--------------------+----------+\n```", "```py\nimport pandas as pd\nfrom pgmpy.models import BayesianNetwork\nfrom pgmpy.estimators import ExpectationMaximization as EM\nurl='https://raw.githubusercontent.com/altdeep/causalML/master/datasets\n↪/transportation_survey.csv'     #1\ndata = pd.read_csv(url)    #1\ndata_sans_E = data[['A', 'S', 'O', 'R', 'T']]    #2\nmodel_with_latent = BayesianNetwork(\n       [\n        ('A', 'E'),\n        ('S', 'E'),\n        ('E', 'O'),\n        ('E', 'R'),\n        ('O', 'T'),\n        ('R', 'T')\n     ],\n     latents={\"E\"}    #3\n)\nestimator = EM(model_with_latent, data_sans_E)     #4\ncmks_with_latent = estimator.get_parameters(latent_card={'E': 2})    #4\nprint(cmks_with_latent[1].to_factor())    #5\n```", "```py\n+------+----------+------+--------------+\n| E    | A        | S    |   phi(E,A,S) |\n+======+==========+======+==============+\n| E(0) | A(adult) | S(F) |       0.1059 |\n+------+----------+------+--------------+\n| E(0) | A(adult) | S(M) |       0.1124 |\n+------+----------+------+--------------+\n| E(0) | A(old)   | S(F) |       0.4033 |\n+------+----------+------+--------------+\n| E(0) | A(old)   | S(M) |       0.2386 |\n+------+----------+------+--------------+\n| E(0) | A(young) | S(F) |       0.4533 |\n+------+----------+------+--------------+\n| E(0) | A(young) | S(M) |       0.6080 |\n+------+----------+------+--------------+\n| E(1) | A(adult) | S(F) |       0.8941 |\n+------+----------+------+--------------+\n| E(1) | A(adult) | S(M) |       0.8876 |\n+------+----------+------+--------------+\n| E(1) | A(old)   | S(F) |       0.5967 |\n+------+----------+------+--------------+\n| E(1) | A(old)   | S(M) |       0.7614 |\n+------+----------+------+--------------+\n| E(1) | A(young) | S(F) |       0.5467 |\n+------+----------+------+--------------+\n| E(1) | A(young) | S(M) |       0.3920 |\n+------+----------+------+--------------+\n```", "```py\nfrom pgmpy.inference import VariableElimination    #1\ninference = VariableElimination(model)     \nquery1 = inference.query(['E'], evidence={\"T\": \"train\"})\nquery2 = inference.query(['E'], evidence={\"T\": \"car\"})\nprint(\"train\")\nprint(query1)\nprint(\"car\")\nprint(query2)\n```", "```py\n\"train\"\n+---------+----------+\n| E       |   phi(E) |\n+=========+==========+\n| E(high) |   0.6162 |\n+---------+----------+\n| E(uni)  |   0.3838 |\n+---------+----------+\n\"car\"\n+---------+----------+\n| E       |   phi(E) |\n+=========+==========+\n| E(high) |   0.5586 |\n+---------+----------+\n| E(uni)  |   0.4414 |\n+---------+----------+\n```", "```py\nimport torch\nimport pyro\nfrom pyro.distributions import Categorical\n\nA_alias = ['young', 'adult', 'old']     #1\nS_alias = ['M', 'F']     #1\nE_alias = ['high', 'uni']    #1\nO_alias = ['emp', 'self']   #1\nR_alias = ['small', 'big']    #1\nT_alias = ['car', 'train', 'other']    #1\n\nA_prob = torch.tensor([0.3,0.5,0.2])    #2\nS_prob = torch.tensor([0.6,0.4])    #2\nE_prob = torch.tensor([[[0.75,0.25], [0.72,0.28], [0.88,0.12]],    #2\n                     [[0.64,0.36], [0.7,0.3], [0.9,0.1]]])     #2\nO_prob = torch.tensor([[0.96,0.04], [0.92,0.08]])     #2\nR_prob = torch.tensor([[0.25,0.75], [0.2,0.8]])     #2\nT_prob = torch.tensor([[[0.48,0.42,0.1], [0.56,0.36,0.08]],     #2\n                     [[0.58,0.24,0.18], [0.7,0.21,0.09]]])     #2\n\ndef model():    #3\n   A = pyro.sample('age', Categorical(probs=A_prob))    #3\n   S = pyro.sample('gender', Categorical(probs=S_prob))    #3\n   E = pyro.sample('education', Categorical(probs=E_prob[S][A]))    #3\n   O = pyro.sample('occupation', Categorical(probs=O_prob[E]))    #3\n   R = pyro.sample('residence', Categorical(probs=R_prob[E]))   #3\n   T = pyro.sample('transportation', Categorical(probs=T_prob[R][O]))   #3  \n   Return {'A': A, 'S': S, 'E': E, 'O': O, 'R': R, 'T': T}     #3\n\npyro.render_model(model)     #4\n```", "```py\nimport numpy as np\nimport pyro\nfrom pyro.distributions import Categorical\nfrom pyro.infer import Importance, EmpiricalMarginal  #1\nimport matplotlib.pyplot as plt\n\nconditioned_model = pyro.condition(    #2\n    model,    #3\n    data={'transportation':torch.tensor(1.)}    #3\n)\n\nm = 5000     #4\nposterior = pyro.infer.Importance(    #5\n    conditioned_model,    #5\n    num_samples=m\n).run()     #6\n\nE_marginal = EmpiricalMarginal(posterior, \"education\")     #7\nE_samples = [E_marginal().item() for _ in range(m)]   #7\nE_unique, E_counts = np.unique(E_samples, return_counts=True)     #8\nE_probs = E_counts / m    #8\n\nplt.bar(E_unique, E_probs, align='center', alpha=0.5)     #9\nplt.xticks(E_unique, E_alias)    #9\nplt.ylabel('probability')    #9\nplt.xlabel('education')     #9\nplt.title('P(E | T = \"train\") - Importance Sampling')    #9\n```", "```py\nfrom dowhy import datasets\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nsim_data = datasets.linear_dataset(     #1\n    beta=10.0,\n    num_treatments=1,    #2\n    num_instruments=2,     #3\n    num_effect_modifiers=2,     #4\n    num_common_causes=5,     #5\n    num_frontdoor_variables=1,   #6\n    num_samples=100,\n\n)\n\ndag = nx.parse_gml(sim_data['gml_graph'])     #7\npos = {    #7\n 'X0': (600, 350),    #7\n 'X1': (600, 250),    #7\n 'FD0': (300, 300),   #7\n 'W0': (0, 400),   #7\n 'W1': (150, 400),   #7\n 'W2': (300, 400),   #7\n 'W3': (450, 400),   #7\n 'W4': (600, 400),   #7\n 'Z0': (10, 250),    #7\n 'Z1': (10, 350),    #7\n 'v0': (100, 300),   #7\n 'y': (500, 300)     #7\n}    #7\noptions = {    #7\n    \"font_size\": 12,    #7\n    \"node_size\": 800,   #7\n    \"node_color\": \"white\",   #7\n    \"edgecolors\": \"black\",   #7\n    \"linewidths\": 1,    #7\n    \"width\": 1,    #7\n}    #7\nnx.draw_networkx(dag, pos, **options)    #7\nax = plt.gca()   #7\nax.margins(x=0.40)    #7\nplt.axis(\"off\")   #7\nplt.show() #7\n```"]