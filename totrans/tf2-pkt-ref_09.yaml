- en: Chapter 9\. Serving TensorFlow Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。为TensorFlow模型提供服务
- en: If you’ve been reading the chapters in this book sequentially, you now know
    a lot about how to handle the data engineering pipeline, build models, launch
    training routines, checkpoint models at each epoch, and even score test data.
    In all the examples thus far, these tasks have mostly been wrapped together for
    didactic purposes. In this chapter, however, you’re going to learn more about
    how to serve TensorFlow models based on the format in which they are saved.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您按顺序阅读本书中的章节，现在您已经了解了如何处理数据工程流水线、构建模型、启动训练例程、在每个时代检查点模型，甚至对测试数据进行评分。到目前为止，所有这些任务大多都被捆绑在一起以进行教学。然而，在本章中，您将更多地了解如何根据保存的格式为TensorFlow模型提供服务。
- en: Another important distinction between this chapter and previous chapters is
    that here you will learn a coding pattern for handling data engineering for test
    data. Previously, you saw that test data and training data are transformed at
    the same runtime. As a machine learning engineer, though, you also have to think
    about the scenarios where your model is deployed.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这一章与之前的章节之间的另一个重要区别是，您将学习处理测试数据的数据工程编码模式。以前，您看到测试数据和训练数据在相同的运行时转换。然而，作为一个机器学习工程师，您还必须考虑到您的模型部署的情况。
- en: 'Imagine that your model is loaded in a Python runtime and ready to go. You
    have a batch of samples or a sample. What do you need to do to the input data
    so that the model can accept it and return predictions? In other words: you have
    a model and raw test data; how do you implement the logic of transforming the
    raw data? In this chapter, you will learn about serving the model through a few
    examples.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，您的模型已经加载到Python运行时并准备就绪。您有一批样本或一个样本。您需要对输入数据进行哪些操作，以便模型可以接受它并返回预测？换句话说：您有一个模型和原始测试数据；您如何实现转换原始数据的逻辑？在本章中，您将通过几个示例了解如何为模型提供服务。
- en: Model Serialization
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型序列化
- en: 'A TensorFlow model can be saved in two different native formats (without any
    optimization): HDF5 (h5) or protobuf (pb). Both formats are standard data serialization
    (saving) formats in Python and other programming languages to persist objects
    or data structures; they are not specific to TensorFlow or even ML models. Before
    TensorFlow 2.0, pb was the only native format available. With TensorFlow 2.0,
    where the Keras API is the de facto high-level API going forward, h5 has become
    an alternative to pb.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow模型可以以两种不同的本机格式（没有任何优化）保存：HDF5（h5）或protobuf（pb）。这两种格式都是Python和其他编程语言中的标准数据序列化（保存）格式，用于持久化对象或数据结构；它们不是特定于TensorFlow甚至ML模型的。在TensorFlow
    2.0之前，pb是唯一可用的本机格式。随着TensorFlow 2.0的到来，其中Keras API是未来的事实高级API，h5已成为pb的替代品。
- en: Today both formats may be used for deployment, especially in various public
    cloud providers. This is because each cloud provider now has its own API that
    wraps a model. As long as the model is saved in your workspace, you can reach
    it through web services such as RESTful (Representational State Transfer) APIs,
    which utilize HTTP methods to make a request over a network. Therefore, regardless
    of format, your model is ready for serving through a RESTful API call from a client
    program.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如今，这两种格式都可以用于部署，特别是在各种公共云提供商中。这是因为每个云提供商现在都有自己的API来包装模型。只要模型保存在您的工作空间中，您就可以通过诸如RESTful（表述性状态转移）API之类的网络服务来访问它，这些API利用HTTP方法在网络上进行请求。因此，无论格式如何，您的模型都可以通过来自客户端程序的RESTful
    API调用进行服务。
- en: Let’s start with the image classification model you built with CIFAR-10 data
    in the last chapter. If you haven’t worked your way through that chapter, use
    the following code snippets to get an image classifier built and trained quickly.
    (By the way, the code here was developed in Google Colab with one GPU.)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从上一章中使用CIFAR-10数据构建的图像分类模型开始。如果您还没有完成那一章，可以使用以下代码片段快速构建和训练图像分类器。（顺便说一句，这里的代码是在Google
    Colab中使用一个GPU开发的。）
- en: 'Import all the necessary libraries first:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先导入所有必要的库：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load and normalize the images:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 加载并标准化图像：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define your image labels:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 定义您的图像标签：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Transform the raw images into a dataset. Since a partition for test images
    is available, the following code separates out the first 500 images in that partition
    for the validation dataset used at the end of each training epoch, and keeps the
    remainder as a test dataset. All of the training images are converted to training
    datasets:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始图像转换为数据集。由于测试图像的分区可用，以下代码将该分区中的前500个图像分离出来，用作每个训练周期结束时用于验证数据集，并将其余部分保留为测试数据集。所有训练图像都被转换为训练数据集：
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To make sure you know the sample size of each dataset, iterate through them
    and display the sample size:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保您知道每个数据集的样本大小，请遍历它们并显示样本大小：
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You should see the following output:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下输出：
- en: '[PRE5]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Define a distribution strategy for distributed training:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为分布式训练定义一个分发策略：
- en: '[PRE6]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You should see one GPU available:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到一个可用的GPU：
- en: '[PRE7]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now set the batch sizes for training:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在设置训练的批次大小：
- en: '[PRE8]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Apply the batches to each dataset and calculate the number of batches for each
    training epoch before evaluating the model’s accuracy with the validation data:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 将批次应用于每个数据集，并在评估模型的准确性之前计算每个训练周期的批次数：
- en: '[PRE9]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Create a function called `build_model` to define the model architecture and
    compile it with the loss function for classification, optimizer, and metrics,
    all within the distributed training strategy scope:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`build_model`的函数来定义模型架构，并在分布式训练策略范围内使用分类损失函数、优化器和指标对其进行编译：
- en: '[PRE10]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now create a model instance by invoking `build_model`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在通过调用`build_model`创建一个模型实例：
- en: '[PRE11]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define an alias for the file path and to save model checkpoints:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为文件路径定义一个别名，并保存模型检查点：
- en: '[PRE12]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Use a `print` statement to display the model name format:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`print`语句显示模型名称格式：
- en: '[PRE13]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now set up your checkpoint directory:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在设置您的检查点目录：
- en: '[PRE14]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The checkpoint directory will be set as a directory with the following pattern:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点目录将设置为具有以下模式的目录：
- en: '[PRE15]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In your current directory level, you will see a *myCIFAR10-20210319-214456*
    directory containing weight files with the prefix *ckpt-{epoch}*, where *{epoch}*
    is the epoch number.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在您当前的目录级别中，您将看到一个包含以*ckpt-{epoch}*为前缀的权重文件的*myCIFAR10-20210319-214456*目录，其中*{epoch}*是epoch编号。
- en: 'Next, define a checkpoint object. Let’s save the model weights at the end of
    an epoch only if the model performance on validation data is improved over the
    previous epoch. Save the weights in the same directory (*myCIFAR10-20210319-214456*)
    so that the latest saved checkpoint weights are from the best epoch. This saves
    time, since you don’t need to determine which epoch presented the best model.
    Make sure that both `save_weights_only` and `save_best_only` are set to True:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，定义一个检查点对象。只有在验证数据上的模型性能比上一个epoch有所改善时，才在一个epoch结束时保存模型权重。将权重保存在相同的目录（*myCIFAR10-20210319-214456*）中，以便最新保存的检查点权重来自最佳epoch。这样可以节省时间，因为您不需要确定哪个epoch呈现了最佳模型。确保`save_weights_only`和`save_best_only`都设置为True：
- en: '[PRE16]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now pass the preceding checkpoint definition into a list, as required by the
    `fit` function:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在将前面的检查点定义传递到列表中，这是`fit`函数所需的：
- en: '[PRE17]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And launch the training process:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后启动训练过程：
- en: '[PRE18]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Throughout the training routine, you can inspect the output. You’ll likely
    see something similar to this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，您可以检查输出。您可能会看到类似于这样的内容：
- en: '[PRE19]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In this example, the highest validation accuracy occurs in epoch 9, where `val_accuracy`
    is 0.7080.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，最高的验证准确率出现在第9个epoch，其中`val_accuracy`为0.7080。
- en: 'Check the checkpoint directory:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 检查检查点目录：
- en: '[PRE20]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You will see its contents:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到它的内容：
- en: '[PRE21]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Since the `myCheckpoint` object has `save_best_only` and `save_weights_only`
    set to True, the last weight is `ckpt-9.data-00000-of-00001`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`myCheckpoint`对象的`save_best_only`和`save_weights_only`设置为True，最后一个权重是`ckpt-9.data-00000-of-00001`。
- en: 'To programmatically locate the last weight file among all the saved weight
    files from a single directory, you can use the `latest_checkpoint` function in
    the `tf.train` API. Run the following command:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 要在单个目录中从所有保存的权重文件中编程地定位最后一个权重文件，您可以使用`tf.train` API中的`latest_checkpoint`函数。运行以下命令：
- en: '[PRE22]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This is the expected output:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这是预期的输出：
- en: '[PRE23]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This identifies the prefix to the last weight file. You can then load the best
    weights to the model:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这将标识最后一个权重文件的前缀。然后可以将最佳权重加载到模型中：
- en: '[PRE24]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now you have the best model, which can be saved in h5 or pb format. We’ll look
    at h5 first.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有了最佳模型，可以保存为h5或pb格式。我们先看h5。
- en: Saving a Model to h5 Format
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型保存为h5格式
- en: 'The high-level `tf.keras` API uses the `save` function as a means to save the
    model in h5 format:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 高级`tf.keras` API使用`save`函数来保存模型为h5格式：
- en: '[PRE25]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Take a look at the directory:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下目录：
- en: '[PRE26]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You will see the model as an h5 file:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到模型作为一个h5文件：
- en: '[PRE27]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In the future, if you want to reload this model for scoring, simply use the
    `load_model` function. (Make sure you also import all the libraries, as indicated
    in the beginning of the section.)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 将来，如果您想重新加载此模型进行评分，只需使用`load_model`函数。（确保您还导入了本节开头指示的所有库。）
- en: '[PRE28]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'For quick scoring, use `test_dataset`, which you prepared at the same time
    as `training_dataset`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速评分，使用您与`training_dataset`同时准备的`test_dataset`：
- en: '[PRE29]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'It will produce results like these:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 它将产生这样的结果：
- en: '[PRE30]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: There are 9,500 elements; each is an array of probabilities. The index of maximum
    probability maps to the label in `CLASS_NAMES`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 有9500个元素；每个元素都是一个概率数组。最大概率的索引映射到`CLASS_NAMES`中的标签。
- en: Saving a Model to pb Format
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型保存为pb格式
- en: 'To save the same model to pb format, you’ll use the `tf.saved_model.save` function:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要将相同的模型保存为pb格式，您将使用`tf.saved_model.save`函数：
- en: '[PRE31]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Look at the contents in `SAVED_MODEL_PATH`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`SAVED_MODEL_PATH`中的内容：
- en: '[PRE32]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The contents should look like this:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 内容应该是这样的：
- en: '[PRE33]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The weight file is in the variables directory. You can inspect it with this
    command:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 权重文件在variables目录中。您可以使用此命令检查它：
- en: '[PRE34]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here’s what you’ll see (more or less):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您将看到的内容（或多或少）：
- en: '[PRE35]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now that you’ve seen the folder structure when a model is saved as a protobuf,
    let’s see how to load a model protobuf. In this case, you’ll need to load it from
    the directory name, which is the directory that contains *saved_model.pb*:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到了将模型保存为protobuf时的文件夹结构，让我们看看如何加载模型protobuf。在这种情况下，您需要从包含*saved_model.pb*的目录名称加载它：
- en: '[PRE36]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If you take a closer look at the preceding commands, you will notice that just
    as in model training, model loading is done within a distribute strategy scope.
    If you are running a cloud TPU or GPU (as in the case of [Google Colab](https://oreil.ly/ZBYwr)),
    set `experimental_io_device` to localhost, which is the node where you saved the
    model. Then use `tf.keras.models.load_model` to load the directory that holds
    *saved_model.pb*: in this case, it is `SAVED_MODEL_PATH`.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仔细查看前面的命令，您会注意到，就像在模型训练中一样，模型加载也是在分布策略范围内完成的。如果您正在运行云TPU或GPU（如[Google Colab](https://oreil.ly/ZBYwr)的情况），请将`experimental_io_device`设置为localhost，即保存模型的节点。然后使用`tf.keras.models.load_model`加载包含*saved_model.pb*的目录：在这种情况下，它是`SAVED_MODEL_PATH`。
- en: 'Now use the model `loaded_pb` to score `test_dataset`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用模型`loaded_pb`对`test_dataset`进行评分：
- en: '[PRE37]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'You will see the same output as in the h5 model’s predictions:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到与h5模型预测中相同的输出：
- en: '[PRE38]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Likewise, each inner bracket is a list of probabilities for a test image. The
    index of maximum probability can be mapped to the correct entry in `CLASS_NAMES`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，每个内部括号都是一个测试图像的概率列表。最大概率的索引可以映射到`CLASS_NAMES`中的正确条目。
- en: 'As you’ve seen, the model in either h5 or pb format can be used for scoring
    test data in dataset format. The model can also score test data in a NumPy array
    format. Recall that `test_images[500:]` is the original NumPy test data format;
    the subset starts at 500 images and goes on (for a total of 9,500 test images).
    You can pass this NumPy array directly into the model for scoring:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所看到的，无论是h5还是pb格式的模型都可以用于对数据集格式中的测试数据进行评分。该模型还可以对NumPy数组格式的测试数据进行评分。回想一下，`test_images[500：]`是原始的NumPy测试数据格式；子集从第500张图像开始，一直持续下去（总共9500张测试图像）。您可以直接将这个NumPy数组传递给模型进行评分：
- en: '[PRE39]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You will see the same output:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到与之前相同的输出：
- en: '[PRE40]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Selecting the Model Format
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You have now seen how to score test data with both the h5 and pb model formats.
    However, choosing which format to use depends on many things. Conceptually, the
    h5 format is very easy to understand; it consists of a model skeleton and weights,
    saved as a single file. This is very similar to how a `pickle` object or file
    works: as long as you import the library, you can open the single file that contains
    everything you need to reinstate the object (in this case, your model). This approach
    is suitable for simple deployments, where a driver program running the Python
    runtime can simply use `tf.keras.models.load_model` to load the model and run
    it over the test data.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if the model has to be run with more complicated settings, then protobuf
    format is a better choice. This is because the pb format is programming-language
    agnostic: it can be read by many other programming languages besides Python, such
    as Java, JavaScript, C, C++, and so forth. In fact, when you take the model to
    production, you will use TensorFlow Serving to host the pb model to score test
    data over the internet. In the next section, you will learn how TensorFlow Serving
    works.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Serving
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow Serving (TFS) is a framework specifically designed for running ML
    models in a production environment. Since scoring test data over the internet
    (or using Internet Protocol in a virtual private network) is arguably the most
    common model-serving scenario, there needs to be an HTTP or HTTPS endpoint that
    serves as the “front door” to the model. The client program, which will pass test
    data to the model, needs to communicate with the model’s endpoint via HTTP. This
    communication follows the style of the RESTful API, which specifies a set of rules
    and formats for data sent over HTTP.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: TFS takes care of all the complexity here for you. Next you will see how to
    run TFS to host this model in your local environment.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Running TensorFlow Serving with a Docker Image
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The easiest way to learn TFS is with a Docker image. If you need some background
    on Docker and general container technology, take a look at [*Using Docker*](https://oreil.ly/FyJOH)
    by Adrian Mouat (O’Reilly). [Chapter 1](ch01.xhtml#introduction_to_tensorflow_two)
    provides a concise explanation of Docker containers, while [Chapter 2](ch02.xhtml#data_storage_and_ingestion)
    shows you how to install a Docker engine and get it up and running in your local
    node.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'Briefly, a Docker image is a lightweight, standalone, executable package of
    software that includes everything needed to run an application: code, runtime,
    system tools, system libraries, and settings. To run a Docker image, you need
    a Docker engine. When you run a Docker image on a Docker engine, the image becomes
    a [*container*](https://oreil.ly/V8KMi).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: For instructions on installing a Docker engine, take a look at the [Docker documentation](https://oreil.ly/77l6U).
    There are versions available for macOS, Windows 10, and Linux. Choose the one
    that works for your environment and follow the installation instructions. For
    the examples and workflow in this chapter, my local system is running macOS Big
    Sur version 11.2, and my Docker engine version is 3.0.3.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Now make sure your Docker engine is up and running: launch it by double-clicking
    its icon in your environment. When it’s running, you will see the Docker whale
    icon in the top bar on a Mac, shown in [Figure 9-1](#docker_engine_running_status)
    or in the notification area (lower right) on a PC.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![Docker engine running status](Images/t2pr_0901.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: Figure 9-1\. Docker engine running status
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once you have your Docker engine installed and running, download TFS’s Docker
    image as a base, add the CIFAR-10 model to the base image, and then build a new
    image. This new image will be served through an HTTP endpoint and a specific TCP/IP
    port. A client program will send test data to this HTTP address and port.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Make sure you save your model in pb format. This time, name it *001*. This directory
    doesn’t have to be named *001*, but it does have to be numeric per TFS’s required
    hierarchy and naming convention.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'Continue with the notebook you made in the previous section, and save the model
    in the local directory by using the following command:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This will produce the following directory structure:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: For now, use the command terminal and navigate to the *models* directory.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the Docker engine is running, you are ready to start pulling a TFS
    image. Type the following command while in the *models* directory:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This command downloads a TFS image to your local Docker environment. Now run
    the image:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The preceding command launches a TFS image as a container named `serv_base_img`.
    Run the following command to add the model you built to the base image:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: It is natural to think of the *saved_model.pb* file as a reference for where
    everything else is. Remember that *CIFAR10* is the local directory, two levels
    up from the pb file. In between them is the directory *001*. Now *CIFAR10* is
    copied into the base image as */models/cifar10*. Notice that in Docker, directory
    names are all lowercase.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, commit the change you made to the base image. Run the following command:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Now you can stop the base image; you don’t need it anymore:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Let’s review what you’ve done so far. You have created a new Docker image by
    adding your CIFAR-10 model to TFS, which is a base image. That model is now deployed
    and running in the TFS container. Once you run the TFS container, the model is
    live and ready to serve any client.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'To serve the TFS container that hosts your CIFAR-10 model, run the following
    command:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This is a relatively lengthy command. Let’s dissect it a bit.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: First, you map local port 8501 to the Docker engine’s port 8501\. There is nothing
    magical about your local port number. If 8501 is in use in your local environment,
    you can use a different port number—say, 8515\. If so, then the command would
    be `-p 8515:8501`. Since the TFS container always runs on port 8501, the second
    target in the preceding command is always 8501.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: The source indicates that below the current directory (*$PWD*) there is a *CIFAR10*
    directory, which is where the model is located. This model is named CIFAR10, and
    the `tensorflow/serving` container is ready to take input.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: You will see the output shown in [Figure 9-2](#command_terminal_running_a_custom_docker).
    It indicates that you are running CIFAR10 model version 1, which is taken from
    the directory named *001*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![Command terminal running a custom Docker container](Images/t2pr_0902.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: Figure 9-2\. Command terminal running a custom Docker container
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Scoring Test Data with TensorFlow Serving
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that your TFS container is running with your model, you are ready to pass
    test data to the model. This is done via an HTTP request. You may use another
    Jupyter Notebook as a client that sends the NumPy array to TFS. The HTTP address
    for TFS is *http://localhost:8501/v1/models/cifar10:predict*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the client code in a different Jupyter Notebook:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary libraries:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Load test images and normalize the pixel value range to be between 0 and 1,
    and then select the images. For simplicity, let’s only select 10 images; we’ll
    use those that are between 500 and 510:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Convert the NumPy array `test_images` into JSON, a commonly used format for
    data exchange between a client and a server over HTTP:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You also need to define headers:'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Now that you have wrapped your NumPy data with the appropriate format and headers,
    you are ready to send the whole package to TFS.
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Construct the entire package as an HTTP request:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Notice that you use the `post` method to solicit a response from TFS. TFS will
    score `DATA` and return the results as `response`. This communication framework,
    which uses a JSON request format and rules to establish and handle communication
    between a client and a server, is also known as a RESTful API.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Take a look at what TFS has predicted:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The preceding command will decode the prediction to an array of probability:'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述命令将解码预测为概率数组：
- en: '[PRE55]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: In the preceding code, you see the first and tenth test images. Each inner array
    consists of 10 probability values, each of which maps to a `CLASS_NAME`.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在上述代码中，您可以看到第一和第十个测试图像。每个内部数组由10个概率值组成，每个值都映射到一个`CLASS_NAME`。
- en: 'To map the maximum probability in each prediction back to a label, you need
    to retrieve the values, shown earlier, from the Python dictionary `response`.
    You can retrieve the values using the key name `predictions` via:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要将每个预测中的最大概率映射回标签，您需要从Python字典`response`中检索先前显示的值。您可以通过键名`predictions`检索这些值：
- en: '[PRE56]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The labels for CIFAR-10 data are:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CIFAR-10数据的标签是：
- en: '[PRE57]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '`CLASS_NAMES` is a list that holds the CIFAR-10 labels.'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`CLASS_NAMES`是一个包含CIFAR-10标签的列表。'
- en: 'Now convert `predictions_prob_list` to a NumPy array, then use `argmax` to
    find the index for the maximum probability value:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在将`predictions_prob_list`转换为NumPy数组，然后使用`argmax`找到最大概率值的索引：
- en: '[PRE58]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Map each index (there are 10) to a CIFAR-10 label:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个索引（共有10个）映射到一个CIFAR-10标签：
- en: '[PRE59]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Your output should look something like this:'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您的输出应该类似于这样：
- en: '[PRE60]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: This is how you decode the probability array back to labels.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这就是您将概率数组解码回标签的方法。
- en: You have just run a TFS Docker container with your own image classification
    model behind an HTTP endpoint. That container accepts input data as a JSON payload
    in a `post` request. TFS unpacks the request body; extracts the JSON payload,
    which contains the NumPy array; scores each array; and returns the results back
    to the client.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 您刚刚在TFS Docker容器中运行了自己的图像分类模型，该模型位于HTTP端点后面。该容器接受JSON有效负载形式的输入数据作为`post`请求。TFS解包请求正文；提取包含NumPy数组的JSON有效负载；对每个数组进行评分；并将结果返回给客户端。
- en: Wrapping Up
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter showed you the basics of model persistence (saving) and model serving
    (scoring). The TensorFlow model is flexible in that it takes advantage of the
    simplicity offered by the `tf.keras` API to save the model as a single HDF5 file.
    This format is easy to handle and share with others.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您展示了模型持久性（保存）和模型服务（评分）的基础知识。TensorFlow模型灵活，利用`tf.keras` API提供的简单性将模型保存为单个HDF5文件。这种格式易于处理并与他人共享。
- en: For a serving framework that caters to a production environment, typically you
    need to have a model hosted in a runtime, and that runtime needs to be accessible
    via a web-based communication protocol such as HTTP. As it turns out, TFS provides
    a framework that handles the HTTP request. All you need to do is copy your protobuf
    model folder to a TFS base image and commit the change to the base image. Now
    you have created a Docker image of your model, and you have the model running
    behind TFS.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于适用于生产环境的服务框架，通常需要在运行时托管模型，并且该运行时需要通过HTTP等基于Web的通信协议进行访问。事实证明，TFS提供了一个处理HTTP请求的框架。您只需要将您的protobuf模型文件夹复制到TFS基础映像中，并提交更改到基础映像。现在，您已经创建了一个包含您的模型的Docker映像，并且该模型在TFS后面运行。
- en: You learned how to use another runtime to create a correctly shaped numeric
    array, wrap it around a JSON-format data payload, and send it using the `post`
    command to the HTTP endpoint hosted by TFS for scoring.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 您学会了如何使用另一个运行时创建正确形状的数值数组，将其包装在JSON格式的数据有效负载中，并使用`post`命令将其发送到由TFS托管的HTTP端点进行评分。
- en: This pretty much completes the knowledge loop of building, training, and serving
    the model. In the next chapter, you will learn more practices for model tuning
    and fairness.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上完成了构建、训练和提供模型的知识循环。在下一章中，您将学习更多关于模型调整和公平性的实践。
