- en: Chapter 9\. Generative Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章。生成模型
- en: All the problems we have looked at so far involve, in some way, translating
    from inputs to outputs. You create a model that takes an input and produces an
    output. Then you train it on input samples from a dataset, optimizing it to produce
    the best output for each one.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们所研究的所有问题都涉及以某种方式将输入转换为输出。您创建一个模型，该模型接受输入并产生输出。然后，您对来自数据集的输入样本进行训练，优化以产生每个样本的最佳输出。
- en: '*Generative models* are different. Instead of taking a sample as input, they
    produce a sample as output. You might train the model on a library of photographs
    of cats, and it would learn to produce new images that look like cats. Or, to
    give a more relevant example, you might train it on a library of known drug molecules,
    and it would learn to generate new “drug-like” molecules for use as candidates
    in a virtual screen. Formally speaking, a generative model is trained on a collection
    of samples that are drawn from some (possibly unknown, probably very complex)
    probability distribution. Its job is to produce new samples from that same probability
    distribution.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成模型*是不同的。它们不是以样本作为输入，而是以样本作为输出。您可以在猫的照片库上训练模型，它将学会生成看起来像猫的新图像。或者，为了给出一个更相关的例子，您可以在已知药物分子的库上对其进行训练，它将学会生成新的“类似药物”的分子，以用作虚拟筛选中的候选物。从形式上讲，生成模型是在从某个（可能未知，可能非常复杂）概率分布中抽取的一组样本上进行训练的。它的工作是从相同概率分布中生成新样本。'
- en: 'In this chapter, we will begin by describing the two most popular types of
    generative models: *variational autoencoders* and *generative adversarial networks*.
    We will then discuss a few applications of these models in the life sciences,
    and work through some code examples.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先描述两种最流行的生成模型类型：*变分自动编码器*和*生成对抗网络*。然后，我们将讨论这些模型在生命科学中的一些应用，并通过一些代码示例进行工作。
- en: Variational Autoencoders
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变分自动编码器
- en: An *autoencoder* is a model that tries to make its output equal to its input.
    You train it on a library of samples and adjust the model parameters so that on
    every sample the output is as close as possible to the input.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '*自动编码器*是一种试图使其输出等于其输入的模型。您对其进行训练，调整模型参数，以便在每个样本上输出尽可能接近输入。'
- en: That sounds trivial. Can’t it just learn to pass the input directly through
    to the output unchanged? If that were actually possible it would indeed be trivial,
    but autoencoders usually have architectures that make it impossible. Most often
    this is done by forcing the data to go through a bottleneck, as shown in [Figure 9-1](#structure_of_a_variational_autoencoder).
    For example, the input and output might each include 1,000 numbers, but in between
    would be a hidden layer containing only 10 numbers. This forces the model to learn
    how to compress the input samples. It must represent 1,000 numbers worth of information
    using only 10 numbers.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来很琐碎。它不能直接学会将输入直接传递到输出而不改变吗？如果这实际上是可能的，那确实是琐碎的，但是自动编码器通常具有使其不可能的架构。通常，这是通过强制数据通过瓶颈来完成的，如[图9-1](#structure_of_a_variational_autoencoder)所示。例如，输入和输出可能各包含1,000个数字，但中间会有一个仅包含10个数字的隐藏层。这迫使模型学习如何压缩输入样本。它必须使用仅10个数字来表示价值1,000个数字的信息。
- en: '![Structure of a variational autoencoder.](Images/dlls_0901.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![变分自动编码器的结构。](Images/dlls_0901.png)'
- en: Figure 9-1\. Structure of a variational autoencoder.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-1. 变分自动编码器的结构。
- en: If the model needed to handle arbitrary inputs, that would be impossible. You
    can’t throw out 99% of the information and still reconstruct the input! But we
    don’t care about arbitrary inputs, only the specific ones in the training set
    (and others that resemble them). Of all possible images, far less than 1% look
    anything like cats. An autoencoder doesn’t need to work for all possible inputs,
    only ones that are drawn from a specific probability distribution. It needs to
    learn the “structure” of that distribution, figure out how to represent the distribution
    using much less information, and then be able to reconstruct the samples based
    on the compressed information.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型需要处理任意输入，那将是不可能的。您不能丢弃99%的信息并仍然重建输入！但我们不关心任意输入，只关心训练集中的特定输入（以及类似它们的其他输入）。在所有可能的图像中，远不到1%的图像看起来像猫。自动编码器不需要为所有可能的输入工作，只需要为从特定概率分布中抽取的输入工作。它需要学习该分布的“结构”，找出如何使用更少的信息表示该分布，然后能够基于压缩信息重建样本。
- en: Now let’s take the model apart. The middle layer, the one that serves as the
    bottleneck, is called the *latent space* of the autoencoder. It is the space of
    compressed representations of samples. The first half of the autoencoder is called
    the *encoder*. Its job is to take samples and convert them to compressed representations.
    The second half is called the *decoder*. It takes compressed representations in
    the latent space and converts them back into the original samples.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们拆解这个模型。中间层，作为瓶颈的那一层，被称为自动编码器的*潜在空间*。这是样本的压缩表示空间。自动编码器的前半部分称为*编码器*。它的工作是接受样本并将其转换为压缩表示。后半部分称为*解码器*。它接受潜在空间中的压缩表示并将其转换回原始样本。
- en: This gives us our first clue about how autoencoders could be used for generative
    modeling. The decoder takes vectors in the latent space and converts them into
    samples, so we could take random vectors in the latent space (picking a random
    value for each component of the vector) and pass them through the decoder. If
    everything goes well, the decoder should produce a completely new sample that
    still resembles the ones it was trained on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我们有关自动编码器如何用于生成建模的第一个线索。解码器接受潜在空间中的向量并将其转换为样本，因此我们可以在潜在空间中取随机向量（为向量的每个分量选择随机值）并通过解码器传递它们。如果一切顺利，解码器应该产生一个完全新的样本，仍然类似于它所训练的样本。
- en: This sort of works, but not very well. The problem is that the encoder may only
    produce vectors in a small region of the latent space. If we pick a vector anywhere
    else in the latent space, we may get an output that looks nothing like the training
    samples. In other words, the decoder has only learned to work for the particular
    latent vectors produced by the encoder, not for arbitrary ones.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有点奏效，但效果并不是很好。问题在于编码器可能只在潜在空间的一个小区域内产生向量。如果我们在潜在空间的其他任何地方选择一个向量，我们可能得到一个看起来与训练样本完全不同的输出。换句话说，解码器只学会为编码器产生的特定潜在向量工作，而不是任意的向量。
- en: A variational autoencoder (VAE) adds two features to overcome this problem.
    First, it adds a term to the loss function that forces the latent vectors to follow
    a specified distribution. Most often they are constrained to have a Gaussian distribution
    with a mean of 0 and a variance of 1\. We don’t leave the encoder free to generate
    vectors wherever it wants. We force it to generate vectors with a known distribution.
    That way, if we pick random vectors from that same distribution, we can expect
    the decoder to work well on them.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自动编码器（VAE）添加了两个特性来克服这个问题。首先，它添加了一个项到损失函数中，强制潜在向量遵循指定的分布。通常它们被限制为具有均值为0和方差为1的高斯分布。我们不让编码器自由生成向量。我们强制它生成具有已知分布的向量。这样，如果我们从相同分布中选择随机向量，我们可以期望解码器在它们上表现良好。
- en: Second, during training we add random noise to the latent vector. The encoder
    converts the input sample to a latent vector, and then we randomly change it a
    little bit before passing it through the decoder, requiring the output to still
    be as close as possible to the original sample. This prevents the decoder from
    being too sensitive to the precise details of the latent vector. If we only change
    it by a little bit, the output should only change by a little bit.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，在训练过程中，我们向潜在向量添加随机噪声。编码器将输入样本转换为潜在向量，然后我们在通过解码器之前随机微调一点，要求输出仍然尽可能接近原始样本。这可以防止解码器对潜在向量的精确细节过于敏感。如果我们只稍微改变一下，输出也应该只有轻微变化。
- en: 'These changes do a good job of improving the results. VAEs are a popular tool
    for generative modeling: they produce excellent results on many problems.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这些改变很好地改善了结果。VAE是生成建模的流行工具：它们在许多问题上产生出色的结果。
- en: Generative Adversarial Networks
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: A generative adversarial network (GAN) has much in common with a VAE. It uses
    the same exact decoder network to convert latent vectors into samples (except
    in a GAN, it is called the *generator* instead of the decoder). But it trains
    that network in a different way. It works by passing random vectors into the generator
    and directly evaluating the outputs on how well they follow the expected distribution.
    Effectively, you create a loss function to measure how well the generated samples
    match the training samples, then use that loss function to optimize the model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）与VAE有很多共同之处。它使用完全相同的解码器网络将潜在向量转换为样本（除了在GAN中，它被称为*生成器*而不是解码器）。但它以不同的方式训练该网络。它通过将随机向量传递到生成器中，并直接评估输出以确定它们如何符合预期分布。有效地，你创建一个损失函数来衡量生成的样本与训练样本匹配的程度，然后使用该损失函数来优化模型。
- en: That sounds simple for a few seconds, until you think about it and realize it
    isn’t simple at all. Could you write a loss function to measure how well an image
    resembles a cat? No, of course not! You wouldn’t know where to begin. So, instead
    of asking you to come up with that loss function yourself, a GAN learns the loss
    function from the data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来简单，只需几秒钟，直到你思考并意识到这根本不简单。你能写一个损失函数来衡量一幅图像多像一只猫吗？当然不行！你根本不知道从哪里开始。因此，不要求你自己想出那个损失函数，GAN从数据中学习损失函数。
- en: As shown in [Figure 9-2](#structure_of_a_generative_adversarial_network), a
    GAN consists of two parts. The generator takes random vectors and generates synthetic
    samples. The second part, called the *discriminator*, tries to distinguish the
    generated samples from real training samples. It takes a sample as input and outputs
    a probability that this is a real training sample. It acts as a loss function
    for the generator.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图9-2](#structure_of_a_generative_adversarial_network)所示，GAN由两部分组成。生成器接受随机向量并生成合成样本。第二部分称为*鉴别器*，试图区分生成的样本和真实训练样本。它以样本作为输入，并输出这是真实训练样本的概率。它充当生成器的损失函数。
- en: '![Structure of a generative adversarial network.](Images/dlls_0902.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![生成对抗网络的结构。](Images/dlls_0902.png)'
- en: Figure 9-2\. Structure of a generative adversarial network.
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-2。生成对抗网络的结构。
- en: Both parts are trained simultaneously. Random vectors are fed into the generator,
    and the output is fed into the discriminator. The parameters of the generator
    are adjusted to make the discriminator’s output as close as possible to 1, while
    the parameters of the discriminator are adjusted to make its output as close as
    possible to 0\. In addition, real samples from the training set are fed into the
    discriminator, and its parameters are adjusted to make the output close to 1.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 两个部分同时进行训练。随机向量被输入到生成器中，输出被输入到鉴别器中。生成器的参数被调整以使鉴别器的输出尽可能接近1，而鉴别器的参数被调整以使其输出尽可能接近0。此外，来自训练集的真实样本被输入到鉴别器中，并且其参数被调整以使输出接近1。
- en: This is the “adversarial” aspect. You can think of it as a competition between
    the generator and discriminator. The discriminator is constantly trying to get
    better at distinguishing real samples from fake ones. The generator is constantly
    trying to get better at fooling the discriminator.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这是“对抗”的方面。你可以将其看作是生成器和鉴别器之间的竞争。鉴别器不断尝试更好地区分真实样本和假样本。生成器不断尝试更好地欺骗鉴别器。
- en: Like VAEs, GANs are a popular type of generative model that produces good results
    on many problems. The two types of models have distinct strengths and weaknesses.
    Very roughly speaking, one might say that GANs tend to produce higher-quality
    samples, while VAEs tend to produce higher-quality distributions. That is, individual
    samples generated by a GAN will more closely resemble training samples, while
    the range of samples generated by a VAE will more closely match the range of training
    samples. Don’t take that statement too literally, though. It all depends on the
    particular problem and the details of the model. Also, countless variations on
    both approaches have been proposed. There even are models that combine a VAE with
    a GAN to try to get the best features of both. This is still a very active field
    of research, and new ideas are published frequently.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与VAEs一样，GANs是一种在许多问题上产生良好结果的流行生成模型。这两种类型的模型各有优势和劣势。非常粗略地说，可以说GANs倾向于产生更高质量的样本，而VAEs倾向于产生更高质量的分布。也就是说，GAN生成的个别样本更接近训练样本，而VAE生成的样本范围更接近训练样本的范围。不过，不要太字面理解这个说法。这完全取决于具体的问题和模型的细节。此外，对这两种方法的无数变体已经被提出。甚至有一些模型将VAE与GAN结合起来，试图获得两者的最佳特性。这仍然是一个非常活跃的研究领域，新的想法经常被发表。
- en: Applications of Generative Models in the Life Sciences
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生命科学中生成模型的应用
- en: Now that we’ve introduced you to the basics of deep generative models, let’s
    start talking about applications. Broadly speaking, generative models bring a
    few superpowers to the table. First, they allow for a semblance of “creativity.”
    New samples can be generated according to the learned distribution. This allows
    for a powerful complement to a creative process that can tie into existing efforts
    in drug or protein design. Second, being able to model complex systems accurately
    with generative models could allow scientists to build an understanding of complex
    biological processes. We’ll discuss these ideas in more depth in this section.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经向您介绍了深度生成模型的基础知识，让我们开始谈论应用。广义上说，生成模型为我们带来了一些超能力。首先，它们允许一种“创造性”的外观。可以根据学习到的分布生成新样本。这为创造性过程提供了一个强大的补充，可以与药物或蛋白质设计的现有工作联系起来。其次，能够准确地用生成模型建模复杂系统可能使科学家们建立对复杂生物过程的理解。我们将在本节中更深入地讨论这些想法。
- en: Generating New Ideas for Lead Compounds
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为引导化合物生成新想法
- en: A major part of a modern drug discovery effort is coming up with new compounds.
    This is mostly done semiannually, with expert human chemists suggesting modifications
    to core structures. Often, this will involve projecting a picture of the current
    molecular series on a screen and having a room full of senior chemists suggest
    modifications to the core structure of the molecule. Some subset of these suggested
    molecules are actually synthesized and tested, and the process repeats until a
    suitable molecule is found or the program is dropped. This process has powerful
    advantages since it can draw upon the deep intuition of expert chemists who may
    be able to identify flaws with a potential structure (perhaps it resembles a compound
    they’ve seen before which caused unexplained liver failure in rats) that may not
    be easy to identify algorithmically.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现代药物发现工作的一个重要部分是提出新的化合物。这通常是由专家人类化学家每隔半年提出对核心结构的修改来完成的。通常情况下，这将涉及在屏幕上投影当前分子系列的图片，并让一群资深化学家提出对分子的核心结构的修改。这些建议的分子中的一部分实际上被合成和测试，这个过程会重复直到找到合适的分子或者项目被放弃。这个过程具有强大的优势，因为它可以利用专家化学家的深刻直觉，他们可能能够识别潜在结构的缺陷（也许它类似于他们以前见过的一种化合物，导致大鼠出现不明原因的肝功能衰竭），这可能不容易通过算法来识别。
- en: At the same time, though, this process is very human-limited. There aren’t that
    many talented and experienced senior chemists in the world, so the process can’t
    scale outward. In addition, it makes it very challenging for a pharmaceutical
    division in a country that has historically lacked drug discovery expertise to
    bootstrap itself. A generative model of molecular structures could serve to overcome
    these limitations. If the model were trained on a suitable molecular representation,
    it might be able to rapidly suggest new alternative compounds. Access to such
    a model could help improve current processes by suggesting new chemical directions
    that may have been missed by human designers. It’s worth noting that such design
    algorithms have serious caveats, though, as we will see a little later in this
    chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，这个过程受到非常大的人类限制。世界上并没有那么多有才华和经验丰富的资深化学家，因此这个过程无法扩展。此外，对于一个历史上缺乏药物发现专业知识的国家的制药部门来说，这也使得自我启动变得非常具有挑战性。分子结构的生成模型可以克服这些限制。如果模型是在适当的分子表示上进行训练的，它可能能够快速提出新的替代化合物。获得这样的模型可以通过建议可能被人类设计师忽略的新化学方向来帮助改进当前的流程。值得注意的是，这样的设计算法也有严重的警告，我们稍后在本章中会看到。
- en: Protein Design
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蛋白质设计
- en: Design of new enzymes and proteins is a major business these days. Engineered
    enzymes are used widely in modern manufacturing. (There’s a good chance your laundry
    detergent holds some enzymes!) However, in general, design of new enzymes has
    proven challenging. Some early work has shown that deep models can have some success
    at predicting protein function from sequence. It’s not unreasonable at all to
    envision using deep generative models to suggest new protein sequences that might
    have desired properties.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 设计新的酶和蛋白质是当今一个重要的业务。工程酶在现代制造业中被广泛使用。（你的洗衣粉很可能含有一些酶！）然而，总体而言，设计新的酶一直是具有挑战性的。一些早期的工作表明，深度模型可以在一定程度上成功地从序列中预测蛋白质功能。使用深度生成模型来建议可能具有期望特性的新蛋白质序列并不是不合理的。
- en: The introduction of generative models for this purpose could be even more impactful
    than for small molecule design. Unlike with small molecules, it can be very tricky
    for human experts to predict the downstream effects of mutations to a given protein.
    Using generative models can allow for richer protein design, enabling directions
    beyond the capability of human experts today.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为此目的引入生成模型可能比小分子设计更具影响力。与小分子不同，人类专家很难预测对给定蛋白质的突变的下游影响。使用生成模型可以实现更丰富的蛋白质设计，使得今天人类专家无法做到的方向成为可能。
- en: A Tool for Scientific Discovery
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 科学发现的工具
- en: Generative models can be a powerful tool for scientific discovery. For example,
    having an accurate generative model of a [tissue development process](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6119234/)
    could be extremely valuable to developmental biologists or as a tool in basic
    science. It might be possible to create “synthetic assays” where we can study
    tissue development in many combinations of environmental conditions by using the
    generative model to run rapid simulations. This future is still a ways off, since
    we’d need generative models that work effectively as initial conditions change.
    This will take some more research beyond the current state of the art. Nevertheless,
    the vision is exciting because generative modeling could allow for biologists
    to build effective models of extremely complex developmental and physiological
    processes and test their hypotheses of how these systems evolve.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型可以成为科学发现的强大工具。例如，拥有一个准确的[tissue development process](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6119234/)的生成模型对发育生物学家或基础科学中的工具可能非常有价值。通过使用生成模型进行快速模拟，可能创建“合成测定”，从而可以研究在许多环境条件下的组织发育。这个未来还有一段距离，因为我们需要生成模型在初始条件改变时能够有效工作。这需要更多超越当前技术水平的研究。然而，这个愿景令人兴奋，因为生成建模可以让生物学家构建极其复杂的发育和生理过程的有效模型，并测试他们对这些系统演变方式的假设。
- en: The Future of Generative Modeling
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成建模的未来
- en: Generative models are challenging! The first GANs were only capable of generating
    blurry images that were barely recognizable as faces. The latest GANs (at the
    time of writing) are capable of generating images of faces that are more or less
    indistinguishable from true photographs. It is likely that in the next decade,
    these models will be further refined to allow for generative videos. These developments
    will have profound repercussions on modern societies. For much of the last century,
    photographs have been routinely used as “proof” of crimes, quality, and more.
    As generative tools develop, this standard of proof will fall short, as arbitrary
    images will be able to be “photoshopped.” This development will pose a major challenge
    for criminal justice and even international relations.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型具有挑战性！最初的GAN只能生成模糊的几乎无法识别为面孔的图像。最新的GAN（撰写时）能够生成几乎与真实照片无法区分的面孔图像。很可能在未来十年，这些模型将进一步完善，以实现生成视频。这些发展将对现代社会产生深远影响。在过去的大部分世纪里，照片一直被常规用作犯罪、质量等的“证据”。随着生成工具的发展，这种证据标准将不足以满足，因为任意图像都可以被“photoshopped”。这一发展将对刑事司法甚至国际关系构成重大挑战。
- en: At the same time, it’s likely that the advent of high-fidelity generative video
    will trigger a revolution in modern science. Imagine high-quality generative models
    of embryonic development! It might be feasible to model the effects of CRISPR
    genetic modifications or understand developmental processes in greater detail
    than has ever been possible. Improvements in generative models will have effects
    in other fields of science too. It’s likely that generative modeling will become
    a powerful tool in physics and climate science, allowing for more powerful simulations
    of complex systems. However, it’s worth emphasizing that these improvements today
    remain in the future; much basic science has to be done to mature these models
    to useful stability.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，高保真度的生成视频的出现可能会引发现代科学的革命。想象一下胚胎发育的高质量生成模型！可能可以模拟CRISPR基因修饰的效果，或者比以往任何时候都更详细地了解发育过程。生成模型的改进也将影响其他科学领域。生成建模可能成为物理学和气候科学中的强大工具，允许对复杂系统进行更强大的模拟。然而，值得强调的是，这些改进今天仍然留在未来；必须进行大量基础科学研究，以使这些模型成熟到有用的稳定性。
- en: Working with Generative Models
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用生成模型
- en: Now let’s work through a code example. We will train a VAE to generate new molecules.
    More specifically, it will output SMILES strings. This choice of representation
    has distinct advantages and disadvantages compared to some of the other representations
    we have discussed. On the one hand, SMILES strings are very simple to work with.
    Each one is just a sequence of characters drawn from a fixed alphabet. That allows
    us to use a very simple model to process them. On the other hand, SMILES strings
    are required to obey a complex grammar. If the model does not learn all the subtleties
    of the grammar, then most of the strings it produces will be invalid and not correspond
    to any molecule.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过一个代码示例来工作。我们将训练一个VAE来生成新的分子。更具体地说，它将输出SMILES字符串。与我们讨论过的其他表示相比，这种表示的选择具有明显的优势和劣势。一方面，SMILES字符串非常容易处理。每个字符串只是从固定字母表中提取的字符序列。这使我们可以使用非常简单的模型来处理它们。另一方面，SMILES字符串必须遵守复杂的语法。如果模型没有学会所有语法的微妙之处，那么它产生的大多数字符串将是无效的，不对应任何分子。
- en: 'The first thing we need is a collection of SMILES strings on which to train
    the model. Fortunately, MoleculeNet provides us with lots to choose from. For
    this example, we will use the MUV dataset. The training set includes 74,469 molecules
    of varying sizes and structures. Let’s begin by loading it:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的第一件事是一组SMILES字符串，用于训练模型。幸运的是，MoleculeNet为我们提供了很多选择。在这个示例中，我们将使用MUV数据集。训练集包括74,469个不同大小和结构的分子。让我们开始加载它：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we need to define the vocabulary our model will work with. What is the
    list of characters (or “tokens”) that can appear in a string? How long are strings
    allowed to be? We can determine these from the training data by creating a sorted
    list of every character that appears in any training molecule:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义模型将使用的词汇表。字符串中可以出现的字符（或“令牌”）列表是什么？字符串允许有多长？我们可以通过创建一个排序列表来确定这些，其中包含任何训练分子中出现的每个字符：
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now we need to create a model. What sort of architecture should we use for
    the encoder and decoder? This is an ongoing field of research. Various papers
    have been published suggesting different models. For this example, we will use
    DeepChem’s `AspuruGuzikAutoEncoder` class, which  implements a particular published
    model. It uses a convolutional network for the encoder and a recurrent network
    for the decoder. You can consult the [original paper](https://arxiv.org/abs/1610.02415)
    if you are interested in the details, but they are not necessary to follow the
    example. Also notice that we use `ExponentialDecay` for the learning rate. The
    rate is initially set to 0.001, then decreased by a little bit (multiplied by
    0.95) after every epoch. This helps optimization to proceed more smoothly in many
    problems:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要创建一个模型。我们应该为编码器和解码器使用什么样的架构？这是一个正在进行的研究领域。已经发表了各种论文，建议使用不同的模型。在这个例子中，我们将使用DeepChem的`AspuruGuzikAutoEncoder`类，它实现了一个特定的已发表模型。它使用卷积网络作为编码器和循环网络作为解码器。如果您对细节感兴趣，可以参考[原始论文](https://arxiv.org/abs/1610.02415)，但这并不是必要的。还要注意，我们使用`ExponentialDecay`来设置学习率。速率最初设置为0.001，然后在每个时代后稍微减少一点（乘以0.95）。这有助于在许多问题中更顺利地进行优化：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We are now ready to train the model. Instead of using the standard `fit()`
    method that takes a `Dataset`, `AspuruGuzikAutoEncoder` provides its own `fit_sequences()`
    method. It takes a Python generator object that produces sequences of tokens (SMILES
    strings in our case). Let’s train for 50 epochs:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备训练模型。不使用标准的`fit()`方法，该方法需要一个`Dataset`，`AspuruGuzikAutoEncoder`提供了自己的`fit_sequences()`方法。它接受一个Python生成器对象，该对象生成令牌序列（在我们的情况下是SMILES字符串）。让我们训练50个时代：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If everything has gone well, the model should now be able to generate entirely
    new molecules. We just need to pick random latent vectors and pass them through
    the decoder. Let’s create a batch of one thousand vectors, each of length 196
    (the size of the model’s latent space).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切顺利，模型现在应该能够生成全新的分子。我们只需要选择随机潜在向量并通过解码器传递它们。让我们创建一个批量包含一千个长度为196的向量的批次（模型潜在空间的大小）。
- en: 'As noted previously, not all outputs will actually be valid SMILES strings.
    In fact, only a small fraction of them are. Fortunately, we can easily use RDKit
    to check them and filter out the invalid ones:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，并非所有输出实际上都是有效的SMILES字符串。实际上，只有其中的一小部分是有效的。幸运的是，我们可以很容易地使用RDKit来检查它们并过滤掉无效的：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Analyzing the Generative Model’s Output
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析生成模型的输出
- en: 'In addition to the problem of invalid outputs, many of the molecules corresponding
    to the SMILES strings that are output may not be characteristic of drug molecules.
    So, we need to develop strategies that will enable us to quickly identify molecules
    that are not drug-like. These strategies can best be explained through a practical
    example. Let’s assume that this is the list of SMILES strings that came from our
    generative model:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 除了无效输出的问题外，与SMILES字符串对应的许多分子可能不具有药物分子的特征。因此，我们需要制定策略，使我们能够快速识别不符合药物样式的分子。这些策略最好通过一个实际例子来解释。假设这是来自我们生成模型的SMILES字符串列表：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The first step in our analysis will be to examine the molecules and determine
    whether there are any that we want to discard. We can use some of the facilities
    in RDKit, which is included as part of DeepChem, to examine the molecules represented
    by these strings. In order to evaluate the strings, we must first convert them
    to molecule objects. We can do this using the following list comprehension:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析的第一步将是检查分子，并确定是否有任何我们想要丢弃的分子。我们可以使用RDKit中的一些功能来检查这些字符串表示的分子。为了评估这些字符串，我们必须首先将它们转换为分子对象。我们可以使用以下列表推导来做到这一点：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'One factor we may want to examine is the size of the molecules. Molecules with
    fewer than 10 atoms are unlikely to generate sufficient interaction energy to
    produce a measurable signal in a biological assay. Conversely, molecules with
    more than 50 atoms may not be capable of dissolving in water and may create other
    problems in biological assays. We can get a rough estimate of the sizes of the
    molecules by calculating the number of non-hydrogen atoms in each molecule. The
    following code creates a list of the number of atoms in each molecule. For convenience,
    we sort the array so that we can more easily understand the distribution (if we
    had a larger list of molecules we would probably want to generate a histogram
    for this distribution):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能要检查的一个因素是分子的大小。少于10个原子的分子不太可能产生足够的相互作用能量，在生物测定中产生可测信号。相反，超过50个原子的分子可能无法溶解在水中，并可能在生物测定中产生其他问题。我们可以通过计算每个分子中非氢原子的数量来粗略估计分子的大小。以下代码创建了每个分子中原子数量的列表。为了方便起见，我们对数组进行排序，以便更容易理解分布（如果我们有一个更大的分子列表，我们可能会想要为此分布生成直方图）：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The results are as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can see that there are four very small molecules as well as two large molecules.
    We can use another list comprehension to remove molecules with 10 or fewer than
    50 atoms:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到有四个非常小的分子以及两个大分子。我们可以使用另一个列表推导来删除具有10个或少于50个原子的分子：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This list comprehension reduces our previous list of 29 molecules to 23.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个列表推导将我们之前的29个分子减少到23个。
- en: In practice, we can use a number of other calculated properties to evaluate
    the quality of the generated molecules. Several recent generative model publications
    use calculated molecular properties to determine which of the generated molecules
    to retain or discard. One of the more common methods for determining whether molecules
    are similar to known drugs, or “drug-like,” is known as the quantitative estimate
    of drug-likeness (QED). The QED metric, which was originally published by Bickerton
    and coworkers,^([1](ch09.xhtml#idm45806164836136)) scores molecules by comparing
    a set of properties calculated for each molecule with distributions of the same
    properties in marketed drugs. This score ranges between 0 and 1, with values closer
    to 1 being considered more drug-like.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们可以使用许多其他计算属性来评估生成的分子的质量。一些最近的生成模型出版物使用计算的分子属性来确定哪些生成的分子保留或丢弃。确定分子是否类似于已知药物或“药物样品”的更常见方法之一被称为药物样品的定量估计（QED）。QED指标最初由Bickerton和同事发表，通过比较为每个分子计算的一组属性与市场上药物中相同属性的分布来评分分子。该分数范围在0到1之间，值越接近1被认为更像药物。
- en: 'We can use RDKit to calculate QED values for our remaining molecules and retain
    only those molecules with QED > 0.5 as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用RDKit来计算剩余分子的QED值，并仅保留那些QED > 0.5的分子如下：
- en: '[PRE10]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As our final step, we can visualize the chemical structures of `final_mol_list`
    and the corresponding QED scores:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，我们可以可视化`final_mol_list`的化学结构和相应的QED分数：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The results are shown in [Figure 9-3](#chem-structures-qed).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在[图9-3](#chem-structures-qed)中。
- en: '![](Images/dlls_0903.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlls_0903.png)'
- en: Figure 9-3\. Chemical structures of the generated molecules along with their
    QED scores.
  id: totrans-70
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-3. 生成分子的化学结构以及它们的QED分数。
- en: While these structures are valid and have reasonably high QED scores, they still
    contain functionality that may be chemically unstable. Strategies for identifying
    and removing problematic molecules like these are discussed in the next section.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些结构是有效的，并且具有相当高的QED分数，但它们仍然包含可能不稳定的官能团。如何识别和删除这些问题分子的策略将在下一节中讨论。
- en: Conclusion
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: While generative models provide an interesting means of producing ideas for
    new molecules, some key issues still need to be resolved to ensure their general
    applicability. The first is ensuring that the generated molecules will be chemically
    stable and that they can be physically synthesized. One current method to assess
    the quality of molecules produced by a generative model is to observe the fraction
    of the generated molecules that obey standard rules of chemical valence—in other
    words, ensuring that each carbon atom has four bonds, each oxygen atom has two
    bonds, each fluorine atom has one bond, and so on. These factors become especially
    important when decoding from a latent space with a SMILES representation. While
    a generative model may have learned the grammar of SMILES, there may be nuances
    that are still missing.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然生成模型提供了一个有趣的方法来产生新分子的想法，但仍然需要解决一些关键问题以确保它们的普遍适用性。第一个问题是确保生成的分子将是化学稳定的，并且它们可以被物理合成。目前评估由生成模型产生的分子质量的一种方法是观察遵守化学价标准规则的生成分子的比例，换句话说，确保每个碳原子有四个键，每个氧原子有两个键，每个氟原子有一个键，依此类推。当从具有SMILES表示的潜在空间解码时，这些因素变得尤为重要。虽然生成模型可能已经学会了SMILES的语法，但仍可能存在一些细微差别。
- en: The fact that a molecule obeys standard rules of valence does not necessarily
    ensure that it will be chemically stable. In some cases, a generative model may
    produce molecules containing functional groups that are known to readily decompose.
    As an example, consider the molecule in [Figure 9-4](#a_molecule_containing_an_unstable_group).
    The functional group highlighted in the circle, known as a hemiacetal, is known
    to readily decompose.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 分子遵守化学价标准规则并不一定能确保其在化学上稳定。在某些情况下，生成模型可能会产生包含已知易分解的官能团的分子。例如，考虑[图9-4](#a_molecule_containing_an_unstable_group)中的分子。圈中突出显示的官能团称为半缩醛，已知易分解。
- en: '![](Images/dlls_0904.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/dlls_0904.png)'
- en: Figure 9-4\. A molecule containing an unstable group.
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图9-4. 包含不稳定官能团的分子。
- en: In practice, the probability of this molecule existing and being chemically
    stable is very small. There are dozens of chemical functionalities like this which
    are known to be unstable or reactive. When synthesizing molecules in a drug discovery
    project, medicinal chemists know to avoid introducing these functional groups.
    One way of imparting this sort of “knowledge” to a generative model is to provide
    a set of filters that can be used to postprocess the model output and remove molecules
    that may be problematic. In [Chapter 11](ch11.xhtml#a_virtual_screening_workflow_example),
    we will provide a further discussion of some of these filters and how they are
    used in virtual screening. Many of the same techniques used to identify potentially
    problematic screening compounds can also be used to evaluate virtual molecules
    that are created by a generative model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这种分子存在并且在化学上稳定的概率非常小。有数十种类似的化学官能团是已知不稳定或具有反应性的。在药物发现项目中合成分子时，药物化学家知道要避免引入这些官能团。向生成模型提供一组过滤器的一种方式是将这种“知识”传达给生成模型，这些过滤器可用于后处理模型输出并删除可能存在问题的分子。在[第11章](ch11.xhtml#a_virtual_screening_workflow_example)中，我们将进一步讨论其中一些过滤器以及它们在虚拟筛选中的使用方式。用于识别潜在问题筛选化合物的许多相同技术也可用于评估由生成模型创建的虚拟分子。
- en: In order to test the biological activity of a molecule produced by a generative
    model, that molecule must first be synthesized by a chemist. The science of organic
    chemical synthesis has a rich history going back more than one hundred years.
    In this time, chemists have developed thousands of chemical reactions to synthesize
    drugs and drug-like molecules. The synthesis of a drug-like molecule typically
    requires somewhere between 5 and 10 chemical reactions, often referred to as “steps.”
    While some drug-like molecules can be readily synthesized, the synthetic route
    to more complex drug molecules may require more than 20 steps. Despite more than
    50 years of work on automating the planning of organic syntheses, much of the
    process is still driven by human intuition followed by trial and error.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试由生成模型产生的分子的生物活性，该分子必须首先由化学家合成。有机化学合成科学有着超过一百年的丰富历史。在这段时间里，化学家们已经开发了成千上万种化学反应来合成药物和类药物分子。合成类药物分子通常需要大约5到10个化学反应，通常被称为“步骤”。虽然一些类药物分子可以很容易地合成，但对于更复杂的药物分子，合成路线可能需要超过20个步骤。尽管在自动化有机合成规划方面已经有50多年的工作，但很多过程仍然是由人类直觉和试错驱动的。
- en: Fortunately, recent developments in deep learning are providing new ways of
    planning the synthesis of drug-like molecules. A number of groups have published
    methods that use deep learning to propose routes that can be used to synthesize
    molecules. As input, the model is given a molecule, often referred to as a *product*,
    and the set of steps that were used to synthesize that molecule. By training with
    thousands of product molecules and the steps used for synthesis, a deep neural
    network is able to learn the relationship between product molecules and reaction
    steps. When presented with a new molecule, the model suggests a set of reactions
    that could be used to synthesize the molecule. In one test, the synthetic routes
    produced by these models were presented to human chemists for evaluation. These
    evaluators felt that the routes generated by the models were comparable in quality
    to routes generated by human chemists.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，深度学习的最新发展提供了规划药物样分子合成的新方法。许多团体已经发表了使用深度学习提出合成分子的路线的方法。模型的输入是一个分子，通常被称为*产物*，以及用于合成该分子的步骤集。通过使用数千个产物分子和用于合成的步骤进行训练，深度神经网络能够学习产物分子和反应步骤之间的关系。当给出一个新分子时，模型会建议一组可用于合成该分子的反应。在一个测试中，这些模型产生的合成路线被呈现给人类化学家进行评估。这些评估者认为模型生成的路线在质量上与人类化学家生成的路线相当。
- en: The application of deep learning to organic synthesis is a relatively new field.
    It is hoped that the field will continue to evolve and that these models become
    an important tool for organic chemists. One can imagine a day in the not too distant
    future where these synthesis planning capabilities could be paired with robotic
    automation to create a fully automated platform. However, there are difficulties
    to overcome.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将深度学习应用于有机合成是一个相对较新的领域。希望这个领域将继续发展，并且这些模型将成为有机化学家的重要工具。人们可以想象，在不久的将来，这些合成规划能力可以与机器自动化相结合，创建一个完全自动化的平台。然而，还有一些困难需要克服。
- en: One potential roadblock in the broad adoption of deep learning in organic synthesis
    is data availability. The majority of the information used to train these models
    is in databases which are the property of a small number of organizations. If
    these organizations decide to only utilize this data for their internal efforts,
    the field will be left with very few alternatives.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在有机合成中广泛采用深度学习的一个潜在障碍是数据的可用性。用于训练这些模型的大部分信息都在数据库中，这些数据库是少数几个组织的财产。如果这些组织决定仅将这些数据用于他们的内部工作，那么该领域将只剩下极少的选择。
- en: Another factor that may limit the advance of generative models is the quality
    of the predictive models that are used to drive molecule generation. Regardless
    of the architecture used to develop a generative model, some function must be
    used to evaluate the generated molecules and to direct the search for new molecules.
    In some cases, we may be able to develop reliable predictive models. In other
    cases, the models may be less reliable. While we can test our models on external
    validation sets, it is often difficult to determine the scope of a predictive
    model. This scope, also known as the “domain of applicability,” is the degree
    to which one can extrapolate outside the molecules on which a model was trained.
    This applicability domain is not well defined, so it may be difficult to determine
    how well a model will work on novel molecules produced by a generative model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能限制生成模型发展的因素是用于驱动分子生成的预测模型的质量。无论用何种架构开发生成模型，都必须使用某种函数来评估生成的分子并指导寻找新分子。在某些情况下，我们可能能够开发可靠的预测模型。在其他情况下，模型可能不太可靠。虽然我们可以在外部验证集上测试我们的模型，但通常很难确定预测模型的范围。这个范围，也称为“适用领域”，是一个模型在训练时可以推断出的分子范围。这种适用领域并不明确定义，因此很难确定模型在生成模型产生的新分子上的工作效果如何。
- en: Generative models are a relatively new technique, and it will be interesting
    to see how this field evolves in the coming years. As our ability to use deep
    learning to predict routes for organic synthesis and build predictive models improves,
    the power of generative models will continue to grow.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型是一种相对较新的技术，有趣的是看到这个领域在未来几年如何发展。随着我们利用深度学习来预测有机合成路线和构建预测模型的能力不断提高，生成模型的威力将继续增长。
- en: ^([1](ch09.xhtml#idm45806164836136-marker)) Bickerton, Richard G. et al. “Quantifying
    the Chemical Beauty of Drugs.” [*http://dx.doi.org/10.1038/nchem.1243*](http://dx.doi.org/10.1038/nchem.1243).
    2012.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '[1] Bickerton, Richard G.等人。“量化药物的化学美。”[*http://dx.doi.org/10.1038/nchem.1243*](http://dx.doi.org/10.1038/nchem.1243)。2012.'
