- en: Chapter 5\. Customizing PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, you have been using built-in PyTorch classes, functions, and libraries
    to design and train various predefined models, model layers, and activation functions.
    But what if you have a novel idea or you’re conducting cutting-edge deep learning
    research? Perhaps you’ve invented a totally new layer architecture or activation
    function. Maybe you’ve developed a new optimization algorithm or a special loss
    function that no one’s ever seen before.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I’ll show you how to create your own custom deep learning components
    and algorithms in PyTorch. We’ll begin by exploring how to create custom layers
    and activation functions, and then we’ll see how to combine these components into
    custom model architectures. Next, I’ll show you how to create your own loss functions
    and optimizer algorithms. Finally, we’ll look at how to create custom loops for
    training, validation, and testing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch offers flexibility: you can extend an existing library or you can combine
    your customizations into your own library or package. By creating custom components
    you can solve new deep learning problems, speed up training, and discover innovative
    ways to perform deep learning.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started by creating some custom deep learning layers and activation
    functions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Custom Layers and Activations
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch offers an extensive set of built-in layers and activation functions.
    However, what makes PyTorch so popular, especially in the research community,
    is how easy it is to create custom layers and activations. The ability to do so
    can facilitate experimentation and accelerate your research.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: If we take a look at the PyTorch source code, we’ll see that layers and activations
    are created using a functional definition and a class implementation. The *functional
    definition* specifies how the outputs are created based on the inputs. It is defined
    in the `nn.functional` module. The *class implementation* is used to create an
    object that calls this function at its core, but it also includes added features
    derived from the `nn.Module` class.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s look at how the fully connected `nn.Linear` layer is implemented.
    The following code shows a simplified version of the functional definition, `nn.functional.linear()`:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `linear()` function multiplies the input tensor by the weight matrix, optionally
    adds the bias vector, and returns the results in a tensor. You can see that the
    code is optimized for performance. When the input has two dimensions and there
    is no bias, you should use the fused-matrix add function, `torch.addmm()`, because
    it’s faster in this case.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Keeping the mathematical computations in a separate functional definition has
    the benefit of keeping optimizations separate from the layer `nn.Module`. The
    functional definitions can also be used as standalone functions when writing code
    in general.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we’ll often use the `nn.Module` class to subclass our NNs. When we
    create an `nn.Module` subclass, we gain all the built-in benefits of the `nn.Module`
    object. In this case, we derive the `nn.Linear` class from `nn.Module`, as shown
    in the following code:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO1-1)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Initialize input and output sizes, weights, and biases.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO1-2)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Define the forward pass.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_customizing_pytorch_CO1-3)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Use the functional definition of `linear()`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: The `nn.Linear` code includes two necessary methods for any `nn.Module` subclass.
    One is `__init__()`, which initializes the class attributes, namely the inputs,
    outputs, weights, and biases in this case. The other is the `forward()` method,
    which defines the processing during the forward pass.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding code, the `forward()` method often calls the
    `nn.functional` definition associated with the layer. This convention is used
    often in PyTorch code for layers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: The convention for creating a custom layer is to first create a function that
    implements the mathematical operations and then create an `nn.Module` subclass
    that uses this function to implement the layer class. Using this approach makes
    it very easy to experiment with new layer designs in your PyTorch model development.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 创建自定义层的约定是首先创建一个实现数学运算的函数，然后创建一个`nn.Module`子类，该子类使用这个函数来实现层类。使用这种方法可以很容易地在PyTorch模型开发中尝试新的层设计。
- en: Custom Layer Example (Complex Linear)
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义层示例（复杂线性）
- en: Next, we’ll look at how to create a custom layer. In this example, we will create
    our own linear layer for a special type of number called a *complex number*. Complex
    numbers are often used in physics and signal processing and consist of a pair
    of numbers—a “real” component and an “imaginary” component. Both components are
    just floating-point numbers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看如何创建一个自定义层。在这个例子中，我们将为一种特殊类型的数字——*复数*创建自己的线性层。复数经常在物理学和信号处理中使用，由一对数字组成——一个“实”部分和一个“虚”部分。这两个部分都是浮点数。
- en: PyTorch is adding support for complex data types; however, they are still in
    beta at the time of writing of this book. Therefore, we’ll implement them using
    two floating-point tensors, one for the real components and one for the imaginary
    components.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch正在添加对复杂数据类型的支持；然而，在撰写本书时，它们仍处于测试阶段。因此，我们将使用两个浮点张量来实现它们，一个用于实部，一个用于虚部。
- en: 'In this case, the inputs, weights, biases, and outputs will all be complex
    numbers and will consist of two tensors instead of one. Complex multiplication
    yields the following equation (where *j* is the complex number <math alttext="StartRoot
    1 EndRoot"><msqrt><mn>1</mn></msqrt></math> ):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，输入、权重、偏置和输出都将是复数，并且将由两个张量组成，而不是一个。复数乘法得到以下方程（其中 *j* 是复数 <math alttext="StartRoot
    1 EndRoot"><msqrt><mn>1</mn></msqrt></math>）：
- en: <math><mrow><mrow><mo>(</mo> <mi>i</mi> <msub><mi>n</mi> <mi>r</mi></msub> <mo>+</mo>
    <mi>i</mi> <msub><mi>n</mi> <mi>i</mi></msub> <mo>*</mo> <mi>j</mi> <mo>)</mo></mrow>
    <mo>*</mo> <mrow><mo>(</mo> <msub><mi>w</mi> <mi>r</mi></msub> <mo>+</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>*</mo> <mi>j</mi> <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo>
    <msub><mi>b</mi> <mi>r</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>i</mi></msub>
    <mo>*</mo> <mi>j</mi> <mo>)</mo></mrow> <mo>=</mo> <mrow><mo>(</mo> <mi>i</mi>
    <msub><mi>n</mi> <mi>r</mi></msub> <mo>*</mo> <msub><mi>w</mi> <mi>r</mi></msub>
    <mo>-</mo> <mi>i</mi> <msub><mi>n</mi> <mi>i</mi></msub> <mo>*</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>r</mi></msub> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mo>(</mo> <mi>i</mi> <msub><mi>n</mi> <mi>r</mi></msub> <mo>*</mo>
    <msub><mi>w</mi> <mi>i</mi></msub> <mo>+</mo> <mi>i</mi> <msub><mi>n</mi> <mi>i</mi></msub>
    <mo>*</mo> <msub><mi>w</mi> <mi>r</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>*</mo> <mi>j</mi></mrow></math>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mo>(</mo> <mi>i</mi> <msub><mi>n</mi> <mi>r</mi></msub> <mo>+</mo>
    <mi>i</mi> <msub><mi>n</mi> <mi>i</mi></msub> <mo>*</mo> <mi>j</mi> <mo>)</mo></mrow>
    <mo>*</mo> <mrow><mo>(</mo> <msub><mi>w</mi> <mi>r</mi></msub> <mo>+</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>*</mo> <mi>j</mi> <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo>
    <msub><mi>b</mi> <mi>r</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>i</mi></msub>
    <mo>*</mo> <mi>j</mi> <mo>)</mo></mrow> <mo>=</mo> <mrow><mo>(</mo> <mi>i</mi>
    <msub><mi>n</mi> <mi>r</mi></msub> <mo>*</mo> <msub><mi>w</mi> <mi>r</mi></msub>
    <mo>-</mo> <mi>i</mi> <msub><mi>n</mi> <mi>i</mi></msub> <mo>*</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>r</mi></msub> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mo>(</mo> <mi>i</mi> <msub><mi>n</mi> <mi>r</mi></msub> <mo>*</mo>
    <msub><mi>w</mi> <mi>i</mi></msub> <mo>+</mo> <mi>i</mi> <msub><mi>n</mi> <mi>i</mi></msub>
    <mo>*</mo> <msub><mi>w</mi> <mi>r</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>*</mo> <mi>j</mi></mrow></math>
- en: 'First we will create a functional version of our complex linear layer, as shown
    in the following code:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个复杂线性层的函数版本，如下面的代码所示：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As you can see, the function applies the complex multiplication formula to
    tensor arrays. Next we create our class version of `ComplexLinear` based on the
    `nn.Module`, as shown in the following code:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，该函数将复杂乘法公式应用于张量数组。接下来，我们根据`nn.Module`创建`ComplexLinear`的类版本，如下面的代码所示：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In our class, we define separate weights and biases for the real and imaginary
    components in our `__init__()` function. Note that the options for the number
    of `in_features` and `out_features` do not change because the number of real and
    imaginary components are the same. Our `forward()` function simply calls the functional
    definition of our complex multiply and add operation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的类中，我们在`__init__()`函数中为实部和虚部定义了单独的权重和偏置。请注意，`in_features`和`out_features`的选项数量不会改变，因为实部和虚部的数量是相同的。我们的`forward()`函数只是调用我们的复杂乘法和加法操作的函数定义。
- en: 'Note that we could also use PyTorch’s existing `nn.Linear` layer to build our
    layer, as shown in the following code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们也可以使用PyTorch现有的`nn.Linear`层来构建我们的层，如下面的代码所示：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this code, we get all the added benefits from `nn.Linear` for free, and we
    do not need to implement a new functional definition. When you create your own
    custom layers, check PyTorch’s built-in layers to see if you can reuse existing
    classes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们可以免费获得`nn.Linear`的所有附加好处，而无需实现新的函数定义。当你创建自己的自定义层时，检查PyTorch的内置层，看看是否可以重用现有的类。
- en: Even though this example was pretty simple, you can use the same approach to
    create more complex layers. In addition, the same approach can also be used to
    create custom activation functions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这个例子非常简单，你也可以使用相同的方法来创建更复杂的层。此外，相同的方法也可以用来创建自定义激活函数。
- en: Activation functions are very similar to NN layers in that they return outputs
    by performing mathematical operations on a set of inputs. They differ in that
    the operations are performed element-wise, and they do not include parameters
    like weights and biases that are adjusted during training. For this reason, activation
    functions can be performed solely with functional versions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数与NN层非常相似，它们通过对一组输入执行数学运算来返回输出。它们的不同之处在于，操作是逐元素执行的，并且不包括在训练过程中调整的权重和偏置等参数。因此，激活函数可以仅使用函数版本执行。
- en: 'For example, let’s take a look at the ReLU activation function. The ReLU function
    is zero for negative values and linear for positive values:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When the activation function has configurable parameters, it’s common to create
    a class version of it. We can add the capability to adjust the threshold and value
    of the ReLU function by creating a ReLU class, as shown in the following code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When building an NN, it is common to use the functional version of the activation
    function, but a class version can also be used if available. The following code
    snippets show how to use both versions of the ReLU activation included in `torch.nn`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the functional version:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO2-1)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: A common way to import the functional package.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO2-2)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The functional version of ReLU is used here.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the class version:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO3-1)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: We are using `nn.Sequential()` since all components are classes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO3-2)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: We are using the class version of ReLU.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Custom Activation Example (Complex ReLU)
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can create our own custom ComplexReLU activation function to handle complex
    values from the `ComplexLinear` layer that we created earlier. The following code
    shows the functional and class versions:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO4-1)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Functional version
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO4-2)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Class version
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve learned how to create your own layers and activations, let’s
    see how you can create your own custom model architectures.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Custom Model Architectures
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Chapters [2](ch02.xhtml#Chapter_2) and [3](ch03.xhtml#deep_learning_development_with_pytorch),
    we used built-in models and created our own models from built-in PyTorch layers.
    In this section, we’ll explore how you can create a library of models similar
    to `torchvision.models` and build flexible model classes that adjust the architecture
    based on configuration parameters provided by the user.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'The `torchvision.models` package provides an `AlexNet` model class and an `alexnet()`
    convenience function to facilitate its use. Let’s look at the `AlexNet` class
    first:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Like all layers, activations, and models, the `AlexNet` class is derived from
    the `nn.Module` class. The `AlexNet` class is a good example of how to create
    and combine submodules into an NN.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: The library defines three subnetworks—`features`, `avgpool`, and `classifier`.
    Each subnetwork is made up of PyTorch layers and activations, and they are connected
    in sequence. AlexNet’s `forward()` function describes the forward pass; that is,
    how the inputs are processed to form the outputs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the PyTorch `torchvision.models` code provides a convenience
    function called `alexnet()` to instantiate or create the model with some options.
    The options here are `pretrained` and `progress`; they determine whether to load
    the model with pretrained parameters and whether to display a progress bar:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `**kwargs` parameter allows you to pass additional options to the AlexNet
    model. In this case, you can change the number of classes to 10 with `alexnet(n_classes
    = 10)`. The function will instantiate the AlexNet model with `n_classes = 10`
    and return the model object. If `pretrained` is `True`, the function will load
    the weights from the specified URL.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: By following a similar approach, you can create your own model architectures.
    Create a top-level model that is derived from `nn.Module`. Define your `__init__()`
    and `forward()` functions and implement your NN based on subnetworks, layers,
    and activations. Your subnetworks, layers, and activations can even be custom
    ones that you created yourself.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the `nn.Module` class makes creating custom models easy. In
    addition to the `Module` class, the `torch.nn` package includes built-in loss
    functions. Let’s take a look at how you can create your own loss functions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Custom Loss Functions
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you recall from [Chapter 3](ch03.xhtml#deep_learning_development_with_pytorch),
    before we can train our NN model, we need to define our loss function. The loss
    function, or cost function, defines a metric which we wish to minimize by adjusting
    the weights of our model during training.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: At first it might appear that the loss function is just a functional definition,
    but remember, the loss function is a function of the parameters of the NN module.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, loss functions actually behave like an extra layer that takes the
    NN outputs as inputs and produces a metric as its output. When we perform backpropagation,
    we’re performing backpropagation on the loss function, not the NN.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'This allows us to call the class directly to compute the loss given the NN
    outputs and true values. Then we can compute the gradients of all the NN parameters
    in one call, namely to perform backpropagation. The following code shows how this
    can be implemented in code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO5-1)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes called `criterion`
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: First we instantiate the loss function, itself and then we call the function
    passing in the outputs (from our model) and the targets (from our data). Finally,
    we call the `backward()` method to perform backpropagation and compute the gradients
    of all the model parameters with respect to the loss.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Similar to layers discussed earlier, loss functions are implemented using a
    functional definition and a class implementation derived from the `nn.Module`
    class.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Simplified versions of the functional definition and the class implementations
    for `mse_loss` are shown in the following code:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s create our own loss function, MSE Loss for Complex Numbers. To create
    our own custom loss function, we’ll first define a functional definition that
    describes the loss function mathematically. Then we’ll create the loss function
    class, as shown in the following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This time, we created an optional setting in the class called `real_only`. When
    we instantiate the loss function with `real_only = True`, the functional `mse_loss()`
    will be used instead of `complex_mse_loss()`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, PyTorch offers exceptional flexibility in building custom model
    architectures and loss functions. Before we get to training, there’s one more
    function you can customize: the optimizer. Let’s see how you can create custom
    optimizers.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Custom Optimizer Algorithms
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The optimizer plays an important part in training your NN model. An optimizer
    is an algorithm that updates the model’s parameters during training. When we perform
    backpropagation using `loss.backward()`, we determine whether the parameters should
    be increased or decreased to minimize the loss. The optimizer uses the gradients
    to determine how much the parameters should be changed during each step and changes
    them.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch has its own submodule called `torch.optim` that contains many built-in
    optimizer algorithms, as we saw in [Chapter 3](ch03.xhtml#deep_learning_development_with_pytorch).
    To create an optimizer, we pass in our model’s parameters and any optimizer-specific
    options. For example, the following code creates an SGD optimizer with a learning
    rate of 0.01 and momentum value of 0.9:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In PyTorch we can also specify different options for different parameters.
    This is useful when you want to specify different learning rates for the different
    layers of your model. Each set of parameters is called a parameter group. We can
    specify different options using dictionaries, as shown in the following code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Assuming we are using the AlexNet model, the preceding code sets the learning
    rate to `1e-3` for the classifier layer and uses the default learning rate of
    `1e-2` for the features layer.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch provides a `torch.optim.Optimizer` base class to make it easy to create
    your own custom optimizers. Here is a simplified version of the `Optimizer` base
    class:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO6-1)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Define `state` as needed.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO6-2)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Define `param_groups` as needed.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_customizing_pytorch_CO6-3)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Define `zero_grad()` as needed.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_customizing_pytorch_CO6-4)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need to write your own `step()`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main attributes or components to the optimizer: `state` and `param_groups`.
    The `state` atribute is a dictionary that can vary across different optimizers.
    It is mainly used to maintain values between each call to the `step()` function.
    The `param_groups` attribute is also a dictionary. It contains the parameters
    themselves and the associated options for each group.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The important methods in the `Optimizer` base class are `zero_grad()` and `step()`.
    The `zero_grad()` method is used to zero or reset the gradients during each training
    iteration. The `step()` method is used to execute the optimizer algorithm, compute
    the change for each parameter, and update the parameters within the model object.
    The `zero_grad()` method is already implemented for you. However, you must create
    your own `step()` method when creating your custom optimizer.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s demonstrate the process by creating our own simple version of SGD. Our
    SDG optimizer will have one option—the learning rate (LR). During each optimizer
    step we will multiply the gradient by the LR and add it to the parameter (i.e.,
    adjust the model’s weights):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `__init__()` function sets the default option values and initializes the
    parameter groups based on the input parameters. Notice that we don’t have to write
    any code to do this since `super(SGD, self).init(params, defaults)` invokes the
    base class initialization method. All we really need to do is write the `step()`
    method. For each parameter group, we update the parameters by first multiplying
    the parameters by the group’s LR and then subtracting the product from the parameter
    itself. This is accomplished by calling `p.add_(d_p, alpha=-group['lr'])`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how we would use our new optimizer:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We could also define a different LR for different layers in the model using
    the following code. Here we assume we’re using AlexNet again as the model with
    layers called `feature` and `classifier`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now that you can create your own optimizers for training your models, let’s
    see how you can create your own custom training, validation, and test loops.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Custom Training, Validation, and Test Loops
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All through this book, we’ve been using custom training, validation, and test
    loops. That’s because in PyTorch all training, validation, and test loops are
    manually created by the programmer.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Unlike in Keras, there’s no `fit()` or `eval()` method that exercises a loop.
    Instead, PyTorch requires that you write your own loops. This is actually a benefit
    in many cases because you’d like to control what happens during training.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the reference design in [“Generative Learning—Generating Fashion-MNIST
    Images with DCGAN”](ch04.xhtml#gan_example_heading) demonstrates how you can create
    a more complex training loop.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we’ll explore a conventional way of writing loops and discuss
    common ways that developers customize their loops. Let’s review some code commonly
    used for training, validation, and testing loops:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This code should look familiar since we’ve used it often throughout the book.
    We assume that `n_epochs`, `model`, `criterion`, `optimizer`, and `train_`, `val_`,
    and `test_dataloader` have already been defined. For each epoch, we perform the
    training and validation loops. The training loop processes each batch one at a
    time, sends the batch input through the model, and computes the loss. We then
    perform backpropagation to compute the gradients and execute the optimizer to
    update the model’s parameters.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The validation loop disables the gradient computation and passes the validation
    data through the network one batch at a time. The test loop passes the test data
    through the model one batch at a time and computes the loss for the test data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add some additional capabilities to our loops. The possibilities are
    endless, but this example will demonstrate some simple tasks like printing information,
    reconfiguring a model, and adjusting a hyperparameter in the middle of training.
    Let’s walk through the following code to see how this is done:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO7-1)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Examples of printing epoch, training, and validation loss
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO7-3)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Examples of reconfiguring a model (best practice)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_customizing_pytorch_CO7-4)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Example of modifying a hyperparameter during training
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we added some variables to keep track of the running
    training and validation loss and we printed them for every epoch. Next we use
    the `train()` or `eval()` method to configure the model for training or evaluation,
    respectively. This only applies if the model’s `forward()` function behaves differently
    for training and evaluation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: For example, some models may use dropout during training, but dropout should
    not be applied during validation or testing. In this case, we can reconfigure
    the model by calling `model.train()` or `model.eval()` before its execution.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we modified the LR in our optimizer halfway through training. This enables
    us to train at a faster rate at first while fine-tuning our parameter updates
    after training on half of the epochs.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: This example is a simple demonstration of how to customize your loops. Training,
    validation, and testing loops can be more complex as you train multiple networks
    simultaneously, use multimodal data, or design more complex networks that can
    even train other networks. PyTorch offers the flexibility to design special and
    innovative processes for training, validation, and testing.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PyTorch Lightning is a third-party PyTorch package that provides boilerplate
    templates for training, validation, and testing loops. The package provides a
    framework that allows you to create customized loops without having to repeatedly
    type the boilerplate code for each model implementation. We’ll discuss PyTorch
    Lightning in [Chapter 8](ch08.xhtml#pytorch_ecosystem_and_additional_resources).
    You can also find more information at [the PyTorch Lightning website](http://pytorch.tips/pytorch-lightning).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned how to create your own custom components for developing
    deep learning models in PyTorch. As your models grow more and more complex, you
    may find that the time you need to train your model may become quite long—perhaps
    days or even weeks. In the next chapter, you’ll see how to use built-in PyTorch
    capabilities to accelerate and optimize your training process to significantly
    reduce your overall model development time.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
