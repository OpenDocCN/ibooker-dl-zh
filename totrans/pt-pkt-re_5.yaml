- en: Chapter 5\. Customizing PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up until now, you have been using built-in PyTorch classes, functions, and libraries
    to design and train various predefined models, model layers, and activation functions.
    But what if you have a novel idea or you’re conducting cutting-edge deep learning
    research? Perhaps you’ve invented a totally new layer architecture or activation
    function. Maybe you’ve developed a new optimization algorithm or a special loss
    function that no one’s ever seen before.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, I’ll show you how to create your own custom deep learning components
    and algorithms in PyTorch. We’ll begin by exploring how to create custom layers
    and activation functions, and then we’ll see how to combine these components into
    custom model architectures. Next, I’ll show you how to create your own loss functions
    and optimizer algorithms. Finally, we’ll look at how to create custom loops for
    training, validation, and testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch offers flexibility: you can extend an existing library or you can combine
    your customizations into your own library or package. By creating custom components
    you can solve new deep learning problems, speed up training, and discover innovative
    ways to perform deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started by creating some custom deep learning layers and activation
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Custom Layers and Activations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PyTorch offers an extensive set of built-in layers and activation functions.
    However, what makes PyTorch so popular, especially in the research community,
    is how easy it is to create custom layers and activations. The ability to do so
    can facilitate experimentation and accelerate your research.
  prefs: []
  type: TYPE_NORMAL
- en: If we take a look at the PyTorch source code, we’ll see that layers and activations
    are created using a functional definition and a class implementation. The *functional
    definition* specifies how the outputs are created based on the inputs. It is defined
    in the `nn.functional` module. The *class implementation* is used to create an
    object that calls this function at its core, but it also includes added features
    derived from the `nn.Module` class.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s look at how the fully connected `nn.Linear` layer is implemented.
    The following code shows a simplified version of the functional definition, `nn.functional.linear()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The `linear()` function multiplies the input tensor by the weight matrix, optionally
    adds the bias vector, and returns the results in a tensor. You can see that the
    code is optimized for performance. When the input has two dimensions and there
    is no bias, you should use the fused-matrix add function, `torch.addmm()`, because
    it’s faster in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping the mathematical computations in a separate functional definition has
    the benefit of keeping optimizations separate from the layer `nn.Module`. The
    functional definitions can also be used as standalone functions when writing code
    in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we’ll often use the `nn.Module` class to subclass our NNs. When we
    create an `nn.Module` subclass, we gain all the built-in benefits of the `nn.Module`
    object. In this case, we derive the `nn.Linear` class from `nn.Module`, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize input and output sizes, weights, and biases.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Define the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_customizing_pytorch_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Use the functional definition of `linear()`.
  prefs: []
  type: TYPE_NORMAL
- en: The `nn.Linear` code includes two necessary methods for any `nn.Module` subclass.
    One is `__init__()`, which initializes the class attributes, namely the inputs,
    outputs, weights, and biases in this case. The other is the `forward()` method,
    which defines the processing during the forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding code, the `forward()` method often calls the
    `nn.functional` definition associated with the layer. This convention is used
    often in PyTorch code for layers.
  prefs: []
  type: TYPE_NORMAL
- en: The convention for creating a custom layer is to first create a function that
    implements the mathematical operations and then create an `nn.Module` subclass
    that uses this function to implement the layer class. Using this approach makes
    it very easy to experiment with new layer designs in your PyTorch model development.
  prefs: []
  type: TYPE_NORMAL
- en: Custom Layer Example (Complex Linear)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we’ll look at how to create a custom layer. In this example, we will create
    our own linear layer for a special type of number called a *complex number*. Complex
    numbers are often used in physics and signal processing and consist of a pair
    of numbers—a “real” component and an “imaginary” component. Both components are
    just floating-point numbers.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch is adding support for complex data types; however, they are still in
    beta at the time of writing of this book. Therefore, we’ll implement them using
    two floating-point tensors, one for the real components and one for the imaginary
    components.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the inputs, weights, biases, and outputs will all be complex
    numbers and will consist of two tensors instead of one. Complex multiplication
    yields the following equation (where *j* is the complex number <math alttext="StartRoot
    1 EndRoot"><msqrt><mn>1</mn></msqrt></math> ):'
  prefs: []
  type: TYPE_NORMAL
- en: <math><mrow><mrow><mo>(</mo> <mi>i</mi> <msub><mi>n</mi> <mi>r</mi></msub> <mo>+</mo>
    <mi>i</mi> <msub><mi>n</mi> <mi>i</mi></msub> <mo>*</mo> <mi>j</mi> <mo>)</mo></mrow>
    <mo>*</mo> <mrow><mo>(</mo> <msub><mi>w</mi> <mi>r</mi></msub> <mo>+</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>*</mo> <mi>j</mi> <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo>
    <msub><mi>b</mi> <mi>r</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>i</mi></msub>
    <mo>*</mo> <mi>j</mi> <mo>)</mo></mrow> <mo>=</mo> <mrow><mo>(</mo> <mi>i</mi>
    <msub><mi>n</mi> <mi>r</mi></msub> <mo>*</mo> <msub><mi>w</mi> <mi>r</mi></msub>
    <mo>-</mo> <mi>i</mi> <msub><mi>n</mi> <mi>i</mi></msub> <mo>*</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>r</mi></msub> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mo>(</mo> <mi>i</mi> <msub><mi>n</mi> <mi>r</mi></msub> <mo>*</mo>
    <msub><mi>w</mi> <mi>i</mi></msub> <mo>+</mo> <mi>i</mi> <msub><mi>n</mi> <mi>i</mi></msub>
    <mo>*</mo> <msub><mi>w</mi> <mi>r</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>*</mo> <mi>j</mi></mrow></math>
  prefs: []
  type: TYPE_NORMAL
- en: 'First we will create a functional version of our complex linear layer, as shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the function applies the complex multiplication formula to
    tensor arrays. Next we create our class version of `ComplexLinear` based on the
    `nn.Module`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In our class, we define separate weights and biases for the real and imaginary
    components in our `__init__()` function. Note that the options for the number
    of `in_features` and `out_features` do not change because the number of real and
    imaginary components are the same. Our `forward()` function simply calls the functional
    definition of our complex multiply and add operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we could also use PyTorch’s existing `nn.Linear` layer to build our
    layer, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this code, we get all the added benefits from `nn.Linear` for free, and we
    do not need to implement a new functional definition. When you create your own
    custom layers, check PyTorch’s built-in layers to see if you can reuse existing
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: Even though this example was pretty simple, you can use the same approach to
    create more complex layers. In addition, the same approach can also be used to
    create custom activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions are very similar to NN layers in that they return outputs
    by performing mathematical operations on a set of inputs. They differ in that
    the operations are performed element-wise, and they do not include parameters
    like weights and biases that are adjusted during training. For this reason, activation
    functions can be performed solely with functional versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s take a look at the ReLU activation function. The ReLU function
    is zero for negative values and linear for positive values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'When the activation function has configurable parameters, it’s common to create
    a class version of it. We can add the capability to adjust the threshold and value
    of the ReLU function by creating a ReLU class, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: When building an NN, it is common to use the functional version of the activation
    function, but a class version can also be used if available. The following code
    snippets show how to use both versions of the ReLU activation included in `torch.nn`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the functional version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: A common way to import the functional package.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The functional version of ReLU is used here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the class version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: We are using `nn.Sequential()` since all components are classes.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: We are using the class version of ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: Custom Activation Example (Complex ReLU)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can create our own custom ComplexReLU activation function to handle complex
    values from the `ComplexLinear` layer that we created earlier. The following code
    shows the functional and class versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Functional version
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Class version
  prefs: []
  type: TYPE_NORMAL
- en: Now that you’ve learned how to create your own layers and activations, let’s
    see how you can create your own custom model architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Custom Model Architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In Chapters [2](ch02.xhtml#Chapter_2) and [3](ch03.xhtml#deep_learning_development_with_pytorch),
    we used built-in models and created our own models from built-in PyTorch layers.
    In this section, we’ll explore how you can create a library of models similar
    to `torchvision.models` and build flexible model classes that adjust the architecture
    based on configuration parameters provided by the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `torchvision.models` package provides an `AlexNet` model class and an `alexnet()`
    convenience function to facilitate its use. Let’s look at the `AlexNet` class
    first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Like all layers, activations, and models, the `AlexNet` class is derived from
    the `nn.Module` class. The `AlexNet` class is a good example of how to create
    and combine submodules into an NN.
  prefs: []
  type: TYPE_NORMAL
- en: The library defines three subnetworks—`features`, `avgpool`, and `classifier`.
    Each subnetwork is made up of PyTorch layers and activations, and they are connected
    in sequence. AlexNet’s `forward()` function describes the forward pass; that is,
    how the inputs are processed to form the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the PyTorch `torchvision.models` code provides a convenience
    function called `alexnet()` to instantiate or create the model with some options.
    The options here are `pretrained` and `progress`; they determine whether to load
    the model with pretrained parameters and whether to display a progress bar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The `**kwargs` parameter allows you to pass additional options to the AlexNet
    model. In this case, you can change the number of classes to 10 with `alexnet(n_classes
    = 10)`. The function will instantiate the AlexNet model with `n_classes = 10`
    and return the model object. If `pretrained` is `True`, the function will load
    the weights from the specified URL.
  prefs: []
  type: TYPE_NORMAL
- en: By following a similar approach, you can create your own model architectures.
    Create a top-level model that is derived from `nn.Module`. Define your `__init__()`
    and `forward()` functions and implement your NN based on subnetworks, layers,
    and activations. Your subnetworks, layers, and activations can even be custom
    ones that you created yourself.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the `nn.Module` class makes creating custom models easy. In
    addition to the `Module` class, the `torch.nn` package includes built-in loss
    functions. Let’s take a look at how you can create your own loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: Custom Loss Functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you recall from [Chapter 3](ch03.xhtml#deep_learning_development_with_pytorch),
    before we can train our NN model, we need to define our loss function. The loss
    function, or cost function, defines a metric which we wish to minimize by adjusting
    the weights of our model during training.
  prefs: []
  type: TYPE_NORMAL
- en: At first it might appear that the loss function is just a functional definition,
    but remember, the loss function is a function of the parameters of the NN module.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, loss functions actually behave like an extra layer that takes the
    NN outputs as inputs and produces a metric as its output. When we perform backpropagation,
    we’re performing backpropagation on the loss function, not the NN.
  prefs: []
  type: TYPE_NORMAL
- en: 'This allows us to call the class directly to compute the loss given the NN
    outputs and true values. Then we can compute the gradients of all the NN parameters
    in one call, namely to perform backpropagation. The following code shows how this
    can be implemented in code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes called `criterion`
  prefs: []
  type: TYPE_NORMAL
- en: First we instantiate the loss function, itself and then we call the function
    passing in the outputs (from our model) and the targets (from our data). Finally,
    we call the `backward()` method to perform backpropagation and compute the gradients
    of all the model parameters with respect to the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to layers discussed earlier, loss functions are implemented using a
    functional definition and a class implementation derived from the `nn.Module`
    class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simplified versions of the functional definition and the class implementations
    for `mse_loss` are shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s create our own loss function, MSE Loss for Complex Numbers. To create
    our own custom loss function, we’ll first define a functional definition that
    describes the loss function mathematically. Then we’ll create the loss function
    class, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This time, we created an optional setting in the class called `real_only`. When
    we instantiate the loss function with `real_only = True`, the functional `mse_loss()`
    will be used instead of `complex_mse_loss()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, PyTorch offers exceptional flexibility in building custom model
    architectures and loss functions. Before we get to training, there’s one more
    function you can customize: the optimizer. Let’s see how you can create custom
    optimizers.'
  prefs: []
  type: TYPE_NORMAL
- en: Custom Optimizer Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The optimizer plays an important part in training your NN model. An optimizer
    is an algorithm that updates the model’s parameters during training. When we perform
    backpropagation using `loss.backward()`, we determine whether the parameters should
    be increased or decreased to minimize the loss. The optimizer uses the gradients
    to determine how much the parameters should be changed during each step and changes
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch has its own submodule called `torch.optim` that contains many built-in
    optimizer algorithms, as we saw in [Chapter 3](ch03.xhtml#deep_learning_development_with_pytorch).
    To create an optimizer, we pass in our model’s parameters and any optimizer-specific
    options. For example, the following code creates an SGD optimizer with a learning
    rate of 0.01 and momentum value of 0.9:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In PyTorch we can also specify different options for different parameters.
    This is useful when you want to specify different learning rates for the different
    layers of your model. Each set of parameters is called a parameter group. We can
    specify different options using dictionaries, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Assuming we are using the AlexNet model, the preceding code sets the learning
    rate to `1e-3` for the classifier layer and uses the default learning rate of
    `1e-2` for the features layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch provides a `torch.optim.Optimizer` base class to make it easy to create
    your own custom optimizers. Here is a simplified version of the `Optimizer` base
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Define `state` as needed.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Define `param_groups` as needed.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_customizing_pytorch_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Define `zero_grad()` as needed.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_customizing_pytorch_CO6-4)'
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need to write your own `step()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main attributes or components to the optimizer: `state` and `param_groups`.
    The `state` atribute is a dictionary that can vary across different optimizers.
    It is mainly used to maintain values between each call to the `step()` function.
    The `param_groups` attribute is also a dictionary. It contains the parameters
    themselves and the associated options for each group.'
  prefs: []
  type: TYPE_NORMAL
- en: The important methods in the `Optimizer` base class are `zero_grad()` and `step()`.
    The `zero_grad()` method is used to zero or reset the gradients during each training
    iteration. The `step()` method is used to execute the optimizer algorithm, compute
    the change for each parameter, and update the parameters within the model object.
    The `zero_grad()` method is already implemented for you. However, you must create
    your own `step()` method when creating your custom optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s demonstrate the process by creating our own simple version of SGD. Our
    SDG optimizer will have one option—the learning rate (LR). During each optimizer
    step we will multiply the gradient by the LR and add it to the parameter (i.e.,
    adjust the model’s weights):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `__init__()` function sets the default option values and initializes the
    parameter groups based on the input parameters. Notice that we don’t have to write
    any code to do this since `super(SGD, self).init(params, defaults)` invokes the
    base class initialization method. All we really need to do is write the `step()`
    method. For each parameter group, we update the parameters by first multiplying
    the parameters by the group’s LR and then subtracting the product from the parameter
    itself. This is accomplished by calling `p.add_(d_p, alpha=-group['lr'])`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how we would use our new optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We could also define a different LR for different layers in the model using
    the following code. Here we assume we’re using AlexNet again as the model with
    layers called `feature` and `classifier`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now that you can create your own optimizers for training your models, let’s
    see how you can create your own custom training, validation, and test loops.
  prefs: []
  type: TYPE_NORMAL
- en: Custom Training, Validation, and Test Loops
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All through this book, we’ve been using custom training, validation, and test
    loops. That’s because in PyTorch all training, validation, and test loops are
    manually created by the programmer.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike in Keras, there’s no `fit()` or `eval()` method that exercises a loop.
    Instead, PyTorch requires that you write your own loops. This is actually a benefit
    in many cases because you’d like to control what happens during training.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the reference design in [“Generative Learning—Generating Fashion-MNIST
    Images with DCGAN”](ch04.xhtml#gan_example_heading) demonstrates how you can create
    a more complex training loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we’ll explore a conventional way of writing loops and discuss
    common ways that developers customize their loops. Let’s review some code commonly
    used for training, validation, and testing loops:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This code should look familiar since we’ve used it often throughout the book.
    We assume that `n_epochs`, `model`, `criterion`, `optimizer`, and `train_`, `val_`,
    and `test_dataloader` have already been defined. For each epoch, we perform the
    training and validation loops. The training loop processes each batch one at a
    time, sends the batch input through the model, and computes the loss. We then
    perform backpropagation to compute the gradients and execute the optimizer to
    update the model’s parameters.
  prefs: []
  type: TYPE_NORMAL
- en: The validation loop disables the gradient computation and passes the validation
    data through the network one batch at a time. The test loop passes the test data
    through the model one batch at a time and computes the loss for the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add some additional capabilities to our loops. The possibilities are
    endless, but this example will demonstrate some simple tasks like printing information,
    reconfiguring a model, and adjusting a hyperparameter in the middle of training.
    Let’s walk through the following code to see how this is done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: Examples of printing epoch, training, and validation loss
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: Examples of reconfiguring a model (best practice)
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_customizing_pytorch_CO7-4)'
  prefs: []
  type: TYPE_NORMAL
- en: Example of modifying a hyperparameter during training
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we added some variables to keep track of the running
    training and validation loss and we printed them for every epoch. Next we use
    the `train()` or `eval()` method to configure the model for training or evaluation,
    respectively. This only applies if the model’s `forward()` function behaves differently
    for training and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: For example, some models may use dropout during training, but dropout should
    not be applied during validation or testing. In this case, we can reconfigure
    the model by calling `model.train()` or `model.eval()` before its execution.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we modified the LR in our optimizer halfway through training. This enables
    us to train at a faster rate at first while fine-tuning our parameter updates
    after training on half of the epochs.
  prefs: []
  type: TYPE_NORMAL
- en: This example is a simple demonstration of how to customize your loops. Training,
    validation, and testing loops can be more complex as you train multiple networks
    simultaneously, use multimodal data, or design more complex networks that can
    even train other networks. PyTorch offers the flexibility to design special and
    innovative processes for training, validation, and testing.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: PyTorch Lightning is a third-party PyTorch package that provides boilerplate
    templates for training, validation, and testing loops. The package provides a
    framework that allows you to create customized loops without having to repeatedly
    type the boilerplate code for each model implementation. We’ll discuss PyTorch
    Lightning in [Chapter 8](ch08.xhtml#pytorch_ecosystem_and_additional_resources).
    You can also find more information at [the PyTorch Lightning website](http://pytorch.tips/pytorch-lightning).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned how to create your own custom components for developing
    deep learning models in PyTorch. As your models grow more and more complex, you
    may find that the time you need to train your model may become quite long—perhaps
    days or even weeks. In the next chapter, you’ll see how to use built-in PyTorch
    capabilities to accelerate and optimize your training process to significantly
    reduce your overall model development time.
  prefs: []
  type: TYPE_NORMAL
