- en: Chapter 5\. Customizing PyTorch
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。自定义PyTorch
- en: Up until now, you have been using built-in PyTorch classes, functions, and libraries
    to design and train various predefined models, model layers, and activation functions.
    But what if you have a novel idea or you’re conducting cutting-edge deep learning
    research? Perhaps you’ve invented a totally new layer architecture or activation
    function. Maybe you’ve developed a new optimization algorithm or a special loss
    function that no one’s ever seen before.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您一直在使用内置的PyTorch类、函数和库来设计和训练各种预定义模型、模型层和激活函数。但是，如果您有一个新颖的想法或正在进行前沿的深度学习研究怎么办？也许您发明了一个全新的层架构或激活函数。也许您开发了一个新的优化算法或一个以前从未见过的特殊损失函数。
- en: In this chapter, I’ll show you how to create your own custom deep learning components
    and algorithms in PyTorch. We’ll begin by exploring how to create custom layers
    and activation functions, and then we’ll see how to combine these components into
    custom model architectures. Next, I’ll show you how to create your own loss functions
    and optimizer algorithms. Finally, we’ll look at how to create custom loops for
    training, validation, and testing.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将向您展示如何在PyTorch中创建自定义的深度学习组件和算法。我们将首先探讨如何创建自定义层和激活函数，然后看看如何将这些组件组合成自定义模型架构。接下来，我将向您展示如何创建自定义的损失函数和优化算法。最后，我们将看看如何创建用于训练、验证和测试的自定义循环。
- en: 'PyTorch offers flexibility: you can extend an existing library or you can combine
    your customizations into your own library or package. By creating custom components
    you can solve new deep learning problems, speed up training, and discover innovative
    ways to perform deep learning.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了灵活性：您可以扩展现有库，也可以将自定义内容组合到自己的库或包中。通过创建自定义组件，您可以解决新的深度学习问题，加快训练速度，并发现执行深度学习的创新方法。
- en: Let’s get started by creating some custom deep learning layers and activation
    functions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始创建一些自定义深度学习层和激活函数。
- en: Custom Layers and Activations
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义层和激活
- en: PyTorch offers an extensive set of built-in layers and activation functions.
    However, what makes PyTorch so popular, especially in the research community,
    is how easy it is to create custom layers and activations. The ability to do so
    can facilitate experimentation and accelerate your research.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了一套广泛的内置层和激活函数。然而，PyTorch如此受欢迎，尤其是在研究社区中，是因为创建自定义层和激活如此简单。这样做的能力可以促进实验并加速您的研究。
- en: If we take a look at the PyTorch source code, we’ll see that layers and activations
    are created using a functional definition and a class implementation. The *functional
    definition* specifies how the outputs are created based on the inputs. It is defined
    in the `nn.functional` module. The *class implementation* is used to create an
    object that calls this function at its core, but it also includes added features
    derived from the `nn.Module` class.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看PyTorch源代码，我们会看到层和激活是使用功能定义和类实现创建的。*功能定义*指定基于输入创建输出的方式。它在`nn.functional`模块中定义。*类实现*用于创建调用此函数的对象，但它还包括从`nn.Module`类派生的附加功能。
- en: 'For example, let’s look at how the fully connected `nn.Linear` layer is implemented.
    The following code shows a simplified version of the functional definition, `nn.functional.linear()`:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看全连接的`nn.Linear`层是如何实现的。以下代码显示了功能定义`nn.functional.linear()`的简化版本：
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `linear()` function multiplies the input tensor by the weight matrix, optionally
    adds the bias vector, and returns the results in a tensor. You can see that the
    code is optimized for performance. When the input has two dimensions and there
    is no bias, you should use the fused-matrix add function, `torch.addmm()`, because
    it’s faster in this case.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '`linear()`函数将输入张量乘以权重矩阵，可选择添加偏置向量，并将结果返回为张量。您可以看到代码针对性能进行了优化。当输入具有两个维度且没有偏置时，应使用融合矩阵加函数`torch.addmm()`，因为在这种情况下速度更快。'
- en: Keeping the mathematical computations in a separate functional definition has
    the benefit of keeping optimizations separate from the layer `nn.Module`. The
    functional definitions can also be used as standalone functions when writing code
    in general.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 将数学计算保留在单独的功能定义中有一个好处，即将优化与层`nn.Module`分开。功能定义也可以在一般编写代码时作为独立函数使用。
- en: 'However, we’ll often use the `nn.Module` class to subclass our NNs. When we
    create an `nn.Module` subclass, we gain all the built-in benefits of the `nn.Module`
    object. In this case, we derive the `nn.Linear` class from `nn.Module`, as shown
    in the following code:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们通常会使用`nn.Module`类来对我们的神经网络进行子类化。当我们创建一个`nn.Module`子类时，我们获得了`nn.Module`对象的所有内置优势。在这种情况下，我们从`nn.Module`派生`nn.Linear`类，如下面的代码所示：
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO1-1)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_customizing_pytorch_CO1-1)'
- en: Initialize input and output sizes, weights, and biases.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化输入和输出大小、权重和偏置。
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO1-2)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_customizing_pytorch_CO1-2)'
- en: Define the forward pass.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 定义前向传递。
- en: '[![3](Images/3.png)](#co_customizing_pytorch_CO1-3)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_customizing_pytorch_CO1-3)'
- en: Use the functional definition of `linear()`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`linear()`的功能定义。
- en: The `nn.Linear` code includes two necessary methods for any `nn.Module` subclass.
    One is `__init__()`, which initializes the class attributes, namely the inputs,
    outputs, weights, and biases in this case. The other is the `forward()` method,
    which defines the processing during the forward pass.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`nn.Linear`代码包括任何`nn.Module`子类所需的两种方法。一种是`__init__()`，它初始化类属性，即在这种情况下的输入、输出、权重和偏置。另一种是`forward()`方法，它定义了前向传递期间的处理。'
- en: As you can see in the preceding code, the `forward()` method often calls the
    `nn.functional` definition associated with the layer. This convention is used
    often in PyTorch code for layers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码所示，`forward()`方法经常调用与层相关的`nn.functional`定义。这种约定在PyTorch代码中经常使用于层。
- en: The convention for creating a custom layer is to first create a function that
    implements the mathematical operations and then create an `nn.Module` subclass
    that uses this function to implement the layer class. Using this approach makes
    it very easy to experiment with new layer designs in your PyTorch model development.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 创建自定义层的约定是首先创建一个实现数学运算的函数，然后创建一个`nn.Module`子类，该子类使用这个函数来实现层类。使用这种方法可以很容易地在PyTorch模型开发中尝试新的层设计。
- en: Custom Layer Example (Complex Linear)
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义层示例（复杂线性）
- en: Next, we’ll look at how to create a custom layer. In this example, we will create
    our own linear layer for a special type of number called a *complex number*. Complex
    numbers are often used in physics and signal processing and consist of a pair
    of numbers—a “real” component and an “imaginary” component. Both components are
    just floating-point numbers.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看如何创建一个自定义层。在这个例子中，我们将为一种特殊类型的数字——*复数*创建自己的线性层。复数经常在物理学和信号处理中使用，由一对数字组成——一个“实”部分和一个“虚”部分。这两个部分都是浮点数。
- en: PyTorch is adding support for complex data types; however, they are still in
    beta at the time of writing of this book. Therefore, we’ll implement them using
    two floating-point tensors, one for the real components and one for the imaginary
    components.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch正在添加对复杂数据类型的支持；然而，在撰写本书时，它们仍处于测试阶段。因此，我们将使用两个浮点张量来实现它们，一个用于实部，一个用于虚部。
- en: 'In this case, the inputs, weights, biases, and outputs will all be complex
    numbers and will consist of two tensors instead of one. Complex multiplication
    yields the following equation (where *j* is the complex number <math alttext="StartRoot
    1 EndRoot"><msqrt><mn>1</mn></msqrt></math> ):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，输入、权重、偏置和输出都将是复数，并且将由两个张量组成，而不是一个。复数乘法得到以下方程（其中 *j* 是复数 <math alttext="StartRoot
    1 EndRoot"><msqrt><mn>1</mn></msqrt></math>）：
- en: <math><mrow><mrow><mo>(</mo> <mi>i</mi> <msub><mi>n</mi> <mi>r</mi></msub> <mo>+</mo>
    <mi>i</mi> <msub><mi>n</mi> <mi>i</mi></msub> <mo>*</mo> <mi>j</mi> <mo>)</mo></mrow>
    <mo>*</mo> <mrow><mo>(</mo> <msub><mi>w</mi> <mi>r</mi></msub> <mo>+</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>*</mo> <mi>j</mi> <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo>
    <msub><mi>b</mi> <mi>r</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>i</mi></msub>
    <mo>*</mo> <mi>j</mi> <mo>)</mo></mrow> <mo>=</mo> <mrow><mo>(</mo> <mi>i</mi>
    <msub><mi>n</mi> <mi>r</mi></msub> <mo>*</mo> <msub><mi>w</mi> <mi>r</mi></msub>
    <mo>-</mo> <mi>i</mi> <msub><mi>n</mi> <mi>i</mi></msub> <mo>*</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>r</mi></msub> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mo>(</mo> <mi>i</mi> <msub><mi>n</mi> <mi>r</mi></msub> <mo>*</mo>
    <msub><mi>w</mi> <mi>i</mi></msub> <mo>+</mo> <mi>i</mi> <msub><mi>n</mi> <mi>i</mi></msub>
    <mo>*</mo> <msub><mi>w</mi> <mi>r</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>*</mo> <mi>j</mi></mrow></math>
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: <math><mrow><mrow><mo>(</mo> <mi>i</mi> <msub><mi>n</mi> <mi>r</mi></msub> <mo>+</mo>
    <mi>i</mi> <msub><mi>n</mi> <mi>i</mi></msub> <mo>*</mo> <mi>j</mi> <mo>)</mo></mrow>
    <mo>*</mo> <mrow><mo>(</mo> <msub><mi>w</mi> <mi>r</mi></msub> <mo>+</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>*</mo> <mi>j</mi> <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo>
    <msub><mi>b</mi> <mi>r</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>i</mi></msub>
    <mo>*</mo> <mi>j</mi> <mo>)</mo></mrow> <mo>=</mo> <mrow><mo>(</mo> <mi>i</mi>
    <msub><mi>n</mi> <mi>r</mi></msub> <mo>*</mo> <msub><mi>w</mi> <mi>r</mi></msub>
    <mo>-</mo> <mi>i</mi> <msub><mi>n</mi> <mi>i</mi></msub> <mo>*</mo> <msub><mi>w</mi>
    <mi>i</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>r</mi></msub> <mo>)</mo></mrow>
    <mo>+</mo> <mrow><mo>(</mo> <mi>i</mi> <msub><mi>n</mi> <mi>r</mi></msub> <mo>*</mo>
    <msub><mi>w</mi> <mi>i</mi></msub> <mo>+</mo> <mi>i</mi> <msub><mi>n</mi> <mi>i</mi></msub>
    <mo>*</mo> <msub><mi>w</mi> <mi>r</mi></msub> <mo>+</mo> <msub><mi>b</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>*</mo> <mi>j</mi></mrow></math>
- en: 'First we will create a functional version of our complex linear layer, as shown
    in the following code:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个复杂线性层的函数版本，如下面的代码所示：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As you can see, the function applies the complex multiplication formula to
    tensor arrays. Next we create our class version of `ComplexLinear` based on the
    `nn.Module`, as shown in the following code:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，该函数将复杂乘法公式应用于张量数组。接下来，我们根据`nn.Module`创建`ComplexLinear`的类版本，如下面的代码所示：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: In our class, we define separate weights and biases for the real and imaginary
    components in our `__init__()` function. Note that the options for the number
    of `in_features` and `out_features` do not change because the number of real and
    imaginary components are the same. Our `forward()` function simply calls the functional
    definition of our complex multiply and add operation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的类中，我们在`__init__()`函数中为实部和虚部定义了单独的权重和偏置。请注意，`in_features`和`out_features`的选项数量不会改变，因为实部和虚部的数量是相同的。我们的`forward()`函数只是调用我们的复杂乘法和加法操作的函数定义。
- en: 'Note that we could also use PyTorch’s existing `nn.Linear` layer to build our
    layer, as shown in the following code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们也可以使用PyTorch现有的`nn.Linear`层来构建我们的层，如下面的代码所示：
- en: '[PRE4]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: In this code, we get all the added benefits from `nn.Linear` for free, and we
    do not need to implement a new functional definition. When you create your own
    custom layers, check PyTorch’s built-in layers to see if you can reuse existing
    classes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们可以免费获得`nn.Linear`的所有附加好处，而无需实现新的函数定义。当你创建自己的自定义层时，检查PyTorch的内置层，看看是否可以重用现有的类。
- en: Even though this example was pretty simple, you can use the same approach to
    create more complex layers. In addition, the same approach can also be used to
    create custom activation functions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这个例子非常简单，你也可以使用相同的方法来创建更复杂的层。此外，相同的方法也可以用来创建自定义激活函数。
- en: Activation functions are very similar to NN layers in that they return outputs
    by performing mathematical operations on a set of inputs. They differ in that
    the operations are performed element-wise, and they do not include parameters
    like weights and biases that are adjusted during training. For this reason, activation
    functions can be performed solely with functional versions.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数与NN层非常相似，它们通过对一组输入执行数学运算来返回输出。它们的不同之处在于，操作是逐元素执行的，并且不包括在训练过程中调整的权重和偏置等参数。因此，激活函数可以仅使用函数版本执行。
- en: 'For example, let’s take a look at the ReLU activation function. The ReLU function
    is zero for negative values and linear for positive values:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们来看看ReLU激活函数。ReLU函数对于负值为零，对于正值为线性：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'When the activation function has configurable parameters, it’s common to create
    a class version of it. We can add the capability to adjust the threshold and value
    of the ReLU function by creating a ReLU class, as shown in the following code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当激活函数具有可配置参数时，通常会创建一个类版本。我们可以通过创建一个ReLU类来添加调整ReLU函数的阈值和值的功能，如下所示：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When building an NN, it is common to use the functional version of the activation
    function, but a class version can also be used if available. The following code
    snippets show how to use both versions of the ReLU activation included in `torch.nn`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建NN时，通常使用激活函数的函数版本，但如果有的话也可以使用类版本。以下代码片段展示了如何使用`torch.nn`中包含的ReLU激活的两个版本。
- en: 'Here’s the functional version:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是函数版本：
- en: '[PRE7]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO2-1)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_customizing_pytorch_CO2-1)'
- en: A common way to import the functional package.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 导入函数包的常见方式。
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO2-2)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_customizing_pytorch_CO2-2)'
- en: The functional version of ReLU is used here.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用了ReLU的函数版本。
- en: 'Here’s the class version:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是类版本：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO3-1)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_customizing_pytorch_CO3-1)'
- en: We are using `nn.Sequential()` since all components are classes.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`nn.Sequential()`因为所有组件都是类。
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO3-2)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_customizing_pytorch_CO3-2)'
- en: We are using the class version of ReLU.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用ReLU的类版本。
- en: Custom Activation Example (Complex ReLU)
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义激活函数示例（Complex ReLU）
- en: 'We can create our own custom ComplexReLU activation function to handle complex
    values from the `ComplexLinear` layer that we created earlier. The following code
    shows the functional and class versions:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以创建自己的自定义ComplexReLU激活函数来处理我们之前创建的`ComplexLinear`层中的复数值。以下代码展示了函数版本和类版本：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO4-1)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_customizing_pytorch_CO4-1)'
- en: Functional version
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 函数版本
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO4-2)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_customizing_pytorch_CO4-2)'
- en: Class version
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 类版本
- en: Now that you’ve learned how to create your own layers and activations, let’s
    see how you can create your own custom model architectures.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经学会了如何创建自己的层和激活函数，让我们看看如何创建自己的自定义模型架构。
- en: Custom Model Architectures
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义模型架构
- en: In Chapters [2](ch02.xhtml#Chapter_2) and [3](ch03.xhtml#deep_learning_development_with_pytorch),
    we used built-in models and created our own models from built-in PyTorch layers.
    In this section, we’ll explore how you can create a library of models similar
    to `torchvision.models` and build flexible model classes that adjust the architecture
    based on configuration parameters provided by the user.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在第[2](ch02.xhtml#Chapter_2)章和第[3](ch03.xhtml#deep_learning_development_with_pytorch)章中，我们使用了内置模型并从内置PyTorch层创建了自己的模型。在本节中，我们将探讨如何创建类似于`torchvision.models`的模型库，并构建灵活的模型类，根据用户提供的配置参数调整架构。
- en: 'The `torchvision.models` package provides an `AlexNet` model class and an `alexnet()`
    convenience function to facilitate its use. Let’s look at the `AlexNet` class
    first:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision.models`包提供了一个`AlexNet`模型类和一个`alexnet()`便利函数来方便其使用。让我们先看看`AlexNet`类：'
- en: '[PRE10]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Like all layers, activations, and models, the `AlexNet` class is derived from
    the `nn.Module` class. The `AlexNet` class is a good example of how to create
    and combine submodules into an NN.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有层、激活函数和模型一样，`AlexNet`类派生自`nn.Module`类。`AlexNet`类是如何创建和组合子模块成为NN的一个很好的例子。
- en: The library defines three subnetworks—`features`, `avgpool`, and `classifier`.
    Each subnetwork is made up of PyTorch layers and activations, and they are connected
    in sequence. AlexNet’s `forward()` function describes the forward pass; that is,
    how the inputs are processed to form the outputs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 该库定义了三个子网络——`features`、`avgpool`和`classifier`。每个子网络由PyTorch层和激活函数组成，并按顺序连接。AlexNet的`forward()`函数描述了前向传播；即输入如何被处理以形成输出。
- en: 'In this case, the PyTorch `torchvision.models` code provides a convenience
    function called `alexnet()` to instantiate or create the model with some options.
    The options here are `pretrained` and `progress`; they determine whether to load
    the model with pretrained parameters and whether to display a progress bar:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，PyTorch的`torchvision.models`代码提供了一个方便的函数`alexnet()`来实例化或创建模型并提供一些选项。这里的选项是`pretrained`和`progress`；它们确定是否加载具有预训练参数的模型以及是否显示进度条：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The `**kwargs` parameter allows you to pass additional options to the AlexNet
    model. In this case, you can change the number of classes to 10 with `alexnet(n_classes
    = 10)`. The function will instantiate the AlexNet model with `n_classes = 10`
    and return the model object. If `pretrained` is `True`, the function will load
    the weights from the specified URL.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`**kwargs`参数允许您向AlexNet模型传递其他选项。在这种情况下，您可以使用`alexnet(n_classes = 10)`将类别数更改为10。该函数将使用`n_classes
    = 10`实例化AlexNet模型并返回模型对象。如果`pretrained`为`True`，函数将从指定的URL加载权重。'
- en: By following a similar approach, you can create your own model architectures.
    Create a top-level model that is derived from `nn.Module`. Define your `__init__()`
    and `forward()` functions and implement your NN based on subnetworks, layers,
    and activations. Your subnetworks, layers, and activations can even be custom
    ones that you created yourself.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 通过采用类似的方法，您可以创建自己的模型架构。创建一个从`nn.Module`派生的顶级模型。定义您的`__init__()`和`forward()`函数，并根据子网络、层和激活函数实现您的NN。您的子网络、层和激活函数甚至可以是您自己创建的自定义的。
- en: As you can see, the `nn.Module` class makes creating custom models easy. In
    addition to the `Module` class, the `torch.nn` package includes built-in loss
    functions. Let’s take a look at how you can create your own loss functions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`nn.Module`类使创建自定义模型变得容易。除了`Module`类外，`torch.nn`包还包括内置的损失函数。让我们看看如何创建自己的损失函数。
- en: Custom Loss Functions
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义损失函数
- en: If you recall from [Chapter 3](ch03.xhtml#deep_learning_development_with_pytorch),
    before we can train our NN model, we need to define our loss function. The loss
    function, or cost function, defines a metric which we wish to minimize by adjusting
    the weights of our model during training.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: At first it might appear that the loss function is just a functional definition,
    but remember, the loss function is a function of the parameters of the NN module.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, loss functions actually behave like an extra layer that takes the
    NN outputs as inputs and produces a metric as its output. When we perform backpropagation,
    we’re performing backpropagation on the loss function, not the NN.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'This allows us to call the class directly to compute the loss given the NN
    outputs and true values. Then we can compute the gradients of all the NN parameters
    in one call, namely to perform backpropagation. The following code shows how this
    can be implemented in code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO5-1)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes called `criterion`
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: First we instantiate the loss function, itself and then we call the function
    passing in the outputs (from our model) and the targets (from our data). Finally,
    we call the `backward()` method to perform backpropagation and compute the gradients
    of all the model parameters with respect to the loss.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Similar to layers discussed earlier, loss functions are implemented using a
    functional definition and a class implementation derived from the `nn.Module`
    class.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Simplified versions of the functional definition and the class implementations
    for `mse_loss` are shown in the following code:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s create our own loss function, MSE Loss for Complex Numbers. To create
    our own custom loss function, we’ll first define a functional definition that
    describes the loss function mathematically. Then we’ll create the loss function
    class, as shown in the following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This time, we created an optional setting in the class called `real_only`. When
    we instantiate the loss function with `real_only = True`, the functional `mse_loss()`
    will be used instead of `complex_mse_loss()`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, PyTorch offers exceptional flexibility in building custom model
    architectures and loss functions. Before we get to training, there’s one more
    function you can customize: the optimizer. Let’s see how you can create custom
    optimizers.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Custom Optimizer Algorithms
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The optimizer plays an important part in training your NN model. An optimizer
    is an algorithm that updates the model’s parameters during training. When we perform
    backpropagation using `loss.backward()`, we determine whether the parameters should
    be increased or decreased to minimize the loss. The optimizer uses the gradients
    to determine how much the parameters should be changed during each step and changes
    them.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch has its own submodule called `torch.optim` that contains many built-in
    optimizer algorithms, as we saw in [Chapter 3](ch03.xhtml#deep_learning_development_with_pytorch).
    To create an optimizer, we pass in our model’s parameters and any optimizer-specific
    options. For example, the following code creates an SGD optimizer with a learning
    rate of 0.01 and momentum value of 0.9:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In PyTorch we can also specify different options for different parameters.
    This is useful when you want to specify different learning rates for the different
    layers of your model. Each set of parameters is called a parameter group. We can
    specify different options using dictionaries, as shown in the following code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Assuming we are using the AlexNet model, the preceding code sets the learning
    rate to `1e-3` for the classifier layer and uses the default learning rate of
    `1e-2` for the features layer.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'PyTorch provides a `torch.optim.Optimizer` base class to make it easy to create
    your own custom optimizers. Here is a simplified version of the `Optimizer` base
    class:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO6-1)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Define `state` as needed.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO6-2)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Define `param_groups` as needed.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](Images/3.png)](#co_customizing_pytorch_CO6-3)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Define `zero_grad()` as needed.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](Images/4.png)](#co_customizing_pytorch_CO6-4)'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: You’ll need to write your own `step()`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main attributes or components to the optimizer: `state` and `param_groups`.
    The `state` atribute is a dictionary that can vary across different optimizers.
    It is mainly used to maintain values between each call to the `step()` function.
    The `param_groups` attribute is also a dictionary. It contains the parameters
    themselves and the associated options for each group.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: The important methods in the `Optimizer` base class are `zero_grad()` and `step()`.
    The `zero_grad()` method is used to zero or reset the gradients during each training
    iteration. The `step()` method is used to execute the optimizer algorithm, compute
    the change for each parameter, and update the parameters within the model object.
    The `zero_grad()` method is already implemented for you. However, you must create
    your own `step()` method when creating your custom optimizer.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s demonstrate the process by creating our own simple version of SGD. Our
    SDG optimizer will have one option—the learning rate (LR). During each optimizer
    step we will multiply the gradient by the LR and add it to the parameter (i.e.,
    adjust the model’s weights):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `__init__()` function sets the default option values and initializes the
    parameter groups based on the input parameters. Notice that we don’t have to write
    any code to do this since `super(SGD, self).init(params, defaults)` invokes the
    base class initialization method. All we really need to do is write the `step()`
    method. For each parameter group, we update the parameters by first multiplying
    the parameters by the group’s LR and then subtracting the product from the parameter
    itself. This is accomplished by calling `p.add_(d_p, alpha=-group['lr'])`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an example of how we would use our new optimizer:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We could also define a different LR for different layers in the model using
    the following code. Here we assume we’re using AlexNet again as the model with
    layers called `feature` and `classifier`:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Now that you can create your own optimizers for training your models, let’s
    see how you can create your own custom training, validation, and test loops.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Custom Training, Validation, and Test Loops
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All through this book, we’ve been using custom training, validation, and test
    loops. That’s because in PyTorch all training, validation, and test loops are
    manually created by the programmer.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Unlike in Keras, there’s no `fit()` or `eval()` method that exercises a loop.
    Instead, PyTorch requires that you write your own loops. This is actually a benefit
    in many cases because you’d like to control what happens during training.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the reference design in [“Generative Learning—Generating Fashion-MNIST
    Images with DCGAN”](ch04.xhtml#gan_example_heading) demonstrates how you can create
    a more complex training loop.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we’ll explore a conventional way of writing loops and discuss
    common ways that developers customize their loops. Let’s review some code commonly
    used for training, validation, and testing loops:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This code should look familiar since we’ve used it often throughout the book.
    We assume that `n_epochs`, `model`, `criterion`, `optimizer`, and `train_`, `val_`,
    and `test_dataloader` have already been defined. For each epoch, we perform the
    training and validation loops. The training loop processes each batch one at a
    time, sends the batch input through the model, and computes the loss. We then
    perform backpropagation to compute the gradients and execute the optimizer to
    update the model’s parameters.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: The validation loop disables the gradient computation and passes the validation
    data through the network one batch at a time. The test loop passes the test data
    through the model one batch at a time and computes the loss for the test data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add some additional capabilities to our loops. The possibilities are
    endless, but this example will demonstrate some simple tasks like printing information,
    reconfiguring a model, and adjusting a hyperparameter in the middle of training.
    Let’s walk through the following code to see how this is done:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为我们的循环添加一些额外的功能。可能性是无限的，但这个示例将演示一些简单的任务，比如打印信息、重新配置模型以及在训练过程中调整超参数。让我们走一遍以下代码，看看如何实现这一点：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[![1](Images/1.png)](#co_customizing_pytorch_CO7-1)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_customizing_pytorch_CO7-1)'
- en: Examples of printing epoch, training, and validation loss
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 打印 epoch、训练和验证损失的示例
- en: '[![2](Images/2.png)](#co_customizing_pytorch_CO7-3)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_customizing_pytorch_CO7-3)'
- en: Examples of reconfiguring a model (best practice)
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 重新配置模型的示例（最佳实践）
- en: '[![3](Images/3.png)](#co_customizing_pytorch_CO7-4)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_customizing_pytorch_CO7-4)'
- en: Example of modifying a hyperparameter during training
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 修改训练过程中的超参数示例
- en: In the preceding code, we added some variables to keep track of the running
    training and validation loss and we printed them for every epoch. Next we use
    the `train()` or `eval()` method to configure the model for training or evaluation,
    respectively. This only applies if the model’s `forward()` function behaves differently
    for training and evaluation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述代码中，我们添加了一些变量来跟踪运行的训练和验证损失，并在每个 epoch 打印它们。接下来，我们使用 `train()` 或 `eval()`
    方法来配置模型进行训练或评估。这仅适用于模型的 `forward()` 函数在训练和评估时表现不同的情况。
- en: For example, some models may use dropout during training, but dropout should
    not be applied during validation or testing. In this case, we can reconfigure
    the model by calling `model.train()` or `model.eval()` before its execution.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一些模型可能在训练过程中使用 dropout，但在验证或测试过程中不应用 dropout。在这种情况下，我们可以通过调用 `model.train()`
    或 `model.eval()` 来重新配置模型，然后执行它。
- en: Lastly, we modified the LR in our optimizer halfway through training. This enables
    us to train at a faster rate at first while fine-tuning our parameter updates
    after training on half of the epochs.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在训练过程中修改了优化器中的学习率。这使我们能够在一半的 epoch 训练后以更快的速度训练，同时在微调参数更新之后。
- en: This example is a simple demonstration of how to customize your loops. Training,
    validation, and testing loops can be more complex as you train multiple networks
    simultaneously, use multimodal data, or design more complex networks that can
    even train other networks. PyTorch offers the flexibility to design special and
    innovative processes for training, validation, and testing.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例是如何自定义您的循环的简单演示。训练、验证和测试循环可能会更加复杂，因为您同时训练多个网络、使用多模态数据，或设计更复杂的网络，甚至可以训练其他网络。PyTorch
    提供了设计用于训练、验证和测试的特殊和创新过程的灵活性。
- en: Tip
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: PyTorch Lightning is a third-party PyTorch package that provides boilerplate
    templates for training, validation, and testing loops. The package provides a
    framework that allows you to create customized loops without having to repeatedly
    type the boilerplate code for each model implementation. We’ll discuss PyTorch
    Lightning in [Chapter 8](ch08.xhtml#pytorch_ecosystem_and_additional_resources).
    You can also find more information at [the PyTorch Lightning website](http://pytorch.tips/pytorch-lightning).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Lightning 是一个第三方 PyTorch 包，提供了用于训练、验证和测试循环的样板模板。该包提供了一个框架，允许您创建自定义循环，而无需为每个模型实现重复输入样板代码。我们将在[第
    8 章](ch08.xhtml#pytorch_ecosystem_and_additional_resources)中讨论 PyTorch Lightning。您也可以在[PyTorch
    Lightning 网站](http://pytorch.tips/pytorch-lightning)上找到更多信息。
- en: In this chapter, you learned how to create your own custom components for developing
    deep learning models in PyTorch. As your models grow more and more complex, you
    may find that the time you need to train your model may become quite long—perhaps
    days or even weeks. In the next chapter, you’ll see how to use built-in PyTorch
    capabilities to accelerate and optimize your training process to significantly
    reduce your overall model development time.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了如何为在 PyTorch 中开发深度学习模型创建自定义组件。随着您的模型变得越来越复杂，您可能会发现您需要训练模型的时间可能会变得相当长——也许是几天甚至几周。在下一章中，您将看到如何利用内置的
    PyTorch 能力来加速和优化您的训练过程，从而显著减少整体模型开发时间。
