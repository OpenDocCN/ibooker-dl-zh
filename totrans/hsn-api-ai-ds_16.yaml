- en: Chapter 13\. Deploying a Machine Learning API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Always in motion is the future.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yoda, *The Empire Strikes Back*
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Fantasy football managers spend most of their time attempting to predict the
    future and plotting strategies based on those predictions. Before the season begins,
    managers want to know how NFL players will perform in the upcoming season so that
    they can build the best team. During their fantasy drafts, managers want to know
    where a player would be picked by other managers so that they can outmaneuver
    their competition. Each week, managers want to know which of their players are
    going to score the most so that they can set their lineups accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Many fantasy websites and platforms provide predictions to these managers. One
    of the tools available to the platforms is a *machine learning* (ML) model, which
    you learned about in [Chapter 12](ch12.html#chapter_12). The platforms train various
    models and use them to make predictions, or *inferences*, to managers. If a model
    processes an entire group of predictions at once, it is called *batch inference*.
    Some fantasy questions are appropriate for batch inference, such as making a week’s
    worth of player predictions all at once. Batch inference may be done by a scheduled
    script or job. But if the predictions are changing minute by minute—like in the
    case of a live score prediction for a game—then real-time inference is needed.
    *Real-time inference* is calling a model to get a single prediction immediately.
    This is where deploying the model as an API is most valuable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will create an ML model and deploy it with an API to make
    real-time inference. As you proceed through the chapter, here are a few terms
    that you will come across:'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs: []
  type: TYPE_NORMAL
- en: A type of model that predicts what category a value will fall into. For example,
    a classification model might predict if a player will be drafted or undrafted.
    Models that perform classification are called *classifiers*.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs: []
  type: TYPE_NORMAL
- en: A type of ML algorithm that creates a recursive tree structure to perform classification
    or regression.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating a model
  prefs: []
  type: TYPE_NORMAL
- en: Comparing the model’s predictions to test data to see how well it would have
    predicted past events.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs: []
  type: TYPE_NORMAL
- en: An ML technique that combines multiple models to create a model that is more
    effective than the individual models.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs: []
  type: TYPE_NORMAL
- en: A type of model that predicts a continuous numeric value. For example, a regression
    model might predict how many points a player will score. Models that perform regression
    are called *regressors*.
  prefs: []
  type: TYPE_NORMAL
- en: Training a model
  prefs: []
  type: TYPE_NORMAL
- en: Using the training portion of historical data to create a model that can make
    inferences based on new data.
  prefs: []
  type: TYPE_NORMAL
- en: Training Machine Learning Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Supervised learning* is a method of creating models by processing existing
    data where the expected values are known. For example, a financial fraud detection
    model might be trained by processing a large number of bank transactions that
    have been *labeled* or categorized as either fraud or nonfraud. Through this process,
    the model recognizes future records that are potentially fraudulent. Through this
    type of supervised learning, ML models can be created that create predictions
    on various data formats including tabular data, images, audio files, and others.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-1](#machine_learning_diagram_ch12) shows this type of training.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning training](assets/haad_1301.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-1\. ML training model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The diagram shows a set of historical rows of data. The goal of the ML model
    in this case would be to predict future values of the output column. The training
    process would involve using software to read the input columns from historical
    data and look for patterns in how they are related to the output column.
  prefs: []
  type: TYPE_NORMAL
- en: When the model has been trained, it can be used to read the input columns from
    new rows of data and predict what the values will be for the output columns. This
    is the *inference* process, shown in [Figure 13-2](#machine_learning_diagram_2_ch12).
  prefs: []
  type: TYPE_NORMAL
- en: '![Machine learning inference model](assets/haad_1302.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-2\. ML inference model
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The model you will create in this chapter is a supervised ML model.
  prefs: []
  type: TYPE_NORMAL
- en: New Software Used in This Chapter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Table 13-1](#tools_table_chapter_13) lists a few of the new software components
    you will begin using in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-1\. Software used in this chapter
  prefs: []
  type: TYPE_NORMAL
- en: '| Software name | Purpose |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ONNX Runtime | A cross-platform tool for using models from a variety of different
    frameworks. |'
  prefs: []
  type: TYPE_TB
- en: '| scikit-learn | An ML framework for training models. You will use the `GradientBoostingRegressor`
    from this library. |'
  prefs: []
  type: TYPE_TB
- en: '| sklearn-onnx | A library that converts scikit-learn models to ONNX format.
    |'
  prefs: []
  type: TYPE_TB
- en: ONNX Runtime
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Open Neural Network Exchange (ONNX) is an open standard for ML models. Because
    such a variety of programming languages and libraries are used to make ML models,
    it can be complicated to deploy and run multiple different models. ONNX is a standard
    format that models from different programming languages and different frameworks
    can be converted to and run in a standard way.
  prefs: []
  type: TYPE_NORMAL
- en: This allows greater interoperability, because when models from different programming
    languages and frameworks are converted to ONNX format, they can be more easily
    deployed using the standard ONNX Runtime. The ONNX Runtime also includes acceleration
    that can improve model inference performance.
  prefs: []
  type: TYPE_NORMAL
- en: After you have developed your model in scikit-learn, you will convert it to
    ONNX format, and then use the [ONNX Runtime](https://oreil.ly/IGEBD) in your API
    to make predictions (inferences).
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The scikit-learn library is a Python framework that allows you to create models
    for classification, regression, clustering, and a variety of other tasks. This
    is one of the more popular ML libraries in Python, along with PyTorch, TensorFlow,
    and XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: sklearn-onnx
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since you are using scikit-learn to create your model, you will use the sklearn-onnx
    library to convert your model into ONNX format. This will be the final step of
    the model training process.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the New Libraries in Your Codespace
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Open the Part III GitHub Codespace that you created in [Chapter 12](ch12.html#chapter_12).
    To install the libraries you need for this chapter, create a file named *chapter13/requirements.txt*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Update *chapter13/requirements.txt* with the following contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deploying_a_machine_learning_api_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The scikit-learn library will be used to create the ML model.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deploying_a_machine_learning_api_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The numpy library will be used to format numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_deploying_a_machine_learning_api_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The pandas library will be used to process the input data file.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_deploying_a_machine_learning_api_CO1-4)'
  prefs: []
  type: TYPE_NORMAL
- en: The skl2onnx library will be used to save the scikit-learn model into ONNX format.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_deploying_a_machine_learning_api_CO1-5)'
  prefs: []
  type: TYPE_NORMAL
- en: Uvicorn is the web server used to host FastAPI.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_deploying_a_machine_learning_api_CO1-6)'
  prefs: []
  type: TYPE_NORMAL
- en: The onnxruntime library is used to perform inference with a saved ONNX model
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Execute the following command to install the new libraries in your Codespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You should see a message that states that these libraries were successfully
    installed.
  prefs: []
  type: TYPE_NORMAL
- en: Using the CRISP-DM Process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ML projects have many steps requiring people with a lot of specialized skills.
    A useful method of organizing an ML modeling project is the Cross-Industry Standard
    Process for Data Mining (Shearer, 2000). This model is widely used in the data
    science community.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are definitions of the stages in CRISP-DM:'
  prefs: []
  type: TYPE_NORMAL
- en: Business understanding
  prefs: []
  type: TYPE_NORMAL
- en: During the this stage, the team identifies business objectives and assesses
    tools and techniques available.
  prefs: []
  type: TYPE_NORMAL
- en: Data understanding
  prefs: []
  type: TYPE_NORMAL
- en: Collecting data that is available to solve the problem, explore it, and verify
    the data quality.
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs: []
  type: TYPE_NORMAL
- en: During this stage, data scientists select specific data elements to be used,
    format them, and merge with any additional sources needed.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a modeling technique and building a model that answers your business
    question.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs: []
  type: TYPE_NORMAL
- en: Review the model for its ability to solve the question and its readiness for
    production.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs: []
  type: TYPE_NORMAL
- en: Models are deployed in an environment where they can be consumed by the customer.
    Monitor and maintain the model.
  prefs: []
  type: TYPE_NORMAL
- en: You will follow this process as you proceed with the chapter. The primary focus
    is on the deployment stage, so I will only touch lightly on some of the other
    stages.
  prefs: []
  type: TYPE_NORMAL
- en: Business Understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first stage of the process is to establish a business understanding of the
    problem you are trying to solve. You are creating a model to serve fantasy football
    managers who are running their own team in a league with other owners. The question
    they need to answer each week of the season is “How much will it cost to acquire
    this player on waivers?”
  prefs: []
  type: TYPE_NORMAL
- en: Fantasy managers can add new players to their rosters through a *waiver request*.
    In many leagues, a blind bidding auction is performed to decide who gets the best
    available players. Managers decide which players they want to bid for and put
    in the dollar amount they want to spend, which is hidden from other managers.
    When the bidding is processed on Tuesday or Wednesday of each week, the highest
    bidder gets the player at full price. Lower bidders miss out (but also don’t lose
    their money).
  prefs: []
  type: TYPE_NORMAL
- en: Each manager has a set amount of money they can use for the season, such as
    $100\. (These aren’t real-world dollars, these are fantasy dollars.) This is sometimes
    called the *free agent acquisition budget* (FAAB). A manager wants to bid high
    enough to win the bid, but not overspend. The best-case scenario would be to win
    the bid at a lower dollar amount—getting a bargain.
  prefs: []
  type: TYPE_NORMAL
- en: 'To help the manager bid enough to win the player they want without overspending,
    you will give the manager a range of predictions: the low-end cost (10th percentile),
    the median cost (50th percentile), and the high-end cost (90th percentile).'
  prefs: []
  type: TYPE_NORMAL
- en: Data Understanding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this stage, you will collect and explore the data that is available for
    your project. In the project repository, you’ll find the file *player_training_data_full.csv*.
    It contains historical fantasy football transaction data with the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: Fantasy regular season weeks remaining
  prefs: []
  type: TYPE_NORMAL
- en: How many weeks are left in the regular season. For example, in week 2 of a season
    with 14 weeks, this would be 12.
  prefs: []
  type: TYPE_NORMAL
- en: League budget percentage remaining
  prefs: []
  type: TYPE_NORMAL
- en: The percent of total dollars available in the league. For example, if $900 remain
    in the league’s original $1,200, this would be 75.
  prefs: []
  type: TYPE_NORMAL
- en: Player season number
  prefs: []
  type: TYPE_NORMAL
- en: The number of seasons this player has been in the league. Rookies have a value
    of 1.
  prefs: []
  type: TYPE_NORMAL
- en: Position
  prefs: []
  type: TYPE_NORMAL
- en: The fantasy football position of the players that was acquired.
  prefs: []
  type: TYPE_NORMAL
- en: Waiver value tier
  prefs: []
  type: TYPE_NORMAL
- en: A qualitative measure of how valuable an individual player is. Each week, some
    players are “top tier” pickup targets, and they would get a 1\. Players who are
    nothing special would get a 5\. This is a categorical feature because putting
    players into the tiers is a qualitative judgment. (You may get these from a fantasy
    website or assign them yourself.)
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin reviewing the data and selecting fields you want to use in your model,
    create a Jupyter Notebook by running the following commands in the Terminal window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Open the *player_acquisition_model.ipynb* file. As you did in [Chapter 9](ch09.html#chapter_9),
    select the Python kernel and enable the Python and Jupyter extensions, then select
    the recommended Python environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter the following title in the Markdown cell and run it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you will import the Python libraries you need. Create and run the following
    Markdown cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Add and run a new Python cell with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deploying_a_machine_learning_api_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This function is used to split data files into train and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deploying_a_machine_learning_api_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: Your model will use the `GradientBoostingRegressor` algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add another Markdown cell with the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Add and run a Python code cell with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deploying_a_machine_learning_api_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This statement removes any existing logging handlers configured by CodeSpaces.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deploying_a_machine_learning_api_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This sets the logging level to record in the log.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin loading your training data, add another Markdown cell with the following
    text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Add and run a Python code cell with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Data Preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next you will select the data to be included in the model. Rather than simply
    trying out all possible variables, you should consider the reason or theory that
    each would make a contribution to your model. Here are the three columns you’ll
    include, and the theory behind each:'
  prefs: []
  type: TYPE_NORMAL
- en: League budget percentage remaining
  prefs: []
  type: TYPE_NORMAL
- en: Your intuition is that a higher budget remaining leads to higher bids. This
    would make this a *linear* feature, in which the output variable goes up or down
    at a consistent rate as this value changes.
  prefs: []
  type: TYPE_NORMAL
- en: Fantasy regular season weeks remaining
  prefs: []
  type: TYPE_NORMAL
- en: The theory here is that players cost more at different points of the season.
    This probably isn’t a strictly linear value. History suggests some of the highest
    bids come in at the beginning of the season when the starting lineups are revealed,
    but other peak bids occur from injured players during the season and when managers
    have “use it or lose it” at the end of the season.
  prefs: []
  type: TYPE_NORMAL
- en: Waiver value tier
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level this is straightforward: higher-value players will cost more.
    But how much more? And how is each tier affected? These are more nuanced questions
    that you hope the model will be able to detect in the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: Modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now you will begin the modeling stage, first by selecting the algorithm and
    ML framework to use for your model. This decision is a combination of technical
    limitations and modeling factors.
  prefs: []
  type: TYPE_NORMAL
- en: Your technical limitations are that you want to use a Python framework and you
    want to convert the model to the ONNX format for inference. You also want to make
    predictions for the 10th, 50th, and 90th percentiles, so you need to use an algorithm
    that meets these technical criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Two modeling factors to consider are the type of output and the features you’ve
    selected. Your output will be numerical dollar values, so you will use a regression
    model (regressor). If your input features were all linear (as your input goes
    up or down, your prediction goes up or down), you could use a linear regressor.
    But your selected features are budget remaining (a linear feature), value tier
    (a categorical feature), and weeks remaining (a slightly more complicated feature).
    Because of the complexity of these features, some type of decision tree regressor
    is more appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: Based on these technical and modeling factors, you will use the [`GradientBoostingRegressor`
    algorithm](https://oreil.ly/_gqSO) from scikit-learn. The gradient boosting algorithm
    is way of combining multiple decision trees into an ensemble model that is more
    predictive than using the individual decision trees by themselves. It also supports
    the multiple predictions by percentile that you want to use. This algorithm is
    also supported by the ONNX format you will be saving the model in.
  prefs: []
  type: TYPE_NORMAL
- en: To get started with the modeling process, you will first split your data into
    multiple variables for the training (80% of the rows) and testing (20% of the
    rows). There are conventions for naming of variables, and you will follow those
    so that your code is understandable by other data scientists. [Table 13-2](#training_variables_ch13)
    explains the purpose of these variable names.
  prefs: []
  type: TYPE_NORMAL
- en: Table 13-2\. Conventional variable names for training models
  prefs: []
  type: TYPE_NORMAL
- en: '| Variable name | Purpose | Columns included | Data included |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| X (uppercase) | Input columns for full data | Input | All |'
  prefs: []
  type: TYPE_TB
- en: '| y (lowercase) | Output columns for full data | Output | All |'
  prefs: []
  type: TYPE_TB
- en: '| X_train | Input columns of training data | Input | Training data (80%) |'
  prefs: []
  type: TYPE_TB
- en: '| y_train | Output columns of training data | Output | Training data (80%)
    |'
  prefs: []
  type: TYPE_TB
- en: '| X_test | Input columns of test data | Input | Test data (20%) |'
  prefs: []
  type: TYPE_TB
- en: '| y_test | Output columns of test data | Output | Test data (20%) |'
  prefs: []
  type: TYPE_TB
- en: 'Add another Markdown cell with the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Add and run a Python code cell with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deploying_a_machine_learning_api_CO4-1)'
  prefs: []
  type: TYPE_NORMAL
- en: You are selecting three of the input columns for X.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deploying_a_machine_learning_api_CO4-2)'
  prefs: []
  type: TYPE_NORMAL
- en: You only include the output column when creating y.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_deploying_a_machine_learning_api_CO4-3)'
  prefs: []
  type: TYPE_NORMAL
- en: The `train_test_split` function reads the X and y variables and then outputs
    the variables explained in [Table 13-2](#training_variables_ch13).
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_deploying_a_machine_learning_api_CO4-4)'
  prefs: []
  type: TYPE_NORMAL
- en: This parameter determines that 20% of the data will be in the test set and 80%
    will be in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_deploying_a_machine_learning_api_CO4-5)'
  prefs: []
  type: TYPE_NORMAL
- en: If you use the same `random_state` value each time you call this method, you
    will get the same rows in the train and test variables.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you split the data, you are ready to build a model. Because you want
    to give a range of predictions, you will create three separate models. When you
    create the API, you will combine the results of these models into a single API
    call.
  prefs: []
  type: TYPE_NORMAL
- en: The process of training your model is called *fitting*, where the library takes
    a general algorithm and *fits* it or applies it to your training data to make
    a specialized model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add another Markdown cell with the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Add and run a Python code cell with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deploying_a_machine_learning_api_CO5-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This command creates a `GradientBoostingRegressor` model that will try to predict
    the 10th percentile values. In this case, this means a dollar amount that will
    be less than 90% of the bids. The next two statements are similar except with
    different percentiles.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deploying_a_machine_learning_api_CO5-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This statement uses the `fit()` method to prepare this model to make predictions
    based on the training data you provided. The next two lines do the same for the
    other two models.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, your models are in scikit_learn format and are only available
    in this Jupyter Notebook. To prepare these models for deployment and make them
    more cross-platform compatible, you will save your models in the ONNX format.
    Before doing this, you’ll need to combine the features from the X variable into
    the two-dimensional array format required by the converter. Add another Markdown
    cell with the following text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Add and run a Python code cell with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deploying_a_machine_learning_api_CO6-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This statement combines the features from the X variable into the two-dimensional
    array format required by the convertor.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deploying_a_machine_learning_api_CO6-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This statement converts the first model to ONNX format. It sets the names of
    the input and output attributes in the model by reading the first row of the `X_array`,
    which contains the element names.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_deploying_a_machine_learning_api_CO6-3)'
  prefs: []
  type: TYPE_NORMAL
- en: This statement creates a file in the local filesystem and saves the model in
    ONNX format.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Planning and training models is an iterative process, so the model at this point
    likely needs improving. In a full project, you would iteratively evaluate the
    model with formal metrics for accuracy, fairness, and other qualities that make
    a model appropriate for production. At that point, you might decide to try a different
    combination of features and tune your model in different ways.
  prefs: []
  type: TYPE_NORMAL
- en: Since this chapter is focused on deploying models, you will not be performing
    those steps. For more information about model evaluation, read *Designing Machine
    Learning Sytems* by Chip Huyen (O’Reilly, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You are are ready to deploy the model for real-time inference, with one API
    call returning a prediction that combines all three models.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 13-3](#model_serving_api_ch13) demonstrates the components used to
    create this API. If you compare this to the components of the API created in Part
    I of this book, you will see many similarities. FastAPI is still used as the controller,
    and Pydantic is still used for data transfer and data validation. However, instead
    of retrieving data from a database like the Part I API did, this API will use
    the ONNX Runtime to perform inference from the models that you trained and saved.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Model serving API components](assets/haad_1303.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-3\. Model-serving API components
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Begin by creating a Pydantic file named *schemas.py* to define the inputs and
    outputs to the API. FastAPI will use these schemas to generate the OAS file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following to this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deploying_a_machine_learning_api_CO7-1)'
  prefs: []
  type: TYPE_NORMAL
- en: The Pydantic library includes a `BaseModel` object that contains the validation
    logic.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deploying_a_machine_learning_api_CO7-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This class defines the input values that users will send to get a prediction
    from the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_deploying_a_machine_learning_api_CO7-3)'
  prefs: []
  type: TYPE_NORMAL
- en: This class defines the output that will be returned from the model. It contains
    three values—one from each model that you trained in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, create the *main.py* file, which will contain the rest of the code for
    your API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'At the top of this file, add the imports and an API description. These will
    be used in the OAS file and then displayed on the Swagger UI documentation that
    FastAPI produces. Add this Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deploying_a_machine_learning_api_CO8-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This library is used to load the models from their files and serve inferences
    in the API.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deploying_a_machine_learning_api_CO8-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This imports the Pydantic schemas, which will be used to define the inputs and
    outputs of the API.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you will add the code that uses the ONNX Runtime to load an inference
    session object for each of the three models. Then, these sessions are used to
    get labels for the input and output expected for this model. You defined the three
    inputs expected and the one output when you created the model in scikit-learn
    and then converted it to ONNX format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code to the bottom of *main.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deploying_a_machine_learning_api_CO9-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This loads the first ONNX model from the file and creates a session object that
    can be used to make inferences. The next two lines do the same for the other model
    files.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deploying_a_machine_learning_api_CO9-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This statement gets the name of the input features from the session object.
    These will be used when making inferences.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_deploying_a_machine_learning_api_CO9-3)'
  prefs: []
  type: TYPE_NORMAL
- en: This statement gets the name of the output features from the session object.
    These will be used when making inferences.
  prefs: []
  type: TYPE_NORMAL
- en: Because you have placed this code outside any function definitions, it will
    run once at startup of the API.
  prefs: []
  type: TYPE_NORMAL
- en: The next section of FastAPI code will be familiar to you if you created the
    API in Part I. The first statement creates the FastAPI `app` object using the
    API description you added previously. Then, the `@app.get()` method creates the
    health check. This is a useful best practice that allows users to check the status
    of the API before making other API calls.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code to the bottom of *main.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deploying_a_machine_learning_api_CO10-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This creates the main FastAPI `app` object using the `api_description` defined
    previously.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deploying_a_machine_learning_api_CO10-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the FastAPI decorator that defines a `GET` endpoint at the root address.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_deploying_a_machine_learning_api_CO10-3)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the function that will be excecuted when this endpoint is called. It
    returns a single Python statement to show that the API is running.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining code defines the API endpoint that provides the prediction capabilities
    for users. It begins with a Python decorator that provides information that will
    end up in the OAS file (and documentation). Then, it has the `predict()` method
    that uses the ONNX Runtime to call each model and put their outputs in the API
    response.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code to the bottom of *main.py*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_deploying_a_machine_learning_api_CO11-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This decorator creates a `POST` endpoint at the */predict* address. It will
    be used to perform inferences.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_deploying_a_machine_learning_api_CO11-2)'
  prefs: []
  type: TYPE_NORMAL
- en: The `ResponseModel` statement is used by FastAPI to define the return type of
    this endpoint. This will be used to generate the OAS file.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_deploying_a_machine_learning_api_CO11-3)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the function that will be called at this endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_deploying_a_machine_learning_api_CO11-4)'
  prefs: []
  type: TYPE_NORMAL
- en: This statement reformats the input variables into a NumPy array, which is expected
    by the ONNX Runtime to call the model.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_deploying_a_machine_learning_api_CO11-5)'
  prefs: []
  type: TYPE_NORMAL
- en: This statement calls the ONNX Runtime and gets an inference for the 10th percentile
    model using the input from the API call. The next two statements use the same
    input to call the other two models.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_deploying_a_machine_learning_api_CO11-6)'
  prefs: []
  type: TYPE_NORMAL
- en: This statement creates a `PredictionOutput` object with the inference values,
    and returns it in the API call. It rounds the values to two decimal places for
    presentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'With all of the API code completed, you are ready to run the API and test out
    the ML model. Enter the following command from the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: You will see the application startup occur as shown in [Figure 13-4](#fast_api_run_ch13).
  prefs: []
  type: TYPE_NORMAL
- en: '![ML model running](assets/haad_1304.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-4\. ML Model API running
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In Codespaces, you will also see a pop-up as shown in [Figure 13-5](#codespaces_api_open_browser).
  prefs: []
  type: TYPE_NORMAL
- en: '![Codespaces browser window popup](assets/haad_1305.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-5\. Codespaces browser window pop-up
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Click “Open in Browser” to open a browser tab outside your Codespaces. This
    browser will show a base URL ending in *app.github.dev* that contains the response
    from your API running on Codespaces. You should see the following health check
    message in your web browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This confirms your API is running. To view the interactive API documentation
    for your API, copy and paste the following onto the end of the base URL in your
    browser: **`/docs`**. For example, the full URL might be *[*https://happy-pine-tree-1234-8000.app.github.dev/docs*](https://happy-pine-tree-1234-8000.app.github.dev/docs)*
    in the browser. You should see documentation, as shown in [Figure 13-6](#swagger_docs_1_ch13).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Documentation for the ML API](assets/haad_1306.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-6\. Documentation for the ML API
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To call the ML API, select the `POST` */predict/* endpoint to open up that section,
    and then select “Try it out,” after which you should see a display as shown in
    [Figure 13-7](#swagger_docs_2_ch13).
  prefs: []
  type: TYPE_NORMAL
- en: '![Trying it out for /predict](assets/haad_1307.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-7\. Trying it out for /predict
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Here is one big difference from the API you created in Part I: you are using
    a `POST` endpoint instead of `GET`. To make a call to a `POST` endpoint, you provide
    an HTTP request body in JSON format. This is where users will provide the input
    values to send to the API. These were automatically generated based on the `FantasyAcquisitionFeatures`
    Pydantic class you defined in the previous section. Update the request body with
    the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Click Execute to send these values to the API. Scroll down and you should see
    a server response with a Code value of 200, which indicates success. The predicted
    values will be returned in the HTTP response body, which matches the definition
    of the `PredictionOutput` Pydantic class. You should see an output similar to
    [Figure 13-8](#swagger_docs_3_ch13), although the predicted values may differ.
  prefs: []
  type: TYPE_NORMAL
- en: '![API response with prediction](assets/haad_1308.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13-8\. API response with prediction
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To see the final structure of your project, execute the `tree` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! You have created the first draft of an ML model and served
    it with a REST API.
  prefs: []
  type: TYPE_NORMAL
- en: Additional Resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about data science projects, read *Practical Data Science with
    Python* by Nathan George (Packt Publishing, 2021).
  prefs: []
  type: TYPE_NORMAL
- en: To get more experience using scikit-learn and other ML libraries, read *Hands-On
    Machine Learning with Scikit-Learn, Keras, and Tensorflow* by Aurélien Géron (O’Reilly,
    2022).
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about deploying models for prediction, read *Designing Machine
    Learning Systems* by Chip Huyen (O’Reilly, 2022).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about the ML lifecycle and created an ML model
    using scikit-learn. Then, you converted the model to ONNX format to make it compatible
    with more frameworks. Finally, you deployed your model using FastAPI and used
    it for real-time inference.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 14](ch14.html#chapter_14), you will use generative AI to call an
    API using LangChain.
  prefs: []
  type: TYPE_NORMAL
