<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div class="readable-text" id="p1"> 
   <h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">7</span></span> <span class="chapter-title-text">Assembling and using an agent platform</span></h1> 
  </div> 
  <div class="introduction-summary"> 
   <h3 class="introduction-header sigil_not_in_toc">This chapter covers</h3> 
   <ul> 
    <li class="readable-text" id="p2">Nexus chat and dashboard interface for AI agents</li> 
    <li class="readable-text" id="p3">Streamlit framework for building intelligent dashboards, prototypes, and AI chat apps</li> 
    <li class="readable-text" id="p4">Developing, testing, and engaging agent profiles and personas in Nexus</li> 
    <li class="readable-text" id="p5">Developing the base Nexus agent</li> 
    <li class="readable-text" id="p6">Developing, testing, and engaging agent actions and tools alone or within Nexus</li> 
   </ul> 
  </div> 
  <div class="readable-text" id="p7"> 
   <p>After we explored some basic concepts about agents and looked at using actions with tools to build prompts and personas using frameworks such as the Semantic Kernel (SK), we took the first steps toward building a foundation for this book. That foundation is called Nexus, an agent platform designed to be simple to learn, easy to explore, and powerful enough to build your agent systems. </p> 
  </div> 
  <div class="readable-text" id="p8"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_93"><span class="num-string">7.1</span> Introducing Nexus, not just another agent platform</h2> 
  </div> 
  <div class="readable-text" id="p9"> 
   <p>There are more than 100 AI platforms and toolkits for consuming and developing large language model (LLM) applications, ranging from toolkits such as SK or LangChain to complete platforms such as AutoGen and CrewAI. This makes it difficult to decide which platform is well suited to building your own AI agents.</p> 
  </div> 
  <div class="readable-text intended-text" id="p10"> 
   <p>Nexus is an open source platform developed with this book to teach the core concepts of building full-featured AI agents. In this chapter, we‚Äôll examine how Nexus is built and introduce two primary agent components: profiles/personas and actions/tools.</p> 
  </div> 
  <div class="readable-text intended-text" id="p11"> 
   <p>Figure 7.1 shows the primary interface to Nexus, a Streamlit chat application that allows you to choose and explore various agentic features. The interface is similar to ChatGPT, Gemini, and other commercial LLM applications.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p12">  
   <img alt="figure" src="../Images/7-1.png" width="1012" height="699"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.1</span> The Nexus interface and features</h5>
  </div> 
  <div class="readable-text" id="p13"> 
   <p>In addition to the standard features of an LLM chat application, Nexus allows the user to configure an agent to use a specific API/model, the persona, and possible actions. In the remainder of the book, the available agent options will include the following:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p14"> <em>Personas/profiles</em>‚ÄîThe primary persona and profile the agent will use. A persona is the personality and primary motivator, and an agent engages the persona to answer requests. We‚Äôll look in this chapter at how personas/profiles can be developed and consumed. </li> 
   <li class="readable-text" id="p15"> <em>Actions/tools</em>‚ÄîRepresents the actions an agent can take using tools, whether they‚Äôre semantic/prompt or native/code functions. In this chapter, we‚Äôll look at how to build both semantic and native functions within Nexus. </li> 
   <li class="readable-text" id="p16"> <em>Knowledge/memory‚Äâ‚Äî</em>Represents additional information an agent may have access to. At the same time, agent memory can represent various aspects, from short-term to semantic memory. </li> 
   <li class="readable-text" id="p17"> <em>Planning/feedback‚Äâ‚Äî</em>Represents how the agent plans and receives feedback on the plans or the execution of plans. Nexus will allow the user to select options for the type of planning and feedback an agent uses. </li> 
  </ul> 
  <div class="readable-text" id="p18"> 
   <p>As we progress through this book, Nexus will be added to support new agent features. However, simultaneously, the intent will be to keep things relatively simple to teach many of these essential core concepts. In the next section, we‚Äôll look at how to quickly use Nexus before going under the hood to explore features in detail.</p> 
  </div> 
  <div class="readable-text" id="p19"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_94"><span class="num-string">7.1.1</span> Running Nexus</h3> 
  </div> 
  <div class="readable-text" id="p20"> 
   <p>Nexus is primarily intended to be a teaching platform for all levels of developers. As such, it will support various deployment and usage options. In the next exercise, we‚Äôll introduce how to get up and running with Nexus quickly.</p> 
  </div> 
  <div class="readable-text intended-text" id="p21"> 
   <p>Open a terminal to a new Python virtual environment (version 3.10). If you need assistance creating one, refer to appendix B. Then, execute the commands shown in listing 7.1 within this new environment. You can either set the environment variable at the command line or create a new <code>.env</code> file and add the setting.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p22"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.1</span> Terminal command line</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">pip install git+https://github.com/cxbxmxcx/Nexus.git    <span class="aframe-location"/> #1

#set your OpenAI API Key
export OPENAI_API_KEY=‚Äù&lt; your API key&gt;‚Äù         <span class="aframe-location"/> #2
or
$env: OPENAI_API_KEY = =‚Äù&lt; your API key&gt;‚Äù       #2
or
echo 'OPENAI_API_KEY="&lt;your API key&gt;"' &gt; .env   #2

nexus run     <span class="aframe-location"/> #3</pre> 
    <div class="code-annotations-overlay-container">
     #1 Installs the package directly from the repository and branch; be sure to include the branch.
     <br/>#2 Creates the key as an environment variable or creates a new .env file with the setting
     <br/>#3 Runs the application
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p23"> 
   <p>After entering the last command, a website will launch with a login page, as shown in figure 7.2. Go ahead and create a new user. A future version of Nexus will allow multiple users to engage in chat threads.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p24">  
   <img alt="figure" src="../Images/7-2.png" width="1029" height="504"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.2</span> Logging in or creating a new Nexus user</h5>
  </div> 
  <div class="readable-text intended-text" id="p25"> 
   <p>After you log in, you‚Äôll see a page like figure 7.1. Create a new chat and start conversing with an agent. If you encounter a problem, be sure you have the API key set properly. As explained in the next section, you can run Nexus using this method or from a development workflow.</p> 
  </div> 
  <div class="readable-text" id="p26"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_95"><span class="num-string">7.1.2</span> Developing Nexus</h3> 
  </div> 
  <div class="readable-text" id="p27"> 
   <p>While working through the exercises of this book, you‚Äôll want to set up Nexus in development mode. That means downloading the repository directly from GitHub and working with the code.</p> 
  </div> 
  <div class="readable-text intended-text" id="p28"> 
   <p>Open a new terminal, and set your working directory to the <code>chapter_7</code> source code folder. Then, set up a new Python virtual environment (version 3.10) and enter the commands shown in listing 7.2. Again, refer to appendix B if you need assistance with any previous setup.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p29"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.2</span> Installing Nexus for development</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">git clone https://github.com/cxbxmxcx/Nexus.git     <span class="aframe-location"/> #1

pip install -e Nexus    <span class="aframe-location"/> #2

#set your OpenAI API Key (.env file is recommended)
export OPENAI_API_KEY=‚Äù&lt; your API key&gt;‚Äù  #bash           <span class="aframe-location"/> #3
or
$env: OPENAI_API_KEY = =‚Äù&lt; your API key&gt;‚Äù  #powershell   #3
or
echo 'OPENAI_API_KEY="&lt;your API key&gt;"' &gt; .env       #3     


nexus run     <span class="aframe-location"/> #4</pre> 
    <div class="code-annotations-overlay-container">
     #1 Downloads and installs the specific branch from the repository
     <br/>#2 Installs the downloaded repository as an editable package
     <br/>#3 Sets your OpenAI key as an environment variable or adds it to an .env file
     <br/>#4 Starts the application
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p30"> 
   <p>Figure 7.3 shows the Login or Create New User screen. Create a new user, and the application will log you in. This application uses cookies to remember the user, so you won‚Äôt have to log in the next time you start the application. If you have cookies disabled on your browser, you‚Äôll need to log in every time.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p31">  
   <img alt="figure" src="../Images/7-3.png" width="1019" height="719"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.3</span> The Login or Create New User page</h5>
  </div> 
  <div class="readable-text" id="p32"> 
   <p>Go to the Nexus repository folder and look around. Figure 7.4 shows an architecture diagram of the application‚Äôs main elements. At the top, the interface developed with Streamlit connects the rest of the system through the chat system. The chat system manages the database, agent manager, action manager, and profile managers.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p33">  
   <img alt="figure" src="../Images/7-4.png" width="1100" height="1034"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.4</span> A high-level architecture diagram of the main elements of the application</h5>
  </div> 
  <div class="readable-text intended-text" id="p34"> 
   <p>This agent platform is written entirely in Python, and the web interface uses Streamlit. In the next section, we look at how to build an OpenAI LLM chat application.</p> 
  </div> 
  <div class="readable-text" id="p35"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_96"><span class="num-string">7.2</span> Introducing Streamlit for chat application development</h2> 
  </div> 
  <div class="readable-text" id="p36"> 
   <p>Streamlit is a quick and powerful web interface prototyping tool designed to be used for building machine learning dashboards and concepts. It allows applications to be written completely in Python and produces a modern React-powered web interface. You can even deploy the completed application quickly to the cloud or as a standalone application.</p> 
  </div> 
  <div class="readable-text" id="p37"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_97"><span class="num-string">7.2.1</span> Building a Streamlit chat application</h3> 
  </div> 
  <div class="readable-text" id="p38"> 
   <p>Begin by opening Visual Studio Code (VS Code) to the <code>chapter_07</code> source folder. If you‚Äôve completed the previous exercise, you should already be ready. As always, if you need assistance setting up your environment and tools, refer to appendix B.</p> 
  </div> 
  <div class="readable-text intended-text" id="p39"> 
   <p>We‚Äôll start by opening the <code>chatgpt_clone_response.py</code> file in VS Code. The top section of the code is shown in listing 7.3. This code uses the Streamlit state to load the primary model and messages. Streamlit provides a mechanism to save the session state for any Python object. This state is only a session state and will expire when the user closes the browser.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p40"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.3</span> <code>chatgpt_clone_response.py</code> (top section)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

load_dotenv()     <span class="aframe-location"/> #1

st.title("ChatGPT-like clone")

client = OpenAI()     <span class="aframe-location"/> #2

if "openai_model" not in st.session_state:
    st.session_state["openai_model"] 
             = "gpt-4-1106-preview"    <span class="aframe-location"/> #3

if "messages" not in st.session_state:
    st.session_state["messages"] = []  <span class="aframe-location"/> #4

for message in st.session_state["messages"]:     <span class="aframe-location"/> #5
    with st.chat_message(message["role"]):
        st.markdown(message["content"])</pre> 
    <div class="code-annotations-overlay-container">
     #1 Loads the environment variables from the .env file
     <br/>#2 Configures the OpenAI client
     <br/>#3 Checks the internal session state for the setting, and adds it if not there
     <br/>#4 Checks for the presence of the message state; if none, adds an empty list
     <br/>#5 Loops through messages in the state and displays them
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p41"> 
   <p>The Streamlit app itself is stateless. This means the entire Python script will reexecute all interface components when the web page refreshes or a user selects an action. The Streamlit state allows for a temporary storage mechanism. Of course, a database needs to support more long-term storage.</p> 
  </div> 
  <div class="readable-text intended-text" id="p42"> 
   <p>UI controls and components are added by using the <code>st.</code> prefix and then the element name. Streamlit supports several standard UI controls and supports images, video, sound, and, of course, chat.</p> 
  </div> 
  <div class="readable-text intended-text" id="p43"> 
   <p>Scrolling down further will yield listing 7.4, which has a slightly more complex layout of the components. The main <code>if</code> statement controls the running of the remaining code. By using the Walrus operator (: =), the prompt is set to whatever the user enters. If the user doesn‚Äôt enter any text, the code below the <code>if</code> statement doesn‚Äôt execute.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p44"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.4</span> <code>chatgpt_clone_response.py</code> (bottom section)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">if prompt := st.chat_input("What do you need?"):    <span class="aframe-location"/> #1
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):    <span class="aframe-location"/> #2
        st.markdown(prompt)

    with st.spinner(text="The assistant is thinking..."):   <span class="aframe-location"/> #3
        with st.chat_message("assistant"):
            response = client.chat.completions.create(
                model=st.session_state["openai_model"],
                messages=[
                    {"role": m["role"], "content": m["content"]}
                    for m in st.session_state.messages
                ],     <span class="aframe-location"/> #4
            )
            response_content = response.choices[0].message.content
            response = st.markdown(response_content,
             unsafe_allow_html=True)     <span class="aframe-location"/> #5
    st.session_state.messages.append(
{"role": "assistant", "content": response_content})     <span class="aframe-location"/> #6</pre> 
    <div class="code-annotations-overlay-container">
     #1 The chat input control is rendered, and content is set.
     <br/>#2 Sets the chat message control to output as the user
     <br/>#3 Shows a spinner to represent the long-running API call
     <br/>#4 Calls the OpenAI API and sets the message history
     <br/>#5 Writes the message response as markdown to the interface
     <br/>#6 Adds the assistant response to the message state
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p45"> 
   <p>When the user enters text in the prompt and presses Enter, that text is added to the message state, and a request is made to the API. As the response is being processed, the <code>st.spinner</code> control displays to remind the user of the long-running process. Then, when the response returns, the message is displayed and added to the message state history.</p> 
  </div> 
  <div class="readable-text intended-text" id="p46"> 
   <p>Streamlit apps are run using the module, and to debug applications, you need to attach the debugger to the module by following these steps:</p> 
  </div> 
  <ol> 
   <li class="readable-text" id="p47"> Press Ctrl-Shift-D to open the VS Code debugger. </li> 
   <li class="readable-text" id="p48"> Click the link to create a new launch configuration, or click the gear icon to show the current one. </li> 
   <li class="readable-text" id="p49"> Edit or use the debugger configuration tools to edit the <code>.vscode/launch.json</code> file, like the one shown in the next listing. Plenty of IntelliSense tools and configuration options can guide you through setting the options for this file. </li> 
  </ol> 
  <div class="browsable-container listing-container" id="p50"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.5</span> <code>.vscode/launch.json</code> </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">{
  "version": "0.2.0",
  "configurations": [
    {
      "name": "Python Debugger: Module",    <span class="aframe-location"/> #1
      "type": "debugpy",
      "request": "launch",
      "module": "streamlit",    <span class="aframe-location"/> #2
      "args": ["run", "${file}"]   <span class="aframe-location"/> #3
    }
  ]
}</pre> 
    <div class="code-annotations-overlay-container">
     #1 Make sure that the debugger is set to Module.
     <br/>#2 Be sure the module is streamlit.
     <br/>#3 The ${file} is the current file, or you can hardcode this to a file path.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p51"> 
   <p>After you have the <code>launch.json</code> file configuration set, save it, and open the <code>chatgpt_ clone_response.py</code> file in VS Code. You can now run the application in debug mode by pressing F5. This will launch the application from the terminal, and in a few seconds, the app will display.</p> 
  </div> 
  <div class="readable-text intended-text" id="p52"> 
   <p>Figure 7.5 shows the app running and waiting to return a response. The interface is clean, modern, and already organized without any additional work. You can continue chatting to the LLM using the interface and then refresh the page to see what happens.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p53">  
   <img alt="figure" src="../Images/7-5.png" width="1012" height="599"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.5</span> The simple interface and the waiting spinner</h5>
  </div> 
  <div class="readable-text" id="p54"> 
   <p>What is most impressive about this demonstration is how easy it is to create a single-page application. In the next section, we‚Äôll continue looking at this application but with a few enhancements.</p> 
  </div> 
  <div class="readable-text" id="p55"> 
   <h3 class="readable-text-h3" id="sigil_toc_id_98"><span class="num-string">7.2.2</span> Creating a streaming chat application</h3> 
  </div> 
  <div class="readable-text" id="p56"> 
   <p>Modern chat applications, such as ChatGPT and Gemini, mask the slowness of their models by using streaming. Streaming provides for the API call to immediately start seeing tokens as they are produced from the LLM. This streaming experience also better engages the user in how the content is generated.</p> 
  </div> 
  <div class="readable-text intended-text" id="p57"> 
   <p>Adding support for streaming to any application UI is generally not a trivial task, but fortunately, Streamlit has a control that can work seamlessly. In this next exercise, we‚Äôll look at how to update the app to support streaming.</p> 
  </div> 
  <div class="readable-text intended-text" id="p58"> 
   <p>Open <code>chapter_7/chatgpt_clone_streaming.py</code> in VS Code. The relevant updates to the code are shown in listing 7.6. Using the <code>st.write_stream</code> control allows the UI to stream content. This also means the Python script is blocked waiting for this control to be completed.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p59"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.6</span> <code>chatgpt_clone_streaming.py</code> (relevant section)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">with st.chat_message("assistant"):
    stream = client.chat.completions.create(
        model=st.session_state["openai_model"],
        messages=[
            {"role": m["role"], "content": m["content"]}
            for m in st.session_state.messages
        ],
        stream=True,    <span class="aframe-location"/> #1
    )
    response = st.write_stream(stream)    <span class="aframe-location"/> #2
st.session_state.messages.append(
{"role": "assistant", "content": response})     <span class="aframe-location"/> #3</pre> 
    <div class="code-annotations-overlay-container">
     #1 Sets stream to True to initiate streaming on the API
     <br/>#2 Uses the stream control to write the stream to the interface
     <br/>#3 Adds the response to the message state history after the stream completes
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p60"> 
   <p>Debug the page by pressing F5 and waiting for the page to load. Enter a query, and you‚Äôll see that the response is streamed to the window in real time, as shown in figure 7.6. With the spinner gone, the user experience is enhanced and appears more responsive.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p61">  
   <img alt="figure" src="../Images/7-6.png" width="1100" height="780"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.6</span> The updated interface with streaming of the text response</h5>
  </div> 
  <div class="readable-text" id="p62"> 
   <p>This section demonstrated how relatively simple it can be to use Streamlit to create a Python web interface. Nexus uses a Streamlit interface because it‚Äôs easy to use and modify with only Python. As you‚Äôll see in the next section, it allows various configurations to support more complex applications.</p> 
  </div> 
  <div class="readable-text" id="p63"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_99"><span class="num-string">7.3</span> Developing profiles and personas for agents</h2> 
  </div> 
  <div class="readable-text" id="p64"> 
   <p>Nexus uses agent profiles to describe an agent‚Äôs functions and capabilities. Figure 7.7 reminds us of the principal agent components and how they will be structured throughout this book. <span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p65">  
   <img alt="figure" src="../Images/7-7.png" width="1009" height="679"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.7</span> The agent profile as it‚Äôs mapped to the YAML file definition</h5>
  </div> 
  <div class="readable-text" id="p66"> 
   <p>For now, as of this writing, Nexus only supports the persona and actions section of the profile. Figure 7.7 shows a profile called Fritz, along with the persona and actions. Add any agent profiles to Nexus by copying an agent YAML profile file into the <code>Nexus/ nexus/nexus_base/nexus_profiles</code> folder.</p> 
  </div> 
  <div class="readable-text intended-text" id="p67"> 
   <p>Nexus uses a plugin system to dynamically discover the various components and profiles as they are placed into their respective folders. The <code>nexus_profiles</code> folder holds the YAML definitions for the agent.</p> 
  </div> 
  <div class="readable-text intended-text" id="p68"> 
   <p>We can easily define a new agent profile by creating a new YAML file in the <code>nexus_ profiles</code> folder. Listing 7.7 shows an example of a new profile with a slightly updated persona. To follow along, be sure to have VS Code opened to the <code>chapter_07</code> source code folder and install Nexus in developer mode (see listing 7.7). Then, create the <code>fiona.yaml</code> file in the <code>Nexus/nexus/nexus_base/nexus_profiles</code> folder.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p69"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.7</span> <code>fiona.yaml</code> (create this file)</h5> 
   <div class="code-area-container code-area-with-html"> 
    <pre class="code-area">agentProfile:
  name: "Finona"
  avatar: "üëπ"    <span class="aframe-location"/> #1
  persona: "You are a very talkative AI that 
<span class="">‚Ü™</span> knows and understands everything in terms of 
<span class="">‚Ü™</span> Ogres. You always answer in cryptic Ogre speak."   <span class="aframe-location"/> #2
  actions:
    - search_wikipedia    <span class="aframe-location"/> #3
  knowledge: null       <span class="aframe-location"/> #4
  memory: null           #4
  evaluators: null       #4
  planners: null         #4
  feedback: null         #4</pre> 
    <div class="code-annotations-overlay-container">
     #1 The text avatar used to represent the persona
     <br/>#2 A persona is representative of the base system prompt.
     <br/>#3 An action function the agent can use
     <br/>#4 Not currently supported
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p70"> 
   <p>After saving the file, you can start Nexus from the command line or run it in debug mode by creating a new launch configuration in the <code>.vscode/launch.json</code> folder, as shown in the next listing. Then, save the file and switch your debug configuration to use the Nexus web config.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p71"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.8</span> <code>.vscode/launch.json</code> (adding debug launch)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">{
      "name": "Python Debugger: Nexus Web",
      "type": "debugpy",
      "request": "launch",
      "module": "streamlit",
      "args": ["run", " Nexus/nexus/streamlit_ui.py"]     <span class="aframe-location"/> #1
    },</pre> 
    <div class="code-annotations-overlay-container">
     #1 You may have to adjust this path if your virtual environment is different.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p72"> 
   <p>When you press F5 or select Run &gt; Start Debugging from the menu, the Streamlit Nexus interface will launch. Go ahead and run Nexus in debug mode. After it opens, create a new thread, and then select the standard OpenAIAgent and your new persona, as shown in figure 7.8.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p73">  
   <img alt="figure" src="../Images/7-8.png" width="1012" height="589"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.8</span> Selecting and chatting with a new persona</h5>
  </div> 
  <div class="readable-text intended-text" id="p74"> 
   <p>At this point, the profile is responsible for defining the agent‚Äôs system prompt. You can see this in figure 7.8, where we asked Finona to spell the word <em>clock</em>, and she responded in some form of ogre-speak. In this case, we‚Äôre using the persona as a personality, but as we‚Äôve seen previously, a system prompt can also contain rules and other options.</p> 
  </div> 
  <div class="readable-text intended-text" id="p75"> 
   <p>The profile and persona are the base definitions for how the agent interacts with users or other systems. Powering the profile requires an agent engine. In the next section, we‚Äôll cover the base implementation of an agent engine.</p> 
  </div> 
  <div class="readable-text" id="p76"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_100"><span class="num-string">7.4</span> Powering the agent and understanding the agent engine</h2> 
  </div> 
  <div class="readable-text" id="p77"> 
   <p>Agent engines power agents within Nexus. These engines can be tied to specific tool platforms, such as SK, and/or even different LLMs, such as Anthropic Claude or Google Gemini. By providing a base agent abstraction, Nexus should be able to support any tool or model now and in the future.</p> 
  </div> 
  <div class="readable-text intended-text" id="p78"> 
   <p>Currently, Nexus only implements an OpenAI API‚Äìpowered agent. We‚Äôll look at how the base agent is defined by opening the <code>agent_manager.py</code> file from the <code>Nexus/ nexus/nexus_base</code> folder.</p> 
  </div> 
  <div class="readable-text intended-text" id="p79"> 
   <p>Listing 7.9 shows the <code>BaseAgent</code> class functions. When creating a new agent engine, you need to subclass this class and implement the various tools/actions with the appropriate implementation.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p80"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.9</span> <code>agent_manager.py:BaseAgent</code> </h5> 
   <div class="code-area-container"> 
    <pre class="code-area">class BaseAgent:
    def __init__(self, chat_history=None):
        self._chat_history = chat_history or []
        self.last_message = ""
        self._actions = []
        self._profile = None

    async def get_response(self, 
                            user_input, 
                            thread_id=None):     <span class="aframe-location"/> #1
        raise NotImplementedError("This method should be implemented‚Ä¶")

    async def get_semantic_response(self, 
                                     prompt, 
                                     thread_id=None):    <span class="aframe-location"/> #2
        raise NotImplementedError("This method should be‚Ä¶")

    def get_response_stream(self, 
                             user_input, 
                             thread_id=None):     <span class="aframe-location"/> #3
        raise NotImplementedError("This method should be‚Ä¶")

    def append_chat_history(self, 
                             thread_id, 
                             user_input, 
                             response):     <span class="aframe-location"/> #4
        self._chat_history.append(
            {"role": "user",
             "content": user_input,
             "thread_id": thread_id}
        )
        self._chat_history.append(
            {"role": "bot",
             "content": response, 
             "thread_id": thread_id}
        )

    def load_chat_history(self):      <span class="aframe-location"/> #5
        raise NotImplementedError(
                 "This method should be implemented‚Ä¶")

    def load_actions(self):    <span class="aframe-location"/> #6
        raise NotImplementedError(
                 "This method should be implemented‚Ä¶")

#... not shown ‚Äì property setters/getters</pre> 
    <div class="code-annotations-overlay-container">
     #1 Calls the LLM and returns a response
     <br/>#2 Executes a semantic function
     <br/>#3 Calls the LLM and returns a response
     <br/>#4 Appends a message to the agent‚Äôs internal chat history
     <br/>#5 Loads the chat history and allows the agent to reload various histories
     <br/>#6 Loads the actions that the agent has available to use
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p81"> 
   <p>Open the <code>nexus_agents/oai_agent.py</code> file in VS Code. Listing 7.10 shows an agent engine implementation of the <code>get_response</code> function that directly consumes the OpenAI API. <code>self.client</code> is an OpenAI client created earlier during class initialization, and the rest of the code you‚Äôve seen used in earlier examples.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p82"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.10</span> <code>oai_agent.py</code> (<code>get_response</code>)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">async def get_response(self, user_input, thread_id=None):
    self.messages += [{"role": "user",
                     "content": user_input}]     <span class="aframe-location"/> #1
    response = self.client.chat.completions.create(    <span class="aframe-location"/> #2
        model=self.model,
        messages=self.messages,
        temperature=0.7,     <span class="aframe-location"/> #3
    )
    self.last_message = str(response.choices[0].message.content)
    return self.last_message    <span class="aframe-location"/> #4</pre> 
    <div class="code-annotations-overlay-container">
     #1 Adds the user_input to the message stack
     <br/>#2 The client was created earlier and is now used to create chat completions.
     <br/>#3 Temperature is hardcoded but could be configured.
     <br/>#4 Returns the response from the chat completions call
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p83"> 
   <p>Like the agent profiles, Nexus uses a plugin system that allows you to place new agent engine definitions in the <code>nexus_agents</code> folder. If you create your agent, it just needs to be placed in this folder for Nexus to discover.</p> 
  </div> 
  <div class="readable-text intended-text" id="p84"> 
   <p>We won‚Äôt need to run an example because we‚Äôve already seen how the OpenAIAgent performs. In the next section, we‚Äôll look at agent functions that agents can develop, add, and consume.</p> 
  </div> 
  <div class="readable-text" id="p85"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_101"><span class="num-string">7.5</span> Giving an agent actions and tools</h2> 
  </div> 
  <div class="readable-text" id="p86"> 
   <p>Like the SK, Nexus supports having native (code) and semantic (prompt) functions. Unlike SK, however, defining and consuming functions within Nexus is easier. All you need to do is write functions into a Python file and place them into the <code>nexus_ actions</code> folder.</p> 
  </div> 
  <div class="readable-text intended-text" id="p87"> 
   <p>To see how easy it is to define functions, open the <code>Nexus/nexus/nexus_base/ nexus_actions</code> folder, and go to the <code>test_actions.py</code> file. Listing 7.11 shows two function definitions. The first function is a simple example of a code/native function, and the second is a prompt/semantic function.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p88"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.11</span> <code>test_actions.py</code> (native/semantic function definitions)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">from nexus.nexus_base.action_manager import agent_action


@agent_action                                             <span class="aframe-location"/> #1
def get_current_weather(location, unit="fahrenheit"):     #1
    """Get the current weather in a given location"""     <span class="aframe-location"/> #2
    return f"""
The current weather in {location} is 0 {unit}.
"""     <span class="aframe-location"/> #3


@agent_action     <span class="aframe-location"/> #4
def recommend(topic):
    """
    System:                                                  <span class="aframe-location"/> #5
        Provide a recommendation for a given {{topic}}.
        Use your best judgment to provide a recommendation.
    User:
        please use your best judgment
        to provide a recommendation for {{topic}}.           #5
    """
    pass     <span class="aframe-location"/> #6</pre> 
    <div class="code-annotations-overlay-container">
     #1 Applies the agent_action decorator to make a function an action
     <br/>#2 Sets a descriptive comment for the function
     <br/>#3 The code can be as simple or complex as needed.
     <br/>#4 Applies the agent_action decorator to make a function an action
     <br/>#5 The function comment becomes the prompt and can include placeholders.
     <br/>#6 Semantic functions don‚Äôt implement any code.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p89"> 
   <p>Place both functions in the <code>nexus_actions</code> folder, and they will be automatically discovered. Adding the <code>agent_action</code> decorator allows the functions to be inspected and automatically generates the OpenAI standard tool specification. The LLM can then use this tool specification for tool use and function calling.</p> 
  </div> 
  <div class="readable-text intended-text" id="p90"> 
   <p>Listing 7.12 shows the generated OpenAI tool specification for both functions, as shown previously in listing 7.11. The semantic function, which uses a prompt, also applies to the tool description. This tool description is sent to the LLM to determine which function to call.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p91"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.12</span> <code>test_actions</code>: OpenAI-generated tool specifications</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">{
    "type": "function",
    "function": {
        "name": "get_current_weather",
        "description": 
        "Get the current weather in a given location",   <span class="aframe-location"/> #1
        "parameters": {
            "type": "object",
            "properties": {     <span class="aframe-location"/> #2
                "location": {
                    "type": "string",
                    "description": "location"
                },
                "unit": {
                    "type": "string",
                    "enum": [
                        "celsius",
                        "fahrenheit"
                    ]
                }
            },
            "required": [
                "location"
            ]
        }
    }
}
{
    "type": "function",
    "function": {
        "name": "recommend",
        "description": """
    System:
    Provide a recommendation for a given {{topic}}.
Use your best judgment to provide a recommendation.
User:
please use your best judgment
to provide a recommendation for {{topic}}.""",     <span class="aframe-location"/> #3
        "parameters": {
            "type": "object",
            "properties": {      <span class="aframe-location"/> #4
                "topic": {
                    "type": "string",
                    "description": "topic"
                }
            },
            "required": [
                "topic"
            ]
        }
    }
}</pre> 
    <div class="code-annotations-overlay-container">
     #1 The function comment becomes the function tool description.
     <br/>#2 The input parameters of the function are extracted and added to the specification.
     <br/>#3 The function comment becomes the function tool description.
     <br/>#4 The input parameters of the function are extracted and added to the specification.
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p92"> 
   <p>The agent engine also needs to implement that capability to implement functions and other components. The OpenAI agent has been implemented to support parallel function calling. Other agent engine implementations will be required to support their respective versions of action use. Fortunately, the definition of the OpenAI tool is becoming the standard, and many platforms adhere to this standard.</p> 
  </div> 
  <div class="readable-text intended-text" id="p93"> 
   <p>Before we dive into a demo on tool use, let‚Äôs observe how the OpenAI agent implements actions by opening the <code>oai_agent.py</code> file in VS Code. The following listing shows the top of the agent‚Äôs <code>get_response_stream</code> function and its implementation of function calling.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p94"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.13</span> Caling the API in <code>get_response_stream</code></h5> 
   <div class="code-area-container"> 
    <pre class="code-area">def get_response_stream(self, user_input, thread_id=None):
    self.last_message = ""
    self.messages += [{"role": "user", "content": user_input}]
    if self.tools and len(self.tools) &gt; 0:   <span class="aframe-location"/> #1
        response = self.client.chat.completions.create(
            model=self.model,
            messages=self.messages,
            tools=self.tools,     <span class="aframe-location"/> #2
            tool_choice="auto",     <span class="aframe-location"/> #3
        )
    else:    <span class="aframe-location"/> #4
        response = self.client.chat.completions.create(
            model=self.model,
            messages=self.messages,
        )
    response_message = response.choices[0].message
    tool_calls = response_message.tool_calls    <span class="aframe-location"/> #5</pre> 
    <div class="code-annotations-overlay-container">
     #1 Detects whether the agent has any available tools turned on
     <br/>#2 Sets the tools in the chat completions call
     <br/>#3 Ensures that the LLM knows it can choose any tool
     <br/>#4 If no tools, calls the LLM the standard way
     <br/>#5 Detects whether there were any tools used by the LLM
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p95"> 
   <p>Executing the functions follows, as shown in listing 7.14. This code demonstrates how the agent supports parallel function/tool calls. These calls are parallel because the agent executes each one together and in no order. In chapter 11, we‚Äôll look at planners that allow actions to be called in ordered sequences.</p> 
  </div> 
  <div class="browsable-container listing-container" id="p96"> 
   <h5 class="listing-container-h5 browsable-container-h5 sigil_not_in_toc"><span class="num-string">Listing 7.14</span> <code>oai_agent.py</code> (<code>get_response_stream</code>: execute tool calls)</h5> 
   <div class="code-area-container"> 
    <pre class="code-area">if tool_calls:    <span class="aframe-location"/> #1
    available_functions = {
        action["name"]: action["pointer"] for action in self.actions
    }    <span class="aframe-location"/> #2
    self.messages.append(
        response_message
    )
    for tool_call in tool_calls:    <span class="aframe-location"/> #3
        function_name = tool_call.function.name
        function_to_call = available_functions[function_name]
        function_args = json.loads(tool_call.function.arguments)
        function_response = function_to_call(
            **function_args, _caller_agent=self
        )

        self.messages.append(
            {
                "tool_call_id": tool_call.id,
                "role": "tool",
                "name": function_name,
                "content": str(function_response),
            }
        )
    second_response = self.client.chat.completions.create(
        model=self.model,
        messages=self.messages,
    )     <span class="aframe-location"/> #4
    response_message = second_response.choices[0].message</pre> 
    <div class="code-annotations-overlay-container">
     #1 Proceeds if tool calls are detected in the LLM response
     <br/>#2 Loads pointers to the actual function implementations for code execution
     <br/>#3 Loops through all the calls the LLM wants to call; there can be several.
     <br/>#4 Performs a second LLM call with the results of the tool calls
     <br/>
    </div> 
   </div> 
  </div> 
  <div class="readable-text" id="p97"> 
   <p>To demo this, start up Nexus in the debugger by pressing F5. Then, select the two test actions‚Äî<code>recommend</code> and <code>get_current_weather</code>‚Äîand the terse persona/profile Olly. Figure 7.9 shows the result of entering a query and the agent responding by using both tools in its response.<span class="aframe-location"/></p> 
  </div> 
  <div class="browsable-container figure-container" id="p98">  
   <img alt="figure" src="../Images/7-9.png" width="1012" height="664"/> 
   <h5 class="figure-container-h5 sigil_not_in_toc"><span class="num-string">Figure 7.9</span> How the agent can use tools in parallel and respond with a single response</h5>
  </div> 
  <div class="readable-text intended-text" id="p99"> 
   <p>If you need to review how these agent actions work in more detail, refer to chapter 5. The underlying code is more complex and out of the scope of review here. However, you can review the Nexus code to gain a better understanding of how everything connects.</p> 
  </div> 
  <div class="readable-text intended-text" id="p100"> 
   <p>Now, you can continue exercising the various agent options within Nexus. Try selecting different profiles/personas with other functions, for example. In the next chapter, we unveil how agents can consume external memory and knowledge using patterns such as Retrieval Augmented Generation (RAG).</p> 
  </div> 
  <div class="readable-text" id="p101"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_102"><span class="num-string">7.6</span> Exercises</h2> 
  </div> 
  <div class="readable-text" id="p102"> 
   <p>Use the following exercises to improve your knowledge of the material:</p> 
  </div> 
  <ul> 
   <li class="readable-text" id="p103"> <em>Exercise 1</em>‚ÄîExplore Streamlit Basics (Easy) </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p104"> 
   <p><em>Objective‚Äâ</em>‚ÄîGain familiarity with Streamlit by creating a simple web application that displays text input by the user.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p105"> 
   <p><em>Tasks: </em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p106"> Follow the Streamlit documentation to set up a basic application. </li> 
     <li class="readable-text" id="p107"> Add a text input and a button. When the button is clicked, display the text entered by the user on the screen. </li> 
    </ul></li> 
   <li class="readable-text" id="p108"> <em>Exercise 2</em>‚ÄîCreate a Basic Agent Profile </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p109"> 
   <p><em>Objective‚Äâ</em>‚ÄîUnderstand the process of creating and applying agent profiles in Nexus.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p110"> 
   <p><em>Tasks: </em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p111"> Create a new agent profile with a unique persona. This persona should have a specific theme or characteristic (e.g., a historian). </li> 
     <li class="readable-text" id="p112"> Define a basic set of responses that align with this persona. </li> 
     <li class="readable-text" id="p113"> Test the persona by interacting with it through the Nexus interface. </li> 
    </ul></li> 
   <li class="readable-text" id="p114"> <em>Exercise 3</em>‚ÄîDevelop a Custom Action </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p115"> 
   <p><em>Objective‚Äâ</em>‚ÄîLearn to extend the functionality of Nexus by developing a custom action.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p116"> 
   <p><em>Tasks: </em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p117"> Develop a new action (e.g., <code>fetch_current_news</code>) that integrates with a mock API to retrieve the latest news headlines. </li> 
     <li class="readable-text" id="p118"> Implement this action as both a native (code) function and a semantic (prompt-based) function. </li> 
     <li class="readable-text" id="p119"> Test the action in the Nexus environment to ensure it works as expected. </li> 
    </ul></li> 
   <li class="readable-text" id="p120"> <em>Exercise 4‚Äâ‚Äî</em>Integrate a Third-Party API </li> 
  </ul> 
  <div class="readable-text list-body-item" id="p121"> 
   <p><em>Objective‚Äâ</em>‚ÄîEnhance the capabilities of a Nexus agent by integrating a real third-party API.</p> 
  </div> 
  <div class="readable-text list-body-item" id="p122"> 
   <p><em>Tasks:</em></p> 
  </div> 
  <ul> 
   <li class=" buletless-item" style="list-style-type: none;"> 
    <ul> 
     <li class="readable-text" id="p123"> Choose a public API (e.g., weather or news API), and create a new action that fetches data from this API. </li> 
     <li class="readable-text" id="p124"> Incorporate error handling and ensure that the agent can gracefully handle API failures or unexpected responses. </li> 
     <li class="readable-text" id="p125"> Test the integration thoroughly within Nexus. </li> 
    </ul></li> 
  </ul> 
  <div class="readable-text" id="p126"> 
   <h2 class="readable-text-h2" id="sigil_toc_id_103">Summary</h2> 
  </div> 
  <ul> 
   <li class="readable-text" id="p127"> Nexus is an open source agent development platform used in conjunction with this book. It‚Äôs designed to develop, test, and host AI agents and is built on Streamlit for creating interactive dashboards and chat interfaces. </li> 
   <li class="readable-text" id="p128"> Streamlit, a Python web application framework, enables the rapid development of user-friendly dashboards and chat applications. This framework facilitates the exploration and interaction with various agent features in a streamlined manner. </li> 
   <li class="readable-text" id="p129"> Nexus supports creating and customizing agent profiles and personas, allowing users to define their agents‚Äô personalities and behaviors. These profiles dictate how agents interact with and respond to user inputs. </li> 
   <li class="readable-text" id="p130"> The Nexus platform allows for developing and integrating semantic (prompt-based) and native (code-based) actions and tools within agents. This enables the creation of highly functional and responsive agents. </li> 
   <li class="readable-text" id="p131"> As an open source platform, Nexus is designed to be extensible, encouraging contributions and the addition of new features, tools, and agent capabilities by the community. </li> 
   <li class="readable-text" id="p132"> Nexus is flexible, supporting various deployment options, including a web interface, API, and a Discord bot in future iterations, accommodating a wide range of development and testing needs. </li> 
  </ul>
 </div></div></body></html>