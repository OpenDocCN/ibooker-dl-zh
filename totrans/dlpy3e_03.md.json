["```py\n>>> import tensorflow as tf\n>>> # Equivalent to np.ones(shape=(2, 1))\n>>> tf.ones(shape=(2, 1))\ntf.Tensor([[1.], [1.]], shape=(2, 1), dtype=float32)\n>>> # Equivalent to np.zeros(shape=(2, 1))\n>>> tf.zeros(shape=(2, 1))\ntf.Tensor([[0.], [0.]], shape=(2, 1), dtype=float32)\n>>> # Equivalent to np.array([1, 2, 3], dtype=\"float32\")\n>>> tf.constant([1, 2, 3], dtype=\"float32\")\ntf.Tensor([1., 2., 3.], shape=(3,), dtype=float32)\n```", "```py\n>>> # Tensor of random values drawn from a normal distribution with\n>>> # mean 0 and standard deviation 1\\. Equivalent to\n>>> # np.random.normal(size=(3, 1), loc=0., scale=1.).\n>>> x = tf.random.normal(shape=(3, 1), mean=0., stddev=1.)\n>>> print(x)\ntf.Tensor(\n[[-0.14208166]\n [-0.95319825]\n [ 1.1096532 ]], shape=(3, 1), dtype=float32)\n>>> # Tensor of random values drawn from a uniform distribution between\n>>> # 0 and 1\\. Equivalent to np.random.uniform(size=(3, 1), low=0.,\n>>> # high=1.).\n>>> x = tf.random.uniform(shape=(3, 1), minval=0., maxval=1.)\n>>> print(x)\ntf.Tensor(\n[[0.33779848]\n [0.06692922]\n [0.7749394 ]], shape=(3, 1), dtype=float32)\n```", "```py\nimport numpy as np\n\nx = np.ones(shape=(2, 2))\nx[0, 0] = 0.0 \n```", "```py\nx = tf.ones(shape=(2, 2))\n# This will fail, as a tensor isn't assignable.\nx[0, 0] = 0.0 \n```", "```py\n>>> v = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))\n>>> print(v)\narray([[-0.75133973],\n       [-0.4872893 ],\n       [ 1.6626885 ]], dtype=float32)>\n```", "```py\n>>> v.assign(tf.ones((3, 1)))\narray([[1.],\n       [1.],\n       [1.]], dtype=float32)>\n```", "```py\n>>> v[0, 0].assign(3.)\narray([[3.],\n       [1.],\n       [1.]], dtype=float32)>\n```", "```py\n>>> v.assign_add(tf.ones((3, 1)))\narray([[2.],\n       [2.],\n       [2.]], dtype=float32)>\n```", "```py\na = tf.ones((2, 2))\n# Takes the square, same as np.square\nb = tf.square(a)\n# Takes the square root, same as np.sqrt\nc = tf.sqrt(a)\n# Adds two tensors (element-wise)\nd = b + c\n# Takes the product of two tensors (see chapter 2), same as np.matmul\ne = tf.matmul(a, b)\n# Concatenates a and b along axis 0, same as np.concatenate\nf = tf.concat((a, b), axis=0) \n```", "```py\ndef dense(inputs, W, b):\n    return tf.nn.relu(tf.matmul(inputs, W) + b) \n```", "```py\ninput_var = tf.Variable(initial_value=3.0)\nwith tf.GradientTape() as tape:\n    result = tf.square(input_var)\ngradient = tape.gradient(result, input_var) \n```", "```py\ninput_const = tf.constant(3.0)\nwith tf.GradientTape() as tape:\n    tape.watch(input_const)\n    result = tf.square(input_const)\ngradient = tape.gradient(result, input_const) \n```", "```py\ntime = tf.Variable(0.0)\nwith tf.GradientTape() as outer_tape:\n    with tf.GradientTape() as inner_tape:\n        position = 4.9 * time**2\n    speed = inner_tape.gradient(position, time)\n# We use the outer tape to compute the gradient of the gradient from\n# the inner tape. Naturally, the answer is 4.9 * 2 = 9.8.\nacceleration = outer_tape.gradient(speed, time) \n```", "```py\n@tf.function\ndef dense(inputs, W, b):\n    return tf.nn.relu(tf.matmul(inputs, W) + b) \n```", "```py\n@tf.function(jit_compile=True)\ndef dense(inputs, W, b):\n    return tf.nn.relu(tf.matmul(inputs, W) + b) \n```", "```py\nimport numpy as np\n\nnum_samples_per_class = 1000\nnegative_samples = np.random.multivariate_normal(\n    # Generates the first class of points: 1,000 random 2D points with\n    # specified \"mean\" and \"covariance matrix.\" Intuitively, the\n    # \"covariance matrix\" describes the shape of the point cloud, and\n    # the \"mean\" describes its position in the plane. `cov=[[1,\n    # 0.5],[0.5, 1]]` corresponds to \"an oval-like point cloud oriented\n    # from bottom left to top right.\"\n    mean=[0, 3], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class\n)\npositive_samples = np.random.multivariate_normal(\n    # Generates the other class of points with a different mean and the\n    # same covariance matrix (point cloud with a different position and\n    # the same shape)\n    mean=[3, 0], cov=[[1, 0.5], [0.5, 1]], size=num_samples_per_class\n) \n```", "```py\ninputs = np.vstack((negative_samples, positive_samples)).astype(np.float32) \n```", "```py\ntargets = np.vstack(\n    (\n        np.zeros((num_samples_per_class, 1), dtype=\"float32\"),\n        np.ones((num_samples_per_class, 1), dtype=\"float32\"),\n    )\n) \n```", "```py\nimport matplotlib.pyplot as plt\n\nplt.scatter(inputs[:, 0], inputs[:, 1], c=targets[:, 0])\nplt.show() \n```", "```py\n# The inputs will be 2D points.\ninput_dim = 2\n# The output predictions will be a single score per sample (close to 0\n# if the sample is predicted to be in class 0, and close to 1 if the\n# sample is predicted to be in class 1).\noutput_dim = 1\nW = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim)))\nb = tf.Variable(initial_value=tf.zeros(shape=(output_dim,))) \n```", "```py\ndef model(inputs, W, b):\n    return tf.matmul(inputs, W) + b \n```", "```py\ndef mean_squared_error(targets, predictions):\n    # per_sample_losses will be a tensor with the same shape as targets\n    # and predictions, containing per-sample loss scores.\n    per_sample_losses = tf.square(targets - predictions)\n    # We need to average these per-sample loss scores into a single\n    # scalar loss value: reduce_mean does this.\n    return tf.reduce_mean(per_sample_losses) \n```", "```py\nlearning_rate = 0.1\n\n# Wraps the function in a tf.function decorator to speed it up\n@tf.function(jit_compile=True)\ndef training_step(inputs, targets, W, b):\n    # Forward pass, inside of a gradient tape scope\n    with tf.GradientTape() as tape:\n        predictions = model(inputs, W, b)\n        loss = mean_squared_error(predictions, targets)\n    # Retrieves the gradient of the loss with regard to weights\n    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])\n    # Updates the weights\n    W.assign_sub(grad_loss_wrt_W * learning_rate)\n    b.assign_sub(grad_loss_wrt_b * learning_rate)\n    return loss \n```", "```py\nfor step in range(40):\n    loss = training_step(inputs, targets, W, b)\n    print(f\"Loss at step {step}: {loss:.4f}\") \n```", "```py\npredictions = model(inputs, W, b)\nplt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)\nplt.show() \n```", "```py\n# Generates 100 regularly spaced numbers between -1 and 4, which we\n# will use to plot our line\nx = np.linspace(-1, 4, 100)\n# This is our line's equation.\ny = -W[0] / W[1] * x + (0.5 - b) / W[1]\n# Plots our line (`\"-r\"` means \"plot it as a red line\")\nplt.plot(x, y, \"-r\")\n# Plots our model's predictions on the same plot\nplt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5) \n```", "```py\n>>> import torch\n>>> # Unlike in other frameworks, the shape argument is named \"size\"\n>>> # rather than \"shape.\"\n>>> torch.ones(size=(2, 1))\ntensor([[1.], [1.]])\n>>> torch.zeros(size=(2, 1))\ntensor([[0.], [0.]])\n>>> # Unlike in other frameworks, you cannot pass dtype=\"float32\" as a\n>>> # string. The dtype argument must be a torch dtype instance.\n>>> torch.tensor([1, 2, 3], dtype=torch.float32)\ntensor([1., 2., 3.])\n```", "```py\n>>> # Equivalent to tf.random.normal(shape=(3, 1), mean=0., stddev=1.)\n>>> torch.normal(\n... mean=torch.zeros(size=(3, 1)),\n... std=torch.ones(size=(3, 1)))\ntensor([[-0.9613],\n        [-2.0169],\n        [ 0.2088]])\n```", "```py\n>>> # Equivalent to tf.random.uniform(shape=(3, 1), minval=0.,\n>>> # maxval=1.)\n>>> torch.rand(3, 1)\n```", "```py\n>>> x = torch.zeros(size=(2, 1))\n>>> x[0, 0] = 1.\n>>> x\ntensor([[1.],\n        [0.]])\n```", "```py\n>>> x = torch.zeros(size=(2, 1))\n>>> # A Parameter can only be created using a torch.Tensor value â€” no\n>>> # NumPy arrays allowed.\n>>> p = torch.nn.parameter.Parameter(data=x)\n```", "```py\na = torch.ones((2, 2))\n# Takes the square, same as np.square\nb = torch.square(a)\n# Takes the square root, same as np.sqrt\nc = torch.sqrt(a)\n# Adds two tensors (element-wise)\nd = b + c\n# Takes the product of two tensors (see chapter 2), same as np.matmul\ne = torch.matmul(a, b)\n# Concatenates a and b along axis 0, same as np.concatenate\nf = torch.cat((a, b), dim=0) \n```", "```py\ndef dense(inputs, W, b):\n    return torch.nn.relu(torch.matmul(inputs, W) + b) \n```", "```py\n>>> # To compute gradients with respect to a tensor, it must be created\n>>> # with requires_grad=True.\n>>> input_var = torch.tensor(3.0, requires_grad=True)\n>>> result = torch.square(input_var)\n>>> # Calling backward() populates the \"grad\" attribute on all tensors\n>>> # create with requires_grad=True.\n>>> result.backward()\n>>> gradient = input_var.grad\n>>> gradient\ntensor(6.)\n```", "```py\n>>> result = torch.square(input_var)\n>>> result.backward()\n>>> # .grad will sum all gradient values from each time backward() is\n>>> # called.\n>>> input_var.grad\ntensor(12.)\n```", "```py\n>>> input_var.grad = None\n```", "```py\ninput_dim = 2\noutput_dim = 1\n\nW = torch.rand(input_dim, output_dim, requires_grad=True)\nb = torch.zeros(output_dim, requires_grad=True) \n```", "```py\ndef model(inputs, W, b):\n    return torch.matmul(inputs, W) + b \n```", "```py\ndef mean_squared_error(targets, predictions):\n    per_sample_losses = torch.square(targets - predictions)\n    return torch.mean(per_sample_losses) \n```", "```py\nlearning_rate = 0.1\n\ndef training_step(inputs, targets, W, b):\n    # Forward pass\n    predictions = model(inputs)\n    loss = mean_squared_error(targets, predictions)\n    # Computes gradients\n    loss.backward()\n    # Retrieves gradients\n    grad_loss_wrt_W, grad_loss_wrt_b = W.grad, b.grad\n    with torch.no_grad():\n        # Updates weights inside a no_grad scope\n        W -= grad_loss_wrt_W * learning_rate\n        b -= grad_loss_wrt_b * learning_rate\n    # Resets gradients\n    W.grad = None\n    b.grad = None\n    return loss \n```", "```py\nclass LinearModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.W = torch.nn.Parameter(torch.rand(input_dim, output_dim))\n        self.b = torch.nn.Parameter(torch.zeros(output_dim))\n\n    def forward(self, inputs):\n        return torch.matmul(inputs, self.W) + self.b \n```", "```py\nmodel = LinearModel() \n```", "```py\ntorch_inputs = torch.tensor(inputs)\noutput = model(torch_inputs) \n```", "```py\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) \n```", "```py\ndef training_step(inputs, targets):\n    predictions = model(inputs)\n    loss = mean_squared_error(targets, predictions)\n    loss.backward()\n    optimizer.step()\n    model.zero_grad()\n    return loss \n```", "```py\nwith torch.no_grad():\n    W -= grad_loss_wrt_W * learning_rate\n    b -= grad_loss_wrt_b * learning_rate \n```", "```py\ncompiled_model = torch.compile(model) \n```", "```py\n@torch.compile\ndef dense(inputs, W, b):\n    return torch.nn.relu(torch.matmul(inputs, W) + b) \n```", "```py\n>>> from jax import numpy as jnp\n>>> jnp.ones(shape=(2, 1))\nArray([[1.],\n       [1.]], dtype=float32)\n>>> jnp.zeros(shape=(2, 1))\nArray([[0.],\n       [0.]], dtype=float32)\n>>> jnp.array([1, 2, 3], dtype=\"float32\")\nArray([1., 2., 3.], dtype=float32)\n```", "```py\n>>> np.random.normal(size=(3,))\narray([-1.68856166,  0.16489586,  0.67707523])\n>>> np.random.normal(size=(3,))\narray([-0.73671259,  0.3053194 ,  0.84124895])\n```", "```py\ndef apply_noise(x, seed):\n    np.random.seed(seed)\n    x = x * np.random.normal((3,))\n    return x\n\nseed = 1337\ny = apply_noise(x, seed)\nseed += 1\nz = apply_noise(x, seed) \n```", "```py\nimport jax\n\nseed_key = jax.random.key(1337) \n```", "```py\n>>> seed_key = jax.random.key(0)\n>>> jax.random.normal(seed_key, shape=(3,))\nArray([ 1.8160863 , -0.48262316,  0.33988908], dtype=float32)\n```", "```py\n>>> seed_key = jax.random.key(123)\n>>> jax.random.normal(seed_key, shape=(3,))\nArray([-0.1470326,  0.5524756,  1.648498 ], dtype=float32)\n>>> jax.random.normal(seed_key, shape=(3,))\nArray([-0.1470326,  0.5524756,  1.648498 ], dtype=float32)\n```", "```py\n>>> seed_key = jax.random.key(123)\n>>> jax.random.normal(seed_key, shape=(3,))\nArray([-0.1470326,  0.5524756,  1.648498 ], dtype=float32)\n>>> # You could even split your key into multiple new keys at once!\n>>> new_seed_key = jax.random.split(seed_key, num=1)[0]\n>>> jax.random.normal(new_seed_key, shape=(3,))\nArray([ 0.5362355, -1.1920372,  2.450225 ], dtype=float32)\n```", "```py\n>>> x = jnp.array([1, 2, 3], dtype=\"float32\")\n>>> new_x = x.at[0].set(10)\n```", "```py\na = jnp.ones((2, 2))\n# Takes the square\nb = jnp.square(a)\n# Takes the square root\nc = jnp.sqrt(a)\n# Adds two tensors (element-wise)\nd = b + c\n# Takes the product of two tensors (see chapter 2)\ne = jnp.matmul(a, b)\n# Multiplies two tensors (element-wise)\ne *= d \n```", "```py\ndef dense(inputs, W, b):\n    return jax.nn.relu(jnp.matmul(inputs, W) + b) \n```", "```py\ndef compute_loss(input_var):\n    return jnp.square(input_var) \n```", "```py\ngrad_fn = jax.grad(compute_loss) \n```", "```py\ninput_var = jnp.array(3.0)\ngrad_of_loss_wrt_input_var = grad_fn(input_var) \n```", "```py\ngrad_fn = jax.value_and_grad(compute_loss)\noutput, grad_of_loss_wrt_input_var = grad_fn(input_var) \n```", "```py\n# state contains a, b, and c. It must be the first argument.\ndef compute_loss(state, x, y):\n    ...\n    return loss\n\ngrad_fn = jax.value_and_grad(compute_loss)\nstate = (a, b, c)\n# grads_of_loss_wrt_state has the same structure as state.\nloss, grads_of_loss_wrt_state = grad_fn(state, x, y) \n```", "```py\ndef compute_loss(state, x, y):\n    ...\n    # Returns a tuple\n    return loss, output\n\n# Passes has_aux=True here\ngrad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n# Gets back a nested tuple\nloss, (grads_of_loss_wrt_state, output) = grad_fn(state, x, y) \n```", "```py\n@jax.jit\ndef dense(inputs, W, b):\n    return jax.nn.relu(jnp.matmul(inputs, W) + b) \n```", "```py\ndef model(inputs, W, b):\n    return jnp.matmul(inputs, W) + b\n\ndef mean_squared_error(targets, predictions):\n    per_sample_losses = jnp.square(targets - predictions)\n    return jnp.mean(per_sample_losses) \n```", "```py\ndef compute_loss(state, inputs, targets):\n    W, b = state\n    predictions = model(inputs, W, b)\n    loss = mean_squared_error(targets, predictions)\n    return loss \n```", "```py\ngrad_fn = jax.value_and_grad(compute_loss) \n```", "```py\nlearning_rate = 0.1\n\n# We use the jax.jit decorator to take advantage of XLA compilation.\n@jax.jit\ndef training_step(inputs, targets, W, b):\n    # Computes the forward pass and backward pass in one go\n    loss, grads = grad_fn((W, b), inputs, targets)\n    grad_wrt_W, grad_wrt_b = grads\n    # Updates W and b\n    W = W - grad_wrt_W * learning_rate\n    b = b - grad_wrt_b * learning_rate\n    # Make sure to return the new values of W and b in addition to the\n    # loss!\n    return loss, W, b \n```", "```py\ninput_dim = 2\noutput_dim = 1\n\nW = jax.numpy.array(np.random.uniform(size=(input_dim, output_dim)))\nb = jax.numpy.array(np.zeros(shape=(output_dim,)))\nstate = (W, b)\nfor step in range(40):\n    loss, W, b = training_step(inputs, targets, W, b)\n    print(f\"Loss at step {step}: {loss:.4f}\") \n```", "```py\nimport os\n\n# Sets the environment variable from within the Python runtime\nos.environ[\"KERAS_BACKEND\"] = \"jax\"\n\n# Only then should you import Keras.\nimport keras \n```", "```py\n{\n    # Default floating-point precision. It should typically not be\n    # changed.\n    \"floatx\": \"float32\",\n    # Default numerical fuzzing factor. It should typically not be\n    # changed.\n    \"epsilon\": 1e-07,\n    # Change \"tensorflow\" to \"jax\" or \"torch.\"\n    \"backend\": \"tensorflow\",\n    # This is the default image layout. We'll talk about this in\n    # chapter 8.\n    \"image_data_format\": \"channels_last\",\n} \n```", "```py\nimport keras\n\n# All Keras layers inherit from the base Layer class.\nclass SimpleDense(keras.Layer):\n    def __init__(self, units, activation=None):\n        super().__init__()\n        self.units = units\n        self.activation = activation\n\n    # Weight creation takes place in the build() method.\n    def build(self, input_shape):\n        batch_dim, input_dim = input_shape\n        # add_weight is a shortcut method for creating weights. It's\n        # also possible to create standalone variables and assign them\n        # as layer attributes, like self.W = keras.Variable(shape=...,\n        # initializer=...).\n        self.W = self.add_weight(\n            shape=(input_dim, self.units), initializer=\"random_normal\"\n        )\n        self.b = self.add_weight(shape=(self.units,), initializer=\"zeros\")\n\n    # We define the forward pass computation in the call() method.\n    def call(self, inputs):\n        y = keras.ops.matmul(inputs, self.W) + self.b\n        if self.activation is not None:\n            y = self.activation(y)\n        return y \n```", "```py\n>>> # Instantiates our layer, defined previously\n>>> my_dense = SimpleDense(units=32, activation=keras.ops.relu)\n>>> # Creates some test inputs\n>>> input_tensor = keras.ops.ones(shape=(2, 784))\n>>> # Calls the layer on the inputs, just like a function\n>>> output_tensor = my_dense(input_tensor)\n>>> print(output_tensor.shape)\n(2, 32)\n```", "```py\nfrom keras import layers\n\n# A dense layer with 32 output units\nlayer = layers.Dense(32, activation=\"relu\") \n```", "```py\nfrom keras import models\nfrom keras import layers\n\nmodel = models.Sequential(\n    [\n        layers.Dense(32, activation=\"relu\"),\n        layers.Dense(32),\n    ]\n) \n```", "```py\nmodel = NaiveSequential(\n    [\n        NaiveDense(input_size=784, output_size=32, activation=\"relu\"),\n        NaiveDense(input_size=32, output_size=64, activation=\"relu\"),\n        NaiveDense(input_size=64, output_size=32, activation=\"relu\"),\n        NaiveDense(input_size=32, output_size=10, activation=\"softmax\"),\n    ]\n) \n```", "```py\ndef __call__(self, inputs):\n    if not self.built:\n        self.build(inputs.shape)\n        self.built = True\n    return self.call(inputs) \n```", "```py\nmodel = keras.Sequential(\n    [\n        SimpleDense(32, activation=\"relu\"),\n        SimpleDense(64, activation=\"relu\"),\n        SimpleDense(32, activation=\"relu\"),\n        SimpleDense(10, activation=\"softmax\"),\n    ]\n) \n```", "```py\n# Defines a linear classifier\nmodel = keras.Sequential([keras.layers.Dense(1)])\nmodel.compile(\n    # Specifies the optimizer by name: RMSprop (it's case-insensitive)\n    optimizer=\"rmsprop\",\n    # Specifies the loss by name: mean squared error\n    loss=\"mean_squared_error\",\n    # Specifies a list of metrics: in this case, only accuracy\n    metrics=[\"accuracy\"],\n) \n```", "```py\nmodel.compile(\n    optimizer=keras.optimizers.RMSprop(),\n    loss=keras.losses.MeanSquaredError(),\n    metrics=[keras.metrics.BinaryAccuracy()],\n) \n```", "```py\nmodel.compile(\n    optimizer=keras.optimizers.RMSprop(learning_rate=1e-4),\n    loss=my_custom_loss,\n    metrics=[my_custom_metric_1, my_custom_metric_2],\n) \n```", "```py\nhistory = model.fit(\n    # The input examples, as a NumPy array\n    inputs,\n    # The corresponding training targets, as a NumPy array\n    targets,\n    # The training loop will iterate over the data 5 times.\n    epochs=5,\n    # The training loop will iterate over the data in batches of 128\n    # examples.\n    batch_size=128,\n) \n```", "```py\n>>> history.history\n{\"binary_accuracy\": [0.855, 0.9565, 0.9555, 0.95, 0.951],\n \"loss\": [0.6573270302042366,\n  0.07434618508815766,\n  0.07687718723714351,\n  0.07412414988875389,\n  0.07617757616937161]}\n```", "```py\nmodel = keras.Sequential([keras.layers.Dense(1)])\nmodel.compile(\n    optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\n    loss=keras.losses.MeanSquaredError(),\n    metrics=[keras.metrics.BinaryAccuracy()],\n)\n\n# To avoid having samples from only one class in the validation data,\n# shuffles the inputs and targets using a random indices permutation\nindices_permutation = np.random.permutation(len(inputs))\nshuffled_inputs = inputs[indices_permutation]\nshuffled_targets = targets[indices_permutation]\n\n# Reserves 30% of the training inputs and targets for \"validation.\"\n# (We'll exclude these samples from training and reserve them to\n# compute the \"validation loss\" and metrics).\nnum_validation_samples = int(0.3 * len(inputs))\nval_inputs = shuffled_inputs[:num_validation_samples]\nval_targets = shuffled_targets[:num_validation_samples]\ntraining_inputs = shuffled_inputs[num_validation_samples:]\ntraining_targets = shuffled_targets[num_validation_samples:]\nmodel.fit(\n    # Training data, used to update the weights of the model\n    training_inputs,\n    training_targets,\n    epochs=5,\n    batch_size=16,\n    # Validation data, used only to monitor the \"validation loss\" and\n    # metrics\n    validation_data=(val_inputs, val_targets),\n) \n```", "```py\n# Takes a NumPy array or a tensor for your current backend and returns\n# a tensor for your current backend\npredictions = model(new_inputs) \n```", "```py\n# Takes a NumPy array or a Dataset and returns a NumPy array\npredictions = model.predict(new_inputs, batch_size=128) \n```", "```py\n>>> predictions = model.predict(val_inputs, batch_size=128)\n>>> print(predictions[:10])\n[[0.3590725 ]\n [0.82706255]\n [0.74428225]\n [0.682058  ]\n [0.7312616 ]\n [0.6059811 ]\n [0.78046083]\n [0.025846  ]\n [0.16594526]\n [0.72068727]]\n```"]