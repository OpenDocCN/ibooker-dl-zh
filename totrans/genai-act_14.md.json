["```py\nfrom openai import AzureOpenAI\nimport evaluate\nfrom bert_score import BERTScorer\n...\n\nAOAI_API_KEY = os.getenv(\"AOAI_KEY\")           #1\nAZURE_ENDPOINT = os.getenv(\"AOAI_ENDPOINT\")   #1\n...\n\nURL = \"https://www.gatesfoundation.org/ideas/articles/ \n             ↪artificial-intelligence-ai-development-principles\"\n\ndef get_article(URL, config):                 #2\n    article = Article(URL, config=config)\n    article.download(recursion_counter=2)\n    article.parse()\n    article.nlp()\n    return article.text, article.summary\n\ndef generate_summary(client, article_text):               #3\n    prompt = f\"Summarize the following article:\\n\\n{article_text}\"\n    conversation = [{\"role\": \"system\", \"content\": \n                             ↪\"You are a helpful assistant.\"}]\n    conversation.append({\"role\": \"user\", \"content\": prompt})\n\n    response = client.chat.completions.create(\n        model=MODEL,\n        messages = conversation,\n        temperature = TEMPERATURE,\n        max_tokens = MAX_TOKENS,\n    )\n    return response.choices[0].message.content.strip()\n\ndef calculate_scores(generated_summary, reference_summary): #4\n    metric = evaluate.load(\"bleu\", trust_remote_code=True)\n    bleu_score = metric.compute(predictions=\n                 ↪[generated_summary], references=[reference_summary])\n\n    metric = evaluate.load(\"rouge\", trust_remote_code=True)\n    rouge_score = metric.compute(predictions=\n                  ↪[generated_summary], references=[reference_summary])\n\n    scorer = BERTScorer(lang=\"en\")\n    p1, r1, f1 = scorer.score([generated_summary], [reference_summary])\n    bert_score = f\"Precision: {p1} Recall: {r1} F1 Score: {f1.tolist()[0]}\"\n\n    return bleu_score, rouge_score, bert_score\n\n# Main code\nclient = AzureOpenAI(\n      azure_endpoint = AZURE_ENDPOINT,\n      api_key=AOAI_API_KEY,\n      api_version=API_VERSION\n)\n\nconfig = Config()                                             #5\nconfig.browser_user_agent = USER_AGENT                       #5\nconfig.request_timeout = 10                                  #5\narticle_text, reference_summary = get_article(URL, config)\n\ngenerated_summary = generate_summary(client, article_text)\nbleu_score, rouge_score, bert_score = calculate_scores(\n                         ↪generated_summary, reference_summary)\n\nprint(f\"BLEU:{bleu_score}, ROUGE:{rouge_score}, BERT: {bert_score}\")\n```", "```py\nBLEU score: {'bleu': 0.04699157347901134, \n             ↪'precisions': [0.32857142857142857, \n                    ↪0.09352517985611511, \n                    ↪0.021739130434782608, \n                    ↪0.0072992700729927005], \n            ↪'brevity_penalty': 1.0, \n            ↪'length_ratio': 1.2727272727272727, \n            ↪'translation_length': 140, [\n            ↪'reference_length': 110}\n\nROUGE score: {'rouge1': 0.3463203463203463, ]\n              ↪'rouge2': 0.09606986899563319, \n              ↪'rougeL': 0.1645021645021645, \n              ↪'rougeLsum': 0.2683982683982684}\n\nBERT score: Precision: tensor([0.8524]) \n            ↪Recall: tensor([0.8710]) \n            ↪F1 Score: 0.8616269826889038\n```", "```py\nEVALUATION_PROMPT_TEMPLATE = \"\"\"                       #1\nYou will be given one summary written for an article. Your task [\n↪is to rate the summary using one metric. Make sure you read \n↪and understand these instructions very carefully. \n\nEvaluation Criteria:\n{criteria}\n\nEvaluation Steps:\n{steps}\n\nExample:\nSource Text:\n\n{document}\n\nSummary:\n{summary}\n\nEvaluation Form (scores ONLY):\n- {metric_name}\n\"\"\"\n\n# Metric 1: Relevance             #2\nRELEVANCY_SCORE_CRITERIA = \"\"\"\nRelevance(1-5) - selection of important content from the source. \\\nThe summary should include only important information from the \n↪source document. \\\nAnnotators were instructed to penalize summaries which contained \n↪redundancies and excess information.\n\"\"\"\n\nRELEVANCY_SCORE_STEPS = \"\"\"        #3\n1\\. Read the summary and the source document carefully.\n2\\. Compare the summary to the source document and identify the \n   ↪main points of the article.\n3\\. Assess how well the summary covers the main points of the article,\n   ↪and how much irrelevant or redundant information it contains.\n4\\. Assign a relevance score from 1 to 5.\n\"\"\"\n\n# Metric 2: Coherence             #4\nCOHERENCE_SCORE_CRITERIA = \"\"\"\nCoherence(1-5) - the collective quality of all sentences. \\\n...\n\nCOHERENCE_SCORE_STEPS = \"\"\"                  #5\n1\\. Read the article carefully and identify the main topic and key points.\n2\\. Read the summary and compare it to the article. Check if the summary \n...\n\n# Metric 3: Consistency                  #6\nCONSISTENCY_SCORE_CRITERIA = \"\"\"\nConsistency(1-5) - the factual alignment between the summary and the \n↪summarized source.\n...\n\nCONSISTENCY_SCORE_STEPS = \"\"\"               #7\n1\\. Read the article carefully and identify its main facts and details.\n2\\. Read the summary and compare it to the article. Check if the \n   ↪summary ...\n\n# Metric 4: Fluency                         #8\nFLUENCY_SCORE_CRITERIA = \"\"\"\nFluency(1-3): the quality of the summary in terms of grammar, spelling, punctuation, word choice, and sentence structure.\n...\n\nFLUENCY_SCORE_STEPS = \"\"\"                 #9\nRead the summary and evaluate its fluency based on the given criteria. \n↪Assign a fluency score from 1 to 3.\n...\n```", "```py\ndef get_article():                  #1\n    with open('./data/gates_foundation_article.txt', 'r') as f:\n        article = f.read()\n    with open('./data/gates_foundation_summary1.txt', 'r') as f:\n        nlp_summary = f.read()\n    with open('./data/gates_foundation_summary2.txt', 'r') as f:\n        llm_summary = f.read()\n    return article, nlp_summary, llm_summary\n\ndef get_geval_score(criteria: str, steps: str, document: str, \n↪summary: str, metric_name: str):                 #2\n    prompt = EVALUATION_PROMPT_TEMPLATE.format(        #3\n        criteria=criteria,\n        steps=steps,\n        metric_name=metric_name,\n        document=document,\n        summary=summary,\n    )\n\n    response = client.chat.completions.create(      #4\n        model=MODEL,\n        messages = [{\"role\": \"user\", \"content\": prompt}],\n        temperature = 0,\n        max_tokens = 5,\n        top_p = 1,\n        frequency_penalty = 0,\n        presence_penalty = 0,\n        stop = None\n    )\n    return response.choices[0].message.content\n\nevaluation_metrics = {\n    \"Relevance\": (RELEVANCY_SCORE_CRITERIA, RELEVANCY_SCORE_STEPS),      \n    \"Coherence\": (COHERENCE_SCORE_CRITERIA, COHERENCE_SCORE_STEPS),\n    \"Consistency\": (CONSISTENCY_SCORE_CRITERIA, CONSISTENCY_SCORE_STEPS),\n    \"Fluency\": (FLUENCY_SCORE_CRITERIA, FLUENCY_SCORE_STEPS),\n}\n\n# Main code\nclient = AzureOpenAI(\n    azure_endpoint = AZURE_ENDPOINT,\n    api_key=AOAI_API_KEY,\n    api_version=API_VERSION\n)\n\narticle_text, nlp_summary, llm_summary = get_article()\n\nsummaries = {\"NLP Summary (1)\": nlp_summary,\n             \"LLM Summary (2)\": llm_summary}\n\ndata = {\"Evaluation Score\": [], \"Summary\": [], \"Score\": []}   #5\n\nprint(\"Starting evaluation...\")\nfor eval_type, (criteria, steps) in evaluation_metrics.items():\n    for summ_type, summary in summaries.items():                #6\n        data[\"Evaluation Score\"].append(eval_type)                 #6\n        data[\"Summary\"].append(summ_type)                          #6\n        result = get_geval_score(criteria, steps, article_text,    #6\n                                 summary, eval_type)               #6 \n        numeric_part = ''.join(filter(str.isdigit, \n                          ↪result.strip()))              #7\n        if numeric_part:\n            score_num = int(float(result.strip()))                  \n            data[\"Score\"].append(score_num)                         #8\n\nmax_values = {key: max(values) for key, values in data.items()}\n\ndf = pd.DataFrame(data)                             #9\n\npivot_df = df.pivot(index='Evaluation Score',                  #10\n                    columns='Summary',\n                    values='Score')\n\nprint(pivot_df)\n```", "```py\nStarting evaluation...\nSummary          LLM Summary (2) NLP Summary (1)\nEvaluation Score\nCoherence                     5*              1\nConsistency                   5*              4\nFluency                       3               2\nRelevance                     5*              2\n```"]