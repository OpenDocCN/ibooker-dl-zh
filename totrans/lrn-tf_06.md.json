["```py\nimport os\nimport math\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.tensorboard.plugins import projector\n\nbatch_size=64\nembedding_dimension = 5\nnegative_samples =8  \nLOG_DIR = \"logs/word2vec_intro\"\n\ndigit_to_word_map = {1:\"One\",2:\"Two\", 3:\"Three\", 4:\"Four\", 5:\"Five\",\n          6:\"Six\",7:\"Seven\",8:\"Eight\",9:\"Nine\"}\nsentences = []\n\n# Create two kinds of sentences - sequences of odd and even digits\nfor i in range(10000):\n  rand_odd_ints = np.random.choice(range(1,10,2),3)\n  sentences.append(\" \".join([digit_to_word_map[r] for r in rand_odd_ints]))\n  rand_even_ints = np.random.choice(range(2,10,2),3)\n  sentences.append(\" \".join([digit_to_word_map[r] for r in rand_even_ints]))\n\n```", "```py\nsentences[0:10]\n\nOut:\n['Seven One Five',\n'Four Four Four',\n'Five One Nine',\n'Eight Two Eight',\n'One Nine Three',\n'Two Six Eight',\n'Nine Seven Seven',\n'Six Eight Six',\n'One Five Five',\n'Four Six Two']\n```", "```py\n# Map words to indices\nword2index_map ={}\nindex=0\nfor sent in sentences:\n  for word in sent.lower().split():\n    if word not in word2index_map:\n      word2index_map[word] = index\n      index+=1  \nindex2word_map = {index: word for word, index in word2index_map.items()}      \nvocabulary_size = len(index2word_map)\n\n```", "```py\n# Generate skip-gram pairs\nskip_gram_pairs = []\nfor sent in sentences:\n  tokenized_sent = sent.lower().split()\n  for i in range(1, len(tokenized_sent)-1) :    \n    word_context_pair = [[word2index_map[tokenized_sent[i-1]],\n               word2index_map[tokenized_sent[i+1]]],\n               word2index_map[tokenized_sent[i]]]\n    skip_gram_pairs.append([word_context_pair[1],\n                word_context_pair[0][0]])\n    skip_gram_pairs.append([word_context_pair[1],\n                word_context_pair[0][1]])\n\ndef get_skipgram_batch(batch_size):\n  instance_indices = list(range(len(skip_gram_pairs)))\n  np.random.shuffle(instance_indices)\n  batch = instance_indices[:batch_size]\n  x = [skip_gram_pairs[i][0] for i in batch]\n  y = [[skip_gram_pairs[i][1]] for i in batch]\n  return x,y\n\n```", "```py\nskip_gram_pairs[0:10]\n\nOut:\n[[1, 0],\n[1, 2],\n[3, 3],\n[3, 3],\n[1, 2],\n[1, 4],\n[6, 5],\n[6, 5],\n[4, 1],\n[4, 7]]\n\n```", "```py\n# Batch example\nx_batch,y_batch = get_skipgram_batch(8)\nx_batch\ny_batch\n[index2word_map[word] for word in x_batch]\n[index2word_map[word[0]] for word in y_batch]\n\nx_batch\n\nOut:\n[6, 2, 1, 1, 3, 0, 7, 2]\n\ny_batch\n\nOut: \n[[5], [0], [4], [0], [5], [4], [1], [7]]\n\n[index2word_map[word] for word in x_batch]\n\nOut: \n['two', 'five', 'one', 'one', 'four', 'seven', 'three', 'five']\n\n[index2word_map[word[0]] for word in y_batch]\n\nOut: \n['eight', 'seven', 'nine', 'seven', 'eight',\n 'nine', 'one', 'three']\n\n```", "```py\n# Input data, labels\ntrain_inputs = tf.placeholder(tf.int32, shape=[batch_size])\ntrain_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n```", "```py\nwith tf.name_scope(\"embeddings\"):\n  embeddings = tf.Variable(\n    tf.random_uniform([vocabulary_size, embedding_dimension],\n             -1.0, 1.0),name='embedding')\n  # This is essentially a lookup table\n  embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n\n```", "```py\n# Create variables for the NCE loss\nnce_weights = tf.Variable(\n    tf.truncated_normal([vocabulary_size, embedding_dimension],\n              stddev=1.0 / math.sqrt(embedding_dimension)))\nnce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n\nloss = tf.reduce_mean(\n  tf.nn.nce_loss(weights = nce_weights, biases = nce_biases, inputs = embed, \n       labels = train_labels, num_sampled = negative_samples, num_classes =\n                vocabulary_size))\n\n```", "```py\n# Learning rate decay\nglobal_step = tf.Variable(0, trainable=False)\nlearningRate = tf.train.exponential_decay(learning_rate=0.1,\n                     global_step= global_step,\n                     decay_steps=1000,\n                     decay_rate= 0.95,\n                     staircase=True)\ntrain_step = tf.train.GradientDescentOptimizer(learningRate).minimize(loss)\n```", "```py\n# Merge all summary ops\nmerged = tf.summary.merge_all()\n\nwith tf.Session() as sess:\n  train_writer = tf.summary.FileWriter(LOG_DIR,\n                    graph=tf.get_default_graph())\n  saver = tf.train.Saver()\n\n  with open(os.path.join(LOG_DIR,'metadata.tsv'), \"w\") as metadata:\n    metadata.write('Name\\tClass\\n')\n    for k,v in index2word_map.items():\n      metadata.write('%s\\t%d\\n' % (v, k))\n\n  config = projector.ProjectorConfig()\n  embedding = config.embeddings.add()\n  embedding.tensor_name = embeddings.name\n  # Link embedding to its metadata file\n  embedding.metadata_path = os.path.join(LOG_DIR,'metadata.tsv')\n  projector.visualize_embeddings(train_writer, config) \n\n  tf.global_variables_initializer().run()\n\n  for step in range(1000):\n    x_batch, y_batch = get_skipgram_batch(batch_size)\n    summary,_ = sess.run([merged,train_step],\n              feed_dict={train_inputs:x_batch,\n                    train_labels:y_batch})\n    train_writer.add_summary(summary, step)\n\n    if step % 100 == 0:\n      saver.save(sess, os.path.join(LOG_DIR, \"w2v_model.ckpt\"), step)\n      loss_value = sess.run(loss,\n                 feed_dict={train_inputs:x_batch,\n                      train_labels:y_batch})\n      print(\"Loss at %d: %.5f\" % (step, loss_value))\n\n  # Normalize embeddings before using\n  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n  normalized_embeddings = embeddings / norm\n  normalized_embeddings_matrix = sess.run(normalized_embeddings)\n```", "```py\nref_word = normalized_embeddings_matrix[word2index_map[\"one\"]]\n\ncosine_dists = np.dot(normalized_embeddings_matrix,ref_word)  \nff = np.argsort(cosine_dists)[::-1][1:10]\nfor f in ff:\n  print(index2word_map[f])\n  print(cosine_dists[f])\n\n```", "```py\nOut:\nseven\n0.946973\nthree\n0.938362\nnine\n0.755187\nfive\n0.701269\neight\n-0.0702622\ntwo\n-0.101749\nsix\n-0.120306\nfour\n-0.159601\n```", "```py\nimportzipfileimportnumpyasnpimporttensorflowastfpath_to_glove=\"*`path/to/glove/file`*\"PRE_TRAINED=TrueGLOVE_SIZE=300batch_size=128embedding_dimension=64num_classes=2hidden_layer_size=32times_steps=6\n```", "```py\ndigit_to_word_map = {1:\"One\",2:\"Two\", 3:\"Three\", 4:\"Four\", 5:\"Five\",\n          6:\"Six\",7:\"Seven\",8:\"Eight\",9:\"Nine\"}\ndigit_to_word_map[0]=\"PAD_TOKEN\"\neven_sentences = []\nodd_sentences = []\nseqlens = []\nfor i in range(10000):\n  rand_seq_len = np.random.choice(range(3,7))\n  seqlens.append(rand_seq_len)\n  rand_odd_ints = np.random.choice(range(1,10,2),\n                  rand_seq_len)\n  rand_even_ints = np.random.choice(range(2,10,2),\n                   rand_seq_len)\n  if rand_seq_len<6:\n    rand_odd_ints = np.append(rand_odd_ints,\n                 [0]*(6-rand_seq_len))\n    rand_even_ints = np.append(rand_even_ints,\n                 [0]*(6-rand_seq_len))\n\n  even_sentences.append(\" \".join([digit_to_word_map[r] for\n               r in rand_odd_ints]))\n  odd_sentences.append(\" \".join([digit_to_word_map[r] for\n               r in rand_even_ints])) \ndata = even_sentences+odd_sentences\n# Same seq lengths for even, odd sentences\nseqlens*=2\nlabels = [1]*10000 + [0]*10000\nfor i in range(len(labels)):\n  label = labels[i]\n  one_hot_encoding = [0]*2\n  one_hot_encoding[label] = 1\n  labels[i] = one_hot_encoding\n\n```", "```py\nword2index_map ={}\nindex=0\nfor sent in data:\n  for word in sent.split():\n    if word not in word2index_map:\n      word2index_map[word] = index\n      index+=1\n\nindex2word_map = {index: word for word, index in word2index_map.items()}      \n\nvocabulary_size = len(index2word_map)\n```", "```py\nword2index_map\n\nOut:\n{'Eight': 7,\n'Five': 1,\n'Four': 6,\n'Nine': 3,\n'One': 5,\n'PAD_TOKEN': 2,\n'Seven': 4,\n'Six': 9,\n'Three': 0,\n'Two': 8}\n```", "```py\ndef get_glove(path_to_glove,word2index_map):\n\n  embedding_weights = {}\n  count_all_words = 0\n  with zipfile.ZipFile(path_to_glove) as z:\n    with z.open(\"glove.840B.300d.txt\") as f:\n      for line in f:\n        vals = line.split()\n        word = str(vals[0].decode(\"utf-8\"))\n        if word in word2index_map:\n          print(word)\n          count_all_words+=1\n          coefs = np.asarray(vals[1:], dtype='float32')\n          coefs/=np.linalg.norm(coefs)\n          embedding_weights[word] = coefs\n        if count_all_words==vocabulary_size -1:\n          break\n  return embedding_weights\nword2embedding_dict = get_glove(path_to_glove,word2index_map)\n\n```", "```py\nembedding_matrix = np.zeros((vocabulary_size ,GLOVE_SIZE))\n\nfor word,index in word2index_map.items():\n  if not word == \"PAD_TOKEN\":\n    word_embedding = word2embedding_dict[word]\n    embedding_matrix[index,:] = word_embedding\n\n```", "```py\ndata_indices = list(range(len(data)))\nnp.random.shuffle(data_indices)\ndata = np.array(data)[data_indices]\nlabels = np.array(labels)[data_indices]\nseqlens = np.array(seqlens)[data_indices]\ntrain_x = data[:10000]\ntrain_y = labels[:10000]\ntrain_seqlens = seqlens[:10000]\n\ntest_x = data[10000:]\ntest_y = labels[10000:]\ntest_seqlens = seqlens[10000:]\n\ndef get_sentence_batch(batch_size,data_x,\n           data_y,data_seqlens):\n  instance_indices = list(range(len(data_x)))\n  np.random.shuffle(instance_indices)\n  batch = instance_indices[:batch_size]\n  x = [[word2index_map[word] for word in data_x[i].split()]\n    for i in batch]\n  y = [data_y[i] for i in batch]\n  seqlens = [data_seqlens[i] for i in batch]\n  return x,y,seqlens  \n\n```", "```py\n_inputs = tf.placeholder(tf.int32, shape=[batch_size,times_steps])\nembedding_placeholder = tf.placeholder(tf.float32, [vocabulary_size,\n                          GLOVE_SIZE])\n\n_labels = tf.placeholder(tf.float32, shape=[batch_size, num_classes])\n_seqlens = tf.placeholder(tf.int32, shape=[batch_size])\n\n```", "```py\nif PRE_TRAINED:\n\n    embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size, \n                GLOVE_SIZE]),\n                trainable=True)\n    # If using pretrained embeddings, assign them to the embeddings variable\n    embedding_init = embeddings.assign(embedding_placeholder)\n    embed = tf.nn.embedding_lookup(embeddings, _inputs)\n\nelse:\n    embeddings = tf.Variable(\n      tf.random_uniform([vocabulary_size,\n               embedding_dimension],\n               -1.0, 1.0))\n    embed = tf.nn.embedding_lookup(embeddings, _inputs)\n\n```", "```py\nwith tf.name_scope(\"biGRU\"):\n  with tf.variable_scope('forward'):\n    gru_fw_cell = tf.contrib.rnn.GRUCell(hidden_layer_size)\n    gru_fw_cell = tf.contrib.rnn.DropoutWrapper(gru_fw_cell)\n\n  with tf.variable_scope('backward'):\n    gru_bw_cell = tf.contrib.rnn.GRUCell(hidden_layer_size)\n    gru_bw_cell = tf.contrib.rnn.DropoutWrapper(gru_bw_cell)\n\n  outputs, states = tf.nn.bidirectional_dynamic_rnn(cell_fw=gru_fw_cell,\n                           cell_bw=gru_bw_cell,\n                           inputs=embed,\n                           sequence_length=\n                           _seqlens,\n                           dtype=tf.float32,\n                           scope=\"BiGRU\")\nstates = tf.concat(values=states, axis=1)\n\n```", "```py\nweights = {\n  'linear_layer': tf.Variable(tf.truncated_normal([2*hidden_layer_size,\n                          num_classes],\n                          mean=0,stddev=.01))\n}\nbiases = {\n  'linear_layer':tf.Variable(tf.truncated_normal([num_classes],\n                         mean=0,stddev=.01))\n}\n\n# extract the final state and use in a linear layer\nfinal_output = tf.matmul(states,\n            weights[\"linear_layer\"]) + biases[\"linear_layer\"]\n\nsoftmax = tf.nn.softmax_cross_entropy_with_logits(logits=final_output,\n                         labels=_labels)            \ncross_entropy = tf.reduce_mean(softmax)\n\ntrain_step = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\ncorrect_prediction = tf.equal(tf.argmax(_labels,1),\n               tf.argmax(final_output,1))\naccuracy = (tf.reduce_mean(tf.cast(correct_prediction,\n                 tf.float32)))*100\n\n```", "```py\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  sess.run(embedding_init, feed_dict=\n      {embedding_placeholder: embedding_matrix})\n  for step in range(1000):\n    x_batch, y_batch,seqlen_batch = get_sentence_batch(batch_size,\n                             train_x,train_y,\n                             train_seqlens)\n    sess.run(train_step,feed_dict={_inputs:x_batch, _labels:y_batch,\n                   _seqlens:seqlen_batch})\n\n    if step % 100 == 0:\n      acc = sess.run(accuracy,feed_dict={_inputs:x_batch,\n                       _labels:y_batch,\n                       _seqlens:seqlen_batch})\n      print(\"Accuracy at %d: %.5f\" % (step, acc))\n\n  for test_batch in range(5):\n    x_test, y_test,seqlen_test = get_sentence_batch(batch_size,\n                            test_x,test_y,\n                            test_seqlens)\n    batch_pred,batch_acc = sess.run([tf.argmax(final_output,1),\n                    accuracy],\n                    feed_dict={_inputs:x_test,\n                         _labels:y_test,\n                         _seqlens:seqlen_test})\n    print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc)) \n    print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc)) \n\n```"]