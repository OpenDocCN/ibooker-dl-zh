["```py\nfrom diffusers import (\n    StableDiffusionPipeline,\n    UNet2DConditionModel,\n    AutoencoderKL,\n    DDIMScheduler,\n)\nfrom torch import autocast\nfrom PIL import Image\nfrom transformers import CLIPTextModel, CLIPTokenizer\nimport torch\nimport numpy as np\n\nfrom tqdm.auto import tqdm\n\ndef image_grid(imgs, rows, cols):\n    assert len(imgs) == rows * cols\n\n    w, h = imgs[0].size\n    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i % cols * w, i // cols * h))\n    return grid\n```", "```py\n# Simple\npipe = StableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n).to(\"cuda\")\n\nn_images = 4\nprompts = [\n    \"masterpiece, best quality, a photo of a horse riding an astronaut, \"\n    \"trending on artstation, photorealistic, qhd, rtx on, 8k\"\n] * n_images\nimages = pipe(prompts, num_inference_steps=28).images\n\nimage_grid(images, rows=2, cols=2)\n```", "```py\n# Detailed\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\ntext_encoder = CLIPTextModel.from_pretrained(\n    \"openai/clip-vit-large-patch14\"\n).to(\"cuda\")\nvae = AutoencoderKL.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", subfolder=\"vae\"\n).to(\"cuda\")\nmodel = UNet2DConditionModel.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\"\n).to(\"cuda\")\n\nscheduler = DDIMScheduler(\n    beta_start = .00085, \n    beta_end = .012, \n    beta_schedule = \"scaled_linear\", \n    clip_sample = False, set_alpha_to_one = False, \n    steps_offset = 1\n)\n```", "```py\ndef get_text_embeds(prompt):\n    text_input = tokenizer(       #1\n        prompt,\n        padding=\"max_length\",\n        max_length=tokenizer.model_max_length,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    with torch.no_grad():\n        text_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0]\n\n    uncond_input = tokenizer(    #2\n        [\"\"] * len(prompt),\n        padding=\"max_length\",\n        max_length=tokenizer.model_max_length,\n        return_tensors=\"pt\",\n    )\n    with torch.no_grad():\n        uncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[\n            0\n        ]\n\n    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])   #3\n    return text_embeddings\n\ndef produce_latents(\n    text_embeddings,\n    height=512,\n    width=512,\n    num_inference_steps=28,\n    guidance_scale=11,\n    latents=None,\n    return_all_latents=False,\n):\n    if latents is None:\n        latents = torch.randn(\n            (\n                text_embeddings.shape[0] // 2,\n                model.in_channels,\n                height // 8,\n                width // 8,\n            )\n        )\n    latents = latents.to(\"cuda\")\n\n    scheduler.set_timesteps(num_inference_steps)\n    latents = latents * scheduler.sigmas[0]\n\n    latent_hist = [latents]\n    with autocast(\"cuda\"):\n        for i, t in tqdm(enumerate(scheduler.timesteps)):\n            latent_model_input = torch.cat([latents] * 2)     #4\n            sigma = scheduler.sigmas[i]\n            latent_model_input = latent_model_input / (\n                (sigma**2 + 1) ** 0.5\n            )\n\n            with torch.no_grad():      #5\n                noise_pred = model(\n                    latent_model_input,\n                    t,\n                    encoder_hidden_states=text_embeddings,\n                )[\"sample\"]\n\n            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)     #6\n            noise_pred = noise_pred_uncond + guidance_scale * (\n                noise_pred_text - noise_pred_uncond\n            )\n            latents = scheduler.step(noise_pred, t, latents)[\"prev_sample\"]  #7\n            latent_hist.append(latents)\n\n    if not return_all_latents:\n        return latents\n\n    all_latents = torch.cat(latent_hist, dim=0)\n    return all_latents\n\ndef decode_img_latents(latents):\n    latents = 1 / 0.18215 * latents\n\n    with torch.no_grad():\n        imgs = vae.decode(latents)[\"sample\"]\n\n    imgs = (imgs / 2 + 0.5).clamp(0, 1)\n    imgs = imgs.detach().cpu().permute(0, 2, 3, 1)\n    imgs = (imgs) * 127.5\n    imgs = imgs.numpy().astype(np.uint8)\n    pil_images = [Image.fromarray(image) for image in imgs]\n    return pil_images\n```", "```py\ndef prompt_to_img(\n    prompts,\n    height=512,\n    width=512,\n    num_inference_steps=28,\n    guidance_scale=11,\n    latents=None,\n):\n    if isinstance(prompts, str):\n        prompts = [prompts]\n\n    text_embeds = get_text_embeds(prompts)    #1\n\n    latents = produce_latents(     #2\n        text_embeds,\n        height=height,\n        width=width,\n        latents=latents,\n        num_inference_steps=num_inference_steps,\n        guidance_scale=guidance_scale,\n    )\n\n    imgs = decode_img_latents(latents)      #3\n\n    return imgs\n\nimgs = prompt_to_img(\n    [\"Super cool fantasy knight, intricate armor, 8k\"] * 4\n)\n\nimage_grid(imgs, rows=2, cols=2)\n```"]