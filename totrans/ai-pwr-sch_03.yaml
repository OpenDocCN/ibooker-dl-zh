- en: 3 Ranking and content-based relevance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 排序和基于内容的相关性
- en: This chapter covers
  id: totrans-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Executing queries and returning search results
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行查询并返回搜索结果
- en: Ranking search results based on how relevant they are to an incoming query
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据与进入查询的相关性对搜索结果进行排名
- en: Keyword match and filtering versus vector-based ranking
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键词匹配和过滤与基于向量的排名
- en: Controlling and specifying custom ranking functions with function queries
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用函数查询控制并指定自定义排名函数
- en: Catering ranking functions to a specific domain
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 针对特定领域定制排名函数
- en: 'Search engines fundamentally do three things: ingest content (*indexing*),
    return content matching incoming queries (*matching*), and sort the returned content
    based on some measure of how well it matches the query (*ranking*). Additional
    layers can be added, allowing users to provide better queries (autosuggest, chatbot
    dialogs, etc.) and to extract better answers from the results or summarize the
    results by using large language models (see chapters 14–15), but the core functions
    of the search engine are matching and ranking on indexed data.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎本质上做三件事：摄取内容（*索引*）、返回与进入查询匹配的内容（*匹配*）以及根据某些衡量标准对返回的内容进行排序（*排序*）。可以添加额外的层，使用户能够提供更好的查询（自动建议、聊天机器人对话等）并从结果中提取更好的答案或使用大型语言模型总结结果（见第14-15章），但搜索引擎的核心功能是对索引数据进行匹配和排序。
- en: '*Relevance* is the notion of how well the returned content matches the query.
    Normally, the content being matched is documents, and the returned and ranked
    content is the matched documents along with corresponding metadata. In most search
    engines, the default relevance sorting is based upon a score indicating how well
    each keyword in a query matches the same keyword in each matched document. Alternatively,
    queries can be mapped into numerical vector representations, with the score then
    representing how similar the query vector is to each matched document. The best
    matches yield the highest relevance score, and they are returned at the top of
    the search results. The relevance calculation is highly configurable and can be
    adjusted on a per-query basis, allowing sophisticated ranking behavior.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '*相关性* 是指返回的内容与查询匹配得有多好的概念。通常，被匹配的内容是文档，返回并排序的内容是匹配的文档及其相应的元数据。在大多数搜索引擎中，默认的相关性排序是基于一个分数，表示查询中的每个关键词与每个匹配文档中相同关键词匹配得有多好。或者，查询可以被映射到数值向量表示，此时分数表示查询向量与每个匹配文档的相似程度。最佳匹配产生最高的相关性分数，并显示在搜索结果的最顶部。相关性计算非常灵活，可以根据每个查询进行调整，从而实现复杂的排序行为。'
- en: In this chapter, we’ll provide an overview of how relevance is calculated, how
    the relevance function can be easily controlled and adjusted through function
    queries, and how we can implement popular domain-specific and user-specific relevance
    ranking features.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将概述如何计算相关性，如何通过函数查询轻松控制并调整相关性函数，以及如何实现流行的特定领域和用户特定相关性排名功能。
- en: 3.1 Scoring query and document vectors with cosine similarity
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 使用余弦相似度对查询和文档向量进行评分
- en: In section 2.3, we demonstrated the idea of measuring the similarity of two
    vectors by calculating the cosine between them. We created vectors (lists of numbers,
    where each number represents the strength of some feature) that represented different
    food items, and we then calculated the cosine (the size of the angle between the
    vectors) to determine their similarity. We’ll expand upon that technique in this
    section, discussing how text queries and documents can map into vectors for ranking
    purposes. We’ll then explore some popular text-based feature-weighting techniques
    and how they can be integrated to create an improved relevance-ranking formula.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2.3节中，我们展示了通过计算两个向量之间的余弦值来衡量两个向量相似度的概念。我们创建了代表不同食品项目的向量（数字列表，其中每个数字代表某些特征的强度），然后计算余弦值（向量之间角度的大小）以确定它们的相似度。在本节中，我们将扩展这一技术，讨论文本查询和文档如何映射到向量以用于排序目的。然后，我们将探讨一些流行的基于文本的特征加权技术以及如何将它们集成以创建改进的相关性排序公式。
- en: Running the code examples
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 运行代码示例
- en: All the code listings in the book are available in Jupyter notebooks running
    in preconfigured Docker containers. This enables you to run interactive versions
    of the code with a single command (`docker compose up`) without needing to spend
    time on complicated system configuration and dependency management. The code examples
    can also run with multiple search engines and vector databases. See appendix A
    for instructions on how to configure and launch the Jupyter notebooks and follow
    along in your web browser.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 书中的所有代码示例都可在预配置的Docker容器中运行的Jupyter笔记本中找到。这使得您可以通过单个命令（`docker compose up`）运行代码的交互式版本，而无需花费时间进行复杂的系统配置和依赖关系管理。代码示例也可以在多个搜索引擎和向量数据库上运行。有关如何配置和启动Jupyter笔记本以及如何在网络浏览器中跟踪的说明，请参阅附录A。
- en: For brevity, the listings in this book may leave out certain lines of code,
    such as imports or ancillary code, but the notebooks contain all implementation
    details.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁，本书中的列表可能省略了某些代码行，例如导入或辅助代码，但笔记本包含所有实现细节。
- en: In this section, we’ll dive into our first code listings for the book. It will
    be helpful to start up the Docker containers needed to run the accompanying Jupyter
    notebooks so you can follow along with the interactive code examples. Instructions
    for doing this are provided in appendix A.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨本书的第一个代码示例。启动运行伴随Jupyter笔记本所需的Docker容器将有助于您跟随交互式代码示例。有关如何执行此操作的说明请参阅附录A。
- en: 3.1.1 Mapping text to vectors
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 文本到向量的映射
- en: In a typical search application, we start with a collection of documents, and
    we then try to rank documents based on how well they match some user’s query.
    In this section, we’ll walk through the process of mapping the text of queries
    and documents into vectors.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的搜索应用中，我们从一个文档集合开始，然后尝试根据它们与某些用户查询的匹配程度对文档进行排序。在本节中，我们将介绍将查询和文档的文本映射到向量的过程。
- en: In the last chapter, we used the example of a search for food and beverage items,
    like `apple juice`, so let’s reuse that example here. Let’s assume we have two
    different documents we would like to sort based on how well they match this query.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用了搜索食品和饮料项目（如`苹果汁`）的例子，所以让我们在这里重用这个例子。假设我们有两个不同的文档，我们希望根据它们与查询的匹配程度进行排序。
- en: '*Query:* `apple` `juice`'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*查询:* `apple` `juice`'
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If we mapped both documents (containing a combined 48 words) to vectors, they
    would map to a 48-word vector space with the following dimensions:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将两个文档（包含总共48个单词）映射到向量，它们将映射到具有以下维度的48个单词向量空间：
- en: '[PRE1]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: If you recall in section 2.3, we proposed thinking of a query for the phrase
    “apple juice” as a vector containing a feature for every word in any of our documents,
    with a value of `1` for the terms “apple” and “juice”, and a value of `0` for
    all other terms.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在2.3节中回忆一下，我们提出将针对短语“苹果汁”的查询视为一个向量，该向量包含我们任何文档中每个单词的特征，对于“苹果”和“juice”这些术语，其值为`1`，而对于所有其他术语，其值为`0`。
- en: Since the term “apple” is in the 3rd position, and “juice” is in the 28th position
    of our 48-word vector space, a query vector for the phrase “apple juice” would
    look as shown in figure 3.1.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于术语“苹果”位于我们的48个单词向量空间的第3位，而“juice”位于第28位，因此短语“苹果汁”的查询向量将如图3.1所示。
- en: '![figure](../Images/CH03_F01_Grainger.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/CH03_F01_Grainger.png)'
- en: Figure 3.1 Query vector. The query for `apple` `juice` is mapped to a vector
    containing one dimension for every known term, with a value of `1` for the terms
    “apple” and “juice” and a value of `0` for all other terms.
  id: totrans-26
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1 查询向量。对于`apple` `juice`的查询被映射到一个包含每个已知术语一个维度的向量，对于“苹果”和“juice”这些术语，其值为`1`，而对于所有其他术语，其值为`0`。
- en: Even though the query vector only contains a nonzero value for two dimensions
    (representing the positions of “apple” and “juice”), it still contains values
    of `0` for all other possible dimensions. Representing a vector like this, including
    every possible value, is known as a *dense vector representation*.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 即使查询向量只包含两个非零值（代表“苹果”和“juice”的位置），它仍然包含所有其他可能维度的`0`值。表示这种包含每个可能值的向量，称为*密集向量表示*。
- en: 'Each of the documents also maps to the same vector space based upon each of
    the terms it contains:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文档也根据其包含的每个术语映射到相同的向量空间：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: With these dense vector representations of our query and documents, we can now
    use linear algebra to measure the similarity between our query vector and each
    of the document vectors.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们查询和文档的密集向量表示，我们现在可以使用线性代数来衡量我们的查询向量和每个文档向量之间的相似度。
- en: 3.1.2 Calculating similarity between dense vector representations
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 计算密集向量表示之间的相似性
- en: To rank our documents, we just need to follow the same process we used in chapter
    2 to calculate the cosine between each document and the query. This cosine value
    will then become the relevance score by which we’ll be able to sort each document.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对文档进行排序，我们只需要遵循第2章中计算每个文档和查询之间余弦值的过程。然后，这个余弦值将成为我们根据其排序每个文档的相关性得分。
- en: The following listing shows how we would represent the query and document vectors
    in code, and how we would calculate the cosine similarity between the query and
    each document.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了如何在代码中表示查询和文档向量，以及如何计算查询和每个文档之间的余弦相似性。
- en: Listing 3.1 Calculating cosine similarity between vectors
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.1 计算向量之间的余弦相似性
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Output:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Interesting . . . both documents received the same relevance score, even though
    the documents contain lengthy vectors with very different content. It might not
    be immediately obvious what’s going on, so let’s simplify the calculation by focusing
    only on the features that matter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是……尽管这两个文档包含内容非常不同的长向量，但它们仍然获得了相同的相关性得分。这可能不是立即显而易见的，所以让我们通过只关注相关的特征来简化计算。
- en: 3.1.3 Calculating similarity between sparse vector representations
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 计算稀疏向量表示之间的相似性
- en: The key to understanding the calculation in the last section is realizing that
    the only relevant features are the ones shared between the query and the document.
    All other features (words appearing in documents that don’t match the query) have
    zero effect on whether one document is ranked higher than another. As a result,
    we can remove all the other insignificant terms from our vector to simplify the
    example, converting from a dense vector representation to what is known as a *sparse
    vector representation*, as shown in figure 3.2\.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 理解上一节计算的关键在于意识到唯一相关的特征是查询和文档之间共享的特征。所有其他特征（出现在不匹配查询的文档中的单词）对判断一个文档是否比另一个文档排名更高没有影响。因此，我们可以从我们的向量中删除所有其他不重要的术语来简化示例，从密集向量表示转换为如图3.2所示的*稀疏向量表示*。
- en: '![figure](../Images/CH03_F02_Grainger.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图](../Images/CH03_F02_Grainger.png)'
- en: Figure 3.2 Sparse vector representations only contain the “present” features,
    unlike dense vector representations, which also contain the 0-valued entries for
    every feature.
  id: totrans-42
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2 稀疏向量表示仅包含“存在”的特征，与密集向量表示不同，密集向量表示还包含每个特征的0值条目。
- en: In most search engine scoring operations, we tend to deal with sparse vector
    representations because they are more efficient to work with when scoring based
    on the small number of features.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数搜索引擎评分操作中，我们倾向于处理稀疏向量表示，因为当基于少量特征评分时，它们更有效率。
- en: In addition, we can further simplify our calculations by creating vectors that
    only include “meaningful entries”—the terms that are present in the query—as shown
    in the following listing.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以通过创建只包含“有意义的条目”——即查询中存在的术语——的向量来进一步简化我们的计算，如下列所示。
- en: Listing 3.2 Cosine similarity of sparse vector representations
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.2 稀疏向量表示的余弦相似性
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Output:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice that doc1 and doc2 still yield equal relevance scores, but now the score
    for each is `1.0`. If you remember, a `1.0` score from a cosine calculation means
    the vectors are perfect matches, which is sensible considering that both vectors
    are identical (`[1,` `1]`).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到doc1和doc2仍然产生相同的相关性得分，但现在每个得分都是`1.0`。如果你还记得，余弦计算中的`1.0`得分意味着向量是完美匹配的，考虑到这两个向量都是相同的(`[1,`
    `1]`)，这是合理的。
- en: 'In fact, you’ll notice several very interesting things:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，你会注意到几个非常有趣的事情：
- en: This simplified sparse vector representation calculation still shows both doc1
    and doc2 returning equivalent relevance scores, since they both match all the
    words in the query.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个简化的稀疏向量表示计算仍然显示doc1和doc2返回等效的相关性得分，因为它们都匹配查询中的所有单词。
- en: Even though the absolute score between the dense vector representation similarity
    (`0.2828`) and the sparse vector representation similarity (`1.0`) are different,
    the scores are still the same relative to each other within each vector type.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管密集向量表示相似性(`0.2828`)和稀疏向量表示相似性(`1.0`)之间的绝对得分不同，但相对于每种向量类型，得分仍然是相同的。
- en: The feature weights for the two query terms (“apple”, “juice”) are the same
    between the query and each of the documents, resulting in a cosine score of `1.0`.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个查询词（“apple”，“juice”）的特征权重在查询和每个文档之间是相同的，导致余弦得分为`1.0`。
- en: Vectors vs. vector representations
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 向量与向量表示
- en: We’ve been careful to refer to “dense vector representations” and “sparse vector
    representations” instead of “dense vectors” and “sparse vectors”. This is because
    there is a conceptual distinction between the idea of a vector and its representation,
    and this distinction often causes confusion.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直小心翼翼地使用“密集向量表示”和“稀疏向量表示”，而不是“密集向量”和“稀疏向量”。这是因为向量的概念与其表示之间存在概念上的区别，这种区别往往会导致混淆。
- en: 'The sparsity of a *vector* refers to the proportion of the vector’s features
    that have meaningful values. Specifically, a dense vector is any vector whose
    features have mostly nonzero values, whereas a sparse vector is any vector whose
    features have mostly zeros, regardless of how they are stored or represented.
    Vector *representations*, on the other hand, deal with the data structures used
    to work with the vectors. For sparse vectors, it can be wasteful to allocate memory
    and storage space for all the zeros, so we will often use a sparse data structure
    (such as an inverted index) to only store the nonzero values. Here is an example:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 向量的稀疏性指的是向量特征中具有有效值的比例。具体来说，密集向量是指特征值大部分不为零的任何向量，而稀疏向量是指特征值大部分为零的任何向量，无论它们是如何存储或表示的。另一方面，向量*表示*涉及处理向量的数据结构。对于稀疏向量，为所有零值分配内存和存储空间可能是浪费的，因此我们通常会使用稀疏数据结构（如倒排索引）来仅存储非零值。以下是一个例子：
- en: '*dense vector*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*密集向量*:'
- en: 'feature_1: **1.1**, feature_2: **2.3**, feature_3: **7.1**, feature_4: **5.2**,
    feature_5: **8.1**'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 'feature_1: **1.1**, feature_2: **2.3**, feature_3: **7.1**, feature_4: **5.2**,
    feature_5: **8.1**'
- en: '*dense vector representation*: `[`**1.1**`,`**2.3**`,`**7.1**`,`**5.2**`,`**8.1**`]`'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*密集向量表示*: `[`**1.1**`,`**2.3**`,`**7.1**`,`**5.2**`,`**8.1**`]`'
- en: '*sparse vector representation*: N/A (the vector is not sparse, so it can’t
    be represented sparsely)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*稀疏向量表示*: N/A（该向量不是稀疏的，因此不能稀疏表示）'
- en: '*sparse vector*: feature_1: **1.1**, feature_2: **0**, feature_3: **0**, feature_4:
    **5.2**, feature_5: **0**'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*稀疏向量*: feature_1: **1.1**, feature_2: **0**, feature_3: **0**, feature_4:
    **5.2**, feature_5: **0**'
- en: '*dense vector representation*: `[`**1.1**`, 0.0, 0.0,`**5.2**`, 0.0 ]`'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*密集向量表示*: `[`**1.1**`, 0.0, 0.0,`**5.2**`, 0.0 ]`'
- en: '*sparse vector representation*: `{ 1:`**1.1**`, 4:`**5.2**`}`, or just `[`**1.1**`,`**5.2**`]`
    if feature positions aren’t needed'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '*稀疏向量表示*: `{ 1:`**1.1**`, 4:`**5.2**`}`, 或者在不需要特征位置的情况下，直接为 `[`**1.1**`,`**5.2**`]`'
- en: Because a sparse vector contains predominantly zeros, and its corresponding
    sparse vector representation contains nearly the opposite (only the nonzero values),
    it is unfortunately common for people to confuse these concepts and incorrectly
    refer to dense vector representations (of sparse vectors) as “dense vectors”,
    or even refer to any vector with many dimensions as a “dense vector” and with
    few dimensions as a “sparse vector”. You may find this confusion in other literature,
    so it’s important to be aware of the distinction.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 由于稀疏向量主要由零组成，其对应的稀疏向量表示几乎相反（只有非零值），因此人们经常混淆这些概念，错误地将稀疏向量的密集向量表示称为“密集向量”，甚至将具有许多维度的任何向量称为“密集向量”，将维度较少的向量称为“稀疏向量”。你可能会在其他文献中找到这种混淆，因此了解这种区别很重要。
- en: Since our query and document vectors are all sparse vectors (most values are
    zero, since the number of features is the number of keywords in the search index),
    it makes sense to use a sparse vector representation when doing keyword search.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的查询和文档向量都是稀疏向量（由于特征数是搜索索引中关键词的数量，大多数值都是零），在进行关键词搜索时使用稀疏向量表示是有意义的。
- en: Search engines adjust for these problems by not just considering each feature
    in the vector as a `1` (exists) or a `0` (does not exist), but instead by providing
    a score for each feature based upon how *well* the feature matches.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎通过不仅将向量中的每个特征视为`1`（存在）或`0`（不存在），而是根据特征与匹配的**好坏**为每个特征提供分数来调整这些问题。
- en: '3.1.4 Term frequency: Measuring how well documents match a term'
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.4 词语频率：衡量文档与词语匹配的程度
- en: The problem we encountered in the last section is that the features in our term
    vectors only signify *if* the word “apple” or “juice” exists in a document, not
    how well each document represents either of the terms. A side effect of representing
    each term from the query with a value of `1` if it exists is that both doc1 and
    doc2 will always have the same cosine similarity score for the query, even though,
    qualitatively, doc2 is a much better match, since it talks about apple juice much
    more.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一个章节中遇到的问题是，我们的术语向量中的特征只表示“苹果”或“果汁”这个词是否存在于文档中，而不是每个文档代表这两个术语的好坏。如果查询中的每个术语存在时用
    `1` 的值来表示每个术语，副作用是 doc1 和 doc2 将始终具有相同的余弦相似度评分，尽管从质量上讲，doc2 是一个更好的匹配，因为它更多地提到了苹果汁。
- en: Instead of using a value of `1` for each existing term, we can emulate “how
    well” a document matches by using the *term frequency* (TF), which is a measure
    of the number of times a term occurs within each document. The idea here is that
    the more frequently a term occurs within a specific document, the greater the
    likelihood that the document is more related to the query.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 *词频*（TF）来模拟“文档匹配得有多好”，而不是为每个现有术语使用 `1` 的值，词频是衡量术语在每个文档中出现的次数的度量。这里的想法是，一个术语在特定文档中出现的频率越高，该文档与查询的相关性就越大。
- en: The following listing shows vectors with a count of the number of times each
    term occurs within the document or query as the feature weights.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了具有每个术语在文档或查询中出现的次数作为特征权重的向量。
- en: Listing 3.3 Cosine similarity of raw TF vectors
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.3 原始 TF 向量的余弦相似度
- en: '[PRE7]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Output:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Contrary to what you might expect, doc1 is considered a better cosine similarity
    match than doc2\. This is because the terms “apple” and “juice” both occur “the
    same proportion of times” (one occurrence of each term for every occurrence of
    the other term) in both the query and in doc1, making them the most textually
    similar. In other words, even though doc2 is intuitively more about the query,
    since it contains the terms in the query significantly more, cosine similarity
    returns doc1, since it’s an exact match for the query, unlike doc2.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 与你预期的相反，doc1 被认为比 doc2 更好的余弦相似度匹配。这是因为“苹果”和“果汁”这两个词在查询和 doc1 中都以“相同的比例”出现（每个词在每个其他词出现一次），使它们在文本上最相似。换句话说，尽管
    doc2 直观上更接近查询，因为它包含的查询中的词显著更多，但余弦相似度返回 doc1，因为它是查询的精确匹配，而 doc2 则不是。
- en: 'Since our goal is for documents like doc2 with higher TF to score higher, we
    can accomplish this by switching from cosine similarity to another scoring function,
    such as *dot product* or *Euclidean distance*, that increases as feature weights
    continue to increase. Let’s use the dot product (`a` `.` `b`), which is equal
    to the cosine similarity multiplied by the length of the query vector times the
    length of the document vector: `a` `.` `b` `=` `|a|` `×` `|b|` `×` `cos(θ)`. The
    dot product will result in documents that contain more matching terms scoring
    higher, as opposed to cosine similarity, which scores documents higher when they
    contain a more similar proportion of matching terms between the query and documents.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的目标是使像 doc2 这样具有更高 TF 的文档得分更高，我们可以通过从余弦相似度切换到另一个评分函数来实现，例如 *点积* 或 *欧几里得距离*，随着特征权重的增加而增加。让我们使用点积
    (`a` `.` `b`)，它等于余弦相似度乘以查询向量长度和文档向量长度的乘积：`a` `.` `b` `=` `|a|` `×` `|b|` `×` `cos(θ)`。点积将导致包含更多匹配项的文档得分更高，与余弦相似度相反，余弦相似度在查询和文档之间包含更多相似比例的匹配项时，会给文档更高的评分。
- en: Phrase matching and other relevance tricks
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 短语匹配和其他相关性技巧
- en: By now, you may be wondering why we keep treating “apple” and “juice” as independent
    terms and why we don’t just treat “apple juice” as a phrase to boost documents
    higher that match the exact phrase. Phrase matching is one of many easy relevance-tuning
    tricks we’ll discuss later in the chapter. For now, we’ll keep our query-processing
    simple and just deal with individual keywords to stay focused on our main goal—explaining
    vector-based relevance scoring and text-based keyword-scoring features.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你可能想知道为什么我们一直将“苹果”和“果汁”作为独立术语处理，为什么我们不直接将“苹果汁”作为一个短语来提高匹配精确短语的文档的得分。短语匹配是我们将在本章后面讨论的许多简单相关性调整技巧之一。现在，我们将保持我们的查询处理简单，只处理单个关键词，以保持我们对主要目标的关注——解释基于向量的相关性评分和基于文本的关键词评分特征。
- en: In the next listing, we replace cosine similarity with a dot product calculation
    to consider the magnitude of the document vectors (which increases with more matches
    for each query term) in the relevance calculation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个列表中，我们将余弦相似度替换为点积计算，以考虑文档向量在相关度计算中的幅度（随着每个查询术语的匹配次数增加而增加）。
- en: Listing 3.4 Dot product of TF vectors
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.4 TF 向量的点积
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Output:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, doc2 now yields a higher relevance score for the query than
    doc1, an improvement that aligns better with our intuition. Note that the relevance
    scores are no longer bounded between `0` and `1`, as they were with cosine similarity.
    This is because the dot product considers the magnitude of the document vectors,
    which can increase unbounded with additional matching keyword occurrences.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，doc2 现在对查询的相关度得分高于 doc1，这种改进与我们的直觉更吻合。请注意，相关度得分不再被限制在 `0` 和 `1` 之间，就像余弦相似度那样。这是因为点积考虑了文档向量的幅度，它可以随着额外匹配的关键词出现次数的无限制增加而增加。
- en: 'While using the TF as the feature weight in our vectors certainly helps, textual
    queries exhibit additional challenges that need to be considered. Thus far, our
    documents have all contained every term from our queries, which does not match
    with most real-world scenarios. The following example will better demonstrate
    some of the limitations still present when using only term-frequency-based weighting
    for our text-based sparse vector similarity scoring. Let’s start with the following
    three text documents:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在我们的向量中用 TF 作为特征权重确实有所帮助，但文本查询表现出额外的挑战，需要考虑。到目前为止，我们的文档都包含查询中的每个术语，这与大多数现实世界场景不符。以下示例将更好地说明，当仅使用基于词频的加权对基于文本的稀疏向量相似度评分进行评分时，仍然存在的一些局限性。让我们从以下三个文本文档开始：
- en: '[PRE11]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Let’s now map these documents into their corresponding (sparse) vector representations
    and calculate a similarity score. The following listing ranks text similarity
    based on raw TFs (term counts).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将这些文档映射到它们对应的（稀疏）向量表示，并计算相似度得分。以下列表根据原始 TF（术语计数）对文本相似度进行排序。
- en: Listing 3.5 Ranking text similarity based on term counts
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.5 基于术语计数的文本相似度排序
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'While we receive different relevance scores now for each document, based on
    the number of times each term matches, the ordering of the results doesn’t necessarily
    match our expectations about which documents are the best matches. Intuitively,
    we would instead expect the following ordering:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们现在根据每个术语匹配的次数为每个文档接收不同的相关度得分，但结果的排序并不一定符合我们对哪些文档是最佳匹配的预期。直观上，我们更期望以下排序：
- en: '*doc2:* because it is about the book *The Cat in the Hat*'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*doc2:* 因为它关于书籍 *《戴帽子的猫》*'
- en: '*doc3:* because it matches all the words “the”, “cat”, “in”, and “hat”'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*doc3:* 因为它与所有单词“the”、“cat”、“in”和“hat”相匹配'
- en: '*doc1:* because it only matches the words “the” and “in”, even though it contains
    them many times'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*doc1:* 因为它只匹配单词“the”和“in”，尽管它包含它们很多次'
- en: The problem here is that since a term is considered just as important every
    time it appears, the relevance score is indiscriminately increasing with every
    additional occurrence of that term. In this case, doc1 is getting the highest
    score, because it contains 14 total term matches (the first “the” five times,
    “in” four times, and the second “the” five times), yielding more total term matches
    than any other document.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的问题在于，由于每次一个术语出现时都被视为同等重要，因此相关度得分会随着该术语出现的每一次额外增加而毫无区别地增加。在这种情况下，doc1 获得了最高的得分，因为它包含总共
    14 个术语匹配（第一个“the”出现五次，“in”出现四次，第二个“the”出现五次），比任何其他文档的总术语匹配数都要多。
- en: It doesn’t really make sense that a document containing these words 14 times
    should be considered 14 times as relevant as a document with a single match, though.
    Instead, a document should be considered more relevant if it matches many different
    terms from a query versus the same terms over and over. Often, real-world TF calculations
    dampen the effect of each additional occurrence of a word by calculating TF as
    a log or square root of the number of occurrences of each term (as we do in figure
    3.3). Additionally, TF is often also normalized relative to document length by
    dividing the TF by the total number of terms in each document. Since longer documents
    are naturally more likely to contain any given term more often, this helps normalize
    the score to account for those document length variabilities (per the denominator
    for figure 3.3). Our final, normalized TF calculation can be seen in figure 3.3.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个包含这些词语出现 14 次的文档被认为比只有一个匹配的文档相关 14 倍，这实际上并不合理。相反，如果一个文档与查询中的许多不同术语匹配，而不是反复匹配相同的术语，那么应该认为该文档更相关。通常，现实世界的
    TF 计算通过将每个术语出现的次数计算为对数或平方根来衰减每个额外出现的影响（如图 3.3 所示）。此外，TF 通常也相对于文档长度进行归一化，通过将 TF
    除以每个文档中的术语总数。由于较长的文档自然更有可能包含任何给定的术语更频繁，这有助于将分数归一化以考虑这些文档长度变化（根据图 3.3 的分母）。我们的最终、归一化的
    TF 计算可以在图 3.3 中看到。
- en: '![figure](../Images/grainger-ch3-eqs-0x.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/grainger-ch3-eqs-0x.png)'
- en: Figure 3.3 Normalized TF calculation. *t* represents a term and `d` represents
    a document. *TF* equals the square root of the number of times the term appears
    in the current document (*f**[t,d]*) divided by the number of terms in the document
    (`∑`*[t']*`[∈]`*d f**[t',d]*). The square root dampens the additional relevance
    contribution of each additional occurrence of a term, while the denominator normalizes
    that dampened frequency to the document length so that longer documents with more
    terms are comparable to shorter documents with fewer terms.
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.3 标准化 TF 计算图。*t* 代表一个术语，`d` 代表一个文档。*TF* 等于当前文档中术语出现的次数的平方根（*f**[t,d]*）除以文档中的术语数量（`∑`*[t']*`[∈]`*d
    f**[t',d]*）。平方根衰减了术语每次额外出现的额外相关性贡献，而分母将这种衰减频率归一化到文档长度，以便较长的文档（包含更多术语）与较短的文档（包含较少术语）具有可比性。
- en: Many variations of the TF calculation exist, only some of which perform document-length
    normalization (the denominator) or dampen the effect of additional term occurrences
    (the square root here, or sometimes using a logarithm). Apache Lucene (the search
    library powering Solr, OpenSearch, and Elasticsearch), for instance, calculates
    TF as only the square root of the numerator, but then multiplies it by a separate
    document-length norm (equivalent to the square root of the denominator in our
    equation) when performing certain ranking calculations.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: TF 计算的变体很多，其中只有一些执行文档长度归一化（分母）或衰减额外术语出现的影响（这里使用平方根，有时使用对数）。例如，Apache Lucene（为
    Solr、OpenSearch 和 Elasticsearch 提供搜索功能的库）计算 TF 时只计算分子部分的平方根，但在进行某些排名计算时，会乘以一个单独的文档长度规范（相当于我们方程中分母的平方根）。
- en: Going forward, we’ll use this normalized TF calculation to ensure additional
    occurrences of the same term continue to improve relevance, but at a diminishing
    rate. The following listing shows the new TF function in effect.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 今后，我们将使用这种归一化的 TF 计算来确保相同术语的额外出现继续提高相关性，但提高的速度会逐渐减慢。以下列表显示了生效的新 TF 函数。
- en: Listing 3.6 Ranking text similarity based on TF
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.6 基于 TF 的文本相似度排名
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Output:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The normalized `tf` function shows an improvement, as doc2 is now ranked the
    highest, as expected. This is mostly because of the dampening effect on the number
    of term occurrences in doc1 (which matched “the” and “in” so many times), such
    that each additional occurrence contributes less to the feature weight than prior
    occurrences. Unfortunately, doc1 is still ranked second-highest, so even the improved
    TF calculation wasn’t enough to get the better matching doc3 higher.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化后的 `tf` 函数显示有所改进，正如预期的那样，doc2 现在排名最高。这主要是因为 doc1（匹配了“the”和“in”如此多次）中术语出现次数的衰减效应，使得每次额外出现对特征权重的贡献比之前出现要小。不幸的是，doc1
    仍然排名第二高，所以即使改进后的 TF 计算也无法使更好的匹配文档 doc3 排名更高。
- en: The next step for improvement will be to factor in the relative importance of
    terms, as “cat” and “hat” are intuitively more important than common words like
    “the” and “in”. Let’s modify our scoring calculation to fix this oversight by
    introducing a new variable that incorporates the importance of each term.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 改进的下一步将是考虑术语的相对重要性，因为“cat”和“hat”在直觉上比“the”和“in”等常见词更重要。让我们修改我们的评分计算，通过引入一个新变量来包含每个术语的重要性，以纠正这一疏忽。
- en: '3.1.5 Inverse document frequency: Measuring the importance of a term in the
    query'
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.5 逆文档频率：衡量查询中术语的重要性
- en: While TF has proven useful at measuring how well a document matches each term
    in a query, it does little to differentiate between the importance of the terms
    in the query. In this section, we’ll introduce a technique using the significance
    of specific keywords based on their frequency of occurrence across documents.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然TF在衡量文档与查询中每个术语的匹配程度方面已被证明是有用的，但它对区分查询中术语的重要性贡献甚微。在本节中，我们将介绍一种使用基于文档中关键词出现频率的显著性的技术。
- en: The *document frequency* (DF) for a term is defined as the total number of documents
    in the search engine that contain the term, and it serves as a good measure of
    a term’s importance. The idea here is that more specific or rare words (like “cat”
    and “hat”) tend to be more important than common words (like “the” and “in”).
    The function used to calculate document frequency is shown in figure 3.4\.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 术语的*文档频率*（DF）定义为搜索引擎中包含该术语的所有文档的总数，它作为衡量术语重要性的良好指标。这里的想法是，更具体或罕见的词（如“cat”和“hat”）通常比常见的词（如“the”和“in”）更重要。用于计算文档频率的函数如图3.4所示。
- en: '![figure](../Images/grainger-ch3-eqs-1x.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/grainger-ch3-eqs-1x.png)'
- en: Figure 3.4 Document frequency calculation. `D` is the set of all documents,
    `t` is the input term, and `D[i]` is the *i*^(th) document in `D`. The lower the
    document frequency for a term (`DF(t)`), the more specific and important the term
    is when seen in queries.
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.4 文档频率计算。`D`是所有文档的集合，`t`是输入术语，`D[i]`是`D`中的第*i*个文档。一个术语的文档频率越低（`DF(t)`），在查询中看到的术语就越具体、越重要。
- en: Since we would like more important words to score higher, we take the *inverse
    document frequency* (IDF), as defined in figure 3.5\.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望更重要的词得分更高，所以我们采用*逆文档频率*（IDF），如图3.5所示。
- en: '![figure](../Images/grainger-ch3-eqs-2x.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![figure](../Images/grainger-ch3-eqs-2x.png)'
- en: Figure 3.5 Inverse document frequency. `|D|` is the total count of all documents,
    `t` is the term, and `DF(t)` is the document frequency. The lower the `IDF(t)`,
    the more insignificant the term, and the higher, the more the term in a query
    should count toward the relevance score.
  id: totrans-115
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5 逆文档频率。`|D|`是所有文档的总数，`t`是术语，`DF(t)`是文档频率。`IDF(t)`越低，术语就越不重要，越高，术语在查询中的相关性得分就应该越高。
- en: Carrying forward our `the` `cat` `in` `the` `hat` query example from the last
    section, a vector of IDFs would thus look like the following listing.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 沿用上一节中我们的“the cat in the hat”查询示例，一个逆文档频率（IDF）向量将看起来如下所示。
- en: Listing 3.7 Calculating inverse document frequency
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.7 计算逆文档频率
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '#1 The IDF function, which dictates the importance of a term in the query'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 决定查询中术语重要性的IDF函数'
- en: '#2 Mocked document counts simulating realistic statistics from an inverted
    index'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 模拟倒排索引中真实统计数据的文档计数'
- en: '#3 IDF is term-dependent, not document-dependent, so it is the same for both
    queries and documents.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '#3 IDF是术语依赖的，不是文档依赖的，所以对于查询和文档都是相同的。'
- en: 'Output:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'These results look encouraging. The terms can now be weighted based on their
    relative descriptiveness or significance to the query:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果看起来很有希望。现在可以根据术语相对于查询的描述性或重要性来加权：
- en: '“hat”: `6.2786`'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '“hat”: `6.2786`'
- en: '“cat”: `5.5953`'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '“cat”: `5.5953`'
- en: '“in”: `1.1053`'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '“in”: `1.1053`'
- en: '“the”: `1.0513`'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '“the”: `1.0513`'
- en: We’ll next combine the TF and IDF ranking techniques you’ve learned thus far
    into a balanced relevance ranking function.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来将到目前为止学到的TF和IDF排名技术结合起来，形成一个平衡的相关性排名函数。
- en: '3.1.6 TF-IDF: A balanced weighting metric for text-based relevance'
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.6 TF-IDF：基于文本相关性的平衡加权指标
- en: 'We now have the two principal components of text-based relevance ranking:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了基于文本的相关性排名的两个主要成分：
- en: TF measures how well a term describes a document.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF衡量一个术语描述文档的好坏。
- en: IDF measures how important each term is.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IDF衡量每个术语的重要性。
- en: Most search engines, and many other data science applications, use a combination
    of these factors as the basis for textual similarity scoring, using a variation
    of the function in figure 3.6.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数搜索引擎以及许多其他数据科学应用都使用这些因素的组合作为文本相似度评分的基础，使用图 3.6 中的函数的变体。
- en: '![figure](../Images/grainger-ch3-eqs-3x.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图形](../Images/grainger-ch3-eqs-3x.png)'
- en: Figure 3.6 TF-IDF score. Combines both the TF and IDF calculations into a balanced
    text-ranking similarity score.
  id: totrans-136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 3.6 TF-IDF 分数。将 TF 和 IDF 计算结合成一个平衡的文本排名相似度分数。
- en: With this improved feature-weighting function in place, we can finally calculate
    a balanced relevance score as follows.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在此改进的特征加权函数到位后，我们最终可以计算一个平衡的相关性分数如下。
- en: Listing 3.8 Calculating TF-IDF for the query `the cat in the hat`
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.8 计算 `the cat in the hat` 查询的 TF-IDF
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Output:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE19]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Finally, our search results make sense! doc2 gets the highest score, since it
    matches the most important words the most, followed by doc3, which contains all
    the words, but not as many times, followed by doc1, which only contains an abundance
    of insignificant words.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的搜索结果变得有意义！doc2 获得最高分数，因为它与最重要的单词匹配得最多，其次是包含所有单词但出现次数较少的 doc3，然后是只包含大量不相关单词的
    doc1。
- en: This TF-IDF calculation is at the heart of many search engine relevance algorithms,
    including the default similarity algorithm, known as BM25, which is currently
    used for keyword-based ranking in most search engines. We’ll introduce BM25 in
    the next section.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这种 TF-IDF 计算是许多搜索引擎相关性算法的核心，包括默认的相似度算法，即 BM25，目前它被用于大多数搜索引擎中的基于关键词的排名。我们将在下一节介绍
    BM25。
- en: 3.2 Controlling the relevance calculation
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 控制相关性计算
- en: In the last section, we showed how queries and documents can be represented
    as vectors and how cosine similarity, or other similarity measurements like the
    dot product, can be used as a relevance function to compare queries and documents.
    We introduced TF-IDF ranking, which can be used to create a feature weight that
    balances both the strength of occurrence (TF) and significance of a term (IDF)
    for each term in a term-based vector.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们展示了如何将查询和文档表示为向量，以及如何使用余弦相似度或其他相似度度量（如点积）作为相关性函数来比较查询和文档。我们介绍了 TF-IDF
    排名，它可以用于创建一个特征权重，平衡基于向量的每个术语的强度（TF）和术语的重要性（IDF）。
- en: In this section, we’ll show how a full relevance function can be specified and
    controlled in a search engine, including common query capabilities, modeling queries
    as functions, ranking versus filtering, and applying different kinds of boosting
    techniques.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将展示如何在搜索引擎中指定和控制完整的相关性函数，包括常见的查询功能、将查询建模为函数、排名与过滤，以及应用不同类型的提升技术。
- en: '3.2.1 BM25: The industry standard default text-similarity algorithm'
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 BM25：行业标准默认文本相似度算法
- en: BM25 is the name of the default similarity algorithm in Apache Lucene, Apache
    Solr, Elasticsearch, OpenSearch, and many other search engines. BM25 (short for
    Okapi “Best Matching” version 25) was first published in 1994, and it demonstrates
    improvements over standard TF-IDF cosine similarity ranking in many real-world,
    text-based ranking evaluations. For now, it still beats ranking models using embeddings
    from most non-fine-tuned LLMs, so it serves as a good baseline for keyword-based
    ranking.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: BM25 是 Apache Lucene、Apache Solr、Elasticsearch、OpenSearch 以及许多其他搜索引擎中默认的相似度算法。BM25（Okapi
    “最佳匹配”版本 25 的缩写）首次于 1994 年发表，它在许多基于文本的排名评估中显示出比标准 TF-IDF 余弦相似度排名的改进。目前，它仍然优于大多数未微调的
    LLMs 的嵌入表示的排名模型，因此它作为基于关键词排名的良好基线。
- en: BM25 still uses TF-IDF at its core, but it also includes several other parameters,
    giving more control over factors like TF saturation point and document length
    normalization. It also sums the weights for each matched keyword instead of calculating
    a cosine.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: BM25 仍然使用 TF-IDF 作为其核心，但它还包括其他几个参数，提供了对 TF 饱和点和文档长度归一化等因素的更多控制。它还总结了每个匹配关键字的权重，而不是计算余弦值。
- en: 'The full BM25 calculation is shown in figure 3.7\. The variables are defined
    as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 全部 BM25 计算如图 3.7 所示。变量定义如下：
- en: '*t* = term; *d* = document; *q* = query.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t* = 术语；*d* = 文档；*q* = 查询。'
- en: '*freq*(*t*, >*d*) is a simple TF, Σ[𝑡ϵ𝑑] 1 showing the number of times term
    `t` occurs in document `d`.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*freq*(*t*, >*d*) 是一个简单的 TF，Σ[𝑡ϵ𝑑] 1 显示了术语 `t` 在文档 `d` 中出现的次数。'
- en: '![equation image](../Images/eq-chapter-3-136-1.png)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![方程式图像](../Images/eq-chapter-3-136-1.png)'
- en: is a variation of IDF used in BM25, where *N* is the total number of documents
    and *N*(*t*) is the number of documents containing term *t*.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|*d*| is the number of terms in document *d*.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*avgdl* is the average number of terms per document in the index.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k* is a free parameter that usually ranges from 1.2 to 2.0 and increases the
    TF saturation point.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b* is a free parameter usually set to around 0.75\. It increases the effect
    of document normalization.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![figure](../Images/CH03_F07_Grainger.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 BM25 scoring function. It still predominantly uses a simplified *TF*
    and a variation of *IDF*, but it provides more control over how much each additional
    occurrence of a term contributes to the score (the `k` parameter) and how much
    scores are normalized based on document length (the `b` parameter).
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You can see that the numerator contains *freq* (simplified TF) and *IDF* parameters,
    while the denominator adds the new normalization parameters *k* and *b*. The TF
    saturation point is controlled by *k*, making additional matches on the same term
    count less as *k* is increased, and by *b*, which controls the level of document
    length normalization more as it increases. The *TF* for each term is calculated
    as *freq*(*t*,*d*) / (*freq*(*t*,*d*) + *k* · (1 – *b* + *b* · |*d*| / *avgd**l*)),
    which is a more complex calculation than the one we used in figure 3.3\.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: Conceptually, BM25 just provides a heuristically determined better way to normalize
    the TF than traditional TF-IDF. It’s also worth noting that several variations
    of the BM25 algorithm exist (BM25F, BM25+), and, depending on the search engine
    you use, you might see some slight alterations and optimizations.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Instead of reimplementing all this math in Python as we test out BM25, let’s
    now switch over to using our search engine and see how it performs the calculation.
    Let’s start by creating a collection in the search engine (listing 3.9). A collection
    contains a specific schema and configuration, and it is the unit upon which we
    will index documents, search, rank, and retrieve search results. Then we’ll index
    some documents (using our previous `the` `cat` `in` `the` `hat` example), as shown
    in listing 3.10.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.9 Creating the `cat_in_the_hat` collection
  id: totrans-164
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '#1 The engine is set to Apache Solr by default. See appendix B to use other
    supported search engines and vector databases.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Listing 3.10 Adding documents to a collection
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Output:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: With our documents added to the search engine, we can now issue our query and
    see the full BM25 scores. The following listing searches with the query `the cat
    in the hat` and requests the relevance calculation explanation for each document.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Listing 3.11 Ranking by and inspecting the BM25 similarity score
  id: totrans-174
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Output:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: For the top-ranked document, doc2, you can see a portion of the score calculation
    using the `tf` and `idf` components, and you can see the high-level scores for
    each matched term in the query for the other two documents. If you would like
    to dig deeper into the math, you can examine the full calculations in the Jupyter
    notebook.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 对于排名最高的文档，doc2，你可以看到使用 `tf` 和 `idf` 组件的部分分数计算，并且你可以看到其他两个文档中每个匹配术语的高级分数。如果你想深入了解数学，可以检查
    Jupyter 笔记本中的完整计算。
- en: 'While the BM25 calculation is more complex than the TF-IDF feature weight calculations,
    it still uses TF-IDF as a core part of its calculation. As a result, the BM25
    ranking is in the same relative order as our TF-IDF calculations from listing
    3.8:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然BM25计算比TF-IDF特征权重计算更复杂，但它仍然将TF-IDF作为其计算的核心部分。因此，BM25排名与列表 3.8 中的TF-IDF计算的相对顺序相同：
- en: '[PRE26]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Our query for `the cat in the hat` can still very much be thought of as a vector
    of the BM25 scores for each of the terms: `["the", "cat", "in", "the", "hat"]`.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 `the cat in the hat` 的查询仍然可以被视为每个术语的BM25分数的向量：`["the", "cat", "in", "the",
    "hat"]`。
- en: 'What may not be obvious is that the feature weights for each of these terms
    are actually overridable functions. Instead of thinking of our query as a bunch
    of keywords, we can think of our query as a mathematical function composed of
    other functions, where some of those functions take keywords as inputs and return
    numerical values (scores) to be used in the relevance calculation. For example,
    our query could alternatively be expressed as this vector:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 可能不明显的是，这些术语的每个特征权重实际上是可以覆盖的函数。我们不应该将查询视为一系列关键词，而可以将查询视为由其他函数组成的数学函数，其中一些函数将关键词作为输入并返回用于相关性计算的数值（分数）。例如，我们的查询可以表示为以下向量：
- en: '[PRE27]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `query` function here simply calculates the BM25 of the term passed in,
    relative to all the documents scored. So the BM25 of the entire query is the sum
    of the TF-IDFs of each term. In Solr query syntax, this would be
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `query` 函数只是计算传入术语相对于所有评分文档的BM25值。因此，整个查询的BM25是每个术语TF-IDF的总和。在Solr查询语法中，这将表示为
- en: '[PRE28]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: If we execute this “functionized” version of the query, we get the exact same
    relevance score as if we had executed the query directly. The following listing
    shows the code that performs this version of the query.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们执行这个“函数化”版本的查询，我们将得到与直接执行查询完全相同的相关性分数。以下列表显示了执行此版本查询的代码。
- en: Listing 3.12 Text similarity using the `query` function
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.12 使用 `query` 函数进行文本相似度
- en: '[PRE29]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Output:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE30]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As expected, the scores are the same as before—we’ve simply substituted explicit
    functions where implicit functions were previously.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，分数与之前相同——我们只是用显式函数替换了之前隐式函数的位置。
- en: 3.2.2 Functions, functions, everywhere!
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 函数，到处都是函数！
- en: We just encountered the `query` function (at the end of the previous section),
    which performs the default (BM25) similarity calculation on keywords. Understanding
    that every part of the query is actually a configurable scoring function opens
    tremendous possibilities for manipulating the relevance algorithm. What *other*
    kinds of functions can be used in queries? Can we use other features in our scoring
    calculation—perhaps some that are not text-based?
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚遇到了 `query` 函数（在前一节的末尾），它对关键词执行默认（BM25）相似度计算。理解查询的每一部分实际上都是一个可配置的评分函数，这为操作相关性算法打开了巨大的可能性。查询中可以使用哪些*其他*类型的函数？我们能否在我们的评分计算中使用其他特征——可能是一些非基于文本的特征？
- en: 'Here is a partial list of functions and scoring techniques commonly applied
    to influence relevance scores:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个部分列表，列出了常用的一些函数和评分技术，这些技术通常用于影响相关性分数：
- en: '*Geospatial boosting*—Documents near the user running the query should rank
    higher.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*地理空间增强*—距离查询用户较近的文档应该排名更高。'
- en: '*Date boosting*—Newer documents should get a higher relevance boost.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*日期增强*—较新的文档应该获得更高的相关性提升。'
- en: '*Popularity boosting*—More popular documents should get a higher relevance
    boost.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*流行度增强*—更受欢迎的文档应该获得更高的相关性提升。'
- en: '*Field boosting*—Terms matching in certain fields should get a higher weight
    than in other fields.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*字段增强*—在特定字段中匹配的术语应该比其他字段获得更高的权重。'
- en: '*Category boosting*—Documents in categories related to query terms should get
    a higher relevance boost.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*类别增强*—与查询术语相关的文档应该获得更高的相关性提升。'
- en: '*Phrase boosting*—Documents matching multi-term phrases in the query should
    rank higher than those only matching the words separately.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*短语增强*——与查询中的多词短语匹配的文档应该比仅匹配单独单词的文档排名更高。'
- en: '*Semantic expansion*—Documents containing other words or concepts that are
    highly related to the query keywords and context should be boosted.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*语义扩展*——包含与查询关键词和上下文高度相关的其他单词或概念的文档应该被增强。'
- en: Using the book’s search-engine-agnostic search API
  id: totrans-202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用本书的搜索引擎无关的搜索API
- en: Throughout the book and codebase, we’ve implemented a set of Python libraries
    providing a generic API for indexing documents (`collection.add_documents( documents)`
    or `collection.write(dataframe)`), querying documents (`collection.search(**query_parameters)`),
    and performing other search engine operations. This allows you to execute the
    same code in the book and corresponding notebooks regardless of which supported
    search engine or vector database you have selected, delegating the creation of
    engine-specific syntax to the client library. See appendix B for details about
    how to switch seamlessly between engines.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书和代码库中，我们实现了一套Python库，提供了一组通用的API用于索引文档（`collection.add_documents(documents)`
    或 `collection.write(dataframe)`），查询文档（`collection.search(**query_parameters)`），以及执行其他搜索引擎操作。这允许你无论选择哪种支持的搜索引擎或向量数据库，都可以在书中和相应的笔记本中执行相同的代码，将特定引擎的语法创建委托给客户端库。有关如何在引擎之间无缝切换的详细信息，请参阅附录B。
- en: While these generic methods for invoking AI-powered search against your favorite
    engine are powerful, it’s also helpful in some cases to see the details of the
    underlying implementation within the search engine, and for more complicated examples
    it can even be difficult to express the full power of what’s going on using the
    higher-level engine-agnostic API. For this reason, we will occasionally also include
    the raw search engine syntax for our default search engine (Apache Solr) in the
    book. If you’re unfamiliar with Apache Solr and its syntax, please don’t get too
    bogged down in the details. The important thing is to understand the concepts
    well-enough that you can apply them to your search engine of choice.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些针对AI驱动的搜索的通用方法非常强大，但在某些情况下，了解搜索引擎底层实现的细节也很有帮助，对于更复杂的示例，甚至可能难以使用高级引擎无关的API完全表达其功能。因此，我们偶尔也会在书中包含默认搜索引擎（Apache
    Solr）的原始搜索引擎语法。如果你不熟悉Apache Solr及其语法，请不要过于纠结于细节。重要的是要充分理解这些概念，以便将它们应用到你的选择搜索引擎中。
- en: 'These techniques (and many more) are supported by most major search engines.
    For example, field boosting can be accomplished in our search client by appending
    a `^BOOST_AMOUNT` after any of the fields specified in `query_fields` for a query:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这些技术（以及更多）被大多数主要搜索引擎支持。例如，字段增强可以通过在`query_fields`中指定的任何字段后附加`^BOOST_AMOUNT`在我们的搜索客户端中实现：
- en: '[PRE31]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This query request provides a 10X relevancy boost for matches in the `title`
    field and a 2.5X relevancy boost for matches in the `description` field. When
    mapped into Solr syntax, it looks like this:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 此查询请求为`title`字段中的匹配项提供10倍的相关性增强，为`description`字段中的匹配项提供2.5倍的相关性增强。当映射到Solr语法时，它看起来像这样：
- en: '[PRE32]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Every search engine is different, but many of these techniques are built into
    specific query parsers in Solr, either through query syntax or through query parser
    options, as with the `edismax` query parser just shown.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 每个搜索引擎都不同，但许多这些技术都内置在Solr的特定查询解析器中，无论是通过查询语法还是通过查询解析器选项，就像刚刚展示的`edismax`查询解析器一样。
- en: 'Boosting on full phrase matching, on two-word phrases, and on three-word phrases
    is also a native feature of Solr’s `edismax` query parser:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在全短语匹配、双词短语和三词短语上增强，也是Solr的`edismax`查询解析器的原生功能：
- en: 'Boost docs containing the exact phrase `"the` `cat` `in` `the` `hat"` in the
    `title` field:'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`title`字段中包含确切短语`"the cat in the hat"`的文档：
- en: '[PRE33]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Boost docs containing the two-word phrases `"the cat"`, `"cat in"`, `"in the"`,
    or `"the hat"` in the `title` or `description` field:'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`title`或`description`字段中包含双词短语`"the cat"`、`"cat in"`、`"in the"`或`"the hat"`的文档：
- en: '[PRE34]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Boost docs containing the three-word phrases `"the` `cat` `in"` or `"in` `the`
    `hat"` in the `description` field:'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`description`字段中包含三词短语`"the cat in"`或`"in the hat"`的文档：
- en: '[PRE35]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Many of the other relevance-boosting techniques require constructing custom
    features using function queries. For example, if we want to create a query that
    only boosts the relevance ranking of documents geographically closest to the user
    running the search, we can issue the following Solr query:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他的相关性提升技术需要使用函数查询来构建自定义特征。例如，如果我们想创建一个查询，只提升运行搜索的用户地理位置最近的文档的相关性排名，我们可以发出以下
    Solr 查询：
- en: '[PRE36]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'That last query uses the `sort` parameter to strictly order documents by the
    `geodist` function, which takes the document’s location field name along with
    the user’s latitude and longitude as parameters. This works great when considering
    a single feature, but what if we want to construct a more complex sort based on
    many features? To accomplish this, we can update our query to apply several functions
    when calculating the relevance score, and then sort by the relevance score:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个查询使用 `sort` 参数严格按 `geodist` 函数对文档进行排序，该函数接受文档的位置字段名称以及用户的纬度和经度作为参数。当考虑单个特征时，这效果很好，但如果我们想根据许多特征构建更复杂的排序怎么办？为了实现这一点，我们可以更新我们的查询，在计算相关性得分时应用多个函数，然后按相关性得分进行排序：
- en: '[PRE37]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'That query has a few interesting characteristics:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 该查询有几个有趣的特性：
- en: 'It constructs a query vector containing four features: BM25 relevance score
    for the keywords (higher is better), geographical distance (lower is better),
    publication date (newer is better), and popularity (higher is better).'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它构建了一个包含四个特征的查询向量：关键词的 BM25 相关性得分（越高越好）、地理距离（越低越好）、发表日期（越新越好）和流行度（越高越好）。
- en: Each of the feature values is scaled between `0` and `25` so they are all comparable,
    with the best score for each feature being `25`, and the worst score near `0`.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个特征值都在 `0` 到 `25` 之间缩放，以便它们都是可比较的，每个特征的最好得分是 `25`，最差得分接近 `0`。
- en: Thus, a “perfect score” will add up to `100` (all 4 features scoring `25`),
    and the worst score will be approximately `0`.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，“完美得分”将累计到 `100`（所有 4 个特征得分 `25`），最差得分将大约为 `0`。
- en: Since the relative contribution of `25` is specified as part of the query for
    each function, we can easily adjust the weights for any features on the fly to
    influence the final relevance calculation.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 `25` 的相对贡献是作为每个函数查询的一部分指定的，因此我们可以轻松地实时调整任何特征的权重，以影响最终的相关性计算。
- en: With the last query, we have fully taken the relevance calculation into our
    own hands by modeling the relevance features and giving them weights. While this
    is very powerful, it still requires significant manual effort to determine which
    features matter most for a given domain and to tune their weights. In chapter
    10, we’ll walk through building machine-learned ranking models to automatically
    make those decisions for us (a process known as *learning to rank*). For now,
    our goal is just to understand the mechanics of modeling features in query vectors
    and how to programmatically control their weights.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 通过最后一个查询，我们已经完全将相关性计算掌握在自己手中，通过建模相关性特征并赋予它们权重。虽然这非常强大，但仍需要大量的手动工作来确定给定领域最重要的特征，并调整它们的权重。在第
    10 章中，我们将介绍构建机器学习排名模型的过程，以自动为我们做出这些决定（这个过程称为 *学习排名*）。目前，我们的目标只是理解在查询向量中建模特征以及如何编程控制它们的权重。
- en: Deeper dives on function queries
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 深入了解函数查询
- en: If you’d like to learn more about how to utilize Solr’s function queries, we
    recommend reading chapter 7 of *Solr in Action*, one of our previous books, by
    Trey Grainger and Timothy Potter (Manning, 2014; [https://mng.bz/n0Y5](https://mng.bz/n0Y5)).
    For a full list of available function queries in Solr, you can also check out
    the documentation in the function query section of the Solr Reference Guide ([https://mng.bz/vJop](https://mng.bz/vJop)).
    If you’re using a different search engine, check out their documentation for similar
    guidance.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于如何利用 Solr 的函数查询的信息，我们建议阅读我们之前的一本书《Solr in Action》的第 7 章，由 Trey Grainger
    和 Timothy Potter 撰写（Manning，2014；[https://mng.bz/n0Y5](https://mng.bz/n0Y5)）。要查看
    Solr 中可用的所有函数查询的完整列表，你还可以查看 Solr 参考指南中函数查询部分的文档（[https://mng.bz/vJop](https://mng.bz/vJop)）。如果你使用的是不同的搜索引擎，请查看它们的文档以获取类似指南。
- en: We’ve seen the power of utilizing functions as features in our queries, but
    so far our examples have all been what are called “additive” boosts, where the
    sum of the values of each function calculation comprises the final relevance score.
    It is also frequently useful to combine functions in a fuzzier, more flexible
    way through “multiplicative” boosts, which we’ll cover in the next section.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了利用函数作为查询特征的力量，但到目前为止，我们的例子都是所谓的“加性”提升，其中每个函数计算的值之和构成了最终的相关性得分。通过“乘性”提升以更模糊、更灵活的方式组合函数也经常很有用，我们将在下一节中介绍。
- en: 3.2.3 Choosing multiplicative vs. additive boosting for relevance functions
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 选择乘性提升与加性提升用于相关性函数
- en: One last topic to address, concerning how we control our relevance functions,
    is multiplicative versus additive boosting of relevance features.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个要讨论的话题，关于我们如何控制我们的相关性函数，是乘性提升与加性提升之间的比较。
- en: 'In all our examples to this point, we have added multiple features into our
    query vector to contribute to the score. For example, the following Solr queries
    will all yield equivalent relevance calculations, assuming they are filtered down
    to the same result set (i.e., `filters=["the cat in the hat"]`):'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止的所有例子中，我们都将多个特征添加到我们的查询向量中以贡献得分。例如，以下Solr查询都将产生等效的相关性计算，假设它们被过滤到相同的结果集（即，`filters=["the
    cat in the hat"]`）：
- en: '[PRE38]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The kind of relevance boosting in each of these examples is known as *additive
    boosting*, and it maps well to our concept of a query as nothing more than a vector
    of features that needs to have its similarity compared across documents. In additive
    boosting, the relative contribution of each feature decreases as more features
    are added, since the total score is just the sum of all the features’ scores.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这些例子中的相关性提升类型被称为*加性提升*，并且很好地映射到我们将查询视为仅仅是一个需要跨文档比较其相似性的特征向量的概念。在加性提升中，随着更多特征的添加，每个特征的相对贡献会减少，因为总得分只是所有特征得分的总和。
- en: '*Multiplicative boosting*, in contrast, allows a document’s entire calculated
    relevance score to be scaled (multiplied) by one or more functions. Multiplicative
    boosting enables boosts to “pile up” on each other, preventing the need for individually
    constraining the weights of different sections of the query, as we did in section
    3.2.2\. There, we had to ensure that the keyword scores, geographical distance,
    age, and popularity of documents were each scaled to 25% of the relevance score
    so that they would add up to a maximum score of 100%.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，*乘性提升*允许文档的整个计算相关性得分通过一个或多个函数进行缩放（乘法）。乘性提升使得提升可以“叠加”在一起，避免了在3.2.2节中我们不得不对查询的不同部分分别约束权重的情况。在那里，我们必须确保关键词得分、地理距离、年龄和文档的流行度各自缩放到相关性得分的25%，以便它们加起来达到最大得分100%。
- en: 'To supply a multiplicative boost in Apache Solr, you can either use the `boost`
    query parser (syntax: `{!boost …}`) in your query vector or, if you are using
    the `edismax` query parser, the simplified `boost` query param. The following
    two queries will multiply a document’s relevance score by ten times the value
    in the `popularity` field:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Apache Solr中提供乘性提升，你可以在查询向量中使用`boost`查询解析器（语法：`{!boost …}`），或者如果你使用的是`edismax`查询解析器，可以使用简化的`boost`查询参数。以下两个查询将文档的相关性得分乘以`popularity`字段值的10倍：
- en: '[PRE39]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: In this example, the query for `the cat in the hat` still uses additive boosting
    (the BM25 value of each keyword is added together), but the final score is multiplied
    by 10 times the value in the `popularity` field. This multiplicative boosting
    allows the popularity to scale the relevance score independently of any other
    features.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，对`the cat in the hat`的查询仍然使用加性提升（每个关键词的BM25值相加），但最终得分乘以`popularity`字段值的10倍。这种乘性提升允许流行度独立于其他任何特征来调整相关性得分。
- en: In general, multiplicative boosts offer you greater flexibility to combine different
    relevance features without having to explicitly predefine a relevance formula
    accounting for every potential contributing factor. On the other hand, this flexibility
    can lead to unexpected consequences if the multiplicative boost values for particular
    features get too high and overshadow other features. In contrast, additive boosts
    can be a pain to manage, because you need to explicitly scale them so that they
    can be combined while still maintaining a predictable contribution to the overall
    score. However, with this explicit scaling, you maintain tight control over the
    relevance scoring calculation and range of scores. Both additive and multiplicative
    boosts can be useful, so it’s best to consider the problem at hand and experiment
    with what gets you the best results.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，乘法增强提供了更大的灵活性，可以在不显式预定义一个考虑每个潜在贡献因素的关联公式的情况下，结合不同的关联特征。另一方面，这种灵活性可能导致意外的后果，如果特定特征的乘法增强值过高，以至于掩盖了其他特征。相比之下，加法增强可能难以管理，因为你需要显式地调整它们，以便在保持对整体得分的可预测贡献的同时进行组合。然而，通过这种显式缩放，你可以保持对关联得分计算和得分范围的紧密控制。无论是加法增强还是乘法增强，都可能很有用，因此最好考虑手头的问题并实验哪种方法能带来最佳结果。
- en: We’ve now covered the major ways of controlling relevance ranking in the search
    engine, but the matching and filtering of documents can often be just as important,
    so we’ll cover them in the next section.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经介绍了控制搜索引擎中相关性排名的主要方法，但文档的匹配和过滤通常同样重要，因此我们将在下一节中介绍它们。
- en: 3.2.4 Differentiating matching (filtering) vs. ranking (scoring) of documents
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.4 区分文档的匹配（过滤）与排名（评分）
- en: We’ve spoken of queries and documents as feature vectors, but we’ve mainly discussed
    search thus far as a process of either calculating a vector similarity (such as
    cosine or dot product) or adding up document scores for each feature (keyword
    or function) in the query.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将查询和文档视为特征向量，但到目前为止，我们主要讨论了搜索作为一个计算向量相似度（如余弦或点积）的过程，或者将查询中每个特征（关键词或函数）的文档得分相加。
- en: 'Once documents are indexed, there are two primary steps involved in executing
    a query:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦文档被索引，执行查询涉及两个主要步骤：
- en: '*Matching*—Filtering results to a known set of possible answers'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*匹配*—将结果过滤到一组已知的可能答案中'
- en: '*Ranking*—Ordering all the possible answers by relevance'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*排名*—按相关性对所有可能的答案进行排序'
- en: We can often completely skip the first step (matching) and still see the exact
    same results on page one (and for many pages), since the most relevant results
    should generally rank the highest and thus show up first. If you think back to
    chapter 2, we even saw some vector scoring calculations (comparing feature vectors
    for food items—i.e., “apple juice” versus “donut”) where we would have been unable
    to filter results at all. We instead had to first score every document to determine
    which ones to return based upon relevance alone. In this scenario (using dense
    vector embeddings), we didn’t even have keywords or other attributes that could
    be used as a filter.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们经常可以完全跳过第一步（匹配），仍然在第一页（以及许多页面）上看到完全相同的结果，因为最相关的结果通常排名最高，因此会首先显示。如果你回想起第2章，我们甚至看到了一些向量得分计算（比较食品项目的特征向量——即“苹果汁”与“甜甜圈”），在这种情况下，我们根本无法过滤结果。我们不得不首先对每个文档进行评分，以确定哪些文档基于相关性返回。在这种情况下（使用密集向量嵌入），我们甚至没有可以作为过滤器的关键词或其他属性。
- en: So, if the initial matching phase is effectively optional, why do it at all?
    One obvious answer is that it provides a significant performance optimization.
    Instead of iterating through every single document and calculating a relevance
    score, we can greatly speed up both our relevance calculations and the overall
    response time of our search engine by first filtering the initial results to a
    smaller set of documents that are logical matches.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果初始匹配阶段实际上是可选的，为什么还要进行它呢？一个明显的答案是它提供了显著的性能优化。我们不必遍历每一份文档并计算关联得分，通过首先将初始结果过滤到一组较小的、逻辑上匹配的文档集合中，我们可以大大加快我们的关联计算和搜索引擎的整体响应时间。
- en: There are additional benefits to being able to filter result sets, in that we
    can provide analytics, such as the number of matching documents or counts of specific
    values found in documents (known as *facets* or *aggregations*). Returning facets
    and similar aggregated metadata from the search results helps the user subsequently
    filter down by specific values to further explore and refine their result set.
    Finally, there are plenty of scenarios where “having logical matches” should be
    considered among the most important features in the ranking function, so simply
    filtering on logical matches upfront can greatly simplify the relevance calculation.
    We’ll discuss these trade-offs in the next section.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 能够过滤结果集还有其他好处，例如我们可以提供分析，如匹配文档的数量或文档中找到的特定值的计数（称为**维面**或**聚合**）。从搜索结果中返回维面和类似的聚合元数据有助于用户随后根据特定值进行过滤，以进一步探索和细化他们的结果集。最后，在许多场景中，“具有逻辑匹配”应被视为排名函数中最重要的特征之一，因此简单地根据逻辑匹配进行过滤可以极大地简化相关性计算。我们将在下一节讨论这些权衡。
- en: '3.2.5 Logical matching: Weighting the relationships between terms in a query'
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.5 逻辑匹配：查询中术语之间关系的加权
- en: We just mentioned that filtering results before scoring them is primarily a
    performance optimization and that the first few pages of search results would
    likely look the same regardless of whether you filter the results or just do relevance
    ranking.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才提到，在评分之前过滤结果主要是性能优化，并且无论你过滤结果还是只进行相关性排名，搜索结果的前几页可能看起来都一样。
- en: 'This only holds true, however, if your relevance function successfully contains
    features that already appropriately boost better logical matches. For example,
    consider the difference between expectations for the following queries:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这只有在你的相关性函数成功包含能够适当提升更好逻辑匹配的特征时才成立。例如，考虑以下查询的期望差异：
- en: '`"statue of liberty"`'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`"statue of liberty"`'
- en: '`statue AND of AND liberty`'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`statue AND of AND liberty`'
- en: '`statue OR of OR liberty`'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`statue OR of OR liberty`'
- en: '`statue of liberty`'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`statue of liberty`'
- en: From a logical matching standpoint, the first query will be very precise, only
    matching documents containing the *exact* phrase “statue of liberty”. The second
    query will only match documents containing all the terms “statue”, “of”, and “liberty”,
    but not necessarily as a phrase. The third query will match any document containing
    any of the three terms, which means documents *only* containing “of” will match,
    but documents containing “statue” and “liberty” should rank much higher due to
    the BM25 scoring calculation.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 从逻辑匹配的角度来看，第一个查询将非常精确，仅匹配包含“statue of liberty”这一**精确**短语的文档。第二个查询将仅匹配包含所有术语“statue”、“of”和“liberty”的文档，但不一定是作为一个短语。第三个查询将匹配包含这三个术语中的任何一个的任何文档，这意味着仅包含“of”的文档将匹配，但包含“statue”和“liberty”的文档应该由于BM25评分计算的权重而排名更高。
- en: In theory, if phrase boosting is turned on as a feature, documents containing
    the full phrase will likely rank highest, followed by documents containing all
    terms, followed by documents containing any of the words. Assuming that happens,
    you should see a similar ordering of results regardless of whether you filter
    them to logical Boolean matches or whether you only sort based on a relevance
    function.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，如果短语提升作为一项功能被启用，包含完整短语的文档可能会排名最高，其次是包含所有术语的文档，然后是包含任何单词的文档。假设这种情况发生，无论你过滤它们以进行逻辑布尔匹配，还是仅根据相关性函数进行排序，你应该都会看到类似的结果顺序。
- en: In practice, though, users often consider the logical structure of their queries
    to be highly relevant to the documents they expect to see, so respecting this
    logical structure and filtering *before* ranking allows you to remove results
    that users’ queries indicate are safe to remove.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在实践中，用户通常认为他们的查询的逻辑结构与他们期望看到的文档高度相关，因此尊重这种逻辑结构并在排名之前进行过滤可以使你移除用户查询表明可以安全移除的结果。
- en: 'Sometimes the logical structure of user queries is ambiguous, however, such
    as with our fourth example: the query `statue` `of` `liberty`. Does this logically
    mean `statue` `AND` `of` `AND` `liberty`, `statue` `OR` `of` `OR` `liberty`, or
    something more nuanced like `(statue` `AND` `of)` `OR` `(statue` `AND` `liberty)`
    `OR` `(of` `AND` `liberty)`, which essentially means “match at least two of three
    terms”. Using the “minimum match” (`min_match`) parameter in our search API enables
    you to control these kinds of matching thresholds easily, even on a per-query
    basis:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 有时用户查询的逻辑结构可能是模糊的，例如，在我们的第四个例子中：查询`statue` `of` `liberty`。这逻辑上意味着`statue` `AND`
    `of` `AND` `liberty`，`statue` `OR` `of` `OR` `liberty`，还是更细微的，例如`(statue` `AND`
    `of)` `OR` `(statue` `AND` `liberty)` `OR` `(of` `AND` `liberty)`，这本质上意味着“至少匹配三个词中的两个”。使用我们搜索API中的“最小匹配”(`min_match`)参数，您可以轻松控制这些匹配阈值，甚至可以针对每个查询进行控制：
- en: '100% of query terms must match (equivalent to `statue` `AND` `of` `AND` `liberty`):'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 100%的查询词必须匹配（相当于`statue` `AND` `of` `AND` `liberty`）：
- en: '[PRE40]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'At least one query term must match (equivalent to `statue` `OR` `of` `OR` `liberty`):'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少有一个查询词必须匹配（相当于`statue` `OR` `of` `OR` `liberty`）：
- en: '[PRE41]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'At least two query terms must match (equivalent to `(statue` `AND` `of)` `OR
    (statue` `AND` `liberty)` `OR` `(of` `AND` `liberty)`):'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少有两个查询词必须匹配（相当于`(statue` `AND` `of)` `OR (statue` `AND` `liberty)` `OR` `(of`
    `AND` `liberty)`）：
- en: '[PRE42]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The `min_match` param in our Python API supports specifying either a minimum
    percentage (0% to 100%) of terms or a number of terms (1 to *N* terms) that must
    match. This parameter corresponds with Solr’s `mm` parameter and OpenSearch’s
    and Elasticsearch’s `minimum_should_match` parameter. In addition to accepting
    a percentage or number of terms to match, those engines also support a step function
    like `mm=2<-30% 5<3`. This example step function means “all terms are required
    if there are less than 2 terms, up to 30% of terms can be missing if there are
    less than 5 terms, and at least 3 terms must exist if there are 5 or more terms”.
    When using Solr, the `mm` parameter works with the `edismax` query parser, which
    is the primary query parser we will use for text-matching queries in this book
    if Solr is configured as your engine (per appendix B). You can consult the “Extended
    DisMax Parameters” section of the Solr Reference Guide for more details on how
    to fine-tune your logical matching rules with these minimum match capabilities
    ([https://mng.bz/mRo8](https://mng.bz/mRo8)).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们Python API中的`min_match`参数支持指定必须匹配的最小百分比（0%到100%）或必须匹配的词数（1到*N*个词）。此参数与Solr的`mm`参数以及OpenSearch和Elasticsearch的`minimum_should_match`参数相对应。除了接受匹配的百分比或词数外，这些引擎还支持类似`mm=2<-30%
    5<3`的步进函数。此步进函数的例子意味着“如果少于2个词，则所有词都必须匹配；如果少于5个词，则最多30%的词可以缺失；如果5个或更多词，则至少必须有3个词”。当使用Solr时，`mm`参数与`edismax`查询解析器一起工作，这是我们将在本书中用于文本匹配查询的主要查询解析器（如果Solr配置为您的引擎，请参阅附录B）。您可以在Solr参考指南的“扩展DisMax参数”部分中查找有关如何使用这些最小匹配功能微调您的逻辑匹配规则的更多详细信息（[https://mng.bz/mRo8](https://mng.bz/mRo8)）。
- en: When thinking about constructing relevance functions, the ideas of filtering
    and scoring can often get mixed up, particularly since most search engines perform
    both for their main query parameter. We’ll attempt to separate these concerns
    in the next section.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 当思考构建相关性函数时，过滤和评分的概念可能会混淆，尤其是大多数搜索引擎都会为它们的主要查询参数执行这两者。我们将在下一节尝试分离这些关注点。
- en: '3.2.6 Separating concerns: Filtering vs. scoring'
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.6 分离关注点：过滤与评分
- en: In section 3.2.4, we differentiated between the ideas of matching and ranking.
    Matching of results is logical and is implemented by filtering search results
    down to a subset of documents, whereas ranking of results is qualitative and is
    implemented by scoring all documents relative to the query and then sorting them
    by that calculated score. In this section, we’ll cover some techniques to provide
    maximum flexibility in controlling matching and ranking by cleanly separating
    out the concerns of filtering and scoring.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在3.2.4节中，我们区分了匹配和排名的概念。结果的匹配是逻辑的，通过过滤搜索结果到文档的子集来实现，而结果的排名是定性的，通过相对于查询评分所有文档然后按该计算分数排序来实现。在本节中，我们将介绍一些技术，通过清晰地分离过滤和评分的关注点，以提供最大限度的灵活性来控制匹配和排名。
- en: 'Our search API has two primary ways to control filtering and scoring: the `query`
    and `filters` parameters. Consider the following request:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的搜索API有两种主要方式来控制过滤和评分：`query`和`filters`参数。考虑以下请求：
- en: '[PRE43]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In this query, the search engine is being instructed to filter the possible
    result set down to only documents with both a value of “books” in the `category`
    field and a value of “kid” in the `audience` field. In addition to those filters,
    however, the query also acts as a filter, so the result set gets further filtered
    down to only documents containing (100%) of the values “the”, “cat”, “in”, and
    “hat” in the `description` field.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个查询中，搜索引擎被指示过滤可能的结果集，只保留 `category` 字段值为“books”和 `audience` 字段值为“kid”的文档。然而，除了这些过滤器之外，查询本身也充当过滤器，因此结果集进一步过滤，只保留
    `description` 字段中包含（100%）“the”、“cat”、“in”和“hat”值的文档。
- en: The logical difference between the `query` and `filters` parameters is that
    `filters` only acts as a filter, whereas `query` acts as *both* a filter and a
    feature vector for relevance ranking. This dual use of the `query` parameter is
    helpful default behavior for queries, but mixing the concerns of filtering and
    scoring in the same parameter can be suboptimal for more advanced queries, especially
    if we’re simply trying to manipulate the relevance calculation and not arbitrarily
    removing results from our document set.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '`query` 和 `filters` 参数之间的逻辑区别在于，`filters` 只作为过滤器，而 `query` 则作为 *同时* 过滤器和相关性排名的特征向量。`query`
    参数的这种双重用途对于查询来说是帮助默认行为，但将过滤和评分的关注点混合在同一个参数中对于更高级的查询来说可能不是最佳选择，尤其是如果我们只是试图操纵相关性计算而不是任意从我们的文档集中删除结果。'
- en: 'There are a few ways to address this:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以解决这个问题：
- en: 'Model the `query` parameter as a function (functions only count toward relevance
    and do not filter):'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 `query` 参数建模为一个函数（函数只计算相关性，不进行过滤）：
- en: '[PRE44]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Make your query match all documents (no filtering or scoring) and apply a boost
    query (`bq`) parameter to influence relevance without scoring:'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使你的查询匹配所有文档（不进行过滤或评分）并应用一个提升查询 (`bq`) 参数来影响相关性而不进行评分：
- en: '[PRE45]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The `query` parameter both filters and then boosts based upon relevance, `filters`
    only filters, and `bq` only boosts. The two preceding approaches are logically
    equivalent, but we recommend the second option, since it’s a bit cleaner to use
    the dedicated `bq` parameter, which was designed to contribute toward the relevance
    calculation without filtering.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`query` 参数既过滤又根据相关性提升，`filters` 只过滤，而 `bq` 只提升。前两种方法在逻辑上是等价的，但我们推荐第二种选项，因为它使用专门的
    `bq` 参数更为简洁，该参数旨在贡献于相关性计算而不进行过滤。'
- en: You may have noticed that both versions of the query also contain a filter query
    `{!cache=false` `v=$user_query}` that filters on the `user_query`. Since the `query`
    parameter intentionally no longer filters our search results, this `filters` parameter
    is now required if we still want to filter to the user-entered query. The special
    `cache=false` parameter is used to turn off caching of the filter. Caching of
    filters is turned on by default in Solr, since filters tend to be reused often
    across requests. Since the `user_query` parameter is user-entered and is wildly
    variable in this case (not frequently reused across requests), it doesn’t make
    sense to pollute the search engine’s caches with these values. If you try to filter
    on user-entered queries without turning the cache off, it will waste system resources
    and likely slow down your search engine.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，查询的两个版本也包含一个过滤查询 `{!cache=false` `v=$user_query}`，它根据 `user_query`
    进行过滤。由于 `query` 参数故意不再过滤我们的搜索结果，因此如果还想过滤到用户输入的查询，现在这个 `filters` 参数是必需的。特殊的 `cache=false`
    参数用于关闭过滤的缓存。在 Solr 中，默认情况下会开启过滤的缓存，因为过滤通常在多个请求中重复使用。由于 `user_query` 参数是用户输入的，并且在这种情况下变化很大（在请求之间不经常重复使用），因此没有必要将这些值污染搜索引擎的缓存。如果你尝试在不关闭缓存的情况下过滤用户输入的查询，这将浪费系统资源，并可能减慢你的搜索引擎速度。
- en: The overarching theme here is that it’s possible to cleanly separate logical
    filtering from ranking features to maintain full control and flexibility over
    your search results. While going through this effort may be overkill for simple
    text-based ranking, separating these concerns becomes critical when attempting
    to build out more sophisticated ranking functions.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里的主旨是，可以干净地将逻辑过滤与排名特征分离，以保持对搜索结果的完全控制和灵活性。虽然对简单的基于文本的排名来说，这种努力可能有些过度，但当尝试构建更复杂的排名函数时，分离这些关注点变得至关重要。
- en: Now that you understand the mechanics of how to construct these kinds of purpose-built
    ranking functions, let’s wrap up this chapter with a brief discussion of how to
    apply these techniques to implement user- and domain-specific relevance ranking.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何构建这类专门用途的排名函数的机制，让我们以简要讨论如何将这些技术应用于实现用户和领域特定的相关性排名来结束本章。
- en: 3.3 Implementing user and domain-specific relevance ranking
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 实现用户和领域特定的相关性排名
- en: In section 3.2, we walked through how to modify the parameters of our query-to-document
    similarity algorithm dynamically. That included passing in our own functions as
    features that contribute to the score, in addition to text-based relevance ranking.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在3.2节中，我们介绍了如何动态修改我们的查询到文档相似度算法的参数。这包括传递我们自己的函数作为有助于评分的特征，除了基于文本的相关性排名之外。
- en: 'While text-based relevance ranking using BM25, TF-IDF, vector cosine similarity,
    or some other kind of statistics-based approach on word occurrences can provide
    decent generic search relevance out of the box, it can’t hold its own against
    good domain-specific relevance factors. Here are some domain-specific factors
    that often matter the most within various domains:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于文本的相关性排名，使用BM25、TF-IDF、向量余弦相似度或其他基于词频的统计方法可以提供不错的通用搜索相关性，但它无法与良好的领域特定相关性因素相媲美。以下是一些在各个领域中最常关注的领域特定因素：
- en: '*Restaurant search*—Geographical proximity, user-specific dietary restrictions,
    user-specific taste preferences, price range'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*餐厅搜索*—地理位置接近性、用户特定的饮食限制、用户特定的口味偏好、价格范围'
- en: '*News search*—Freshness (date), popularity, geographical area'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*新闻搜索*—新鲜度（日期）、流行度、地理区域'
- en: '*E-commerce*—Likelihood of conversion (click-through, add-to-cart, and/or purchase)'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*电子商务*—转换可能性（点击率、加入购物车和/或购买）'
- en: '*Movie search*—Name match (title, actor, etc.), popularity of movie, release
    date, critic review score'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*电影搜索*—名称匹配（标题、演员等）、电影流行度、上映日期、影评评分'
- en: '*Job search*—Job title, job level, compensation range, geographical proximity,
    job industry'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*职位搜索*—职位名称、职位级别、薪酬范围、地理位置接近性、行业'
- en: '*Web search*—Keyword match on page, popularity of page, popularity of website,
    location of match on page (in title, header, body, etc.), quality of page (duplicate
    content, spammy content, etc.), topic match between page and query'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*网页搜索*—页面上的关键词匹配、页面流行度、网站流行度、页面上的匹配位置（标题、页眉、正文等）、页面质量（重复内容、垃圾内容等）、页面与查询之间的主题匹配'
- en: These are just examples, but most search engines and domains have unique features
    that need to be considered to deliver an optimal search experience. This chapter
    has barely scratched the surface of the countless ways you can control the matching
    and ranking functions to return the best content. An entire profession exists—called
    *relevance engineering*—that is dedicated in many organizations to tuning search
    relevance. If you’d like to dive deeper, we highly recommend one of our prior
    books, *Relevant Search* by Doug Turnbull and John Berryman (Manning, 2016), which
    is a guide to this kind of relevance engineering.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是例子，但大多数搜索引擎和领域都有独特的特征需要考虑，以提供最佳的搜索体验。本章仅仅触及了无数种你可以控制匹配和排名函数以返回最佳内容的方法的表面。有一个整个行业——称为*相关性工程*——在许多组织中致力于调整搜索相关性。如果你想要深入了解，我们强烈推荐我们之前的一本书，由Doug
    Turnbull和John Berryman合著的《Relevant Search》（Manning, 2016），这是一本关于此类相关性工程的指南。
- en: Every search engine and domain have unique features that need to be considered
    to deliver an optimal search experience. Instead of having to manually model these
    relevance features, an AI-powered search engine can utilize machine learning to
    automatically generate and weight such features.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 每个搜索引擎和领域都有独特的特征需要考虑，以提供最佳的搜索体验。而不是手动建模这些相关性特征，一个由人工智能驱动的搜索引擎可以利用机器学习自动生成和权衡这些特征。
- en: The goal of this chapter was to give you the knowledge and tools you’ll need
    in the coming chapters to affect relevance ranking as we begin integrating more
    automated machine learning techniques. We’ll begin applying this in our next chapter
    on crowdsourced relevance.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是为你提供在接下来的章节中需要的知识和工具，以便在我们开始集成更多自动化机器学习技术时影响相关性排名。我们将在下一章关于众包相关性中开始应用这些技术。
- en: Summary
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: We can represent queries and documents as dense or sparse numerical vectors
    and assign documents a relevance rank based on a vector similarity calculation
    (such as cosine similarity).
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将查询和文档表示为密集或稀疏的数值向量，并根据向量相似度计算（如余弦相似度）为文档分配相关性排名。
- en: Using TF-IDF or the BM25 similarity calculations (also based upon TF-IDF) for
    our text similarity scores provides a more meaningful measure of feature (keyword)
    importance in our queries and documents, enabling improved text ranking over just
    looking at term matches alone.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用TF-IDF或基于TF-IDF的BM25相似度计算（同样基于TF-IDF）来计算我们的文本相似度得分，为我们查询和文档中的特征（关键词）重要性提供了一个更有意义的度量，使得仅通过查看术语匹配就能实现文本排名的改进。
- en: Text similarity scoring is one of many kinds of functions we can invoke as a
    feature within our queries for relevance ranking. We can inject functions within
    our queries, along with keyword matching and scoring, as each keyword phrase is
    effectively just a ranking function.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本相似度评分是我们可以在查询中调用的许多函数之一，用于相关性排名。我们可以在查询中注入函数，包括关键词匹配和评分，因为每个关键词短语实际上就是一个排名函数。
- en: Treating “filtering” and “scoring” as separate concerns provides better control
    when specifying our own ranking functions.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将“过滤”和“评分”视为两个独立的问题，在指定我们自己的排名函数时提供了更好的控制。
- en: To optimize relevance, we need to both create domain-specific relevance functions
    and use user-specific features instead of relying just on keyword matching and
    ranking.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了优化相关性，我们需要同时创建特定领域的相关性函数和使用特定于用户的特征，而不是仅仅依赖于关键词匹配和排名。
