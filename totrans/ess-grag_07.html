<html><head></head><body>
<div id="sbo-rt-content"><div class="readable-text" id="p1">
<h1 class="readable-text-h1"><span class="chapter-title-numbering"><span class="num-string">8</span> </span> <span class="chapter-title-text">RAG application evaluation</span></h1>
</div>
<div class="introduction-summary">
<h3 class="introduction-header">This chapter covers</h3>
<ul>
<li class="readable-text" id="p2">Benchmarking RAG applications and agent capabilities</li>
<li class="readable-text" id="p3">Designing evaluation datasets</li>
<li class="readable-text" id="p4">Applying RAGAS metrics: recall, faithfulness, correctness</li>
</ul>
</div>
<div class="readable-text" id="p5">
<p>In this chapter, you will explore the importance of evaluating your RAG application performance using carefully constructed benchmark questions. As your RAG pipeline grows more sophisticated and complex, it becomes essential to ensure that your agent’s answers remain both accurate and coherent across a wide range of queries. A benchmark evaluation provides the system needed to measure the agent’s capabilities while also helping to clearly define and scope the agent. </p>
</div>
<div class="readable-text intended-text" id="p6">
<p>Evaluating RAG applications involves multiple approaches, each addressing different steps of the application, as shown in figure 8.1, which illustrates a high-level overview of a pipeline for a question-answering system powered by an LLM with retrieval capabilities. It begins with the user posing a question to the system. The LLM then identifies the most suitable retrieval tool to fetch the necessary information. This step is critical and can be evaluated for the accuracy of the tool selection process.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p7">
<img alt="figure" height="459" src="../Images/8-1.png" width="982"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.1</span> Evaluating different steps of a RAG pipeline</h5>
</div>
<div class="readable-text" id="p8">
<p>Throughout this book, you have implemented various retrieval tool designs, starting with vector search and progressing to more structured approaches like text2cypher and Cypher templates. Each retrieval method serves different needs:</p>
</div>
<ul>
<li class="readable-text" id="p9"> Vector search efficiently retrieves semantically relevant documents. </li>
<li class="readable-text" id="p10"> Cypher templates allow precise, structured queries to databases. </li>
<li class="readable-text" id="p11"> Text2cypher allows dynamic and flexible querying, benefiting from the expressive power of graph-based retrieval. </li>
</ul>
<div class="readable-text" id="p12">
<p>Evaluating which tool the LLM selects and how well it matches the query’s needs is crucial for optimizing retrieval performance.</p>
</div>
<div class="readable-text intended-text" id="p13">
<p>Once the appropriate tool is chosen, it retrieves relevant context or data from a knowledge base. The relevance of this retrieved context to the user’s question is another key evaluation point. A well-chosen retrieval method should ensure that the fetched context is both accurate and sufficient for answering the query.</p>
</div>
<div class="readable-text intended-text" id="p14">
<p>Using the retrieved context, the LLM generates an answer, which is then presented to the user. At this stage, we can assess not only the coherence and accuracy of the generated response but also the model’s ability to understand and integrate the provided context effectively. A particularly important evaluation criterion is whether the LLM produces the correct answer when given the correct context. This allows us to measure the model’s reasoning and synthesis capabilities separately from retrieval performance.</p>
</div>
<div class="readable-text intended-text" id="p15">
<p>Additionally, the entire pipeline can be evaluated holistically to measure its effectiveness in providing accurate and contextually relevant answers to user queries. By analyzing failures at different stages—tool selection, retrieval relevance, and final response generation—we can iteratively improve both the retrieval mechanisms and the LLM’s ability to utilize retrieved information.</p>
</div>
<div class="readable-text intended-text" id="p16">
<p>Say you are responsible for evaluating the performance of the LLM agent implemented in chapter 5. To gain deeper insight into its effectiveness, you will use the RAGAS Python library to design and conduct a benchmark analysis. But first, you need to design the benchmark dataset. In the remainder of this chapter, we’ll move from concepts to code and walk through the implementation step by step. To follow along, you’ll need access to a running Neo4j instance. This can be a local installation or a cloud-hosted instance. In the implementation of this chapter, we use what we call the “Movies dataset.” See the appendix for more information on the dataset and various ways to load it. You can follow the implementation directly in the accompanying Jupyter notebook available here: <a href="https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch08.ipynb">https://github.com/tomasonjo/kg-rag/blob/main/notebooks/ch08.ipynb</a>. </p>
</div>
<div class="readable-text intended-text" id="p17">
<p>Let’s dive in. </p>
</div>
<div class="readable-text" id="p18">
<h2 class="readable-text-h2"><span class="num-string">8.1</span> Designing the benchmark dataset</h2>
</div>
<div class="readable-text" id="p19">
<p>Creating a benchmark dataset requires designing input queries that test various aspects of the system’s decision making and response generation. Since each step in the RAG pipeline plays a vital role, the dataset should include diverse questions that challenge different components: </p>
</div>
<ul>
<li class="readable-text" id="p20"> <em>Tool selection evaluation</em> —Ssome queries should evaluate whether the system selects the correct retrieval method, ensuring it identifies the most relevant source of information. </li>
<li class="readable-text" id="p21"> <em>Entity and value mapping</em>—Other queries might focus on testing specific tasks, such as mapping entities or values from user input to the corresponding entries in a database. </li>
<li class="readable-text" id="p22"> <em>Multistep retrieval scenarios</em> —Some agents have the ability to execute multiple retrieval steps, where the initially retrieved data serves as input for a second retrieval step. The benchmark should include cases where the system needs to refine or expand upon the first retrieval to fully answer the query. These cases are particularly important for answering complex questions that depend on dynamically chaining multiple queries. </li>
<li class="readable-text" id="p23"> <em>Edge cases and functional coverage</em> —To fully understand system performance, the benchmark must cover all functionalities and known edge cases. This includes handling ambiguous queries, long-tail concepts, and scenarios where multiple retrieval methods might be applicable. </li>
<li class="readable-text" id="p24"> <em>Conversational usability</em> —Additionally, it may be useful to evaluate the agent’s ability to handle greetings, clarify ambiguous queries, and effectively communicate its capabilities to ensure a smooth and user-friendly experience. </li>
</ul>
<div class="readable-text" id="p25">
<p>By systematically benchmarking these aspects, we gain a clearer understanding of how well the agent performs under different conditions. This allows for targeted improvements, ensuring robustness and reliability in real-world deployments.</p>
</div>
<div class="readable-text" id="p26">
<h3 class="readable-text-h3"><span class="num-string">8.1.1</span> Coming up with test examples</h3>
</div>
<div class="readable-text" id="p27">
<p>To evaluate the system comprehensively, you need well-defined end-to-end test examples. Each example consists of a question and its corresponding ground truth response, as shown in figure 8.2, ensuring that the system’s output can be reliably assessed. <span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p28">
<img alt="figure" height="224" src="../Images/8-2.png" width="722"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.2</span> Benchmark test example</h5>
</div>
<div class="readable-text" id="p29">
<p>Instead of providing a static string as the expected answer, we can use Cypher queries to define the ground truth dynamically. Since we are dealing with a graph database, this approach offers a significant advantage: even if the underlying data changes, the benchmark remains valid. This ensures that test cases, as shown in figure 8.3, remain accurate over time without requiring constant updates.<span class="aframe-location"/></p>
</div>
<div class="browsable-container figure-container" id="p30">
<img alt="figure" height="309" src="../Images/8-3.png" width="1012"/>
<h5 class="figure-container-h5"><span class="num-string">Figure 8.3</span> Benchmark test example with a Cypher statement as ground truth</h5>
</div>
<div class="readable-text" id="p31">
<p>When designing a benchmark dataset, you should include diverse examples to evaluate different aspects of the agent’s performance. For instance, you can evaluate how the agent responds to greetings like “Hello,” provides guidance to the user, or handles irrelevant queries, as demonstrated in table 8.1.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p32">
<h5 class="browsable-container-h5"><span class="num-string">Table 8.1</span> Benchmark examples that test simple greetings and irrelevant questions</h5>
<table>
<thead>
<tr>
<th>
<div>
         Question 
       </div></th>
<th>
<div>
         Cypher 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Hello <br/></td>
<td> <code>RETURN “greeting and reminder it can only answer questions related to movies.”</code> <br/></td>
</tr>
<tr>
<td>  What can you do? <br/></td>
<td> <code>RETURN “answer questions related to movies and their cast.”</code> <br/></td>
</tr>
<tr>
<td>  What is the weather like in Spain? <br/></td>
<td> <code>RETURN “irrelevant question as we can answer questions related to movies and their cast only.”</code> <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text intended-text" id="p33">
<p>This table provides examples of how the agent responds to simple greetings, user guidance requests, and irrelevant queries. It shows how you can use a simple <code>RETURN</code> Cypher statement to define static answers that don’t need to look for information in the database. For example, when greeted with “Hello,” the agent replies with a greeting and a reminder of its scope. If asked what it can do, it clarifies that it answers questions about movies and their casts. For unrelated queries, like about the weather, the agent simply states that it only handles movie-related questions. </p>
</div>
<div class="readable-text intended-text" id="p34">
<p>Next, we can define a set of questions to evaluate both tool usage and the LLM’s ability to generate accurate answers using those tools. The examples are shown in table 8.2.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p35">
<h5 class="browsable-container-h5"><span class="num-string">Table 8.2</span> Benchmark examples that test tools usage and value mapping</h5>
<table>
<thead>
<tr>
<th>
<div>
         Question 
       </div></th>
<th>
<div>
         Cypher 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Who acted in Top Gun? <br/></td>
<td> <code>RETURN "MATCH (p:Person)-[:ACTED_IN]→(m:Movie {title: "Top Gun"}) RETURN p.name"</code> <br/></td>
</tr>
<tr>
<td>  Who acted in top gun? <br/></td>
<td> <code>RETURN "MATCH (p:Person)-[:ACTED_IN]→(m:Movie {title: "Top Gun"}) RETURN p.name"</code> <br/></td>
</tr>
<tr>
<td>  In which movies did Tom Hanks act in? <br/></td>
<td> <code>MATCH (p:Person {name: "Tom Hanks"})-[:ACTED_IN]→(m:Movie) RETURN m.title</code> <br/></td>
</tr>
<tr>
<td>  In which movies did tom Hanks act in? <br/></td>
<td> <code>MATCH (p:Person {name: "Tom Hanks"})-[:ACTED_IN]→(m:Movie) RETURN m.title</code> <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p36">
<p>The examples in table 8.2 demonstrate cases where the LLM needs to retrieve relevant data from the database using available tools. Here, the LLM should utilize two key tools: one for finding movies by actor and another for finding actors by movie, ensuring fast and reliable responses.</p>
</div>
<div class="readable-text intended-text" id="p37">
<p>Additionally, these examples allow us to evaluate how well the agent maps user input to database values. For well-known movies and actors, the LLM often generates correct queries out of the box based on its pretraining. However, for lesser-known or private datasets, a dedicated mapping system is essential for accurate entity resolution. Implementing such a system ensures that user inputs are correctly linked to database entries, improving both accuracy and reliability.</p>
</div>
<div class="readable-text intended-text" id="p38">
<p>You should also include some examples where the LLM will need to use the text2cypher tool, as shown in table 8.3.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p39">
<h5 class="browsable-container-h5"><span class="num-string">Table 8.3</span> Benchmark examples that test queries involving aggregations and filtering</h5>
<table>
<thead>
<tr>
<th>
<div>
         Question 
       </div></th>
<th>
<div>
         Cypher 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Who acted in the most movies? <br/></td>
<td> <code>MATCH (p:Person)-[:ACTED_IN]→(m:Movie) RETURN p.name, COUNT(m) AS movieCount ORDER BY movieCount DESC LIMIT 1</code> <br/></td>
</tr>
<tr>
<td>  List people born before 1940. <br/></td>
<td> <code>MATCH (p:Person) WHERE p.born &lt; 1940 RETURN p.name</code> <br/></td>
</tr>
<tr>
<td>  Who was born in 1965 and has directed a movie? <br/></td>
<td> <code>MATCH (p:Person)-[:DIRECTED]→(m:Movie) WHERE p.born = 1965 RETURN p.name</code> <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p40">
<p>Table 8.3 includes queries that involve aggregations, filtering, and relationships, such as finding the actor with the most movie roles, listing people born before a certain year, and identifying directors born in a specific year. Since no dedicated tool is implemented to handle these queries, the LLM must rely on text2cypher to construct the appropriate Cypher statements based on the provided graph schema.</p>
</div>
<div class="readable-text intended-text" id="p41">
<p>You should also test edge cases, such as queries where relevant data is missing but still within the domain, as demonstrated in table 8.4.</p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p42">
<h5 class="browsable-container-h5"><span class="num-string">Table 8.4</span> Benchmark examples that test questions where data is missing</h5>
<table>
<thead>
<tr>
<th>
<div>
         Question 
       </div></th>
<th>
<div>
         Cypher 
       </div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  Which movie has the most Oscars? <br/></td>
<td> <code>RETURN “This information is missing”</code> <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p43">
<p>The benchmark will be very dependent on the functionalities of your agent. The specific capabilities, such as retrieval strategies, reasoning methods, and structured output handling, will influence the benchmark’s effectiveness in assessing performance. When designing a benchmark, it is crucial to ensure comprehensive coverage of your agent’s functionalities. By incorporating a variety of examples, you can effectively test how well your agent handles different challenges.</p>
</div>
<div class="readable-text intended-text" id="p44">
<p>The benchmark has 17 examples in total, with some not shown here. You can now evaluate them. </p>
</div>
<div class="readable-text" id="p45">
<h2 class="readable-text-h2"><span class="num-string">8.2</span> Evaluation</h2>
</div>
<div class="readable-text" id="p46">
<p>To assess the performance of your benchmark, you will use RAGAS, a framework designed for evaluating RAG systems. As mentioned, the evaluation focuses on three key metrics, discussed next.</p>
</div>
<div class="readable-text" id="p47">
<h3 class="readable-text-h3"><span class="num-string">8.2.1</span> Context recall</h3>
</div>
<div class="readable-text" id="p48">
<p>Context recall measures how many relevant pieces of information were successfully retrieved using the prompt in “Context recall evaluation.” A high score indicates that the retrieval system effectively captures all necessary context needed to answer the query. </p>
</div>
<div class="readable-text prompt prompt-header" id="p49">
<p><b>Context recall evaluation</b></p>
</div>
<div class="readable-text prompt" id="p50">
<p>Goal: Given a context and an answer, analyze each sentence in the answer and classify whether the sentence can be attributed to the given context or not. Use only 'Yes' (1) or 'No' (0) as a binary classification. Output JSON with reasoning.</p>
</div>
<div class="readable-text" id="p51">
<p>The prompt in “Context recall evaluation” ensures that every sentence in the generated answer is explicitly supported by the retrieved context. By doing so, it helps evaluate how effectively the retrieval system captures relevant information.</p>
</div>
<div class="readable-text intended-text" id="p52">
<p>Next, the faithfulness assessment ensures that the generated response remains factually aligned with the retrieved content. </p>
</div>
<div class="readable-text" id="p53">
<h3 class="readable-text-h3"><span class="num-string">8.2.2</span> Faithfulness</h3>
</div>
<div class="readable-text" id="p54">
<p>Faithfulness evaluates whether the generated response remains factually consistent with the retrieved context. A response is considered faithful if all its claims can be directly supported by the provided documents, minimizing the risk of hallucination. Faithfulness is assessed using a two-step process. In the first step, it decomposes the answer into atomic statements using the prompt in “Faithfulness statement breakdown,” ensuring that each unit of information is clear and self-contained, making verification easier. </p>
</div>
<div class="readable-text prompt prompt-header" id="p55">
<p><b>Faithfulness statement breakdown</b></p>
</div>
<div class="readable-text prompt" id="p56">
<p>Goal: Given a question and an answer, analyze the complexity of each sentence in the answer. Break down each sentence into one or more fully understandable statements. Ensure that no pronouns are used in any statement. Format the outputs in JSON.</p>
</div>
<div class="readable-text" id="p57">
<p>Once the statements are generated, it evaluates their faithfulness using the prompt in “Faithfulness evaluation.”</p>
</div>
<div class="readable-text prompt prompt-header" id="p58">
<p><b>Faithfulness evaluation</b></p>
</div>
<div class="readable-text prompt" id="p59">
<p>Goal: Your task is to judge the faithfulness of a series of statements based on a given context. For each statement, return a verdict as 1 if the statement can be directly inferred from the context or 0 if the statement cannot be directly inferred from the context.</p>
</div>
<div class="readable-text" id="p60">
<p>The prompt in “Faithfulness evaluation” checks whether the statements in the generated response are factually grounded in the retrieved context. It ensures that the model does not introduce unsupported claims.</p>
</div>
<div class="readable-text intended-text" id="p61">
<p>Finally, we evaluate answer correctness by comparing the generated response with the ground truth. </p>
</div>
<div class="readable-text" id="p62">
<h3 class="readable-text-h3"><span class="num-string">8.2.3</span> Answer correctness</h3>
</div>
<div class="readable-text" id="p63">
<p>Answer correctness assesses how accurately and completely the response addresses the user’s query. It considers both factual accuracy and relevance to ensure the response aligns with the intent of the question. Answer correctness uses the same process as faithfulness to generate statements and then evaluates them using the prompt in “Answer correctness evaluation.” </p>
</div>
<div class="readable-text prompt prompt-header" id="p64">
<p><b>Answer correctness evaluation</b></p>
</div>
<div class="readable-text prompt" id="p65">
<p>Goal: Given a ground truth and an answer statement, analyze each statement and classify it into one of the following categories:</p>
</div>
<div class="readable-text prompt" id="p66">
<p>TP (true positive): Statements present in the answer that are also directly supported by one or more statements in the ground truth. FP (false positive): Statements present in the answer but not directly supported by any statement in the ground truth. FN (false negative): Statements found in the ground truth but not present in the answer.</p>
</div>
<div class="readable-text prompt" id="p67">
<p>Each statement can only belong to one of these categories. Provide a reason for each classification.</p>
</div>
<div class="readable-text" id="p68">
<p>The prompt in “Answer correctness evaluation” ensures that the response is both factually correct and aligned with the expected answer by systematically comparing the generated statements with the ground truth.</p>
</div>
<div class="readable-text intended-text" id="p69">
<p>By analyzing these metrics, you can determine how well the system retrieves relevant data, maintains factual consistency, and generates correct responses. This evaluation will help identify potential weaknesses, such as missing context, inconsistencies, or inaccurate answers, allowing for iterative refinement and improved performance. </p>
</div>
<div class="readable-text" id="p70">
<h3 class="readable-text-h3"><span class="num-string">8.2.4</span> Loading the dataset</h3>
</div>
<div class="readable-text" id="p71">
<p>The benchmark dataset is provided as a CSV file in the accompanying repository, making it easy to load and use, as demonstrated in the following listing. </p>
</div>
<div class="browsable-container listing-container" id="p72">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 8.1</span> Loading benchmark dataset from CSV</h5>
<div class="code-area-container">
<pre class="code-area">test_data = pd.read_csv("../data/benchmark_data.csv", delimiter=";")</pre>
</div>
</div>
<div class="readable-text" id="p73">
<h3 class="readable-text-h3"><span class="num-string">8.2.5</span> Running evaluation</h3>
</div>
<div class="readable-text" id="p74">
<p>To evaluate the system’s performance, you will generate answers for the benchmark dataset and compare them against the expected ground truth responses. First, you need to obtain the ground truth by executing the corresponding Cypher statements and generating answers using the agent, as shown in listing 8.2. Additionally, you must record latency and retrieved contexts to analyze the system’s efficiency and relevance. </p>
</div>
<div class="browsable-container listing-container" id="p75">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 8.2</span> Generating answers and ground truth responses</h5>
<div class="code-area-container">
<pre class="code-area">answers = []
ground_truths = []
latencies = []
contexts = []

for i, row in tqdm(test_data.iterrows(), total=len(test_data), desc="Processing rows"):
    ground_truth, _, _ = neo4j_driver.execute_query(row["cypher"])  <span class="aframe-location"/> #1
    ground_truths.append([str(el.data()) for el in ground_truth])
    start = datetime.now()
    try:
        answer, context = get_answer(row["question"])  <span class="aframe-location"/> #2
        context = [el['content'] for el in context]
    except Exception:
        answer, context = None, []
    latencies.append((datetime.now() - start).total_seconds())  <span class="aframe-location"/> #3
    answers.append(answer)
    contexts.append(context)
 #4
test_data['ground_truth'] = [str(el) for el in ground_truths]
test_data['answer'] = answers
test_data['latency'] = latencies
test_data['retrieved_contexts'] = contexts</pre>
<div class="code-annotations-overlay-container">
     #1 The provided Cypher statement returns the ground truth.
     <br/>#2 Executes the agent to generate a response to the question
     <br/>#3 Calculates the latency
     <br/>#4 Stores the results back to the dataframe
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p76">
<p>Now that we have collected all the necessary input data, including generated answers and ground truth responses, we can proceed with the evaluation.</p>
</div>
<div class="browsable-container listing-container" id="p77">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 8.3</span> Evaluating the generated answer and retrieved context</h5>
<div class="code-area-container">
<pre class="code-area">dataset = Dataset.from_pandas(test_data.fillna("I don't know"))  <span class="aframe-location"/> #1

result = evaluate(<span class="aframe-location"/> #2
    dataset,

    metrics=[<span class="aframe-location"/> #3
        answer_correctness,
        context_recall,
        faithfulness,
    ],
)</pre>
<div class="code-annotations-overlay-container">
     #1 Changes missing response answers to “I don’t know”
     <br/>#2 Runs the evaluation using RAGAS framework
     <br/>#3 Relevant metrics
     <br/>
</div>
</div>
</div>
<div class="readable-text" id="p78">
<p>This code in listing 8.3 runs the evaluation using the RAGAS framework, which requires non-null values, so you fill in missing responses with “I don’t know.” It then evaluates the generated answers based on answer correctness, context recall, and faithfulness.</p>
</div>
<div class="readable-text intended-text" id="p79">
<p>The final step is to analyze the results to understand the system’s performance. </p>
</div>
<div class="readable-text" id="p80">
<h3 class="readable-text-h3"><span class="num-string">8.2.6</span> Observations</h3>
</div>
<div class="readable-text" id="p81">
<p>You can review the overall summary in 8.5 to get an overview of the agent’s performance. </p>
</div>
<div class="browsable-container browsable-table-container framemaker-table-container" id="p82">
<h5 class="browsable-container-h5"><span class="num-string">Table 8.5</span> Benchmark summary</h5>
<table>
<thead>
<tr>
<th>
<div>
<strong><code>answer_correctness</code></strong>
</div></th>
<th>
<div>
<strong><code>context_recall</code></strong>
</div></th>
<th>
<div>
<strong><code>faithfulness</code></strong>
</div></th>
</tr>
</thead>
<tbody>
<tr>
<td>  0.7774 <br/></td>
<td>  0.7941 <br/></td>
<td>  0.9657 <br/></td>
</tr>
</tbody>
</table>
</div>
<div class="readable-text" id="p83">
<p>The results in table 8.5 provide an overall assessment of the system’s performance based on three key metrics. With an answer correctness score of 0.7774, the model gets things right most of the time but still misses the mark in about a quarter of cases. The context recall score of 0.7941 shows that while the retrieval system is doing a decent job, it occasionally fails to pull in all the necessary information, which could be holding back the overall accuracy. On the bright side, the faithfulness score of 0.9657 is excellent, meaning the model rarely makes things up and stays true to the retrieved context.</p>
</div>
<div class="readable-text intended-text" id="p84">
<p>Overall, the high faithfulness score shows that the model does not introduce incorrect information, but the answer correctness and context recall lower scores suggest that improving retrieval mechanisms could lead to better response accuracy. Enhancing retrieval coverage and refining how the LLM formulates answers could improve overall performance. These insights can guide further optimizations, such as refining the retrieval system, improving query reformulation, or implementing better entity mapping for ambiguous queries.</p>
</div>
<div class="readable-text intended-text" id="p85">
<p>You can further analyze each response to identify areas for improvement by using the code in the following listing.</p>
</div>
<div class="browsable-container listing-container" id="p86">
<h5 class="listing-container-h5 browsable-container-h5"><span class="num-string">Listing 8.4</span> Extracting metrics and adding them to the dataframe</h5>
<div class="code-area-container">
<pre class="code-area">for key in ["answer_correctness", "context_recall", "faithfulness"]:
    test_data[key] = [el[key] for el in result.scores]
test_data</pre>
</div>
</div>
<div class="readable-text" id="p87">
<p>The full response is too large to include in the book, but there are several key takeaways from analyzing individual examples. One noticeable pattern is that latency is significantly lower for queries that don’t require text2cypher, as avoiding an additional LLM call speeds up the response. Another observation is that since we rely on an LLM as a judge, some scores may seem inconsistent, such as in the Hello example.</p>
</div>
<div class="readable-text intended-text" id="p88">
<p>One clear limitation is that the system fails to answer the question “Who has the longest name among all actors?” This happens because the model isn’t equipped to generate the appropriate Cypher query. To address this, you could add a few-shot example to guide text2cypher or implement a dedicated tool specifically for handling such queries.</p>
</div>
<div class="readable-text intended-text" id="p89">
<p>This analysis demonstrates how a benchmark helps us evaluate results and make informed decisions about future improvements. As the system evolves, the benchmark dataset should continue to grow, ensuring ongoing refinement and better performance.</p>
</div>
<div class="readable-text intended-text" id="p90">
<p>Throughout this book, you have explored how to build knowledge graph RAG systems. You’ve learned how different retrieval strategies enable your agent to fetch relevant information, whether from structured or unstructured data. Understanding when to use methods like vector search or Cypher templates is key to designing an efficient and accurate system.</p>
</div>
<div class="readable-text intended-text" id="p91">
<p>By implementing and refining retrieval strategies, you now have the foundation to build a powerful knowledge graph–based agent. You’ve seen how structured queries can enhance precision and how retrieval choices impact answer quality, and you’ve learned how to systematically evaluate performance. This chapter introduced benchmarking as a way to measure accuracy, recall, and faithfulness, giving you the tools to continuously improve your agent. </p>
</div>
<div class="readable-text" id="p92">
<h2 class="readable-text-h2"><span class="num-string">8.3</span> Next steps</h2>
</div>
<div class="readable-text" id="p93">
<p>You’re now equipped with the knowledge and tools to build and refine intelligent retrieval systems powered by knowledge graphs. Whether you’re creating a sophisticated question-answering agent or tailoring retrieval pipelines for specific domains, you have the foundation to design robust, high-performing, knowledge-driven AI systems. </p>
</div>
<div class="readable-text intended-text" id="p94">
<p>LLMs are rapidly improving, not only in their ability to understand and generate language but also in how effectively they can use external tools for data retrieval, transformation, and manipulation. As these models become more capable, they will be able to perform increasingly complex tasks with minimal prompting. However, their effectiveness still depends on the quality, design, and integration of the tools you provide. It’s your job to implement those tools thoughtfully and efficiently, ensuring they are well suited to your system’s goals and constraints.</p>
</div>
<div class="readable-text intended-text" id="p95">
<p>With this foundation, you can now begin building your own agentic GraphRAG systems. You are equipped to work with unstructured data in a variety of ways: you can embed text directly to enable fast similarity-based retrieval or go a step further and extract structured information—such as entities, relationships, and events—to populate a knowledge graph that supports more precise, semantic, and multihop queries. By combining these approaches, you can build retrieval systems that not only find relevant information but truly understand it, paving the way for powerful, context-aware AI applications. </p>
</div>
<div class="readable-text" id="p96">
<h2 class="readable-text-h2">Summary</h2>
</div>
<ul>
<li class="readable-text" id="p97"> Evaluating a RAG pipeline is crucial for ensuring accurate and coherent answers. A benchmark evaluation helps measure performance and define the agent’s capabilities. </li>
<li class="readable-text" id="p98"> The evaluation process involves assessing various stages: retrieval tool selection, context retrieval relevance, answer generation quality, and overall system effectiveness. </li>
<li class="readable-text" id="p99"> A well-structured benchmark dataset should include diverse queries that test retrieval accuracy, entity mapping, the handling of greetings, irrelevant queries, and various Cypher-based database lookups. </li>
<li class="readable-text" id="p100"> Instead of static expected answers, using Cypher queries as ground truth ensures the benchmark remains valid even if the underlying data changes. </li>
<li class="readable-text" id="p101"> Context recall measures how well the system retrieves relevant information. </li>
<li class="readable-text" id="p102"> Faithfulness evaluates if the generated answer is factually consistent with the retrieved content. </li>
<li class="readable-text" id="p103"> Answer correctness assesses whether the response fully and accurately addresses the query. </li>
</ul>
</div></body></html>