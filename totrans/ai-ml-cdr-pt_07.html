<html><head></head><body><section data-pdf-bookmark="Chapter 6. Making Sentiment Programmable by Using Embeddings" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch06_clean_making_sentiment_programmable_by_using_embeddings_1748752380728888">
      <h1><span class="label">Chapter 6. </span>Making Sentiment Programmable <span class="keep-together">by Using Embeddings</span></h1>
      <p>In <a data-type="xref" href="ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759">Chapter 5</a>, you saw how to take words<a contenteditable="false" data-primary="sentiment" data-type="indexterm" id="ch6sentall"/><a contenteditable="false" data-primary="meaning of words" data-secondary="about" data-type="indexterm" id="id1150"/><a contenteditable="false" data-primary="embeddings" data-secondary="about" data-type="indexterm" id="id1151"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="embeddings" data-tertiary="about" data-type="indexterm" id="id1152"/><a contenteditable="false" data-primary="encoding language into numbers" data-secondary="about embeddings" data-type="indexterm" id="id1153"/><a contenteditable="false" data-primary="words" data-secondary="about sentiment" data-type="indexterm" id="id1154"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="sentiment" data-type="indexterm" id="id1155"/> and encode them into tokens. Then, you saw how to encode sentences full of words into sequences full of tokens, padding or truncating them as appropriate to end up with a well-shaped set of data that you can use to train a neural network. However, in none of that was there any type of modeling of the <em>meaning</em> of a word. And while it’s true that there’s no absolute numeric encoding that could encapsulate meaning, there are relative ones. </p>
      <p>In this chapter, you’ll learn about techniques to encapsulate meaning, and in particular, the concept of <em>embeddings</em>, in which vectors in high-dimensional space are created to represent words. The directions of these vectors can be learned over time, based on the use of the words in the corpus. Then, when you’re given a sentence, you can investigate the directions of the word vectors, sum them up, and from the overall direction of the summation, establish the sentiment of the sentence as a product of its words. Also, related to this, as the model scans the sentences, the positioning of the words in the sentence can also help train an appropriate embedding.</p>
      <p>In this chapter, we’ll also explore how that works. Using the News Headlines Dataset for Sarcasm Detection dataset from <a data-type="xref" href="ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759">Chapter 5</a>, you’ll build embeddings to help a model detect sarcasm in a sentence. You’ll also work with some cool visualization tools that help you understand how words in a corpus get mapped to vectors so you can see which words determine the overall classification.</p>
      <section data-pdf-bookmark="Establishing Meaning from Words" data-type="sect1"><div class="sect1" id="ch06_clean_establishing_meaning_from_words_1748752380729137">
        <h1>Establishing Meaning from Words</h1>
        <p>Before we get into the higher-dimensional <a contenteditable="false" data-primary="embeddings" data-secondary="meaning from words" data-type="indexterm" id="ch6mean"/><a contenteditable="false" data-primary="words" data-secondary="meaning from" data-type="indexterm" id="ch6mean2"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="words" data-tertiary="meaning from" data-type="indexterm" id="ch6mean3"/><a contenteditable="false" data-primary="meaning of words" data-type="indexterm" id="ch6mean4"/>vectors for embeddings, let’s use some simple examples to try to visualize how meaning can be derived from numerics. <span class="keep-together">Consider</span> this: using the sarcasm dataset from <a data-type="xref" href="ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759">Chapter 5</a>, what would happen if you encoded all of the words that make up sarcastic headlines with positive numbers and those that make up realistic headlines with negative numbers?</p>
        <section data-pdf-bookmark="A Simple Example: Positives and Negatives" data-type="sect2"><div class="sect2" id="ch06_clean_a_simple_example_positives_and_negatives_1748752380729204">
          <h2>A Simple Example: Positives and Negatives</h2>
          <p>Take, for example, this sarcastic headline from the dataset:<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="words" data-tertiary="positives and negatives" data-type="indexterm" id="id1156"/><a contenteditable="false" data-primary="meaning of words" data-secondary="positives and negatives" data-type="indexterm" id="id1157"/><a contenteditable="false" data-primary="words" data-secondary="meaning from" data-tertiary="positives and negatives" data-type="indexterm" id="id1158"/><a contenteditable="false" data-primary="sarcasm detector" data-secondary="positive and negative values of words" data-type="indexterm" id="id1159"/></p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">christian</code> <code class="n">bale</code> <code class="n">given</code> <code class="n">neutered</code> <code class="n">male</code> <code class="n">statuette</code> <code class="n">named</code> <code class="n">oscar</code></pre>
          <p>Assuming that all words in our vocabulary start with a value of 0, we could add 1 to the value for each of the words in this sentence, and we would end up with this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="p">{</code> <code class="s2">"christian"</code> <code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"bale"</code> <code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"given"</code> <code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"neutered"</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"male"</code> <code class="p">:</code> <code class="mi">1</code><code class="p">,</code> 
  <code class="s2">"statuette"</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"named"</code> <code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"oscar"</code><code class="p">:</code> <code class="mi">1</code><code class="p">}</code></pre>
          <div data-type="note" epub:type="note"><h6>Note</h6>
            <p>This isn’t the same as the <em>tokenization</em> of words that you did in the last chapter. You could consider replacing each word (e.g., <em>christian</em>) with the token representing it that is encoded from the corpus, but I’ll leave the words in for now to make the code easier to read.</p></div>
          <p>Then, in the next step, consider an ordinary headline (not a sarcastic one), like this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">gareth</code> <code class="n">bale</code> <code class="n">scores</code> <code class="n">wonder</code> <code class="n">goal</code> <code class="n">against</code> <code class="n">germany</code></pre>
          <p>Because this is a different sentiment, we could instead subtract 1 from the current value of each word, so our value set would look like this:</p>
<pre data-code-language="python" data-type="programlisting">
<code class="p">{</code> <code class="s2">"christian"</code> <code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"bale"</code> <code class="p">:</code> <code class="mi">0</code><code class="p">,</code> <code class="s2">"given"</code> <code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"neutered"</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"male"</code> <code class="p">:</code> <code class="mi">1</code><code class="p">,</code>
  <code class="s2">"statuette"</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"named"</code> <code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"oscar"</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s2">"gareth"</code> <code class="p">:</code> <code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="s2">"scores"</code><code class="p">:</code> <code class="err">–</code><code class="mi">1</code><code class="p">,</code>
  <code class="s2">"wonder"</code> <code class="p">:</code> <code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="s2">"goal"</code> <code class="p">:</code> <code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="s2">"against"</code> <code class="p">:</code> <code class="err">–</code><code class="mi">1</code><code class="p">,</code> <code class="s2">"germany"</code> <code class="p">:</code> <code class="err">–</code><code class="mi">1</code><code class="p">}</code></pre>
          <p>Note that the sarcastic <code>bale</code> (from <code>christian bale</code>) has been offset by the nonsarcastic <code>bale</code> (from <code>gareth bale</code>), so its score ends up as 0. Repeat this process thousands of times and you’ll end up with a huge list of words from your corpus that are scored based on their usage.</p>
          <p>Now, imagine we want to establish the sentiment of this sentence:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">neutered</code> <code class="n">male</code> <code class="n">named</code> <code class="n">against</code> <code class="n">germany</code><code class="p">,</code> <code class="n">wins</code> <code class="n">statuette</code><code class="err">!</code></pre>
          <p>Using our existing value set, we could look at the scores of each word and add them up. We would get a score of 2, indicating (because it’s a positive number) that this is a sarcastic sentence.</p>
          <div data-type="note" epub:type="note"><h6>Note</h6>
            <p>For what it’s worth, the word <em>bale</em> is used five times in the Sarcasm dataset, twice in a normal headline and three times in a sarcastic one. So, in a model like this, the word <em>bale</em> would be scored –1 across the whole dataset.</p>
          </div>
        </div></section>
        <section data-pdf-bookmark="Going a Little Deeper: Vectors" data-type="sect2"><div class="sect2" id="ch06_clean_going_a_little_deeper_vectors_1748752380729258">
          <h2>Going a Little Deeper: Vectors</h2>
          <p>Hopefully, the previous example<a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="words" data-tertiary="vectors" data-type="indexterm" id="id1160"/><a contenteditable="false" data-primary="meaning of words" data-secondary="vectors" data-tertiary="about" data-type="indexterm" id="id1161"/><a contenteditable="false" data-primary="words" data-secondary="meaning from" data-tertiary="vectors" data-type="indexterm" id="id1162"/><a contenteditable="false" data-primary="vectors for word meanings" data-secondary="about" data-type="indexterm" id="id1163"/> has helped you understand the mental model of establishing some form of <em>relative</em> meaning for a word, through its association with other words in the same “direction.” In our case, while the computer doesn’t understand the meanings of individual words, it can move labeled words from a known sarcastic headline in one direction (by adding 1) and move labeled words from a known normal headline in another direction (by subtracting 1). This gives us a basic understanding of the meaning of the words, but it does lose some nuance.</p>
          <p>But what if we increased the dimensionality of the direction to try to capture some more information? For example, suppose we were to look at characters from the Jane Austen novel <em>Pride and Prejudice</em>, considering the dimensions of gender and nobility. We could plot the former on the <em>x</em>-axis and the latter on the <em>y</em>-axis, with the length of the vector denoting each character’s wealth (see <a data-type="xref" href="#ch06_clean_figure_1_1748752380714921">Figure 6-1</a>).</p>
          <figure><div class="figure" id="ch06_clean_figure_1_1748752380714921">
            <img alt="" src="assets/aiml_0601.png"/>
            <h6><span class="label">Figure 6-1. </span>Characters in <span class="roman">Pride and Prejudice</span> as vectors</h6>
          </div></figure>
          <p>From an inspection of the graph, you can derive a fair amount of information about each character. Three of them are male. Mr. Darcy is extremely wealthy, but his nobility isn’t clear (he’s called “Mister,” unlike the less wealthy but apparently more noble Sir William Lucas). The other “Mister,” Mr. Bennet, is clearly not nobility and is struggling financially. Elizabeth Bennet, his daughter, is similar to him but female. Lady Catherine, the other female character in our example, is noble and incredibly wealthy. The romance between Mr. Darcy and Elizabeth causes tension—with <em>prejudice</em> coming from the noble side of the vectors toward the less-noble.</p>
          <p>As this example shows, by considering multiple dimensions, we can begin to see real meaning in the words (which are character names here). Again, we’re not talking about concrete definitions but more about a <em>relative</em> meaning based on the axes and the relationship between the vector for one word and the other vectors.</p>
          <p>This leads us to the concept of an <em>embedding</em>, which is simply a vector representation of a word that is learned while training a neural network. We’ll explore that next.<a contenteditable="false" data-primary="" data-startref="ch6mean" data-type="indexterm" id="id1164"/><a contenteditable="false" data-primary="" data-startref="ch6mean2" data-type="indexterm" id="id1165"/><a contenteditable="false" data-primary="" data-startref="ch6mean3" data-type="indexterm" id="id1166"/><a contenteditable="false" data-primary="" data-startref="ch6mean4" data-type="indexterm" id="id1167"/></p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Embeddings in PyTorch" data-type="sect1"><div class="sect1" id="ch06_clean_embeddings_in_pytorch_1748752380729323">
        <h1>Embeddings in PyTorch</h1>
        <p>Much like you’ve seen with <code>Linear</code> and <code>Conv2D</code>, PyTorch implements<a contenteditable="false" data-primary="embeddings" data-type="indexterm" id="ch6emball"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="embeddings" data-type="indexterm" id="ch6emball2"/><a contenteditable="false" data-primary="meaning of words" data-secondary="embeddings" data-type="indexterm" id="ch6emball3"/><a contenteditable="false" data-primary="words" data-secondary="embeddings" data-type="indexterm" id="ch6emball4"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="embeddings" data-tertiary="about" data-type="indexterm" id="id1168"/><a contenteditable="false" data-primary="words" data-secondary="embeddings" data-tertiary="about" data-type="indexterm" id="id1169"/><a contenteditable="false" data-primary="meaning of words" data-secondary="embeddings" data-tertiary="about" data-type="indexterm" id="id1170"/><a contenteditable="false" data-primary="words" data-secondary="meaning from" data-tertiary="vectors" data-type="indexterm" id="id1171"/><a contenteditable="false" data-primary="vectors for word meanings" data-secondary="embeddings" data-type="indexterm" id="id1172"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="words" data-tertiary="vectors" data-type="indexterm" id="id1173"/><a contenteditable="false" data-primary="meaning of words" data-secondary="vectors" data-tertiary="embeddings" data-type="indexterm" id="id1174"/> embeddings by using a layer. This creates a lookup table that maps from an integer to an embedding table, the contents of which are the coefficients of the vector representing the word identified by that integer. So, in the <em>Pride and Prejudice</em> example from the previous section, the <em>x</em> and <em>y</em> coordinates would give us the embeddings for a particular character from the book. Of course, in a real NLP problem, we’ll use far more than two dimensions. Thus, the direction of a vector in the vector space could be seen as encoding the “meaning” of a word, and words with similar vectors (i.e., pointing in roughly the same direction) could be considered related to that word.</p>
        <p>The embedding layer will be initialized randomly—that is, the coordinates of the <a contenteditable="false" data-primary="backpropagation" data-secondary="embedding layer vectors" data-type="indexterm" id="id1175"/>vectors will be completely random to start with and will be learned during training by using backpropagation. When training is complete, the embeddings will roughly encode similarities between words, allowing us to identify words that are somewhat similar based on the direction of the vectors for those words.</p>
        <p>This is all quite abstract, so I think the best way to understand how to use embeddings is to roll up your sleeves and give them a try. Let’s start with a sarcasm detector using the Sarcasm dataset from <a data-type="xref" href="ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759">Chapter 5</a>.</p>
        <section data-pdf-bookmark="Building a Sarcasm Detector by Using Embeddings" data-type="sect2"><div class="sect2" id="ch06_clean_building_a_sarcasm_detector_by_using_embeddings_1748752380729390">
          <h2>Building a Sarcasm Detector by Using Embeddings</h2>
          <p>In <a data-type="xref" href="ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759">Chapter 5</a>, you loaded and did<a contenteditable="false" data-primary="embeddings" data-secondary="sarcasm detector" data-type="indexterm" id="ch6sar"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="embeddings" data-tertiary="sarcasm detector" data-type="indexterm" id="ch6sar2"/><a contenteditable="false" data-primary="meaning of words" data-secondary="embeddings" data-tertiary="sarcasm detector" data-type="indexterm" id="ch6sar3"/><a contenteditable="false" data-primary="words" data-secondary="embeddings" data-tertiary="sarcasm detector" data-type="indexterm" id="ch6sar4"/><a contenteditable="false" data-primary="sarcasm detector" data-secondary="embeddings" data-type="indexterm" id="ch6sar5"/><a contenteditable="false" data-primary="datasets" data-secondary="natural language processing" data-tertiary="News Headlines Dataset for Sarcasm Detection" data-type="indexterm" id="ch6sar6"/><a contenteditable="false" data-primary="sarcasm detector" data-secondary="sarcasm dataset" data-type="indexterm" id="ch6sar7"/> some preprocessing on a JSON dataset called the News Headlines Dataset for Sarcasm Detection (the sarcasm dataset, for short). By the time you were done, you had lists of training and testing data and labels: </p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">training_size</code> <code class="o">=</code> <code class="mi">28000</code>
<code class="n">training_sentences</code> <code class="o">=</code> <code class="n">sentences</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="n">training_size</code><code class="p">]</code>
<code class="n">testing_sentences</code> <code class="o">=</code> <code class="n">sentences</code><code class="p">[</code><code class="n">training_size</code><code class="p">:]</code>
<code class="n">training_labels</code> <code class="o">=</code> <code class="n">labels</code><code class="p">[</code><code class="mi">0</code><code class="p">:</code><code class="n">training_size</code><code class="p">]</code>
<code class="n">testing_labels</code> <code class="o">=</code> <code class="n">labels</code><code class="p">[</code><code class="n">training_size</code><code class="p">:]</code></pre>
          <p>For the training data, you created a <code>build_vocab</code> helper function to create a dictionary of the frequency of each word, sorted in order of the most frequent. The size of this dictionary is the <code>vocab_size</code>.</p>
          <p>To get an embedding layer in PyTorch, you can use the <code>nn.Embedding</code> layer type, like this, by specifying the desired vocab size and the number of embedding dimensions:<a contenteditable="false" data-primary="words" data-secondary="meaning from" data-tertiary="vectors" data-type="indexterm" id="id1176"/><a contenteditable="false" data-primary="vectors for word meanings" data-secondary="embeddings" data-type="indexterm" id="id1177"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="words" data-tertiary="vectors" data-type="indexterm" id="id1178"/><a contenteditable="false" data-primary="meaning of words" data-secondary="vectors" data-tertiary="embeddings" data-type="indexterm" id="id1179"/></p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">nn</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">)</code></pre>
          <p>This will initialize a vector with <code>embedding_dim</code> axes for each word. So, for example, if <code>embedding_dim</code> is <code>16</code>, then every word in the vocabulary will be assigned a 16-dimensional vector.</p>
          <p>Over time, the attributes for each token (encoded as values for the vector in each of its dimensions) will be learned through backpropagation as the network learns by matching the training data to its labels.</p>
          <p>An important next step is feeding the output of the embedding layer into a dense layer. <a contenteditable="false" data-primary="pooling" data-secondary="embeddings in NLP" data-type="indexterm" id="id1180"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="embeddings" data-tertiary="pooling" data-type="indexterm" id="id1181"/><a contenteditable="false" data-primary="embeddings" data-secondary="sarcasm detector" data-tertiary="pooling" data-type="indexterm" id="id1182"/>The easiest way to do this, similar to how you would when using a convolutional neural network, is to use pooling. In this instance, the dimensions of the embeddings are averaged out to produce a fixed-length output vector, and <code>Adaptive​A⁠ve​Pool1d(1)</code> reduces the input along the length of the sequence to a fixed vector size of 1.</p>
          <p>As an example, consider this model architecture:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="bp">self</code><code class="o">.</code><code class="n">embedding</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">)</code>
<code class="bp">self</code><code class="o">.</code><code class="n">global_pool</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">AdaptiveAvgPool1d</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
<code class="bp">self</code><code class="o">.</code><code class="n">fc1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">embedding_dim</code><code class="p">,</code> <code class="mi">24</code><code class="p">)</code>
<code class="bp">self</code><code class="o">.</code><code class="n">fc2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">24</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="bp">self</code><code class="o">.</code><code class="n">relu</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">()</code>
<code class="bp">self</code><code class="o">.</code><code class="n">sigmoid</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">()</code></pre>
          <p>Here, an embedding layer is defined, and it’s given the vocab size and an embedding dimension. Let’s take a look at the number of trainable parameters in the network, using <code>torchinfo.summary</code>:</p>
<pre data-code-language="python" data-type="programlisting">
<code class="o">==========================================================================</code>
<code class="n">Layer</code> <code class="p">(</code><code class="nb">type</code><code class="p">:</code><code class="n">depth</code><code class="o">-</code><code class="n">idx</code><code class="p">)</code>                   <code class="n">Output</code> <code class="n">Shape</code>              <code class="n">Param</code> <code class="c1">#</code>
<code class="o">==========================================================================</code>
<code class="n">TextClassificationModel</code>                  <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>                   <code class="o">--</code>
<code class="err">├─</code><code class="n">Embedding</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">1</code>                         <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">100</code><code class="p">,</code> <code class="mi">100</code><code class="p">]</code>            <code class="mi">2</code><code class="p">,</code><code class="mi">429</code><code class="p">,</code><code class="mi">200</code>
<code class="err">├─</code><code class="n">AdaptiveAvgPool1d</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">2</code>                 <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">100</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>              <code class="o">--</code>
<code class="err">├─</code><code class="n">Linear</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">3</code>                            <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">24</code><code class="p">]</code>                  <code class="mi">2</code><code class="p">,</code><code class="mi">424</code>
<code class="err">├─</code><code class="n">ReLU</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">4</code>                              <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">24</code><code class="p">]</code>                  <code class="o">--</code>
<code class="err">├─</code><code class="n">Linear</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">5</code>                            <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>                   <code class="mi">25</code>
<code class="err">├─</code><code class="n">Sigmoid</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">6</code>                           <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>                   <code class="o">--</code>
<code class="o">==========================================================================</code>
<code class="n">Total</code> <code class="n">params</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code><code class="mi">431</code><code class="p">,</code><code class="mi">649</code>
<code class="n">Trainable</code> <code class="n">params</code><code class="p">:</code> <code class="mi">2</code><code class="p">,</code><code class="mi">431</code><code class="p">,</code><code class="mi">649</code>
<code class="n">Non</code><code class="o">-</code><code class="n">trainable</code> <code class="n">params</code><code class="p">:</code> <code class="mi">0</code>
<code class="n">Total</code> <code class="n">mult</code><code class="o">-</code><code class="n">adds</code> <code class="p">(</code><code class="n">M</code><code class="p">):</code> <code class="mf">77.81</code>
<code class="o">==========================================================================</code>
<code class="n">Input</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">0.03</code>
<code class="n">Forward</code><code class="o">/</code><code class="n">backward</code> <code class="k">pass</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">2.57</code>
<code class="n">Params</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">9.73</code>
<code class="n">Estimated</code> <code class="n">Total</code> <code class="n">Size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">12.32</code>
<code class="o">==========================================================================</code></pre>
          <p>The vocabulary size is 24,292 words, and as the embedding has 100 dimensions, the total number of trainable parameters in the embedding layer will be 2,429,200. The first linear layer has 100 values in with 24 values out, so that’s a total of 2,400 weights, but each of the neurons also has a bias, so add 24 to get to 24, 24. </p>
          <p>Similarly, the last linear has 24 values in, with just a single neuron out. For a total of 24 parameters, plus one for the bias, this equals 25. The entire network has 2,431,649 parameters to learn. Note that the average pooling layer has 0 trainable parameters, as it’s just averaging the parameters in the embedding layer before it to get a single 16-value vector.</p>
          <p>If we train this model, we’ll get a pretty decent training accuracy of 99%+ after 30 epochs—but our validation accuracy will be below 80% (see <a data-type="xref" href="#ch06_clean_figure_2_1748752380714952">Figure 6-2</a>).</p>
          <figure><div class="figure" id="ch06_clean_figure_2_1748752380714952">
            <img src="assets/aiml_0602.png"/>
            <h6><span class="label">Figure 6-2. </span>Training accuracy versus validation accuracy</h6>
          </div></figure>
          <p>That might seem to be a reasonable curve,<a contenteditable="false" data-primary="overfitting" data-secondary="reducing in language models" data-tertiary="embeddings" data-type="indexterm" id="id1183"/><a contenteditable="false" data-primary="embeddings" data-secondary="sarcasm detector" data-tertiary="overfitting" data-type="indexterm" id="id1184"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="embeddings" data-tertiary="overfitting" data-type="indexterm" id="id1185"/><a contenteditable="false" data-primary="meaning of words" data-secondary="embeddings" data-tertiary="overfitting" data-type="indexterm" id="id1186"/><a contenteditable="false" data-primary="words" data-secondary="embeddings" data-tertiary="overfitting" data-type="indexterm" id="id1187"/> given that the validation data likely contains many words that aren’t present in the training data. However, if you examine the loss curves for training versus validation over one hundred epochs, you’ll see a problem. Although you would expect to see that the training accuracy is higher than the validation accuracy, a clear indicator of overfitting is that while the validation accuracy is dropping a little over time (as shown in <a data-type="xref" href="#ch06_clean_figure_2_1748752380714952">Figure 6-2</a>), its loss is increasing sharply, as shown in <a data-type="xref" href="#ch06_clean_figure_3_1748752380714977">Figure 6-3</a>.</p>
          <figure><div class="figure" id="ch06_clean_figure_3_1748752380714977">
            <img src="assets/aiml_0603.png"/>
            <h6><span class="label">Figure 6-3. </span>Training loss versus validation loss</h6>
          </div></figure>
          <p>Overfitting like this is common with NLP models, due to the somewhat unpredictable nature of language. In the next sections, we’ll look at how to reduce this effect by using a number of techniques.<a contenteditable="false" data-primary="" data-startref="ch6sar" data-type="indexterm" id="id1188"/><a contenteditable="false" data-primary="" data-startref="ch6sar2" data-type="indexterm" id="id1189"/><a contenteditable="false" data-primary="" data-startref="ch6sar3" data-type="indexterm" id="id1190"/><a contenteditable="false" data-primary="" data-startref="ch6sar4" data-type="indexterm" id="id1191"/><a contenteditable="false" data-primary="" data-startref="ch6sar5" data-type="indexterm" id="id1192"/><a contenteditable="false" data-primary="" data-startref="ch6sar6" data-type="indexterm" id="id1193"/><a contenteditable="false" data-primary="" data-startref="ch6sar7" data-type="indexterm" id="id1194"/></p>
        </div></section>
        <section data-pdf-bookmark="Reducing Overfitting in Language Models" data-type="sect2"><div class="sect2" id="ch06_clean_reducing_overfitting_in_language_models_1748752380729495">
          <h2>Reducing Overfitting in Language Models</h2>
          <p>Overfitting happens when<a contenteditable="false" data-primary="embeddings" data-secondary="overfitting reduced" data-type="indexterm" id="ch6redovr2"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="embeddings" data-tertiary="overfitting reduced" data-type="indexterm" id="ch6redovr3"/><a contenteditable="false" data-primary="meaning of words" data-secondary="embeddings" data-tertiary="overfitting reduced" data-type="indexterm" id="ch6redovr4"/><a contenteditable="false" data-primary="words" data-secondary="embeddings" data-tertiary="overfitting reduced" data-type="indexterm" id="ch6redovr5"/><a contenteditable="false" data-primary="overfitting" data-secondary="reducing in language models" data-type="indexterm" id="ch6redovr6"/><a contenteditable="false" data-primary="overfitting" data-secondary="reducing in language models" data-tertiary="about overfitting" data-type="indexterm" id="id1195"/><a contenteditable="false" data-primary="embeddings" data-secondary="overfitting reduced" data-tertiary="about overfitting" data-type="indexterm" id="id1196"/> the network becomes overspecialized to the training data, and one part of this involves the network becoming very good at matching patterns in “noisy” data in the training set that doesn’t exist anywhere else. Because this particular noise isn’t present in the validation set, the better the network gets at matching it, the worse the loss of the validation set will be. This can result in the escalating loss that you saw in <a data-type="xref" href="#ch06_clean_figure_3_1748752380714977">Figure 6-3</a>. </p>
          <p>In this section, we’ll explore several ways to generalize the model and reduce <span class="keep-together">overfitting.</span></p>
          <section data-pdf-bookmark="Adjusting the learning rate" data-type="sect3"><div class="sect3" id="ch06_clean_adjusting_the_learning_rate_1748752380729560">
            <h3>Adjusting the learning rate</h3>
            <p>A hyperparameter of the optimizer<a contenteditable="false" data-primary="overfitting" data-secondary="reducing in language models" data-tertiary="adjusting the learning rate" data-type="indexterm" id="ch6adj"/><a contenteditable="false" data-primary="embeddings" data-secondary="overfitting reduced" data-tertiary="adjusting the learning rate" data-type="indexterm" id="ch6adj2"/><a contenteditable="false" data-primary="learning rate (LR)" data-secondary="overfitting reduced by adjusting" data-type="indexterm" id="ch6adj3"/><a contenteditable="false" data-primary="optimizers" data-secondary="learning rate as hyperparameter" data-type="indexterm" id="id1197"/><a contenteditable="false" data-primary="learning rate (LR)" data-secondary="optimizer hyperparameter" data-type="indexterm" id="id1198"/> is the learning rate (LR). The details of this parameter are beyond the scope of this chapter, but consider it to be a value that if too high will cause the network to potentially learn too quickly and miss nuance. The flipside is also true—if you set it too low, your network may not learn effectively.</p>
            <p>Perhaps the biggest factor that can lead to overfitting is whether the LR of your optimizer is too high. If it is, then your network learns <em>too quickly</em>. For this example, the code to define the optimizer was as follows:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.001</code><code class="p">,</code> 
                       <code class="n">betas</code><code class="o">=</code><code class="p">(</code><code class="mf">0.9</code><code class="p">,</code> <code class="mf">0.999</code><code class="p">),</code> <code class="n">amsgrad</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code></pre>
            <p>These are the defaults for the <code>Adam</code> optimizer. <a contenteditable="false" data-primary="Adam optimizer" data-secondary="learning rate parameter adjustment" data-type="indexterm" id="id1199"/><a contenteditable="false" data-primary="optimizers" data-secondary="Adam" data-tertiary="learning rate tuned" data-type="indexterm" id="id1200"/>One thing to experiment with is the <code>learning rate</code> parameter (<code>lr</code>), and in the following code, you’ll see the results of an instance when I reduced by an order of 10 to 0.0001, like this:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(</code><code class="p">)</code><code class="p">,</code> <strong><code class="n">lr</code><code class="o">=</code></strong><strong><code class="mf">0.0001</code></strong><code class="p">,</code> 
                       <code class="n">betas</code><code class="o">=</code><code class="p">(</code><code class="mf">0.9</code><code class="p">,</code> <code class="mf">0.999</code><code class="p">)</code><code class="p">,</code> <code class="n">amsgrad</code><code class="o">=</code><code class="kc">False</code><code class="p">)</code></pre>
            <p>The <code>betas</code> values stay at their defaults, as does <code>amsgrad</code>. Also note that both <code>beta</code> values must be between 0 and 1, and typically, both are close to 1. <a contenteditable="false" data-primary="Adam optimizer" data-secondary="Amsgrad as alternative implementation" data-type="indexterm" id="id1201"/><a contenteditable="false" data-primary="Amsgrad optimizer" data-type="indexterm" id="id1202"/><a contenteditable="false" data-primary="optimizers" data-secondary="Adam" data-tertiary="Amsgrad as alternative" data-type="indexterm" id="id1203"/><a contenteditable="false" data-primary="“On the Convergence of Adam and Beyond” (Reddi, Kale, and Kumar)" data-primary-sortas="On the Convergence of Adam and Beyond" data-type="indexterm" id="id1204"/><a contenteditable="false" data-primary="Reddi, Sashank" data-type="indexterm" id="id1205"/><a contenteditable="false" data-primary="Kale, Satyen" data-type="indexterm" id="id1206"/><a contenteditable="false" data-primary="Kumar, Sanjiv" data-type="indexterm" id="id1207"/>Amsgrad is an alternative implementation of the Adam optimizer that was introduced in the paper <a href="https://oreil.ly/FhTDi">“On the Convergence of Adam and Beyond” by Sashank Reddi, Satyen Kale, and Sanjiv Kumar</a>.</p>
            <p>This much lower LR has a profound impact on the network. <a data-type="xref" href="#ch06_clean_figure_4_1748752380714997">Figure 6-4</a> shows the accuracy of the network over one hundred epochs. The lower LR can be seen in the first 10 epochs or so, where it appears that the network isn’t learning, before it “breaks out” and starts to learn quickly.</p>
            <p>Exploring the loss (as illustrated in <a data-type="xref" href="#ch06_clean_figure_5_1748752380715014">Figure 6-5</a>), we can see that even while the accuracy wasn’t going up for the first few epochs, the loss was going down. You could therefore be confident that the network would eventually start to learn, if you were watching it epoch by epoch.</p>
            <figure><div class="figure" id="ch06_clean_figure_4_1748752380714997">
              <img src="assets/aiml_0604.png"/>
              <h6><span class="label">Figure 6-4. </span>Accuracy with a lower LR</h6>
            </div></figure>
            <figure><div class="figure" id="ch06_clean_figure_5_1748752380715014">
              <img src="assets/aiml_0605.png"/>
              <h6><span class="label">Figure 6-5. </span>Loss with a lower LR</h6>
            </div></figure>
            <p>And while the loss does start to show the same curve of overfitting that you saw in <a data-type="xref" href="#ch06_clean_figure_3_1748752380714977">Figure 6-3</a>, note that it happens much later and at a much lower rate. By epoch 30, the loss is at about 0.49, whereas with the higher LR in <a data-type="xref" href="#ch06_clean_figure_3_1748752380714977">Figure 6-3</a>, it was more than double that amount. And while it takes the network longer to get to a good accuracy rate, it does so with less loss, so you can be more confident in the results. With these hyperparameters, the loss on the validation set started to increase at about epoch 60, at which point, the training set had 90% accuracy and the validation set had about 81% accuracy, showing that we have quite an effective network.</p>
            <p>Of course, it’s easy to just tweak the optimizer and then declare victory, but there are a number of other methods you can use to improve your model. You’ll learn about those in the next few sections, and for them, I’ve reverted back to using the default Adam optimizer so the effects of tweaking the LR won’t hide the benefits offered by these other techniques.<a contenteditable="false" data-primary="" data-startref="ch6adj" data-type="indexterm" id="id1208"/><a contenteditable="false" data-primary="" data-startref="ch6adj2" data-type="indexterm" id="id1209"/><a contenteditable="false" data-primary="" data-startref="ch6adj3" data-type="indexterm" id="id1210"/></p>
          </div></section>
          <section data-pdf-bookmark="Exploring vocabulary size" data-type="sect3"><div class="sect3" id="ch06_clean_exploring_vocabulary_size_1748752380729610">
            <h3>Exploring vocabulary size</h3>
            <p>The sarcasm dataset deals with words,<a contenteditable="false" data-primary="overfitting" data-secondary="reducing in language models" data-tertiary="vocabulary size" data-type="indexterm" id="id1211"/><a contenteditable="false" data-primary="embeddings" data-secondary="overfitting reduced" data-tertiary="vocabulary size" data-type="indexterm" id="id1212"/><a contenteditable="false" data-primary="vocabulary size and overfitting reduction" data-type="indexterm" id="id1213"/><a contenteditable="false" data-primary="words" data-secondary="vocabulary size and overfitting reduction" data-type="indexterm" id="id1214"/> so if you explore the words in the dataset and in particular their frequency, you might get a clue that helps fix the overfitting issue.</p>
            <p>I’ve provided a <code>word_frequency</code> helper function<a contenteditable="false" data-primary="words" data-secondary="word frequency helper function code" data-type="indexterm" id="id1215"/><a contenteditable="false" data-primary="word frequency helper function code" data-type="indexterm" id="id1216"/> that lets you explore the frequency of words in the vocabulary. It looks like this:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">word_frequency</code><code class="p">(</code><code class="n">sentences</code><code class="p">,</code> <code class="n">word_dict</code><code class="p">):</code>
    <code class="n">frequency</code> <code class="o">=</code> <code class="p">{</code><code class="n">word</code><code class="p">:</code> <code class="mi">0</code> <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">word_dict</code><code class="p">}</code>
 
    <code class="k">for</code> <code class="n">sentence</code> <code class="ow">in</code> <code class="n">sentences</code><code class="p">:</code>
        <code class="n">words</code> <code class="o">=</code> <code class="n">sentence</code><code class="o">.</code><code class="n">lower</code><code class="p">()</code><code class="o">.</code><code class="n">split</code><code class="p">()</code>
        <code class="k">for</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">words</code><code class="p">:</code>
            <code class="k">if</code> <code class="n">word</code> <code class="ow">in</code> <code class="n">frequency</code><code class="p">:</code>
                <code class="n">frequency</code><code class="p">[</code><code class="n">word</code><code class="p">]</code> <code class="o">+=</code> <code class="mi">1</code>
 
    <code class="k">return</code> <code class="n">frequency</code></pre>
            <p>You can run it with code like this:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">word_freq</code> <code class="o">=</code> <code class="n">word_frequency</code><code class="p">(</code><code class="n">training_sentences</code><code class="p">,</code> <code class="n">word_index</code><code class="p">)</code>
<code class="nb">print</code><code class="p">(</code><code class="n">word_freq</code><code class="p">)</code></pre>
            <p>You’ll then see results like this: a dictionary containing the frequency of each word, starting with the most frequently used one, and moving on from there. Here are the first few words:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="p">{</code><code class="s1">'new'</code><code class="p">:</code> <code class="mi">1318</code><code class="p">,</code> <code class="s1">'trump'</code><code class="p">:</code> <code class="mi">1117</code><code class="p">,</code> <code class="s1">'man'</code><code class="p">:</code> <code class="mi">1075</code><code class="p">,</code> <code class="s1">'not'</code><code class="p">:</code> <code class="mi">634</code><code class="p">,</code> <code class="s1">'just'</code><code class="p">:</code> <code class="mi">501</code><code class="p">,</code> 
 <code class="s1">'will'</code><code class="p">:</code> <code class="mi">484</code><code class="p">,</code> <code class="s1">'one'</code><code class="p">:</code> <code class="mi">469</code><code class="p">,</code> <code class="s1">'year'</code><code class="p">:</code> <code class="mi">440</code><code class="p">,</code> <code class="err">…</code></pre>
            <p>If you want to plot this, you can iterate through each item in the list and make the <em>x</em> value the ordinal of where you are (1 for the first item, 2 for the second item, etc.). The <em>y</em> value will then be a <code>newlist[item]</code>, which you can plot with <code>matplotlib</code>. Here’s the code:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
<code class="kn">from</code> <code class="nn">collections</code> <code class="kn">import</code> <code class="n">OrderedDict</code>
<code class="n">newlist</code> <code class="o">=</code> <code class="p">(</code><code class="n">OrderedDict</code><code class="p">(</code><code class="nb">sorted</code><code class="p">(</code><code class="n">word_freq</code><code class="o">.</code><code class="n">items</code><code class="p">(),</code> <code class="n">key</code><code class="o">=</code><code class="k">lambda</code> <code class="n">t</code><code class="p">:</code> <code class="n">t</code><code class="p">[</code><code class="mi">1</code><code class="p">],</code> 
                       <code class="n">reverse</code><code class="o">=</code><code class="kc">True</code><code class="p">)))</code>
 
<code class="n">xs</code><code class="o">=</code><code class="p">[]</code>
<code class="n">ys</code><code class="o">=</code><code class="p">[]</code>
<code class="n">curr_x</code> <code class="o">=</code> <code class="mi">1</code>
<code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">newlist</code><code class="p">:</code>
  <code class="n">xs</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">curr_x</code><code class="p">)</code>
  <code class="n">curr_x</code><code class="o">=</code><code class="n">curr_x</code><code class="o">+</code><code class="mi">1</code>
  <code class="n">ys</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">newlist</code><code class="p">[</code><code class="n">item</code><code class="p">])</code>
 
<code class="nb">print</code><code class="p">(</code><code class="n">ys</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code><code class="n">ys</code><code class="p">)</code>
 </pre>
            <p>The result is shown in <a data-type="xref" href="#ch06_clean_figure_6_1748752380715030">Figure 6-6</a>.</p>
            <figure><div class="figure" id="ch06_clean_figure_6_1748752380715030">
              <img alt="" src="assets/aiml_0606.png"/>
              <h6><span class="label">Figure 6-6. </span>Exploring the frequency of words</h6>
            </div></figure>
            <p>This “hockey stick” curve shows us that very few words are used many times, whereas most words are used very few times. But every word is effectively weighted equally because every word has an “entry” in the embedding. Given that we have a relatively large training set in comparison with the validation set, we’re ending up in a situation where there are many words present in the training set that aren’t present in the validation set.</p>
            <p>You can zoom in on the data by changing the axis of the plot just before calling <code>plt.show</code>. For example, to look at the volume of words from 300 to 10,000 on the <em>x</em>-axis with the scale from 0 to 100 on the <em>y</em>-axis, you can use this code:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code><code class="n">ys</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">([</code><code class="mi">300</code><code class="p">,</code><code class="mi">10000</code><code class="p">,</code><code class="mi">0</code><code class="p">,</code><code class="mi">100</code><code class="p">])</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
            <p>The result is in <a data-type="xref" href="#ch06_clean_figure_7_1748752380715045">Figure 6-7</a>.</p>
            <figure><div class="figure" id="ch06_clean_figure_7_1748752380715045">
              <img alt="Frequency of words 300–10,000" src="assets/aiml_0607.png"/>
              <h6><span class="label">Figure 6-7. </span>Frequency of words from 300 to 10,000</h6>
            </div></figure>
            <p>There are almost 25,000 words in the corpus, and the code is set up to only train for all of them! But if we look at the words in positions 2,000 onward, which is over 90% of our vocabulary, we’ll see that they’re each used fewer than 20 times in the entire corpus!</p>
            <p>This could explain the overfitting, so the logical next step is to see if we can reduce the vocabulary we are training for. Within the <code>build_vocab</code> helper function, we can add a parameter for the maximum vocab size we’re interested in, like this:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="k">def</code> <code class="nf">build_vocab</code><code class="p">(</code><code class="n">sentences</code><code class="p">,</code> <code class="n">max_vocab_size</code><code class="o">=</code><code class="mi">10000</code><code class="p">):</code>
    <code class="n">counter</code> <code class="o">=</code> <code class="n">Counter</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">text</code> <code class="ow">in</code> <code class="n">sentences</code><code class="p">:</code>
        <code class="n">counter</code><code class="o">.</code><code class="n">update</code><code class="p">(</code><code class="n">tokenize</code><code class="p">(</code><code class="n">text</code><code class="p">))</code>
 
<code class="c1"># Take only the top max_vocab_size-1 most frequent words </code>
<code class="c1"># (leave room for special tokens)</code>
    <code class="n">most_common</code> <code class="o">=</code> <code class="n">counter</code><code class="o">.</code><code class="n">most_common</code><code class="p">(</code><code class="n">max_vocab_size</code> <code class="err">–</code> <code class="mi">2</code><code class="p">)</code>  
    <code class="c1"># -2 for &lt;pad&gt; and &lt;unk&gt;</code>
 
    <code class="c1"># Create vocabulary with indices starting from 2</code>
    <code class="n">vocab</code> <code class="o">=</code> <code class="p">{</code><code class="n">word</code><code class="p">:</code> <code class="n">idx</code> <code class="o">+</code> <code class="mi">2</code> <code class="k">for</code> <code class="n">idx</code><code class="p">,</code> <code class="p">(</code><code class="n">word</code><code class="p">,</code> <code class="n">_</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">most_common</code><code class="p">)}</code>
    <code class="n">vocab</code><code class="p">[</code><code class="s1">'&lt;pad&gt;'</code><code class="p">]</code> <code class="o">=</code> <code class="mi">0</code>  <code class="c1"># Add padding token</code>
    <code class="n">vocab</code><code class="p">[</code><code class="s1">'&lt;unk&gt;'</code><code class="p">]</code> <code class="o">=</code> <code class="mi">1</code>  <code class="c1"># Add unknown token</code>
    <code class="k">return</code> <code class="n">vocab</code></pre>
            <p class="pagebreak-before">Then, when building our <code>word_index</code>, we can specify a maximum vocab size that we’re interested in exploring:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">vocab_size</code> <code class="o">=</code> <code class="mi">2000</code>
<code class="n">word_index</code> <code class="o">=</code> <code class="n">build_vocab</code><code class="p">(</code><code class="n">training_sentences</code><code class="p">,</code> <code class="n">max_vocab_size</code><code class="o">=</code><code class="n">vocab_size</code><code class="p">)</code></pre>
            <p>The embedding layer was already initialized with the vocab size, so the model architecture doesn’t need to change. Indeed, with the reduced vocab size, the number of learned parameters drops sharply, giving us a simpler network that learns faster:</p>
<pre data-code-language="python" data-type="programlisting">

<code class="o">==========================================================================</code>
<code class="n">Layer</code> <code class="p">(</code><code class="nb">type</code><code class="p">:</code><code class="n">depth</code><code class="o">-</code><code class="n">idx</code><code class="p">)</code>                   <code class="n">Output</code> <code class="n">Shape</code>              <code class="n">Param</code> <code class="c1">#</code>
<code class="o">==========================================================================</code>
<code class="n">TextClassificationModel</code>                  <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>                   <code class="o">--</code>
<code class="err">├─</code><code class="n">Embedding</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">1</code>                         <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">100</code><code class="p">,</code> <code class="mi">100</code><code class="p">]</code>            <code class="mi">200</code><code class="p">,</code><code class="mi">100</code>
<code class="err">├─</code><code class="n">AdaptiveAvgPool1d</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">2</code>                 <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">100</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>              <code class="o">--</code>
<code class="err">├─</code><code class="n">Linear</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">3</code>                            <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">24</code><code class="p">]</code>                  <code class="mi">2</code><code class="p">,</code><code class="mi">424</code>
<code class="err">├─</code><code class="n">ReLU</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">4</code>                              <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">24</code><code class="p">]</code>                  <code class="o">--</code>
<code class="err">├─</code><code class="n">Linear</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">5</code>                            <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>                   <code class="mi">25</code>
<code class="err">├─</code><code class="n">Sigmoid</code><code class="p">:</code> <code class="mi">1</code><code class="o">-</code><code class="mi">6</code>                           <code class="p">[</code><code class="mi">32</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>                   <code class="o">--</code>
<code class="o">==========================================================================</code>
<code class="n">Total</code> <code class="n">params</code><code class="p">:</code> <code class="mi">202</code><code class="p">,</code><code class="mi">549</code>
<code class="n">Trainable</code> <code class="n">params</code><code class="p">:</code> <code class="mi">202</code><code class="p">,</code><code class="mi">549</code>
<code class="n">Non</code><code class="o">-</code><code class="n">trainable</code> <code class="n">params</code><code class="p">:</code> <code class="mi">0</code>
<code class="n">Total</code> <code class="n">mult</code><code class="o">-</code><code class="n">adds</code> <code class="p">(</code><code class="n">M</code><code class="p">):</code> <code class="mf">6.48</code>
<code class="o">==========================================================================</code>
<code class="n">Input</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">0.03</code>
<code class="n">Forward</code><code class="o">/</code><code class="n">backward</code> <code class="k">pass</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">2.57</code>
<code class="n">Params</code> <code class="n">size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">0.81</code>
<code class="n">Estimated</code> <code class="n">Total</code> <code class="n">Size</code> <code class="p">(</code><code class="n">MB</code><code class="p">):</code> <code class="mf">3.40</code>
<code class="o">==========================================================================</code>
</pre>
            <p>The model has shrunk from 2.4 million parameters to only 202,549. </p>
            <p>After retraining and exploring the smaller model, we can see that the results have changed.</p>
            <p class="pagebreak-before"><a data-type="xref" href="#ch06_clean_figure_8_1748752380715056">Figure 6-8</a> shows the accuracy metrics. Now, the training set accuracy is about 82% and the validation accuracy is about 76%. They’re closer to each other and not diverging, which is a good sign that we’ve gotten rid of most of the overfitting.</p>
            <figure><div class="figure" id="ch06_clean_figure_8_1748752380715056">
              <img src="assets/aiml_0608.png"/>
              <h6><span class="label">Figure 6-8. </span>Accuracy with a two thousand–word vocabulary</h6>
            </div></figure>
            <p>This is somewhat reinforced by the loss plot in <a data-type="xref" href="#ch06_clean_figure_9_1748752380715067">Figure 6-9</a>. The loss on the validation set is rising but much slower than before, so reducing the size of the vocabulary to prevent the training set from overfitting on low-frequency words that were possibly only present in the training set appears to have worked.</p>
            <p>It’s worth experimenting with different vocab sizes, but remember that you can also have too small of a vocab size and overfit to that. You’ll need to find a balance. In this case, my choice of taking words that appear 20 times or more was purely arbitrary.</p>
                        <figure><div class="figure" id="ch06_clean_figure_9_1748752380715067">
              <img src="assets/aiml_0609.png"/>
              <h6><span class="label">Figure 6-9. </span>Loss with a two thousand–word vocabulary</h6>
            </div></figure>
          </div></section>
          <section data-pdf-bookmark="Exploring embedding dimensions" data-type="sect3"><div class="sect3" id="ch06_clean_exploring_embedding_dimensions_1748752380729659">
            <h3>Exploring embedding dimensions</h3>
            <p>For this example, I arbitrarily chose<a contenteditable="false" data-primary="overfitting" data-secondary="reducing in language models" data-tertiary="embedding dimensions" data-type="indexterm" id="ch6emdm"/><a contenteditable="false" data-primary="embeddings" data-secondary="overfitting reduced" data-tertiary="embedding dimensions" data-type="indexterm" id="ch6emdm2"/><a contenteditable="false" data-primary="vectors for word meanings" data-secondary="embeddings" data-tertiary="embedding dimensions" data-type="indexterm" id="ch6emdm3"/> an embedding dimension of 16. In this instance, words are encoded as vectors in 16-dimensional space, with their directions indicating their overall meaning. But is 16 a good number? With only two thousand words in our vocabulary, it might be on the high side, leading to a high degree of sparseness of direction. </p>
            <div data-type="note" epub:type="note"><h6>Note</h6>
              <p>I believe that the best way to think<a contenteditable="false" data-primary="sparseness" data-type="indexterm" id="id1217"/><a contenteditable="false" data-primary="vectors for word meanings" data-secondary="sparseness" data-type="indexterm" id="id1218"/> about sparseness is to project into three dimensions. Think of it like the earth, with one thousand vectors pointing from the core to a place on the surface. The vectors are in three dimensions, <em>x</em>, <em>y</em>, and <em>z</em>. There’s a lot of surface area for them to cover, but if many of them are missing <em>x</em> and <em>y</em>, meaning they’re just zero, a lot of them will be pointing to (0, 0, <em>z</em>) and a whole lot of the earth’s surface will be untouched! Thus, there will be a total lack of distinctiveness.</p>
            </div>
            <p class="pagebreak-before less_space">Research has shown that a<a contenteditable="false" data-primary="embeddings" data-secondary="embedding size as fourth root of vocabulary size" data-type="indexterm" id="id1219"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="embeddings" data-tertiary="embedding size as fourth root of vocabulary size" data-type="indexterm" id="id1220"/><a contenteditable="false" data-primary="meaning of words" data-secondary="embeddings" data-tertiary="embedding size as fourth root of vocabulary size" data-type="indexterm" id="id1221"/><a contenteditable="false" data-primary="words" data-secondary="embeddings" data-tertiary="embedding size as fourth root of vocabulary size" data-type="indexterm" id="id1222"/> best practice for embedding size is to have it be the fourth root of the vocabulary size. The fourth root of 2,000 is 6.687, so let’s explore what happens if we round this up and change the embedding dimension to 7.</p>
            <p>You can see the result of training for one hundred epochs in <a data-type="xref" href="#ch06_clean_figure_10_1748752380715076">Figure 6-10</a>. The training set’s accuracy stabilized at about 83%, and the validation set’s accuracy stabilized at about 77%. Despite some jitters, the lines are pretty flat, showing that the model has converged. This isn’t much different from the results in <a data-type="xref" href="#ch06_clean_figure_8_1748752380715056">Figure 6-8</a>, but reducing the embedding dimensionality allows the model to train significantly faster.</p>
            <figure><div class="figure" id="ch06_clean_figure_10_1748752380715076">
              <img src="assets/aiml_0610.png"/>
              <h6><span class="label">Figure 6-10. </span>Training versus validation accuracy for seven dimensions</h6>
            </div></figure>
            <p><a data-type="xref" href="#ch06_clean_figure_11_1748752380715085">Figure 6-11</a> shows the loss in training and validation. While it initially appeared that the loss was climbing at about epoch 20, it soon flattened out. Again, a good sign!</p>
            <figure><div class="figure" id="ch06_clean_figure_11_1748752380715085">
              <img src="assets/aiml_0611.png"/>
              <h6><span class="label">Figure 6-11. </span>Training versus validation loss for seven dimensions</h6>
            </div></figure>
            <p>Now that the dimensionality has been reduced, we can do a bit more tweaking of the model architecture.<a contenteditable="false" data-primary="" data-startref="ch6emdm" data-type="indexterm" id="id1223"/><a contenteditable="false" data-primary="" data-startref="ch6emdm2" data-type="indexterm" id="id1224"/><a contenteditable="false" data-primary="" data-startref="ch6emdm3" data-type="indexterm" id="id1225"/></p>
          </div></section>
          <section data-pdf-bookmark="Exploring the model architecture" data-type="sect3"><div class="sect3" id="ch06_clean_exploring_the_model_architecture_1748752380729707">
            <h3>Exploring the model architecture</h3>
            <p>After the optimizations in the previous sections, the model architecture looks like this:<a contenteditable="false" data-primary="overfitting" data-secondary="reducing in language models" data-tertiary="model architecture" data-type="indexterm" id="ch6arch"/><a contenteditable="false" data-primary="embeddings" data-secondary="overfitting reduced" data-tertiary="model architecture" data-type="indexterm" id="ch6arch2"/></p>
            <pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">TextClassificationModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">vocab_size</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="o">=</code><code class="mi">24</code><code class="p">):</code>
        <code class="nb">super</code><code class="p">(</code><code class="n">TextClassificationModel</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">embedding</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">global_pool</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">AdaptiveAvgPool1d</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">embedding_dim</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_dim</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">relu</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">()</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">sigmoid</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">()</code>
 </pre>
            <p class="pagebreak-before">One thing that comes to mind<a contenteditable="false" data-primary="hidden layers" data-secondary="adjusting to reduce overfitting" data-type="indexterm" id="id1226"/> is the dimensionality—the <code>GlobalAveragePooling1D</code> layer now emits just 7 dimensions, but they’re being fed into a hidden layer of 24 neurons, which is overkill. Let’s explore what happens when this is reduced to 8 neurons and trained for 100 epochs.</p>
            <p>You can see the training versus validation accuracy in <a data-type="xref" href="#ch06_clean_figure_12_1748752380715093">Figure 6-12</a>. When compared to <a data-type="xref" href="#ch06_clean_figure_7_1748752380715045">Figure 6-7</a>, where 24 neurons were used, the overall result is quite similar, but the model was somewhat faster to train.</p>
            <figure><div class="figure" id="ch06_clean_figure_12_1748752380715093">
              <img src="assets/aiml_0612.png"/>
              <h6><span class="label">Figure 6-12. </span>Reduced dense-architecture accuracy results</h6>
            </div></figure>
            <p>The loss curves in <a data-type="xref" href="#ch06_clean_figure_13_1748752380715102">Figure 6-13</a> show similar results.</p>
            <p>By following these exercises, we were able to reduce the model architecture significantly, reducing the number of parameters while improving the quality and mitigating overfitting. But there are a few more things we can do—starting with dropout.<a contenteditable="false" data-primary="" data-startref="ch6arch" data-type="indexterm" id="id1227"/><a contenteditable="false" data-primary="" data-startref="ch6arch2" data-type="indexterm" id="id1228"/></p>
                        <figure><div class="figure" id="ch06_clean_figure_13_1748752380715102">
              <img src="assets/aiml_0613.png"/>
              <h6><span class="label">Figure 6-13. </span>Reduced dense architecture loss results</h6>
            </div></figure>
          </div></section>
          <section data-pdf-bookmark="Using dropout" data-type="sect3"><div class="sect3" id="ch06_clean_using_dropout_1748752380729754">
            <h3>Using dropout</h3>
            <p>A common technique for<a contenteditable="false" data-primary="overfitting" data-secondary="reducing in language models" data-tertiary="dropout" data-type="indexterm" id="ch6do"/><a contenteditable="false" data-primary="embeddings" data-secondary="overfitting reduced" data-tertiary="dropout" data-type="indexterm" id="ch6do2"/><a contenteditable="false" data-primary="dropout regularization" data-type="indexterm" id="ch6do3"/><a contenteditable="false" data-primary="overfitting" data-secondary="dropout helping overcome" data-type="indexterm" id="ch6do4"/> reducing overfitting is to add dropout to a dense neural network. We explored this for convolutional neural networks back in <a data-type="xref" href="ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912">Chapter 3</a>, so it’s tempting to go straight to it here to see its effects on overfitting. But in this case, I want to wait until the vocabulary size, embedding size, and architecture complexity have been addressed. Those changes can often have a much larger impact than using dropout, and we’ve already seen some nice results.</p>
            <p>Now that our architecture has been simplified to have only eight neurons in the middle dense layer, the effect of dropout may be minimized—but let’s explore it anyway. Here’s the updated code for the model architecture to add a dropout of 0.25 (which equates to two of our eight neurons):</p>
            <pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">TextClassificationModel</code><code class="p">(</code><code class="n">nn</code><code class="o">.</code><code class="n">Module</code><code class="p">)</code><code class="p">:</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">vocab_size</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="o">=</code><code class="mi">8</code><code class="p">,</code> 
                       <strong><code class="n">dropout_rate</code><code class="o">=</code></strong><strong><code class="mf">0.25</code></strong><strong><code class="p">)</code><code class="p">:</code></strong>
        <code class="nb">super</code><code class="p">(</code><code class="n">TextClassificationModel</code><code class="p">,</code> <code class="bp">self</code><code class="p">)</code><code class="o">.</code><code class="fm">__init__</code><code class="p">(</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">embedding</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">global_pool</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">AdaptiveAvgPool1d</code><code class="p">(</code><code class="mi">1</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc1</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">embedding_dim</code><code class="p">,</code> <code class="n">hidden_dim</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Dropout</code><code class="p">(</code><code class="n">p</code><code class="o">=</code><code class="n">dropout_rate</code><code class="p">)</code>  <code class="c1"># Add dropout layer</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">fc2</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="n">hidden_dim</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">relu</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(</code><code class="p">)</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">sigmoid</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Sigmoid</code><code class="p">(</code><code class="p">)</code>
 
    <code class="k">def</code> <code class="nf">forward</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">)</code><code class="p">:</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">embedding</code><code class="p">(</code><code class="n">x</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="n">x</code><code class="o">.</code><code class="n">transpose</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>  <code class="c1"># Change for pooling layer</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">global_pool</code><code class="p">(</code><code class="n">x</code><code class="p">)</code><code class="o">.</code><code class="n">squeeze</code><code class="p">(</code><code class="mi">2</code><code class="p">)</code>
        <strong><code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">dropout</code></strong><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">relu</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">fc1</code><code class="p">(</code><code class="n">x</code><code class="p">)</code><code class="p">)</code><code class="p">)</code>
        <code class="n">x</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">sigmoid</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">fc2</code><code class="p">(</code><code class="n">x</code><code class="p">)</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">x</code>
 </pre>
            <p><a data-type="xref" href="#ch06_clean_figure_14_1748752380715110">Figure 6-14</a> shows the accuracy results when trained for one hundred epochs. This time, we see that the training accuracy and validation accuracy are converging, with the training accuracy now lower than before. Similarly, the loss curves in <a data-type="xref" href="#fig-6-15">Figure 6-15</a> show convergence, so while dropout is making our network a little <em>less</em> accurate, it appears to generalize it better. </p>
            <p>But do exercise caution before declaring victory! A close examination of the curves shows that the losses have nicely converged but that they <em>are</em> higher than previously. The training loss is above 0.5 with dropout but was around 0.3 without. It is also trending downward, so it’s worth experimenting to see whether longer training will produce a better result.</p>
            <figure><div class="figure" id="ch06_clean_figure_14_1748752380715110">
              
              <img src="assets/aiml_0614.png"/>
              <h6><span class="label">Figure 6-14. </span>Accuracy with added dropout</h6>
            </div></figure>
            

<figure><div class="figure" id="fig-6-15">
<img alt="" src="assets/aiml_0615.png"/>
<h6><span class="label">Figure 6-15. </span>Loss with added dropout</h6>
</div></figure>

            <p>You can also see that the model is heading back to its previous pattern of increasing validation loss over time. It’s not nearly as bad as before, but it’s heading in the wrong direction.</p>
            <p>In this case, when there were very few neurons, introducing dropout probably wasn’t the right idea. It’s still good to have this tool in your arsenal, though, so be sure to keep it in mind for more sophisticated architectures than this one.<a contenteditable="false" data-primary="" data-startref="ch6do" data-type="indexterm" id="id1229"/><a contenteditable="false" data-primary="" data-startref="ch6do2" data-type="indexterm" id="id1230"/><a contenteditable="false" data-primary="" data-startref="ch6do3" data-type="indexterm" id="id1231"/><a contenteditable="false" data-primary="" data-startref="ch6do4" data-type="indexterm" id="id1232"/></p>
          </div></section>
          <section data-pdf-bookmark="Using regularization" data-type="sect3"><div class="sect3" id="ch06_clean_using_regularization_1748752380729801">
            <h3>Using regularization</h3>
            <p><em>Regularization</em> is a technique<a contenteditable="false" data-primary="overfitting" data-secondary="reducing in language models" data-tertiary="regularization" data-type="indexterm" id="id1233"/><a contenteditable="false" data-primary="embeddings" data-secondary="overfitting reduced" data-tertiary="regularization" data-type="indexterm" id="id1234"/><a contenteditable="false" data-primary="regularization to reduce overfitting" data-type="indexterm" id="id1235"/><a contenteditable="false" data-primary="weights" data-secondary="regularization to reduce overfitting" data-type="indexterm" id="id1236"/> that helps prevent overfitting by reducing the polarization of weights. If the weights on some of the neurons are too heavy, regularization effectively punishes them. Broadly speaking, there are two types of regularization:</p>
            <dl>
              <dt>L1 regularization </dt>
              <dd>
                <p>This is often called <a contenteditable="false" data-primary="regularization to reduce overfitting" data-secondary="L1 regularization" data-type="indexterm" id="id1237"/><em>least absolute shrinkage</em> and <em>selection operator</em> (<em>lasso</em>) regularization. It effectively helps us ignore the zero or close-to-zero weights when calculating a result in a layer.</p>
              </dd>
              <dt>L2 regularization </dt>
              <dd>
                <p>This is often called <em>ridge</em> regression<a contenteditable="false" data-primary="regularization to reduce overfitting" data-secondary="L2 regularization" data-type="indexterm" id="id1238"/> because it pushes values apart by taking their squares. This tends to amplify the differences between nonzero values and zero or close-to-zero ones, creating a ridge effect.</p>
              </dd>
            </dl>
            <p>The two approaches can also be combined into what is sometimes called <em>elastic</em> <span class="keep-together">regularization.</span></p>
            <p>For NLP problems like the one we’re considering, L2 is most commonly used. It can be added as the <code>weight_decay</code> attribute to the <code>optimizer.</code> Here’s an example:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">optimizer</code> <code class="o">=</code> <code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(</code><code class="p">)</code><code class="p">,</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.001</code><code class="p">,</code> <code class="n">betas</code><code class="o">=</code><code class="p">(</code><code class="mf">0.9</code><code class="p">,</code> <code class="mf">0.999</code><code class="p">)</code><code class="p">,</code> 
                       <code class="n">amsgrad</code><code class="o">=</code><code class="kc">False</code><code class="p">,</code> <strong><code class="n">weight_decay</code><code class="o">=</code></strong><strong><code class="mf">0.01</code></strong><code class="p">)</code></pre>
            <p>This will apply the <code>weight_decay</code> of <code>0.01.</code> (Usually, you’ll have a value between 0.01 and 0.001 here). Alternatively, a neat trick you can do with PyTorch is to define different weight decays for different layers by specifying them within the <code>Adam</code> declaration call, like this:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="c1"># Different weight decay for different layers</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">([</code>
<code class="c1"># L2 reg on fc1</code>
        <code class="p">{</code><code class="s1">'params'</code><code class="p">:</code> <code class="n">model</code><code class="o">.</code><code class="n">fc1</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="s1">'weight_decay'</code><code class="p">:</code> <code class="mf">0.01</code><code class="p">},</code>      
    <code class="c1"># No L2 reg on other layers</code>
<code class="p">{</code><code class="s1">'params'</code><code class="p">:</code> <code class="p">[</code><code class="n">p</code> <code class="k">for</code> <code class="n">name</code><code class="p">,</code> <code class="n">p</code> <code class="ow">in</code> <code class="n">model</code><code class="o">.</code><code class="n">named_parameters</code><code class="p">()</code> 
            <code class="k">if</code> <code class="s1">'fc1'</code> <code class="ow">not</code> <code class="ow">in</code> <code class="n">name</code><code class="p">]}</code>  
<code class="p">],</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.0001</code><code class="p">)</code></pre>
            <p>The impact of adding regularization in a simple model like this isn’t particularly large, but it does smooth out our training loss and validation loss somewhat. It might be overkill for this scenario, but as with dropout, it’s a good idea to understand how to use regularization to prevent your model from getting overspecialized.</p>
          </div></section>
          <section data-pdf-bookmark="Other optimization considerations" data-type="sect3"><div class="sect3" id="ch06_clean_other_optimization_considerations_1748752380729848">
            <h3>Other optimization considerations</h3>
            <p>While the modifications we’ve made<a contenteditable="false" data-primary="overfitting" data-secondary="reducing in language models" data-tertiary="sentence length" data-type="indexterm" id="id1239"/><a contenteditable="false" data-primary="embeddings" data-secondary="overfitting reduced" data-tertiary="sentence length" data-type="indexterm" id="id1240"/><a contenteditable="false" data-primary="sentences" data-secondary="maximum sentence length setting" data-type="indexterm" id="id1241"/> have given us a much-improved model with less overfitting, there are other hyperparameters that you can experiment with. For example, we chose to make the maximum sentence length one hundred words, but that was purely arbitrary and probably not optimal. It’s a good idea to explore the corpus and see what a better sentence length might be. Here’s a snippet of code that looks at the sentences and plots the lengths of each one, sorted from low to high:</p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">xs</code><code class="o">=</code><code class="p">[]</code>
<code class="n">ys</code><code class="o">=</code><code class="p">[]</code>
<code class="n">current_item</code><code class="o">=</code><code class="mi">1</code>
<code class="k">for</code> <code class="n">item</code> <code class="ow">in</code> <code class="n">sentences</code><code class="p">:</code>
  <code class="n">xs</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">current_item</code><code class="p">)</code>
  <code class="n">current_item</code><code class="o">=</code><code class="n">current_item</code><code class="o">+</code><code class="mi">1</code>
  <code class="n">ys</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">item</code><code class="p">))</code>
<code class="n">newys</code> <code class="o">=</code> <code class="nb">sorted</code><code class="p">(</code><code class="n">ys</code><code class="p">)</code>
 
<code class="kn">import</code> <code class="nn">matplotlib.pyplot</code> <code class="k">as</code> <code class="nn">plt</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">xs</code><code class="p">,</code><code class="n">newys</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>
            <p class="pagebreak-before less_space">See <a data-type="xref" href="#ch06_clean_figure_15_1748752380715124">Figure 6-16</a> for the results of this.</p>
            <figure><div class="figure" id="ch06_clean_figure_15_1748752380715124">
              <img alt="" src="assets/aiml_0616.png"/>
              <h6><span class="label">Figure 6-16. </span>Exploring sentence length</h6>
            </div></figure>
            <p>Less than 200 sentences in the total corpus of 26,000+ have a length of 100 words or greater, so by choosing this as the maximum length, we’re introducing a lot of padding that isn’t necessary and thus affecting the model’s performance. Reducing the maximum to 85 words would still keep 26,000 of the sentences (99%+) with greatly reduced padding.<a contenteditable="false" data-primary="" data-startref="ch6redovr2" data-type="indexterm" id="id1242"/><a contenteditable="false" data-primary="" data-startref="ch6redovr3" data-type="indexterm" id="id1243"/><a contenteditable="false" data-primary="" data-startref="ch6redovr4" data-type="indexterm" id="id1244"/><a contenteditable="false" data-primary="" data-startref="ch6redovr5" data-type="indexterm" id="id1245"/><a contenteditable="false" data-primary="" data-startref="ch6redovr6" data-type="indexterm" id="id1246"/></p>
          </div></section>
        </div></section>
        <section data-pdf-bookmark="Putting It All Together" data-type="sect2"><div class="sect2" id="ch06_clean_putting_it_all_together_1748752380729895">
          <h2>Putting It All Together</h2>
          <p>Taking all of the preceding optimizations<a contenteditable="false" data-primary="embeddings" data-secondary="sarcasm detector" data-tertiary="overfitting reduction results" data-type="indexterm" id="id1247"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="embeddings" data-tertiary="overfitting reduction results" data-type="indexterm" id="id1248"/><a contenteditable="false" data-primary="meaning of words" data-secondary="embeddings" data-tertiary="overfitting reduction results" data-type="indexterm" id="id1249"/><a contenteditable="false" data-primary="words" data-secondary="embeddings" data-tertiary="overfitting reduction results" data-type="indexterm" id="id1250"/> into effect and retraining the model for three hundred epochs gives you the results in <a data-type="xref" href="#ch06_clean_figure_16_1748752380715140">Figure 6-17</a> for training and validation accuracies. Given that their curves are roughly matched, it shows that we’ve taken huge steps toward avoiding overfitting and that we have a network that’s learning effectively.</p>
          <p>Similarly, the training and validation loss curves over three hundred epochs are showing remarkable similarity, as depicted in <a data-type="xref" href="#ch06_clean_figure_17_1748752380715155">Figure 6-18</a>, which indicates that the optimizations are a step in the right direction to prevent overfitting for this model.</p>
                    <figure><div class="figure" id="ch06_clean_figure_16_1748752380715140">
            <img src="assets/aiml_0617.png"/>
            <h6><span class="label">Figure 6-17. </span>Optimized training and validation accuracy</h6>
          </div></figure>
          <figure><div class="figure" id="ch06_clean_figure_17_1748752380715155">
            <img src="assets/aiml_0618.png"/>
            <h6><span class="label">Figure 6-18. </span>Optimized training and validation loss</h6>
          </div></figure>
        </div></section>
        <section data-pdf-bookmark="Using the Model to Classify a Sentence" data-type="sect2"><div class="sect2" id="ch06_clean_using_the_model_to_classify_a_sentence_1748752380729946">
          <h2>Using the Model to Classify a Sentence</h2>
          <p>Now that you’ve created the model,<a contenteditable="false" data-primary="embeddings" data-secondary="sarcasm detector" data-tertiary="classifying sentences with model" data-type="indexterm" id="ch6class"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="embeddings" data-tertiary="sarcasm detector classifying sentences" data-type="indexterm" id="ch6class2"/><a contenteditable="false" data-primary="meaning of words" data-secondary="embeddings" data-tertiary="sarcasm detector classifying sentences" data-type="indexterm" id="ch6class3"/><a contenteditable="false" data-primary="words" data-secondary="embeddings" data-tertiary="sarcasm detector classifying sentences" data-type="indexterm" id="ch6class4"/> trained it, and optimized it to remove a lot of the problems that caused the overfitting, the next step is to run the model and inspect its results. To do this, you’ll create an array of new sentences. Consider, for example:</p>
<pre data-code-language="python" data-type="programlisting">
<code class="n">test_sentences</code> <code class="o">=</code> <code class="p">[</code>
             <code class="s2">"granny starting to fear spiders in the garden might be real"</code><code class="p">,</code> 
             <code class="s2">"game of thrones season finale showing this sunday night"</code><code class="p">,</code> 
             <code class="s2">"PyTorch book will be a best seller"</code><code class="p">]</code></pre>
          <p>You can then encode these by using the same tokenizer that you used when creating the vocabulary for training:</p> 
<pre data-code-language="python" data-type="programlisting">
<code class="nb">print</code><code class="p">(</code><code class="n">texts_to_sequences</code><code class="p">(</code><code class="n">test_sentences</code><code class="p">,</code> <code class="n">word_index</code><code class="p">))</code>
</pre>
          <p>It’s important to use this tokenizer because it has the tokens for the words that the network was trained on!</p>
          <p>The output of the print statement will be the sequences for the preceding sentences:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="p">[</code>
<code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">803</code><code class="p">,</code> <code class="mi">753</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">312</code><code class="p">,</code> <code class="mi">97</code><code class="p">],</code> 
<code class="p">[</code><code class="mi">123</code><code class="p">,</code> <code class="mi">1183</code><code class="p">,</code> <code class="mi">160</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1543</code><code class="p">,</code> <code class="mi">152</code><code class="p">],</code> 
<code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">235</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="mi">47</code><code class="p">,</code> <code class="mi">1</code><code class="p">]</code>
<code class="p">]</code></pre>
          <p>There are a lot of <code>1</code> tokens here (“&lt;OOV&gt;”), because words like <em>granny</em> and <em>spiders</em> don’t appear in the dictionary. The sequences are also shorter because the stopwords have been removed.</p>
          <p>Next, before you can pass the sequences to the model, you’ll need to put them in the shape that the model expects—that is, the desired length. You can do this with <code>pad_sequences</code> in the same way you did when training the model:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">padded</code> <code class="o">=</code> <code class="n">pad_sequences</code><code class="p">(</code><code class="n">sequences</code><code class="p">,</code> <code class="n">max_len</code><code class="p">)</code></pre>
          <p>This will output the sentences as sequences of length <code>85</code>, so the output for the first sequence will be as follows:</p>
          <pre data-code-language="python" data-type="programlisting">
<code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="mi">803</code><code class="p">,</code> <code class="mi">753</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">312</code><code class="p">,</code> <code class="mi">97</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> 
 <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code>
 <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code>
 <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">0</code><code class="p">]</code></pre>
          <p>It was a very short sentence, so it’s padded up to 85 characters with a lot of zeros!</p>
          <p>Now that you’ve padded and tokenized the sentences to fit the model’s expectations for the input dimensions, it’s time to pass them to the model and get predictions back.</p>
          <p class="pagebreak-before less_space">This involves multiple steps. First, convert the padded sequence into an input tensor:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Convert to tensor</code>
<code class="n">input_ids</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">tensor</code><code class="p">(</code><code class="n">padded</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">long</code><code class="p">)</code><code class="o">.</code><code class="n">to</code><code class="p">(</code><code class="n">device</code><code class="p">)</code></pre>
          <p>Next, put the model into evaluation mode to get the predictions, and then simply pass the <code>input_ids</code> to it to get the outputs:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="c1"># Get predictions</code>
<code class="n">model</code><code class="o">.</code><code class="n">eval</code><code class="p">()</code>
<code class="k">with</code> <code class="n">torch</code><code class="o">.</code><code class="n">no_grad</code><code class="p">():</code>
    <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">input_ids</code><code class="p">)</code></pre>
          <p>The results will be passed back as a list and printed, with high values indicating likely sarcasm. Here are the results for our sample sentences:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">tensor</code><code class="p">([[</code><code class="mf">0.5516</code><code class="p">],</code>
        <code class="p">[</code><code class="mf">0.0765</code><code class="p">],</code>
        <code class="p">[</code><code class="mf">0.0987</code><code class="p">]],</code> <code class="n">device</code><code class="o">=</code><code class="s1">'cuda:0'</code><code class="p">)</code></pre>
          <p>The high score for the first sentence (“granny starting to fear spiders in the garden might be real”), despite it having a lot of stopwords and being padded with a lot of zeros, indicates that there is a level of sarcasm there. The other two sentences scored much lower, indicating a lower likelihood of sarcasm in them.</p>
          <p>To get the probabilities, you can call the <code>squeeze()</code> method to retrieve the tensor values. And if you want to make a comparison to a threshold to get your prediction—for example, above 0.5 indicates sarcasm and below 0.5 indicates no sarcasm—then you can use code like this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">probabilities</code> <code class="o">=</code> <code class="n">outputs</code><code class="o">.</code><code class="n">squeeze</code><code class="p">()</code><code class="o">.</code><code class="n">cpu</code><code class="p">()</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code>
<code class="n">predictions</code> <code class="o">=</code> <code class="p">(</code><code class="n">probabilities</code> <code class="o">&gt;=</code> <code class="n">threshold</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="nb">int</code><code class="p">)</code></pre>
          <p>Based on your network tuning, you could also establish what you think the appropriate threshold should be. Running this with a 0.5 threshold gives us the following:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">Text</code><code class="p">:</code> <code class="n">granny</code> <code class="n">starting</code> <code class="n">to</code> <code class="n">fear</code> <code class="n">spiders</code> <code class="ow">in</code> <code class="n">the</code> <code class="n">garden</code> <code class="n">might</code> <code class="n">be</code> <code class="n">real</code>
<code class="n">Probability</code><code class="p">:</code> <code class="mf">0.5516</code>
<code class="n">Classification</code><code class="p">:</code> <strong><code class="n">Sarcastic</code></strong>
<code class="n">Confidence</code><code class="p">:</code> <code class="mf">0.5516</code>
<code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code>
 
<code class="n">Text</code><code class="p">:</code> <code class="n">game</code> <code class="n">of</code> <code class="n">thrones</code> <code class="n">season</code> <code class="n">finale</code> <code class="n">showing</code> <code class="n">this</code> <code class="n">sunday</code> <code class="n">night</code>
<code class="n">Probability</code><code class="p">:</code> <code class="mf">0.0765</code>
<code class="n">Classification</code><code class="p">:</code> <code class="n">Not</code> <code class="n">Sarcastic</code>
<code class="n">Confidence</code><code class="p">:</code> <code class="mf">0.9235</code>
<code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code>
 
<code class="n">Text</code><code class="p">:</code> <code class="n">PyTorch</code> <code class="n">book</code> <code class="n">will</code> <code class="n">be</code> <code class="n">a</code> <code class="n">best</code> <code class="n">seller</code>
<code class="n">Probability</code><code class="p">:</code> <code class="mf">0.0987</code>
<code class="n">Classification</code><code class="p">:</code> <strong><code class="n">Not</code> <code class="n">Sarcastic</code></strong>
<code class="n">Confidence</code><code class="p">:</code> <code class="mf">0.9013</code>
<code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code><code class="o">-</code></pre>
          <p>So, with these test sentences, we’re beginning to get a good indication that our network is performing as desired. You should test it with other data to see if you can break it, and if you break it consistently, then it’ll be time to try a different model architecture, use transfer learning from an existing working network, or explore using pretrained embeddings. </p>
          <p>We’ll learn about this in the next section, but before that, I’d like to show you how you can visualize the custom embeddings that this network learned.<a contenteditable="false" data-primary="" data-startref="ch6class" data-type="indexterm" id="id1251"/><a contenteditable="false" data-primary="" data-startref="ch6class2" data-type="indexterm" id="id1252"/><a contenteditable="false" data-primary="" data-startref="ch6class3" data-type="indexterm" id="id1253"/><a contenteditable="false" data-primary="" data-startref="ch6class4" data-type="indexterm" id="id1254"/></p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Visualizing the Embeddings" data-type="sect1"><div class="sect1" id="ch06_clean_visualizing_the_embeddings_1748752380729995">
        <h1>Visualizing the Embeddings</h1>
        <p>To visualize embeddings, you can<a contenteditable="false" data-primary="embeddings" data-secondary="visualizing the embeddings" data-type="indexterm" id="ch6vis"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="embeddings" data-tertiary="visualizing the embeddings" data-type="indexterm" id="ch6vis2"/><a contenteditable="false" data-primary="meaning of words" data-secondary="embeddings" data-tertiary="visualizing the embeddings" data-type="indexterm" id="ch6vis3"/><a contenteditable="false" data-primary="words" data-secondary="embeddings" data-tertiary="visualizing the embeddings" data-type="indexterm" id="ch6vis4"/><a contenteditable="false" data-primary="visualizing embeddings" data-type="indexterm" id="ch6vis5"/><a contenteditable="false" data-primary="Embedding Projector" data-type="indexterm" id="ch6vis6"/><a contenteditable="false" data-primary="vectors for word meanings" data-secondary="embeddings" data-tertiary="Embedding Projector for visualizing embeddings" data-type="indexterm" id="ch6vis7"/> use an online tool called the <a href="http://projector.tensorflow.org">Embedding Projector</a>. It comes preloaded with many existing datasets, but in this section, you’ll see how to take the data from the model you’ve just trained and visualize it by using this tool.</p>
        <p>But first, you’ll need a function to reverse the word index. It currently has the word as the token and the key as the value, but you need to invert it so you’ll have word values to plot on the projector. Here’s the code to do this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">reverse_word_index</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">([(</code><code class="n">value</code><code class="p">,</code> <code class="n">key</code><code class="p">)</code>
<code class="k">for</code> <code class="p">(</code><code class="n">key</code><code class="p">,</code> <code class="n">value</code><code class="p">)</code> <code class="ow">in</code> <code class="n">word_index</code><code class="o">.</code><code class="n">items</code><code class="p">()])</code>
 </pre>
        <p>You’ll also need to extract the weights of the vectors in the embeddings:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">embedding_weights</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">embedding</code><code class="o">.</code><code class="n">weight</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">cpu</code><code class="p">()</code><code class="o">.</code><code class="n">numpy</code><code class="p">()</code>
<code class="nb">print</code><code class="p">(</code><code class="n">embedding_weights</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code>
 </pre>
        <p>If you’ve followed the optimizations in this chapter, the output of this will be <code>(2000,7)</code> because we used a 2,000 word vocabulary and 7 dimensions for the embedding. If you want to explore a word and its vector details, you can do so with code like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="nb">print</code><code class="p">(</code><code class="n">reverse_word_index</code><code class="p">[</code><code class="mi">2</code><code class="p">])</code>
<code class="nb">print</code><code class="p">(</code><code class="n">embedding_weights</code><code class="p">[</code><code class="mi">2</code><code class="p">])</code>
 </pre>
        <p>This will produce the following output:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="n">new</code>
<code class="p">[</code><code class="err">–</code><code class="mf">0.27116913</code> <code class="err">–</code><code class="mf">1.3026129</code>   <code class="mf">1.6390767</code>   <code class="mf">0.4922502</code>  <code class="err">–</code><code class="mf">0.6025921</code>   <code class="mf">1.4584142</code>
  <code class="mf">0.05054485</code><code class="p">]</code>
 </pre>
        <p>So, the word <em>new</em> is represented by a vector with those seven coefficients on its axes.</p>
        <p class="pagebreak-before less_space">The Embedding Projector uses two tab-separated values (TSV) files, one for the vector dimensions and one for metadata. This code will generate them for you:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">io</code>
<code class="n">out_v</code> <code class="o">=</code> <code class="n">io</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="s1">'vecs.tsv'</code><code class="p">,</code> <code class="s1">'w'</code><code class="p">,</code> <code class="n">encoding</code><code class="o">=</code><code class="s1">'utf-8'</code><code class="p">)</code>
<code class="n">out_m</code> <code class="o">=</code> <code class="n">io</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="s1">'meta.tsv'</code><code class="p">,</code> <code class="s1">'w'</code><code class="p">,</code> <code class="n">encoding</code><code class="o">=</code><code class="s1">'utf-8'</code><code class="p">)</code>
<code class="k">for</code> <code class="n">word_num</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="n">vocab_size</code><code class="p">):</code>
  <code class="n">word</code> <code class="o">=</code> <code class="n">reverse_word_index</code><code class="p">[</code><code class="n">word_num</code><code class="p">]</code>
  <code class="n">embeddings</code> <code class="o">=</code> <code class="n">embedding_weights</code><code class="p">[</code><code class="n">word_num</code><code class="p">]</code>
  <code class="n">out_m</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="n">word</code> <code class="o">+</code> <code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">)</code>
  <code class="n">out_v</code><code class="o">.</code><code class="n">write</code><code class="p">(</code><code class="s1">'</code><code class="se">\t</code><code class="s1">'</code><code class="o">.</code><code class="n">join</code><code class="p">([</code><code class="nb">str</code><code class="p">(</code><code class="n">x</code><code class="p">)</code> <code class="k">for</code> <code class="n">x</code> <code class="ow">in</code> <code class="n">embeddings</code><code class="p">])</code> <code class="o">+</code> <code class="s2">"</code><code class="se">\n</code><code class="s2">"</code><code class="p">)</code>
<code class="n">out_v</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
<code class="n">out_m</code><code class="o">.</code><code class="n">close</code><code class="p">()</code>
 </pre>
        <p>Alternatively, if you are using Google Colab, you can download the TSV files with the following code or from the Files pane:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="k">try</code><code class="p">:</code>
  <code class="kn">from</code> <code class="nn">google.colab</code> <code class="kn">import</code> <code class="n">files</code>
<code class="k">except</code> <code class="ne">ImportError</code><code class="p">:</code>
  <code class="k">pass</code>
<code class="k">else</code><code class="p">:</code>
  <code class="n">files</code><code class="o">.</code><code class="n">download</code><code class="p">(</code><code class="s1">'vecs.tsv'</code><code class="p">)</code>
  <code class="n">files</code><code class="o">.</code><code class="n">download</code><code class="p">(</code><code class="s1">'meta.tsv'</code><code class="p">)</code>
 </pre>
        <p>Once you have the files, you can press the Load button on the projector to visualize the embeddings (see <a data-type="xref" href="#ch06_clean_figure_18_1748752380715170">Figure 6-19</a>).</p>
        <figure><div class="figure" id="ch06_clean_figure_18_1748752380715170">
          <img alt="" src="assets/aiml_0619.png"/>
          <h6><span class="label">Figure 6-19. </span>Using the Embeddings Projector</h6>
        </div></figure>
        <p class="pagebreak-before">You can also use the vectors and meta TSV files where recommended in the resulting dialog and then click Sphereize Data on the projector. This will cause the words to be clustered in a sphere and will give you a clear visualization of the binary nature of this classifier. It’s only been trained on sarcastic and nonsarcastic sentences, so words tend to cluster toward one label or another (see <a data-type="xref" href="#ch06_clean_figure_19_1748752380715184">Figure 6-20</a>).</p>
        <figure><div class="figure" id="ch06_clean_figure_19_1748752380715184">
          <img alt="" src="assets/aiml_0620.png"/>
          <h6><span class="label">Figure 6-20. </span>Visualizing the sarcasm embeddings</h6>
        </div></figure>
        <p>Screenshots don’t do all of this justice—you should try it for yourself! You can rotate the center sphere and explore the words on each “pole” to see the impact they have on the overall classification, and you can also select words and show related ones in the righthand pane. Have a play and experiment!<a contenteditable="false" data-primary="" data-startref="ch6vis" data-type="indexterm" id="id1255"/><a contenteditable="false" data-primary="" data-startref="ch6vis2" data-type="indexterm" id="id1256"/><a contenteditable="false" data-primary="" data-startref="ch6vis3" data-type="indexterm" id="id1257"/><a contenteditable="false" data-primary="" data-startref="ch6vis4" data-type="indexterm" id="id1258"/><a contenteditable="false" data-primary="" data-startref="ch6vis5" data-type="indexterm" id="id1259"/><a contenteditable="false" data-primary="" data-startref="ch6vis6" data-type="indexterm" id="id1260"/><a contenteditable="false" data-primary="" data-startref="ch6vis7" data-type="indexterm" id="id1261"/></p>
      </div></section>
      <section data-pdf-bookmark="Using Pretrained Embeddings" data-type="sect1"><div class="sect1" id="ch06_clean_using_pre_trained_embeddings_1748752380730043">
        <h1>Using Pretrained Embeddings</h1>
        <p>An alternative to training your<a contenteditable="false" data-primary="embeddings" data-secondary="pretrained embeddings" data-type="indexterm" id="ch6pre"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="embeddings" data-tertiary="pretrained embeddings" data-type="indexterm" id="ch6pre2"/><a contenteditable="false" data-primary="meaning of words" data-secondary="embeddings" data-tertiary="pretrained embeddings" data-type="indexterm" id="ch6pre3"/><a contenteditable="false" data-primary="words" data-secondary="embeddings" data-tertiary="pretrained embeddings" data-type="indexterm" id="ch6pre4"/><a contenteditable="false" data-primary="pretrained embeddings" data-type="indexterm" id="ch6pre5"/><a contenteditable="false" data-primary="Global Vectors for Word Representation (GloVe) pretrained embeddings" data-type="indexterm" id="ch6pre6"/><a contenteditable="false" data-primary="Stanford Global Vectors for Word Representation (GloVe) pretrained embeddings" data-type="indexterm" id="ch6pre7"/> own embeddings is to use ones that have been pretrained by others on your behalf. There are many sources where you can find these, including Kaggle and Hugging Face. You can even find pretrained embeddings posted alongside research results. One such set of pretrained embeddings is the <a href="https://oreil.ly/s1YWw">Stanford GloVe embeddings</a>, and we’ll explore those here.</p>
        <p>Note, however, that when<a contenteditable="false" data-primary="embeddings" data-secondary="pretrained embeddings" data-tertiary="tokenizer update to match rules" data-type="indexterm" id="id1262"/><a contenteditable="false" data-primary="natural language processing (NLP)" data-secondary="embeddings" data-tertiary="pretrained requiring tokenizer update" data-type="indexterm" id="id1263"/><a contenteditable="false" data-primary="meaning of words" data-secondary="embeddings" data-tertiary="pretrained requiring tokenizer update" data-type="indexterm" id="id1264"/><a contenteditable="false" data-primary="words" data-secondary="embeddings" data-tertiary="pretrained requiring tokenizer update" data-type="indexterm" id="id1265"/><a contenteditable="false" data-primary="pretrained embeddings" data-secondary="tokenizer update to match rules" data-type="indexterm" id="id1266"/> using embeddings that have been pretrained, you should also consider updating and changing your tokenizer to match any rules used with the pretrained embeddings. </p>
        <p>For example, with the GloVE pretrained embeddings—which simply comprise a large text file of words with their pretrained embedding in a number of dimensions from 50 to 300—the rules used to tokenize words are a little different from those for the handmade tokenizer we’ve been using for raw data. So, for GloVe, you should consider rules such as all of the words being lowercase or numbers being normalized <span class="keep-together">to 0.</span></p>
        <p>Once you’ve done this (I’ve provided code for GloVe in the downloads, and I discuss it in a little more detail in the next chapter), then it’s simply a matter of loading the weights of the pretrained embeddings to your model definition like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Initialize embedding layer</code>
<code class="bp">self</code><code class="o">.</code><code class="n">embedding</code> <code class="o">=</code> <code class="n">nn</code><code class="o">.</code><code class="n">Embedding</code><code class="p">(</code><code class="n">vocab_size</code><code class="p">,</code> <code class="n">embedding_dim</code><code class="p">)</code>
 
<code class="c1"># Load pretrained embeddings if provided</code>
<code class="k">if</code> <code class="n">pretrained_embeddings</code> <code class="ow">is</code> <code class="ow">not</code> <code class="kc">None</code><code class="p">:</code>
    <code class="bp">self</code><code class="o">.</code><code class="n">embedding</code><code class="o">.</code><code class="n">weight</code><code class="o">.</code><code class="n">data</code><code class="o">.</code><code class="n">copy_</code><code class="p">(</code><code class="n">pretrained_embeddings</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">freeze_embeddings</code><code class="p">:</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">embedding</code><code class="o">.</code><code class="n">weight</code><code class="o">.</code><code class="n">requires_grad</code> <code class="o">=</code> <code class="kc">False</code></pre>
        <p>If you don’t want to learn from these embeddings and you want to just use them, then you should set <code>freeze_embeddings</code> to <code>True</code>. Otherwise, the network will fine-tune by using the pre-loaded embedding weights as a starting point.</p>
        <p>This model will rapidly reach peak accuracy in training, and it will not overfit as much as we saw previously. The accuracy over three hundred epochs shows that training and validation are very much in step with each other (see <a data-type="xref" href="#ch06_clean_figure_21_1748752380715231">Figure 6-22</a>). The loss values are also in step, which shows that we are fitting very nicely over the first couple of hundred epochs. However, they also begin to diverge (see <a data-type="xref" href="#ch06_clean_figure_21_1748752380715231">Figure 6-22</a>).</p>
        <p>On the other hand, it is worth noting that the overall accuracy (at about 70%) is quite low, considering that a coin flip would have a 50% chance of getting it right! So, while using pretrained embeddings can make for much faster training with less overfitting, you should also understand what it is that they’re useful for and that they may not always be best for your scenario. You may therefore need to explore optimization methods or alternatives where appropriate.<a contenteditable="false" data-primary="" data-startref="ch6sentall" data-type="indexterm" id="id1267"/><a contenteditable="false" data-primary="" data-startref="ch6emball" data-type="indexterm" id="id1268"/><a contenteditable="false" data-primary="" data-startref="ch6emball2" data-type="indexterm" id="id1269"/><a contenteditable="false" data-primary="" data-startref="ch6emball3" data-type="indexterm" id="id1270"/><a contenteditable="false" data-primary="" data-startref="ch6emball4" data-type="indexterm" id="id1271"/><a contenteditable="false" data-primary="" data-startref="ch6pre" data-type="indexterm" id="id1272"/><a contenteditable="false" data-primary="" data-startref="ch6pre2" data-type="indexterm" id="id1273"/><a contenteditable="false" data-primary="" data-startref="ch6pre3" data-type="indexterm" id="id1274"/><a contenteditable="false" data-primary="" data-startref="ch6pre4" data-type="indexterm" id="id1275"/><a contenteditable="false" data-primary="" data-startref="ch6pre5" data-type="indexterm" id="id1276"/><a contenteditable="false" data-primary="" data-startref="ch6pre6" data-type="indexterm" id="id1277"/><a contenteditable="false" data-primary="" data-startref="ch6pre7" data-type="indexterm" id="id1278"/></p>
        <figure><div class="figure" id="ch06_clean_figure_20_1748752380715219">
          <img src="assets/aiml_0621.png"/>
         <h6><span class="label">Figure 6-21. </span>Accuracy metrics using GloVe embeddings</h6>
        </div></figure>
        <figure><div class="figure" id="ch06_clean_figure_21_1748752380715231">
          <img src="assets/aiml_0622.png"/>
          <h6><span class="label">Figure 6-22. </span>Loss metrics using GloVe embeddings</h6>
        </div></figure>
        
      </div></section>
      <section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch06_clean_summary_1748752380730088">
        <h1>Summary</h1>
        <p>In this chapter, you built your first model that can understand sentiment in text. It did this by taking the tokenized text from <a data-type="xref" href="ch05.html#ch05_introduction_to_natural_language_processing_1748549080743759">Chapter 5</a> and mapping it to vectors. Then, using backpropagation, it learned the appropriate “direction” for each vector based on the label for the sentence containing it. Finally, it was able to use all of the vectors for a collection of words to build up an idea of the sentiment within the sentence. </p>
        <p>You also explored ways to optimize your model to avoid overfitting, and you saw a neat visualization of the final vectors representing your words. But while this was a nice way to classify sentences, it simply treated each sentence as a bunch of words. There was no inherent sequence involved, and the order of appearance of words is very important in determining the real meaning of a sentence. </p>
        <p>Therefore, it’s a good idea to see if we can improve our models by taking sequence into account. We’ll explore that in the next chapter with the introduction of a new layer type: a <em>recurrent</em> layer, which is the foundation of recurrent neural networks. </p>
      </div></section>
    </div></section></body></html>