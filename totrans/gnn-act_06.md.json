["```py\nimport numpy as np\n\nfilename = 'data/new_AMZN_electronics.npz'\n\ndata = np.load(filename)\n\nloader = dict(data)\nprint(loader)\n```", "```py\n{'adj_data': array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]],\\\n dtype=float32), 'attr_data': \\\narray([[0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 1., 0., ..., 0., 0., 0.],\n       [1., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 1.]],\\\n dtype=float32), 'labels': \\\narray([6, 4, 3, ..., 1, 2, 3]),\\\n 'class_names': array(['Film Photography',\\\n 'Digital Cameras', 'Binoculars & Scopes',\n       'Lenses', 'Tripods & Monopods', 'Video Surveillance',\n       'Lighting & Studio', 'Flashes'], dtype='<U19')}\n```", "```py\nadj_matrix = torch.tensor(loader['adj_data'])\nif not adj_matrix.is_sparse:\n    adj_matrix = adj_matrix.to_sparse()\n\nfeature_matrix = torch.tensor(loader['attr_data'])\nlabels = loader['labels']\n\nclass_names = loader.get('class_names')\nmetadata = loader.get('metadata')\n\nnum_nodes = adj_matrix.size(0)\nnum_edges = adj_matrix.coalesce().values().size(0)   #1\ndensity = num_edges / (num_nodes \\\n* (num_nodes - 1) / 2) if num_nodes \\\n> 1 else 0  #2\n```", "```py\ndegrees = adj_matrix.coalesce().indices().numpy()[0]    #1\ndegree_count = np.bincount(degrees, minlength=num_nodes)\n\nplt.figure(figsize=(10, 5))\nplt.hist(degree_count, bins=25, alpha=0.75, color='blue')\nplt.xlabel('Degree')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n```", "```py\nfrom torch_geometric.nn import GCNConv   #1\n\nclass GCNEncoder(torch.nn.Module):                         #2\n    def __init__(self, input_size, layers, latent_dim):    #2\n        super().__init__() #2\n        self.conv0 = GCNConv(input_size, layers[0])    #3\n        self.conv1 = GCNConv(layers[0], layers[1])     #3\n        self.conv2 = GCNConv(layers[1], latent_dim)    #3\n\n    def forward(self, x, edge_index):           #4\n        x = self.conv0(x, edge_index).relu()    #4\n        x = self.conv1(x, edge_index).relu()    #4\n        return self.conv2(x, edge_index)        #4\n```", "```py\nclass InnerProductDecoder(torch.nn.Module):     #1\n    def __init__(self):                        \n         super().__init__()                    \n\ndef forward(self, z, edge_index):     #2\n        value = (z[edge_index[0]] * \\\nz[edge_index[1]]).sum(dim=1)   #3\n        return torch.sigmoid(value)\n```", "```py\n   class GraphAutoEncoder(torch.nn.Module):\n        def __init__(self, input_size, layers, latent_dims):\n            super().__init__()\n            self.encoder = GCNEncoder(input_size, \\\n   layers, latent_dims)     #1\n            self.decoder = InnerProductDecoder()      #2\n\n        def forward(self, x):\n            z = self.encoder(x)\n            return self.decoder(z)\n```", "```py\nfrom torch_geometrics.utils import to_edge_index   #1\n\nedge_index, edge_attr = to_edge_index(adj_matrix)   #2\nnum_nodes = adj_matrix.size(0)\n```", "```py\ndata = Data(x=feature_matrix,         #1\n            edge_index=edge_index,   \n            edge_attr=edge_attr,     \n            y=labels)                \n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntransform = T.Compose([\\\n     T.NormalizeFeatures(),\\                  #2\n     T.ToDevice(device),                     \n     T.RandomLinkSplit(num_val=0.05,\\\n     num_test=0.1, is_undirected=True,       \n     add_negative_train_samples=False)])     \ntrain_data, val_data, test_data = transform(data)\n```", "```py\ninput_size, latent_dims = feature_matrix.shape[1], 16   #1\nlayers = [512, 256]                                    \nmodel = GraphAutoEncoder(input_size, layers, latent_dims)   #2\nmodel = model.to(device)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = torch.nn.BCEWithLogitsLoss()   #3\n```", "```py\ndef train(model, criterion, optimizer):\n\n    model.train() \n\n    optimizer.zero_grad() \n    z = model.encoder(train_data.x,\\\n    train_data.edge_index)   #1\n\n    neg_edge_index = negative_sampling(\\         #2\n    edge_index=train_data.edge_index,\\\n    num_nodes=train_data.num_nodes,             \n    num_neg_samples=train_data.\\\n    edge_label_index.size(1), method='sparse')  \n\n    edge_label_index = torch.cat(                     #3\n    [train_data.edge_label_index, neg_edge_index],    #3\n    dim=-1,)                                          #3\n\n    out = model.decoder(z, edge_label_index).view(-1)   #4\n\n    edge_label = torch.cat([        #5\n    train_data.edge_label,         \ntrain_data.edge_label.new_zeros\\\n(neg_edge_index.size(1))           \n    ], dim=0)                      \n    loss = criterion(out, edge_label)   #6\n    loss.backward()                     #6\n    optimizer.step() #6\n\n    return loss\n```", "```py\nfrom sklearn.metrics import roc_auc_score\n\n@torch.no_grad() \ndef test(data):\n    model.eval() \n    z = model.encode(data.x, data.edge_index)   #1\n    out = model.decode(z, \\\n    data.edge_label_index).view(-1).sigmoid()   #2\n    loss = roc_auc_score(data.edge_label.cpu().numpy(),   #3\n                        out.cpu().numpy())                #3\n    return loss\n```", "```py\nbest_val_auc = final_test_auc = 0 \nfor epoch in range(1, 201): \n    loss = train(model, criterion, optimizer)  #1\n    val_auc = test(val_data)   #2\n    if val_auc > best_val_auc: \n        best_val_auc = val_auc \ntest_auc = test(test_data)   #3\n```", "```py\nclass VariationalGCNEncoder(torch.nn.Module):            #1\n  def __init__(self, input_size, layers, latent_dims):\n    super().__init__()\n    self.layer0 = GCNConv(input_size, layers[0])\n    self.layer1 = GCNConv(layers[0], layers[1])\n    self.mu = GCNConv(layers[1], latent_dims)           \n    self.logvar = GCNConv(layers[1], latent_dims)       \n\n  def forward(self, x, edge_index):\n    x = self.layer0(x, edge_index).relu()\n    x = self.layer1(x, edge_index).relu()\n    mu = self.mu(x, edge_index)\n    logvar = self.logvar(x, edge_index)\n    return mu, logvar                      #2\n```", "```py\nfrom torch_geometric.nn import VGAE   #1\nmodel = VGAE(VariationalGCNEncoder(input_size,\\\n layers, latent_dims))\n```", "```py\ndef train(model, criterion, optimizer):\n    model.train() \n    optimizer.zero_grad() \n    z = model.encode(train_data.x, train_data.edge_index)      #1\n\n    neg_edge_index = negative_sampling( \n    edge_index=train_data.edge_index, num_nodes=train_data.num_nodes,\n    num_neg_samples=train_data.edge_label_index.size(1), method='sparse')\n\n    edge_label_index = torch.cat( \n    [train_data.edge_label_index, neg_edge_index], \n    dim=-1,) \n    out = model.decode(z, edge_label_index).view(-1)          \n\n    edge_label = torch.cat([ \n    train_data.edge_label,\n    train_data.edge_label.new_zeros(neg_edge_index.size(1))\n    ], dim=0)\n\n    loss = criterion(out, edge_label)            #2\n+ (1 / train_data.num_nodes) * model.kl_loss()  \n\n    loss.backward() \n    optimizer.step()\n\n    return loss\n```", "```py\n     smiles     logP     qed     SAS\n0     CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1\n     5.05060     0.702012     2.084095\n1     C[C@@H]1CC(Nc2cncc(-c3nncn3C)c2)C[C@@H](C)C1\n     3.11370     0.928975     3.432004\n2     N#Cc1ccc(-c2ccc(O[C@@H](C(=O)N3CCCC3)c3ccccc3)...\n     4.96778     0.599682     2.470633\n3     CCOC(=O)[C@@H]1CCCN(C(=O)c2nc\n      (-c3ccc(C)cc3)n3c...     \n      4.00022     0.690944     2.822753\n4     N#CC1=C(SCC(=O)Nc2cccc(Cl)c2)N=C([O-])\n      [C@H](C#...     3.60956     0.789027     4.035182\n```", "```py\nimport requests\nimport pandas as pd\n\ndef download_file(url, filename):\n     response = requests.get(url)\n     response.raise_for_status() \n     with open(filename, 'wb') as f:\n     f.write(response.content)\n\nurl = \"https://raw.githubusercontent.com/\naspuru-guzikgroup/chemical_vae/master/models/\nzinc_properties/250k_rndm_zinc_drugs_clean_3.csv\"\nfilename = \"250k_rndm_zinc_drugs_clean_3.csv\"\n\ndownload_file(url, filename)\n\ndf = pd.read_csv(filename)\ndf[\"smiles\"] = df[\"smiles\"].apply(lambda s: s.replace(\"\\n\", \"\"))\n```", "```py\n   from torch_geometric.data import Data\n   import torch\n   from rdkit import Chem\n\n   def smiles_to_graph(smiles, qed):\n     mol = Chem.MolFromSmiles(smiles)\n        if not mol:\n             return None\n\n        edges = []\n        edge_features = []\n        for bond in mol.GetBonds():\n             edges.append([bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()])\n             bond_type = bond.GetBondTypeAsDouble()\n             bond_feature = [1 if i == bond_type\\\n             else 0 for i in range(4)]\n             edge_features.append(bond_feature)\n\n        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n        edge_attr = torch.tensor(edge_features, dtype=torch.float)\n        x = torch.tensor([atom.GetAtomicNum()\\\n for atom in mol.GetAtoms()], \\\n dtype=torch.float).view(-1, 1)\n\n        num_heavy_atoms = mol.GetNumHeavyAtoms()\n\n        return Data(x=x, edge_index=edge_index,\\\n edge_attr=edge_attr, \\\nqed=torch.tensor([qed], \\\ndtype=torch.float), \\\nnum_heavy_atoms=num_heavy_atoms)\n```", "```py\n        def calculate_loss(self, pred_adj, \\\n   true_adj, qed_pred, qed_true, mu, logvar):\n             adj_loss = F.binary_cross_entropy\\\n   (pred_adj, true_adj)   #1\n\n             qed_loss = F.mse_loss\\\n(qed_pred.view(-1), qed_true.view(-1))     #2\n\n             kl_loss = -0.5 * torch.mean\\\n(torch.sum(1 + logvar - mu.pow(2)\\     #3\n - logvar.exp(), dim=1))\n\n             return adj_loss + qed_loss + kl_loss\n```", "```py\nfrom torch.nn import Parameter\nfrom torch_geometric.nn import MessagePassing\n\n   class HeterogeneousGraphConv(MessagePassing):\ndef __init__(self, in_channels, out_channels, num_relations, bias=True):\n        super(HeterogeneousGraphConv, self).\\\n__init__(aggr='add')       #1\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_relations = num_relations\n\n        self.weight = Parameter(torch.\\\nTensor(num_relations, in_channels, \\\nout_channels))  #2\n        if bias:\n             self.bias = Parameter(torch.Tensor(out_channels))\n        else:\n             self.register_parameter('bias', None)\n\n        self.reset_parameters()\n\n        def reset_parameters(self):\n             torch.nn.init.xavier_uniform_(self.weight)\n             if self.bias is not None:\n                  torch.nn.init.zeros_(self.bias)\n\n        def forward(self, x, edge_index, edge_type):\n\n       return self.propagate\\\n(edge_index, size=(x.size(0), \nx.size(0)), x=x, edge_type=edge_type)  #3\n\n       def message(self, x_j, edge_type, index, size):   #4\n\n            W = self.weight[edge_type]    #5\n            x_j = torch.matmul(x_j.unsqueeze(1), W).squeeze(1)\n\n            return x_j\n\n       def update(self, aggr_out):\n            if self.bias is not None:\n                 aggr_out += self.bias\n            return aggr_out\n```", "```py\n   class VariationalGCEncoder(torch.nn.Module):\n        def __init__(self, input_size, layers, latent_dims, num_relations):\n             super().__init__()\n             self.layer0 = HeterogeneousGraphConv(input_size, \n   layers[0], num_relations)                                     #1\n             self.layer1 = HeterogeneousGraphConv(layers[0], \n   layers[1], num_relations)                                    \n             self.layer2 = HeterogeneousGraphConv(layers[1], \n   latent_dims, num_relations)                                  \n\n        def forward(self, x, edge_index, edge_type):\n             x = F.relu(self.layer0\\\n(x, edge_index, edge_type))              #2\n             x = F.relu(self.layer1\\\n(x, edge_index, edge_type))             \n             mu = self.mu(x, edge_index) \n             logvar = self.logvar(x, edge_index)\n             return mu, logvar\n```", "```py\nclass GraphDecoder(nn.Module):\n        def __init__(self, latent_dim, adjacency_shape, feature_shape):\n        super(GraphDecoder, self).__init__()\n\n        self.dense1 = nn.Linear(latent_dim, 128)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(0.1)\n\n        self.dense2 = nn.Linear(128, 256)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(0.1)\n\n        self.dense3 = nn.Linear(256, 512)\n        self.relu3 = nn.ReLU()\n        self.dropout3 = nn.Dropout(0.1)\n\n        self.adjacency_output = nn.Linear(512,\\\ntorch.prod(torch.tensor(adjacency_shape)).item())\n        self.feature_output = nn.Linear(512,\\\ntorch.prod(torch.tensor(feature_shape)).item())\n\n        def forward(self, z):\n             x = self.dropout1(self.relu1(self.dense1(z)))\n             x = self.dropout2(self.relu2(self.dense2(x)))\n             x = self.dropout3(self.relu3(self.dense3(x)))\n\n             adj = self.adjacency_output(x)   #1\n             adj = adj.view(-1, *self.adjacency_shape)\n             adj = (adj + adj.transpose(-1, -2)) / 2    #2\n             adj = F.softmax(adj, dim=-1)                     #3\n\n             features = self.feature_output(x)   #4\n             features = features.view(-1, *self.feature_shape)\n             features = F.softmax(features, dim=-1)    #5\n\n             return adj, features\n```", "```py\n   import torch\n   import torch.nn as nn\n   import torch.nn.functional as F\n   from torch_geometric.nn import MessagePassing\n\n   class VGAEWithPropertyPrediction(nn.Module):\n        def __init__(self, encoder, decoder, latent_dim):\n             super(VGAEWithPropertyPrediction, self).__init__()\n             self.encoder = encoder\n             self.decoder = decoder\n             self.property_prediction_layer = nn.Linear(latent_dim, 1)\n\n        def reparameterize(self, mu, logvar):\n             std = torch.exp(logvar / 2)\n             eps = torch.randn_like(std)\n             return eps.mul(std).add_(mu)\n\n        def forward(self, data):\n             mu, logvar = self.encoder(data.x, \\\ndata.edge_index, data.edge_attr)\n             z = self.reparameterize(mu, logvar)\n             adj_recon, x_recon = self.decoder(z)\n             qed_pred = self.property_prediction_layer(z)\n             return adj_recon, x_recon, qed_pred, mu, logvar, z\n```", "```py\n   def train(model, optimizer, data, test=False):\n        model.train()\n        optimizer.zero_grad()\n\n        pred_adj, pred_feat, pred_qed, mu, logvar, _ = model(data)\n\n        real_adj = create_adjacency_matrix\\\n(data.edge_index, data.edge_attr, \\\nnum_nodes=NUM_ATOMS)\n        real_x = data.x\n        real_qed = data.qed\n\n        loss = calculate_loss\\\n(pred_adj[0], real_adj, pred_qed, \\\nreal_qed, mu, logvar)   #1\n\n        total_loss = loss\n\n        if not test:\n             total_loss.backward()\n        optimizer.step()\n        return total_loss.item()\n```"]