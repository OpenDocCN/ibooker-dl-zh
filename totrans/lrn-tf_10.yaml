- en: Chapter 10\. Exporting and Serving Models with TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。使用TensorFlow导出和提供模型
- en: In this chapter we will learn how to save and export models by using both simple
    and advanced production-ready methods. For the latter we introduce TensorFlow
    Serving, one of TensorFlow’s most practical tools for creating production environments.
    We start this chapter with a quick overview of two simple ways to save models
    and variables: first by manually saving the weights and reassigning them, and
    then by using the `Saver` class that creates training checkpoints for our variables
    and also exports our model. Finally, we shift to more advanced applications where
    we can deploy our model on a server by using TensorFlow Serving.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何使用简单和高级的生产就绪方法保存和导出模型。对于后者，我们介绍了TensorFlow Serving，这是TensorFlow中最实用的用于创建生产环境的工具之一。我们将从快速概述两种简单的保存模型和变量的方法开始：首先是通过手动保存权重并重新分配它们，然后是使用`Saver`类创建训练检查点以及导出我们的模型。最后，我们将转向更高级的应用程序，通过使用TensorFlow
    Serving在服务器上部署我们的模型。
- en: Saving and Exporting Our Model
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存和导出我们的模型
- en: So far we’ve dealt with how to create, train, and track models with TensorFlow.
    Now we will see how to save a trained model. Saving the current state of our weights
    is crucial for obvious practical reasons—we don’t want to have to retrain our
    model from scratch every time, and we also want a convenient way to share the
    state of our model with others (as in the pretrained models we saw in [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications)).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了如何使用TensorFlow创建、训练和跟踪模型。现在我们将学习如何保存训练好的模型。保存当前权重状态对于明显的实际原因至关重要——我们不想每次都从头开始重新训练模型，我们也希望有一种方便的方式与他人分享我们模型的状态（就像我们在[第7章](ch07.html#tensorflow_abstractions_and_simplifications)中看到的预训练模型一样）。
- en: In this section we go over the basics of saving and exporting. We start with
    a simple way of saving and loading our weights to and from files. Then we will
    see how to use TensorFlow’s `Saver` object to keep serialized model checkpoints
    that include information about both the state of our weights and our constructed
    graph.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论保存和导出的基础知识。我们首先介绍了一种简单的保存和加载权重到文件的方法。然后我们将看到如何使用TensorFlow的`Saver`对象来保持序列化模型检查点，其中包含有关权重状态和构建图的信息。
- en: Assigning Loaded Weights
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分配加载的权重
- en: A naive but practical way to reuse our weights after training is saving them
    to a file, which we can later load to have them reassigned to the model.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练后重复使用权重的一个天真但实用的方法是将它们保存到文件中，稍后可以加载它们并重新分配给模型。
- en: 'Let’s look at some examples. Say we wish to save the weights of the basic softmax
    model used for the MNIST data in [Chapter 2](ch02.html#go_with_the_flow). After
    fetching them from the session, we have the weights represented as a NumPy array,
    and we save them in some format of our choice:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一些例子。假设我们希望保存用于MNIST数据的基本softmax模型的权重，我们从会话中获取它们后，将权重表示为NumPy数组，并以我们选择的某种格式保存它们：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Given that we have the exact same graph constructed, we can then load the file
    and assign the loaded weight values to the corresponding variables by using the
    `.assign()` method within a session:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们构建了完全相同的图，我们可以加载文件并使用会话中的`.assign()`方法将加载的权重值分配给相应的变量：
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we will perform the same procedure, but this time for the CNN model used
    for the MNIST data in [Chapter 4](ch04.html#convolutional_neural_networks). Here
    we have eight different sets of weights: two filter weights and their corresponding
    biases for the convolution layers 1 and 2, and two sets of weights and biases
    for the fully connected layer. We encapsulate the model inside a class so we can
    conveniently keep an updated list of these eight parameters.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将执行相同的过程，但这次是针对[第4章](ch04.html#convolutional_neural_networks)中用于MNIST数据的CNN模型。在这里，我们有八组不同的权重：两个卷积层1和2的滤波器权重及其对应的偏置，以及两组全连接层的权重和偏置。我们将模型封装在一个类中，以便方便地保持这八个参数的更新列表。
- en: 'We also add optional arguments for weights to load:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还为要加载的权重添加了可选参数：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'and a function to assign their values when weights are passed:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以及在传递权重时分配其值的函数：
- en: '[PRE3]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In its entirety:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个过程中：
- en: '[PRE4]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In this example the model was already trained and the weights were saved as
    `cnn_weights`. We load the weights and pass them to our CNN object. When we run
    the model on the test data, it will be using the pretrained weights:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，模型已经训练好，并且权重已保存为`cnn_weights`。我们加载权重并将它们传递给我们的CNN对象。当我们在测试数据上运行模型时，它将使用预训练的权重：
- en: '[PRE5]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: And we obtain high accuracy without the need to retrain.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以获得高准确度，而无需重新训练。
- en: The Saver Class
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Saver类
- en: TensorFlow also has a built-in class we can use for the same purpose as in the
    previous examples, offering additional useful features as we will see shortly.
    This class is referred to as the `Saver` class (already briefly presented in [Chapter 5](ch05.html#text_i)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow还有一个内置的类，我们可以用于与前面的示例相同的目的，提供额外有用的功能，我们很快就会看到。这个类被称为`Saver`类（在[第5章](ch05.html#text_i)中已经简要介绍过）。
- en: '`Saver` adds operations that allow us to save and restore the model’s parameters
    by using binary files called *checkpoint files*, mapping the tensor values to
    the names of the variables. Unlike the method used in the previous section, here
    we don’t have to keep track of our parameters—`Saver` does it automatically for
    us.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '`Saver`添加了操作，允许我们通过使用称为*检查点文件*的二进制文件保存和恢复模型的参数，将张量值映射到变量的名称。与前一节中使用的方法不同，这里我们不必跟踪我们的参数——`Saver`会自动为我们完成。'
- en: Using `Saver` is straightforward. We first create a saver instance by using
    `tf.train.Saver()`, indicating how many recent variable checkpoints we would like
    to keep and optionally the time interval at which to keep them.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`Saver`非常简单。我们首先通过`tf.train.Saver()`创建一个saver实例，指示我们希望保留多少最近的变量检查点，以及可选的保留它们的时间间隔。
- en: 'For example, in the following code we ask that only the seven most recent checkpoints
    will be kept, and in addition we specify that one checkpoint be kept each half
    hour (this can be useful for performance and progression evaluation analysis):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在下面的代码中，我们要求只保留最近的七个检查点，并且另外指定每半小时保留一个检查点（这对于性能和进展评估分析可能很有用）：
- en: '[PRE6]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: If no inputs are given, the default is to keep the last five checkpoints, and
    the `every_n_hours` feature is effectively disabled (it’s set to `10000` by default).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有给出输入，那么默认情况下会保留最后五个检查点，并且`every_n_hours`功能会被有效地禁用（默认设置为`10000`）。
- en: Next we save the checkpoint files by using the `.save()` method of the `saver`
    instance, passing the session argument, the path where the files are to be saved,
    and also the step number (`global_step`), which is automatically concatenated
    to the name of each checkpoint file as an indication of its iteration count. This
    creates multiple checkpoints at different steps while training a model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`saver`实例的`.save()`方法保存检查点文件，传递会话参数、文件保存路径以及步数（`global_step`），它会自动连接到每个检查点文件的名称中，表示迭代次数。在训练模型时，这会创建不同步骤的多个检查点。
- en: 'In this code example, every 50 training iterations a file will be saved in
    the designated directory:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码示例中，每50个训练迭代将在指定目录中保存一个文件：
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'An additional saved file carrying the name *checkpoint* contains the list of
    saved checkpoints, and also the path to the most recent checkpoint:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个保存的文件名为*checkpoint*包含保存的检查点列表，以及最近检查点的路径：
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the following code we use `Saver` to save the state of the weights:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们使用`Saver`保存权重的状态：
- en: '[PRE9]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'And now we simply restore the checkpoint we want for the same graph model by
    using `saver.restore()`, and the weights are automatically assigned to the model:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们只需使用`saver.restore()`为相同的图模型恢复我们想要的检查点，权重会自动分配给模型：
- en: '[PRE10]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Resetting the graph before restoring
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在恢复之前重置图
- en: 'The loaded variables need to be paired with the ones in the current graph,
    and thus should have matching names. If for some reason the names don’t match,
    then an error similar to this might follow:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 加载的变量需要与当前图中的变量配对，因此应该具有匹配的名称。如果由于某种原因名称不匹配，那么可能会出现类似于这样的错误：
- en: '[PRE11]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This can happen if the names were used by some old, irrelevant graph. By using
    the `tf.reset_default_graph()` command to reset the graph, you can solve this
    issue.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果名称被一些旧的、无关紧要的图使用，就会发生这种情况。通过使用`tf.reset_default_graph()`命令重置图，您可以解决这个问题。
- en: So far, in both methods we needed to re-create the graph for the restored parameters
    to be reassigned. `Saver`, however, also allows us to restore the graph without
    having to reconstruct it by generating *.meta* checkpoint files containing all
    the required information about it.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在这两种方法中，我们需要重新创建图以重新分配恢复的参数。然而，`Saver`还允许我们恢复图而无需重建它，通过生成包含有关图的所有必要信息的*.meta*检查点文件。
- en: The information about the graph and how to incorporate the saved weights in
    it (metainformation) is referred to as the `MetaGraphDef`. This information is
    serialized—transformed to a string—using protocol buffers (see [“Serialization
    and Protocol Buffers”](#serial_proto_buff)), and it includes several parts. The
    information about the architecture of the network is kept in `graph_def`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 关于图的信息以及如何将保存的权重合并到其中（元信息）被称为`MetaGraphDef`。这些信息被序列化——转换为一个字符串——使用协议缓冲区（参见[“序列化和协议缓冲区”](#serial_proto_buff)），它包括几个部分。网络架构的信息保存在`graph_def`中。
- en: 'Here is a little sample of textual serialization of the graph information (more
    about serialization follows):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是图信息的文本序列化的一个小样本（更多关于序列化的内容将在后面介绍）：
- en: '[PRE12]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In order to load the saved graph, we use `tf.train.import_meta_graph()`, passing
    the name of the checkpoint file we want (with the *.meta* extension). TensorFlow
    already knows what to do with the restored weights, since this information is
    also kept:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加载保存的图，我们使用`tf.train.import_meta_graph()`，传递我们想要的检查点文件的名称（带有*.meta*扩展名）。TensorFlow已经知道如何处理恢复的权重，因为这些信息也被保存了：
- en: '[PRE13]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Simply importing the graph and restoring the weights, however, is not enough
    and will result in an error. The reason is that importing the model and restoring
    the weights doesn’t give us additional access to the variables used as arguments
    when running the session (`fetches` and keys of `feed_dict`)—the model doesn’t
    know what the inputs and outputs are, what measures we wish to calculate, etc.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，仅仅导入图并恢复权重是不够的，会导致错误。原因是导入模型并恢复权重并不会给我们额外访问在运行会话时使用的变量（`fetches`和`feed_dict`的键）——模型不知道输入和输出是什么，我们希望计算什么度量等等。
- en: One way to solve this problem is by saving them in a collection. A collection
    is a TensorFlow object similar to a dictionary, in which we can keep our graph
    components in an orderly, accessible fashion.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的一种方法是将它们保存在一个集合中。集合是一个类似于字典的 TensorFlow 对象，我们可以以有序、可访问的方式保存我们的图组件。
- en: 'In this example we want to have access to the measure `accuracy` (which we
    wish to fetch) and the feed keys `x` and `y_true`. We add them to a collection
    before saving the model under the name of `train_var`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们希望访问度量`accuracy`（我们希望获取）和 feed 键`x`和`y_true`。我们在将模型保存为`train_var`的名称之前将它们添加到一个集合中：
- en: '[PRE14]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As shown, the `saver.save()` method automatically saves the graph architecture
    together with the weights’ checkpoints. We can also save the graph explicitly
    using `saver.export_meta.graph()`, and then add a collection (passed as the second
    argument):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如所示，`saver.save()`方法会自动保存图结构以及权重的检查点。我们还可以使用`saver.export_meta.graph()`显式保存图，然后添加一个集合（作为第二个参数传递）：
- en: '[PRE15]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we retrieve the graph together with the collection, from which we can extract
    the required variables:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们从集合中检索图，从中可以提取所需的变量：
- en: '[PRE16]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: When defining the graph, think about which variables/operations you would like
    to retrieve once the graph has been saved and restored, such as the accuracy operation
    in the preceding example. In the next section, when we talk about Serving, we’ll
    see that it has built-in functionality for guiding the exported model without
    the need to save the variables as we do here.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义图形时，请考虑一旦图形已保存和恢复，您想要检索哪些变量/操作，例如前面示例中的准确性操作。在下一节中，当我们谈论Serving时，我们将看到它具有内置功能，可以引导导出的模型，而无需像我们在这里做的那样保存变量。
- en: Introduction to TensorFlow Serving
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow Serving简介
- en: TensorFlow Serving, written in C++, is a high-performance serving framework
    with which we can deploy our model in a production setting. It makes our model
    usable for production by enabling client software to access it and pass inputs
    through Serving’s API ([Figure 10-1](#model_linked_to_external_app)). Of course,
    TensorFlow Serving is designed to have seamless integration with TensorFlow models.
    Serving features many optimizations to reduce latency and increase throughput
    of predictions, useful for real-time, large-scale applications. It’s not only
    about accessibility and efficient serving of predictions, but also about flexibility—it’s
    quite common to want to keep a model updated for various reasons, like having
    additional training data for improving the model, making changes to the network
    architecture, and more.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving是用C++编写的高性能服务框架，我们可以在生产环境中部署我们的模型。通过使客户端软件能够访问它并通过Serving的API传递输入，使我们的模型可以用于生产（[图10-1](#model_linked_to_external_app)）。当然，TensorFlow
    Serving旨在与TensorFlow模型无缝集成。Serving具有许多优化功能，可减少延迟并增加预测的吞吐量，适用于实时、大规模应用。这不仅仅是关于预测的可访问性和高效服务，还涉及灵活性——通常希望出于各种原因保持模型更新，例如获得额外的训练数据以改进模型，对网络架构进行更改等。
- en: '![](assets/letf_1001.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_1001.png)'
- en: Figure 10-1\. Serving links our trained model to external applications, allowing
    client software easy access.
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-1。Serving将我们训练好的模型链接到外部应用程序，使客户端软件可以轻松访问。
- en: Overview
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: Say that we run a speech-recognition service and we want to deploy our models
    with TensorFlow Serving. In addition to optimized serving, it is important for
    us to update our models periodically as we obtain more data or experiment with
    new network architectures. In slightly more technical terms, we’d like to have
    the ability to load new models and serve their outputs, and unload old ones, all
    while streamlining model life-cycle management and version policies.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们运行一个语音识别服务，并且我们希望使用TensorFlow Serving部署我们的模型。除了优化服务外，对我们来说定期更新模型也很重要，因为我们获取更多数据或尝试新的网络架构。稍微更技术化一点，我们希望能够加载新模型并提供其输出，卸载旧模型，同时简化模型生命周期管理和版本策略。
- en: In general terms, we can accomplish this with Serving as follows. In Python,
    we define the model and prepare it to be serialized in a way that can be parsed
    by the different modules responsible for loading, serving, and managing versions,
    for example. The core Serving “engine” resides in a C++ module that we will need
    to access only if we wish to control specific tuning and customization of Serving
    behaviors.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，我们可以通过以下方式实现Serving。在Python中，我们定义模型并准备将其序列化，以便可以被负责加载、提供和管理版本的不同模块解析。Serving的核心“引擎”位于一个C++模块中，只有在我们希望控制Serving行为的特定调整和定制时才需要访问它。
- en: 'In a nutshell, this is how Serving’s architecture works ([Figure 10-2](#an_outline_of_the_serving_architecture)):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这就是Serving架构的工作方式（[图10-2](#an_outline_of_the_serving_architecture)）：
- en: A module called `Source` identifies new models to be loaded by monitoring plugged-in
    filesystems, which contain our models and their associated information that we
    exported upon creation. `Source` includes submodules that periodically inspect
    the filesystem and determine the latest relevant model versions.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个名为`Source`的模块通过监视插入的文件系统来识别需要加载的新模型，这些文件系统包含我们在创建时导出的模型及其相关信息。`Source`包括子模块，定期检查文件系统并确定最新相关的模型版本。
- en: When it identifies a new model version, *source* creates a *loader*. The loader
    passes its *servables* (objects that clients use to perform computations such
    as predictions) to a *manager*. The manager handles the full life cycle of servables
    (loading, unloading, and serving) according to a version policy (gradual rollout,
    reverting versions, etc.).
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当它识别到新的模型版本时，*source*会创建一个*loader*。加载器将其*servables*（客户端用于执行计算的对象，如预测）传递给*manager*。根据版本策略（渐进式发布、回滚版本等），管理器处理可服务内容的完整生命周期（加载、卸载和提供）。
- en: Finally, the manager provides an interface for client access to servables.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，管理器提供了一个接口，供客户端访问可服务的内容。
- en: '![](assets/letf_1002.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/letf_1002.png)'
- en: Figure 10-2\. An outline of the Serving architecture.
  id: totrans-68
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图10-2。Serving架构概述。
- en: What’s especially nice about how Serving is built is that it’s designed to be
    flexible and extendable. It supports building various plug-ins to customize system
    behavior, while using the generic builds of other core components.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Serving的设计特别之处在于它具有灵活和可扩展的特性。它支持构建各种插件来定制系统行为，同时使用其他核心组件的通用构建。
- en: In the next section we will build and deploy a TensorFlow model with Serving,
    demonstrating some of its key functionalities and inner workings. In advanced
    applications it is likely that we may have to control for different types of optimizations
    and customization; for example, controlling version policies and more. In this
    chapter we show you how to get up and running with Serving and understand its
    fundamentals, laying the foundations for production-ready deployment.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用Serving构建和部署一个TensorFlow模型，展示一些其关键功能和内部工作原理。在高级应用中，我们可能需要控制不同类型的优化和定制；例如，控制版本策略等。在本章中，我们将向您展示如何开始并理解Serving的基础知识，为生产就绪的部署奠定基础。
- en: Installation
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: Serving requires several installations, including some third-party components.
    The installation can be done from source or using Docker, which we use here to
    get you started quickly. A Docker container bundles together a software application
    with everything needed to run it (for example, code, files, etc.). We also use
    Bazel, Google’s own build tool for building client and server software. In this
    chapter we only briefly touch on the technicalities behind tools such as Bazel
    and Docker. More comprehensive descriptions appear in [the appendix](app01.html#appendix),
    at the end of the book.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Serving需要安装一些组件，包括一些第三方组件。安装可以从源代码或使用Docker进行，我们在这里使用Docker来让您快速开始。Docker容器将软件应用程序与运行所需的一切（例如代码、文件等）捆绑在一起。我们还使用Bazel，谷歌自己的构建工具，用于构建客户端和服务器软件。在本章中，我们只简要介绍了Bazel和Docker等工具背后的技术细节。更全面的描述出现在书末的[附录](app01.html#appendix)中。
- en: Installing Serving
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装Serving
- en: Docker installation instructions can be found in on [the Docker website](https://docs.docker.com/engine/installation/).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Docker安装说明可以在[ Docker网站](https://docs.docker.com/engine/installation/)上找到。
- en: Here, we demonstrate the Docker setup using [Ubuntu](https://docs.docker.com/engine/installation/linux/ubuntu/).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们演示使用[Ubuntu](https://docs.docker.com/engine/installation/linux/ubuntu/)进行Docker设置。
- en: Docker containers are created from a local Docker image, which is built from
    a dockerfile, and encapsulates everything we need (dependency installations, project
    code, etc.). Once we have Docker installed, we need to [download the TensorFlow
    Serving dockerfile](http://bit.ly/2t7ewMb).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Docker容器是从本地Docker镜像创建的，该镜像是从dockerfile构建的，并封装了我们需要的一切（依赖安装、项目代码等）。一旦我们安装了Docker，我们需要[下载TensorFlow
    Serving的dockerfile](http://bit.ly/2t7ewMb)。
- en: This dockerfile contains all of the dependencies needed to build TensorFlow
    Serving.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这个dockerfile包含了构建TensorFlow Serving所需的所有依赖项。
- en: 'First, we produce the image from which we can run containers (this may take
    some time):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们生成镜像，然后可以运行容器（这可能需要一些时间）：
- en: '[PRE17]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now that we’ve got the image created locally on our machine, we can create
    and run a container by using:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在本地机器上创建了镜像，我们可以使用以下命令创建和运行容器：
- en: '[PRE18]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `docker run -it $USER/tensorflow-serving-devel` command would suffice to
    create and run a container, but we make two additions to this command.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`docker run -it $USER/tensorflow-serving-devel`命令足以创建和运行容器，但我们对此命令进行了两次添加。'
- en: First, we add *-v $HOME/home_dir:/docker_dir*, where `-v` (volume) indicates
    a request for a shared filesystem so we have a convenient way to transfer files
    between the Docker container and the host. Here we created the shared folders
    *docker_files* on our host and *host_files* on our Docker container. Another way
    to transfer files is simply by using the command `docker cp foo.txt *mycontainer*:/foo.txt`.
    The second addition is `-p <*host port*>:<*container port*>`, which makes the
    service in the container accessible from anywhere by having the indicated port
    exposed.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们添加*-v $HOME/home_dir:/docker_dir*，其中`-v`（卷）表示请求共享文件系统，这样我们就可以方便地在Docker容器和主机之间传输文件。在这里，我们在主机上创建了共享文件夹*docker_files*，在我们的Docker容器上创建了*host_files*。另一种传输文件的方法是简单地使用命令`docker
    cp foo.txt *mycontainer*:/foo.txt`。第二个添加是`-p <*host port*>:<*container port*>`，这使得容器中的服务可以通过指定的端口暴露在任何地方。
- en: Once we enter our `run` command, a container will be created and started, and
    a terminal will be opened. We can have a look at our container’s status by using
    the command `docker ps -a` (outside the Docker terminal). Note that each time
    we use the `docker run` command, we create another container; to enter the terminal
    of an existing container, we need to use `docker exec -it <*container id*> bash`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们输入我们的`run`命令，一个容器将被创建和启动，并且一个终端将被打开。我们可以使用命令`docker ps -a`（在Docker终端之外）查看我们容器的状态。请注意，每次使用`docker
    run`命令时，我们都会创建另一个容器；要进入现有容器的终端，我们需要使用`docker exec -it <*container id*> bash`。
- en: 'Finally, within the opened terminal we clone and configure TensorFlow Serving:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在打开的终端中，我们克隆并配置TensorFlow Serving：
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: And that’s it; we’re ready to go!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样，我们准备好了！
- en: Building and Exporting
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建和导出
- en: Now that Serving is cloned and operational, we can start exploring its features
    and how to use it. The cloned TensorFlow Serving libraries are organized in a
    Bazel architecture. The source code Bazel builds upon is organized in a workspace
    directory, inside nested hierarchies of packages that group related source files
    together. Each package has a *BUILD* file, specifying the output to be built from
    the files inside that package.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Serving已经克隆并运行，我们可以开始探索其功能和如何使用它。克隆的TensorFlow Serving库是按照Bazel架构组织的。Bazel构建的源代码组织在一个工作区目录中，里面有一系列分组相关源文件的包。每个包都有一个*BUILD*文件，指定从该包内的文件构建的输出。
- en: The workspace in our cloned library is located in the */serving* folder, containing
    the *WORKSPACE* text file and the */tensorflow_serving* package, which we will
    return to later.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们克隆库中的工作区位于*/serving*文件夹中，包含*WORKSPACE*文本文件和*/tensorflow_serving*包，稍后我们将返回到这里。
- en: We now turn to look at the Python script that handles the training and exportation
    of the model, and see how to export our model in a manner ready for serving.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向查看处理训练和导出模型的Python脚本，并看看如何以一种适合进行Serving的方式导出我们的模型。
- en: Exporting our model
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 导出我们的模型
- en: As when we used the `Saver` class, our trained model will be serialized and
    exported to two files: one that contains information about our variables, and
    another that holds information about our graph and other metadata. As we shall
    see shortly, Serving requires a specific serialization format and metadata, so
    we cannot simply use the `Saver` class, as we saw at the beginning of this chapter.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们使用`Saver`类时一样，我们训练的模型将被序列化并导出到两个文件中：一个包含有关变量的信息，另一个包含有关图形和其他元数据的信息。正如我们很快将看到的，Serving需要特定的序列化格式和元数据，因此我们不能简单地使用`Saver`类，就像我们在本章开头看到的那样。
- en: 'The steps we are going to take are as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要采取的步骤如下：
- en: Define our model as in previous chapters.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像前几章一样定义我们的模型。
- en: Create a model builder instance.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个模型构建器实例。
- en: Have our metadata (model, method, inputs and outputs, etc.) defined in the builder
    in a serialized format (this is referred to as `SignatureDef`).
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建器中定义我们的元数据（模型、方法、输入和输出等）以序列化格式（称为 `SignatureDef`）。
- en: Save our model by using the builder.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用构建器保存我们的模型。
- en: 'We start by creating a builder instance using Serving’s `SavedModelBuilder`
    module, passing the location to which we want our files to be exported (the directory
    will be created if it does not exist). `SavedModelBuilder` exports serialized
    files representing our model in the required format:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过使用 Serving 的 `SavedModelBuilder` 模块创建一个构建器实例，传递我们希望将文件导出到的位置（如果目录不存在，则将创建）。`SavedModelBuilder`
    导出表示我们的模型的序列化文件，格式如下所需：
- en: '[PRE20]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The serialized model files we need will be contained in a directory whose name
    will specify the model and its version:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的序列化模型文件将包含在一个目录中，该目录的名称将指定模型及其版本：
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This way, each version will be exported to a distinct subdirectory with its
    corresponding path.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，每个版本将被导出到一个具有相应路径的不同子目录中。
- en: Note that the `export_path_base` is obtained as input from the command line
    with `sys.argv`, and the version is kept as a flag (presented in the previous
    chapter). Flag parsing is handled by `tf.app.run()`, as we will see shortly.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`export_path_base` 是从命令行输入的，使用 `sys.argv` 获取，版本作为标志保留（在上一章中介绍）。标志解析由 `tf.app.run()`
    处理，我们很快就会看到。
- en: Next, we want to define the input (shape of the input tensor of the graph) and
    output (tensor of the prediction) signatures. In the first part of this chapter
    we used TensorFlow collection objects to specify the relation between input and
    output data and their corresponding placeholders, and also operations for computing
    predictions and accuracy. Here, signatures serve a somewhat analogous purpose.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们要定义输入（图的输入张量的形状）和输出（预测张量）签名。在本章的第一部分中，我们使用 TensorFlow 集合对象来指定输入和输出数据之间的关系及其相应的占位符，以及用于计算预测和准确性的操作。在这里，签名起到了类似的作用。
- en: 'We use the builder instance we created to add both the variables and meta graph
    information, using the `SavedModelBuilder.add_meta_graph_and_variables()` method:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用创建的构建器实例添加变量和元图信息，使用 `SavedModelBuilder.add_meta_graph_and_variables()`
    方法：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We need to pass four arguments: the session, tags (to “serve” or “train”), the
    signature map, and some initializations.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要传递四个参数：会话、标签（用于“服务”或“训练”）、签名映射和一些初始化。
- en: 'We pass a dictionary with the prediction and classification signatures.  We
    start with the prediction signature, which again can be thought of as analogical
    to specifying and saving a prediction op in a TensorFlow collection as we saw
    earlier:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们传递一个包含预测和分类签名的字典。我们从预测签名开始，可以将其视为在 TensorFlow 集合中指定和保存预测操作，就像我们之前看到的那样：
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`images` and `scores` here are arbitrary names that we will use to refer to
    our `x` and `y` Tensors later. The images and scores are encoded into the required
    format by using the following commands:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 `images` 和 `scores` 是我们稍后将用来引用我们的 `x` 和 `y` 张量的任意名称。通过以下命令将图像和分数编码为所需格式：
- en: '[PRE24]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Similar to the prediction signature, we have the classification signature,
    where we input the information about the scores (the probability values of the
    top `k` classes) and the corresponding classes:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 与预测签名类似，我们有分类签名，其中我们输入关于分数（前 `k` 个类的概率值）和相应类的信息：
- en: '[PRE25]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, we save our model by using the `save()` command:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 `save()` 命令保存我们的模型：
- en: '[PRE27]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This, in a nutshell, wraps all the parts together in a format ready to be serialized
    and exported upon execution of the script, as we shall see immediately.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，将所有部分整合在一起，以准备在脚本执行时序列化和导出，我们将立即看到。
- en: 'Here is the final code for our main Python model script, including our model
    (the CNN model from [Chapter 4](ch04.html#convolutional_neural_networks)):'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们主要的 Python 模型脚本的最终代码，包括我们的模型（来自[第四章](ch04.html#convolutional_neural_networks)的
    CNN 模型）：
- en: '[PRE28]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `tf.app.run()` command gives us a nice wrapper that handles parsing command-line
    arguments.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.app.run()` 命令为我们提供了一个很好的包装器，用于处理解析命令行参数。'
- en: In the final part of our introduction to Serving, we use Bazel for the actual
    exporting and deployment of our model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们介绍 Serving 的最后部分中，我们使用 Bazel 实际导出和部署我们的模型。
- en: Most Bazel *BUILD* files consist only of declarations of build rules specifying
    the relationship between inputs and outputs, and the steps to build the outputs.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 Bazel *BUILD* 文件仅包含构建规则的声明，指定输入和输出之间的关系，以及构建输出的步骤。
- en: 'For instance, in this *BUILD* file we have a Python rule `py_binary` to build
    executable programs. Here we have three attributes, `name` for the name of the
    rule, `srcs` for the list of files that are processed to create the target (our
    Python script), and `deps` for the list of other libraries to be linked into the
    binary target:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在这个 *BUILD* 文件中，我们有一个 Python 规则 `py_binary` 用于构建可执行程序。这里有三个属性，`name` 用于规则的名称，`srcs`
    用于处理以创建目标（我们的 Python 脚本）的文件列表，`deps` 用于链接到二进制目标中的其他库的列表：
- en: '[PRE29]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next we run and export the model by using Bazel, training with 1,000 iterations
    and exporting the first version of the model:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 Bazel 运行和导出模型，进行 1,000 次迭代训练并导出模型的第一个版本：
- en: '[PRE30]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To train the second version of the model, we just use:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练模型的第二个版本，我们只需使用：
- en: '[PRE31]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In the designated subdirectory we will find two files, *saved_model.pb* and
    *variables*, that contain the serialized information about our graph (including
    metadata) and its variables, respectively. In the next lines we load the exported
    model with the standard TensorFlow model server:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在指定的子目录中，我们将找到两个文件，*saved_model.pb* 和 *variables*，它们包含有关我们的图（包括元数据）和其变量的序列化信息。在接下来的行中，我们使用标准的
    TensorFlow 模型服务器加载导出的模型：
- en: '[PRE32]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, our model is now served and ready for action at `localhost:8000`. We
    can test the server with a simple client utility, `mnist_client`:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的模型现在已经被提供并准备在 `localhost:8000` 上运行。我们可以使用一个简单的客户端实用程序 `mnist_client` 来测试服务器：
- en: '[PRE33]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Summary
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter dealt with how to save, export, and serve models, from simply saving
    and reassigning of weights using the built-in `Saver` utility to an advanced model-deployment
    mechanism for production. The last part of this chapter touched on TensorFlow
    Serving, a great tool for making our models commercial-ready with dynamic version
    control. Serving is a rich utility with many functionalities, and we strongly
    recommend that readers who are interested in mastering it seek out more in-depth
    technical material online.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了如何保存、导出和提供模型，从简单保存和重新分配权重使用内置的“Saver”实用程序到用于生产的高级模型部署机制。本章的最后部分涉及TensorFlow
    Serving，这是一个非常好的工具，可以通过动态版本控制使我们的模型商业化准备就绪。Serving是一个功能丰富的实用程序，具有许多功能，我们强烈建议对掌握它感兴趣的读者在网上寻找更深入的技术资料。
