- en: Chapter 10\. Exporting and Serving Models with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we will learn how to save and export models by using both simple
    and advanced production-ready methods. For the latter we introduce TensorFlow
    Serving, one of TensorFlow’s most practical tools for creating production environments.
    We start this chapter with a quick overview of two simple ways to save models
    and variables: first by manually saving the weights and reassigning them, and
    then by using the `Saver` class that creates training checkpoints for our variables
    and also exports our model. Finally, we shift to more advanced applications where
    we can deploy our model on a server by using TensorFlow Serving.
  prefs: []
  type: TYPE_NORMAL
- en: Saving and Exporting Our Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we’ve dealt with how to create, train, and track models with TensorFlow.
    Now we will see how to save a trained model. Saving the current state of our weights
    is crucial for obvious practical reasons—we don’t want to have to retrain our
    model from scratch every time, and we also want a convenient way to share the
    state of our model with others (as in the pretrained models we saw in [Chapter 7](ch07.html#tensorflow_abstractions_and_simplifications)).
  prefs: []
  type: TYPE_NORMAL
- en: In this section we go over the basics of saving and exporting. We start with
    a simple way of saving and loading our weights to and from files. Then we will
    see how to use TensorFlow’s `Saver` object to keep serialized model checkpoints
    that include information about both the state of our weights and our constructed
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: Assigning Loaded Weights
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A naive but practical way to reuse our weights after training is saving them
    to a file, which we can later load to have them reassigned to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at some examples. Say we wish to save the weights of the basic softmax
    model used for the MNIST data in [Chapter 2](ch02.html#go_with_the_flow). After
    fetching them from the session, we have the weights represented as a NumPy array,
    and we save them in some format of our choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Given that we have the exact same graph constructed, we can then load the file
    and assign the loaded weight values to the corresponding variables by using the
    `.assign()` method within a session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will perform the same procedure, but this time for the CNN model used
    for the MNIST data in [Chapter 4](ch04.html#convolutional_neural_networks). Here
    we have eight different sets of weights: two filter weights and their corresponding
    biases for the convolution layers 1 and 2, and two sets of weights and biases
    for the fully connected layer. We encapsulate the model inside a class so we can
    conveniently keep an updated list of these eight parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We also add optional arguments for weights to load:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'and a function to assign their values when weights are passed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In its entirety:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example the model was already trained and the weights were saved as
    `cnn_weights`. We load the weights and pass them to our CNN object. When we run
    the model on the test data, it will be using the pretrained weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: And we obtain high accuracy without the need to retrain.
  prefs: []
  type: TYPE_NORMAL
- en: The Saver Class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorFlow also has a built-in class we can use for the same purpose as in the
    previous examples, offering additional useful features as we will see shortly.
    This class is referred to as the `Saver` class (already briefly presented in [Chapter 5](ch05.html#text_i)).
  prefs: []
  type: TYPE_NORMAL
- en: '`Saver` adds operations that allow us to save and restore the model’s parameters
    by using binary files called *checkpoint files*, mapping the tensor values to
    the names of the variables. Unlike the method used in the previous section, here
    we don’t have to keep track of our parameters—`Saver` does it automatically for
    us.'
  prefs: []
  type: TYPE_NORMAL
- en: Using `Saver` is straightforward. We first create a saver instance by using
    `tf.train.Saver()`, indicating how many recent variable checkpoints we would like
    to keep and optionally the time interval at which to keep them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the following code we ask that only the seven most recent checkpoints
    will be kept, and in addition we specify that one checkpoint be kept each half
    hour (this can be useful for performance and progression evaluation analysis):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: If no inputs are given, the default is to keep the last five checkpoints, and
    the `every_n_hours` feature is effectively disabled (it’s set to `10000` by default).
  prefs: []
  type: TYPE_NORMAL
- en: Next we save the checkpoint files by using the `.save()` method of the `saver`
    instance, passing the session argument, the path where the files are to be saved,
    and also the step number (`global_step`), which is automatically concatenated
    to the name of each checkpoint file as an indication of its iteration count. This
    creates multiple checkpoints at different steps while training a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this code example, every 50 training iterations a file will be saved in
    the designated directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'An additional saved file carrying the name *checkpoint* contains the list of
    saved checkpoints, and also the path to the most recent checkpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code we use `Saver` to save the state of the weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we simply restore the checkpoint we want for the same graph model by
    using `saver.restore()`, and the weights are automatically assigned to the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Resetting the graph before restoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The loaded variables need to be paired with the ones in the current graph,
    and thus should have matching names. If for some reason the names don’t match,
    then an error similar to this might follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This can happen if the names were used by some old, irrelevant graph. By using
    the `tf.reset_default_graph()` command to reset the graph, you can solve this
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: So far, in both methods we needed to re-create the graph for the restored parameters
    to be reassigned. `Saver`, however, also allows us to restore the graph without
    having to reconstruct it by generating *.meta* checkpoint files containing all
    the required information about it.
  prefs: []
  type: TYPE_NORMAL
- en: The information about the graph and how to incorporate the saved weights in
    it (metainformation) is referred to as the `MetaGraphDef`. This information is
    serialized—transformed to a string—using protocol buffers (see [“Serialization
    and Protocol Buffers”](#serial_proto_buff)), and it includes several parts. The
    information about the architecture of the network is kept in `graph_def`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a little sample of textual serialization of the graph information (more
    about serialization follows):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to load the saved graph, we use `tf.train.import_meta_graph()`, passing
    the name of the checkpoint file we want (with the *.meta* extension). TensorFlow
    already knows what to do with the restored weights, since this information is
    also kept:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Simply importing the graph and restoring the weights, however, is not enough
    and will result in an error. The reason is that importing the model and restoring
    the weights doesn’t give us additional access to the variables used as arguments
    when running the session (`fetches` and keys of `feed_dict`)—the model doesn’t
    know what the inputs and outputs are, what measures we wish to calculate, etc.
  prefs: []
  type: TYPE_NORMAL
- en: One way to solve this problem is by saving them in a collection. A collection
    is a TensorFlow object similar to a dictionary, in which we can keep our graph
    components in an orderly, accessible fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example we want to have access to the measure `accuracy` (which we
    wish to fetch) and the feed keys `x` and `y_true`. We add them to a collection
    before saving the model under the name of `train_var`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown, the `saver.save()` method automatically saves the graph architecture
    together with the weights’ checkpoints. We can also save the graph explicitly
    using `saver.export_meta.graph()`, and then add a collection (passed as the second
    argument):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we retrieve the graph together with the collection, from which we can extract
    the required variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: When defining the graph, think about which variables/operations you would like
    to retrieve once the graph has been saved and restored, such as the accuracy operation
    in the preceding example. In the next section, when we talk about Serving, we’ll
    see that it has built-in functionality for guiding the exported model without
    the need to save the variables as we do here.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to TensorFlow Serving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow Serving, written in C++, is a high-performance serving framework
    with which we can deploy our model in a production setting. It makes our model
    usable for production by enabling client software to access it and pass inputs
    through Serving’s API ([Figure 10-1](#model_linked_to_external_app)). Of course,
    TensorFlow Serving is designed to have seamless integration with TensorFlow models.
    Serving features many optimizations to reduce latency and increase throughput
    of predictions, useful for real-time, large-scale applications. It’s not only
    about accessibility and efficient serving of predictions, but also about flexibility—it’s
    quite common to want to keep a model updated for various reasons, like having
    additional training data for improving the model, making changes to the network
    architecture, and more.
  prefs: []
  type: TYPE_NORMAL
- en: '![](assets/letf_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. Serving links our trained model to external applications, allowing
    client software easy access.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Say that we run a speech-recognition service and we want to deploy our models
    with TensorFlow Serving. In addition to optimized serving, it is important for
    us to update our models periodically as we obtain more data or experiment with
    new network architectures. In slightly more technical terms, we’d like to have
    the ability to load new models and serve their outputs, and unload old ones, all
    while streamlining model life-cycle management and version policies.
  prefs: []
  type: TYPE_NORMAL
- en: In general terms, we can accomplish this with Serving as follows. In Python,
    we define the model and prepare it to be serialized in a way that can be parsed
    by the different modules responsible for loading, serving, and managing versions,
    for example. The core Serving “engine” resides in a C++ module that we will need
    to access only if we wish to control specific tuning and customization of Serving
    behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, this is how Serving’s architecture works ([Figure 10-2](#an_outline_of_the_serving_architecture)):'
  prefs: []
  type: TYPE_NORMAL
- en: A module called `Source` identifies new models to be loaded by monitoring plugged-in
    filesystems, which contain our models and their associated information that we
    exported upon creation. `Source` includes submodules that periodically inspect
    the filesystem and determine the latest relevant model versions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When it identifies a new model version, *source* creates a *loader*. The loader
    passes its *servables* (objects that clients use to perform computations such
    as predictions) to a *manager*. The manager handles the full life cycle of servables
    (loading, unloading, and serving) according to a version policy (gradual rollout,
    reverting versions, etc.).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the manager provides an interface for client access to servables.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](assets/letf_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. An outline of the Serving architecture.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: What’s especially nice about how Serving is built is that it’s designed to be
    flexible and extendable. It supports building various plug-ins to customize system
    behavior, while using the generic builds of other core components.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section we will build and deploy a TensorFlow model with Serving,
    demonstrating some of its key functionalities and inner workings. In advanced
    applications it is likely that we may have to control for different types of optimizations
    and customization; for example, controlling version policies and more. In this
    chapter we show you how to get up and running with Serving and understand its
    fundamentals, laying the foundations for production-ready deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Installation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Serving requires several installations, including some third-party components.
    The installation can be done from source or using Docker, which we use here to
    get you started quickly. A Docker container bundles together a software application
    with everything needed to run it (for example, code, files, etc.). We also use
    Bazel, Google’s own build tool for building client and server software. In this
    chapter we only briefly touch on the technicalities behind tools such as Bazel
    and Docker. More comprehensive descriptions appear in [the appendix](app01.html#appendix),
    at the end of the book.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Serving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Docker installation instructions can be found in on [the Docker website](https://docs.docker.com/engine/installation/).
  prefs: []
  type: TYPE_NORMAL
- en: Here, we demonstrate the Docker setup using [Ubuntu](https://docs.docker.com/engine/installation/linux/ubuntu/).
  prefs: []
  type: TYPE_NORMAL
- en: Docker containers are created from a local Docker image, which is built from
    a dockerfile, and encapsulates everything we need (dependency installations, project
    code, etc.). Once we have Docker installed, we need to [download the TensorFlow
    Serving dockerfile](http://bit.ly/2t7ewMb).
  prefs: []
  type: TYPE_NORMAL
- en: This dockerfile contains all of the dependencies needed to build TensorFlow
    Serving.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we produce the image from which we can run containers (this may take
    some time):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we’ve got the image created locally on our machine, we can create
    and run a container by using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The `docker run -it $USER/tensorflow-serving-devel` command would suffice to
    create and run a container, but we make two additions to this command.
  prefs: []
  type: TYPE_NORMAL
- en: First, we add *-v $HOME/home_dir:/docker_dir*, where `-v` (volume) indicates
    a request for a shared filesystem so we have a convenient way to transfer files
    between the Docker container and the host. Here we created the shared folders
    *docker_files* on our host and *host_files* on our Docker container. Another way
    to transfer files is simply by using the command `docker cp foo.txt *mycontainer*:/foo.txt`.
    The second addition is `-p <*host port*>:<*container port*>`, which makes the
    service in the container accessible from anywhere by having the indicated port
    exposed.
  prefs: []
  type: TYPE_NORMAL
- en: Once we enter our `run` command, a container will be created and started, and
    a terminal will be opened. We can have a look at our container’s status by using
    the command `docker ps -a` (outside the Docker terminal). Note that each time
    we use the `docker run` command, we create another container; to enter the terminal
    of an existing container, we need to use `docker exec -it <*container id*> bash`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, within the opened terminal we clone and configure TensorFlow Serving:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it; we’re ready to go!
  prefs: []
  type: TYPE_NORMAL
- en: Building and Exporting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that Serving is cloned and operational, we can start exploring its features
    and how to use it. The cloned TensorFlow Serving libraries are organized in a
    Bazel architecture. The source code Bazel builds upon is organized in a workspace
    directory, inside nested hierarchies of packages that group related source files
    together. Each package has a *BUILD* file, specifying the output to be built from
    the files inside that package.
  prefs: []
  type: TYPE_NORMAL
- en: The workspace in our cloned library is located in the */serving* folder, containing
    the *WORKSPACE* text file and the */tensorflow_serving* package, which we will
    return to later.
  prefs: []
  type: TYPE_NORMAL
- en: We now turn to look at the Python script that handles the training and exportation
    of the model, and see how to export our model in a manner ready for serving.
  prefs: []
  type: TYPE_NORMAL
- en: Exporting our model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As when we used the `Saver` class, our trained model will be serialized and
    exported to two files: one that contains information about our variables, and
    another that holds information about our graph and other metadata. As we shall
    see shortly, Serving requires a specific serialization format and metadata, so
    we cannot simply use the `Saver` class, as we saw at the beginning of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps we are going to take are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Define our model as in previous chapters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a model builder instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Have our metadata (model, method, inputs and outputs, etc.) defined in the builder
    in a serialized format (this is referred to as `SignatureDef`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Save our model by using the builder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We start by creating a builder instance using Serving’s `SavedModelBuilder`
    module, passing the location to which we want our files to be exported (the directory
    will be created if it does not exist). `SavedModelBuilder` exports serialized
    files representing our model in the required format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The serialized model files we need will be contained in a directory whose name
    will specify the model and its version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This way, each version will be exported to a distinct subdirectory with its
    corresponding path.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `export_path_base` is obtained as input from the command line
    with `sys.argv`, and the version is kept as a flag (presented in the previous
    chapter). Flag parsing is handled by `tf.app.run()`, as we will see shortly.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we want to define the input (shape of the input tensor of the graph) and
    output (tensor of the prediction) signatures. In the first part of this chapter
    we used TensorFlow collection objects to specify the relation between input and
    output data and their corresponding placeholders, and also operations for computing
    predictions and accuracy. Here, signatures serve a somewhat analogous purpose.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the builder instance we created to add both the variables and meta graph
    information, using the `SavedModelBuilder.add_meta_graph_and_variables()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to pass four arguments: the session, tags (to “serve” or “train”), the
    signature map, and some initializations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We pass a dictionary with the prediction and classification signatures.  We
    start with the prediction signature, which again can be thought of as analogical
    to specifying and saving a prediction op in a TensorFlow collection as we saw
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`images` and `scores` here are arbitrary names that we will use to refer to
    our `x` and `y` Tensors later. The images and scores are encoded into the required
    format by using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to the prediction signature, we have the classification signature,
    where we input the information about the scores (the probability values of the
    top `k` classes) and the corresponding classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we save our model by using the `save()` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This, in a nutshell, wraps all the parts together in a format ready to be serialized
    and exported upon execution of the script, as we shall see immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the final code for our main Python model script, including our model
    (the CNN model from [Chapter 4](ch04.html#convolutional_neural_networks)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The `tf.app.run()` command gives us a nice wrapper that handles parsing command-line
    arguments.
  prefs: []
  type: TYPE_NORMAL
- en: In the final part of our introduction to Serving, we use Bazel for the actual
    exporting and deployment of our model.
  prefs: []
  type: TYPE_NORMAL
- en: Most Bazel *BUILD* files consist only of declarations of build rules specifying
    the relationship between inputs and outputs, and the steps to build the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, in this *BUILD* file we have a Python rule `py_binary` to build
    executable programs. Here we have three attributes, `name` for the name of the
    rule, `srcs` for the list of files that are processed to create the target (our
    Python script), and `deps` for the list of other libraries to be linked into the
    binary target:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we run and export the model by using Bazel, training with 1,000 iterations
    and exporting the first version of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the second version of the model, we just use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'In the designated subdirectory we will find two files, *saved_model.pb* and
    *variables*, that contain the serialized information about our graph (including
    metadata) and its variables, respectively. In the next lines we load the exported
    model with the standard TensorFlow model server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, our model is now served and ready for action at `localhost:8000`. We
    can test the server with a simple client utility, `mnist_client`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter dealt with how to save, export, and serve models, from simply saving
    and reassigning of weights using the built-in `Saver` utility to an advanced model-deployment
    mechanism for production. The last part of this chapter touched on TensorFlow
    Serving, a great tool for making our models commercial-ready with dynamic version
    control. Serving is a rich utility with many functionalities, and we strongly
    recommend that readers who are interested in mastering it seek out more in-depth
    technical material online.
  prefs: []
  type: TYPE_NORMAL
