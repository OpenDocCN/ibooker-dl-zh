- en: Chapter 11\. Representation Learning and Embeddings
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章\. 表示学习和嵌入
- en: In the previous chapter, we learned how we can interface language models with
    external tools, including data stores. External data can be present in the form
    of text files, database tables, and knowledge graphs. Data can span a wide variety
    of content types, from proprietary domain-specific knowledge bases to intermediate
    results and outputs generated by LLMs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何将语言模型与外部工具接口，包括数据存储。外部数据可以以文本文件、数据库表和知识图谱的形式存在。数据可以跨越广泛的内容类型，从专有领域的知识库到LLM生成的中间结果和输出。
- en: If the data are structured, for example residing in a relational database, the
    language model can issue a SQL query to retrieve the data it needs. But what if
    the data are present in unstructured form?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据是有结构的，例如存储在关系数据库中，语言模型可以发出SQL查询以检索所需的数据。但如果是非结构化形式的数据呢？
- en: 'One way to retrieve data from unstructured text datasets is to search by keywords
    or use regular expressions. For the Apple CFO example in the previous chapter,
    we can retrieve text containing CFO mentions from a corpus containing financial
    disclosures, hoping that it will contain the join date or tenure information.
    For instance, you can use the regex:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从非结构化文本数据集中检索数据的一种方法是通过关键词搜索或使用正则表达式。对于上一章中提到的苹果公司首席财务官（CFO）的例子，我们可以从包含财务披露的语料库中检索包含CFO提及的文本，希望其中包含入职日期或任期信息。例如，你可以使用以下正则表达式：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Keyword search is limited in its effectiveness. There are a very large number
    of ways to express CFO join date or tenure in a corpus, if it is present at all.
    Trying to use a catch-all regex like the above could result in a large proportion
    of false positives.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 关键词搜索在有效性方面有限。如果语料库中存在CFO的入职日期或任期，那么表达这些信息的方式有非常多种。尝试使用上述通用的正则表达式可能会导致大量误报。
- en: Therefore, we need to move beyond keyword search. Over the last few decades,
    the field of information retrieval has developed several methods like BM25 that
    have shaped search systems. We will learn more about these methods in [Chapter 12](ch12.html#ch12).
    In the LLM era, embedding-based search systems are fast becoming the standard
    way of implementing search.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要超越关键词搜索。在过去的几十年里，信息检索领域已经发展出多种方法，如BM25，这些方法塑造了搜索系统。我们将在[第12章](ch12.html#ch12)中了解更多关于这些方法的内容。在LLM时代，基于嵌入的搜索系统正迅速成为实现搜索的标准方式。
- en: In this chapter, we will learn how embeddings work. We will explore the concept
    of semantic similarity and examine various similarity measures. We will learn
    how to use popular embedding models and evaluate their performance. We will also
    show how to fine-tune embedding models to suit specific use cases and domains.
    We will show how to interpret these embeddings using sparse autoencoders (SAEs).
    Finally, we will discuss techniques for optimizing embeddings to reduce storage
    requirements and computational overhead.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习嵌入是如何工作的。我们将探讨语义相似性的概念，并检查各种相似度度量。我们将学习如何使用流行的嵌入模型并评估它们的性能。我们还将展示如何微调嵌入模型以适应特定的用例和领域。我们将展示如何使用稀疏自动编码器（SAEs）来解释这些嵌入。最后，我们将讨论优化嵌入以减少存储需求和计算开销的技术。
- en: Introduction to Embeddings
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌入简介
- en: Representation learning is a subfield of machine learning that deals with learning
    to represent data in a way that captures its meaningful features, often in a low
    dimensional space. In the context of NLP, this involves transforming textual units
    like words, sentences, or paragraphs into vector form, called embeddings. Embeddings
    capture semantic (meaning-related) and pragmatic (social context-related) features
    of the input.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 表示学习是机器学习的一个子领域，它涉及学习以捕捉数据的有意义特征的方式来表示数据，通常是在低维空间中。在NLP的背景下，这涉及到将文本单元（如单词、句子或段落）转换为向量形式，称为嵌入。嵌入捕捉输入的语义（与意义相关）和语用（与社会语境相关）特征。
- en: Embeddings can be generated using both open source libraries and paywalled APIs.
    [Sentence Transformers](https://oreil.ly/4OSVd) is a very well-known open source
    library for generating embeddings, and it provides access to embedding models
    that performs competitively with respect to proprietary ones.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入可以使用开源库和付费API生成。[Sentence Transformers](https://oreil.ly/4OSVd)是一个非常著名的开源库，用于生成嵌入，它提供了与专有模型竞争的嵌入模型。
- en: 'Let’s generate embeddings using the `Sentence Transformers` library:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`Sentence Transformers`库生成嵌入：
- en: '[PRE1]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Output:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For this model, the embedding size is 768, which means each vector has 768 dimensions.
    The sequence length of this particular model is 512, which means the input text
    is restricted to 512 tokens, beyond which it will be truncated. The embedding
    vector is made up of floating-point numbers, which by themselves are not interpretable.
    We will discuss techniques for interpreting embeddings later in this chapter.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个模型，嵌入大小是768，这意味着每个向量有768个维度。这个特定模型的序列长度是512，这意味着输入文本被限制在512个标记内，超出这个范围将被截断。嵌入向量由浮点数组成，这些浮点数本身是不可解释的。我们将在本章后面讨论解释嵌入的技术。
- en: 'Most embedding models used today are based on encoder-only language models,
    which we introduced in [Chapter 4](ch04.html#chapter_transformer-architecture).
    The underlying models are BERT, RoBERTa, MPNet, etc., and are typically fine-tuned
    on paraphrasing/question-answering/natural language inference datasets. Let’s
    see how to derive embeddings from these types of models (which is what the `sentence_transformers.encode()`
    function does under the hood):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 目前大多数使用的嵌入模型都是基于仅编码器语言模型，我们已在[第4章](ch04.html#chapter_transformer-architecture)中介绍过。这些底层模型包括BERT、RoBERTa、MPNet等，通常在释义/问答/自然语言推理数据集上进行微调。让我们看看如何从这些类型的模型中推导出嵌入（这正是`sentence_transformers.encode()`函数在幕后所做的事情）：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In this example, the embedding is drawn from the [CLS] token of the last layer
    of the DistilBERT model. Other ways of extracting embeddings from models include:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，嵌入是从DistilBERT模型的最后一层的[CLS]标记中提取的。从模型中提取嵌入的其他方法包括：
- en: Mean pooling, where the average is taken across all token outputs in the sequence
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值池化，其中取序列中所有标记输出的平均值
- en: Max pooling, where the maximum value in each dimension across all tokens is
    taken
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大池化，其中取所有标记在每个维度上的最大值
- en: Weighted mean, where more weight is given to the last few tokens
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权均值，其中给予最后几个标记更多的权重
- en: Last token, where the embedding is just the encoder output of the last token
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个标记，其中嵌入只是最后一个标记的编码器输出
- en: Tip
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Whether the last token (or the first token) contains good representations of
    the entire sequence depends a lot on the pre-training and the fine-tuning objective.
    BERT’s pre-training objective (next-sentence prediction) ensures that the [CLS]
    token is much richer in representation than, say, RoBERTa, which doesn’t use the
    next-sentence prediction objective and thus its <s> start sequence token isn’t
    as informative.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个标记（或第一个标记）是否包含整个序列的良好表示，很大程度上取决于预训练和微调目标。BERT的预训练目标（下一句预测）确保了[CLS]标记比，比如说，不使用下一句预测目标的RoBERTa具有更丰富的表示，因此其<s>起始序列标记的信息量并不那么丰富。
- en: Recently, decoder-based embedding models have started gaining prominence, like
    the [SGPT family of models](https://oreil.ly/AztT9). OpenAI exposes a single embedding
    endpoint for both search and similarity. OpenAI embeddings have a much larger
    maximum sequence length (8,192 tokens), and a much larger dimension size (1,536–3,072).
    Cohere and Jina are examples of other embedding providers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于解码器的嵌入模型开始受到重视，如[SGPT模型系列](https://oreil.ly/AztT9)。OpenAI为搜索和相似度暴露了一个单一的嵌入端点。OpenAI嵌入具有更大的最大序列长度（8,192个标记）和更大的维度大小（1,536–3,072）。Cohere和Jina是其他嵌入提供商的例子。
- en: Choosing the right model for your task depends on cost, latency, storage limitations,
    performance, and the data domain of your use case. I suggest starting off with
    the small but effective all-mpnet-base-v2 model available through the Sentence
    Transformers library, which I consider the workhorse of the field of NLP. As always,
    experimenting with different models never hurts. More tips on selecting the right
    models will be provided throughout the rest of the chapter. Later in the chapter,
    we will also show how to evaluate embedding models and introduce popular benchmarks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适合您任务的正确模型取决于成本、延迟、存储限制、性能以及您用例的数据域。我建议从Sentence Transformers库中提供的有效但小巧的all-mpnet-base-v2模型开始，我认为它是NLP领域的“工作马”。像往常一样，尝试不同的模型不会有坏处。本章的其余部分将提供有关选择正确模型的更多提示。在章节的后面部分，我们还将展示如何评估嵌入模型并介绍流行的基准。
- en: Warning
  id: totrans-27
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: There is no such thing as infinite compression! Embedding sizes are fixed, so
    the longer your input, the less information can be encoded in its embedding. Managing
    this tradeoff differs by use case.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么是无限压缩的！嵌入大小是固定的，所以您的输入越长，其嵌入中可以编码的信息就越少。管理这种权衡因用例而异。
- en: Semantic Search
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 语义搜索
- en: The true value of embeddings can be appreciated when we use them for representing
    a large text corpus. The vectors representing the data occupy what we call an
    embedding space. Similar texts are located closer to each other in the embedding
    space. This property allows us to use similarity measures to accomplish meaningful
    tasks like clustering or semantic search. Semantic search refers to techniques
    that take into account the meaning and context of queries and documents to identify
    documents that are most relevant to a given query.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用嵌入表示大型文本语料库时，可以欣赏嵌入的真正价值。表示数据的数据向量占据我们所说的嵌入空间。在嵌入空间中，相似文本彼此更接近。这一特性使我们能够使用相似度度量来完成诸如聚类或语义搜索等有意义的工作。语义搜索是指考虑查询和文档的意义和上下文的技术，以识别与给定查询最相关的文档。
- en: We can visualize the embedding space by using dimensionality reduction techniques
    like [PCA](https://oreil.ly/Rk1M9) or [t-SNE](https://oreil.ly/0xNrB).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用[PCA](https://oreil.ly/Rk1M9)或[t-SNE](https://oreil.ly/0xNrB)等降维技术来可视化嵌入空间。
- en: '[Figure 11-1](#embedding-visualization) depicts the visualization of embeddings
    of posts on X (formerly Twitter) by members of the US Congress created by [Nomic
    AI](https://oreil.ly/XsXls) using its Atlas tool. You can view a detailed version
    of the visualization at [Nomic’s blog](https://oreil.ly/AORpk).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-1](#embedding-visualization)展示了由[Nomic AI](https://oreil.ly/XsXls)使用其Atlas工具创建的，美国国会成员在X（前身为Twitter）上发布的帖子嵌入的可视化。您可以在[Nomic的博客](https://oreil.ly/AORpk)上查看可视化详细版本。'
- en: Let’s explore how we can use embeddings for semantic search. For a given user
    query, we can generate an embedding of the query and then identify document embeddings
    closest to it in the vector space. The texts corresponding to the top-k (k can
    be as small as 1 but can vary according to application needs) closest vectors
    are provided as a response to the search query. This process is called *retrieval*.
    The texts are then fed into the LLM prompt along with the user query, and the
    LLM uses the information provided in the context to answer the user query. This
    two-step process has traditionally been called the *retriever-reader* framework,
    with the LLM playing the role of the reader in this example.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探讨如何使用嵌入进行语义搜索。对于给定的用户查询，我们可以生成查询的嵌入，然后识别在向量空间中最接近它的文档嵌入。与最接近的k个向量（k可以小到1，但可以根据应用需求变化）对应的文本作为对搜索查询的响应。这个过程被称为*检索*。然后，这些文本与用户查询一起输入到LLM提示中，LLM使用上下文中提供的信息来回答用户查询。这个两步过程传统上被称为*检索器-阅读器*框架，在这个例子中，LLM扮演的是读者的角色。
- en: '![embedding-visualization](assets/dllm_1101.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![embedding-visualization](assets/dllm_1101.png)'
- en: Figure 11-1\. Embedding space visualization
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-1\. 嵌入空间可视化
- en: 'As a simple illustrative example, consider two sentences that make up our corpus:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作为简单的说明示例，考虑组成我们语料库的两个句子：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Given the query “president of usa,” we can encode the query and the chunks
    using Sentence Transformers:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 给定查询“美国总统”，我们可以使用Sentence Transformers对查询和块进行编码：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是：
- en: '[PRE6]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see, the similarity score is much higher for the first sentence,
    and thus we return the first sentence as the query response.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，第一句话的相似度得分要高得多，因此我们返回第一句话作为查询响应。
- en: Note
  id: totrans-43
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: There is a distinction between symmetric semantic search and asymmetric semantic
    search. In symmetric search, the query text is of similar size as the document
    text. In asymmetric search, the query text is much shorter than the document text,
    as with search engine and question-answering assistant queries. There are models
    available that are specialized for only symmetric or asymmetric search. In some
    models, the query and chunk texts are encoded using separate models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对称语义搜索和非对称语义搜索之间有一个区别。在对称搜索中，查询文本的大小与文档文本相似。在非对称搜索中，查询文本比文档文本短得多，例如搜索引擎和问答助手查询。有专门针对仅对称或非对称搜索的模型。在某些模型中，查询和块文本使用不同的模型进行编码。
- en: Similarity Measures
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相似度度量
- en: Commonly used similarity measures include dot product, cosine similarity, and
    Euclidean distance. Refer to the [Pinecone](https://oreil.ly/X_qcD) tutorial on
    similarity measures if you need a backgrounder. While using embedding models,
    use the similarity measure that was used to train the model. You will find this
    information in the model card or Hugging Face model hub page.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的相似度度量包括点积、余弦相似度和欧几里得距离。如果你需要背景信息，请参阅 [Pinecone](https://oreil.ly/X_qcD) 关于相似度度量的教程。在使用嵌入模型时，请使用用于训练模型的相似度度量。你可以在模型卡片或
    Hugging Face 模型中心页面上找到此信息。
- en: Tip
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you set `normalize_embeddings` to `True` as an argument in the `encode()`
    function, it will normalize the embeddings to unit length. This will ensure that
    both dot product and cosine similarity will have the same values. Note that dot
    product is a faster operation than cosine similarity. Sentence Transformers provides
    [separate models](https://oreil.ly/LOu75) trained on dot product and cosine similarity
    and mentions that models trained on dot product tend to prefer longer chunks during
    retrieval.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将 `normalize_embeddings` 作为 `encode()` 函数的参数设置为 `True`，它将规范化嵌入到单位长度。这将确保点积和余弦相似度将具有相同的值。请注意，点积比余弦相似度更快。Sentence
    Transformers 提供了在点积和余弦相似度上训练的[单独的模型](https://oreil.ly/LOu75)，并提到在点积上训练的模型在检索时倾向于更喜欢较长的片段。
- en: While the notion of semantic similarity is powerful, it is not a panacea for
    all applications. The semantic similarity task is underspecified. To start with,
    there are several notions of similarity. Similarity refers to the sameness or
    alikeness of the entities being compared. But for the same two entities, some
    dimensions are similar and some are different.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然语义相似度的概念很强大，但它并不是所有应用的万能药。语义相似度任务是不完全指定的。首先，有几个相似度的概念。相似度指的是被比较的实体之间的相同或相似性。但对于相同的两个实体，有些维度是相似的，而有些则是不同的。
- en: 'For example, consider the three sentences:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下三个句子：
- en: After his 25th anniversary at the company, Mr. Pomorenko confirmed that he is
    not retiring.
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在公司工作满25周年后，波莫伦科先生确认他不会退休。
- en: ''
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mr. Pomorenko announced his retirement yesterday.
  id: totrans-53
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 波莫伦科先生昨天宣布了他的退休。
- en: ''
  id: totrans-54
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mr. Pomorenko did not announce his retirement yesterday.
  id: totrans-55
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 波莫伦科先生昨天没有宣布他的退休。
- en: 'Now let’s use the Sentence Transformers all-mpnet-base-v2 embedding model to
    encode these sentences and calculate their similarity:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用 Sentence Transformers 的 all-mpnet-base-v2 嵌入模型来编码这些句子并计算它们的相似度：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Output:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If you replace the second sentence with “Mr. Pomorenko did not announce his
    retirement yesterday,” the output is:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将第二句话替换为“波莫伦科先生昨天没有宣布他的退休”，输出结果将是：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can see, both these sentences are perceived as equally similar to the
    first sentence. In some aspects, this is true. They are similar because they both
    talk about Mr. Pomorenko. They are also similar because both deal with the subject
    of retirement. On the other hand, one sentence conveys the opposite meaning to
    the other, by suggesting a retirement is happening versus not happening.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这两句话都被认为与第一句话同样相似。在某些方面，这是真的。它们相似是因为它们都谈论波莫伦科先生。它们也相似是因为它们都涉及退休的主题。另一方面，一句话传达了与另一句话相反的意义，通过暗示退休正在发生而不是没有发生。
- en: Tip
  id: totrans-63
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: One way to handle the false positives arising due to the model using undesirable
    similarity dimensions (like negation) is to just increase the k value in the top-k
    results that are returned as a response to the query. Then, the LLM can distinguish
    between false positives and use the correct information for answering the query.
    However, increasing the top-k also increases the context length of the prompt,
    increasing latency and cost.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 处理由于模型使用不希望的相似度维度（如否定）而产生的假阳性的一种方法是在返回给查询的响应中返回的 top-k 结果中仅增加 k 值。然后，LLM 可以区分假阳性和使用正确信息来回答查询。然而，增加
    top-k 也会增加提示的上下文长度，增加延迟和成本。
- en: Our application requirements determine which similarity dimensions are important
    to us. If negation is an important relation for our application to distinguish,
    it might be a good idea to reflect that in our embedding space. This is where
    fine-tuning embedding models can come in handy. Fine-tuning embedding models allows
    you to “edit” your embedding space to your own liking. The process is relatively
    simple and can be potentially quite beneficial.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的应用需求决定了哪些相似度维度对我们来说很重要。如果否定关系对我们应用区分很重要，那么在嵌入空间中反映这一点可能是个好主意。这正是微调嵌入模型能派上用场的地方。微调嵌入模型允许你“编辑”你的嵌入空间以满足你的需求。这个过程相对简单，并且可能非常有益。
- en: Fine-tuning embeddings can also be very useful when you are working with specialized
    data domains whose token distribution deviates from general-purpose data. Let’s
    now discuss how to fine-tune embedding models.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处理具有与通用数据不同的标记分布的特定数据领域时，微调嵌入模型也非常有用。现在让我们讨论如何微调嵌入模型。
- en: Fine-Tuning Embedding Models
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 微调嵌入模型
- en: The Sentence Transformers library facilitates fine-tuning embedding models using
    the [`SentenceTransformerTrainer` class](https://oreil.ly/Jahep). To fine-tune
    an embedding model, we need a base model to fine-tune on, a training dataset,
    and a learning objective.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`SentenceTransformerTrainer` 类（[链接](https://oreil.ly/Jahep)）简化了使用基础模型进行嵌入模型微调的过程。为了微调一个嵌入模型，我们需要一个用于微调的基础模型、一个训练数据集和一个学习目标。'
- en: Base Models
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础模型
- en: You can fine-tune a fine-tuned model like all-mpnet-base-v2, or you can fine-tune
    a base model like MPNet, from which all-mpnet-base-v2 is defined. You will need
    more training data to fine-tune a base model than to further fine-tune an already
    fine-tuned model. Other candidates’ models for fine-tuning include [BGE-M3](https://oreil.ly/Sh8pZ)
    and [jina-embeddings-v3](https://oreil.ly/lFiWX). A full list of models available
    through Sentence Transformers can be accessed [online](https://oreil.ly/Onyuv).
    Remember to check the licenses for a given model before using it for commercial
    purposes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以微调像all-mpnet-base-v2这样的微调模型，或者微调像MPNet这样的基础模型，all-mpnet-base-v2就是基于MPNet定义的。微调基础模型比进一步微调已经微调过的模型需要更多的训练数据。其他适合微调的候选模型包括[BGE-M3](https://oreil.ly/Sh8pZ)和[jina-embeddings-v3](https://oreil.ly/lFiWX)。通过Sentence
    Transformers可以访问的模型完整列表可以在[线上](https://oreil.ly/Onyuv)找到。记住在使用模型进行商业用途之前检查其许可证。
- en: Some of the factors to keep in mind while choosing a base model include the
    performance of the base model, the size of the embedding models (which determines
    how fast the model can encode text), the number of dimensions of the model (which
    determines the amount of storage taken up by the embeddings), and the licensing
    implications. The MPNet or all-mpnet-base-v2 is a solid first choice that has
    served me well on many projects.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 选择基础模型时需要考虑的一些因素包括基础模型的表现、嵌入模型的大小（这决定了模型编码文本的速度）、模型维度数（这决定了嵌入所占用的存储量），以及许可的影响。MPNet或all-mpnet-base-v2是一个稳固的首选，它在许多项目中都为我提供了良好的服务。
- en: Tip
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: If a model has been fine-tuned for a particular task like semantic search, it
    is not optimal to further fine-tune it on a different task.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个模型已经针对特定任务（如语义搜索）进行了微调，那么在另一个任务上进一步微调它并不是最佳选择。
- en: Training Dataset
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练数据集
- en: There are many different ways to structure your dataset. The most common way
    is in the form of triplets consisting of (anchor, positive, negative) examples.
    For a given anchor sentence, the positive sentence is a sentence we would like
    to be closer to the anchor sentence in embedding space, and the negative sentence
    is a sentence we would like to be farther apart from the anchor in embedding space.
    For example, to fine-tune the model to help it distinguish negation sentences,
    our training set can be composed of triplets where the negative sentence contradicts
    the anchor and the positive sentences.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化数据集的方法有很多种。最常见的方式是以三元组的形式，包括（锚点，正面，负面）示例。对于一个给定的锚点句子，正面句子是我们希望其在嵌入空间中更接近锚点句子的句子，而负面句子是我们希望其在嵌入空间中与锚点距离更远的句子。例如，为了微调模型以帮助其区分否定句子，我们的训练集可以由三元组组成，其中负面句子与锚点相矛盾，而正面句子则与之相符。
- en: '[Figure 11-2](#embed-dataset) shows an embedding dataset composed of triplets
    for helping the model distinguish negation.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11-2](#embed-dataset) 展示了一个用于帮助模型区分否定词的嵌入数据集，该数据集由三元组组成。'
- en: '![embed-dataset](assets/dllm_1102.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![embed-dataset](assets/dllm_1102.png)'
- en: Figure 11-2\. Fine-tuning dataset for negation
  id: totrans-78
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-2\. 否定关系的微调数据集
- en: Datasets can also be composed of sentence pairs, where the sentences could represent
    a (query, response) pair, or a (passage, summary) pair, or a pair of paraphrases.
    The downstream use cases determine the type of dataset needed. The [Sentence Transformers
    website](https://oreil.ly/geI1M) shows all the different ways a dataset can be
    formatted.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集也可以由句子对组成，其中句子可以代表一个 (query, response) 对，或一个 (passage, summary) 对，或一对释义。下游用例决定了所需数据集的类型。The
    [Sentence Transformers 网站](https://oreil.ly/geI1M) 展示了数据集可以格式化的所有不同方式。
- en: Training datasets can be as small as a few thousand examples, to [billions of
    tokens](https://oreil.ly/oNI4n) when used for domain adaptation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集可以小到几千个示例，当用于领域自适应时，可以达到数十亿个标记。[billions of tokens](https://oreil.ly/oNI4n)。
- en: Note that certain loss functions require your dataset to be in a specific format.
    We will discuss loss functions in detail next.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，某些损失函数需要您的数据集以特定的格式。我们将在下一节详细讨论损失函数。
- en: Loss Functions
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数
- en: Recall our discussion on loss functions for training LLMs in [Chapter 4](ch04.html#chapter_transformer-architecture).
    The [Sentence Transformers library](https://oreil.ly/9Qaop) supports a wide range
    of loss functions for training embedding models. Let’s explore a few commonly
    used ones.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们在 [第4章](ch04.html#chapter_transformer-architecture) 中关于训练 LLMs 的损失函数的讨论。The
    [Sentence Transformers 库](https://oreil.ly/9Qaop) 支持广泛的损失函数用于训练嵌入模型。让我们探索一些常用的函数。
- en: For a triplet dataset, you can compute a [triplet loss](https://oreil.ly/yXHNU).
    For a training dataset consisting of an (anchor, positive, negative) triplet,
    the triplet loss minimizes the distance between the anchor sentence and the positive
    sentence, and maximizes the distance between the anchor sentence and the negative
    sentence.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于三元组数据集，你可以计算一个 [triplet loss](https://oreil.ly/yXHNU)。对于一个由 (anchor, positive,
    negative) 三元组组成的训练数据集，三元组损失最小化锚句和正句之间的距离，并最大化锚句和负句之间的距离。
- en: 'Mathematically, the loss is calculated as:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，损失的计算如下：
- en: Loss = max(d(a, p) – d(a, n) + margin, 0)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 = max(d(a, p) – d(a, n) + margin, 0)
- en: where d is a distance measure, typically Euclidean distance. The margin is a
    hyperparameter that represents the distance by which the negative example should
    be farther away from the anchor than the positive example. When using Euclidean
    distance as the distance measure, I suggest a margin of 5, but make sure to tune
    it if you are not getting sufficient results.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 d 是距离度量，通常是欧几里得距离。margin 是一个超参数，表示负例应该比正例远离锚点的距离。当使用欧几里得距离作为距离度量时，我建议 margin
    为 5，但如果你没有得到足够的结果，请确保调整它。
- en: If you are using a dataset composed of pairs like (query, response), (passage,
    summary), etc., you can use the [Multiple Negatives Ranking Loss](https://oreil.ly/oNcsQ).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是由 (query, response)，(passage, summary) 等对组成的数据集，你可以使用 [Multiple Negatives
    Ranking Loss](https://oreil.ly/oNcsQ)。
- en: In a batch containing (query, response) pairs (q1, r1), (q2, r2)…​(qn, rn),
    for each query, there will be a positive pair, e.g., (q1, r1) and n – 1 negative
    pairs, e.g., (q1, r2), (q1, r3)…​etc. The loss function minimizes the negative
    log likelihood.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在包含 (query, response) 对 (q1, r1), (q2, r2)…(qn, rn) 的批次中，对于每个查询，将有一个正对，例如 (q1,
    r1) 和 n – 1 个负对，例如 (q1, r2), (q1, r3)…等等。损失函数最小化负对数似然。
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Use [`CachedMultipleNegativesRankingLoss`](https://oreil.ly/QwBlI), available
    in Sentence Transformers, which allows you to use larger batch sizes, leading
    to better performance.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Sentence Transformers 中可用的 `CachedMultipleNegativesRankingLoss`，这允许你使用更大的批次大小，从而提高性能。
- en: 'Now that we have discussed all the ingredients needed for fine-tuning, let’s
    put it all together with the `SentenceTransformerTrainer` class:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了所有需要的微调成分，让我们使用 `SentenceTransformerTrainer` 类将它们全部组合起来：
- en: '[PRE10]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The full code is available in the book’s [GitHub repo](https://oreil.ly/llm-playbooks).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在书籍的 [GitHub 仓库](https://oreil.ly/llm-playbooks) 中找到。
- en: Tip
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Watch out for overfitting! You can reduce your learning rate if you notice the
    model overfitting.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意过拟合！如果你注意到模型过拟合，你可以降低学习率。
- en: Note
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '[Zhou et al.](https://oreil.ly/BPdRD) show that in the context of embeddings,
    cosine similarity tends to underestimate the similarity between high-frequency
    words. This is because high-frequency words occupy distinct regions in the embedding
    space, leading to larger distances from other words. On the other hand, low-frequency
    words tend to be more concentrated geometrically.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '[周等人](https://oreil.ly/BPdRD)表明，在嵌入的上下文中，余弦相似度往往会低估高频词之间的相似性。这是因为高频词在嵌入空间中占据不同的区域，导致与其他词的距离更大。另一方面，低频词在几何上往往更集中。'
- en: Instruction Embeddings
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 指令嵌入
- en: So far we have seen that embedding models are specialized for solving a specific
    task, like semantic search or paraphrasing. A recent development ties together
    embedding models and the concept of instruction-tuning, which we discussed in
    [Chapter 6](ch06.html#llm-fine-tuning). Imagine if you could use the same embedding
    model to generate different embeddings for the same document, based on the task
    it is going to be used for. One such model is called Instructor. [Instructor embeddings](https://oreil.ly/mSIhG)
    allow you to optionally specify the domain, text type (whether it is a sentence,
    paragraph, etc.), and task, along with the text during encoding.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到嵌入模型是专门用于解决特定任务的，如语义搜索或释义。最近的一项发展将嵌入模型和指令微调的概念结合起来，我们在[第6章](ch06.html#llm-fine-tuning)中讨论了这一概念。想象一下，如果你可以使用相同的嵌入模型根据将要使用的任务为同一文档生成不同的嵌入。这样一个模型被称为Instructor。[Instructor嵌入](https://oreil.ly/mSIhG)允许你在编码文本时可选地指定领域、文本类型（是否为句子、段落等）和任务。
- en: 'Here is an example:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子：
- en: '[PRE11]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The creators of Instructor recommend using this instruction template:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Instructor的创建者建议使用以下指令模板：
- en: '[PRE12]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: where `{domain}` represents the domain of the text like law, finance, etc. The
    optional `{text_type}` represents the unit of text being encoded, like a question,
    sentence, paragraph, etc. `{task_objective}` represents the task for which we
    are using the embeddings, like semantic search, paraphrase detection, etc.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `{domain}` 表示文本的领域，如法律、金融等。可选的 `{text_type}` 表示正在编码的文本单位，如问题、句子、段落等。`{task_objective}`
    表示我们使用嵌入的任务，如语义搜索、释义检测等。
- en: In the context of semantic search, they recommend the instruction “Represent
    the question for retrieving supporting documents” for queries, and “Represent
    the sentence for retrieval” for documents.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在语义搜索的上下文中，他们建议对查询使用指令“表示用于检索支持文档的问题”，对文档使用指令“表示用于检索的句子”。
- en: 'Another way the principle of instruction-tuning can be applied to retrieval
    is with *description-based retrieval*, where the query can be the description
    of the text that needs to be retrieved, rather than an instantiation (example)
    of the text that needs to be retrieved. [Ravfogel et al.](https://oreil.ly/rp8Q-)
    have published description-based retrieval models that in my experience are very
    effective. Note that these models have a dual-encoder setup: separate models are
    used to encode the query and documents.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 指令微调原则应用于检索的另一种方式是使用*基于描述的检索*，其中查询可以是需要检索的文本的描述，而不是需要检索的文本的实例（示例）。[Ravfogel等人](https://oreil.ly/rp8Q-)已经发布了基于描述的检索模型，在我的经验中，这些模型非常有效。请注意，这些模型具有双编码器设置：使用不同的模型来编码查询和文档。
- en: Optimizing Embedding Size
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化嵌入大小
- en: Many applications involve generating billions of embeddings. As we have seen,
    modern embeddings sometimes have as many as thousands of dimensions. If each dimension
    is represented in float32, then it needs four bytes of memory per dimension. Therefore,
    storing 100 million vectors generated from the all-mpnet-base-v2 model, which
    has 768 dimensions, needs close to 300 GB of memory!
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 许多应用涉及生成数十亿个嵌入。正如我们所见，现代嵌入有时有数千个维度。如果每个维度都用float32表示，那么每个维度需要4个字节的内存。因此，存储从all-mpnet-base-v2模型生成的1亿个向量，该模型有768个维度，需要接近300
    GB的内存！
- en: It is not uncommon to represent a single sentence, almost always no longer than
    40 tokens, with a 768-dimension vector. Do we really need 768 dimensions to represent
    40 tokens? The reality is that embedding training is very inefficient, and a large
    number of dimensions are not really useful.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 用768维向量表示单个句子并不罕见，几乎总是不超过40个标记。我们真的需要768个维度来表示40个标记吗？现实是嵌入训练非常低效，大量的维度实际上并不有用。
- en: Therefore, several embedding truncation and quantization approaches have been
    developed to optimize embedding size and reduce storage and compute requirements.
    If you are operating in an environment with more than a few million vectors, these
    techniques are likely to be useful to you. Let’s look at some of these approaches.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，已经开发了几种嵌入截断和量化方法来优化嵌入大小并减少存储和计算需求。如果您在处理超过几百万个向量的环境中工作，这些技术可能对您很有用。让我们看看这些方法中的一些。
- en: Matryoshka Embeddings
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Matryoshka嵌入
- en: Matryoshka embeddings are named after [Matryoshka dolls](https://oreil.ly/OC6Yj),
    which refer to a set of wooden dolls that are placed inside each other in decreasing
    order of size, originating from Russia. Matryoshka embeddings are trained such
    that the earlier dimensions of the vector contain more important information than
    the later dimensions. This allows us to truncate vectors depending on the requirements
    of the application with respect to cost, latency, and performance.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Matryoshka嵌入是以[俄罗斯套娃](https://oreil.ly/OC6Yj)命名的，它指的是一套按大小递减顺序放置在彼此内部的木制娃娃，起源于俄罗斯。Matryoshka嵌入的训练方式是，向量的早期维度包含比后期维度更重要的信息。这使得我们可以根据应用对成本、延迟和性能的要求来截断向量。
- en: The technique used to train these embeddings is called Matryoshka Representation
    Learning (MRL). In MRL, we first choose a set of truncation dimensions. For example
    a 1,024-dimension vector can have truncation dimensions 128, 256, 512, and 768\.
    During the training process, we calculate the loss over each of the truncation
    dimensions as well as the full dimension. The losses are then added and weighted.
    In our example, the first 128 dimensions learn from the loss calculated over the
    first 128, 256, 512, 768, and 1,024 dimensions of the vector. The end result is
    that the initial dimensions of the vector will encode more important information
    because they learn from richer losses.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练这些嵌入的技术称为Matryoshka表示学习（MRL）。在MRL中，我们首先选择一组截断维度。例如，一个1,024维度的向量可以有截断维度128、256、512和768。在训练过程中，我们计算每个截断维度以及完整维度的损失。然后，损失被相加并加权。在我们的例子中，前128个维度从计算在向量的前128、256、512、768和1,024维度的损失中学习。最终结果是，向量的初始维度将编码更重要的信息，因为它们从更丰富的损失中学习。
- en: 'Training using MRL is supported by the Sentence Transformers library. Let’s
    see how it works in practice:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MRL进行训练由Sentence Transformers库支持。让我们看看它在实际中的应用：
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[Tom Aarsen](https://oreil.ly/sA5fo) observed in his experiments that even
    at 8.3% of the original embedding size, the Matryoshka model preserves 98.37%
    of the original performance. This makes it a very effective technique that will
    come in handy when you are working with large datasets.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[汤姆·阿森](https://oreil.ly/sA5fo)在他的实验中观察到，即使在原始嵌入大小的8.3%时，Matryoshka模型也能保留98.37%的原始性能。这使得它成为一种非常有效的技术，当您处理大型数据集时将非常有用。'
- en: Similar to how we can reduce the effective dimension of our embeddings using
    MRL, we can also reduce the effective number of layers of the embedding model,
    leading to faster inference. This is done by extracting embeddings from the lower
    layers of the model. To facilitate the lower layers of the model aligning high-quality
    embeddings with the embeddings of the last layer of the model, a K-L divergence
    loss is employed between the final layer and each of the lower layers. This technique
    was first introduced by [Li et al.’s](https://oreil.ly/fzIPD) Espresso Sentence
    Embeddings.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们如何使用MRL减少嵌入的有效维度类似，我们也可以减少嵌入模型的有效层数，从而实现更快的推理。这是通过从模型的底层提取嵌入来实现的。为了促进模型的底层与模型的最后一层的嵌入对齐，采用了K-L散度损失。这种技术最初由[李等人](https://oreil.ly/fzIPD)的Espresso句子嵌入提出。
- en: '[Tom Aarsen](https://oreil.ly/DIoTe) observed in his experiments that removing
    half the layers leads to a 2x improvement in speed with 85% of the original performance
    preserved.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '[汤姆·阿森](https://oreil.ly/DIoTe)在他的实验中观察到，移除一半的层可以将速度提高2倍，同时保留原始性能的85%。'
- en: The Sentence Transformers library allows you to combine Matryoshka representations
    with layer reduction using the [Matryoshka2dLoss](https://oreil.ly/xzG-a).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Sentence Transformers库允许您使用[Matryoshka2dLoss](https://oreil.ly/xzG-a)将Matryoshka表示与层减少相结合。
- en: Binary and Integer Embeddings
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 二进制和整数嵌入
- en: An alternative to truncation is quantization. With binary and integer quantization,
    the number of vector dimensions remains the same, but each dimension is represented
    by fewer bits. Recall that typically embedding vectors are represented in float32,
    thus taking four bytes of memory per dimension.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 截断的另一种选择是量化。在二进制和整数量化中，向量的维度数量保持不变，但每个维度由更少的位表示。回想一下，通常嵌入向量以 float32 表示，因此每个维度占用四个字节的内存。
- en: At the extreme level, the four bytes can be represented with just one bit, resulting
    in a 32x reduction in storage requirements. This type of compression is generally
    done by sacrificing the precision of the vector values.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在极端情况下，四个字节可以用一个比特表示，从而在存储需求上减少 32 倍。这种压缩通常是通过牺牲向量值的精度来实现的。
- en: A simple way to convert a four-byte vector to a one-bit vector is to assign
    a value of 1 if the original value is positive, and 0 if it is negative. Note
    that you might need to perform some scaling to achieve best results. After packing
    these bits into bytes, a 512-dimension vector can be represented in just 512 /
    8 = 64 bytes, instead of 512 × 4 = 2,048 bytes.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 将四个字节的向量转换为单比特向量的简单方法是将原始值为正时分配值为 1，为负时分配值为 0。请注意，您可能需要进行一些缩放以达到最佳效果。将这些比特打包到字节中后，一个
    512 维向量可以用 512 / 8 = 64 字节表示，而不是 512 × 4 = 2,048 字节。
- en: Another advantage with using binary embeddings is that computing similarity
    only needs simple bitwise operations, thus vastly speeding up retrieval. However,
    quantization negatively affects performance.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用二进制嵌入的另一个优点是，计算相似性只需要简单的位操作，从而大大加快检索速度。然而，量化对性能有负面影响。
- en: 'You can use the `Sentence Transformers` library to quantize embeddings:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `Sentence Transformers` 库来量化嵌入：
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`quantize_embeddings` also supports int8 quantization. In this scheme, the
    four bytes representing each dimension are converted into an integer value, represented
    in one byte. The integer can be either signed or unsigned, thus representing values
    between –127 and 127 or between 0 and 255, respectively. The conversion process
    is guided using a calibration dataset of embeddings, from which we calculate the
    minimum and maximum value of each dimension. These values are then used in the
    normalization formula to convert the numbers from one range to another.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`quantize_embeddings` 也支持 int8 量化。在这个方案中，代表每个维度的四个字节被转换成一个整数值，用一个字节表示。这个整数可以是带符号的或无符号的，因此可以表示
    -127 到 127 或 0 到 255 之间的值。转换过程使用嵌入的校准数据集指导，从中我们计算出每个维度的最小值和最大值。然后，这些值被用于归一化公式中，将数字从一个范围转换到另一个范围。'
- en: Tip
  id: totrans-129
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: It has been shown that for some [embedding models](https://oreil.ly/Mp3pu),
    binary embeddings perform better than int8 embeddings despite the reduced precision!
    This is largely because of the calibration dataset used and the challenge involved
    in mapping float values to buckets of int8 values.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明，对于某些 [嵌入模型](https://oreil.ly/Mp3pu)，尽管精度降低，但二进制嵌入的性能优于 int8 嵌入。这很大程度上归因于使用的校准数据集和将浮点值映射到
    int8 值桶的挑战。
- en: Product Quantization
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 产品量化
- en: Another promising quantization method is called [*product quantization*](https://oreil.ly/aJq2C).
    In this technique, a vector is divided into chunks of equal size. The chunks are
    then clustered. The number of clusters is set to the number of values that can
    be represented by the quantized embedding. For example, if we aim to quantize
    to int8, then the number of values that can be represented is 256, and thus the
    number of clusters is 256\. Each cluster is associated with an identifier, which
    is a unique value between 0 and 255\. Each chunk belongs to the cluster whose
    centroid the chunk is closest to.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有希望的量化方法是称为 [*产品量化*](https://oreil.ly/aJq2C)。在这种技术中，向量被分成大小相等的块。然后，这些块被聚类。聚类的数量设置为量化嵌入可以表示的值的数量。例如，如果我们旨在量化到
    int8，那么可以表示的值是 256，因此聚类的数量是 256。每个聚类都与一个标识符相关联，这是一个介于 0 和 255 之间的唯一值。每个块属于其质心与块最近的聚类。
- en: Thus, the original float32 vector can now be represented by a list of cluster
    identifiers corresponding to the clusters the chunks belong to. The larger the
    chunk size, the more the compression. Thus if the vector is divided into five
    chunks, the resulting embedding will have only five dimensions. Unlike int8 and
    binary quantization, product quantization also reduces the number of dimensions
    needed to represent a vector. However, the performance drop is higher.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，原始的float32向量现在可以由一个列表表示，该列表对应于块所属的簇标识符。块的大小越大，压缩效果越好。因此，如果将向量分为五个块，则生成的嵌入将只有五个维度。与int8和二进制量化不同，产品量化也减少了表示向量所需的维度数。然而，性能下降更大。
- en: Choose your quantization technique by determining your relative product priorities
    for criteria like cost, performance, and speed.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通过确定相对产品优先级，如成本、性能和速度等标准，来选择你的量化技术。
- en: Tip
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: Optimizing embeddings for storage come with a performance hit. However, if there
    is plenty of redundancy in the document corpus, answers to typical user queries
    might be found in several documents, and hence the user may not feel this performance
    drop.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 优化嵌入以存储会带来性能损失。然而，如果文档语料库中存在大量冗余，典型用户查询的答案可能分布在多个文档中，因此用户可能不会感觉到这种性能下降。
- en: Now that we have seen various techniques to practically implement embedding-based
    retrieval, let’s next figure out the textual units we need to embed into distinct
    vectors.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了各种基于嵌入的检索的实际实现技术，接下来让我们确定需要嵌入到不同向量中的文本单元。
- en: Chunking
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 块分割
- en: As noted in [“Introduction to Embeddings”](#introduction-to-embeddings), embedding
    models support very limited context lengths, and the effectiveness of embedding
    similarity matching decreases as the text length increases. Therefore, it is natural
    to split documents into manageable units called chunks and embed each chunk into
    one or more vectors.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如在[“嵌入简介”](#introduction-to-embeddings)中所述，嵌入模型支持非常有限的语言长度，随着文本长度的增加，嵌入相似度匹配的有效性会降低。因此，将文档分割成可管理的单元（称为块）并将每个块嵌入到一个或多个向量中是自然而然的。
- en: A chunk can be defined as a semantically coherent and not necessarily contiguous
    part of a document. The average chunk length depends on the context length supported
    by the language model, and the number of chunks returned to the model (the top-k)
    in response to a user query. As models become increasingly affordable to operate
    and support ever-larger context lengths, the permissible chunk size grows.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 块可以被定义为文档中语义上连贯且不一定连续的部分。平均块长度取决于语言模型支持的语言长度，以及针对用户查询返回给模型的块数量（top-k）。随着模型变得越来越易于操作并支持更长的上下文长度，允许的块大小也随之增长。
- en: Each chunk can either be represented by a single vector or can be further broken
    down into units, with each unit being represented by a separate vector. A unit
    could be a sentence, a paragraph, or even a section. Typically, the smaller the
    unit, the better. For your application, test your expected user queries against
    different granularities and see what works best.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 每个块可以由一个单独的向量表示，也可以进一步分解成单元，每个单元由一个单独的向量表示。一个单元可以是句子、段落，甚至是章节。通常，单元越小越好。对于你的应用，测试你预期的用户查询对不同粒度的影响，看看哪种效果最好。
- en: Consider a scenario where a document corpus has been broken down into units
    represented by embeddings. For a given user query, we can calculate the cosine
    similarity between the user query vector and each of the document vectors. The
    chunks corresponding to the most similar vectors are then retrieved. This ensures
    that the embedding matching happens at a lower granularity, like a sentence, but
    the model receives the entirety of the chunk the sentence belongs to, thus providing
    sufficient background context to the model.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个场景，其中文档语料库已经被分解为由嵌入表示的单元。对于给定的用户查询，我们可以计算用户查询向量和每个文档向量之间的余弦相似度。然后检索与最相似向量对应的块。这确保了嵌入匹配发生在更低的粒度上，如句子，但模型接收到的却是句子所属的整个块，从而为模型提供了足够的背景上下文。
- en: A question I am frequently asked by ML practitioners is, “What is the ideal
    chunk size and what are some effective chunking strategies?” Determining the right
    chunk size and boundaries are key challenges practitioners face when using embedding-based
    retrieval. In this section, we will discuss a few chunking strategies, introduced
    in order of increasing complexity.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我经常被机器学习从业者问到的问题之一是，“理想的分块大小是什么，有哪些有效的分块策略？”确定合适的分块大小和边界是从业者在使用基于嵌入的检索时面临的关键挑战。在本节中，我们将讨论一些分块策略，按照复杂度递增的顺序介绍。
- en: In the basic implementation of embedding-based retrieval, each vector is a distinct
    island, disconnected from all other islands. The text represented by Vector A
    is not able to influence text represented by Vector B in any way. Therefore, we
    need to connect these islands in some way or make these islands as self-contained
    as possible. With these objectives in mind, let’s look at some chunking strategies
    that go beyond naive paragraph or section splitting.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于嵌入的检索的基本实现中，每个向量都是一个独立的岛屿，与其他岛屿完全断开连接。由向量A表示的文本无法以任何方式影响由向量B表示的文本。因此，我们需要以某种方式连接这些岛屿，或者尽可能使这些岛屿尽可能自包含。带着这些目标，让我们看看一些超越简单段落或章节分割的分块策略。
- en: Sliding Window Chunking
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 滑动窗口分块
- en: Consider a situation where the embedding similarity function returns a unit
    in Chunk 45 as the most similar vector to your query vector. However, text in
    Chunk 44, which immediately precedes Chunk 45 in the document, contains relevant
    information contextualizing Chunk 45\. The vectors in Chunk 44 have a very low
    similarity score with the query, and as a result, Chunk 44 is not selected for
    retrieval. One way to fix this is by using sliding window chunking, where each
    text can be present in multiple chunks, thus allowing neighboring context to be
    effectively represented in a coherent block.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一种情况，其中嵌入相似度函数将Chunk 45中的单位作为与查询向量的最相似向量。然而，在文档中紧接在Chunk 45之前的Chunk 44中包含有关Chunk
    45的上下文信息的相关文本。Chunk 44中的向量与查询具有非常低的相似度得分，因此，Chunk 44没有被选中用于检索。一种修复方法是通过使用滑动窗口分块，其中每个文本可以出现在多个分块中，从而有效地在连贯的块中表示邻近的上下文。
- en: Metadata-Aware Chunking
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 元数据感知分块
- en: Any metadata that you have about the document can be leveraged to determine
    chunking boundaries. Useful metadata information includes paragraph boundaries,
    section and subsection boundaries, etc. If the metadata isn’t already available,
    you might need to use document parsing techniques to extract this information.
    Several libraries can facilitate this, including [Unstructured](https://oreil.ly/CoX46).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您关于文档的任何元数据都可以用来确定分块边界。有用的元数据信息包括段落边界、章节和子章节边界等。如果元数据尚未可用，您可能需要使用文档解析技术来提取这些信息。有几个库可以简化这一过程，包括[Unstructured](https://oreil.ly/CoX46)。
- en: Layout-Aware Chunking
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 布局感知分块
- en: A more involved form of metadata-aware chunking is layout-aware chunking. In
    this approach we use computer vision techniques to extract layout information
    about the document, including the placement and scope of textual elements, the
    titles, subtitles, font size of text, etc.; use this metadata to inform the chunking
    process. Both open source and proprietary tools can facilitate layout extraction.
    They include tools like [Amazon Textractor](https://oreil.ly/fvkiT), [Unstructured](https://oreil.ly/CoX46),
    and layout-aware language models like [LayoutLMv3](https://oreil.ly/Od5fA).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 更复杂的一种元数据感知分块是布局感知分块。在这种方法中，我们使用计算机视觉技术来提取文档的布局信息，包括文本元素的位置和范围、标题、副标题、文本的字体大小等；使用这些元数据来指导分块过程。开源和专有工具都可以简化布局提取。它们包括像[Amazon
    Textractor](https://oreil.ly/fvkiT)、[Unstructured](https://oreil.ly/CoX46)这样的工具，以及布局感知语言模型如[LayoutLMv3](https://oreil.ly/Od5fA)。
- en: For example, using this approach we can know the scope of a subsection, and
    thus insert the subsection title at the beginning of each chunk comprising text
    from that subsection.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用这种方法，我们可以知道子章节的范围，因此可以在包含该子章节文本的每个分块的开头插入子章节标题。
- en: You can also use techniques like ColPali that employ vision models to directly
    embed a page or section of the document and perform retrieval over it. This may
    remove the need for chunking entirely but might be more expensive overall.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用像ColPali这样的技术，这些技术采用视觉模型直接嵌入文档的页面或部分，并在其上进行检索。这可能会完全消除分块的需求，但整体上可能成本更高。
- en: Semantic Chunking
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义分块
- en: The principle behind semantic chunking is that similar information should be
    grouped into coherent chunks. Paragraph boundaries provide a weak signal for semantic
    chunking, but more advanced methods can be employed. One approach is to cluster
    the document based on topics, with each chunk containing information pertaining
    to the same topic. The chunks need not necessarily be built from contiguous text
    if it makes sense for the application. A more advanced approach is to use [Bollinger
    bands-based chunking](https://oreil.ly/1MwK1). The book’s [GitHub repository](https://oreil.ly/llm-playbooks)
    contains an experimental implementation of this form of chunking.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分块背后的原理是相似信息应该被分组到连贯的块中。段落边界为语义分块提供了一个弱信号，但可以采用更高级的方法。一种方法是基于主题对文档进行聚类，每个块包含与同一主题相关的信息。如果对应用有意义，块不必一定由连续的文本构建。一种更高级的方法是使用[基于布林格带分块](https://oreil.ly/1MwK1)。本书的[GitHub仓库](https://oreil.ly/llm-playbooks)包含这种分块形式的实验实现。
- en: Semantic chunking can also be employed to connect different chunks with each
    other. Once the chunks have been assigned, similar chunks can be grouped based
    on embedding similarity, allowing them to be retrieved along with the chunk having
    the highest similarity score. Each chunk does not necessarily need to consist
    of content from the same document, as long as the metadata associated with each
    sub-chunk is retained.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分块还可以用来连接不同的块。一旦块被分配，可以根据嵌入相似度将相似的块分组，这样它们就可以与具有最高相似度分数的块一起检索。每个块不必一定由同一文档的内容组成，只要保留每个子块相关的元数据即可。
- en: A basic implementation of semantic chunking is available in [LangChain](https://oreil.ly/tm8tk).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '[LangChain](https://oreil.ly/tm8tk)中提供了一个基本的语义分块实现。'
- en: Note
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Highly performant semantic chunking can be performed through LLMs. But it will
    be a huge cost overhead if the size of your data corpus is very large. Sometimes
    good old regex can be enough. Jina AI created a complex 50-line [regex-based chunker](https://oreil.ly/x5UO8)
    that you can try as an initial option.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 通过LLM可以执行高度性能的语义分块。但如果你的数据语料库非常大，这将是一个巨大的成本开销。有时简单的正则表达式就足够了。Jina AI创建了一个复杂的50行[基于正则表达式的分块器](https://oreil.ly/x5UO8)，你可以将其作为初始选项尝试。
- en: 'Despite using all these techniques, effective chunking still remains a problem.
    Consider the following real-world example from a financial document:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用了所有这些技术，有效的分块仍然是一个问题。考虑以下来自金融文档的现实世界示例：
- en: 'Page 5: *All numbers in the document are in millions*'
  id: totrans-160
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第5页：*文档中的所有数字都是以百万为单位的*
- en: ''
  id: totrans-161
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Page 84: *The related party transaction amounts to $213.45*'
  id: totrans-162
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 第84页：*相关方交易金额为213.45美元*
- en: In this case the related party transaction actually amounts to $213M dollars
    but the LLM would never know this because the text from page 5 is not likely to
    be part of the same chunk.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，相关方交易实际上相当于2130万美元，但LLM永远不会知道这一点，因为第5页的文本不太可能成为同一块的一部分。
- en: A related problem is the difficulty in understanding scope boundaries. When
    does a subsection end and a new subsection begins? What is the scope of the rule
    in page 5 in the given example? What if it is overridden in the middle of a document?
    Not all documents have perfect visual cues or structure. Not all documents are
    well structured into sections, subsections, and paragraphs. These are unsolved
    problems and are the cause of a sizable proportion of RAG failure modes.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关问题是理解范围边界困难。一个子节何时结束，新的子节何时开始？给定示例中第5页的规则范围是什么？如果它在文档中间被覆盖怎么办？并非所有文档都有完美的视觉提示或结构。并非所有文档都很好地组织成章节、子章节和段落。这些都是未解决的问题，也是RAG失败模式中占相当比例的原因。
- en: Late Chunking
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 后期分块
- en: One way of supporting long-range dependencies in text is to use [late chunking](https://oreil.ly/IxTQx),
    a method introduced by Jina AI. Recall from earlier in the chapter that embeddings
    are generated by typically pooling the vectors from the last layer of the underlying
    language model.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本中支持长距离依赖关系的一种方法是通过使用[Jina AI提出的后期分块](https://oreil.ly/IxTQx)，这是一种方法。回顾本章前面的内容，我们知道嵌入通常是通过从底层语言模型的最后一层池化向量生成的。
- en: Given that we have access to long-context language models that can accept an
    entire long document in a single input, we can use such a long-context model as
    our underlying model for generating embeddings. We feed an entire document (or
    as large a part as the model can handle) to the long-context model, so that vectors
    are generated for each of the input tokens. As explained in [Chapter 4](ch04.html#chapter_transformer-architecture),
    each token vector encapsulates its meaning based on its relationship with all
    other tokens in the sequence. This enables long-context dependencies to be captured.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以访问能够接受整个长文档的单个输入的长上下文语言模型，我们可以使用这样的长上下文模型作为生成嵌入的底层模型。我们将整个文档（或模型可以处理的尽可能大的部分）输入到长上下文模型中，以便为每个输入标记生成向量。如[第4章](ch04.html#chapter_transformer-architecture)中所述，每个标记向量基于其与序列中所有其他标记的关系封装其含义。这使得可以捕获长上下文依赖关系。
- en: The pooling operation to extract the embeddings is performed on smaller segments
    of the input, where the segment boundaries can be determined by any of the chunking
    algorithms. Thus, we can have several embeddings representing the same document
    but each of them representing distinct parts of the input.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 提取嵌入的池化操作是在输入的小段上执行的，其中段边界可以由任何分块算法确定。因此，我们可以有多个嵌入表示同一文档，但每个嵌入代表输入的不同部分。
- en: Vector Databases
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量数据库
- en: Depending on your application, you may have to deal with millions or billions
    of vectors, with the need to generate and store new vectors and their associated
    metadata tags every day. Vector databases facilitate this. Both self-hosted and
    cloud-based, open source, and proprietary options are available. Weviate, Milvus,
    Pinecone, Chroma, Qdrant, and LanceDB are some of the popular vector databases.
    More established players like ElasticSearch, Redis, and Postgres also provide
    vector database support.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的应用，您可能需要处理数百万或数十亿个向量，每天都需要生成和存储新的向量和相关的元数据标签。向量数据库简化了这一过程。无论是自托管还是基于云，开源还是专有选项都可用。Weviate、Milvus、Pinecone、Chroma、Qdrant和LanceDB是一些流行的向量数据库。更成熟的玩家如ElasticSearch、Redis和Postgres也提供了向量数据库支持。
- en: These days, the features provided by vector databases are converging, given
    the prevalence of a small set of very popular retrieval use cases.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于一小部分非常流行的检索用例的普遍性，这些天向量数据库提供的功能正在趋同。
- en: 'Let’s now look at how vector databases work. Probably the simplest one to get
    started with is Chroma, which is open source and can run locally on your machine
    or can be deployed on AWS:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看向量数据库是如何工作的。可能最简单的一个开始使用的是Chroma，它是开源的，可以在您的本地机器上运行，也可以部署在AWS上：
- en: '[PRE15]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Most vector databases offer:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数向量数据库提供以下功能：
- en: Approximate nearest neighbor search in addition to exact search, to reduce latency
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了精确搜索外，还支持近似最近邻搜索，以减少延迟
- en: Ability to filter using metadata, like the *where* clause in SQL
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够使用元数据过滤，如SQL中的WHERE子句
- en: Ability to integrate keyword search or algorithms like BM25
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够集成关键词搜索或类似BM25的算法
- en: Support Boolean search operations, so that multiple search clauses can be combined
    with AND or OR operations
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持布尔搜索操作，可以将多个搜索子句通过AND或OR操作组合
- en: Ability to update or delete entries in the database in real time
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够实时更新或删除数据库中的条目
- en: Interpreting Embeddings
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释嵌入
- en: What features of text do embeddings learn? Why are two sentences sometimes closer
    to/farther from each other in the embedding space than we expect? Can we know
    what each dimension of an embedding vector represents?
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入学习文本的哪些特征？为什么有时两个句子在嵌入空间中的距离比我们预期的更近或更远？我们能否知道嵌入向量的每个维度代表什么？
- en: A key limitation in embedding-based retrieval compared to traditional techniques
    is the lack of interpretability in ranking decisions. There is a whole body of
    research dedicated to improving interpretability of neural networks, LLMs, and
    embeddings. In [Chapter 5](ch05.html#chapter_utilizing_llms), we introduced some
    interpretability techniques for understanding LLMs. In this section, we will focus
    on embedding interpretability in particular. One benefit of understanding the
    features represented in embedding space is that we could leverage that knowledge
    to steer embeddings for our own purposes.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统技术相比，基于嵌入的检索的一个关键限制是排名决策中缺乏可解释性。有一系列研究致力于提高神经网络、LLMs和嵌入的可解释性。在[第5章](ch05.html#chapter_utilizing_llms)中，我们介绍了理解LLMs的一些可解释性技术。在本节中，我们将特别关注嵌入的可解释性。理解嵌入空间中表示的特征的好处是，我们可以利用这些知识来引导嵌入以实现我们的目的。
- en: One promising technique for imparting interpretability is to use SAEs. Let’s
    understand what they mean and how they are trained and used to enhance interpretability.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 一种赋予可解释性的有希望的技术是使用SAEs。让我们了解它们的意义以及它们是如何被训练和用于增强可解释性的。
- en: A language model may learn millions of features, but for any given input, only
    a few of those features are relevant or activated. This is what we mean by sparsity.
    Even as they learn lots of features, there are only a limited number of dimensions
    in an embedding vector. Therefore, each dimension contributes to many features
    that can interfere with each other. If you train a [sparse autoencoder](https://oreil.ly/oiXb7)
    over these embeddings, you can derive independent interpretable features.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型可能学习数百万个特征，但对于任何给定的输入，只有其中的一小部分特征是相关或被激活的。这就是我们所说的稀疏性。即使它们学习了许多特征，嵌入向量中的维度数量也是有限的。因此，每个维度都对许多可能相互干扰的特征做出贡献。如果你在这些嵌入上训练一个[sparse
    autoencoder](https://oreil.ly/oiXb7)，你可以推导出独立的可解释特征。
- en: In his [Prism project](https://oreil.ly/efzz1), Linus Lee uses SAEs to explore
    the features of a T5-based embedding model.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的[Prism项目](https://oreil.ly/efzz1)中，Linus Lee使用SAEs来探索基于T5的嵌入模型的特征。
- en: 'Some of the identified features include:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 一些已识别的特征包括：
- en: Presence of negation
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句首的否定存在
- en: Expression of possibility or speculation
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能性或推测的表达
- en: Employment and labor concepts
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就业和劳动概念
- en: Possessive syntax at sentence start
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 句首的所有格语法
- en: For a longer list of identified features, refer to [Linus Lee’s blog post](https://oreil.ly/efzz1).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于已识别特征的更长时间列表，请参阅[Linus Lee的博客文章](https://oreil.ly/efzz1)。
- en: Summary
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced the concept of embeddings, examined their internals,
    and showed various techniques for generating them. We also discussed techniques
    for fine-tuning embeddings on our own data. We learned how to determine the data
    granularities at which we construct embeddings, discussing several chunking techniques
    in the process. Finally, we explored techniques to visualize and interpret embeddings.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了嵌入的概念，检查了它们的内部结构，并展示了生成嵌入的各种技术。我们还讨论了在自身数据上微调嵌入的技术。我们学习了如何确定构建嵌入的数据粒度，并在过程中讨论了几种分块技术。最后，我们探讨了可视化和解释嵌入的技术。
- en: In the next chapter, we will explore RAG, an application paradigm that is by
    far the most popular use case for embeddings today. We will present the steps
    involved in a typical RAG workflow and review each of these steps in detail. We
    will also discuss the technical decisions involved in building a RAG application
    and provide pointers on how to think through various tradeoffs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨RAG，这是一种迄今为止最流行的嵌入应用范式。我们将介绍典型RAG工作流程中涉及的步骤，并详细回顾这些步骤。我们还将讨论构建RAG应用时涉及的技术决策，并提供关于如何权衡各种权衡的指导。
