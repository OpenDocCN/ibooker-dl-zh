- en: Chapter 11\. Representation Learning and Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how we can interface language models with
    external tools, including data stores. External data can be present in the form
    of text files, database tables, and knowledge graphs. Data can span a wide variety
    of content types, from proprietary domain-specific knowledge bases to intermediate
    results and outputs generated by LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: If the data are structured, for example residing in a relational database, the
    language model can issue a SQL query to retrieve the data it needs. But what if
    the data are present in unstructured form?
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to retrieve data from unstructured text datasets is to search by keywords
    or use regular expressions. For the Apple CFO example in the previous chapter,
    we can retrieve text containing CFO mentions from a corpus containing financial
    disclosures, hoping that it will contain the join date or tenure information.
    For instance, you can use the regex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Keyword search is limited in its effectiveness. There are a very large number
    of ways to express CFO join date or tenure in a corpus, if it is present at all.
    Trying to use a catch-all regex like the above could result in a large proportion
    of false positives.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we need to move beyond keyword search. Over the last few decades,
    the field of information retrieval has developed several methods like BM25 that
    have shaped search systems. We will learn more about these methods in [Chapter 12](ch12.html#ch12).
    In the LLM era, embedding-based search systems are fast becoming the standard
    way of implementing search.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how embeddings work. We will explore the concept
    of semantic similarity and examine various similarity measures. We will learn
    how to use popular embedding models and evaluate their performance. We will also
    show how to fine-tune embedding models to suit specific use cases and domains.
    We will show how to interpret these embeddings using sparse autoencoders (SAEs).
    Finally, we will discuss techniques for optimizing embeddings to reduce storage
    requirements and computational overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Representation learning is a subfield of machine learning that deals with learning
    to represent data in a way that captures its meaningful features, often in a low
    dimensional space. In the context of NLP, this involves transforming textual units
    like words, sentences, or paragraphs into vector form, called embeddings. Embeddings
    capture semantic (meaning-related) and pragmatic (social context-related) features
    of the input.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings can be generated using both open source libraries and paywalled APIs.
    [Sentence Transformers](https://oreil.ly/4OSVd) is a very well-known open source
    library for generating embeddings, and it provides access to embedding models
    that performs competitively with respect to proprietary ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s generate embeddings using the `Sentence Transformers` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1] `convert_to_tensor``=``True``)` `print``(``"Embedding size:"``,` `embedding``.``shape``[``0``])`
    `print``(``embedding``)` [PRE2]'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]` Output:    [PRE4]    For this model, the embedding size is 768, which
    means each vector has 768 dimensions. The sequence length of this particular model
    is 512, which means the input text is restricted to 512 tokens, beyond which it
    will be truncated. The embedding vector is made up of floating-point numbers,
    which by themselves are not interpretable. We will discuss techniques for interpreting
    embeddings later in this chapter.    Most embedding models used today are based
    on encoder-only language models, which we introduced in [Chapter 4](ch04.html#chapter_transformer-architecture).
    The underlying models are BERT, RoBERTa, MPNet, etc., and are typically fine-tuned
    on paraphrasing/question-answering/natural language inference datasets. Let’s
    see how to derive embeddings from these types of models (which is what the `sentence_transformers.encode()`
    function does under the hood):    [PRE5]   `In this example, the embedding is
    drawn from the [CLS] token of the last layer of the DistilBERT model. Other ways
    of extracting embeddings from models include:    *   Mean pooling, where the average
    is taken across all token outputs in the sequence           *   Max pooling, where
    the maximum value in each dimension across all tokens is taken           *   Weighted
    mean, where more weight is given to the last few tokens           *   Last token,
    where the embedding is just the encoder output of the last token              ######
    Tip    Whether the last token (or the first token) contains good representations
    of the entire sequence depends a lot on the pre-training and the fine-tuning objective.
    BERT’s pre-training objective (next-sentence prediction) ensures that the [CLS]
    token is much richer in representation than, say, RoBERTa, which doesn’t use the
    next-sentence prediction objective and thus its <s> start sequence token isn’t
    as informative.    Recently, decoder-based embedding models have started gaining
    prominence, like the [SGPT family of models](https://oreil.ly/AztT9). OpenAI exposes
    a single embedding endpoint for both search and similarity. OpenAI embeddings
    have a much larger maximum sequence length (8,192 tokens), and a much larger dimension
    size (1,536–3,072). Cohere and Jina are examples of other embedding providers.    Choosing
    the right model for your task depends on cost, latency, storage limitations, performance,
    and the data domain of your use case. I suggest starting off with the small but
    effective all-mpnet-base-v2 model available through the Sentence Transformers
    library, which I consider the workhorse of the field of NLP. As always, experimenting
    with different models never hurts. More tips on selecting the right models will
    be provided throughout the rest of the chapter. Later in the chapter, we will
    also show how to evaluate embedding models and introduce popular benchmarks.    ######
    Warning    There is no such thing as infinite compression! Embedding sizes are
    fixed, so the longer your input, the less information can be encoded in its embedding.
    Managing this tradeoff differs by use case.` [PRE6]``  [PRE7][PRE8]``py[PRE9][PRE10]
    chunks = [''The President of the U.S is Joe Biden'', ''Ramen consumption has increased
    in the last 5 months''] [PRE11] from sentence_transformers import SentenceTransformer,
    util sbert_model = SentenceTransformer(''msmarco-distilbert-base-tas-b'') chunk_embeddings
    = sbert_model.encode(chunks, show_progress_bar=True, device=''cuda'', normalize_embeddings=True,
    convert_to_tensor=True) query_embedding = sbert_model.encode(query, device=''cuda'',
    normalize_embeddings=True, convert_to_tensor=True) matches = util.semantic_search(query_embedding,
    chunk_embeddings, score_function=util.dot_score) [PRE12] [[{''corpus_id'': 0,
    ''score'': 0.8643729090690613},   {''corpus_id'': 1, ''score'': 0.6223753690719604}]]
    [PRE13] !pip install sentence-transformers  from sentence_transformers import
    SentenceTransformer, util model = SentenceTransformer(''all-mpnet-base-v2'')  sentences
    = [''After his 25th anniversary at the company, Mr. Pomorenko `confirmed` `that`
    `he` `is` `not` `retiring``'',  ''``Mr``.` `Pomorenko` `announced` `his` `retirement`
    `yesterday``'']` [PRE14] [PRE15]`` [PRE16] Cosine Similarity: 0.7870 [PRE17] Cosine
    Similarity: 0.7677! [PRE18]` [PRE19][PRE20][PRE21] from datasets import load_dataset
    from sentence_transformers import SentenceTransformer, SentenceTransformerTrainer
    from sentence_transformers.losses import TripletLoss  model = SentenceTransformer(
    "''all-mpnet-base-v2''")  dataset = load_dataset("csv", data_files="negatives_dataset.csv")  loss
    = TripletLoss(model)  trainer = SentenceTransformerTrainer(     model=model,     train_dataset=dataset     loss=loss    )
    trainer.train() model.save_pretrained("mpnet_finetuned_negatives") [PRE22] !pip
    install InstructorEmbedding  from InstructorEmbedding import INSTRUCTOR model
    = INSTRUCTOR(''hkunlp/instructor-large'')  customized_embeddings = model.encode(
    [[''Represent the question for retrieving supporting documents:'',   ''Who is
    the CEO of Apple''],  [''Represent the sentence for retrieval:'',   ''Tim Cook
    is the CEO of Apple''],  [''Represent the sentence for retrieval:'',   ''He is
    a musically gifted CEO''], ) [PRE23] ‘Represent the {domain} {text_type} for {task_objective}:’
    [PRE24] from sentence_transformers import SentenceTransformer from sentence_transformers
    import SentenceTransformerTrainer, losses from datasets import load_dataset  model
    = SentenceTransformer("all-mpnet-base-v2") train_dataset = load_dataset("csv",
    data_files="finetune_dataset.csv") loss = losses.MultipleNegativesRankingLoss(model)
    loss = losses.MatryoshkaLoss(model, loss, [768, 512, 256, 128]])  trainer = SentenceTransformerTrainer(     model=model,     train_dataset=train_dataset,     loss=loss,
    ) trainer.train() [PRE25] from sentence_transformers.quantization import quantize_embeddings  model
    = SentenceTransformer("all-mpnet-base-v2") embeddings = model.encode(["I heard
    the horses are excited for Halloween.", "Dalmatians are the most patriotic of
    dogs.", "This restaurant is making me `nostalgic``.``"])` [PRE26] [PRE27] `` `quantize_embeddings`
    also supports int8 quantization. In this scheme, the four bytes representing each
    dimension are converted into an integer value, represented in one byte. The integer
    can be either signed or unsigned, thus representing values between –127 and 127
    or between 0 and 255, respectively. The conversion process is guided using a calibration
    dataset of embeddings, from which we calculate the minimum and maximum value of
    each dimension. These values are then used in the normalization formula to convert
    the numbers from one range to another.    ###### Tip    It has been shown that
    for some [embedding models](https://oreil.ly/Mp3pu), binary embeddings perform
    better than int8 embeddings despite the reduced precision! This is largely because
    of the calibration dataset used and the challenge involved in mapping float values
    to buckets of int8 values. `` [PRE28]`` [PRE29] !pip install chromadb  import
    chromadb chroma_client = chromadb.Client()  collection = chroma_client.create_collection(name="mango_science")
    chunks = [''353 varieties of mangoes are now extinct'', ''Mangoes are grown in
    the tropics''] metadata = [{"topic": "extinction", "chapter": "2"}, {"topic":
    "regions",   "chapter": "5"}] unique_ids = [str(i) for i in range(len(chunks))]  collection.add(    documents=chunks,    metadatas=metadata,    ids=unique_ids   )
    results = collection.query(    query_texts=["Where are mangoes grown?"],    n_results=2,    where={"chapter":
    { "$ne": "2"}},    where_document={"$contains":"grown"} ) [PRE30]` [PRE31][PRE32][PRE33][PRE34][PRE35]
    [PRE36]'
  prefs: []
  type: TYPE_NORMAL
