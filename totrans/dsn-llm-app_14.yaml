- en: Chapter 11\. Representation Learning and Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how we can interface language models with
    external tools, including data stores. External data can be present in the form
    of text files, database tables, and knowledge graphs. Data can span a wide variety
    of content types, from proprietary domain-specific knowledge bases to intermediate
    results and outputs generated by LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: If the data are structured, for example residing in a relational database, the
    language model can issue a SQL query to retrieve the data it needs. But what if
    the data are present in unstructured form?
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to retrieve data from unstructured text datasets is to search by keywords
    or use regular expressions. For the Apple CFO example in the previous chapter,
    we can retrieve text containing CFO mentions from a corpus containing financial
    disclosures, hoping that it will contain the join date or tenure information.
    For instance, you can use the regex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Keyword search is limited in its effectiveness. There are a very large number
    of ways to express CFO join date or tenure in a corpus, if it is present at all.
    Trying to use a catch-all regex like the above could result in a large proportion
    of false positives.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we need to move beyond keyword search. Over the last few decades,
    the field of information retrieval has developed several methods like BM25 that
    have shaped search systems. We will learn more about these methods in [Chapter 12](ch12.html#ch12).
    In the LLM era, embedding-based search systems are fast becoming the standard
    way of implementing search.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how embeddings work. We will explore the concept
    of semantic similarity and examine various similarity measures. We will learn
    how to use popular embedding models and evaluate their performance. We will also
    show how to fine-tune embedding models to suit specific use cases and domains.
    We will show how to interpret these embeddings using sparse autoencoders (SAEs).
    Finally, we will discuss techniques for optimizing embeddings to reduce storage
    requirements and computational overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Representation learning is a subfield of machine learning that deals with learning
    to represent data in a way that captures its meaningful features, often in a low
    dimensional space. In the context of NLP, this involves transforming textual units
    like words, sentences, or paragraphs into vector form, called embeddings. Embeddings
    capture semantic (meaning-related) and pragmatic (social context-related) features
    of the input.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings can be generated using both open source libraries and paywalled APIs.
    [Sentence Transformers](https://oreil.ly/4OSVd) is a very well-known open source
    library for generating embeddings, and it provides access to embedding models
    that performs competitively with respect to proprietary ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s generate embeddings using the `Sentence Transformers` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: For this model, the embedding size is 768, which means each vector has 768 dimensions.
    The sequence length of this particular model is 512, which means the input text
    is restricted to 512 tokens, beyond which it will be truncated. The embedding
    vector is made up of floating-point numbers, which by themselves are not interpretable.
    We will discuss techniques for interpreting embeddings later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most embedding models used today are based on encoder-only language models,
    which we introduced in [Chapter 4](ch04.html#chapter_transformer-architecture).
    The underlying models are BERT, RoBERTa, MPNet, etc., and are typically fine-tuned
    on paraphrasing/question-answering/natural language inference datasets. Let’s
    see how to derive embeddings from these types of models (which is what the `sentence_transformers.encode()`
    function does under the hood):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the embedding is drawn from the [CLS] token of the last layer
    of the DistilBERT model. Other ways of extracting embeddings from models include:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean pooling, where the average is taken across all token outputs in the sequence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max pooling, where the maximum value in each dimension across all tokens is
    taken
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighted mean, where more weight is given to the last few tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Last token, where the embedding is just the encoder output of the last token
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Whether the last token (or the first token) contains good representations of
    the entire sequence depends a lot on the pre-training and the fine-tuning objective.
    BERT’s pre-training objective (next-sentence prediction) ensures that the [CLS]
    token is much richer in representation than, say, RoBERTa, which doesn’t use the
    next-sentence prediction objective and thus its <s> start sequence token isn’t
    as informative.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, decoder-based embedding models have started gaining prominence, like
    the [SGPT family of models](https://oreil.ly/AztT9). OpenAI exposes a single embedding
    endpoint for both search and similarity. OpenAI embeddings have a much larger
    maximum sequence length (8,192 tokens), and a much larger dimension size (1,536–3,072).
    Cohere and Jina are examples of other embedding providers.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the right model for your task depends on cost, latency, storage limitations,
    performance, and the data domain of your use case. I suggest starting off with
    the small but effective all-mpnet-base-v2 model available through the Sentence
    Transformers library, which I consider the workhorse of the field of NLP. As always,
    experimenting with different models never hurts. More tips on selecting the right
    models will be provided throughout the rest of the chapter. Later in the chapter,
    we will also show how to evaluate embedding models and introduce popular benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: Warning
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There is no such thing as infinite compression! Embedding sizes are fixed, so
    the longer your input, the less information can be encoded in its embedding. Managing
    this tradeoff differs by use case.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Search
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The true value of embeddings can be appreciated when we use them for representing
    a large text corpus. The vectors representing the data occupy what we call an
    embedding space. Similar texts are located closer to each other in the embedding
    space. This property allows us to use similarity measures to accomplish meaningful
    tasks like clustering or semantic search. Semantic search refers to techniques
    that take into account the meaning and context of queries and documents to identify
    documents that are most relevant to a given query.
  prefs: []
  type: TYPE_NORMAL
- en: We can visualize the embedding space by using dimensionality reduction techniques
    like [PCA](https://oreil.ly/Rk1M9) or [t-SNE](https://oreil.ly/0xNrB).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-1](#embedding-visualization) depicts the visualization of embeddings
    of posts on X (formerly Twitter) by members of the US Congress created by [Nomic
    AI](https://oreil.ly/XsXls) using its Atlas tool. You can view a detailed version
    of the visualization at [Nomic’s blog](https://oreil.ly/AORpk).'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore how we can use embeddings for semantic search. For a given user
    query, we can generate an embedding of the query and then identify document embeddings
    closest to it in the vector space. The texts corresponding to the top-k (k can
    be as small as 1 but can vary according to application needs) closest vectors
    are provided as a response to the search query. This process is called *retrieval*.
    The texts are then fed into the LLM prompt along with the user query, and the
    LLM uses the information provided in the context to answer the user query. This
    two-step process has traditionally been called the *retriever-reader* framework,
    with the LLM playing the role of the reader in this example.
  prefs: []
  type: TYPE_NORMAL
- en: '![embedding-visualization](assets/dllm_1101.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-1\. Embedding space visualization
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As a simple illustrative example, consider two sentences that make up our corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Given the query “president of usa,” we can encode the query and the chunks
    using Sentence Transformers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the similarity score is much higher for the first sentence,
    and thus we return the first sentence as the query response.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: There is a distinction between symmetric semantic search and asymmetric semantic
    search. In symmetric search, the query text is of similar size as the document
    text. In asymmetric search, the query text is much shorter than the document text,
    as with search engine and question-answering assistant queries. There are models
    available that are specialized for only symmetric or asymmetric search. In some
    models, the query and chunk texts are encoded using separate models.
  prefs: []
  type: TYPE_NORMAL
- en: Similarity Measures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Commonly used similarity measures include dot product, cosine similarity, and
    Euclidean distance. Refer to the [Pinecone](https://oreil.ly/X_qcD) tutorial on
    similarity measures if you need a backgrounder. While using embedding models,
    use the similarity measure that was used to train the model. You will find this
    information in the model card or Hugging Face model hub page.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you set `normalize_embeddings` to `True` as an argument in the `encode()`
    function, it will normalize the embeddings to unit length. This will ensure that
    both dot product and cosine similarity will have the same values. Note that dot
    product is a faster operation than cosine similarity. Sentence Transformers provides
    [separate models](https://oreil.ly/LOu75) trained on dot product and cosine similarity
    and mentions that models trained on dot product tend to prefer longer chunks during
    retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: While the notion of semantic similarity is powerful, it is not a panacea for
    all applications. The semantic similarity task is underspecified. To start with,
    there are several notions of similarity. Similarity refers to the sameness or
    alikeness of the entities being compared. But for the same two entities, some
    dimensions are similar and some are different.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the three sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: After his 25th anniversary at the company, Mr. Pomorenko confirmed that he is
    not retiring.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mr. Pomorenko announced his retirement yesterday.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Mr. Pomorenko did not announce his retirement yesterday.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Now let’s use the Sentence Transformers all-mpnet-base-v2 embedding model to
    encode these sentences and calculate their similarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'If you replace the second sentence with “Mr. Pomorenko did not announce his
    retirement yesterday,” the output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, both these sentences are perceived as equally similar to the
    first sentence. In some aspects, this is true. They are similar because they both
    talk about Mr. Pomorenko. They are also similar because both deal with the subject
    of retirement. On the other hand, one sentence conveys the opposite meaning to
    the other, by suggesting a retirement is happening versus not happening.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One way to handle the false positives arising due to the model using undesirable
    similarity dimensions (like negation) is to just increase the k value in the top-k
    results that are returned as a response to the query. Then, the LLM can distinguish
    between false positives and use the correct information for answering the query.
    However, increasing the top-k also increases the context length of the prompt,
    increasing latency and cost.
  prefs: []
  type: TYPE_NORMAL
- en: Our application requirements determine which similarity dimensions are important
    to us. If negation is an important relation for our application to distinguish,
    it might be a good idea to reflect that in our embedding space. This is where
    fine-tuning embedding models can come in handy. Fine-tuning embedding models allows
    you to “edit” your embedding space to your own liking. The process is relatively
    simple and can be potentially quite beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning embeddings can also be very useful when you are working with specialized
    data domains whose token distribution deviates from general-purpose data. Let’s
    now discuss how to fine-tune embedding models.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-Tuning Embedding Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Sentence Transformers library facilitates fine-tuning embedding models using
    the [`SentenceTransformerTrainer` class](https://oreil.ly/Jahep). To fine-tune
    an embedding model, we need a base model to fine-tune on, a training dataset,
    and a learning objective.
  prefs: []
  type: TYPE_NORMAL
- en: Base Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can fine-tune a fine-tuned model like all-mpnet-base-v2, or you can fine-tune
    a base model like MPNet, from which all-mpnet-base-v2 is defined. You will need
    more training data to fine-tune a base model than to further fine-tune an already
    fine-tuned model. Other candidates’ models for fine-tuning include [BGE-M3](https://oreil.ly/Sh8pZ)
    and [jina-embeddings-v3](https://oreil.ly/lFiWX). A full list of models available
    through Sentence Transformers can be accessed [online](https://oreil.ly/Onyuv).
    Remember to check the licenses for a given model before using it for commercial
    purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the factors to keep in mind while choosing a base model include the
    performance of the base model, the size of the embedding models (which determines
    how fast the model can encode text), the number of dimensions of the model (which
    determines the amount of storage taken up by the embeddings), and the licensing
    implications. The MPNet or all-mpnet-base-v2 is a solid first choice that has
    served me well on many projects.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If a model has been fine-tuned for a particular task like semantic search, it
    is not optimal to further fine-tune it on a different task.
  prefs: []
  type: TYPE_NORMAL
- en: Training Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are many different ways to structure your dataset. The most common way
    is in the form of triplets consisting of (anchor, positive, negative) examples.
    For a given anchor sentence, the positive sentence is a sentence we would like
    to be closer to the anchor sentence in embedding space, and the negative sentence
    is a sentence we would like to be farther apart from the anchor in embedding space.
    For example, to fine-tune the model to help it distinguish negation sentences,
    our training set can be composed of triplets where the negative sentence contradicts
    the anchor and the positive sentences.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 11-2](#embed-dataset) shows an embedding dataset composed of triplets
    for helping the model distinguish negation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![embed-dataset](assets/dllm_1102.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11-2\. Fine-tuning dataset for negation
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Datasets can also be composed of sentence pairs, where the sentences could represent
    a (query, response) pair, or a (passage, summary) pair, or a pair of paraphrases.
    The downstream use cases determine the type of dataset needed. The [Sentence Transformers
    website](https://oreil.ly/geI1M) shows all the different ways a dataset can be
    formatted.
  prefs: []
  type: TYPE_NORMAL
- en: Training datasets can be as small as a few thousand examples, to [billions of
    tokens](https://oreil.ly/oNI4n) when used for domain adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: Note that certain loss functions require your dataset to be in a specific format.
    We will discuss loss functions in detail next.
  prefs: []
  type: TYPE_NORMAL
- en: Loss Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall our discussion on loss functions for training LLMs in [Chapter 4](ch04.html#chapter_transformer-architecture).
    The [Sentence Transformers library](https://oreil.ly/9Qaop) supports a wide range
    of loss functions for training embedding models. Let’s explore a few commonly
    used ones.
  prefs: []
  type: TYPE_NORMAL
- en: For a triplet dataset, you can compute a [triplet loss](https://oreil.ly/yXHNU).
    For a training dataset consisting of an (anchor, positive, negative) triplet,
    the triplet loss minimizes the distance between the anchor sentence and the positive
    sentence, and maximizes the distance between the anchor sentence and the negative
    sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the loss is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: Loss = max(d(a, p) – d(a, n) + margin, 0)
  prefs: []
  type: TYPE_NORMAL
- en: where d is a distance measure, typically Euclidean distance. The margin is a
    hyperparameter that represents the distance by which the negative example should
    be farther away from the anchor than the positive example. When using Euclidean
    distance as the distance measure, I suggest a margin of 5, but make sure to tune
    it if you are not getting sufficient results.
  prefs: []
  type: TYPE_NORMAL
- en: If you are using a dataset composed of pairs like (query, response), (passage,
    summary), etc., you can use the [Multiple Negatives Ranking Loss](https://oreil.ly/oNcsQ).
  prefs: []
  type: TYPE_NORMAL
- en: In a batch containing (query, response) pairs (q1, r1), (q2, r2)…​(qn, rn),
    for each query, there will be a positive pair, e.g., (q1, r1) and n – 1 negative
    pairs, e.g., (q1, r2), (q1, r3)…​etc. The loss function minimizes the negative
    log likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Use [`CachedMultipleNegativesRankingLoss`](https://oreil.ly/QwBlI), available
    in Sentence Transformers, which allows you to use larger batch sizes, leading
    to better performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have discussed all the ingredients needed for fine-tuning, let’s
    put it all together with the `SentenceTransformerTrainer` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The full code is available in the book’s [GitHub repo](https://oreil.ly/llm-playbooks).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Watch out for overfitting! You can reduce your learning rate if you notice the
    model overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '[Zhou et al.](https://oreil.ly/BPdRD) show that in the context of embeddings,
    cosine similarity tends to underestimate the similarity between high-frequency
    words. This is because high-frequency words occupy distinct regions in the embedding
    space, leading to larger distances from other words. On the other hand, low-frequency
    words tend to be more concentrated geometrically.'
  prefs: []
  type: TYPE_NORMAL
- en: Instruction Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far we have seen that embedding models are specialized for solving a specific
    task, like semantic search or paraphrasing. A recent development ties together
    embedding models and the concept of instruction-tuning, which we discussed in
    [Chapter 6](ch06.html#llm-fine-tuning). Imagine if you could use the same embedding
    model to generate different embeddings for the same document, based on the task
    it is going to be used for. One such model is called Instructor. [Instructor embeddings](https://oreil.ly/mSIhG)
    allow you to optionally specify the domain, text type (whether it is a sentence,
    paragraph, etc.), and task, along with the text during encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The creators of Instructor recommend using this instruction template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: where `{domain}` represents the domain of the text like law, finance, etc. The
    optional `{text_type}` represents the unit of text being encoded, like a question,
    sentence, paragraph, etc. `{task_objective}` represents the task for which we
    are using the embeddings, like semantic search, paraphrase detection, etc.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of semantic search, they recommend the instruction “Represent
    the question for retrieving supporting documents” for queries, and “Represent
    the sentence for retrieval” for documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another way the principle of instruction-tuning can be applied to retrieval
    is with *description-based retrieval*, where the query can be the description
    of the text that needs to be retrieved, rather than an instantiation (example)
    of the text that needs to be retrieved. [Ravfogel et al.](https://oreil.ly/rp8Q-)
    have published description-based retrieval models that in my experience are very
    effective. Note that these models have a dual-encoder setup: separate models are
    used to encode the query and documents.'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Embedding Size
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many applications involve generating billions of embeddings. As we have seen,
    modern embeddings sometimes have as many as thousands of dimensions. If each dimension
    is represented in float32, then it needs four bytes of memory per dimension. Therefore,
    storing 100 million vectors generated from the all-mpnet-base-v2 model, which
    has 768 dimensions, needs close to 300 GB of memory!
  prefs: []
  type: TYPE_NORMAL
- en: It is not uncommon to represent a single sentence, almost always no longer than
    40 tokens, with a 768-dimension vector. Do we really need 768 dimensions to represent
    40 tokens? The reality is that embedding training is very inefficient, and a large
    number of dimensions are not really useful.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, several embedding truncation and quantization approaches have been
    developed to optimize embedding size and reduce storage and compute requirements.
    If you are operating in an environment with more than a few million vectors, these
    techniques are likely to be useful to you. Let’s look at some of these approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Matryoshka Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Matryoshka embeddings are named after [Matryoshka dolls](https://oreil.ly/OC6Yj),
    which refer to a set of wooden dolls that are placed inside each other in decreasing
    order of size, originating from Russia. Matryoshka embeddings are trained such
    that the earlier dimensions of the vector contain more important information than
    the later dimensions. This allows us to truncate vectors depending on the requirements
    of the application with respect to cost, latency, and performance.
  prefs: []
  type: TYPE_NORMAL
- en: The technique used to train these embeddings is called Matryoshka Representation
    Learning (MRL). In MRL, we first choose a set of truncation dimensions. For example
    a 1,024-dimension vector can have truncation dimensions 128, 256, 512, and 768\.
    During the training process, we calculate the loss over each of the truncation
    dimensions as well as the full dimension. The losses are then added and weighted.
    In our example, the first 128 dimensions learn from the loss calculated over the
    first 128, 256, 512, 768, and 1,024 dimensions of the vector. The end result is
    that the initial dimensions of the vector will encode more important information
    because they learn from richer losses.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training using MRL is supported by the Sentence Transformers library. Let’s
    see how it works in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[Tom Aarsen](https://oreil.ly/sA5fo) observed in his experiments that even
    at 8.3% of the original embedding size, the Matryoshka model preserves 98.37%
    of the original performance. This makes it a very effective technique that will
    come in handy when you are working with large datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to how we can reduce the effective dimension of our embeddings using
    MRL, we can also reduce the effective number of layers of the embedding model,
    leading to faster inference. This is done by extracting embeddings from the lower
    layers of the model. To facilitate the lower layers of the model aligning high-quality
    embeddings with the embeddings of the last layer of the model, a K-L divergence
    loss is employed between the final layer and each of the lower layers. This technique
    was first introduced by [Li et al.’s](https://oreil.ly/fzIPD) Espresso Sentence
    Embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: '[Tom Aarsen](https://oreil.ly/DIoTe) observed in his experiments that removing
    half the layers leads to a 2x improvement in speed with 85% of the original performance
    preserved.'
  prefs: []
  type: TYPE_NORMAL
- en: The Sentence Transformers library allows you to combine Matryoshka representations
    with layer reduction using the [Matryoshka2dLoss](https://oreil.ly/xzG-a).
  prefs: []
  type: TYPE_NORMAL
- en: Binary and Integer Embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An alternative to truncation is quantization. With binary and integer quantization,
    the number of vector dimensions remains the same, but each dimension is represented
    by fewer bits. Recall that typically embedding vectors are represented in float32,
    thus taking four bytes of memory per dimension.
  prefs: []
  type: TYPE_NORMAL
- en: At the extreme level, the four bytes can be represented with just one bit, resulting
    in a 32x reduction in storage requirements. This type of compression is generally
    done by sacrificing the precision of the vector values.
  prefs: []
  type: TYPE_NORMAL
- en: A simple way to convert a four-byte vector to a one-bit vector is to assign
    a value of 1 if the original value is positive, and 0 if it is negative. Note
    that you might need to perform some scaling to achieve best results. After packing
    these bits into bytes, a 512-dimension vector can be represented in just 512 /
    8 = 64 bytes, instead of 512 × 4 = 2,048 bytes.
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage with using binary embeddings is that computing similarity
    only needs simple bitwise operations, thus vastly speeding up retrieval. However,
    quantization negatively affects performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use the `Sentence Transformers` library to quantize embeddings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '`quantize_embeddings` also supports int8 quantization. In this scheme, the
    four bytes representing each dimension are converted into an integer value, represented
    in one byte. The integer can be either signed or unsigned, thus representing values
    between –127 and 127 or between 0 and 255, respectively. The conversion process
    is guided using a calibration dataset of embeddings, from which we calculate the
    minimum and maximum value of each dimension. These values are then used in the
    normalization formula to convert the numbers from one range to another.'
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It has been shown that for some [embedding models](https://oreil.ly/Mp3pu),
    binary embeddings perform better than int8 embeddings despite the reduced precision!
    This is largely because of the calibration dataset used and the challenge involved
    in mapping float values to buckets of int8 values.
  prefs: []
  type: TYPE_NORMAL
- en: Product Quantization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another promising quantization method is called [*product quantization*](https://oreil.ly/aJq2C).
    In this technique, a vector is divided into chunks of equal size. The chunks are
    then clustered. The number of clusters is set to the number of values that can
    be represented by the quantized embedding. For example, if we aim to quantize
    to int8, then the number of values that can be represented is 256, and thus the
    number of clusters is 256\. Each cluster is associated with an identifier, which
    is a unique value between 0 and 255\. Each chunk belongs to the cluster whose
    centroid the chunk is closest to.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the original float32 vector can now be represented by a list of cluster
    identifiers corresponding to the clusters the chunks belong to. The larger the
    chunk size, the more the compression. Thus if the vector is divided into five
    chunks, the resulting embedding will have only five dimensions. Unlike int8 and
    binary quantization, product quantization also reduces the number of dimensions
    needed to represent a vector. However, the performance drop is higher.
  prefs: []
  type: TYPE_NORMAL
- en: Choose your quantization technique by determining your relative product priorities
    for criteria like cost, performance, and speed.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Optimizing embeddings for storage come with a performance hit. However, if there
    is plenty of redundancy in the document corpus, answers to typical user queries
    might be found in several documents, and hence the user may not feel this performance
    drop.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen various techniques to practically implement embedding-based
    retrieval, let’s next figure out the textual units we need to embed into distinct
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Chunking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As noted in [“Introduction to Embeddings”](#introduction-to-embeddings), embedding
    models support very limited context lengths, and the effectiveness of embedding
    similarity matching decreases as the text length increases. Therefore, it is natural
    to split documents into manageable units called chunks and embed each chunk into
    one or more vectors.
  prefs: []
  type: TYPE_NORMAL
- en: A chunk can be defined as a semantically coherent and not necessarily contiguous
    part of a document. The average chunk length depends on the context length supported
    by the language model, and the number of chunks returned to the model (the top-k)
    in response to a user query. As models become increasingly affordable to operate
    and support ever-larger context lengths, the permissible chunk size grows.
  prefs: []
  type: TYPE_NORMAL
- en: Each chunk can either be represented by a single vector or can be further broken
    down into units, with each unit being represented by a separate vector. A unit
    could be a sentence, a paragraph, or even a section. Typically, the smaller the
    unit, the better. For your application, test your expected user queries against
    different granularities and see what works best.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a scenario where a document corpus has been broken down into units
    represented by embeddings. For a given user query, we can calculate the cosine
    similarity between the user query vector and each of the document vectors. The
    chunks corresponding to the most similar vectors are then retrieved. This ensures
    that the embedding matching happens at a lower granularity, like a sentence, but
    the model receives the entirety of the chunk the sentence belongs to, thus providing
    sufficient background context to the model.
  prefs: []
  type: TYPE_NORMAL
- en: A question I am frequently asked by ML practitioners is, “What is the ideal
    chunk size and what are some effective chunking strategies?” Determining the right
    chunk size and boundaries are key challenges practitioners face when using embedding-based
    retrieval. In this section, we will discuss a few chunking strategies, introduced
    in order of increasing complexity.
  prefs: []
  type: TYPE_NORMAL
- en: In the basic implementation of embedding-based retrieval, each vector is a distinct
    island, disconnected from all other islands. The text represented by Vector A
    is not able to influence text represented by Vector B in any way. Therefore, we
    need to connect these islands in some way or make these islands as self-contained
    as possible. With these objectives in mind, let’s look at some chunking strategies
    that go beyond naive paragraph or section splitting.
  prefs: []
  type: TYPE_NORMAL
- en: Sliding Window Chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Consider a situation where the embedding similarity function returns a unit
    in Chunk 45 as the most similar vector to your query vector. However, text in
    Chunk 44, which immediately precedes Chunk 45 in the document, contains relevant
    information contextualizing Chunk 45\. The vectors in Chunk 44 have a very low
    similarity score with the query, and as a result, Chunk 44 is not selected for
    retrieval. One way to fix this is by using sliding window chunking, where each
    text can be present in multiple chunks, thus allowing neighboring context to be
    effectively represented in a coherent block.
  prefs: []
  type: TYPE_NORMAL
- en: Metadata-Aware Chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any metadata that you have about the document can be leveraged to determine
    chunking boundaries. Useful metadata information includes paragraph boundaries,
    section and subsection boundaries, etc. If the metadata isn’t already available,
    you might need to use document parsing techniques to extract this information.
    Several libraries can facilitate this, including [Unstructured](https://oreil.ly/CoX46).
  prefs: []
  type: TYPE_NORMAL
- en: Layout-Aware Chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A more involved form of metadata-aware chunking is layout-aware chunking. In
    this approach we use computer vision techniques to extract layout information
    about the document, including the placement and scope of textual elements, the
    titles, subtitles, font size of text, etc.; use this metadata to inform the chunking
    process. Both open source and proprietary tools can facilitate layout extraction.
    They include tools like [Amazon Textractor](https://oreil.ly/fvkiT), [Unstructured](https://oreil.ly/CoX46),
    and layout-aware language models like [LayoutLMv3](https://oreil.ly/Od5fA).
  prefs: []
  type: TYPE_NORMAL
- en: For example, using this approach we can know the scope of a subsection, and
    thus insert the subsection title at the beginning of each chunk comprising text
    from that subsection.
  prefs: []
  type: TYPE_NORMAL
- en: You can also use techniques like ColPali that employ vision models to directly
    embed a page or section of the document and perform retrieval over it. This may
    remove the need for chunking entirely but might be more expensive overall.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic Chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The principle behind semantic chunking is that similar information should be
    grouped into coherent chunks. Paragraph boundaries provide a weak signal for semantic
    chunking, but more advanced methods can be employed. One approach is to cluster
    the document based on topics, with each chunk containing information pertaining
    to the same topic. The chunks need not necessarily be built from contiguous text
    if it makes sense for the application. A more advanced approach is to use [Bollinger
    bands-based chunking](https://oreil.ly/1MwK1). The book’s [GitHub repository](https://oreil.ly/llm-playbooks)
    contains an experimental implementation of this form of chunking.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic chunking can also be employed to connect different chunks with each
    other. Once the chunks have been assigned, similar chunks can be grouped based
    on embedding similarity, allowing them to be retrieved along with the chunk having
    the highest similarity score. Each chunk does not necessarily need to consist
    of content from the same document, as long as the metadata associated with each
    sub-chunk is retained.
  prefs: []
  type: TYPE_NORMAL
- en: A basic implementation of semantic chunking is available in [LangChain](https://oreil.ly/tm8tk).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Highly performant semantic chunking can be performed through LLMs. But it will
    be a huge cost overhead if the size of your data corpus is very large. Sometimes
    good old regex can be enough. Jina AI created a complex 50-line [regex-based chunker](https://oreil.ly/x5UO8)
    that you can try as an initial option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite using all these techniques, effective chunking still remains a problem.
    Consider the following real-world example from a financial document:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Page 5: *All numbers in the document are in millions*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Page 84: *The related party transaction amounts to $213.45*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In this case the related party transaction actually amounts to $213M dollars
    but the LLM would never know this because the text from page 5 is not likely to
    be part of the same chunk.
  prefs: []
  type: TYPE_NORMAL
- en: A related problem is the difficulty in understanding scope boundaries. When
    does a subsection end and a new subsection begins? What is the scope of the rule
    in page 5 in the given example? What if it is overridden in the middle of a document?
    Not all documents have perfect visual cues or structure. Not all documents are
    well structured into sections, subsections, and paragraphs. These are unsolved
    problems and are the cause of a sizable proportion of RAG failure modes.
  prefs: []
  type: TYPE_NORMAL
- en: Late Chunking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One way of supporting long-range dependencies in text is to use [late chunking](https://oreil.ly/IxTQx),
    a method introduced by Jina AI. Recall from earlier in the chapter that embeddings
    are generated by typically pooling the vectors from the last layer of the underlying
    language model.
  prefs: []
  type: TYPE_NORMAL
- en: Given that we have access to long-context language models that can accept an
    entire long document in a single input, we can use such a long-context model as
    our underlying model for generating embeddings. We feed an entire document (or
    as large a part as the model can handle) to the long-context model, so that vectors
    are generated for each of the input tokens. As explained in [Chapter 4](ch04.html#chapter_transformer-architecture),
    each token vector encapsulates its meaning based on its relationship with all
    other tokens in the sequence. This enables long-context dependencies to be captured.
  prefs: []
  type: TYPE_NORMAL
- en: The pooling operation to extract the embeddings is performed on smaller segments
    of the input, where the segment boundaries can be determined by any of the chunking
    algorithms. Thus, we can have several embeddings representing the same document
    but each of them representing distinct parts of the input.
  prefs: []
  type: TYPE_NORMAL
- en: Vector Databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Depending on your application, you may have to deal with millions or billions
    of vectors, with the need to generate and store new vectors and their associated
    metadata tags every day. Vector databases facilitate this. Both self-hosted and
    cloud-based, open source, and proprietary options are available. Weviate, Milvus,
    Pinecone, Chroma, Qdrant, and LanceDB are some of the popular vector databases.
    More established players like ElasticSearch, Redis, and Postgres also provide
    vector database support.
  prefs: []
  type: TYPE_NORMAL
- en: These days, the features provided by vector databases are converging, given
    the prevalence of a small set of very popular retrieval use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now look at how vector databases work. Probably the simplest one to get
    started with is Chroma, which is open source and can run locally on your machine
    or can be deployed on AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Most vector databases offer:'
  prefs: []
  type: TYPE_NORMAL
- en: Approximate nearest neighbor search in addition to exact search, to reduce latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to filter using metadata, like the *where* clause in SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to integrate keyword search or algorithms like BM25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support Boolean search operations, so that multiple search clauses can be combined
    with AND or OR operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ability to update or delete entries in the database in real time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interpreting Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What features of text do embeddings learn? Why are two sentences sometimes closer
    to/farther from each other in the embedding space than we expect? Can we know
    what each dimension of an embedding vector represents?
  prefs: []
  type: TYPE_NORMAL
- en: A key limitation in embedding-based retrieval compared to traditional techniques
    is the lack of interpretability in ranking decisions. There is a whole body of
    research dedicated to improving interpretability of neural networks, LLMs, and
    embeddings. In [Chapter 5](ch05.html#chapter_utilizing_llms), we introduced some
    interpretability techniques for understanding LLMs. In this section, we will focus
    on embedding interpretability in particular. One benefit of understanding the
    features represented in embedding space is that we could leverage that knowledge
    to steer embeddings for our own purposes.
  prefs: []
  type: TYPE_NORMAL
- en: One promising technique for imparting interpretability is to use SAEs. Let’s
    understand what they mean and how they are trained and used to enhance interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: A language model may learn millions of features, but for any given input, only
    a few of those features are relevant or activated. This is what we mean by sparsity.
    Even as they learn lots of features, there are only a limited number of dimensions
    in an embedding vector. Therefore, each dimension contributes to many features
    that can interfere with each other. If you train a [sparse autoencoder](https://oreil.ly/oiXb7)
    over these embeddings, you can derive independent interpretable features.
  prefs: []
  type: TYPE_NORMAL
- en: In his [Prism project](https://oreil.ly/efzz1), Linus Lee uses SAEs to explore
    the features of a T5-based embedding model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the identified features include:'
  prefs: []
  type: TYPE_NORMAL
- en: Presence of negation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expression of possibility or speculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Employment and labor concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possessive syntax at sentence start
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a longer list of identified features, refer to [Linus Lee’s blog post](https://oreil.ly/efzz1).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we introduced the concept of embeddings, examined their internals,
    and showed various techniques for generating them. We also discussed techniques
    for fine-tuning embeddings on our own data. We learned how to determine the data
    granularities at which we construct embeddings, discussing several chunking techniques
    in the process. Finally, we explored techniques to visualize and interpret embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore RAG, an application paradigm that is by
    far the most popular use case for embeddings today. We will present the steps
    involved in a typical RAG workflow and review each of these steps in detail. We
    will also discuss the technical decisions involved in building a RAG application
    and provide pointers on how to think through various tradeoffs.
  prefs: []
  type: TYPE_NORMAL
