["```py\nCurrent working directory\n    pos\n        p1.txt\n        p2.txt\n    neg\n        n1.txt\n        n2.txt\n```", "```py\nimport io\nimport os\nimport re\nimport shutil\nimport string\nimport tensorflow as tf\n\nurl = \"https://ai.stanford.edu/~amaas/data/sentiment/\n       aclImdb_v1.tar.gz\"\n\nds = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n                                    untar=True, cache_dir='.',\n                                    cache_subdir='')\n```", "```py\nds_dir = os.path.join(os.path.dirname(ds), 'aclImdb')\n```", "```py\ntrain_dir = os.path.join(ds_dir, 'train')\nos.listdir(train_dir)\n\n['neg',\n 'unsup',\n 'urls_neg.txt',\n 'urls_unsup.txt',\n 'pos',\n 'urls_pos.txt',\n 'unsupBow.feat',\n 'labeledBow.feat']\n```", "```py\nunused_dir = os.path.join(train_dir, 'unsup')\nshutil.rmtree(unused_dir)\n```", "```py\n!ls -lrt ./aclImdb/train\n-rw-r--r-- 1 7297 1000  2450000 Apr 12  2011 urls_unsup.txt\ndrwxr-xr-x 2 7297 1000   364544 Apr 12  2011 pos\ndrwxr-xr-x 2 7297 1000   356352 Apr 12  2011 neg\n-rw-r--r-- 1 7297 1000   612500 Apr 12  2011 urls_pos.txt\n-rw-r--r-- 1 7297 1000   612500 Apr 12  2011 urls_neg.txt\n-rw-r--r-- 1 7297 1000 21021197 Apr 12  2011 labeledBow.feat\n-rw-r--r-- 1 7297 1000 41348699 Apr 12  2011 unsupBow.feat\n```", "```py\nbatch_size = 1024\nseed = 123\n```", "```py\ntrain_ds = tf.keras.preprocessing.text_dataset_from_directory(\n    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n    subset='training', seed=seed)\n```", "```py\nval_ds = tf.keras.preprocessing.text_dataset_from_directory(\n    'aclImdb/train', batch_size=batch_size, validation_split=0.2,\n    subset='validation', seed=seed)\n```", "```py\nFound 25000 files belonging to 2 classes.\nUsing 20000 files for training.\nFound 25000 files belonging to 2 classes.\nUsing 5000 files for validation.\n```", "```py\nimport random\nidx = random.sample(range(1, batch_size), 5)\nfor text_batch, label_batch in train_ds.take(1):\n  for i in idx:\n    print(label_batch[i].numpy(), text_batch.numpy()[i])\n```", "```py\n1 b'Very Slight Spoiler<br /><br /> This movie (despite being\u2026.\n1 b\"Not to mention easily Pierce Brosnan's best performance\u2026.\n0 b'Bah. Another tired, desultory reworking of an out of copyright\u2026\n0 b'All the funny things happening in this sitcom is based on the\u2026\n0 b'This is another North East Florida production, filmed mainly\u2026\n\n```", "```py\nfile_name,label\n7176723954_e41618edc1_n.jpg,sunflowers\n2788276815_8f730bd942.jpg,roses\n6103898045_e066cdeedf_n.jpg,dandelion\n1441939151_b271408c8d_n.jpg,daisy\n2491600761_7e9d6776e8_m.jpg,roses\n```", "```py\n!wget https://data.mendeley.com/public-files/datasets/jxmfrvhpyz/\nfiles/283004ff-e529-4c3c-a1ee-4fb90024dc94/file_downloaded \\\n--output-document flower_photos.zip\n```", "```py\n!unzip -q flower_photos.zip\n```", "```py\ndrwxr-xr-x 3 root root      4096 Nov  9 03:24 flower_photos\n-rw-r--r-- 1 root root 228396554 Nov  9 20:14 flower_photos.zip\n```", "```py\n!ls -alt flower_photos\n```", "```py\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n```", "```py\ntraindf=pd.read_csv('flower_photos/all_labels.csv',dtype=str)\n```", "```py\ndata_root = 'flower_photos/flowers'\nIMAGE_SIZE = (224, 224)\nTRAINING_DATA_DIR = str(data_root)\nBATCH_SIZE = 32\n```", "```py\ndatagen_kwargs = dict(rescale=1./255, validation_split=.20)\n```", "```py\ndataflow_kwargs = dict(target_size=IMAGE_SIZE, \nbatch_size=BATCH_SIZE,\ninterpolation=\"bilinear\")\n```", "```py\ntrain_datagen = tf.keras.preprocessing.image.\n                ImageDataGenerator(**datagen_kwargs)\n```", "```py\ntrain_generator=train_datagen.flow_from_dataframe(\ndataframe=traindf,\ndirectory=data_root,\nx_col=\"file_name\",\ny_col=\"label\",\nsubset=\"training\",\nseed=10,\nshuffle=True,\nclass_mode=\"categorical\",\n**dataflow_kwargs)\n```", "```py\nvalid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n**datagen_kwargs)\nvalid_generator=valid_datagen.flow_from_dataframe(\ndataframe=traindf,\ndirectory=data_root,\nx_col=\"file_name\",\ny_col=\"label\",\nsubset=\"validation\",\nseed=10,\nshuffle=True,\nclass_mode=\"categorical\",\n**dataflow_kwargs)\n```", "```py\nimage_batch, label_batch = next(iter(train_generator))\nfig, axes = plt.subplots(8, 4, figsize=(20, 40))\naxes = axes.flatten()\nfor img, lbl, ax in zip(image_batch, label_batch, axes):\n    ax.imshow(img)\n    label_ = np.argmax(lbl)\n    label = idx_labels[label_]\n    ax.set_title(label)\n    ax.axis('off')\nplt.show()\n```", "```py\nmdl = tf.keras.Sequential([\n      tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n                 hub.KerasLayer(\n\"https://tfhub.dev/tensorflow/resnet_50/feature_vector/1\", \ntrainable=False),\n\ntf.keras.layers.Dense(5, activation='softmax', \nname = 'custom_class')\n])\nmdl.build([None, 224, 224, 3])\n```", "```py\nmdl.compile(\n  optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9),\n  loss=tf.keras.losses.CategoricalCrossentropy(\n  from_logits=True, \n  label_smoothing=0.1),\n  metrics=['accuracy'])\n```", "```py\nsteps_per_epoch = train_generator.samples // \ntrain_generator.batch_size\nvalidation_steps = valid_generator.samples // \nvalid_generator.batch_size\n\nmdl.fit(\n    train_generator,\n    epochs=13, steps_per_epoch=steps_per_epoch,\n    validation_data=valid_generator,\n    validation_steps=validation_steps)\n```", "```py\nEpoch 10/13\n90/90 [==============================] - 17s 194ms/step\nloss: 1.0338 - accuracy: 0.9602 - val_loss: 1.0779\nval_accuracy: 0.9020\nEpoch 11/13\n90/90 [==============================] - 17s 194ms/step\nloss: 1.0311 - accuracy: 0.9623 - val_loss: 1.0750\nval_accuracy: 0.9077\nEpoch 12/13\n90/90 [==============================] - 17s 193ms/step\nloss: 1.0289 - accuracy: 0.9672 - val_loss: 1.0741\nval_accuracy: 0.9091\nEpoch 13/13\n90/90 [==============================] - 17s 192ms/step\nloss: 1.0266 - accuracy: 0.9693 - val_loss: 1.0728\nval_accuracy: 0.9034\n```", "```py\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfashion_mnist = tf.keras.datasets.fashion_mnist\n(train_images, train_labels), \n(test_images, test_labels) = fashion_mnist.load_data()\n```", "```py\nprint(type(train_images), type(train_labels))\n\n<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n```", "```py\nprint(train_images.shape, train_labels.shape)\n\n(60000, 28, 28) (60000,)\n```", "```py\nplt.figure()\nplt.imshow(train_images[5])\nplt.colorbar()\nplt.grid(False)\nplt.show()\n```", "```py\ntrain_images = train_images/255\n```", "```py\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train_images, \ntrain_labels))\n```", "```py\nSHUFFLE_BUFFER_SIZE = 10000\nTRAIN_BATCH_SIZE = 50\nVALIDATION_BATCH_SIZE = 10000\n\nvalidation_ds = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).\ntake(VALIDATION_SAMPLE_SIZE).\nbatch(VALIDATION_BATCH_SIZE)\n\ntrain_ds = train_dataset.skip(VALIDATION_BATCH_SIZE).\nbatch(TRAIN_BATCH_SIZE).repeat()\n```", "```py\nsteps_per_epoch = 50000 // TRAIN_BATCH_SIZE\nvalidation_steps = 10000 // VALIDATION_BATCH_SIZE\n```", "```py\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28)),\n    tf.keras.layers.Dense(30, activation='relu'),\n    tf.keras.layers.Dense(10)\n])\n\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(),\n  loss=tf.keras.losses.SparseCategoricalCrossentropy(\n  from_logits=True),\n  metrics=['sparse_categorical_accuracy'])\n```", "```py\nmodel.fit(\n    train_ds,\n    epochs=13, steps_per_epoch=steps_per_epoch,\n    validation_data=validation_ds,\n    validation_steps=validation_steps)\n```", "```py\n\u2026\nEpoch 10/13\n1562/1562 [==============================] - 4s 3ms/step\nloss: 0.2982 - sparse_categorical_accuracy: 0.8931\nval_loss: 0.3476 - val_sparse_categorical_accuracy: 0.8778\nEpoch 11/13\n1562/1562 [==============================] - 4s 3ms/step\nloss: 0.2923 - sparse_categorical_accuracy: 0.8954\nval_loss: 0.3431 - val_sparse_categorical_accuracy: 0.8831\nEpoch 12/13\n1562/1562 [==============================] - 4s 3ms/step\nloss: 0.2867 - sparse_categorical_accuracy: 0.8990\nval_loss: 0.3385 - val_sparse_categorical_accuracy: 0.8854\nEpoch 13/13\n1562/1562 [==============================] - 4s 3ms/step\nloss: 0.2826 - sparse_categorical_accuracy: 0.8997\nval_loss: 0.3553 - val_sparse_categorical_accuracy: 0.8811\n\n```"]