<html><head></head><body>
<div class="calibre1" id="sbo-rt-content"><h1 class="tochead" id="heading_id_2">16 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/><a id="idTextAnchor005"/>Pretrained large<br class="calibre8"/>
  language models and the LangChain library</h1>
<p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="idIndexMarker001"/><a id="marker-365"/></p>
<ul class="calibre5">
<li class="co-summary-bullet">Using pretrained large language models for text, image, speech, and code generation</li>
<li class="co-summary-bullet">Few-shot, one-shot, and zero-shot prompting techniques</li>
<li class="co-summary-bullet">Creating a zero-shot personal assistant with LangChain</li>
<li class="co-summary-bullet">Limitations and ethical concerns of generative AI</li>
</ul>
<p class="body">The rise of pretrained large language models (LLMs) has transformed the field of natural language processing (NLP) and generative tasks. OpenAI’s GPT series, a notable example, showcases the extensive capabilities of these models in producing life-like text, images, speech, and even code. The effective utilization of these pretrained LLMs is essential for several reasons. It enables us to deploy advanced AI functionalities without the need for vast resources to develop and train these models. Moreover, understanding these LLMs paves the way for innovative applications that leverage NLP and generative AI, fostering progress across various industries.<a id="idIndexMarker002"/></p>
<p class="body">In a world increasingly influenced by AI, mastering the integration and customization of pretrained LLMs offers a crucial competitive advantage. As AI evolves, leveraging these sophisticated models becomes vital for innovation and success in the digital landscape.</p>
<p class="body">Typically, these models are operated through browser-based interfaces, which vary across different LLMs that function independently of each other. Each model has unique strengths and specialties. Interfacing through a browser limits our ability to fully take advantage of the potential of each specific LLM. Utilizing programming languages like Python, particularly through tools such as the LangChain library, provides substantial benefits for the following reasons.</p>
<p class="body">Python’s role in interacting with LLMs enhances the automation of workflows and processes. Python scripts, capable of running autonomously, facilitate uninterrupted operations without the need for manual input. This is especially beneficial for businesses that regularly handle large amounts of data. For instance, a Python script could autonomously generate monthly reports by querying an LLM, synthesizing the data insights, and disseminating these findings via email or into a database. Python offers a greater level of customization and control in managing interactions with LLMs than browser-based interfaces do, enabling us to craft custom code to meet specific operational needs such as implementing conditional logic, processing multiple requests in loops, or managing exceptions. This adaptability is essential for customizing outputs to meet particular business objectives or research inquiries.</p>
<p class="body">Python’s extensive collection of libraries makes it ideally suited for integrating LLMs with existing software and systems. A prime example of this is the LangChain library, which extends Python’s functionality with LLMs. LangChain enables the combination of multiple LLMs or the integration of LLM capabilities with other services, such as the Wikipedia API or the Wolfram Alpha API, which will be covered later in this chapter. This capability of “chaining” different services allows for the construction of sophisticated, multistep AI systems where tasks are segmented and handled by the best-suited models or services, enhancing both performance and accuracy.<a id="idIndexMarker003"/><a id="idIndexMarker004"/><a id="marker-366"/></p>
<p class="body">To that end, in this chapter, you’ll first learn how to use the OpenAI API to create various content using Python programming: text, images, speech, and Python code. You’ll also learn the difference between few-shot, one-shot, and zero-shot content generation. Few-shot prompting means you give the model multiple examples to help it understand the task, while one-shot or zero-shot prompting means one example or no example is provided.</p>
<p class="body">Modern LLMs such as ChatGPT are trained on preexisting knowledge a few months ago so they cannot provide recent or real-time information such as weather conditions, flight status, or stock prices. You’ll learn to combine LLMs with Wolfram Alpha and Wikipedia APIs using the LangChain library to create a zero-shot know-it-all personal assistant.</p>
<p class="body">Despite LLMs’ impressive capabilities, they do not possess an intrinsic understanding of the content. This can lead to errors in logic, factual inaccuracies, and a failure to grasp complex concepts or nuances. The rapid advancement and widespread application of these models also lead to various ethical concerns such as bias, misinformation, privacy, and copyright. These issues demand careful consideration and proactive measures to ensure that the development and deployment of LLMs align with ethical standards and societal values. <a id="idIndexMarker005"/><a id="idIndexMarker006"/></p>
<h2 class="fm-head" id="heading_id_3">16.1 Content generation with the OpenAI API</h2>
<p class="body">While there are other LLMs such as Meta’s LLAMA and Google’s Gemini, OpenAI’s GPT series is the most prominent one. We therefore use OpenAI GPTs as our examples in this chapter. <a id="idIndexMarker007"/><a id="idIndexMarker008"/><a id="marker-367"/><a id="idIndexMarker009"/></p>
<p class="body">OpenAI allows you to use LLMs to generate various content such as text, images, audio, and code. You can access their service either through a web browser or an API. We’ll focus on content generation with Python programs via an API in this chapter due to the advantages of interacting with LLMs using Python mentioned earlier.</p>
<p class="body">You do need your OpenAI API key for the programs in this chapter to work. I assume you have already obtained your API key in chapter 15. If not, go back to chapter 15 for detailed instructions on how to get one.</p>
<p class="body">I’ll focus mainly on text generation in this section but will provide an example for each of the cases of code, image, and speech generation.</p>
<p class="body">This chapter involves the use of several new Python libraries. To install them, run the following lines of code in a new cell in your Jupypter Notebook app on your computer:</p>
<pre class="programlisting">!pip install --upgrade openai langchain_openai langchain
!pip install wolframalpha langchainhub
!pip install --upgrade --quiet wikipedia</pre>
<p class="body">Follow the on-screen instructions to finish the installation.</p>
<h3 class="fm-head1" id="heading_id_4">16.1.1 Text generation tasks with OpenAI API</h3>
<p class="body">You can generate text for many different purposes, such as question-answering, text summarization, and creative writing. <a id="idIndexMarker010"/><a id="idIndexMarker011"/><a id="idIndexMarker012"/></p>
<p class="body">When you ask OpenAI GPT a question, keep in mind that all LLMs, including OpenAI GPTs, are trained on historical data gathered through automated web crawling. As of this writing, GPT-4 was trained using data up to December 2023, with a three-month lag. GPT-3.5 was trained on data up to September 2021.</p>
<p class="body">Let’s first ask GPT a question about historical facts. Enter the lines of code in the following listing in a new cell.</p>
<p class="fm-code-listing-caption">Listing 16.1 Checking historical facts with OpenAI API</p>
<pre class="programlisting">from openai import OpenAI
  
openai_api_key=put your actual OpenAI API key here, in quotes  <span class="fm-combinumeral">①</span>
client=OpenAI(api_key=openai_api_key)                          <span class="fm-combinumeral">②</span>
  
completion = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content":                              <span class="fm-combinumeral">③</span>
     '''You are a helpful assistant, knowledgeable about recent facts.'''},
    {"role": "user", "content": 
     '''Who won the Nobel Prize in Economics in 2000?'''}      <span class="fm-combinumeral">④</span>
  ]
)
print(completion.choices[0].message.content)</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Provides your OpenAI API key</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates an OpenAI() class instance and names it client</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Defines the role of the system</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Asks the question</p>
<p class="body">Make sure you provide your OpenAI API key in the listing 16.1. We first instantiate the <code class="fm-code-in-text">OpenAI()</code> class and call it <code class="fm-code-in-text">client</code>. In the <code class="fm-code-in-text">chat.completions.create()</code> method, we specify the model as <code class="fm-code-in-text">gpt-3.5-turbo</code>. The site <a class="url" href="https://platform.openai.com/docs/models">https://platform.openai.com/docs/models</a> provides various models available. You can use either gpt-4 or gpt-3.5-turbo for text generation. The former provides better results but also incurs higher expenses. We’ll use the latter for most cases since our examples are simple enough, so it provides equally good results. <a id="idIndexMarker013"/><a id="idIndexMarker014"/><a id="marker-368"/></p>
<p class="body">The <code class="fm-code-in-text">messages</code> parameter in the preceding code block consists of several message objects, with each object containing a role (which can be “system,” “user,” or “assistant”) and content. A system message determines the assistant’s behavior; absent a system message, the default setting characterizes the assistant as “a helpful assistant.” User messages include inquiries or remarks for the assistant to address. For instance, in the previous example, the user message is “Who won the Nobel Prize in Economics in 2000?” The output is<a id="idIndexMarker015"/></p>
<pre class="programlisting">The Nobel Prize in Economics in 2000 was awarded to James J. Heckman and 
Daniel L. McFadden for their work on microeconometrics and microeconomic theory. </pre>
<p class="body">OpenAI has provided the correct answer.</p>
<p class="body">You can also ask the LLM to write an essay on a certain topic. Next, we ask it to write a short essay on the importance of self-motivation:</p>
<pre class="programlisting">completion = client.chat.completions.create(
  model="gpt-3.5-turbo",
  n=1,
  messages=[
    {"role": "system", "content": 
     '''You are a helpful assistant, capable of writing essays.'''},
    {"role": "user", "content": 
     '''Write a short essay on the importance of self-motivation.'''}
  ]
)
print(completion.choices[0].message.content)</pre>
<p class="body">The <code class="fm-code-in-text">n=1</code> argument here tells the assistant to generate one response. If you want multiple responses, you can set n to a different number. The default value for n is 1. The output is <a id="idIndexMarker016"/></p>
<pre class="programlisting">Self-motivation is a key factor in achieving success and personal growth in
 various aspects of life. It serves as the driving force behind our 
 actions, decisions, and goals, pushing us to overcome obstacles and 
 challenges along the way.
  
One of the primary benefits of self-motivation is that it helps individuals
 take initiative and control of their lives…</pre>
<p class="body">The output is six paragraphs long, and I have included only the first few sentences. You can go to the book’s GitHub repository (<a class="url" href="https://github.com/markhliu/DGAI">https://github.com/markhliu/DGAI</a>) to see the whole essay. As you can see, the writing is coherent, to the point, and without grammatical errors.</p>
<p class="body">You can even ask OpenAI’s GPT to write a joke for you:</p>
<pre class="programlisting">completion = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": 
     '''You are a helpful assistant, capable of telling jokes.'''},
    {"role": "user", "content": 
     '''Tell me a math joke.'''}
  ]
)
print(completion.choices[0].message.content)</pre>
<p class="body"><a id="marker-369"/>We asked it to tell a math joke, and the result is</p>
<pre class="programlisting">Why was the equal sign so humble? Because he knew he wasn't less than or greater than anyone else!</pre>
<p class="body">You can carry out back-and-forth conversations with the assistant. The messages parameter automatically includes conversation history. For example, after running the previous code block, if you run the following:</p>
<pre class="programlisting">completion = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "user", "content": 
     '''Haha, that's funny! Tell me another one.'''} 
  ]
)
print(completion.choices[0].message.content)</pre>
<p class="body">you’ll get a response similar to</p>
<pre class="programlisting">Sure! Why was the math book sad? Because it had too many problems.</pre>
<p class="body">The user’s query “Haha, that’s funny! Tell me another one.” only makes sense in the context of the prior messages where you ask the assistant to tell a math joke. Other text generation capabilities include text summarization and classification, and you’ll see such examples later in the chapter.<a id="idIndexMarker017"/><a id="idIndexMarker018"/><a id="idIndexMarker019"/></p>
<h3 class="fm-head1" id="heading_id_5">16.1.2 Code generation with OpenAI API</h3>
<p class="body">Codex is specifically designed to understand and generate code. It can work with multiple programming languages and can translate natural language descriptions into code. Codex is now an integrated part of OpenAI GPTs. <a id="idIndexMarker020"/><a id="idIndexMarker021"/><a id="idIndexMarker022"/><a id="marker-370"/></p>
<p class="body">Next, we ask OpenAI GPT to produce a Python program to plot a sine curve:</p>
<pre class="programlisting">completion = client.chat.completions.create(
  model="gpt-3.5-turbo",
  messages=[
    {"role": "system", "content": 
     '''You are a helpful assistant, 
     capable of generating Python programs.'''},
    {"role": "user", "content": 
     '''Write a Python program to plot a sine graph.'''}
  ]
)
print(completion.choices[0].message.content)</pre>
<p class="body">Note that ChatGPT uses GPT-3.5-Turbo to manage both the conversational aspect and the code generation tasks effectively. It does not provide a separate Codex model for code generation only. The output is</p>
<pre class="programlisting">Sure! Here's a simple Python program using the matplotlib library to plot 
a sine graph:
  
```python
import matplotlib.pyplot as plt
import numpy as np
 
# Generate x values from 0 to 2*pi
x = np.linspace(0, 2*np.pi, 100)
  
# Calculate y values using the sine function
y = np.sin(x)
  
# Plot the sine graph
plt.figure()
plt.plot(x, y)
plt.title('Sine Graph')
plt.xlabel('x')
plt.ylabel('sin(x)')
plt.grid(True)
plt.show()
```
  
You can run this program in a Python environment with the matplotlib 
library installed to visualize the sine graph.</pre>
<p class="body">If you copy and paste the generated Python program into a cell in Jupyter Notebook and run it, you’ll see an image similar to figure 16.1.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="367" src="../../OEBPS/Images/CH16_F01_Liu.png" width="528"/></p>
<p class="figurecaption">Figure 16.1 Use OpenAI GPT-3.5 to generate Python code to plot a sine curve. We use the text description “Write a Python program to plot a sine graph” to ask it to generate a Python program. We then run the program to create the graph.</p>
</div>
<p class="body">The LLM not only provides the Python code, but it also lets you know that you need to run the code in a Python environment with the matplotlib library installed.</p>
<h3 class="fm-head1" id="heading_id_6">16.1.3 Image generation with OpenAI DALL-E 2</h3>
<p class="body">DALL-E 2 is an AI model developed by OpenAI, designed to generate images from textual descriptions. It is a successor to the original DALL-E model and represents advancements in the field of generative AI for visual content.<a id="idIndexMarker023"/><a id="idIndexMarker024"/><a id="idIndexMarker025"/><a id="idIndexMarker026"/><a id="idIndexMarker027"/><a id="marker-371"/></p>
<p class="body">DALL-E 2 uses a diffusion model similar to what we discussed in chapter 15, which starts with a random pattern of pixels and gradually refines it into a coherent image that matches the input text. It has improved upon the original DALL-E by producing higher-quality images with more accurate and detailed representations of the textual descriptions.</p>
<p class="body">Incorporating DALL-E 2 into OpenAI’s GPT series allows us to not only generate text but also create images based on text prompts. Next, we ask DALL-E 2 to create an image of someone fishing at the riverbank:</p>
<pre class="programlisting">response = client.images.generate(
  model="dall-e-2",
  prompt="someone fishing at the river bank",
  size="512x512",
  quality="standard",
  n=1,
)
image_url = response.data[0].url
print(image_url)</pre>
<p class="body">The code block generates a URL. If you click on the URL, you’ll see an image similar to figure 16.2.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="375" src="../../OEBPS/Images/CH16_F02_Liu.png" width="375"/></p>
<p class="figurecaption">Figure 16.2 An image generated by DALL-E 2 with the text prompt “someone fishing at the riverbank”</p>
</div>
<p class="body">The URL expires in an hour, so make sure you access it promptly. Furthermore, the image generated by DALL-E 2 is slightly different even if you use the same text prompt because the output is randomly generated.</p>
<h3 class="fm-head1" id="heading_id_7">16.1.4 Speech generation with OpenAI API</h3>
<p class="body">Text-to-speech (TTS) is a technology that converts written text into spoken words. TTS is trained through multimodal Transformers in which the input is text and the output is in audio format. In the context of ChatGPT, integrating TTS capabilities means that the LLM can not only generate textual responses but can also speak them out loud. Next, we ask OpenAI API to convert a short text into speech:<a id="idIndexMarker028"/><a id="idIndexMarker029"/><a id="idIndexMarker030"/><a id="marker-372"/><a id="idIndexMarker031"/></p>
<pre class="programlisting">response = client.audio.speech.create(
  model="tts-1-hd",
  voice="shimmer",
  input='''This is an audio file generated by 
    OpenAI's text to speech AI model.'''
)
response.stream_to_file("files/speech.mp3")</pre>
<p class="body">After running the previous code cell, a file, speech.mp3, is saved on your computer, and you can listen to it. The documentation site (<a class="url" href="https://platform.openai.com/docs/guides/text-to-speech">https://platform.openai.com/docs/guides/text-to-speech</a>) provides voice options. Here we have chosen the <code class="fm-code-in-text">shimmer</code> option. Other options include <code class="fm-code-in-text">alloy</code>, <code class="fm-code-in-text">echo</code>, and so on. <a id="idIndexMarker032"/><a id="idIndexMarker033"/><a id="idIndexMarker034"/><a id="idIndexMarker035"/><a id="idIndexMarker036"/><a id="idIndexMarker037"/></p>
<h2 class="fm-head" id="heading_id_8">16.2 Introduction to LangChain</h2>
<p class="body">LangChain is a Python library designed to facilitate the use of LLMs in various applications. It provides a suite of tools and abstractions that make it easier to build, deploy, and manage applications powered by LLMs like GPT-3, GPT-4, and other similar models.<a id="idIndexMarker038"/><a id="idIndexMarker039"/></p>
<p class="body">LangChain abstracts away the complexities of interacting with different LLMs and applications, allowing developers to focus on building their application logic without worrying about the underlying model specifics. It is particularly well suited for building a “know-it-all” agent by chaining together an LLM with applications like Wolfram Alpha and Wikipedia that can provide real-time information or recent facts. LangChain’s modular architecture allows for easy integration of different components, enabling the agent to leverage the strengths of various LLMs and applications.</p>
<h3 class="fm-head1" id="heading_id_9">16.2.1 The need for the LangChain library</h3>
<p class="body">Imagine that your goal is to build a zero-shot know-it-all agent so that it can produce various content, retrieve real-time information, and answer factual questions for us. You want the agent to automatically go to the right source to retrieve the relevant information based on the task at hand without explicitly telling it what to do. The LangChain library is the right tool for this.<a id="idIndexMarker040"/><a id="marker-373"/><a id="idIndexMarker041"/></p>
<p class="body">In this project, you’ll learn to use the LangChain library to combine LLMs with the Wolfram Alpha and Wikipedia APIs to create a zero-shot know-it-all agent. We use Wolfram Alpha API to retrieve real-time information and the Wikipedia API to answer questions about recent facts. LangChain allows us to create an agent to utilize multiple tools to answer a question. The agent first understands the query and then decides which tool in the toolbox to use to answer the question.</p>
<p class="body">To show you that even the most advanced LLMs lack these abilities, let’s ask who won the Best Actor Award in the 2024 Academy Awards:</p>
<pre class="programlisting">completion = client.chat.completions.create(
  model="gpt-4",
  messages=[
    {"role": "system", "content": 
     '''You are a helpful assistant, knowledgeable about recent facts.'''},
    {"role": "user", "content": 
     '''Who won the Best Actor Award in 2024 Academy Awards?'''}
  ]
)
print(completion.choices[0].message.content)</pre>
<p class="body">The output is</p>
<pre class="programlisting">I'm sorry, but I cannot provide real-time information or make predictions 
about future events such as the 2024 Academy Awards. For the most accurate 
and up-to-date information, I recommend checking reliable sources or news 
outlets closer to the date of the awards show.</pre>
<p class="body">I made this query on March 17, 2024, and GPT-4 was not able to answer the question. It’s possible that when you make the same query, you’ll get the correct answer because the model has been updated using more recent data. If that’s the case, change the question to an event a few days ago, and you should get a similar response.</p>
<p class="body">Therefore, we’ll use LangChain to chain together an LLM with the Wolfram Alpha and Wikipedia APIs. Wolfram Alpha is good at scientific computations and retrieving real-time information, while Wikipedia is famous for providing information on both historical and recent events and facts.</p>
<h3 class="fm-head1" id="heading_id_10">16.2.2 Using the OpenAI API in LangChain</h3>
<p class="body">The langchain-openai library you installed earlier in this chapter allows you to use OpenAI GPTs with minimal prompt engineering. You only need to explain what you want the LLM to do in plain English. <a id="idIndexMarker042"/><a id="idIndexMarker043"/><a id="marker-374"/><a id="idIndexMarker044"/></p>
<p class="body">Here is an example of how we ask it to correct grammar errors in text:</p>
<pre class="programlisting">from langchain_openai import OpenAI
  
llm = OpenAI(openai_api_key=openai_api_key)
  
prompt = """
Correct the grammar errors in the text:
  
i had went to stor buy phone. No good. returned get new phone.
"""
  
res=llm.invoke(prompt)
print(res)</pre>
<p class="body">The output is</p>
<pre class="programlisting">I went to the store to buy a phone, but it was no good. I returned it and 
got a new phone.</pre>
<p class="body">Note that we didn’t use any prompt engineering. We didn’t specify which model to use either. LangChain found the best model for the job based on the task requirements and other factors such as cost, latency, and performance. It also automatically formats and structures the queries to be suitable for the model it uses. The preceding prompt simply asks the agent, in plain English, to correct the grammar errors in the text. It returns text with the correct grammar, as shown in the previous output.</p>
<p class="body">Here is another example. We asked the agent to name the capital city of Kentucky:</p>
<pre class="programlisting">prompt = """
What is the capital city of the state of Kentucky?
"""
res=llm.invoke(prompt)
print(res)</pre>
<p class="body">The output is</p>
<pre class="programlisting">The capital city of Kentucky is Frankfort.</pre>
<p class="body">It tells us the correct answer, which is Frankfort, Kentucky.</p>
<h3 class="fm-head1" id="heading_id_11">16.2.3 Zero-shot, one-shot, and few-shot prompting</h3>
<p class="body">Few-shot, one-shot, and zero-shot prompting refer to different ways of providing examples or instructions to LLMs to guide their responses. These techniques are used to help the model understand the task at hand and generate more accurate or relevant outputs.<a id="idIndexMarker045"/><a id="idIndexMarker046"/><a id="idIndexMarker047"/></p>
<p class="body">In zero-shot prompting, the model is given a task or a question without any examples. The prompt typically includes a clear description of what is expected, but the model must generate a response based solely on its preexisting knowledge and understanding. In one-shot prompting, the model is provided with a single example to illustrate the task. In few-shot prompting, the model is given multiple examples to help it understand the task. Few-shot prompting is based on the idea that providing more examples can help the model better grasp the pattern or the rules of the task, leading to more accurate responses.</p>
<p class="body">All your interactions so far with OpenAI GPTs are zero-shot prompting since you haven’t provided them with any examples.</p>
<p class="body"><a id="marker-375"/>Let’s try an example of few-shot prompting. Suppose you want the LLM to conduct sentiment analysis: you want it to classify a sentence as positive or negative. You can provide several examples in the prompt:</p>
<pre class="programlisting">prompt = """
The movie is awesome! // Positive
It is so bad! // Negative
Wow, the movie was incredible! // Positive
How horrible the movie is! //
"""
res=llm.invoke(prompt)
print(res)</pre>
<p class="body">The output is</p>
<pre class="programlisting">Negative</pre>
<p class="body">In the prompt, we provided three examples. Two reviews are classified as positive, while one is classified as negative. We then provided the sentence, “How horrible the movie is!” The LLM classified it correctly as negative.</p>
<p class="body">We used <code class="fm-code-in-text">//</code> to separate the sentence and the corresponding sentiment in the previous example. You can use other separators such as <code class="fm-code-in-text">-&gt;</code>, so long as you are consistent.</p>
<p class="body">Here is an example of one-shot prompting:</p>
<pre class="programlisting">prompt = """
Car -&gt; Driver
Plane -&gt;
"""
res=llm.invoke(prompt)
print(res)</pre>
<p class="body">The output is</p>
<pre class="programlisting">Pilot</pre>
<p class="body">By providing one single example, we are effectively asking the LLM, “What is to a plane as a driver is to a car?” The LLM correctly answered <code class="fm-code-in-text">Pilot</code>.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 16.1</p>
<p class="fm-sidebar-text">Suppose you want to ask the LLM, “What is to a garden as a chef is to a kitchen?” Use one-shot prompting to get the answer.</p>
</div>
<p class="body">Finally, here is an example of zero-shot prompting:</p>
<pre class="programlisting">prompt = """
Is the tone in the sentence "Today is a great day for me" positive, 
negative, or neutral?
"""
res=llm.invoke(prompt)
print(res)</pre>
<p class="body">The output is</p>
<pre class="programlisting">Positive</pre>
<p class="body">We didn’t provide any examples in the prompt. However, we provided instruction in plain English to ask the LLM to classify the tone in the sentence as positive, negative, or neutral.<a id="idIndexMarker048"/><a id="idIndexMarker049"/><a id="marker-376"/></p>
<h2 class="fm-head" id="heading_id_12">16.3 A zero-shot know-it-all agent in LangChain</h2>
<p class="body">You’ll learn to create a zero-shot know-it-all agent in LangChain in this section. You’ll use OpenAI GPTs to generate various content such as text, images, and code. To compensate for LLM’s inability to provide real-time information, you’ll learn to add Wolfram Alpha and Wikipedia APIs to the toolbox.<a id="idIndexMarker050"/><a id="idIndexMarker051"/><a id="idIndexMarker052"/></p>
<p class="body">Wolfram Alpha is a computational knowledge engine designed to handle factual queries online, specializing in numerical and computational tasks, particularly in the science and technology fields. By integrating the Wolfram Alpha API, the agent gains the ability to answer virtually any question across various subjects. Should Wolfram Alpha be unable to provide a response, we will use Wikipedia as a secondary source for fact-based questions on specific topics.<a id="idIndexMarker053"/></p>
<p class="body">Figure 16.3 is a diagram of the steps we’ll take to create the zero-shot know-it-all agent in this section.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="276" src="../../OEBPS/Images/CH16_F03_Liu.png" width="567"/></p>
<p class="figurecaption">Figure 16.3 Steps to create a zero-shot know-it-all agent with the LangChain library</p>
</div>
<p class="body">Specifically, we’ll fi<a id="idTextAnchor006"/>rst create an agent in LangChain with just one tool—the Wolfram Alpha API—to answer questions related to real-time information and recent facts. We’ll then add the Wikipedia API to the toolbox as a backup on questions related to recent facts. We’ll add various tools utilizing the OpenAI API such as text summarizer, joke teller, and sentiment classifier. Finally, we’ll add image and code generation functionalities.</p>
<h3 class="fm-head1" id="heading_id_13">16.3.1 Applying for a Wolfram Alpha API Key</h3>
<p class="body">Wolfram Alpha gives you up to 2,000 noncommercial API calls per month for free. To obtain an API key, first go to <a class="url" href="https://account.wolfram.com/login/create/">https://account.wolfram.com/login/create/</a> and complete the steps to create an account. <a id="idIndexMarker054"/><a id="idIndexMarker055"/><a id="idIndexMarker056"/><a id="marker-377"/><a id="idIndexMarker057"/></p>
<p class="body">The Wolfram account itself gives you only browser access; you need to apply for an API key at <a class="url" href="https://products.wolframalpha.com/api/">https://products.wolframalpha.com/api/</a>. Once there, click Get API Access in the bottom left corner. A small dialog should pop up, fill in the fields Name and Description, select <i class="fm-italics">Simple API</i> from the dropdown menu, and then click Submit, as shown in figure 16.4.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="728" src="../../OEBPS/Images/CH16_F04_Liu.png" width="619"/></p>
<p class="figurecaption">Figure 16.4 Applying for a Wolfram Alpha AppID</p>
</div>
<p class="body">After that, your AppID should appear in a new window. Copy the API key and save it in a file for later use.</p>
<p class="body">Here is how you can use the Wolfram Alpha API to conduct math operations:</p>
<pre class="programlisting">import os

os.environ['WOLFRAM_ALPHA_APPID'] = "your Wolfram Alpha AppID" 

from langchain_community.utilities.wolfram_alpha import \
WolframAlphaAPIWrapper
wolfram = WolframAlphaAPIWrapper()
res=wolfram.run("how much is 23*55+123?")
print(res)</pre>
<p class="body">The output is</p>
<pre class="programlisting">Assumption: 23×55 + 123 
Answer: 1388</pre>
<p class="body">The Wolfram Alpha API provides the correct answer.</p>
<p class="body">We’ll also include the Wikipedia API to provide answers to various topics. You don’t need to apply for an API key if you have installed the Wikipedia library on your computer. Here is an example of using the Wikipedia API in the LangChain library:</p>
<pre class="programlisting">from langchain.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())
res=wikipedia.run("University of Kentucky")
print(res)</pre>
<p class="body">The output is</p>
<pre class="programlisting">Page: University of Kentucky
Summary: The University of Kentucky (UK, UKY, or U of K) is a public 
land-grant research university in Lexington, Kentucky. Founded in 1865 by 
John Bryan Bowman as the Agricultural and Mechanical College of Kentucky, 
the university is one of the state's two land-grant universities (the 
other being Kentucky State University)… </pre>
<p class="body">We have omitted most of the output for brevity.</p>
<h3 class="fm-head1" id="heading_id_14">16.3.2 Creating an agent in LangChain</h3>
<p class="body"><a id="marker-378"/>Next, we’ll create an agent in LangChain, with only the Wolfram Alpha API in the toolbox. An agent in this context refers to an individual entity designed to handle specific tasks or processes through natural language interactions. We’ll then gradually add more tools to the chain so that the agent becomes capable of handling more tasks. <a id="idIndexMarker058"/><a id="idIndexMarker059"/></p>
<p class="fm-code-listing-caption">Listing 16.2 Creating an agent in LangChain</p>
<pre class="programlisting">os.environ['OPENAI_API_KEY'] = openai_api_key 
from langchain.agents import load_tools
from langchain_openai import ChatOpenAI
from langchain import hub
from langchain.agents import AgentExecutor, create_react_agent
from langchain_openai import OpenAI
  
prompt = hub.pull("hwchase17/react")
llm = ChatOpenAI(model_name='gpt-3.5-turbo')              <span class="fm-combinumeral">①</span>
tool_names = ["wolfram-alpha"]
tools = load_tools(tool_names,llm=llm)                    <span class="fm-combinumeral">②</span>
agent = create_react_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools,
       handle_parsing_errors=True,verbose=True)           <span class="fm-combinumeral">③</span>
  
res=agent_executor.invoke({"input": """
What is the temperature in Lexington, Kentucky now?
"""})                                                     <span class="fm-combinumeral">④</span>
print(res["output"])</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines which LLM to use</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Adds Wolfram Alpha to the toolbox</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Defines an agent</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Asks the agent a question</p>
<p class="body">The <code class="fm-code-in-text">hwchase17/react</code> in LangChain refers to a specific type of ReAct agent configuration. ReAct stands for Reactive Action, which is a framework within LangChain designed to optimize the use of language model capabilities in combination with other tools to solve complex tasks effectively. See <a class="url" href="https://python.langchain.com/docs/how_to/migrate_agent/">https://python.langchain.com/docs/how_to/migrate_agent/</a> for more details. When you create an agent in LangChain, you need to specify the tools to be used by the agent. In the previous example, we use only one tool, the Wolfram Alpha API. <a id="idIndexMarker060"/><a id="marker-379"/></p>
<p class="body">As an example, we ask the current temperature in Lexington, Kentucky, and here is the output:</p>
<pre class="programlisting">&gt; Entering new AgentExecutor chain...
I should use Wolfram Alpha to find the current temperature in Lexington, 
Kentucky.
Action: wolfram_alpha
Action Input: temperature in Lexington, KentuckyAssumption: temperature | 
Lexington, Kentucky 
Answer: 44 °F (wind chill: 41 °F)
(27 minutes ago)I now know the current temperature in Lexington, Kentucky.
Final Answer: The temperature in Lexington, Kentucky is 44 °F with a wind 
chill of 41 °F.
  
&gt; Finished chain.
The temperature in Lexington, Kentucky is 44 °F with a wind chill of 41 °F.</pre>
<p class="body">The output not only shows the final answer, which says the current temperature in Lexington, Kentucky, is 44 degrees Fahrenheit, but it also shows the chain of thoughts. It uses Wolfram Alpha as the source to obtain the answer.</p>
<p class="body">We can also add Wikipedia to the toolbox:</p>
<pre class="programlisting">tool_names += ["wikipedia"]
tools = load_tools(tool_names,llm=llm)
agent = create_react_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools,
       handle_parsing_errors=True,verbose=True)
  
res=agent_executor.invoke({"input": """
Who won the Best Actor Award in 2024 Academy Awards?
"""})
print(res["output"])</pre>
<p class="body">I ask who won the Best Actor Award in the 2024 Academy Awards, and the agent uses Wikipedia to get the correct answer:</p>
<pre class="programlisting">I need to find information about the winner of the Best Actor Award at the 
2024 Academy Awards.
Action: wikipedia
Action Input: 2024 Academy Awards Best Actor
…
Cillian Murphy won the Best Actor Award at the 2024 Academy Awards for his 
performance in Oppenheimer.</pre>
<p class="body">In the preceding output, the agent first decides to use Wikipedia as the tool to solve the problem. After searching through various Wikipedia sources, the agent provides the correct answer.</p>
<p class="body">Next, you’ll learn to add various OpenAI GPT tools to the agent’s toolbox.</p>
<h3 class="fm-head1" id="heading_id_15">16.3.3 Adding tools by using OpenAI GPTs</h3>
<p class="body">We first add a text summarizer so that the agent can summarize text.<a id="idIndexMarker061"/><a id="idIndexMarker062"/><a id="idIndexMarker063"/><a id="marker-380"/></p>
<p class="fm-code-listing-caption">Listing 16.3 Adding a text summarizer to the agent’s tool box</p>
<pre class="programlisting">from langchain.agents import Tool
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
  
temp = PromptTemplate(input_variables=["text"],               <span class="fm-combinumeral">①</span>
template="Write a one sentence summary of the following text: {text}")
  
summarizer = LLMChain(llm=llm, prompt=temp)                   <span class="fm-combinumeral">②</span>
  
sum_tool = Tool.from_function(
    func=summarizer.run,
    name="Text Summarizer",
    description="A tool for summarizing texts")               <span class="fm-combinumeral">③</span>
tools+=[sum_tool]
agent = create_react_agent(llm, tools, prompt)                <span class="fm-combinumeral">④</span>
agent_executor = AgentExecutor(agent=agent, tools=tools,
       handle_parsing_errors=True,verbose=True)
res=agent_executor.invoke({"input": 
'''Write a one sentence summary of the following text:
The University of Kentucky's Master of Science
 in Finance (MSF) degree prepares students for
 a professional career in the finance and banking
 industries. The program is designed to provide
 rigorous and focused training in finance, 
 broaden opportunities in your career, and 
 sharpened skills for the fast-changing 
 and competitive world of modern finance.'''})
print(res["output"])</pre>
<p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines a template</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Defines a summarizer function</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Adds summarizer as a tool</p>
<p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Redefines the agent with the updated toolbox</p>
<p class="body">We first provide a template to summarize text. We then define a summarizer function and add it to the toolbox. Finally, we redefine the agent by using the updated toolbox and ask it to summarize the example text with one sentence. Make sure your prompt has the same format as those described in the template so that the agent knows which tool to use.</p>
<p class="body">The output from listing 16.3 is</p>
<pre class="programlisting">&gt; Entering new AgentExecutor chain...
I need to summarize the text provided.
Action: Summarizer
…
&gt; Finished chain.
The University of Kentucky's MSF program offers specialized training in 
finance to prepare students for successful careers in the finance and 
banking industries.</pre>
<p class="body">The agent chooses the summarizer as the tool for the task since the input matches the template described in the summarizer function. We use two long sentences as the text input and the preceding output is a one-sentence summary.</p>
<p class="body">You can add as many tools as you like. For example, you can add a tool to tell a joke on a certain subject:</p>
<pre class="programlisting">temp = PromptTemplate(input_variables=["text"],
template="Tell a joke on the following subject: {subject}")
  
joke_teller = LLMChain(llm=llm, prompt=temp)
  
tools+=[Tool.from_function(name='Joke Teller',
       func=joke_teller.run,
       description='A tool for telling jokes')]
agent = create_react_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools,
       handle_parsing_errors=True,verbose=True)
  
res=agent_executor.invoke({"input": 
'''Tell a joke on the following subject: coding'''})
print(res["output"])</pre>
<p class="body"><a id="marker-381"/>The output is</p>
<pre class="programlisting">&gt; Entering new AgentExecutor chain...
I should use the Joke Teller tool to find a coding-related joke.
Action: Joke Teller
Action Input: coding
Observation: Why was the JavaScript developer sad?
  
Because he didn't know how to "null" his feelings.
Thought:That joke was funny!
Final Answer: Why was the JavaScript developer sad? Because he didn't know 
how to "null" his feelings.
  
&gt; Finished chain.
Why was the JavaScript developer sad? Because he didn't know how to "null" 
his feelings.</pre>
<p class="body">We ask the agent to tell a joke on the subject of <span>coding</span>. The agent identifies <span>Joke Teller</span> as the tool. The joke is indeed related to coding.</p>
<div class="fm-sidebar-block">
<p class="fm-sidebar-title">Exercise 16.2</p>
<p class="fm-sidebar-text">Add a tool to the agent’s toolbox to conduct sentiment analysis. Name the tool Sentiment Classifier. Then ask the agent to classify the text “this movie is so-so” as positive, negative, or neutral.</p>
</div>
<h3 class="fm-head1" id="heading_id_16">16.3.4 Adding tools to generate code and images</h3>
<p class="body"><a id="marker-382"/>You can add various tools to the toolbox in LangChain. Interested readers can find more details at <a class="url" href="https://python.langchain.com/docs/how_to/#tools">https://python.langchain.com/docs/how_to/#tools</a>. Next, we add tools to generate other content forms such as code and images.<a id="idIndexMarker064"/><a id="idIndexMarker065"/><a id="idIndexMarker066"/><a id="idIndexMarker067"/></p>
<p class="body">To add a tool to generate code, you can do the following:</p>
<pre class="programlisting">temp = PromptTemplate(input_variables=["text"],
template='''Write a Python program based on the 
    description in the following text: {text}''')
   
code_generator = LLMChain(llm=llm, prompt=temp)
  
tools+=[Tool.from_function(name='Code Generator',
       func=code_generator.run,
       description='A tool to generate code')]
agent = create_react_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools,
       handle_parsing_errors=True,verbose=True)
  
res=agent_executor.invoke({"input": 
'''Write a Python program based on the 
    description in the following text: 
write a python program to plot a sine curve and a cosine curve
in the same graph. The sine curve is in solid line and the cosine
curve is in dashed line. Add a legend to the graph. Set the x-axis 
range to -5 to 5. The title should be "Comparing sine and cosine curves."
'''})
print(res["output"])</pre>
<p class="body">The output is</p>
<pre class="programlisting">&gt; Entering new AgentExecutor chain...
I should use the Code Generator tool to generate the Python program based on the given description.
Action: Code Generator
Action Input: Write a Python program to plot a sine curve and a cosine 
curve in the same graph. The sine curve is in solid line and the cosine 
curve is in dashed line. Add a legend to the graph. Set the x-axis range 
to -5 to 5. The title should be "Comparing sine and cosine curves."
Observation: import matplotlib.pyplot as plt
import numpy as np
  
x = np.linspace(-5, 5, 100)
y1 = np.sin(x)
y2 = np.cos(x)
  
plt.plot(x, y1, label='Sine Curve', linestyle='solid')
plt.plot(x, y2, label='Cosine Curve', linestyle='dashed')
plt.legend()
plt.title('Comparing Sine and Cosine Curves')
plt.xlim(-5, 5)
plt.show()
Thought:The Python program has been successfully generated to plot the sine
 and cosine curves. I now know the final answer.
  
Final Answer: The Python program to plot a sine curve and a cosine curve in
 the same graph with the specified requirements has been generated.
  
&gt; Finished chain.
The Python program to plot a sine curve and a cosine curve in the same 
graph with the specified requirements has been generated.</pre>
<p class="body">If you run the generated code in a cell, you’ll see an image as in figure 16.5.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="361" src="../../OEBPS/Images/CH16_F05_Liu.png" width="528"/></p>
<p class="figurecaption">Figure 16.5 Adding a tool in LangChain to generate Python code. The tool then generates code to plot sine and cosine curves in the same graph, with a legend and line styles.</p>
</div>
<p class="body"><a id="marker-383"/>To add an image generator, you can do the following:</p>
<pre class="programlisting">from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper
temp = PromptTemplate(input_variables=["text"],
template="Create an image base on the following text: {text}")
  
grapher = LLMChain(llm=llm, prompt=temp)
  
tools+=[Tool.from_function(name='Text to image',
       func=grapher.run,
       description='A tool for text to image')]
agent = create_react_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools,
       handle_parsing_errors=True,verbose=True)
image_url = DallEAPIWrapper().run(agent_executor.invoke({"input": 
'''Create an image base on the following text: 
    a horse grazes on the grassland.'''})["output"])
print(image_url)</pre>
<p class="body">The output is a URL for you to visualize and download an image. We asked the agent to create an image of a horse grazing on the grassland. The image is shown in figure 16.6.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="375" src="../../OEBPS/Images/CH16_F06_Liu.png" width="375"/></p>
<p class="figurecaption">Figure 16.6 An image generated by a know-it-all agent in LangChain</p>
</div>
<p class="body">With that, you have learned how to create a zero-shot know-it-all agent in LangChain. You can add more tools to the toolbox depending on what you want the agent to accomplish.</p>
<h2 class="fm-head" id="heading_id_17">16.4 Limitations and ethical concerns of LLMs</h2>
<p class="body">LLMs such as OpenAI’s GPT series have made significant strides in the field of NLP and generative AI. Despite their impressive capabilities, these models are not without limitations. Understanding these constraints is crucial for both leveraging their strengths and mitigating their weaknesses. <a id="idIndexMarker068"/><a id="marker-384"/><a id="idIndexMarker069"/></p>
<p class="body">At the same time, the rapid advancement and widespread application of these models have also given rise to a host of ethical concerns such as bias, inaccuracies, breach of privacy, and copyright infringements. These issues demand careful consideration and proactive measures to ensure that the development and deployment of LLMs align with ethical standards and societal values.</p>
<p class="body">In this section, we’ll explore the limitations of LLMs, provide insights into why these issues persist, and present examples of notable failures to underscore the importance of addressing these challenges. We’ll also examine the key ethical concerns associated with LLMs and propose pathways for mitigating these concerns.</p>
<h3 class="fm-head1" id="heading_id_18">16.4.1 Limitations of LLMs</h3>
<p class="body">One of the fundamental limitations of LLMs is their lack of true understanding and reasoning. While they can generate coherent and contextually relevant responses, they do not possess an intrinsic understanding of the content. This can lead to errors in logic, factual inaccuracies, and a failure to grasp complex concepts or nuances.<a id="idIndexMarker070"/><a id="idIndexMarker071"/></p>
<p class="body">This manifests in many epic mistakes made by LLMs. The book <span>Smart Until It’s Dumb</span> provides many entertaining instances of such mistakes made by GPT-3 and ChatGPT.<sup class="footnotenumber" id="footnote-001-backlink"><a class="url1" href="#footnote-001">1</a></sup> For example, consider this question: Mrs. March gave the mother tea and gruel, while she dressed the little baby as tenderly as if it had been her own. Who’s the baby’s mother? The answer provided by GPT-3 is Mrs. March.</p>
<p class="body">To be fair, with the rapid advancement of LLMs, many of these mistakes are corrected over time. However, LLMs still make low-level mistakes. A LinkedIn article in June 2023 by David Johnston (<a class="url" href="https://www.linkedin.com/pulse/intelligence-tests-llms-fail-why-david-johnston/">https://www.linkedin.com/pulse/intelligence-tests-llms-fail-why<br class="calibre8"/>
  -david-johnston/</a>) tests the intelligence of LLMs on a dozen problems that humans can easily solve. LLMs, including GPT-4, struggle with these problems. One of the problems is as follows: name an animal such that the length of the word is equal to the number of legs they have minus the number of tails they have.</p>
<p class="body">This mistake has not been corrected as of this writing. Figure 16.7 is a screenshot of the answer by GPT-4 when I used a browser interface.</p>
<div class="figure">
<p class="figure1"><img alt="" class="calibre4" height="243" src="../../OEBPS/Images/CH16_F07_Liu.png" width="674"/></p>
<p class="figurecaption">Figure 16.7 How GPT-4 still makes low-level mistakes</p>
</div>
<p class="body">The output in figure 16.7 shows that, according to GPT-4, five is equal to the number of letters in the word “bee.”</p>
<h3 class="fm-head1" id="heading_id_19">16.4.2 Ethical concerns for LLMs</h3>
<p class="body"><a id="marker-385"/>One of the most pressing ethical concerns is the potential for LLMs to perpetuate and amplify biases in their training data. Since these models learn from vast datasets often derived from human-generated content, they can inherit biases related to gender, race, ethnicity, and other social factors. This can result in biased outputs that reinforce stereotypes and discrimination.<a id="idIndexMarker072"/><a id="idIndexMarker073"/></p>
<p class="body">To mitigate bias, it is essential to adopt diverse and inclusive training datasets, implement bias detection and correction algorithms, and ensure transparency in model development and evaluation. It’s particularly important to establish industry-wide collaboration to set standards for bias mitigation practices and promote responsible AI development.</p>
<p class="body">However, we must keep in mind not to overcorrect. A counterexample is that Google’s Gemini overcorrected the stereotypes in image generation by including people of color in groups like Nazi-era German soldiers.<sup class="footnotenumber" id="footnote-000-backlink"><a class="url1" href="#footnote-000">2</a></sup></p>
<p class="body">Another concern for LLMs is their potential for misinformation and manipulation. LLMs have the ability to generate realistic and persuasive text, which can be exploited for creating and spreading misinformation, propaganda, or manipulative content. This poses significant risks to public discourse, democracy, and trust in information.</p>
<p class="body">The solution to this concern lies in developing robust content moderation systems. Establishing guidelines for responsible use and fostering collaborations between AI developers, policymakers, and media organizations are crucial steps in combating misinformation.</p>
<p class="body"><a id="marker-386"/>The third concern is related to privacy. The vast amount of data used to train LLMs raises privacy concerns, as sensitive information can be inadvertently revealed in the model’s outputs. Additionally, the potential for LLMs to be used in cyberattacks or to bypass security measures poses significant security risks.</p>
<p class="body">Furthermore, the data used to train LLMs is mostly gathered without authorization. Supporters argue that the way data is used to train LLMs is transformative: the model doesn’t merely regurgitate the data but uses it to generate new, original content. This transformation could qualify under the “fair use” doctrine, which allows limited use of copyrighted material without permission if the use adds new expression or meaning. Critics argue that LLMs are trained on vast amounts of copyrighted texts without permission, which goes beyond what might be considered fair use. The scale of data used and the direct ingestion of copyrighted material without transformation during training could be seen as infringing. The debate is ongoing. The current copyright laws were not designed with generative AI in mind, leading to ambiguities about how they apply to technologies like LLMs. It’s a debate that likely needs to be resolved by legislative and judicial bodies to provide clear guidelines and ensure that the interests of all parties are fairly represented.</p>
<p class="body">The ethical concerns surrounding LLMs are multifaceted and require a holistic approach. Collaborative efforts among AI researchers, developers, and policymakers are crucial in developing ethical guidelines and frameworks that guide the responsible development and deployment of these powerful models. As we continue to harness the potential of LLMs, ethical considerations must remain at the forefront of our endeavors to ensure that AI advances in harmony with societal values and human well-being.<a id="idIndexMarker074"/><a id="idIndexMarker075"/></p>
<h2 class="fm-head" id="heading_id_20">Summary</h2>
<ul class="calibre5">
<li class="fm-list-bullet">
<p class="list">Few-shot prompting means you give LLMs multiple examples to help them understand the task, while one-shot or zero-shot prompting means one example or no example is provided.</p>
</li>
<li class="fm-list-bullet">
<p class="list">LangChain is a Python library designed to facilitate the use of LLMs in various applications. It abstracts away the complexities of interacting with different LLMs and applications. It allows the agent to automatically go to the right tool in the toolbox based on the task at hand without explicitly telling it what to do.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Modern pretrained LLMs such as OpenAI’s GPT series can create various formats of content such as text, images, audio, and code.</p>
</li>
<li class="fm-list-bullet">
<p class="list">Despite their impressive achievements, LLMs lack a true understanding of the content or the ability to reason. These limitations can lead to errors in logic, factual inaccuracies, and a failure to grasp complex concepts or nuances. Furthermore, the rapid advancement and widespread application of these models have given rise to a host of ethical concerns such as bias, misinformation, breach of privacy, and copyright infringements. These issues demand careful consideration and proactive measures to ensure that the development and deployment of LLMs align with ethical standards and societal values.<a id="marker-387"/></p>
</li>
</ul>
<hr class="calibre6"/>
<p class="fm-footnote"><a id="footnote-001"/><sup class="footnotenumber1"><a class="url1" href="#footnote-001-backlink">1</a></sup>  Maggiori, Emmanuel, 2023, <i class="fm-italics">Smart Until It’s Dumb: Why Artificial Intelligence Keeps Making Epic Mistakes (and Why the AI Bubble Will Burst)</i>, Applied Maths Ltd. Kindle Edition.</p>
<p class="fm-footnote"><a id="footnote-000"/><sup class="footnotenumber1"><a class="url1" href="#footnote-000-backlink">2</a></sup>  Adi Robertson, February 21, 2024, “Google Apologizes for 'Missing the Mark’ after Gemini Generated Racially Diverse Nazis.” The Verge, <a id="idTextAnchor007"/><a class="url" href="https://mng.bz/2ga9">https://mng.bz/2ga9</a>.</p>
</div></body></html>