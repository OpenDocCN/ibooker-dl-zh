- en: Chapter 10\. Using APIs in Data Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In their simplest form, pipelines may extract only data from one source such
    as a REST API and load to a destination such as a SQL table in a data warehouse.
    In practice, however, pipelines typically consist of multiple steps ... before
    delivering data to its final destination.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: James Densmore *Data Pipelines Pocket Reference* (O’Reilly, 2021)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In [Chapter 9](ch09.html#chapter_9), you used a Jupyter Notebook to query APIs
    and create data analytics. Querying directly in a notebook is useful for exploratory
    data analysis, but it requires you to keep querying the API over and over again.
    When data teams create analytics products for production, they implement scheduled
    processes to keep an up-to-date copy of source data in the format they need. These
    structured processes are called *data pipelines* because source data flows into
    the pipeline and is prepared and stored to create data products. Other common
    terms for these processes are *Extract, Transform, Load (ETL)* or *Extract, Load,
    Transform (ELT)*, depending on the technical details of how they are implemented.
    *Data engineer* is the specialized role that focuses on the development and operation
    of data pipelines, but in many organizations, data scientists, data analysts,
    and infrastructure engineers also perform this work.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will create a data pipeline to read SportsWorldCentral
    fantasy football player data using Apache Airflow, a popular open source tool
    for managing data pipelines using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Data Sources for Data Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The potential data sources for data pipelines are almost endless. Here are
    a few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: APIs
  prefs: []
  type: TYPE_NORMAL
- en: REST APIs are the focus of this book, and they are an important data source
    for data pipelines. They are better suited for incremental updates than full loads,
    because sending the full contents of a data source may require many network calls.
    Other API styles such as GraphQL and SOAP are also common.
  prefs: []
  type: TYPE_NORMAL
- en: Bulk files
  prefs: []
  type: TYPE_NORMAL
- en: Large datasets are often shared in some type of bulk file that can be downloaded
    and processed. This is an efficient way to process a very large data source. The
    file format of these may vary, but CSV and Parquet are popular formats for data
    science applications.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming data and message queues
  prefs: []
  type: TYPE_NORMAL
- en: For near-real-time updates of data, streaming sources such as Apache Kafka or
    AWS Kinesis provide continuous feeds of updates.
  prefs: []
  type: TYPE_NORMAL
- en: Message queues
  prefs: []
  type: TYPE_NORMAL
- en: Message queue software such as RabbitMQ or AWS SQS provides asynchronous messaging,
    which allows transactions to be published in a holding location and picked up
    later by a subscriber.
  prefs: []
  type: TYPE_NORMAL
- en: Direct database connections
  prefs: []
  type: TYPE_NORMAL
- en: A connection to the source database allows a consumer to get data in its original
    format. These are more common for sharing data inside organizations than to outside
    consumers.
  prefs: []
  type: TYPE_NORMAL
- en: You will be creating a pipeline that uses REST APIs and bulk files in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Planning Your Data Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your goal is to read SportsWorldCentral data and store it in a local database
    that you can keep up to date. This allows you to create analytics products such
    as reports and dashboards. For this scenario, you’ll assume that the API does
    not allow full downloads of the data, so you will need to use a bulk file for
    the initial load.
  prefs: []
  type: TYPE_NORMAL
- en: After that initial load, you want to get a daily update of any new records or
    records that have been updated. These changed records are commonly referred to
    as *delta* or *deltas*, using the mathematical term for “change.” By processing
    only the changed records, the update process will run more quickly and use fewer
    resources (and spend less money).
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-1](#date_pipeline_plan_ch10) displays the data pipeline you are
    planning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Plan for your data pipeline](assets/haad_1001.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-1\. Plan for your data pipeline
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The pipeline includes two sources: bulk data files and an API. The rounded
    boxes represent two ETL processes and they both will update the analytics database,
    a local database that is used to create analytics products like dashboards and
    reports.'
  prefs: []
  type: TYPE_NORMAL
- en: Orchestrating the Data Pipeline with Apache Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Airflow is best thought of as a spider in a web: it sits in the middle of your
    data processes and coordinates work happening across the different (distributed)
    systems.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Julian de Ruiter and Bas Harenslak, *Data Pipelines with Apache Airflow* (Manning,
    2021)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Running multiple data processing work streams in production gets complicated
    quickly. Scheduling, error handling, and restarting failed processes require significant
    planning and design. These tasks are called *orchestration*, and this is what
    Apache Airflow is used for. As the number of data pipelines grows, you will benefit
    from using orchestration software instead of coding all of these tasks yourself.
    Airflow is a full-featured open source engine that uses Python for its configuration,
    and it handles many of the recurring tasks involved in data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow has some specialized terminology that is not used in other data science
    programming. Astronomer’s [Airflow glossary](https://oreil.ly/IjTM4) is a complete
    source for these, but I will share some of the most important ones with you.
  prefs: []
  type: TYPE_NORMAL
- en: Airflow uses terminology from mathematical graph theory. In graph theory, a
    *node* is a process and an *edge* is a flow between nodes. Using this terminology,
    a *directed acyclic graph* (DAG) is a top-level process that contains steps proceeding
    in one direction without any loops or recursive logic.
  prefs: []
  type: TYPE_NORMAL
- en: '[Figure 10-2](#directed_acyclic_graph_ch10) shows how nodes and edges relate
    to each other in a DAG.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Directed acyclic graph](assets/haad_1002.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-2\. Directed acyclic graph
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You will create one Python file for each DAG. Each of the steps in a DAG is
    called a *task*, the basic unit of execution in Airflow. Each task will be displayed
    as a single box on the graph diagram of a DAG.
  prefs: []
  type: TYPE_NORMAL
- en: An *operator* is a predefined template for a task. In this chapter, you will
    use an `Http​Op⁠erator` to call your API and a `PythonOperator` to update your
    analytics database. Airflow has built-in operators to interact with databases,
    S3 buckets, and several other functions. Dozens more are available from the community
    and are listed in the [Airflow Operators and Hooks Reference](https://oreil.ly/8k6mr).
  prefs: []
  type: TYPE_NORMAL
- en: The last thing you will learn to use is an *XCom*, which stands for *cross-communications*.
    XComs are used to pass information and data between tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Apache Airflow in GitHub Codespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Figure 10-3](#ch10_architecture_airflow) shows the high-level architecture
    of the project you will create in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Architecture of Airflow project](assets/haad_1003.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-3\. Architecture of the Airflow project
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You will be working with the Part II GitHub Codespace that you created in [“Getting
    Started with Your GitHub Codespace”](ch08.html#ch08_getting_started). If you haven’t
    created your Part II Codespace yet, you can complete that section now.
  prefs: []
  type: TYPE_NORMAL
- en: Before launching the Codespace, change the machine type to a four-core machine
    by clicking the ellipsis next to the Codespace and then clicking “Change machine
    type.” This is necessary because Airflow runs multiple services at once.
  prefs: []
  type: TYPE_NORMAL
- en: You will be installing Airflow in the Codespace and performing that basic configuration
    that allows you to create the data pipeline from the diagram. (This will be a
    non-production setup for demonstration purposes. Before using Airflow in production,
    additional setup would be required.)
  prefs: []
  type: TYPE_NORMAL
- en: Airflow can be installed using Docker or `pip`. You will be using the Docker
    version. You will follow the instructions from [“Running Airflow in Docker”](https://oreil.ly/ORZKy),
    with a few customizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, create an *airflow* directory in the *chapter10* folder of your Codespace
    and change to that directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, use the `curl` command to retrieve a copy of the *docker-compose.yaml*
    file that is used to run the Docker version of Airflow. Get this from the [official
    Airflow website](https://airflow.apache.org), and specify the version. This chapter
    demonstrates with version 2.9.3, but you can follow the latest stable version
    listed in the [Airflow documentation](https://oreil.ly/QTlk_):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The file *docker-compose.yaml* contains instructions for the images to download
    from [Docker Hub](https://oreil.ly/q7y53) along with environment options for configuring
    the software in your environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open *docker_compose.yaml* and take a look at the `volumes:` section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This section creates Docker *volumes*, which are virtual drives available inside
    the Docker containers that are mapped to files in your Codespace storage. They
    are relative to the Airflow project directory, which will be *airflow* in your
    Codespace. For example, *airflow/dags* in your Codespace will be referenced as
    */opt/airflow/dags* to the Airflow application running in Docker. (This will be
    important when you create connections later in this chapter.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the directories that are mapped to those volumes and then configure
    an environment variable for the Airflow user ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create *docker-compose.override.yaml*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You will use this file to override some of the standard configuration settings
    from the *docker-compose.yaml* file you downloaded. Using an override file allows
    you to keep the *docker-compose.yaml* file exactly like you downloaded it and
    put all of your customizations together, which makes troubleshooting easier. It
    also allows you to update *docker-compose.yaml* with a new version when Airflow
    is upgraded. Update *docker-compose.override.yaml* with the following contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_using_apis_in_data_pipelines_CO1-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This setting will hide the built-in Airflow examples so that they are not distracting
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_using_apis_in_data_pipelines_CO1-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This setting will allow you to use the Airflow web interface in Codespaces.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_using_apis_in_data_pipelines_CO1-3)'
  prefs: []
  type: TYPE_NORMAL
- en: This setting tells Airflow to look for changes to your code more frequently
    while you are developing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now you are ready to initialize the Docker environment using *docker-compose.yaml*
    and *docker-compose.override.yaml* with the `docker compose up airflow-init` command.
    This command will download the Airflow software and provision user IDs and other
    configuration details. Execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This command will run for several minutes, with many commands executed. If the
    output ends with “exited with code 0” it was successful. Your environment has
    been initialized, and you don’t need to execute this command again.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are ready to run Airflow. To launch the Airflow web interface, execute
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Although you will see a pop-up window to launch the web UI, I have found that
    sometimes the web UI takes a few minutes to prepare, so don’t click OK. Instead,
    wait a couple of minutes and then select the Ports tab in your Codespace. You
    will see the forwarded address of the web interface. Click the globe icon to open
    the UI in the browser, as shown in [Figure 10-4](#open_in_browser_ch10).
  prefs: []
  type: TYPE_NORMAL
- en: '![Open Airflow web interface](assets/haad_1004.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-4\. Open Airflow web interface
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You will see the login page. Enter a username of **`airflow`** and password
    of **`airflow`** and click “Sign in.” (These starter credentials are used for
    demonstration only.) You will see the web interface of the Airflow application
    running in your Codespace, as shown in [Figure 10-5](#airflow_home_page_ch10).
    When you begin, there are no DAGs listed. You will learn more about the capabilities
    of Airflow as you create DAGs to complete your data pipeline requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '![Airflow home page](assets/haad_1005.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-5\. Airflow home page
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Creating Your Local Analytics Database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Your data pipeline will be used to insert and update player records into a
    local database. This is a common data science pattern: updating a database from
    source data and then creating models, metrics, and reports from the database.
    Change the directory to *dags* and create a database and the `player` table as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Launching Your API in Codespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Your Airflow pipeline needs a running copy of the SportsWorldCentral API to
    gather updates. Follow the directions in [“Running the SportsWorldCentral (SWC)
    API Locally”](ch08.html#sportsworldcentral) to get your API running in a separate
    terminal window of Codespaces, and copy the base URL from the browser bar. You
    will configure Airflow to reference the base URL from that API in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Airflow Connections
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Airflow *connections* allow you to store information about data sources and
    targets in the server instead of in your code. This is useful for maintaining
    separate Airflow environments for development, testing, and production. You will
    create connections for your API and your analytics database.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Airflow UI, select Admin > Connections. Click the plus sign to add a
    new connection record. Now you will use the *volume* mappings that you viewed
    earlier in the *docker-compose.yaml* file. Use the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Connection ID*: **`analytics_database`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Connection Type*: Sqlite'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Description*: **`Database to store local analytics data.`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Schema*: **`/opt/airflow/dags/analytics_database.db`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leave the rest of the values empty, and click Save.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, add the connection for the API connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Connection ID*: **`sportsworldcentral_url`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Connection Type*: HTTP'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Description*: **`URL for calling the SportsWorldCentral API.`**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Host*: Enter the base URL of the API running in Codespaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leave the rest of the values empty, and click Save. You should see two connections
    listed, as shown in [Figure 10-6](#configured_connections_ch10).
  prefs: []
  type: TYPE_NORMAL
- en: '![Configured Airflow connections](assets/haad_1006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-6\. Configured Airflow connections
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Creating Your First DAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Figure 10-7](#date_pipeline_airflow_ch10) displays an implementation of your
    pipeline with Airflow, using two DAGs. The *bulk_player_file_load.py* DAG would
    perform an initial load of the analytics database from a bulk file, which was
    provided in [Part I](part01.html#part_1) of this book. That file is available
    in the *chapter10/complete* folder of your repository, but this chapter does not
    walk through it due to space constraints.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Airflow components of your data pipeline](assets/haad_1007.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-7\. Airflow components of your data pipeline
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Create the DAG that uses API data, *recurring_player_api_insert_update_dag.py*.
    This DAG performs incremental updates of your database, using the SportsWorldCentral
    API. Change the directory to *dags* and create the *recurring_player_api_insert_update_dag.py*
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following contents to the *recurring_player_api_insert_update_dag.py*
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_using_apis_in_data_pipelines_CO2-1)'
  prefs: []
  type: TYPE_NORMAL
- en: This import allows you to define the DAG using a `@dag` decorator.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_using_apis_in_data_pipelines_CO2-2)'
  prefs: []
  type: TYPE_NORMAL
- en: These two imports allow you to use predefined operators in your tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_using_apis_in_data_pipelines_CO2-3)'
  prefs: []
  type: TYPE_NORMAL
- en: This is an import of a separate Python file with a function that is shared between
    two DAGs.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_using_apis_in_data_pipelines_CO2-4)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the code to verify the response of `api_health_check_task` defined below.
    This is the first task, and it allows the DAG to verify the status of the API
    before proceeding with other tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_using_apis_in_data_pipelines_CO2-5)'
  prefs: []
  type: TYPE_NORMAL
- en: This defines a function that will be called by a task.
  prefs: []
  type: TYPE_NORMAL
- en: '[![6](assets/6.png)](#co_using_apis_in_data_pipelines_CO2-6)'
  prefs: []
  type: TYPE_NORMAL
- en: This line of code uses XCom to retrieve data from the second task.
  prefs: []
  type: TYPE_NORMAL
- en: '[![7](assets/7.png)](#co_using_apis_in_data_pipelines_CO2-7)'
  prefs: []
  type: TYPE_NORMAL
- en: Here it passes the data from XCom to the shared `upsert_player_data` function,
    which is defined in a separate file.
  prefs: []
  type: TYPE_NORMAL
- en: '[![8](assets/8.png)](#co_using_apis_in_data_pipelines_CO2-8)'
  prefs: []
  type: TYPE_NORMAL
- en: This is the main DAG definition. It uses the `@dag` decorator to define the
    Python function as a DAG. The tasks are defined within this method.
  prefs: []
  type: TYPE_NORMAL
- en: '[![9](assets/9.png)](#co_using_apis_in_data_pipelines_CO2-9)'
  prefs: []
  type: TYPE_NORMAL
- en: The first task uses an `HttpOperator` template to call the API’s health check
    endpoint. It adds a `response_check` method to check the API’s status before continuing.
  prefs: []
  type: TYPE_NORMAL
- en: '[![10](assets/10.png)](#co_using_apis_in_data_pipelines_CO2-10)'
  prefs: []
  type: TYPE_NORMAL
- en: The minimum last changed date is hardcoded in this example. In production, an
    [Airflow template variable](https://oreil.ly/pFHaG) could be used to get the last
    day’s updates.
  prefs: []
  type: TYPE_NORMAL
- en: '[![11](assets/11.png)](#co_using_apis_in_data_pipelines_CO2-11)'
  prefs: []
  type: TYPE_NORMAL
- en: The second task uses an `HttpOperator` to call the API’s player endpoint with
    a query parameter to restrict the records that are returned.
  prefs: []
  type: TYPE_NORMAL
- en: '[![12](assets/12.png)](#co_using_apis_in_data_pipelines_CO2-12)'
  prefs: []
  type: TYPE_NORMAL
- en: The third task is a `PythonOperator` that calls the `insert_update_player_data`
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '[![13](assets/13.png)](#co_using_apis_in_data_pipelines_CO2-13)'
  prefs: []
  type: TYPE_NORMAL
- en: This statement sets the dependency of the tasks using bitshift operators.
  prefs: []
  type: TYPE_NORMAL
- en: '[![14](assets/14.png)](#co_using_apis_in_data_pipelines_CO2-14)'
  prefs: []
  type: TYPE_NORMAL
- en: The final statement is required to instantiate the DAG that is defined by the
    `@dag` decorator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a moment to compare this code to [Figure 10-7](#date_pipeline_airflow_ch10).
    The key parts of the DAG file are toward the bottom of this file: the `@dag` decorator
    defines the main DAG wrapper. Inside the DAG are three tasks: two that use `HttpOperator`s
    to connect to the API and one that uses a `PythonOperator` to connect to the SQLite
    database.'
  prefs: []
  type: TYPE_NORMAL
- en: The statement `api_health_check_task >> api_player_query_task >> player_sqlite_upsert_task`
    sets the dependency between the tasks using a right-shift operator, `>>`. These
    tasks have a very simple sequential dependency, but Airflow is capable of implementing
    very intricate dependencies between tasks. For more information about this capability,
    read Astronomer’s [“Manage task and task group dependencies in Airflow”](https://oreil.ly/PTa4M).
  prefs: []
  type: TYPE_NORMAL
- en: Coding a Shared Function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although the sources of the two DAGs are different, they both perform an *upsert*
    on the analytics database, which means that if a source record already exists
    in the database, the code updates it, otherwise it inserts a new record. Because
    this task is shared between the two DAGs, you will create a separate Python file
    with a shared function. Create the *shared_functions.py* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Add the following contents to the *shared_functions.py* file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[![1](assets/1.png)](#co_using_apis_in_data_pipelines_CO3-1)'
  prefs: []
  type: TYPE_NORMAL
- en: These two import statements are placed inside the Python method. This is because
    Airflow frequently parses DAG code and will reload imported libraries that are
    at the top of the Python file.
  prefs: []
  type: TYPE_NORMAL
- en: '[![2](assets/2.png)](#co_using_apis_in_data_pipelines_CO3-2)'
  prefs: []
  type: TYPE_NORMAL
- en: This statement uses an Airflow hook to retrieve the connection that you defined
    in the Airflow user interface.
  prefs: []
  type: TYPE_NORMAL
- en: '[![3](assets/3.png)](#co_using_apis_in_data_pipelines_CO3-3)'
  prefs: []
  type: TYPE_NORMAL
- en: This uses a database cursor to execute SQL queries on your analytics database.
  prefs: []
  type: TYPE_NORMAL
- en: '[![4](assets/4.png)](#co_using_apis_in_data_pipelines_CO3-4)'
  prefs: []
  type: TYPE_NORMAL
- en: This statement uses the database cursor to execute a parameterized SQL query.
  prefs: []
  type: TYPE_NORMAL
- en: '[![5](assets/5.png)](#co_using_apis_in_data_pipelines_CO3-5)'
  prefs: []
  type: TYPE_NORMAL
- en: This SQL statement provides the upsert capability, which updates a record if
    it already exists or inserts it if not.
  prefs: []
  type: TYPE_NORMAL
- en: This function receives the data from the API as a parameter and then loads data
    into the SQLite database using the Airflow connection that you defined in the
    user interface. This is a parameterized SQL query, in which the input data is
    referenced with `VALUES (?, ?, ?, ?, ?, ?)`. This is an important measure to protect
    against SQL injection attacks, which could occur if a malicious actor inserted
    code into the source data’s fields, where your process was expecting data.
  prefs: []
  type: TYPE_NORMAL
- en: Running Your DAG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before you run the DAG, check that your API is up and running. Navigate back
    to the Airflow UI and you will see your DAG listed, as shown in [Figure 10-8](#DAG_listing_main_page_ch10).
    The user interface has too many features to cover in this chapter, but you can
    read about the user interface at [“UI / Screenshots”](https://oreil.ly/DfOSC).
  prefs: []
  type: TYPE_NORMAL
- en: '![DAG listed on Airflow home page](assets/haad_1008.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-8\. DAG listed on the Airflow home page
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Click `recurring_player_api_insert_update_dag`, and then Graph. You will see
    the sequence of Airflow tasks using the `task_id` names that you assigned in your
    code, as shown in [Figure 10-9](#graph_view_ch10).
  prefs: []
  type: TYPE_NORMAL
- en: '![Graph view of first DAG](assets/haad_1009.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-9\. Graph view of the first DAG
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Click the Trigger DAG button, which has a triangle icon to your DAG. If everything
    is configured correctly with your code and connections, each of the tasks in your
    DAG should complete with a green box in a minute or so. Click the first box, labeled
    `check_api_health_check_endpoint`. Your view should look similar to [Figure 10-10](#successful_dag_run_ch10).
    If you encounter an error, click the task that has the error, and click Logs to
    diagnose the issue.
  prefs: []
  type: TYPE_NORMAL
- en: '![Successful DAG run](assets/haad_1010.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10-10\. Successful DAG run
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To confirm that your analytics database was successfully upserted, go back
    to the terminal and open the database with SQLite. Query the Player table, to
    confirm that 1,018 player records are present in the table. These are the records
    retrieved from your API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations! You created a data pipeline that updates your database with
    records from an API!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to create a data pipeline calling APIs to maintain
    current data for analytics products. You installed and configured Apache Airflow,
    and you created a DAG with multiple tasks to update your database from an API.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 11](ch11.html#chapter_11), you will create a Streamlit data app
    using data from an API.
  prefs: []
  type: TYPE_NORMAL
