- en: Chapter 6\. Question Matching
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。问题匹配
- en: 'We’ve now seen a few examples of how we can construct and use word embeddings
    to compare terms with one another. It’s natural to ask how we can extend this
    idea to larger blocks of text. Can we create semantic embeddings of entire sentences
    or paragraphs? In this chapter, we’ll try to do just that: we’re going to use
    data from Stack Exchange to build embeddings for entire questions; we can then
    use those embeddings to find similar documents or questions.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经看到了一些示例，说明我们如何构建和使用词嵌入来比较术语。自然而然地，我们会问如何将这个想法扩展到更大的文本块。我们能否为整个句子或段落创建语义嵌入？在本章中，我们将尝试做到这一点：我们将使用来自Stack
    Exchange的数据为整个问题构建嵌入；然后我们可以使用这些嵌入来查找相似的文档或问题。
- en: We’ll start out by downloading and parsing our training data from the Internet
    Archive. Then we’ll briefly explore how Pandas can be helpful for analyzing data.
    We let Keras do the heavy lifting when it comes to featurizing our data and building
    a model for the task at hand. We then look into how to feed this model from a
    Pandas `DataFrame` and how we can run it to draw conclusions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从互联网档案馆下载和解析我们的训练数据。然后我们将简要探讨Pandas如何帮助分析数据。当涉及到对数据进行特征化和构建模型时，我们让Keras来处理繁重的工作。然后我们将研究如何从Pandas
    `DataFrame`中提供此模型以及如何运行它以得出结论。
- en: 'The code for this chapter can be found in the following notebook:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下笔记本中找到：
- en: '[PRE0]'
  id: totrans-4
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 6.1 Acquiring Data from Stack Exchange
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.1 从Stack Exchange获取数据
- en: Problem
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You need to access a large set of questions to kick-start your training.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要访问大量问题来启动您的训练。
- en: Solution
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the Internet Archive to retrieve a dump of questions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用互联网档案馆检索问题的转储。
- en: 'A Stack Exchange data dump is freely available on the [Internet Archive](https://archive.org/details/stackexchange),
    which hosts a number of interesting datasets (as well as striving to provide an
    archive of the entire web). The data is laid out with one ZIP file for each area
    on Stack Exchange (e.g., travel, sci-fi, etc.). Let’s download the file for the
    travel section:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Stack Exchange数据转储可以在[互联网档案馆](https://archive.org/details/stackexchange)上免费获取，该网站提供许多有趣的数据集（并努力提供整个网络的存档）。数据以一个ZIP文件的形式布置在Stack
    Exchange的每个领域上（例如旅行、科幻等）。让我们下载旅行部分的文件：
- en: '[PRE1]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'While the input is technically an XML file, the structure is simple enough
    that we can get away with just reading individual lines and splitting out the
    fields. This is a bit brittle, of course. We will limit ourselves to processing
    1 million records from the dataset; this keeps our memory usage from blowing up
    and should be enough data for us to work with. We’ll save the processed data as
    a JSON file so we won’t have to do the processing again the next time around:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然输入技术上是一个XML文件，但结构足够简单，我们可以通过仅读取单独的行并拆分字段来处理。当然，这有点脆弱。我们将限制自己处理数据集中的100万条记录；这可以避免内存使用过多，并且应该足够让我们处理。我们将处理后的数据保存为JSON文件，这样下次就不必再次处理：
- en: '[PRE2]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Discussion
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The Stack Exchange datasets is a great source for question/answer pairs that
    comes with a nice reuse license. As long as you give attribution you can use it
    in pretty much any way you want. Converting the zipped XML into the more easily
    consumable JSON format is a good preprocessing step.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Stack Exchange数据集是一个提供问题/答案对的很好的来源，并附带一个很好的可重用许可证。只要您进行归属，您可以以几乎任何方式使用它。将压缩的XML转换为更易消耗的JSON格式是一个很好的预处理步骤。
- en: 6.2 Exploring Data Using Pandas
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.2 使用Pandas探索数据
- en: Problem
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you quickly explore a large dataset so you can make sure it contains
    what you expect?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如何快速探索大型数据集，以确保其中包含您期望的内容？
- en: Solution
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use Python’s Pandas.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Python的Pandas。
- en: 'Pandas is a powerful framework for data processing in Python. In some ways
    it is comparable to a spreadsheet; the data is stored in rows and columns and
    we can quickly filter, convert, and aggregate on the records. Let’s start by converting
    our rows of Python dictionaries into a `DataFrame`. Pandas tries to “guess” the
    types of some columns. We’ll coerce the columns we care about into the right format:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas是Python中用于数据处理的强大框架。在某些方面，它类似于电子表格；数据存储在行和列中，我们可以快速过滤、转换和聚合记录。让我们首先将我们的Python字典行转换为`DataFrame`。Pandas会尝试“猜测”一些列的类型。我们将强制将我们关心的列转换为正确的格式：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With `df.head` we can now see what’s going on in our database.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过`df.head`，我们现在可以看到我们数据库中发生了什么。
- en: 'We can also use Pandas to take a quick look at popular questions in our data:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用Pandas快速查看我们数据中的热门问题：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you might expect, the most popular questions are general questions about
    frequently used languages.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所期望的那样，最受欢迎的问题是关于经常使用的语言的一般问题。
- en: Discussion
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: Pandas is a great tool for many types of data analysis, whether you just want
    to have a casual look at the data or you want to do in-depth analysis. It can
    be tempting to try to leverage Pandas for many tasks, but unfortunately the Pandas
    interface is not at all regular and for complex operations the performance can
    be significantly worse than using a real database. Lookups in Pandas are significantly
    more expensive than using a Python dictionary, so be careful!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas是一个很好的工具，适用于许多类型的数据分析，无论您只是想简单查看数据还是想进行深入分析。尽管很容易尝试利用Pandas来完成许多任务，但不幸的是，Pandas的接口并不规范，对于复杂操作，性能可能会明显不如使用真正的数据库。在Pandas中进行查找比使用Python字典要昂贵得多，所以要小心！
- en: 6.3 Using Keras to Featurize Text
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.3 使用Keras对文本进行特征化
- en: Problem
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you quickly create feature vectors from text?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如何快速从文本创建特征向量？
- en: Solution
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Use the `Tokenizer` class from Keras.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Keras的`Tokenizer`类。
- en: 'Before we can feed text into a model, we need to convert it into feature vectors.
    A common way to do this is to assign an integer to each of the top *N* words in
    a text and then replace each word by its integer. Keras makes this really straightforward:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以将文本输入模型之前，我们需要将其转换为特征向量。一个常见的方法是为文本中的前*N*个单词分配一个整数，然后用其整数替换每个单词。Keras使这变得非常简单：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now let’s tokenize the titles and bodies of our whole dataset:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们对整个数据集的标题和正文进行标记化：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Discussion
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Converting text to a series of numbers by using a tokenizer is one of the classic
    ways of making text consumable by a neural network. In the previous chapter we
    converted text on a per-character basis. Character-based models take as input
    individual characters (removing the need for a tokenizer). The trade-off is in
    how long it takes to train the model: because you’re forcing the model to learn
    how to tokenize and stem words, you need more training data and more time.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用标记器将文本转换为一系列数字是使文本可被神经网络消化的经典方法之一。在前一章中，我们是基于每个字符进行文本转换的。基于字符的模型以单个字符作为输入（无需标记器）。权衡在于训练模型所需的时间：因为您强制模型学习如何对单词进行标记化和词干化，所以您需要更多的训练数据和更多的时间。
- en: One of the drawbacks of processing texts on a per-word basis is the fact that
    there is no practical upper limit to the number of different words that can appear
    in the texts, especially if we have to handle typos and errors. In this recipe
    we only pay attention to words that appear in the top 50,000 by count, which is
    one way around this problem.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 基于每个单词处理文本的一个缺点是，文本中出现的不同单词的数量没有实际上限，特别是如果我们必须处理拼写错误和错误。在这个示例中，我们只关注出现在前 50,000
    位的单词，这是解决这个问题的一种方法。
- en: 6.4 Building a Question/Answer Model
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.4 构建问题/答案模型
- en: Problem
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you calculate embeddings for questions?
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如何计算问题的嵌入？
- en: Solution
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Train a model to predict whether a question and an answer from the Stack Exchange
    dataset match.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个模型来预测 Stack Exchange 数据集中的问题和答案是否匹配。
- en: 'Whenever we construct a model, the first question we should ask is: “What is
    our objective?” That is, what is the model going to try to classify?'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们构建一个模型时，我们应该问自己的第一个问题是：“我们的目标是什么？”也就是说，模型将尝试对什么进行分类？
- en: 'Ideally we’d have a list of “similar questions to this one,” which we could
    use to train our model. Unfortunately, it would be very expensive to acquire such
    a dataset! Instead, we’ll rely on a surrogate objective: let’s see if we can train
    our model to, given a question, distinguish between the matching answer and an
    answer from a random question. This will force the model to learn a good representation
    of titles and bodies.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们会有一个“与此类似的问题列表”，我们可以用来训练我们的模型。不幸的是，获取这样的数据集将非常昂贵！相反，我们将依赖于一个替代目标：让我们看看是否可以训练我们的模型，即给定一个问题，区分匹配答案和来自随机问题的答案。这将迫使模型学习标题和正文的良好表示。
- en: 'We start off our model by defining our inputs. In this case we have two inputs,
    the title (question) and body (answer):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过定义我们的输入来启动我们的模型。在这种情况下，我们有两个输入，标题（问题）和正文（答案）：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Both are of varying length, so we have to pad them. The data for each field
    will be a list of integers, one for each word in the title or the body.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 两者长度不同，因此我们必须对它们进行填充。每个字段的数据将是一个整数列表，每个整数对应标题或正文中的一个单词。
- en: 'Now we want to define a shared set of layers that both inputs will be passed
    through. We’re first going to construct an embedding for the inputs, then mask
    out the invalid values, and add all of the words’ values together:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想要定义一组共享的层，两个输入都将通过这些层。我们首先要为输入构建一个嵌入，然后屏蔽无效值，并将所有单词的值相加：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Here, we’ve specified a `vocab_size` (how many words are in our vocabulary)
    and an `embedding_size` (how wide our embedding of each word should be; the GoogleNews
    vectors are 300 dimensions, for example).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们指定了 `vocab_size`（我们的词汇表中有多少单词）和 `embedding_size`（每个单词的嵌入应该有多宽；例如，GoogleNews
    的向量是 300 维）。
- en: 'Now let’s apply these layers to our word inputs:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将这些层应用于我们的单词输入：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now that we have a single vector for our title and body, we can compare them
    to each other with a cosine distance, just like we did in [Recipe 4.2](ch04.html#training-movie-embeddings).
    In Keras, that is expressed via the `dot` layer:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了标题和正文的单个向量，我们可以通过余弦距离将它们相互比较，就像我们在 [Recipe 4.2](ch04.html#training-movie-embeddings)
    中所做的那样。在 Keras 中，这通过 `dot` 层来表示：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, we can define our model. It takes the title and the body in and outputs
    the similarity between the two:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以定义我们的模型。它接受标题和正文作为输入，并输出两者之间的相似度：
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Discussion
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: The model we’ve built here learns to match questions and answers but really
    the only freedom we give it is to change the embeddings of the words such that
    the sums of the embeddings of the title and the body match. This should get us
    embeddings for questions such that questions that are similar will have similar
    embeddings, because similar questions will have similar answers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里构建的模型学习匹配问题和答案，但实际上我们给它的唯一自由是改变单词的嵌入，使标题和正文的嵌入之和相匹配。这应该为我们提供问题的嵌入，使得相似的问题具有相似的嵌入，因为相似的问题将有相似的答案。
- en: 'Our training model is compiled with two parameters telling Keras how to improve
    the model:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的训练模型编译时使用了两个参数，告诉 Keras 如何改进模型：
- en: The loss function
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数
- en: This tells the system how “wrong” a given answer is. For example, if we told
    the network that `title_a` and `body_a` should output 1.0, but the network predicts
    0.8, how bad of an error is that? This becomes a more complex problem when we
    have multiple outputs, but we’ll cover that later. For this model, we’re going
    to use *mean squared error*. For the previous example, this means we would penalize
    the model by (1.0–0.8) ** 2, or 0.04\. This loss will be propagated back through
    the model and improve the embeddings each time the model sees an example.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉系统一个给定答案有多“错误”。例如，如果我们告诉网络 `title_a` 和 `body_a` 应该输出 1.0，但网络预测为 0.8，那么这是多么糟糕的错误？当我们有多个输出时，这将变得更加复杂，但我们稍后会涵盖这一点。对于这个模型，我们将使用
    *均方误差*。对于前面的例子，这意味着我们将通过 (1.0–0.8) ** 2，或 0.04 来惩罚模型。这种损失将通过模型传播回去，并在模型看到一个示例时改进每次的嵌入。
- en: The optimizer
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 优化器
- en: 'There are many ways that loss can be used to improve our model. These are called
    *optimization strategies*, or *optimizers*. Fortunately, Keras comes with a number
    of reliable optimizers built in, so we won’t have to worry much about this: we
    can just pick a suitable one. In this case, we’re using the `rmsprop` optimizer,
    which tends to perform very well across a wide range of problems.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以利用损失来改进我们的模型。这些被称为*优化策略*或*优化器*。幸运的是，Keras内置了许多可靠的优化器，所以我们不必太担心这个问题：我们只需要选择一个合适的。在这种情况下，我们使用`rmsprop`优化器，这个优化器在各种问题上表现非常好。
- en: 6.5 Training a Model with Pandas
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.5 使用Pandas训练模型
- en: Problem
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: How do you train a model on data contained in Pandas?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如何在Pandas中包含的数据上训练模型？
- en: Solution
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Build a data generator that leverages the filter and sample features of Pandas.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个数据生成器，利用Pandas的过滤和采样特性。
- en: 'As in the previous recipe, we are going to train our model to distinguish between
    a question title and the correct answer (body) versus the answer to another random
    question. We can write that out as a generator that iterates over our dataset.
    It will output a 1 for the correct question title and body and a 0 for a random
    title and body:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一个配方一样，我们将训练我们的模型来区分问题标题和正确答案（正文）与另一个随机问题的答案。我们可以将其写成一个迭代我们数据集的生成器。它将为正确的问题标题和正文输出1，为随机标题和正文输出0：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The only complication here is the batching of the data. This is not strictly
    necessary, but extremely important for performance. All deep learning models are
    optimized to work on chunks of data at a time. The best batch size to use depends
    on the problem you’re working on. Using larger batches means your model sees more
    data for each update and therefore can more accurately update its weights, but
    on the flip side it can’t update as often. Bigger batch sizes also take more memory.
    It’s best to start small and keep doubling the batch size until the results no
    longer improve.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这里唯一的复杂性是数据的分批处理。这并不是绝对必要的，但对性能非常重要。所有深度学习模型都被优化为一次处理数据块。要使用的最佳批量大小取决于您正在处理的问题。使用更大的批量意味着您的模型在每次更新时看到更多数据，因此可以更准确地更新其权重，但另一方面它不能经常更新。更大的批量大小也需要更多内存。最好从小批量开始，然后将批量大小加倍，直到结果不再改善。
- en: 'Now let’s train the model:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们训练模型：
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We’ll train it for 10,000 steps, divided into 10 epochs of 1,000 steps each.
    Each step will process 128 documents, so our network will end up seeing 1.28M
    training examples. If you have a GPU, you’ll be surprised how quickly this runs!
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将进行10,000步的训练，分为10个包含1,000步的时期。每一步将处理128个文档，因此我们的网络最终将看到1.28M个训练示例。如果您有GPU，您会惊讶地发现这个过程运行得多么快！
- en: 6.6 Checking Similarities
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6.6 检查相似性
- en: Problem
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: You’d like to use Keras to predict values by using the weights of another network.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 您想使用Keras通过另一个网络的权重来预测值。
- en: Solution
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案
- en: Construct a second model that uses different input and output layers from the
    original network, but shares some of the other layers.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个使用原始网络不同输入和输出层的第二个模型，但共享其他一些层。
- en: 'Our `sim_model` has been trained and as part of that learned how to go from
    a title to a `title_sum`, which is really what we are after. The model that just
    does that is:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的`sim_model`已经训练过了，并且作为其中的一部分学会了如何从标题到`title_sum`，这正是我们想要的。只做这件事的模型是：
- en: '[PRE15]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can now use the “embedding” model to compute a representation for each question
    in our dataset. Let’s wrap this up in a class for easy reuse:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用“嵌入”模型为数据集中的每个问题计算一个表示。让我们将这个封装成一个易于重复使用的类：
- en: '[PRE16]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'And now we can use it:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用它了：
- en: '[PRE17]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This produces the following results:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下结果：
- en: '| Similarity | Question |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 相似性 | 问题 |'
- en: '| --- | --- |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 0.892392 | working with django and sqlalchemy but backend… |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 0.892392 | 使用django和SQLAlchemy但后端... |'
- en: '| 0.893417 | Python ORM that auto-generates/updates tables … |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 0.893417 | 自动生成/更新表的Python ORM... |'
- en: '| 0.893883 | Dynamic Table Creation and ORM mapping in SqlA… |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 0.893883 | 在SqlA中进行动态表创建和ORM映射... |'
- en: '| 0.896096 | SQLAlchemy with count, group_by and order_by u… |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 0.896096 | 使用count、group_by和order_by的SQLAlchemy... |'
- en: '| 0.897706 | SQLAlchemy: Scan huge tables using ORM? |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 0.897706 | 使用ORM扫描大表？ |'
- en: '| 0.902693 | Efficiently updating database using SQLAlchemy… |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 0.902693 | 使用SQLAlchemy高效更新数据库... |'
- en: '| 0.911446 | What are some good Python ORM solutions? |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 0.911446 | 有哪些好的Python ORM解决方案？ |'
- en: '| 0.922449 | python orm |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 0.922449 | Python ORM |'
- en: '| 0.924316 | Python libraries to construct classes from a r… |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 0.924316 | 从r...构建类的Python库... |'
- en: '| 0.930865 | python ORM allowing for table creation and bul… |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 0.930865 | 允许创建表和构建的Python ORM... |'
- en: In a very short training time, our network managed to figure out that “SQL,”
    “query,” and “INSERT” are all related to Postgres!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在非常短的训练时间内，我们的网络设法弄清楚了“SQL”、“查询”和“插入”都与Postgres有关！
- en: Discussion
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 讨论
- en: In this recipe we saw how we can use part of a network to predict the values
    we’re after, even if the overall network was trained to predict something else.
    The functional API of Keras provides a nice separation between the layers, how
    they are connected, and which combination of input and output layers forms a model.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们看到了如何使用网络的一部分来预测我们想要的值，即使整个网络是为了预测其他东西而训练的。Keras的功能API提供了层之间、它们如何连接以及哪种输入和输出层组成模型的良好分离。
- en: As we’ll see later in this book, this gives us a lot of flexibility. We can
    take a pre-trained network and use one of the middle layers as an output layer,
    or we can take one of those middle layers and add some new layers (see [Chapter 9](ch09.html#transfer_learning)).
    We can even run the network backwards (see [Chapter 12](ch12.html#image_style)).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将在本书后面看到的，这给了我们很大的灵活性。我们可以取一个预训练的网络，并使用其中一个中间层作为输出层，或者我们可以取其中一个中间层并添加一些新的层（参见[第9章](ch09.html#transfer_learning)）。我们甚至可以反向运行网络（参见[第12章](ch12.html#image_style)）。
