- en: Chapter 6\. Question Matching
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ve now seen a few examples of how we can construct and use word embeddings
    to compare terms with one another. It’s natural to ask how we can extend this
    idea to larger blocks of text. Can we create semantic embeddings of entire sentences
    or paragraphs? In this chapter, we’ll try to do just that: we’re going to use
    data from Stack Exchange to build embeddings for entire questions; we can then
    use those embeddings to find similar documents or questions.'
  prefs: []
  type: TYPE_NORMAL
- en: We’ll start out by downloading and parsing our training data from the Internet
    Archive. Then we’ll briefly explore how Pandas can be helpful for analyzing data.
    We let Keras do the heavy lifting when it comes to featurizing our data and building
    a model for the task at hand. We then look into how to feed this model from a
    Pandas `DataFrame` and how we can run it to draw conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found in the following notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 6.1 Acquiring Data from Stack Exchange
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You need to access a large set of questions to kick-start your training.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the Internet Archive to retrieve a dump of questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Stack Exchange data dump is freely available on the [Internet Archive](https://archive.org/details/stackexchange),
    which hosts a number of interesting datasets (as well as striving to provide an
    archive of the entire web). The data is laid out with one ZIP file for each area
    on Stack Exchange (e.g., travel, sci-fi, etc.). Let’s download the file for the
    travel section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'While the input is technically an XML file, the structure is simple enough
    that we can get away with just reading individual lines and splitting out the
    fields. This is a bit brittle, of course. We will limit ourselves to processing
    1 million records from the dataset; this keeps our memory usage from blowing up
    and should be enough data for us to work with. We’ll save the processed data as
    a JSON file so we won’t have to do the processing again the next time around:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Stack Exchange datasets is a great source for question/answer pairs that
    comes with a nice reuse license. As long as you give attribution you can use it
    in pretty much any way you want. Converting the zipped XML into the more easily
    consumable JSON format is a good preprocessing step.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Exploring Data Using Pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you quickly explore a large dataset so you can make sure it contains
    what you expect?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use Python’s Pandas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pandas is a powerful framework for data processing in Python. In some ways
    it is comparable to a spreadsheet; the data is stored in rows and columns and
    we can quickly filter, convert, and aggregate on the records. Let’s start by converting
    our rows of Python dictionaries into a `DataFrame`. Pandas tries to “guess” the
    types of some columns. We’ll coerce the columns we care about into the right format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With `df.head` we can now see what’s going on in our database.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use Pandas to take a quick look at popular questions in our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: As you might expect, the most popular questions are general questions about
    frequently used languages.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pandas is a great tool for many types of data analysis, whether you just want
    to have a casual look at the data or you want to do in-depth analysis. It can
    be tempting to try to leverage Pandas for many tasks, but unfortunately the Pandas
    interface is not at all regular and for complex operations the performance can
    be significantly worse than using a real database. Lookups in Pandas are significantly
    more expensive than using a Python dictionary, so be careful!
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Using Keras to Featurize Text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you quickly create feature vectors from text?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Use the `Tokenizer` class from Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we can feed text into a model, we need to convert it into feature vectors.
    A common way to do this is to assign an integer to each of the top *N* words in
    a text and then replace each word by its integer. Keras makes this really straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let’s tokenize the titles and bodies of our whole dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Converting text to a series of numbers by using a tokenizer is one of the classic
    ways of making text consumable by a neural network. In the previous chapter we
    converted text on a per-character basis. Character-based models take as input
    individual characters (removing the need for a tokenizer). The trade-off is in
    how long it takes to train the model: because you’re forcing the model to learn
    how to tokenize and stem words, you need more training data and more time.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the drawbacks of processing texts on a per-word basis is the fact that
    there is no practical upper limit to the number of different words that can appear
    in the texts, especially if we have to handle typos and errors. In this recipe
    we only pay attention to words that appear in the top 50,000 by count, which is
    one way around this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Building a Question/Answer Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you calculate embeddings for questions?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Train a model to predict whether a question and an answer from the Stack Exchange
    dataset match.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whenever we construct a model, the first question we should ask is: “What is
    our objective?” That is, what is the model going to try to classify?'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally we’d have a list of “similar questions to this one,” which we could
    use to train our model. Unfortunately, it would be very expensive to acquire such
    a dataset! Instead, we’ll rely on a surrogate objective: let’s see if we can train
    our model to, given a question, distinguish between the matching answer and an
    answer from a random question. This will force the model to learn a good representation
    of titles and bodies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start off our model by defining our inputs. In this case we have two inputs,
    the title (question) and body (answer):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Both are of varying length, so we have to pad them. The data for each field
    will be a list of integers, one for each word in the title or the body.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we want to define a shared set of layers that both inputs will be passed
    through. We’re first going to construct an embedding for the inputs, then mask
    out the invalid values, and add all of the words’ values together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Here, we’ve specified a `vocab_size` (how many words are in our vocabulary)
    and an `embedding_size` (how wide our embedding of each word should be; the GoogleNews
    vectors are 300 dimensions, for example).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s apply these layers to our word inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a single vector for our title and body, we can compare them
    to each other with a cosine distance, just like we did in [Recipe 4.2](ch04.html#training-movie-embeddings).
    In Keras, that is expressed via the `dot` layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can define our model. It takes the title and the body in and outputs
    the similarity between the two:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The model we’ve built here learns to match questions and answers but really
    the only freedom we give it is to change the embeddings of the words such that
    the sums of the embeddings of the title and the body match. This should get us
    embeddings for questions such that questions that are similar will have similar
    embeddings, because similar questions will have similar answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our training model is compiled with two parameters telling Keras how to improve
    the model:'
  prefs: []
  type: TYPE_NORMAL
- en: The loss function
  prefs: []
  type: TYPE_NORMAL
- en: This tells the system how “wrong” a given answer is. For example, if we told
    the network that `title_a` and `body_a` should output 1.0, but the network predicts
    0.8, how bad of an error is that? This becomes a more complex problem when we
    have multiple outputs, but we’ll cover that later. For this model, we’re going
    to use *mean squared error*. For the previous example, this means we would penalize
    the model by (1.0–0.8) ** 2, or 0.04\. This loss will be propagated back through
    the model and improve the embeddings each time the model sees an example.
  prefs: []
  type: TYPE_NORMAL
- en: The optimizer
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many ways that loss can be used to improve our model. These are called
    *optimization strategies*, or *optimizers*. Fortunately, Keras comes with a number
    of reliable optimizers built in, so we won’t have to worry much about this: we
    can just pick a suitable one. In this case, we’re using the `rmsprop` optimizer,
    which tends to perform very well across a wide range of problems.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Training a Model with Pandas
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: How do you train a model on data contained in Pandas?
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Build a data generator that leverages the filter and sample features of Pandas.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the previous recipe, we are going to train our model to distinguish between
    a question title and the correct answer (body) versus the answer to another random
    question. We can write that out as a generator that iterates over our dataset.
    It will output a 1 for the correct question title and body and a 0 for a random
    title and body:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The only complication here is the batching of the data. This is not strictly
    necessary, but extremely important for performance. All deep learning models are
    optimized to work on chunks of data at a time. The best batch size to use depends
    on the problem you’re working on. Using larger batches means your model sees more
    data for each update and therefore can more accurately update its weights, but
    on the flip side it can’t update as often. Bigger batch sizes also take more memory.
    It’s best to start small and keep doubling the batch size until the results no
    longer improve.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We’ll train it for 10,000 steps, divided into 10 epochs of 1,000 steps each.
    Each step will process 128 documents, so our network will end up seeing 1.28M
    training examples. If you have a GPU, you’ll be surprised how quickly this runs!
  prefs: []
  type: TYPE_NORMAL
- en: 6.6 Checking Similarities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You’d like to use Keras to predict values by using the weights of another network.
  prefs: []
  type: TYPE_NORMAL
- en: Solution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Construct a second model that uses different input and output layers from the
    original network, but shares some of the other layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our `sim_model` has been trained and as part of that learned how to go from
    a title to a `title_sum`, which is really what we are after. The model that just
    does that is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now use the “embedding” model to compute a representation for each question
    in our dataset. Let’s wrap this up in a class for easy reuse:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we can use it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Similarity | Question |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0.892392 | working with django and sqlalchemy but backend… |'
  prefs: []
  type: TYPE_TB
- en: '| 0.893417 | Python ORM that auto-generates/updates tables … |'
  prefs: []
  type: TYPE_TB
- en: '| 0.893883 | Dynamic Table Creation and ORM mapping in SqlA… |'
  prefs: []
  type: TYPE_TB
- en: '| 0.896096 | SQLAlchemy with count, group_by and order_by u… |'
  prefs: []
  type: TYPE_TB
- en: '| 0.897706 | SQLAlchemy: Scan huge tables using ORM? |'
  prefs: []
  type: TYPE_TB
- en: '| 0.902693 | Efficiently updating database using SQLAlchemy… |'
  prefs: []
  type: TYPE_TB
- en: '| 0.911446 | What are some good Python ORM solutions? |'
  prefs: []
  type: TYPE_TB
- en: '| 0.922449 | python orm |'
  prefs: []
  type: TYPE_TB
- en: '| 0.924316 | Python libraries to construct classes from a r… |'
  prefs: []
  type: TYPE_TB
- en: '| 0.930865 | python ORM allowing for table creation and bul… |'
  prefs: []
  type: TYPE_TB
- en: In a very short training time, our network managed to figure out that “SQL,”
    “query,” and “INSERT” are all related to Postgres!
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe we saw how we can use part of a network to predict the values
    we’re after, even if the overall network was trained to predict something else.
    The functional API of Keras provides a nice separation between the layers, how
    they are connected, and which combination of input and output layers forms a model.
  prefs: []
  type: TYPE_NORMAL
- en: As we’ll see later in this book, this gives us a lot of flexibility. We can
    take a pre-trained network and use one of the middle layers as an output layer,
    or we can take one of those middle layers and add some new layers (see [Chapter 9](ch09.html#transfer_learning)).
    We can even run the network backwards (see [Chapter 12](ch12.html#image_style)).
  prefs: []
  type: TYPE_NORMAL
