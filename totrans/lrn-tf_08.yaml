- en: Chapter 8\. Queues, Threads, and Reading Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章。队列、线程和读取数据
- en: In this chapter we introduce the use of queues and threads in TensorFlow, with
    the main motivation of streamlining the process of reading input data. We show
    how to write and read TFRecords, the efficient TensorFlow file format. We then
    demonstrate queues, threads, and related functionalities, and connect all the
    dots in a full working example of a multithreaded input pipeline for image data
    that includes pre-processing, batching, and training.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了在TensorFlow中使用队列和线程的方法，主要目的是简化读取输入数据的过程。我们展示了如何编写和读取TFRecords，这是高效的TensorFlow文件格式。然后我们演示了队列、线程和相关功能，并在一个完整的工作示例中连接所有要点，展示了一个包括预处理、批处理和训练的图像数据的多线程输入管道。
- en: The Input Pipeline
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 输入管道
- en: When dealing with small datasets that can be stored in memory, such as MNIST
    images, it is reasonable to simply load all data into memory, then use feeding
    to push data into a TensorFlow graph. For larger datasets, however, this can become
    unwieldy. A natural paradigm for handling such cases is to keep the data on disk
    and load chunks of it as needed (such as mini-batches for training), such that
    the only limit is the size of your hard drive.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理可以存储在内存中的小数据集时，比如MNIST图像，将所有数据加载到内存中，然后使用feeding将数据推送到TensorFlow图中是合理的。然而，对于更大的数据集，这可能变得难以管理。处理这种情况的一个自然范式是将数据保留在磁盘上，并根据需要加载其中的块（比如用于训练的小批量），这样唯一的限制就是硬盘的大小。
- en: In addition, in many cases in practice, a typical data pipeline often includes
    steps such as reading input files with different formats, changing the shape or
    structure of input, normalizing or doing other forms of pre-processing, and shuffling
    the input, all before training has even started.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在实践中，一个典型的数据管道通常包括诸如读取具有不同格式的输入文件、更改输入的形状或结构、归一化或进行其他形式的预处理、对输入进行洗牌等步骤，甚至在训练开始之前。
- en: Much of this process can trivially be decoupled and broken into modular components. Pre-processing,
    for example, does not involve training, and thus naively inputs can be preprocessed
    all at once and then fed to training. Since our training works on batches of examples
    in any case, we could in principle handle batches of inputs on the fly, reading
    them from disk, applying pre-processing, and then feeding them into the computational
    graph for training.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程的很多部分可以轻松地解耦并分解为模块化组件。例如，预处理不涉及训练，因此可以一次性对输入进行预处理，然后将其馈送到训练中。由于我们的训练无论如何都是批量处理示例，原则上我们可以在运行时处理输入批次，从磁盘中读取它们，应用预处理，然后将它们馈送到计算图中进行训练。
- en: This approach, however, can be wasteful. Because pre-processing is independent
    of training, waiting for each batch to be pre-processed would lead to severe I/O
    latency, forcing each training step to (impatiently) wait for mini-batches of
    data to be loaded and processed. A more scalable practice would be to prefetch
    the data and use independent threads for loading and processing and for training.
    But this practice, in turn, could become messy when working with many files kept
    on disk that need to be repeatedly read and shuffled, and require a fair amount
    of bookkeeping and technicalities to run seamlessly.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法可能是浪费的。因为预处理与训练无关，等待每个批次进行预处理会导致严重的I/O延迟，迫使每个训练步骤（急切地）等待加载和处理数据的小批量。更具可扩展性的做法是预取数据，并使用独立的线程进行加载和处理以及训练。但是，当需要重复读取和洗牌许多保存在磁盘上的文件时，这种做法可能变得混乱，并且需要大量的簿记和技术性来无缝运行。
- en: It’s important to note that even without taking pre-processing into consideration,
    using the standard feeding mechanism (with a `feed_dict`) we saw in previous chapters
    is wasteful in itself. `feed_dict` does a single-threaded copy of data from the
    Python runtime to the TensorFlow runtime, causing further latency and slowdowns. We
    would like to avoid this by somehow reading data directly into native TensorFlow.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，即使不考虑预处理，使用在前几章中看到的标准馈送机制（使用`feed_dict`）本身也是浪费的。`feed_dict`会将数据从Python运行时单线程复制到TensorFlow运行时，导致进一步的延迟和减速。我们希望通过某种方式直接将数据读取到本机TensorFlow中，避免这种情况。
- en: To make our lives easier (and faster), TensorFlow comes with a set of tools
    to streamline this input-pipeline process. The main building blocks are a standard
    TensorFlow file format, utilities for encoding and decoding this format, queues
    of data, and multithreading.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的生活更轻松（和更快），TensorFlow提供了一套工具来简化这个输入管道过程。主要的构建模块是标准的TensorFlow文件格式，用于编码和解码这种格式的实用工具，数据队列和多线程。
- en: We will go over these key components one by one, exploring how they work and
    building toward an end-to-end multithreaded input pipeline. We begin by introducing
    TFRecords, the recommended file format for TensorFlow, which will come in useful
    later on.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐一讨论这些关键组件，探索它们的工作原理，并构建一个端到端的多线程输入管道。我们首先介绍TFRecords，这是TensorFlow推荐的文件格式，以后会派上用场。
- en: TFRecords
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TFRecords
- en: Datasets, of course, can come in many formats, sometimes even mixed (such as
    images and audio files). It can often be convenient—and useful—to convert input
    files into one unifying format, regardless of their original formats. TensorFlow’s
    default, standard data format is the TFRecord. A TFRecord file is simply a binary
    file, containing serialized input data. Serialization is based on protocol buffers
    (*protobufs*), which in plain words convert data for storage by using a schema
    describing the data structure, independently of what platform or language is being
    used (much like XML).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集当然可以采用许多格式，有时甚至是混合的（比如图像和音频文件）。将输入文件转换为一个统一的格式，无论其原始格式如何，通常是方便和有用的。TensorFlow的默认标准数据格式是TFRecord。TFRecord文件只是一个包含序列化输入数据的二进制文件。序列化基于协议缓冲区（protobufs），简单地说，它通过使用描述数据结构的模式将数据转换为存储，独立于所使用的平台或语言（就像XML一样）。
- en: In our setting, using TFRecords (and protobufs/binary files in general) has
    many advantages over just working with raw data files. This unified format allows
    for a tidy way to organize input data, with all relevant attributes for an input
    instance kept together, avoiding the need for many directories and subdirectories. TFRecord
    files enable very fast processing. All data is kept in one block of memory, as
    opposed to storing each input file separately, cutting the time needed to read
    data from memory. It’s also important to note that TensorFlow comes with many
    implementations and utilities optimized for TFRecords, making it well suited for
    use as part of a multithreaded input pipeline.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Writing with TFRecordWriter
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We begin by writing our input files to TFRecord format, to allow us to work
    with them (in other cases, we may already have the data stored in this format).
    In this example we will convert MNIST images to this format, but the same ideas
    carry on to other types of data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we download the MNIST data to `save_dir`, using a utility function from
    `tensorflow.contrib.learn`:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Our downloaded data includes train, test, and validation images, each in a
    separate *split*. We go over each split, putting examples in a suitable format
    and using `TFRecordWriter()` to write to disk:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s break this code down to understand the different components.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'We first instantiate a `TFRecordWriter` object, giving it the path corresponding
    to the data split:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We then go over each image, converting it from a NumPy array to a byte string:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we convert images to their protobuf format. `tf.train.Example` is a structure
    for storing our data.  An `Example` object contains a `Features` object, which
    in turn contains a map from attribute name to a `Feature`. A `Feature` can contain
    an `Int64List`, a `BytesList`, or a `FloatList` (not used here). For example,
    here we encode the label of the image:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And here is the encoding for the actual raw image:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s take a look at what our saved data looks like. We do this with `tf.python_io.tf_record_iterator`,
    an iterator that reads records from a TFRecords file:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`serialized_img` is a byte string. To recover the structure we used when saving
    the image to a TFRecord, we parse this byte string, allowing us to access all
    the attributes we stored earlier:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Our image was saved as a byte string too, so we convert it back to a NumPy
    array and reshape it back to a tensor with shape (28,28,1):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This basic example should have given you a feel for TFRecords and how to write
    and read them. In practice, we will typically want to read TFRecords into a queue
    of prefetched data as part of a multithreaded process. In the next section, we
    first introduce TensorFlow queues before showing how to use them with TFRecords.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Queues
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A TensorFlow queue is similar to an ordinary queue, allowing us to enqueue new
    items, dequeue existing items, etc. The important difference from ordinary queues
    is that, just like anything else in TensorFlow, the queue is part of a computational
    graph. Its operations are symbolic as usual, and other nodes in the graph can
    alter its state (much like with Variables). This can be slightly confusing at
    first, so let’s walk through some examples to get acquainted with basic queue
    functionalities.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Enqueuing and Dequeuing
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here we create a *first-in*, *first-out* (FIFO) queue of strings, with a maximal
    number of 10 elements that can be stored in the queue. Since queues are part of
    a computational graph, they are run within a session. In this example, we use
    a `tf.InteractiveSession()`:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Behind the scenes, TensorFlow creates a memory buffer for storing the 10 items.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'Just like any other operation in TensorFlow, to add items to the queue, we
    create an op:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Since you are by now familiar with the concept of a computational graph in
    TensorFlow, it should be no surprise that defining the `enque_op` does not add
    anything to the queue—we need to run the op. So, if we look at the size of `queue1`
    before running the op, we get this:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您现在已经熟悉了TensorFlow中计算图的概念，因此定义`enque_op`并不会向队列中添加任何内容——我们需要运行该操作。因此，如果我们在运行操作之前查看`queue1`的大小，我们会得到这个结果：
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After running the op, our queue now has one item populating it:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 运行操作后，我们的队列现在有一个项目在其中：
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Let’s add some more items to `queue1`, and look at its size again:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们向`queue1`添加更多项目，并再次查看其大小：
- en: '[PRE13]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we dequeue items. Dequeuing too is an op, whose output evaluates to a
    tensor corresponding to the dequeued item:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们出队项目。出队也是一个操作，其输出评估为对应于出队项目的张量：
- en: '[PRE14]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that if we were to run `x.eval()` one more time, on an empty queue, our
    main thread would hang forever. As we will see later in this chapter, in practice
    we use code that knows when to stop dequeuing and avoid hanging.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果我们再次对空队列运行`x.eval()`，我们的主线程将永远挂起。正如我们将在本章后面看到的，实际上我们使用的代码知道何时停止出队并避免挂起。
- en: 'Another way to dequeue is by retrieving multiple items at once, with the `dequeue_many()`
    operation. This op requires that we specify the shape of items in advance:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种出队的方法是一次检索多个项目，使用`dequeue_many()`操作。此操作要求我们提前指定项目的形状：
- en: '[PRE15]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here we fill the queue exactly as before, and then dequeue four items at once:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们像以前一样填充队列，然后一次出队四个项目：
- en: '[PRE16]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Multithreading
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多线程
- en: A TensorFlow session is multithreaded—multiple threads can use the same session
    and run ops in parallel. Individual ops have parallel implementations that are
    used by default with multiple CPU cores or GPU threads. However, if a single call
    to `sess.run()` does not make full use of the available resources, one can increase
    throughput by making multiple parallel calls. For example, in a typical scenario,
    we may have multiple threads apply pre-processing to images and push them into
    a queue, while another thread pulls pre-processed images from the queue for training
    (in the next chapter, we will discuss distributed training, which is conceptually
    related, with important differences).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow会话是多线程的——多个线程可以使用同一个会话并行运行操作。单个操作具有并行实现，默认情况下使用多个CPU核心或GPU线程。然而，如果单个对`sess.run()`的调用没有充分利用可用资源，可以通过进行多个并行调用来提高吞吐量。例如，在典型情况下，我们可能有多个线程对图像进行预处理并将其推送到队列中，而另一个线程则从队列中拉取预处理后的图像进行训练（在下一章中，我们将讨论分布式训练，这在概念上是相关的，但有重要的区别）。
- en: Let’s walk our way through a few simple examples introducing threading in TensorFlow
    and the natural interplay with queues, before connecting all the dots later on
    in a full example with MNIST images.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一些简单的示例来介绍在TensorFlow中引入线程以及与队列的自然互动，然后在MNIST图像的完整示例中将所有内容连接起来。
- en: 'We start by creating a FIFO queue with capacity of 100 items, where each item
    is a random float generated with `tf.random_normal()`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个容量为100个项目的FIFO队列，其中每个项目是使用`tf.random_normal()`生成的随机浮点数：
- en: '[PRE18]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note, again, that the `enque` op does not actually add the random numbers to
    the queue (and they are not yet generated) prior to graph execution. Items will
    be enqueued using the function `add()` we create that adds 10 items to the queue
    by calling `sess.run()` multiple times.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 再次注意，`enque`操作实际上并没有将随机数添加到队列中（它们尚未生成）在图执行之前。项目将使用我们创建的`add()`函数进行入队，该函数通过多次调用`sess.run()`向队列中添加10个项目。
- en: 'Next, we create 10 threads, each running `add()` in parallel, thus each pushing
    10 items to the queue, asynchronously. We could think (for now) of these random
    numbers as training data being added into a queue:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建10个线程，每个线程并行运行`add()`，因此每个线程异步地向队列中添加10个项目。我们可以（暂时）将这些随机数视为添加到队列中的训练数据：
- en: '[PRE19]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We have created a list of threads, and now we execute them, printing the size
    of the queue at short intervals as it grows from 0 to 100:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了一个线程列表，现在我们执行它们，以短间隔打印队列的大小，从0增长到100：
- en: '[PRE20]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we dequeue 10 items at once with `dequeue_many()`, and examine the
    results:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`dequeue_many()`一次出队10个项目，并检查结果：
- en: '[PRE22]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Coordinator and QueueRunner
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协调器和QueueRunner
- en: In realistic scenarios (as we shall see later in this chapter), it can be more
    complicated to run multiple threads effectively. Threads should be able to stop
    properly (to avoid “zombie” threads, for example, or to close all threads together
    when one fails), queues need to be closed after stopping, and there are other
    technical but important issues that need to be addressed.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实场景中（正如我们将在本章后面看到的），有效地运行多个线程可能会更加复杂。线程应该能够正确停止（例如，避免“僵尸”线程，或者在一个线程失败时一起关闭所有线程），在停止后需要关闭队列，并且还有其他需要解决的技术但重要的问题。
- en: TensorFlow comes equipped with tools to help us in this process. Key among them
    are `tf.train.Coordinator`, for coordinating the termination of a set of threads,
    and `tf.train.QueueRunner`, which streamlines the process of getting multiple
    threads to enqueue data with seamless cooperation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow配备了一些工具来帮助我们进行这个过程。其中最重要的是`tf.train.Coordinator`，用于协调一组线程的终止，以及`tf.train.QueueRunner`，它简化了让多个线程与无缝协作地将数据入队的过程。
- en: tf.train.Coordinator
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: tf.train.Coordinator
- en: We first demonstrate how to use `tf.train.Coordinator` with a simple, toy example.
    In the next section, we’ll see how to use it as part of a real input pipeline.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先演示如何在一个简单的玩具示例中使用`tf.train.Coordinator`。在下一节中，我们将看到如何将其作为真实输入管道的一部分使用。
- en: 'We use the code similar to that in the previous section, altering the `add()`
    function and adding a coordinator:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与上一节类似的代码，修改`add()`函数并添加一个协调器：
- en: '[PRE24]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Any thread can call `coord.request_stop()` to get all other threads to stop. Threads
    typically run loops that check whether to stop, using `coord.should_stop()`. Here,
    we pass the thread index `i` to `add()`, and use a condition that is never satisfied
    (`i==11`) to request a stop. Thus, our threads complete their job, adding the
    full 100 items to the queue. However, if we were to alter `add()` as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 任何线程都可以调用`coord.request_stop()`来让所有其他线程停止。线程通常运行循环来检查是否停止，使用`coord.should_stop()`。在这里，我们将线程索引`i`传递给`add()`，并使用一个永远不满足的条件（`i==11`）来请求停止。因此，我们的线程完成了它们的工作，将全部100个项目添加到队列中。但是，如果我们将`add()`修改如下：
- en: '[PRE25]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'then thread `i=1` would use the coordinator to request all threads to stop,
    stopping all enqueueing early:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后线程`i=1`将使用协调器请求所有线程停止，提前停止所有入队操作：
- en: '[PRE26]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: tf.train.QueueRunner and tf.RandomShuffleQueue
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: tf.train.QueueRunner和tf.RandomShuffleQueue
- en: While we can create a number of threads that repeatedly run an enqueue op, it
    is better practice to use the built-in `tf.train.QueueRunner`, which does exactly
    that, while closing the queue upon an exception.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以创建多个重复运行入队操作的线程，但最好使用内置的`tf.train.QueueRunner`，它正是这样做的，同时在异常发生时关闭队列。
- en: 'Here we create a queue runner that will run four threads in parallel to enqueue
    items:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建一个队列运行器，将并行运行四个线程以入队项目：
- en: '[PRE28]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that `qr.create_threads()` takes our session as an argument, along with
    our coordinator.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`qr.create_threads()`将我们的会话作为参数，以及我们的协调器。
- en: In this example, we used a `tf.RandomShuffleQueue` rather than the FIFO queue.
    A `RandomShuffleQueue` is simply a queue with a dequeue op that pops items in
    random order. This is useful when training deep neural networks with stochastic
    gradient-descent optimization, which requires shuffling the data. The `min_after_dequeue`
    argument specifies the minimum number of items that will remain in the queue after
    calling a dequeue op—a bigger number entails better mixing (random sampling),
    but more memory.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用了`tf.RandomShuffleQueue`而不是FIFO队列。`RandomShuffleQueue`只是一个带有以随机顺序弹出项目的出队操作的队列。这在训练使用随机梯度下降优化的深度神经网络时非常有用，这需要对数据进行洗牌。`min_after_dequeue`参数指定在调用出队操作后队列中将保留的最小项目数，更大的数字意味着更好的混合（随机抽样），但需要更多的内存。
- en: A Full Multithreaded Input Pipeline
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一个完整的多线程输入管道
- en: We now put all the pieces together in a working example with MNIST images, from
    writing data to TensorFlow’s efficient file format, through data loading and pre-processing,
    to training a model. We do so by building on the queuing and multithreading functionality
    demonstrated earlier, and along the way introduce some more useful components
    for reading and processing data in TensorFlow.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将所有部分组合在一起，使用MNIST图像的工作示例，从将数据写入TensorFlow的高效文件格式，通过数据加载和预处理，到训练模型。我们通过在之前演示的排队和多线程功能的基础上构建，并在此过程中介绍一些更有用的组件来读取和处理TensorFlow中的数据。
- en: 'First, we write the MNIST data to TFRecords, with the same code we used at
    the start of this chapter:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将MNIST数据写入TFRecords，使用与本章开头使用的相同代码：
- en: '[PRE29]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: tf.train.string_input_producer() and tf.TFRecordReader()
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tf.train.string_input_producer()和tf.TFRecordReader()
- en: '`tf.train.string_input_producer()` simply creates a `QueueRunner` behind the
    scenes, outputting filename strings to a queue for our input pipeline. This filename
    queue will be shared among multiple threads:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.train.string_input_producer()`只是在幕后创建一个`QueueRunner`，将文件名字符串输出到我们的输入管道的队列中。这个文件名队列将在多个线程之间共享：'
- en: '[PRE30]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `num_epochs` argument tells `string_input_producer()` to produce each filename
    string `num_epochs` times.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`num_epochs`参数告诉`string_input_producer()`将每个文件名字符串生成`num_epochs`次。'
- en: 'Next, we read files from this queue using `TFRecordReader()`, which takes a
    queue of filenames and dequeues filename by filename off the `filename_queue`.
    Internally, `TFRecordReader()` uses the state of the graph to keep track of the
    location of the TFRecord being read, as it loads “chunk after chunk” of input
    data from the disk:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`TFRecordReader()`从这个队列中读取文件，该函数接受一个文件名队列并从`filename_queue`中逐个出队文件名。在内部，`TFRecordReader()`使用图的状态来跟踪正在读取的TFRecord的位置，因为它从磁盘加载输入数据的“块之后的块”：
- en: '[PRE31]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: tf.train.shuffle_batch()
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tf.train.shuffle_batch()
- en: 'We decode the raw byte string data, do (very) basic pre-processing to convert
    pixel values to floats, and then shuffle the image instances and collect them
    into `batch_size` batches with `tf.train.shuffle_batch()`, which internally uses
    a `RandomShuffleQueue` and accumulates examples until it contains `batch_size`
    + `min_after_dequeue` elements:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解码原始字节字符串数据，进行（非常）基本的预处理将像素值转换为浮点数，然后使用`tf.train.shuffle_batch()`将图像实例洗牌并收集到`batch_size`批次中，该函数内部使用`RandomShuffleQueue`并累积示例，直到包含`batch_size`
    + `min_after_dequeue`个元素：
- en: '[PRE32]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The `capacity` and `min_after_dequeue` parameters are used in the same manner
    as discussed previously. The mini-batches that are returned by `shuffle_batch()`
    are the result of a `dequeue_many()` call on the `RandomShuffleQueue` that is
    created internally.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`capacity`和`min_after_dequeue`参数的使用方式与之前讨论的相同。由`shuffle_batch()`返回的小批次是在内部创建的`RandomShuffleQueue`上调用`dequeue_many()`的结果。'
- en: tf.train.start_queue_runners() and Wrapping Up
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tf.train.start_queue_runners()和总结
- en: 'We define our simple softmax classification model as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简单的softmax分类模型定义如下：
- en: '[PRE33]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, we create threads that enqueue data to queues by calling `tf.train.start_queue_runners()`.
    Unlike other calls, this one is not symbolic and actually creates the threads
    (and thus needs to be done after initialization):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过调用`tf.train.start_queue_runners()`创建线程将数据入队到队列中。与其他调用不同，这个调用不是符号化的，实际上创建了线程（因此需要在初始化之后完成）：
- en: '[PRE34]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let’s take a look at the list of created threads:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下创建的线程列表：
- en: '[PRE35]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Having everything in place, we are now ready to run the multithreaded process,
    from reading and pre-processing batches into a queue to training a model. It’s
    important to note that we do not use the familiar `feed_dict` argument anymore—this
    avoids data copies and offers speedups, as discussed earlier in this chapter:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一切就绪后，我们现在准备运行多线程过程，从读取和预处理批次到将其放入队列再到训练模型。重要的是要注意，我们不再使用熟悉的`feed_dict`参数——这样可以避免数据复制并提供加速，正如本章前面讨论的那样：
- en: '[PRE36]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We train until a `tf.errors.OutOfRangeError` error is thrown, indicating that
    queues are empty and we are done:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 直到抛出`tf.errors.OutOfRangeError`错误，表示队列为空，我们已经完成训练：
- en: '[PRE37]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Future input pipeline
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未来的输入管道
- en: In mid-2017, the TensorFlow development team announced the Dataset API, a new
    preliminary input pipeline abstraction offering some simplifications and speedups.
    The concepts presented in this chapter, such as TFRecords and queues, are fundamental
    and remain at the core of TensorFlow and its input pipeline process. TensorFlow
    is still very much a work in progress, and exciting and important changes naturally
    occur from time to time. See [the issue tracker](https://github.com/tensorflow/tensorflow/issues/7951)
    for an ongoing discussion.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年中期，TensorFlow开发团队宣布了Dataset API，这是一个新的初步输入管道抽象，提供了一些简化和加速。本章介绍的概念，如TFRecords和队列，是TensorFlow及其输入管道过程的基础，仍然处于核心地位。TensorFlow仍然在不断发展中，自然会不时发生令人兴奋和重要的变化。请参阅[问题跟踪器](https://github.com/tensorflow/tensorflow/issues/7951)进行持续讨论。
- en: Summary
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we saw how to use queues and threads in TensorFlow, and how
    to create a multithreaded input pipeline. This process can help increase throughput
    and utilization of resources. In the next chapter, we take this a step forward
    and show how to work in a distributed setting with TensorFlow, across multiple
    devices and machines.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了如何在TensorFlow中使用队列和线程，以及如何创建一个多线程输入管道。这个过程可以帮助增加吞吐量和资源利用率。在下一章中，我们将进一步展示如何在分布式环境中使用TensorFlow，在多个设备和机器之间进行工作。
