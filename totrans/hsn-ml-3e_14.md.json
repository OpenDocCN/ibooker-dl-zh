["```py\n>>> import tensorflow as tf\n>>> t = tf.constant([[1., 2., 3.], [4., 5., 6.]])  # matrix\n>>> t\n<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[1., 2., 3.],\n [4., 5., 6.]], dtype=float32)>\n```", "```py\n>>> t.shape\nTensorShape([2, 3])\n>>> t.dtype\ntf.float32\n```", "```py\n>>> t[:, 1:]\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[2., 3.],\n [5., 6.]], dtype=float32)>\n>>> t[..., 1, tf.newaxis]\n<tf.Tensor: shape=(2, 1), dtype=float32, numpy=\narray([[2.],\n [5.]], dtype=float32)>\n```", "```py\n>>> t + 10\n<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[11., 12., 13.],\n [14., 15., 16.]], dtype=float32)>\n>>> tf.square(t)\n<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\narray([[ 1.,  4.,  9.],\n [16., 25., 36.]], dtype=float32)>\n>>> t @ tf.transpose(t)\n<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[14., 32.],\n [32., 77.]], dtype=float32)>\n```", "```py\n>>> tf.constant(42)\n<tf.Tensor: shape=(), dtype=int32, numpy=42>\n```", "```py\n>>> import numpy as np\n>>> a = np.array([2., 4., 5.])\n>>> tf.constant(a)\n<tf.Tensor: id=111, shape=(3,), dtype=float64, numpy=array([2., 4., 5.])>\n>>> t.numpy()  # or np.array(t)\narray([[1., 2., 3.],\n [4., 5., 6.]], dtype=float32)\n>>> tf.square(a)\n<tf.Tensor: id=116, shape=(3,), dtype=float64, numpy=array([4., 16., 25.])>\n>>> np.square(t)\narray([[ 1.,  4.,  9.],\n [16., 25., 36.]], dtype=float32)\n```", "```py\n>>> tf.constant(2.) + tf.constant(40)\n[...] InvalidArgumentError: [...] expected to be a float tensor [...]\n>>> tf.constant(2.) + tf.constant(40., dtype=tf.float64)\n[...] InvalidArgumentError: [...] expected to be a float tensor [...]\n```", "```py\n>>> t2 = tf.constant(40., dtype=tf.float64)\n>>> tf.constant(2.0) + tf.cast(t2, tf.float32)\n<tf.Tensor: id=136, shape=(), dtype=float32, numpy=42.0>\n```", "```py\n>>> v = tf.Variable([[1., 2., 3.], [4., 5., 6.]])\n>>> v\n<tf.Variable 'Variable:0' shape=(2, 3) dtype=float32, numpy=\narray([[1., 2., 3.],\n [4., 5., 6.]], dtype=float32)>\n```", "```py\nv.assign(2 * v)           # v now equals [[2., 4., 6.], [8., 10., 12.]]\nv[0, 1].assign(42)        # v now equals [[2., 42., 6.], [8., 10., 12.]]\nv[:, 2].assign([0., 1.])  # v now equals [[2., 42., 0.], [8., 10., 1.]]\nv.scatter_nd_update(      # v now equals [[100., 42., 0.], [8., 10., 200.]]\n    indices=[[0, 0], [1, 2]], updates=[100., 200.])\n```", "```py\n>>> v[1] = [7., 8., 9.]\n[...] TypeError: 'ResourceVariable' object does not support item assignment\n```", "```py\ndef huber_fn(y_true, y_pred):\n    error = y_true - y_pred\n    is_small_error = tf.abs(error) < 1\n    squared_loss = tf.square(error) / 2\n    linear_loss  = tf.abs(error) - 0.5\n    return tf.where(is_small_error, squared_loss, linear_loss)\n```", "```py\nmodel.compile(loss=huber_fn, optimizer=\"nadam\")\nmodel.fit(X_train, y_train, [...])\n```", "```py\nmodel = tf.keras.models.load_model(\"my_model_with_a_custom_loss\",\n                                   custom_objects={\"huber_fn\": huber_fn})\n```", "```py\ndef create_huber(threshold=1.0):\n    def huber_fn(y_true, y_pred):\n        error = y_true - y_pred\n        is_small_error = tf.abs(error) < threshold\n        squared_loss = tf.square(error) / 2\n        linear_loss  = threshold * tf.abs(error) - threshold ** 2 / 2\n        return tf.where(is_small_error, squared_loss, linear_loss)\n    return huber_fn\n\nmodel.compile(loss=create_huber(2.0), optimizer=\"nadam\")\n```", "```py\nmodel = tf.keras.models.load_model(\n    \"my_model_with_a_custom_loss_threshold_2\",\n    custom_objects={\"huber_fn\": create_huber(2.0)}\n)\n```", "```py\nclass HuberLoss(tf.keras.losses.Loss):\n    def __init__(self, threshold=1.0, **kwargs):\n        self.threshold = threshold\n        super().__init__(**kwargs)\n\n    def call(self, y_true, y_pred):\n        error = y_true - y_pred\n        is_small_error = tf.abs(error) < self.threshold\n        squared_loss = tf.square(error) / 2\n        linear_loss  = self.threshold * tf.abs(error) - self.threshold**2 / 2\n        return tf.where(is_small_error, squared_loss, linear_loss)\n\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"threshold\": self.threshold}\n```", "```py\nmodel.compile(loss=HuberLoss(2.), optimizer=\"nadam\")\n```", "```py\nmodel = tf.keras.models.load_model(\"my_model_with_a_custom_loss_class\",\n                                   custom_objects={\"HuberLoss\": HuberLoss})\n```", "```py\ndef my_softplus(z):\n    return tf.math.log(1.0 + tf.exp(z))\n\ndef my_glorot_initializer(shape, dtype=tf.float32):\n    stddev = tf.sqrt(2. / (shape[0] + shape[1]))\n    return tf.random.normal(shape, stddev=stddev, dtype=dtype)\n\ndef my_l1_regularizer(weights):\n    return tf.reduce_sum(tf.abs(0.01 * weights))\n\ndef my_positive_weights(weights):  # return value is just tf.nn.relu(weights)\n    return tf.where(weights < 0., tf.zeros_like(weights), weights)\n```", "```py\nlayer = tf.keras.layers.Dense(1, activation=my_softplus,\n                              kernel_initializer=my_glorot_initializer,\n                              kernel_regularizer=my_l1_regularizer,\n                              kernel_constraint=my_positive_weights)\n```", "```py\nclass MyL1Regularizer(tf.keras.regularizers.Regularizer):\n    def __init__(self, factor):\n        self.factor = factor\n\n    def __call__(self, weights):\n        return tf.reduce_sum(tf.abs(self.factor * weights))\n\n    def get_config(self):\n        return {\"factor\": self.factor}\n```", "```py\nmodel.compile(loss=\"mse\", optimizer=\"nadam\", metrics=[create_huber(2.0)])\n```", "```py\n>>> precision = tf.keras.metrics.Precision()\n>>> precision([0, 1, 1, 1, 0, 1, 0, 1], [1, 1, 0, 1, 0, 1, 0, 1])\n<tf.Tensor: shape=(), dtype=float32, numpy=0.8>\n>>> precision([0, 1, 0, 0, 1, 0, 1, 1], [1, 0, 1, 1, 0, 0, 0, 0])\n<tf.Tensor: shape=(), dtype=float32, numpy=0.5>\n```", "```py\n>>> precision.result()\n<tf.Tensor: shape=(), dtype=float32, numpy=0.5>\n>>> precision.variables\n[<tf.Variable 'true_positives:0' [...], numpy=array([4.], dtype=float32)>,\n <tf.Variable 'false_positives:0' [...], numpy=array([4.], dtype=float32)>]\n>>> precision.reset_states()  # both variables get reset to 0.0\n```", "```py\nclass HuberMetric(tf.keras.metrics.Metric):\n    def __init__(self, threshold=1.0, **kwargs):\n        super().__init__(**kwargs)  # handles base args (e.g., dtype)\n        self.threshold = threshold\n        self.huber_fn = create_huber(threshold)\n        self.total = self.add_weight(\"total\", initializer=\"zeros\")\n        self.count = self.add_weight(\"count\", initializer=\"zeros\")\n\n    def update_state(self, y_true, y_pred, sample_weight=None):\n        sample_metrics = self.huber_fn(y_true, y_pred)\n        self.total.assign_add(tf.reduce_sum(sample_metrics))\n        self.count.assign_add(tf.cast(tf.size(y_true), tf.float32))\n\n    def result(self):\n        return self.total / self.count\n\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"threshold\": self.threshold}\n```", "```py\nexponential_layer = tf.keras.layers.Lambda(lambda x: tf.exp(x))\n```", "```py\nclass MyDense(tf.keras.layers.Layer):\n    def __init__(self, units, activation=None, **kwargs):\n        super().__init__(**kwargs)\n        self.units = units\n        self.activation = tf.keras.activations.get(activation)\n\n    def build(self, batch_input_shape):\n        self.kernel = self.add_weight(\n            name=\"kernel\", shape=[batch_input_shape[-1], self.units],\n            initializer=\"glorot_normal\")\n        self.bias = self.add_weight(\n            name=\"bias\", shape=[self.units], initializer=\"zeros\")\n\n    def call(self, X):\n        return self.activation(X @ self.kernel + self.bias)\n\n    def get_config(self):\n        base_config = super().get_config()\n        return {**base_config, \"units\": self.units,\n                \"activation\": tf.keras.activations.serialize(self.activation)}\n```", "```py\nclass MyMultiLayer(tf.keras.layers.Layer):\n    def call(self, X):\n        X1, X2 = X\n        return X1 + X2, X1 * X2, X1 / X2\n```", "```py\nclass MyGaussianNoise(tf.keras.layers.Layer):\n    def __init__(self, stddev, **kwargs):\n        super().__init__(**kwargs)\n        self.stddev = stddev\n\n    def call(self, X, training=False):\n        if training:\n            noise = tf.random.normal(tf.shape(X), stddev=self.stddev)\n            return X + noise\n        else:\n            return X\n```", "```py\nclass ResidualBlock(tf.keras.layers.Layer):\n    def __init__(self, n_layers, n_neurons, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\",\n                                             kernel_initializer=\"he_normal\")\n                       for _ in range(n_layers)]\n\n    def call(self, inputs):\n        Z = inputs\n        for layer in self.hidden:\n            Z = layer(Z)\n        return inputs + Z\n```", "```py\nclass ResidualRegressor(tf.keras.Model):\n    def __init__(self, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden1 = tf.keras.layers.Dense(30, activation=\"relu\",\n                                             kernel_initializer=\"he_normal\")\n        self.block1 = ResidualBlock(2, 30)\n        self.block2 = ResidualBlock(2, 30)\n        self.out = tf.keras.layers.Dense(output_dim)\n\n    def call(self, inputs):\n        Z = self.hidden1(inputs)\n        for _ in range(1 + 3):\n            Z = self.block1(Z)\n        Z = self.block2(Z)\n        return self.out(Z)\n```", "```py\nclass ReconstructingRegressor(tf.keras.Model):\n    def __init__(self, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.hidden = [tf.keras.layers.Dense(30, activation=\"relu\",\n                                             kernel_initializer=\"he_normal\")\n                       for _ in range(5)]\n        self.out = tf.keras.layers.Dense(output_dim)\n        self.reconstruction_mean = tf.keras.metrics.Mean(\n            name=\"reconstruction_error\")\n\n    def build(self, batch_input_shape):\n        n_inputs = batch_input_shape[-1]\n        self.reconstruct = tf.keras.layers.Dense(n_inputs)\n\n    def call(self, inputs, training=False):\n        Z = inputs\n        for layer in self.hidden:\n            Z = layer(Z)\n        reconstruction = self.reconstruct(Z)\n        recon_loss = tf.reduce_mean(tf.square(reconstruction - inputs))\n        self.add_loss(0.05 * recon_loss)\n        if training:\n            result = self.reconstruction_mean(recon_loss)\n            self.add_metric(result)\n        return self.out(Z)\n```", "```py\nEpoch 1/5\n363/363 [========] - 1s 820us/step - loss: 0.7640 - reconstruction_error: 1.2728\nEpoch 2/5\n363/363 [========] - 0s 809us/step - loss: 0.4584 - reconstruction_error: 0.6340\n[...]\n```", "```py\ndef f(w1, w2):\n    return 3 * w1 ** 2 + 2 * w1 * w2\n```", "```py\n>>> w1, w2 = 5, 3\n>>> eps = 1e-6\n>>> (f(w1 + eps, w2) - f(w1, w2)) / eps\n36.000003007075065\n>>> (f(w1, w2 + eps) - f(w1, w2)) / eps\n10.000000003174137\n```", "```py\nw1, w2 = tf.Variable(5.), tf.Variable(3.)\nwith tf.GradientTape() as tape:\n    z = f(w1, w2)\n\ngradients = tape.gradient(z, [w1, w2])\n```", "```py\n>>> gradients\n[<tf.Tensor: shape=(), dtype=float32, numpy=36.0>,\n <tf.Tensor: shape=(), dtype=float32, numpy=10.0>]\n```", "```py\nwith tf.GradientTape() as tape:\n    z = f(w1, w2)\n\ndz_dw1 = tape.gradient(z, w1)  # returns tensor 36.0\ndz_dw2 = tape.gradient(z, w2)  # raises a RuntimeError!\n```", "```py\nwith tf.GradientTape(persistent=True) as tape:\n    z = f(w1, w2)\n\ndz_dw1 = tape.gradient(z, w1)  # returns tensor 36.0\ndz_dw2 = tape.gradient(z, w2)  # returns tensor 10.0, works fine now!\ndel tape\n```", "```py\nc1, c2 = tf.constant(5.), tf.constant(3.)\nwith tf.GradientTape() as tape:\n    z = f(c1, c2)\n\ngradients = tape.gradient(z, [c1, c2])  # returns [None, None]\n```", "```py\nwith tf.GradientTape() as tape:\n    tape.watch(c1)\n    tape.watch(c2)\n    z = f(c1, c2)\n\ngradients = tape.gradient(z, [c1, c2])  # returns [tensor 36., tensor 10.]\n```", "```py\ndef f(w1, w2):\n    return 3 * w1 ** 2 + tf.stop_gradient(2 * w1 * w2)\n\nwith tf.GradientTape() as tape:\n    z = f(w1, w2)  # the forward pass is not affected by stop_gradient()\n\ngradients = tape.gradient(z, [w1, w2])  # returns [tensor 30., None]\n```", "```py\n>>> x = tf.Variable(1e-50)\n>>> with tf.GradientTape() as tape:\n...     z = tf.sqrt(x)\n...\n>>> tape.gradient(z, [x])\n[<tf.Tensor: shape=(), dtype=float32, numpy=inf>]\n```", "```py\ndef my_softplus(z):\n    return tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)\n```", "```py\n@tf.custom_gradient\ndef my_softplus(z):\n    def my_softplus_gradients(grads):  # grads = backprop'ed from upper layers\n        return grads * (1 - 1 / (1 + tf.exp(z)))  # stable grads of softplus\n\n    result = tf.math.log(1 + tf.exp(-tf.abs(z))) + tf.maximum(0., z)\n    return result, my_softplus_gradients\n```", "```py\nl2_reg = tf.keras.regularizers.l2(0.05)\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Dense(30, activation=\"relu\", kernel_initializer=\"he_normal\",\n                          kernel_regularizer=l2_reg),\n    tf.keras.layers.Dense(1, kernel_regularizer=l2_reg)\n])\n```", "```py\ndef random_batch(X, y, batch_size=32):\n    idx = np.random.randint(len(X), size=batch_size)\n    return X[idx], y[idx]\n```", "```py\ndef print_status_bar(step, total, loss, metrics=None):\n    metrics = \" - \".join([f\"{m.name}: {m.result():.4f}\"\n                          for m in [loss] + (metrics or [])])\n    end = \"\" if step < total else \"\\n\"\n    print(f\"\\r{step}/{total} - \" + metrics, end=end)\n```", "```py\nn_epochs = 5\nbatch_size = 32\nn_steps = len(X_train) // batch_size\noptimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\nloss_fn = tf.keras.losses.mean_squared_error\nmean_loss = tf.keras.metrics.Mean(name=\"mean_loss\")\nmetrics = [tf.keras.metrics.MeanAbsoluteError()]\n```", "```py\nfor epoch in range(1, n_epochs + 1):\n    print(\"Epoch {}/{}\".format(epoch, n_epochs))\n    for step in range(1, n_steps + 1):\n        X_batch, y_batch = random_batch(X_train_scaled, y_train)\n        with tf.GradientTape() as tape:\n            y_pred = model(X_batch, training=True)\n            main_loss = tf.reduce_mean(loss_fn(y_batch, y_pred))\n            loss = tf.add_n([main_loss] + model.losses)\n\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        mean_loss(loss)\n        for metric in metrics:\n            metric(y_batch, y_pred)\n\n        print_status_bar(step, n_steps, mean_loss, metrics)\n\n    for metric in [mean_loss] + metrics:\n        metric.reset_states()\n```", "```py\nfor variable in model.variables:\n    if variable.constraint is not None:\n        variable.assign(variable.constraint(variable))\n```", "```py\ndef cube(x):\n    return x ** 3\n```", "```py\n>>> cube(2)\n8\n>>> cube(tf.constant(2.0))\n<tf.Tensor: shape=(), dtype=float32, numpy=8.0>\n```", "```py\n>>> tf_cube = tf.function(cube)\n>>> tf_cube\n<tensorflow.python.eager.def_function.Function at 0x7fbfe0c54d50>\n```", "```py\n>>> tf_cube(2)\n<tf.Tensor: shape=(), dtype=int32, numpy=8>\n>>> tf_cube(tf.constant(2.0))\n<tf.Tensor: shape=(), dtype=float32, numpy=8.0>\n```", "```py\n@tf.function\ndef tf_cube(x):\n    return x ** 3\n```", "```py\n>>> tf_cube.python_function(2)\n8\n```"]