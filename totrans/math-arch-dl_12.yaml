- en: 13 Fully Bayes model parameter estimation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs: []
  type: TYPE_NORMAL
- en: Fully Bayes parameter estimation for unsupervised modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Injecting prior belief into parameter estimation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating Gaussian likelihood parameters with known or unknown mean and precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normal-gamma and Wishart distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Suppose we have a data set of interest: say, all images containing a cat. If
    we represent images as points in a high-dimensional feature space, our data set
    of interest forms a subspace of that feature space. Now we want to create an *unsupervised*
    model for our data set of interest. This means we want to identify a probability
    density function *p*(![](../../OEBPS/Images/AR_x.png)) whose sample cloud (the
    set of points obtained by repeatedly sampling the probability distribution many
    times) largely overlaps our subspace of interest. Of course, we do not know the
    exact subspace of interest, but we have collected a set of samples *X* from the
    data set of interest: that is, the training data. We can use the point cloud for
    *X* as a surrogate for the unknown subspace of interest. Thus, we are essentially
    trying to identify a probability density function *p*(![](../../OEBPS/Images/AR_x.png))
    whose sample cloud, by and large, overlaps *X*.'
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the model *p*(![](../../OEBPS/Images/AR_x.png)), we can use it
    to generate more data samples; these will be computer-generated cat images. This
    is generative modeling. Also, given a new image ![](../../OEBPS/Images/AR_a.png),
    we can estimate the probability of it being an image of a cat by evaluating *p*(![](../../OEBPS/Images/AR_a.png)).
  prefs: []
  type: TYPE_NORMAL
- en: '13.1 Fully Bayes estimation: An informal introduction'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let‚Äôs recap Bayes‚Äô theorem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-01.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 13.1
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *X* = {![](../../OEBPS/Images/AR_x.png)[1], ![](../../OEBPS/Images/AR_x.png)[2],‚ãØ}
    denotes the training data set. Our ultimate goal is to identify the likelihood
    function *p*(![](../../OEBPS/Images/AR_x.png)|*Œ∏*). Estimating the likelihood
    function has two aspects: selecting the function family and estimating the parameters.
    We usually preselect the family from our knowledge of the problem and then estimate
    the model parameters. For instance, the family for our model likelihood function
    might be Gaussian: *p*(![](../../OEBPS/Images/AR_x.png)|*Œ∏*) = ùí©(![](../../OEBPS/Images/AR_x.png);
    ![](../../OEBPS/Images/AR_micro.png),¬†**Œ£**) as before, the semicolon separates
    the model variables from model parameters). Then *Œ∏* = {![](../../OEBPS/Images/AR_micro.png),¬†**Œ£**}
    are the model parameters to estimate. We estimate *Œ∏* such that the overall likelihood
    *p*(*X*|*Œ∏*) = ‚àè*[i¬†]p*(![](../../OEBPS/Images/AR_x.png)*[i]*|*Œ∏*) best fits the
    training data *X*.'
  prefs: []
  type: TYPE_NORMAL
- en: We want to re-emphasize the mental picture that *best fit* implies that the
    sample cloud of the likelihood function (repeated samples from *p*(![](../../OEBPS/Images/AR_x.png)|*Œ∏*))
    largely overlaps the training data set *X*. For the Gaussian case, this implies
    that the mean ![](../../OEBPS/Images/AR_micro.png) should fall at a place where
    there is a very high concentration of training data points, and the covariance
    matrix Œ£ should be such that the elliptical base of the likelihood function tightly
    contains as many training data points as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 13.1.1 Parameter estimation and belief injection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are various possible approaches to parameter estimation. The simplest
    approach is *maximum likelihood parameter estimation* MLE), introduced in section
    [6.6.2](../Text/06.xhtml#sec-max_likelihood_estimation). In MLE, we choose the
    parameter values that maximize *p*(*X*|*Œ∏*), the likelihood of observing the training
    data set. This makes some sense. After all, the only thing we know to be true
    is that the input data set *X* *has been observed*‚Äîthis being unsupervised data,
    we do not know anything else. It is reasonable to choose the parameters that maximize
    the probability of that known truth. If the training data set is large, MLE estimation
    works well.
  prefs: []
  type: TYPE_NORMAL
- en: However, in the absence of a sufficiently large amount of training data, it
    often helps to inject our prior knowledge about the system into the estimation‚Äîprior
    knowledge can cover for the lack of data. This injection of guess/belief into
    the system is done via the prior probability density. To do this, we can no longer
    maximize the likelihood, as likelihood ignores the prior. We have to do maximum
    a posteriori (MAP) estimation, which maximizes the posterior probability. The
    posterior probability is the product of likelihood (which depends on the data)
    and the prior (which does not depend on data; we will make it reflect our prior
    belief).
  prefs: []
  type: TYPE_NORMAL
- en: There are two possible MAP paradigms. We saw one of them in section [6.6.3](../Text/06.xhtml#sec-MAP_estimation),
    where we injected our belief that the unknown parameters must be *small* in magnitude
    and set *p*(*Œ∏*) ‚àù *e*^(‚àí||*Œ∏*||¬≤) as a *regularizer*. The system was incentivized
    to select parameter values that are relatively smaller in magnitude. In this chapter,
    we study a different paradigm; let‚Äôs illustrate it with an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we model the likelihood as a Gaussian: *p*(![](../../OEBPS/Images/AR_x.png)|*Œ∏*)
    = ùí©(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_micro.png),¬†**Œ£**).
    We have to estimate the parameters *Œ∏* = {![](../../OEBPS/Images/AR_micro.png),¬†**Œ£**}
    from the training data *X*, for which we must maximize the posterior probability.
    To compute the posterior probability, we need the prior probability. In addition,
    we must somehow inject constant values as our belief (lacking observed data) about
    the parameter values. How about modeling the prior probability as a Gaussian probability
    density function in the parameter space? Ignoring the covariance matrix parameter
    for the sake of simplicity, we can model the probability density of the mean parameter
    as *p*(![](../../OEBPS/Images/AR_micro.png)) = ùí©(![](../../OEBPS/Images/AR_micro.png);
    ![](../../OEBPS/Images/AR_micro.png)[0], **Œ£**[0]). We are essentially saying
    that we believe the parameter ![](../../OEBPS/Images/AR_micro.png) is likely to
    have a value near ![](../../OEBPS/Images/AR_micro.png)[0] with a confidence **Œ£**[0].
    In other words, we are injecting a constant value as our belief in the parameter
    ![](../../OEBPS/Images/AR_micro.png). We can treat the covariance similarly. Later,
    we prove that in this paradigm, with a low volume of training data, the prior
    dominates. Once sufficient training data is digested, the effect of the prior
    fades, and the solution gets closer and closer to the MLE. This is the fully Bayes
    parameter estimation technique in a nutshell.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we discuss Bayes estimation of parameters for a Gaussian likelihood
    function for a series of increasingly complex scenarios. In section [13.3](#sec-bayesinf-muonly),
    we deal with the case where the variance of the parameters to be estimated is
    known (constant) but the mean is unknown, so the mean is expressed as a (Gaussian)
    random variable. Then, in section [13.6](#sec-bayesinf-sigmaonly), we examine
    the case where the mean is known (constant) but the variance is unknown. Finally,
    in section [13.7](#sec-bayesinf-musigma), we examine the case where both are unknown.
    Both the univariate and multivariate cases are dealt with for each scenario.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for this chapter, executable via Jupyter Notebook,
    can be found at [http://mng.bz/woYW](http://mng.bz/woYW).
  prefs: []
  type: TYPE_NORMAL
- en: 13.2 MLE for Gaussian parameter values recap)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have discussed the details of this in section [6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation).
    Here we recap the main results. Suppose we have a data set *X* = {*x*^((1)), *x*^((2)),‚ãØ,
    *x*^((*n*))}. We have decided to model the data distribution as a Gaussian ùí©(*x*;*Œº*,
    *œÉ*)‚Äîwe want to estimate the parameters *Œº*, *œÉ* that best ‚Äúexplain" or ‚Äúfit"
    the observed data set *X*. MLE is one of the simplest approaches to solving this
    problem. Here we estimate the parameters such that *the likelihood of the data
    observed during training is maximized*. This can be loosely visualized as estimating
    a probability density function whose peak coincides with the region in the input
    space with the densest population of training data. We looked at MLE in section
    [6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation). Here we simply restate
    the expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Let‚Äôs denote the (as yet unknown) mean and variance of the data distribution
    as *Œº* and *œÉ*. Then from equation [5.22](../Text/05.xhtml#eq-univar-normal),
    we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-01-a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Maximizing the log-likelihood *p*(*X*|*Œº*, *œÉ*) has a closed-form solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-02.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 13.2
  prefs: []
  type: TYPE_NORMAL
- en: Thus the MLE mean and variance are essentially the mean and variance of the
    training data samples (see section [6.6](../Text/06.xhtml#sec-model_param_estimation)
    for the derivation of these expressions).
  prefs: []
  type: TYPE_NORMAL
- en: The corresponding expressions for multivariate Gaussian MLE are
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-03.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 13.3
  prefs: []
  type: TYPE_NORMAL
- en: These MLE parameter values are to be used to evaluate *p*(*x*) = ùí©(*x*;*Œº*,
    *œÉ*)‚Äîthe probability of an unknown data point *x* coming from the distribution
    represented by the training data set *X*.
  prefs: []
  type: TYPE_NORMAL
- en: '13.3 Fully Bayes parameter estimation: Gaussian, unknown mean, known precision'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MLE may not be that accurate when the available data set is small that is,
    *n*, size of the data set *X*, is small). In many problems, we have a prior idea
    of the mean and sigma of the data set. Unfortunately, MLE provides no way to bake
    such a prior belief into the estimation. Fully Bayes parameter estimation techniques
    try to fix this drawback: here we are not simply maximizing the likelihood of
    the observed data. Instead, we maximize the posterior probability of the estimated
    parameter(s). This posterior probability involves the product of the likelihood
    and a prior probability (see equation [13.1](#eq-posterior-prior-likelihood-evidence-ch12)).
    The likelihood term captures the effect of the training data‚Äîmaximizing it alone
    is MLE‚Äîbut does not capture the effect of a prior belief. On the other hand, the
    prior term does not depend on the data. This is where we bake in our belief or
    guess or prior knowledge about the data distribution. Thus, our estimate for the
    data distribution parameters will consider the data and the prior guess. We will
    soon see that the estimation is such that as the size of the data set (*n*, length
    of *X*) increases, the effect of the prior term decreases, and the effect of the
    likelihood term increases. In the limit, at infinite data availability, the Bayesian
    inference ields the MLE. At the other extreme, when no data is available (*n*
    = 0), the Fully Bayes estimates for the parameters are the same as the prior estimates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs we examine Bayesian parameter estimation. For starters, we deal with
    a relatively simple case where we have a Gaussian data distribution with a known
    (constant) variance but unknown and modeled mean. The data distribution is Gaussian
    (as usual, the semicolon in ùí©(*x*;*Œº[n]*, *œÉ*) separates the variables from parameters):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-03-a.png)'
  prefs: []
  type: TYPE_IMG
- en: The training data set is denoted *X* = {*x*^((1)), *x*^((2)),‚ãØ, *x*^((*n*))},
    and its overall likelihood is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-03-b.png)'
  prefs: []
  type: TYPE_IMG
- en: The variance is known by assumption‚Äîhence it is treated as a constant instead
    of a random variable. The mean *Œº* is unknown and is treated as a Gaussian random
    variable, with mean *Œº*[0] and variance *œÉ*[0] (not to be confused with *Œº* and
    *œÉ*, the mean and variance of the data itself ). So, the prior is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-03-c.png)'
  prefs: []
  type: TYPE_IMG
- en: The posterior probability of the unknown *Œº* parameter is a product of two Gaussians,
    which is a Gaussian itself. Let‚Äôs denote the as-yet-unknown) mean and variance
    of this product Gaussian as *Œº[n]* and *œÉ[n]*. Here the subscript *n* is to remind
    us that the posterior has been obtained by digesting *n* data instances from *X*
    = {*x*^((1)), *x*^((2)),‚ãØ, *x*^((*n*))}. Thus, the Gaussian posterior can be denoted
    as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-03-d.png)'
  prefs: []
  type: TYPE_IMG
- en: Using Bayes‚Äô theorem,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-03-e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By comparing the coefficients of *Œº*¬≤ and *Œº* on the exponents of the left
    and right sides, we determine the unknown parameters of the posterior distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-04.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 13.4
  prefs: []
  type: TYPE_NORMAL
- en: 'The significance of various closely named variables should be clearly understood:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Œº*, *œÉ* are the mean and variance of the *data* distribution *p*(*x*)‚Äîassumed
    to be Gaussian. The final goal is to estimate *Œº*, *œÉ* that best fits the data
    set *X*. On the other hand, *Œº*[0], *œÉ*[0] are the mean and variance of the *parameter*
    distribution *p*(*Œº*), which captures our prior belief about the value of the
    data mean *Œº* (remember, by assumption, the data mean is also a Gaussian random
    variable). *Œº[n]*, *œÉ[n]* are the mean and variance of the posterior distribution
    *p*(*Œº*|*X*) for the data mean *Œº* as computed from *n* data point samples. This
    is a Gaussian random variable because it is a product of two Gaussians.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The posterior distribution of the unknown mean parameter, *p*(*Œº*|*X*), is a
    Gaussian with mean *Œº[n]*. So, it will attain a maximum when *Œº* = *Œº[n]*. In
    other words, the MAP estimate for the unknown mean *Œº* is *Œº*[MAP] = *Œº[n]*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though *Œº[n]* is the best estimate of *Œº*, *œÉ[n]* is not approximating
    the *œÉ* of the data, *œÉ* is known in this case by assumption. Here, *œÉ[n]* is
    the variance of the posterior distribution, reflecting our *uncertainty* about
    the estimate of *Œº*. That is why, as the number of data instances becomes very
    large, *œÉ[n]* approaches 0 (indicating we have zero uncertainty or full confidence
    in the estimate of the mean.)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The estimate for our data distribution is *p*(*x*) = ùí©(*x*;*Œº[n]*, *œÉ*), where
    *Œº[n]* is given by equation [13.4](#eq-bayesinf-muonlyunivar). Note that it is
    a combination of the MLE *xÃÑ* and prior guess *Œº*[0]. Using this, given any arbitrary
    data instance *x*, we can infer the probability of *x* belonging to the class
    of the training data set *X*.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for Bayesian estimation with unknown mean and known
    variance, executable via Jupyter Notebook, can be found at [http://mng.bz](http://mng.bz/ZA75)
    [/ZA75](http://mng.bz/ZA75).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.1 PyTorch- Bayesian estimation with unknown mean, known variance
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Parameters of the prior
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Mean of the posterior, following equation [13.4](#eq-bayesinf-muonlyunivar)
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ Standard deviation of the posterior, following equation [13.4](#eq-bayesinf-muonlyunivar)
  prefs: []
  type: TYPE_NORMAL
- en: 13.4 Small and large volumes of training data, and strong and weak priors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let‚Äôs examine the behavior of equation [13.4](#eq-bayesinf-muonlyunivar) when
    *n* = 0 (no data) and when *n* ‚Üí ‚àû (lots of data):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-04-a.png)'
  prefs: []
  type: TYPE_IMG
- en: This agrees with our notion that with little data, the posterior is dominated
    by the prior, while with lots of data, the posterior is dominated by the likelihood.
    With lots of data, the variance of the parameter is zero (we are saying with *full
    certainty* that the best value for the mean is the sample mean for the data, aka
    the MLE estimate for the mean). In general, with more training data (that is,
    larger values of *n*), the posterior shifts closer to the likelihood. This can
    be seen by analyzing equation [13.4](#eq-bayesinf-muonlyunivar). It agrees with
    our intuition that with little data, we try to compensate with our pre-existing
    (prior) belief as to the value of the parameters. As the number of training data
    instances increases, the effect of the prior is reduced, and the likelihood (which
    is a function of the data) begins to dominate.
  prefs: []
  type: TYPE_NORMAL
- en: 'A low variance for the prior (that is, small *œÉ*[0]) essentially means we have
    low uncertainty in our prior belief (remember, the entropy/uncertainty of a Gaussian
    is proportional to its variance). Such high-confidence priors resist being overwhelmed
    by the data and are called *strong priors*. On the other hand, a large *œÉ*[0]
    implies low /confidence in the prior mean value. This is a *weak prior* that is
    easily overwhelmed by the data. We can see this in the final expression for mean
    in equation [13.4](#eq-bayesinf-muonlyunivar): we have *nœÉ*[0]¬≤/*œÉ*¬≤ in the denominator
    of the second term. In general, the second term vanishes with larger *n*, thereby
    removing the effect of the prior *Œº*[0] and making the posterior mean coincide
    with the MLE mean. But the smaller the *œÉ*[0], the larger the *n* required to
    achieve this, and vice versa.'
  prefs: []
  type: TYPE_NORMAL
- en: 13.5 Conjugate priors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In section [13.3](#sec-bayesinf-muonly), given a Gaussian likelihood, choosing
    the Gaussian family for the prior made the posterior also belong to the Gaussian
    family. This simplified things considerably. If the prior was chosen from another
    family, the posterior‚Äîwhich is the product of the likelihood and prior‚Äîmay not
    belong to a simple or even known distribution family.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, a Gaussian likelihood with a Gaussian prior results in a Gaussian posterior
    for the mean. Such priors are said to be *conjugate*. Formally, for a specific
    family of likelihood, the choice of the prior that results in the posterior belonging
    to the same family as the prior is called a conjugate prior. For instance, Gaussians
    for the mean (with known variance) are conjugate to a Gaussian likelihood. Soon
    we will see that for a Gaussian likelihood, a gamma distribution for the precision
    (inverse of the variance) results in a gamma posterior. In other words, a gamma
    prior to the precision is conjugate to a Gaussian likelihood. In the multivariate
    case, instead of gamma, we have the Wishart distribution as a conjugate prior.
  prefs: []
  type: TYPE_NORMAL
- en: '13.6 Fully Bayes parameter estimation: Gaussian, unknown precision, known mean'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In section [13.3](#sec-bayesinf-muonly), we discussed fully Bayes parameter
    estimation with the assumption that we somehow know the variance *œÉ* and only
    want to estimate the mean *Œº*. Now we examine the case where the mean is known
    but the variance is unknown and expressed as a random variable. The computations
    become simpler if we use precision *Œª* instead of variance *œÉ*. They are related
    by the expression 1/*œÉ*¬≤. Thus we have a data set *X*, which is assumed to be
    sampled from a Gaussian distribution with a constant mean *Œº*, while the precision
    *Œª* is a random variable with a gamma distribution. The probability density function
    for the data is thus *p*(*x*|*Œº*, *Œª*) = ùí©(*x*; *Œº*, 1/‚àö*Œª*).
  prefs: []
  type: TYPE_NORMAL
- en: We model the prior random variable for precision with a gamma distribution.
    The likelihood is Gaussian, and since the product of a Gaussian and gamma is another
    gamma (due to the conjugate prior property of gamma), the resulting posterior
    is a gamma. The gamma function parameters for the posterior can be derived via
    coefficient comparison. The maximum of the posterior is our estimate for the parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Gamma distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'The gamma distribution is introduced in the appendix; if necessary, please
    read that first. Here we state the relevant properties. The probability density
    function for a random variable *Œª* having a gamma distribution is a function with
    two parameters *Œ±*, *Œ≤*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-05.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 13.5
  prefs: []
  type: TYPE_NORMAL
- en: Maximum of a gamma distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'To maximize the gamma probability density function *p*(*Œª*|*X*) = *Œª*^((*Œ±[n]*
    ‚àí 1))*e*^(‚àí*Œ≤[n]Œª*) for a random variable *Œª*, we take the derivative and equate
    to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-05-a.png)'
  prefs: []
  type: TYPE_IMG
- en: 13.6.1 Estimating the precision parameter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let‚Äôs return to the fully Bayes estimation of the precision parameter when
    the mean is known. We model the data distribution with a Gaussian: *p*(*x*|*Œº*,
    *Œª*) = ùí©(*x*; *Œº*, 1/‚àö*Œª*) (we have expressed this Gaussian in terms of the precision,
    *Œª*, which is related to the variance *œÉ* as *Œª* = 1/*œÉ* ¬≤). The training data
    set is *X* = {*x*^((1)), *x*^((2)),‚ãØ, *x*^((*n*))}, and its overall likelihood
    is'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-05-b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We model the prior for the precision with a gamma distribution with parameters
    *Œ±*[0], *Œ≤*[0]:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*Œª*) = *Œ≥*(*Œª*;*Œ±*[0], *Œ≤*[0]) ‚àù *Œª*^((*Œ±*[0]‚àí1))*e*^(‚àí*Œ≤*[0]*Œª*)'
  prefs: []
  type: TYPE_NORMAL
- en: We know the corresponding posterior‚Äîa product of a Gaussian and a gamma‚Äîis gamma
    distribution (due to the conjugate prior property of gamma distribution). Let‚Äôs
    denote the posterior as
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*Œª*|*X*) = *Œ≥*(*Œª*;*Œ±[n]*, *Œ≤[n]*)'
  prefs: []
  type: TYPE_NORMAL
- en: From Bayes‚Äô theorem,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-05-c.png)'
  prefs: []
  type: TYPE_IMG
- en: Substituting
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-05-d.png)'
  prefs: []
  type: TYPE_IMG
- en: and comparing the powers of *Œª* and *e*, we get
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-06.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 13.6
  prefs: []
  type: TYPE_NORMAL
- en: Notice that as before, at low values of *n*, the posterior is dominated by the
    prior but gets closer and closer to the likelihood estimate as *n* increases.
    In other words, in the absence of sufficient data, we let our belief take over
    the estimation; but if and when data is available, the estimation is dominated
    by the data-based entity likelihood.
  prefs: []
  type: TYPE_NORMAL
- en: The MAP point estimate for the parameter *Œª* given data set *X* is obtained
    by maximizing this posterior distribution *p*(*Œª*|*X*) = *Œ≥*(*Œª*;*Œ±[n]*, *Œ≤[n]*),
    which yields *Œª[MAP]* = 1/*œÉ[MAP]*¬≤ = (*Œ±[n]*‚Äì1/*Œ≤[n]*). (Section A.5 in the appendix
    shows how to obtain the maximum of a gamma distribution.) Thus our estimate for
    the training data distribution is *p*(*x*) = ùí©(*x*; *Œº*, *œÉ*[MAP]), where 1/*œÉ[MAP]*¬≤
    = (*Œ±[n]*‚Äì1/*Œ≤[n]*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a large volume of data, the MAP estimate for the unknown precision/variance
    becomes identical to the MLE estimate (proof outline shown):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-06-a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the other hand, given no data, the MAP estimate for the unknown precision/variance
    is completely determined by the prior (proof outline shown):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-06-b.png)'
  prefs: []
  type: TYPE_IMG
- en: NOTE Fully functional code for Bayesian estimation with a known mean and unknown
    variance, executable via Jupyter Notebook, can be found at [http://mng.bz](http://mng.bz/2nZ9)
    [/2nZ9](http://mng.bz/2nZ9).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.2 PyTorch- Bayesian estimation with unknown variance, known mean
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Parameters of the prior
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Parameters of the posterior
  prefs: []
  type: TYPE_NORMAL
- en: '13.7 Fully Bayes parameter estimation: Gaussian, unknown mean, unknown precision'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In section [13.3](#sec-bayesinf-muonly), we saw that if the variance is known,
    the conjugate prior to the mean is a Gaussian (aka normal) distribution. Likewise,
    when the mean is known, the conjugate prior to the precision is a gamma distribution.
    If both are unknown, we end up with a normal-gamma distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 13.7.1 Normal-gamma distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Normal-gamma is a probability distribution of two random variables, say, *Œº*
    and *Œª*, whose density is defined in terms of four parameters *Œº*^‚Ä≤, *Œª*^‚Ä≤, *Œ±*^‚Ä≤,
    and *Œ≤*^‚Ä≤, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-06-c.png)'
  prefs: []
  type: TYPE_IMG
- en: Although it looks complicated, a simple way to remember it is a product of a
    normal and a gamma distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The normal-gamma distribution attains a maximum at
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-06-d.png)'
  prefs: []
  type: TYPE_IMG
- en: 13.7.2 Estimating the mean and precision parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As before, we model the data distribution with a Gaussian: *p*(*x*|*Œº* , *Œª*)
    = ùí©(*x*; *Œº*, 1/‚àö*Œª*) we have expressed this Gaussian in terms of the precision,
    *Œª*, which is related to the variance *œÉ* as *Œª* = 1/*œÉ*¬≤). The training data
    set is *X* = {*x*^((1)), *x*^((2)),‚ãØ, *x*^((*n*))}, and its overall likelihood
    is'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-06-e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We model the prior for the mean as a Gaussian with mean *Œº*[0] and precision
    *Œª*[0]*Œª*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-06-f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We model the prior for the precision as a gamma distribution with parameters
    *Œ±*[0], *Œ≤*[0]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-06-g.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The overall prior probability for the mean and precision parameters is the
    product of the two, a normal-gamma distribution with parameters *Œº*‚Å∞, *Œª*‚Å∞, *Œ±*‚Å∞,
    *Œ≤*‚Å∞:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-06-h.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The posterior probability for the mean and precision parameters is the joint
    (that is, product) of the likelihood and the prior. As such, we know it is another
    normal-gamma distribution (due to the conjugate prior property of normal-gamma):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-06-i.png)'
  prefs: []
  type: TYPE_IMG
- en: Using Bayes‚Äô theorem,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-06-j.png)'
  prefs: []
  type: TYPE_IMG
- en: Substituting
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-06-k.png)'
  prefs: []
  type: TYPE_IMG
- en: 'and comparing coefficients, the unknown parameters of the posterior distribution
    can be determined:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07.png)'
  prefs: []
  type: TYPE_IMG
- en: Equation 13.7
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain the fully Bayes parameter estimate, we take the maximum of the normal-gamma
    posterior probability density function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-a.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus the final probability density function for the data is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-b.png)'
  prefs: []
  type: TYPE_IMG
- en: NOTE Fully functional code for Bayesian estimation with an unknown mean and
    unknown variance, executable via Jupyter Notebook, can be found at [http://mng.bz/1oQy](http://mng.bz/1oQy).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.3 PyTorch code for a normal-gamma distribution
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Since PyTorch doesn‚Äôt implement normal-gamma distribution, we implement a
    bare-bones version.
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 13.4 PyTorch: Bayesian estimation with unknown mean, unknown variance'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Parameters of the prior
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Parameters of the posterior
  prefs: []
  type: TYPE_NORMAL
- en: '13.8 Example: Fully Bayesian inferencing'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let‚Äôs revisit the problem discussed in section [6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation)
    of predicting whether a resident of Statsville is female based on height. For
    this purpose, we have collected height samples from adult female residents of
    Statsville. Unfortunately, due to unforeseen circumstances, we collected a very
    small sample. Armed with our knowledge of Bayesian inference, we do not want to
    let this deter us from trying to build a model. Based on physical considerations,
    we can assume that the distribution of heights is Gaussian. Our goal is to estimate
    the parameters (*Œº*, *œÉ*) of this Gaussian.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for this example, executable via Jupyter Notebook,
    can be found at [http://mng.bz/Pn4g](http://mng.bz/Pn4g).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let‚Äôs first create the data set by sampling five points from a Gaussian distribution
    with *Œº* = 152 and *œÉ* = 8. In real-life scenarios, we do not know the mean and
    standard deviation of the true distribution. But for the sake of this example,
    let‚Äôs assume that the mean height is 152 cm and the standard deviation is 8 cm.
    Our data matrix, *X*, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-c.png)'
  prefs: []
  type: TYPE_IMG
- en: 13.8.1 Maximum likelihood estimation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If we relied on MLE, our approach would be to compute the mean and standard
    deviation of the data set and use this normal distribution as our model. We use
    the following equations to compute the mean and standard deviation of our normal
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-d.png)'
  prefs: []
  type: TYPE_IMG
- en: The mean, *Œº*, comes out to be 149.68, and the standard deviation, *œÉ*, is 11.52\.
    This differs significantly from the true mean (152) and standard deviation (8)
    because the number of data points is low. In such low-data scenarios, the maximum
    likelihood estimates are not very reliable.
  prefs: []
  type: TYPE_NORMAL
- en: 13.8.2 Bayesian inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Can we do better than MLE? One potential method is to use Bayesian inference
    with a good prior. How do we select a good prior? Well, let‚Äôs say that we know
    from an old survey that the average and standard deviation of the height of adult
    female residents of Neighborville, the neighboring town, are 150 cm and 9 cm,
    respectively. Additionally, we have no reason to believe that the distribution
    of heights at Statsville is significantly different. So we can use this information
    to ‚Äúinitialize" our prior. The prior distribution encodes our beliefs about the
    parameter values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that we are dealing with an unknown mean and unknown variance, we model
    the prior as a normal-gamma distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*Œ∏*) = ùí©*Œ≥*(*Œº*[0], *Œª*[0], *Œ±*[0], *Œ≤*[0])'
  prefs: []
  type: TYPE_NORMAL
- en: We choose *p*(*Œ∏*) such that *Œº*[0] = 150, *Œª*[0] = 100, *Œ±*[0] = 10.5, and
    *Œ≤*[0] = 810\. This implies that
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*Œ∏*) = ùí©*Œ≥*(150, 100, 10.5, 810)'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*Œ∏*|*X*) is a normal-gamma distribution whose parameters can be computed
    using equations described in section [13.7](#eq-normal-gamma). The PyTorch code
    for computing the posterior is shown next.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.5 PyTorch- Computing posterior probability using Bayesian inference
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Initializes the normal-gamma distribution
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Computes the posterior
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë¢ The mode of the distribution refers to parameter values with the highest probability
    density.
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë£ Computes the standard deviation using precision
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë§ map_mu and map_std refer to the parameter values that maximize the posterior
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The MAP estimates for *Œº* and *œÉ* obtained using Bayesian inference are 149.98
    and 9.56, respectively, which are better than the MLE estimates of 149.68 and
    11.52 (the true *Œº* and *œÉ* are 152 and 9, respectively).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we‚Äôve estimated the parameters, we can find out the probability that
    a sample lies in the range using the formula
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-e.png)'
  prefs: []
  type: TYPE_IMG
- en: The details of this can be found in section [6.8](../Text/06.xhtml#sec-gauss_max_likelihood_estimation).
  prefs: []
  type: TYPE_NORMAL
- en: '13.9 Fully Bayes parameter estimation: Multivariate Gaussian, unknown mean,
    known precision'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the multivariate case; the univariate version is discussed in section
    [13.3](#sec-bayesinf-muonly). The computations follow along the same lines as
    the univariate ones.
  prefs: []
  type: TYPE_NORMAL
- en: We model the data distribution as a Gaussian *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_micro.png),
    **Œõ**) = ùí©(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_micro.png),
    **Œõ**^(‚àí1)), where we have expressed the Gaussian in terms of the *precision matrix*
    Œõ instead of the covariance matrix Œ£, where **Œõ** = **Œ£**^(‚àí1). The training data
    set is *X* ‚â° {![](../../OEBPS/Images/AR_x.png)^((1)), ![](../../OEBPS/Images/AR_x.png)^((2)),
    ‚ãØ, ![](../../OEBPS/Images/AR_x.png)^((*i*)), ‚ãØ, ![](../../OEBPS/Images/AR_x.png)^((*n*))},
    and its overall likelihood is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We model the prior for the mean as a Gaussian:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-g.png)'
  prefs: []
  type: TYPE_IMG
- en: The posterior probability density is a Gaussian (because it is the product of
    two Gaussians). Let‚Äôs denote it as
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-h.png)'
  prefs: []
  type: TYPE_IMG
- en: Using Bayes‚Äô theorem,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-i.png)'
  prefs: []
  type: TYPE_IMG
- en: Let‚Äôs examine the exponent of the rightmost expression.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-j.png)'
  prefs: []
  type: TYPE_IMG
- en: We ignored the last constant terms because they will be rolled into the overall
    constant of proportionality). Thus
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-k.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Comparing coefficients:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-l.png)'
  prefs: []
  type: TYPE_IMG
- en: The posterior probability maximizes at *![](../../OEBPS/Images/AR_micro.png)[n]*.
    Thus *![](../../OEBPS/Images/AR_micro.png)[MAP]* = *![](../../OEBPS/Images/AR_micro.png)[n]*
  prefs: []
  type: TYPE_NORMAL
- en: 'is the MAP estimate for the mean parameter of the multivariate Gaussian data
    distribution: *p*(![](../../OEBPS/Images/AR_x.png)) = ùí©(![](../../OEBPS/Images/AR_x.png);
    *![](../../OEBPS/Images/AR_micro.png)[n]*, **Œõ**^(-1)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-m.png)'
  prefs: []
  type: TYPE_IMG
- en: With a large volume of data, the estimated mean parameter *![](../../OEBPS/Images/AR_micro.png)[MAP]*
    = *![](../../OEBPS/Images/AR_micro.png)[n]* approaches the MLE *![](../../OEBPS/Images/AR_micro.png)[MLE]*
    = ![](../../OEBPS/Images/AR_x2.png).
  prefs: []
  type: TYPE_NORMAL
- en: With a low volume of data, the estimated posterior mean parameter *![](../../OEBPS/Images/AR_micro.png)[MAP]*
    = *![](../../OEBPS/Images/AR_micro.png)[n]* approaches the prior ![](../../OEBPS/Images/AR_micro.png)[0].
  prefs: []
  type: TYPE_NORMAL
- en: NOTE Fully functional code for multivariate Bayesian inferencing of the mean
    of a Gaussian likelihood with known precision, executable via Jupyter Notebook,
    can be found at [http://mng.bz/J2AP](http://mng.bz/J2AP).
  prefs: []
  type: TYPE_NORMAL
- en: Listing 13.6 PyTorch- Multivariate Bayesian inferencing, unknown mean
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ‚ë† Parameters of the prior
  prefs: []
  type: TYPE_NORMAL
- en: ‚ë° Parameters of the posterior
  prefs: []
  type: TYPE_NORMAL
- en: '13.10 Fully Bayes parameter estimation: Multivariate, unknown precision, known
    mean'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In section [13.6](#sec-bayesinf-sigmaonly), we discussed the univariate case,
    and now we examine the multivariate case. For the univariate case, we had to look
    at the gamma distribution. For the multivariate case, we have to look at the Wishart
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 13.10.1 Wishart distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose we have a Gaussian random data vector ![](../../OEBPS/Images/AR_x.png)
    with probability density function ùí©(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_micro.png),
    **Œ£**). Once again, we use *precision matrix* Œõ instead of the covariance matrix
    Œ£, where **Œõ** = **Œ£**^(‚àí1). Consider the case where we know the mean ![](../../OEBPS/Images/AR_micro.png)
    but want to estimate the precision Œõ. How do we express the prior? Note that *p*(**Œõ**)
    is the probability density function of a *matrix*. So far, we have encountered
    probability distributions of scalars and vectors, not a matrix. Also, this is
    not an arbitrary matrix. We are talking about a *symmetric, non-negative definite*
    matrix (all covariance and precision matrices belong to this category). Consequently,
    the distribution we are looking for is not a joint distribution of all the *d*¬≤
    matrix elements *d* denotes the dimensionality of the data: that is, all ![](../../OEBPS/Images/AR_x.png)
    and ![](../../OEBPS/Images/AR_micro.png) vectors are *d* √ó 1). Rather, it is a
    joint distribution of (*d*(*d* + 1))/2 elements in the matrix‚Äîthe diagonal and
    those above or below (diagonal elements above and below are identical because
    the matrix is symmetric).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The space of such matrices is called a *Wishart ensemble*. The probability
    of a random-precision matrix Œõ of size *d* √ó *d* can be expressed as a Wishart
    distribution. This distribution has two parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*ŒΩ*, a scalar, satisfying *ŒΩ* > *d* ‚àí 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: W, a *d* √ó *d* symmetric non-negative definite matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The probability density function is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-n1.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: ùí≤ denotes Wishart.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|**W**|, |**Œõ**| denote the determinants of the matrices W and Œõ, respectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Tr*(*A*) denotes the trace of a matrix *A* (sum of the diagonal elements).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Œì* denotes the multivariate gamma function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-n2.png)'
  prefs: []
  type: TYPE_IMG
- en: The Wishart is the multivariate version of the gamma distribution. Its expected
    value is
  prefs: []
  type: TYPE_NORMAL
- en: ùîº(**Œõ**) = *ŒΩ***W**
  prefs: []
  type: TYPE_NORMAL
- en: Its maxima occur at
  prefs: []
  type: TYPE_NORMAL
- en: '**Œõ** = (*ŒΩ* ‚àí *d* ‚àí 1)**W** for *ŒΩ* ‚â• *d* + 1'
  prefs: []
  type: TYPE_NORMAL
- en: 13.10.2 Estimating precision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As before, we model the data distribution as a Gaussian *p*(![](../../OEBPS/Images/AR_x.png)|![](../../OEBPS/Images/AR_micro.png),
    **Œõ**) = ùí©(![](../../OEBPS/Images/AR_x.png); ![](../../OEBPS/Images/AR_micro.png),
    **Œõ**^(‚àí1)), where we have expressed the Gaussian in terms of the *precision matrix*
    Œõ instead of the covariance matrix Œ£, where **Œõ** = **Œ£**^(‚àí1).
  prefs: []
  type: TYPE_NORMAL
- en: The training data set is *X* ‚â° {![](../../OEBPS/Images/AR_x.png)^((1)), ![](../../OEBPS/Images/AR_x.png)^((2)),‚ãØ,
    ![](../../OEBPS/Images/AR_x.png)^((*i*)),‚ãØ, ![](../../OEBPS/Images/AR_x.png)^((*n*))},
    and its overall likelihood is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-n3.png)'
  prefs: []
  type: TYPE_IMG
- en: We model the prior probability of the precision matrix as a Wishart distribution.
    Hence,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-n4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The posterior is another Wishart (owing to the Wishart conjugate prior property):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-n5.png)'
  prefs: []
  type: TYPE_IMG
- en: Using Bayes‚Äô theorem for the training data set *X*,
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-o1.png)'
  prefs: []
  type: TYPE_IMG
- en: Let‚Äôs study a pair of simple lemmas that will come in handy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-o2.png)'
  prefs: []
  type: TYPE_IMG
- en: where *Tr* refers to Trace of a matrix (sum of diagonal elements).
  prefs: []
  type: TYPE_NORMAL
- en: The first lemma is almost trivial‚Äîthe quadratic form ![](../../OEBPS/Images/AR_x.png)*^TA*![](../../OEBPS/Images/AR_x.png)
    is a scalar, so of course it is the same as its trace.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second lemma follows directly from the matrix property of a trace: *Tr*(*BC*)
    = *Tr*(*CB*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the lemmas, the exponent of the likelihood term is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-o3.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-o4.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the posterior density is
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-o5.png)'
  prefs: []
  type: TYPE_IMG
- en: Since *Tr*(*A*) + *Tr*(*B*) = *Tr*(*A* + *B*),
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-o6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Comparing coefficients, we determine the unknown parameters of the posterior
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-o7.png)'
  prefs: []
  type: TYPE_IMG
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-o8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The maximum of the posterior density function, **ùí≤**(**Œõ**;*ŒΩ[n]*, **W**[n]),
    ields an estimate for the precision parameter of the data distribution: **Œõ**
    = (*ŒΩ[n]* ‚àí *d* ‚àí 1)**W**[n] for *ŒΩ[n]* ‚â• *d* + 1 i.e.,'
  prefs: []
  type: TYPE_NORMAL
- en: '![](../../OEBPS/Images/eq_13-07-o9.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A generative model that models the underlying data distribution can be more
    powerful than a black box discriminative model. Once we choose a model family,
    we need to estimate the model parameters, *Œ∏*. We can estimate the best values
    of *Œ∏* from the training data *X* using Bayes‚Äô theorem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The posterior distribution *p*(*Œ∏*|*X*) is a function of the product of likelihood
    *p*(*X*|*Œ∏*) and the prior *p*(*Œ∏*). The prior expresses our belief in the value
    of the parameters. The posterior is dominated by the prior for small data sets
    and the likelihood for large data sets. Injecting belief via a good prior distribution
    can be helpful in settings with very little training data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximum likelihood estimation only relies on the data, in contrast to maximum
    a posteriori (MAP) estimation, which relies on the data as well as the prior information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use Bayesian estimation for the mean of a Gaussian likelihood when the
    variance is known. When the likelihood is Gaussian *p*(*X*) ‚àº *N*(*Œº*, *œÉ*), we
    model the prior as a normal distribution *p*(*Œº*) ‚àº *N*(*Œº*[0], *œÉ*[0]). The posterior
    distribution is also a normal distribution *p*(*Œº*|*X*) ‚àº *N*(*Œº[n]*, *œÉ[n]*),
    where ![](../../OEBPS/Images/eq_13-07-p1a.png) and ![](../../OEBPS/Images/eq_13-07-p2a.png).
    We can also use the estimated parameter to make predictions about new instances
    of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weak priors imply a high degree of uncertainty/lower confidence in our prior
    belief and can easily be overwhelmed by the data. In contrast, strong priors imply
    a lower degree of uncertainty/higher confidence in our prior belief and will resist
    data overload.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For a specific family of likelihood, the choice of the prior that results in
    the posterior belonging to the same family as the prior is called a conjugate
    prior.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gamma function is ![](../../OEBPS/Images/eq_13-07-p3a.png), and the gamma
    distribution is ![](../../OEBPS/Images/eq_13-07-p4a.png). The gamma distribution
    varies with different values of *Œ±* and *Œ≤*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of Bayesian estimation of the precision of the Gaussian likelihood
    for a known mean, the precision *Œª* is the inverse of the variance. We can model
    the prior as a gamma distribution ![](../../OEBPS/Images/eq_13-07-p5a.png). The
    posterior distribution is also a gamma distribution, ![](../../OEBPS/Images/eq_13-07-p6a.png),
    where ![](../../OEBPS/Images/eq_13-07-p7a.png) and ![](../../OEBPS/Images/eq_13-07-p8a.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Bayesian estimation of both the mean and precision of a Gaussian likelihood,
    we model the prior as a normal-gamma distribution. The posterior is another normal-gamma
    distribution. The posterior distribution can be used to predict new data instances.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The multivariate setting of Bayesian inferencing of the mean of a Gaussian likelihood
    is known as precision. We can model the prior as a multivariate normal distribution;
    the posterior is also a multivariate normal distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Wishart distribution is the multivariate version of the gamma distribution.
    With multivariate Bayesian inferencing of the precision of a Gaussian likelihood
    with a known mean, we can model the prior as a Wishart distribution. The corresponding
    posterior is also a Wishart distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
