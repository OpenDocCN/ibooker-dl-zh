<html><head></head><body><section data-pdf-bookmark="Chapter 4. Using Data with PyTorch" data-type="chapter" epub:type="chapter"><div class="chapter" id="ch04_using_data_with_pytorch_1748548966496246">
      <h1><span class="label">Chapter 4. </span>Using Data with PyTorch</h1>
      <p>In the first three chapters of this book,<a contenteditable="false" data-primary="datasets" data-secondary="about PyTorch dataset tools" data-type="indexterm" id="id958"/> you trained models using a variety of data, from the Fashion MNIST dataset that was conveniently bundled via an API to the image-based “Horses or Humans” and “Dogs vs. Cats” datasets, which were available as ZIP files that you had to download and preprocess. So by now, you’ve probably realized that there are lots of different ways of getting the data with which to train a model.</p>
      <p>However, many public datasets require you to learn lots of different domain-specific skills before you begin to consider your model architecture. <a contenteditable="false" data-primary="datasets" data-secondary="torch.utils.data.Dataset library" data-type="indexterm" id="id959"/>The goal behind PyTorch domains and the tools available at the <code>torch.utils.data.Datasets</code> namespace is to expose datasets in a way that’s easy to consume, where all the preprocessing steps of acquiring the data and getting it into PyTorch-friendly APIs are done for you.</p>
      <p>You’ve already seen a little of this idea<a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="data loader" data-type="indexterm" id="id960"/> in how PyTorch handled Fashion MNIST back in <a data-type="xref" href="ch02.html#ch02_introduction_to_computer_vision_1748548889076080">Chapter 2</a>. As a recap, all you had to do to get the data was this:</p>
      <pre data-code-language="python" data-type="programlisting"><code class="n">train_dataset</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">FashionMNIST</code><code class="p">(</code><code class="n">root</code><code class="o">=</code><code class="s1">'./data'</code><code class="p">,</code> <code class="n">train</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code>
                             <code class="n">download</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">transform</code><code class="o">=</code><code class="n">transform</code><code class="p">)</code>
 </pre>
      <p>In the case of this dataset,<a contenteditable="false" data-primary="datasets" data-secondary="torchvision library" data-type="indexterm" id="id961"/><a contenteditable="false" data-primary="torchvision library" data-type="indexterm" id="id962"/><a contenteditable="false" data-primary="computer vision" data-secondary="torchvision library" data-type="indexterm" id="id963"/> we also did an import from the torchvision library to get the datasets object that contained the reference to Fashion MNIST:</p>
      <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">torchvision</code> <code class="kn">import</code> <code class="n">datasets</code></pre>
      <p>Given that it’s a computer vision–oriented dataset, it makes sense that it would be in the torchvision library. </p>
      <p class="pagebreak-before less_space">PyTorch has many other datasets of different data types that can be loaded in the same way. These include the following:<a contenteditable="false" data-primary="datasets" data-secondary="about PyTorch built-in datasets" data-type="indexterm" id="id964"/></p>
      <dl>
        <dt>Vision</dt>
        <dd>
          <p>Fashion MNIST is in the aforementioned<a contenteditable="false" data-primary="datasets" data-secondary="torchvision library" data-type="indexterm" id="id965"/><a contenteditable="false" data-primary="torchvision library" data-type="indexterm" id="id966"/><a contenteditable="false" data-primary="computer vision" data-secondary="torchvision library" data-type="indexterm" id="id967"/> torchvision library. It’s one of the “Image Classification” built-in datasets, but there are many more for other scenarios like Image Detection, Segmentation, Optical Flow, Stereo Matching, Image Pairing, Image Captioning, Video Classification, Video Predictions, and more.</p>
        </dd>
        <dt>Text</dt>
        <dd>
          <p>Common text datasets are available in the torchtext library. <a contenteditable="false" data-primary="datasets" data-secondary="torchtext library" data-type="indexterm" id="id968"/><a contenteditable="false" data-primary="text datasets for natural language processing" data-secondary="torchtext library text datasets" data-type="indexterm" id="id969"/><a contenteditable="false" data-primary="torchtext library" data-type="indexterm" id="id970"/><a contenteditable="false" data-primary="online resources" data-secondary="text datasets" data-type="indexterm" id="id971"/>There are far too many to list here, but there are ones for Text Classification, Language Modeling, Machine Translation, Sequence Tagging, Question and Answer, and Unsupervised Learning. You can find more details on these in the <a href="https://oreil.ly/aFamN">PyTorch documentation</a>. Note that this library isn’t limited to the datasets; it also has many helper functions that you will use when processing text. </p>
        </dd>
      </dl>
      <p>
        <em>Audio</em>
      </p>
      <p>The torchaudio library contains<a contenteditable="false" data-primary="datasets" data-secondary="torchaudio library" data-type="indexterm" id="id972"/><a contenteditable="false" data-primary="torchaudio library" data-type="indexterm" id="id973"/><a contenteditable="false" data-primary="audio datasets in torchaudio library" data-type="indexterm" id="id974"/><a contenteditable="false" data-primary="online resources" data-secondary="audio datasets" data-type="indexterm" id="id975"/> many datasets that can be used in machine learning scenarios for sound or speech. Details can be found in the <a href="https://oreil.ly/tvDe4">PyTorch documentation</a>.</p>
      <p>All datasets are subclasses of <code>torch.utils.data.Dataset,</code> so it’s important<a contenteditable="false" data-primary="datasets" data-secondary="torch.utils.data.Dataset library" data-type="indexterm" id="id976"/><a contenteditable="false" data-primary="torch.utils.data.Dataset library" data-type="indexterm" id="id977"/> to take a look at this library and understand it well. That will help you not only consume existing datasets but also create your own to share with others.</p>
      <section data-pdf-bookmark="Getting Started with Datasets" data-type="sect1"><div class="sect1" id="ch04_getting_started_with_datasets_1748548966496467">
        <h1>Getting Started with Datasets</h1>
        <p>The <code>torch.utils.data.Dataset</code> is an <a contenteditable="false" data-primary="datasets" data-secondary="getting started with" data-type="indexterm" id="ch4strt"/><a contenteditable="false" data-primary="datasets" data-secondary="getting started with" data-tertiary="custom dataset creation" data-type="indexterm" id="id978"/><a contenteditable="false" data-primary="torch.utils.data.Dataset library" data-secondary="custom dataset creation" data-type="indexterm" id="id979"/><a contenteditable="false" data-primary="datasets" data-secondary="torch.utils.data.Dataset library" data-tertiary="custom dataset creation" data-type="indexterm" id="id980"/>abstract class that represents a dataset. To create a custom dataset, you just need to subclass it and implement these methods:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="fm">__len__</code><code class="p">(</code><code class="bp">self</code><code class="p">)</code> </pre>
        <p>This should return the total number of items in your dataset:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="fm">__getitem__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">index</code><code class="p">)</code> </pre>
        <p>This should return a single item from your dataset at the specified index. This item will be transformed before sending it to the model.</p>
        <p>Here’s an example:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">Dataset</code>
 
<code class="k">class</code> <code class="nc">CustomDataset</code><code class="p">(</code><code class="n">Dataset</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">data</code><code class="p">,</code> <code class="n">transforms</code><code class="o">=</code><code class="kc">None</code><code class="p">):</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">data</code> <code class="o">=</code> <code class="n">data</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">transforms</code> <code class="o">=</code> <code class="n">transforms</code>
 
    <code class="k">def</code> <code class="fm">__len__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="k">return</code> <code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">)</code>
 
    <code class="k">def</code> <code class="fm">__getitem__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">idx</code><code class="p">):</code>
        <code class="n">sample</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">data</code><code class="p">[</code><code class="n">idx</code><code class="p">]</code>
        <code class="k">if</code> <code class="bp">self</code><code class="o">.</code><code class="n">transforms</code><code class="p">:</code>
            <code class="n">sample</code> <code class="o">=</code> <code class="bp">self</code><code class="o">.</code><code class="n">transforms</code><code class="p">(</code><code class="n">sample</code><code class="p">)</code>
        <code class="k">return</code> <code class="n">sample</code></pre>
        <p>That’s pretty much it at a very low level. The data itself is in an array called <code>data[]</code>. <a contenteditable="false" data-primary="datasets" data-secondary="getting started with" data-tertiary="custom with linear relationship" data-type="indexterm" id="id981"/><a contenteditable="false" data-primary="torch.utils.data.Dataset library" data-secondary="custom with linear relationship" data-type="indexterm" id="id982"/><a contenteditable="false" data-primary="datasets" data-secondary="torch.utils.data.Dataset library" data-tertiary="custom with linear relationship" data-type="indexterm" id="id983"/>Now, imagine we want to create a dataset with a linear relationship between an <em>x</em> value and a <em>y</em> value like we had in <a data-type="xref" href="ch01.html#ch01_introduction_to_pytorch_1748548870019566">Chapter 1</a>. How would we use it? </p>
        <p>Let’s say we start with some simple synthetic data, like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Generate synthetic data</code>
<code class="n">torch</code><code class="o">.</code><code class="n">manual_seed</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code>  <code class="c1"># For reproducibility</code>
<code class="n">x</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">100</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="n">torch</code><code class="o">.</code><code class="n">float32</code><code class="p">)</code>
<code class="n">y</code> <code class="o">=</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">x</code> <code class="err">–</code> <code class="mi">1</code></pre>
        <p>Then, we could turn it into a dataset like this:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="k">class</code> <code class="nc">CustomDataset</code><code class="p">(</code><code class="n">Dataset</code><code class="p">):</code>
    <code class="k">def</code> <code class="fm">__init__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">):</code>
        <code class="sd">"""</code>
<code class="sd">        Initialize the dataset with x and y values.</code>
<code class="sd">        Arguments:</code>
<code class="sd">        x (torch.Tensor): The input features.</code>
<code class="sd">        y (torch.Tensor): The output labels.</code>
<code class="sd">        """</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">x</code> <code class="o">=</code> <code class="n">x</code>
        <code class="bp">self</code><code class="o">.</code><code class="n">y</code> <code class="o">=</code> <code class="n">y</code>
 
    <code class="k">def</code> <code class="fm">__len__</code><code class="p">(</code><code class="bp">self</code><code class="p">):</code>
        <code class="sd">"""</code>
<code class="sd">        Return the total number of samples in the dataset.</code>
<code class="sd">        """</code>
        <code class="k">return</code> <code class="nb">len</code><code class="p">(</code><code class="bp">self</code><code class="o">.</code><code class="n">x</code><code class="p">)</code>
 
    <code class="k">def</code> <code class="fm">__getitem__</code><code class="p">(</code><code class="bp">self</code><code class="p">,</code> <code class="n">idx</code><code class="p">):</code>
        <code class="sd">"""</code>
<code class="sd">        Fetch the sample at index `idx` from the dataset.</code>
<code class="sd">        Arguments:</code>
<code class="sd">        idx (int): The index of the sample to retrieve.</code>
<code class="sd">        """</code>
        <code class="k">return</code> <code class="bp">self</code><code class="o">.</code><code class="n">x</code><code class="p">[</code><code class="n">idx</code><code class="p">],</code> <code class="bp">self</code><code class="o">.</code><code class="n">y</code><code class="p">[</code><code class="n">idx</code><code class="p">]</code></pre>
        <p>Then, to use the dataset, we simply create an instance of the class, initialize it with our <em>x</em> and <em>y </em>values, <a contenteditable="false" data-primary="datasets" data-secondary="DataLoader" data-type="indexterm" id="id984"/><a contenteditable="false" data-primary="torch.utils.data.Dataset library" data-secondary="DataLoader" data-type="indexterm" id="id985"/><a contenteditable="false" data-primary="DataLoader" data-secondary="using a dataset" data-type="indexterm" id="id986"/>pass it to a <code>DataLoader</code>, and enumerate that:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Create an instance of CustomDataset</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">CustomDataset</code><code class="p">(</code><code class="n">x</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
 
<code class="c1"># Use DataLoader to handle batching and shuffling</code>
<code class="n">data_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
 
<code class="c1"># Iterate over the DataLoader</code>
<code class="k">for</code> <code class="n">batch_idx</code><code class="p">,</code> <code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">labels</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">data_loader</code><code class="p">):</code>
    <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Batch </code><code class="si">{</code><code class="n">batch_idx</code><code class="o">+</code><code class="mi">1</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Inputs:"</code><code class="p">,</code> <code class="n">inputs</code><code class="p">)</code>
    <code class="nb">print</code><code class="p">(</code><code class="s2">"Labels:"</code><code class="p">,</code> <code class="n">labels</code><code class="p">)</code>
    <code class="c1"># Break after the first batch for demonstration</code>
    <code class="k">if</code> <code class="n">batch_idx</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
        <code class="k">break</code></pre>
        <p>With this foundation, you can now explore the dataset classes that have been made available in the various libraries we’ve mentioned since the beginning of this chapter. Given that they will build on or extend this class, the APIs should look familiar.<a contenteditable="false" data-primary="" data-startref="ch4strt" data-type="indexterm" id="id987"/> </p>
      </div></section>
      <section data-pdf-bookmark="Exploring the FashionMNIST Class" data-type="sect1"><div class="sect1" id="ch04_exploring_the_fashion_mnist_class_1748548966496527">
        <h1>Exploring the FashionMNIST Class</h1>
        <p>Earlier in the book, we saw <a contenteditable="false" data-primary="datasets" data-secondary="Fashion MNIST database" data-tertiary="train parameter for training versus test/validation data" data-type="indexterm" id="id988"/><a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="train parameter for training versus test/validation data" data-type="indexterm" id="id989"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="train parameter for training versus test/validation data" data-type="indexterm" id="id990"/>the <code>FashionMNIST</code> class—which provides access to the Fashion-MNIST dataset, which is a training set of 60,000 examples of 10 classes of clothing—and an accompanying test set of 10,000 examples. Each of these examples is a 28 × 28 grayscale image. </p>
        <p>In the case of this dataset, you use the <em>same</em> class whether you’re using training data or test/validation data, and the data that you receive is based on the <code>train</code> parameter that you pass to it. Here’s an example:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Create the FashionMNIST dataset</code>
<code class="n">fashion_mnist_train</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">FashionMNIST</code><code class="p">(</code><code class="n">root</code><code class="o">=</code><code class="s1">'./data'</code><code class="p">,</code> <code class="n">train</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> 
                                   <code class="n">download</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">transform</code><code class="o">=</code><code class="n">transform</code><code class="p">)</code></pre>
        <p>When you set <code>train=True,</code> the code that overrides the class init method will take the 60,000 records and return them to the caller. Other parameters are there, like specifying the root for where the data should go and even whether or not to download the data. <a contenteditable="false" data-primary="datasets" data-secondary="transform parameter" data-type="indexterm" id="id991"/><a contenteditable="false" data-primary="Fashion MNIST database" data-secondary="transform parameter" data-type="indexterm" id="id992"/><a contenteditable="false" data-primary="MNIST (Modified National Institute of Standards and Technology) Fashion database" data-secondary="transform parameter" data-type="indexterm" id="id993"/>Finally, as you’ll commonly see when downloading data, there is the <code>transform=</code> parameter. As you saw in the preceding base class, this parameter will be available as an optional parameter for all datasets, and it will apply a transform when set.</p>
      </div></section>
      <section data-pdf-bookmark="Generic Dataset Classes" data-type="sect1"><div class="sect1" id="ch04_generic_dataset_classes_1748548966496578">
        <h1>Generic Dataset Classes</h1>
        <p>You may need to use some data<a contenteditable="false" data-primary="datasets" data-secondary="generic classes" data-type="indexterm" id="ch4gen"/><a contenteditable="false" data-primary="datasets" data-secondary="generic classes" data-tertiary="about" data-type="indexterm" id="id994"/> that isn’t available in the dataset classes, like <code>FashionMNIST</code>, but you’ll also want to take advantage of everything in the data ecosystem—such as the ability to transform your data, things like splitting, and all the good stuff in the <code>DataLoader</code> class you’ll see later in this chapter. To that end, <code>torch.utils.data</code> provides a number of generic dataset classes you could use.</p>
        <section class="pagebreak-before less_space" data-pdf-bookmark="ImageFolder" data-type="sect2"><div class="sect2" id="ch04_imagefolder_1748548966496626">
          <h2>ImageFolder</h2>
          <p>In <a data-type="xref" href="ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912">Chapter 3</a>, we used the “Horses or Humans,” “Rock, Paper, Scissors,” and “Cats vs. Dogs” datasets,<a contenteditable="false" data-primary="datasets" data-secondary="generic classes" data-tertiary="ImageFolder" data-type="indexterm" id="id995"/><a contenteditable="false" data-primary="ImageFolder" data-type="indexterm" id="id996"/><a contenteditable="false" data-primary="DataLoader" data-secondary="ImageFolder as" data-type="indexterm" id="id997"/><a contenteditable="false" data-primary="ImageFolder" data-secondary="as DataLoader" data-secondary-sortas="DataLoader" data-type="indexterm" id="id998"/> which are not available in a class directly but instead as a ZIP file containing the images. When we downloaded them and saved them into subdirectories for the different image types (e.g., one folder for “Horses” and another for “Humans”), the generic <code>ImageFolder</code> dataset class could act as a dataset for us.</p>
          <p>In that case, the images were<a contenteditable="false" data-primary="DataLoader" data-secondary="subdirectories assigned as image labels" data-type="indexterm" id="id999"/> streamed (via a <code>DataLoader</code>) from the directory according to the batch size and other rules on the <code>DataLoader</code>. The labels were derived from the directory names, and the associated class indices were the labels in alphabetical order. So, “Horses” would be class 0, and “Humans” would be class 1. Please watch out for that when building and debugging because you might miss out on this <span class="keep-together">ordering!</span> </p>
          <p>For example, we tend to say, “Rock, Paper, Scissors” in that order, and we would therefore expect them to be classes 0, 1, and 2, respectively. But in <em>alphabetical</em> order, Paper would be class 0, Rock would be class 1, and Scissors would be class 2!</p>
          <div data-type="tip"><h6>Tip</h6>
            <p>One tool to use for this is to create a custom index like the one below:<a contenteditable="false" data-primary="ImageFolder" data-secondary="custom index" data-type="indexterm" id="id1000"/></p>
            <pre data-code-language="python" data-type="programlisting"><code class="n">custom_class_to_idx</code> <code class="o">=</code> <code class="p">{</code><code class="s1">'rabbit'</code><code class="p">:</code> <code class="mi">0</code><code class="p">,</code> <code class="s1">'dog'</code><code class="p">:</code> <code class="mi">1</code><code class="p">,</code> <code class="s1">'cat'</code><code class="p">:</code> <code class="mi">2</code><code class="p">}</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">ImageFolder</code><code class="p">(</code>
  <code class="n">root</code><code class="o">=</code><code class="s1">'data/animals'</code><code class="p">,</code>
  <code class="n">target_transform</code><code class="o">=</code>
    <code class="k">lambda</code> <code class="n">x</code><code class="p">:</code> <code class="n">custom_class_to_idx</code><code class="p">[</code><code class="n">dataset</code><code class="o">.</code><code class="n">classes</code><code class="p">[</code><code class="n">x</code><code class="p">]]</code>
<code class="p">)</code>
<code class="n">dataset</code><code class="o">.</code><code class="n">class_to_idx</code> <code class="o">=</code> <code class="n">custom_class_to_idx</code>
<code class="nb">print</code><code class="p">(</code><code class="n">dataset</code><code class="o">.</code><code class="n">class_to_idx</code><code class="p">)</code></pre>
          </div>
        </div></section>
        <section data-pdf-bookmark="DatasetFolder" data-type="sect2"><div class="sect2" id="ch04_datasetfolder_1748548966496672">
          <h2>DatasetFolder</h2>
          <p><code>ImageFolder</code> is actually a subclass<a contenteditable="false" data-primary="datasets" data-secondary="generic classes" data-tertiary="DatasetFolder" data-type="indexterm" id="id1001"/><a contenteditable="false" data-primary="ImageFolder" data-secondary="subclass of DatasetFolder" data-type="indexterm" id="id1002"/><a contenteditable="false" data-primary="DatasetFolder" data-type="indexterm" id="id1003"/><a contenteditable="false" data-primary="DatasetFolder" data-secondary="directories for labels" data-type="indexterm" id="id1004"/> of the more generic <code>DatasetFolder</code> class, one that’s customized for images. The <code>DatasetFolder</code> class isn’t limited to image data, and you can use it for anything. It also allows you to use directories for labels. So, for example, say you have text files that contain text of different classes with a directory structure like this:</p>
          <pre data-code-language="python" data-type="programlisting"><code class="n">root</code><code class="o">/</code><code class="n">sarcasm</code><code class="o">/</code><code class="n">document1</code><code class="o">.</code><code class="n">txt</code>
<code class="n">root</code><code class="o">/</code><code class="n">sarcasm</code><code class="o">/</code><code class="n">document2</code><code class="o">.</code><code class="n">txt</code>
<code class="n">root</code><code class="o">/</code><code class="n">sarcasm</code><code class="o">/</code><code class="n">document3</code><code class="o">.</code><code class="n">txt</code>
<code class="n">root</code><code class="o">/</code><code class="n">factual</code><code class="o">/</code><code class="n">factdoc1</code><code class="o">.</code><code class="n">rtf</code>
<code class="n">root</code><code class="o">/</code><code class="n">factual</code><code class="o">/</code><code class="n">factdoc2</code><code class="o">.</code><code class="n">doc</code></pre>
          <p>You could then use a <code>DatasetFolder</code> to stream the documents according to the correct labels. <a contenteditable="false" data-primary="DatasetFolder" data-secondary="extracting from file via transform" data-type="indexterm" id="id1005"/>Also, because this class is document based, you could apply a transform to extract from the file!</p>
        </div></section>
        <section data-pdf-bookmark="FakeData" data-type="sect2"><div class="sect2" id="ch04_fakedata_1748548966496716">
          <h2>FakeData</h2>
          <p><code>FakeData</code> is a useful <a contenteditable="false" data-primary="datasets" data-secondary="generic classes" data-tertiary="FakeData" data-type="indexterm" id="id1006"/><a contenteditable="false" data-primary="FakeData" data-type="indexterm" id="id1007"/><a contenteditable="false" data-primary="FakeData" data-secondary="image data only" data-type="indexterm" id="id1008"/>generic dataset that, as its name suggests, provides you with fake data. At the time of writing, it only supports creating fake image data. It’s also very useful if you don’t have data on hand but want to experiment with different architectures, or if you want to benchmark your system.</p>
          <p>You can use <code>FakeData</code> in the same way you’d use any of the datasets you’ve seen so far in this book. So, for example, if you wanted to create a set of <code>FakeData</code> for the MobileNet model, which uses 224 × 224 color images, you’d do it with code like this:<a contenteditable="false" data-primary="DataLoader" data-secondary="FakeData" data-type="indexterm" id="id1009"/></p>
          <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">torch</code>
<code class="kn">from</code> <code class="nn">torchvision.datasets</code> <code class="kn">import</code> <code class="n">FakeData</code>
<code class="kn">import</code> <code class="nn">torchvision.transforms</code> <code class="k">as</code> <code class="nn">transforms</code>
<code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">DataLoader</code>
 
<code class="c1"># Define transformations (if needed)</code>
<code class="n">transform</code> <code class="o">=</code> <code class="n">transforms</code><code class="o">.</code><code class="n">Compose</code><code class="p">([</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">ToTensor</code><code class="p">(),</code>  
    <code class="n">transforms</code><code class="o">.</code><code class="n">Normalize</code><code class="p">((</code><code class="mf">0.5</code><code class="p">,),</code> <code class="p">(</code><code class="mf">0.5</code><code class="p">,))</code>  
<code class="p">])</code>
 
<code class="c1"># Create FakeData</code>
<code class="n">fake_dataset</code> <code class="o">=</code> <code class="n">FakeData</code><code class="p">(</code><code class="n">size</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">image_size</code><code class="o">=</code><code class="p">(</code><code class="mi">3</code><code class="p">,</code> <code class="mi">224</code><code class="p">,</code> <code class="mi">224</code><code class="p">),</code> 
                        <code class="n">num_classes</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">transform</code><code class="o">=</code><code class="n">transform</code><code class="p">)</code>
 
<code class="c1"># DataLoader</code>
<code class="n">data_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">fake_dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>
          <p>This would create 100 images (containing just noise) of the desired size and span them across 10 classes. You could then use this data in a <code>DataLoader</code> in the same way you’d use any other dataset.</p>
          <p>While <code>FakeData</code> only gives image types, you could relatively easily create your own CustomData (as we looked at earlier) to provide fake data in other formats, such as numeric or sequence data.<a contenteditable="false" data-primary="" data-startref="ch4gen" data-type="indexterm" id="id1010"/></p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Using Custom Splits" data-type="sect1"><div class="sect1" id="ch04_using_custom_splits_1748548966496763">
        <h1>Using Custom Splits</h1>
        <p>Up to this point, all of the data<a contenteditable="false" data-primary="datasets" data-secondary="custom splits" data-type="indexterm" id="id1011"/><a contenteditable="false" data-primary="APIs" data-secondary="custom splits of datasets" data-type="indexterm" id="id1012"/> you’ve been using to build models has been pre-split for you into training and test sets. For example, with Fashion MNIST, you had 60,000 and 10,000 records, respectively. But what if you don’t want to use those splits? What if you want to split the data yourself, according to your own needs? </p>
        <p>Thankfully, when using datasets, you can generally do this with an easy and intuitive API.</p>
        <p>So, for example, when you loaded the <code>FashionMNIST</code> class previously, you specified the <code>train</code> parameter to get it to give you the training data (60,000 records) or the test data (10,000 records). </p>
        <p>To override this, you simply ignore it, and you’ll get all of the data:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Load the entire Fashion-MNIST dataset</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">FashionMNIST</code><code class="p">(</code><code class="n">root</code><code class="o">=</code><code class="s1">'./data'</code><code class="p">,</code> 
                                <code class="n">download</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">transform</code><code class="o">=</code><code class="n">transform</code><code class="p">)</code></pre>
        <p>To create your own split, you can use<a contenteditable="false" data-primary="torch.utils.data namespace" data-secondary="custom splits of datasets" data-type="indexterm" id="id1013"/> the <code>torch.utils.data</code> namespace, which contains a function called <code>random_split</code>. So, for example, if you want to have a validation set that <code>FashionMNIST</code> doesn’t provide, you can divide the dataset into three datasets with <code>random_split</code>. Here’s the code that will assign 70% of the data to a training set, 15% to a testing set, and 15% to a validation set:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">torch.utils.data</code> <code class="kn">import</code> <code class="n">random_split</code>
 
<code class="n">total_count</code> <code class="o">=</code> <code class="nb">len</code><code class="p">(</code><code class="n">dataset</code><code class="p">)</code>
<code class="n">train_count</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="mf">0.7</code> <code class="o">*</code> <code class="n">total_count</code><code class="p">)</code>
<code class="n">val_count</code> <code class="o">=</code> <code class="nb">int</code><code class="p">(</code><code class="mf">0.15</code> <code class="o">*</code> <code class="n">total_count</code><code class="p">)</code>
 
<code class="c1"># Ensures all data is used</code>
<code class="n">test_count</code> <code class="o">=</code> <code class="n">total_count</code> <code class="err">–</code> <code class="n">train_count</code> <code class="err">–</code> <code class="n">val_count</code>  
 
<code class="n">train_dataset</code><code class="p">,</code> <code class="n">val_dataset</code><code class="p">,</code> <code class="n">test_dataset</code> <code class="o">=</code> 
     <code class="n">random_split</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code> <code class="p">[</code><code class="n">train_count</code><code class="p">,</code> <code class="n">val_count</code><code class="p">,</code> <code class="n">test_count</code><code class="p">])</code></pre>
        <p>As you can see, this process is pretty straightforward. We get the number of records in the dataset as <code>total_count</code> and then calculate 70% of them (0.7 times the total count) to be the training count and 15% of them to be the validation count. When making calculations like this, you may end up with rounding errors that leave some records out—so instead of using 15% for the test count, you can just set the quotient for training to be the total minus the training and validation records. This will ensure all the data is used and none is wasted.</p>
        <p>What’s really nice about this approach is that it gives you a really simple way to get new and different slices of your dataset. As you train models, it gives you a new way to evaluate them for accuracy. </p>
        <p>For example, one slice of the dataset may train at high accuracy while another does so at low accuracy, indicating that there are likely issues in your model architecture that make it overfit on one dataset. On the other hand, if you try multiple different splits of your data and the model training and validation results are consistent, then you’ll have a signal that your architecture is sound.</p>
        <p>I would definitely encourage you to use custom splits when training models, as it can really help you get over some gotchas!</p>
        <p>One more thing to consider when<a contenteditable="false" data-primary="datasets" data-secondary="custom splits" data-tertiary="random_split explained" data-type="indexterm" id="id1014"/> using custom splits is that the name <code>random</code> doesn’t mean that this approach <em>shuffles</em> or randomizes your dataset. It merely slices the dataset at random points to give you different slices each time. Should you want to also shuffle the dataset, you can do it in the DataLoader, which we’ll explore in the next section.</p>
      </div></section>
      <section data-pdf-bookmark="The ETL Process for Managing Data in Machine Learning" data-type="sect1"><div class="sect1" id="ch04_the_etl_process_for_managing_data_in_machine_learn_1748548966496814">
        <h1>The ETL Process for Managing Data in Machine Learning</h1>
        <p><em>Extract, Transfer, Load</em> (ETL) is the core pattern<a contenteditable="false" data-primary="datasets" data-secondary="ETL process for managing" data-type="indexterm" id="id1015"/><a contenteditable="false" data-primary="Extract, Transfer, Load (ETL) process" data-type="indexterm" id="id1016"/> for training ML models, regardless of scale. We’ve been exploring small-scale, single-computer model building in this book, but we can use the same technology for large-scale training across multiple machines with massive datasets.</p>
        <p>The Extract, Transfer, Load process consists of the three phases that are in the process’s name:</p>
        <dl>
          <dt>Extract phase </dt>
          <dd>
            <p>This occurs when the raw data is loaded from wherever it is stored and prepared in a way that can be transformed. </p>
          </dd>
          <dt>Transform phase </dt>
          <dd>
            <p>This occurs when the data is manipulated in a way that makes it suitable or improved for training. For example, batching, image augmentation, mapping to feature columns, and other such logic applied to the data can be considered part of this phase. </p>
          </dd>
          <dt>Load phase </dt>
          <dd>
            <p>This occurs when the data is loaded into the neural network for training.</p>
          </dd>
        </dl>
        <p>Consider the code from <a data-type="xref" href="ch03.html#ch03_going_beyond_the_basics_detecting_features_in_ima_1748570891074912">Chapter 3</a> that we used to train the “Horses or Humans” classifier. At the top of the code, you saw a chunk like this:<a contenteditable="false" data-primary="horses versus humans distinguished" data-secondary="Extract, Transfer, Load process" data-type="indexterm" id="id1017"/></p>
        <pre data-code-language="python" data-type="programlisting"><code class="c1"># Define transformations</code>
<code class="n">train_transform</code> <code class="o">=</code> <code class="n">transforms</code><code class="o">.</code><code class="n">Compose</code><code class="p">([</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">Resize</code><code class="p">((</code><code class="mi">150</code><code class="p">,</code><code class="mi">150</code><code class="p">)),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">RandomHorizontalFlip</code><code class="p">(),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">RandomRotation</code><code class="p">(</code><code class="mi">20</code><code class="p">),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">RandomAffine</code><code class="p">(</code>
        <code class="n">degrees</code><code class="o">=</code><code class="mi">0</code><code class="p">,</code>  <code class="c1"># No rotation</code>
        <code class="n">translate</code><code class="o">=</code><code class="p">(</code><code class="mf">0.2</code><code class="p">,</code> <code class="mf">0.2</code><code class="p">),</code>  <code class="c1"># Translate up to 20% x and y</code>
        <code class="n">scale</code><code class="o">=</code><code class="p">(</code><code class="mf">0.8</code><code class="p">,</code> <code class="mf">1.2</code><code class="p">),</code>  <code class="c1"># Zoom in or out by 20%</code>
        <code class="n">shear</code><code class="o">=</code><code class="mi">20</code><code class="p">,</code>  <code class="c1"># Shear by up to 20 degrees</code>
    <code class="p">),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">ToTensor</code><code class="p">(),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">Normalize</code><code class="p">(</code><code class="n">mean</code><code class="o">=</code><code class="p">[</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">],</code> <code class="n">std</code><code class="o">=</code><code class="p">[</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">]),</code>
<code class="p">])</code>
 
<code class="c1"># Load the datasets</code>
<code class="n">train_dataset</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">ImageFolder</code><code class="p">(</code><code class="n">root</code><code class="o">=</code><code class="n">training_dir</code><code class="p">,</code> 
                                     <code class="n">transform</code><code class="o">=</code><code class="n">train_transform</code><code class="p">)</code>
<code class="n">val_dataset</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">ImageFolder</code><code class="p">(</code><code class="n">root</code><code class="o">=</code><code class="n">validation_dir</code><code class="p">,</code> 
                                   <code class="n">transform</code><code class="o">=</code><code class="n">train_transform</code><code class="p">)</code>
 
<code class="c1"># Data loaders</code>
<code class="n">train_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">train_dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code>
<code class="n">val_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">val_dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">32</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">)</code></pre>
        <p>This is the ETL pattern embodied in code! </p>
        <p>The code begins by defining the <code>transform</code> (the “T”), but the active code doesn’t begin until the lines under the <code>Load the datasets</code> comment. <a contenteditable="false" data-primary="DataLoader" data-secondary="ImageFolder as" data-type="indexterm" id="id1018"/><a contenteditable="false" data-primary="ImageFolder" data-secondary="as DataLoader" data-secondary-sortas="DataLoader" data-type="indexterm" id="id1019"/>Look carefully here and you’ll see that the <code>ImageFolder</code> is being used to <em>extract</em> the data from its location at rest on disk. </p>
        <p>Then, as the data is extracted, the <code>transform</code> that we defined is applied.</p>
        <p>Then, under the <code>Data loaders</code> comment, we perform the <em>load</em> of the data using the <code>train_loader</code> and <code>val_loader</code> we’ve defined. Strictly speaking, the actual loading doesn’t take place until we execute the training loop to pull the data out of the <span class="keep-together">loaders.</span></p>
        <p>It’s important to know that using this process can make your data pipelines less susceptible to changes in the data and the underlying schema. When you use this approach to extract data, the same underlying structure is used regardless of whether the data is small enough to fit in memory or large enough that it cannot be contained even on a simple machine. The APIs for applying the transformation are also consistent, so you can use similar ones regardless of the underlying data source. And of course, once the data is transformed, the process of loading the data is also consistent, regardless of your training backend.</p>
        <p>However, how you load the data can have a huge impact on your training speed. Let’s take a look at that next.</p>
      </div></section>
      <section data-pdf-bookmark="Optimizing the Load Phase" data-type="sect1"><div class="sect1" id="ch04_optimizing_the_load_phase_1748548966496863">
        <h1>Optimizing the Load Phase</h1>
        <p>Let’s take a closer look at the ETL process<a contenteditable="false" data-primary="datasets" data-secondary="ETL process for managing" data-tertiary="optimizing the Load phase" data-type="indexterm" id="id1020"/><a contenteditable="false" data-primary="Extract, Transfer, Load (ETL) process" data-secondary="optimizing the Load phase" data-type="indexterm" id="id1021"/><a contenteditable="false" data-primary="CPU (central processing unit)" data-secondary="extraction and transformation of ETL process" data-type="indexterm" id="id1022"/><a contenteditable="false" data-primary="GPU (graphics processing unit)" data-secondary="handing Load of ETL process" data-type="indexterm" id="id1023"/><a contenteditable="false" data-primary="TPU (tensor processing unit)" data-secondary="handing Load of ETL process" data-type="indexterm" id="id1024"/> when you’re training a model. We can consider the extraction and transformation of the data to be possible on any processor, including a CPU. In fact, the code you use in these phases to perform tasks like downloading data, unzipping it, and going through it record by record and processing those records is not what GPUs and TPUs are built for, so the code will likely execute on the CPU anyway. When it comes to training, however, you can get great benefits from a GPU or TPU, so it makes sense for you to use one for this phase if possible. Thus, in a situation where a GPU or TPU is available to you, you should ideally split the workload between the CPU and the GPU/TPU, with Extract and Transform taking place on the CPU and Load taking place on the GPU/TPU.</p>
        <p>If you explore the code that<a contenteditable="false" data-primary="model.to(device) for accelerator" data-type="indexterm" id="id1025"/><a contenteditable="false" data-primary="PyTorch" data-secondary="model.to(device) for accelerator" data-type="indexterm" id="id1026"/><a contenteditable="false" data-primary="accelerators" data-secondary="model.to(device) for accelerator" data-type="indexterm" id="id1027"/><a contenteditable="false" data-primary=".to(device) for accelerator" data-primary-sortas="to(device)" data-type="indexterm" id="id1028"/><a contenteditable="false" data-primary="Cuda (cu)" data-secondary="model.to(“cuda”) for accelerator" data-type="indexterm" id="id1029"/><a contenteditable="false" data-primary="GPU (graphics processing unit)" data-secondary=".to(device) for accelerator" data-type="indexterm" id="id1030"/> you’ve been using in this book, you’ll notice that we’ve used the .<code>to(device)</code> methodology. Whenever you’re dealing with training or inference and you want the data or model to be on the accelerator, you’ll see something like .<code>to(“cuda”)</code>, but for the extraction and transform, you won’t see it because it would be a waste of the GPU’s resources.</p>
        <p>Suppose you’re working with a large dataset.<a contenteditable="false" data-primary="batching data" data-secondary="pipelining" data-type="indexterm" id="id1031"/> Assuming it’s so large that you have to prepare the data (i.e., do the extraction and transformation) in batches, you’ll end up with a situation like that shown in <a data-type="xref" href="#ch04_figure_1_1748548966489345">Figure 4-1</a>. While the first batch is being prepared, the GPU or TPU is idle. Then, when that batch is ready, you can send it to the GPU/TPU for training, but then the CPU will be idle until the training is done and it can start preparing the second batch. There’s a lot of idle time here, so we can see that there’s room for optimization.</p>
        <figure><div class="figure" id="ch04_figure_1_1748548966489345">
          <img alt="Training on a CPU/GPU" src="assets/aiml_0401.png"/>
          <h6><span class="label">Figure 4-1. </span>Training on a CPU or GPU/TPU</h6>
        </div></figure>
        <p>The logical solution is to do the <a contenteditable="false" data-primary="pipelining in Load phase of ETL process" data-type="indexterm" id="id1032"/><a contenteditable="false" data-primary="DataLoader" data-secondary="parallel data loading" data-tertiary="pipelining" data-type="indexterm" id="id1033"/>work in parallel, preparing and training side by side. This process is called <em>pipelining</em> and is illustrated in <a data-type="xref" href="#ch04_figure_2_1748548966489381">Figure 4-2</a>.</p>
        <figure><div class="figure" id="ch04_figure_2_1748548966489381">
          <img alt="" src="assets/aiml_0402.png"/>
          <h6><span class="label">Figure 4-2. </span>Pipelining</h6>
        </div></figure>
        <p>In this case, while the CPU is preparing the first batch, the GPU/TPU again has nothing to work on, so it’s idle. When the first batch is done, the GPU/TPU can start training—but in parallel with this, the CPU will prepare the second batch. Of course, the time it takes to train batch <em>n</em> – 1 and prepare batch <em>n</em> won’t always be the same, and if the training time is shorter, you’ll have periods of idle time on the GPU/TPU, and if the training time is longer, you’ll have periods of idle time on the CPU. Choosing the correct batch size can help you optimize here—and as GPU/TPU time is likely more expensive, you’ll probably want to reduce its idle time as much as possible.</p>
        <p>This is one of the reasons why we use batching, even for the simple examples like MNIST: the pipelining model is in place so that regardless of how large your dataset is, you’ll continue to use a consistent pattern for ETL on it.</p>
      </div></section>
      <section data-pdf-bookmark="Using the DataLoader Class" data-type="sect1"><div class="sect1" id="ch04_using_the_dataloader_class_1748548966496910">
        <h1>Using the DataLoader Class</h1>
        <p>We’ve seen the <code>DataLoader</code> class many<a contenteditable="false" data-primary="DataLoader" data-type="indexterm" id="id1034"/> times already, but it’s good to take a slightly deeper look at it now to help you get the most out of it in your ML workflows. It provides the following features that you can use.</p>
        <section data-pdf-bookmark="Batching" data-type="sect2"><div class="sect2" id="ch04_batching_1748548966496954">
          <h2>Batching</h2>
          <p>Intuitively, you might think that the<a contenteditable="false" data-primary="DataLoader" data-secondary="batching" data-type="indexterm" id="id1035"/><a contenteditable="false" data-primary="stochastic gradient descent (sgd) optimizer" data-secondary="better when inputs in batches" data-type="indexterm" id="id1036"/><a contenteditable="false" data-primary="sgd (stochastic gradient descent) optimizer" data-secondary="better when inputs in batches" data-type="indexterm" id="id1037"/><a contenteditable="false" data-primary="batching data" data-secondary="DataLoader" data-type="indexterm" id="id1038"/> forward pass works one data item at a time. You <em>could</em> do that, but some optimizers, like stochastic gradient descent, do much better when inputs are passed in batches so that they can calculate more accurately. Batching can also speed up your training in larger scenarios, where you are using GPUs with fixed memory sizes. It’s most efficient to maximize the use of that memory by having a batch of data that fits it fully. If you’re using a <code>DataLoader</code>, batching is simply a matter of setting a parameter.</p>
        </div></section>
        <section data-pdf-bookmark="Shuffling" data-type="sect2"><div class="sect2" id="ch04_shuffling_1748548966496997">
          <h2>Shuffling</h2>
          <p>Shuffling the data is very important,<a contenteditable="false" data-primary="DataLoader" data-secondary="shuffling data" data-type="indexterm" id="id1039"/><a contenteditable="false" data-primary="shuffling data with DataLoader" data-type="indexterm" id="id1040"/> particularly when you do batching. Consider the following scenario with something like Fashion MNIST. </p>
          <p>You have 60,000 samples each for 10 classes, and they are not shuffled. You batch one thousand records at a time, so your first batch of one thousand is all for class 0, the second batch is all for class 1, and so on. In this scenario, the model may not effectively learn because each batch is biased toward a particular label—but the ability of your model to generalize will improve if the batches are shuffled, meaning your first thousand items will have varied labels, etc.</p>
        </div></section>
        <section data-pdf-bookmark="Parallel Data Loading" data-type="sect2"><div class="sect2" id="ch04_parallel_data_loading_1748548966497039">
          <h2>Parallel Data Loading</h2>
          <p>Often, and in particular with<a contenteditable="false" data-primary="DataLoader" data-secondary="parallel data loading" data-type="indexterm" id="id1041"/><a contenteditable="false" data-primary="parallel data loading by DataLoader" data-type="indexterm" id="id1042"/> complex data, loading data into the model for the forward pass can be time-consuming. But the <code>DataLoader</code> class offers parallelism through Python’s multiprocessing model, which can significantly speed this up. </p>
          <p>As you saw previously, you should consider your data loading/transformation and your model learning to be two separate processes. You want to avoid scenarios where the model training has no data to work with and is sitting idle, waiting for data to be loaded, and you also want to avoid scenarios where you have tons of data piled up in memory but the model can’t get to it. Parallel data loading, when well tuned, can be helpful here, and there’s a skill you can learn to ensure that you’re getting the most out of your training by running this at peak efficiency. You’ll learn how to do this in the next section.</p>
        </div></section>
        <section data-pdf-bookmark="Custom Data Sampling" data-type="sect2"><div class="sect2" id="ch04_custom_data_sampling_1748548966497082">
          <h2>Custom Data Sampling</h2>
          <p>In addition to shuffling for <a contenteditable="false" data-primary="random data sampling via shuffling" data-type="indexterm" id="id1043"/><a contenteditable="false" data-primary="DataLoader" data-secondary="shuffling data" data-tertiary="random data sampling via" data-type="indexterm" id="id1044"/><a contenteditable="false" data-primary="DataLoader" data-secondary="custom data sampling" data-type="indexterm" id="id1045"/><a contenteditable="false" data-primary="custom data sampling" data-type="indexterm" id="id1046"/><a contenteditable="false" data-primary="DataLoader" data-secondary="custom data sampling" data-tertiary="torch.utils.data.Sampler base class" data-type="indexterm" id="id1047"/><a contenteditable="false" data-primary="custom data sampling" data-secondary="torch.utils.data.Sampler base class" data-type="indexterm" id="id1048"/><a contenteditable="false" data-primary="torch.utils.data.Sampler base class for custom data sampling" data-type="indexterm" id="id1049"/>random data sampling, you can create custom <span class="keep-together"> data sampling,</span> in which you can specify how the data will be loaded. The <code>torch.utils.data.Sampler</code> class provides a base class that you can build a custom sampler on. This process is beyond the scope of this book, but there are many excellent examples of it online.</p>
        </div></section>
      </div></section>
      <section data-pdf-bookmark="Parallelizing ETL to Improve Training Performance" data-type="sect1"><div class="sect1" id="ch04_parallelizing_etl_to_improve_training_performance_1748548966497129">
        <h1>Parallelizing ETL to Improve Training Performance</h1>
        <p>If you’re using the <code>DataLoader</code> class,<a contenteditable="false" data-primary="Extract, Transfer, Load (ETL) process" data-secondary="parallelizing to improve training performance" data-type="indexterm" id="id1050"/><a contenteditable="false" data-primary="DataLoader" data-secondary="parallel data loading" data-tertiary="num_workers parameter for" data-type="indexterm" id="id1051"/><a contenteditable="false" data-primary="parallel data loading by DataLoader" data-secondary="num_workers parameter" data-type="indexterm" id="id1052"/><a contenteditable="false" data-primary="datasets" data-secondary="CIFAR10" data-type="indexterm" id="id1053"/><a contenteditable="false" data-primary="training" data-secondary="parallelizing ETL to improve training performance" data-type="indexterm" id="id1054"/> you can easily perform parallelization by using the <code>num_workers</code> parameter. So, for example, say you want to train a model on the <code>CIFAR10</code> dataset, and you want to use parallel training. Let’s take a look at how to do this, step-by-step.</p>
        <p>First, we’ll explore the Extract and Transform steps:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">torchvision.transforms</code> <code class="k">as</code> <code class="nn">transforms</code>
<code class="kn">from</code> <code class="nn">torchvision.datasets</code> <code class="kn">import</code> <code class="n">CIFAR10</code>
 
<code class="c1"># Define transformations</code>
<code class="n">transform</code> <code class="o">=</code> <code class="n">transforms</code><code class="o">.</code><code class="n">Compose</code><code class="p">([</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">ToTensor</code><code class="p">(),</code>
    <code class="n">transforms</code><code class="o">.</code><code class="n">Normalize</code><code class="p">((</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">),</code> <code class="p">(</code><code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">))</code>
<code class="p">])</code>
 
<code class="c1"># Load CIFAR10 dataset</code>
<code class="n">dataset</code> <code class="o">=</code> <code class="n">CIFAR10</code><code class="p">(</code><code class="n">root</code><code class="o">=</code><code class="s1">'./data'</code><code class="p">,</code> <code class="n">train</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">download</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> <code class="n">transform</code><code class="o">=</code><code class="n">transform</code><code class="p">)</code></pre>
        <p>Then, we’ll configure and create the DataLoader to the Load step:</p>
        <pre data-code-language="python" data-type="programlisting"><code class="kn">from</code> <code class="nn">torch</code><code class="nn">.</code><code class="nn">utils</code><code class="nn">.</code><code class="nn">data</code> <code class="kn">import</code> <code class="n">DataLoader</code>
 
<code class="c1"># DataLoader with multiple workers</code>
<code class="n">data_loader</code> <code class="o">=</code> <code class="n">DataLoader</code><code class="p">(</code><code class="n">dataset</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="mi">64</code><code class="p">,</code> <code class="n">shuffle</code><code class="o">=</code><code class="kc">True</code><code class="p">,</code> 
                         <strong><code class="n">num_workers</code><code class="o">=</code></strong><strong><code class="mi">4</code></strong><code class="p">)</code></pre>
        <p>Note the <code>num_workers=4</code> parameter, which will create four subprocesses to load the data in parallel simultaneously. Based on the hardware you have available and the number of cores, the speed of your CPU, etc., you can experiment with this number to reduce the overall bottlenecks.</p>
        <p>What’s really nice about this approach is that the ETL process is neatly encapsulated in it, so your model training loop doesn’t have to change in any way, even though you’re loading the data by using parallelism! Here’s the code for the simple <code>CIFAR</code> model that uses this data:</p>
        <pre class="pagebreak-before" data-code-language="python" data-type="programlisting"><code class="kn">import</code> <code class="nn">torch</code>
 
<code class="c1"># Dummy model and optimizer setup</code>
<code class="n">model</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Sequential</code><code class="p">(</code>
    <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">3</code> <code class="o">*</code> <code class="mi">32</code> <code class="o">*</code> <code class="mi">32</code><code class="p">,</code> <code class="mi">500</code><code class="p">),</code>
    <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">ReLU</code><code class="p">(),</code>
    <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">Linear</code><code class="p">(</code><code class="mi">500</code><code class="p">,</code> <code class="mi">10</code><code class="p">)</code>
<code class="p">)</code>
<code class="n">optimizer</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">optim</code><code class="o">.</code><code class="n">Adam</code><code class="p">(</code><code class="n">model</code><code class="o">.</code><code class="n">parameters</code><code class="p">(),</code> <code class="n">lr</code><code class="o">=</code><code class="mf">0.001</code><code class="p">)</code>
<code class="n">criterion</code> <code class="o">=</code> <code class="n">torch</code><code class="o">.</code><code class="n">nn</code><code class="o">.</code><code class="n">CrossEntropyLoss</code><code class="p">()</code>
 
<code class="c1"># Training loop</code>
<code class="k">def</code> <code class="nf">train</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">data_loader</code><code class="p">):</code>
    <code class="n">model</code><code class="o">.</code><code class="n">train</code><code class="p">()</code>
    <code class="k">for</code> <code class="n">batch_idx</code><code class="p">,</code> <code class="p">(</code><code class="n">inputs</code><code class="p">,</code> <code class="n">targets</code><code class="p">)</code> <code class="ow">in</code> <code class="nb">enumerate</code><code class="p">(</code><code class="n">data_loader</code><code class="p">):</code>
        <code class="c1"># Reshape inputs to match the model's expected input</code>
        <code class="n">inputs</code> <code class="o">=</code> <code class="n">inputs</code><code class="o">.</code><code class="n">view</code><code class="p">(</code><code class="n">inputs</code><code class="o">.</code><code class="n">size</code><code class="p">(</code><code class="mi">0</code><code class="p">),</code> <code class="err">–</code><code class="mi">1</code><code class="p">)</code>
 
        <code class="c1"># Forward pass</code>
        <code class="n">outputs</code> <code class="o">=</code> <code class="n">model</code><code class="p">(</code><code class="n">inputs</code><code class="p">)</code>
        <code class="n">loss</code> <code class="o">=</code> <code class="n">criterion</code><code class="p">(</code><code class="n">outputs</code><code class="p">,</code> <code class="n">targets</code><code class="p">)</code>
 
        <code class="c1"># Backward pass and optimize</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">zero_grad</code><code class="p">()</code>
        <code class="n">loss</code><code class="o">.</code><code class="n">backward</code><code class="p">()</code>
        <code class="n">optimizer</code><code class="o">.</code><code class="n">step</code><code class="p">()</code>
 
        <code class="k">if</code> <code class="n">batch_idx</code> <code class="o">%</code> <code class="mi">100</code> <code class="o">==</code> <code class="mi">0</code><code class="p">:</code>
            <code class="nb">print</code><code class="p">(</code><code class="sa">f</code><code class="s2">"Train Epoch: </code><code class="si">{</code><code class="n">batch_idx</code><code class="si">}</code><code class="s2"> Loss: </code><code class="si">{</code><code class="n">loss</code><code class="o">.</code><code class="n">item</code><code class="p">()</code><code class="si">}</code><code class="s2">"</code><code class="p">)</code>
 
<code class="n">train</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">data_loader</code><code class="p">)</code></pre>
        <p>Parallelizing is another tool that’s available to you when you’re training your models. There’s no one-size-fits-all approach, but it’s a good tool to use when you experience training slowdowns. It’s easy to assume that training operates slowly just because of the network architecture, but you may be surprised at how much of that time is wasted by the forward pass waiting for new data! By adding this type of parallelism, you have the potential to greatly speed up training.</p>
      </div></section>
      <section data-pdf-bookmark="Summary" data-type="sect1"><div class="sect1" id="ch04_summary_1748548966497173">
        <h1>Summary</h1>
        <p>This chapter covered the data ecosystem in PyTorch and introduced you to the <code>dataset</code> and <code>DataLoader</code> classes. You saw how they use a common API and a common format to help reduce the amount of code you have to write to get access to data, and you also saw how to use the ETL process, which is at the heart of the common design patterns in training models with PyTorch. In particular, we explored parallelizing the extraction, transformation, and loading of data to improve training performance. </p>
        <p>So now that you’ve had a chance to look at the process, see if you can create your own dataset! Maybe do it from some photos in your albums, or some test, or just random noise like we did here.</p>
        <p>In the next chapter, you’ll take what you’ve learned so far and start applying it to natural language processing problems.</p>
      </div></section>
    </div></section></body></html>