["```py\nOPENAI_API_KEY=\nSUPABASE_URL=\nSUPABASE_SERVICE_ROLE_KEY=\n\n# for tracing\nLANGCHAIN_TRACING_V2=true\nLANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\nLANGCHAIN_API_KEY=\n```", "```py\n## Enable the pgvector extension to work with embedding vectors\ncreate extension vector;\n```", "```py\n## Create a table to store your documents\n\ncreate table documents (\n  id bigserial primary key,\n  content text, -- corresponds to Document.pageContent\n  metadata jsonb, -- corresponds to Document.metadata\n  embedding vector(1536) -- 1536 works for OpenAI embeddings, change if needed\n);\n```", "```py\n## Create a function to search for documents\ncreate function match_documents (\n  query_embedding vector(1536),\n  match_count int DEFAULT null,\n  filter jsonb DEFAULT '{}'\n) returns table (\n  id bigint,\n  content text,\n  metadata jsonb,\n  embedding jsonb,\n  similarity float\n)\nlanguage plpgsql\nas $$\n#variable_conflict use_column\nbegin\n  return query\n  select\n    id,\n    content,\n    metadata,\n    (embedding::text)::jsonb as embedding,\n    1 - (documents.embedding <=> query_embedding) as similarity\n  from documents\n  where metadata @> filter\n  order by documents.embedding <=> query_embedding\n  limit match_count;\nend;\n$$;\n```", "```py\nimport os\n\nfrom langchain_community.vectorstores import SupabaseVectorStore\nfrom langchain_openai import OpenAIEmbeddings\nfrom supabase.client import Client, create_client\n\nsupabase_url = os.environ.get(\"SUPABASE_URL\")\nsupabase_key = os.environ.get(\"SUPABASE_SERVICE_ROLE_KEY\")\nsupabase: Client = create_client(supabase_url, supabase_key)\n\nembeddings = OpenAIEmbeddings()\n\n## Assuming you've already generated embeddings of your data\n\nvector_store = SupabaseVectorStore(\n    embedding=embeddings,\n    client=supabase,\n    table_name=\"documents\",\n    query_name=\"match_documents\",\n)\n\n## Test that similarity search is working\n\nquery = \"What is this document about?\"\nmatched_docs = vector_store.similarity_search(query)\n\nprint(matched_docs[0].page_content)\n```", "```py\nimport {\n  SupabaseVectorStore\n} from \"@langchain/community/vectorstores/supabase\";\nimport { OpenAIEmbeddings } from \"@langchain/openai\";\n\nimport { createClient } from \"@supabase/supabase-js\";\n\nconst embeddings = new OpenAIEmbeddings();\n\nconst supabaseClient = createClient(\n  process.env.SUPABASE_URL,\n  process.env.SUPABASE_SERVICE_ROLE_KEY\n);\n\nconst vectorStore = new SupabaseVectorStore(embeddings, {\n  client: supabaseClient,\n  tableName: \"documents\",\n  queryName: \"match_documents\",\n});\n\n// Example documents structure of your data\n\nconst document1: Document = {\n  pageContent: \"The powerhouse of the cell is the mitochondria\",\n  metadata: { source: \"https://example.com\" },\n};\n\nconst document2: Document = {\n  pageContent: \"Buildings are made out of brick\",\n  metadata: { source: \"https://example.com\" },\n};\n\nconst documents = [document1, document2]\n\n//Embed and store the data in the database\n\nawait vectorStore.addDocuments(documents, { ids: [\"1\", \"2\"] });\n\n// Query the Vector Store\n\nconst filter = { source: \"https://example.com\" };\n\nconst similaritySearchResults = await vectorStore.similaritySearch(\n  \"biology\",\n  2,\n  filter\n);\n\nfor (const doc of similaritySearchResults) {\n  console.log(`* ${doc.pageContent} [${JSON.stringify(doc.metadata, null)}]`);\n}\n\n```", "```py\nThe powerhouse of the cell is the mitochondria [{\"source\":\"https://example.com\"}]\n\n```", "```py\n{\n    \"dependencies\": [\"./my_agent\"],\n    \"graphs\": {\n        \"agent\": \"./my_agent/agent.py:graph\"\n    },\n    \"env\": \".env\"\n}\n```", "```py\nmy-app/\n├── my_agent # all project code lies within here\n│   ├── utils # utilities for your graph\n│   │   ├── __init__.py\n│   │   ├── tools.py # tools for your graph\n│   │   ├── nodes.py # node functions for you graph\n│   │   └── state.py # state definition of your graph\n│   ├── requirements.txt # package dependencies\n│   ├── __init__.py\n│   └── agent.py # code for constructing your graph\n├── .env # environment variables\n└── langgraph.json # configuration file for LangGraph\n```", "```py\npip install -U \"langgraph-cli[inmem]\"\n```", "```py\nnpm i @langchain/langgraph-cli\n\n```", "```py\nlanggraph dev\n```", "```py\nReady!\nAPI: http://localhost:2024\nDocs: http://localhost:2024/docs\n```", "```py\ncurl --request POST \\\n    --url http://localhost:8123/runs/stream \\\n    --header 'Content-Type: application/json' \\\n    --data '{\n    \"assistant_id\": \"agent\",\n    \"input\": {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"How are you?\"\n            }\n        ]\n    },\n    \"metadata\": {},\n    \"config\": {\n        \"configurable\": {}\n    },\n    \"multitask_strategy\": \"reject\",\n    \"stream_mode\": [\n        \"values\"\n    ]\n}'\n```", "```py\nfrom langgraph_sdk import get_client\n\n# only pass the url argument to get_client() if you changed the default port \n# when calling langgraph up\nclient = get_client()\n# Using the graph deployed with the name \"agent\"\nassistant_id = \"agent\"\nthread = await client.threads.create()\n\ninput = {\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf\"}]}\nasync for chunk in client.runs.stream(\n    thread[\"thread_id\"],\n    assistant_id,\n    input=input,\n    stream_mode=\"updates\",\n):\n    print(f\"Receiving new event of type: {chunk.event}...\")\n    print(chunk.data)\n    print(\"\\n\\n\")\n```", "```py\nimport { Client } from \"@langchain/langgraph-sdk\";\n\n// only set the apiUrl if you changed the default port when calling langgraph up\nconst client = new Client();\n// Using the graph deployed with the name \"agent\"\nconst assistantId = \"agent\";\nconst thread = await client.threads.create();\n\nconst input = {\n  messages: [{ \"role\": \"user\", \"content\": \"what's the weather in sf\"}]\n}\n\nconst streamResponse = client.runs.stream(\n  thread[\"thread_id\"],\n  assistantId,\n  {\n    input: input,\n    streamMode: \"updates\",\n  }\n);\nfor await (const chunk of streamResponse) {\n  console.log(`Receiving new event of type: ${chunk.event}...`);\n  console.log(chunk.data);\n  console.log(\"\\n\\n\");\n}\n```"]