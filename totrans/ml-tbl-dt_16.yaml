- en: Appendix B. K-nearest neighbors and support vector machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this appendix, we examine classical machine learning algorithms with a more
    computational nature that we didn’t treat in the book because they are less frequently
    used nowadays and are considered outdated compared to decision tree ensembles
    in most applications. Overall, support vector machines (SVMs) are still a practical
    machine learning algorithm well suited for high-dimensional, noisy, or small-sized
    data applications. On the other end, k-nearest neighbors (k-NN) is well suited
    for running applications where the data has few features, there can be outliers,
    and it is not necessary to get a high degree of accuracy in predictions. For instance,
    SVMs can still be used to classify medical images, such as mammograms and X-rays;
    for vehicle detection and tracking in the automotive industry; or to detect email
    spam. Instead, k-NN is mainly applied in recommender systems, particularly collaborative
    filtering approaches, to recommend products or services based on users’ past behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'They are suited in most tabular data situations when your data is not too small
    or exceedingly big—as a rule of thumb, where there are fewer than 10,000 rows.
    We will start with k-NN, an algorithm that data scientists have used for decades
    in machine learning problems and that is easy to understand and implement. Then
    we will complete our overview with SVMs and a brief excursus on using GPUs to
    have these algorithms perform when using a moderately sized dataset. All the examples
    need the Airbnb NYC Dataset presented in chapter 4\. You can reprise it by executing
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: ① List of column names to be excluded from the analysis
  prefs: []
  type: TYPE_NORMAL
- en: ② List of names of columns that likely represent categorical variables in the
    dataset
  prefs: []
  type: TYPE_NORMAL
- en: ③ List of names of columns that represent continuous numerical variables in
    the dataset
  prefs: []
  type: TYPE_NORMAL
- en: ④ A binary balanced target
  prefs: []
  type: TYPE_NORMAL
- en: The code will load your dataset and define what features to be excluded from
    the analysis or considered as continuous or categorical for processing purposes.
  prefs: []
  type: TYPE_NORMAL
- en: B.1 k-NN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Applicable to regression and classification tasks, the k-NN algorithm is considered
    one of the simplest and most intuitive algorithms for making predictions. It finds
    the k (where k is an integer number) closest examples from the training set and
    uses their information to make a prediction. For example, if the task is a regression,
    it will take the average of the k closest examples. If the task is a classification,
    it will choose the most common class among the k closest examples.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, k-NN is commonly regarded as an instance-based learning algorithm
    because it memorizes the training examples as they are. It is also regarded as
    a “lazy algorithm” because, contrary to most machine learning algorithms, there
    is little processing at training time. During training, there is usually some
    processing of the distances by optimized algorithms and data structures that render
    it less computationally costly afterward to look for the neighboring points near
    a training example. Most of the computational work is done at testing time (see
    figure B.1).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/APPB_F01_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure B.1 Classifying new samples (the triangles) with a k-NN where k = 3
  prefs: []
  type: TYPE_NORMAL
- en: We apply a k-NN classifier to the Airbnb NYC data, as seen in chapter 4, in
    listing B.1\. Since k-NN works based on distances, to obtain a functioning solution,
    features must be on the same scale, thus assuring an equal weight to each dimension
    in the distance measurement process. If a feature is on a different or smaller
    scale, it would be overweighted in the process. The contrary would happen if a
    larger scale characterizes a feature. To give an idea of the problem, let’s consider
    what happens when we compare distances based on kilometers, meters, and centimeters.
    Even if distances are comparable, meters and centimeters will numerically exceed
    kilometers measurements. This problem is usually solved by scaling features—for
    instance, by subtracting their mean and dividing by their standard deviation (an
    operation known as z-score normalization
  prefs: []
  type: TYPE_NORMAL
- en: or standardization). Also, techniques such as dimensional reduction or feature
    selection are helpful with this algorithm because rearranging predictors or different
    sets of predictors may result in more or less predictive performances on the problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, as it is often with tabular data, the situation is complicated
    by categorical features, which, once one-hot encoded, will turn into binaries
    ranging from 0 to 1, with a different scale from normalized features. The solution
    we propose is first to discretize the numeric features, thus effectively turning
    them into binary features, each representing if a feature’s numeric values will
    fall into a specific range. Binarization of continuous features is obtained thanks
    to the KBinsDiscretizer class ([https://mng.bz/N12N](https://mng.bz/N12N)) embedded
    into the `numeric_discretizing` pipeline, which will turn each numeric feature
    into five binary ones, each one covering a bin of values. At processing time,
    we also apply principal component analysis (PCA) to reduce the dimensionality
    and make all the features unrelated. However, we might attenuate nonlinearities
    inside the data since PCA is a technique based on linear combinations of the variables.
    Having uncorrelated resulting features is a characteristic of data processed by
    PCA, which suits k-NN: k-NN is based on distances, and distance measurement properly
    works if dimensions are unrelated. Therefore, any distance change is due to changes
    in a single dimension, not multiple ones. The following listing shows the code
    implementing the data transformation process and training the k-NN.'
  prefs: []
  type: TYPE_NORMAL
- en: Listing B.1 k-NN classifier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a scoring function using the accuracy_score metric
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a five-fold crossvalidation iterator with shuffling and a fixed random
    state
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates an instance of the KNeighborsClassifier with specified hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: ④ Defines a ColumnTransformer to preprocess features, applying one-hot encoding
    to categorical features with low cardinality and discretization to numerical features
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Creates a pipeline that sequentially applies the column transformation, performs
    PCA dimensionality reduction, and then fits the k-nn model to the data
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Performs cross-validation on the data using the defined pipeline, with accuracy
    scoring
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Prints the mean and standard deviation of test scores
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the script, you will obtain a result that is close to the performance
    of the Naive Bayes solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The performance is good, though the inference time is relatively high. Since
    this algorithm works by analogy (it will look for similar cases in your training
    to get an idea of the possible prediction), it performs better with large enough
    datasets where there is a higher likelihood of finding some instances resembling
    those to be predicted. Naturally, the right size for the dataset is dictated by
    the number of features used because the more features, the more cases you will
    need for the algorithm to generalize well.
  prefs: []
  type: TYPE_NORMAL
- en: 'Though often the emphasis is placed on setting the best value to the k parameter
    as the key to balancing the underfitting and overfitting of the algorithm to the
    training data, we instead raise attention to other aspects for an effective employ
    of this model. As the algorithm works by analogy and distances in complex spaces,
    we consider two important matters about this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: The dimensions to measure and the curse of dimensionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The appropriate distance measure and how to process the features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In k-NN, the classification or the regression estimates depend on the most similar
    examples based on a distance metric computed on the features. However, in a dataset,
    not all features can be deemed important in judging an example similar to the
    other, and not all of the features can be compared in the same way. Prior knowledge
    of the problem does count a lot when using k-NN because you have to select only
    the features relevant to the task you want to solve. If you assemble too many
    features for the problem, you will rely on too much complex space to navigate.
    Figure B.1 shows how a k-NN algorithm works with just two features (represented
    on the x and y dimensions), and you can intuitively grasp that classifying new
    instances (the triangles in the figure) may be difficult if there are mixed classes
    in an area or if there are no train examples near to a new instance. You have
    to rely on farther ones.
  prefs: []
  type: TYPE_NORMAL
- en: Here comes into the game the curse of dimensionality, which says that as the
    number of features increases, the more examples you should have available to maintain
    a meaningful distance between data points. In addition, the curse implies that
    the number of necessary examples grows exponentially with respect to the number
    of features. For a k-NN algorithm, it means that if you provide too many features,
    it will work in an empty space if the number of examples is not enough. Looking
    for neighbors will become daunting. In addition, if you have just assembled relevant
    and irrelevant features, the risk is that the algorithm will mark as neighbors
    some examples that are very far from the case you have to predict, and the choice
    could be based on features that are not useful for the problem. Hence, if you
    are going to use k-NN, you should choose with great care the features to be used
    (if you don’t know which to use, you need to rely on feature selection) or be
    very familiar with the problem to determine what should go into the algorithm.
    Parsimony is essential for the proper working of a k-NN.
  prefs: []
  type: TYPE_NORMAL
- en: When you have decided about the features, regarding the distance metric you
    will be using, you will need to standardize, remove redundancies, and transform
    the features. This is because distance metrics are based on absolute measurements,
    and different scales can weigh in different ways. Consider using measurements
    in kilometers, meters, and centimeters together. The centimeters will likely predominate
    because they will easily have the largest numbers. Also, having features similar
    to each other (the problem of multicollinearity) can entail the distance measurement
    to overweight certain sets of features over others. Finally, a distance measurement
    implies having the same dimensions to measure. However, in a dataset, you may
    find different kinds of data— numeric, categorical, and time-based—which often
    need to fit together better in a distance calculation because they have different
    numeric characteristics.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, in addition to carefully selecting beforehand which features
    to use, when employing k-NN, we suggest using features that are all of the same
    kind (or all numeric or all categorical) to standardize them if necessary and
    also to reduce their informative redundancy by methods such as PCA ([https://mng.bz/8OrZ](https://mng.bz/8OrZ)),
    which will reformulate the dataset into a new one where features are not correlated
    between themselves.
  prefs: []
  type: TYPE_NORMAL
- en: B.2 SVMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before the 2010s, SVMs had a reputation as the most promising algorithm for
    tabular problems. However, in the past 10 years, tree-based models have eclipsed
    SVMs as the go-to approach for tabular data. However, SVMs remain a family of
    techniques for handling binary, multiclass, regression, and anomaly/novelty detection.
    They are based on the idea that if your observations can be represented as points
    in a multidimensional space, there is a hyperplane (i.e., a separation plane cutting
    through multiple dimensions) that can separate them into classes or values that,
    by assuring the largest separation possible between them, also guarantees the
    most robust and reliable predictions. Figure B.2 shows a simple example of an
    SVM applied to a binary classification problem with two features, represented
    on the x- and y-axis, as predictors. The SVM model produces a separator line with
    the largest slack space between the two groups, as shown in the figure, where
    the dashed lines delimit the slack space. In doing so, it considers only a few
    points near the separator, called the support vectors. Instead, it ignores the
    points that are near but are confusing for the algorithm because, for instance,
    they are on the wrong side. It also ignores the points that are far away from
    the separator line. Outliers have little influence on this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/APPB_F02_Ryan2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure B.2 A separating hyperplane from a SVM
  prefs: []
  type: TYPE_NORMAL
- en: The strong points of SVMs are their robust handling of overfitting, noise in
    data, and outliers and how they can successfully handle datasets that include
    numerous multicollinear features. Applying different nonlinear approaches to data,
    SVMs don’t require the transformations (such as polynomial expansion) we have
    seen for logistic regression. However, they can use domain-based feature engineering
    like all other machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: On the weak side, SVM optimization is complex and can be feasible only for a
    limited number of examples. Moreover, they are best fit for binary predictions
    and for just class prediction; they are not a probabilistic algorithm, and you
    need to wrap them with another algorithm for calibration (such as logistic regression)
    to extract probabilities from them. That renders SVMs valid only for a limited
    range of tasks in risk estimation.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we reapply our problem with Airbnb NYC data using a binary classification
    SVM using a radial basis function kernel, an approach capable of automatically
    modeling complex nonlinear relationships between the provided features.
  prefs: []
  type: TYPE_NORMAL
- en: Listing B.2 Support vector classifier
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a scoring function using the accuracy_score metric
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a five-fold crossvalidation iterator with shuffling and a fixed random
    state
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates an instance of the Support Vector Classifier with specified hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: ④ Defines a ColumnTransformer to preprocess features, applying one-hot encoding
    to categorical features with low cardinality and standardization to numerical
    features
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Creates a pipeline that sequentially applies the column transformation
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Performs cross-validation on the data using the defined pipeline, with accuracy
    scoring
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Prints the mean and standard deviation of test scores
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are pretty interesting, and they can probably even turn better
    by adjusting the hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: However, the time needed for training a single fold is excessive compared to
    all the previous machine learning algorithms. In the next section of this appendix,
    we will discuss how GPU cards can speed up the process while still using the Scikit-learn
    API.
  prefs: []
  type: TYPE_NORMAL
- en: B.3 Using GPUs for machine learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to the rapid rise of deep learning in the data science field, GPUs are now
    widespread and accessible both for local and cloud computing. Earlier, you only
    heard about GPUs in 3D gaming, graphic processing rendering, and animation. Since
    they are cheap and apt at fast matrix multiplication tasks, academics and practitioners
    have rapidly picked up GPUs for neural network computations. RAPIDS, developed
    by NVIDIA (one of the top manufacturers of GPUs), is a set of packages for doing
    the full spectrum of data science, not just deep learning, on GPUs. The RAPIDS
    packages promise to help in all phases of a machine learning pipeline, end to
    end. That’s a game changer for many classical machine learning algorithms, especially
    for the SVMs, the most credible choice for more complex tasks involving noisy,
    outlying observations and vast datasets (having a large set of features, especially
    if multicollinear or sparse ones). In the RAPIDS packages (table B.1), all commands
    have adopted existing APIs for their commands. Such assures an immediate market
    adoption of the packages, and for the user, there is no need to relearn how the
    wheel works.
  prefs: []
  type: TYPE_NORMAL
- en: Table B.1 Rapids packages
  prefs: []
  type: TYPE_NORMAL
- en: '| Rapids package | Task | API mimicked |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| cuPy | Array operations | NumPy |'
  prefs: []
  type: TYPE_TB
- en: '| cuDF | Data processing | pandas |'
  prefs: []
  type: TYPE_TB
- en: '| cuML | Machine learning | Scikit-learn |'
  prefs: []
  type: TYPE_TB
- en: This section will focus on how easy it is to replace your Scikit-learn algorithms
    with the RAPIDS cuML package. Currently, this package includes implementations
    for linear models, k-NN, and SVMs, as well as for clustering and dimensionality
    reduction. The following listing shows the code for testing the support vector
    classifier with the radial basis function kernel we just tried in the previous
    section in its RAPIDS implementation (using a P100 GPU).
  prefs: []
  type: TYPE_NORMAL
- en: Listing B.3 Support vector classifier from RAPIDS cuML
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: ① Creates a scoring function using the accuracy_score metric
  prefs: []
  type: TYPE_NORMAL
- en: ② Creates a five-fold crossvalidation iterator with shuffling and a fixed random
    state
  prefs: []
  type: TYPE_NORMAL
- en: ③ Creates an instance of a Support Vector Classifier from the GPU-accelerated
    cuML library with specified hyperparameters
  prefs: []
  type: TYPE_NORMAL
- en: ④ Defines a ColumnTransformer to preprocess features, applying one-hot encoding
    to categorical features with low cardinality and standardization to numerical
    features
  prefs: []
  type: TYPE_NORMAL
- en: ⑤ Creates a pipeline that sequentially applies the column transformation and
    the model to the data
  prefs: []
  type: TYPE_NORMAL
- en: ⑥ Performs cross-validation on the data using the defined pipeline, with accuracy
    scoring
  prefs: []
  type: TYPE_NORMAL
- en: ⑦ Prints the mean and standard deviation of test scores
  prefs: []
  type: TYPE_NORMAL
- en: The results we obtained are
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we obtained the same results by reusing the same code but relying
    on cuML. However, the processing time has dropped from 102 seconds per folder
    to 4 seconds per folder. If you calculate the time savings, that’s a 25x speed
    increase. The exact performance benefit depends on the GPU model you use; the
    more powerful the GPU, the speedier the results because it depends on how fast
    the GPU card can transfer data from CPU memory and how fast it can process a matrix
    multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Based on such performances on standard GPUs accessible to the general public,
    we recently saw applications fusing tabular data with large embeddings from deep
    learning models (such as text or images). SVMs work well with numerous features
    (but not more than the examples) and sparse values (many zero values). In such
    situations, SVMs can easily obtain a state-of-the-art result, outperforming other
    more popular tabular algorithms at this time—namely XGBoost and other gradient
    boosting implementations as well as end-to-end deep learning solutions, which
    are weaker when you don’t have enough cases to feed them with.
  prefs: []
  type: TYPE_NORMAL
- en: Having a GPU and adapting your code to use RAPIDS algorithms makes certain classic
    algorithms for tabular machine learning quite competitive again, as a general
    rule, based on the principle that there is no free lunch in machine learning (more
    details about no-free-lunch theorems are available at [http://www.no-free-lunch.org/](http://www.no-free-lunch.org/)).
    Taking into account your project constraints (for instance, you may not have certain
    resources available in your project environment), never exclude apriori testing
    your problem against all available algorithms, if this is feasible.
  prefs: []
  type: TYPE_NORMAL
