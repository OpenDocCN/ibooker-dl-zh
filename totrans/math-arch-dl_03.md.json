["```py\ndef pca(X):                                  ①\n    covariance_matrix = torch.cov(X.T)\n    l, e = torch.linalg.eig(covariance_matrix)\n    return l, e\n```", "```py\nx_0 = torch.normal(0, 100, (N,))              ①\n\nx_1 = 2 * x_0 + torch.normal(0, 20, (N,))     ②\n\n  ③\nX = torch.column_stack((x_0, x_1))\n\n  ④\nprincipal_values, principal_vectors = pca(X)  ⑤\n\nX_proj = torch.matmul(X, first_princpal_vec)  ⑥\n```", "```py\nPrincipal values are: [62.6133, 48991.0469]\nFirst Principal Vector is: [-0.44, -0.89]\n```", "```py\nx_0 = torch.normal(0, 100, (N,))\nx_1 = torch.normal(0, 100, (N,))             ①\nX = torch.column_stack((x_0, x_1))\n\nprincipal_values, principal_vectors = pca(X) ②\n```", "```py\nPrincipal values are [ 9736.4033, 7876.6592]\n```", "```py\nx_0 = torch.normal(0, 100, (N,))\nx_1 = 2 * (x_0 ** 2) + torch.normal(0, 5, (N,))\nX = torch.column_stack((x_0, x_1))\n\nprincipal_values, principal_vectors = pca(X) ①\n```", "```py\nPrincipal values are [9.3440e+03, 5.3373e+08] \nMean loss in information: 68.0108526887 - high\n```", "```py\nA = torch.tensor([[1, 2, 1], [2, 2, 3], [1, 3, 3]]).float()\nb = torch.tensor([8, 15, 16]).float()      ①\n\nx_0 = torch.matmul(torch.linalg.inv(A), b) ②\n\nU, S, V_t = torch.linalg.svd(A)            ③\n\ny1 = torch.matmul(U.T, b)                  ④\n\nS_inv = torch.diag(1 / S)\n\ny2 = torch.matmul(S_inv, y1)               ⑤\n\nx_1 = torch.matmul(V_t.T, y2)              ⑥\n\nassert torch.allclose(x_0, x_1)            ⑦\n```", "```py\nSolution via inverse: [1.0, 2.0, 3.0] \nSolution via SVD: [1.0, 2.0, 3.0]\n```", "```py\nA = torch.tensor([[0.11, 0.09], [0.01, 0.02],\n              [0.98, 0.91], [0.12, 0.21],\n              [0.98, 0.99], [0.85, 0.87],\n              [0.03, 0.14], [0.55, 0.45],\n              [0.49, 0.51], [0.99, 0.01],\n              [0.02, 0.89], [0.31, 0.47],\n              [0.55, 0.29], [0.87, 0.76],\n              [0.63, 0.24]])                         ①\nA = torch.column_stack((A, torch.ones(15)))\nb = torch.tensor([-0.8, -0.97, 0.89, -0.67,\n              0.97, 0.72, -0.83, 0.00,\n              0.00, 0.00, -0.09, -0.22,\n              -0.16, 0.63, 0.37])\n\nx_0 = torch.matmul(torch.linalg.pinv(A), b)          ②\n\nU, S, V_t = torch.linalg.svd(A, full_matrices=False) ③\n\ny1 = torch.matmul(U.T, b)\nS_inv = torch.diag(1 / S)\ny2 = torch.matmul(S_inv, y1)\nx_1 = torch.matmul(V_t.T, y2)\n\nassert torch.allclose(x_0, x_1)                      ④\n```", "```py\nSolution via pseudo-inverse: [ 1.0766, 0.8976, -0.9582] \nSolution via SVD: [ 1.0766, 0.8976, -0.9582]\n```", "```py\n  ①                ②               ③\nprincipal_values, principal_vectors = pca(X) ④\n\nX_mean = X - torch.mean(X, axis=0)\n\n  ⑤\nU, S, V_t = torch.linalg.svd(X_mean)         ⑥\n\nV = V_t.T                                    ⑦\n```", "```py\nPrincipal components obtained via PCA: \n[[-0.44588404 -0.89509073]\n [-0.89509073 0.44588404]] \nPrincipal components obtained via SVD:\n[[-0.44588404 0.89509073] \n [-0.89509073 -0.44588404]]\n```", "```py\nterms = [\"violence\", \"gun\", \"america\", \"roses\"]         ①\nX = torch.tensor([[0, 0, 0, 2],\n              [1, 1, 1, 0],\n              [2, 2, 0, 0],\n              [3, 3, 0, 0],\n              [5, 5, 0, 0],\n              [0, 1, 0, 0],\n              [1, 0, 0, 0]]).float()                    ②\n\nU, S, V_t = torch.linalg.svd(X)                         ③\n\nV = V_t.T\n\nrank = 1\nU = U[:, :rank]\nV = V[:, :rank]                                         ④\n\ntopic0_term_weights = list(zip(terms, V[:, 0]))         ⑤\n\ndef cosine_similarity(vec_1, vec_2):\n    vec_1_norm = torch.linalg.norm(vec_1)\n    vec_2_norm = torch.linalg.norm(vec_2)\n    return torch.dot(vec_1, vec_2) / (vec_1_norm * vec_2_norm)\n\nd5_d6_cosine_similarity = cosine_similarity(X[5], X[6]) ⑥\n\ndoc_topic_projection = torch.dot(X, V)\nd5_d6_lsa_similarity = cosine_similarity(doc_topic_projection[5],\n                                         doc_topic_projection[6])\n```", "```py\nPrincipal Values from S matrix: 8.89, 2.00, 1.00, 0.99\n(Topic 0 has disproportionately high weight. We discard others)\n\ntopic0_term_weights (Topic zero is about \"gun\" and \"violence\"):\n[\n ('violence', -0.706990662151775)\n ('gun', -0.7069906621517749)\n ('america', -0.018122010384881156)\n ('roses', 2.9413274625621952e-18)\n]\nDocument 5, document 6 Cosine similarity in original space: 0.0\nDocument 5, document 6 Cosine similarity in topic space: 1.0\n```", "```py\nnum_examples = 500\nx0 = torch.normal(0, 100, (num_examples,)).round()\nrandom_noise = torch.normal(0, 2, (num_examples,)).round()\nx1 = 2*x0 + random_noise\nx2 = torch.normal(0, 5, (num_examples,)).round()\nX = torch.column_stack((x0, x1, x2))                       ①\n\n                ②\nU, S, V_t = torch.linalg.svd(X)                            ③\nV = V_t.T                                                  ③\n```", "```py\nSingular values are: 4867.56982, 118.05858, 19.68604\n```"]