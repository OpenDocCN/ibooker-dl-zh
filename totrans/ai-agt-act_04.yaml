- en: 5 Empowering agents with actions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: How an agent acts outside of itself using actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining and using OpenAI functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Semantic Kernel and how to use semantic functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synergizing semantic and native functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instantiating a GPT interface with Semantic Kernel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we explore actions through the use of functions and how agents
    can use them as well. We’ll start by looking at OpenAI function calling and then
    quickly move on to another project from Microsoft called Semantic Kernel (SK),
    which we’ll use to build and manage skills and functions for agents or as agents.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll finish the chapter using SK to host our first agent system. This will
    be a complete chapter with plenty of annotated code examples.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Defining agent actions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ChatGPT plugins were first introduced to provide a session with abilities, skills,
    or tools. With a plugin, you can search the web or create spreadsheets or graphs.
    Plugins provide ChatGPT with the means to extend the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.1 shows how a ChatGPT plugin works. In this example, a new movie recommender
    plugin has been installed in ChatGPT. When a user asks ChatGPT to recommend a
    new movie, the large language model (LLM) recognizes that it has a plugin to manage
    that action. It then breaks down the user request into actionable parameters,
    which it passes to the new movie recommender.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 How a ChatGPT plugin operates and how plugins and other external
    tools (e.g., APIs) align with the Use External Tools prompt engineering strategy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: The recommender then scrapes a website showcasing new movies and appends that
    information to a new prompt request to an LLM. With this information, the LLM
    responds to the recommender, which passes this back to ChatGPT. ChatGPT then responds
    to the user with the recommended request.
  prefs: []
  type: TYPE_NORMAL
- en: We can think of plugins as proxies for actions. A plugin generally encapsulates
    one or more abilities, such as calling an API or scraping a website. Actions,
    therefore, are extensions of plugins—they give a plugin its abilities.
  prefs: []
  type: TYPE_NORMAL
- en: AI agents can be considered plugins and consumers of plugins, tools, skills,
    and other agents. Adding skills, functions, and tools to an agent/plugin allows
    it to execute well-defined actions—figure 5.2 highlights where agent actions occur
    and their interaction with LLMs and other systems.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 How an agent uses actions to perform external tasks
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An agent action is an ability that allows it to use a function, skill, or tool.
    What gets confusing is that different frameworks use different terminology. We’ll
    define an action as anything an agent can do to establish some basic definitions.
  prefs: []
  type: TYPE_NORMAL
- en: ChatGPT plugins and functions represent an actionable ability that ChatGPT or
    an agent system can use to perform additional actions. Now let’s examine the basis
    for OpenAI plugins and the function definition.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Executing OpenAI functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenAI, with the enablement of plugins, introduced a structure specification
    for defining the interface between functions/plugins an LLM could action. This
    specification is becoming a standard that LLM systems can follow to provide actionable
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: These same function definitions are now also being used to define plugins for
    ChatGPT and other systems. Next, we’ll explore how to use functions directly with
    an LLM call.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Adding functions to LLM API calls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Figure 5.3 demonstrates how an LLM recognizes and uses the function definition
    to cast its response as the function call.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 How a single LLM request, including tools, gets interpreted by an
    LLM
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Listing 5.1 shows the details of an LLM API call using tools and a function
    definition. Adding a function definition allows the LLM to reply regarding the
    function’s input parameters. This means the LLM will identify the correct function
    and parse the relevant parameters for the user’s request.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.1 `first_function.py` (API call)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '#1 New parameter called tools'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Sets the type of tool to function'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Provides an excellent description of what the function does'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Defines the type of parameters for input; an object represents a JSON document.'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Excellent descriptions for each input parameter'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 You can even describe in terms of enumerations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To see how this works, open Visual Studio Code (VS Code) to the book’s source
    code folder: `chapter_4/first_function.py`. It’s a good practice to open the relevant
    chapter folder in VS Code to create a new Python environment and install the `requirements.txt`
    file. If you need assistance with this, consult appendix B.'
  prefs: []
  type: TYPE_NORMAL
- en: Before starting, correctly set up an `.env` file in the `chapter_4` folder with
    your API credentials. Function calling is an extra capability provided by the
    LLM commercial service. At the time of writing, this feature wasn’t an option
    for open source LLM deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at the bottom of the code in `first_function.py,` as shown
    in listing 5.2\. Here are just two examples of calls made to an LLM using the
    request previously specified in listing 5.1\. Here, each request shows the generated
    output from running the example.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.2 `first_function.py` (exercising the API)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Previously defined function'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Returned in the name of the function to call and the extracted input parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Previously defined function'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Returned in the name of the function to call and the extracted input parameters'
  prefs: []
  type: TYPE_NORMAL
- en: Run the `first_function.py` Python script in VS Code using the debugger (F5)
    or the terminal to see the same results. Here, the LLM parses the input request
    to match any registered tools. In this case, the tool is the single function definition,
    that is, the recommended function. The LLM extracts the input parameters from
    this function and parses those from the request. Then, it replies with the named
    function and designated input parameters.
  prefs: []
  type: TYPE_NORMAL
- en: NOTE  The actual function isn’t being called. The LLM only returns the suggested
    function and the relevant input parameters. The name and parameters must be extracted
    and passed into a function matching the signature to act on the function. We’ll
    look at an example of this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Actioning function calls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we understand that an LLM doesn’t execute the function or plugin directly,
    we can look at an example that executes the tools. Keeping with the recommender
    theme, we’ll look at another example that adds a Python function for simple recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 shows how this simple example will work. We’ll submit a single request
    that includes a tool function definition, asking for three recommendations. The
    LLM, in turn, will reply with the three function calls with input parameters (time
    travel, recipe, and gift). The results from executing the functions are then passed
    back to the LLM, which converts them back to natural language and returns a reply.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 A sample request returns three tool function calls and then submits
    the results back to the LLM to return a natural language response.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Now that we understand the example, open `parallel_functions.py` in VS Code.
    Listing 5.3 shows the Python function that you want to call to give recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.3 `parallel_functions.py` (recommend function)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Checks to see if the string is contained within the topic input'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 If no topic is detected, returns the default'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Returns a JSON object'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll look at the function called `run_conversation`, where all the work
    starts with the request construction.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.4 `parallel_functions.py` (`run_conversation`, `request`)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The user message asks for three recommendations.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Note that there is no system message.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Adds the function definition to the tools part of the request'
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.5 shows the request being made, which we’ve covered before, but there
    are a few things to note. This call uses a lower model such as GPT-3.5 because
    delegating functions is a more straightforward task and can be done using older,
    cheaper, less sophisticated language models.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.5 `parallel_functions.py` (`run_conversation`, API call)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '#1 LLMs that delegate to functions can be simpler models.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Adds the messages and tools definitions'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 auto is the default.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 The returned message from the LLM'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, after the API call, the response should hold the information
    for the required function calls. Remember, we asked the LLM to provide us with
    three recommendations, which means it should also provide us with three function
    call outputs, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.6 `parallel_functions.py` (`run_conversation`, `tool_calls`)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '#1 If the response contains tool calls, execute them.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Only one function but could contain several'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Loops through the calls and replays the content back to the LLM'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Executes the recommend function from extracted parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Appends the results of each function call to the set of messages'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Sends another request to the LLM with updated information and returns the
    message reply'
  prefs: []
  type: TYPE_NORMAL
- en: The tool call outputs and the calls to the recommender function results are
    appended to the messages. Notice how messages now also contain the history of
    the first call. This is then passed back to the LLM to construct a reply in natural
    language.
  prefs: []
  type: TYPE_NORMAL
- en: Debug this example in VS Code by pressing the F5 key with the file open. The
    following listing shows the output of running `parallel_functions.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.7 `parallel_functions.py` (output)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This completes this simple demonstration. For more advanced applications, the
    functions could do any number of things, from scraping websites to calling search
    engines to completing far more complex tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Functions are an excellent way to cast outputs for a particular task. However,
    the work of handling functions or tools and making secondary calls can be done
    in a cleaner and more efficient way. The following section will uncover a more
    robust system of adding actions to agents.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Introducing Semantic Kernel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Semantic Kernel (SK) is another open source project from Microsoft intended
    to help build AI applications, which we call agents. At its core, the project
    is best used to define actions, or what the platform calls *semantic plugins*,
    which are wrappers for skills and functions.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 shows how the SK can be used as a plugin and a consumer of OpenAI
    plugins. The SK relies on the OpenAI plugin definition to define a plugin. That
    way, it can consume and publish itself or other plugins to other systems.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 How the Semantic Kernel integrates as a plugin and can also consume
    plugins
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: An OpenAI plugin definition maps precisely to the function definitions in listing
    5.4\. This means that SK is the orchestrator of API tool calls, aka plugins. That
    also means that SK can help organize multiple plugins with a chat interface or
    an agent.
  prefs: []
  type: TYPE_NORMAL
- en: Note  The team at SK originally labeled the functional modules as *skills.*
    However, to be more consistent with OpenAI, they have since renamed *skills* to
    *plugins.* What is more confusing is that the code still uses the term *skills.*
    Therefore, throughout this chapter, we’ll use *skills* and *plugins* to mean the
    same thing.
  prefs: []
  type: TYPE_NORMAL
- en: SK is a useful tool for managing multiple plugins (actions for agents) and,
    as we’ll see later, can also assist with memory and planning tools. For this chapter,
    we’ll focus on the actions/plugins. In the next section, we look at how to get
    started using SK.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Getting started with SK semantic functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SK is easy to install and works within Python, Java, and C#. This is excellent
    news as it also allows plugins developed in one language to be consumed in a different
    language. However, you can’t yet develop a native function in one language and
    use it in another.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll continue from where we left off for the Python environment using the `chapter_4`
    workspace in VS Code. Be sure you have a workspace configured if you want to explore
    and run any examples.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.8 shows how to install SK from a terminal within VS Code. You can
    also install the SK extension for VS Code. The extension can be a helpful tool
    to create plugins/skills, but it isn’t required.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.8 Installing Semantic Kernel
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Uninstalls any previous installations of SK'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Clones the repository to a local folder'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Changes to the source folder'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Installs the editable package from the source folder'
  prefs: []
  type: TYPE_NORMAL
- en: Once you finish the installation, open `SK_connecting.py` in VS Code. Listing
    5.9 shows a demo of running an example quickly through SK. The example creates
    a chat completion service using either OpenAI or Azure OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.9 `SK_connecting.py`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Sets the service you’re using (OpenAI or Azure OpenAI)'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates the kernel'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Loads secrets from the .env file and sets them on the chat service'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Loads secrets from the .env file and sets them on the chat service'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Invokes the prompt'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Calls the function asynchronously'
  prefs: []
  type: TYPE_NORMAL
- en: Run the example by pressing F5 (debugging), and you should see an output similar
    to listing 5.9\. This example demonstrates how a semantic function can be created
    with SK and executed. A semantic function is the equivalent of a prompt template
    in prompt flow, another Microsoft tool. In this example, we define a simple prompt
    as a function.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that this semantic function isn’t defined as a plugin.
    However, the kernel can create the function as a self-contained semantic element
    that can be executed against an LLM. Semantic functions can be used alone or registered
    as plugins, as you’ll see later. Let’s jump to the next section, where we introduce
    contextual variables.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Semantic functions and context variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Expanding on the previous example, we can look at adding contextual variables
    to the semantic function. This pattern of adding placeholders to prompt templates
    is one we’ll review over and over. In this example, we look at a prompt template
    that has placeholders for subject, genre, format, and custom.
  prefs: []
  type: TYPE_NORMAL
- en: Open `SK_context_variables.py` in VS Code, as shown in the next listing. The
    prompt is equivalent to setting aside a `system` and `user` section of the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.10 `SK_context_variables.py`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Defines a prompt with placeholders'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Configures a prompt template and input variable definitions'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Creates a kernel function from the prompt'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Creates an asynchronous function to wrap the function call'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Sets the kernel function arguments'
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and debug this example (F5), and wait for the output to be generated.
    That is the basis for setting up SK and creating and exercising semantic functions.
    In the next section, we move on to see how a semantic function can be registered
    as a skill/plugin.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Synergizing semantic and native functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Semantic functions encapsulate a prompt/profile and execute through interaction
    with an LLM. Native functions are the encapsulation of code that may perform anything
    from scraping websites to searching the web. Both semantic and native functions
    can register as plugins/skills in the SK kernel.
  prefs: []
  type: TYPE_NORMAL
- en: A function, semantic or native, can be registered as a plugin and used the same
    way we registered the earlier function directly with our API calls. When a function
    is registered as a plugin, it becomes accessible to chat or agent interfaces,
    depending on the use case. The next section looks at how a semantic function is
    created and registered with the kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 Creating and registering a semantic skill/plugin
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The VS Code extension for SK provides helpful tools for creating plugins/skills.
    In this section, we’ll use the SK extension to create a plugin/skill and then
    edit the components of that extension. After that, we’ll register and execute
    the plugin in the SK.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 shows the process for creating a new skill within VS Code using the
    SK extension. (Refer to appendix B for directions if you need to install this
    extension.) You’ll then be given the option for the skill/plugin folder to place
    the function. Always group functions that are similar together. After creating
    a skill, enter the name and description of the function you want to develop. Be
    sure to describe the function as if the LLM were going to use it.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 The process of creating a new skill/plugin
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: You can see the completed skills and functions by opening the `skills/plugin`
    folder and reviewing the files. We’ll follow the previously constructed example,
    so open the `skills/Recommender/Recommend_Movies` folder, as shown in figure 5.7\.
    Inside this folder is a `config.json` file, the function description, and the
    semantic function/prompt in a file called `skprompt.txt`.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 The file and folder structure of a semantic function skill/plugin
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Listing 5.11 shows the contents of the semantic function definition, also known
    as the plugin definition. Note that the type is marked as `completion` and not
    of type `function` because this is a semantic function. We would define a native
    function as a type function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.11 `Recommend_Movies/config.json`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Semantic functions are functions of type completion.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 We can also set the completion parameters for how the function is called.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Defines the parameters input into the semantic function'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can look at the definition of the semantic function prompt, as shown
    in listing 5.12\. The format is a little different, but what we see here matches
    the earlier examples using templating. This prompt recommends movies based on
    a list of movies the user has previously seen.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.12 `Recommend_Movies/skprompt.txt`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now, we’ll dive into the code that loads the skill/plugin and executes it in
    a simple example. Open the `SK_first_skill.py` file in VS Code. The following
    listing shows an abridged version highlighting the new sections.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.13 SK_first_skill.py (abridged listing)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Loads the prompt from the plugins folder'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 List of user’s previously seen movies'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Input is set to joined list of seen movies.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Function is executed asynchronously.'
  prefs: []
  type: TYPE_NORMAL
- en: The code loads the skill/plugin from the `skills` directory and the `plugin`
    folder. When a skill is loaded into the kernel and not just created, it becomes
    a registered plugin. That means it can be executed directly as is done here or
    through an LLM chat conversation via the plugin interface.
  prefs: []
  type: TYPE_NORMAL
- en: Run the code (F5), and you should see an output like listing 5.13\. We now have
    a simple semantic function that can be hosted as a plugin. However, this function
    requires users to input a complete list of movies they have watched. We’ll look
    at a means to fix this by introducing native functions in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.2 Applying native functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As stated, native functions are code that can do anything. In the following
    example, we’ll introduce a native function to assist the semantic function we
    built earlier.
  prefs: []
  type: TYPE_NORMAL
- en: This native function will load a list of movies the user has previously seen,
    from a file. While this function introduces the concept of memory, we’ll defer
    that discussion until chapter 8\. Consider this new native function as any code
    that could virtually do anything.
  prefs: []
  type: TYPE_NORMAL
- en: Native functions can be created and registered using the SK extension. For this
    example, we’ll create a native function directly in code to make the example easier
    to follow.
  prefs: []
  type: TYPE_NORMAL
- en: Open `SK_native_functions.py` in VS Code. We’ll start by looking at how the
    native function is defined. A native function is typically defined within a class,
    which simplifies managing and instantiating native functions.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.14 `SK_native_functions.py` (`MySeenMovieDatabase`)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Provides a description for the container class'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Uses a decorator to provide function description and name'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 The actual function returns a list of movies in a comma-separated string.'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Loads seen movies from the text file'
  prefs: []
  type: TYPE_NORMAL
- en: With the native function defined, we can see how it’s used by scrolling down
    in the file, as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.15 `SK_native_functions` (remaining code)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Loads the semantic function as shown previously'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Imports the skill into the kernel and registers the function as a plugin'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Loads the native function'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Executes the function and returns the list as a string'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Wraps the plugin call in an asynchronous function and executes'
  prefs: []
  type: TYPE_NORMAL
- en: One important aspect to note is how the native function was imported into the
    kernel. The act of importing to the kernel registers that function as a plugin/skill.
    This means the function can be used as a skill from the kernel through other conversations
    or interactions. We’ll see how to embed a native function within a semantic function
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.3 Embedding native functions within semantic functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are plenty of powerful features within SK, but one beneficial feature
    is the ability to embed native or semantic functions within other semantic functions.
    The following listing shows how a native function can be embedded within a semantic
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.16 `SK_semantic_native_functions.py` (`skprompt`)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '#1 The exact instruction text as previous'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The native function is referenced and identified by class name and function
    name.'
  prefs: []
  type: TYPE_NORMAL
- en: The next example, `SK_semantic_native_functions.py`, uses inline native and
    semantic functions. Open the file in VS Code, and the following listing shows
    the code to create, register, and execute the functions.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.17 `SK_semantic_native_functions.py` (abridged)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Creates the prompt template config for the prompt'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Creates an inline semantic function from the prompt'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Executes the semantic function asynchronously'
  prefs: []
  type: TYPE_NORMAL
- en: Run the code, and you should see an output like listing 5.17\. One important
    aspect to note is that the native function is registered with the kernel, but
    the semantic function is not. This is important because function creation doesn’t
    register a function.
  prefs: []
  type: TYPE_NORMAL
- en: For this example to work correctly, the native function must be registered with
    the kernel, which uses the `import_plugin` function call—the first line in listing
    5.17\. However, the semantic function itself isn’t registered. An easy way to
    register the function is to make it a plugin and import it.
  prefs: []
  type: TYPE_NORMAL
- en: These simple exercises showcase ways to integrate plugins and skills into chat
    or agent interfaces. In the next section, we’ll look at a complete example demonstrating
    adding a plugin representing a service or GPT interface to a chat function.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Semantic Kernel as an interactive service agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In chapter 1, we introduced the concept of the GPT interface—a new paradigm
    in connecting services and other components to LLMs via plugins and semantic layers.
    SK provides an excellent abstraction for converting any service to a GPT interface.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8 shows a GPT interface constructed around an API service called The
    Movie Database (TMDB; [www.themoviedb.org](http://www.themoviedb.org)). The TMDB
    site provides a free API that exposes information about movies and TV shows.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 This layer architecture diagram shows the role of a GPT interface
    and the Semantic Kernel being exposed to chat or agent interfaces.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: To follow along with the exercises in this section, you must register for a
    free account from TMDB and create an API key. Instructions for getting an API
    key can be found at the TMDB website ([www.themoviedb.org](http://www.themoviedb.org))
    or by asking a GPT-4 turbo or a more recent LLM.
  prefs: []
  type: TYPE_NORMAL
- en: Over the next set of subsections, we’ll create a GPT interface using an SK set
    of native functions. Then, we’ll use the SK kernel to test the interface and,
    later in this chapter, implement it as plugins into a chat function. In the next
    section, we look at building a GPT interface against the TMDB API.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.1 Building a semantic GPT interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TMDB is an excellent service, but it provides no semantic services or services
    that can be plugged into ChatGPT or an agent. To do that, we must wrap the API
    calls that TMDB exposes in a semantic service layer.
  prefs: []
  type: TYPE_NORMAL
- en: A semantic service layer is a GPT interface that exposes functions through natural
    language. As discussed, to expose functions to ChatGPT or other interfaces such
    as agents, they must be defined as plugins. Fortunately, SK can create the plugins
    for us automatically, given that we write our semantic service layer correctly.
  prefs: []
  type: TYPE_NORMAL
- en: A native plugin or set of skills can act as a semantic layer. To create a native
    plugin, create a new plugin folder, and put a Python file holding a class containing
    the set of native functions inside that folder. The SK extension currently doesn’t
    do this well, so manually creating the module works best.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.9 shows the structure of the new plugin called `Movies` and the semantic
    service layer called `tmdb.py`. For native functions, the parent folder’s name
    (`Movies`) is used in the import.
  prefs: []
  type: TYPE_NORMAL
- en: '![figure](../Images/5-9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 The folder and file structure of the TMDB plugin
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Open the `tmdb.py` file in VS Code, and look at the top of the file, as shown
    in listing 5.18\. This file contains a class called `TMDbService`, which exposes
    several functions that map to API endpoint calls. The idea is to map the various
    relevant API function calls in this semantic service layer. This will expose the
    functions as plugins for a chat or agent interface.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.18 `tmdb.py` (top of file)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Prints the calls to the functions for debugging'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Top-level service and decorator used to describe the function (good descriptions
    are important)'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Function wrapped in semantic wrapper; should return str'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Calls the API endpoint, and, if good (code 200), checks for matching genre'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Found the genre, returns the id'
  prefs: []
  type: TYPE_NORMAL
- en: The bulk of the code for the `TMDbService` and the functions to call the TMDB
    endpoints was written with the help of GPT-4 Turbo. Then, each function was wrapped
    with the `sk_function` decorator to expose it semantically.
  prefs: []
  type: TYPE_NORMAL
- en: A few of the TMDB API calls have been mapped semantically. Listing 5.19 shows
    another example of a function exposed to the semantic service layer. This function
    pulls a current top 10 list of movies playing for a particular genre.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.19 `tmdb.py` (`get_top_movies_by_genre`)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Decorates the function with descriptions'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Finds the genre id for the given genre name'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Gets a list of currently playing movies'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Converts genre_ids to strings'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Checks to see if the genre id matches movie genres'
  prefs: []
  type: TYPE_NORMAL
- en: Look through the various other API calls mapped semantically. As you can see,
    there is a well-defined pattern for converting API calls to a semantic service.
    Before we run the full service, we’ll test each of the functions in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.2 Testing semantic services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a real-world application, you’ll likely want to write a complete set of unit
    or integration tests for each semantic service function. We won’t do that here;
    instead, we’ll write a quick helper script to test the various functions.
  prefs: []
  type: TYPE_NORMAL
- en: Open `test_tmdb_service.py` in VS Code, and review the code, as shown in listing
    5.20\. You can comment and uncomment any functions to test them in isolation.
    Be sure to have only one function uncommented at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.20 `test_tmdb_service.py`
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Instantiates the kernel'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Imports the plugin service'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Inputs parameter to functions, when needed'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Executes and tests the various functions'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Inputs parameter to functions, when needed'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Executes and tests the various functions'
  prefs: []
  type: TYPE_NORMAL
- en: '#7 Inputs parameter to functions, when needed'
  prefs: []
  type: TYPE_NORMAL
- en: '#8 Executes and tests the various functions'
  prefs: []
  type: TYPE_NORMAL
- en: '#9 Executes and tests the various functions'
  prefs: []
  type: TYPE_NORMAL
- en: '#10 Executes main asynchronously'
  prefs: []
  type: TYPE_NORMAL
- en: '#11 Calls print function details to notify when the function is being called'
  prefs: []
  type: TYPE_NORMAL
- en: The real power of SK is shown in this test. Notice how the `TMDbService` class
    is imported as a plugin, but we don’t have to define any plugin configurations
    other than what we already did? By just writing one class that wrapped a few API
    functions, we’ve exposed part of the TMDB API semantically. Now, with the functions
    exposed, we can look at how they can be used as plugins for a chat interface in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.3 Interactive chat with the semantic service layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the TMDB functions exposed semantically, we can move on to integrating
    them into a chat interface. This will allow us to converse naturally in this interface
    to get various information, such as current top movies.
  prefs: []
  type: TYPE_NORMAL
- en: Open `SK_service_chat.py` in VS Code. Scroll down to the start of the new section
    of code that creates the functions, as shown in listing 5.21\. The functions created
    here are now exposed as plugins, except we filter out the chat function, which
    we don’t want to expose as a plugin. The chat function here allows the user to
    converse directly with the LLM and shouldn’t be a plugin.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.21 `SK_service_chat.py` (function setup)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Imports the TMDbService as a plugin'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Configures the execution settings and adds filtered tools'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Configures the prompt configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Defines the input template and takes full strings as user input'
  prefs: []
  type: TYPE_NORMAL
- en: '#5 Adds the chat history object and populates some history'
  prefs: []
  type: TYPE_NORMAL
- en: '#6 Creates the chat function'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we can continue by scrolling in the same file to review the chat function,
    as shown in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.22 `SK_service_chat.py` (chat function)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Input is taken directly from the terminal/console.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 If the user types exit, then exit the chat.'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Creates arguments to pass to the function'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 Uses the utility function to call the function and execute the tool'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, scroll down to the bottom of the file, and review the primary function.
    This is the code that calls the chat function in a loop.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.23 `SK_service_chat.py` (main function)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Introduction to the user'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Continues until chatting is False'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 Calls the chat function asynchronously'
  prefs: []
  type: TYPE_NORMAL
- en: Run the chat interface, run the file (F5), and then ask about movies or television
    shows of a particular genre. An example conversation session is shown in listing
    5.24\. This output shows how a request to list movies from two genres made the
    chat interface make multiple calls to the `get_top_movie_by_genre` function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.24 `SK_service_chat.py` (example conversation)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '#1 LLM makes two calls to get_top_movies_by_genre.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Internal call to get the genre id'
  prefs: []
  type: TYPE_NORMAL
- en: '#3 List of the top current action movies'
  prefs: []
  type: TYPE_NORMAL
- en: '#4 List of the top current comedy movies'
  prefs: []
  type: TYPE_NORMAL
- en: Be sure to explore the chat interface’s boundaries and what you can ask for
    from the TMDB service. For example, try asking for a list of genres for movies
    or television shows. This service is a good first try, but we can perhaps do better,
    as we’ll see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Thinking semantically when writing semantic services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now we’ve seen an excellent demonstration of converting an API into a semantic
    service interface. As it is, the functions return the titles of the top movies
    and television shows currently playing. However, by just returning the titles,
    we’re limiting the ability of the LLM to parse the results on its own.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we’ll create a v2 version of `TMDbService` to correct this and return
    the results as JSON strings. Open the file `tmdb_v2.py` in VS Code, and scroll
    down to the `get_top_movies_by_genre` function.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.25 `tmdb_v2.py` (`get_top_movies_by_genre`)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Now returns a filtered list as a JSON string'
  prefs: []
  type: TYPE_NORMAL
- en: Now open `SK_service_chat.py` in VS Code, and comment and uncomment the line
    shown in listing 5.26\. This will then use version 2 of the `TMDbService` that
    outputs results as full JSON documents in a single string.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.26 `SK_service_chat.py` (modifying imports)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '#1 Comment out this line.'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 Uncomment this line to use version 2 of the service.'
  prefs: []
  type: TYPE_NORMAL
- en: Rerun the `SK_service_chat.py` file in VS Code, and alter your query slightly,
    as shown by the output in the following listing.
  prefs: []
  type: TYPE_NORMAL
- en: Listing 5.27 `SK_service_chat.py` (`TMDb_v2` service output)
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '#1 New query asks to include an additional filter for space'
  prefs: []
  type: TYPE_NORMAL
- en: '#2 The LLM calls the service and then reviews the returned results that match
    the filter.'
  prefs: []
  type: TYPE_NORMAL
- en: Because the semantic service functions now return the complete movie listing
    in JSON, the LLM can apply additional filtering. This is the real power of semantic
    services, allowing you to process the data through the LLM. We won’t see this
    power by just returning a list of titles.
  prefs: []
  type: TYPE_NORMAL
- en: This last exercise demonstrated the change in mentality you need to make when
    writing semantic service layers. Generally, you’ll typically want to return as
    much information as possible. Returning more information takes advantage of the
    LLM abilities to filter, sort, and transform data independently. In the next chapter,
    we’ll explore building autonomous agents using behavior trees.
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Exercises
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Complete the following exercises to improve your knowledge of the material:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Exercise 1*—Creating a Basic Plugin for Temperature Conversion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Familiarize yourself with creating a simple plugin for the OpenAI
    chat completions API.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks:*'
  prefs: []
  type: TYPE_NORMAL
- en: Develop a plugin that converts temperatures between Celsius and Fahrenheit.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Test the plugin by integrating it into a simple OpenAI chat session where users
    can ask for temperature conversions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exercise 2*—Developing a Weather Information Plugin'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Learn to create a plugin that performs a unique task.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks:*'
  prefs: []
  type: TYPE_NORMAL
- en: Create a plugin for the OpenAI chat completions API that fetches weather information
    from a public API.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure the plugin can handle user requests for current weather conditions in
    different cities.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exercise 3*—Crafting a Creative Semantic Function'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Explore the creation of semantic functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks:*'
  prefs: []
  type: TYPE_NORMAL
- en: Develop a semantic function that writes a poem or tells a children’s story based
    on user input.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Test the function in a chat session to ensure it generates creative and coherent
    outputs.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exercise 4*—Enhancing Semantic Functions with Native Functions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Understand how to combine semantic and native functions.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks:*'
  prefs: []
  type: TYPE_NORMAL
- en: Create a semantic function that uses a native function to enhance its capabilities.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, develop a semantic function that generates a meal plan and uses
    a native function to fetch nutritional information for the ingredients.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Exercise 5*—Wrapping an Existing Web API with Semantic Kernel'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Objective *—Learn to wrap existing web APIs as semantic service plugins.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Tasks:*'
  prefs: []
  type: TYPE_NORMAL
- en: Use SK to wrap a news API and expose it as a semantic service plugin in a chat
    agent.
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure the plugin can handle user requests for the latest news articles on various
    topics.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Agent actions extend the capabilities of an agent system, such as ChatGPT. This
    includes the ability to add plugins to ChatGPT and LLMs to function as proxies
    for actions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI supports function definitions and plugins within an OpenAI API session.
    This includes adding function definitions to LLM API calls and understanding how
    these functions allow the LLM to perform additional actions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Semantic Kernel (SK) is an open source project from Microsoft that can be
    used to build AI applications and agent systems. This includes the role of semantic
    plugins in defining native and semantic functions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic functions encapsulate the prompt/profile template used to engage an
    LLM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Native functions encapsulate code that performs or executes an action using
    an API or other interface.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic functions can be combined with other semantic or native functions and
    layered within one another as execution stages.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SK can be used to create a GPT interface over the top of API calls in a semantic
    service layer and expose them as chat or agent interface plugins.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semantic services represent the interaction between LLMs and plugins, as well
    as the practical implementation of these concepts in creating efficient AI agents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
