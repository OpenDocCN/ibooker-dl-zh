<html><head></head><body>
  <h1 class="tochead" id="heading_id_2">8 Getting started with deep learning with tabular data<a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/><a id="idTextAnchor005"/><a id="idTextAnchor006"/><a id="idTextAnchor007"/></h1>

  <p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="marker-299"/></p>

  <ul class="calibre5">
    <li class="co-summary-bullet">An introduction to deep learning with tabular data stack<a class="calibre" id="idTextAnchor008"/>s—low-level frameworks and high-level APIs for deep learning</li>

    <li class="co-summary-bullet">The PyTorch with fastai stack</li>

    <li class="co-summary-bullet">The PyTorch with TabNet stack</li>

    <li class="co-summary-bullet">The PyTorch with Lightning Flash stack</li>

    <li class="co-summary-bullet">The stacks we didn’t exercise and why we didn’t exercise them</li>

    <li class="co-summary-bullet">A comparison of the pros and cons of deep learning with tabular data stacks</li>
  </ul>

  <p class="body">Up to this point, we have focused on classical machine learning tools and algorithms to analyze tabular data. Ranging from traditional regression algorithms to more sophisticated gradient boosting techniques, these approaches offer advantages in simplicity, transparency, and efficacy. That said, deep learning tools have become much easier to access and use, and they also provide a powerful alternative for handling tabular data.</p>

  <p class="body">In this chapter, we will review a set of deep learning stacks (low-level framework, high-level API, and deep learning for tabular data library) and use three of these stacks—fastai, PyTorch with TabNet, and Lightning Flash—to solve the Airbnb NYC problem. We’ll work the same problem three times, once with each stack. The goal is to illustrate both the general form of the deep learning approach and to highlight the unique characteristics of the three tools we’ve selected. <a id="idTextAnchor009"/></p>

  <h2 class="fm-head" id="heading_id_3">8.1 The deep learning with tabular data stack</h2>

  <p class="body">Before we examine the stacks that are available for deep learning with tabular data in general, let’s look at a specific example: the Keras-based deep learning solution for the Airbnb NYC price prediction problem from chapter 3.<a id="idTextAnchor010"/><a id="idIndexMarker001"/><a id="idIndexMarker002"/><a id="marker-300"/></p>

  <div class="fm-sidebar-block">
    <p class="fm-sidebar-title">The Keras solution vs. the XGBoost solution</p>

    <p class="fm-sidebar-text">The code that is distinct for the Keras solution is contained in the training notebook. In particular, the key differences between the Keras solution and the XGBoost solution described in chapter 3 include</p>

    <ul class="calibre5">
      <li class="fm-list-bullet1">
        <p class="list-s"><i class="fm-italics">Model definition</i>—The Keras model has a large function to define the layers that make up the model, with each class of column (continuous, categorical, and text) getting a specific set of layers.</p>
      </li>

      <li class="fm-list-bullet1">
        <p class="list-s"><i class="fm-italics">Model training</i>—The Keras model includes additional code to define the callbacks required to make the training process efficient, including a callback to stop the training process early if the training is no longer making the model better and a callback to ensure that the optimal model is saved during the training process.</p>
      </li>
    </ul>

    <p class="fm-sidebar-text">The Keras solution that we examined in chapter 3 gives us a concrete baseline with which to compare the other stacks we will examine in this chapter. In this chapter, we will exercise a set of other stacks so you can see the pros and cons of each choice.</p>

    <p class="fm-sidebar-text">We will also discuss an additional set of stacks that we weren’t able to exercise and explain what this experience tells us about these choices. It is important to understand the stack choices and the pros and cons of the choices so that you can select a deep learning with tabular data stack that works best for your requirements.</p>
  </div>

  <p class="body">Let’s briefly review the Keras solution from chapter 3. Figure 8.1 shows the files that make up the Keras solution, with the training notebook highlighted.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F01_Ryan2.png"/></p>

    <p class="figurecaption">Figure 8.1 Files that make up the Keras solution to the Airbnb problem</p>
  </div>

  <p class="body">The training notebook contains the code that varies between the Keras solution and the other solutions that we will explore in this chapter. The other files stay consistent across all the deep learning solutions, with the exception of some settings in the training config file.<a id="idTextAnchor011"/></p>

  <p class="body">Figure 8.2 shows the components that make up the stack for this solution. These components are used in the training notebook.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F02_Ryan2.png"/></p>

    <p class="figurecaption">Figure 8.2 The stack for the Airbnb NYC solution in chapter 3</p>
  </div>

  <p class="body"><a id="idTextAnchor012"/>In this stack the underlying, low-level deep learning framework is TensorFlow. Since Keras is delivered as part of the TensorFlow distribution and is the recommended high-level API for TensorFlow, it may sound a bit redundant to talk about TensorFlow and Keras separately, but keeping them distinct will make the description of the general stack choices clearer. In the deep learning solution from chapter 3, we used custom-written code to define the model itself. For example, listing 8.1 shows the custom code that defines layers for categorical columns in the deep learning solution from chapter 3. The listing also shows the statements in the <code class="fm-code-in-text">get_model()</code> function that define the layers for categorical columns.<a id="idTextAnchor013"/><a id="idIndexMarker003"/><a id="idIndexMarker004"/><a id="marker-301"/></p>

  <p class="fm-code-listing-caption">Listing 8.1 Statement in the <code class="fm-code-in-text">get_model()</code>function for categorical column layers</p>
  <pre class="programlisting">    for col in collist:
        catinputs[col] = Input(shape=[1],name=col)                   <span class="fm-combinumeral">①</span>
        inputlayerlist.append(catinputs[col])                        <span class="fm-combinumeral">②</span>
        embeddings[col] = \
(Embedding(max_dict[col],catemb) (catinputs[col]))                   <span class="fm-combinumeral">③</span>
        embeddings[col]=(BatchNormalization()(embeddings[col]))      <span class="fm-combinumeral">④</span>
        collistfix.append(embeddings[col])                           <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines an input layer to the model for the current column</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Adds the input layer that was just defined to the list of input layers</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Defines an embedding layer for the current column</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Defines a batch normalization layer for the current column</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Adds the set of layers defined to this column to the overall list of layers</p>

  <p class="body">The <code class="fm-code-in-text">get_model()</code> function specifies the Keras layers in the model for three types of input columns: categorical, continuous, and text. The <code class="fm-code-in-text">get_model()</code> function shown in listing 8.1 also contains statements that define the model layers for continuous layers and text layers. Note that this model has multiple inputs (each column selected to train the model is an input) and a single output: a prediction of whether or not the price of a given Airbnb listing will be above or below the median. The details of how the layers for each of the input columns are defined is beyond the scope of this chapter, so we won’t go through those now.<a id="idIndexMarker005"/></p>

  <p class="body">Now that we have reviewed what the stack looks like for the deep learning Airbnb NYC solution in chapter 3, let’s generalize to other deep learning approaches to tabular data. Figure 8.3 shows a selection of choices for the deep learning stack for tabular data problems.<a id="idTextAnchor014"/><a id="marker-302"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F03_Ryan2.png"/></p>

    <p class="figurecaption">Figure 8.3 Deep learning stacks for tabular data</p>
  </div>

  <p class="body">Let’s examine each layer of the stacks in more detail:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Low-level framework</i>—There are two predominant low-level deep learning frameworks. TensorFlow is used most frequently in industry. PyTorch is the most popular choice for researchers.<a id="idIndexMarker006"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">High-level API</i>—To make it easier for beginners to create deep learning applications and to abstract some of the complexity for experienced developers, in the mid-2010s the need was identified for a high-level API for deep learning. Initially, you could use Keras as a front end for several low-level frameworks. In 2019, Keras was integrated into the TensorFlow ecosystem and identified as the recommended high-level framework for TensorFlow. There isn’t an exact analogy for Keras in the PyTorch world. The overall design of PyTorch is supposed to make it more accessible than TensorFlow and reduce the need for a high-level API. Nevertheless, there are two high-level APIs that abstract different aspects of PyTorch. fastai is intended specifically for people coming from other disciplines who want to use deep learning to solve problems in their discipline and has as its central ethic being able to define, train, and exercise a deep learning model with just a handful of lines of code. Lightning, by contrast, abstracts a single aspect of PyTorch, the training loop. Lightning Flash, which is built on top of Lightning, is, according to its documentation, “a high-level deep learning framework for fast prototyping, baselining, fine-tuning and solving deep learning problems.” While both fastai and Lightning have devoted communities of users, neither has attracted the popularity in the PyTorch world that Keras has in the TensorFlow world.<a id="idIndexMarker007"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Tabular data library</i>—The low-level framework and high-level API provide an environment for deep learning in general. The deep learning libraries provide capabilities specifically for dealing with tabular data. As we demonstrated with the deep learning solution for the Airbnb NYC price prediction problem in chapter 3, you don’t need to use a tabular data library to do deep learning with tabular data.<a id="idIndexMarker008"/></p>
    </li>
  </ul>

  <p class="body">Two details to note about the tabular data libraries are</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Tabular data libraries may be supported for both TensorFlow and PyTorch. TabNet is an example of a library that is supported for both low-level deep learning frameworks.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">fastai is a general-purpose, high-level API as well as a tabular data library. fastai fits into both categories because it abstracts some of the complexity of PyTorch to make it easier to build and train models on a variety of data types (including image and text) and also has facilities aimed specifically at tabular data (for example, automatically handling basic operations required for categorical features in tabular datasets).</p>
    </li>
  </ul>

  <p class="body">Now that we have examined the deep learning with tabular data stack, let’s look at the stacks that we will examine in this chapter by applying them to solve the Airbnb NYC price prediction problem:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">PyTorch with fastai</i>—This is the most “traditional” approach since fastai is an established framework with tens of thousands of developers using it. fastai is the most popular framework that explicitly supports tabular data, according to repo stars. fastai is particularly popular with people who are learning about deep learning and hobbyists.<a id="idIndexMarker009"/><a id="marker-303"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">PyTorch with TabNet</i>—TabNet is the next most popular tabular data library after fastai according to repo stars. TabNet is a library for tabular data highlighted by Google in its documentation (<a class="url" href="https://mng.bz/av1m">https://mng.bz/av1m</a>). This stack demonstrates how a dedicated tabular data library can be used to create a model trained on tabular data.<a id="idIndexMarker010"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Lightning Flash</i>—PyTorch Lightning is a popular framework that abstracts some of the complexity of PyTorch. Lightning Flash is built on top of PyTorch Lightning and offers an easily accessible way to create deep learning applications. It also includes explicit support for tabular data and thus is an interesting comparison point for the other stacks we review in this chapter.<a id="idIndexMarker011"/></p>
    </li>
  </ul>

  <p class="body">The next three sections in this chapter describe the solution to the Airbnb NYC price prediction problem using each of these three stacks. In each section we will review the code for a solution and compare the pros and cons of the solution with those of our baseline, the Keras solution from ch<a id="idTextAnchor015"/>apter 3.<a id="idIndexMarker012"/><a id="idIndexMarker013"/></p>

  <h2 class="fm-head" id="heading_id_4">8.2 PyTorch with fastai</h2>

  <p class="body">Now let’s take a look at one of the toolkits: PyTorch/fastai. Because we’re considering the same dataset and problem we just discussed, we won’t rehash it here. Much of the solution is quite similar among the different toolkits. Here, we’ll concentrate on the distinctive portions of the PyTorch code. You can find the complete solution in the code repository for the book: <a class="url" href="https://mng.bz/gaBv">https://mn<span id="idTextAnchor016">g.bz/gaBv</span></a>.<a id="idIndexMarker014"/><a id="idIndexMarker015"/><a id="idIndexMarker016"/><a id="idIndexMarker017"/></p>

  <h3 class="fm-head1" id="heading_id_5">8.2.1 Reviewing the key code aspects of the fastai solution</h3>

  <p class="body"><a id="marker-304"/>Now let’s dive into the fastai solution to our Airbnb NYC listing price prediction problem. To start with, fastai has a unique set of imports, as shown in listing 8.2, that get the libraries required to use fastai in a Jup<a id="idTextAnchor017"/>yter notebook.<a id="idIndexMarker018"/><a id="idIndexMarker019"/></p>

  <p class="fm-code-listing-caption">Listing 8.2 Import statements for the fastai solution</p>
  <pre class="programlisting">!pip install -Uqq fastbook                               <span class="fm-combinumeral">①</span>
import fastbook
from fastbook import *                                   <span class="fm-combinumeral">②</span>
from fastai.tabular.all import *                         <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Installs the libraries for using fastai in a Jupyter notebook</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Imports the libraries for using fastai in a Jupyter notebook</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Imports the libraries for working with tabular datasets in fastai</p>

  <p class="body">With these libraries imported as shown in listing 8.2, you have the libraries required to run a fastai tabular data application in a Jupyter notebook.</p>

  <p class="body">Next, fastai needs to have the characteristics of the tabular dataset defined, including the column that contains the target for the model (called the <i class="fm-italics">dependent variable</i> in fastai) and the lists for the categorical and continuous columns, as shown in the fo<a id="idTextAnchor018"/>llowing listing.<a id="idIndexMarker020"/></p>

  <p class="fm-code-listing-caption">Listing 8.3 Dataset definition statements for the fastai solution</p>
  <pre class="programlisting">dep_var = 'target'                                        <span class="fm-combinumeral">①</span>
cat = ['neighbourhood_group','neighbourhood','room_type'] <span class="fm-combinumeral">②</span>
cont = \
['minimum_nights','number_of_reviews',\
'reviews_per_month','calculated_host_listings_count']     <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Specifies the column in the dataset that contains the target, the value that is being predicted by the trained model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Specifies the columns in the dataset that are categorical</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Specifies the columns in the dataset that are continuous</p>

  <p class="body">We’ll use the values defined in listing 8.3 when we define the <code class="fm-code-in-text">TabularDataloaders</code> (<a class="url" href="https://mng.bz/5gwO">https://mng.bz/5gwO</a>) object for this model. The <code class="fm-code-in-text">TabularDatalloaders</code> object encapsulates the samples from the dataset, including the labels, to make it easy to work with the dataset.<a id="idIndexMarker021"/><a id="idIndexMarker022"/></p>

  <p class="body">Next, we need to ensure that the target column contains string values:</p>
  <pre class="programlisting">merged_data['target'] =merged_data.target.astype(str)</pre>

  <p class="body">If we don’t do this, we will encounter a subtle problem. To try it for yourself, comment out this statement and run the fastai training notebook. You will see that the training produces some strange results, as show<a id="idTextAnchor019"/>n in figure 8.4.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F04_Ryan2.png"/></p>

    <p class="figurecaption">Figure 8.4 fastai training results when the target column is not explicitly converted to string values<a id="idIndexMarker023"/></p>
  </div>

  <p class="body"><a id="marker-305"/>Figure 8.4 shows the results for each epoch of the training process, including the training loss, the validation loss, and the accuracy. The accuracy values shown in figure 8.4 are significantly lower than the accuracy we saw in chapter 3 for the XGBoost and Keras deep learning solutions (between 79% and 81%), and accuracy does not improve from one epoch to another. Training for a larger number of epochs doesn’t help; the accuracy stays the same. Why does fastai produce such disappointing results? There’s a clue in the output of the <code class="fm-code-in-text">learn.loss_func</code> statement, as shown in the<a id="idTextAnchor020"/> following listing.<a id="idIndexMarker024"/></p>

  <p class="fm-code-listing-caption">Listing 8.4 Statement to show the loss function used in model training</p>
  <pre class="programlisting">learn.loss_func                                        <span class="fm-combinumeral">①</span>
FlattenedLoss of MSELoss()                             <span class="fm-combinumeral">②</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Statement that returns the loss function used in training the fastai model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Statement output showing the loss function used in training the fastai model</p>

  <p class="body">The output shown in listing 8.4 shows the loss function being used for the model. If you don’t specify a loss function for the fastai model, fastai selects a loss function based on the values in the target column. We want to train a classification model, so the loss function should be cross-entropy. However, it looks like fastai selected a loss function for a regression problem rather than a classification problem. That’s why the training results shown in listing 8.4 are bad—fastai is trying to solve a classification problem (predicting a continuous value) rather than the classification problem we intended (predicting a 0 or a 1 to indicate whether the listing has a price above or below the median price).</p>

  <p class="body">The output of <code class="fm-code-in-text">dls.valid.show_batch()</code>, as shown in figure 8.5, gives us another clue because the values in the <code class="fm-code-in-text">target</code> column are floating point when they shou<a id="idTextAnchor021"/>ld be “0” or “1.”<a id="idIndexMarker025"/><a id="marker-306"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F05_Ryan2.png"/></p>

    <p class="figurecaption">Figure 8.5 Sample batch values when the target column is not explicitly converted to string values</p>
  </div>

  <p class="body">If we go back and look at the dataset using <code class="fm-code-in-text">merged_data.head()</code>, as shown in figure 8.6, the values in the <code class="fm-code-in-text">target</code> column all <a id="idTextAnchor022"/>look like 0 or 1.<a id="idIndexMarker026"/></p>

  <p class="body">These values in the <code class="fm-code-in-text">target</code> column are, in fact, numeric values, which means that if we don’t explicitly convert them to strings, then by default fastai will assume that if we use this dataset to train a model, the model desired is a regression model.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F06_Ryan2.png"/></p>

    <p class="figurecaption">Figure 8.6 Sample batch values when the target column is explicitly converted to string values</p>
  </div>

  <p class="body">Now that we have examined why it’s critical to convert the values in the <code class="fm-code-in-text">target</code> column to string values, let’s review the rest of the code to create a trained fastai model. Listing 8.5 shows the block of code that defines the <code class="fm-code-in-text">TabularDataLoaders</code> object. This object is a tabular-data specific wrapper around the PyTorch <code class="fm-code-in-text">DataLoader</code> (<a class="url" href="https://mng.bz/6eDe">https://mng.bz/6eDe</a>) object, which is an iterable encapsulation of the samples and <a id="idTextAnchor023"/>labels in a dataset.<a id="marker-307"/><a id="idIndexMarker027"/><a id="idIndexMarker028"/></p>

  <p class="fm-code-listing-caption">Listing 8.5 Defining the <code class="fm-code-in-text">TabularDataLoaders</code> object</p>
  <pre class="programlisting">path = '.'                                                <span class="fm-combinumeral">①</span>
procs = [FillMissing,Categorify, Normalize]               <span class="fm-combinumeral">②</span>
dls = TabularDataLoaders.from_df(dtrain,                  <span class="fm-combinumeral">③</span>
                                 path,
                                 procs= procs,            <span class="fm-combinumeral">④</span>
                                 cat_names= cat,          <span class="fm-combinumeral">⑤</span>
                                 cont_names = cont,       <span class="fm-combinumeral">⑥</span>
                                 y_names = dep_var,       <span class="fm-combinumeral">⑦</span>
                                 valid_idx=\
list(range((merged_data.shape[0]-10000),
merged_data.shape[0])),                                   <span class="fm-combinumeral">⑧</span>
                                 bs=32)                   <span class="fm-combinumeral">⑨</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Sets a placeholder value for the path object</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Defines the transformations procedures to be applied to the dataset in the implied pipeline</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Specifies that the TabularDataLoaders object is based on the merged_data dataframe</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Specifies the list of transformations to apply with the TabularDataLoaders object</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Specifies the categorical features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Specifies the continuous features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Specifies the target feature</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Specifies the subset of the dataset to use for validation in the training process</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Specifies the batch size</p>

  <p class="body">We will use the <code class="fm-code-in-text">TabularDataLoaders</code> object defined in listing 8.5 to define the fastai model shown in listing 8.7.<a id="idIndexMarker029"/></p>

  <p class="body">One of the characteristics of fastai is a set of convenience functions that makes it easy to examine the dataset through the stages of training. The <code class="fm-code-in-text">show_batch()</code> statement shown in the following listing is an example of such <a id="idTextAnchor024"/>a convenience function.<a id="marker-308"/><a id="idIndexMarker030"/></p>

  <p class="fm-code-listing-caption">Listing 8.6 Statement to show a batch of the training data</p>
  <pre class="programlisting">dls.valid.show_batch()</pre>

  <p class="body">The statement shown in listing 8.6 makes it easy to see what the data that is training the model looks like after the transformations specified in the <code class="fm-code-in-text">procs</code> parameter of the <code class="fm-code-in-text">TabularDataLoaders</code> definition. Figure 8.7 shows the ou<a id="idTextAnchor025"/>tp<a id="idTextAnchor026"/>ut of this statement.<a id="idIndexMarker031"/><a id="idIndexMarker032"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F07_Ryan2.png"/></p>

    <p class="figurecaption">Figure 8.7 Output of the <code class="fm-code-in-text">show_batch()</code> statement<a id="idIndexMarker033"/></p>
  </div>

  <p class="body">Now that we have specified the data that will be used to train the model, it’s time to d<a id="idTextAnchor027"/>efine and train the model.</p>

  <p class="fm-code-listing-caption">Listing 8.7 Defining and fitting the fastai model</p>
  <pre class="programlisting">learn = tabular_learner(dls, metrics=accuracy)             <span class="fm-combinumeral">①</span>
learn.fit_one_cycle(3)                                     <span class="fm-combinumeral">②</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines the model as a tabular_learner object using the TabularDataLoaders object dls and using accuracy as the performance measurement for training</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Trains the model with three epochs</p>

  <p class="body">Note that the statements in listing 8.7 that define and fit the model are much simpler than the model definition and fit statements that we saw for the Keras model in chapter 3. In this sense, the code for the fastai solution resembles the code for the XGBoost solution that we saw in chapter 3.</p>

  <p class="body">Figure 8.8 shows the output of the fit statement. For each epoch, the training loss, validation loss, and accuracy are listed. If we compare the training results shown in figure 8.4 (when the target column was not explicitly converted to string values) with the training results shown in figure 8.8 (when the target column was converted to string values), it’s clear that we get better results when fastai treats the problem as a classification problem rather tha<a id="marker-309"/><a id="idTextAnchor028"/>n a regression problem.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F08_Ryan2.png"/></p>

    <p class="figurecaption">Figure 8.8 Output of the fit statement<a id="idIndexMarker034"/></p>
  </div>

  <p class="body">Before continuing with the rest of the fastai solution, let’s take a moment to discuss the relationship between training loss, validation loss, and test loss. Figure 8.8 shows the training and validation loss at each epoch. Training loss that is lower than validation loss indicates that the model could be underfit or that regularization techniques that only apply to training (such as dropout) are having an outsize effect. Figure 8.8 shows the validation loss as being lower than the training loss in the first epoch. For the subsequent epochs, the training loss drops faster than the validation loss until it is lower than the validation loss by the final epoch.</p>

  <p class="body">The following listing confirms that fastai is treating the problem as a classification problem because the loss function is <code class="fm-code-in-text">CrossEntropyLoss()</code>, a loss function that is appropriate for<a id="idTextAnchor029"/> a classification problem.<a id="idIndexMarker035"/></p>

  <p class="fm-code-listing-caption">Listing 8.8 Statement to show the loss function used in model training</p>
  <pre class="programlisting">learn.loss_func                                         <span class="fm-combinumeral">①</span>
FlattenedLoss of CrossEntropyLoss()                     <span class="fm-combinumeral">②</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Statement that returns the loss function used in training the fastai model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Statement output showing the loss function used in training the fastai model</p>

  <p class="body">The output shown in listing 8.8 establishes that we now get the desired loss function after setting the target column to contain string values. Now let’s look at what layers fastai defines for the model. The following listing shows the <code class="fm-code-in-text">summary()</code> statement, which lets us see the layers that<a id="idTextAnchor030"/> make up the fastai model. <a id="marker-310"/><a id="idIndexMarker036"/></p>

  <p class="fm-code-listing-caption">Listing 8.9 Statement to get a summary of the fastai model</p>
  <pre class="programlisting">learn.summary()</pre>

  <p class="body">The output of the statement in listing 8.9 is shown in figure 8.9, which shows the output of the <code class="fm-code-in-text">summary()</code> statement, including the layers that make up the model along with the number of parameters in the <a id="idTextAnchor031"/>model and callbacks used.<a id="idIndexMarker037"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F09_Ryan2.png"/></p>

    <p class="figurecaption">Figure 8.9 Output of the <code class="fm-code-in-text">summary()</code> statement<a id="idIndexMarker038"/></p>
  </div>

  <p class="body">Now that we have examined the key code areas in the fastai solution, let’s revisit the stack diagram to see where the fastai stack fits. Figure 8.10 shows the deep learning with tabular data stack from the example in this section. Note that the figure shows fastai as both a high-level API and a tabular data library since fastai plays<a id="idTextAnchor032"/> both roles in the stack.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F10_Ryan2.png"/></p>

    <p class="figurecaption">Figure 8.10 The stack for PyTorch with fastai</p>
  </div>

  <p class="body">Now that we have reviewed the code in the fastai solution, in the next section we’ll compare this solution to the Keras soluti<a id="idTextAnchor033"/>on that we saw in chapter 3.<a id="idIndexMarker039"/><a id="idIndexMarker040"/></p>

  <h3 class="fm-head1" id="heading_id_6">8.2.2 Comparing the fastai solution with the Keras solution</h3>

  <p class="body"><a id="marker-311"/>We have now seen two deep learning solutions to the Airbnb NYC listing price prediction problem: the Keras solution and the fastai solution. In this section, we’ll compare the two solutions and review the pros and cons of each.<a id="idIndexMarker041"/><a id="idIndexMarker042"/><a id="idIndexMarker043"/></p>

  <p class="body">The fastai and Keras solutions make interesting comparison points because they are very different. The Keras solution contains a lot of custom code, and all the details are evident. The fastai framework infers details about the model from the dataset and makes assumptions about the defaults to use so that there aren’t many parameters that we need to specify to get a working model. The benefit of this is that the fastai code is much more compact than the Keras code. In particular, the Keras solution has multiple lines of code to specify the pipeline and the details of the layers that make up the model. In the fastai solution, we get the pipeline for free by simply specifying the transformations that we want applied to the input data (as shown in listing 8.6), and we don’t need to specify the layers that make up the model. The downside of the compactness of the fastai solution is that subtle problems can get introduced if we’re not careful. In the previous section, we saw that if we don’t explicitly convert the target column to string values, then fastai will interpret the values in the target column as continuous values and assume we want to train a regression model rather than a classification model.</p>

  <p class="body">Table 8.1 shows a summary of the pros and cons of the Keras and fastai solutions to the Airbnb NYC problem. If we compare the performance of the two solutions, the Keras model gets between 70% and 74% accuracy, while the fastai model consi<a id="idTextAnchor034"/>stently gets around 81% accuracy.<a id="idIndexMarker044"/><a id="idIndexMarker045"/><a id="idIndexMarker046"/><a id="idIndexMarker047"/></p>

  <p class="fm-table-caption">Table 8.1 Summary of the pros and cons of the Keras and fastai solutions</p>

  <table border="1" class="contenttable-1-table" id="table001" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="20%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
    </colgroup>

    <thead class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <th class="contenttable-1-th"/>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Keras</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">fastai</p>
        </th>
      </tr>
    </thead>

    <tbody class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Pro</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Model details are transparent.</p>

          <p class="fm-table-body">Large community using the framework means that it’s easy to find solutions to common problems</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Framework includes explicit support for tabular data models, which means the code is much more compact.</p>

          <p class="fm-table-body">Framework automatically defines pipeline.</p>

          <p class="fm-table-body">Framework includes convenience functions that make it easy to examine the dataset.</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Con</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">No built-in support for tabular data, which means we need to define custom code to define the pipeline and layers for the model.</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Assumptions made by the framework can lead to tricky problems that are hard to debug.</p>

          <p class="fm-table-body">User community is smaller and less involved in deploying production applications than the Keras community, which means it can be harder to find solutions to problems.</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">Let’s cover one more point of comparison between the Keras solution and the fastai solution: the underlying low-level deep learning framework. For Keras, the underlying framework is TensorFlow, while fastai is built on top of PyTorch. This means we have now reviewed deep learning solutions for tabular data problems with both of the major deep learning frameworks.</p>

  <p class="body">One of the similarities between Keras and fastai is that they are both general-purpose high-level deep learning APIs. We have seen they can both be used for tabular data problems, but they are also designed to deal with a range of data types, not just tabular data. In the next section, we look at a deep learning library that is specifically designed for tabular data problems: TabNet. We examine a solution for the Airbnb NYC problem that uses TabNet and then contrast it with the Keras solution.</p>

  <h2 class="fm-head" id="heading_id_7">8.3 <a id="idTextAnchor035"/>PyTorch with TabNet</h2>

  <p class="body"><a id="marker-312"/>The two tools we’ve considered so far are designed as general deep learning libraries. Now, we will try out a purpose-built tabular data library: TabNet. Again, we’ll skip the introduction to the problem and concentrate our discussion only on the parts of the solution that differ from the previous examples. You can find the code for this solution at <a class="url" href="https://mng.bz/oK1Z">https://mng.bz/oK1Z</a>.<a id="idIndexMarker048"/><a id="idTextAnchor036"/><a id="idIndexMarker049"/><a id="idIndexMarker050"/></p>

  <h3 class="fm-head1" id="heading_id_8">8.3.1 Key code aspects of the TabNet solution</h3>

  <p class="body">In this section, we’ll go through the key parts of the code that make up the TabNet solution to the Airbnb NYC listing price prediction problem.<a id="idIndexMarker051"/><a id="idIndexMarker052"/><a id="idIndexMarker053"/></p>

  <p class="body">The TabNet solution requires a set of imports, as shown in the following lis<a id="idTextAnchor037"/>ting.</p>

  <p class="fm-code-listing-caption">Listing 8.10 Import statements for TabNet</p>
  <pre class="programlisting">! pip install pytorch-tabnet                                <span class="fm-combinumeral">①</span>
import torch                                                <span class="fm-combinumeral">②</span>
from pytorch_tabnet.tab_model import TabNetClassifier       <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Installs the PyTorch implementation of TabNet</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Imports the torch tensor library</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Imports the TabNetClassifier library. We will use this library to define the model.</p>

  <p class="body">Note that, unlike the fastai import statements, the import statements for TabNet in listing 8.10 include an explicit statement to import the PyTorch library <code class="fm-code-in-text">torch</code>.</p>

  <p class="body"><a id="marker-313"/>Unlike the fastai solution, which does not have any explicit code to define the pipeline and has unique code to define the dataset, the TabNet solution uses the same code as the Keras and XGBoost solutions up to and including the definition of the pipelines. After the pipeline definitions, the TabNet solutions use code similar to XGBoost to convert the list of NumPy arrays that comes out of the pipeline into a NumPy array of lists, as shown in the following lis<a id="idTextAnchor038"/>ting.</p>

  <p class="fm-code-listing-caption">Listing 8.11 Statements to generate NumPy arrays of lists</p>
  <pre class="programlisting">list_of_lists_train = []
list_of_lists_test = []
list_of_lists_valid = []
for i in range(0,7):                                              <span class="fm-combinumeral">①</span>
    list_of_lists_train.append(X_train_list[i].tolist())
    list_of_lists_valid.append(X_valid_list[i].tolist())
    list_of_lists_test.append(X_test_list[i].tolist())
X_train = np.array(list_of_lists_train).T                         <span class="fm-combinumeral">②</span>
X_valid = np.array(list_of_lists_valid).T                         <span class="fm-combinumeral">③</span>
X_test = np.array(list_of_lists_test).T                           <span class="fm-combinumeral">③</span>
y_train = dtrain.target                                           <span class="fm-combinumeral">④</span>
y_valid = dvalid.target
y_test = test.target</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines lists of lists for the training, validation, and test datasets (one list for each feature)</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Converts the train list of lists into a NumPy array of lists</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Converts the validation list of lists into a NumPy array of lists</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Defines variables for the training, validation, and test target sets</p>

  <p class="body">The transformations shown in listing 8.11 are needed because the TabNet solution expects the input to the model to be in the form of a NumPy array of lists. Next, the TabNet solution includes code to define the m<a id="idTextAnchor039"/>odel.</p>

  <p class="fm-code-listing-caption">Listing 8.12 Statements to define the TabNet model</p>
  <pre class="programlisting">tb_cls = TabNetClassifier(optimizer_fn=torch.optim.Adam,           <span class="fm-combinumeral">①</span>
                    optimizer_params=dict(lr=1e-3),                <span class="fm-combinumeral">②</span>
                    scheduler_params={"step_size":10,"gamma":0.9}, <span class="fm-combinumeral">③</span>
                    scheduler_fn=torch.optim.lr_scheduler.StepLR,
                    mask_type='entmax' # "sparsemax"
                    )</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines a TabNetClassifier object as the model for the solution and specify an adam optimizer</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Sets the learning rate for the model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Sets parameters for the learning rate scheduler</p>

  <p class="body">The model definition shown in listing 8.12 specifies a set of hyperparameters, including the optimizer and learning rate. Next, the TabNet solution includes code to train the model.<a id="marker-314"/><a id="idTextAnchor040"/></p>

  <p class="fm-code-listing-caption">Listing 8.13 Statements to train the TabNet model</p>
  <pre class="programlisting">tb_cls.fit(X_train, y_train,                                       <span class="fm-combinumeral">①</span>
               eval_set=[(X_train, y_train),(X_valid, y_valid)],   <span class="fm-combinumeral">②</span>
               eval_name=['train', 'valid'],                       <span class="fm-combinumeral">③</span>
               eval_metric=['accuracy'],                           <span class="fm-combinumeral">④</span>
               max_epochs=10 , patience=3,                         <span class="fm-combinumeral">⑤</span>
               batch_size=28, drop_last=False)                     <span class="fm-combinumeral">⑥</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Specifies the training dataset</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Specifies the validation dataset</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Specifies the labels for the training and validation results</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Specifies the metric used to track training performance</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Specifies the number of epochs in the training run and how many epochs to run once the model stops improving</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Specifies the batch size</p>

  <p class="body">The statement in listing 8.13 that specifies the training for the TabNet model includes early stopping setting, including the <code class="fm-code-in-text">patience</code> parameter that indicates how many epochs the training will continue once the model stops improving.</p>

  <p class="body">The output of the training statement shows the results of each epoch, including loss, training accuracy, and validation accuracy, as well as the effect of early stopping, as shown in figure 8.1<a id="idTextAnchor041"/>1.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F11_Ryan2.png"/></p>

    <p class="figurecaption">Figure 8.11 Output of the TabNet fit statement</p>
  </div>

  <p class="body">In the training run output shown in figure 8.11, the maximum number of epochs (10) are run because the validation accuracy does not stop improving for more than 2 epochs until the maximum number of epochs is reached. This means that the <code class="fm-code-in-text">patience</code> threshold of 3 set in the <code class="fm-code-in-text">fit</code> statement is never crossed, so the training run goes for the maximum number of epochs. Figure 8.12 shows the deep learning with tabular data stack from the example in this section<a id="idTextAnchor042"/>.<a id="idIndexMarker054"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F12_Ryan2.png"/></p>

    <p class="figurecaption">Figure 8.12 The stack for PyTorch with TabNet</p>
  </div>

  <p class="body">Now that we have reviewed the code in the TabNet solution, in the next section we’ll compare this solution to the Keras solution that we saw in chapte<a id="idTextAnchor043"/>r 3.<a id="idIndexMarker055"/><a id="idIndexMarker056"/><a id="idIndexMarker057"/></p>

  <h3 class="fm-head1" id="heading_id_9">8.3.2 Comparing the TabNet solution with the Keras solution</h3>

  <p class="body"><a id="marker-315"/>We have now seen three deep learning solutions to the Airbnb NYC listing price prediction problem: the Keras solution, the fastai solution, and the TabNet solution. In this section we’ll compare the Keras solution with the TabNet solution and review the pros and cons of each.<a id="idIndexMarker058"/><a id="idIndexMarker059"/><a id="idIndexMarker060"/><a id="idIndexMarker061"/></p>

  <p class="body">The Keras solution and the TabNet solution are interesting to compare because they demonstrate some of the strengths and weaknesses of their underlying frameworks, TensorFlow and PyTorch. The Keras solution benefits from the simple <code class="fm-code-in-text">summary()</code> statement that gives a compact list of the layers that make up the model. PyTorch lacks this elegant feature, so the TabNet solution is also missing that benefit. Keras, on the other hand, does not provide built-in control for the training process, so you have to define callbacks to ensure that you end up with the optimal model from the training run at the end of the run and that you don’t waste resources running epochs when the model has stopped improving. PyTorch, on the other hand, incorporates early stopping and saving of the optimal model by default, so the TabNet solution doesn’t need to include code to explicitly define callbacks to optimize the training process. Table 8.2 shows a summary of the pros and cons of the Keras and TabNet with PyTorch solutions to the Airbnb N<a id="idTextAnchor044"/>YC problem.<a id="idIndexMarker062"/></p>

  <p class="fm-table-caption">Table 8.2 Summary of the pros and cons of the Keras and TabNet solutions</p>

  <table border="1" class="contenttable-1-table" id="table002" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="20%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
    </colgroup>

    <thead class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <th class="contenttable-1-th"/>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Keras</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">TabNet</p>
        </th>
      </tr>
    </thead>

    <tbody class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Pro</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Large community using the framework means that it’s easy to find solutions to common problems</p>

          <p class="fm-table-body">Simple summary statement to show the layers in the model</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Simple statements to define and train the model</p>

          <p class="fm-table-body">Don’t need an explicitly defined callback to get the benefit of early stopping</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Con</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">No built-in support for tabular data, which means the model definition needs to be hand-coded.</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Training process is much slower. The Keras model training notebook took ~20 seconds to run. The TabNet training notebook took over 4 minutes.</p>

          <p class="fm-table-body">No one-stop summary statement to see the structure of the model</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">In this section we reviewed the Airbnb NYC pricing prediction solution for PyTorch TabNet. In the next section we will review our final approach to the Airbnb problem: PyTorch with Lightning Flash.<a id="idIndexMarker063"/><a id="idTextAnchor045"/><a id="idIndexMarker064"/><a id="idIndexMarker065"/></p>

  <h2 class="fm-head" id="heading_id_10">8.4 PyTorch with Lightning Flash</h2>

  <p class="body">So far, we’ve considered Keras, fastai, and TabNet PyTorch solutions for the Airbnb NYC price prediction problem. Now let’s turn to our final stack: Lightning Flash. As a platform designed for fast prototyping, baselining, and fine-tuning, along with a clear API and outstanding documentation, Lightning Flash potentially offers advantages over the stacks we have explored so far. <a id="idIndexMarker066"/><a id="idIndexMarker067"/><a id="idIndexMarker068"/><a id="idIndexMarker069"/><a id="marker-316"/></p>

  <p class="body">You can find the code for this solution at <a class="url" href="https://mng.bz/vKnp">https://mng.bz/vKnp</a>. Figure 8.13 shows the fastai and Tabnet on PyTorch stac<a id="idTextAnchor046"/>ks.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F13_Ryan2.png"/></p>

    <p class="figurecaption">Figure 8.13 The fastai and Tabnet on PyTorch s<a id="idTextAnchor047"/>tacks</p>
  </div>

  <h3 class="fm-head1" id="heading_id_11">8.4.1 The key code aspects of the Lightning Flash solution</h3>

  <p class="body">The code for the Lightning Flash solution has many aspects that are different from the solutions we have seen so far. In this section, we go through the model training notebook (<a class="url" href="https://mng.bz/4aDR">https://mng.bz/4aDR</a>) to highlight the most interesting points about this solution.<a id="idIndexMarker070"/><a id="idIndexMarker071"/><a id="idIndexMarker072"/><a id="idIndexMarker073"/></p>

  <p class="body">To work in Colab, the Lightning Flash solution requires a set of installs done in a particular order, as shown in listing 8.14. The source for this list is <a class="url" href="https://mng.bz/QDP6">https://mng<span id="idTextAnchor048">.bz/QDP6</span></a>.</p>

  <p class="fm-code-listing-caption">Listing 8.14 Installs required to get Lightning Flash to work in Colab</p>
  <pre class="programlisting">!pip install torch==1.8.1+cu102 -f 
https://download.pytorch.org/whl/torch_stable.html          <span class="fm-combinumeral">①</span>
!pip install icevision #==0.9.0a1
!pip install effdet 
!pip install lightning-flash[image]
!pip install git+https://github.com/PyTorchLightning/lightning-flash.git
!pip install torchtext==0.9.1
!pip uninstall fastai -y                                    <span class="fm-combinumeral">②</span>
!curl https://raw.githubusercontent.com/airctic/ \
icevision/944b47c5694243ba3f3c8c11a6ef56f05fb111eb/ \
icevision/core/record_components.py –output \
    /usr/local/lib/python3.7/dist- \
packages/icevision/core/record_components.py                <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Series of pip installs to get the required levels of PyTorch Lightning</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> To eliminate potential conflicts between fastai and Lightning Flash, fastai needs to be uninstalled.</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Manually applies a fix for a bug in the current release of icevision</p>

  <p class="body"><a id="marker-317"/>Listing 8.14 includes a long list of installs (and one uninstall) to get Lightning Flash to work in Colab. From experience, we know that this very specific list of installs is required or there will be conflicts between the levels of libraries that Lightning Flash requires and the default library levels for Colab.</p>

  <p class="body">Next, the libraries required by Lightning Flash need to be <a id="idTextAnchor049"/>imported.</p>

  <p class="fm-code-listing-caption">Listing 8.15 Library imports required by Lightning Flash</p>
  <pre class="programlisting">import torch                                                <span class="fm-combinumeral">①</span>
import flash                                                <span class="fm-combinumeral">②</span>
from flash.tabular import TabularClassificationData, 
from flash.tabular import TabularClassifier                 <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Imports the torch tensor library</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Imports the flash library</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Imports the objects needed for a tabular classification model</p>

  <p class="body">Note that the torch import statement in listing 8.15 is the same statement that you saw to import torch in listing 8.11 for TabNet.</p>

  <p class="body">Next, we define the parameters for the dataset that we will use to train t<a id="idTextAnchor050"/>he model.</p>

  <p class="fm-code-listing-caption">Listing 8.16 Setting dataset parameters</p>
  <pre class="programlisting">dep_var = 'target'                                          <span class="fm-combinumeral">①</span>
cat=['neighbourhood_group','neighbourhood','room_type']     <span class="fm-combinumeral">②</span>
cont = ['minimum_nights','number_of_reviews',
'reviews_per_month','calculated_host_listings_count']       <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Sets the target field as the value that the trained model will predict</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Defines the list of categorical features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Defines the list of continuous features</p>

  <p class="body">The definitions in listing 8.16 should remind you of a similar block of code in the fastai solution (listing 8.3), where we defined the target feature along with lists for the categorical and continuous features.</p>

  <p class="body">Next we use the values we just defined to define a <code class="fm-code-in-text">TabularClassificationData</code> object. This object specifies the minimum characteristics of the dataset that we will use to train t<a id="idTextAnchor051"/>he model.<a id="marker-318"/><a id="idIndexMarker074"/></p>

  <p class="fm-code-listing-caption">Listing 8.17 Defining a <code class="fm-code-in-text">TabularClassificationData</code> </p>
  <pre class="programlisting">datamodule = TabularClassificationData.from_csv(
    categorical_fields=cat,                                <span class="fm-combinumeral">①</span>
    numerical_fields=cont,                                 <span class="fm-combinumeral">②</span>
    target_fields="target",                                <span class="fm-combinumeral">③</span>
    train_file='../data/train.csv',                        <span class="fm-combinumeral">④</span>
    val_file='../data/valid.csv',                          <span class="fm-combinumeral">⑤</span>
    predict_file='../data/test.csv',                       <span class="fm-combinumeral">⑥</span>
    batch_size=64
)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines the categorical features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Defines the continuous features</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Defines the target feature</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Defines the training dataset</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Defines the validation dataset</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Defines the test dataset</p>

  <p class="body">We should explain why the definition of the <code class="fm-code-in-text">TabularClassificationData</code> object shown in listing 8.17 uses separate CSV files for the train, validation, and test datasets. For all the other solutions, the dataset is loaded from a pickle file that is the output of the data cleanup notebook (<a class="url" href="https://mng.bz/XxN9">https://mng.bz/XxN9</a>), which is then split into train, validation, and test datasets in the model training notebook. The Lightning Flash solution is different because it has distinct CSV files for each segment of the dataset. The reason for this is that the very specific installation requirements shown in listing 8.14 were incompatible with loading the pickle file that contains the output dataframe from the data cleanup notebook. As a workaround, we loaded that pickle file in a separate notebook into a pandas DataFrame and saved the separate CSV files for the training, validation, and test that you see in the definition of the <code class="fm-code-in-text">TabularClassificationData</code> object in listing 8.17. As an exercise, you could update the data cleanup notebook for the Lightning Flash solution so that it saves the cleaned-up dataset as three separate CSV files rather than as a single pickle file.<a id="idIndexMarker075"/><a id="idIndexMarker076"/></p>

  <p class="body">Now that we have specified the details about the dataset, we are ready to define and train the model, as shown in the follow<a id="idTextAnchor052"/>ing listing.</p>

  <p class="fm-code-listing-caption">Listing 8.18 Setting dataset parameters</p>
  <pre class="programlisting">model = TabularClassifier.from_data(datamodule, 
learning_rate=0.1)                                          <span class="fm-combinumeral">①</span>
trainer = flash.Trainer(max_epochs=3, 
gpus=torch.cuda.device_count())                             <span class="fm-combinumeral">②</span>
trainer.fit(model, datamodule=datamodule)                   <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines the model using the TabularClassifierData object defined in listing 8.18</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Defines a Trainer object</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Fits the model</p>

  <p class="body">The training code in listing 8.18 generates the output shown in fig<a id="idTextAnchor053"/>ure 8.14.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F14_Ryan2.png"/></p>

    <p class="figurecaption">Figure 8.14 Output of the Lightning Flash training process</p>
  </div>

  <p class="body"><a id="marker-319"/>Note the output includes the validation accuracy and training accuracy for the model. Figure 8.15 shows the deep learning with tabular data stack from the example in this <a id="idTextAnchor054"/>section.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F15_Ryan2.png"/></p>

    <p class="figurecaption">Figure 8.15 The PyTorch with Lightning Flash stack</p>
  </div>

  <p class="body">The code for the Lightning Flash solution incorporates some very elegant ideas, such as being able to specify the training, validation, and test datasets in the same object where you define the overall characteristics of the dataset. Overall, the API for Lightning Flash is easy to understand. Unfortunately, these benefits are undermined because Lightning Flash requires such specific requirements to run in Colab. Otherwise, Lightning Flash could have been a favorite, combining the simplicity of fastai with the intuitiveness<a id="idTextAnchor055"/> of Keras.<a id="idIndexMarker077"/><a id="idIndexMarker078"/><a id="idIndexMarker079"/><a id="idIndexMarker080"/></p>

  <h3 class="fm-head1" id="heading_id_12">8.4.2 Comparing the Lightning Flash solution with the Keras solution</h3>

  <p class="body">We have now seen four deep learning solutions to the Airbnb NYC listing price prediction problem: the Keras solution, the fastai solution, the TabNet solution, and, finally, the Lightning Flash solution. In this section we’ll compare the Keras solution with the Lightning Flash solution and review the pros and cons of each.<a id="idIndexMarker081"/><a id="idIndexMarker082"/><a id="idIndexMarker083"/><a id="idIndexMarker084"/><a id="idIndexMarker085"/></p>

  <p class="body">We’ve seen that Lightning Flash has some real advantages for rolling out a fast, simple solution. However, the lack of a beaten path to using Lightning Flash in Colab is concerning, and one has to wonder how long the elaborate set of installs shown in listing 8.15 will continue to make it possible to run Lightning Flash experiments in Colab. Table 8.3 shows a summary of the pros and cons of the Keras and Lightning Flash solutions to the <a id="idTextAnchor056"/>Airbnb NYC problem.</p>

  <p class="fm-table-caption">Table 8.3 Summary of the pros and cons of the Keras and TabNet solutions</p>

  <table border="1" class="contenttable-1-table" id="table003" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="20%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
    </colgroup>

    <thead class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <th class="contenttable-1-th"/>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Keras</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Lightning Flash</p>
        </th>
      </tr>
    </thead>

    <tbody class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Pro</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Large community using the framework means that it’s easy to find solutions to common problems.</p>

          <p class="fm-table-body">Simple summary statement to show the layers in the model</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Simple statements to define and train the model</p>

          <p class="fm-table-body">Don’t need to explicitly define a pipeline—just need to identify the categorical and continuous columns.</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Con</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">No built-in support for tabular data, which means the model definition needs to be hand-coded.</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">To get working in Colab, you need a very specific order and level of installs. Does not seem to be widely used, at least not with Colab.</p>

          <p class="fm-table-body">Out of the box, test accuracy was worse than other solutions.</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">So far in this chapter we have applied three deep learning with tabular data stacks to solve the Airbnb NYC price prediction problem and compared the pros and cons of each solution. In the next section, we review an overall comparison of all the stacks we exercised in this chapter.<a id="idIndexMarker086"/><a id="idTextAnchor057"/><a id="idIndexMarker087"/><a id="idIndexMarker088"/><a id="idIndexMarker089"/></p>

  <h2 class="fm-head" id="heading_id_13">8.5 Overall comparison of the stacks</h2>

  <p class="body">We have looked at a total of four deep learning for tabular data stacks. Table 8.4 summarizes the performance of all of these stacks on a default Colab setup, with a standard GPU Colab runtime option selected for the deep learning solutio<a id="idTextAnchor058"/>ns.<a id="idIndexMarker090"/><a id="marker-320"/></p>

  <p class="fm-table-caption">Table 8.4 Summary of accuracy and running time of the deep learning with tabular data stacks (plus XGBoost) for the Airbnb NYC listing price prediction problem</p>

  <table border="1" class="contenttable-1-table" id="table004" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="20%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
      <col class="contenttable-0-col" span="1" width="40%"/>
    </colgroup>

    <thead class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <th class="contenttable-1-th"/>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Test Accuracy</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Notebook running time</p>
        </th>
      </tr>
    </thead>

    <tbody class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">TensorFlow with Keras</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">81%</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">16 seconds</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">fastai with PyTorch</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">83%</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">69 seconds</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">TabNet with PyTorch</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">81%</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">568 seconds</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Lightning Flash with PyTorch</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">TBD</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">14 seconds</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">XGBoost</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">79%</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">14 seconds</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">It is important to note here that for the purposes of this comparison we didn’t do any tuning of any of the approaches. We wanted to have a genuine “apples-to-apples” comparison of how the approaches compare to the Keras baseline without additional tuning. In subsequent chapters, we will discuss some of the tuning that can be done to get optimal results from a deep learning solution.</p>

  <p class="body">For accuracy, fastai is slightly better than the other stacks. For execution time, Lightning Flash is best, and TabNet is the slowest. So what is the best stack to use for tabular data? First, if we compare XGBoost with any of the deep learning solutions, which is best? We will answer this question in more detail in chapter 9, but we can say now that if it’s a straight-up comparison between deep learning and gradient boosting, for the sake of simplicity and overall performance “out of the box,” gradient boosting approaches like XGBoost are currently better than any of the deep learning solutions for most datasets. There are use cases where it is worthwhile to explore one of the deep learning solutions, and we will review these use cases in chapter 9.</p>

  <p class="body">If you do decide to use a deep learning solution for a tabular data problem, of the four we explored so far, which one should you use? Here is our overall advice:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">If you are new to deep learning and are primarily interested in exploring a solution without needing to immediately implement the solution in production, fastai is the best choice. It is the simplest stack to use, and it has a big enough user community that you are not likely to be stuck with a problem that nobody has seen before. Fastai includes many convenience features to make it easy to work with tabular data, so you can rapidly prototype your solution. However, if you need to move your solution rapidly to production, fastai is probably not your best choice because it is not commonly used in production.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">If you are already comfortable with deep learning and you need to get an application into production, we recommend the Keras stack. First, TensorFlow, the low-level component of the stack, is the deep learning framework that is used most commonly in industry. Second, Keras has a huge user community. While Keras does not yet have native support for tabular data the same way that fastai does, Keras is high-level enough to lend itself to tabular data problems.</p>
    </li>
  </ul>

  <p class="body">This advice may change as more people use deep learning with tabular data. One of the other stacks, such as TabNet on PyTorch, could mature and become the default choice for deep learning with tabular data. However, looking at the current state of the art, we recommend fastai for beginners and explorers and Keras for people who are more experienced and need to proceed to production rapidly.</p>

  <p class="body"><a id="marker-321"/>In the next section we will discuss the stacks that we didn’t explore in this chapter.<a id="idIndexMarker091"/><a id="idTextAnchor059"/></p>

  <h2 class="fm-head" id="heading_id_14">8.6 The stacks we didn’t explore</h2>

  <p class="body">You may have asked why we chose three particular stacks (fastai, PyTorch TabNet, and Lightning Flash) to solve the Airbnb NYC problem and didn’t explore the other options, such as TabNet on TensorFlow, SAINT, or PyTorch Tabular. In this section, we’ll look at this question and see what the answer tells us about the options that we have for deep learning with tabular data stacks. Figure 8.16 shows the deep learning with tabular data stacks that we didn’t explore.<a id="idIndexMarker092"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F16_Ryan2.png"/><a id="idTextAnchor060"/></p>

    <p class="figurecaption">Figure 8.16 The stacks we didn’t explore</p>
  </div>

  <p class="body"><a id="marker-322"/>There are a couple of characteristics that the unexplored stacks have in common:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">All of the unexplored stacks involve dedicated tabular data libraries.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">We were not able to get any of the unexplored stacks to work in Colab, the environment that we used to exercise the code examples in this book.</p>
    </li>
  </ul>

  <p class="body">Our inability to get the unexplored stacks to work in Colab could be due to a number of reasons. There could theoretically be some limitations with Colab. However, Colab is a very common environment for exploration, so it’s not a good sign if a library cannot be coaxed to life in Colab. All of the stacks featured “hello world” examples for exercising the stack, and for all the unexplored stacks these examples generated errors, mostly to do with contradictory Python library prerequisites. It’s possible that if we had been more patient, or looked a little harder, we would have been able to work through the errors to get the basic examples to work in Colab. Of the stacks that we did explore, Keras, fastai, and PyTorch TabNet all worked “out of the box” in Colab. Lighting Flash, on the other hand, did require some tweaking before it worked in Colab.</p>

  <p class="body">The difference between Lightning Flash and the unexplored stacks is that it was clear that other people had tried to get Lightning Flash to work in Colab, and we could find informal documentation that showed exactly what we needed to do to get it to work in Colab. If your goal is to solve a tabular data problem with deep learning, you want to focus on the problem, not on fiddling with conda and pip installing boutique levels of libraries to avoid incompatibilities. By that criteria, Keras, fastai, PyTorch TabNet, and, to a lesser extent, Lightning Flash are viable choices for deep learning with tabular data in Colab. SAINT, DeepTables, PyTorch Tabular, and TabNet on TensorFlow are not viable choices for exploration in Colab because they don’t work right away, and the recipes to get them to work either don’t exist or are not easy to find.</p>

  <p class="body">While it is disappointing that we weren’t able to exercise the Airbnb NYC price prediction problem with more of the dedicated deep learning with tabular data libraries, we were still able to accomplish the goal of this chapter by exploring three deep learning with tabular data stacks. Figure 8.17 shows all the stacks that we were able to explore.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH08_F17_Ryan2.png"/></p>

    <p class="figurecaption"><a id="idTextAnchor061"/>Figure 8.17 The stacks we explored</p>
  </div>

  <p class="body"><a id="marker-323"/>The stacks we explored in this chapter, plus TensorFlow with Keras, present a well-rounded set of options:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">TensorFlow with Keras is a rock-solid stack with a huge community of users. For just about any problem that you encounter with this stack, you can bet that somebody else has hit the problem and posted a solution. The stack works flawlessly in Colab, so doing initial investigation is easy. TensorFlow with Keras is commonly used in production for all kinds of applications. On the downside, Keras does not have built-in support for tabular data, so you need to be prepared to write some custom code to tackle tabular data problems.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">PyTorch with fastai is designed to be easy to get started with, and it works flawlessly in Colab, so you can expect to prototype deep learning with tabular data problems with this stack with minimal hassle. fastai treats tabular data as a first-class citizen, so you get built-in support for dealing with categorical and continuous features and you don’t need to worry about hand-coding a pipeline to ensure that when you feed data to the trained model to get a prediction, the data goes through the same transformations as the data used to train the model. On the downside, you can pay a price for the simplicity of fastai code. The automated steps that fastai takes can lead to some hard-to-debug problems (such as the problem we described in this chapter where the wrong kind of model gets trained if the target column isn’t explicitly converted to a string type), and you need to be prepared to dig deep into the fastai API if you want to go off the beaten path provided by fastai’s tabular data structures.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">PyTorch with TabNet distinguishes itself among the tabular-data-specific libraries by working in Colab without any fuss. Like fastai, with TabNet you can define and train a deep learning model on tabular data with just a handful of lines of code. Unlike fastai, TabNet uses conventional APIs that are easy for anybody who has used Scikit-learn to understand. Compared to the other stacks, TabNet took longer to train the model. Also, as a library specifically designed for tabular data, TabNet has a smaller user community than fastai and Keras, which means that if you run into problems it will be less likely that somebody else has already hit them and documented a fix on Stack Overflow.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">PyTorch with Lightning Flash trains the model quickly and has a simple API, once you get it to work. Lightning has a large community—not as big as Keras but bigger than fastai. However, the niche of using Lightning Flash on Colab to do tabular data problems is not that big, and we were only just able to get Lightning Flash to work on Colab.</p>
    </li>
  </ul>

  <p class="body">In this chapter we have explored a set of deep learning with tabular data stacks, compared the pros and cons of each approach, and discussed why we left some other stacks unexplored. In the next chapter we are going to review the best practices for deep learning with tabular data.<a id="idIndexMarker093"/><a id="marker-324"/><a id="idTextAnchor062"/></p>

  <h2 class="fm-head" id="heading_id_15">Summary</h2>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">There are two low-level deep learning frameworks: TensorFlow and PyTorch.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">TensorFlow is used more frequently in industry.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">PyTorch is the predominant deep learning framework for research.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Keras is a high-level API for TensorFlow.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">fastai is both a general-purpose, high-level API for PyTorch and a tabular data library.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">PyTorch Lightning is a high-level API that abstracts some of the details of PyTorch. Lightning Flash is a tabular data library based on Lightning.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Lightning Flash and fastai each provide some of the same benefits for PyTorch that Keras does for TensorFlow by abstracting aspects of the underlying PyTorch framework.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">TabNet is a tabular data library that is available for both TensorFlow and PyTorch.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">SAINT is a tabular data library for TensorFlow.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">PyTorch Tabular is a tabular data library for PyTorch.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Of all the choices available, TensorFlow with Keras, PyTorch with fastai, PyTorch with TabNet, and PyTorch with Lightning Flash are all valid options for deep learning with tabular data on Colab.<a id="marker-325"/></p>
    </li>
  </ul>
</body></html>