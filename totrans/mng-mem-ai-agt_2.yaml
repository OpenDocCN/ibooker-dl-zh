- en: 'Chapter 2\. Long-Term Memory: Building Persistent Learning Agents'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：长期记忆：构建持久学习代理
- en: 'There’s no single, universal definition for the different types of memory in
    agent systems—every company seems to have its own take. For example, Anthropic,
    OpenAI, and Google all use slightly different terminology and approaches. But
    the more interesting question is: how does an agent actually decide what counts
    as procedural, semantic, or episodic memory? Or put another way: what gets treated
    as short-term, long-term, or contextual memory?'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 对于代理系统中不同类型的记忆，没有单一、普遍的定义——每家公司似乎都有自己的看法。例如，Anthropic、OpenAI和Google都使用略有不同的术语和方法。但更有趣的问题是：代理实际上是如何决定什么算作程序性、语义性或情景记忆的？或者换句话说：什么被视为短期、长期或情境记忆？
- en: Along these lines, semantic caching might play a big role. Sometimes short-term
    memories can be promoted to long-term if they’re accessed frequently enough. Conversely,
    long-term memories that aren’t used much might get summarized, become less detailed,
    or even be dropped from the system altogether. These are the kinds of trade-offs
    and decisions that go into designing agent memory. Ultimately, it all comes down
    to how the system is built to manage and retain information. Since it’s nearly
    impossible to program an agent to handle every scenario, we rely on constraints
    and parameters to guide what gets treated as episodic, semantic, or procedural
    memory.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着这个思路，语义缓存可能发挥重要作用。有时，如果短期记忆被频繁访问，它们可以被提升为长期记忆。相反，很少使用的长期记忆可能会被总结，变得不那么详细，甚至可能完全从系统中删除。这些就是设计代理记忆时需要考虑的权衡和决策。最终，一切都取决于系统是如何构建来管理和保留信息的。由于几乎不可能编程代理来处理每个场景，我们依赖于约束和参数来指导哪些被视为情景记忆、语义记忆或程序记忆。
- en: Types of Long-Term Memory
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长期记忆类型
- en: 'The industry has converged on three primary types of long-term memory, although
    implementations vary significantly:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 行业已经汇聚为三种主要的长期记忆类型，尽管实现方式差异很大：
- en: Episodic memory
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 情景记忆
- en: Stores specific past experiences and events, functioning like human autobiographical
    memory. Companies typically implement this through RAG systems on conversation
    histories, extracting relevant chunks instead of keeping the full history.^([1](ch02.html#id64))
    The common approach uses few-shot example prompting where agents learn from past
    sequences, with key events, actions, and outcomes logged in structured formats.^([2](ch02.html#id65))
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 存储特定的过去经验和事件，功能类似于人类的自传体记忆。公司通常通过在对话历史中实施RAG系统来实现这一点，提取相关的片段而不是保留完整的历史。[1](ch02.html#id64)
    常见的方法是使用少样本示例提示，其中代理从过去的序列中学习，关键事件、行动和结果以结构化格式记录。[2](ch02.html#id65)
- en: Semantic memory
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 语义记忆
- en: Maintains structured factual knowledge—facts, definitions, rules—implemented
    through knowledge bases, symbolic AI, or vector embeddings. LLMs extract information
    from conversations, storing it as user or entity profiles that are retrieved and
    inserted into system prompts to influence future responses.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 维护结构化事实知识——事实、定义、规则——通过知识库、符号人工智能或向量嵌入实现。LLM从对话中提取信息，将其存储为用户或实体配置文件，这些配置文件被检索并插入到系统提示中，以影响未来的响应。
- en: Procedural memory
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 程序记忆
- en: The least common area, but a growing one, which stores skills, rules, and learned
    behaviors for automatic task performance. This combines LLM weights, agent code,
    and system prompts, with some agents updating their own prompts through “reflection”
    or metaprompting.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最少共同区域，但正在增长，它存储了用于自动任务执行的技能、规则和已学习的行为。这结合了LLM权重、代理代码和系统提示，一些代理通过“反思”或元提示来更新自己的提示。
- en: The field of agentic AI is moving away from rigid definitions of memory toward
    more flexible hybrid approaches, where memories can transition between types based
    on usage patterns and importance scoring. As Rowan Trollope, CEO of Redis, observes,
    this mirrors human memory consolidation. Just as our REM cycle compresses information
    from short-term to long-term memory as we sleep, the goal is to build agents to
    engage in similar processes.^([3](ch02.html#id66))
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 代理人工智能领域正从对记忆的严格定义转向更灵活的混合方法，其中记忆可以根据使用模式和重要性评分在类型之间转换。正如Redis的首席执行官Rowan Trollope观察到的那样，这反映了人类记忆的巩固。正如我们的REM周期在我们睡觉时将信息从短期记忆压缩到长期记忆一样，目标是构建能够参与类似过程的代理。[3](ch02.html#id66)
- en: Long-Term Memory in Frameworks
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 框架中的长期记忆
- en: 'The practical implementation of long-term memory varies dramatically across
    frameworks, with each taking a different philosophical approach to the challenge:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 长期记忆的实际实现跨框架差异很大，每个框架都采取了不同的哲学方法来应对挑战：
- en: LangGraph Stores
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: LangGraph Stores
- en: From the creators of LangChain, a framework for LLM and agent development, this
    system organizes memory in namespaces as JSON documents with unique identifiers,
    supporting semantic facts, user preferences, episodic examples, and procedural
    system prompts. The LangMem SDK adds tools for extracting information from conversations,
    optimizing prompts, and maintaining persistent memory.^([4](ch02.html#id67))
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 来自LangChain的创作者，LangChain是一个用于LLM和代理开发的框架，该系统以具有唯一标识符的JSON文档形式在命名空间中组织内存，支持语义事实、用户偏好、情景示例和程序化系统提示。LangMem
    SDK增加了从对话中提取信息、优化提示和维持持久记忆的工具。[4](ch02.html#id67)
- en: Mem0 (Memory-Zero)
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Mem0（Memory-Zero）
- en: This framework extracts key facts from interactions and updates long-term memory
    selectively. Rather than storing complete chat histories, it maintains concise
    entries to reduce memory usage and improve retrieval speed. Users can add a graph-based
    extension, Mem0g, that maps relationships between entities for additional context.^([5](ch02.html#id68))
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架从交互中提取关键事实并选择性地更新长期记忆。它不是存储完整的聊天历史，而是保持简洁的条目以减少内存使用并提高检索速度。用户可以添加基于图的扩展Mem0g，以映射实体之间的关系以提供额外的上下文。[5](ch02.html#id68)
- en: Redis Semantic Caching (LangCache)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Redis语义缓存（LangCache）
- en: This framework addresses the repetitive nature of agent queries through semantic
    caching. The implementation includes configurable search criteria, a REST API,
    and user-specific security features. At the time of writing, LangCache is in private
    preview, but it should soon be available to the general public.^([6](ch02.html#id69))
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架通过语义缓存解决了代理查询的重复性。其实施包括可配置的搜索标准、REST API和用户特定的安全功能。在撰写本文时，LangCache处于私有预览阶段，但很快应该对公众开放。[6](ch02.html#id69)
- en: ADK MemoryService (Google’s Agent Development Kit)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ADK MemoryService（谷歌的代理开发套件）
- en: 'This framework provides a BaseMemoryService interface with two primary functions:
    adding completed sessions to storage and searching stored information. Developers
    can choose between InMemoryService for RAM-based keyword searches (nonpersistent)
    or VertexAIMemoryBankService for production environments with persistent semantic
    search.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 该框架提供了一个BaseMemoryService接口，具有两个主要功能：将完成的会话添加到存储中并搜索存储的信息。开发者可以在基于RAM的关键字搜索（非持久）的InMemoryService和用于具有持久语义搜索的生产环境的VertexAIMemoryBankService之间进行选择。
- en: Technologies and Solutions for Advanced Memory Management
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级内存管理的技术和解决方案
- en: 'The landscape of agent memory technologies reflects a fundamental tension:
    the need for both sophistication and simplicity, performance and flexibility,
    innovation and reliability. Each solution represents a different philosophy about
    how agents should remember.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 代理内存技术领域反映了根本性的紧张关系：需要复杂性和简单性、性能和灵活性、创新和可靠性。每个解决方案都代表了对代理如何记忆的不同哲学。
- en: 'Each framework reflects different priorities and philosophies:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 每个框架都反映了不同的优先级和哲学：
- en: LangGraph
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: LangGraph
- en: Seeks simplicity with its document store approach. Easy integration and namespace
    organization make it ideal for rapid prototyping and standard agent workflows,
    though it lacks the sophistication of specialized solutions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 采用文档存储方法寻求简单性。易于集成和命名空间组织使其成为快速原型设计和标准代理工作流程的理想选择，尽管它缺乏专业解决方案的复杂性。
- en: ADK MemoryService
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ADK MemoryService
- en: Provides enterprise-grade reliability with clear interfaces and Vertex AI integration.
    The trade-off is its limitation to the Google ecosystem—perfect for Google Cloud
    deployments but less flexible for multicloud architectures.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 提供企业级可靠性，具有清晰的接口和Vertex AI集成。其权衡是其限制在谷歌生态系统中——非常适合谷歌云部署，但不太适合多云架构。
- en: Vector database alternatives
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库替代方案
- en: Each serves varying levels of demand. Pinecone offers managed services that
    are excellent for scale. Weaviate provides open source with hybrid search capabilities.
    Qdrant focuses on performance with advanced filtering. Chroma remains lightweight
    and developer friendly. Pgvector leverages familiar PostgreSQL tooling for teams
    already invested in that ecosystem. And Redis leverages its ubiquity as a caching
    system by adding RediSearch capabilities for indexing and search.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 每个都满足不同层次的需求。Pinecone提供出色的托管服务，非常适合扩展。Weaviate提供开源的混合搜索功能。Qdrant专注于性能和高级过滤。Chroma保持轻量级且对开发者友好。Pgvector利用熟悉的后端工具，为已经投资于该生态系统的团队提供支持。Redis通过添加RediSearch功能来索引和搜索，利用其作为缓存系统的普遍性。
- en: The choice of a vector database isn’t simple ([Figure 2-1](#ch02_figure_1_1758256567754182)).
    Do you need the raw performance of Redis? The intelligent abstraction of Mem0?
    The simplicity of LangGraph? The enterprise features of ADK? Your answer shapes
    your agent’s memory architecture and your overall system design.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 选择向量数据库并不简单（[图2-1](#ch02_figure_1_1758256567754182)）。你需要Redis的原始性能吗？Mem0的智能抽象？LangGraph的简单性？ADK的企业功能？你的答案将塑造你的代理记忆架构和整体系统设计。
- en: '![Diagram illustrating the agent memory architecture, showing the process from
    input processing with named-entity recognition, through embedding and scoring,
    to retrieval from a vector database using cosine similarity for efficient memory
    storage and contextual retrieval.](assets/mmai_0201.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图示代理记忆架构，展示从使用命名实体识别进行输入处理，通过嵌入和评分，到使用余弦相似度从向量数据库检索，以实现高效的记忆存储和上下文检索的过程。](assets/mmai_0201.png)'
- en: Figure 2-1\. Scoring and retrieval outline
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 评分和检索概述
- en: Enhancing Memory Accuracy with Named-Entity Recognition
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用命名实体识别增强记忆准确性
- en: Named-entity recognition (NER) transforms the fuzzy world of natural language
    into the structured precision that computers—and agents—can reliably work with.
    In agent memory systems, NER isn’t just a nice-to-have feature; it’s becoming
    essential for accurate, retrievable memory.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）将自然语言的模糊世界转化为计算机和代理可以可靠地工作的结构化精确度。在代理记忆系统中，NER不仅仅是一个锦上添花的特性；它正成为准确、可检索记忆的必要条件。
- en: The NER Pipeline and Its Role in Memory
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NER管道及其在记忆中的作用
- en: NER is a technique that helps computers pick out and label important things—such
    as people, places, organizations, or dates—from plain text. Think of it as a way
    for an agent to read a sentence and automatically highlight the names, companies,
    or locations mentioned, turning messy language into structured data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: NER是一种帮助计算机从普通文本中挑选并标注重要事物——如人物、地点、组织或日期——的技术。将其视为一个代理读取句子并自动突出显示提到的名称、公司或地点的方式，将混乱的语言转化为结构化数据。
- en: Why does this matter for agents? When an agent can reliably identify and tag
    entities in conversations or documents, it can organize and retrieve information
    much more effectively. For example, an agent might use NER to keep track of which
    people were mentioned in a meeting, pull up all previous discussions about a specific
    project, or link related facts across different conversations. This structured
    approach makes memory retrieval more accurate and allows agents to answer questions
    like “What did John say about the budget?” or “When was the last time we discussed
    Paris?” with much greater precision.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这对代理有什么意义呢？当代理能够可靠地识别和标注对话或文档中的实体时，它可以更有效地组织和检索信息。例如，代理可能会使用NER来跟踪会议中提到的人物，检索所有关于特定项目的先前讨论，或在不同对话中链接相关事实。这种结构化方法使记忆检索更加准确，并允许代理以更高的精度回答诸如“约翰对预算说了什么？”或“我们上次讨论巴黎是什么时候？”等问题。
- en: Modern NER systems are impressively accurate, especially when fine-tuned for
    conversational AI.^([7](ch02.html#id70)) Some agents go even further, using NER-enhanced
    pipelines to store entities as metadata, combine semantic and entity-based search,
    and maintain relationships between entities over time. This turns fuzzy, unstructured
    memory into something agents can more easily reason about and use.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现代NER系统非常准确，尤其是在针对对话AI进行微调时（^([7](ch02.html#id70)））。一些代理甚至更进一步，使用增强的NER管道将实体作为元数据存储，结合语义和基于实体的搜索，并维护实体之间的长期关系。这将模糊、无结构化的记忆转化为代理可以更容易推理和使用的东西。
- en: 'NER can undergo the following three phases:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: NER可以经历以下三个阶段：
- en: Entity extraction
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 实体提取
- en: Identifies people, locations, organizations, dates, and custom entities, tagging
    each with confidence scores and linking entities across conversation turns
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 识别人物、地点、组织、日期和自定义实体，并为每个实体标记置信度分数，并在对话轮次之间链接实体
- en: Memory storage enhancement
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 内存存储增强
- en: Stores entities as structured metadata alongside embeddings, enables hybrid
    search combining semantic and entity-based approaches, and creates entity-centric
    memory indexes
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 将实体作为结构化元数据与嵌入一起存储，实现结合语义和基于实体的方法的混合搜索，并创建以实体为中心的记忆索引
- en: Retrieval improvement
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 检索改进
- en: Allows for querying memories by specific entities, filtering results by entity
    type, and maintaining entity relationships over time
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 允许通过特定实体查询记忆，通过实体类型过滤结果，并维护实体关系随时间的变化
- en: Improving Memory Retrieval and Accuracy with Structured Data
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 利用结构化数据提高记忆检索和准确性
- en: NER isn’t just about labeling words—it’s about making memory search smarter
    and more precise. For example, if you ask an agent, “What did John say about the
    budget?” NER lets the agent filter its memory for both the person (John) and the
    topic (budget) instead of just doing a broad keyword search. This can shrink the
    search space dramatically and make answers more relevant.^([8](ch02.html#id71))
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）不仅仅是关于标记单词——它是关于使内存搜索更智能和更精确。例如，如果你问一个智能体，“约翰对预算说了什么？”NER让智能体过滤其记忆中的人（约翰）和主题（预算），而不仅仅是进行广泛的关键词搜索。这可以大大缩小搜索空间，并使答案更加相关.^([8](ch02.html#id71))
- en: Beyond simple filtering, NER helps agents build knowledge graphs—connecting
    people, places, and things and tracking how they relate over time. This reduces
    confusion (such as distinguishing “Apple” the company from “apple” the fruit)
    and helps agents resolve pronouns or ambiguous references.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 除此之外，命名实体识别（NER）帮助智能体构建知识图谱——连接人、地点和事物，并跟踪它们随时间的关系。这减少了混淆（例如区分“苹果”公司与其水果“苹果”），并帮助智能体解决代词或模糊引用。
- en: 'In practice, many production systems already use NER for better memory: Redis
    Agent Memory Server, Mem0g, and LangChain all include entity recognition or entity
    memory modules. As the field evolves, NER has begun to expand to handle not just
    text but also images and audio as well as to adapt to new domains with minimal
    training.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，许多生产系统已经使用命名实体识别（NER）来改善记忆：Redis Agent Memory Server、Mem0g和LangChain都包括实体识别或实体记忆模块。随着该领域的发展，命名实体识别（NER）已经开始扩展，不仅处理文本，还处理图像和音频，并适应新的领域，而所需的训练最少。
- en: Structured data from NER turns vague, fuzzy memory into something agents can
    actually use and reason about—making them more helpful and accurate in real-world
    scenarios.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）的结构化数据将模糊、不清晰的记忆转化为智能体可以实际使用和推理的东西——使它们在现实场景中更加有用和准确。
- en: Summary
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Before moving into economic considerations, let’s recap what we’ve learned about
    memory so far.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入经济考虑之前，让我们回顾一下到目前为止我们关于记忆所学到的内容。
- en: Agent memory is the process of processing, interpreting, storing, and retrieving
    data dynamically. This simple statement, which opened our exploration, has big
    implications for how we build and think about AI agents. Throughout this chapter,
    we’ve seen how this idea shapes everything about agent memory systems—from the
    way we draw inspiration from human cognition to the nuts and bolts of real-world
    implementations.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体记忆是处理、解释、动态存储和检索数据的过程。这个简单的陈述，开启了我们的探索之旅，对我们构建和思考AI智能体的方式有着深远的影响。在本章中，我们看到了这一理念如何塑造智能体记忆系统的方方面面——从我们从人类认知中汲取灵感的方式，到现实世界实施的具体细节。
- en: 'The real challenge isn’t just technical: it’s about embracing the unpredictability
    and flexibility that agents require. Agents aren’t just databases with a fancy
    interface; they’re dynamic, adaptive systems that need to balance remembering,
    forgetting, and learning on the fly.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的挑战不仅仅是技术性的：它关乎拥抱智能体所需的不可预测性和灵活性。智能体不仅仅是具有花哨界面的数据库；它们是动态的、自适应的系统，需要在即时记住、忘记和学习之间取得平衡。
- en: If there’s one takeaway, it’s that building agent memory isn’t about stuffing
    more data into bigger context windows or chasing the latest database. It’s about
    designing systems that can manage information intelligently—knowing what to keep,
    what to compress, and what to let go. The future of agent memory will be shaped
    by how well we can give our agents the ability to adapt, prioritize, and make
    sense of the information they encounter.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, memory is just data—but when managed well, it becomes knowledge,
    and when applied dynamically, it becomes intelligence. That’s the real promise
    of agent memory systems: turning raw data into something agents and humans can
    actually use, one interaction at a time.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch02.html#id64-marker)) Cole Stryker, “What Is AI Agent Memory?” IBM,
    accessed September 8, 2025, [*https://www.ibm.com/think/topics/ai-agent-memory*](https://www.ibm.com/think/topics/ai-agent-memory).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch02.html#id65-marker)) “Memory: Agent Development Kit,” Google ADK Documentation,
    [*https://google.​git⁠hub.​io/adk-docs/sessions/memory*](https://google.github.io/adk-docs/sessions/memory).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch02.html#id66-marker)) Rowan Trollope, personal interview, July 11, 2025.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch02.html#id67-marker)) “Launching Long-Term Memory Support in LangGraph,”
    LangChain Blog, October 8, 2024, [*https://blog.langchain.com/launching-long-term-memory-support-in-langgraph/*](https://blog.langchain.com/launching-long-term-memory-support-in-langgraph/);
    “LangGraph Memory Concepts,” LangChain Documentation, [*https://langchain-ai.github.io/langgraph/concepts/memory/*](https://langchain-ai.github.io/langgraph/concepts/memory/);
    “LangMem SDK for Agent Long-Term Memory,” LangChain Blog, February 18, 2025, [*https://blog.langchain.com/langmem-sdk-launch/*](https://blog.langchain.com/langmem-sdk-launch/).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch02.html#id68-marker)) Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet
    Singh, and Deshraj Yadav, “Mem0: Building Production-Ready AI Agents with Scalable
    Long-Term Memory,” arXiv:2504.19413, 2025.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch02.html#id69-marker)) Rowan Trollope, “Introducing LangCache and Vector
    Sets, Simple Solutions for High-Performing AI Apps,” Redis Blog, April 8, 2025,
    [*https://redis.io/blog/spring-release-2025/*](https://redis.io/blog/spring-release-2025/);
    Jim Allen Wallace, “Semantic Caching for Faster, Smarter LLM Apps,” Redis Blog,
    July 9, 2024, [*https://redis.io/blog/what-is-semantic-caching/*](https://redis.io/blog/what-is-semantic-caching/);
    Rowan Trollope, personal interview, July 11, 2025.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch02.html#id70-marker)) Imed Keraghel, Stanislas Morbieu, and Mohamed
    Nadif, “Recent Advances in Named Entity Recognition: A Comprehensive Survey and
    Comparative Study,” arXiv:2401.10825, 2024.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch02.html#id71-marker)) “Entity-Based Memory Retrieval in Agent Systems,”
    in *Proceedings of AAAI Conference on Artificial Intelligence*, 2024.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
