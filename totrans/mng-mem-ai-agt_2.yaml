- en: 'Chapter 2\. Long-Term Memory: Building Persistent Learning Agents'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There’s no single, universal definition for the different types of memory in
    agent systems—every company seems to have its own take. For example, Anthropic,
    OpenAI, and Google all use slightly different terminology and approaches. But
    the more interesting question is: how does an agent actually decide what counts
    as procedural, semantic, or episodic memory? Or put another way: what gets treated
    as short-term, long-term, or contextual memory?'
  prefs: []
  type: TYPE_NORMAL
- en: Along these lines, semantic caching might play a big role. Sometimes short-term
    memories can be promoted to long-term if they’re accessed frequently enough. Conversely,
    long-term memories that aren’t used much might get summarized, become less detailed,
    or even be dropped from the system altogether. These are the kinds of trade-offs
    and decisions that go into designing agent memory. Ultimately, it all comes down
    to how the system is built to manage and retain information. Since it’s nearly
    impossible to program an agent to handle every scenario, we rely on constraints
    and parameters to guide what gets treated as episodic, semantic, or procedural
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: Types of Long-Term Memory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The industry has converged on three primary types of long-term memory, although
    implementations vary significantly:'
  prefs: []
  type: TYPE_NORMAL
- en: Episodic memory
  prefs: []
  type: TYPE_NORMAL
- en: Stores specific past experiences and events, functioning like human autobiographical
    memory. Companies typically implement this through RAG systems on conversation
    histories, extracting relevant chunks instead of keeping the full history.^([1](ch02.html#id64))
    The common approach uses few-shot example prompting where agents learn from past
    sequences, with key events, actions, and outcomes logged in structured formats.^([2](ch02.html#id65))
  prefs: []
  type: TYPE_NORMAL
- en: Semantic memory
  prefs: []
  type: TYPE_NORMAL
- en: Maintains structured factual knowledge—facts, definitions, rules—implemented
    through knowledge bases, symbolic AI, or vector embeddings. LLMs extract information
    from conversations, storing it as user or entity profiles that are retrieved and
    inserted into system prompts to influence future responses.
  prefs: []
  type: TYPE_NORMAL
- en: Procedural memory
  prefs: []
  type: TYPE_NORMAL
- en: The least common area, but a growing one, which stores skills, rules, and learned
    behaviors for automatic task performance. This combines LLM weights, agent code,
    and system prompts, with some agents updating their own prompts through “reflection”
    or metaprompting.
  prefs: []
  type: TYPE_NORMAL
- en: The field of agentic AI is moving away from rigid definitions of memory toward
    more flexible hybrid approaches, where memories can transition between types based
    on usage patterns and importance scoring. As Rowan Trollope, CEO of Redis, observes,
    this mirrors human memory consolidation. Just as our REM cycle compresses information
    from short-term to long-term memory as we sleep, the goal is to build agents to
    engage in similar processes.^([3](ch02.html#id66))
  prefs: []
  type: TYPE_NORMAL
- en: Long-Term Memory in Frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The practical implementation of long-term memory varies dramatically across
    frameworks, with each taking a different philosophical approach to the challenge:'
  prefs: []
  type: TYPE_NORMAL
- en: LangGraph Stores
  prefs: []
  type: TYPE_NORMAL
- en: From the creators of LangChain, a framework for LLM and agent development, this
    system organizes memory in namespaces as JSON documents with unique identifiers,
    supporting semantic facts, user preferences, episodic examples, and procedural
    system prompts. The LangMem SDK adds tools for extracting information from conversations,
    optimizing prompts, and maintaining persistent memory.^([4](ch02.html#id67))
  prefs: []
  type: TYPE_NORMAL
- en: Mem0 (Memory-Zero)
  prefs: []
  type: TYPE_NORMAL
- en: This framework extracts key facts from interactions and updates long-term memory
    selectively. Rather than storing complete chat histories, it maintains concise
    entries to reduce memory usage and improve retrieval speed. Users can add a graph-based
    extension, Mem0g, that maps relationships between entities for additional context.^([5](ch02.html#id68))
  prefs: []
  type: TYPE_NORMAL
- en: Redis Semantic Caching (LangCache)
  prefs: []
  type: TYPE_NORMAL
- en: This framework addresses the repetitive nature of agent queries through semantic
    caching. The implementation includes configurable search criteria, a REST API,
    and user-specific security features. At the time of writing, LangCache is in private
    preview, but it should soon be available to the general public.^([6](ch02.html#id69))
  prefs: []
  type: TYPE_NORMAL
- en: ADK MemoryService (Google’s Agent Development Kit)
  prefs: []
  type: TYPE_NORMAL
- en: 'This framework provides a BaseMemoryService interface with two primary functions:
    adding completed sessions to storage and searching stored information. Developers
    can choose between InMemoryService for RAM-based keyword searches (nonpersistent)
    or VertexAIMemoryBankService for production environments with persistent semantic
    search.'
  prefs: []
  type: TYPE_NORMAL
- en: Technologies and Solutions for Advanced Memory Management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The landscape of agent memory technologies reflects a fundamental tension:
    the need for both sophistication and simplicity, performance and flexibility,
    innovation and reliability. Each solution represents a different philosophy about
    how agents should remember.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each framework reflects different priorities and philosophies:'
  prefs: []
  type: TYPE_NORMAL
- en: LangGraph
  prefs: []
  type: TYPE_NORMAL
- en: Seeks simplicity with its document store approach. Easy integration and namespace
    organization make it ideal for rapid prototyping and standard agent workflows,
    though it lacks the sophistication of specialized solutions.
  prefs: []
  type: TYPE_NORMAL
- en: ADK MemoryService
  prefs: []
  type: TYPE_NORMAL
- en: Provides enterprise-grade reliability with clear interfaces and Vertex AI integration.
    The trade-off is its limitation to the Google ecosystem—perfect for Google Cloud
    deployments but less flexible for multicloud architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Vector database alternatives
  prefs: []
  type: TYPE_NORMAL
- en: Each serves varying levels of demand. Pinecone offers managed services that
    are excellent for scale. Weaviate provides open source with hybrid search capabilities.
    Qdrant focuses on performance with advanced filtering. Chroma remains lightweight
    and developer friendly. Pgvector leverages familiar PostgreSQL tooling for teams
    already invested in that ecosystem. And Redis leverages its ubiquity as a caching
    system by adding RediSearch capabilities for indexing and search.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of a vector database isn’t simple ([Figure 2-1](#ch02_figure_1_1758256567754182)).
    Do you need the raw performance of Redis? The intelligent abstraction of Mem0?
    The simplicity of LangGraph? The enterprise features of ADK? Your answer shapes
    your agent’s memory architecture and your overall system design.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram illustrating the agent memory architecture, showing the process from
    input processing with named-entity recognition, through embedding and scoring,
    to retrieval from a vector database using cosine similarity for efficient memory
    storage and contextual retrieval.](assets/mmai_0201.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2-1\. Scoring and retrieval outline
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Enhancing Memory Accuracy with Named-Entity Recognition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Named-entity recognition (NER) transforms the fuzzy world of natural language
    into the structured precision that computers—and agents—can reliably work with.
    In agent memory systems, NER isn’t just a nice-to-have feature; it’s becoming
    essential for accurate, retrievable memory.
  prefs: []
  type: TYPE_NORMAL
- en: The NER Pipeline and Its Role in Memory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NER is a technique that helps computers pick out and label important things—such
    as people, places, organizations, or dates—from plain text. Think of it as a way
    for an agent to read a sentence and automatically highlight the names, companies,
    or locations mentioned, turning messy language into structured data.
  prefs: []
  type: TYPE_NORMAL
- en: Why does this matter for agents? When an agent can reliably identify and tag
    entities in conversations or documents, it can organize and retrieve information
    much more effectively. For example, an agent might use NER to keep track of which
    people were mentioned in a meeting, pull up all previous discussions about a specific
    project, or link related facts across different conversations. This structured
    approach makes memory retrieval more accurate and allows agents to answer questions
    like “What did John say about the budget?” or “When was the last time we discussed
    Paris?” with much greater precision.
  prefs: []
  type: TYPE_NORMAL
- en: Modern NER systems are impressively accurate, especially when fine-tuned for
    conversational AI.^([7](ch02.html#id70)) Some agents go even further, using NER-enhanced
    pipelines to store entities as metadata, combine semantic and entity-based search,
    and maintain relationships between entities over time. This turns fuzzy, unstructured
    memory into something agents can more easily reason about and use.
  prefs: []
  type: TYPE_NORMAL
- en: 'NER can undergo the following three phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Entity extraction
  prefs: []
  type: TYPE_NORMAL
- en: Identifies people, locations, organizations, dates, and custom entities, tagging
    each with confidence scores and linking entities across conversation turns
  prefs: []
  type: TYPE_NORMAL
- en: Memory storage enhancement
  prefs: []
  type: TYPE_NORMAL
- en: Stores entities as structured metadata alongside embeddings, enables hybrid
    search combining semantic and entity-based approaches, and creates entity-centric
    memory indexes
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval improvement
  prefs: []
  type: TYPE_NORMAL
- en: Allows for querying memories by specific entities, filtering results by entity
    type, and maintaining entity relationships over time
  prefs: []
  type: TYPE_NORMAL
- en: Improving Memory Retrieval and Accuracy with Structured Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: NER isn’t just about labeling words—it’s about making memory search smarter
    and more precise. For example, if you ask an agent, “What did John say about the
    budget?” NER lets the agent filter its memory for both the person (John) and the
    topic (budget) instead of just doing a broad keyword search. This can shrink the
    search space dramatically and make answers more relevant.^([8](ch02.html#id71))
  prefs: []
  type: TYPE_NORMAL
- en: Beyond simple filtering, NER helps agents build knowledge graphs—connecting
    people, places, and things and tracking how they relate over time. This reduces
    confusion (such as distinguishing “Apple” the company from “apple” the fruit)
    and helps agents resolve pronouns or ambiguous references.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, many production systems already use NER for better memory: Redis
    Agent Memory Server, Mem0g, and LangChain all include entity recognition or entity
    memory modules. As the field evolves, NER has begun to expand to handle not just
    text but also images and audio as well as to adapt to new domains with minimal
    training.'
  prefs: []
  type: TYPE_NORMAL
- en: Structured data from NER turns vague, fuzzy memory into something agents can
    actually use and reason about—making them more helpful and accurate in real-world
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before moving into economic considerations, let’s recap what we’ve learned about
    memory so far.
  prefs: []
  type: TYPE_NORMAL
- en: Agent memory is the process of processing, interpreting, storing, and retrieving
    data dynamically. This simple statement, which opened our exploration, has big
    implications for how we build and think about AI agents. Throughout this chapter,
    we’ve seen how this idea shapes everything about agent memory systems—from the
    way we draw inspiration from human cognition to the nuts and bolts of real-world
    implementations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The real challenge isn’t just technical: it’s about embracing the unpredictability
    and flexibility that agents require. Agents aren’t just databases with a fancy
    interface; they’re dynamic, adaptive systems that need to balance remembering,
    forgetting, and learning on the fly.'
  prefs: []
  type: TYPE_NORMAL
- en: If there’s one takeaway, it’s that building agent memory isn’t about stuffing
    more data into bigger context windows or chasing the latest database. It’s about
    designing systems that can manage information intelligently—knowing what to keep,
    what to compress, and what to let go. The future of agent memory will be shaped
    by how well we can give our agents the ability to adapt, prioritize, and make
    sense of the information they encounter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the end, memory is just data—but when managed well, it becomes knowledge,
    and when applied dynamically, it becomes intelligence. That’s the real promise
    of agent memory systems: turning raw data into something agents and humans can
    actually use, one interaction at a time.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([1](ch02.html#id64-marker)) Cole Stryker, “What Is AI Agent Memory?” IBM,
    accessed September 8, 2025, [*https://www.ibm.com/think/topics/ai-agent-memory*](https://www.ibm.com/think/topics/ai-agent-memory).
  prefs: []
  type: TYPE_NORMAL
- en: '^([2](ch02.html#id65-marker)) “Memory: Agent Development Kit,” Google ADK Documentation,
    [*https://google.​git⁠hub.​io/adk-docs/sessions/memory*](https://google.github.io/adk-docs/sessions/memory).'
  prefs: []
  type: TYPE_NORMAL
- en: ^([3](ch02.html#id66-marker)) Rowan Trollope, personal interview, July 11, 2025.
  prefs: []
  type: TYPE_NORMAL
- en: ^([4](ch02.html#id67-marker)) “Launching Long-Term Memory Support in LangGraph,”
    LangChain Blog, October 8, 2024, [*https://blog.langchain.com/launching-long-term-memory-support-in-langgraph/*](https://blog.langchain.com/launching-long-term-memory-support-in-langgraph/);
    “LangGraph Memory Concepts,” LangChain Documentation, [*https://langchain-ai.github.io/langgraph/concepts/memory/*](https://langchain-ai.github.io/langgraph/concepts/memory/);
    “LangMem SDK for Agent Long-Term Memory,” LangChain Blog, February 18, 2025, [*https://blog.langchain.com/langmem-sdk-launch/*](https://blog.langchain.com/langmem-sdk-launch/).
  prefs: []
  type: TYPE_NORMAL
- en: '^([5](ch02.html#id68-marker)) Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet
    Singh, and Deshraj Yadav, “Mem0: Building Production-Ready AI Agents with Scalable
    Long-Term Memory,” arXiv:2504.19413, 2025.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([6](ch02.html#id69-marker)) Rowan Trollope, “Introducing LangCache and Vector
    Sets, Simple Solutions for High-Performing AI Apps,” Redis Blog, April 8, 2025,
    [*https://redis.io/blog/spring-release-2025/*](https://redis.io/blog/spring-release-2025/);
    Jim Allen Wallace, “Semantic Caching for Faster, Smarter LLM Apps,” Redis Blog,
    July 9, 2024, [*https://redis.io/blog/what-is-semantic-caching/*](https://redis.io/blog/what-is-semantic-caching/);
    Rowan Trollope, personal interview, July 11, 2025.
  prefs: []
  type: TYPE_NORMAL
- en: '^([7](ch02.html#id70-marker)) Imed Keraghel, Stanislas Morbieu, and Mohamed
    Nadif, “Recent Advances in Named Entity Recognition: A Comprehensive Survey and
    Comparative Study,” arXiv:2401.10825, 2024.'
  prefs: []
  type: TYPE_NORMAL
- en: ^([8](ch02.html#id71-marker)) “Entity-Based Memory Retrieval in Agent Systems,”
    in *Proceedings of AAAI Conference on Artificial Intelligence*, 2024.
  prefs: []
  type: TYPE_NORMAL
