- en: Image segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://deeplearningwithpython.io/chapters/chapter11_image-segmentation](https://deeplearningwithpython.io/chapters/chapter11_image-segmentation)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Chapter 8 gave you a first introduction to deep learning for computer vision
    via a simple use case: binary image classification. But there’s more to computer
    vision than image classification! This chapter dives deeper into another essential
    computer vision application — image segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: Computer vision tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we’ve focused on image classification models: an image goes in, a label
    comes out. “This image likely contains a cat; this other one likely contains a
    dog.” But image classification is only one of several possible applications of
    deep learning in computer vision. In general, there are three essential computer
    vision tasks you need to know about:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Image classification*, where the goal is to assign one or more labels to an
    image. It may be either single-label classification (meaning categories are mutually
    exclusive) or multilabel classification (tagging all categories that an image
    belongs to, as shown in figure 11.1). For example, when you search for a keyword
    on the Google Photos app, behind the scenes you’re querying a very large multilabel
    classification model — one with over 20,000 different classes, trained on millions
    of images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Image segmentation*, where the goal is to “segment” or “partition” an image
    into different areas, with each area usually representing a category (as shown
    in figure 11.1). For instance, when Zoom or Google Meet displays a custom background
    behind you in a video call, it’s using an image segmentation model to distinguish
    your face from what’s behind it, with pixel-level precision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Object detection*, where the goal is to draw rectangles (called *bounding
    boxes*) around objects of interest in an image and associate each rectangle with
    a class. A self-driving car could use an object detection model to monitor cars,
    pedestrians, and signs in view of its cameras, for instance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/2045275007f5b1bc39eac7d6965c23da.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 11.1](#figure-11-1): The three main computer vision tasks: classification,
    segmentation, and detection'
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning for computer vision also encompasses a number of somewhat more
    niche tasks besides these three, such as image similarity scoring (estimating
    how visually similar two images are), keypoint detection (pinpointing attributes
    of interest in an image, such as facial features), pose estimation, 3D mesh estimation,
    depth estimation, and so on. But to start with, image classification, image segmentation,
    and object detection form the foundation that every machine learning engineer
    should be familiar with. Almost all computer vision applications boil down to
    one of these three.
  prefs: []
  type: TYPE_NORMAL
- en: You’ve seen image classification in action in Chapter 8. Next, let’s dive into
    image segmentation. It’s a very useful and very versatile technique, and you can
    straightforwardly approach it with what you’ve already learned so far. Then, in
    the next chapter, you’ll learn about object detection in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Types of image segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Image segmentation with deep learning is about using a model to assign a class
    to each pixel in an image, thus *segmenting* the image into different zones (such
    as “background” and “foreground” or “road,” “car,” and “sidewalk”). This general
    category of techniques can be used to power a considerable variety of valuable
    applications in image and video editing, autonomous driving, robotics, medical
    imaging, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three different flavors of image segmentation that you should know
    about:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Semantic segmentation*, where each pixel is independently classified into
    a semantic category, like “cat.” If there are two cats in the image, the corresponding
    pixels are all mapped to the same generic “cat” category (see figure 11.2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Instance segmentation*, which seeks to parse out individual object instances.
    In an image with two cats in it, instance segmentation would distinguish between
    pixels belonging to “cat 1” and pixels belonging to “cat 2” (see figure 11.2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Panoptic segmentation*, which combines semantic segmentation and instance
    segmentation by assigning to each pixel in an image both a semantic label (like
    “cat”) and an instance label (like “cat 2”). This is the most informative of all
    three segmentation types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](../Images/7fd2d264b2190f8be9fac2be63b59e45.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 11.2](#figure-11-2): Semantic segmentation vs. instance segmentation'
  prefs: []
  type: TYPE_NORMAL
- en: To get more familiar with segmentation, let’s get started with training a small
    segmentation model from scratch on your own data.
  prefs: []
  type: TYPE_NORMAL
- en: Training a segmentation model from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this first example, we’ll focus on semantic segmentation. We’ll be looking
    once again at images of cats and dogs, and this time we’ll be learning to tell
    apart the main subject and its background.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading a segmentation dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We’ll work with the Oxford-IIIT Pets dataset ([https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/)),
    which contains 7,390 pictures of various breeds of cats and dogs, together with
    foreground-background *segmentation masks* for each picture. A segmentation mask
    is the image segmentation equivalent of a label: it’s an image the same size as
    the input image, with a single color channel where each integer value corresponds
    to the class of the corresponding pixel in the input image. In our case, the pixels
    of our segmentation masks can take one of three integer values:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 (foreground)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2 (background)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3 (contour)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s start by downloading and uncompressing our dataset, using the `wget`
    and `tar` shell utilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The input pictures are stored as JPG files in the `images/` folder (such as
    `images/Abyssinian_1.jpg`), and the corresponding segmentation mask is stored
    as a PNG file with the same name in the `annotations/trimaps/` folder (such as
    `annotations/trimaps/Abyssinian_1.png`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s prepare the list of input file paths, as well as the list of the corresponding
    mask file paths:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, what does one of these inputs and its mask look like? Let’s take a quick
    look (see figure 11.3).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/539e2f264128e5b1ac6b451aa3d2e9a4.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 11.3](#figure-11-3): An example image'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at its target mask as well (see figure 11.4):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2686b58935fc0c0e5d28d781497c7142.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 11.4](#figure-11-4): The corresponding target mask'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s load our inputs and targets into two NumPy arrays. Since the dataset
    is very small, we can load everything into memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As always, let’s split the arrays into a training and a validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Building and training the segmentation model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, it’s time to define our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The first half of the model closely resembles the kind of ConvNet you’d use
    for image classification: a stack of `Conv2D` layers, with gradually increasing
    filter sizes. We downsample our images three times by a factor of two each — ending
    up with activations of size `(25, 25, 256)`. The purpose of this first half is
    to encode the images into smaller feature maps, where each spatial location (or
    “pixel”) contains information about a large spatial chunk of the original image.
    You can understand it as a kind of compression.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One important difference between the first half of this model and the classification
    models you’ve seen before is the way we do downsampling: in the classification
    ConvNets from chapter 8, we used `MaxPooling2D` layers to downsample feature maps.
    Here, we downsample by adding *strides* to every other convolution layer (if you
    don’t remember the details of how convolution strides work, see chapter 8, section
    8.1.1). We do this because, in the case of image segmentation, we care a lot about
    the spatial location of information in the image since we need to produce per-pixel
    target masks as output of the model. When you do 2 × 2 max pooling, you are completely
    destroying location information within each pooling window: you return one scalar
    value per window, with zero knowledge of which of the four locations in the windows
    the value came from.'
  prefs: []
  type: TYPE_NORMAL
- en: So, while max pooling layers perform well for classification tasks, they would
    hurt us quite a bit for a segmentation task. Meanwhile, strided convolutions do
    a better job at downsampling feature maps while retaining location information.
    Throughout this book, you’ll notice that we tend to use strides instead of max
    pooling in any model that cares about feature location, such as the generative
    models in chapter 17.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second half of the model is a stack of `Conv2DTranspose` layers. What are
    those? Well, the output of the first half of the model is a feature map of shape
    `(25, 25, 256)`, but we want our final output to predict a class for each pixel,
    matching the original spatial dimensions. The final model output will have shape
    `(200, 200, num_classes)`, which is `(200, 200, 3)` here. Therefore, we need to
    apply a kind of *inverse* of the transformations we’ve applied so far, something
    that will *upsample* the feature maps instead of downsampling them. That’s the
    purpose of the `Conv2DTranspose` layer: you can think of it as a kind of convolution
    layer that *learns to upsample*. If you have an input of shape `(100, 100, 64)`
    and you run it through the layer `Conv2D(128, 3, strides=2, padding="same")`,
    you get an output of shape `(50, 50, 128)`. If you run this output through the
    layer `Conv2DTranspose(64, 3, strides=2, padding="same")`, you get back an output
    of shape `(100, 100, 64)`, the same as the original. So after compressing our
    inputs into feature maps of shape `(25, 25, 256)` via a stack of `Conv2D` layers,
    we can simply apply the corresponding sequence of `Conv2DTranspose` layers followed
    by a final `Conv2D` layer to produce outputs of shape `(200, 200, 3)`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the model, we’ll use a metric named *Intersection over Union* (IoU).
    It’s a measure of the match between the ground truth segmentation masks and the
    predicted masks. It can be computed separately for each class or averaged over
    multiple classes. Here’s how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the *intersection* between the masks, the area where the prediction
    and ground truth overlap.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the *union* of the masks, the total area covered by both masks combined.
    This is the whole space we’re interested in — the target object and any extra
    bits your model might have included by mistake.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide the intersection area by the union area to get the IoU. It’s a number
    between 0 and 1, where 1 denotes a perfect match, and 0 denotes a complete miss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can simply use a built-in Keras metric rather than building this ourselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now compile and fit our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s display our training and validation loss (see figure 11.5):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9f64353f124760b76a5383248fac2a7f.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 11.5](#figure-11-5): Displaying training and validation loss curves'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that we start overfitting midway, around epoch 25\. Let’s reload
    our best-performing model according to validation loss and demonstrate how to
    use it to predict a segmentation mask (see figure 11.6):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5df726de6b0d2524f74f492e8d42c2d1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 11.6](#figure-11-6): A test image and its predicted segmentation mask'
  prefs: []
  type: TYPE_NORMAL
- en: There are a couple of small artifacts in our predicted mask, caused by geometric
    shapes in the foreground and background. Nevertheless, our model appears to work
    nicely.
  prefs: []
  type: TYPE_NORMAL
- en: Using a pretrained segmentation model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the image classification example from chapter 8, you saw how using a pretrained
    model could significantly boost your accuracy — especially when you only have
    a few samples to train on. Image segmentation is no different.
  prefs: []
  type: TYPE_NORMAL
- en: The *Segment Anything Model*,^([[1]](#footnote-1)) or SAM for short, is a powerful
    pretrained segmentation model you can use for, well, almost anything. It was developed
    by Meta AI and released in April 2023\. It was trained on 11 million images and
    their segmentation masks, covering over 1 billion object instances. This massive
    amount of training data provides the model with built-in knowledge of virtually
    any object that appears in natural images.
  prefs: []
  type: TYPE_NORMAL
- en: The main innovation of SAM is that it’s not limited to a predefined set of object
    classes. You can use it for segmenting new objects simply by providing an example
    of what you’re looking for. You don’t even need to fine-tune the model first.
    Let’s see how that works.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the Segment Anything Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, let’s instantiate SAM and download its weights. Once again, we can use
    the KerasHub package to use this pretrained model without needing to implement
    it ourselves from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember the `ImageClassifier` task we used in the previous chapter? We can
    use another KerasHub task `ImageSegmenter` for wrapping pretrained image segmentation
    models into a high-level model with standard inputs and outputs. Here, we’ll use
    the `sam_huge_sa1b` pretrained model, where `sam` stands for the model, `huge`
    refers to the number of parameters in the model, and `sa1b` stands for the SA-1B
    dataset released along with the model, with 1 billion annotated masks. Let’s download
    it now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'One thing we can note off the bat is that our model is, indeed, huge:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: At 641 million parameters, SAM is the largest model we have used so far in this
    book. The trend of pretrained models getting larger and larger and using more
    and more data will be discussed in more detail in chapter 16.
  prefs: []
  type: TYPE_NORMAL
- en: How Segment Anything works
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we try running some segmentation with the model, let’s talk a little
    more about how SAM works. Much of the capability of the model comes from the scale
    of the pretraining dataset. Meta developed the SA-1B dataset along with the model,
    where the partially trained model was used to assist with the data labeling process.
    That is, the dataset and model were developed together in a feedback loop of sorts.
  prefs: []
  type: TYPE_NORMAL
- en: The goal with the SA-1B dataset is to create fully segmented images, where every
    object in an image is given a unique segmentation mask. See figure 11.7 as an
    example. Each image in the dataset has ~100 masks on average, and some images
    have over 500 individually masked objects. This was done through a pipeline of
    increasingly automated data collection. At first, human experts manually segmented
    a small example dataset of images, which was used to train an initial model. This
    model was used to help drive a semiautomated stage of data collection, where images
    were first segmented by SAM and improved by human correction and further annotation.
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/f7fee6b67ad6bf846f8c26fdfbde4e3e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 11.7](#figure-11-7): An example image from the SA-1B dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model is trained on `(image, prompt, mask)` triples. `image` and `prompt`
    are the model inputs. The image can be any input image, and the prompt can take
    a couple of forms:'
  prefs: []
  type: TYPE_NORMAL
- en: A point inside the object to mask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A box around the object to mask
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the `image` and `prompt` input, the model is expected to produce an accurate
    predicted mask for the object indicated by the prompt, which is compared with
    a ground truth `mask` label.
  prefs: []
  type: TYPE_NORMAL
- en: The model consists of a few separate components. An image encoder similar to
    the Xception model we used in previous chapters, will take an input image and
    output a much smaller image embedding. This is something we already know how to
    build.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we add a prompt encoder, which is responsible for mapping prompts in any
    of the previously mentioned forms to an embedded vector, and a mask decoder, which
    takes in both the image embedding and prompt embedding and outputs a few possible
    predicted masks. We won’t get into the details of the prompt encoder and mask
    decoder here, as they use some modeling techniques we won’t see until later chapters.
    We can compare these predicted masks with our ground truth mask much like we did
    in the earlier section of this chapter (see figure 11.8).
  prefs: []
  type: TYPE_NORMAL
- en: '![](../Images/02a451fa68c4d16b5f76ddfbd8d7e7e4.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 11.8](#figure-11-8): The Segment Anything high-level architecture overview'
  prefs: []
  type: TYPE_NORMAL
- en: All of these subcomponents are trained simultaneously by forming batches of
    new `(image, prompt, mask)` triples to train on from the SA-1B image and mask
    data. The process here is actually quite simple. For a given input image, choose
    a random mask in the input. Next, randomly choose whether to create a box prompt
    or a point prompt. To create a point prompt, choose a random pixel inside the
    mask label. To create a box prompt, draw a box around all points inside the mask
    label. We can repeat this process indefinitely, sampling a number of `(image,
    prompt, mask)` triples from each image input.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing a test image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s make this a little more concrete by trying the model out. We can start
    by loading a test image for our segmentation work. We’ll use a picture of a bowl
    of fruits (see figure 11.9):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/cb3ddcfe0d2fd3d54cc924ac0bb4ea5c.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 11.9](#figure-11-9): Our test image'
  prefs: []
  type: TYPE_NORMAL
- en: 'SAM expects inputs that are 1024 × 1024\. However, forcibly resizing arbitrary
    images to 1024 × 1024 would distort their aspect ratio — for instance, our image
    isn’t square. It’s better to first resize the image so that its longest side becomes
    1,024 pixels long and then pad the remaining pixels with a filler value, such
    as 0\. We can achieve this with the `pad_to_aspect_ratio` argument in the `keras.ops.image.resize()`
    operation, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Next, let’s define a few utilities that will come in handy when using the model.
    We’re going to need to
  prefs: []
  type: TYPE_NORMAL
- en: Display images.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Display segmentation masks overlaid on an image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highlight specific points on an image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Display boxes overlaid on an image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All our utilities take a Matplotlib `axis` object (noted `ax`) so that they
    can all write to the same figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Prompting the model with a target point
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To use SAM, you need to prompt it. This means we need one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Point prompts* — Select a point in an image and let the model segment the
    object that the point belongs to.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Box prompts* — Draw an approximate box around an object (it does not need
    to be particularly precise) and let the model segment the object in the box.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start with a point prompt. Points are labeled, with 1 indicating the foreground
    (the object you want to segment) and 0 indicating the background (everything around
    the object). In ambiguous cases, to improve your results, you could pass multiple
    labeled points, instead of a single point, to refine your definition of what should
    be included (points labeled 1) and what should be excluded (points labeled 0).
  prefs: []
  type: TYPE_NORMAL
- en: 'We try a single foreground point (see figure 11.10). Here’s a test point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1fb3211965623f1112b05761214c02f1.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 11.10](#figure-11-10): A prompt point, landing on a peach'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s prompt SAM with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The return value `outputs` has a `"masks"` field which contains four 256 ×
    256 candidate masks for the target object, ranked by decreasing match quality.
    The quality scores of the masks are available under the `"iou_pred"` field as
    part of the model’s output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s overlay the first mask on the image (see figure 11.11):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/9a04a54d693d52d5d46a0779a32ef31d.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 11.11](#figure-11-11): Segmented peach'
  prefs: []
  type: TYPE_NORMAL
- en: Pretty good!
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s try a banana. We’ll prompt the model with coordinates `(300, 550)`,
    which land on the second banana from the left (see figure 11.12):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/d57b293d144b43710f13bcb5dc407813.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 11.12](#figure-11-12): Segmented banana'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, what about the other mask candidates? Those can come in handy for ambiguous
    prompts. Let’s try to plot the other three masks (see figure 11.13):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/2825d36b86ac06b70d4c188bdeeb0383.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 11.13](#figure-11-13): Alternative segmentation masks for the banana
    prompt'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see here, an alternative segmentation found by the model includes
    both bananas.
  prefs: []
  type: TYPE_NORMAL
- en: Prompting the model with a target box
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Besides providing one or more target points, you can also provide boxes approximating
    the location of the object to segment. These boxes should be passed via the coordinates
    of their top-left and bottom-right corners. Here’s a box around the mango (see
    figure 11.14):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/611f443ab648ad48cdc642f1c265fd6e.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 11.14](#figure-11-14): Box prompt around the mango'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s prompt SAM with it (see figure 11.15):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/afe8dd8fc5b03bf9c605eb1d054321d6.png)'
  prefs: []
  type: TYPE_IMG
- en: '[Figure 11.15](#figure-11-15): Segmented mango'
  prefs: []
  type: TYPE_NORMAL
- en: SAM can be a powerful tool to quickly create large datasets of images annotated
    with segmentation masks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Image segmentation is one of the main categories of computer vision tasks. It
    consists of computing segmentation masks that describe the contents of an image
    at the pixel level.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To build your own segmentation model, use a stack of strided `Conv2D` layers
    to “compress” the input image into a smaller feature map, followed by a stack
    of corresponding `Conv2DTranspose` layers to “expand” the feature map into a segmentation
    mask the same size as the input image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also use a pretrained segmentation model. Segment Anything, included
    in KerasHub, is a powerful model that supports image prompting, text prompting,
    point prompting, and box prompting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Footnotes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kirillov et al., “Segment Anything,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, arXiv (2023), [https://arxiv.org/abs/2304.02643](https://arxiv.org/abs/2304.02643).
    [[↩]](#footnote-link-1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
