- en: Chapter 4\. Data Engineering for LLMs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章：LLMs的数据工程
- en: In this chapter, you will learn about data engineering, data management practices,
    and the database tools and systems available. The discussion will be geared toward
    data, DevOps, and MLOps engineers who want to become LLMOps engineers and/or lead
    their company’s data engineering efforts. By the end of this chapter, you will
    have a strong grasp of the foundations of data engineering, as well as best practices
    for LLMs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解数据工程、数据管理实践以及可用的数据库工具和系统。讨论将面向希望成为LLMOps工程师或领导其公司数据工程工作的数据、DevOps和MLOps工程师。到本章结束时，你将深刻理解数据工程的基础，以及LLMs的最佳实践。
- en: Data Engineering and the Rise of LLMs
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据工程与LLMs的兴起
- en: 'In the late 1960s, British computer scientist Edgar F. Codd, fresh from finishing
    his doctorate in self-replicating computers, was working at IBM. Codd became fascinated
    by the theory of data arrangement and in 1970 published an internal IBM paper
    called [“A Relational Model of Data for Large Shared Data Banks”](https://oreil.ly/JG1bn)
    that introduced what we know today as *relational databases*. For example, instead
    of a sales table in which each record contains all the information about the products
    and the customers to whom they’ve been sold, relational databases store this data
    in multiple related tables: one for customers, one for products, and one for sales.
    Before relational databases, something as simple as a change in customer address
    would require changing all sales records for that customer, which was an expensive
    operation in mainframes. In a relational database, you can change just the customer
    record, and all the related records will be updated.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在20世纪60年代末，英国计算机科学家爱德华·F·科德博士刚刚完成他在自我复制计算机方面的博士学位，当时他在IBM工作。科德对数据排列理论产生了浓厚的兴趣，并在1970年发表了一篇内部IBM论文，题为[“大型共享数据银行的关系数据模型”](https://oreil.ly/JG1bn)，介绍了我们今天所知的*关系数据库*。例如，与包含所有关于产品及其销售给客户的客户信息的销售表不同，关系数据库将这些数据存储在多个相关表中：一个用于客户，一个用于产品，一个用于销售。在关系数据库出现之前，像客户地址变更这样简单的事情就需要更改该客户的全部销售记录，这在主机上是一项昂贵的操作。在关系数据库中，你只需更改客户记录，所有相关的记录都会被更新。
- en: 'While it didn’t fascinate anyone at IBM right away, the paper caught the fancy
    of several other computer scientists and hobbyists, including Oracle founder Larry
    Ellison, who developed and sold the first relational database compatible with
    IBM mainframes. IBM also developed a language to query databases, originally named
    SEQUEL but now called Structured Query Language (SQL), which later became a standard.
    In 1981, Codd’s work on relational databases won him a Turing Award, the most
    prestigious accolade in computer science. Recognizing the popularity of relational
    databases and the need for systems to manage them, in 1983 IBM created its own
    database management system, called DB2\. Relational databases became the industry
    standard, used everywhere: for indexing, cataloging, and so on. The people who
    managed these systems for enterprises at IBM and Oracle were called *database
    administrators*, usually abbreviated as DBAs. (The title *data engineer* became
    popular alongside cloud computing in the 2010s.)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这篇论文一开始并没有引起IBM任何人的兴趣，但它却吸引了包括Oracle创始人拉里·埃里森在内的几位其他计算机科学家和爱好者的注意，埃里森开发了并销售了第一个与IBM主机兼容的关系型数据库。IBM还开发了一种查询数据库的语言，最初命名为SEQUEL，但现在称为结构化查询语言（SQL），后来成为了一种标准。1981年，Codd在关系型数据库方面的工作为他赢得了图灵奖，这是计算机科学界最负盛名的奖项。鉴于关系型数据库的普及以及管理系统需求的增加，1983年IBM创建了其自己的数据库管理系统，称为DB2。关系型数据库成为了行业标准，被广泛应用于索引、编目等方面。在IBM和Oracle管理这些企业系统的人员被称为*数据库管理员*，通常缩写为DBAs。（数据工程师这一称号在2010年代随着云计算的兴起而变得流行。）
- en: 'Codd later cowrote another paper, [“Providing OLAP to User-Analysts: An IT
    Mandate,”](https://oreil.ly/gUwKl) which coined the term *online analytical processing*
    (OLAP) to refer to a system for quickly processing and querying multidimensional
    data. OLAP is the foundation of most data-processing systems today.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Codd后来与人合著了另一篇论文，[“为用户分析师提供OLAP：一项IT指令，”](https://oreil.ly/gUwKl)，这篇论文提出了*在线分析处理*（OLAP）这一术语，用来指代一种快速处理和查询多维数据的系统。OLAP是当今大多数数据处理系统的基础。
- en: In 1990, Tim Berners-Lee created the World Wide Web, which exponentially increased
    the volume of data being generated and recorded. While a lot of this data was
    structured, meaning of fixed maximum length and type like a postal code, a lot
    of it was also unstructured, of variable length and type, like music, essays,
    and videos. Relational databases organize information into tables with predefined
    columns and strongly enforced data types. Because every row in a table must follow
    the same schema, they excel at handling highly structured data and supporting
    complex, SQL-based queries that join many tables together with consistent ACID
    (atomicity, consistency, isolation, durability) guarantees. This makes them the
    go-to choice for transactional systems such as banking, inventory, and traditional
    business applications where data integrity and cross-table relationships are required,
    but not as suitable for the unstructured data that exists in the internet.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在1990年，蒂姆·伯纳斯-李创建了万维网，这极大地增加了生成和记录的数据量。虽然其中很多数据是有结构的，意味着具有固定最大长度和类型，如邮政编码，但也有很多数据是无结构的，长度和类型可变，如音乐、论文和视频。关系型数据库将信息组织到具有预定义列和强数据类型的表中。由于表中的每一行都必须遵循相同的模式，因此它们在处理高度结构化数据和支持复杂、基于SQL的查询方面表现出色，这些查询将多个表连接在一起，并具有一致的ACID（原子性、一致性、隔离性、持久性）保证。这使得它们成为交易系统（如银行、库存和传统商业应用）的首选选择，在这些系统中需要数据完整性和跨表关系，但不太适合互联网中存在的无结构数据。
- en: '*Nonrelational* (NoSQL) databases emerged to address workloads that relational
    systems handle less efficiently—massive, rapidly changing, or loosely structured
    datasets. Key-value stores provide fast lookups by pairing a unique key with data.
    A specific type of key-value store, document databases, store each record as a
    self-contained JSON document, allowing every document to have its own shape. This
    flexibility is ideal for content management systems, product catalogs, and other
    domains where fields vary across records, which is very common with internet data.
    Key-value databases can also store binary files, such as videos and images, in
    *blobs* (binary large objects), making them ideally suited for use in this new
    data environment.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '*非关系型*（NoSQL）数据库的出现是为了解决关系型系统处理效率较低的工作负载——大量、快速变化或结构松散的数据集。键值存储通过将唯一键与数据配对来实现快速查找。一种特定的键值存储，即文档数据库，将每条记录存储为自包含的JSON文档，允许每个文档都有自己的形状。这种灵活性非常适合内容管理系统、产品目录和其他字段在记录之间变化的领域，这在互联网数据中非常常见。键值数据库还可以在*大对象*（blobs）中存储二进制文件，如视频和图像，这使得它们非常适合在新的数据环境中使用。'
- en: In addition, we have vector and graph databases. *Graph databases* focus on
    representing interconnected entities. Instead of tables or documents, they store
    nodes and edges, enabling millisecond-level pathfinding queries such as social
    network friend-of-a-friend searches, supply chain impact analysis, or discovery
    of the relationships between documents.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还有向量数据库和图数据库。*图数据库*专注于表示相互关联的实体。它们不存储表或文档，而是存储节点和边，这使得可以进行毫秒级路径查找查询，例如社交网络中的朋友的朋友搜索、供应链影响分析或发现文档之间的关系。
- en: '*Vector databases* are designed to store and index high-dimensional embeddings—dense
    numeric vectors that capture the semantic meaning of text, images, audio, or other
    content. Instead of looking for exact matches, they use *approximate nearest neighbor*
    (ANN) algorithms to return the items whose vectors lie closest to a query vector
    in that multidimensional space. This makes them the engine behind semantic search,
    recommendation systems, image-or-audio similarity matching, and *retrieval-augmented
    generation*(*RAG*) pipelines that supply LLM prompts with relevant context in
    milliseconds.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*向量数据库*旨在存储和索引高维嵌入——密集的数值向量，这些向量捕捉了文本、图像、音频或其他内容的语义意义。它们不是寻找精确匹配，而是使用*近似最近邻*（ANN）算法，在多维空间中返回与查询向量最接近的项的向量。这使得它们成为语义搜索、推荐系统、图像或音频相似度匹配以及提供相关上下文以毫秒级速度的*检索增强生成*（RAG）管道背后的引擎。'
- en: 'Because each model excels at a different access pattern, modern applications
    often combine them: a relational store for ACID-compliant transactions; a document
    or key-value store for flexible, semistructured content and media blobs; a graph
    database for when relationships themselves are the primary data; and a vector
    database for anything that hinges on “meaningful similarity.” Much of the data
    engineering work consists of choosing the right balance among these database types
    and combining their data for the desired application.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个模型在访问模式上都有其优势，现代应用通常将它们结合起来：一个关系型存储用于ACID兼容的事务；一个文档或键值存储用于灵活的半结构化内容和媒体块；一个图数据库用于关系本身是主要数据的情况；以及一个向量数据库用于任何依赖于“有意义相似性”的东西。大部分数据工程工作包括在这些数据库类型之间选择合适的平衡，并将它们的数据组合成所需的应用。
- en: LLMs are driving another revolution in data storage and management. Before LLMs,
    data scientists and analysts relied on simpler techniques for NLP tasks, such
    as representing textual data as numerical features (which ML algorithms require).
    Two of the most commonly used methods to analyze a collection of documents (called
    a *corpus*) were called *bag of words* (BoW) and *term frequency–inverse document
    frequency* (TF-IDF). Both methods transform unstructured text into structured,
    matrix-based formats that can be processed by traditional ML algorithms. BoW represents
    text as a sparse matrix, where each row corresponds to a document and each column
    corresponds to a word from the corpus. The value in each cell reflects the number
    of times that word appears in the document (called *term frequency*, or TF), ignoring
    word order but preserving frequency. TF-IDF builds on BoW by weighting TF with
    a measure of how rare that word is across the entire corpus (called *inverse document
    frequency*, or IDF). This adjustment reduces the impact of common words like *the*
    and *and* while emphasizing terms that are more informative in context.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs正在推动数据存储和管理领域的另一次革命。在LLMs出现之前，数据科学家和分析师依赖于更简单的NLP任务技术，例如将文本数据表示为数值特征（这是机器学习算法所需的）。分析文档集合（称为*语料库*）的两种最常用的方法被称为*词袋模型*（BoW）和*词频-逆文档频率*（TF-IDF）。这两种方法都将非结构化文本转换为结构化的、基于矩阵的格式，这些格式可以由传统的机器学习算法处理。BoW将文本表示为一个稀疏矩阵，其中每一行对应一个文档，每一列对应语料库中的一个单词。每个单元格中的值反映了该单词在文档中出现的次数（称为*词频*，或TF），忽略单词顺序但保留频率。TF-IDF在BoW的基础上通过使用一个衡量该单词在整个语料库中稀有程度的指标来加权TF（称为*逆文档频率*，或IDF）。这种调整减少了像*the*和*and*这样的常见单词的影响，同时强调了在上下文中更有信息量的术语。
- en: These matrix representations are typically stored in structured or binary data
    formats and processed with tools suited to the size of the data. Before LLMs became
    mainstream, the backbone of NLP workflows was a set of tools that included Python
    packages like Pandas and NumPy, to provide efficient frameworks for manipulating
    BoW and TF-IDF matrices, and Parquet and HDF5, for storing and querying larger,
    preprocessed datasets. In production environments, databases like PostgreSQL,
    MongoDB, and Elasticsearch were widely used to store, index, and query NLP data,
    particularly for applications requiring fast retrieval or search capabilities.
    These tools enabled the development of applications like search engines, recommendation
    systems, sentiment analysis, and text classification models.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这些矩阵表示通常存储在结构化或二进制数据格式中，并使用适合数据大小的工具进行处理。在LLMs（大型语言模型）成为主流之前，NLP（自然语言处理）工作流程的核心是一套工具，包括Python包如Pandas和NumPy，它们提供了高效框架来操作BoW（词袋模型）和TF-IDF（词频-逆文档频率）矩阵，以及Parquet和HDF5，用于存储和查询更大的、预处理过的数据集。在生产环境中，PostgreSQL、MongoDB和Elasticsearch等数据库被广泛用于存储、索引和查询NLP数据，尤其是对于需要快速检索或搜索能力的应用。这些工具使得搜索引擎、推荐系统、情感分析和文本分类模型等应用的开发成为可能。
- en: 'One of the main contributors to the rise of LLMs was the development of embeddings.
    As you learned in [Chapter 1](ch01.html#ch01_introduction_to_large_language_models_1748895465615150),
    embeddings are algorithms that transform textual data into a numerical representation—vectors
    of real numbers—that also encodes meaning. With embeddings, words and phrases
    with similar meanings are “closer” to each other than words and phrases with different
    meanings. ​This led to the introduction of the *vector database*, a new kind of
    database that can store vectors along with other metadata items and use ML algorithms
    to query them. As mentioned in [Chapter 2](ch02.html#ch02_introduction_to_llmops_1748895480208948),
    LLMOps is a framework for making and maintaining LLM applications that are reliable,
    robust, and scalable in production. However, as the adage goes, your model is
    only as good as your data: “Garbage in, garbage out.” Let’s take it a step further
    and say that your LLMOps maturity is only as good as your data engineering system.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs兴起的主要贡献之一是嵌入（embeddings）的发展。正如你在[第1章](ch01.html#ch01_introduction_to_large_language_models_1748895465615150)中学到的，嵌入是将文本数据转换为数值表示——实数向量——的算法，同时也编码了意义。有了嵌入，具有相似意义的单词和短语比具有不同意义的单词和短语“更接近”。这导致了*向量数据库*的引入，这是一种新型数据库，可以存储向量以及其他元数据项，并使用机器学习算法进行查询。正如[第2章](ch02.html#ch02_introduction_to_llmops_1748895480208948)中提到的，LLMOps是一个用于创建和维护在生产中可靠、健壮和可扩展的LLM应用的框架。然而，正如俗话所说，你的模型只比你的数据好：“垃圾进，垃圾出。”让我们更进一步，说你的LLMOps成熟度只与你的数据工程系统相当。
- en: 'In its early years, data management work was primarily about acquiring, storing,
    and retrieving data. Machine learning and LLMs have added new steps like transforming
    the data into appropriate representations, which requires additional skills. To
    acquire these skills, companies have two options, as we discussed in the previous
    chapter: either hire LLM engineers, upskill them, and integrate them into the
    data team or hire data engineers, integrate them into LLM development teams, and
    upskill them into LLMOps engineers.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在其早期，数据管理工作主要是关于获取、存储和检索数据。机器学习和LLMs增加了新的步骤，如将数据转换为适当的表示，这需要额外的技能。为了获得这些技能，公司有两个选择，正如我们在上一章中讨论的：要么雇佣LLM工程师，提升他们的技能，并将他们整合到数据团队中；要么雇佣数据工程师，将他们整合到LLM开发团队中，并将他们提升为LLMOps工程师。
- en: Either way, the shift from task-specific to task-agnostic machine learning models
    is only going to continue. The data market is huge and growing, and companies
    working with LLMs need skilled professionals to manage their data engineering
    systems.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种方式，从特定任务到通用机器学习模型的转变只会继续下去。数据市场巨大且在增长，与LLMs合作的公司需要熟练的专业人士来管理他们的数据工程系统。
- en: The DataOps Engineer Role
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据操作工程师的角色
- en: DataOps engineers usually have prior experience as data engineers or data scientists,
    with additional expertise to navigate the complexities of domain composition,
    data quantity, and data quality at scale. They are skilled in advanced techniques
    such as global deduplication and dynamic data selection for continuous fine-tuning.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 数据操作工程师通常具有作为数据工程师或数据科学家的先前经验，并具备额外的专业知识，以应对大规模领域组成、数据数量和数据质量等方面的复杂性。他们擅长诸如全局去重和动态数据选择等高级技术，以实现持续的微调。
- en: Data engineering for LLMs involves designing, developing, and managing data
    pipelines and infrastructure to support training, evaluating, and deploying these
    models. DataOps engineers implement and optimize scaling laws; balance trade-offs
    between quality and quantity; and manage diverse, large-scale datasets. However,
    their role goes beyond managing data pipelines. They orchestrate the entire data
    lifecycle for LLMs, from data acquisition to deployment, continuously improving
    model performance in a highly complex and evolving landscape.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为大型语言模型（LLMs）进行数据工程涉及设计、开发和管理工作流程和基础设施，以支持这些模型的训练、评估和部署。数据操作工程师（DataOps engineers）实施和优化扩展定律；在质量和数量之间平衡权衡；并管理多样化的、大规模的数据集。然而，他们的角色不仅限于管理数据流程。他们负责LLMs的整个数据生命周期，从数据获取到部署，并在高度复杂和不断演变的领域中持续提升模型性能。
- en: This specialization marks a significant evolution from the data engineering
    and management practices of the past, requiring a much more sophisticated and
    targeted approach to the daily work of a data engineer. Before the LLM era, data
    engineering was dominated by pipelines moving well-defined, mostly structured
    data from operational sources into data warehouses and lakes for reporting or
    analytics. The emphasis was on batch ETL/ELT jobs, dimensional modeling, slowly
    changing dimensions, and governance practices that treated data quality as a matter
    of schema conformance, referential integrity, and basic deduplication. Unstructured
    text might be archived in data lakes, but it was rarely a first-class citizen;
    search and analytics workloads still revolved around rows, columns, and aggregate
    SQL.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这个专业方向标志着从过去的数据工程和管理实践到一次重大的演变，需要数据工程师的日常工作采用更加复杂和有针对性的方法。在LLM时代之前，数据工程主要由将定义良好、主要是结构化数据从运营源移动到数据仓库和湖泊以进行报告或分析的数据管道主导。重点在于批量的ETL/ELT作业、维度建模、缓慢变化的维度以及将数据质量视为模式符合性、引用完整性和基本去重等治理实践。非结构化文本可能会存档在数据湖中，但它很少被视为一等公民；搜索和分析工作负载仍然围绕行、列和聚合SQL进行。
- en: LLM-centric workloads change everything. Now the raw material is heterogeneous
    text, code, images, audio, and chat logs whose value depends on *semantic richness*—that
    is, the informational value of the content—rather than a rigid structure. Pipelines
    must tokenize, chunk, embed, and version this content; store it in vector indexes
    for similarity search; and apply filters for personally identifiable information,
    toxicity, and licensing constraints. Instead of ETL jobs, teams run continuous
    ingestion and re-embedding loops so that RAG systems stay fresh, and they log
    every prompt–response pair so that the inputs and outputs can be evaluated and
    improve the future performance of this system. Data quality in this context is
    judged by grounding, factuality, and bias metrics—attributes that require automated
    red-teaming and human-in-the-loop (HITL) review rather than the data structure
    violation checks of the past.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以LLM为中心的工作负载改变了所有的一切。现在，原材料是异构的文本、代码、图像、音频和聊天日志，其价值取决于*语义丰富性*——即内容的信息价值——而不是严格的结构。管道必须标记、分块、嵌入和版本化此内容；将其存储在向量索引中进行相似性搜索；并应用过滤以保护个人身份信息、毒性以及许可约束。而不是ETL作业，团队运行连续的摄取和重新嵌入循环，以便RAG系统保持新鲜，并记录每个提示-响应对，以便可以评估输入和输出并提高该系统的未来性能。在此背景下，数据质量由扎根、事实性和偏见指标来判断——这些属性需要自动化的红队和人工介入（HITL）审查，而不是过去的数据结构违规检查。
- en: As a result, modern data engineering stacks now blend traditional warehouses
    with object stores, vector databases, and feature stores. Orchestration frameworks
    like Airflow and Dagster coexist with LLMOps tools, and governance expands to
    cover model cards, dataset nutrition labels, and lineage tracing of each token
    back to its legal source. Supporting LLMs transforms data engineering from “plumbing
    with rows and columns” into “becoming the owners and guardians of language and
    knowledge.”
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现代数据工程堆栈现在将传统的仓库与对象存储、向量数据库和特征存储相结合。Airflow和Dagster等编排框架与LLMOps工具共存，治理范围扩展到涵盖模型卡片、数据集营养标签以及每个标记的来源追溯。支持LLM将数据工程从“行和列的管道”转变为“成为语言和知识的所有者和守护者”。
- en: Data engineering directly impacts how well ML applications and LLMs perform.
    The quality, type, and amount of data used during training can make or break a
    model’s effectiveness. There are two additional complications here. The first
    is that almost all of the data used for LLMs is unstructured. The second is that
    there is a lot more data. These two differences make some tasks substantially
    harder. For example, in traditional machine learning, you can check a data input
    for outliers, perhaps by discarding all records in which someone’s age is listed
    as negative or over 130 years old. This is much harder to do with unstructured
    data, making data management for LLMs substantially more complex than data engineering
    for non-generative ML models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据工程直接影响到机器学习和LLM的表现。在训练过程中使用的质量、类型和数量的数据可以决定模型的有效性。这里还有两个额外的复杂性。第一个是，几乎所有用于LLM的数据都是非结构化的。第二个是，数据量更多。这两个差异使得一些任务变得更加困难。例如，在传统的机器学习中，你可以检查数据输入是否存在异常值，可能通过丢弃所有将某人年龄列为负数或超过130岁的记录。这在使用非结构化数据时要困难得多，使得LLM的数据管理比非生成性ML模型的数据工程要复杂得多。
- en: Data Management
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据管理
- en: While *data management* focuses on managing an organization’s data assets, *data
    engineering* involves designing and building infrastructure for data storage,
    processing, and analysis. An effective data engineering team for LLMs requires
    both a DataOps engineer, to focus on data management, and a data engineer, to
    focus on data pipeline design and management (see [Figure 4-1](#ch04_figure_1_1748895507345505)).
    Together they integrate diverse data sources, help LLMs learn better, and help
    avoid problems like hallucination and bias.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然*数据管理*侧重于管理组织的资产数据，*数据工程*涉及设计并构建数据存储、处理和分析的基础设施。一个有效的 LLM 数据工程团队需要一位数据操作工程师，专注于数据管理，以及一位数据工程师，专注于数据管道设计和管理（见[图
    4-1](#ch04_figure_1_1748895507345505)）。他们共同整合多样化的数据源，帮助 LLM 更好地学习，并帮助避免诸如幻觉和偏差等问题。
- en: '![](assets/llmo_0401.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![LLM 组件](assets/llmo_0401.png)'
- en: Figure 4-1\. Components of data engineering for LLMs
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. LLM 数据工程的组件
- en: 'There are two basic approaches to data management for LLMs: static and dynamic.
    *Static data management* means keeping the dataset the same throughout training.
    This can lead to issues like repetitive data that does not adapt to the model’s
    changing needs. *Dynamic data management* involves continuously updating and tweaking
    the data as the model trains. This method is more flexible, but it can be trickier
    to handle because it requires constant attention to the data’s quality and relevance.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 LLM 的数据管理有两种基本方法：静态和动态。*静态数据管理*意味着在整个训练过程中保持数据集不变。这可能导致诸如重复数据不适应模型变化需求等问题。*动态数据管理*涉及在模型训练过程中持续更新和调整数据。这种方法更灵活，但处理起来可能更复杂，因为它需要持续关注数据的质量和相关性。
- en: Some methods adjust the dataset dynamically during training. For example, *dynamic
    data pruning* removes  less useful examples as training progresses, and *binary
    classifiers* can help determine when to stop early, based on how well the model
    follows instructions. Other techniques involve choosing tasks that provide the
    most information or refining tasks through an iterative process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法在训练过程中动态调整数据集。例如，*动态数据修剪*随着训练的进行移除不那么有用的示例，而*二元分类器*可以根据模型遵循指令的好坏来帮助确定何时提前停止。其他技术包括选择提供最多信息的任务或通过迭代过程细化任务。
- en: Synthetic Data
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 合成数据
- en: As of this writing, newer models, such as Microsoft Phi-4 and DeepSeek-R1, have
    shown performance improvements using *synthetic data*—data that is automatically
    created from existing data while maintaining its statistical properties. For example,
    from a dataset that contains the heights, positions, and scoring records of 100
    real basketball players, you could use statistical techniques to create a large
    number of records of nonexistent players who are similar to the existing ones,
    augmenting the dataset. To synthetically create the type of long-form text used
    to train LLMs, DataOps engineers frequently use older generations of text-generating
    models.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 到本文写作时，一些新的模型，如微软 Phi-4 和 DeepSeek-R1，已经通过使用*合成数据*——从现有数据自动创建并保持其统计属性的数据——展示了性能提升。例如，从一个包含100名真实篮球运动员的身高、位置和得分记录的数据集中，你可以使用统计技术创建大量与现有运动员相似但不存在运动员的记录，从而增强数据集。为了合成创建用于训练
    LLM 的长格式文本，数据操作工程师经常使用老一代的文本生成模型。
- en: 'As we mentioned earlier, when it comes to task composition, balancing quantity
    with quality is the key. Larger datasets usually mean more diverse and higher-quality
    data, which generally leads to better performance but also requires efficient
    data-processing pipelines. To build strong models, DataOps engineers need to master
    orchestration: automatically applying these techniques at the appropriate times.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，在任务组成方面，平衡数量和质量是关键。更大的数据集通常意味着更多样化和高质量的数据，这通常会导致更好的性能，但也需要高效的数据处理管道。为了构建强大的模型，数据操作工程师需要掌握编排：在适当的时候自动应用这些技术。
- en: LLM Pipelines
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LLM 管道
- en: So what’s changed between conventional ML and LLMs, and why do we need a different
    pipeline?
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，传统机器学习和 LLM 之间有什么不同，为什么我们需要不同的管道？
- en: As mentioned, in conventional ML you’re typically dealing with *structured data*—numbers
    neatly organized in tables or spreadsheets. The data comes from databases, sensors,
    or APIs. It’s clean, manageable, and straightforward. Conventional ML leans heavily
    on *feature engineering*, where data engineers take raw data and shape it into
    something useful, crafting numerical features that feed the model what it needs
    to make predictions. It’s a hands-on process where the human touch really matters.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在传统的ML中，你通常处理的是**结构化数据**——在表格或电子表格中整齐排列的数字。数据来自数据库、传感器或API。它是干净的、可管理的、直接的。传统的ML在很大程度上依赖于**特征工程**，其中数据工程师将原始数据塑造成有用的形式，制作出数值特征，为模型提供它需要做出预测的信息。这是一个需要动手操作的过程，其中人的触摸真的很重要。
- en: In most cases, you’re working with smaller datasets. You don’t need massive
    amounts of data, and processing can be handled with traditional CPUs or GPUs.
    This approach is efficient and controlled, and it works well when the task is
    clear. It remains the go-to for tasks that need clear predictions or classifications,
    where the data is structured and the problem has a defined boundary.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，你正在处理较小的数据集。你不需要大量的数据，处理可以使用传统的CPU或GPU完成。这种方法是有效且可控的，当任务明确时，它工作得很好。对于需要明确预测或分类的任务，数据是结构化的，问题有明确的边界，这种方法仍然是首选。
- en: 'When it comes to LLMs, though, it’s all about *unstructured data*—text that’s
    messy, sprawling, and unorganized, such as articles, code, and social media posts.
    The data sources range from web scraping to document repositories and text APIs.
    It’s a flood of information, far more chaotic than the clean spreadsheets of traditional
    ML. Real-world data is unstructured. For example, imagine that you’re using data
    from news websites to train a model (note that an appropriate license is required
    for this use). If you go check a few news websites right now, you’ll likely see
    lots of other artifacts mixed with the news articles: advertisements, images,
    boxes explaining some concepts in additional detail, boxes with related news,
    a list of editor picks, and so on, all interspersed with the article itself and
    each one using a different format. In addition, news websites are massive datasets—enormous
    volumes of text that require powerful graphics processing units (GPUs) or even
    specialized hardware like tensor processing units (TPUs) to process. This is data
    at scale, and the processing power needs to match it.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到大型语言模型（LLMs）时，一切都关乎**非结构化数据**——杂乱无章、蔓延开来且无组织的文本，例如文章、代码和社交媒体帖子。数据来源从网络爬虫到文档库和文本API。这是一场信息洪流，比传统机器学习（ML）中整洁的电子表格要混乱得多。现实世界的数据是非结构化的。例如，想象一下你正在使用新闻网站的数据来训练一个模型（请注意，这种用途需要适当的许可证）。如果你现在就去检查几个新闻网站，你可能会看到许多其他与新闻文章混合的元素：广告、图片、解释某些概念的额外详细说明的框、相关新闻的框、编辑精选列表等等，所有这些都与文章本身交织在一起，并且每个元素都使用不同的格式。此外，新闻网站是庞大的数据集——大量的文本，需要强大的图形处理单元（GPUs）甚至专门的硬件，如张量处理单元（TPUs）来处理。这是大规模的数据，处理能力需要与之匹配。
- en: A final difference is that while traditional ML models have very clear performance
    metrics, such as precision and recall, LLMs need to produce content that is human-like,
    and judging whether the output is human-like frequently requires submitting the
    generated output to humans. This task requires a lot more experimentation than
    models that work with structured data. For example, with structured data, you
    can easily improve the model by dropping outliers or incorrect examples, which
    means that some classes of data are clearly more valuable than others. Given the
    state of the current research, it’s still not clear what classes of data are more
    valuable for LLM training; for example, text that contains wrong information may
    still be helpful if it has a good sentence structure. Besides dealing with unstructured
    data and requiring more computing power, the LLMOps engineer needs to perform
    experiments to improve the data input and to measure the desired outputs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个区别是，虽然传统的ML模型有非常明确的性能指标，如精确度和召回率，但LLMs需要生成类似人类的内容，而判断输出是否类似人类通常需要将生成的输出提交给人类。这项任务比处理结构化数据的模型需要更多的实验。例如，对于结构化数据，你可以通过删除异常值或错误示例来轻松提高模型，这意味着某些类别的数据比其他类别更有价值。鉴于当前的研究状况，对于LLM训练来说，哪些类别的数据更有价值仍然不清楚；例如，包含错误信息的文本，如果它有良好的句子结构，可能仍然是有帮助的。除了处理非结构化数据和需要更多的计算能力之外，LLMOps工程师还需要进行实验来改进数据输入并衡量期望的输出。
- en: Training an LLM
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练LLM
- en: 'At a high level, training an LLM has two steps: pretraining and instruction
    fine-tuning ([Figure 4-2](#ch04_figure_2_1748895507345543)). In the *pretraining*
    step, the model learns the general rules and facts of language—grammar, syntax,
    style, domain knowledge. This occurs before you ever ask it to follow instructions
    or specialize for a task. The pretraining step is usually done by occlusion: getting
    a chunk of data, hiding a word, and training a machine learning model to guess
    the word. Your goal is to train a model that minimizes the errors of guessing
    words. *Fine-tuning* is usually done by providing a set of complex instructions
    and expected answers. With this step, your goal is to train a model that minimizes
    the errors in the answers. As it is traditional in machine learning, the better
    data you have, the better your result, so we will spend the rest of this chapter
    talking about strategies to ensure your data engineering is done well.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，训练LLM有两个步骤：预训练和指令微调([图4-2](#ch04_figure_2_1748895507345543))。在*预训练*步骤中，模型学习语言的通用规则和事实——语法、句法、风格、领域知识。这发生在你从未要求它遵循指令或专门针对一项任务之前。预训练步骤通常通过遮挡来完成：获取一块数据，隐藏一个单词，并训练一个机器学习模型来猜测这个单词。你的目标是训练一个最小化猜测单词错误的模型。*微调*通常通过提供一组复杂指令和预期答案来完成。在这一步中，你的目标是训练一个最小化答案错误的模型。正如机器学习传统那样，你拥有的数据越好，你的结果就越好，所以我们将在本章的剩余部分讨论确保数据工程做得好的策略。
- en: '![](assets/llmo_0402.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/llmo_0402.png)'
- en: Figure 4-2\. Example of an LLM training pipeline
  id: totrans-41
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-2. LLM训练管道的示例
- en: The original data engineering lifecycle
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原始数据工程生命周期
- en: Conventionally, the *data engineering lifecycle* (DELC) for ML teams has looked
    like the diagram in [Figure 4-3](#ch04_figure_3_1748895507345568). The DELC comprises
    five stages that turn raw data into a useful end product, ready for consumption
    by analysts, data scientists, ML engineers, and others. As such, the role of a
    data engineer before LLMs was to develop and maintain data pipelines and ensure
    data quality.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，机器学习团队的*数据工程生命周期*（DEL C）看起来像[图4-3](#ch04_figure_3_1748895507345568)中的图表。DEL
    C包括五个阶段，将原始数据转化为有用的最终产品，供分析师、数据科学家、机器学习工程师和其他人消费。因此，在LLM出现之前，数据工程师的角色是开发和维护数据管道，并确保数据质量。
- en: '![](assets/llmo_0403.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/llmo_0403.png)'
- en: 'Figure 4-3\. The data engineering lifecycle (source: [Fundamentals of Data
    Engineering](https://learning.oreilly.com/library/view/fundamentals-of-data/9781098108298/))'
  id: totrans-45
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-3. 数据工程生命周期（来源：[数据工程基础](https://learning.oreilly.com/library/view/fundamentals-of-data/9781098108298/)）
- en: 'Back then, the five key components of the DELC were:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当时，DEL C的五个关键组成部分是：
- en: Generation
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 生成
- en: This involves working with the teams and processes that are generating the data.
    For example, if the data is being generated by an API or a survey, the engineer
    works with the team that is building the API or survey to ensure that the data
    generated is of high quality. This component also includes creating synthetic
    data, if required.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这涉及到与生成数据的团队和流程合作。例如，如果数据是由API或调查生成的，工程师将与构建API或调查的团队合作，以确保生成数据的质量高。此组件还包括在需要时创建合成数据。
- en: Ingestion
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 摄入
- en: This includes collecting and transferring data into the appropriate data storage.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括收集和将数据传输到适当的数据存储中。
- en: Storage
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 存储
- en: The engineer merges the data into data lakes and stores it in a database.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 工程师将数据合并到数据湖中，并将其存储在数据库中。
- en: Transformation
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 转换
- en: This includes *data cleaning*, which is the process of dealing with outliers,
    missing data, and duplicate data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括*数据清洗*，即处理异常值、缺失数据和重复数据的过程。
- en: Serving
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 服务
- en: Transformed data is made available to end users and/or data science teams.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的数据可供最终用户和/或数据科学团队使用。
- en: Emerging questions in data engineering
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据工程中的新兴问题
- en: Creating a DELC for LLMOps (see [Figure 4-4](#ch04_figure_4_1748895507345590))
    requires answering new questions, many of which remain unaddressed in the data
    engineering literature and practice.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为LLMOps创建DEL C（见[图4-4](#ch04_figure_4_1748895507345590)）需要回答新的问题，其中许多在数据工程文献和实践中都未得到解决。
- en: '![](assets/llmo_0404.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](assets/llmo_0404.png)'
- en: Figure 4-4\. A data management pipeline for LLMs (based on an image by [Wang
    et al., 2023](https://oreil.ly/o9BYJ))
  id: totrans-60
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-4. LLMs的数据管理管道（基于Wang等人2023年的图像[王等，2023](https://oreil.ly/o9BYJ)）
- en: 'This chapter will build toward a new DELC by addressing concerns like ​these:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将通过解决像这些问题来构建一个新的DEL C：
- en: What kind of data composition is ideal for your LLM applications?
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于您的LLM应用，哪种数据组成是理想的？
- en: Which scaling law applies to you?
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪个缩放定律适用于您？
- en: How much repetition is acceptable?
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以接受多少重复？
- en: What techniques should you use for data quality filtering?
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您应该使用哪些技术进行数据质量过滤？
- en: What models should you use to remove duplicates from the data?
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您应该使用哪些模型从数据中删除重复项？
- en: How should you handle toxic and biased data?
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您应该如何处理有毒和有偏见的数据？
- en: How much data diversity do you need?
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要多少数据多样性？
- en: How can you craft the best prompts when generating synthetic data?
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生成合成数据时，您如何制定最佳的提示？
- en: How should you monitor data aging?
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您应该如何监控数据老化？
- en: If you don’t know some of these terms, it’s totally all right; the next section
    discusses them in detail. In fact, in the latter half of this chapter, you will
    learn how to think about these problems methodologically. But first, let’s examine
    these emerging concerns one by one.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不了解这些术语，那完全没问题；下一节将详细讨论它们。实际上，在本章的后半部分，您将学习如何方法论地思考这些问题。但首先，让我们逐一审视这些新兴的担忧。
- en: Data Composition
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据组成
- en: Publicly available training datasets typically consist of a diverse array of
    data sourced from multiple domains. This multi-domain approach is common among
    LLMs. Early training corpora were composed of highly diverse data, including sources
    like web pages and Wikipedia, and were valued for their broad coverage and variety.
    However, as the focus on data quality increased, more specialized, higher-quality
    content was needed, such as books and academic papers. This shift was driven by
    the need for language models to perform advanced tasks and exhibit enhanced capabilities.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 公共可用的训练数据集通常由来自多个领域的多种数据组成。这种多领域方法在LLM中很常见。早期的训练语料库由高度多样化的数据组成，包括网页和维基百科等来源，并因其广泛的覆盖面和多样性而受到重视。然而，随着对数据质量的关注增加，需要更多专业化和高质量的内容，如书籍和学术论文。这种转变是由语言模型执行高级任务和展示增强能力的需求所驱动的。
- en: '[More-recent research shows](https://oreil.ly/nXFWr) that LLMs trained on both
    computer code and unstructured text are better at solving unstructured tasks.
    It turns out that what LLMs learn when they train on relatively structured coding
    data also transfers to unstructured tasks. At the same time, LLMs’ ability to
    generate code has improved, and software engineers and data scientists are now
    using tools like GitHub Copilot widely. These innovations mean that domains like
    code and mathematical texts have recently begun to occupy a larger proportion
    of the total training data.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[更近期的研究表明](https://oreil.ly/nXFWr)，在计算机代码和无结构文本上训练的LLM在解决无结构任务方面表现得更好。事实证明，当LLM在相对结构化的编码数据上训练时，它们学习到的知识也转移到无结构任务中。同时，LLM生成代码的能力也得到了提高，软件工程师和数据科学家现在广泛使用GitHub
    Copilot等工具。这些创新意味着代码和数学文本等领域的训练数据在最近开始占据更大的比例。'
- en: This trend suggests that, over time, a broader range of domains is being included
    in training datasets to provide LLMs with more diverse and powerful abilities.
    [Research has shown](https://oreil.ly/5WJNK) that an appropriately mixed, multi-domain
    training dataset can be vital for developing models with robust generalized capabilities.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这种趋势表明，随着时间的推移，越来越多的领域被纳入训练数据集中，以向LLM提供更多样化和强大的能力。[研究表明](https://oreil.ly/5WJNK)，一个适当混合的多领域训练数据集对于开发具有稳健泛化能力的模型至关重要。
- en: As of this writing, researchers are creating guiding principles for determining
    the optimal domain mixture ratios in pretraining. Early efforts combined careful
    experimentation with intuitive reasoning; [more recent advancements](https://oreil.ly/13j5J)
    have introduced automated methods for assigning domain weights to create a suitable
    target distribution.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，研究人员正在为确定预训练中最佳领域混合比例制定指导原则。早期的努力结合了仔细的实验和直观的推理；[更近期的进展](https://oreil.ly/13j5J)引入了自动方法来分配领域权重，以创建合适的目标分布。
- en: Scaling Laws
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩放定律
- en: Even before the widespread adoption of LLMs, the relationship between the size
    of the training dataset and the performance of transformer-based language models
    had already gained significant attention from researchers. Training a model involves
    minimizing loss, or the rate of errors generated by the model. For example, if
    the model needs to guess the next word in the phrase “I was very thirsty so I
    drank…,” guessing “bookshelf” will result in a greater loss than guessing “water.”
    [Kaplan et al. showed](https://oreil.ly/h2AGb) in 2020 that a language model’s
    loss follows a power law relationship, in which one quantity changes proportionally
    to another quantity raised to a fixed exponent. For example, the volume of a sphere
    follows a power law relationship with the cube of its radius. In language models,
    loss follows a power law relationship with either the training dataset size or
    the model size (number of parameters), provided that neither of these factors
    is a bottleneck and that the training computational budget is adequate.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在LLM（大型语言模型）广泛采用之前，训练数据集大小与基于transformer的语言模型性能之间的关系已经引起了研究者的广泛关注。训练一个模型涉及最小化损失，即模型产生的错误率。例如，如果模型需要猜测短语“I
    was very thirsty so I drank…”中的下一个单词，猜测“bookshelf”将比猜测“water”导致更大的损失。[Kaplan等人于2020年](https://oreil.ly/h2AGb)表明，语言模型的损失遵循幂律关系，其中一个量与另一个量以固定的指数成正比。例如，球体的体积与其半径的立方成正比。在语言模型中，损失与训练数据集大小或模型大小（参数数量）成正比，前提是这两个因素都不是瓶颈，并且训练计算预算充足。
- en: 'Mathematically, this relationship can be expressed as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，这种关系可以表示为：
- en: $Loss proportional-to left-parenthesis StartFraction 1 Over upper N EndFraction
    right-parenthesis Superscript alpha Baseline or Loss proportional-to left-parenthesis
    StartFraction 1 Over upper D EndFraction right-parenthesis Superscript beta$
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: $损失与左括号开始分数 1 上标 alpha 基准或损失与左括号开始分数 1 上标 beta 右括号结束$
- en: 'where:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*N* represents the model size'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N* 代表模型大小'
- en: '*D* represents the training dataset size'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*D* 代表训练数据集大小'
- en: α and β are constants that depend on the specific conditions of the model and
    dataset
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: α和β是依赖于模型和数据集具体条件的常数
- en: Kaplan and coauthors conclude that model loss decreases predictably as long
    as both the model size and training dataset size are scaled up simultaneously.
    Furthermore, they suggest that to maintain optimal performance, the sizes of the
    model and the training dataset should scale at roughly the same rate, assuming
    a large enough computing budget.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Kaplan及其合作者得出结论，只要模型大小和训练数据集大小同时扩大，模型损失就会可预测地减少。此外，他们建议，在足够大的计算预算下，为了保持最佳性能，模型和训练数据集的大小应大致以相同的速率增长。
- en: 'Additionally, they analyze the optimal allocation of resources, given a fixed
    computing budget *C*, and find that the optimal training dataset size and the
    optimal model size should have the following relationship:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，他们分析了在固定计算预算 *C* 的情况下资源的最优分配，并发现最优训练数据集大小和最优模型大小应满足以下关系：
- en: $upper D Subscript opt Baseline proportional-to upper C Superscript 0.27 Baseline
    and upper N Subscript opt Baseline proportional-to upper C Superscript 0.73$
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: $上标 D Subscript opt 基准与上标 C 上标 0.27 基准成正比，上标 N Subscript opt 基准与上标 C 上标 0.73
    成正比$
- en: This indicates that the model size should increase faster than the training
    dataset size to achieve the best performance, provided that the computing budget
    is fixed.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明，在计算预算固定的情况下，为了达到最佳性能，模型大小应比训练数据集大小增长得更快。
- en: As LLMs became larger and more widely adopted, extracting the best possible
    training from a computing budget became a lot more economically important. Building
    on Kaplan’s work, [Hoffmann and coauthors](https://oreil.ly/VFC8X) conducted experiments
    with much larger language models and proposed a new scaling law, often referred
    to as the *Chinchilla scaling law*, that highlights the trade-off between model
    size and the amount of training data. It suggests that many existing LLMs, like
    GPT-3, are *undertrained* relative to their size, meaning they have more parameters
    than necessary for the data they were trained on. The law posits that, to achieve
    optimal performance, there should be a balance between increasing the number of
    parameters and increasing the volume of the training data, with a preference for
    scaling data if the compute budget is fixed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLM变得更大并被更广泛地采用，从计算预算中提取最佳训练效果变得在经济上更加重要。在Kaplan的工作基础上，[Hoffmann及其合作者](https://oreil.ly/VFC8X)进行了更大语言模型的实验，并提出了一种新的缩放定律，通常被称为*Chinchilla缩放定律*，它突出了模型大小与训练数据量之间的权衡。该定律表明，许多现有的LLM，如GPT-3，相对于其大小而言是*欠训练*的，这意味着它们具有比训练数据所需更多的参数。该定律认为，为了实现最佳性能，应在增加参数数量和增加训练数据量之间取得平衡，如果计算预算固定，则优先考虑缩放数据。
- en: Models that follow this principle achieve better performance than larger models
    trained on less data while requiring fewer computational resources. This insight
    has shifted the focus of LLM research from simply making models larger and larger
    to allocating compute more efficiently between model size and data quantity.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循这一原则的模型在训练数据较少的情况下，比大型模型实现了更好的性能，同时需要的计算资源更少。这一洞察使LLM研究的重点从简单地使模型越来越大转移到了在模型大小和数据量之间更有效地分配计算资源。
- en: Data Repetition
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据重复
- en: While early studies on scaling laws focused on models trained on unique data
    for only one epoch, recent research has explored the effects of *repeating* data
    within the training dataset. As models continue to grow, the demand for high-quality
    training data increases, raising concerns about potentially exhausting the supply
    of such data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然早期关于缩放定律的研究主要集中在只对一个epoch进行唯一数据训练的模型上，但最近的研究已经探索了在训练数据集中*重复*数据的影响。随着模型持续增长，对高质量训练数据的需求增加，引发了关于可能耗尽此类数据供应的担忧。
- en: To address these concerns, several studies have examined the impact of repeated
    pretraining on entire datasets for multiple epochs. These studies have introduced
    a scaling law for repeated training, which highlights diminishing returns as repetition
    increases and model sizes become larger. This phenomenon, of model performance
    deterioration over successive epochs, is known as *multi-epoch degradation*. This
    degradation is influenced by factors such as dataset size, model parameters, and
    training objectives. As expected, [researchers tried](https://oreil.ly/NWMdx)
    the classic data improvement techniques to see if they could improve the effectiveness
    of the existing data for training. Most of them proved largely ineffective except
    for a technique called *dropout*, which has shown some benefit.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些担忧，一些研究考察了重复预训练对整个数据集多个epoch的影响。这些研究引入了重复训练的缩放定律，突出了随着重复次数增加和模型大小增大而出现的递减回报。这种在连续epoch中模型性能下降的现象被称为*多epoch退化*。这种退化受数据集大小、模型参数和训练目标等因素的影响。正如预期的那样，[研究人员尝试](https://oreil.ly/NWMdx)了经典的数据改进技术，以查看它们是否可以提高现有数据的训练效果。其中大部分证明效果不大，除了称为*dropout*的技术，它显示出一些益处。
- en: Data Quality
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量
- en: A variety of quality control techniques are used in pretraining LLMs to ensure
    the datasets are clean and effective. These include quality filtering, deduplication,
    and toxicity filtering. Other aspects of the data that are important to improving
    LLM performance include data diversity and data age.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练大型语言模型（LLM）时，使用了各种质量控制技术以确保数据集干净且有效。这些技术包括质量过滤、去重和毒性过滤。对提高LLM性能至关重要的数据方面还包括数据多样性和数据年龄。
- en: 'Let’s take a look at these points more closely:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这些要点：
- en: Quality filtering
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 质量过滤
- en: Public datasets often contain low-quality data that can hinder LLMs’ training.
    Common Crawl, for example, is a publicly available web archive that provides vast
    amounts of raw data collected through web scraping. It includes a diverse range
    of content, such as blog posts, news articles, forum discussions, and even spam
    or irrelevant web pages. While this breadth can be valuable, its quality is uneven;
    Common Crawl datasets frequently include outdated, redundant, or poorly formatted
    text, as well as content with bias, misinformation, or offensive material.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 公共数据集通常包含可能阻碍大型语言模型训练的低质量数据。例如，Common Crawl是一个公开可用的网络存档，它通过网络抓取收集了大量的原始数据。它包括各种内容，如博客文章、新闻文章、论坛讨论，甚至垃圾邮件或不相关的网页。虽然这种广度可能很有价值，但其质量参差不齐；Common
    Crawl数据集经常包括过时、冗余或格式不佳的文本，以及带有偏见、错误信息或冒犯性材料的内容。
- en: Deduplication
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 去重
- en: '*Deduplication* means ensuring that the dataset is free from repeated or redundant
    content. This process is important for several reasons. First, it reduces the
    risk of *memorization*, ​where the model learns specific phrases or examples by
    repetition instead of generalizing patterns across the data. Second, it minimizes
    the *train–test overlap*, which happens when identical or very similar data appears
    in both the training set and evaluation tests, potentially making the test dataset
    less effective and inflating performance metrics. Third, removing unnecessary
    repetitions allows the model to focus on diverse and unique content, which leads
    to better generalization. Deduplication aims to help the model to achieve low
    *perplexity*, a measure of how well the model predicts the next word.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*去重*意味着确保数据集不包含重复或冗余的内容。这个过程有几个重要的原因。首先，它减少了*记忆化*的风险，即模型通过重复学习特定的短语或例子，而不是在数据中泛化模式。其次，它最小化了*训练-测试重叠*，这种情况发生在相同或非常相似的数据同时出现在训练集和评估测试中，可能会使测试数据集的效果降低并夸大性能指标。第三，移除不必要的重复内容使得模型能够专注于多样化和独特的内容，这有助于更好的泛化。去重旨在帮助模型实现低*困惑度*，这是衡量模型预测下一个单词效果好坏的一个指标。'
- en: Toxicity and bias filtering
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 毒性及偏见过滤
- en: '*Toxicity filtering* means removing content that is rude, disrespectful, or
    otherwise likely to generate negative interactions. Since raw text corpora often
    contain toxic content, toxicity filters help prevent LLMs from generating harmful
    outputs. These filters typically use heuristic and rule-based methods, as well
    as *n*-gram classifiers. While toxicity filtering is effective in reducing the
    risk of generating toxic context, it can sometimes compromise the model’s ability
    to generalize as well as its ability to identify toxic content.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*毒性过滤*意味着移除粗鲁、不尊重或不利于产生负面互动的内容。由于原始文本语料库通常包含毒性内容，毒性过滤器有助于防止大型语言模型生成有害的输出。这些过滤器通常使用启发式和基于规则的方法，以及*n*-gram分类器。虽然毒性过滤在减少生成毒性上下文的风险方面是有效的，但它有时可能会损害模型泛化的能力，以及识别毒性内容的能力。'
- en: In fact, it’s hard to filter content appropriately. For example, texts about
    marginalized groups frequently contain terms that are deemed toxic. When filtering
    documents containing toxic terms, you may also filter away documents that are
    useful for marginalized groups. This increases the risk of marginalizing minority
    groups in the data, [presenting a challenge](https://oreil.ly/B2y3x) in [building
    unbiased LLMs](https://oreil.ly/p4sBy).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，适当过滤内容是有难度的。例如，关于边缘化群体的文本经常包含被认为是毒性的术语。在过滤包含毒性术语的文档时，你也可能会过滤掉对边缘化群体有用的文档。这增加了在数据中边缘化少数群体的风险，[提出了一个挑战](https://oreil.ly/B2y3x)，在[构建无偏见的LLMs](https://oreil.ly/p4sBy)中。
- en: Data diversity
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 数据多样性
- en: '*Data diversity* ensures that the model learns from a wide range of linguistic
    styles, cultural contexts, and knowledge domains. For example, including text
    from scientific articles, creative writing, legal documents, social media, and
    conversational dialog helps the model respond appropriately in different scenarios.
    Moreover, linguistic diversity—covering multiple languages, dialects, and regional
    expressions—ensures that the model is accessible to and effective for a global
    audience. Without sufficient diversity, LLMs risk becoming narrowly specialized
    or biased, reducing their usefulness and fairness in real-world applications.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据多样性*确保模型从广泛的语篇风格、文化背景和知识领域学习。例如，包括科学文章、创意写作、法律文件、社交媒体和对话文本，有助于模型在不同场景中做出适当的响应。此外，语言多样性——涵盖多种语言、方言和地区表达——确保模型对全球受众来说是可访问和有效的。如果没有足够的多样性，LLM可能会变得过于专业化或存在偏见，降低其在现实世界应用中的有用性和公平性。'
- en: Achieving data diversity comes with significant challenges. Public datasets
    often overrepresent certain languages, regions, or demographics while underrepresenting
    others. For example, web-based datasets like Common Crawl often disproportionately
    feature English-language content and informal text, leaving many languages and
    formal writing styles underrepresented.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 实现数据多样性伴随着重大的挑战。公共数据集往往过度代表某些语言、地区或人口统计，而其他方面则代表性不足。例如，基于网络的公共数据集，如Common Crawl，通常不成比例地突出英语内容和非正式文本，导致许多语言和正式写作风格代表性不足。
- en: Data age
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 数据时代
- en: Recent LLMs are often pretrained using newer data, since some of the knowledge
    the data contains can be time-sensitive. The temporal shift between the pretraining
    data and the evaluation data can lead to inaccurate performance estimates. This
    can be hard to correct through fine-tuning, especially for larger models. This
    issue underlines the importance of considering data age in the pretraining process.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 近期的大型语言模型（LLM）通常使用较新的数据进行预训练，因为数据中包含的一些知识可能是时效性的。预训练数据和评估数据之间的时间差异可能导致性能估计不准确。这个问题很难通过微调来纠正，尤其是对于较大的模型。这个问题强调了在预训练过程中考虑数据年龄的重要性。
- en: Maintaining a balance among domain composition, data quantity, and data quality
    in pretraining LLMs is challenging due to several complex interdependencies.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在预训练大型语言模型（LLM）时，在领域组成、数据量和数据质量之间保持平衡是一项挑战，因为存在几个复杂的相互依赖关系。
- en: As of this writing, researchers have proposed scaling laws to help us understand
    how different factors—like data quantity, domain composition, and data quality—work
    together to influence model performance. [One 2024 study](https://oreil.ly/Vp_SB)
    has shown a positive correlation between data quality and model scale when the
    total amount of data remains constant. These interactions make it difficult to
    optimize one aspect without affecting others, so balancing these factors often
    involves various trade-offs. For example, increasing data quantity can sometimes
    lead to lower data quality if the additional data is less relevant or more noisy.
    Similarly, focusing on high-quality data might reduce the overall quantity available
    for training. These trade-offs become even more pronounced when working within
    a fixed computational budget, as optimizing for one factor may necessitate compromises
    in others.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，研究人员已经提出了缩放定律，帮助我们理解不同因素——如数据量、领域组成和数据质量——如何共同影响模型性能。[一项2024年的研究](https://oreil.ly/Vp_SB)表明，当数据总量保持不变时，数据质量与模型规模之间存在正相关关系。这些相互作用使得在不影响其他方面的情况下优化一个方面变得困难，因此平衡这些因素通常涉及各种权衡。例如，增加数据量有时会导致数据质量下降，如果额外数据的相关性较低或噪声较多。同样，专注于高质量数据可能会减少可用于训练的整体数据量。当在固定的计算预算内工作时，这些权衡变得更加明显，因为优化一个因素可能需要在其他方面做出妥协。
- en: '*Global deduplication*, which removes overlaps among different domains, adds
    another layer of complexity. While it is essential for reducing redundancy and
    improving model efficiency, it can also inadvertently remove valuable information,
    especially if the overlaps are not perfectly identified. [Another study](https://oreil.ly/x7ovs)
    suggests that domains with higher quality and diversity are more beneficial to
    model performance than others, further complicating the selection process.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*全球去重*，它消除了不同领域之间的重叠，增加了另一层复杂性。虽然这对于减少冗余和提高模型效率至关重要，但它也可能无意中删除有价值的信息，尤其是如果重叠部分没有完全识别出来。[另一项研究](https://oreil.ly/x7ovs)表明，高质量和多样化的领域比其他领域更有利于模型性能，进一步复杂化了选择过程。'
- en: Finally, the relationship among domain composition, data quantity, and data
    quality isn’t static. These factors interact dynamically and synergistically,
    meaning that changes in one area can have unpredictable effects on the others.
    This complexity makes it challenging to develop a one-size-fits-all strategy.
    Optimizing LLM training requires continuous adjustment and fine-tuning based on
    specific model goals and constraints. However, the following section provides
    a general preprocessing pipeline that you can adapt as needed. Later in the chapter,
    you’ll learn about the specific challenges of preprocessing instructional datasets.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，领域组成、数据量和数据质量之间的关系并非静态。这些因素动态且协同地相互作用，意味着一个领域的变动可能会对其他领域产生不可预测的影响。这种复杂性使得制定一种通用的策略变得具有挑战性。优化LLM训练需要根据具体的模型目标和约束进行持续调整和微调。然而，接下来的部分提供了一个通用的预处理流程，你可以根据需要对其进行调整。在章节的后面，你将了解到预处理教学数据集的具体挑战。
- en: A General Data-Preprocessing Pipeline for LLMs
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM的通用数据预处理流程
- en: 'The pipeline presented here has 10 basic steps. Before you begin, however,
    there is a “step 0”: defining how you’ll measure success. Once you do all these
    steps, how will you know whether the process worked and so the new version improves
    on earlier ones? These techniques are discussed in detail in [Chapter 7](ch07.html#ch07_evaluation_for_llms_1748896751667823),
    but here’s a general approach to basic measurement.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提出的流程有10个基本步骤。但在开始之前，有一个“步骤0”：定义你将如何衡量成功。完成所有这些步骤后，你将如何知道这个过程是否成功，以及新版本是否比早期版本有所改进？这些技术将在第7章中详细讨论，但这里有一个基本的测量方法。
- en: 'First, establish a small set of *core metrics* that you can compute after every
    pipeline run. A quick way to do this is to have a set of prompts for which you
    know the correct answer and that you can quickly evaluate, ranging from simple
    questions such as “What’s 2 + 2?” and “What’s the capital of France?” to more
    complex questions such as “Answer with either yes or no: Is this a picture of
    a bird?” or “Answer either yes or no: Is Tom Cruise the son of Mary Lee Pfeiffer?”
    These fast-to-compute answers act as smoke alarms:, so if a change to data collection,
    deduplication, or weighting derails the model, the problem shows up immediately.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，建立一个小的核心指标集，你可以在每次流程运行后进行计算。一种快速的方法是准备一组你知道正确答案且可以快速评估的提示，从简单的问题如“2 + 2等于多少？”和“法国的首都是哪里？”到更复杂的问题如“是或否回答：这是一张鸟的图片吗？”或“是或否回答：汤姆·克鲁斯是玛丽·李·佩弗的儿子的吗？”这些快速计算的答案就像烟雾报警器一样，如果数据收集、去重或加权的改变导致模型出现问题，问题会立即显现。
- en: In addition, run periodic LLM evaluations using traditional benchmarks such
    as Massive Multitask Language Understanding (MMLU; see [Chapter 7](ch07.html#ch07_evaluation_for_llms_1748896751667823))
    and your safety and bias check prompts (see [Chapter 8](ch08.html#ch08_governance_monitoring_privacy_and_security_1748896766177413)).
    Minor variations up and down are expected, but as you train, you want to make
    sure that things are moving up in aggregate rather than getting stuck at a low
    point or sharply dropping once you make a change to one of the steps of data preprocessing.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，定期使用传统的基准（如大规模多任务语言理解MMLU；参见第7章）和你的安全与偏见检查提示（参见第8章）对LLM进行评估。上下微小的变化是可以预期的，但在训练过程中，你想要确保整体上是向上的，而不是在某个低点停滞或在你对数据预处理的某个步骤进行更改后急剧下降。
- en: With that introduction to measurement out of the way, let’s go through the 10
    steps of data preprocessing.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍测量方法之后，让我们来过一遍数据预处理的10个步骤。
- en: 'Step 1: Catalog Your Data'
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步：整理你的数据
- en: Before anything else, you need to get clear on what kind of data you actually
    need. What’s the end goal? How will the model be applied? Those answers should
    guide how you select your pretraining data. Defining your data types, language,
    domain, and quality standards early on means setting yourself up with a clear
    target so you’re not just collecting data for the sake of it. Organize your sources
    into a database so that you can tag the data you collect later.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在做任何事情之前，你需要明确你实际上需要什么类型的数据。最终目标是什么？模型将如何应用？这些答案应该指导你选择预训练数据。尽早定义你的数据类型、语言、领域和质量标准意味着为自己设定一个明确的目标，这样你就不会只是为了收集数据而收集数据。将你的来源组织成数据库，这样你就可以标记你收集的数据。
- en: 'Step 2: Check Privacy and Legal Compliance'
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二步：检查隐私和合规性
- en: Next, ensure that you’ll stay within the bounds of data privacy laws and in
    compliance with legal regulations. This step isn’t just about protecting yourself;
    it’s about respecting the data and the people it may represent. Make sure you
    have the appropriate licenses for the data that you want in your data catalog.
    Make a note of the license in a database so that you use it to tag the data you
    collect later.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，确保你将遵守数据隐私法律和法律法规。这一步不仅仅是保护自己，更是尊重数据及其可能代表的人。确保你有你想要在数据目录中的数据的适当许可证。在数据库中记录许可证，以便你以后用它来标记你收集的数据。
- en: 'Step 3: Filter the Data'
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步：过滤数据
- en: Not all data is created equal, and the quality of your sources matters. Look
    to a variety of sources—websites, books, academic papers—but make sure they match
    your predefined criteria. Reliable, accurate data is crucial. For instance, in
    a great example of mitigating risks early, the [CulturaX corpus](https://oreil.ly/ZJqyL)
    filters out harmful content through a blacklist. Specialized filters, or even
    cloud-based solutions, can help you avoid low-quality sources before you begin.
    Here you have the option to discard the data you don’t use or to simply mark in
    a database that you filtered it out. The latter strategy consumes more storage
    but makes it easier to add and remove filters later.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有数据都是平等的，你数据源的质量至关重要。参考各种来源——网站、书籍、学术论文——但确保它们符合你预定义的标准。可靠、准确的数据至关重要。例如，在早期缓解风险的优秀例子中，[CulturaX语料库](https://oreil.ly/ZJqyL)通过黑名单过滤掉有害内容。专门的过滤器，甚至基于云的解决方案，可以帮助你在开始之前避免低质量的数据源。这里你有选择丢弃不使用的数据或简单地在一个数据库中标记你已过滤掉它的选项。后者策略消耗更多的存储空间，但以后添加和删除过滤器更容易。
- en: 'Step 4: Perform Data Deduplication'
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步：执行数据去重
- en: The success of your data collection depends on having a strategy. What’s your
    time frame? How large is your data scope? How often will you be collecting data?
    Answering these questions up front means you’ll be able to gather a diverse set
    of data while keeping up with the real-time nature of most applications. This
    is where cloud-based platforms, which provide scalable data management, become
    especially useful. Again, you can discard or tag the data as duplicate.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你的数据收集成功取决于有一个策略。你的时间框架是什么？你的数据范围有多大？你多久收集一次数据？提前回答这些问题意味着你将能够收集多样化的数据，同时跟上大多数应用实时性的本质。这正是基于云的平台特别有用之处，它们提供可扩展的数据管理。再次强调，你可以丢弃或标记重复的数据。
- en: 'Step 5: Collect Data'
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5步：收集数据
- en: This is where the real work begins. Use web crawlers, APIs, and other tools
    to collect data from the sources you’ve identified (be sure to check the terms
    of service and avoid any copyright violations). Whether you’re using HTML parsing
    or PDF text extraction, make sure the data is clean and structured. One popular
    way is to use curated datasets like [Falcon](https://oreil.ly/-5wkX) or [CommonCrawl](https://oreil.ly/TaXZi).
    CommonCrawl offers data in the [WARC (Web ARChive) format](https://oreil.ly/-kH_a),
    containing all the raw data for the page, and in the WET (WARC encapsulated text)
    format, the subset of WARC that contains only the plain text of the body. Even
    when you’re building your own dataset, adhering to these formats is helpful as
    you can use one of the many tools available in GitHub to process files in the
    given format.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这才是真正的工作开始的地方。使用网络爬虫、API和其他工具从你已识别的来源收集数据（务必检查服务条款，避免任何版权侵犯）。无论你是使用HTML解析还是PDF文本提取，确保数据是干净和结构化的。一种流行的方式是使用像[Falcon](https://oreil.ly/-5wkX)或[CommonCrawl](https://oreil.ly/TaXZi)这样的精选数据集。CommonCrawl提供的数据以[WARC
    (Web ARChive)格式](https://oreil.ly/-kH_a)提供，包含页面的所有原始数据，以及WET (WARC封装文本)格式，只包含WARC中正文部分的纯文本。即使你在构建自己的数据集时，遵守这些格式也是有帮助的，因为你可以使用GitHub上可用的许多工具来处理给定格式的文件。
- en: As you collect the data, add the metadata from the previous steps.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集数据时，添加之前步骤中的元数据。
- en: 'Step 6: Detect Encoding'
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6步：检测编码
- en: Ensuring proper encoding is nonnegotiable. Incorrect text encoding can ruin
    your data, as encoding errors can look normal at a glance and thus be hard to
    detect. They can also generate gibberish—in some cases just for a few characters
    of your text. You can make use of encoding detection tools, such as the open source
    [Chardet](https://oreil.ly/V7Ub0), to make sure you’re processing the text files
    using the correct encoding. In this way, you’ll be able to detect errors before
    you see the model’s output. Add the encoding to the metadata of the data you collected
    in step 5.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 确保适当的编码是不可协商的。不正确的文本编码会毁掉你的数据，因为编码错误在表面上看起来可能是正常的，因此很难检测。它们还可以生成乱码——在某些情况下，只是你的文本的几个字符。你可以使用编码检测工具（如开源的[Chardet](https://oreil.ly/V7Ub0)）来确保你正在使用正确的编码处理文本文件。这样，你将能够在看到模型输出之前检测到错误。将编码添加到你在第5步收集的数据的元数据中。
- en: 'Step 7: Detect Languages'
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7步：检测语言
- en: Next up, identify the languages within your data using language detection tools
    such as [lingua-py](https://oreil.ly/0jVgJ). Then separate the data into subsets
    by language. Knowing the language of the training data is often useful, and you
    can also check that you have enough language representation in your LLM for your
    use case. For example, if you’re generating an LLM that is supposed to speak Portuguese,
    you want to have a fair amount of Portuguese data for training. Add the language
    to the metadata.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用语言检测工具（如[lingua-py](https://oreil.ly/0jVgJ)）识别你数据中的语言。然后根据语言将数据分成子集。了解训练数据的语言通常很有用，你也可以检查你的LLM是否为你的用例提供了足够的语言表示。例如，如果你正在生成一个应该讲葡萄牙语的LLM，你希望有相当数量的葡萄牙语数据用于训练。将语言添加到元数据中。
- en: 'Step 8: Chunking'
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第8步：分块
- en: Once you’ve gathered your raw data and ensured you can read it in the proper
    encoding and language, it’s time to break it down into usable parts. Extract the
    textual elements and parse them into manageable chunks. Most models have a maximum
    text size that they can use. In this step, break your input text into chunks that
    are less than that size.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你收集了原始数据并确保你可以用正确的编码和语言读取它，就是时候将其分解成可用的部分了。提取文本元素并将它们解析成可管理的分块。大多数模型都有一个它们可以使用的大文本大小限制。在这一步，将你的输入文本分成小于该大小的分块。
- en: 'There are multiple ways to do chunking:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 分块有多种方法：
- en: Fixed-size chunks are easy to code but can break ideas into separate chunks.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 固定大小的分块易于编码，但可能会将想法分成单独的分块。
- en: Sentence-based chunks are great for documents with clear, distinct ideas.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于句子的分块非常适合具有清晰、明确观点的文档。
- en: Paragraph-based chunks keep the broader context intact, but they are larger
    than sentence-based chunks.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于段落的分块可以保持更广泛的上下文完整，但它们比基于句子的分块要大。
- en: There are more advanced text-chunking techniques. For example, you can add additional
    metadata for each chunk by using an existing LLM to evaluate each chunk for sentiment
    and determine the chunk’s topic, or even submit entire documents to an existing
    LLM and ask it to return the chunks. An [even more sophisticated approach](https://oreil.ly/ZETMR),
    called agentic chunking, involves uploading each document to an existing LLM,
    creating an agent that simulates an analyst asking questions about the document,
    and recording the most frequently used chunks.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 存在更多高级的文本分块技术。例如，你可以通过使用现有的LLM（大型语言模型）来评估每个分块的情感并确定分块的主题，为每个分块添加额外的元数据，或者甚至将整个文档提交给现有的LLM并要求它返回分块。一种更复杂的方法（[更复杂的方法](https://oreil.ly/ZETMR)），称为代理分块，涉及将每个文档上传到现有的LLM，创建一个模拟分析师询问文档问题的代理，并记录最常使用的分块。
- en: Note that using LLMs for chunking can become very expensive in terms of compute
    resources. Regardless of the method used, chunks should contain all the metadata
    from the previous steps such as data source, license, encoding, and language.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，使用LLM进行分块在计算资源方面可能会变得非常昂贵。无论使用哪种方法，分块都应该包含来自之前步骤的所有元数据，例如数据来源、许可证、编码和语言。
- en: 'Step 9: Back Up Your Data'
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第9步：备份你的数据
- en: It may sound simple, but regular backups are a crucial safety net. Data loss
    can be devastating, and routine backups ensure you always have something to fall
    back on.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然听起来很简单，但定期备份是一个至关重要的安全网。数据丢失可能是灾难性的，而常规备份确保你总有东西可以依靠。
- en: 'Step 10: Perform Maintenance and Updates'
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第10步：执行维护和更新
- en: Finally, this isn’t a one-and-done process. Your data collection system needs
    maintenance. Regular updates and refinements, by updating sources or improving
    strategies, ensure that your data stays fresh and relevant. Continuous improvement
    is key.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这并不是一个一次性的过程。你的数据收集系统需要维护。通过更新来源或改进策略进行定期更新和改进，确保你的数据保持新鲜和相关性。持续改进是关键。
- en: These steps can be used to generate raw preprocessed chunks for both the pretraining
    and fine-tuning steps. [Chapter 5](ch05.html#ch05_model_domain_adaptation_for_llm_based_applications_1748896666813361)
    talks about LLM training in detail. In order to understand a quick way of generating
    the data for the fine-tuning step, it’s helpful to understand vectorization, discussed
    next.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤可以用于生成预训练和微调步骤的原始预处理的文本块。[第5章](ch05.html#ch05_model_domain_adaptation_for_llm_based_applications_1748896666813361)详细讨论了LLM的训练。为了快速生成微调步骤的数据，了解下一节中讨论的向量化是有帮助的。
- en: Vectorization
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 向量化
- en: '*Vectorization* is the process of converting text data into a high-dimensional
    numerical representation, or *vector*, that captures its essential characteristics.
    The verb *embedding* is often used interchangeably with *vectorizing*. *Embedding*
    (as a noun) can also refer to the resulting vector itself.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '*向量化*是将文本数据转换为高维数值表示，或称为*向量*的过程，它捕捉了其基本特征。动词*嵌入*通常与*向量化*互换使用。*嵌入*（作为名词）也可以指代生成的向量本身。'
- en: Once vectors are generated, they can be stored in vector databases. Vectorization
    was popular many years before LLMs. For example, the ElasticSearch database system
    made vectors available in 2019 to allow users to query for text that was similar
    to text they provided. Today’s vector databases extend the capabilities of vector
    storage and search to massive datasets.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦生成了向量，它们可以存储在向量数据库中。向量化在LLM出现之前就已经很流行了。例如，ElasticSearch数据库系统在2019年使向量可用，以便用户可以查询与提供的文本相似的其他文本。今天的向量数据库扩展了向量存储和搜索的能力，使其能够处理大规模数据集。
- en: The most desired characteristic of an embedding process (also called an *embedding
    model*) is that it generates embeddings that are good at capturing differences
    in meaning. A good embedding model will generate embeddings that are close to
    each other when the meanings of words or phrases are close, and distant when the
    meanings of words and phrases are different.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入过程（也称为*嵌入模型*）最期望的特性是它生成的嵌入能够很好地捕捉意义的差异。一个好的嵌入模型会在单词或短语的含义相近时生成彼此接近的嵌入，而在单词和短语的含义不同时生成距离较远的嵌入。
- en: If you generated your dataset using the 10 steps described in the previous section,
    you have a dataset of several chunks of text that you generated in step 8 of that
    process. You can enrich that data with embeddings by using a model to add an embedding
    field to each chunk.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用前一部分中描述的10个步骤生成了你的数据集，那么你就有了一个由你在该过程的第8步生成的几个文本块组成的数据集。你可以通过使用模型为每个文本块添加一个嵌入字段来丰富这些数据。
- en: There are many models for generating embeddings. One way is simply to upload
    the data to a vector database (discussed in more detail in the next section).
    Another is to generate custom embeddings yourself by using models like OpenAI’s
    `text-embedding-3-large` or BERT and storing the results in the vector database.
    The best approach depends on your specific needs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多用于生成嵌入的模型。一种方法是将数据上传到向量数据库（在下一节中更详细地讨论）。另一种方法是使用像OpenAI的`text-embedding-3-large`或BERT这样的模型自行生成定制嵌入，并将结果存储在向量数据库中。最佳方法取决于你的具体需求。
- en: For example, OpenAI’s algorithm `text-embedding-3-small` accepts an [input of
    up to 8,191 tokens and outputs a vector of 1,536 real numbers, regardless of the
    size of the input](https://oreil.ly/2rapG). If you try to embed one token with
    two characters, like “no,” the output embedding will have a size of 1,536 real
    numbers. If you try to embed a paragraph containing 8,000 characters, the embedding
    will also have a size of 1,536 real numbers. Therefore, this model is a good choice
    if the chunks you generated in step 8 have up to 8,000 characters (with one token
    equivalent to about 4 characters on average), but it’s potentially wasteful if
    the chunks you generated are very small, like 100 characters. In that case, it’s
    possible that a smaller embedding model (like BERT) would offer the same performance
    at a lower price.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，OpenAI的算法`text-embedding-3-small`接受多达8,191个token的输入，并输出一个包含1,536个实数的向量，无论输入的大小如何[链接](https://oreil.ly/2rapG)。如果你尝试嵌入一个由两个字符组成的token，如“no”，输出嵌入将包含1,536个实数。如果你尝试嵌入一个包含8,000个字符的段落，嵌入也将包含1,536个实数。因此，如果步骤8中生成的片段最多有8,000个字符（平均每个token相当于约4个字符），这个模型是一个不错的选择，但如果生成的片段非常小，如100个字符，那么它可能是一种浪费。在这种情况下，可能一个更小的嵌入模型（如BERT）在较低的价格下提供相同的性能。
- en: 'Vectorization is used for an important use case for LLMs: retrieval-augmented
    generation. RAG uses an LLM to generate answers by first searching a dataset for
    chunks that are similar to a query and then using the LLM to “glue” the retrieved
    chunks together to make the answer look natural. Vector databases can speed up
    the retrieval step substantially. We talk more about RAG applications in [Chapter 6](ch06.html#ch06_api_first_llm_deployment_1748919660052702),
    but let’s first discuss vector databases.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 向量化是LLM（大型语言模型）的一个重要用例：检索增强生成（RAG）。RAG首先在数据集中搜索与查询相似的片段，然后使用LLM将检索到的片段“粘合”在一起，使答案看起来自然。向量数据库可以显著加快检索步骤。我们将在[第6章](ch06.html#ch06_api_first_llm_deployment_1748919660052702)中更多地讨论RAG应用，但首先让我们讨论向量数据库。
- en: Vector Databases
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向量数据库
- en: In many applications, you need to find text that is similar to some other text.
    For example, when looking for a product at Amazon, you might want to find similar
    products. *Vector databases* make this easy by finding text with similar meaning.
    A vector database can connect an Amazon search of “laptop backpack with USB charger”
    to items tagged “tech-friendly daypack, 17-inch, built-in power bank,” even when
    none of the keywords line up perfectly.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多应用中，你需要找到与某些其他文本相似的文字。例如，在亚马逊寻找产品时，你可能希望找到类似的产品。*向量数据库*通过找到具有相似意义的文本来简化这一过程。即使没有任何关键词完全匹配，向量数据库也能将“带有USB充电器的笔记本电脑背包”的亚马逊搜索与标记为“技术友好型日背包、17英寸、内置电源银行”的项目相连接。
- en: 'For many search applications, you don’t need a vector database. Unless your
    dataset is very big (and when training LLMs or running a global ecommerce site,
    it will be), remember the [advice](https://oreil.ly/ez8fn) of Andrej Karpathy,
    former director of AI at Tesla and part of the founding team at OpenAI: when starting
    a project, all you might need is the free NumPy Python library.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多搜索应用，你不需要向量数据库。除非你的数据集非常大（在训练LLM或运行全球电子商务网站时，情况通常如此），请记住特斯拉前AI总监、OpenAI创始团队一员的安德烈·卡帕西（Andrej
    Karpathy）的建议[建议](https://oreil.ly/ez8fn)：在开始一个项目时，你可能需要的只是一个免费的NumPy Python库。
- en: Vector databases are designed to store embeddings and quickly perform searches
    for embeddings that are close to a given embedding. Imagine you have a bunch of
    data—maybe text, images, or a mix of both. The first step is to vectorize it.
    Once that’s done, the database can store these vectors in a way that makes it
    easy to find similar data points. This process is called *indexing*. When a text
    query is submitted, the query is also vectorized, and then the database calculates
    the distance of the submitted query to distances of the stored records by performing
    a [*nearest neighbor search*](https://oreil.ly/oq2Du). This means that you can
    find the most similar items to a given query in a vector space.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库被设计用来存储嵌入并快速执行与给定嵌入接近的嵌入的搜索。想象一下，你有一堆数据——可能是文本、图像，或者两者的混合。第一步是将这些数据向量化。一旦完成，数据库就可以以易于找到相似数据点的方式存储这些向量。这个过程被称为*索引*。当提交文本查询时，查询也会被向量化，然后数据库通过执行[*最近邻搜索*](https://oreil.ly/oq2Du)来计算提交的查询与存储记录的距离。这意味着你可以在向量空间中找到与给定查询最相似的项目。
- en: Vector databases allow you to store metadata along with your data and its embedding
    vector. You can use this to narrow down your search space before performing a
    vector similarity search. This can significantly improve query efficiency, especially
    for large datasets. For example, if you already know the language of your query
    and you have a language field in your metadata, you can use it to improve performance.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库允许你存储与你的数据和其嵌入向量相关的元数据。你可以在执行向量相似度搜索之前使用它来缩小搜索范围。这可以显著提高查询效率，尤其是在大型数据集上。例如，如果你已经知道你的查询语言，并且你的元数据中有一个语言字段，你可以使用它来提高性能。
- en: When choosing a vector database, a few considerations are scalability, fault
    tolerance, and availability of indexing techniques, in addition to cost. These
    characteristics are ​detailed ​in [Table 4-1](#ch04_table_1_1748895507351911).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当选择向量数据库时，除了成本之外，还需要考虑可扩展性、容错性和索引技术的可用性。这些特性在[表4-1](#ch04_table_1_1748895507351911)中详细说明。
- en: Table 4-1\. Choosing vector databases
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表4-1\. 选择向量数据库
- en: '| Feature | Description | Common metrics | Indexing techniques (impact) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 描述 | 常见指标 | 索引技术（影响） |'
- en: '| --- | --- | --- | --- |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Scalability | Ability to handle increasing data volume and query load | Throughput
    (queries per second)Latency (query execution time)Storage capacity | Horizontal
    scaling capability (affects throughput)Sharding strategy (impacts query efficiency)
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 可扩展性 | 处理不断增加的数据量和查询负载的能力 | 吞吐量（每秒查询数）延迟（查询执行时间）存储容量 | 水平扩展能力（影响吞吐量）分片策略（影响查询效率）
    |'
- en: '| Fault tolerance | Ability to maintain availability during failures | Uptime
    percentageTime to recovery (MTTR) | Data replication (ensures data availability)High
    availability (HA) features (minimize downtime) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 容错性 | 在故障期间保持可用性的能力 | 正常运行时间百分比恢复时间（MTTR） | 数据复制（确保数据可用性）高可用性（HA）功能（最小化停机时间）
    |'
- en: '| Indexing techniques | Methods for efficient search in high-dimensional vector
    spaces | Search accuracy (retrieval of relevant vectors)Search speed | Metric
    trees (HNSW, VP-Tree): Efficient for moderate data and dimensionalityHashing (LSH):
    Faster for approximate nearest neighbors (trade-off with accuracy)Inverted file
    (IVF): Improves speed if you can use several metadata filters |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 索引技术 | 在高维向量空间中进行高效搜索的方法 | 搜索精度（检索相关向量）搜索速度 | 度量树（HNSW、VP-Tree）：适用于中等数据和维度Hashing（LSH）：在近似最近邻搜索中更快（与精度权衡）倒排文件（IVF）：如果你可以使用多个元数据过滤器，可以提高速度
    |'
- en: Maintaining Fresh Data
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维护新鲜数据
- en: 'Step 10 of developing the preprocessing pipeline is about keeping the data
    up-to-date. Keeping the index synchronized with real-time changes in your underlying
    data can be challenging, with methods on a continuum from “Tell me if there have
    been any changes when I ask” to “Tell me instantly about changes whenever they
    happen.” *Polling* is the simplest approach: you periodically query the source
    data and compare the latest snapshot with what is already in the database. It
    is easy to understand and works when your data volume or freshness requirements
    are modest, but it wastes resources by repeatedly asking even when nothing has
    changed. It also introduces latency equal to the frequency with which you perform
    queries. If you refresh your data every year, it will take at least a year to
    detect a change.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理流程开发的第10步是确保数据保持最新。将索引与底层数据的实时变化同步可能会很具挑战性，方法从“当我询问时告诉我是否有任何变化”到“无论何时发生变化都立即告诉我”连续不断。*轮询*是最简单的方法：你定期查询源数据，并将最新的快照与数据库中已有的数据进行比较。这种方法易于理解，当你的数据量或新鲜度要求不高时，它也能正常工作，但它会浪费资源，即使没有任何变化也会反复询问。它还引入了与查询频率相等的延迟。如果你每年刷新一次数据，至少需要一年时间才能检测到变化。
- en: A more precise alternative is *change data capture* (CDC), which taps directly
    into the source’s transaction or commit log or some metadata about freshness (like
    the document’s last modified date). Instead of hunting for differences in the
    whole set of documents, you can read a list of documents that have changed and
    update only them. CDC still involves pulling data, but it eliminates guesswork
    and minimizes the bandwidth spent on checks that would result in meaningless updates.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更精确的替代方案是*变更数据捕获*（CDC），它直接连接到源的事务或提交日志或有关新鲜度的某些元数据（如文档的最后修改日期）。你不必在整个文档集中寻找差异，而是可以读取已更改的文档列表并仅更新它们。CDC仍然涉及拉取数据，但它消除了猜测并最小化了检查所花费的带宽，这些检查会导致无意义的更新。
- en: When the producer of the data can actively push information, you move into event-driven
    and streaming territory. In an *event-driven* update model, the owner of the data
    sends messages such as “product description changed” or “article updated.” Each
    event is self-contained and can trigger updates in the database immediately.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据的生产者能够积极推送信息时，你就进入了事件驱动和流式处理的领域。在*事件驱动*更新模型中，数据的所有者发送诸如“产品描述已更改”或“文章已更新”的消息。每个事件都是自包含的，并且可以立即触发数据库中的更新。
- en: Generating the Fine-Tuning Dataset
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成微调数据集
- en: If you just do the 10 steps described in the preprocessing data section, you’ll
    have enough data to perform the pretraining step of your LLM. However, for almost
    all practical applications, you need your LLM to do something in addition to guessing
    obscured words. The most common task for an LLM is answering questions, and for
    that, you’ll need to provide it with a list of questions and expected answers.
    This is called an *instruction dataset* or a *fine-tuning training dataset*.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只执行预处理数据部分中描述的10个步骤，你将拥有足够的数据来执行你LLM的预训练步骤。然而，对于几乎所有实际应用，你都需要你的LLM除了猜测隐藏的单词之外还能做其他事情。LLM最常见的任务是回答问题，为此，你需要提供给它一个问题和预期答案的列表。这被称为*指令数据集*或*微调训练数据集*。
- en: 'There are four primary approaches to creating fine-tuning training datasets:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 创建微调训练数据集有四种主要方法：
- en: Curate it manually
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 手动整理
- en: This is the most hands-on approach. You and your team curate and design the
    dataset yourself, carefully selecting and crafting each instruction to fit your
    needs. It’s slow and labor intensive, but it gives you control, which pays off
    when you need a dataset that’s highly specific or tailored to a particular task.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最实际的方法。你和你的团队自己整理和设计数据集，仔细选择和制作每条指令以满足你的需求。这个过程很慢且劳动密集，但它能让你掌握控制权，当你需要高度特定或针对特定任务定制的数据集时，这种控制权会带来回报。
- en: Collect and improve existing open source datasets
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 收集和改进现有的开源数据集
- en: Why reinvent the wheel when there’s already valuable data out there? You can
    pull from open source datasets, refining and improving them to better suit your
    needs. This is a shortcut that doesn’t sacrifice quality, especially when paired
    with some strategic enhancements. By fine-tuning what already exists, you’re leveraging
    the collective work of the community to accelerate your own progress.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当已经有有价值的数据存在时，为什么还要重新发明轮子呢？你可以从开源数据集中提取，对它们进行精炼和改进，以更好地满足你的需求。这是一个不牺牲质量的捷径，尤其是在与一些战略性的增强相结合时。通过微调现有内容，你正在利用社区的集体工作来加速你自己的进步。
- en: Generate it with an LLM
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM生成
- en: You can use an LLM to generate an instruction-tuning dataset. For this, you
    need to have your chunks in a vector database. This process is described in detail
    in the next section.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用LLM生成一个指令调整数据集。为此，你需要将你的数据块存储在向量数据库中。这个过程将在下一节中详细描述。
- en: Hybrid method
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 混合方法
- en: Finally, don’t forget that there’s strength in combining these approaches. You
    can blend manual creation, open source dataset curation, and model generation
    to cover all your bases. A simple way to combine these methods is to simply use
    them all, appending each dataset to the other. This hybrid method gives you flexibility—letting
    you tap into the best of each approach based on the task at hand.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，别忘了结合这些方法的力量。你可以将手动创建、开源数据集整理和模型生成结合起来，全面覆盖你的需求。简单来说，就是将所有方法都使用上，将每个数据集附加到另一个数据集上。这种混合方法为你提供了灵活性——让你可以根据手头的任务，充分利用每种方法的优点。
- en: There are two general categories of fine-tuning datasets. *General instruction
    fine-tuning datasets* are broad and cover a wide range of tasks across many domains.
    They’re intended to improve the model’s ability to follow general instructions,
    making it more versatile. The broader the dataset, the better your model becomes
    at understanding and executing varied instructions. *Domain-specific instruction
    fine-tuning datasets*, on the other hand, are narrow in focus and built for specialized
    fields. For example, a medical instruction dataset will train the model to handle
    tasks like diagnostics or healthcare assistance. By focusing on a specific domain,
    you’re sharpening the model’s expertise in that particular area.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 微调数据集通常分为两大类。*通用指令微调数据集*范围广泛，覆盖许多领域的各种任务。它们的目的是提高模型遵循通用指令的能力，使其更加灵活。数据集越广泛，您的模型在理解和执行各种指令方面的能力就越强。另一方面，*特定领域指令微调数据集*则专注于特定领域，为专业领域构建。例如，一个医疗指令数据集将训练模型处理诊断或医疗辅助等任务。通过专注于特定领域，您正在增强模型在该特定领域的专业知识。
- en: Automatically Generating an Instruction Fine-Tuning Dataset
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动生成指令微调数据集
- en: 'Step 1: Preprocessing and vectorization'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步：预处理和向量化
- en: We recommend using [LlamaIndex](https://oreil.ly/1uMXc) to process your large
    corpus of text data. LlamaIndex can perform many of the preprocessing steps in
    the general pipeline outlined previously, like tokenization and cleaning. It can
    also generate high-quality vector representations for each chunk in your corpus.
    You can store these document vectors in many different databases.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们推荐使用[LlamaIndex](https://oreil.ly/1uMXc)来处理您的大量文本数据。LlamaIndex可以执行之前概述的一般流程中的许多预处理步骤，如分词和清理。它还可以为您的语料库中的每个片段生成高质量的向量表示。您可以将这些文档向量存储在不同的数据库中。
- en: 'Step 2: Building a retrieval mechanism'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步：构建检索机制
- en: Create a simple program that, given a question, retrieves the closest associated
    chunks from the vector database you created in step 1\. You can use LlamaIndex’s
    out-of-the-box `VectorIndexRetriever` functionality for this.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个简单的程序，给定一个问题，从您在第1步中创建的向量数据库中检索最相关的关联块。您可以使用LlamaIndex的现成`VectorIndexRetriever`功能来完成此操作。
- en: 'Step 3: Generating questions'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步：生成问题
- en: Submit documents (not chunks) to an existing LLM and ask it to generate sets
    of questions that can be answered by the document. You may need to break up the
    document into multiple parts, but they can be much larger than the chunks in your
    database. For example, a chunk usually will have 8,000 characters (so that you
    can generate an embedding for it), while GPT-4o can generate a list of questions
    for a document containing approximately 400,000 characters.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 将文档（而不是内容块）提交给现有的LLM，并要求它生成可以由文档回答的问题集。您可能需要将文档分成多个部分，但它们可以比数据库中的内容块大得多。例如，一个内容块通常将有8,000个字符（以便为其生成嵌入），而GPT-4o可以为包含大约400,000个字符的文档生成问题列表。
- en: 'Step 4: Asking an existing LLM to decide the best answer'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 第4步：让现有的LLM决定最佳答案
- en: 'Submit each question you generated in step 3 to the program you wrote in step
    2\. The result will be a list of chunks that are closely related to the question.
    Send both the question and the list of answers to an existing LLM and ask it to
    select the chunk that contains the best answer and use that chunk to generate
    an intelligible, complete answer. You can ask the LLM to generate the output as
    a JSON-formatted record of the form `{"instruction": <question>, "input": "",
    "output": <answer>}`. Repeat the cycle until you have the desired number of examples.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '将在第3步中生成的每个问题提交到第2步中编写的程序。结果将是一个与问题密切相关的内容块列表。将问题和答案列表发送给现有的LLM，并要求它选择包含最佳答案的块，并使用该块生成一个清晰、完整的答案。您可以要求LLM以JSON格式的记录形式生成输出，格式为`{"instruction":
    <question>, "input": "", "output": <answer>}`。重复此循环，直到您获得所需数量的示例。'
- en: 'After generating questions and answers, run basic hygiene checks: deduplicate
    almost-identical question–answer pairs by computing text-level cosine similarity,
    filter out answers that introduce information not present in the retrieved passages,
    and then spot-check a small random sample by hand to be sure the automatic filters
    are calibrated correctly. If you want to make the dataset richer, you can prompt
    the LLM to ask follow-up questions that go deeper on the same passage, or to return
    answers in constrained formats such as valid JSON—both strategies teach the fine-tuned
    model to handle more complex instructions.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成问题和答案后，进行基本卫生检查：通过计算文本级别的余弦相似度来去重几乎相同的问答对，过滤掉引入了检索段落中不存在的信息的答案，然后手动检查一小部分随机样本，以确保自动过滤器校准正确。如果你想使数据集更丰富，可以提示LLM提出更深入的后续问题，或者以有效的JSON等约束格式返回答案——这两种策略都教会了微调模型处理更复杂的指令。
- en: The entire pipeline can be pared down when resources are scarce. For a small
    corpus, you might precompute embeddings with a lightweight library like BERT,
    skip the retrieval step, and have the LLM generate both the question and its answer
    from a single passage, verifying with cosine similarity that the answer remains
    close to the source text.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 当资源稀缺时，整个流程可以缩减。对于一个小型语料库，你可能可以使用BERT等轻量级库预先计算嵌入，跳过检索步骤，让LLM从一个段落中生成问题和答案，并通过余弦相似度验证答案与源文本保持接近。
- en: Conclusion
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: This chapter discussed the end-to-end data engineering pipeline for LLMs. While
    data engineering for LLMs is still a nascent field, the tips and guidelines provided
    in this chapter should give you a good foundation for every step of the process
    so you can optimize your pipelines for your specific use case.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论了大型语言模型（LLM）的端到端数据工程流程。虽然LLM的数据工程仍是一个新兴领域，但本章提供的提示和指南应该为你提供了每个步骤的良好基础，以便你可以针对特定的用例优化你的流程。
- en: References
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Chang, Ernie, et al. [“Scaling Parameter-Constrained Language Models with Quality
    Data”](https://oreil.ly/4v-i1), arXiv, October 2024.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Chang, Ernie, et al. [“Scaling Parameter-Constrained Language Models with Quality
    Data”](https://oreil.ly/4v-i1), arXiv, October 2024.
- en: 'Chardet. n.d. [“Chardet: The Universal Character Encoding Detector”](https://oreil.ly/NU-hl),
    accessed May 21, 2025.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 'Chardet. n.d. [“Chardet: The Universal Character Encoding Detector”](https://oreil.ly/NU-hl),
    accessed May 21, 2025.'
- en: 'Codd, E. F. [“A Relational Model of Data for Large Shared Data Banks”](https://oreil.ly/14TGS),
    *Communications of the ACM*13 (6): 377–87 (1970).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 'Codd, E. F. [“A Relational Model of Data for Large Shared Data Banks”](https://oreil.ly/14TGS),
    *Communications of the ACM*13 (6): 377–87 (1970).'
- en: Common Crawl. n.d. [Common Crawl](https://oreil.ly/TaXZi), accessed May 21,
    2025.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Common Crawl. n.d. [Common Crawl](https://oreil.ly/TaXZi), accessed May 21,
    2025.
- en: 'Dodge, Jesse, et al. [“Documenting Large Webtext Corpora: A Case Study on the
    Colossal Clean Crawled Corpus”](https://oreil.ly/g1Gek) arXiv, September 2021.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 'Dodge, Jesse, et al. [“Documenting Large Webtext Corpora: A Case Study on the
    Colossal Clean Crawled Corpus”](https://oreil.ly/g1Gek) arXiv, September 2021.'
- en: 'Gao, Yunfan, et al. [“Retrieval-Augmented Generation for Large Language Models:
    A Survey”](https://oreil.ly/9D0yi), arXiv, March 27, 2024.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 'Gao, Yunfan, et al. [“Retrieval-Augmented Generation for Large Language Models:
    A Survey”](https://oreil.ly/9D0yi), arXiv, March 27, 2024.'
- en: Hoffmann, Jordan, et al. [“Training Compute-Optimal Large Language Models”](https://oreil.ly/F2I7y),
    arXiv, March 2022.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Hoffmann, Jordan, et al. [“Training Compute-Optimal Large Language Models”](https://oreil.ly/F2I7y),
    arXiv, March 2022.
- en: Kaplan, Jared, et al. [“Scaling Laws for Neural Language Models”](https://oreil.ly/IwsIC),
    arXiv, January 2020.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Kaplan, Jared, et al. [“Scaling Laws for Neural Language Models”](https://oreil.ly/IwsIC),
    arXiv, January 2020.
- en: 'Lee, Cinoo, et al. [“People Who Share Encounters with Racism Are Silenced Online
    by Humans and Machines, but a Guideline-Reframing Intervention Holds Promise”](https://oreil.ly/e7va9),
    *Proceedings of the National Academy of Sciences* 121 (38): e2322764121 (2024).'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 'Lee, Cinoo, et al. [“People Who Share Encounters with Racism Are Silenced Online
    by Humans and Machines, but a Guideline-Reframing Intervention Holds Promise”](https://oreil.ly/e7va9),
    *Proceedings of the National Academy of Sciences* 121 (38): e2322764121 (2024).'
- en: LlamaIndex. n.d. [“Vector Stores”](https://oreil.ly/05_gK), accessed May 21,
    2025.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: LlamaIndex. n.d. [“Vector Stores”](https://oreil.ly/05_gK), accessed May 21,
    2025.
- en: Ma, Yingwei, et al. [“At Which Training Stage Does Code Data Help LLMs Reasoning?”](https://oreil.ly/ZVMyR),
    arXiv, September 2023.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Ma, Yingwei, et al. [“At Which Training Stage Does Code Data Help LLMs Reasoning?”](https://oreil.ly/ZVMyR),
    arXiv, September 2023.
- en: 'Nguyen, Thuat, et al., [“CulturaX: A Cleaned, Enormous, and Multilingual Dataset
    for Large Language Models in 167 Languages”](https://oreil.ly/Hfb9j), arXiv, September
    2023.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 'Nguyen, Thuat, et al., [“CulturaX: A Cleaned, Enormous, and Multilingual Dataset
    for Large Language Models in 167 Languages”](https://oreil.ly/Hfb9j), arXiv, September
    2023.'
- en: OpenAi Platform. n.d. [“Vector Embeddings”](https://oreil.ly/zj-2l), accessed
    May 21, 2025.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAi 平台. 未知日期. [“向量嵌入”](https://oreil.ly/zj-2l), 访问日期：2025年5月21日.
- en: Pemistahl. n.d. [lingua-py](https://oreil.ly/VIIBH), accessed May 21, 2025.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: Pemistahl. 未知日期. [lingua-py](https://oreil.ly/VIIBH), 访问日期：2025年5月21日.
- en: 'Penedo, Guilherme, et al. [“The RefinedWeb Dataset for Falcon LLM: Outperforming
    Curated Corpora with Web Data, and Web Data Only”](https://oreil.ly/ZBNxW), arXiv,
    June 2023.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Penedo, Guilherme, 等人. [“Falcon LLM 的 RefinedWeb 数据集：仅使用网页数据就超越精心挑选的语料库”](https://oreil.ly/ZBNxW),
    arXiv, 2023年6月.
- en: Reis, Joe and Matt Housley. [*Fundamentals of Data Engineering*](https://learning.oreilly.com/library/view/fundamentals-of-data/9781098108298/),
    O’Reilly, 2022.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Reis, Joe 和 Matt Housley. [*《数据工程基础》*](https://learning.oreilly.com/library/view/fundamentals-of-data/9781098108298/),
    O’Reilly, 2022年.
- en: 'Salley, C., et al. [“Providing OLAP to User-Analysts: An IT Mandate”](https://oreil.ly/u549E)
    (1998).'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Salley, C. 等. [“为用户分析师提供 OLAP：一项 IT 命令”](https://oreil.ly/u549E) (1998年).
- en: 'Wang, Zige, et al. [“Data Management for Large Language Models: A Survey”](https://oreil.ly/JWI_9),
    arXiv, August 2024.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: Wang, Zige, 等人. [“大型语言模型的数据管理：综述”](https://oreil.ly/JWI_9), arXiv, 2024年8月.
- en: WARC Specifications. n.d. [“The WARC Format 1.0”](https://oreil.ly/QnaDE), accessed
    May 21, 2025.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: WARC 规范. 未知日期. [“WARC 格式 1.0”](https://oreil.ly/QnaDE), 访问日期：2025年5月21日.
- en: 'Xu, Yipei, et al. [“Source Prompt: Coordinated Pre-Training of Language Models
    on Diverse Corpora from Multiple Sources”](https://oreil.ly/8-tjT), arXiv, November
    2023.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Xu, Yipei, 等人. [“源提示：从多个来源的多样化语料库上对语言模型进行协调预训练”](https://oreil.ly/8-tjT), arXiv,
    2023年11月.
- en: 'Xue, Fuzhao, et al. [“To Repeat or Not to Repeat: Insights from Scaling LLM
    Under Token-Crisis”](https://oreil.ly/5AAg5), arXiv, October 2023.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Xue, Fuzhao, 等人. [“重复与否：在令牌危机下扩展 LLM 的见解”](https://oreil.ly/5AAg5), arXiv, 2023年10月.
- en: 'Yang, Rui, et al. [“RAGVA: Engineering Retrieval Augmented Generation-Based
    Virtual Assistants in Practice”](https://oreil.ly/Zv3UP), arXiv, February 2025.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Yang, Rui, 等人. [“RAGVA：在实践中构建基于检索增强生成的虚拟助手”](https://oreil.ly/Zv3UP), arXiv,
    2025年2月.
- en: Further Reading
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Gao, Leo, et al. [“The Pile: An 800GB Dataset of Diverse Text for Language
    Modeling”](https://oreil.ly/H9Ycv), arXiv, December 2020.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Gao, Leo, 等人. [“Pile：用于语言模型的多样化文本数据集，800GB”](https://oreil.ly/H9Ycv), arXiv,
    2020年12月.
