<html><head></head><body>
  <h1 class="tochead" id="heading_id_2">7 <a id="idTextAnchor000"/><a id="idTextAnchor001"/><a id="idTextAnchor002"/><a id="idTextAnchor003"/><a id="idTextAnchor004"/>An end-to-end example using XGBoost</h1>

  <p class="co-summary-head">This chapter covers<a id="idIndexMarker000"/><a id="marker-242"/></p>

  <ul class="calibre5">
    <li class="co-summary-bullet">Gathering and preparing data from the internet, using generative AI to help</li>

    <li class="co-summary-bullet">Drafting a baseline and first tentative model to be optimized</li>

    <li class="co-summary-bullet">Figuring out how the model works and inspecting it</li>
  </ul>

  <p class="body">This chapter concludes our overview of classical machine learning for tabular data. To wrap things up, we’ll work through a complete example from the field of data journalism. Along the way, we’ll summarize all the concepts and techniques we’ve used so far. We will also use a generative AI tool, ChatGPT, to help you get the job done and demonstrate a few use cases where having a large language model (LLM) can improve your work with tabular data.<a id="idIndexMarker001"/></p>

  <p class="body">We will finally build a model to predict prices, this time using a regression-based approach. Doing this will help us understand how the model works and why it performs in a particular manner to gain further insights into the pricing dynamics for Airbnb listings and challenge our initial hypothesis regarding how pricing happens for short-term rentals.<a id="idTextAnchor005"/></p>

  <h2 class="fm-head" id="heading_id_3">7.1 Preparing and exploring your data</h2>

  <p class="body">To get started, we’ll focus on a different dataset as we continue our analysis of short-term and long-term Airbnb rental listings in New York City. This dataset comes directly from the Inside Airbnb Network initiative (<a class="url" href="http://insideairbnb.com/">http://insideairbnb.com/</a>), “a mission-driven project that provides data and advocacy about Airbnb’s effect on residential communities.” We will also be using public data from other online services, such as Foursquare (<a class="url" href="https://foursquare.com">https://foursquare.com</a>), the social network and geolocation technology company.<a id="idIndexMarker002"/><a id="idIndexMarker003"/><a id="marker-243"/></p>

  <p class="body">Following the data acquisition phase, we will organize and conduct comprehensive feature engineering based on relevant business hypotheses to extract valuable insights for our modeling stage. During this process, we will also perform basic exploratory analysis on our predictors and target variables, making necessary adjustments or exclusions of examples and features to ensure we get the optimal data for our proje<a id="idTextAnchor006"/>ct.</p>

  <h3 class="fm-head1" id="heading_id_4">7.1.1 Using generative AI to help prepare data</h3>

  <p class="body">ChatGPT is an advanced language model developed by OpenAI. To create and train a generative pretrained transformer (GPT) model like ChatGPT, OpenAI applied vast amounts of diverse internet text to help the model learn to understand and generate human-like text by predicting the next word in a sequence of words based on its contextual understanding. This pretraining allows ChatGPT to capture grammar, context, and even nuanced information, but it is not enough to make it a helpful assistant in every circumstance. In fact, these models have the potential to produce outputs that are inaccurate or harmful or contain toxic content. The reason for this is that the training dat<a id="idTextAnchor007"/>aset—that is, the internet—contains text that is varied and, at times, unreliable. To enhance the safety, utility, and alignment of ChatGPT models, a method known as reinforcement learning from human feedback is employed. In the reinforcement learning from human feedback process, human labelers provide feedback illustrating the preferred model behavior, and they evaluate multiple outputs produced by the model through ranking. This data is subsequently utilized to fine-tune GPT-3.5 further, refining its responses based on the human feedback.<a id="idIndexMarker004"/><a id="idIndexMarker005"/><a id="idIndexMarker006"/></p>

  <p class="body">To use the free version of ChatGPT (at the moment, that is ChatGPT 3.5, which is updated with information up to January 2022), you must first create an account at <a class="url" href="https://chat.openai.com">https://chat.openai.com</a>. Once you have an account, you can start using ChatGPT by simply entering a prompt. A prompt, in the context of an LLM like ChatGPT, is a written instruction or input provided to the model to generate a specific output. It serves as a query or request that guides the model in producing a relevant response. Prompts can vary in complexity, ranging from simple commands to more detailed descriptions or inquiries, and they play a crucial role in shaping the nature of the language model’s output. The prompt’s quality and clarity significantly influence the generated content’s accuracy and relevance, but selecting the right prompt is not always straightforward. <a id="idIndexMarker007"/></p>

  <p class="body">The effectiveness of different prompts can vary depending on the specific language model they are designed for. Each language model has its own strengths, weaknesses, and nuances, making it essential to tailor prompts accordingly. When working with ChatGPT, for instance, we obtained the best results starting with simple prompts, evaluating their results, and then proceeding by adding more specifications to the prompt to refine the results toward our expectations. ChatGPT tend to work better when you tell it to “write,” “create,” “show me how to,” or “summarize.” Sometimes showing examples and how you expect ChatGPT to elaborate them is quite helpful. Also, let ChatGPT know your expectations in terms of the answer, like how long the response should be, what information it should include, if you want just code as a result or just text, and how the answer should be structured in return; for instance, you could ask using the JSON format or a Python style list.<a id="idIndexMarker008"/></p>

  <p class="body">LLMs such as ChatGPT, and the related Copilot feature in GitHub, have proven to be useful assistants for a variety of programming tasks. This usefulness applies to tabular data applications. You can ask these models various questions or request assistance in coding tasks, and they can assist by providing code snippets, explanations about how code works, or guidance in using specific commands or algorithms. However, although LLMs such as ChatGPT can assist users by generating code snippets for data manipulation, cleaning, and transformation tasks, as well as provide explanations and guidance on various statistical and machine learning techniques applicable to tabular datasets, our intention in this chapter is to show a few select and less obvious LLM capabilities that you can use for your tabular data analysis and m<a id="idTextAnchor008"/>odeling.<a id="marker-244"/><a id="idIndexMarker009"/></p>

  <h3 class="fm-head1" id="heading_id_5">7.1.2 Getting and preparing your data</h3>

  <p class="body">As a starting point, we will navigate the Inside Airbnb Network website (<a class="url" href="http://insideairbnb.com/">http://insideairbnb.com/</a>) and find the data we need. Our goal is to explore the situation in a completely different city: Tokyo. First, you have to manually download the data and store it in a working directory on your computer or cloud instance. To do so, on the home page of the Inside Airbnb Network initiative, as shown in figure 7.1, in the Data menu, choose Get the<a id="idTextAnchor009"/> Data.<a id="idIndexMarker010"/><a id="idIndexMarker011"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F01_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.1 Choosing from the Data menu</p>
  </div>

  <p class="body">Once you choose the menu item, you will be moved to a new page containing Data Downloads, presenting various cities and their data file to be downloaded. Scroll the page until you find the city of Tokyo.</p>

  <p class="body">Figure 7.2 shows the portion of the page containing the data files for Tokyo as they were at the time of writing thi<a id="idTextAnchor010"/>s book.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F02_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.2 The Tokyo Data Downloads section</p>
  </div>

  <p class="body"><a id="marker-245"/>For our analysis, we need two files from the page: <code class="fm-code-in-text">listings.csv</code>, which contains the summary listings and other information about the Airbnb accommodations in Tokyo, and <code class="fm-code-in-text">calendar.csv.gz</code>, a zipped file containing <code class="fm-code-in-text">calendar.csv</code>, a dataset containing occupancy and price information for a given year for each listing. Hover over the links, right-click, and select to save them to disk in your working directory. For example, in Google Chrome, you need to select “Save link as,” and in Mozilla Firefox, you have to select “Save target as.” At this point, you will just need to extract the files into your working directory. Once the files we need are unzipped in our local directory, we can ingest them into a pandas DataFrame using the <code class="fm-code-in-text">read_csv</code> command:<a id="idIndexMarker012"/></p>
  <pre class="programlisting">import pandas as pd
summary_listings = pd.read_csv("listings.csv")</pre>

  <p class="body">With the list and type of columns, we can get an idea of the kind of data we will be dealing with:</p>
  <pre class="programlisting">summary_listings.dtypes</pre>

  <p class="body">The list comprises 18 columns, largely the same as the Airbnb NYC dataset introduced in chapter 3. Here we describe each of them:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">id</code>—A unique identifier for each listing on Airbnb. It is an <code class="fm-code-in-text">int64</code> data type, meaning it is a numerical ID representation. In other tables, it can be referred to as <code class="fm-code-in-text">listing_id</code>.<a id="idIndexMarker013"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">name</code>—The description of the Airbnb listing. It is of the <code class="fm-code-in-text">object d</code>ata type, which typically represents a string or text.<a id="idIndexMarker014"/><a id="marker-246"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">host_id</code>—A unique identifier for each host on Airbnb. It is an <code class="fm-code-in-text">int64</code> data type.<a id="idIndexMarker015"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">host_name</code>—The name of the host who owns the listing. It is of the <code class="fm-code-in-text">object</code> data type.<a id="idIndexMarker016"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">neighbourhood_group</code>—Represents the broader area or region the neighborhood belongs to. It is stored as a <code class="fm-code-in-text">float64</code> data type, but it is important to note that using a float data type to represent groups or categories is uncommon. In this case, the presence of float values indicates that the data for this field is entirely made up of missing values.<a id="idIndexMarker017"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">neighbourhood</code>—The specific neighborhood where the listing is located. It is of the <code class="fm-code-in-text">object</code> data type.<a id="idIndexMarker018"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">latitude</code>—The latitude coordinates of the listing’s location. It is of the <code class="fm-code-in-text">float64</code> data type.<a id="idIndexMarker019"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">longitude</code>—The longitude coordinates of the listing’s location. It is of the <code class="fm-code-in-text">float64</code> data type.<a id="idIndexMarker020"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">room_type</code>—The type of room or accommodation offered in the listing (e.g., entire home/apartment, private room, shared room). It is of the <code class="fm-code-in-text">object</code> data type.<a id="idIndexMarker021"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">price</code>—The price per night to rent the listing. It is of the <code class="fm-code-in-text">int64</code> data type, representing an integer price value.<a id="idIndexMarker022"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">minimum_nights</code>—The minimum number of nights that is required for booking the listing. It is of the <code class="fm-code-in-text">int64</code> data type.<a id="idIndexMarker023"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">number_of_reviews</code>—The total number of reviews received by the listing. It is of the <code class="fm-code-in-text">int64</code> data type.<a id="idIndexMarker024"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">last_review</code>—The date of the last review received by the listing. It is of the <code class="fm-code-in-text">object</code> data type, which could represent date and time information, but it might require further parsing to be used effectively.<a id="idIndexMarker025"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">reviews_per_month</code>—The average number of reviews per month for the listing. It is of the <code class="fm-code-in-text">float64</code> data type.<a id="idIndexMarker026"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">calculated_host_listings_count</code>—The total number of listings the host has on Airbnb. It is of the <code class="fm-code-in-text">int64</code> data type.<a id="idIndexMarker027"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">availability_365</code>—The number of days the listing is available for booking in a year (out of 365 days). It is of the <code class="fm-code-in-text">int64</code> data type.<a id="idIndexMarker028"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">number_of_reviews_ltm</code>—The number of reviews received in the last 12 months. It is of the <code class="fm-code-in-text">int64</code> data type.<a id="idIndexMarker029"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">license</code>—The license number or information related to the listing. It is of the <code class="fm-code-in-text">object</code> data type, which typically represents a string or text.<a id="idIndexMarker030"/></p>
    </li>
  </ul>

  <p class="body">We can safely ignore features such as <code class="fm-code-in-text">host_id</code>, <code class="fm-code-in-text">host_name</code>, <code class="fm-code-in-text">neighbourhood_group</code> (because they are completely missing), or <code class="fm-code-in-text">license</code> (which is a kind of identifier based on the host’s license).</p>

  <p class="body">As for the other features, whereas most of them are numeric, the <code class="fm-code-in-text">name</code> feature is a string containing various pieces of information to be extracted based on how the data has been organized. By visualizing a single example from it, we can have an idea of its organization: <a id="idIndexMarker031"/><a id="marker-247"/></p>
  <pre class="programlisting">summary_listings['name'].iloc[0]</pre>

  <p class="body">The string is arranged into five distinct parts separated by conventional signs and with some kind of partially structured and repeated content:</p>
  <pre class="programlisting">'Rental unit in Sumida · <span class="times">★</span>4.78 · 1 bedroom · 2 beds · 1 bath'</pre>

  <p class="body">The first portion of the string is a description of the unit type and location. The second portion is the average score received from guests. The third portion is the number of bedrooms, the fourth portion is the number of beds, and the last is the number of bathrooms.</p>

  <p class="body">Apart from the numeric values, we can also extract some specific information related to the kind of accommodation or services offered—for instance, if the apartment is a studio, if the bath is shared, and if it is a half-bath (a room with a toilet and washbasin but no bath or shower). We can deal with such information by creating simple string correspondence checks and obtaining a binary feature pointing out the presence or absence of the characteristic or using regex commands. Regex (short for regular expressions) commands are a sequence of characters constituting a search pattern. They are used for pattern matching within strings. Table 7.1 shows the transformations we apply to the description field and highlights what strings we strive to match, what regex command we use, and what resulting feature we obtain.<a id="idIndexMarker032"/></p>

  <p class="fm-table-caption">Table 7.1 Regex commands for feature engineering</p>

  <table border="1" class="contenttable-1-table" id="table001" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="50%"/>
      <col class="contenttable-0-col" span="1" width="30%"/>
      <col class="contenttable-0-col" span="1" width="20%"/>
    </colgroup>

    <thead class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <th class="contenttable-1-th">
          <p class="fm-table-head">Matched description</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Regex</p>
        </th>

        <th class="contenttable-1-th">
          <p class="fm-table-head">Resulting feature</p>
        </th>
      </tr>
    </thead>

    <tbody class="contenttable-1-thead">
      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Text that starts with “in,” followed by a space, then captures any characters until another space, and ends with a dot</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body"><code class="fm-code-in-text">r'in\s(.*?)\s·'</code> </p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Area of Tokyo of the listing</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Text that starts with the character “<span class="times">★</span>” (a star), followed by one or more digits, a dot, and one or more additional digits (e.g., <span class="times">★</span>4.5)</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body"><code class="fm-code-in-text">r'</code><span class="times">★</span><code class="fm-code-in-text">(\d+\.\d+)'</code> </p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Star ratings</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Text containing a numerical value followed by zero or more whitespace characters and the words “bedroom” or “bedrooms” (with or without the “s” at the end)</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body"><code class="fm-code-in-text">r'(\d+)\s*(?:bedroom|bedrooms)'</code> </p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Number of bedrooms</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Text containing a numerical value followed by one or more whitespace characters and the word “bed” or “beds” as a whole word (with or without the “s” at the end)</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body"><code class="fm-code-in-text">r'(\d+)\s+(?:beds?\b)'</code> </p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Number of beds</p>
        </td>
      </tr>

      <tr class="contenttable-c-tr">
        <td class="contenttable-1-td">
          <p class="fm-table-body">Text containing a numerical value representing the number of baths</p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body"><code class="fm-code-in-text">r'(?P&lt;baths&gt;\d+)\s*(shared\s+)?(?:half-)?baths?\b'</code> </p>
        </td>

        <td class="contenttable-1-td">
          <p class="fm-table-body">Number of baths</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">Working with regex commands is a bit complicated. Hence this is the first application where generative AI could help. Most LLMs, such as ChatGPT, have a good knowledge of different programming languages (in particular, most of them are quite strong in Python) because they have been trained on text and information extracted from the internet, where there is plenty of information about how to code even very specific problems. In our case, showing an example of the strings in the prompt and asking for the desired information to be extracted should do the trick:</p>
  <pre class="programlisting">I have a series of strings in this format: 'Rental unit in Sumida · <span class="times">★</span>4.78
 · 1 bedroom · 2 beds · 1 bath' Show me a series of regex commands in 
order to extract the following information: 1) the area of Tokyo of the 
listing 2) the star ratings expressed as floats 3) the number of bedrooms 
4) the number of beds 5) the number of baths.</pre>

  <p class="body"><a id="marker-248"/>The output should be something already suitable for usage, arranged in blocks of code snippets with some explanation about the extraction rule, such as shown in figure 7.3.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F03_Ryan2.png"/></p>

    <p class="figurecaption">F<a id="idTextAnchor011"/>igure 7.3 Results from a prompt on regex processing on ChatGPT 3.5</p>
  </div>

  <p class="body">Without being asked, the language model should simply decide to propose a Python-based solution, and you just click on the “Copy code” icon on top of the code listing to copy the snippet on the clipboard and then paste it into your notebook or IDE editor.</p>

  <p class="body">Usually, the solutions provided may vary from query to query and can differ from the solution we provided in the table. This is because LLMs are, in the end, probabilistic machines. Temperature is the parameter usually set to influence the randomness of the model’s output. It is used during text generation to control the generated content’s creativity. In simple terms, temperature affects the likelihood of the model choosing the next word in a sequence. Low-temperature values result in more deterministic and expected output. In contrast, high-temperature values introduce more randomness and creativity in the generated output because the model tends to choose less probable words.</p>

  <p class="body">After getting an apparently useful answer, one important step prior to using the solutions proposed by an LLM is to test them on more examples than the one or two shown in the prompt. Such a step may reveal that the code is not working well, and you may also need to tell the model that the example doesn’t work or signal the problem you are experiencing by more detailed instructions to the model. For instance, we found that sometimes the commands didn’t work properly if there were upper-case letters in some parts of the input string. Hence, we had to find a supplemental solution. All these regex commands operate lowercase, thanks to the <code class="fm-code-in-text">re.IGNORECASE</code> flag, which makes the match operating case insensitive. In the following listing, we proceed to extract information from the text descriptions using the regex commands we found using ChatGPT.<a id="idIndexMarker033"/><a id="marker-249"/></p>

  <p class="fm-code-listing-caption"><a id="idTextAnchor012"/>Listing 7.1 Extracting information from text descriptions</p>
  <pre class="programlisting">import pandas as pd
import re
 
classification_list = [
   'aparthotel', 'barn', 'bed and breakfast', 'boutique hotel',
   'bungalow', 'cabin', 'camper/rv', 'chalet', 'condo', 'cottage',
   'earthen home', 'farm stay', 'guest suite', 'guesthouse', 'home',
   'hostel', 'hotel', 'houseboat', 'hut', 'loft', 'place to stay',
   'rental unit', 'resort', 'ryokan', 'serviced apartment',
   'tiny home', 'townhouse', 'treehouse', 'vacation home', 'villa']
 
summary_listings = summary_listings.assign(
    type_of_accommodation=(
        summary_listings['name']
        .str.extract(
            f"({'|'.join(classification_list)})", 
            flags=re.IGNORECASE)),                          <span class="fm-combinumeral">①</span>
    area_of_tokyo=(
        summary_listings['name']
        .str.extract(
            r'in\s(.*?)\s·', 
            flags=re.IGNORECASE)),                          <span class="fm-combinumeral">②</span>
    score=(
        summary_listings['name']
        .str.extract(
            r'<span class="times">★</span>(\d+\.\d+)',
            flags=re.IGNORECASE)
        .astype(float)),                                    <span class="fm-combinumeral">③</span>
    number_of_bedrooms=(
         summary_listings['name']
         .str.extract(
            r'(\d+)\s*(?:bedroom|bedrooms)', 
            flags=re.IGNORECASE)
         .fillna(0)
         .astype(int)),                                     <span class="fm-combinumeral">④</span>
    number_of_beds=(
         summary_listings['name']
         .str.extract(
             r'(\d+)\s+(?:beds?\b)', 
             flags=re.IGNORECASE)
         .fillna(0)
         .astype(int)),                                     <span class="fm-combinumeral">⑤</span>
    number_of_baths=(
         summary_listings['name']
         .str.extract(
             r'(?P&lt;baths&gt;\d+)\s*(shared\s+)?' +
             r'(?:half-)?baths?\b', 
             flags=re.IGNORECASE)["baths"]
         .fillna(0)
         .astype(int)),                                     <span class="fm-combinumeral">⑥</span>
)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Extracts the type of accommodation from a list of options</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Extracts the area of Tokyo mentioned in the listing name</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Extracts the rating score from a star symbol followed by a numerical value</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Extracts the number of bedrooms from the listing name</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Extracts the number of beds from the listing name</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Extracts the number of baths from the listing name</p>

  <p class="body">Listing 7.2 completes the feature extraction work by creating additional Boolean columns based on specific keywords in the name column. It computes two calculated features based on a difference telling us the number of days between today’s date and the <code class="fm-code-in-text">last_review</code> date and a ratio expressing how the number of reviews in the last year relates to the total number of reviews. Such a ratio can reveal if the bulk of reviews is recent or if the listing has been successful mainly in the past.<a id="marker-250"/><a id="idIndexMarker034"/><a id="idTextAnchor013"/></p>

  <p class="fm-code-listing-caption">Listing 7.2 Extracting binary flags and time information</p>
  <pre class="programlisting">import numpy as np
import pandas as pd
from datetime import datetime
 
summary_listings = summary_listings.assign(
    is_new=(summary_listings['name']
               .str.contains('new', case=False)
               .astype(int)),                              <span class="fm-combinumeral">①</span>
    is_studio=(summary_listings['name']
               .str.contains('studio', case=False)
               .astype(int)),                              <span class="fm-combinumeral">②</span>
    has_shared_bath=(summary_listings['name']
                     .str.contains('shared', case=False)
                     .astype(int)),                        <span class="fm-combinumeral">③</span>
    has_half_bath=(summary_listings['name']
                   .str.contains('half', case=False)
                   .astype(int)),                          <span class="fm-combinumeral">④</span>
)
 
summary_listings['days_since_last_review'] = (
    datetime.today() – 
    pd.to_datetime(
        summary_listings['last_review'])
).dt.days
summary_listings['days_since_last_review'] = (
    summary_listings['days_since_last_review'] -
    summary_listings['days_since_last_review'].min()
)                                                          <span class="fm-combinumeral">⑤</span>
 
zero_reviews = summary_listings['number_of_reviews'] == 0
ratio = summary_listings['number_of_reviews_ltm'] / 
summary_listings['number_of_reviews']
summary_listings['number_of_reviews_ltm_ratio'] = (
    np.where(zero_reviews, 0, ratio)
)                                                          <span class="fm-combinumeral">⑥</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Checks if the word “new” is present in the name (case-insensitive)</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Checks if the word “studio” is present in the name (case-insensitive)</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Checks if the word “shared” is present in the name (case-insensitive)</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Checks if the word “half” is present in the name (case-insensitive)</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Calculates the number of days between today’s date and the last_review date</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Calculates the ratio of number_of_reviews_ltm to number_of_reviews for each listing</p>

  <p class="body"><a id="marker-251"/>The <code class="fm-code-in-text">summary_listings</code> data also has a price feature that we could use as a target. Still, we prefer to create it by aggregating the <code class="fm-code-in-text">calendar.csv</code> data to decide whether to pick an average of all the prices, the minimum, or the maximum. The <code class="fm-code-in-text">calendar.csv</code> contains information for each day about the availability of the accommodation, its price (also adjusted for discounts), and the minimum and maximum number of nights allowed for booking at that time. We are interested in processing the adjusted price as a target, representing the effective market price of the accommodation.<a id="idTextAnchor014"/><a id="idIndexMarker035"/></p>

  <p class="fm-code-listing-caption">Listing 7.3 Creating the target from daily listings</p>
  <pre class="programlisting">calendar = pd.read_csv("calendar.csv")
 
calendar["adjusted_price"] = (
    calendar["adjusted_price"]
    .apply(lambda x: float(
        x.replace('$', '')
         .replace(',', ''))
         )
)                                                         <span class="fm-combinumeral">①</span>
 
price_stats = (
    calendar.groupby('listing_id')['adjusted_price']      <span class="fm-combinumeral">②</span>
    .agg(['mean', 'min', 'max'])
)                                                         <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Removes the dollar sign ($) and commas (,) from the values and then converts them to float</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> A group by operation on the calendar DataFrame based on the listing_id column</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Calculates three statistics for the adjusted_price column: the mean, minimum, and maximum values</p>

  <p class="body">After completing the aggregation, we can check the result by requiring the first five rows of this newly created dataset:</p>
  <pre class="programlisting">price_stats.head()</pre>

  <p class="body">Figure 7.4 verifies how we now have both a mean price per listing available together with the maximum and minimum prices.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F04_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.4 Price statistics for Airbnb listings</p>
  </div>

  <p class="body">We will save this <code class="fm-code-in-text">price_stats</code> DataFrame and concentrate in the next section on improving the number and effectiveness of our features.<a id="idTextAnchor015"/><a id="idIndexMarker036"/></p>

  <h3 class="fm-head1" id="heading_id_6">7.1.3 Engineering more complex features</h3>

  <p class="body">Real estate assets have a pretty peculiar behavior, distinguishable from other products or services you find on the market. An adage in the real estate business mentions that all that matters when dealing with buildings and facilities is “location, location, location.” The position of an apartment in a city or a road can make a difference to the value of a property. We will adopt this adage for Airbnb listings and develop some feature engineering based on location.<a id="idIndexMarker037"/><a id="idIndexMarker038"/><a id="marker-252"/></p>

  <p class="body">As a first step, we will reprise the example from the previous chapter, where we created small geographic subdivisions that we later target encoded. By this approach, you should be able to capture the specific characteristics of an area, though it will be difficult to explain why renting in one particular location costs more than in others. We should prepare more specific features to provide some explainability to listings in Tokyo. Here is another point where generative AI can come to the aid of the tabular data practitioner by providing help in the form of suggestions and idea generation. LLMs have sifted through more data than you can imagine, and if queried with enough details (and some role-playing, i.e., asking them to personify an expert proficient in a specific field), they can return hints and reflections that could have cost you multiple hours of research and readings on the web.</p>

  <p class="body">Our prompt for ChatGPT is</p>

  <table border="0" class="contenttable-c-table" width="100%">
    <colgroup class="contenttable-0-colgroup">
      <col class="contenttable-0-col" span="1" width="7%"/>
      <col class="contenttable-0-col" span="1" width="93%"/>
    </colgroup>

    <tbody class="contenttable-c-tbody">
      <tr class="contenttable-c-tr">
        <td class="contenttable-c-td">
          <div class="figure2">
            <p class="figure1"><img alt="" class="calibre8" src="../Images/logo-MR.png"/></p>
          </div>
        </td>

        <td class="contenttable-c-td">
          <p class="fm-exercise">You are an expert data scientist and you have downloaded a dataset containing summarized Airbnb listings. This is the structure of the dataset: id (int64), name (object), neighbourhood (object), latitude (float64), longitude (float64), room_type (object), price (int64), minimum_nights (int64), number_of_reviews (int64), last_review (object), reviews_per_month (float64), calculated_host_listings_count (int64), availability_365 (int64), number_of_reviews_ltm (int64). You are training a machine learning model to predict the price of listings. Which features should you engineer to improve the performance of the model?</p>
        </td>
      </tr>
    </tbody>
  </table>

  <p class="body">Our strategy is to set a persona (“you are an expert data scientist”) and to provide some further information about the features available (removing the features that we actually already decided not to use) and the target variable. Here we also used the fact that we expected ChatGPT to already know something about the dataset we were using (“a dataset containing summarized Airbnb listings”), but you can also propose less known problems to the LLM by briefly describing the dataset, including the types of features and their relationships to each other. In addition, if you have any domain expertise relevant to the dataset, for instance, any hypotheses about how the features in the dataset might be related to the target variable, share them with the LLM. This information can help it identify features that may be important for the task. Moreover, providing a list of the existing features in the dataset, as we did, may not be enough for more challenging tasks. Providing an explanation or description of each variable and clearly stating the objective of your machine learning model, such as predicting customer churn, estimating real estate values, or forecasting sales, will help the LLM identify gaps or opportunities for creating new well-engineered features.<a id="marker-253"/></p>

  <p class="body">The following are the suggested features, or feature types, to strive to generate to obtain the best results in our dataset and type of problem, as stated by ChatGPT:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Distance from key locations</i>—Calculate the distance of each listing from key locations or landmarks in the city. This can be done using the latitude and longitude information.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Neighborhood statistics</i>—Aggregate statistics for each neighborhood, such as the average price, minimum and maximum prices, and the number of listings in each neighborhood.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Room type encoding</i>—Convert the categorical variable <code class="fm-code-in-text">room_type</code> into numerical values using one-hot encoding or label encoding.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Temporal features</i>—Extract relevant features from the <code class="fm-code-in-text">last_review</code> column, such as the month or season of the last review or the time since the last review.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Host experience</i>—Combine information from <code class="fm-code-in-text">calculated_host_listings_count</code> and <code class="fm-code-in-text">number_of_reviews_ltm</code> to create a feature representing the host’s experience or popularity.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Booking density</i>—Calculate the ratio of <code class="fm-code-in-text">number_of_reviews</code> to <code class="fm-code-in-text">availability_365</code> to represent how frequently a listing is booked relative to its availability.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Review sentiment</i>—If you have access to the actual review texts, you can use natural language processing techniques to analyze sentiment and extract sentiment-based features.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Interaction terms</i>—Create interaction terms between relevant features, such as the product of <code class="fm-code-in-text">minimum_nights</code> and <code class="fm-code-in-text">availability_365</code> to capture the overall availability for longer stays.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Price binning</i>—Convert the continuous <code class="fm-code-in-text">price</code> variable into bins or categories, creating a new categorical feature representing different price ranges.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Geographical clustering</i>—Use clustering algorithms to group listings based on geographical proximity. This can capture spatial patterns in pricing.</p>
    </li>
  </ul>

  <p class="body">Of the proposed points, a few do resonate with our experience, such as computing distance from key locations and neighborhood statistics or creating geographical clustering. Also, we already provided room-type encoding with our previous work on text fields and regex commands. Leaving aside review sentiment and price binning (price is actually our target), what is left is a set of ideas that can guide or inspire you to improve your work. Use the outputs of LLMs to your advantage, but don’t forget that your previous knowledge, experience, sensibility, and critical thinking can play an even more important role in the task of a tabular data analysis and that LLMs are just facilitators in your work.</p>

  <p class="body">Keeping in mind the proposed features suggested by ChatGPT, we proceed to create some of them. Regarding geographical clustering, you can find all you need in listing 7.4 to create a high cardinality geographical feature from coordinates. Later, in the data pipeline, we will target encode the high cardinality categorical feature we are going to generate with the code<a id="idTextAnchor016"/>.<a id="marker-254"/></p>

  <p class="fm-code-listing-caption">Listing 7.4 Creating a high cardinality geographical feature</p>
  <pre class="programlisting">def bin_2_cat(feature, bins=32):
    min_value = feature.min()
    bin_size = (feature.max() - min_value) / bins
    return ((feature - min_value) / bin_size).astype(int) <span class="fm-combinumeral">①</span>
    
summary_listings['coordinates'] = (
    bin_2_cat(summary_listings['latitude']) * 
    1000 +   
    bin_2_cat(summary_listings['longitude'])
)                                                         <span class="fm-combinumeral">②</span>
 
print(summary_listings['coordinates'].nunique())</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Discretizes the latitude and longitude by bin size</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Composes the new coordinates feature by summing the discretized latitude and longitude</p>

  <p class="body">This code generates a feature with 317 unique values. We covered all the Tokyo municipalities in a 32 × 32 grid, which means potentially 1,024 values. Only 317 of these coordinates contain a listing, meaning that in the future, our model can effectively predict based on this feature only if a new listing falls in one of the 317 slots previously defined. In case a new area appears, thanks to the target encoder novelty dealing capabilities (see the description of the <code class="fm-code-in-text">handle_unknown</code> parameter at <a class="url" href="https://mng.bz/oK1p">https://mng.bz/oK1p</a>), we can simply impute unknown values to the target means using the setting <code class="fm-code-in-text">handle_unknown</code>=“value”.<a id="idIndexMarker039"/></p>

  <p class="body">A critical aspect of the Tokyo real estate market is that there is an important geographical center for cultural and historical reasons, which is the Imperial Palace. Locations near this site tend to have higher real estate valuations, and some of Japan’s most expensive flats are located close to the Imperial Palace. We try to reflect this reality by creating a feature comparing the location of our Airbnb accommodation with the area of the Imperial Palace (which can be taken from sites such as latlong.net: <a class="url" href="https://mng.bz/nRW2">https://mng.bz/nRW2</a>). For distances, we convert the value into meters using a formula that involves the cosine of the radiants multiplied by a conversion factor for the measure to be intelligible to a human examination. We also adopt the Manhattan distance for better-representing distances in a city, which is the summed difference in absolute values between the latitudes and longitu<a id="idTextAnchor017"/>des.<a id="marker-255"/></p>

  <p class="fm-code-listing-caption">Listing 7.5 Computing a distance metric from the city center</p>
  <pre class="programlisting">imperial_palace_lat = 35.6841822633
imperial_palace_lon = 139.751471994
 
def degrees_to_meters(distance_degrees, latitude):
    conversion_factor = 111000                              <span class="fm-combinumeral">①</span>
    distance_meters = (distance_degrees * conversion_factor 
                       * np.cos(np.radians(latitude)))      <span class="fm-combinumeral">②</span>
    return distance_meters 
 
distance_degrees = (
    np.abs(
        summary_listings['latitude'] 
        imperial_palace_lat) + 
    np.abs(
        summary_listings['longitude'] 
        imperial_palace_lon)
)                                                           <span class="fm-combinumeral">③</span>
 
summary_listings['imperial_palace_distance'] = (
    degrees_to_meters(distance_degrees,
    summary_listings['latitude']
)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Conversion factor representing the approximate number of meters per degree of latitude</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Calculates the distance in meters by multiplying the degree-based distance by the conversion factor and adjusting for the latitude’s cosine</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Calculates the absolute distance in degrees by subtracting the Imperial Palace’s latitude and longitude from the values in the dataset</p>

  <p class="body">When dealing with coordinates inside a city, the choice between Euclidean distance and Manhattan distance as a feature for machine learning depends on the specific context and the problem you are trying to solve:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Euclidean distance</i> is based on the straight-line distance between two points in an Euclidean space. It assumes a direct path between the points and can be more suitable when considering physical distances. You may also hear it referred to as the <i class="fm-italics">L2 norm</i>, which instead is a mathematical concept referring to the distance between the vector and the origin of the vector space. Since the L2 norm is based on the Euclidean distance formula, it is used interchangeably because it is a closely related mathematical concept.<a id="idIndexMarker040"/><a id="idIndexMarker041"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Manhattan distance</i>, also known as city block distance or taxicab distance, measures the distance between two points by adding up the absolute differences between their coordinates. It considers only horizontal and vertical movements and disregards diagonal paths. Similarly to the Euclidean distance, you may hear the Manhattan distance referred to as the <i class="fm-italics">L1 norm</i> when operating with vectors and vector spaces. <a id="idIndexMarker042"/><a id="idIndexMarker043"/></p>
    </li>
  </ul>

  <p class="body"><a id="marker-256"/>Manhattan distance can be more appropriate when considering the actual movement or navigation within a city, where travel often occurs along streets and road networks. Considering that coordinates are inside a city, where the road network structure and navigation along streets matter, Manhattan distance might be more suitable for capturing the movement and accessibility between locations. It aligns with the concept of following the roads and making right-angle turns.</p>

  <p class="body">After calculating the <code class="fm-code-in-text">imperial_palace_distance</code> feature, we can examine its average value, expressed in meters, using the following code:</p>
  <pre class="programlisting">summary_listings.imperial_palace_distance.mean()</pre>

  <p class="body">The result shows that the average distance to the Imperial Palace is around 7.9 kilometers.</p>

  <p class="body">Next, we can identify the listing that is located nearest to the Imperial Palace. To achieve this, we can use the <code class="fm-code-in-text">idxmin()</code> function to find the index of the listing with the minimum distance and then access its corresponding details:<a id="idIndexMarker044"/></p>
  <pre class="programlisting">(summary_listings[
    ['id', 'name', 'neighbourhood', 'imperial_palace_distance']
].iloc[np.argmin(summary_listings['imperial_palace_distance'])])</pre>

  <p class="body">The result is as follows, which is a bit surprising:</p>
  <pre class="programlisting">id                                                       874407512426725982
name                        Home in Shibuya City · <span class="times">★</span>New · 3 bedrooms · ...
neighbourhood                                                    Chiyoda Ku
imperial_palace_distance                                         137.394271
Name: 10255, dtype: object</pre>

  <p class="body">Indeed, the listing is not situated close to the Imperial Palace, which emphasizes the presence of potentially misleading errors in the geolocation of listings. It is not uncommon to encounter similar problems in datasets, no matter how carefully they are curated. As discussed in chapter 2, from a general point of view, after some data quality checks where you look for consistency among features and marking likely inexact values, you have a few viable options, listed here in descending order of effort from more demanding to less demanding:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Geocoding</i> (from address to coordinate) and <i class="fm-italics">reverse geocoding</i> (from coordinates to address)—To figure out if location information matches with the provided latitude and longitude coordinates and decide whether to trust the provided address or coordinates<a id="idIndexMarker045"/><a id="idIndexMarker046"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Data imputation</i>—Imputing the dubious values as they were missing, using the coordinates of a default location <a id="idIndexMarker047"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Listwise deletion</i>—Removing all rows that have some dubious values<a id="idIndexMarker048"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Leave it to XGBoost</i>—Tree-based methods tend to be less affected by erroneous and dubious values being robust to outliers and noise in the data</p>
    </li>
  </ul>

  <p class="body"><a id="marker-257"/>In our example, we decided to leave the situation to XGBoost because our model is not so critical as to require a thorough data quality check. It could be different with your own project, and you may evaluate a solution requiring more data-cleaning efforts.</p>

  <p class="body">Distances from landmarks and services work well as features in real estate modeling. Hence we do not limit ourselves to computing the distance from the Imperial Palace, the center of Tokyo. In the paper “Modeling User Activity Preference by Leveraging User Spatial Temporal Characteristics in LBSNs” by Dingqi Yang, Daqing Zhang, Vincent W. Zheng, Zhiyong Yu (<i class="fm-italics">IEEE Transactions. on Systems, Man, and Cybernetics: Systems</i>, 45(1), 129-142, 2015), the authors collected datasets from Foursquare check-ins in New York City and Tokyo with their geographical coordinates and the type of location they refer to; we can access the data from Kaggle Datasets (<a class="url" href="https://mng.bz/4aDj">https://mng.bz/4aDj</a>).</p>

  <p class="body">Foursquare is a social network based on geopositioning. Thanks to its mobile app, it allows users to discover nearby venues to visit, such as restaurants and shops, and means of transportation and to share information about the places they visit. Another characteristic of the app is check-ins, which happen when using the platform at a venue. Check-ins refer to the presence of a user at a specific location. When a user checks into a place, they share their positioning with their Foursquare friends, and they may also have the option to post about their visit to other social media platforms like Facebook and X. To map the value of a listing in terms of convenience, we have available airports, bus, train, and subway stations among commonly checked-in venues. Together with convenience stores, a type of retail store that mainly sells a wide selection of everyday items and products to customers for their convenience, the proximity of such venues can add value to an accommodation.</p>

  <p class="body">Hence, to enrich our dataset, we first extracted the GPS coordinates of these Tokyo locations directly from Kaggle. The code and the extracted dataset are available at <a class="url" href="https://mng.bz/QDPv">https://mng.bz/QDPv</a>, and you can download the processed file into your working directory from the page <a class="url" href="https://mng.bz/XxNa">https://mng.bz/XxNa</a> where you can get the file <code class="fm-code-in-text">relevant_spots_Tokyo.csv</code>. The file contains information about 3,560 convenience store locations, 1,878 bus stations and stops, 439 subway stations, and 264 locations associated with airports. Then, using listing 7.6, we can compare the location of our Airbnb Tokyo listings with each of these venues and report the nearest distance of each one. Our idea is that the closer a listing is to a convenience store and means of transportation, the higher the expected price.</p>

  <p class="body">In listing 7.6 we do not compare each accommodation with all the possible venues we have gathered because that would take too much time and computation. Instead, we utilize the k-dimensional tree (KDTree) data structure from Scikit-learn, an optimized algorithm designed to efficiently find the nearest point among many points for a given location. Scikit-learn is used for algorithms such as K-nearest neighbors in situations when you have to find the most similar examples to a test sample in the training set. In our case, the training set is the set of venues, and the algorithm is trained to find the nearest venue to a given location based on Manhat<a id="idTextAnchor018"/>tan distance.<a id="idIndexMarker049"/><a id="marker-258"/></p>

  <p class="fm-code-listing-caption">Listing 7.6 Finding the nearest facilities and transportation</p>
  <pre class="programlisting">from sklearn.neighbors import KDTree
 
relevant_spots = pd.read_csv("relevant_spots_Tokyo.csv")
 
venue_categories = ['Convenience Store', 'Train Station', 
                    'Airport', 'Bus Station', 'Subway']
min_distances = {'listing_id': summary_listings['id']}      <span class="fm-combinumeral">①</span>
 
for venue in venue_categories:
    venue_filter = relevant_spots['venueCategory'] == venue
    venues = relevant_spots[
        ['latitude', 'longitude']
    ][venue_filter]                                         <span class="fm-combinumeral">②</span>
    tree = KDTree(venues, metric='manhattan')               <span class="fm-combinumeral">③</span>
    distance, index = tree.query(
        summary_listings[['latitude', 'longitude']],
        k=1
     )                                                      <span class="fm-combinumeral">④</span>
    min_distances[
        'nearest_' + 
        venue.lower().replace(" ", "_")
    ] = degrees_to_meters(
            np.ravel(distance), 
            summary_listings['latitude']
        )
min_distances = pd.DataFrame(min_distances)                 <span class="fm-combinumeral">⑤</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Stores the minimum distances in a dictionary</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Filters the relevant venue locations</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates a KDTree using the filtered venue locations with the Manhattan metric for fast nearest-neighbor searches</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Queries the KDTree to find the nearest point and its distance to each Airbnb listing (k=1 returns the nearest one)</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> The dictionary of the minimum distances for each type of venue is converted into a DataFrame.</p>

  <p class="body">The code will run quite quickly by iterating over each type of venue, training on the locations of selected venues, and finding, using the KDTree, the nearest location to each accommodation and automatically calculating the distance. Ultimately, we just have to wrap up the results into a pandas DataFrame after converting the distances (Manhattan distances in degrees) to meters by the previously seen function <code class="fm-code-in-text">degrees_to_meters</code>. We can verify the results by inspecting the first five rows of the resulting dataset:<a id="idIndexMarker050"/></p>
  <pre class="programlisting">min_distances.head()</pre>

  <p class="body">Figure 7.5 shows the results, representing the contents from the created <code class="fm-code-in-text">min_distance<a class="calibre" id="idTextAnchor019"/>s</code> DataFrame.<a id="idIndexMarker051"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F05_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.5 The first five rows of the <code class="fm-code-in-text">min_distances</code> DataFrame<a id="idIndexMarker052"/></p>
  </div>

  <p class="body">Having the minimum distance to our selection of venues for each listing of our dataset, we now proceed with putting together all these new features into a final dataset of predictors and extracting a target series or vector to be us<a id="idTextAnchor020"/>ed for modeling.</p>

  <h3 class="fm-head1" id="heading_id_7">7.1.4 Finalizing your data</h3>

  <p class="body"><a id="marker-259"/>After creating some additional features, we can finalize our predictive features and their target. In listing 7.7, we join the <code class="fm-code-in-text">summary_listing</code> dataset to the minimum distance to our selected landmarks (airports, subway, train, bus stations, convenience stores). Then, we rearrange the joined data with respect to our target: the mean price we computed in the <code class="fm-code-in-text">price_stats_<a class="calibre" id="idTextAnchor021"/>ordered</code> dataset.<a id="idIndexMarker053"/><a id="idIndexMarker054"/><a id="idIndexMarker055"/></p>

  <p class="fm-code-listing-caption">Listing 7.7 Assembling data</p>
  <pre class="programlisting">summary_listings_features = [
    'neighbourhood',
    'coordinates',
    'room_type',
    'minimum_nights', 'number_of_reviews', 'days_since_last_review',
    'reviews_per_month', 'calculated_host_listings_count',
    'availability_365', 'number_of_reviews_ltm', 
'number_of_reviews_ltm_ratio',
    'number_of_bedrooms', 'number_of_beds', 'number_of_baths',
    'type_of_accommodation', 'score', 'is_new',
    'is_studio', 'has_shared_bath', 'has_half_bath',
    'imperial_palace_distance'
]
 
summarized = summary_listings[['id'] + 
summary_listings_features].rename({'id': 'listing_id'}, axis=1)
 
X = summarized.merge(min_distances, on='listing_id').set_index('listing_id')
 
X = X.reindex(price_stats.index)                              <span class="fm-combinumeral">①</span>
price_stats_ordered = price_stats.reindex(X.index)            <span class="fm-combinumeral">②</span>
y = price_stats_ordered['mean'].copy()                        <span class="fm-combinumeral">③</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Reindexes X to match the index of price_stats</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Reindexes price_stats to match the index of X, ensuring the reindexed price statistics align with the listings in X</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Copies the “mean” price column as the target variable</p>

  <p class="body">Once we have completed the script, we can visualize our dataset:</p>
  <pre class="programlisting">X.head()</pre>

  <p class="body">As index, we have our <code class="fm-code-in-text">listing_id</code>, and among the columns, there are all the features we have prepared for the problem, as show<a id="idTextAnchor022"/>n in figure 7.6.<a id="idIndexMarker056"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F06_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.6 Top rows of the predictors’ dataset</p>
  </div>

  <p class="body"><a id="marker-260"/>At this point, we can start examining the data in detail and figure out if there are any additional problems to be fixed or other insights to be discovered that could play an essential role in how we develop<a id="idTextAnchor023"/> our XGBoost model.</p>

  <h3 class="fm-head1" id="heading_id_8">7.1.5 Exploring and fixing your data</h3>

  <p class="body">The next step after having assembled all the predictors into a single dataset is to explore it to detect any problems, such as missing data or extreme values, that may affect the performance of a machine learning algorithm. Therefore, our first action is to check for any missing data with the following command:<a id="idIndexMarker057"/><a id="idIndexMarker058"/></p>
  <pre class="programlisting">X.isna().sum()</pre>

  <p class="body">The resulting list points out that there are three features with some missing data:</p>
  <pre class="programlisting">days_since_last_review            1252
reviews_per_month                 1252
score                             2381</pre>

  <p class="body">As we discussed in chapter 2 and then again in the previous chapter, it is crucial in the presence of missing data to investigate why there are missing values and if that could be deemed a missing completely at random, missing at random, or missing not at random situation. In this specific case, missing values are not at all at random, but they depend on the fact that there are no reviews or there are not enough reviews to compute a score. In fact, by inspecting how many accommodations are there without reviews, you will notice how the figure matches the number of missing cases on two features with missing values:</p>
  <pre class="programlisting">(X.number_of_reviews==0).sum()</pre>

  <p class="body">As anticipated, the result is 1,252, matching the number of missing values. In this case, it is better to refrain from using the capabilities of XGBoost and other GBDT implementations to deal with missing data because its behavior will mimic an average situation. Missing reviews is an extreme yet legitimate case when an accommodation has just entered the market or is seldom chosen. Here, you need to directly input a number that may help any machine learning algorithm figure out that there are no reviews, hence the missing values. A common strategy is to use a value at the boundaries of the existing distribution, usually a negative number if we are representing counts or positive values. A quick check can assure us if there are the prerequisites for such a missing values strategy:</p>
  <pre class="programlisting">X[["days_since_last_review", "reviews_per_month", "score"]].describe()</pre>

  <p class="body"><a id="marker-261"/>As a result, we confirmed that the minimum value is always greater than zero for all three considered features. It means we can simply replace the mis<a id="idTextAnchor024"/>sing values using the –1 value (we cannot use zero because the minimum value is zero for <code class="fm-code-in-text">days_since_last_review</code>), which will work as a solution both for a linear model (it is at the lower extreme of the existing distributions) and for tree-based ensembles (they will just split on that negative figure):</p>
  <pre class="programlisting">X.fillna(-1, inplace=True)</pre>

  <p class="body">As a next step, we will be on the lookout for extreme values among our numeric features. As discussed in chapter 2, a straightforward approach to looking for outliers and extreme values is to chart a boxplot for each numeric feature arranged in a panel of subplots or a single plot if they have comparable scales. In our case, in the following listing, we have prepared a panel of boxplots to be inspecte<a id="idTextAnchor025"/>d for extreme values.</p>

  <p class="fm-code-listing-caption">Listing 7.8 Plotting boxplots for numeric features</p>
  <pre class="programlisting">import matplotlib.pyplot as plt
 
numeric = ['minimum_nights', 'number_of_reviews', 
           'days_since_last_review', 'reviews_per_month',
           'calculated_host_listings_count', 
           'availability_365', 'score', 
           'number_of_reviews_ltm', 
           'number_of_reviews_ltm_ratio', 
           'number_of_bedrooms', 'number_of_beds', 
           'number_of_baths', 'imperial_palace_distance', 
           'nearest_convenience_store',
           'nearest_train_station', 'nearest_airport', 
           'nearest_bus_station', 'nearest_subway']
 
num_plots = len(numeric)
num_rows = (num_plots + 2) // 3                          <span class="fm-combinumeral">①</span>
num_cols = min(num_plots, 3)                             <span class="fm-combinumeral">②</span>
 
fig, axes = plt.subplots(
    num_rows,
    num_cols,
    figsize=(8, 12)
)                                                        <span class="fm-combinumeral">③</span>
axes = axes.flatten()                                    <span class="fm-combinumeral">④</span>
 
for i, feat in enumerate(numeric):
    X[[feat]].boxplot(ax=axes[i])
 
fig.tight_layout()
plt.show()</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Estimates the number of rows needed to arrange subplots</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Calculates the number of columns needed to arrange subplots</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates a figure with subplots</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Flattens the axes array to a 1D array so that it can be iterated through</p>

  <p class="body"><a id="marker-262"/>Figure 7.7 shows the charted results. By inspecting the values outside the whiskers of the plots, represented as empty points, we immediately notice that almost all the distributions have heavy tails on the right, with values decisively much larger than the mean value. If, for distance-based features, such extreme values may sound reasonable because of the extension of Tokyo’s metropolitan area, as for features such as <code class="fm-code-in-text">minimum_night</code> and <code class="fm-code-in-text">number_of_reviews</code>, such extreme values may represent outliers quite far from the core of the distribution. We could use winsorizing to solve such a problem, using a solution we proposed in chapter 2. This data transformation technique replaces extreme values in a dataset with less extreme values to reduce the influence of outliers on statistical anal<a id="idTextAnchor026"/>yses and modeling.</p>

  <p class="body">In listing 7.9, using the <code class="fm-code-in-text">winsorize</code> function from the Scipy package, we winsorize the 0.1% of the upper part of the distribution of the <code class="fm-code-in-text">minimum_nights</code> feature. All values above the 0.999 percentile will be changed to the value of the 0.999 percentile, thus removing<a id="idTextAnchor027"/> any extreme values.<a id="idIndexMarker059"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F07_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.7 Panel of boxplots illustrating the skewed distributions of most numeric features<a id="marker-263"/></p>
  </div>

  <p class="fm-code-listing-caption">Listing 7.9 Winsorizing extreme values</p>
  <pre class="programlisting">from scipy.stats.mstats import winsorize
 
lower_cut_percentile = 0.00                           <span class="fm-combinumeral">①</span>
upper_cut_percentile = 0.001                          <span class="fm-combinumeral">②</span>
 
X['minimum_nights'] = winsorize(X['minimum_nights'].values, 
                                limits=(lower_cut_percentile, 
upper_cut_percentile))
 
X[['minimum_nights']].boxplot()</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Indicates the lower percentile below which values will not be changed during winsorization</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Indicates the upper percentile above which values will not be changed during winsorization</p>

  <p class="body"><a id="marker-264"/>Figure 7.8 shows the highest value is now 120, not ove<a id="idTextAnchor028"/>r 1,000 as before.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F08_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.8 Boxplot of winsorized <code class="fm-code-in-text">minimum_nights</code> feature</p>
  </div>

  <p class="body">We replicate the same also for the <code class="fm-code-in-text">number_of_reviews</code> feature:</p>
  <pre class="programlisting">X['number_of_reviews'] = winsorize(X['number_of_reviews'].values, 
                                limits=(lower_cut_percentile, 
upper_cut_percentile))
 
X[['number_of_reviews']].boxplot()</pre>

  <p class="body">Figure 7.9 shows that now the feature still has a heavy right tail. However, the extreme values have been compressed to bel<a id="idTextAnchor029"/>ow the 500 value.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F09_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.9 Boxplot of winsorized <code class="fm-code-in-text">number_of_reviews</code> feature</p>
  </div>

  <p class="body">Having completed checking and remediating for missing values and extreme values in our dataset of predictors, we can proceed in the next subsection to look at<a id="idTextAnchor030"/> the target itself.</p>

  <h3 class="fm-head1" id="heading_id_9">7.1.6 Exploring your target</h3>

  <p class="body">When dealing with exploratory data analysis (EDA), it is critical to examine the predictors and the target, and sometimes both predictors and target together and how they relate to each other. For our example, being a regression problem, we simply start by figuring out the mean and the range of the target:<a id="idIndexMarker060"/><a id="marker-265"/><a id="idIndexMarker061"/><a id="idIndexMarker062"/></p>
  <pre class="programlisting">print(f»minimum: {y.min()}»)
print(f"average: {y.mean().round(2)}")
print(f"maximum: {y.max()}")</pre>

  <p class="body">These commands will print the minimum, the average, and the maximum values in the target variable y:</p>
  <pre class="programlisting">minimum: 1450.0
average: 36573.1
maximum: 1306500.0</pre>

  <p class="body">We immediately notice that the maximum is on a quite different scale than the average and the minimum. Estimating percentiles can be helpful to understand better if there is a problem with extreme values or skewed distribution in the target. The presence of extreme values can be better understood by requiring a range of percentiles focusing on the extremities of the distribution:</p>
  <pre class="programlisting">perc = [1, 5, 10, 25, 50, 75, 90, 95, 99]
for p in perc:
    print(f"percentile {p:2}: {np.percentile(y, p).round(2)}")</pre>

  <p class="body">The following are the output percentiles, and we have confirmation of the presence of extreme values at the right of the distribution since even the 99th percentile is quite far from the maximum we had previously reported:</p>
  <pre class="programlisting">percentile  1: 3000.0
percentile  5: 5198.02
percentile 10: 7315.67
percentile 25: 11870.07
percentile 50: 19830.78
percentile 75: 37741.64
percentile 90: 83936.03
percentile 95: 84857.11
percentile 99: 304531.4</pre>

  <p class="body"><a id="marker-266"/>Not only is the maximum value quite far from the 99th percentile, but there also appears to be a significant gap between the 95th and 99th percentiles. Our decision is to focus on the core of the distribution by dropping 10% of the distribution: 5% on the lower and 5% on the upper part. We manage this by selection using a Boolean selection variable:</p>
  <pre class="programlisting">valid_samples = (y &gt;= 5200) &amp; (y &lt;=84857)</pre>

  <p class="body">Before definitely applying the selection, we plot the r<a id="idTextAnchor031"/>esulting distribution.</p>

  <p class="fm-code-listing-caption">Listing 7.10 Plotting the target distribution</p>
  <pre class="programlisting">import matplotlib.pyplot as plt
import seaborn as sns
 
valid_y = y[valid_samples]                                    <span class="fm-combinumeral">①</span>
sns.kdeplot(valid_y, fill=True)
 
median = np.median(valid_y)                                   <span class="fm-combinumeral">②</span>
plt.axvline(median, color='r', linestyle='--', linewidth=2, label='Median')
 
plt.xlabel('Values')
plt.ylabel('Density')
plt.title('Distribution Curve with Median')
plt.legend()
plt.show()</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Selects only the part of the target distribution we consider to model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Represents the median value of the distribution</p>

  <p class="body">The previous code snippet will output a density plot, revealing the distribution and the median value for the selection of the target based on the selection variable we have just defined. Figure 7.10 shows <a id="idTextAnchor032"/>the resulting plot.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F10_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.10 Density distribution of the target variable</p>
  </div>

  <p class="body">The resulting distribution shown in figure 7.10 is definitely skewed to the right, a condition also called a positive skew. In particular, you can observe a lump of data at the start and a long decreasing tail following to the right, although, by the end, we can notice another small lump closing the distribution, probably a separate cluster of high-end accommodations. However, the range and distribution of the target are fine now. Therefore, we will select both the target and data using the previously defined Boolean selection variable:</p>
  <pre class="programlisting">X = X[valid_samples]
y = y[valid_samples]</pre>

  <p class="body">In the next section, we will proceed to define both aspects of the validation process and the data pipeline necessary for modeling. Afterward, we will try a baseline model using classical machine learning linear models and a first tentative XGBoost model and optimize it before training our definitive model for the Tokyo Ai<a id="idTextAnchor033"/>rbnb dataset problem.<a id="marker-267"/></p>

  <h2 class="fm-head" id="heading_id_10">7.2 Building and optimizing your model</h2>

  <p class="body">In this section, we will use the data we have prepared to build a model. Before getting a complete final model, we will address various challenges related to defining the cross-validation strategy, preparing the data pipeline, and building first a baseline model and then a tentative<a id="idTextAnchor034"/> first XGBoost model.<a id="idIndexMarker063"/><a id="idIndexMarker064"/></p>

  <h3 class="fm-head1" id="heading_id_11">7.2.1 Preparing a cross-validation strategy</h3>

  <p class="body">Generally, a K-fold cross-validation strategy works quite well in most cases, but in our specific situation, we are dealing with real estate units whose value is strongly influenced by their location. In our case, Stratified K-fold cross-validation is more appropriate, controlling for the effect of location. Although similar to K-fold cross-validation, there is a key difference in stratified K-fold cross-validation: the class distribution of a feature of our choice in the dataset is preserved in each fold. Such stratified sampling will allow folds to have a similar mix of territories as the complete dataset. However, it is important to check beforehand if some territories are difficult to split among folds because of their low numerosity. If we count the different neighborhoods represented in the data, we get a long list with many locations, some showcasing a considerable number of listings and others only a limited few:<a id="idIndexMarker065"/><a id="idIndexMarker066"/></p>
  <pre class="programlisting">X['neighbourhood'].value_counts()</pre>

  <p class="body">Clearly you cannot accept considering all the neighborhoods having less than a certain number of examples because, if you are going to split the data into folds, you will hardly have them well represented. Since areas are spatially distributed, just gathering them into an extra class won’t do because you will mix very different situations of areas quite far from each other. In listing 7.11, we solve this problem by aggregating areas with less than 30 examples (implying about 6 examples for each fold if we use a five-fold validation split) with their nearest larger neighborhood. To achieve that, we use a KDTree data structure again. Thus we can match each area with less than 30 accommodations with its neares<a id="idTextAnchor035"/>t area with more than 30.<a id="marker-268"/></p>

  <p class="fm-code-listing-caption">Listing 7.11 Aggregating nearby neighborhood areas</p>
  <pre class="programlisting">neighbourhoods = (
    summary_listings[
        ['neighbourhood', 'latitude', 'longitude']
    ]
    .groupby('neighbourhood')
    .agg({'latitude': 'mean', 
          'longitude': 'mean',
          'neighbourhood': 'count'})
)                                                              <span class="fm-combinumeral">①</span>
 
less_than_30 = (
    neighbourhoods[neighbourhoods['neighbourhood'] &lt; 30]
)
more_than_30 = (
    neighbourhoods[neighbourhoods['neighbourhood'] &gt; 30]
)                                                              <span class="fm-combinumeral">②</span>
 
kdtree = KDTree(
    more_than_30[['latitude', 'longitude']]
)                                                              <span class="fm-combinumeral">③</span>
change_list = {}                                               <span class="fm-combinumeral">④</span>
 
for i in range(len(less_than_30)):
    row = less_than_30.iloc[[i]]
    _, idx = kdtree.query(
        row[['latitude', 'longitude']]
    )                                                          <span class="fm-combinumeral">⑤</span>
    change_list[row.index[0]] = more_than_30.index[idx[0, 0]]
 
X["neighbourhood_more_than_30"] = (
    X["neighbourhood"].replace(change_list)
)                                                              <span class="fm-combinumeral">⑥</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Calculates the mean latitude, mean longitude, and the count of listings in each neighborhood</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Separates the neighborhoods into two groups based on the number of listings</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates a KDTree using the mean latitude and longitude values of neighborhoods with counts greater than 30</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Initializes an empty dictionary to store the mappings of the neighborhoods</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Iterates through each neighborhood with counts less than 30 and queries the KDTree to find the nearest neighborhood with a count greater than 30</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Replaces the original neighborhood values with the new neighborhood values based on the mapping in change_list</p>

  <p class="body">After having run the code, you can check how the mapping has been performed and if the resulting aggregation has neighborhood areas with less than 30 listings by issuing the following commands:</p>
  <pre class="programlisting">print(change_list)
print(X["neighbourhood_more_than_30"].value_counts())</pre>

  <p class="body">Having elaborated a suitable area subdivision, we can now proceed to define our stratified cross-validation strategy <a id="idTextAnchor036"/>in the following listing.<a id="marker-269"/></p>

  <p class="fm-code-listing-caption">Listing 7.12 Defining a stratified strategy</p>
  <pre class="programlisting">from sklearn.model_selection import StratifiedKFold
 
cv = StratifiedKFold(5, shuffle=True, random_state=0)          <span class="fm-combinumeral">①</span>
cv_splits = cv.split(
    X, y=X["neighbourhood_more_than_30"]
)                                                              <span class="fm-combinumeral">②</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines a five-fold stratified random splitting</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Generates the cross-validation splits while maintaining the same distribution of neighborhoods with more than 30 listings in each fold</p>

  <p class="body">The resulting <code class="fm-code-in-text">cv_splits</code> is a generator, and you can examine it using the following command:</p>
  <pre class="programlisting">print(cv_splits)</pre>

  <p class="body">The output is the type of object:</p>
  <pre class="programlisting">&lt;generator object _BaseKFold.split at 0x78356223c660&gt;</pre>

  <p class="body">Since <code class="fm-code-in-text">cv_splits</code> is a generator, it can be used a single time, but you can reinstantiate an identical one by simply re-executing the commands in listing 7.12. In the next subsection, we will deal with the data pipeline and determine which transformati<a id="idTextAnchor037"/>ons to apply to our data.</p>

  <h3 class="fm-head1" id="heading_id_12">7.2.2 Preparing your pipeline</h3>

  <p class="body">A second preparatory step is to define a pipeline for transforming our predictors in the most appropriate way for running generally with all classical machine learning algorithms, not just gradient boosting. Ideally, it would be better to have multiple pipelines based on how each model deals with the different features. For instance, in our pipeline, we are going to ordinally encode a couple of categorical features, and such encoding, though fit for tree-based models, doesn’t always work properly with linear models. However, while having unique pipelines can lead to better performance, it can also become a maintenance nightmare. The human effort required to create and manage multiple pipelines may outweigh the marginal performance gains achieved by customizing each pipeline for specific models. Therefore it is better to decide on multiple pipelines if you have evidence that it is worth it.<a id="idIndexMarker067"/></p>

  <p class="body">Let’s start by classifying the different kinds of features that we will be using into categorical features, numeric, and binary features:<a id="marker-270"/></p>
  <pre class="programlisting">categorical = [
    'room_type',
    'neighbourhood_more_than_30', 
    'type_of_accommodation',
    'coordinates'
]
numeric = [
    'minimum_nights',
    'number_of_reviews', 
    'days_since_last_review',
    'reviews_per_month',
    'calculated_host_listings_count', 
    'availability_365',
    'score', 
    'number_of_reviews_ltm', 
    'number_of_reviews_ltm_ratio', 
    'number_of_bedrooms', 
    'number_of_beds',
    'number_of_baths', 
    'imperial_palace_distance', 
    'nearest_convenience_store',
    'nearest_train_station',
    'nearest_airport', 
    'nearest_bus_station',
    'nearest_subway'
]
binary = [
    'is_new',
    'is_studio',
    'has_shared_bath',
    'has_half_bath'
]</pre>

  <p class="body">A further inspection of the categorical features is necessary because we need to understand whether to treat them as high cardinality features or not. We can understand better what to do after having counted how many unique values each categorical feature has:</p>
  <pre class="programlisting">for feat in categorical:
    print(f"{feat} has {X[feat].nunique()} unique values")</pre>

  <p class="body">From the results, we can determine that probably the only feature that could be considered as a high cardinality categorical is the <code class="fm-code-in-text">coordinates</code> feature, which has almost 300 unique values. As for <code class="fm-code-in-text">neighbourhood_more_than_30</code> and <code class="fm-code-in-text">type_of_accomodation</code>, we could apply ordinal encoding to them for tree-based modeling, whereas for linear models, it would be better to apply one-hot-encoding to these features (thus producing about 50 new binary features) or target encoded: <a id="idIndexMarker068"/><a id="marker-271"/></p>
  <pre class="programlisting">room_type has 4 unique values
neighbourhood_more_than_30 has 24 unique values
type_of_accommodation has 29 unique values
coordinates has 296 unique values</pre>

  <p class="body">Since our example revolves around XGBoost and demonstrating how it can work with similar problems, we decide to one-hot encode only the <code class="fm-code-in-text">room_type</code>, ordinal encoding <code class="fm-code-in-text">neighbourhood_more_than_30</code> and <code class="fm-code-in-text">type_of_accomodation</code>, and target encoding <code class="fm-code-in-text">coordinates</code>:</p>
  <pre class="programlisting">onehot_encoding = ['room_type']
ordinal_encoding = ['neighbourhood_more_than_30', 'type_of_accommodation']
target_encoding = ['coordinates']</pre>

  <p class="body">Our choice of working with XGBoost, a tree-based model, also justifies leaving all the numeric features as-is. Using linear models, statistical standardization, for better convergence when using regularization or generalized linear models, and feature transformation, for better fitting nonlinearities, are usually the standard.</p>

  <p class="body">In listing 7.13, we define all the necessary feature transformations and ensemble them in a Scikit-learn’s Column Transformer, which will be part of the pipeline that also contains the machine learning model of our choice. It is also important to note that we take steps in defining the column transformers to handle unknown categories and missing values that may unexpectedly appear at test time. Our strategy for one-hot encoding is to ignore new unknown categories. For ordinal encoding, the value assigned to the parameter <code class="fm-code-in-text">unknown_value</code>, which is by default <code class="fm-code-in-text">np.nan,</code> will be used to encode unknown categories. This means an XGBoost model will deal with such situations as missing cases using the most frequent split. Other machine learning algorithms may instead break into similar occurrences, which is an advantage of XGBoost. As for the target encoder, the target mean is substituted for unknown categories. Don’t forget to install the <code class="fm-code-in-text">category_encoders</code> package. If it is unavailable on your system, use the <code class="fm-code-in-text">pip install</code><a id="idTextAnchor038"/> <code class="fm-code-in-text">category_encoders</code> command.<a id="idIndexMarker069"/><a id="idIndexMarker070"/><a id="idIndexMarker071"/></p>

  <p class="fm-code-listing-caption">Listing 7.13 Defining column transformations</p>
  <pre class="programlisting">from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
 
from category_encoders.target_encoder import TargetEncoder
 
onehot_encoder = OneHotEncoder(handle_unknown='ignore')          <span class="fm-combinumeral">①</span>
ordinal_enconder = OrdinalEncoder(handle_unknown="use_encoded_value",
                                  unknown_value=np.nan)          <span class="fm-combinumeral">②</span>
target_encoder = TargetEncoder(
    cols=target_encoding, 
    handle_unknown="value", 
    smoothing=0.5
)                                                                <span class="fm-combinumeral">③</span>
 
column_transform = ColumnTransformer(
    [('onehot_encoding', onehot_encoder, onehot_encoding),
     ('ordinal_encoding', ordinal_enconder, ordinal_encoding),
     ('target_encoding', target_encoder, target_encoding),
     ('numeric', 'passthrough', numeric),
     ('binary', 'passthrough', binary)],                         <span class="fm-combinumeral">④</span>
    remainder='drop',                                            <span class="fm-combinumeral">⑤</span>
    verbose_feature_names_out=True,                              <span class="fm-combinumeral">⑥</span>
    sparse_threshold=0.0)                                        <span class="fm-combinumeral">⑦</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a One-Hot Encoder object with the option to handle unknown categories by ignoring them during encoding</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Creates an Ordinal Encoder object with handling of unknown categories and the unknown value replaced by np.nan</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates a TargetEncoder object, handling unknown values by encoding them using the mean target value and applying smoothing with a parameter of 0.5</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> A Column Transformer object applying the specified encoders to the respective columns</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Drops remaining columns that are not specified in the transformer</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Keeps verbose feature names for transformed columns</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Ensures that the transformed data is kept as dense arrays</p>

  <p class="body"><a id="marker-272"/>After having run the code listing, we can immediately test transforming the data we have and checking the transformed column names:</p>
  <pre class="programlisting">Xt = column_transform.fit_transform(X, y)
column_transform.get_feature_names_out()</pre>

  <p class="body">The output shows that now the features are preceded by a prefix pointing out what transformation they underwent. Binary features created by one-hot encoding are also followed by the category they represent:</p>
  <pre class="programlisting">array(['onehot_encoding__room_type_Entire home/apt',
       'onehot_encoding__room_type_Hotel room',
       'onehot_encoding__room_type_Private room',
       'onehot_encoding__room_type_Shared room',
       'ordinal_encoding__neighbourhood_more_than_30',
       'ordinal_encoding__type_of_accommodation',
       'target_encoding__coordinates', 'numeric__minimum_nights',
       'numeric__number_of_reviews', 'numeric__days_since_last_review',
       'numeric__reviews_per_month',
       'numeric__calculated_host_listings_count',
       'numeric__availability_365', 'numeric__score',
       'numeric__number_of_reviews_ltm',
       'numeric__number_of_reviews_ltm_ratio',
       'numeric__number_of_bedrooms', 'numeric__number_of_beds',
       'numeric__number_of_baths', 'numeric__imperial_palace_distance',
       'numeric__nearest_convenience_store',
       'numeric__nearest_train_station', 'numeric__nearest_airport',
       'numeric__nearest_bus_station', 'numeric__nearest_subway',
       'binary__is_new', 'binary__is_studio', 'binary__has_shared_bath',
       'binary__has_half_bath'], dtype=object)</pre>

  <p class="body">As a final step, we just store away into a single CSV file both the processed features and the target. We will use such data again later, in chapter 12, when we test a deep learning solution and compare its performance with the XGBoost model we train in this chapter.</p>
  <pre class="programlisting">data = pd.DataFrame(
    Xt, 
    columns=column_transform.get_feature_names_out(),
    index=y.index
)
data = data.assign(target=y).reset_index()
data.to_csv("airbnb_tokyo.csv", index=False)</pre>

  <p class="body">Now that we have the data processing part of our pipeline, we can proceed to define a baseline model and then, f<a id="idTextAnchor039"/>inally, an XGBoost regressor.</p>

  <h3 class="fm-head1" id="heading_id_13">7.2.3 Building a baseline model</h3>

  <p class="body">Having a baseline model in machine learning is important for several reasons:<a id="marker-273"/><a id="idIndexMarker072"/><a id="idIndexMarker073"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Comparing performance</i>—The baseline model serves as a benchmark to compare the performance of more complex models, helping you to understand whether more complexity really adds value.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Detecting overfitting</i>—By comparing the performance of your advanced model against the baseline on unseen data, you can identify if the advanced model is overfitting because the baseline model will perform much better.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Understanding the problem</i>—Creating a simple baseline model, especially if it is a linear model, forces you to understand the data and the problem better.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Debugging and validation</i>—A baseline model can help you validate that your data preprocessing pipeline is correct because the effects of the variables on the model won’t be hidden by its complexity.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Providing a minimum viable model</i>—A baseline model provides a minimum viable solution to the problem at hand.</p>
    </li>
  </ul>

  <p class="body">For all these reasons, we won’t immediately jump into training our model using a gradient boosting model, which we expect will perform well on the problem, but we take a step back and test a simple linear model. In addition, at this time, we will try to get predictions from a type of model we can easily evaluate and compare. Instead of just evaluating metrics through cross-validation, we’ll employ cross-validation prediction. This method provides unbiased predictions for all training cases by making predictions on validation folds within cross-validation.</p>

  <p class="body">During cross-validation, evaluation metrics are calculated separately for each fold. These metrics represent the performance of the model on each fold individually. The final evaluation metric reported from cross-validation is usually an average (mean or median) of the individual fold metrics. This aggregated metric provides an estimate of the model’s generalization performance on unseen data. However, if we are using cross-validation predictions, we concentrate on the ability of the model to perform on the data at hand. In fact, the primary use of cross-validation predictions is to analyze the predictions made by the model on different parts of the data used as validation. Using such predictions helps us understand how well the model performs across different subsets of the data and identifies if the model is overfitting or underfitting because we can compare the predictions <a id="idTextAnchor040"/>with the expected target values.<a id="marker-274"/></p>

  <p class="fm-code-listing-caption">Listing 7.14 Linear regression baseline model with diagnostic plots</p>
  <pre class="programlisting">import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
 
lm = LinearRegression(fit_intercept=False)                  <span class="fm-combinumeral">①</span>
lm.fit(Xt, y)                                               <span class="fm-combinumeral">②</span>
 
cv_splits = cv.split(
    X,
    y = X["neighbourhood_more_than_30"]
)                                                           <span class="fm-combinumeral">③</span>
y_pred_cv = cross_val_predict(
    lm, Xt, y, cv=cv_splits
)                                                           <span class="fm-combinumeral">④</span>
prediction_range = y_pred_cv.min()} - {y_pred_cv.max()
print(f"prediction range: {prediction_range}")              <span class="fm-combinumeral">⑤</span>
 
r2 = r2_score(y, y_pred_cv)
rmse = np.sqrt(mean_squared_error(y, y_pred_cv))
mae = mean_absolute_error(y, y_pred_cv)                     <span class="fm-combinumeral">⑥</span>
 
print(f'R-squared: {r2:.3f}')
print(f'RMSE: {rmse:.3f}')
print(f'MAE: {mae:.3f}')
 
plt.scatter(y, y_pred_cv)                                   <span class="fm-combinumeral">⑦</span>
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='Ideal Fit')
plt.axhline(
   0, color='orange', linestyle='--', label='Zero Line'
)                                                           <span class="fm-combinumeral">⑧</span>
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('Linear Regression - Fitted Results')
plt.legend()
plt.show()</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Initializes a LinearRegression model without intercept</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Fits the LinearRegression model to the transformed training data</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Creates stratified cross-validation splits based on the neighborhoods with more than 30 counts</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Performs cross-validated predictions</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Prints the range of cross-validated predictions</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Calculates R-squared, root mean squared error, and mean absolute error evaluation metrics to assess the model’s performance</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Creates a scatter plot of actual vs. predicted values</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Plots a dashed orange zero line to the plot as a reference for the ideal fit</p>

  <p class="body">After running the code, we obtain the evaluation results, and we can immediately notice how some predictions are negative. Since a linear regression model is not bounded in its predictions, the mean absolute error (MAE) is quite high (over 12,000 Yen), and the R squared, a typical measure of fit measuring how much of the variance of the target is captured by the model, is just a modest 0.32:<a id="idIndexMarker074"/></p>
  <pre class="programlisting">prediction range: -34929.50241836217 - 136479.60736257263
R-squared: 0.320
RMSE: 17197.323
MAE: 12568.371</pre>

  <p class="body">Clearly, the fitting of the model is not particularly impressive, and we can get a confirmation as shown in figure 7.11, where we represent the scatterplot of the cross-validation predictions on the y-axis against the expected target values on the x-axis. Apart from a few negative predictions at the start of the target distribution, we can also notice how the predictions depart from the ideal fit dashed line, showing a flat trend, a clear sign of underfitting, and how there a<a id="idTextAnchor041"/>re a few outlying predictions<a id="marker-275"/>.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F11_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.11 Plot of the fitted results from the baseline linear regression against their ideal value</p>
  </div>

  <p class="body">As a first step in examining the results, we ask for the percentage of predictions that are below or equal to zero, an unfeasible prediction because listings should be positive:</p>
  <pre class="programlisting">(y_pred_cv &lt;= 0).sum() / len(y_pred_cv)</pre>

  <p class="body">The result is a minimal percentage, about 0.5%:</p>
  <pre class="programlisting">0.005178767055074196</pre>

  <p class="body">Ideally, our predictions should be greater than zero, and in a linear model that could be achieved by a target transformation—for instance, a logarithmic transformation. However, the role of a baseline model is not to be a perfect model but just a model to highlight challenges in the data and be a helpful comparison for more sophisticated models.</p>

  <p class="body">Now we proceed to locate the rows that are positive outliers:</p>
  <pre class="programlisting">print(np.where(y_pred_cv &gt; 100_000))</pre>

  <p class="body">We received two cases: 5509 and 8307:</p>
  <pre class="programlisting">(array([5509, 8307]),)</pre>

  <p class="body">We also inquiry about the negative outliers:</p>
  <pre class="programlisting">print(np.where(y_pred_cv &lt; -25_000))</pre>

  <p class="body">Here we get a single case, 182:</p>
  <pre class="programlisting">(array([182]),)</pre>

  <p class="body">In listing 7.15, we define a function that can help us check for outliers. For each predictor feature, this function prints the coefficient and the resulting multiplication of the coefficient with the value of the feature for that case, thus making explicit the contributions of<a id="idTextAnchor042"/> each feature to the prediction.<a id="marker-276"/></p>

  <p class="fm-code-listing-caption">Listing 7.15 Inspection of the coefficients</p>
  <pre class="programlisting">def report_case(model, data, feature_names, case_no):
    case_values = data[case_no]                             <span class="fm-combinumeral">①</span>
    coef_values = case_values * model.coef_                 <span class="fm-combinumeral">②</span>
    for feature_name, value, coef_value in zip(
            feature_names, case_values, coef_values):
        print(f"{feature_name:50s}" +
              f"({value:10.2f}) : " +
              f"{coef_value:+0.2f}")                        <span class="fm-combinumeral">③</span>
    print("-" * 80)
    print(" "*66 + f"{np.sum(coef_values):+0.2f}")          <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Extracts the feature values for the specified case number from the data array</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Calculates the coefficient values for each feature by multiplying its value with the corresponding coefficient from the model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Loops and prints through the feature names, their values, and corresponding coefficient values</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Prints the sum of the calculated coefficient values for the case</p>

  <p class="body">Having our inspection function ready, we can start examining case 8307, which represents a case of too positively large outlier in the predictions:</p>
  <pre class="programlisting">report_case(model=lm, 
            data=Xt, 
            feature_names=column_transform.get_feature_names_out(), 
            case_no=8307)</pre>

  <p class="body">The following are the results for case 8307, where it is evident that the extra contribution that made the prediction an outlier is because of the number of bedrooms (hinting that this property is probably a hostel). This high value pushed the final predicted listing upwards:<a id="marker-277"/></p>
  <pre class="programlisting">onehot_encoding__room_type_Entire home/apt         (      1.00) : -8295.89
onehot_encoding__room_type_Hotel room              (      0.00) : -0.00
onehot_encoding__room_type_Private room            (      0.00) : -0.00
onehot_encoding__room_type_Shared room             (      0.00) : -0.00
ordinal_encoding__neighbourhood_more_than_30       (     12.00) : +576.48
ordinal_encoding__type_of_accommodation            (     20.00) : +2377.99
target_encoding__coordinates                       (  29649.71) : +26556.25
numeric__minimum_nights                            (      1.00) : -268.05
numeric__number_of_reviews                         (      0.00) : -0.00
numeric__days_since_last_review                    (     -1.00) : -0.66
numeric__reviews_per_month                         (     -1.00) : -172.50
numeric__calculated_host_listings_count            (     15.00) : +1470.92
numeric__availability_365                          (    354.00) : +16503.11
numeric__score                                     (     -1.00) : +524.08
numeric__number_of_reviews_ltm                     (      0.00) : -0.00
numeric__number_of_reviews_ltm_ratio               (      0.00) : +0.00
<b class="fm-bold">numeric__number_of_bedrooms                        (     18.00) : +64407.67</b>
numeric__number_of_beds                            (     18.00) : +31283.70
numeric__number_of_baths                           (      2.00) : -1787.41
numeric__imperial_palace_distance                  (   2279.80) : -859.33
numeric__nearest_convenience_store                 (    149.84) : +549.07
numeric__nearest_train_station                     (    545.08) : -1043.20
numeric__nearest_airport                           (    389.85) : -137.44
numeric__nearest_bus_station                       (    322.04) : -266.55
numeric__nearest_subway                            (    221.93) : -17.29
binary__is_new                                     (      0.00) : -0.00
binary__is_studio                                  (      0.00) : +0.00
binary__has_shared_bath                            (      0.00) : -0.00
binary__has_half_bath                              (      0.00) : -0.00
----------------------------------------------------------------------------
                                                                  +131400.95</pre>

  <p class="body">Similar problems are due to the fact that each feature is modeled in a linear way. Thus the prediction contribution of a feature is unbounded, having no maximum or minimum but decreasing and increasing in accordance with the feature value. Typically, introducing nonlinearities and interactions into the model nonlinearities and interactions can mitigate such problems. Let’s now check the only negative outlier:</p>
  <pre class="programlisting">report_case(model=lm, 
            data=Xt, 
            feature_names=column_transform.get_feature_names_out(), 
            case_no=182)</pre>

  <p class="body">Here the problem is represented by the value of the minimum of nights, which is again too high and drags down the estimated value. In fact, some listings act as seasonal accommodation, typically for workers or students, not just for short stays. The model is indeed too simple to catch such nuances, and again, having introduced nonlinearities and interactions could have helped:<a id="marker-278"/></p>
  <pre class="programlisting">onehot_encoding__room_type_Entire home/apt         (      0.00) : -0.00
onehot_encoding__room_type_Hotel room              (      0.00) : -0.00
onehot_encoding__room_type_Private room            (      1.00) : -11573.69
onehot_encoding__room_type_Shared room             (      0.00) : -0.00
ordinal_encoding__neighbourhood_more_than_30       (      6.00) : +288.24
ordinal_encoding__type_of_accommodation            (     14.00) : +1664.59
target_encoding__coordinates                       (  27178.66) : +24343.02
<b class="fm-bold">numeric__minimum_nights                            (    120.00) : -32166.38</b>
numeric__number_of_reviews                         (    122.00) : -1241.88
numeric__days_since_last_review                    (    132.00) : +87.20
numeric__reviews_per_month                         (      1.33) : +229.43
numeric__calculated_host_listings_count            (      4.00) : +392.25
numeric__availability_365                          (      0.00) : +0.00
numeric__score                                     (      4.98) : -2609.92
numeric__number_of_reviews_ltm                     (      4.00) : -18.33
numeric__number_of_reviews_ltm_ratio               (      0.03) : +54.17
numeric__number_of_bedrooms                        (      1.00) : +3578.20
numeric__number_of_beds                            (      0.00) : +0.00
numeric__number_of_baths                           (      0.00) : -0.00
numeric__imperial_palace_distance                  (  32506.70) : -12252.79
numeric__nearest_convenience_store                 (   5020.81) : +18397.51
numeric__nearest_train_station                     (   5689.32) : -10888.48
numeric__nearest_airport                           (  11438.81) : -4032.76
numeric__nearest_bus_station                       (   4999.17) : -4137.76
numeric__nearest_subway                            (  16976.69) : -1322.52
binary__is_new                                     (      0.00) : -0.00
binary__is_studio                                  (      0.00) : +0.00
binary__has_shared_bath                            (      0.00) : -0.00
binary__has_half_bath                              (      0.00) : -0.00
----------------------------------------------------------------------------
                                                                  -31209.90</pre>

  <p class="body">In conclusion, our baseline model has signaled us that successfully solving the problem presented by the Tokyo Airbnb dataset requires a better fitting mode that can handle positive predictions (they should be necessarily positive) and can represent nonlinear relationships and interactions between a particular characteristic of the accommodation (a large number of bedrooms indicate a hostel, a high number of minimum stay nights indicates an accommodation for seasonal tenants). In the next subsection, we will solve all these problems at once by using an XGBoost model, which should be able to deal with this data in a m<a id="idTextAnchor043"/>ore sophisticated and smart way.</p>

  <h3 class="fm-head1" id="heading_id_14">7.2.4 Building a first tentative model</h3>

  <p class="body">First, we chose an XGBoost regressor, trying to incorporate some of the insights we gained from our previous EDA and baseline model inspections. We decided to use a gamma objective function, commonly used in regression problems, for modeling positive continuous variables that are positive and right-skewed. Gamma is particularly useful when the target variable is always positive, and it includes many small values and a few larger values, as it handles such distribution characteristics quite well.<a id="idIndexMarker075"/></p>

  <p class="body">In addition, since our baseline model has shown signs of underfitting and not handling interactions or linearities properly, we decide on a max depth of at most six for the decision trees composing the boosted ensemble, thus allowing for an adequate number of splits to handle most common similar data characteristics.</p>

  <p class="body">In the following listing, arranged similarly as the previous listing training the linear regression baseline, we train an XGBoost regressor, and we test its out-of-f<a id="idTextAnchor044"/>old cross-validation predictions.<a id="marker-279"/></p>

  <p class="fm-code-listing-caption">Listing 7.16 First XGBoost model</p>
  <pre class="programlisting">from sklearn.model_selection import cross_validate
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from XGBoost import XGBRegressor
 
xgb = XGBRegressor(booster='gbtree',                              <span class="fm-combinumeral">①</span>
                   objective='reg:gamma',                         <span class="fm-combinumeral">②</span>
                   n_estimators=300,
                   max_depth=6)
 
cv_splits = cv.split(
    X, y=X["neighbourhood_more_than_30"]
)                                                                 <span class="fm-combinumeral">③</span>
y_pred_cv = cross_val_predict(
    xgb, Xt, y, cv=cv_splits
)                                                                 <span class="fm-combinumeral">④</span>
prediction_range = y_pred_cv.min()} - {y_pred_cv.max()
print(f"prediction range: {prediction_range}")                    <span class="fm-combinumeral">⑤</span>
 
r2 = r2_score(y, y_pred_cv)
rmse = np.sqrt(mean_squared_error(y, y_pred_cv))
mae = mean_absolute_error(y, y_pred_cv)                           <span class="fm-combinumeral">⑥</span>
 
print(f'R-squared: {r2:.3f}')
print(f'RMSE: {rmse:.3f}')
print(f'MAE: {mae:.3f}')
 
plt.scatter(y, y_pred_cv)                                         <span class="fm-combinumeral">⑦</span>
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='Ideal Fit')
plt.axhline(0, color='orange', linestyle='--', label='Zero Line') <span class="fm-combinumeral">⑧</span>
plt.xlabel('Actual')
plt.ylabel('Predicted')
plt.title('XGBoost - Fitted Results')
plt.legend()
plt.show()</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Sets up an XGBoost regressor with specific hyperparameters</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Defines ‘reg:gamma’ as the objective function</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Generates cross-validation splits based on the neighbourhood_more_than_30 feature</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Performs cross-validated predictions</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Prints the range of predicted values</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Calculates R-squared, root mean squared error, and MAE evaluation metrics to assess the model’s performance</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Creates a scatter plot of actual vs. predicted values</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Adds reference lines to the plot for an ideal fit and a zero line</p>

  <p class="body">This time, the prediction range is strictly in the positive range, as we expected. The MAE is almost half of those of the baseline linear model and the R-squared scores almost 0.7, which is a reasonably good result showing how the model is now able to intercept most of the variance present in the target:</p>
  <pre class="programlisting">prediction range: 3291.401123046875 - 123069.8828125
R-squared: 0.693
RMSE: 11562.836
MAE: 7227.435</pre>

  <p class="body">A further inspection of the fitted results, represented in figure 7.12 as a scatterplot between the cross-validation predictions (on the y-axis) and the expected target values (on the x-axis), shows that the points are now slightly more in line with our ideal fit. In addition, it is important to notice how the XGBoost model tends to extrapolate the predictions: the column of predictions at the rightmost part of the chart indicates that our model predicted values sometimes higher than the observed maximum in the target, whereas, on the leftmost part of the chart, there a<a id="idTextAnchor045"/><a id="marker-280"/>re no nonpositive estimations.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F12_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.12 Plot of the fitted results for the XGBoost model against their ideal value</p>
  </div>

  <p class="body">Typically, you shouldn’t limit yourself to a single model in a data science project, as in our example. Because of space constraints, we just focus on an XGBoost model. Still, it is advisable in a working project to try even more diverse classical machine learning algorithms, such as other gradient boosting implementations, as well as more tree ensembles, generalized linear models, and even more unusual, nowadays, classical machine learning models (such as k-nearest neighbors or support vector machines). There is no free lunch in machine learning, and you may find reasonable solutions to this problem even with different algorithms that may better suit your necessities in terms of performance, speed of inference, memory occupancy, and portability onto other systems.</p>

  <p class="body">In the following subsection, we optimize our XGBoost solution using Bayesian optimization to strive to perform as <a id="idTextAnchor046"/>best as possible on our problem.</p>

  <h3 class="fm-head1" id="heading_id_15">7.2.5 Optimizing your model</h3>

  <p class="body">Since XGBoost works quite well for the problem, we’ll take some time to refine its parameters and test different boosting approaches and objectives. We are going to use Optuna, a Bayesian optimizer presented in the previous chapter, because it can efficiently explore a GBDT hyperparameter search space, adaptively choosing in a short set of rounds, based on the outcomes of the previous experiments, the next set of hyperparameters to be evaluated.<a id="idIndexMarker076"/><a id="marker-281"/><a id="idIndexMarker077"/></p>

  <p class="body">If you don’t have Optuna available on your system, you can install it by issuing the command <code class="fm-code-in-text">pip</code> <code class="fm-code-in-text">install</code> <code class="fm-code-in-text">optuna</code> in a shell or a notebook cell.<a id="idIndexMarker078"/></p>

  <p class="body">Listing 7.17 performs hyperparameter optimization using Optuna for our previously tested XGBoost Regressor model to find the best hyperparameters that minimize the MAE of the model on the Tokyo Airbnb dataset. The core of the listing is the objective function that suggests to Optuna different hyperparameter values using t<code class="fm-code-in-text">rial.suggest_...</code> methods. In particular, it tests the classic <code class="fm-code-in-text">gbtree</code> booster (gradient boosting) and also the <code class="fm-code-in-text">gblinear</code>. This booster utilizes a linear model as its base learner, incorporating both L1 and L2 regularization instead of employing a decision tree. Regarding the objective function, it tests the classical squared error, the gamma objective, and the Tweedie, blending aspects of the gamma and Poisson distributions. When selecting the gblinear boosting or the Tweedie objective, the code overrides the chosen parameters. It makes modifications and additions to them to fit the requirements of the gblinear booster or the Tweedie objective. Finally, it then creates an XGBoost Regressor with the suggested hyperparameters at each test and performs cross-validation to evaluate the MAE. The process is repeated for a specified number of trials (60 in this case). After optimization, the best achieved MAE and correspondin<a id="idTextAnchor047"/>g best hyperparameters are printed.<a id="idIndexMarker079"/><a id="idIndexMarker080"/></p>

  <p class="fm-code-listing-caption">Listing 7.17 Optimizing the XGBoost regressor</p>
  <pre class="programlisting">import optuna
 
def objective(trial):                                       <span class="fm-combinumeral">①</span>
    
    params = {                                              <span class="fm-combinumeral">②</span>
        'booster': trial.suggest_categorical(
            'booster', 
            ['gbtree', 'gblinear']
        ),
        'objective': trial.suggest_categorical(
            'objective', 
            ['reg:squarederror', 'reg:gamma', 'reg:tweedie']
        ),
        'n_estimators': trial.suggest_int(
            'n_estimators', 100, 1000
        ),
        'learning_rate': trial.suggest_float(
            'learning_rate', 0.01, 1.0, log=True
        ),
        'subsample': trial.suggest_float(
            'subsample', 0.3, 1.0
        ),
        'colsample_bytree': trial.suggest_float(
            'colsample_bytree', 0.3, 1.0
        ),
        'max_depth': trial.suggest_int('max_depth', 1, 7),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),
        'reg_lambda': trial.suggest_float(
             'reg_lambda', 1e-9, 100.0, log=True
        ),
        'reg_alpha': trial.suggest_float(
             'reg_alpha', 1e-9, 100.0, log=True
        ),
    }
    
    if params['booster'] == 'gblinear':                      <span class="fm-combinumeral">③</span>
        keys_to_remove = [
            "colsample_bytree", "max_depth", 
            "min_child_weight", "subsample"
        ]
        params = {
            key:value for key, value in params.items()
            if key not in keys_to_remove
        }
        
    if params['objective'] == 'reg:tweedie':                 <span class="fm-combinumeral">④</span>
        # Must be between in range [1, 2) : 1=poisson 2=gamma
        params['tweedie_variance_power'] = trial.suggest_float(
            'tweedie_variance_power', 1.01, 1.99
        )
    
    xgb = XGBRegressor(**params)                             <span class="fm-combinumeral">⑤</span>
    model_pipeline = Pipeline(
        [('processing', column_transform), 
         ('xgb', xgb)]
    )
    cv_splits = cv.split(X, y=X["neighbourhood_more_than_30"])
 
    cv_scores = cross_validate(
        estimator=model_pipeline, 
        X=X, 
        y=y,
        scoring='neg_mean_absolute_error',
        cv=cv_splits
    )                                                        <span class="fm-combinumeral">⑥</span><a id="marker-282"/>
    cv_evaluation = np.mean(
        np.abs(cv_scores['test_score'])
    )                                                        <span class="fm-combinumeral">⑦</span>
    return cv_evaluation                                     <span class="fm-combinumeral">⑧</span>
 
sqlite_db = "sqlite:///sqlite.db"
study_name = "optimize_XGBoost_tokyo_airbnb"
study = optuna.create_study(
    storage=sqlite_db, 
    study_name=study_name, 
    direction="minimize",
    load_if_exists=True
)                                                            <span class="fm-combinumeral">⑨</span>
 
study.optimize(objective, n_trials=100)                      <span class="fm-combinumeral">⑩</span>
print(study.best_value)                                      <span class="fm-combinumeral">⑪</span>
print(study.best_params)                                     <span class="fm-combinumeral">⑫</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Defines an optimization objective function using the Optuna library</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Dictionary containing hyperparameters for optimization, including booster type, objectives, and others</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Adjusts hyperparameters based on the chosen booster type</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Suggests the additional parameter 'tweedie_variance_power' for a tweedie objective</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Initializes an XGBoost Regressor with the suggested hyperparameters</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Performs cross-validation using the defined pipeline and optimizing for MAE</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑦</span> Calculates the MAE from the negative MAE scores</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑧</span> Returns the calculated evaluation metric value to be minimized</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑨</span> Creates an Optuna study with storage in a SQLite database for optimization</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑩</span> Performs optimization for a specified number of trials</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑪</span> Prints the best evaluation metric value achieved during optimization</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑫</span> Prints the best hyperparameters found during optimization</p>

  <p class="body">After having the optimization run for a while, we obtain a reduced MAE in respect of our first attempt and a set of suitable hyperparameters, showing a max depth of seven levels, about 900 estimators, and a Tweedie objective function with a variance power of 1.5, indicating a mixed distribution between Poisson and gamma:</p>
  <pre class="programlisting">6616.859370931483
{'booster': 'gbtree', 
 'colsample_bytree': 0.946407058507176,
 'learning_rate': 0.06867015067874482,
 'max_depth': 7,
 'min_child_weight': 5,
 'n_estimators': 901,
 'objective': 'reg:tweedie',
 'reg_alpha': 0.0006368936493084075,
 'reg_lambda': 3.8302865696045996,
 'subsample': 0.8956307610431394,
 'tweedie_variance_power': 1.560801988491813
}</pre>

  <p class="body"><a id="marker-283"/>At this point, we can also plot some diagnostic charts to understand better how the optimization went. For instance, we can first plot how the optimization proceeds across the 60 trials that we initially set:</p>
  <pre class="programlisting">fig = optuna.visualization.plot_optimization_history(study)
fig.show()</pre>

  <p class="body">Figure 7.13 shows how the best value was achieved quite early, before 20 trials, and it didn’t improve after that. This is important information because if the best optimization could have been achieved later, you may have suspected further improvements lying ahead in some more rounds of hyperparameter exploration by Optuna. Actually, you may have achieved that anytime by rerunning the command <code class="fm-code-in-text">study.optimize(objective,</code> <code class="fm-code-in-text">n_trials=100)</code>, setting your intended number of extra trials instead of the initial 100. Since we set to store trials on an SQLite database, you may reprise the optimization any time from the point it stopped (which is a strong point for using Optuna instead of other optimization options). Another important fact to gather from the chart is that there is quite a crowd of hyperparameter sets that are optimal or almost optimal. That means that there is no single optimization possible for this problem. That allows you to explore the different settings and decide on the solution that suits your needs. For instance, you may decide on a near-optimal solution that samples more features or requires fewer estimators since <a id="idTextAnchor048"/>they are faster at inference time.<a id="idIndexMarker081"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F13_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.13 How tests by Optuna progressively performed during the optimization</p>
  </div>

  <p class="body"><a id="marker-284"/>After observing how the optimization proceeded, another important piece of information is provided by charting the importance of the hyperparameters because it could hint at expanding the search space for such hyperparameters if they proved so important for the optimization process:</p>
  <pre class="programlisting">fig = optuna.visualization.plot_param_importances(study)
fig.show()</pre>

  <p class="body">In our case, the most critical factors proved to be <code class="fm-code-in-text">colsample_bytree</code> and <code class="fm-code-in-text">min_child_weight</code>, hyperparameters that caused the most variance i<a id="idTextAnchor049"/>n results, as seen in figure 7.14.<a id="idIndexMarker082"/><a id="idIndexMarker083"/></p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F14_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.14 Importance of hyperparameters in the tuning process</p>
  </div>

  <p class="body">Now we have a good set of hyperparameters. In the next subsection, we will complete our training phase by testing the model with cross-validation, an evaluation for generalization purposes, and training the fin<a id="idTextAnchor050"/>al model using all the available data.</p>

  <h3 class="fm-head1" id="heading_id_16">7.2.6 Training the final model</h3>

  <p class="body">Having completed the optimization, we can conclude our work by testing the results directly by cross-validation and then training the model on all available data. The code presented in listing 7.18 doesn’t change much from the code we previously used. Notice that now, for our estimation, we are using a cross-validation procedure, not cross-validation predictions, because we are more interested in understanding the generalization capabilities of our model and not ho<a id="idTextAnchor051"/>w it fits precisely the data at hand. <a id="idIndexMarker084"/><a id="marker-285"/></p>

  <p class="fm-code-listing-caption">Listing 7.18 Training the model with full data</p>
  <pre class="programlisting">best_params = study.best_params
print(best_params)
 
xgb = XGBRegressor(**best_params)                               <span class="fm-combinumeral">①</span>
model_pipeline = Pipeline([('processing', column_transform), ('xgb', xgb)])
 
cv_splits = cv.split(X, y=X["neighbourhood_more_than_30"])      <span class="fm-combinumeral">②</span>
 
r2_scores = []
rmse_scores = []
mae_scores = []
 
for train_index, test_index in cv_splits:                       <span class="fm-combinumeral">③</span>
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
 
    model_pipeline.fit(X_train, y_train)
    y_pred = model_pipeline.predict(X_test)
 
    r2_scores.append(r2_score(y_test, y_pred))
    rmse_scores.append(np.sqrt(mean_squared_error(y_test, y_pred)))
    mae_scores.append(mean_absolute_error(y_test, y_pred))
 
print(f"Mean cv R-squared: {np.mean(r2_scores):.3f}")
print(f"Mean cv RMSE: {np.mean(rmse_scores):.3f}")
print(f"Mean cv MAE: {np.mean(mae_scores):.3f}")
 
model_pipeline.fit(X, y)                                        <span class="fm-combinumeral">④</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Initializes an XGBoost Regressor using the best hyperparameters obtained from the Optuna study</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Splits the data using the specified StratifiedKFold strategy</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Iterates through the cross-validation folds to test the model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Trains the final model on the entire dataset</p>

  <p class="body">The following is the output you receive when running the code, containing the used parameters and the evaluation metrics, all based on our cross-validation strategy:</p>
  <pre class="programlisting">{'booster': 'gbtree', 
 'colsample_bytree': 0.946407058507176,
 'learning_rate': 0.06867015067874482,
 'max_depth': 7,
 'min_child_weight': 5,
 'n_estimators': 901,
 'objective': 'reg:tweedie',
 'reg_alpha': 0.0006368936493084075,
 'reg_lambda': 3.8302865696045996,
 'subsample': 0.8956307610431394,
 'tweedie_variance_power': 1.560801988491813
}
 
Mean cv R-squared: 0.727
Mean cv RMSE: 10886.568
Mean cv MAE: 6667.187</pre>

  <p class="body"><a id="marker-286"/>We can also visualize, as shown in figure 7.15, the complete pipeline comprising the column transformer, accepting the different features for its distinct transformation operations, and the XGBoost model receiving all the assembl<a id="idTextAnchor052"/>ed data from the column transformer.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F15_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.15 Pipeline comprising column transformations and XGBoost model</p>
  </div>

  <p class="body">Having thoroughly trained our model, we could say that we are done. Actually, this could be just the first cycle of multiple iterations because models have to be retrained often to escape what is called concept drift, as we explained in chapter 2, where the relationships between the predictors and the target change over time, rendering past models ineffective after a while.</p>

  <p class="body">In addition, often, the work of a machine learning engineer and of a data scientist doesn’t end with a working model because it is crucial to be able to figure out how it works and how the predictors actually relate with the target, providing insights into how a model arrives at its predictions. Explaining how a model works helps build trust, facilitates debugging, aids regulatory compliance, and enables humans to understand, validate, and improve the decision-making process of AI systems, which is the topic of th<a id="idTextAnchor053"/>e concluding section of this chapter.</p>

  <h2 class="fm-head" id="heading_id_17">7.3 Explaining your model with SHAP</h2>

  <p class="body"><a id="marker-287"/>To conclude, we spend some time trying to understand how our XGBoost model works because, as EDA helps you understand how your model can use data, explainability techniques such as SHAP (SHapley Additive exPlanations) or partial dependence plots (described in the previous chapter) can help you know how your model uses the data to come to its predictions. Explainability can provide valuable insights that help you better prepare your data, revise previous assumptions, and discard unuseful or detrimental features. <a id="idIndexMarker085"/><a id="idIndexMarker086"/><a id="idIndexMarker087"/></p>

  <p class="body">In addition, explainability plays other softer roles in a data science project than providing insights into how the model uses its features and generates predictions:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Human-AI collaboration</i>—When working with tabular data, data scientists collaborate with domain experts or business stakeholders who may not be well-versed in complex models. Explainability allows data scientists to communicate model insights effectively to nontechnical audiences.<a id="idIndexMarker088"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Building trust</i>—In certain domains, such as healthcare or finance, model explainability is essential to build trust with stakeholders and regulatory bodies.<a id="idIndexMarker089"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Compliance and regulations</i>—In some geographical areas and industries, there are regulatory requirements for model transparency and explainability, such as in the European Union, where the General Data Protection Regulation emphasizes the “right to explanation” for automated decision-making systems.<a id="idIndexMarker090"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Bias detection and mitigation</i>—Explainability can help identify biases in the data and the model’s decision-making process, highlighting if the model’s decision-making process could disadvantage any sensible group.<a id="idIndexMarker091"/></p>
    </li>
  </ul>

  <p class="body">Given all these reasons, we decided to produce SHAP values, which can be generated by the SHAP package (<a class="url" href="https://github.com/shap/shap">https://github.com/shap/shap</a>; install with <code class="fm-code-in-text">pip</code> <code class="fm-code-in-text">install</code> <code class="fm-code-in-text">shap</code>) and its TreeSHAP algorithm for tree-based models but also natively and more efficiently by XGBoost, as well as LightGBM, using a simple procedure. <a id="idIndexMarker092"/><a id="idIndexMarker093"/></p>

  <p class="body">SHAP values are a method that can explain the way predictions of machine learning models are built. They are based on Shapley values, a cooperative game theory concept that fairly distributes each feature’s “credit” or “importance” in a model’s prediction for a specific data instance. In other words, SHAP values allocate the contribution of each feature to the model’s output using a simple additive formula.<a id="idIndexMarker094"/></p>

  <p class="body">Shapley values consider the contribution of a feature across all possible combinations of features, which can be thought of as “games” in the model. These “games” involve training the model on different feature subsets. SHAP values approximate Shapley values using a resampling strategy to avoid computing all possible games for the model and feature sets. By using SHAP values, we gain insights into how each feature influences the model’s predictions on specific instances. This information is valuable for model debugging, feature engineering, and enhancing machine learning models’ overall interpretability and trustworthiness.</p>

  <p class="body">We implemented SHAP values in listing 7.19 to gain insights into our previously built XGBoost model. In the code, we first retrieve the trained XGBoost model from a pipeline. In particular, we get its booster, the core component of the XGBoost model responsible for implementing the gradient boosting algorithm. Then we transformed the training data two times: first because we could not use the pipeline to feed the data into the booster directly. Hence, we preprocess it by hand and extract its feature names for reference. Second, we transform the data into a DMatrix data structure (see the XGBoost documentation at <a class="url" href="https://mng.bz/yWQd">https://mng.bz/yWQd</a>), a specific XGBoost data structure for efficient processing that is required for feeding the booster directly. At this point, we compute the SHAP values by a predict command with the parameter <code class="fm-code-in-text">pred_contribs</code> set to true. Another simple predict command just provides us with the predic<a id="idTextAnchor054"/>tions from the model to be used for comparison. <a id="idIndexMarker095"/><a id="marker-288"/><a id="idIndexMarker096"/></p>

  <p class="fm-code-listing-caption">Listing 7.19 SHAP values as an XGBoost output<a id="idIndexMarker097"/></p>
  <pre class="programlisting">from XGBoost import DMatrix
 
booster = model_pipeline['xgb'].get_booster()              <span class="fm-combinumeral">①</span>
 
Xt = model_pipeline['processing'].transform(X)             <span class="fm-combinumeral">②</span>
feature_names = (
    model_pipeline['processing']
    .get_feature_names_out()
)                                                          <span class="fm-combinumeral">③</span>
Xd = DMatrix(Xt)                                           <span class="fm-combinumeral">④</span>
 
shap_values = booster.predict(Xd, pred_contribs=True)      <span class="fm-combinumeral">⑤</span>
preds = booster.predict(Xd)                                <span class="fm-combinumeral">⑥</span></pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Retrieves the trained XGBoost booster object from the pipeline’s trained XGBoost model</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Transforms the input data X using the processing pipeline</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Gets the names of the transformed features after the processing pipeline’s transformations</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Creates a DMatrix from the transformed input data</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑤</span> Calculates SHAP values using the booster’s predict function with the pred_contribs=True argument</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">⑥</span> Gets the raw predicted values for the input data</p>

  <p class="body">Just for comparison, we have to note that LightGBM is also capable of the same, using the same prediction method with the <code class="fm-code-in-text">pred_contribs</code> parameter set to true. The only difference is that you do not need to extract any booster from the trained LightGBM model. You just use the model itself directly.<a id="idIndexMarker098"/></p>

  <p class="body">Note that whether you are doing a classification or a regression, the resulting SHAP values obtained by this method are log transformations of a multiplicative model. It means that if you want to recreate the original prediction, you first have to exponentiate the values and then multiply them by themselves, as demonstrated by the following code snippet, reconstructing for the first example the prediction from the SHAP values and comparing it to the effective prediction:</p>
  <pre class="programlisting">np.prod(np.exp(shap_values[0])), preds[0]
 
(10627.659, 10627.469)</pre>

  <p class="body">As you can see, there are slight discrepancies in the reconstruction, which can be attributed to approximations and small errors. However, in general, the SHAP values provide a good approximation of the predictions themselves. When applying the same approach to the entire training set and assessing its adherence to the original predictions using Pearson’s correlation, it demonstrates a strong fit of the SHAP values to the predictions:</p>
  <pre class="programlisting">np.corrcoef(preds, np.prod(np.exp(shap_values), axis=1))
 
array([[1., 1.],
       [1., 1.]])</pre>

  <p class="body"><a id="marker-289"/>As an alternative to directly outputting the SHAP values as an XGBoost prediction, you can use the <code class="fm-code-in-text">TreeExplainer</code> function from the SHAP package (<a class="url" href="https://mng.bz/pKXR">https://mng.bz/pKXR</a>). The function, though being declared built with fast C++ implementations, is way slower than the direct predictions from XGBoost. However, using the <code class="fm-code-in-text">TreeExplainer</code>, you can specify more output options, particularly the output type and the calculation method, which can allow you to reconstruct the original prediction as seen previously (using the parameter <code class="fm-code-in-text">feature_perturbation="tree_path_dependent"</code>) or using a method that “breaks the dependencies between features according to the rules dictated by casual inference,” thus providing more reliable insights when there is strong collinearity among the features (using the parameter <code class="fm-code-in-text">feature_perturbation="interventional"</code>). You can obtain the interventional SHAP values using the following code snippet:<a id="idIndexMarker099"/><a id="idIndexMarker100"/><a id="idIndexMarker101"/></p>
  <pre class="programlisting">from shap import TreeExplainer
 
explainer = TreeExplainer(model_pipeline['xgb'], data=Xt, model_output='raw', feature_perturbation='interventional')
interventional_shap_values = explainer.shap_values(Xt)</pre>

  <p class="body">The resulting SHAP values matrix is less truthful to the original data and cannot reconstruct the predictions as seen before. Still, such an approach could provide more reliable contribution estimates “true to the model” as explained in technical terms in the following GitHub problem: <a class="url" href="https://github.com/shap/shap/issues/1098">https://github.com/shap/shap/issues/1098</a>. Based on our experience, we suggest using <code class="fm-code-in-text">TreeExplainer</code> and the interventional approach, although it may require longer computation times when dealing with data presenting highly multicollinear features.</p>

  <p class="body">Up to now, we have used the SHAP values as a method for explaining individual samples. We investigated, by the inspection of feature contributions, the reasons why a certain prediction is made. However, we can consider all the SHAP values together and reason about them to figure out a general explanation for the entire model. In this case, as for other methods, we can plot some summary and diagnostic charts to figure this out better. The first listing we propose quantifies the relative importance of the features by an average of the SHAP values. Here, we use plotting facilities from the <code class="fm-code-in-text">shap</code> package. You install the package by running the command <a id="idTextAnchor055"/><code class="fm-code-in-text">pip</code> <code class="fm-code-in-text">install</code> <code class="fm-code-in-text">shap</code> in a shell or a cell of your notebook.<a id="idIndexMarker102"/><a id="marker-290"/></p>

  <p class="fm-code-listing-caption">Listing 7.20 SHAP importance plot</p>
  <pre class="programlisting">import shap
 
shap.summary_plot(
    shap_values[:,:-1],
    Xt,
    plot_type="bar",
    feature_names=feature_names,
    max_display=10,
    show=False
)                                                          <span class="fm-combinumeral">①</span>
plt.xticks(fontsize=8)
plt.yticks(fontsize=8)
plt.xlabel("SHAP Importance", fontsize=10)
plt.show()</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Generates a summary plot of SHAP feature importance for the top 10 most important features</p>

  <p class="body">Figure 7.16 shows the resulting plot, and you can immediately determine that four features tend to dominate the predictions, which are the availability, which is also a proxy for the market offer-demand dynamic for a certain accommodation (less availability may imply a shared use or less demand for that accommodation); target encoded coordinates (i.e., the position of the accommodation in the city); the number of bedrooms, a proxy of how large the accommodation is; and the number of beds, which helps together with the previous figure to distinguish the hostel-like listings, which are usually less pricey. All the other features play a lesser role, which can be seen from the scale of the plot: the importance of the last of the 10 most importa<a id="idTextAnchor056"/>nt features is a fifth of the top important features.</p>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F16_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.16 SHAP importance</p>
  </div>

  <p class="body">Importance, however, tells just a part of the story. We also need directionality. Hence, the violin chart can provide even more information on the model’s behavior. In a violin plot produced by the <code class="fm-code-in-text">shap</code> package, you can get hints from these details:<a id="idIndexMarker103"/><a id="marker-291"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Feature importance</i>—The width of the violin plot indicates the density of SHAP values. Wider sections represent more instances with similar SHAP values for that feature. Thus, features with broader violin plots are generally more important in the model’s predictions.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Shape of the violin</i>—The violin’s shape indicates the distribution of SHAP values for the corresponding feature. If the violin is symmetric, it suggests that SHAP values are evenly distributed around the median, signifying a balanced effect on predictions. Asymmetry indicates skewness and suggests that certain feature values have more significant effects than others.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Positive and negative contributions</i>—The violin plot’s center line (median) is usually zero. The left and right halves of the violin represent their respective contributions for features with positive and negative SHAP values. Positive SHAP values push predictions higher, while negative SHAP values push them lower.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Association with the feature value</i>—The color of the violin plot can help you associate blue areas, where the feature has lower values, and red areas, where the feature has higher values, with specific SHAP contributions. This helps in understanding how the feature is generally related to the outcome.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Outliers</i>—Outliers or extreme SHAP values outside the range of the violin plot suggest instances where the corresponding feature has an unusually strong effect on the prediction.</p>
    </li>
  </ul>

  <p class="body">In the following listing, the violin plot provides useful insights into the distribution <a id="idTextAnchor057"/>and the role of each feature on the model’s predictions<a id="marker-292"/>.</p>

  <p class="fm-code-listing-caption">Listing 7.21 SHAP violin plot</p>
  <pre class="programlisting">shap.summary_plot(shap_values[:,:-1], Xt, 
                  plot_type="violin", 
                  feature_names=feature_names,
                  show=False)                              <span class="fm-combinumeral">①</span>
plt.yticks(fontsize=8)
plt.show()</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Creates a SHAP summary plot using violin plots to visualize the distribution of SHAP values for each feature</p>

  <p class="body">Figure 7.17 shows the resulting violin plot. As for our top important features, we can figure out the following:<a id="idIndexMarker104"/></p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">Numeric__availability_365</code>—Higher availability corresponds to a positive effect on price. Listings with lower availability are usually penalized.<a id="idIndexMarker105"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">Target_encoding__coordinates</code>—It is difficult to interpret since its values are unrelated to a specific directionality. We can observe that there are long tails on both sides with a prevalence of a negative contribution to the pricing of the accommodation.<a id="idIndexMarker106"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">Numeric__number_of_bedrooms</code>—A higher number of bedrooms implies a higher price, with a long skewed tail to the right.<a id="idIndexMarker107"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><code class="fm-code-in-text">Numeric__number_of_beds</code>—Similarly, a higher number of beds implies a higher price, with a long skewed tail to the right.<a id="idIndexMarker108"/></p>
    </li>
  </ul>

  <div class="figure">
    <p class="figure1"><img alt="" class="calibre4" src="../Images/CH07_F17_Ryan2.png"/></p>

    <p class="figurecaption">Figure 7.17 SHAP violin plot</p>
  </div>

  <p class="body">A glance at other features provides an idea of how the model behaves intuitively. For instance, the nearer the accommodation is to t<a id="idTextAnchor058"/><a id="marker-293"/>he Imperial Palace or the airports, the higher the price.</p>

  <p class="body">This concludes our end-to-end example using gradient boosting. In the next chapter, we are going to get back to the Airbnb NYC problem and will review a set of deep learning stacks (low-level framework, high-level API, and deep learning for tabular data library) and use three of these stacks (fastai, PyTorch with TabNet, and Lightning Flash) to solve it and compare the different solutions.</p>

  <p class="body">In this phase, a generative AI tool such as ChatGPT could be useful for creating narratives explaining the SHAP values assigned to each example. Being able to create an easy explanation for each prediction could prove to be a strong point when demonstrating the potentialities of your model or trying to persuade clients and stakeholders. In addition, the need for a narration explaining the model’s predictions of the dataset is crucial under regulations such as those in the European Union. Transparency and interpretability are essential components of regulations striving at data protection and preserving privacy, such as the General Data Protection Regulation in the EU. According to these regulations, individuals have the right to make sense of the logic behind automated decision-making processes that significantly affect their lives. Providing a clear and comprehensible explanation for why a specific prediction has been made ensures transparency and accountability and promotes fairness: it gives individuals the power to seek clarification, challenge unfair decisions, and ultimately safeguard their rights.<a id="idIndexMarker109"/><a id="idIndexMarker110"/><a id="idIndexMarker111"/></p>

  <p class="body">You can actually generate each of these narratives by single prompts to create explanations on the fly or by using the ChatGPT API and have the model process batches of explanations that you can later recall when questioned about the reason for a specific prediction. The recipe is, however, the same in both on-the-fly or batch processing approaches: you have to tell the LLM to explain by providing the list of the features (detailed with their description or meaning, if necessary), the original value in the dataset, and the SHAP value relative to the feature. Of course, it is necessary to mention the resulting prediction. Gluing together all such information for the LLM to process using JSON (a dictionary of dictionaries) could be ideal. In listing 7.22, we offer a solution for preparing a JSON structure to facilitate the prompt request to ChatGPT to explain a specific example in the dataset, identified by its row index. The code generates a data structure that encompasses all the info<a id="idTextAnchor059"/>rmation required to construct a coherent narrative explanation.<a id="marker-294"/></p>

  <p class="fm-code-listing-caption">Listing 7.22 Building a JSON of SHAP explanations as part of a prompt<a id="idIndexMarker112"/></p>
  <pre class="programlisting">def generate_prediction_explanation(
    index,
    X,
    feature_names, 
    shapley_values,
    predictions
):
    explanation = {}                                        <span class="fm-combinumeral">①</span>
    explanation["prediction"] = predictions[index]          <span class="fm-combinumeral">②</span>
    for feature, original_value, shap_value in zip(
        feature_names, 
        X[index],
        shapley_values[index, :]
    ):                                                      <span class="fm-combinumeral">③</span>
        explanation[feature] = {
            "original_value": original_value, 
            "shap_value": shap_value
        }
    return explanation
 
index_to_explain = 5                                        <span class="fm-combinumeral">④</span>
explanation_json = generate_prediction_explanation(
    index_to_explain, 
    feature_names, 
    Xt,
    shap_values,
    preds
)
print(explanation_json)</pre>

  <p class="fm-code-annotation"><span class="fm-combinumeral">①</span> Instantiates the JSON data structure as a Python dictionary</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">②</span> Includes the predicted value to explain in the JSON</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">③</span> Iterates over the features, original value, and SHAP values of the examined row</p>

  <p class="fm-code-annotation"><span class="fm-combinumeral">④</span> Index of the prediction to explain</p>

  <p class="body">In our example, we require a description of why the model predicted a particular value for row 5 of the dataset. The printed JSON can then be enclosed in a prompt such as</p>
  <pre class="programlisting">You are an expert data scientist, and you need to interpret the predictions
 of a regression model based on the shape values provided in a JSON file.
 You build the explanations as a narration of how the most important 
variables contribute to the prediction. Here is the JSON file:
{'prediction': 55225.176, 'onehot_encoding__room_type_Entire home/apt': 
{'original_value': 1.0, 'shap_value': 0.03404991}, 
'onehot_encoding__room_type_Hotel room': {'original_value': 0.0, 
'shap_value': 0.00020163489}, … }</pre>

  <p class="body"><a id="marker-295"/>When you submit this prompt to ChatGPT, you will receive a text organized in bullet points categorized by types of variables. This text describes the influence of each individual variable or group of variables on the outcome. The following is an excerpt of the insights we derived for the specific instance represented by row 5:</p>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Room type</i>—The “Entire home/apt” room type has a positive effect on the predicted price, contributing a SHAP value of 0.034. This suggests that listings with the entire home/apartment as the room type tend to have higher prices.<a id="idIndexMarker113"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">The other room types</i> (“Hotel room,” “Private room,” and “Shared room”)—These have smaller positive or negligible contributions, indicating that their effect on the price is not as significant.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Neighborhood</i>—The feature <code class="fm-code-in-text">neighbourhood_more_than_30</code> has a positive SHAP value of 0.083, suggesting that being in a neighborhood with more than 30 listings positively influences the price.<a id="idIndexMarker114"/></p>
    </li>

    <li class="fm-list-bullet">
      <p class="list"><i class="fm-italics">Type of accommodation</i>—The <code class="fm-code-in-text">type_of_accommodation</code> feature has a small negative effect with a SHAP value of –0.008. This implies that certain types of accommodation might have a slightly lower price.<a id="idIndexMarker115"/></p>
    </li>
  </ul>

  <p class="body">The complete text actually touches on all the features, and you can prompt the LLM to reduce the results to only the top 5 or 10 impactful features, if you prefer. Certainly, using a language model for th<a id="idTextAnchor060"/>is job makes a difficult task simple and automates it in a breeze. <a id="idIndexMarker116"/><a id="idIndexMarker117"/></p>

  <h2 class="fm-head" id="heading_id_18">Summary</h2>

  <ul class="calibre5">
    <li class="fm-list-bullet">
      <p class="list">Getting and preparing your data requires downloading, restructuring, and assembling it all together. It is often a long and laborious part of the work in an end-to-end project, but it is an indispensable one, building the foundations for the success of your following work.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Feature engineering is not just magic or randomly combining features; most often, it is embedding prior knowledge about a problem and how to solve it in the features you will be using to train your model. Exploring the set of domain knowledge related to a problem is the first step to model your data effectively.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Exploring your predictions and target in the EDA phase is an essential part of your schedule for modeling a tabular problem. Look for outliers and extreme values, missing data, and any other peculiarity from the data. Feel free to drop examples if you are unsure they can provide real value to your model.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Before delving into modeling, check for your validation strategy, which may require extra work, EDA, and your data pipeline. Both can make a difference in the modeling phase. Ideally, prepare a pipeline for each type of model you want to test because each model has different ways of dealing with the various types of data you find in tabular datasets. In our example, as a simplification, we tried a one-size-fits-all approach. Please remember that such examples work well in books, but there are better ways to do it in real-world projects.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Building a baseline model is an often-neglected phase in modeling tabular data problems. Still, it can provide valuable insights by inspecting how the model underfits or overfits the data and its internal coefficients. A baseline model should necessarily be simple, meaning that linear and logistic regression are the best candidates for regression and classification problems.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">After you get insights from your baseline model, you can proceed to more complex models such as XGBoost. Cues about underfitting, nonlinearities, interactions, targets, and the predictors’ characteristics should be considered when setting up the first tentative values for key hyperparameters.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Optimizing your model using Optuna can save you a lot of time if you set your search space to incorporate insights and hypotheses you have developed so far regarding how your model should handle the data and the problem. Once the optimization has been completed, further insights can be gained from observing hyperparameter importance and optimization path charts.</p>
    </li>

    <li class="fm-list-bullet">
      <p class="list">Explaining your trained model can be easily done with XGBoost and LightGBM using the predict method with the parameter <code class="fm-code-in-text">pred_contribs</code> set to true. Once the SHAP values, which are effectively multipliers with respect to the prediction, are obtained, you can use standard charts from the <code class="fm-code-in-text">shap</code> package, such as the importance plot or the violin plot.<a id="idIndexMarker118"/><a id="marker-296"/><a id="idIndexMarker119"/></p>
    </li>
  </ul>
</body></html>