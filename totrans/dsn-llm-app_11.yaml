- en: Chapter 9\. Inference Optimization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章\. 推理优化
- en: In the past few chapters, we learned several techniques for adapting and utilizing
    LLMs to solve specific tasks. In this chapter, we will learn how to efficiently
    perform inference on them for real-world usage. LLMs’ large size make deployment
    and inference particularly challenging, as they exert significant pressure on
    compute, memory, and energy requirements. This proves to be especially challenging
    on edge devices like mobile phones.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我们学习了多种适应和利用LLM来解决特定任务的技术。在本章中，我们将学习如何高效地对它们进行推理，以用于实际应用。LLM的大规模使得部署和推理特别具有挑战性，因为它们对计算、内存和能源需求施加了巨大的压力。这在移动手机等边缘设备上尤其具有挑战性。
- en: For the rest of the chapter, we will focus on the field of inference optimization,
    discussing the factors influencing LLM inference time. We will then showcase a
    variety of optimization techniques including caching, knowledge distillation,
    early exiting, quantization, parallel and speculative decoding, and more.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将专注于推理优化的领域，讨论影响大型语言模型（LLM）推理时间的因素。然后，我们将展示包括缓存、知识蒸馏、早期退出、量化、并行和推测解码等多种优化技术。
- en: LLM Inference Challenges
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM推理挑战
- en: 'What are the bottlenecks affecting LLM inference? As we all know, their gargantuan
    sizes necessitate vast computing and memory resources. Apart from that, two additional
    factors exacerbate the situation:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 影响LLM推理的瓶颈有哪些？众所周知，它们庞大的体积需要大量的计算和内存资源。除此之外，还有两个额外的因素加剧了这种情况：
- en: As seen in [Chapter 4](ch04.html#chapter_transformer-architecture), contemporary
    LLMs are based largely on decoder-only models that operate autoregressively. This
    means that each token is generated one after the other, thus imposing a sequential
    limitation. Later in this chapter, we will discuss techniques for parallel and
    speculative decoding that aim to speed up the decoding process.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如[第4章](ch04.html#chapter_transformer-architecture)所示，当代的LLM主要基于仅解码器模型，它们以自回归的方式运行。这意味着每个标记都是依次生成的，从而施加了顺序限制。在本章的后面部分，我们将讨论旨在加快解码过程的并行和推测解码技术。
- en: As the input sequence length increases, the amount of compute needed increases
    quadratically. Later this chapter, we will discuss techniques like K-V caching
    that aim to alleviate this bottleneck.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着输入序列长度的增加，所需的计算量呈平方增长。在本章的后面部分，我们将讨论旨在缓解这一瓶颈的技术，如K-V缓存。
- en: Let’s dive into the techniques used to optimize inference.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解用于优化推理的技术。
- en: Inference Optimization Techniques
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推理优化技术
- en: Since this is a problem that severely impacts the deployment of LLMs in real-world
    use cases, considerable attention has been given to inference optimization research
    in major industry and academic labs. Dozens of optimization techniques have been
    developed in recent years, without which the present ubiquity of LLMs would not
    have been achieved. For a near-comprehensive survey of the various types of techniques
    used for optimizing inference, check out [Zhou et al.’s survey paper](https://oreil.ly/MtzNn).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个严重影响LLM在实际用例中部署的问题，因此在主要的工业和学术实验室中，对推理优化研究给予了极大的关注。近年来，已经开发出数十种优化技术，没有这些技术，LLM的当前普及就不会实现。要了解用于优化推理的各种类型技术的近全面调查，请参阅[周等人综述论文](https://oreil.ly/MtzNn)。
- en: We will now focus on some of the most promising and effective inference optimization
    techniques used in LLM deployments. While most of you may not be implementing
    these techniques by yourself but instead rely on third-party tools, understanding
    the optimization techniques and the tradeoffs involved provide valuable insights
    that can help you choose among various solutions.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将关注在LLM部署中使用的一些最有希望和最有效的推理优化技术。虽然你们中的大多数人可能不会亲自实施这些技术，而是依赖第三方工具，但了解优化技术和涉及的权衡可以为你们在选择各种解决方案时提供宝贵的见解。
- en: 'Techniques for efficient inference aim to achieve the following three goals:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 高效推理的技术旨在实现以下三个目标：
- en: Reduce compute
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 减少计算
- en: Techniques like caching, knowledge distillation, and early exit, each of them
    employing distinct strategies to reduce computation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 像缓存、知识蒸馏和早期退出这样的技术，每个都采用不同的策略来减少计算。
- en: Speed up decoding
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 加速解码
- en: 'Techniques for parallel and speculative decoding aim to improve the throughput
    of the model: the number of tokens generated per second.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 并行和推测解码的技术旨在提高模型的吞吐量：每秒生成的标记数。
- en: Reduce storage needs
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 减少存储需求
- en: Quantization techniques aim to reduce the amount of storage needed for weights
    and activations of the model, by reducing space required to store numbers from
    32 bits to 16, 8, or even 4 bits.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 量化技术旨在通过减少存储权重和激活所需的存储空间来减少模型，通过将存储数字的空间从32位减少到16位、8位甚至4位。
- en: Techniques for Reducing Compute
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少计算的技巧
- en: 'We can reduce compute required during inference by:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式减少推理过程中所需的计算：
- en: Trading compute for extra storage, using methods like caching.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用缓存等方法以计算换取额外的存储。
- en: Foregoing certain operations during inference, using methods like *early exit*.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在推理过程中省略某些操作，使用如*早期退出*等方法。
- en: Deriving a smaller model from a larger model while preserving as many characteristics
    and capabilities from the larger model as possible, using techniques like knowledge
    distillation.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用知识蒸馏等技术，从较大的模型中衍生出较小的模型，同时尽可能保留较大的模型的特征和能力。
- en: The next sections will explore each of these methods in detail.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个章节将详细探讨这些方法中的每一个。
- en: K-V Caching
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K-V 缓存
- en: As seen in [Chapter 1](ch01.html#chapter_llm-introduction), LLMs do not have
    session memory; at every turn in an LLM conversation, the previous conversation
    history is added to the input. This means that every request to an LLM could potentially
    contain a lot of repetitive content in the prompt. For the repetitive parts of
    the prompt, the same computation is performed during the inference step again
    and again. Moreover, in autoregressive decoding, each token is generated as a
    function of the entire input and the previously generated tokens. Thus, there
    is a lot of duplicative computation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](ch01.html#chapter_llm-introduction)中所示，LLM没有会话记忆；在LLM对话的每个回合，之前的对话历史都会添加到输入中。这意味着每个对LLM的请求都可能包含大量重复的内容在提示中。对于提示的重复部分，在推理步骤中会反复执行相同的计算。此外，在自回归解码中，每个标记都是作为整个输入和之前生成的标记的函数生成的。因此，存在大量的重复计算。
- en: One way to alleviate this duplicative computation is to cache the data and reuse
    them when required. More specifically, we cache the keys (K) and values (V) of
    the self-attention blocks of the Transformer architecture, referred to as the
    K-V cache. Recall our discussion in [Chapter 4](ch04.html#chapter_transformer-architecture)
    about keys and values in the self-attention block of the Transformer.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 缓解这种重复计算的一种方法是在需要时缓存数据并重用它们。更具体地说，我们缓存了Transformer架构中自注意力块的键（K）和值（V），这被称为K-V缓存。回想一下我们在[第4章](ch04.html#chapter_transformer-architecture)中关于Transformer自注意力块中键和值的讨论。
- en: Let’s look at some examples. Consider the task of analyzing sentiment of movie
    reviews. You might have a lengthy prompt providing detailed instructions on the
    nuances involved in analyzing sentiment. These instructions are included in the
    prompt for every input review being fed to the LLM.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些例子。考虑分析电影评论情感的任务。您可能有一个提供详细说明的冗长提示，说明分析情感所涉及的细微差别。这些说明包含在LLM接收到的每个输入评论的提示中。
- en: Instead of incurring unnecessary overhead by repetitively processing the instruction
    tokens, the cache is consulted to fetch the K-V values for these tokens.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是通过重复处理指令标记来产生不必要的开销，缓存被用来检索这些标记的K-V值。
- en: Similarly, consider the example of a question-answering assistant that provides
    customer support by answering questions from a product manual. In this case, the
    K-V values representing the product manual tokens can be cached and then reused
    for any requests where the product manual needs to be part of the prompt.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，考虑一个提供客户支持的问答助手，该助手通过回答产品手册中的问题来提供客户支持。在这种情况下，代表产品手册标记的K-V值可以缓存，然后用于任何需要产品手册作为提示部分的请求。
- en: Tip
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Caching can also enable adding a lot of few-shot examples in the prompt. This
    can sometimes be a lightweight alternative to fine-tuning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存还可以使在提示中添加大量少样本示例成为可能。这有时可以作为一种轻量级的替代方案来微调。
- en: Major LLM providers like Google’s Gemini and Anthropic’s Claude provide caching
    support for their models through their APIs, calling it context caching. This
    also vastly reduces the cost for end users, as cached tokens are billed only once.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 主要的LLM提供商，如谷歌的Gemini和Anthropic的Claude，通过它们的API为它们的模型提供缓存支持，称之为上下文缓存。这也大大降低了最终用户的成本，因为缓存的标记只计费一次。
- en: Warning
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Note that in the caching strategy, we are trading compute for additional storage.
    K-V caches can get unfeasibly large, especially at longer sequence lengths.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在缓存策略中，我们是以计算换取额外的存储。K-V缓存可能会变得非常大，尤其是在较长的序列长度下。
- en: To keep costs under control, LLM providers typically limit the age of the cache
    to a short period or price users by caching duration.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了控制成本，LLM提供商通常将缓存的年龄限制在一个较短的时间段，或者通过缓存持续时间向用户收费。
- en: 'As an example, let’s look at a request to Anthropic’s Claude suite of models
    that utilizes context caching:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看一个请求Anthropic的Claude模型套件，该套件利用上下文缓存：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '``The `cache_control` parameter is used to specify that the system prompt and
    the product manual is to be cached. As of the book’s writing, Claude’s cache is
    live by default for five minutes.    ###### Tip    Organize your prompt to place
    the cacheable components at the beginning of the prompt, i.e., the prompt prefix.    Ultimately,
    caching can be very valuable in reducing inference time, especially in settings
    where instructions are repeated for a large number of calls, or the context window
    contains data like API documentation or RAG output that needs to persist across
    multiple calls.    Next, we’ll explore the early exit method for reducing inference-time
    compute.``  `## Early Exit    As shown in [Chapter 4](ch04.html#chapter_transformer-architecture),
    the Transformer architecture is made up of repeating blocks called layers. The
    output of each layer is an intermediate representation that gets fed as input
    to the layer above it. One simple way of reducing compute during inference is
    to exit inference at an intermediate layer and interpret it as the final output.
    This technique is called early exit. [Figure 9-1](#early-exit) shows early exit
    in practice.  ![early-exit](assets/dllm_0901.png)  ###### Figure 9-1\. Early exit
    in practice    Early exit can happen both at the sequence level and at the token
    level.    ### Sequence-level early exit    In this scenario, the forward pass
    in the Transformer is stopped at a particular layer for the entire input sequence,
    and the intermediate representations from that layer are taken as the final output.
    The layer at which to exit can be determined in advance or can be dynamically
    decided based on the input sequence.    To dynamically decide the layer to exit,
    you can train adapters on top of each layer, as shown in [Chapter 7](ch07.html#ch07).
    These modules can then be used to predict whether the exit can happen at the current
    layer. For example, [FastBERT](https://oreil.ly/GCfpt) implements modules at each
    layer that learn to solve a binary classification problem (to exit or not exit).    Not
    all methods depend on adding trainable modules. For example, the [hash-based early
    exiting approach (HashEE)](https://oreil.ly/_JqqH) by Sun et al. uses an annotated
    set of sequences along with their exit layers as the basis for determining the
    exit layers for new input sequences. This method is based on the hypothesis that
    similar sequences should exit at the same layers.    The second early exit option
    is token-level early exit.    ### Token-level early exit    In this approach,
    different tokens from the same sequence can exit at different layers. This is
    more complex to implement than sequence-level early exit.    Similar to sequence-level
    early exit techniques, you can implement binary classifiers to decide whether
    to exit at a particular layer, but this happens at each token at each layer, instead
    of the entire sequence. For more details on token-level early exit, refer to [Schuster
    et al.](https://oreil.ly/hfdCd), who introduced the technique Confident Adaptive
    Language Modeling (CALM), that implements token-level early exit.    Recall that
    in the self-attention subblock of the Transformer, the representation of a token
    is calculated using the representations of all other tokens in the sequence in
    the same layer. But if we are using token-level early exit, it is possible that
    some tokens in a sequence might already have exited before that layer. The easiest
    way to resolve this issue is to copy the representations of the exited token to
    every layer above it.    While token-level early exit can be more fine-grained
    and effective than sequence-level early exit, it is slower than sequence-level
    early exit.    ###### Note    In early exit, the reduction in compute comes at
    the cost of performance. However, this can be minimized by learning to exit at
    the optimal layer.    Dynamic early exit belongs to a class of techniques called
    *dynamic inference*, where the inference compute is determined dynamically, based
    on the characteristics of the input. One important example is the mixture of experts
    (MoE) class of models, introduced in [Chapter 4](ch04.html#chapter_transformer-architecture).
    In MoE models, a routing function chooses a small subset of expert modules to
    run inference on, thus reducing the amount of compute required.    Next, let’s
    explore how we can reduce inference time by creating a smaller derivative model
    from a larger model while limiting performance degradation, using a technique
    called knowledge distillation.    ## Knowledge Distillation    In [Chapter 5](ch05.html#chapter_utilizing_llms),
    we briefly introduced distilled versions of models, like [DistilBERT](https://oreil.ly/rgiHZ).
    These are smaller models that approximate the capabilities of the larger models
    they are distilled from, thus enabling speedier inference.    Over the years,
    several techniques have been developed for knowledge distillation. For a survey
    of research advances in this field, refer to [Xu et al.’s](https://oreil.ly/JZQf3)
    survey paper.    The process of knowledge distillation can be divided into two
    steps: distillation data preparation and training. The base model is referred
    to as the teacher model and the distilled model is referred to as the student
    model.    [Figure 9-2](#knowledge-distillation) depicts the process of distilling
    a model.  ![knowledge-distillation](assets/dllm_0902.png)  ###### Figure 9-2\.
    Knowledge distillation    Here’s how the distillation data preparation and training
    steps work.    ### Distillation data preparation    Data for distillation is typically
    prepared by appropriately querying the teacher model and using the teacher’s outputs
    as the *knowledge* to be distilled. Ways to elicit relevant outputs from the teacher
    include:    Unsupervised generation      In this technique, the teacher is prompted
    with instructions and/or examples for solving a task. The teacher’s responses
    comprise the distillation dataset. This technique is commonly used to teach capabilities
    like CoT or instruction-following to smaller models. To accomplish that, teacher
    models are asked to respond to queries with the thought process leading up to
    the answer.      Data augmentation      In this technique, the teacher is shown
    a set of seed input-output examples. Based on the seed examples, the teacher generates
    similar input-output examples, constituting the distillation dataset. Note that
    both the input and output are generated by the teacher model in this setting.
    The limitation of this technique is that the teacher is unable to generate sufficiently
    diverse examples.      Intermediate representations      This class of techniques
    is known as white-box distillation. Here the distillation dataset consists of
    intermediate representations from a model, which can include activations or output
    logits. This data can be used to align the student model with the teacher model.
    The alignment is learned using methods like KL-divergence, discussed in [Chapter 4](ch04.html#chapter_transformer-architecture).      Teacher
    feedback      In this class of techniques, the outputs from a student model are
    assessed by the teacher model to generate feedback. The teacher model can be used
    to generate preference data, i.e., the quality ranking of outputs from the student.
    Feedback can also be given in the form of detailed instructions on how to improve
    on a given task. A popular technique using teacher feedback is RLAIF, which we
    introduced in [Chapter 5](ch05.html#chapter_utilizing_llms).      Self-teaching      In
    this class of techniques, the teacher and student model are one and the same.
    The student model progressively refines its own outputs and uses them as the distillation
    set. One way of self-teaching is to generate multiple outputs for each task, along
    with reasoning steps, and choosing the best one to be part of the distillation
    set.      How many distillation examples do you need? Perhaps surprisingly, not
    a whole lot. [Zhou et al.](https://oreil.ly/MuOOj) show that even one thousand
    very high-quality examples are enough to create a strong distillation set.    ######
    Warning    Just like fine-tuning and continued pre-training, knowledge distillation
    is susceptible to the catastrophic forgetting problem (introduced in [Chapter 7](ch07.html#ch07)).    Now
    that we have seen the various ways to create distillation datasets, let’s turn
    to the actual distillation process.    ### Distillation    Here are some techniques
    used to perform the distillation task. For a more detailed survey of techniques,
    refer to [Xu et al.](https://oreil.ly/9mbiN):    Supervised fine-tuning      This
    is the simplest way to accomplish knowledge distillation. The student model is
    fine-tuned using the distillation set with the objective of aligning its predictions
    with that of the teacher model. This method is typically used in black-box knowledge
    distillation settings, where the distillation set does not comprise any internal
    representations.      K-L divergence of output probabilities      In this method,
    our objective function is to minimize the K-L divergence between the output probability
    distribution of the teacher model and the student model.      Internal representation
    similarity      Conversely, instead of minimizing divergence, you can maximize
    similarity between aspects of the teacher and student model. This can be leveraged
    to perform layerwise distillation, where the internal representations of the teacher
    and the student are aligned at each layer. Refer to [Liang et al.](https://oreil.ly/g-C4L)
    for an effective technique for layerwise distillation.      Reinforcement learning      This
    involves training a reward model using the distillation data. The student model
    is then trained to maximize the reward as per the reward model. Recall our discussion
    on reinforcement learning in [Chapter 8](ch08.html#ch8).      Ultimately, the
    technique you choose to distill your models depends on whether you have access
    to the teacher weights. If you do not have access to the teacher weights, then
    you can perform only supervised fine-tuning. White-box distillation, where you
    are trying to align intermediate representations and not just the output tokens,
    can be challenging to achieve. Note that all knowledge distillation techniques
    carry the risk of capability degradation or catastrophic forgetting, so you will
    have to evaluate the student model very carefully to quantify the difference in
    capabilities from the teacher model.    In this section, we discussed three distinct
    techniques for reducing compute during inference: caching, early exit, and knowledge
    distillation. Next, let’s discuss techniques that can accelerate the decoding
    process.`  `# Techniques for Accelerating Decoding    As we know, autoregressive
    models output one token at a time, where the next token being generated is a function
    of the input tokens and all the previously generated tokens. This imposes a sequential
    limitation as you have to wait for the current token to be generated before generating
    the next one. Can we bypass this limitation? Several recent techniques like *speculative
    decoding* and *parallel decoding* have been developed. Let’s examine them in detail.    ##
    Speculative Decoding    The concept behind speculative decoding is simple. A smaller
    model, called a draft model, is used to generate several subsequent candidate
    output tokens. Then, the main larger model is used to compute the conditional
    probabilities of the candidate output tokens at once, using them to decide which
    tokens to accept and which ones to reject. The more draft tokens accepted, the
    better the draft model.    [Figure 9-3](#speculative-decoding) depicts the speculative
    decoding process.  ![speculative-decoding](assets/dllm_0903.png)  ###### Figure
    9-3\. Speculative decoding in action    Two important metrics in speculative decoding
    are:    Token acceptance rate      This is the percentage of tokens generated
    by the draft model that are accepted. Typically, this does not reach 1, because
    if it did, there is no need to use the main larger model.      Decoding speedup      This
    refers to the reduction in latency between a model purely using autoregressive
    decoding versus one using speculative decoding.      ## Parallel Decoding    Can
    we generate more than one token at the same time? This can be done either using
    the same model (multi-token decoding) or multiple instances of the same model.    For
    the latter, we can control parallel generation through the prompt. For example,
    say you are writing an article about a tourist site, containing sections like
    Food, Stay, Safety Tips, etc. You can prompt the LLM to list the sections, marked
    with special tokens. These sections can then be generated in parallel, assuming
    the sections are fully independent of each other.    [Figure 9-4](#parallel-generation)
    depicts the workflow of a system that generates parts of the output in a parallel
    fashion.    Let’s now explore how the same model can generate multiple tokens
    at a time, called multi-token decoding. Several techniques have been proposed
    recently for multi-token decoding, one of the most promising being Medusa by [Cai
    et al.](https://oreil.ly/qT94i)    In Medusa, additional decoding heads are added
    to the model. These decoding heads represent subsequent tokens to be generated.
    For example, the standard decoding head is predicting the next (n + 1st) token
    in the sequence, and the additional decoding heads are predicting the n + 2nd,
    n + 3rd, and so on, tokens, respectively. Refer to the Medusa paper for more details
    on how this is implemented.    So far, we have learned techniques to accelerate
    the decoding process and to reduce compute. Next, let’s dive into quantization,
    a class of techniques to reduce the storage required by the model.  ![Parallel
    Generation](assets/dllm_0904.png)  ###### Figure 9-4\. Parallel decoding workflow    #
    Techniques for Reducing Storage Needs    In Chapters [5](ch05.html#chapter_utilizing_llms)
    and [6](ch06.html#llm-fine-tuning), we briefly touched upon quantization but promised
    to go into detail later. Let’s dive in!    The forward pass of a language model
    involves numbers representing inputs, weights, and activations. How are these
    numbers represented in memory?    Several types of numerical representation formats
    are available, like integer, floating point, etc. Typically, numbers in language
    models are represented in floating-point32 (FP32), also called single-precision
    floating point, which refers to a floating point number composed of 32 bits, or
    4 bytes.    A number represented in FP32 is composed of three parts:    *   A
    sign bit           *   The exponent (8 bits)           *   The mantissa/significand
    (23 bits)              For more details on how FP32 works, see [“Demystifying
    Floating Point Precision”](https://oreil.ly/uCYYl).    The maximum and minimum
    value that can be represented using FP32 is 3.4028237 × 10^(38) and 1.175494 ×
    10^(38), respectively. This is referred to as the range of values that can be
    represented by this format. Similarly, a number represented in float16 (FP16),
    also referred to as half-precision floating point, is composed of these three
    parts:    *   A sign bit           *   The exponent (5 bits)           *   The
    mantissa/significand (10 bits)              What happens when you take a number
    that is represented using FP32 and represent it in FP16? This amounts to a lossy
    conversion. In this case, both the range and the precision are impacted, because
    in FP16, 65,504 is the largest number you can represent, compared to 3.4 × 10^(38)
    for FP32\. The precision is impacted too, as the 32-bit version offers ~7 digits
    of precision, but the 16-bit version only offers ~3 digits of precision.    To
    prevent the massive loss in precision with FP16, bfloat16 (BF16), also called
    the brain floating point, was invented by Google Brain. In BF16, there are 8 digits
    for the exponent, and 7 bits for the mantissa. This keeps the range of numbers
    represented the same as that of float32 at the cost of reduced precision.    ######
    Note    Older GPUs like the NVIDIA T4 do not support BF16.    The process of converting
    representation of a number from a higher-precision format to a lower-precision
    format is called quantization. We can quantize 32-bit values to 8-bit integer
    formats as well. This reduces memory requirements by a factor of 4, at the cost
    of even more precision. With 8-bit quantization, we can represent numbers between
    –127 and 127, without any decimal point.    Integer quantization can be performed
    either symmetrically or asymmetrically.    ## Symmetric Quantization    In this
    setting, the *0* value in the original format is mapped to the *0* value in the
    integer representation. This means that when you quantize 0 represented in fp32
    to int8, the value remains 0.    The remaining values can be mapped using various
    techniques, the most common being absmax quantization. In this method, if we know
    or can estimate the range of numbers that need to be represented, we can take
    the absolute maximum of the range and map it to the largest number in int8 (127),
    while the negative of the absolute maximum is mapped to the smallest number in
    int8 (–127). The remaining numbers are mapped according to scale.    [Figure 9-5](#symmetric-quantization)
    depicts absmax quantization at work, quantizing a number represented in FP32 to
    int8.  ![Absmax quantization](assets/dllm_0905.png)  ###### Figure 9-5\. Absmax
    quantization    ## Asymmetric Quantization    In this setting, the *0* value in
    the original format is not guaranteed to be mapped to the *0* value in the integer
    representation.    A common technique is to take the minimum and maximum value
    that we need represented and map it to the minimum (–127) and maximum (127) values
    that can be represented in int8, respectively. For example, if the range of numbers
    we want represented is –23 to 87, then –23 is mapped to –127 and 87 is mapped
    to 127.    ###### Tip    If the range of numbers you want represented include
    outliers, they can play spoilsport. You can take care of outliers by clipping
    them, so that all outliers will be represented by the same maximum/minimum value.    How
    is quantization used in practice? Typically, quantization is applied after training.
    Both the model’s weights and activations can be quantized.    Quantizing weights
    is much easier than quantizing activations. Since we know the weights beforehand,
    we can calculate the range, outliers, scaling factors, etc. that are needed for
    the quantization algorithm.    For activations, depending on how much latency
    we can tolerate, we can either perform dynamic or static scaling. In dynamic scaling,
    statistics like range, outliers, etc. are calculated dynamically during inference
    at each layer. In static scaling, we take a reference calibration dataset to estimate
    the statistics. While this approach speeds up inference, it can result in more
    quantization errors.    For more details on implementing quantization, see [“A
    Visual Guide to Quantization”](https://oreil.ly/bpi3b) by Maarten Grootendorst.    #
    Summary    In this chapter, we discussed the causes of bottlenecks in LLM inference.
    We discussed a wide variety of techniques to make LLM inference more efficient,
    including techniques to reduce compute requirements, reduce storage requirements,
    and accelerate the decoding process. We explored techniques like caching, early
    exit, knowledge distillation, speculative and parallel decoding techniques, and
    quantization. In the next and final part of the book, we will explore LLM application
    paradigms and discuss the nuances involved in building full-fledged applications.`'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
